{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import random\n",
    "import heapq\n",
    "\n",
    "from gym.envs.registration import register\n",
    "# register(\n",
    "#    id='FrozenLakeNotSlippery-v0',\n",
    "#    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "#    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "#    max_episode_steps=100,\n",
    "#    reward_threshold=0.78, # optimum = .8196\n",
    "# )\n",
    "\n",
    "#env = gym.make('FrozenLake8x8-v0')\n",
    "env = gym.make('FrozenLake-v0')\n",
    "#env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.render()\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "class value_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(value_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 40)\n",
    "        #self.batch1 = nn.BatchNorm1d(40)\n",
    "        self.linear2 = nn.Linear(40, 40, bias=True)\n",
    "        #self.batch2 = nn.BatchNorm1d(40)\n",
    "        self.linear3 = nn.Linear(40, 40, bias=True)\n",
    "        #self.batch2 = nn.BatchNorm1d(40)\n",
    "        self.linear4 = nn.Linear(40, 40, bias=True)\n",
    "        #self.batch2 = nn.BatchNorm1d(40)\n",
    "        self.linear5 = nn.Linear(40, 1, bias=False)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "#         print(x.shape)\n",
    "#         print(x)\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "        x = x.view(-1,1)\n",
    "        x = F.sigmoid(self.linear1(x))\n",
    "        #x = F.softmax(self.linear2(x), dim=0)\n",
    "        #x = self.batch1(x)        \n",
    "        #x = self.dropout(x)\n",
    "        x = F.sigmoid(self.linear2(x))\n",
    "        x = F.sigmoid(self.linear3(x))\n",
    "        x = F.sigmoid(self.linear4(x))\n",
    "        #x = self.batch2(x)        \n",
    "        x = self.linear5(x)\n",
    "        x = x.view(-1,1)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class policy_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(policy_net, self).__init__()\n",
    "        #self.batch1 = nn.BatchNorm1d(1)\n",
    "        self.linear1 = nn.Linear(1, 64)\n",
    "        #self.batch2 = nn.BatchNorm1d(64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64, 64)\n",
    "        self.linear4 = nn.Linear(64, 64)\n",
    "        self.linear5 = nn.Linear(64, 4, bias=False)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        #print(x\n",
    "        x = x.view(-1,1)\n",
    "        #x = self.batch1(x)\n",
    "        x = F.tanh(self.linear1(x))\n",
    "        #x = self.batch2(x)\n",
    "        #x = self.dropout()\n",
    "        x = F.sigmoid(self.linear2(x))        \n",
    "#         x = F.tanh(self.linear3(x))        \n",
    "#         x = F.tanh(self.linear4(x))        \n",
    "        x = self.linear5(x)\n",
    "        x = x.view(-1,4)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'log_prob','action_prob','log_action_prob', 'next_state', 'reward','entropy_impact' ,'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    \n",
    "    \n",
    "class ReplayMemoryNoReplacement(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.h = []\n",
    "        \n",
    "    def push(self, *args):\n",
    "        random_index = random.random()\n",
    "        heapq.heappush(self.h, (random_index, Transition(*args)))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "    \n",
    "        result = []\n",
    "        for i in range(batch_size):            \n",
    "            result.append(heapq.heappop(self.h)[1])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.h)\n",
    "\n",
    "class ReplayMemoryNew(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.h = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, *args):\n",
    "        tran = Transition(*args)\n",
    "        self.push_transition(tran)\n",
    "\n",
    "    def push_transition(self, tran):\n",
    "        if self.capacity <= len(self.h):\n",
    "            heapq.heappop(self.h)\n",
    "        random_index = random.random()\n",
    "        heapq.heappush(self.h, (random_index, tran))\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        result = []\n",
    "        for i in range(batch_size):\n",
    "            el = heapq.heappop(self.h)[1]\n",
    "            result.append(el)            \n",
    "            heapq.heappush(self.h, (random.random(), el))\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_v_table():\n",
    "    for i in range(16):\n",
    "        st = np.array(get_state_repr(i))\n",
    "        st = np.expand_dims(st, axis=0)\n",
    "        v_net.eval()\n",
    "        action_probs = v_net(FloatTensor(st))\n",
    "        #action_probs = F.softmax(action_probs, dim=1)\n",
    "        outp = \" state (\" +str(i) + \") \"\n",
    "        n = 0\n",
    "        for tensr in action_probs:\n",
    "            for cell in tensr:\n",
    "                outp = outp + \" A[\" + str(n) + \"]:(\" + str(cell.item()) + \")\"\n",
    "                n += 1\n",
    "        print(outp)\n",
    "\n",
    "def print_pi_table():\n",
    "    for i in range(16):\n",
    "        st = np.array(get_state_repr(i))\n",
    "        st = np.expand_dims(st, axis=0)\n",
    "        pi_net.eval()\n",
    "        action_probs = pi_net(FloatTensor(st))\n",
    "        action_probs = F.softmax(action_probs, dim=1)\n",
    "        outp = \" state (\" +str(i) + \") \"\n",
    "        n = 0\n",
    "        for tensr in action_probs:\n",
    "            for cell in tensr:\n",
    "                outp = outp + \" A[\" + str(n) + \"]:(\" + str(cell.item()) + \")\"\n",
    "                n += 1\n",
    "        print(outp)\n",
    "        \n",
    "def get_state_repr(state_idx):\n",
    "    return state_idx * 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1, out_features=40, bias=True)\n",
      "Linear(in_features=40, out_features=40, bias=True)\n",
      "Linear(in_features=40, out_features=40, bias=True)\n",
      "Linear(in_features=40, out_features=40, bias=True)\n",
      "Linear(in_features=40, out_features=1, bias=False)\n",
      "Linear(in_features=1, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=64, bias=True)\n",
      "Linear(in_features=64, out_features=4, bias=False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "random.seed(1999)\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "# custom weights initialization \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print classname\n",
    "    #print q_net\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.01, 0.02)\n",
    "        #if not m.bias is None:\n",
    "        #    m.bias.data.normal_(0.1, 0.02)\n",
    "        #m.weight.data.uniform_(0.0, 0.02)        \n",
    "        #m.weight.data.fill_(0.01)\n",
    "        if not m.bias is None:\n",
    "            m.bias.data.fill_(0.0)\n",
    "        print m\n",
    "        \n",
    "\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 300\n",
    "GAMMA = 0.99\n",
    "TARGET_UPDATE = 1000\n",
    "PRINT_OUT_TIMES = 1000\n",
    "ENTROPY_REDUCTION_STEPS = 100000.0\n",
    "NUM_EPISODES =           1000000\n",
    "#NUM_STEPS_VALUE_FUNCTION_LEARNS = NUM_EPISODES\n",
    "NUM_STEPS_VALUE_FUNCTION_LEARNS = (ENTROPY_REDUCTION_STEPS * 1)\n",
    "\n",
    "v_net = value_net()\n",
    "v_net.apply(weights_init)\n",
    "v_net.to(device)\n",
    "target_v_net = value_net()\n",
    "target_v_net.load_state_dict(v_net.state_dict())\n",
    "target_v_net.to(device)\n",
    "pi_net = policy_net()\n",
    "pi_net.apply(weights_init).to(device)\n",
    "\n",
    "# prepare for optimizer, merge both networks parameters\n",
    "\n",
    "# parameters = set()\n",
    "# for net_ in [v_net, pi_net]:\n",
    "#     parameters |= set(net_.parameters())\n",
    "\n",
    "#optimizer = optim.RMSprop(online_net.parameters(), lr=0.001)\n",
    "\n",
    "#optimizer = optim.Adam(parameters, lr=0.0001)\n",
    "\n",
    "\n",
    "v_optimizer = optim.Adam(v_net.parameters(), lr=0.0001)\n",
    "pi_optimizer =  optim.Adam(pi_net.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "#scheduler = StepLR(v_optimizer, step_size=10000, gamma=0.5)\n",
    "\n",
    "\n",
    "MEMORY_SIZE = 2000\n",
    "#memory = ReplayMemoryNoReplacement(MEMORY_SIZE)\n",
    "memory = ReplayMemoryNew(MEMORY_SIZE)\n",
    "#memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "value_loss_cum = []\n",
    "\n",
    "def get_expected_value_fixed(s):\n",
    "    r = 0\n",
    "    if s == 0:\n",
    "        r = 0.050\n",
    "    elif s == 1:\n",
    "        r = 0.092\n",
    "    elif s == 2:\n",
    "        r = 0.083\n",
    "    elif s == 3:\n",
    "        r = 0.1258\n",
    "    elif s == 4:\n",
    "        r = 0.1235\n",
    "    elif s == 5:\n",
    "        r = 0.0\n",
    "    elif s == 6:\n",
    "        r = 0.1421\n",
    "    elif s == 7:\n",
    "        r = 0.0\n",
    "    elif s == 8:\n",
    "        r = 0.203364819288\n",
    "    elif s == 9:\n",
    "        r = 0.349448651075\n",
    "    elif s == 10:\n",
    "        r = 0.393933832645\n",
    "    elif s == 11:\n",
    "        r = 0.0\n",
    "    elif s == 12:\n",
    "        r = 0.0\n",
    "    elif s == 13:\n",
    "        r = 0.565665841103\n",
    "    elif s == 14:\n",
    "        r = 0.99\n",
    "    elif s == 15:\n",
    "        r = 0.0\n",
    "    return r\n",
    "\n",
    "def optimize(k):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "        \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    final_mask = torch.tensor(tuple(map(lambda d: d is True,\n",
    "                                         batch.done)), device=device, dtype=torch.uint8).unsqueeze(1)\n",
    "    final_mask_list = [d for d in batch.done if d is True ]\n",
    "    # Compute states that are final.\n",
    "#     next_state_final_mask = torch.tensor(tuple(map(lambda d: (d) in [5,7,11,12,15],\n",
    "#                                           batch.next_state)), device=device, dtype=torch.uint8).unsqueeze(1) \n",
    "#     next_state_finak_list = [d for d in batch.next_state if d in [5,7,11,12,15] ]\n",
    "    \n",
    "    \n",
    "    # Unpack the parameters from the memory\n",
    "        \n",
    "    state_batch = FloatTensor(batch.state)\n",
    "    state_batch = state_batch.view(BATCH_SIZE, 1)\n",
    "    next_state_batch = FloatTensor(batch.next_state)\n",
    "    next_state_batch = next_state_batch.view(BATCH_SIZE, 1)    \n",
    "    action_batch = LongTensor(batch.action).view(BATCH_SIZE,1)    \n",
    "    reward_batch = Tensor(batch.reward).view(BATCH_SIZE,1)            \n",
    "    entropy_impact_batch = FloatTensor(batch.entropy_impact).view(BATCH_SIZE,1)\n",
    "    #log_prob_batch = torch.cat(batch.log_prob).view(BATCH_SIZE, 1)\n",
    "    #action_probs_batch = torch.cat(batch.action_prob).view(BATCH_SIZE,4)\n",
    "    #log_action_probs_batch = torch.cat(batch.log_action_prob).view(BATCH_SIZE,4)\n",
    "    \n",
    "    \n",
    "    #FIRST , calculate V(next_state)and backpropagate MSE on V\n",
    "    \n",
    "        \n",
    "    target_v_net.eval()\n",
    "    v_next = target_v_net(next_state_batch).detach()\n",
    "    #v_next[next_state_final_mask] = torch.zeros(len(next_state_finak_list), device=device).view(len(next_state_finak_list))\n",
    "    v_next[final_mask] = torch.zeros(len(final_mask_list), device=device).view(len(final_mask_list))\n",
    "    \n",
    "    \n",
    "    ##HACK FIXING expected value\n",
    "#     v_current_fixed = [get_expected_value_fixed(_st) for _st in batch.state]\n",
    "#     v_current_fixed = FloatTensor(v_current_fixed).view(BATCH_SIZE,1)    \n",
    "    ##HACK FIXING expected value\n",
    "    \n",
    "    ##HACK FIXING current value\n",
    "#     v_next_fixed = [get_expected_value_fixed(_st) for _st in batch.next_state]\n",
    "#     v_next_fixed = FloatTensor(v_next_fixed).view(BATCH_SIZE,1)    \n",
    "    #v_next = v_next_fixed\n",
    "    ##HACK FIXING current value\n",
    "    \n",
    "    \n",
    "    expected_value = reward_batch + v_next * GAMMA\n",
    "    \n",
    "    \n",
    "    ##HACK FIXING expected value\n",
    "    #expected_value = expected_value_fixed\n",
    "    ##HACK FIXING expected value\n",
    "    \n",
    "    \n",
    "    # calculate V(current_state)\n",
    "    if k <= NUM_STEPS_VALUE_FUNCTION_LEARNS:\n",
    "        v_net.train()\n",
    "    else:\n",
    "        v_net.eval()\n",
    "        \n",
    "    v_current = v_net(state_batch)\n",
    "    \n",
    "    # backpropagate:\n",
    "    value_loss = torch.sum((expected_value - v_current)** 2)\n",
    "    \n",
    "    if k <= NUM_STEPS_VALUE_FUNCTION_LEARNS:\n",
    "        v_optimizer.zero_grad()\n",
    "        #value_loss.backward(retain_graph=True) # keep graph for policy net optimizer\n",
    "        value_loss.backward() # keep graph for policy net optimizer\n",
    "        v_optimizer.step()\n",
    "        #scheduler.step()\n",
    "    \n",
    "    value_loss_cum.append(value_loss.item())\n",
    "    \n",
    "    v_current = v_current.detach()\n",
    "    \n",
    "    \n",
    "    ##HACK FIXING expected value\n",
    "    #v_current = v_current_fixed\n",
    "    ##HACK FIXING expected value\n",
    "    \n",
    "    \n",
    "    # SECOND, calculate gradient loss:\n",
    "    # H(X) = P(X) log ( P(X) )\n",
    "\n",
    "    # calculate the action probability\n",
    "    actions_distr = pi_net(state_batch)\n",
    "    actions_prob_batch = F.softmax(actions_distr, dim=1)\n",
    "    log_actions_prob_batch = F.log_softmax(actions_distr, dim=1)\n",
    "    \n",
    "    action_batch = action_batch\n",
    "    action_mask = FloatTensor(BATCH_SIZE, 4).zero_()    \n",
    "    action_mask.scatter_(1,action_batch,1) # This will have shape (BATCH_SIZE, 4), and its contents will be \n",
    "                                            # like : [[0,0,1,0],[1,0,0,0],...]\n",
    "    #log_prob_batch = log_actions_prob_batch.gather(1,action_batch)\n",
    "    log_prob_batch = torch.sum(log_actions_prob_batch * action_mask, dim=1).view(BATCH_SIZE,1) # sum up across rows (ending tensor is shape (BATCH_SIZE, 1))\n",
    "    \n",
    "    entropy = entropy_impact_batch * torch.sum(actions_prob_batch * log_actions_prob_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    policy_loss = torch.sum( -log_prob_batch * (expected_value - v_current) + entropy) \n",
    "    \n",
    "    pi_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    pi_optimizer.step()\n",
    "    \n",
    "    return policy_loss.item(), value_loss.item()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.2592,  0.2636,  0.2332,  0.2440]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2598,  0.2658,  0.2316,  0.2428]])\n",
      "On state=4, selected action=3\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.259172201157) A[1]:(0.263595551252) A[2]:(0.233213186264) A[3]:(0.244019031525)\n",
      " state (1)  A[0]:(0.259381532669) A[1]:(0.264406919479) A[2]:(0.23262527585) A[3]:(0.243586286902)\n",
      " state (2)  A[0]:(0.259546518326) A[1]:(0.265036821365) A[2]:(0.232174575329) A[3]:(0.243242040277)\n",
      " state (3)  A[0]:(0.259664624929) A[1]:(0.265478551388) A[2]:(0.231860592961) A[3]:(0.242996245623)\n",
      " state (4)  A[0]:(0.259751915932) A[1]:(0.265794813633) A[2]:(0.231637343764) A[3]:(0.242815881968)\n",
      " state (5)  A[0]:(0.259819746017) A[1]:(0.266033053398) A[2]:(0.231472760439) A[3]:(0.242674484849)\n",
      " state (6)  A[0]:(0.259872943163) A[1]:(0.266219079494) A[2]:(0.23134881258) A[3]:(0.242559149861)\n",
      " state (7)  A[0]:(0.259913891554) A[1]:(0.266367197037) A[2]:(0.23125462234) A[3]:(0.242464348674)\n",
      " state (8)  A[0]:(0.259944081306) A[1]:(0.266486138105) A[2]:(0.231182798743) A[3]:(0.242386996746)\n",
      " state (9)  A[0]:(0.259965151548) A[1]:(0.266582041979) A[2]:(0.231128022075) A[3]:(0.242324784398)\n",
      " state (10)  A[0]:(0.259978681803) A[1]:(0.266659528017) A[2]:(0.231086343527) A[3]:(0.242275491357)\n",
      " state (11)  A[0]:(0.259986102581) A[1]:(0.266722202301) A[2]:(0.231054708362) A[3]:(0.24223703146)\n",
      " state (12)  A[0]:(0.259988754988) A[1]:(0.266772925854) A[2]:(0.231030881405) A[3]:(0.242207452655)\n",
      " state (13)  A[0]:(0.259987801313) A[1]:(0.266813993454) A[2]:(0.231013089418) A[3]:(0.242185086012)\n",
      " state (14)  A[0]:(0.259984225035) A[1]:(0.266847223043) A[2]:(0.231000050902) A[3]:(0.242168545723)\n",
      " state (15)  A[0]:(0.259978741407) A[1]:(0.266874015331) A[2]:(0.230990678072) A[3]:(0.242156520486)\n",
      " state (0)  A[0]:(0.0961933806539)\n",
      " state (1)  A[0]:(0.0962013229728)\n",
      " state (2)  A[0]:(0.0962083637714)\n",
      " state (3)  A[0]:(0.0962141528726)\n",
      " state (4)  A[0]:(0.0962186530232)\n",
      " state (5)  A[0]:(0.0962221547961)\n",
      " state (6)  A[0]:(0.0962249040604)\n",
      " state (7)  A[0]:(0.0962270870805)\n",
      " state (8)  A[0]:(0.0962288528681)\n",
      " state (9)  A[0]:(0.096230328083)\n",
      " state (10)  A[0]:(0.0962315499783)\n",
      " state (11)  A[0]:(0.0962325930595)\n",
      " state (12)  A[0]:(0.096233509481)\n",
      " state (13)  A[0]:(0.0962342992425)\n",
      " state (14)  A[0]:(0.0962349697948)\n",
      " state (15)  A[0]:(0.0962355807424)\n",
      "Episode 0 finished after 2 . Running score: 0.0. Policy_loss: 1.0, Value_loss: 1.0. Times trained:               2. Times reached goal: 0.               Steps done: 2.\n",
      " state (0)  A[0]:(0.249586746097) A[1]:(0.250033169985) A[2]:(0.250190913677) A[3]:(0.250189185143)\n",
      " state (1)  A[0]:(0.249477624893) A[1]:(0.250021576881) A[2]:(0.250200897455) A[3]:(0.25029990077)\n",
      " state (2)  A[0]:(0.249430418015) A[1]:(0.250001877546) A[2]:(0.250215947628) A[3]:(0.250351756811)\n",
      " state (3)  A[0]:(0.249425560236) A[1]:(0.249970585108) A[2]:(0.250234216452) A[3]:(0.250369668007)\n",
      " state (4)  A[0]:(0.2494276613) A[1]:(0.249936178327) A[2]:(0.250251173973) A[3]:(0.250384986401)\n",
      " state (5)  A[0]:(0.249421358109) A[1]:(0.249907612801) A[2]:(0.25026473403) A[3]:(0.250406354666)\n",
      " state (6)  A[0]:(0.249404445291) A[1]:(0.249888494611) A[2]:(0.250274896622) A[3]:(0.250432133675)\n",
      " state (7)  A[0]:(0.249379351735) A[1]:(0.249878495932) A[2]:(0.25028270483) A[3]:(0.250459462404)\n",
      " state (8)  A[0]:(0.249348923564) A[1]:(0.249875620008) A[2]:(0.250289082527) A[3]:(0.250486373901)\n",
      " state (9)  A[0]:(0.249315574765) A[1]:(0.24987770617) A[2]:(0.250294744968) A[3]:(0.250512033701)\n",
      " state (10)  A[0]:(0.249280959368) A[1]:(0.249882891774) A[2]:(0.250299990177) A[3]:(0.250536173582)\n",
      " state (11)  A[0]:(0.24924620986) A[1]:(0.249889910221) A[2]:(0.250305056572) A[3]:(0.250558882952)\n",
      " state (12)  A[0]:(0.249212011695) A[1]:(0.249897763133) A[2]:(0.250309944153) A[3]:(0.250580251217)\n",
      " state (13)  A[0]:(0.24917884171) A[1]:(0.249905899167) A[2]:(0.250314712524) A[3]:(0.250600486994)\n",
      " state (14)  A[0]:(0.249147057533) A[1]:(0.249913975596) A[2]:(0.250319331884) A[3]:(0.250619679689)\n",
      " state (15)  A[0]:(0.249116703868) A[1]:(0.249921768904) A[2]:(0.250323683023) A[3]:(0.250637859106)\n",
      " state (0)  A[0]:(0.0869712084532)\n",
      " state (1)  A[0]:(0.0868363231421)\n",
      " state (2)  A[0]:(0.0867771357298)\n",
      " state (3)  A[0]:(0.0867590308189)\n",
      " state (4)  A[0]:(0.0867522060871)\n",
      " state (5)  A[0]:(0.0867484733462)\n",
      " state (6)  A[0]:(0.086745813489)\n",
      " state (7)  A[0]:(0.0867436379194)\n",
      " state (8)  A[0]:(0.0867417305708)\n",
      " state (9)  A[0]:(0.086740039289)\n",
      " state (10)  A[0]:(0.0867384821177)\n",
      " state (11)  A[0]:(0.0867370516062)\n",
      " state (12)  A[0]:(0.086735740304)\n",
      " state (13)  A[0]:(0.0867345109582)\n",
      " state (14)  A[0]:(0.0867333933711)\n",
      " state (15)  A[0]:(0.0867323428392)\n",
      "Episode 1000 finished after 19 . Running score: 0.0. Policy_loss: -100945.978381, Value_loss: 1.0529167993. Times trained:               7827. Times reached goal: 12.               Steps done: 7829.\n",
      " state (0)  A[0]:(0.250249147415) A[1]:(0.249513179064) A[2]:(0.250124335289) A[3]:(0.250113338232)\n",
      " state (1)  A[0]:(0.250269174576) A[1]:(0.249449595809) A[2]:(0.250177443027) A[3]:(0.250103801489)\n",
      " state (2)  A[0]:(0.250285357237) A[1]:(0.249410882592) A[2]:(0.250219136477) A[3]:(0.250084578991)\n",
      " state (3)  A[0]:(0.250289648771) A[1]:(0.249399453402) A[2]:(0.250243753195) A[3]:(0.250067174435)\n",
      " state (4)  A[0]:(0.250275939703) A[1]:(0.249405533075) A[2]:(0.250253111124) A[3]:(0.250065416098)\n",
      " state (5)  A[0]:(0.250244766474) A[1]:(0.249420464039) A[2]:(0.250252336264) A[3]:(0.250082403421)\n",
      " state (6)  A[0]:(0.250200361013) A[1]:(0.249439135194) A[2]:(0.250246107578) A[3]:(0.250114381313)\n",
      " state (7)  A[0]:(0.250147551298) A[1]:(0.249458894134) A[2]:(0.250237494707) A[3]:(0.25015604496)\n",
      " state (8)  A[0]:(0.250090181828) A[1]:(0.249478548765) A[2]:(0.250228315592) A[3]:(0.250202983618)\n",
      " state (9)  A[0]:(0.250030964613) A[1]:(0.249497532845) A[2]:(0.2502194345) A[3]:(0.25025203824)\n",
      " state (10)  A[0]:(0.249971821904) A[1]:(0.249515756965) A[2]:(0.25021135807) A[3]:(0.250301092863)\n",
      " state (11)  A[0]:(0.249913826585) A[1]:(0.249533191323) A[2]:(0.250204235315) A[3]:(0.250348776579)\n",
      " state (12)  A[0]:(0.249857723713) A[1]:(0.249549865723) A[2]:(0.250198125839) A[3]:(0.250394254923)\n",
      " state (13)  A[0]:(0.249803945422) A[1]:(0.249565899372) A[2]:(0.250193089247) A[3]:(0.25043708086)\n",
      " state (14)  A[0]:(0.249752670527) A[1]:(0.249581336975) A[2]:(0.250189006329) A[3]:(0.25047698617)\n",
      " state (15)  A[0]:(0.24970407784) A[1]:(0.249596208334) A[2]:(0.250185847282) A[3]:(0.250513881445)\n",
      " state (0)  A[0]:(0.0807769522071)\n",
      " state (1)  A[0]:(0.072969339788)\n",
      " state (2)  A[0]:(0.0704279765487)\n",
      " state (3)  A[0]:(0.0699785202742)\n",
      " state (4)  A[0]:(0.0698919147253)\n",
      " state (5)  A[0]:(0.0698742568493)\n",
      " state (6)  A[0]:(0.069870531559)\n",
      " state (7)  A[0]:(0.0698697268963)\n",
      " state (8)  A[0]:(0.0698695629835)\n",
      " state (9)  A[0]:(0.06986951828)\n",
      " state (10)  A[0]:(0.06986951828)\n",
      " state (11)  A[0]:(0.06986951828)\n",
      " state (12)  A[0]:(0.06986951828)\n",
      " state (13)  A[0]:(0.06986951828)\n",
      " state (14)  A[0]:(0.06986951828)\n",
      " state (15)  A[0]:(0.06986951828)\n",
      "Episode 2000 finished after 6 . Running score: 0.02. Policy_loss: -100160.273608, Value_loss: 1.30040702287. Times trained:               7858. Times reached goal: 11.               Steps done: 15687.\n",
      " state (0)  A[0]:(0.250031471252) A[1]:(0.249507769942) A[2]:(0.250166356564) A[3]:(0.250294357538)\n",
      " state (1)  A[0]:(0.250009000301) A[1]:(0.249570757151) A[2]:(0.250074476004) A[3]:(0.250345736742)\n",
      " state (2)  A[0]:(0.250005871058) A[1]:(0.24963529408) A[2]:(0.250018060207) A[3]:(0.250340789557)\n",
      " state (3)  A[0]:(0.250012397766) A[1]:(0.249705672264) A[2]:(0.249996066093) A[3]:(0.250285834074)\n",
      " state (4)  A[0]:(0.250010579824) A[1]:(0.249784708023) A[2]:(0.249992579222) A[3]:(0.250212132931)\n",
      " state (5)  A[0]:(0.249991908669) A[1]:(0.249872326851) A[2]:(0.249995201826) A[3]:(0.250140577555)\n",
      " state (6)  A[0]:(0.249956160784) A[1]:(0.249966755509) A[2]:(0.249997735023) A[3]:(0.250079363585)\n",
      " state (7)  A[0]:(0.249906674027) A[1]:(0.250065505505) A[2]:(0.249997869134) A[3]:(0.250029951334)\n",
      " state (8)  A[0]:(0.249847531319) A[1]:(0.250166267157) A[2]:(0.249995008111) A[3]:(0.249991163611)\n",
      " state (9)  A[0]:(0.249782323837) A[1]:(0.250267088413) A[2]:(0.249989405274) A[3]:(0.249961152673)\n",
      " state (10)  A[0]:(0.249713972211) A[1]:(0.250366508961) A[2]:(0.249981403351) A[3]:(0.249938070774)\n",
      " state (11)  A[0]:(0.249644652009) A[1]:(0.250463485718) A[2]:(0.249971449375) A[3]:(0.249920412898)\n",
      " state (12)  A[0]:(0.249575912952) A[1]:(0.250557303429) A[2]:(0.249959886074) A[3]:(0.249906897545)\n",
      " state (13)  A[0]:(0.249508813024) A[1]:(0.250647544861) A[2]:(0.249947071075) A[3]:(0.249896630645)\n",
      " state (14)  A[0]:(0.249444052577) A[1]:(0.250733911991) A[2]:(0.249933168292) A[3]:(0.249888837337)\n",
      " state (15)  A[0]:(0.249382138252) A[1]:(0.250816375017) A[2]:(0.249918490648) A[3]:(0.249882996082)\n",
      " state (0)  A[0]:(0.0790707319975)\n",
      " state (1)  A[0]:(0.0659843981266)\n",
      " state (2)  A[0]:(0.0642090216279)\n",
      " state (3)  A[0]:(0.0640756189823)\n",
      " state (4)  A[0]:(0.0640635043383)\n",
      " state (5)  A[0]:(0.064062282443)\n",
      " state (6)  A[0]:(0.0640621632338)\n",
      " state (7)  A[0]:(0.0640621483326)\n",
      " state (8)  A[0]:(0.0640621483326)\n",
      " state (9)  A[0]:(0.0640621483326)\n",
      " state (10)  A[0]:(0.0640621483326)\n",
      " state (11)  A[0]:(0.0640621483326)\n",
      " state (12)  A[0]:(0.0640621483326)\n",
      " state (13)  A[0]:(0.0640621483326)\n",
      " state (14)  A[0]:(0.0640621483326)\n",
      " state (15)  A[0]:(0.0640621483326)\n",
      "Episode 3000 finished after 23 . Running score: 0.01. Policy_loss: -98776.0394854, Value_loss: 1.02491467396. Times trained:               8017. Times reached goal: 17.               Steps done: 23704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250259190798) A[1]:(0.249478399754) A[2]:(0.250165313482) A[3]:(0.250097066164)\n",
      " state (1)  A[0]:(0.250386476517) A[1]:(0.24946436286) A[2]:(0.250065892935) A[3]:(0.250083267689)\n",
      " state (2)  A[0]:(0.250506669283) A[1]:(0.249422028661) A[2]:(0.250045269728) A[3]:(0.250025987625)\n",
      " state (3)  A[0]:(0.250571191311) A[1]:(0.249385923147) A[2]:(0.250055789948) A[3]:(0.249987065792)\n",
      " state (4)  A[0]:(0.250577270985) A[1]:(0.249366939068) A[2]:(0.250070154667) A[3]:(0.249985620379)\n",
      " state (5)  A[0]:(0.250543177128) A[1]:(0.249361112714) A[2]:(0.250082135201) A[3]:(0.250013530254)\n",
      " state (6)  A[0]:(0.250485748053) A[1]:(0.249363139272) A[2]:(0.25009137392) A[3]:(0.250059694052)\n",
      " state (7)  A[0]:(0.250416129827) A[1]:(0.249369427562) A[2]:(0.250098526478) A[3]:(0.250115871429)\n",
      " state (8)  A[0]:(0.250340938568) A[1]:(0.249377816916) A[2]:(0.250104278326) A[3]:(0.25017696619)\n",
      " state (9)  A[0]:(0.250263929367) A[1]:(0.24938711524) A[2]:(0.250109106302) A[3]:(0.250239908695)\n",
      " state (10)  A[0]:(0.250187188387) A[1]:(0.249396651983) A[2]:(0.250113248825) A[3]:(0.250302940607)\n",
      " state (11)  A[0]:(0.250111907721) A[1]:(0.249406114221) A[2]:(0.250116914511) A[3]:(0.250365108252)\n",
      " state (12)  A[0]:(0.250038713217) A[1]:(0.249415308237) A[2]:(0.250120222569) A[3]:(0.250425815582)\n",
      " state (13)  A[0]:(0.249968022108) A[1]:(0.249424099922) A[2]:(0.250123143196) A[3]:(0.250484734774)\n",
      " state (14)  A[0]:(0.249900043011) A[1]:(0.249432474375) A[2]:(0.250125795603) A[3]:(0.250541687012)\n",
      " state (15)  A[0]:(0.249834880233) A[1]:(0.249440386891) A[2]:(0.250128149986) A[3]:(0.250596553087)\n",
      " state (0)  A[0]:(0.0715191364288)\n",
      " state (1)  A[0]:(0.0565043725073)\n",
      " state (2)  A[0]:(0.0553273111582)\n",
      " state (3)  A[0]:(0.0552743636072)\n",
      " state (4)  A[0]:(0.0552713163197)\n",
      " state (5)  A[0]:(0.0552711188793)\n",
      " state (6)  A[0]:(0.0552711039782)\n",
      " state (7)  A[0]:(0.0552711114287)\n",
      " state (8)  A[0]:(0.0552711114287)\n",
      " state (9)  A[0]:(0.0552711114287)\n",
      " state (10)  A[0]:(0.0552711114287)\n",
      " state (11)  A[0]:(0.0552711114287)\n",
      " state (12)  A[0]:(0.0552711114287)\n",
      " state (13)  A[0]:(0.0552711114287)\n",
      " state (14)  A[0]:(0.0552711114287)\n",
      " state (15)  A[0]:(0.0552711114287)\n",
      "Episode 4000 finished after 5 . Running score: 0.0. Policy_loss: -98713.3149648, Value_loss: 1.02891774972. Times trained:               7636. Times reached goal: 15.               Steps done: 31340.\n",
      "action_dist \n",
      "tensor([[ 0.2495,  0.2503,  0.2502,  0.2500]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2497,  0.2502,  0.2502,  0.2499]])\n",
      "On state=1, selected action=0\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2502,  0.2503,  0.2496]])\n",
      "On state=1, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2501,  0.2505,  0.2491]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2500,  0.2505,  0.2490]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2499,  0.2504,  0.2491]])\n",
      "On state=1, selected action=1\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250928044319) A[1]:(0.249682128429) A[2]:(0.250462472439) A[3]:(0.248927310109)\n",
      " state (1)  A[0]:(0.250880867243) A[1]:(0.249631851912) A[2]:(0.250367611647) A[3]:(0.249119699001)\n",
      " state (2)  A[0]:(0.250978767872) A[1]:(0.249584063888) A[2]:(0.250380963087) A[3]:(0.249056220055)\n",
      " state (3)  A[0]:(0.251042664051) A[1]:(0.249564483762) A[2]:(0.250411957502) A[3]:(0.248980939388)\n",
      " state (4)  A[0]:(0.251059651375) A[1]:(0.24956806004) A[2]:(0.250443130732) A[3]:(0.24892911315)\n",
      " state (5)  A[0]:(0.251045376062) A[1]:(0.249585703015) A[2]:(0.250472277403) A[3]:(0.248896613717)\n",
      " state (6)  A[0]:(0.251012176275) A[1]:(0.249610990286) A[2]:(0.250498920679) A[3]:(0.248877853155)\n",
      " state (7)  A[0]:(0.250967741013) A[1]:(0.249640181661) A[2]:(0.250523149967) A[3]:(0.248868942261)\n",
      " state (8)  A[0]:(0.250916570425) A[1]:(0.249671161175) A[2]:(0.250545084476) A[3]:(0.248867124319)\n",
      " state (9)  A[0]:(0.25086158514) A[1]:(0.249702811241) A[2]:(0.250565052032) A[3]:(0.248870551586)\n",
      " state (10)  A[0]:(0.2508046031) A[1]:(0.249734431505) A[2]:(0.250583201647) A[3]:(0.248877808452)\n",
      " state (11)  A[0]:(0.250746876001) A[1]:(0.249765589833) A[2]:(0.250599712133) A[3]:(0.248887762427)\n",
      " state (12)  A[0]:(0.250689327717) A[1]:(0.249796077609) A[2]:(0.250614851713) A[3]:(0.248899698257)\n",
      " state (13)  A[0]:(0.250632584095) A[1]:(0.249825716019) A[2]:(0.250628709793) A[3]:(0.248912975192)\n",
      " state (14)  A[0]:(0.250577062368) A[1]:(0.249854400754) A[2]:(0.250641405582) A[3]:(0.248927071691)\n",
      " state (15)  A[0]:(0.250523120165) A[1]:(0.249882102013) A[2]:(0.250653088093) A[3]:(0.24894168973)\n",
      " state (0)  A[0]:(0.0579906478524)\n",
      " state (1)  A[0]:(0.0416258983314)\n",
      " state (2)  A[0]:(0.0407736487687)\n",
      " state (3)  A[0]:(0.0407495349646)\n",
      " state (4)  A[0]:(0.0407488457859)\n",
      " state (5)  A[0]:(0.0407488644123)\n",
      " state (6)  A[0]:(0.0407488681376)\n",
      " state (7)  A[0]:(0.0407488681376)\n",
      " state (8)  A[0]:(0.0407488755882)\n",
      " state (9)  A[0]:(0.0407488755882)\n",
      " state (10)  A[0]:(0.0407488755882)\n",
      " state (11)  A[0]:(0.0407488755882)\n",
      " state (12)  A[0]:(0.0407488755882)\n",
      " state (13)  A[0]:(0.0407488755882)\n",
      " state (14)  A[0]:(0.0407488755882)\n",
      " state (15)  A[0]:(0.0407488755882)\n",
      "Episode 5000 finished after 6 . Running score: 0.02. Policy_loss: -98097.2931636, Value_loss: 1.0254871264. Times trained:               7535. Times reached goal: 9.               Steps done: 38875.\n",
      " state (0)  A[0]:(0.250602900982) A[1]:(0.249342143536) A[2]:(0.250009208918) A[3]:(0.250045776367)\n",
      " state (1)  A[0]:(0.250592827797) A[1]:(0.249420374632) A[2]:(0.24996368587) A[3]:(0.250023156404)\n",
      " state (2)  A[0]:(0.250712394714) A[1]:(0.249390870333) A[2]:(0.249994412065) A[3]:(0.24990233779)\n",
      " state (3)  A[0]:(0.250766098499) A[1]:(0.249365910888) A[2]:(0.25002387166) A[3]:(0.249844118953)\n",
      " state (4)  A[0]:(0.250765115023) A[1]:(0.249351531267) A[2]:(0.250046104193) A[3]:(0.249837264419)\n",
      " state (5)  A[0]:(0.250735282898) A[1]:(0.249342963099) A[2]:(0.250063359737) A[3]:(0.249858409166)\n",
      " state (6)  A[0]:(0.25069218874) A[1]:(0.249336928129) A[2]:(0.250077605247) A[3]:(0.249893337488)\n",
      " state (7)  A[0]:(0.250643730164) A[1]:(0.249331772327) A[2]:(0.250089794397) A[3]:(0.24993468821)\n",
      " state (8)  A[0]:(0.250593692064) A[1]:(0.249326765537) A[2]:(0.250100672245) A[3]:(0.249978855252)\n",
      " state (9)  A[0]:(0.250543832779) A[1]:(0.249321609735) A[2]:(0.250110447407) A[3]:(0.250024080276)\n",
      " state (10)  A[0]:(0.250494986773) A[1]:(0.249316230416) A[2]:(0.250119358301) A[3]:(0.250069409609)\n",
      " state (11)  A[0]:(0.250447571278) A[1]:(0.249310687184) A[2]:(0.250127464533) A[3]:(0.250114321709)\n",
      " state (12)  A[0]:(0.250401765108) A[1]:(0.249304965138) A[2]:(0.250134855509) A[3]:(0.250158399343)\n",
      " state (13)  A[0]:(0.250357687473) A[1]:(0.249299183488) A[2]:(0.250141620636) A[3]:(0.250201523304)\n",
      " state (14)  A[0]:(0.250315397978) A[1]:(0.249293372035) A[2]:(0.250147789717) A[3]:(0.250243455172)\n",
      " state (15)  A[0]:(0.250274896622) A[1]:(0.249287590384) A[2]:(0.250153392553) A[3]:(0.250284135342)\n",
      " state (0)  A[0]:(0.0466823764145)\n",
      " state (1)  A[0]:(0.0355223491788)\n",
      " state (2)  A[0]:(0.0355654545128)\n",
      " state (3)  A[0]:(0.0356118641794)\n",
      " state (4)  A[0]:(0.0356196500361)\n",
      " state (5)  A[0]:(0.0356209613383)\n",
      " state (6)  A[0]:(0.0356211997569)\n",
      " state (7)  A[0]:(0.0356212407351)\n",
      " state (8)  A[0]:(0.0356212519109)\n",
      " state (9)  A[0]:(0.0356212519109)\n",
      " state (10)  A[0]:(0.0356212519109)\n",
      " state (11)  A[0]:(0.0356212519109)\n",
      " state (12)  A[0]:(0.0356212519109)\n",
      " state (13)  A[0]:(0.0356212519109)\n",
      " state (14)  A[0]:(0.0356212519109)\n",
      " state (15)  A[0]:(0.0356212519109)\n",
      "Episode 6000 finished after 15 . Running score: 0.0. Policy_loss: -97866.2347314, Value_loss: 1.02328875073. Times trained:               7783. Times reached goal: 15.               Steps done: 46658.\n",
      " state (0)  A[0]:(0.24926134944) A[1]:(0.249297112226) A[2]:(0.249773219228) A[3]:(0.251668274403)\n",
      " state (1)  A[0]:(0.249120056629) A[1]:(0.249061986804) A[2]:(0.250000834465) A[3]:(0.251817166805)\n",
      " state (2)  A[0]:(0.249132752419) A[1]:(0.248848050833) A[2]:(0.250316828489) A[3]:(0.251702398062)\n",
      " state (3)  A[0]:(0.249059230089) A[1]:(0.248688951135) A[2]:(0.250615298748) A[3]:(0.251636475325)\n",
      " state (4)  A[0]:(0.248939231038) A[1]:(0.248564958572) A[2]:(0.250889867544) A[3]:(0.251605927944)\n",
      " state (5)  A[0]:(0.248805835843) A[1]:(0.24846161902) A[2]:(0.251140981913) A[3]:(0.251591533422)\n",
      " state (6)  A[0]:(0.24867489934) A[1]:(0.248371586204) A[2]:(0.251369923353) A[3]:(0.251583635807)\n",
      " state (7)  A[0]:(0.248552799225) A[1]:(0.248291388154) A[2]:(0.251577794552) A[3]:(0.251578062773)\n",
      " state (8)  A[0]:(0.248441517353) A[1]:(0.248219370842) A[2]:(0.251765906811) A[3]:(0.25157314539)\n",
      " state (9)  A[0]:(0.248341292143) A[1]:(0.248154580593) A[2]:(0.251935690641) A[3]:(0.251568436623)\n",
      " state (10)  A[0]:(0.248251438141) A[1]:(0.248096317053) A[2]:(0.252088487148) A[3]:(0.251563757658)\n",
      " state (11)  A[0]:(0.248171061277) A[1]:(0.248043999076) A[2]:(0.252225726843) A[3]:(0.251559197903)\n",
      " state (12)  A[0]:(0.24809923768) A[1]:(0.247997060418) A[2]:(0.252348810434) A[3]:(0.251554876566)\n",
      " state (13)  A[0]:(0.248035013676) A[1]:(0.247955024242) A[2]:(0.252459079027) A[3]:(0.251550883055)\n",
      " state (14)  A[0]:(0.247977554798) A[1]:(0.247917354107) A[2]:(0.252557784319) A[3]:(0.251547276974)\n",
      " state (15)  A[0]:(0.247926071286) A[1]:(0.247883677483) A[2]:(0.252646118402) A[3]:(0.251544088125)\n",
      " state (0)  A[0]:(0.0525776781142)\n",
      " state (1)  A[0]:(0.0436320081353)\n",
      " state (2)  A[0]:(0.0443677231669)\n",
      " state (3)  A[0]:(0.0445617288351)\n",
      " state (4)  A[0]:(0.0446061342955)\n",
      " state (5)  A[0]:(0.044617112726)\n",
      " state (6)  A[0]:(0.0446200147271)\n",
      " state (7)  A[0]:(0.044620834291)\n",
      " state (8)  A[0]:(0.0446210615337)\n",
      " state (9)  A[0]:(0.0446211397648)\n",
      " state (10)  A[0]:(0.0446211583912)\n",
      " state (11)  A[0]:(0.0446211695671)\n",
      " state (12)  A[0]:(0.0446211770177)\n",
      " state (13)  A[0]:(0.0446211770177)\n",
      " state (14)  A[0]:(0.0446211770177)\n",
      " state (15)  A[0]:(0.0446211770177)\n",
      "Episode 7000 finished after 21 . Running score: 0.03. Policy_loss: -97843.9349446, Value_loss: 1.02253706893. Times trained:               7652. Times reached goal: 13.               Steps done: 54310.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250934064388) A[1]:(0.249246135354) A[2]:(0.250331312418) A[3]:(0.24948848784)\n",
      " state (1)  A[0]:(0.25082975626) A[1]:(0.249211832881) A[2]:(0.250146478415) A[3]:(0.249811977148)\n",
      " state (2)  A[0]:(0.250874042511) A[1]:(0.24917639792) A[2]:(0.250079900026) A[3]:(0.249869689345)\n",
      " state (3)  A[0]:(0.250786632299) A[1]:(0.249189332128) A[2]:(0.250025063753) A[3]:(0.249998942018)\n",
      " state (4)  A[0]:(0.250628888607) A[1]:(0.249228030443) A[2]:(0.249975755811) A[3]:(0.25016734004)\n",
      " state (5)  A[0]:(0.25044515729) A[1]:(0.249276444316) A[2]:(0.249930068851) A[3]:(0.250348299742)\n",
      " state (6)  A[0]:(0.25025755167) A[1]:(0.249326631427) A[2]:(0.249887287617) A[3]:(0.250528514385)\n",
      " state (7)  A[0]:(0.250075787306) A[1]:(0.249375030398) A[2]:(0.249847114086) A[3]:(0.250702023506)\n",
      " state (8)  A[0]:(0.249903991818) A[1]:(0.249420195818) A[2]:(0.249809443951) A[3]:(0.250866413116)\n",
      " state (9)  A[0]:(0.249743595719) A[1]:(0.24946154654) A[2]:(0.249774098396) A[3]:(0.251020729542)\n",
      " state (10)  A[0]:(0.24959513545) A[1]:(0.24949900806) A[2]:(0.24974103272) A[3]:(0.251164853573)\n",
      " state (11)  A[0]:(0.249458417296) A[1]:(0.249532580376) A[2]:(0.249710097909) A[3]:(0.251298904419)\n",
      " state (12)  A[0]:(0.249333038926) A[1]:(0.249562472105) A[2]:(0.249681219459) A[3]:(0.251423209906)\n",
      " state (13)  A[0]:(0.249218568206) A[1]:(0.249588921666) A[2]:(0.249654322863) A[3]:(0.25153824687)\n",
      " state (14)  A[0]:(0.24911428988) A[1]:(0.249612078071) A[2]:(0.249629244208) A[3]:(0.251644432545)\n",
      " state (15)  A[0]:(0.249019578099) A[1]:(0.249632209539) A[2]:(0.24960590899) A[3]:(0.251742273569)\n",
      " state (0)  A[0]:(0.0438920073211)\n",
      " state (1)  A[0]:(0.0345366261899)\n",
      " state (2)  A[0]:(0.0358636751771)\n",
      " state (3)  A[0]:(0.0363491810858)\n",
      " state (4)  A[0]:(0.0365214794874)\n",
      " state (5)  A[0]:(0.0365899354219)\n",
      " state (6)  A[0]:(0.0366199277341)\n",
      " state (7)  A[0]:(0.0366339795291)\n",
      " state (8)  A[0]:(0.0366408452392)\n",
      " state (9)  A[0]:(0.036644294858)\n",
      " state (10)  A[0]:(0.036646053195)\n",
      " state (11)  A[0]:(0.0366469621658)\n",
      " state (12)  A[0]:(0.0366474352777)\n",
      " state (13)  A[0]:(0.0366476923227)\n",
      " state (14)  A[0]:(0.0366478227079)\n",
      " state (15)  A[0]:(0.0366478860378)\n",
      "Episode 8000 finished after 4 . Running score: 0.02. Policy_loss: -95828.7220134, Value_loss: 1.00749672769. Times trained:               7572. Times reached goal: 16.               Steps done: 61882.\n",
      " state (0)  A[0]:(0.250577777624) A[1]:(0.249365285039) A[2]:(0.249824225903) A[3]:(0.250232666731)\n",
      " state (1)  A[0]:(0.250468462706) A[1]:(0.249567449093) A[2]:(0.249590903521) A[3]:(0.250373154879)\n",
      " state (2)  A[0]:(0.250533789396) A[1]:(0.24970805645) A[2]:(0.249571174383) A[3]:(0.250186920166)\n",
      " state (3)  A[0]:(0.250470012426) A[1]:(0.249859571457) A[2]:(0.249577820301) A[3]:(0.250092595816)\n",
      " state (4)  A[0]:(0.250338047743) A[1]:(0.250012069941) A[2]:(0.249595969915) A[3]:(0.250053882599)\n",
      " state (5)  A[0]:(0.25018170476) A[1]:(0.250157624483) A[2]:(0.249618545175) A[3]:(0.250042170286)\n",
      " state (6)  A[0]:(0.250023663044) A[1]:(0.250292390585) A[2]:(0.249642267823) A[3]:(0.250041693449)\n",
      " state (7)  A[0]:(0.249874040484) A[1]:(0.250415146351) A[2]:(0.24966584146) A[3]:(0.250044941902)\n",
      " state (8)  A[0]:(0.249736666679) A[1]:(0.250526100397) A[2]:(0.249688625336) A[3]:(0.250048577785)\n",
      " state (9)  A[0]:(0.249612405896) A[1]:(0.250625997782) A[2]:(0.24971036613) A[3]:(0.25005120039)\n",
      " state (10)  A[0]:(0.249500900507) A[1]:(0.250715762377) A[2]:(0.249730929732) A[3]:(0.250052392483)\n",
      " state (11)  A[0]:(0.249401301146) A[1]:(0.250796377659) A[2]:(0.249750256538) A[3]:(0.250052064657)\n",
      " state (12)  A[0]:(0.249312564731) A[1]:(0.2508687675) A[2]:(0.249768361449) A[3]:(0.250050336123)\n",
      " state (13)  A[0]:(0.249233663082) A[1]:(0.25093370676) A[2]:(0.249785229564) A[3]:(0.25004735589)\n",
      " state (14)  A[0]:(0.249163672328) A[1]:(0.250992029905) A[2]:(0.249800935388) A[3]:(0.250043332577)\n",
      " state (15)  A[0]:(0.249101668596) A[1]:(0.251044392586) A[2]:(0.249815538526) A[3]:(0.250038444996)\n",
      " state (0)  A[0]:(0.0369019918144)\n",
      " state (1)  A[0]:(0.0283120945096)\n",
      " state (2)  A[0]:(0.0300569981337)\n",
      " state (3)  A[0]:(0.0308449491858)\n",
      " state (4)  A[0]:(0.0312155298889)\n",
      " state (5)  A[0]:(0.0314267538488)\n",
      " state (6)  A[0]:(0.0315704159439)\n",
      " state (7)  A[0]:(0.0316800586879)\n",
      " state (8)  A[0]:(0.0317691750824)\n",
      " state (9)  A[0]:(0.0318439230323)\n",
      " state (10)  A[0]:(0.0319075509906)\n",
      " state (11)  A[0]:(0.0319620594382)\n",
      " state (12)  A[0]:(0.0320088341832)\n",
      " state (13)  A[0]:(0.0320489667356)\n",
      " state (14)  A[0]:(0.0320833399892)\n",
      " state (15)  A[0]:(0.0321127399802)\n",
      "Episode 9000 finished after 9 . Running score: 0.03. Policy_loss: -95805.3391262, Value_loss: 1.00668721294. Times trained:               7330. Times reached goal: 13.               Steps done: 69212.\n",
      "action_dist \n",
      "tensor([[ 0.2497,  0.2510,  0.2497,  0.2497]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2505,  0.2501,  0.2495]])\n",
      "On state=4, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2498,  0.2510,  0.2497,  0.2495]])\n",
      "On state=0, selected action=2\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2498,  0.2510,  0.2497,  0.2495]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2498,  0.2509,  0.2497,  0.2496]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2508,  0.2497,  0.2496]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2505,  0.2497,  0.2499]])\n",
      "On state=1, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2505,  0.2497,  0.2497]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2504,  0.2498,  0.2498]])\n",
      "On state=0, selected action=2\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2502,  0.2498,  0.2498]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2501,  0.2498,  0.2498]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2498,  0.2498,  0.2501]])\n",
      "On state=1, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2499,  0.2499,  0.2499]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2498,  0.2499,  0.2499]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2498,  0.2499,  0.2500]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2496,  0.2499,  0.2502]])\n",
      "On state=1, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2496,  0.2499,  0.2503]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2495,  0.2500,  0.2501]])\n",
      "On state=2, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2496,  0.2499,  0.2503]])\n",
      "On state=1, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2497,  0.2500,  0.2502]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2496,  0.2499,  0.2504]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2497,  0.2499,  0.2502]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2498,  0.2499,  0.2503]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2498,  0.2499,  0.2503]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2497,  0.2498,  0.2504]])\n",
      "On state=1, selected action=2\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2497,  0.2500,  0.2502]])\n",
      "On state=2, selected action=1\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2497,  0.2501,  0.2501]])\n",
      "On state=3, selected action=1\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.249932959676) A[1]:(0.249926045537) A[2]:(0.249838262796) A[3]:(0.250302702188)\n",
      " state (1)  A[0]:(0.249923169613) A[1]:(0.24981443584) A[2]:(0.249814465642) A[3]:(0.250447958708)\n",
      " state (2)  A[0]:(0.250096738338) A[1]:(0.249702304602) A[2]:(0.249950870872) A[3]:(0.250250101089)\n",
      " state (3)  A[0]:(0.25013166666) A[1]:(0.24963375926) A[2]:(0.250096350908) A[3]:(0.250138163567)\n",
      " state (4)  A[0]:(0.250088512897) A[1]:(0.249591454864) A[2]:(0.2502399683) A[3]:(0.25008007884)\n",
      " state (5)  A[0]:(0.250007987022) A[1]:(0.249562352896) A[2]:(0.25037753582) A[3]:(0.250052154064)\n",
      " state (6)  A[0]:(0.249912187457) A[1]:(0.249538972974) A[2]:(0.25050792098) A[3]:(0.250040918589)\n",
      " state (7)  A[0]:(0.24981226027) A[1]:(0.249517425895) A[2]:(0.250631093979) A[3]:(0.250039190054)\n",
      " state (8)  A[0]:(0.24971356988) A[1]:(0.249495789409) A[2]:(0.250747382641) A[3]:(0.250043272972)\n",
      " state (9)  A[0]:(0.249618574977) A[1]:(0.249473229051) A[2]:(0.250857144594) A[3]:(0.250051051378)\n",
      " state (10)  A[0]:(0.249528303742) A[1]:(0.249449461699) A[2]:(0.250960797071) A[3]:(0.250061482191)\n",
      " state (11)  A[0]:(0.2494430691) A[1]:(0.24942445755) A[2]:(0.251058638096) A[3]:(0.250073850155)\n",
      " state (12)  A[0]:(0.249362885952) A[1]:(0.249398395419) A[2]:(0.251151055098) A[3]:(0.250087708235)\n",
      " state (13)  A[0]:(0.249287545681) A[1]:(0.249371439219) A[2]:(0.251238286495) A[3]:(0.250102758408)\n",
      " state (14)  A[0]:(0.249216839671) A[1]:(0.249343812466) A[2]:(0.251320630312) A[3]:(0.25011870265)\n",
      " state (15)  A[0]:(0.249150544405) A[1]:(0.249315798283) A[2]:(0.251398354769) A[3]:(0.250135362148)\n",
      " state (0)  A[0]:(0.0340980775654)\n",
      " state (1)  A[0]:(0.0259083546698)\n",
      " state (2)  A[0]:(0.0275882817805)\n",
      " state (3)  A[0]:(0.0286631993949)\n",
      " state (4)  A[0]:(0.0294100493193)\n",
      " state (5)  A[0]:(0.0299953743815)\n",
      " state (6)  A[0]:(0.0304884761572)\n",
      " state (7)  A[0]:(0.0309175625443)\n",
      " state (8)  A[0]:(0.031295184046)\n",
      " state (9)  A[0]:(0.031627882272)\n",
      " state (10)  A[0]:(0.0319200307131)\n",
      " state (11)  A[0]:(0.0321752466261)\n",
      " state (12)  A[0]:(0.0323969088495)\n",
      " state (13)  A[0]:(0.032588340342)\n",
      " state (14)  A[0]:(0.0327528007329)\n",
      " state (15)  A[0]:(0.032893422991)\n",
      "Episode 10000 finished after 27 . Running score: 0.01. Policy_loss: -95791.7346355, Value_loss: 1.00494166216. Times trained:               7572. Times reached goal: 15.               Steps done: 76784.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.249867156148) A[1]:(0.250306338072) A[2]:(0.249898865819) A[3]:(0.24992762506)\n",
      " state (1)  A[0]:(0.24988090992) A[1]:(0.250226944685) A[2]:(0.249745115638) A[3]:(0.250147044659)\n",
      " state (2)  A[0]:(0.249957293272) A[1]:(0.250244557858) A[2]:(0.249736219645) A[3]:(0.250061929226)\n",
      " state (3)  A[0]:(0.24986115098) A[1]:(0.250331073999) A[2]:(0.249725759029) A[3]:(0.250081956387)\n",
      " state (4)  A[0]:(0.249700456858) A[1]:(0.25044220686) A[2]:(0.249712392688) A[3]:(0.250144988298)\n",
      " state (5)  A[0]:(0.249523773789) A[1]:(0.250558227301) A[2]:(0.249697461724) A[3]:(0.250220477581)\n",
      " state (6)  A[0]:(0.249348968267) A[1]:(0.250671893358) A[2]:(0.249682247639) A[3]:(0.250296860933)\n",
      " state (7)  A[0]:(0.249181896448) A[1]:(0.250780642033) A[2]:(0.249667301774) A[3]:(0.250370115042)\n",
      " state (8)  A[0]:(0.249024271965) A[1]:(0.250883698463) A[2]:(0.249652981758) A[3]:(0.250438988209)\n",
      " state (9)  A[0]:(0.248876422644) A[1]:(0.250980943441) A[2]:(0.249639421701) A[3]:(0.250503242016)\n",
      " state (10)  A[0]:(0.248738050461) A[1]:(0.251072406769) A[2]:(0.249626666307) A[3]:(0.250562876463)\n",
      " state (11)  A[0]:(0.248608797789) A[1]:(0.251158297062) A[2]:(0.249614715576) A[3]:(0.250618159771)\n",
      " state (12)  A[0]:(0.248488232493) A[1]:(0.251238852739) A[2]:(0.249603614211) A[3]:(0.250669330359)\n",
      " state (13)  A[0]:(0.248375803232) A[1]:(0.251314282417) A[2]:(0.249593257904) A[3]:(0.250716656446)\n",
      " state (14)  A[0]:(0.248271018267) A[1]:(0.251384854317) A[2]:(0.249583631754) A[3]:(0.250760436058)\n",
      " state (15)  A[0]:(0.24817340076) A[1]:(0.251450866461) A[2]:(0.249574750662) A[3]:(0.250800937414)\n",
      " state (0)  A[0]:(0.0290346704423)\n",
      " state (1)  A[0]:(0.0211244150996)\n",
      " state (2)  A[0]:(0.0224918238819)\n",
      " state (3)  A[0]:(0.0237836912274)\n",
      " state (4)  A[0]:(0.0250635556877)\n",
      " state (5)  A[0]:(0.0263176523149)\n",
      " state (6)  A[0]:(0.0275208130479)\n",
      " state (7)  A[0]:(0.0286493487656)\n",
      " state (8)  A[0]:(0.0296854935586)\n",
      " state (9)  A[0]:(0.0306186974049)\n",
      " state (10)  A[0]:(0.0314452834427)\n",
      " state (11)  A[0]:(0.0321671925485)\n",
      " state (12)  A[0]:(0.0327903963625)\n",
      " state (13)  A[0]:(0.0333232879639)\n",
      " state (14)  A[0]:(0.0337754189968)\n",
      " state (15)  A[0]:(0.0341566763818)\n",
      "Episode 11000 finished after 2 . Running score: 0.01. Policy_loss: -95780.7659644, Value_loss: 1.28573098606. Times trained:               7601. Times reached goal: 16.               Steps done: 84385.\n",
      " state (0)  A[0]:(0.250230818987) A[1]:(0.249510020018) A[2]:(0.250370830297) A[3]:(0.249888375401)\n",
      " state (1)  A[0]:(0.250132292509) A[1]:(0.249557688832) A[2]:(0.250139206648) A[3]:(0.250170767307)\n",
      " state (2)  A[0]:(0.250188082457) A[1]:(0.249588623643) A[2]:(0.250157952309) A[3]:(0.250065386295)\n",
      " state (3)  A[0]:(0.250125348568) A[1]:(0.249648541212) A[2]:(0.250189125538) A[3]:(0.25003695488)\n",
      " state (4)  A[0]:(0.250012725592) A[1]:(0.249719277024) A[2]:(0.250222027302) A[3]:(0.250045984983)\n",
      " state (5)  A[0]:(0.249886736274) A[1]:(0.249790042639) A[2]:(0.250253230333) A[3]:(0.250069975853)\n",
      " state (6)  A[0]:(0.249763399363) A[1]:(0.249856218696) A[2]:(0.250281780958) A[3]:(0.250098645687)\n",
      " state (7)  A[0]:(0.249648690224) A[1]:(0.249916300178) A[2]:(0.25030747056) A[3]:(0.250127583742)\n",
      " state (8)  A[0]:(0.24954430759) A[1]:(0.249970182776) A[2]:(0.250330448151) A[3]:(0.250155061483)\n",
      " state (9)  A[0]:(0.249450266361) A[1]:(0.250018298626) A[2]:(0.250350952148) A[3]:(0.250180453062)\n",
      " state (10)  A[0]:(0.249365925789) A[1]:(0.250061154366) A[2]:(0.250369220972) A[3]:(0.250203698874)\n",
      " state (11)  A[0]:(0.249290406704) A[1]:(0.250099331141) A[2]:(0.25038549304) A[3]:(0.250224769115)\n",
      " state (12)  A[0]:(0.249222874641) A[1]:(0.25013333559) A[2]:(0.250399976969) A[3]:(0.250243842602)\n",
      " state (13)  A[0]:(0.249162510037) A[1]:(0.250163584948) A[2]:(0.250412851572) A[3]:(0.250261038542)\n",
      " state (14)  A[0]:(0.249108612537) A[1]:(0.250190556049) A[2]:(0.250424325466) A[3]:(0.250276565552)\n",
      " state (15)  A[0]:(0.249060481787) A[1]:(0.250214517117) A[2]:(0.25043451786) A[3]:(0.250290483236)\n",
      " state (0)  A[0]:(0.0279965959489)\n",
      " state (1)  A[0]:(0.0212136395276)\n",
      " state (2)  A[0]:(0.0230177566409)\n",
      " state (3)  A[0]:(0.024821639061)\n",
      " state (4)  A[0]:(0.0267673060298)\n",
      " state (5)  A[0]:(0.0288201868534)\n",
      " state (6)  A[0]:(0.0309142507613)\n",
      " state (7)  A[0]:(0.032979208976)\n",
      " state (8)  A[0]:(0.0349524356425)\n",
      " state (9)  A[0]:(0.036785915494)\n",
      " state (10)  A[0]:(0.0384489297867)\n",
      " state (11)  A[0]:(0.0399271771312)\n",
      " state (12)  A[0]:(0.0412196442485)\n",
      " state (13)  A[0]:(0.0423347949982)\n",
      " state (14)  A[0]:(0.0432868115604)\n",
      " state (15)  A[0]:(0.0440928637981)\n",
      "Episode 12000 finished after 10 . Running score: 0.03. Policy_loss: -95775.2286331, Value_loss: 1.0060563154. Times trained:               7618. Times reached goal: 19.               Steps done: 92003.\n",
      " state (0)  A[0]:(0.249987527728) A[1]:(0.249888777733) A[2]:(0.250182062387) A[3]:(0.249941632152)\n",
      " state (1)  A[0]:(0.249908655882) A[1]:(0.249956846237) A[2]:(0.249986603856) A[3]:(0.250147908926)\n",
      " state (2)  A[0]:(0.249920338392) A[1]:(0.250008136034) A[2]:(0.250052839518) A[3]:(0.250018626451)\n",
      " state (3)  A[0]:(0.249819397926) A[1]:(0.250081777573) A[2]:(0.250112891197) A[3]:(0.249985992908)\n",
      " state (4)  A[0]:(0.249682426453) A[1]:(0.250161021948) A[2]:(0.250163018703) A[3]:(0.249993532896)\n",
      " state (5)  A[0]:(0.249542802572) A[1]:(0.250237047672) A[2]:(0.250205039978) A[3]:(0.250015109777)\n",
      " state (6)  A[0]:(0.249412417412) A[1]:(0.25030657649) A[2]:(0.250240772963) A[3]:(0.250040233135)\n",
      " state (7)  A[0]:(0.249294728041) A[1]:(0.250368952751) A[2]:(0.250271469355) A[3]:(0.250064879656)\n",
      " state (8)  A[0]:(0.249189987779) A[1]:(0.250424355268) A[2]:(0.250298023224) A[3]:(0.25008764863)\n",
      " state (9)  A[0]:(0.249097391963) A[1]:(0.250473350286) A[2]:(0.250321090221) A[3]:(0.250108122826)\n",
      " state (10)  A[0]:(0.249015927315) A[1]:(0.250516563654) A[2]:(0.250341176987) A[3]:(0.25012627244)\n",
      " state (11)  A[0]:(0.248944476247) A[1]:(0.25055462122) A[2]:(0.25035867095) A[3]:(0.250142276287)\n",
      " state (12)  A[0]:(0.248881861567) A[1]:(0.250587999821) A[2]:(0.250373870134) A[3]:(0.250156223774)\n",
      " state (13)  A[0]:(0.248827174306) A[1]:(0.250617325306) A[2]:(0.250387102365) A[3]:(0.250168442726)\n",
      " state (14)  A[0]:(0.248779445887) A[1]:(0.250642985106) A[2]:(0.250398546457) A[3]:(0.250179022551)\n",
      " state (15)  A[0]:(0.248737856746) A[1]:(0.250665456057) A[2]:(0.250408530235) A[3]:(0.250188171864)\n",
      " state (0)  A[0]:(0.0248328745365)\n",
      " state (1)  A[0]:(0.0173557810485)\n",
      " state (2)  A[0]:(0.0195657834411)\n",
      " state (3)  A[0]:(0.0218115486205)\n",
      " state (4)  A[0]:(0.0242953114212)\n",
      " state (5)  A[0]:(0.0269961729646)\n",
      " state (6)  A[0]:(0.0298380255699)\n",
      " state (7)  A[0]:(0.0327262245119)\n",
      " state (8)  A[0]:(0.0355653427541)\n",
      " state (9)  A[0]:(0.038272280246)\n",
      " state (10)  A[0]:(0.040784612298)\n",
      " state (11)  A[0]:(0.0430633276701)\n",
      " state (12)  A[0]:(0.045091047883)\n",
      " state (13)  A[0]:(0.0468676760793)\n",
      " state (14)  A[0]:(0.0484051443636)\n",
      " state (15)  A[0]:(0.0497227869928)\n",
      "Episode 13000 finished after 2 . Running score: 0.02. Policy_loss: -95762.7584264, Value_loss: 1.27502973825. Times trained:               7651. Times reached goal: 14.               Steps done: 99654.\n",
      " state (0)  A[0]:(0.250229239464) A[1]:(0.249836862087) A[2]:(0.249910593033) A[3]:(0.250023275614)\n",
      " state (1)  A[0]:(0.249852120876) A[1]:(0.250184834003) A[2]:(0.249713882804) A[3]:(0.250249117613)\n",
      " state (2)  A[0]:(0.249866992235) A[1]:(0.250256568193) A[2]:(0.249746784568) A[3]:(0.250129699707)\n",
      " state (3)  A[0]:(0.249795123935) A[1]:(0.250334650278) A[2]:(0.249769732356) A[3]:(0.250100493431)\n",
      " state (4)  A[0]:(0.249685436487) A[1]:(0.250420629978) A[2]:(0.249787002802) A[3]:(0.250106871128)\n",
      " state (5)  A[0]:(0.249566867948) A[1]:(0.250506430864) A[2]:(0.249800860882) A[3]:(0.250125795603)\n",
      " state (6)  A[0]:(0.249452233315) A[1]:(0.250587433577) A[2]:(0.249812379479) A[3]:(0.250147938728)\n",
      " state (7)  A[0]:(0.249346554279) A[1]:(0.250661730766) A[2]:(0.249821990728) A[3]:(0.250169694424)\n",
      " state (8)  A[0]:(0.249251395464) A[1]:(0.250728845596) A[2]:(0.249830037355) A[3]:(0.250189781189)\n",
      " state (9)  A[0]:(0.249166756868) A[1]:(0.250788867474) A[2]:(0.249836623669) A[3]:(0.250207751989)\n",
      " state (10)  A[0]:(0.249092161655) A[1]:(0.250842243433) A[2]:(0.249841988087) A[3]:(0.25022366643)\n",
      " state (11)  A[0]:(0.249026760459) A[1]:(0.250889450312) A[2]:(0.249846190214) A[3]:(0.250237613916)\n",
      " state (12)  A[0]:(0.248969733715) A[1]:(0.250931054354) A[2]:(0.249849423766) A[3]:(0.250249773264)\n",
      " state (13)  A[0]:(0.248920157552) A[1]:(0.250967621803) A[2]:(0.249851793051) A[3]:(0.250260382891)\n",
      " state (14)  A[0]:(0.248877227306) A[1]:(0.250999689102) A[2]:(0.249853476882) A[3]:(0.250269591808)\n",
      " state (15)  A[0]:(0.248840108514) A[1]:(0.251027733088) A[2]:(0.249854534864) A[3]:(0.250277578831)\n",
      " state (0)  A[0]:(0.0195098258555)\n",
      " state (1)  A[0]:(0.014390848577)\n",
      " state (2)  A[0]:(0.0165565572679)\n",
      " state (3)  A[0]:(0.0186954028904)\n",
      " state (4)  A[0]:(0.0210899561644)\n",
      " state (5)  A[0]:(0.0237428285182)\n",
      " state (6)  A[0]:(0.0265980996192)\n",
      " state (7)  A[0]:(0.029576074332)\n",
      " state (8)  A[0]:(0.0325868614018)\n",
      " state (9)  A[0]:(0.0355423241854)\n",
      " state (10)  A[0]:(0.0383663587272)\n",
      " state (11)  A[0]:(0.0410011932254)\n",
      " state (12)  A[0]:(0.0434096306562)\n",
      " state (13)  A[0]:(0.0455736070871)\n",
      " state (14)  A[0]:(0.0474906750023)\n",
      " state (15)  A[0]:(0.0491697639227)\n",
      "Episode 14000 finished after 7 . Running score: 0.02. Policy_loss: -95757.4637417, Value_loss: 1.27878433968. Times trained:               7839. Times reached goal: 8.               Steps done: 107493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.2502,  0.2499,  0.2497,  0.2502]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2503,  0.2497,  0.2499]])\n",
      "On state=4, selected action=2\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2497,  0.2506,  0.2497,  0.2500]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2497,  0.2506,  0.2497,  0.2500]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2496,  0.2506,  0.2498,  0.2500]])\n",
      "On state=10, selected action=3\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2504,  0.2498,  0.2499]])\n",
      "On state=6, selected action=1\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250223785639) A[1]:(0.24973398447) A[2]:(0.249911785126) A[3]:(0.25013038516)\n",
      " state (1)  A[0]:(0.250104308128) A[1]:(0.249998196959) A[2]:(0.249840974808) A[3]:(0.250056564808)\n",
      " state (2)  A[0]:(0.250187247992) A[1]:(0.250049293041) A[2]:(0.249855130911) A[3]:(0.249908342957)\n",
      " state (3)  A[0]:(0.250161021948) A[1]:(0.250118017197) A[2]:(0.249855026603) A[3]:(0.249865874648)\n",
      " state (4)  A[0]:(0.250089049339) A[1]:(0.25019749999) A[2]:(0.249851122499) A[3]:(0.249862343073)\n",
      " state (5)  A[0]:(0.250002890825) A[1]:(0.250277966261) A[2]:(0.24984639883) A[3]:(0.249872758985)\n",
      " state (6)  A[0]:(0.249915838242) A[1]:(0.250354766846) A[2]:(0.249841853976) A[3]:(0.249887481332)\n",
      " state (7)  A[0]:(0.249833062291) A[1]:(0.250426203012) A[2]:(0.249837815762) A[3]:(0.249902874231)\n",
      " state (8)  A[0]:(0.249756395817) A[1]:(0.250491768122) A[2]:(0.249834284186) A[3]:(0.249917581677)\n",
      " state (9)  A[0]:(0.249686211348) A[1]:(0.250551432371) A[2]:(0.249831214547) A[3]:(0.249931082129)\n",
      " state (10)  A[0]:(0.249622553587) A[1]:(0.250605523586) A[2]:(0.249828577042) A[3]:(0.249943330884)\n",
      " state (11)  A[0]:(0.249565064907) A[1]:(0.25065433979) A[2]:(0.249826297164) A[3]:(0.249954298139)\n",
      " state (12)  A[0]:(0.249513372779) A[1]:(0.250698238611) A[2]:(0.249824300408) A[3]:(0.249964088202)\n",
      " state (13)  A[0]:(0.249467074871) A[1]:(0.250737637281) A[2]:(0.249822556973) A[3]:(0.249972775578)\n",
      " state (14)  A[0]:(0.249425709248) A[1]:(0.250772863626) A[2]:(0.249821007252) A[3]:(0.249980434775)\n",
      " state (15)  A[0]:(0.249388858676) A[1]:(0.250804305077) A[2]:(0.249819606543) A[3]:(0.249987185001)\n",
      " state (0)  A[0]:(0.0180733278394)\n",
      " state (1)  A[0]:(0.0121827349067)\n",
      " state (2)  A[0]:(0.0141632147133)\n",
      " state (3)  A[0]:(0.0161437690258)\n",
      " state (4)  A[0]:(0.0183856599033)\n",
      " state (5)  A[0]:(0.0209047086537)\n",
      " state (6)  A[0]:(0.0236679539084)\n",
      " state (7)  A[0]:(0.0266191996634)\n",
      " state (8)  A[0]:(0.0296865515411)\n",
      " state (9)  A[0]:(0.0327905118465)\n",
      " state (10)  A[0]:(0.0358527265489)\n",
      " state (11)  A[0]:(0.0388038270175)\n",
      " state (12)  A[0]:(0.0415887907147)\n",
      " state (13)  A[0]:(0.0441691689193)\n",
      " state (14)  A[0]:(0.0465228334069)\n",
      " state (15)  A[0]:(0.0486416444182)\n",
      "Episode 15000 finished after 6 . Running score: 0.0. Policy_loss: -95748.1028787, Value_loss: 1.00578678525. Times trained:               7565. Times reached goal: 9.               Steps done: 115058.\n",
      " state (0)  A[0]:(0.250247508287) A[1]:(0.249812602997) A[2]:(0.250022143126) A[3]:(0.249917790294)\n",
      " state (1)  A[0]:(0.2501937747) A[1]:(0.249782845378) A[2]:(0.250022441149) A[3]:(0.250000983477)\n",
      " state (2)  A[0]:(0.250238180161) A[1]:(0.249807476997) A[2]:(0.250093877316) A[3]:(0.249860465527)\n",
      " state (3)  A[0]:(0.250189691782) A[1]:(0.249868884683) A[2]:(0.250150769949) A[3]:(0.24979069829)\n",
      " state (4)  A[0]:(0.250106722116) A[1]:(0.249944314361) A[2]:(0.250199735165) A[3]:(0.249749228358)\n",
      " state (5)  A[0]:(0.250015377998) A[1]:(0.250021338463) A[2]:(0.250243455172) A[3]:(0.249719902873)\n",
      " state (6)  A[0]:(0.249926000834) A[1]:(0.250094681978) A[2]:(0.250283032656) A[3]:(0.249696269631)\n",
      " state (7)  A[0]:(0.249842450023) A[1]:(0.250162541866) A[2]:(0.250319093466) A[3]:(0.249675914645)\n",
      " state (8)  A[0]:(0.249765858054) A[1]:(0.250224471092) A[2]:(0.250351935625) A[3]:(0.249657779932)\n",
      " state (9)  A[0]:(0.249696329236) A[1]:(0.250280559063) A[2]:(0.25038176775) A[3]:(0.249641403556)\n",
      " state (10)  A[0]:(0.249633580446) A[1]:(0.250331044197) A[2]:(0.250408768654) A[3]:(0.249626547098)\n",
      " state (11)  A[0]:(0.249577239156) A[1]:(0.250376433134) A[2]:(0.250433176756) A[3]:(0.249613165855)\n",
      " state (12)  A[0]:(0.249526768923) A[1]:(0.250417053699) A[2]:(0.250455141068) A[3]:(0.249601066113)\n",
      " state (13)  A[0]:(0.249481722713) A[1]:(0.250453323126) A[2]:(0.250474840403) A[3]:(0.249590143561)\n",
      " state (14)  A[0]:(0.249441593885) A[1]:(0.250485628843) A[2]:(0.250492453575) A[3]:(0.2495803684)\n",
      " state (15)  A[0]:(0.249405950308) A[1]:(0.25051432848) A[2]:(0.250508159399) A[3]:(0.249571576715)\n",
      " state (0)  A[0]:(0.0155277960002)\n",
      " state (1)  A[0]:(0.009128395468)\n",
      " state (2)  A[0]:(0.0111046433449)\n",
      " state (3)  A[0]:(0.0132191926241)\n",
      " state (4)  A[0]:(0.0156852006912)\n",
      " state (5)  A[0]:(0.0185240134597)\n",
      " state (6)  A[0]:(0.0217182263732)\n",
      " state (7)  A[0]:(0.0252230539918)\n",
      " state (8)  A[0]:(0.0289690345526)\n",
      " state (9)  A[0]:(0.0328677929938)\n",
      " state (10)  A[0]:(0.0368212908506)\n",
      " state (11)  A[0]:(0.0407323539257)\n",
      " state (12)  A[0]:(0.0445140302181)\n",
      " state (13)  A[0]:(0.0480963625014)\n",
      " state (14)  A[0]:(0.0514294095337)\n",
      " state (15)  A[0]:(0.0544831752777)\n",
      "Episode 16000 finished after 9 . Running score: 0.01. Policy_loss: -95737.0373843, Value_loss: 1.00465332557. Times trained:               7964. Times reached goal: 18.               Steps done: 123022.\n",
      " state (0)  A[0]:(0.250284343958) A[1]:(0.249880105257) A[2]:(0.2499101758) A[3]:(0.249925389886)\n",
      " state (1)  A[0]:(0.250324338675) A[1]:(0.2498665452) A[2]:(0.249860480428) A[3]:(0.249948680401)\n",
      " state (2)  A[0]:(0.250313967466) A[1]:(0.249890998006) A[2]:(0.249891608953) A[3]:(0.249903410673)\n",
      " state (3)  A[0]:(0.250226169825) A[1]:(0.249945357442) A[2]:(0.249915063381) A[3]:(0.249913349748)\n",
      " state (4)  A[0]:(0.250119686127) A[1]:(0.25000539422) A[2]:(0.249935910106) A[3]:(0.249939054251)\n",
      " state (5)  A[0]:(0.250013023615) A[1]:(0.250063538551) A[2]:(0.249955251813) A[3]:(0.249968230724)\n",
      " state (6)  A[0]:(0.249911561608) A[1]:(0.250118017197) A[2]:(0.249973297119) A[3]:(0.249997124076)\n",
      " state (7)  A[0]:(0.249816685915) A[1]:(0.250168651342) A[2]:(0.24999012053) A[3]:(0.250024586916)\n",
      " state (8)  A[0]:(0.249728545547) A[1]:(0.250215530396) A[2]:(0.250005662441) A[3]:(0.250050246716)\n",
      " state (9)  A[0]:(0.249647006392) A[1]:(0.250258862972) A[2]:(0.250020027161) A[3]:(0.250074088573)\n",
      " state (10)  A[0]:(0.249571800232) A[1]:(0.250298857689) A[2]:(0.250033229589) A[3]:(0.25009611249)\n",
      " state (11)  A[0]:(0.249502643943) A[1]:(0.250335663557) A[2]:(0.250045329332) A[3]:(0.250116407871)\n",
      " state (12)  A[0]:(0.2494392097) A[1]:(0.250369429588) A[2]:(0.250056385994) A[3]:(0.25013500452)\n",
      " state (13)  A[0]:(0.249381154776) A[1]:(0.250400334597) A[2]:(0.250066429377) A[3]:(0.250152051449)\n",
      " state (14)  A[0]:(0.249328196049) A[1]:(0.250428587198) A[2]:(0.250075608492) A[3]:(0.250167638063)\n",
      " state (15)  A[0]:(0.249279946089) A[1]:(0.250454306602) A[2]:(0.250083863735) A[3]:(0.250181823969)\n",
      " state (0)  A[0]:(0.0130421258509)\n",
      " state (1)  A[0]:(0.00590198487043)\n",
      " state (2)  A[0]:(0.00794126465917)\n",
      " state (3)  A[0]:(0.0103447139263)\n",
      " state (4)  A[0]:(0.0132369101048)\n",
      " state (5)  A[0]:(0.0166547633708)\n",
      " state (6)  A[0]:(0.020603787154)\n",
      " state (7)  A[0]:(0.0250541642308)\n",
      " state (8)  A[0]:(0.0299375392497)\n",
      " state (9)  A[0]:(0.0351498834789)\n",
      " state (10)  A[0]:(0.0405607894063)\n",
      " state (11)  A[0]:(0.046027854085)\n",
      " state (12)  A[0]:(0.0514123141766)\n",
      " state (13)  A[0]:(0.0565928444266)\n",
      " state (14)  A[0]:(0.0614745393395)\n",
      " state (15)  A[0]:(0.0659924596548)\n",
      "Episode 17000 finished after 3 . Running score: 0.01. Policy_loss: -95733.4098116, Value_loss: 1.00391005529. Times trained:               7827. Times reached goal: 20.               Steps done: 130849.\n",
      " state (0)  A[0]:(0.250551253557) A[1]:(0.249981760979) A[2]:(0.24965724349) A[3]:(0.24980969727)\n",
      " state (1)  A[0]:(0.250410109758) A[1]:(0.250046491623) A[2]:(0.24969060719) A[3]:(0.249852821231)\n",
      " state (2)  A[0]:(0.250350773335) A[1]:(0.250081449747) A[2]:(0.249722391367) A[3]:(0.249845400453)\n",
      " state (3)  A[0]:(0.250244319439) A[1]:(0.250135570765) A[2]:(0.249744191766) A[3]:(0.249875962734)\n",
      " state (4)  A[0]:(0.250130683184) A[1]:(0.250192970037) A[2]:(0.24976220727) A[3]:(0.249914169312)\n",
      " state (5)  A[0]:(0.250022143126) A[1]:(0.250247895718) A[2]:(0.249778062105) A[3]:(0.249951884151)\n",
      " state (6)  A[0]:(0.249922022223) A[1]:(0.25029873848) A[2]:(0.249792203307) A[3]:(0.249986976385)\n",
      " state (7)  A[0]:(0.249830886722) A[1]:(0.2503452003) A[2]:(0.249804913998) A[3]:(0.250018984079)\n",
      " state (8)  A[0]:(0.249748557806) A[1]:(0.250387281179) A[2]:(0.249816223979) A[3]:(0.250047892332)\n",
      " state (9)  A[0]:(0.249674648046) A[1]:(0.250425219536) A[2]:(0.249826341867) A[3]:(0.250073820353)\n",
      " state (10)  A[0]:(0.249608576298) A[1]:(0.250459194183) A[2]:(0.249835282564) A[3]:(0.250096946955)\n",
      " state (11)  A[0]:(0.249549821019) A[1]:(0.250489503145) A[2]:(0.249843239784) A[3]:(0.250117480755)\n",
      " state (12)  A[0]:(0.24949772656) A[1]:(0.25051638484) A[2]:(0.24985024333) A[3]:(0.250135600567)\n",
      " state (13)  A[0]:(0.249451756477) A[1]:(0.250540196896) A[2]:(0.249856442213) A[3]:(0.250151604414)\n",
      " state (14)  A[0]:(0.249411270022) A[1]:(0.250561177731) A[2]:(0.249861910939) A[3]:(0.250165641308)\n",
      " state (15)  A[0]:(0.249375715852) A[1]:(0.250579595566) A[2]:(0.249866709113) A[3]:(0.250177919865)\n",
      " state (0)  A[0]:(0.00749372690916)\n",
      " state (1)  A[0]:(0.00255228951573)\n",
      " state (2)  A[0]:(0.0052330493927)\n",
      " state (3)  A[0]:(0.00865343213081)\n",
      " state (4)  A[0]:(0.0129430778325)\n",
      " state (5)  A[0]:(0.0181595310569)\n",
      " state (6)  A[0]:(0.0242790095508)\n",
      " state (7)  A[0]:(0.0311796963215)\n",
      " state (8)  A[0]:(0.038644824177)\n",
      " state (9)  A[0]:(0.0463908240199)\n",
      " state (10)  A[0]:(0.0541135594249)\n",
      " state (11)  A[0]:(0.0615368783474)\n",
      " state (12)  A[0]:(0.0684478878975)\n",
      " state (13)  A[0]:(0.0747116133571)\n",
      " state (14)  A[0]:(0.0802667811513)\n",
      " state (15)  A[0]:(0.0851104706526)\n",
      "Episode 18000 finished after 5 . Running score: 0.02. Policy_loss: -95730.6210838, Value_loss: 1.26615577322. Times trained:               7700. Times reached goal: 13.               Steps done: 138549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.249297052622) A[1]:(0.251458078623) A[2]:(0.248692244291) A[3]:(0.250552594662)\n",
      " state (1)  A[0]:(0.249341309071) A[1]:(0.25128158927) A[2]:(0.249016672373) A[3]:(0.250360399485)\n",
      " state (2)  A[0]:(0.249261513352) A[1]:(0.251327961683) A[2]:(0.249083325267) A[3]:(0.250327199697)\n",
      " state (3)  A[0]:(0.249140307307) A[1]:(0.251400142908) A[2]:(0.249123349786) A[3]:(0.250336259604)\n",
      " state (4)  A[0]:(0.249018743634) A[1]:(0.25147369504) A[2]:(0.249153003097) A[3]:(0.250354558229)\n",
      " state (5)  A[0]:(0.248908475041) A[1]:(0.251541376114) A[2]:(0.249176606536) A[3]:(0.250373601913)\n",
      " state (6)  A[0]:(0.248811915517) A[1]:(0.251601338387) A[2]:(0.249195620418) A[3]:(0.250391095877)\n",
      " state (7)  A[0]:(0.248728826642) A[1]:(0.251653581858) A[2]:(0.249211058021) A[3]:(0.250406563282)\n",
      " state (8)  A[0]:(0.248658061028) A[1]:(0.25169852376) A[2]:(0.249223440886) A[3]:(0.250419974327)\n",
      " state (9)  A[0]:(0.248598262668) A[1]:(0.251736879349) A[2]:(0.249233350158) A[3]:(0.250431507826)\n",
      " state (10)  A[0]:(0.248548060656) A[1]:(0.251769423485) A[2]:(0.249241217971) A[3]:(0.250441342592)\n",
      " state (11)  A[0]:(0.248506054282) A[1]:(0.251796901226) A[2]:(0.249247401953) A[3]:(0.250449687243)\n",
      " state (12)  A[0]:(0.248471021652) A[1]:(0.251820027828) A[2]:(0.249252215028) A[3]:(0.250456750393)\n",
      " state (13)  A[0]:(0.248441845179) A[1]:(0.251839518547) A[2]:(0.249255955219) A[3]:(0.25046274066)\n",
      " state (14)  A[0]:(0.248417511582) A[1]:(0.251855909824) A[2]:(0.249258771539) A[3]:(0.250467807055)\n",
      " state (15)  A[0]:(0.248397231102) A[1]:(0.251869767904) A[2]:(0.249260902405) A[3]:(0.250472128391)\n",
      " state (0)  A[0]:(0.0117322877049)\n",
      " state (1)  A[0]:(0.00979255884886)\n",
      " state (2)  A[0]:(0.0134829841554)\n",
      " state (3)  A[0]:(0.0183022804558)\n",
      " state (4)  A[0]:(0.0245605446398)\n",
      " state (5)  A[0]:(0.0323409475386)\n",
      " state (6)  A[0]:(0.0415294244885)\n",
      " state (7)  A[0]:(0.0517901033163)\n",
      " state (8)  A[0]:(0.0626086741686)\n",
      " state (9)  A[0]:(0.0734016373754)\n",
      " state (10)  A[0]:(0.0836441665888)\n",
      " state (11)  A[0]:(0.0929604321718)\n",
      " state (12)  A[0]:(0.101150564849)\n",
      " state (13)  A[0]:(0.108166120946)\n",
      " state (14)  A[0]:(0.114062726498)\n",
      " state (15)  A[0]:(0.118953205645)\n",
      "Episode 19000 finished after 4 . Running score: 0.03. Policy_loss: -95728.1707607, Value_loss: 1.00996678379. Times trained:               7749. Times reached goal: 13.               Steps done: 146298.\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2500,  0.2502,  0.2499]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2500,  0.2502,  0.2499]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2500,  0.2502,  0.2499]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2501,  0.2500,  0.2499]])\n",
      "On state=1, selected action=0\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2501,  0.2500,  0.2499]])\n",
      "On state=1, selected action=1\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2501,  0.2500,  0.2498]])\n",
      "On state=2, selected action=2\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2502,  0.2500,  0.2498]])\n",
      "On state=3, selected action=0\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2502,  0.2500,  0.2498]])\n",
      "On state=3, selected action=2\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.249916911125) A[1]:(0.250018507242) A[2]:(0.250169992447) A[3]:(0.249894589186)\n",
      " state (1)  A[0]:(0.249976873398) A[1]:(0.250115990639) A[2]:(0.250037372112) A[3]:(0.249869823456)\n",
      " state (2)  A[0]:(0.249979034066) A[1]:(0.250150382519) A[2]:(0.250034332275) A[3]:(0.249836221337)\n",
      " state (3)  A[0]:(0.249940499663) A[1]:(0.250193744898) A[2]:(0.250032484531) A[3]:(0.249833270907)\n",
      " state (4)  A[0]:(0.249887317419) A[1]:(0.25024035573) A[2]:(0.250030755997) A[3]:(0.24984151125)\n",
      " state (5)  A[0]:(0.249830678105) A[1]:(0.250286489725) A[2]:(0.250029146671) A[3]:(0.249853730202)\n",
      " state (6)  A[0]:(0.249774903059) A[1]:(0.250330507755) A[2]:(0.250027567148) A[3]:(0.24986705184)\n",
      " state (7)  A[0]:(0.249721646309) A[1]:(0.250371903181) A[2]:(0.250026106834) A[3]:(0.249880373478)\n",
      " state (8)  A[0]:(0.249671414495) A[1]:(0.250410586596) A[2]:(0.250024735928) A[3]:(0.249893277884)\n",
      " state (9)  A[0]:(0.249624297023) A[1]:(0.250446647406) A[2]:(0.250023454428) A[3]:(0.249905630946)\n",
      " state (10)  A[0]:(0.249580234289) A[1]:(0.250480204821) A[2]:(0.250022262335) A[3]:(0.24991735816)\n",
      " state (11)  A[0]:(0.249539092183) A[1]:(0.25051137805) A[2]:(0.250021100044) A[3]:(0.249928429723)\n",
      " state (12)  A[0]:(0.249500736594) A[1]:(0.250540316105) A[2]:(0.250020056963) A[3]:(0.249938875437)\n",
      " state (13)  A[0]:(0.249465048313) A[1]:(0.250567138195) A[2]:(0.250019073486) A[3]:(0.249948725104)\n",
      " state (14)  A[0]:(0.249431878328) A[1]:(0.250591993332) A[2]:(0.250018179417) A[3]:(0.249957993627)\n",
      " state (15)  A[0]:(0.249401062727) A[1]:(0.25061494112) A[2]:(0.250017344952) A[3]:(0.249966695905)\n",
      " state (0)  A[0]:(0.0128491856158)\n",
      " state (1)  A[0]:(0.00841933861375)\n",
      " state (2)  A[0]:(0.0111204944551)\n",
      " state (3)  A[0]:(0.0144040472806)\n",
      " state (4)  A[0]:(0.0185905247927)\n",
      " state (5)  A[0]:(0.0238227806985)\n",
      " state (6)  A[0]:(0.0301834456623)\n",
      " state (7)  A[0]:(0.0376742966473)\n",
      " state (8)  A[0]:(0.0461933128536)\n",
      " state (9)  A[0]:(0.0555289387703)\n",
      " state (10)  A[0]:(0.0653792470694)\n",
      " state (11)  A[0]:(0.0753937363625)\n",
      " state (12)  A[0]:(0.0852259844542)\n",
      " state (13)  A[0]:(0.0945804938674)\n",
      " state (14)  A[0]:(0.10324165225)\n",
      " state (15)  A[0]:(0.111081019044)\n",
      "Episode 20000 finished after 8 . Running score: 0.0. Policy_loss: -95725.7524529, Value_loss: 1.0065230914. Times trained:               7594. Times reached goal: 15.               Steps done: 153892.\n",
      " state (0)  A[0]:(0.250171840191) A[1]:(0.249938845634) A[2]:(0.24986949563) A[3]:(0.250019788742)\n",
      " state (1)  A[0]:(0.250116467476) A[1]:(0.250122159719) A[2]:(0.249864786863) A[3]:(0.249896585941)\n",
      " state (2)  A[0]:(0.250055462122) A[1]:(0.250226706266) A[2]:(0.249881237745) A[3]:(0.249836549163)\n",
      " state (3)  A[0]:(0.249965250492) A[1]:(0.250330299139) A[2]:(0.249891459942) A[3]:(0.249812975526)\n",
      " state (4)  A[0]:(0.249868437648) A[1]:(0.25042912364) A[2]:(0.249898865819) A[3]:(0.249803617597)\n",
      " state (5)  A[0]:(0.24977478385) A[1]:(0.250520646572) A[2]:(0.249904677272) A[3]:(0.249799922109)\n",
      " state (6)  A[0]:(0.249688059092) A[1]:(0.250603884459) A[2]:(0.24990940094) A[3]:(0.249798685312)\n",
      " state (7)  A[0]:(0.249609440565) A[1]:(0.250678688288) A[2]:(0.249913319945) A[3]:(0.249798595905)\n",
      " state (8)  A[0]:(0.249539077282) A[1]:(0.250745296478) A[2]:(0.249916568398) A[3]:(0.249799102545)\n",
      " state (9)  A[0]:(0.249476715922) A[1]:(0.250804126263) A[2]:(0.249919265509) A[3]:(0.24979993701)\n",
      " state (10)  A[0]:(0.249421849847) A[1]:(0.250855714083) A[2]:(0.249921500683) A[3]:(0.249800950289)\n",
      " state (11)  A[0]:(0.249373927712) A[1]:(0.250900685787) A[2]:(0.249923378229) A[3]:(0.249802052975)\n",
      " state (12)  A[0]:(0.249332308769) A[1]:(0.25093960762) A[2]:(0.249924913049) A[3]:(0.249803170562)\n",
      " state (13)  A[0]:(0.24929638207) A[1]:(0.250973135233) A[2]:(0.24992620945) A[3]:(0.249804273248)\n",
      " state (14)  A[0]:(0.249265536666) A[1]:(0.251001864672) A[2]:(0.249927267432) A[3]:(0.249805301428)\n",
      " state (15)  A[0]:(0.249239191413) A[1]:(0.251026391983) A[2]:(0.249928176403) A[3]:(0.249806299806)\n",
      " state (0)  A[0]:(0.0114937201142)\n",
      " state (1)  A[0]:(0.00704666227102)\n",
      " state (2)  A[0]:(0.00942669808865)\n",
      " state (3)  A[0]:(0.0122612677515)\n",
      " state (4)  A[0]:(0.0158612169325)\n",
      " state (5)  A[0]:(0.0203728154302)\n",
      " state (6)  A[0]:(0.0259075351059)\n",
      " state (7)  A[0]:(0.0325306020677)\n",
      " state (8)  A[0]:(0.0402380302548)\n",
      " state (9)  A[0]:(0.0489389412105)\n",
      " state (10)  A[0]:(0.0584507770836)\n",
      " state (11)  A[0]:(0.0685125514865)\n",
      " state (12)  A[0]:(0.078815497458)\n",
      " state (13)  A[0]:(0.0890434458852)\n",
      " state (14)  A[0]:(0.0989121720195)\n",
      " state (15)  A[0]:(0.108197391033)\n",
      "Episode 21000 finished after 5 . Running score: 0.01. Policy_loss: -95721.8281958, Value_loss: 1.00922176753. Times trained:               7693. Times reached goal: 12.               Steps done: 161585.\n",
      " state (0)  A[0]:(0.250446796417) A[1]:(0.24991646409) A[2]:(0.249960780144) A[3]:(0.249675959349)\n",
      " state (1)  A[0]:(0.250361680984) A[1]:(0.249957501888) A[2]:(0.249972373247) A[3]:(0.249708488584)\n",
      " state (2)  A[0]:(0.250313103199) A[1]:(0.249994680285) A[2]:(0.249990865588) A[3]:(0.249701306224)\n",
      " state (3)  A[0]:(0.250236779451) A[1]:(0.250043660402) A[2]:(0.250005662441) A[3]:(0.249713912606)\n",
      " state (4)  A[0]:(0.25015193224) A[1]:(0.250096291304) A[2]:(0.250018984079) A[3]:(0.24973282218)\n",
      " state (5)  A[0]:(0.250066488981) A[1]:(0.2501488626) A[2]:(0.250031381845) A[3]:(0.249753281474)\n",
      " state (6)  A[0]:(0.249983653426) A[1]:(0.250199794769) A[2]:(0.250042945147) A[3]:(0.249773621559)\n",
      " state (7)  A[0]:(0.249904870987) A[1]:(0.250248283148) A[2]:(0.250053733587) A[3]:(0.249793186784)\n",
      " state (8)  A[0]:(0.249830856919) A[1]:(0.250293850899) A[2]:(0.250063627958) A[3]:(0.249811664224)\n",
      " state (9)  A[0]:(0.249762102962) A[1]:(0.250336229801) A[2]:(0.250072717667) A[3]:(0.249828949571)\n",
      " state (10)  A[0]:(0.249698847532) A[1]:(0.250375211239) A[2]:(0.25008097291) A[3]:(0.249844953418)\n",
      " state (11)  A[0]:(0.249641209841) A[1]:(0.250410735607) A[2]:(0.250088423491) A[3]:(0.249859631062)\n",
      " state (12)  A[0]:(0.249589130282) A[1]:(0.250442773104) A[2]:(0.250095069408) A[3]:(0.249872982502)\n",
      " state (13)  A[0]:(0.249542459846) A[1]:(0.250471442938) A[2]:(0.250101000071) A[3]:(0.249885082245)\n",
      " state (14)  A[0]:(0.24950094521) A[1]:(0.250496894121) A[2]:(0.250106215477) A[3]:(0.249895960093)\n",
      " state (15)  A[0]:(0.24946424365) A[1]:(0.250519275665) A[2]:(0.250110805035) A[3]:(0.24990567565)\n",
      " state (0)  A[0]:(0.0103170014918)\n",
      " state (1)  A[0]:(0.00340408831835)\n",
      " state (2)  A[0]:(0.00560677051544)\n",
      " state (3)  A[0]:(0.00850491225719)\n",
      " state (4)  A[0]:(0.0122736394405)\n",
      " state (5)  A[0]:(0.0170918218791)\n",
      " state (6)  A[0]:(0.0231231227517)\n",
      " state (7)  A[0]:(0.0304892994463)\n",
      " state (8)  A[0]:(0.0392376929522)\n",
      " state (9)  A[0]:(0.0493110381067)\n",
      " state (10)  A[0]:(0.0605301111937)\n",
      " state (11)  A[0]:(0.0725989788771)\n",
      " state (12)  A[0]:(0.0851365625858)\n",
      " state (13)  A[0]:(0.0977283865213)\n",
      " state (14)  A[0]:(0.109983831644)\n",
      " state (15)  A[0]:(0.121582001448)\n",
      "Episode 22000 finished after 2 . Running score: 0.02. Policy_loss: -95721.4999171, Value_loss: 1.01398369905. Times trained:               7420. Times reached goal: 18.               Steps done: 169005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.249841094017) A[1]:(0.250043451786) A[2]:(0.250212937593) A[3]:(0.249902531505)\n",
      " state (1)  A[0]:(0.249800160527) A[1]:(0.250109314919) A[2]:(0.250224590302) A[3]:(0.249865993857)\n",
      " state (2)  A[0]:(0.249713078141) A[1]:(0.250171095133) A[2]:(0.250265806913) A[3]:(0.249850049615)\n",
      " state (3)  A[0]:(0.249600455165) A[1]:(0.250239670277) A[2]:(0.250306487083) A[3]:(0.249853357673)\n",
      " state (4)  A[0]:(0.249482005835) A[1]:(0.250308960676) A[2]:(0.250345677137) A[3]:(0.249863386154)\n",
      " state (5)  A[0]:(0.249366268516) A[1]:(0.250375628471) A[2]:(0.250382661819) A[3]:(0.249875470996)\n",
      " state (6)  A[0]:(0.249256938696) A[1]:(0.250438123941) A[2]:(0.250417083502) A[3]:(0.24988783896)\n",
      " state (7)  A[0]:(0.249155715108) A[1]:(0.250495731831) A[2]:(0.250448733568) A[3]:(0.249899759889)\n",
      " state (8)  A[0]:(0.249063357711) A[1]:(0.250548124313) A[2]:(0.250477552414) A[3]:(0.24991093576)\n",
      " state (9)  A[0]:(0.248980119824) A[1]:(0.250595152378) A[2]:(0.250503480434) A[3]:(0.249921232462)\n",
      " state (10)  A[0]:(0.248905926943) A[1]:(0.250636905432) A[2]:(0.250526607037) A[3]:(0.24993057549)\n",
      " state (11)  A[0]:(0.248840466142) A[1]:(0.250673562288) A[2]:(0.250546991825) A[3]:(0.249938949943)\n",
      " state (12)  A[0]:(0.248783260584) A[1]:(0.250705480576) A[2]:(0.250564843416) A[3]:(0.249946445227)\n",
      " state (13)  A[0]:(0.248733595014) A[1]:(0.250732988119) A[2]:(0.250580370426) A[3]:(0.249953061342)\n",
      " state (14)  A[0]:(0.248690754175) A[1]:(0.250756561756) A[2]:(0.250593811274) A[3]:(0.249958917499)\n",
      " state (15)  A[0]:(0.248653948307) A[1]:(0.250776618719) A[2]:(0.250605374575) A[3]:(0.2499640733)\n",
      " state (0)  A[0]:(0.00688603147864)\n",
      " state (1)  A[0]:(0.00142017379403)\n",
      " state (2)  A[0]:(0.00357669964433)\n",
      " state (3)  A[0]:(0.00649109110236)\n",
      " state (4)  A[0]:(0.010341655463)\n",
      " state (5)  A[0]:(0.0153396762908)\n",
      " state (6)  A[0]:(0.0216874964535)\n",
      " state (7)  A[0]:(0.0295464508235)\n",
      " state (8)  A[0]:(0.0389966256917)\n",
      " state (9)  A[0]:(0.0499968081713)\n",
      " state (10)  A[0]:(0.0623579844832)\n",
      " state (11)  A[0]:(0.0757443010807)\n",
      " state (12)  A[0]:(0.0897087603807)\n",
      " state (13)  A[0]:(0.103757470846)\n",
      " state (14)  A[0]:(0.117422685027)\n",
      " state (15)  A[0]:(0.130322232842)\n",
      "Episode 23000 finished after 4 . Running score: 0.01. Policy_loss: -95718.2904324, Value_loss: 1.24954138188. Times trained:               7659. Times reached goal: 14.               Steps done: 176664.\n",
      " state (0)  A[0]:(0.249052152038) A[1]:(0.25133484602) A[2]:(0.248905867338) A[3]:(0.250707089901)\n",
      " state (1)  A[0]:(0.249103322625) A[1]:(0.251304686069) A[2]:(0.249116837978) A[3]:(0.250475138426)\n",
      " state (2)  A[0]:(0.248985871673) A[1]:(0.251430094242) A[2]:(0.249174460769) A[3]:(0.250409573317)\n",
      " state (3)  A[0]:(0.24883261323) A[1]:(0.251576155424) A[2]:(0.249213293195) A[3]:(0.250377953053)\n",
      " state (4)  A[0]:(0.248673558235) A[1]:(0.251723796129) A[2]:(0.249243482947) A[3]:(0.250359207392)\n",
      " state (5)  A[0]:(0.248520076275) A[1]:(0.251864939928) A[2]:(0.249267831445) A[3]:(0.250347107649)\n",
      " state (6)  A[0]:(0.248377323151) A[1]:(0.251995533705) A[2]:(0.249287560582) A[3]:(0.250339597464)\n",
      " state (7)  A[0]:(0.248247906566) A[1]:(0.252113163471) A[2]:(0.249303326011) A[3]:(0.25033557415)\n",
      " state (8)  A[0]:(0.248133137822) A[1]:(0.252216726542) A[2]:(0.249315842986) A[3]:(0.250334322453)\n",
      " state (9)  A[0]:(0.248033240438) A[1]:(0.252306014299) A[2]:(0.249325588346) A[3]:(0.250335127115)\n",
      " state (10)  A[0]:(0.247947752476) A[1]:(0.252381712198) A[2]:(0.249333128333) A[3]:(0.250337421894)\n",
      " state (11)  A[0]:(0.247875571251) A[1]:(0.252444893122) A[2]:(0.249338895082) A[3]:(0.250340610743)\n",
      " state (12)  A[0]:(0.247815400362) A[1]:(0.252497017384) A[2]:(0.249343305826) A[3]:(0.250344306231)\n",
      " state (13)  A[0]:(0.247765719891) A[1]:(0.252539515495) A[2]:(0.249346598983) A[3]:(0.250348120928)\n",
      " state (14)  A[0]:(0.247725144029) A[1]:(0.252573937178) A[2]:(0.249349147081) A[3]:(0.250351816416)\n",
      " state (15)  A[0]:(0.247692227364) A[1]:(0.252601563931) A[2]:(0.249351039529) A[3]:(0.250355213881)\n",
      " state (0)  A[0]:(0.0411366522312)\n",
      " state (1)  A[0]:(0.0380694642663)\n",
      " state (2)  A[0]:(0.041097946465)\n",
      " state (3)  A[0]:(0.0454073101282)\n",
      " state (4)  A[0]:(0.0514141842723)\n",
      " state (5)  A[0]:(0.0595406070352)\n",
      " state (6)  A[0]:(0.0701347589493)\n",
      " state (7)  A[0]:(0.0833550319076)\n",
      " state (8)  A[0]:(0.0990503802896)\n",
      " state (9)  A[0]:(0.11669677496)\n",
      " state (10)  A[0]:(0.13544754684)\n",
      " state (11)  A[0]:(0.154303655028)\n",
      " state (12)  A[0]:(0.172334566712)\n",
      " state (13)  A[0]:(0.188849136233)\n",
      " state (14)  A[0]:(0.203459531069)\n",
      " state (15)  A[0]:(0.216049164534)\n",
      "Episode 24000 finished after 14 . Running score: 0.02. Policy_loss: -95716.0043, Value_loss: 1.01542663865. Times trained:               7695. Times reached goal: 12.               Steps done: 184359.\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2497,  0.2500,  0.2498]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2497,  0.2500,  0.2498]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2501,  0.2496]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2497,  0.2500,  0.2498]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2497,  0.2500,  0.2498]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2497,  0.2500,  0.2498]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2497,  0.2500,  0.2498]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2498,  0.2500,  0.2498]])\n",
      "On state=1, selected action=1\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2499,  0.2500,  0.2497]])\n",
      "On state=2, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2498,  0.2500,  0.2498]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2499,  0.2500,  0.2497]])\n",
      "On state=2, selected action=2\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2502,  0.2502,  0.2496]])\n",
      "On state=6, selected action=0\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250345110893) A[1]:(0.249758571386) A[2]:(0.250027924776) A[3]:(0.249868392944)\n",
      " state (1)  A[0]:(0.250368058681) A[1]:(0.249821856618) A[2]:(0.25002682209) A[3]:(0.249783247709)\n",
      " state (2)  A[0]:(0.250340938568) A[1]:(0.249884575605) A[2]:(0.250049620867) A[3]:(0.249724820256)\n",
      " state (3)  A[0]:(0.25027987361) A[1]:(0.249959990382) A[2]:(0.250076383352) A[3]:(0.249683722854)\n",
      " state (4)  A[0]:(0.250202983618) A[1]:(0.250042051077) A[2]:(0.250105381012) A[3]:(0.249649643898)\n",
      " state (5)  A[0]:(0.250119000673) A[1]:(0.250127136707) A[2]:(0.250135570765) A[3]:(0.249618336558)\n",
      " state (6)  A[0]:(0.250031977892) A[1]:(0.250213325024) A[2]:(0.250166416168) A[3]:(0.249588310719)\n",
      " state (7)  A[0]:(0.249943986535) A[1]:(0.250299423933) A[2]:(0.25019749999) A[3]:(0.249559134245)\n",
      " state (8)  A[0]:(0.249856337905) A[1]:(0.250384479761) A[2]:(0.250228494406) A[3]:(0.249530747533)\n",
      " state (9)  A[0]:(0.249770104885) A[1]:(0.250467598438) A[2]:(0.250258982182) A[3]:(0.249503299594)\n",
      " state (10)  A[0]:(0.249686360359) A[1]:(0.250547975302) A[2]:(0.250288635492) A[3]:(0.249476984143)\n",
      " state (11)  A[0]:(0.249606072903) A[1]:(0.250624835491) A[2]:(0.250317126513) A[3]:(0.249451994896)\n",
      " state (12)  A[0]:(0.249530002475) A[1]:(0.250697463751) A[2]:(0.250344067812) A[3]:(0.249428421259)\n",
      " state (13)  A[0]:(0.24945884943) A[1]:(0.250765383244) A[2]:(0.250369250774) A[3]:(0.249406456947)\n",
      " state (14)  A[0]:(0.249393090606) A[1]:(0.250828266144) A[2]:(0.25039255619) A[3]:(0.249386131763)\n",
      " state (15)  A[0]:(0.24933283031) A[1]:(0.250885903835) A[2]:(0.250413835049) A[3]:(0.249367460608)\n",
      " state (0)  A[0]:(0.0423530414701)\n",
      " state (1)  A[0]:(0.0351677834988)\n",
      " state (2)  A[0]:(0.0370972901583)\n",
      " state (3)  A[0]:(0.0396773964167)\n",
      " state (4)  A[0]:(0.0431090928614)\n",
      " state (5)  A[0]:(0.0476099997759)\n",
      " state (6)  A[0]:(0.0534069910645)\n",
      " state (7)  A[0]:(0.0607144832611)\n",
      " state (8)  A[0]:(0.0697003602982)\n",
      " state (9)  A[0]:(0.0804441198707)\n",
      " state (10)  A[0]:(0.0928957462311)\n",
      " state (11)  A[0]:(0.106848075986)\n",
      " state (12)  A[0]:(0.121937826276)\n",
      " state (13)  A[0]:(0.13768234849)\n",
      " state (14)  A[0]:(0.153546184301)\n",
      " state (15)  A[0]:(0.169017970562)\n",
      "Episode 25000 finished after 12 . Running score: 0.0. Policy_loss: -95710.300892, Value_loss: 1.02523766631. Times trained:               7831. Times reached goal: 11.               Steps done: 192190.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250174969435) A[1]:(0.249848052859) A[2]:(0.249751999974) A[3]:(0.250225007534)\n",
      " state (1)  A[0]:(0.25018543005) A[1]:(0.249903902411) A[2]:(0.249783858657) A[3]:(0.250126779079)\n",
      " state (2)  A[0]:(0.250135242939) A[1]:(0.249982565641) A[2]:(0.249818816781) A[3]:(0.250063359737)\n",
      " state (3)  A[0]:(0.250050544739) A[1]:(0.250076502562) A[2]:(0.249855488539) A[3]:(0.250017434359)\n",
      " state (4)  A[0]:(0.249947249889) A[1]:(0.250179320574) A[2]:(0.249893501401) A[3]:(0.249979883432)\n",
      " state (5)  A[0]:(0.249834522605) A[1]:(0.250286728144) A[2]:(0.249932184815) A[3]:(0.249946564436)\n",
      " state (6)  A[0]:(0.249717220664) A[1]:(0.250396102667) A[2]:(0.249971017241) A[3]:(0.249915704131)\n",
      " state (7)  A[0]:(0.249598130584) A[1]:(0.250505656004) A[2]:(0.250009596348) A[3]:(0.249886587262)\n",
      " state (8)  A[0]:(0.249479323626) A[1]:(0.250613987446) A[2]:(0.250047594309) A[3]:(0.249859079719)\n",
      " state (9)  A[0]:(0.249362573028) A[1]:(0.250719726086) A[2]:(0.250084549189) A[3]:(0.249833092093)\n",
      " state (10)  A[0]:(0.249249517918) A[1]:(0.250821590424) A[2]:(0.250120133162) A[3]:(0.249808758497)\n",
      " state (11)  A[0]:(0.249141722918) A[1]:(0.250918328762) A[2]:(0.25015386939) A[3]:(0.249786108732)\n",
      " state (12)  A[0]:(0.249040439725) A[1]:(0.25100889802) A[2]:(0.25018543005) A[3]:(0.249765172601)\n",
      " state (13)  A[0]:(0.248946771026) A[1]:(0.251092582941) A[2]:(0.250214606524) A[3]:(0.249746024609)\n",
      " state (14)  A[0]:(0.248861283064) A[1]:(0.251168906689) A[2]:(0.250241219997) A[3]:(0.249728620052)\n",
      " state (15)  A[0]:(0.248784184456) A[1]:(0.251237690449) A[2]:(0.250265181065) A[3]:(0.249712899327)\n",
      " state (0)  A[0]:(0.0372498109937)\n",
      " state (1)  A[0]:(0.0243887621909)\n",
      " state (2)  A[0]:(0.0258922167122)\n",
      " state (3)  A[0]:(0.0280171204358)\n",
      " state (4)  A[0]:(0.0308361388743)\n",
      " state (5)  A[0]:(0.0345268175006)\n",
      " state (6)  A[0]:(0.0392886660993)\n",
      " state (7)  A[0]:(0.0453276857734)\n",
      " state (8)  A[0]:(0.0528354346752)\n",
      " state (9)  A[0]:(0.0619601048529)\n",
      " state (10)  A[0]:(0.0727718025446)\n",
      " state (11)  A[0]:(0.0852282643318)\n",
      " state (12)  A[0]:(0.0991512686014)\n",
      " state (13)  A[0]:(0.114225022495)\n",
      " state (14)  A[0]:(0.13002294302)\n",
      " state (15)  A[0]:(0.146059826016)\n",
      "Episode 26000 finished after 4 . Running score: 0.0. Policy_loss: -95708.4952046, Value_loss: 1.02184811505. Times trained:               7762. Times reached goal: 17.               Steps done: 199952.\n",
      " state (0)  A[0]:(0.250048816204) A[1]:(0.250263094902) A[2]:(0.249552801251) A[3]:(0.250135302544)\n",
      " state (1)  A[0]:(0.250036329031) A[1]:(0.250282466412) A[2]:(0.249633803964) A[3]:(0.25004735589)\n",
      " state (2)  A[0]:(0.249984055758) A[1]:(0.250340759754) A[2]:(0.249694928527) A[3]:(0.249980211258)\n",
      " state (3)  A[0]:(0.249909773469) A[1]:(0.250410020351) A[2]:(0.2497523278) A[3]:(0.24992787838)\n",
      " state (4)  A[0]:(0.249821767211) A[1]:(0.250486403704) A[2]:(0.249807626009) A[3]:(0.249884262681)\n",
      " state (5)  A[0]:(0.249725982547) A[1]:(0.250566840172) A[2]:(0.249861225486) A[3]:(0.2498460114)\n",
      " state (6)  A[0]:(0.249626234174) A[1]:(0.250649183989) A[2]:(0.249913170934) A[3]:(0.2498113662)\n",
      " state (7)  A[0]:(0.2495251894) A[1]:(0.250731855631) A[2]:(0.249963402748) A[3]:(0.24977953732)\n",
      " state (8)  A[0]:(0.249424785376) A[1]:(0.25081345439) A[2]:(0.25001168251) A[3]:(0.249750107527)\n",
      " state (9)  A[0]:(0.249326810241) A[1]:(0.250892668962) A[2]:(0.250057607889) A[3]:(0.249722927809)\n",
      " state (10)  A[0]:(0.249232947826) A[1]:(0.250968247652) A[2]:(0.250100821257) A[3]:(0.249697968364)\n",
      " state (11)  A[0]:(0.249144658446) A[1]:(0.251039117575) A[2]:(0.25014090538) A[3]:(0.249675273895)\n",
      " state (12)  A[0]:(0.249063149095) A[1]:(0.251104444265) A[2]:(0.250177562237) A[3]:(0.249654874206)\n",
      " state (13)  A[0]:(0.248989135027) A[1]:(0.251163601875) A[2]:(0.25021058321) A[3]:(0.249636650085)\n",
      " state (14)  A[0]:(0.248923003674) A[1]:(0.251216441393) A[2]:(0.2502399683) A[3]:(0.249620616436)\n",
      " state (15)  A[0]:(0.24886469543) A[1]:(0.251262962818) A[2]:(0.250265747309) A[3]:(0.249606564641)\n",
      " state (0)  A[0]:(0.0327457971871)\n",
      " state (1)  A[0]:(0.0218725986779)\n",
      " state (2)  A[0]:(0.0231404583901)\n",
      " state (3)  A[0]:(0.0249373242259)\n",
      " state (4)  A[0]:(0.0273265335709)\n",
      " state (5)  A[0]:(0.0304628107697)\n",
      " state (6)  A[0]:(0.0345236957073)\n",
      " state (7)  A[0]:(0.0396977216005)\n",
      " state (8)  A[0]:(0.0461687147617)\n",
      " state (9)  A[0]:(0.054092861712)\n",
      " state (10)  A[0]:(0.0635698586702)\n",
      " state (11)  A[0]:(0.0746114999056)\n",
      " state (12)  A[0]:(0.0871153771877)\n",
      " state (13)  A[0]:(0.100853376091)\n",
      " state (14)  A[0]:(0.115482777357)\n",
      " state (15)  A[0]:(0.130582273006)\n",
      "Episode 27000 finished after 7 . Running score: 0.01. Policy_loss: -95706.9687479, Value_loss: 1.01889902894. Times trained:               8408. Times reached goal: 9.               Steps done: 208360.\n",
      " state (0)  A[0]:(0.249932914972) A[1]:(0.249500855803) A[2]:(0.250321835279) A[3]:(0.250244438648)\n",
      " state (1)  A[0]:(0.249931231141) A[1]:(0.249601691961) A[2]:(0.250327974558) A[3]:(0.250139117241)\n",
      " state (2)  A[0]:(0.249872460961) A[1]:(0.249685555696) A[2]:(0.250382691622) A[3]:(0.250059306622)\n",
      " state (3)  A[0]:(0.249784782529) A[1]:(0.249772623181) A[2]:(0.250448763371) A[3]:(0.249993771315)\n",
      " state (4)  A[0]:(0.249678477645) A[1]:(0.249862059951) A[2]:(0.25052306056) A[3]:(0.249936431646)\n",
      " state (5)  A[0]:(0.249561026692) A[1]:(0.249952033162) A[2]:(0.250602692366) A[3]:(0.249884203076)\n",
      " state (6)  A[0]:(0.249438196421) A[1]:(0.250040709972) A[2]:(0.250685065985) A[3]:(0.249836027622)\n",
      " state (7)  A[0]:(0.249314859509) A[1]:(0.250126153231) A[2]:(0.250767320395) A[3]:(0.249791651964)\n",
      " state (8)  A[0]:(0.249195545912) A[1]:(0.250206440687) A[2]:(0.250846654177) A[3]:(0.249751329422)\n",
      " state (9)  A[0]:(0.24908426404) A[1]:(0.250279933214) A[2]:(0.250920474529) A[3]:(0.249715358019)\n",
      " state (10)  A[0]:(0.248983919621) A[1]:(0.250345349312) A[2]:(0.250986814499) A[3]:(0.249683886766)\n",
      " state (11)  A[0]:(0.248896196485) A[1]:(0.250402271748) A[2]:(0.251044660807) A[3]:(0.249656915665)\n",
      " state (12)  A[0]:(0.24882145226) A[1]:(0.250450670719) A[2]:(0.251093745232) A[3]:(0.24963414669)\n",
      " state (13)  A[0]:(0.2487590909) A[1]:(0.250491142273) A[2]:(0.251134544611) A[3]:(0.249615207314)\n",
      " state (14)  A[0]:(0.248707905412) A[1]:(0.250524550676) A[2]:(0.251167923212) A[3]:(0.249599650502)\n",
      " state (15)  A[0]:(0.248666360974) A[1]:(0.250551819801) A[2]:(0.251194864511) A[3]:(0.249586910009)\n",
      " state (0)  A[0]:(0.0264551993459)\n",
      " state (1)  A[0]:(0.0185383372009)\n",
      " state (2)  A[0]:(0.0198453385383)\n",
      " state (3)  A[0]:(0.0216935444623)\n",
      " state (4)  A[0]:(0.0242083631456)\n",
      " state (5)  A[0]:(0.0275824982673)\n",
      " state (6)  A[0]:(0.0320341438055)\n",
      " state (7)  A[0]:(0.0377919003367)\n",
      " state (8)  A[0]:(0.045070387423)\n",
      " state (9)  A[0]:(0.0540350116789)\n",
      " state (10)  A[0]:(0.0647592097521)\n",
      " state (11)  A[0]:(0.0771826654673)\n",
      " state (12)  A[0]:(0.0910846143961)\n",
      " state (13)  A[0]:(0.106087803841)\n",
      " state (14)  A[0]:(0.12169944495)\n",
      " state (15)  A[0]:(0.137381508946)\n",
      "Episode 28000 finished after 16 . Running score: 0.01. Policy_loss: -95705.7333234, Value_loss: 1.01832763304. Times trained:               7359. Times reached goal: 6.               Steps done: 215719.\n",
      " state (0)  A[0]:(0.250410526991) A[1]:(0.249827057123) A[2]:(0.250182121992) A[3]:(0.24958024919)\n",
      " state (1)  A[0]:(0.250357121229) A[1]:(0.249892547727) A[2]:(0.250194191933) A[3]:(0.249556183815)\n",
      " state (2)  A[0]:(0.250265836716) A[1]:(0.249973207712) A[2]:(0.250249117613) A[3]:(0.249511808157)\n",
      " state (3)  A[0]:(0.250149995089) A[1]:(0.250063359737) A[2]:(0.250315576792) A[3]:(0.249471023679)\n",
      " state (4)  A[0]:(0.250021934509) A[1]:(0.250158309937) A[2]:(0.250388652086) A[3]:(0.249431043863)\n",
      " state (5)  A[0]:(0.249888405204) A[1]:(0.250254929066) A[2]:(0.250465452671) A[3]:(0.249391213059)\n",
      " state (6)  A[0]:(0.249753370881) A[1]:(0.250350952148) A[2]:(0.250543802977) A[3]:(0.24935182929)\n",
      " state (7)  A[0]:(0.24962028861) A[1]:(0.250444322824) A[2]:(0.25062161684) A[3]:(0.249313756824)\n",
      " state (8)  A[0]:(0.249492689967) A[1]:(0.250532746315) A[2]:(0.250696539879) A[3]:(0.249277964234)\n",
      " state (9)  A[0]:(0.249374046922) A[1]:(0.25061416626) A[2]:(0.25076636672) A[3]:(0.249245375395)\n",
      " state (10)  A[0]:(0.249267190695) A[1]:(0.250686973333) A[2]:(0.250829309225) A[3]:(0.249216586351)\n",
      " state (11)  A[0]:(0.249173775315) A[1]:(0.250750184059) A[2]:(0.250884234905) A[3]:(0.249191850424)\n",
      " state (12)  A[0]:(0.249094292521) A[1]:(0.250803768635) A[2]:(0.25093087554) A[3]:(0.249171108007)\n",
      " state (13)  A[0]:(0.24902819097) A[1]:(0.250848203897) A[2]:(0.250969588757) A[3]:(0.249154061079)\n",
      " state (14)  A[0]:(0.248974144459) A[1]:(0.250884473324) A[2]:(0.251001119614) A[3]:(0.249140277505)\n",
      " state (15)  A[0]:(0.248930603266) A[1]:(0.250913709402) A[2]:(0.251026451588) A[3]:(0.249129250646)\n",
      " state (0)  A[0]:(0.0185781642795)\n",
      " state (1)  A[0]:(0.0110785569996)\n",
      " state (2)  A[0]:(0.0127261001617)\n",
      " state (3)  A[0]:(0.0151818040758)\n",
      " state (4)  A[0]:(0.0187071431428)\n",
      " state (5)  A[0]:(0.0236580744386)\n",
      " state (6)  A[0]:(0.0304283108562)\n",
      " state (7)  A[0]:(0.0393979586661)\n",
      " state (8)  A[0]:(0.0508516803384)\n",
      " state (9)  A[0]:(0.0648737326264)\n",
      " state (10)  A[0]:(0.0812503397465)\n",
      " state (11)  A[0]:(0.0994311273098)\n",
      " state (12)  A[0]:(0.118592642248)\n",
      " state (13)  A[0]:(0.137798309326)\n",
      " state (14)  A[0]:(0.156189441681)\n",
      " state (15)  A[0]:(0.173126906157)\n",
      "Episode 29000 finished after 6 . Running score: 0.02. Policy_loss: -95704.4974128, Value_loss: 1.01714200938. Times trained:               7425. Times reached goal: 10.               Steps done: 223144.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2499,  0.2498]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2499,  0.2498]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2499,  0.2498]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2503,  0.2502,  0.2496]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2499,  0.2498]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2500,  0.2497]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2501,  0.2500,  0.2497]])\n",
      "On state=2, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2500,  0.2497]])\n",
      "On state=1, selected action=2\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2501,  0.2500,  0.2497]])\n",
      "On state=2, selected action=0\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2501,  0.2500,  0.2497]])\n",
      "On state=2, selected action=2\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2501,  0.2500,  0.2497]])\n",
      "On state=2, selected action=0\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2500,  0.2497]])\n",
      "On state=1, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2499,  0.2498]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2500,  0.2497]])\n",
      "On state=1, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2499,  0.2498]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2500,  0.2497]])\n",
      "On state=1, selected action=1\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2501,  0.2500,  0.2497]])\n",
      "On state=2, selected action=1\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2496,  0.2505,  0.2504,  0.2496]])\n",
      "On state=6, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2490,  0.2508,  0.2507,  0.2494]])\n",
      "On state=10, selected action=2\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250297993422) A[1]:(0.250006079674) A[2]:(0.249906271696) A[3]:(0.249789685011)\n",
      " state (1)  A[0]:(0.250268638134) A[1]:(0.250037163496) A[2]:(0.249947011471) A[3]:(0.249747231603)\n",
      " state (2)  A[0]:(0.250176757574) A[1]:(0.250103980303) A[2]:(0.250015497208) A[3]:(0.249703735113)\n",
      " state (3)  A[0]:(0.250053733587) A[1]:(0.25018543005) A[2]:(0.250094592571) A[3]:(0.249666273594)\n",
      " state (4)  A[0]:(0.249915376306) A[1]:(0.250274270773) A[2]:(0.250179469585) A[3]:(0.249630942941)\n",
      " state (5)  A[0]:(0.249769091606) A[1]:(0.250366926193) A[2]:(0.250267475843) A[3]:(0.249596506357)\n",
      " state (6)  A[0]:(0.249618500471) A[1]:(0.25046145916) A[2]:(0.250357180834) A[3]:(0.249562889338)\n",
      " state (7)  A[0]:(0.249466121197) A[1]:(0.25055629015) A[2]:(0.250447243452) A[3]:(0.249530360103)\n",
      " state (8)  A[0]:(0.249314710498) A[1]:(0.250649690628) A[2]:(0.250536173582) A[3]:(0.24949939549)\n",
      " state (9)  A[0]:(0.249167501926) A[1]:(0.250739783049) A[2]:(0.250622183084) A[3]:(0.24947053194)\n",
      " state (10)  A[0]:(0.24902780354) A[1]:(0.25082463026) A[2]:(0.250703424215) A[3]:(0.249444171786)\n",
      " state (11)  A[0]:(0.248898595572) A[1]:(0.250902533531) A[2]:(0.250778228045) A[3]:(0.249420627952)\n",
      " state (12)  A[0]:(0.248782172799) A[1]:(0.250972360373) A[2]:(0.250845432281) A[3]:(0.249400064349)\n",
      " state (13)  A[0]:(0.248679697514) A[1]:(0.251033514738) A[2]:(0.250904411077) A[3]:(0.249382421374)\n",
      " state (14)  A[0]:(0.248591408134) A[1]:(0.251085996628) A[2]:(0.250955104828) A[3]:(0.249367520213)\n",
      " state (15)  A[0]:(0.248516693711) A[1]:(0.251130253077) A[2]:(0.250997930765) A[3]:(0.249355107546)\n",
      " state (0)  A[0]:(0.0157053750008)\n",
      " state (1)  A[0]:(0.0102551374584)\n",
      " state (2)  A[0]:(0.0116101875901)\n",
      " state (3)  A[0]:(0.0135688446462)\n",
      " state (4)  A[0]:(0.0163443125784)\n",
      " state (5)  A[0]:(0.0202194154263)\n",
      " state (6)  A[0]:(0.0255230758339)\n",
      " state (7)  A[0]:(0.0326108895242)\n",
      " state (8)  A[0]:(0.041824400425)\n",
      " state (9)  A[0]:(0.0534265637398)\n",
      " state (10)  A[0]:(0.0675182640553)\n",
      " state (11)  A[0]:(0.0839565321803)\n",
      " state (12)  A[0]:(0.102311015129)\n",
      " state (13)  A[0]:(0.121893756092)\n",
      " state (14)  A[0]:(0.141869276762)\n",
      " state (15)  A[0]:(0.161408126354)\n",
      "Episode 30000 finished after 19 . Running score: 0.01. Policy_loss: -95703.4814387, Value_loss: 1.02083672653. Times trained:               7467. Times reached goal: 17.               Steps done: 230611.\n",
      " state (0)  A[0]:(0.250235646963) A[1]:(0.249944895506) A[2]:(0.249842017889) A[3]:(0.24997740984)\n",
      " state (1)  A[0]:(0.250241607428) A[1]:(0.249983415008) A[2]:(0.249847263098) A[3]:(0.249927669764)\n",
      " state (2)  A[0]:(0.25013422966) A[1]:(0.250092566013) A[2]:(0.249872103333) A[3]:(0.249901160598)\n",
      " state (3)  A[0]:(0.249977007508) A[1]:(0.250233083963) A[2]:(0.249904572964) A[3]:(0.249885350466)\n",
      " state (4)  A[0]:(0.249795228243) A[1]:(0.250390380621) A[2]:(0.249940559268) A[3]:(0.24987386167)\n",
      " state (5)  A[0]:(0.249597802758) A[1]:(0.250558823347) A[2]:(0.249978423119) A[3]:(0.249864980578)\n",
      " state (6)  A[0]:(0.249388501048) A[1]:(0.250735610723) A[2]:(0.250017493963) A[3]:(0.249858364463)\n",
      " state (7)  A[0]:(0.249170720577) A[1]:(0.250918000937) A[2]:(0.250057220459) A[3]:(0.249854072928)\n",
      " state (8)  A[0]:(0.248948901892) A[1]:(0.251102209091) A[2]:(0.25009688735) A[3]:(0.249851986766)\n",
      " state (9)  A[0]:(0.248728737235) A[1]:(0.251283705235) A[2]:(0.250135630369) A[3]:(0.249851942062)\n",
      " state (10)  A[0]:(0.248516365886) A[1]:(0.25145766139) A[2]:(0.25017246604) A[3]:(0.249853536487)\n",
      " state (11)  A[0]:(0.248317316175) A[1]:(0.251619786024) A[2]:(0.250206619501) A[3]:(0.249856308103)\n",
      " state (12)  A[0]:(0.248135820031) A[1]:(0.251766949892) A[2]:(0.250237464905) A[3]:(0.249859809875)\n",
      " state (13)  A[0]:(0.24797424674) A[1]:(0.251897454262) A[2]:(0.250264704227) A[3]:(0.249863639474)\n",
      " state (14)  A[0]:(0.247833281755) A[1]:(0.252010941505) A[2]:(0.250288307667) A[3]:(0.249867483974)\n",
      " state (15)  A[0]:(0.247712269425) A[1]:(0.252108126879) A[2]:(0.250308454037) A[3]:(0.249871194363)\n",
      " state (0)  A[0]:(0.0146537218243)\n",
      " state (1)  A[0]:(0.00841281190515)\n",
      " state (2)  A[0]:(0.0095299705863)\n",
      " state (3)  A[0]:(0.0112360361964)\n",
      " state (4)  A[0]:(0.0136962030083)\n",
      " state (5)  A[0]:(0.017176406458)\n",
      " state (6)  A[0]:(0.0219992492348)\n",
      " state (7)  A[0]:(0.0285308025777)\n",
      " state (8)  A[0]:(0.0371509753168)\n",
      " state (9)  A[0]:(0.0481995791197)\n",
      " state (10)  A[0]:(0.0618969127536)\n",
      " state (11)  A[0]:(0.078249707818)\n",
      " state (12)  A[0]:(0.0969736948609)\n",
      " state (13)  A[0]:(0.117475792766)\n",
      " state (14)  A[0]:(0.138925880194)\n",
      " state (15)  A[0]:(0.160404026508)\n",
      "Episode 31000 finished after 8 . Running score: 0.01. Policy_loss: -95702.5884698, Value_loss: 1.02538871431. Times trained:               7667. Times reached goal: 16.               Steps done: 238278.\n",
      " state (0)  A[0]:(0.250037908554) A[1]:(0.249684765935) A[2]:(0.249940469861) A[3]:(0.250336885452)\n",
      " state (1)  A[0]:(0.250037282705) A[1]:(0.249791562557) A[2]:(0.249952480197) A[3]:(0.250218689442)\n",
      " state (2)  A[0]:(0.249847307801) A[1]:(0.249995574355) A[2]:(0.250015318394) A[3]:(0.250141859055)\n",
      " state (3)  A[0]:(0.249573528767) A[1]:(0.250253736973) A[2]:(0.250098615885) A[3]:(0.25007417798)\n",
      " state (4)  A[0]:(0.249251097441) A[1]:(0.250545561314) A[2]:(0.250193804502) A[3]:(0.250009566545)\n",
      " state (5)  A[0]:(0.248890861869) A[1]:(0.250864088535) A[2]:(0.250298112631) A[3]:(0.249946966767)\n",
      " state (6)  A[0]:(0.248498111963) A[1]:(0.251205295324) A[2]:(0.250410020351) A[3]:(0.249886617064)\n",
      " state (7)  A[0]:(0.24808062613) A[1]:(0.251562774181) A[2]:(0.250527381897) A[3]:(0.249829277396)\n",
      " state (8)  A[0]:(0.24765111506) A[1]:(0.25192630291) A[2]:(0.250646710396) A[3]:(0.249775856733)\n",
      " state (9)  A[0]:(0.247226148844) A[1]:(0.252282857895) A[2]:(0.250763714314) A[3]:(0.249727249146)\n",
      " state (10)  A[0]:(0.246822819114) A[1]:(0.252619087696) A[2]:(0.250873953104) A[3]:(0.249684140086)\n",
      " state (11)  A[0]:(0.246455237269) A[1]:(0.252924114466) A[2]:(0.250973820686) A[3]:(0.249646782875)\n",
      " state (12)  A[0]:(0.246132299304) A[1]:(0.253191381693) A[2]:(0.251061201096) A[3]:(0.24961514771)\n",
      " state (13)  A[0]:(0.24585711956) A[1]:(0.253418684006) A[2]:(0.251135379076) A[3]:(0.249588787556)\n",
      " state (14)  A[0]:(0.245628386736) A[1]:(0.25360751152) A[2]:(0.251196950674) A[3]:(0.24956715107)\n",
      " state (15)  A[0]:(0.245441794395) A[1]:(0.253761559725) A[2]:(0.25124707818) A[3]:(0.249549597502)\n",
      " state (0)  A[0]:(0.00891187228262)\n",
      " state (1)  A[0]:(0.0043208822608)\n",
      " state (2)  A[0]:(0.00626644305885)\n",
      " state (3)  A[0]:(0.0095124989748)\n",
      " state (4)  A[0]:(0.0146784242243)\n",
      " state (5)  A[0]:(0.0226350333542)\n",
      " state (6)  A[0]:(0.0344126671553)\n",
      " state (7)  A[0]:(0.0510286167264)\n",
      " state (8)  A[0]:(0.0731426849961)\n",
      " state (9)  A[0]:(0.10059005022)\n",
      " state (10)  A[0]:(0.132055580616)\n",
      " state (11)  A[0]:(0.16524989903)\n",
      " state (12)  A[0]:(0.197621196508)\n",
      " state (13)  A[0]:(0.227146402001)\n",
      " state (14)  A[0]:(0.252718925476)\n",
      " state (15)  A[0]:(0.274072021246)\n",
      "Episode 32000 finished after 11 . Running score: 0.03. Policy_loss: -95701.5507953, Value_loss: 1.01716976582. Times trained:               7761. Times reached goal: 18.               Steps done: 246039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250729203224) A[1]:(0.249881282449) A[2]:(0.249825596809) A[3]:(0.249563917518)\n",
      " state (1)  A[0]:(0.250721782446) A[1]:(0.250109940767) A[2]:(0.249864861369) A[3]:(0.249303370714)\n",
      " state (2)  A[0]:(0.25058221817) A[1]:(0.250444322824) A[2]:(0.249936625361) A[3]:(0.249036863446)\n",
      " state (3)  A[0]:(0.250363707542) A[1]:(0.250849038363) A[2]:(0.250027090311) A[3]:(0.248760208488)\n",
      " state (4)  A[0]:(0.250097751617) A[1]:(0.251308053732) A[2]:(0.250129789114) A[3]:(0.248464375734)\n",
      " state (5)  A[0]:(0.249796956778) A[1]:(0.251813590527) A[2]:(0.250241816044) A[3]:(0.248147577047)\n",
      " state (6)  A[0]:(0.249467402697) A[1]:(0.252358466387) A[2]:(0.250361174345) A[3]:(0.247812896967)\n",
      " state (7)  A[0]:(0.249116510153) A[1]:(0.252930253744) A[2]:(0.250485092402) A[3]:(0.247468158603)\n",
      " state (8)  A[0]:(0.248755931854) A[1]:(0.253509521484) A[2]:(0.250609487295) A[3]:(0.247125074267)\n",
      " state (9)  A[0]:(0.248400896788) A[1]:(0.254072397947) A[2]:(0.250729441643) A[3]:(0.24679723382)\n",
      " state (10)  A[0]:(0.248067066073) A[1]:(0.254595607519) A[2]:(0.250840246677) A[3]:(0.246497124434)\n",
      " state (11)  A[0]:(0.247766822577) A[1]:(0.25506144762) A[2]:(0.250938385725) A[3]:(0.246233358979)\n",
      " state (12)  A[0]:(0.247507452965) A[1]:(0.255460590124) A[2]:(0.251022189856) A[3]:(0.246009781957)\n",
      " state (13)  A[0]:(0.247290819883) A[1]:(0.255791753531) A[2]:(0.251091480255) A[3]:(0.24582593143)\n",
      " state (14)  A[0]:(0.247114628553) A[1]:(0.256059587002) A[2]:(0.251147419214) A[3]:(0.245678290725)\n",
      " state (15)  A[0]:(0.246974274516) A[1]:(0.256272047758) A[2]:(0.25119176507) A[3]:(0.245561867952)\n",
      " state (0)  A[0]:(0.00981764122844)\n",
      " state (1)  A[0]:(0.00870629400015)\n",
      " state (2)  A[0]:(0.0103340819478)\n",
      " state (3)  A[0]:(0.012714676559)\n",
      " state (4)  A[0]:(0.0162417348474)\n",
      " state (5)  A[0]:(0.0214597396553)\n",
      " state (6)  A[0]:(0.0290320031345)\n",
      " state (7)  A[0]:(0.0397050455213)\n",
      " state (8)  A[0]:(0.0542104169726)\n",
      " state (9)  A[0]:(0.073075518012)\n",
      " state (10)  A[0]:(0.0963612049818)\n",
      " state (11)  A[0]:(0.123432181776)\n",
      " state (12)  A[0]:(0.15293520689)\n",
      " state (13)  A[0]:(0.183081969619)\n",
      " state (14)  A[0]:(0.212117552757)\n",
      " state (15)  A[0]:(0.238718539476)\n",
      "Episode 33000 finished after 5 . Running score: 0.01. Policy_loss: -95691.4887793, Value_loss: 1.03971311644. Times trained:               7716. Times reached goal: 12.               Steps done: 253755.\n",
      " state (0)  A[0]:(0.25040358305) A[1]:(0.249825209379) A[2]:(0.249779030681) A[3]:(0.249992147088)\n",
      " state (1)  A[0]:(0.250514745712) A[1]:(0.249888509512) A[2]:(0.249786138535) A[3]:(0.24981059134)\n",
      " state (2)  A[0]:(0.2504760921) A[1]:(0.250065535307) A[2]:(0.249824762344) A[3]:(0.249633595347)\n",
      " state (3)  A[0]:(0.250354528427) A[1]:(0.250307947397) A[2]:(0.249881610274) A[3]:(0.249455854297)\n",
      " state (4)  A[0]:(0.250185340643) A[1]:(0.250595986843) A[2]:(0.249949961901) A[3]:(0.249268710613)\n",
      " state (5)  A[0]:(0.249984458089) A[1]:(0.250920087099) A[2]:(0.25002643466) A[3]:(0.249069079757)\n",
      " state (6)  A[0]:(0.249757185578) A[1]:(0.251277059317) A[2]:(0.250109672546) A[3]:(0.248856022954)\n",
      " state (7)  A[0]:(0.249505475163) A[1]:(0.251665145159) A[2]:(0.250199168921) A[3]:(0.248630180955)\n",
      " state (8)  A[0]:(0.249231353402) A[1]:(0.252081096172) A[2]:(0.25029399991) A[3]:(0.248393550515)\n",
      " state (9)  A[0]:(0.248938634992) A[1]:(0.252518802881) A[2]:(0.250392824411) A[3]:(0.248149722815)\n",
      " state (10)  A[0]:(0.24863345921) A[1]:(0.252969145775) A[2]:(0.250493615866) A[3]:(0.247903749347)\n",
      " state (11)  A[0]:(0.248323813081) A[1]:(0.253420710564) A[2]:(0.250593900681) A[3]:(0.247661545873)\n",
      " state (12)  A[0]:(0.248018592596) A[1]:(0.253861218691) A[2]:(0.250691115856) A[3]:(0.247429028153)\n",
      " state (13)  A[0]:(0.247726291418) A[1]:(0.254279404879) A[2]:(0.250782936811) A[3]:(0.247211396694)\n",
      " state (14)  A[0]:(0.247453764081) A[1]:(0.254666298628) A[2]:(0.250867456198) A[3]:(0.247012451291)\n",
      " state (15)  A[0]:(0.24720582366) A[1]:(0.25501614809) A[2]:(0.250943630934) A[3]:(0.246834397316)\n",
      " state (0)  A[0]:(0.0117716770619)\n",
      " state (1)  A[0]:(0.00972767546773)\n",
      " state (2)  A[0]:(0.0107024908066)\n",
      " state (3)  A[0]:(0.0120438951999)\n",
      " state (4)  A[0]:(0.0139261912555)\n",
      " state (5)  A[0]:(0.0165766086429)\n",
      " state (6)  A[0]:(0.0202726833522)\n",
      " state (7)  A[0]:(0.0253478623927)\n",
      " state (8)  A[0]:(0.0321869440377)\n",
      " state (9)  A[0]:(0.0412051454186)\n",
      " state (10)  A[0]:(0.0528037548065)\n",
      " state (11)  A[0]:(0.0672965422273)\n",
      " state (12)  A[0]:(0.0848111137748)\n",
      " state (13)  A[0]:(0.105189479887)\n",
      " state (14)  A[0]:(0.127932325006)\n",
      " state (15)  A[0]:(0.152230486274)\n",
      "Episode 34000 finished after 7 . Running score: 0.0. Policy_loss: -95690.8735802, Value_loss: 1.05089843474. Times trained:               7743. Times reached goal: 15.               Steps done: 261498.\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2498,  0.2497,  0.2504]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2498,  0.2497,  0.2504]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2498,  0.2497,  0.2504]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2498,  0.2497,  0.2504]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2498,  0.2497,  0.2504]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2498,  0.2497,  0.2504]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2498,  0.2497,  0.2504]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2503,  0.2499,  0.2500]])\n",
      "On state=4, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2488,  0.2514,  0.2502,  0.2496]])\n",
      "On state=8, selected action=3\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2488,  0.2514,  0.2503,  0.2496]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250113368034) A[1]:(0.249761596322) A[2]:(0.249756604433) A[3]:(0.250368475914)\n",
      " state (1)  A[0]:(0.25019878149) A[1]:(0.249773904681) A[2]:(0.249761208892) A[3]:(0.250266134739)\n",
      " state (2)  A[0]:(0.250152349472) A[1]:(0.249880760908) A[2]:(0.249794617295) A[3]:(0.250172317028)\n",
      " state (3)  A[0]:(0.250026851892) A[1]:(0.250046372414) A[2]:(0.249846607447) A[3]:(0.250080168247)\n",
      " state (4)  A[0]:(0.249844953418) A[1]:(0.250257283449) A[2]:(0.24991260469) A[3]:(0.249985158443)\n",
      " state (5)  A[0]:(0.249621272087) A[1]:(0.250503838062) A[2]:(0.249989032745) A[3]:(0.249885827303)\n",
      " state (6)  A[0]:(0.249362498522) A[1]:(0.250781565905) A[2]:(0.25007417798) A[3]:(0.249781757593)\n",
      " state (7)  A[0]:(0.249071359634) A[1]:(0.251088410616) A[2]:(0.250167191029) A[3]:(0.249673068523)\n",
      " state (8)  A[0]:(0.248750016093) A[1]:(0.251422256231) A[2]:(0.250267297029) A[3]:(0.249560400844)\n",
      " state (9)  A[0]:(0.248402193189) A[1]:(0.251779407263) A[2]:(0.250373393297) A[3]:(0.249445006251)\n",
      " state (10)  A[0]:(0.248034000397) A[1]:(0.25215369463) A[2]:(0.250483632088) A[3]:(0.249328643084)\n",
      " state (11)  A[0]:(0.247654035687) A[1]:(0.252536773682) A[2]:(0.250595659018) A[3]:(0.249213546515)\n",
      " state (12)  A[0]:(0.247272461653) A[1]:(0.252918809652) A[2]:(0.250706642866) A[3]:(0.249102026224)\n",
      " state (13)  A[0]:(0.246899798512) A[1]:(0.253289908171) A[2]:(0.250813901424) A[3]:(0.248996347189)\n",
      " state (14)  A[0]:(0.246545433998) A[1]:(0.253641307354) A[2]:(0.250915020704) A[3]:(0.248898297548)\n",
      " state (15)  A[0]:(0.246216520667) A[1]:(0.253966271877) A[2]:(0.251008123159) A[3]:(0.248809039593)\n",
      " state (0)  A[0]:(0.0241028126329)\n",
      " state (1)  A[0]:(0.0200233068317)\n",
      " state (2)  A[0]:(0.0213523693383)\n",
      " state (3)  A[0]:(0.0235453061759)\n",
      " state (4)  A[0]:(0.0270643699914)\n",
      " state (5)  A[0]:(0.0325657576323)\n",
      " state (6)  A[0]:(0.0409113317728)\n",
      " state (7)  A[0]:(0.0531378239393)\n",
      " state (8)  A[0]:(0.0703229010105)\n",
      " state (9)  A[0]:(0.0932829529047)\n",
      " state (10)  A[0]:(0.122112974524)\n",
      " state (11)  A[0]:(0.155781313777)\n",
      " state (12)  A[0]:(0.192146003246)\n",
      " state (13)  A[0]:(0.228543743491)\n",
      " state (14)  A[0]:(0.262613683939)\n",
      " state (15)  A[0]:(0.292840301991)\n",
      "Episode 35000 finished after 10 . Running score: 0.02. Policy_loss: -95689.6907649, Value_loss: 1.50146126936. Times trained:               7495. Times reached goal: 18.               Steps done: 268993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250446170568) A[1]:(0.250061273575) A[2]:(0.249757915735) A[3]:(0.249734655023)\n",
      " state (1)  A[0]:(0.25048160553) A[1]:(0.250103056431) A[2]:(0.249801889062) A[3]:(0.249613404274)\n",
      " state (2)  A[0]:(0.250339031219) A[1]:(0.25029155612) A[2]:(0.249893546104) A[3]:(0.249475896358)\n",
      " state (3)  A[0]:(0.250081539154) A[1]:(0.250569194555) A[2]:(0.2500179708) A[3]:(0.249331340194)\n",
      " state (4)  A[0]:(0.249744907022) A[1]:(0.250913798809) A[2]:(0.250167220831) A[3]:(0.249174013734)\n",
      " state (5)  A[0]:(0.249344915152) A[1]:(0.251315891743) A[2]:(0.250337511301) A[3]:(0.249001681805)\n",
      " state (6)  A[0]:(0.248886466026) A[1]:(0.251772046089) A[2]:(0.250527352095) A[3]:(0.248814105988)\n",
      " state (7)  A[0]:(0.24837449193) A[1]:(0.25227740407) A[2]:(0.250734716654) A[3]:(0.248613402247)\n",
      " state (8)  A[0]:(0.247819721699) A[1]:(0.252820909023) A[2]:(0.250955253839) A[3]:(0.248404115438)\n",
      " state (9)  A[0]:(0.247240751982) A[1]:(0.253384411335) A[2]:(0.251181811094) A[3]:(0.248193055391)\n",
      " state (10)  A[0]:(0.246661677957) A[1]:(0.253944754601) A[2]:(0.251405477524) A[3]:(0.247988075018)\n",
      " state (11)  A[0]:(0.24610760808) A[1]:(0.254478394985) A[2]:(0.251617223024) A[3]:(0.247796773911)\n",
      " state (12)  A[0]:(0.245599448681) A[1]:(0.254965960979) A[2]:(0.251809805632) A[3]:(0.247624754906)\n",
      " state (13)  A[0]:(0.245150759816) A[1]:(0.255395233631) A[2]:(0.251978725195) A[3]:(0.247475251555)\n",
      " state (14)  A[0]:(0.244767069817) A[1]:(0.255761504173) A[2]:(0.252122461796) A[3]:(0.247348949313)\n",
      " state (15)  A[0]:(0.244447350502) A[1]:(0.256066203117) A[2]:(0.252241760492) A[3]:(0.247244715691)\n",
      " state (0)  A[0]:(0.0217191576958)\n",
      " state (1)  A[0]:(0.0172469504178)\n",
      " state (2)  A[0]:(0.0184428561479)\n",
      " state (3)  A[0]:(0.0204301644117)\n",
      " state (4)  A[0]:(0.0236547850072)\n",
      " state (5)  A[0]:(0.0287576820701)\n",
      " state (6)  A[0]:(0.036606810987)\n",
      " state (7)  A[0]:(0.0482966937125)\n",
      " state (8)  A[0]:(0.065057978034)\n",
      " state (9)  A[0]:(0.0879918411374)\n",
      " state (10)  A[0]:(0.117584049702)\n",
      " state (11)  A[0]:(0.153153687716)\n",
      " state (12)  A[0]:(0.192652881145)\n",
      " state (13)  A[0]:(0.233150854707)\n",
      " state (14)  A[0]:(0.271779000759)\n",
      " state (15)  A[0]:(0.306509405375)\n",
      "Episode 36000 finished after 27 . Running score: 0.02. Policy_loss: -95689.2624529, Value_loss: 1.00821568834. Times trained:               7590. Times reached goal: 15.               Steps done: 276583.\n",
      " state (0)  A[0]:(0.250032305717) A[1]:(0.250041097403) A[2]:(0.249947845936) A[3]:(0.249978780746)\n",
      " state (1)  A[0]:(0.250165253878) A[1]:(0.250065147877) A[2]:(0.249941706657) A[3]:(0.249827891588)\n",
      " state (2)  A[0]:(0.250133186579) A[1]:(0.250214248896) A[2]:(0.249985292554) A[3]:(0.249667301774)\n",
      " state (3)  A[0]:(0.249993726611) A[1]:(0.250440388918) A[2]:(0.250058948994) A[3]:(0.249506875873)\n",
      " state (4)  A[0]:(0.24977658689) A[1]:(0.250729441643) A[2]:(0.250156134367) A[3]:(0.249337866902)\n",
      " state (5)  A[0]:(0.249500244856) A[1]:(0.251071870327) A[2]:(0.250272274017) A[3]:(0.249155655503)\n",
      " state (6)  A[0]:(0.249171748757) A[1]:(0.251464664936) A[2]:(0.250405728817) A[3]:(0.248957812786)\n",
      " state (7)  A[0]:(0.248792752624) A[1]:(0.251907646656) A[2]:(0.250556081533) A[3]:(0.248743504286)\n",
      " state (8)  A[0]:(0.248364269733) A[1]:(0.252399682999) A[2]:(0.250722765923) A[3]:(0.248513236642)\n",
      " state (9)  A[0]:(0.247890084982) A[1]:(0.252936452627) A[2]:(0.250904142857) A[3]:(0.248269349337)\n",
      " state (10)  A[0]:(0.247378140688) A[1]:(0.253508925438) A[2]:(0.251097023487) A[3]:(0.248015895486)\n",
      " state (11)  A[0]:(0.246841013432) A[1]:(0.254103600979) A[2]:(0.251296788454) A[3]:(0.247758612037)\n",
      " state (12)  A[0]:(0.246294468641) A[1]:(0.254703700542) A[2]:(0.251497715712) A[3]:(0.2475040555)\n",
      " state (13)  A[0]:(0.245755538344) A[1]:(0.255291640759) A[2]:(0.251693993807) A[3]:(0.247258856893)\n",
      " state (14)  A[0]:(0.245239764452) A[1]:(0.255851328373) A[2]:(0.251880228519) A[3]:(0.247028648853)\n",
      " state (15)  A[0]:(0.244759649038) A[1]:(0.256370335817) A[2]:(0.252052396536) A[3]:(0.246817648411)\n",
      " state (0)  A[0]:(0.0198191143572)\n",
      " state (1)  A[0]:(0.0154566597193)\n",
      " state (2)  A[0]:(0.016393609345)\n",
      " state (3)  A[0]:(0.0179173015058)\n",
      " state (4)  A[0]:(0.0203516166657)\n",
      " state (5)  A[0]:(0.0241643544286)\n",
      " state (6)  A[0]:(0.0299978461117)\n",
      " state (7)  A[0]:(0.0386931374669)\n",
      " state (8)  A[0]:(0.0512781962752)\n",
      " state (9)  A[0]:(0.0688686519861)\n",
      " state (10)  A[0]:(0.0924114584923)\n",
      " state (11)  A[0]:(0.122254446149)\n",
      " state (12)  A[0]:(0.157697558403)\n",
      " state (13)  A[0]:(0.196862697601)\n",
      " state (14)  A[0]:(0.237115681171)\n",
      " state (15)  A[0]:(0.275847017765)\n",
      "Episode 37000 finished after 2 . Running score: 0.0. Policy_loss: -95687.9495596, Value_loss: 1.02606665405. Times trained:               7418. Times reached goal: 22.               Steps done: 284001.\n",
      " state (0)  A[0]:(0.250436246395) A[1]:(0.24967366457) A[2]:(0.24998036027) A[3]:(0.249909713864)\n",
      " state (1)  A[0]:(0.25050470233) A[1]:(0.24968791008) A[2]:(0.249984994531) A[3]:(0.249822422862)\n",
      " state (2)  A[0]:(0.250430166721) A[1]:(0.249802023172) A[2]:(0.25004029274) A[3]:(0.249727487564)\n",
      " state (3)  A[0]:(0.250251352787) A[1]:(0.249985858798) A[2]:(0.250127285719) A[3]:(0.249635517597)\n",
      " state (4)  A[0]:(0.24998986721) A[1]:(0.250227957964) A[2]:(0.250241279602) A[3]:(0.249540939927)\n",
      " state (5)  A[0]:(0.249662324786) A[1]:(0.250518620014) A[2]:(0.250378400087) A[3]:(0.249440595508)\n",
      " state (6)  A[0]:(0.249275624752) A[1]:(0.25085401535) A[2]:(0.250537335873) A[3]:(0.249332949519)\n",
      " state (7)  A[0]:(0.248831257224) A[1]:(0.251233428717) A[2]:(0.250717967749) A[3]:(0.249217405915)\n",
      " state (8)  A[0]:(0.24833008647) A[1]:(0.251655995846) A[2]:(0.250919818878) A[3]:(0.249094128609)\n",
      " state (9)  A[0]:(0.247776076198) A[1]:(0.252118349075) A[2]:(0.251141309738) A[3]:(0.248964309692)\n",
      " state (10)  A[0]:(0.247178167105) A[1]:(0.252613097429) A[2]:(0.251378715038) A[3]:(0.24883005023)\n",
      " state (11)  A[0]:(0.246550783515) A[1]:(0.253128647804) A[2]:(0.251626223326) A[3]:(0.248694300652)\n",
      " state (12)  A[0]:(0.245912536979) A[1]:(0.253650307655) A[2]:(0.251876711845) A[3]:(0.248560458422)\n",
      " state (13)  A[0]:(0.245283529162) A[1]:(0.254162222147) A[2]:(0.252122312784) A[3]:(0.248431921005)\n",
      " state (14)  A[0]:(0.24468255043) A[1]:(0.254649817944) A[2]:(0.252356052399) A[3]:(0.248311638832)\n",
      " state (15)  A[0]:(0.244124516845) A[1]:(0.255101501942) A[2]:(0.252572268248) A[3]:(0.248201727867)\n",
      " state (0)  A[0]:(0.0158640239388)\n",
      " state (1)  A[0]:(0.0121687985957)\n",
      " state (2)  A[0]:(0.0130833387375)\n",
      " state (3)  A[0]:(0.0146076474339)\n",
      " state (4)  A[0]:(0.0171036422253)\n",
      " state (5)  A[0]:(0.0211036391556)\n",
      " state (6)  A[0]:(0.0273532271385)\n",
      " state (7)  A[0]:(0.0368437021971)\n",
      " state (8)  A[0]:(0.0507934838533)\n",
      " state (9)  A[0]:(0.0704983398318)\n",
      " state (10)  A[0]:(0.0969520807266)\n",
      " state (11)  A[0]:(0.13025161624)\n",
      " state (12)  A[0]:(0.16909314692)\n",
      " state (13)  A[0]:(0.210850968957)\n",
      " state (14)  A[0]:(0.252371400595)\n",
      " state (15)  A[0]:(0.290973305702)\n",
      "Episode 38000 finished after 11 . Running score: 0.01. Policy_loss: -95692.3713181, Value_loss: 1.00560216382. Times trained:               7808. Times reached goal: 15.               Steps done: 291809.\n",
      " state (0)  A[0]:(0.25006493926) A[1]:(0.250177562237) A[2]:(0.250038683414) A[3]:(0.249718755484)\n",
      " state (1)  A[0]:(0.250119745731) A[1]:(0.250191599131) A[2]:(0.250078052282) A[3]:(0.249610543251)\n",
      " state (2)  A[0]:(0.249951943755) A[1]:(0.250362962484) A[2]:(0.250187605619) A[3]:(0.249497503042)\n",
      " state (3)  A[0]:(0.249614939094) A[1]:(0.250648111105) A[2]:(0.250349104404) A[3]:(0.249387845397)\n",
      " state (4)  A[0]:(0.249139457941) A[1]:(0.251030534506) A[2]:(0.250557899475) A[3]:(0.249272152781)\n",
      " state (5)  A[0]:(0.248539581895) A[1]:(0.25150308013) A[2]:(0.250812381506) A[3]:(0.249144986272)\n",
      " state (6)  A[0]:(0.247815564275) A[1]:(0.252066701651) A[2]:(0.251113802195) A[3]:(0.249003887177)\n",
      " state (7)  A[0]:(0.246967330575) A[1]:(0.25272154808) A[2]:(0.251462459564) A[3]:(0.248848617077)\n",
      " state (8)  A[0]:(0.246005624533) A[1]:(0.253459364176) A[2]:(0.251853823662) A[3]:(0.248681232333)\n",
      " state (9)  A[0]:(0.244957923889) A[1]:(0.254259377718) A[2]:(0.252276688814) A[3]:(0.248505994678)\n",
      " state (10)  A[0]:(0.243867620826) A[1]:(0.25508928299) A[2]:(0.252713859081) A[3]:(0.2483291924)\n",
      " state (11)  A[0]:(0.242786228657) A[1]:(0.255910873413) A[2]:(0.253145217896) A[3]:(0.248157694936)\n",
      " state (12)  A[0]:(0.241762310266) A[1]:(0.256688028574) A[2]:(0.253551930189) A[3]:(0.247997745872)\n",
      " state (13)  A[0]:(0.240832656622) A[1]:(0.257393479347) A[2]:(0.253919988871) A[3]:(0.247853919864)\n",
      " state (14)  A[0]:(0.240018174052) A[1]:(0.258011609316) A[2]:(0.254241645336) A[3]:(0.247728556395)\n",
      " state (15)  A[0]:(0.239324972034) A[1]:(0.258537918329) A[2]:(0.25451490283) A[3]:(0.247622162104)\n",
      " state (0)  A[0]:(0.0142759755254)\n",
      " state (1)  A[0]:(0.0106381438673)\n",
      " state (2)  A[0]:(0.0117276255041)\n",
      " state (3)  A[0]:(0.0136718805879)\n",
      " state (4)  A[0]:(0.0170655865222)\n",
      " state (5)  A[0]:(0.0228360053152)\n",
      " state (6)  A[0]:(0.0323633477092)\n",
      " state (7)  A[0]:(0.0475916787982)\n",
      " state (8)  A[0]:(0.0709938034415)\n",
      " state (9)  A[0]:(0.105077557266)\n",
      " state (10)  A[0]:(0.151123002172)\n",
      " state (11)  A[0]:(0.207570880651)\n",
      " state (12)  A[0]:(0.269610971212)\n",
      " state (13)  A[0]:(0.330979824066)\n",
      " state (14)  A[0]:(0.38657900691)\n",
      " state (15)  A[0]:(0.433805316687)\n",
      "Episode 39000 finished after 4 . Running score: 0.04. Policy_loss: -95685.6197163, Value_loss: 1.36137769921. Times trained:               7871. Times reached goal: 28.               Steps done: 299680.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.2504,  0.2500,  0.2498,  0.2498]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2500,  0.2498,  0.2498]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2500,  0.2498,  0.2498]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2501,  0.2498,  0.2494]])\n",
      "On state=1, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2501,  0.2498,  0.2494]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2500,  0.2498,  0.2494]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=2\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2500,  0.2498,  0.2494]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=2\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=2\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=1\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2510,  0.2502,  0.2486]])\n",
      "On state=4, selected action=3\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2510,  0.2502,  0.2486]])\n",
      "On state=4, selected action=2\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2498,  0.2498]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2500,  0.2498,  0.2494]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2500,  0.2498,  0.2494]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2503,  0.2499,  0.2491]])\n",
      "On state=2, selected action=0\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2500,  0.2498,  0.2494]])\n",
      "On state=1, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250460892916) A[1]:(0.2499371171) A[2]:(0.249801248312) A[3]:(0.249800726771)\n",
      " state (1)  A[0]:(0.250712960958) A[1]:(0.250027686357) A[2]:(0.249816149473) A[3]:(0.249443158507)\n",
      " state (2)  A[0]:(0.250729709864) A[1]:(0.250255376101) A[2]:(0.249892219901) A[3]:(0.24912263453)\n",
      " state (3)  A[0]:(0.250550150871) A[1]:(0.250589430332) A[2]:(0.250017881393) A[3]:(0.248842552304)\n",
      " state (4)  A[0]:(0.250204145908) A[1]:(0.251030266285) A[2]:(0.250191062689) A[3]:(0.248574495316)\n",
      " state (5)  A[0]:(0.249715894461) A[1]:(0.251576423645) A[2]:(0.25040897727) A[3]:(0.248298674822)\n",
      " state (6)  A[0]:(0.249094203115) A[1]:(0.252230137587) A[2]:(0.2506711483) A[3]:(0.248004481196)\n",
      " state (7)  A[0]:(0.248342067003) A[1]:(0.252992480993) A[2]:(0.250977247953) A[3]:(0.247688189149)\n",
      " state (8)  A[0]:(0.247468441725) A[1]:(0.253856122494) A[2]:(0.251323759556) A[3]:(0.247351676226)\n",
      " state (9)  A[0]:(0.24649630487) A[1]:(0.254800021648) A[2]:(0.251701831818) A[3]:(0.247001871467)\n",
      " state (10)  A[0]:(0.245464324951) A[1]:(0.255789130926) A[2]:(0.252096921206) A[3]:(0.246649652719)\n",
      " state (11)  A[0]:(0.244421377778) A[1]:(0.256779581308) A[2]:(0.252491295338) A[3]:(0.246307730675)\n",
      " state (12)  A[0]:(0.243416845798) A[1]:(0.257727563381) A[2]:(0.252867460251) A[3]:(0.245988100767)\n",
      " state (13)  A[0]:(0.242490932345) A[1]:(0.258597642183) A[2]:(0.253211468458) A[3]:(0.245699897408)\n",
      " state (14)  A[0]:(0.24166944623) A[1]:(0.259367525578) A[2]:(0.2535148561) A[3]:(0.24544814229)\n",
      " state (15)  A[0]:(0.240963175893) A[1]:(0.260028332472) A[2]:(0.253774493933) A[3]:(0.245234057307)\n",
      " state (0)  A[0]:(0.0187807157636)\n",
      " state (1)  A[0]:(0.0176342837512)\n",
      " state (2)  A[0]:(0.0183320976794)\n",
      " state (3)  A[0]:(0.0194551050663)\n",
      " state (4)  A[0]:(0.0212661270052)\n",
      " state (5)  A[0]:(0.024148479104)\n",
      " state (6)  A[0]:(0.0286515764892)\n",
      " state (7)  A[0]:(0.0355413220823)\n",
      " state (8)  A[0]:(0.0458463951945)\n",
      " state (9)  A[0]:(0.0608663223684)\n",
      " state (10)  A[0]:(0.0820616409183)\n",
      " state (11)  A[0]:(0.110713802278)\n",
      " state (12)  A[0]:(0.147321581841)\n",
      " state (13)  A[0]:(0.190972819924)\n",
      " state (14)  A[0]:(0.239192575216)\n",
      " state (15)  A[0]:(0.288570672274)\n",
      "Episode 40000 finished after 28 . Running score: 0.0. Policy_loss: -95685.5521774, Value_loss: 1.04097204427. Times trained:               7495. Times reached goal: 11.               Steps done: 307175.\n",
      " state (0)  A[0]:(0.25016990304) A[1]:(0.249762892723) A[2]:(0.250094622374) A[3]:(0.249972522259)\n",
      " state (1)  A[0]:(0.250505238771) A[1]:(0.249651014805) A[2]:(0.250049829483) A[3]:(0.249793887138)\n",
      " state (2)  A[0]:(0.25068295002) A[1]:(0.249642699957) A[2]:(0.250063091516) A[3]:(0.249611318111)\n",
      " state (3)  A[0]:(0.250732153654) A[1]:(0.249701097608) A[2]:(0.250108510256) A[3]:(0.249458163977)\n",
      " state (4)  A[0]:(0.250664919615) A[1]:(0.249824911356) A[2]:(0.250186294317) A[3]:(0.24932384491)\n",
      " state (5)  A[0]:(0.250505775213) A[1]:(0.25000411272) A[2]:(0.250292807817) A[3]:(0.249197378755)\n",
      " state (6)  A[0]:(0.250273436308) A[1]:(0.250230431557) A[2]:(0.250425100327) A[3]:(0.249070972204)\n",
      " state (7)  A[0]:(0.249977096915) A[1]:(0.250500440598) A[2]:(0.250582426786) A[3]:(0.248940005898)\n",
      " state (8)  A[0]:(0.249619081616) A[1]:(0.250813752413) A[2]:(0.250765353441) A[3]:(0.248801872134)\n",
      " state (9)  A[0]:(0.249198615551) A[1]:(0.251171380281) A[2]:(0.250974804163) A[3]:(0.248655170202)\n",
      " state (10)  A[0]:(0.248714894056) A[1]:(0.251574009657) A[2]:(0.251211494207) A[3]:(0.248499646783)\n",
      " state (11)  A[0]:(0.248168990016) A[1]:(0.252020508051) A[2]:(0.251474797726) A[3]:(0.248335763812)\n",
      " state (12)  A[0]:(0.247565507889) A[1]:(0.252507150173) A[2]:(0.251762449741) A[3]:(0.248164892197)\n",
      " state (13)  A[0]:(0.24691323936) A[1]:(0.253027230501) A[2]:(0.252070456743) A[3]:(0.247989133)\n",
      " state (14)  A[0]:(0.246224686503) A[1]:(0.253571182489) A[2]:(0.252392917871) A[3]:(0.247811168432)\n",
      " state (15)  A[0]:(0.245515346527) A[1]:(0.254127621651) A[2]:(0.252722978592) A[3]:(0.247634083033)\n",
      " state (0)  A[0]:(0.0184138640761)\n",
      " state (1)  A[0]:(0.0156734343618)\n",
      " state (2)  A[0]:(0.0161845367402)\n",
      " state (3)  A[0]:(0.0170151088387)\n",
      " state (4)  A[0]:(0.0183422528207)\n",
      " state (5)  A[0]:(0.0204305201769)\n",
      " state (6)  A[0]:(0.0236621052027)\n",
      " state (7)  A[0]:(0.0285736806691)\n",
      " state (8)  A[0]:(0.0358985289931)\n",
      " state (9)  A[0]:(0.0466019101441)\n",
      " state (10)  A[0]:(0.0618728362024)\n",
      " state (11)  A[0]:(0.0829994380474)\n",
      " state (12)  A[0]:(0.111044280231)\n",
      " state (13)  A[0]:(0.146325707436)\n",
      " state (14)  A[0]:(0.18793182075)\n",
      " state (15)  A[0]:(0.233654648066)\n",
      "Episode 41000 finished after 18 . Running score: 0.0. Policy_loss: -95678.2581219, Value_loss: 1.00545094251. Times trained:               7811. Times reached goal: 17.               Steps done: 314986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.249892756343) A[1]:(0.249729409814) A[2]:(0.250022232533) A[3]:(0.250355660915)\n",
      " state (1)  A[0]:(0.250193685293) A[1]:(0.249684870243) A[2]:(0.249991178513) A[3]:(0.250130295753)\n",
      " state (2)  A[0]:(0.250370323658) A[1]:(0.249737247825) A[2]:(0.250011473894) A[3]:(0.249880895019)\n",
      " state (3)  A[0]:(0.250429272652) A[1]:(0.249849170446) A[2]:(0.250063061714) A[3]:(0.249658465385)\n",
      " state (4)  A[0]:(0.250366181135) A[1]:(0.25002977252) A[2]:(0.250150263309) A[3]:(0.249453693628)\n",
      " state (5)  A[0]:(0.250189632177) A[1]:(0.250280439854) A[2]:(0.250273942947) A[3]:(0.249255985022)\n",
      " state (6)  A[0]:(0.249909073114) A[1]:(0.250600397587) A[2]:(0.250434011221) A[3]:(0.249056488276)\n",
      " state (7)  A[0]:(0.249529525638) A[1]:(0.250990122557) A[2]:(0.250631004572) A[3]:(0.248849287629)\n",
      " state (8)  A[0]:(0.249052613974) A[1]:(0.251450538635) A[2]:(0.250865727663) A[3]:(0.248631104827)\n",
      " state (9)  A[0]:(0.248480379581) A[1]:(0.25198084116) A[2]:(0.251137822866) A[3]:(0.24840092659)\n",
      " state (10)  A[0]:(0.247819334269) A[1]:(0.252575933933) A[2]:(0.251444607973) A[3]:(0.248160108924)\n",
      " state (11)  A[0]:(0.247083082795) A[1]:(0.253224760294) A[2]:(0.251780182123) A[3]:(0.247911959887)\n",
      " state (12)  A[0]:(0.246292874217) A[1]:(0.253910243511) A[2]:(0.252135366201) A[3]:(0.247661471367)\n",
      " state (13)  A[0]:(0.245475590229) A[1]:(0.25461101532) A[2]:(0.252498775721) A[3]:(0.247414574027)\n",
      " state (14)  A[0]:(0.244660064578) A[1]:(0.255304425955) A[2]:(0.25285834074) A[3]:(0.247177198529)\n",
      " state (15)  A[0]:(0.243872821331) A[1]:(0.255969673395) A[2]:(0.253203094006) A[3]:(0.246954426169)\n",
      " state (0)  A[0]:(0.0167320743203)\n",
      " state (1)  A[0]:(0.0135733401403)\n",
      " state (2)  A[0]:(0.0139666516334)\n",
      " state (3)  A[0]:(0.0146044138819)\n",
      " state (4)  A[0]:(0.0156119829044)\n",
      " state (5)  A[0]:(0.0171793308109)\n",
      " state (6)  A[0]:(0.019580245018)\n",
      " state (7)  A[0]:(0.0231974944472)\n",
      " state (8)  A[0]:(0.028553340584)\n",
      " state (9)  A[0]:(0.0363404490054)\n",
      " state (10)  A[0]:(0.0474357865751)\n",
      " state (11)  A[0]:(0.0628580823541)\n",
      " state (12)  A[0]:(0.0836095884442)\n",
      " state (13)  A[0]:(0.110362440348)\n",
      " state (14)  A[0]:(0.143054276705)\n",
      " state (15)  A[0]:(0.18061196804)\n",
      "Episode 42000 finished after 3 . Running score: 0.0. Policy_loss: -95677.861971, Value_loss: 1.04264113347. Times trained:               7840. Times reached goal: 17.               Steps done: 322826.\n",
      " state (0)  A[0]:(0.249635145068) A[1]:(0.249529659748) A[2]:(0.25022611022) A[3]:(0.25060904026)\n",
      " state (1)  A[0]:(0.250006467104) A[1]:(0.249398633838) A[2]:(0.250162124634) A[3]:(0.250432789326)\n",
      " state (2)  A[0]:(0.250232577324) A[1]:(0.249371409416) A[2]:(0.250165194273) A[3]:(0.250230818987)\n",
      " state (3)  A[0]:(0.250310093164) A[1]:(0.249421909451) A[2]:(0.250210344791) A[3]:(0.250057697296)\n",
      " state (4)  A[0]:(0.250238120556) A[1]:(0.249557361007) A[2]:(0.250301390886) A[3]:(0.249903127551)\n",
      " state (5)  A[0]:(0.250033825636) A[1]:(0.249772250652) A[2]:(0.250437259674) A[3]:(0.249756619334)\n",
      " state (6)  A[0]:(0.249711945653) A[1]:(0.25006043911) A[2]:(0.25061699748) A[3]:(0.249610558152)\n",
      " state (7)  A[0]:(0.249278411269) A[1]:(0.250419735909) A[2]:(0.250841468573) A[3]:(0.24946038425)\n",
      " state (8)  A[0]:(0.2487334162) A[1]:(0.250850588083) A[2]:(0.251112341881) A[3]:(0.249303713441)\n",
      " state (9)  A[0]:(0.248077124357) A[1]:(0.251352727413) A[2]:(0.251430213451) A[3]:(0.249139890075)\n",
      " state (10)  A[0]:(0.24731515348) A[1]:(0.251922219992) A[2]:(0.25179284811) A[3]:(0.248969718814)\n",
      " state (11)  A[0]:(0.246461853385) A[1]:(0.252549082041) A[2]:(0.252193808556) A[3]:(0.248795285821)\n",
      " state (12)  A[0]:(0.245541080832) A[1]:(0.25321701169) A[2]:(0.252622246742) A[3]:(0.248619645834)\n",
      " state (13)  A[0]:(0.244583964348) A[1]:(0.253905057907) A[2]:(0.253064364195) A[3]:(0.248446598649)\n",
      " state (14)  A[0]:(0.243624418974) A[1]:(0.254590511322) A[2]:(0.253505110741) A[3]:(0.248279929161)\n",
      " state (15)  A[0]:(0.242694064975) A[1]:(0.255252301693) A[2]:(0.253930568695) A[3]:(0.248123019934)\n",
      " state (0)  A[0]:(0.020664408803)\n",
      " state (1)  A[0]:(0.0160768479109)\n",
      " state (2)  A[0]:(0.0167298335582)\n",
      " state (3)  A[0]:(0.0179203599691)\n",
      " state (4)  A[0]:(0.0200499668717)\n",
      " state (5)  A[0]:(0.0237563215196)\n",
      " state (6)  A[0]:(0.0300261918455)\n",
      " state (7)  A[0]:(0.0403338372707)\n",
      " state (8)  A[0]:(0.0567474141717)\n",
      " state (9)  A[0]:(0.0817885473371)\n",
      " state (10)  A[0]:(0.117656171322)\n",
      " state (11)  A[0]:(0.164697706699)\n",
      " state (12)  A[0]:(0.220112472773)\n",
      " state (13)  A[0]:(0.27850317955)\n",
      " state (14)  A[0]:(0.334213405848)\n",
      " state (15)  A[0]:(0.383412152529)\n",
      "Episode 43000 finished after 2 . Running score: 0.03. Policy_loss: -95677.2374517, Value_loss: 1.0170522934. Times trained:               7519. Times reached goal: 23.               Steps done: 330345.\n",
      " state (0)  A[0]:(0.250335127115) A[1]:(0.249875664711) A[2]:(0.249544963241) A[3]:(0.250244259834)\n",
      " state (1)  A[0]:(0.250830262899) A[1]:(0.249713093042) A[2]:(0.249529749155) A[3]:(0.249926909804)\n",
      " state (2)  A[0]:(0.251135468483) A[1]:(0.249696627259) A[2]:(0.249592855573) A[3]:(0.249575048685)\n",
      " state (3)  A[0]:(0.251217156649) A[1]:(0.249801874161) A[2]:(0.249713003635) A[3]:(0.249267920852)\n",
      " state (4)  A[0]:(0.251081049442) A[1]:(0.250034838915) A[2]:(0.249900460243) A[3]:(0.248983636498)\n",
      " state (5)  A[0]:(0.250755548477) A[1]:(0.250388026237) A[2]:(0.250156372786) A[3]:(0.248700097203)\n",
      " state (6)  A[0]:(0.250259399414) A[1]:(0.25085708499) A[2]:(0.250481963158) A[3]:(0.248401492834)\n",
      " state (7)  A[0]:(0.249597847462) A[1]:(0.251443326473) A[2]:(0.250880479813) A[3]:(0.248078346252)\n",
      " state (8)  A[0]:(0.24877230823) A[1]:(0.252147614956) A[2]:(0.251353681087) A[3]:(0.247726351023)\n",
      " state (9)  A[0]:(0.247792035341) A[1]:(0.252963364124) A[2]:(0.251897960901) A[3]:(0.247346609831)\n",
      " state (10)  A[0]:(0.246682092547) A[1]:(0.25387135148) A[2]:(0.252500981092) A[3]:(0.24694557488)\n",
      " state (11)  A[0]:(0.245485186577) A[1]:(0.254838973284) A[2]:(0.253141403198) A[3]:(0.246534466743)\n",
      " state (12)  A[0]:(0.244256138802) A[1]:(0.25582459569) A[2]:(0.253791958094) A[3]:(0.24612736702)\n",
      " state (13)  A[0]:(0.243051871657) A[1]:(0.256785213947) A[2]:(0.254424422979) A[3]:(0.245738491416)\n",
      " state (14)  A[0]:(0.241921082139) A[1]:(0.257684230804) A[2]:(0.255015015602) A[3]:(0.245379656553)\n",
      " state (15)  A[0]:(0.240897700191) A[1]:(0.25849622488) A[2]:(0.255547344685) A[3]:(0.245058655739)\n",
      " state (0)  A[0]:(0.0173547845334)\n",
      " state (1)  A[0]:(0.0146047975868)\n",
      " state (2)  A[0]:(0.0151579007506)\n",
      " state (3)  A[0]:(0.0161534659564)\n",
      " state (4)  A[0]:(0.0179183967412)\n",
      " state (5)  A[0]:(0.0209729149938)\n",
      " state (6)  A[0]:(0.0261186547577)\n",
      " state (7)  A[0]:(0.0345512852073)\n",
      " state (8)  A[0]:(0.0479625575244)\n",
      " state (9)  A[0]:(0.0684868916869)\n",
      " state (10)  A[0]:(0.0981961265206)\n",
      " state (11)  A[0]:(0.137945950031)\n",
      " state (12)  A[0]:(0.186122909188)\n",
      " state (13)  A[0]:(0.238597124815)\n",
      " state (14)  A[0]:(0.290343791246)\n",
      " state (15)  A[0]:(0.337404966354)\n",
      "Episode 44000 finished after 9 . Running score: 0.01. Policy_loss: -95681.3565879, Value_loss: 1.1896475332. Times trained:               7880. Times reached goal: 15.               Steps done: 338225.\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2501,  0.2495,  0.2501]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2501,  0.2495,  0.2501]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2504,  0.2495,  0.2494]])\n",
      "On state=4, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250255286694) A[1]:(0.250081717968) A[2]:(0.24954290688) A[3]:(0.250120073557)\n",
      " state (1)  A[0]:(0.250639557838) A[1]:(0.249981254339) A[2]:(0.249453604221) A[3]:(0.249925538898)\n",
      " state (2)  A[0]:(0.250860333443) A[1]:(0.250015944242) A[2]:(0.249419242144) A[3]:(0.249704450369)\n",
      " state (3)  A[0]:(0.250876218081) A[1]:(0.250157952309) A[2]:(0.249444931746) A[3]:(0.249520942569)\n",
      " state (4)  A[0]:(0.250688701868) A[1]:(0.250416308641) A[2]:(0.249533593655) A[3]:(0.249361395836)\n",
      " state (5)  A[0]:(0.250321626663) A[1]:(0.250787675381) A[2]:(0.249678999186) A[3]:(0.249211698771)\n",
      " state (6)  A[0]:(0.249790042639) A[1]:(0.251271486282) A[2]:(0.24987654388) A[3]:(0.249061957002)\n",
      " state (7)  A[0]:(0.249095618725) A[1]:(0.25187253952) A[2]:(0.250125467777) A[3]:(0.248906329274)\n",
      " state (8)  A[0]:(0.248235240579) A[1]:(0.252596199512) A[2]:(0.250426501036) A[3]:(0.24874201417)\n",
      " state (9)  A[0]:(0.247211664915) A[1]:(0.253441631794) A[2]:(0.250778377056) A[3]:(0.248568356037)\n",
      " state (10)  A[0]:(0.246041938663) A[1]:(0.254396170378) A[2]:(0.251175105572) A[3]:(0.248386800289)\n",
      " state (11)  A[0]:(0.244761362672) A[1]:(0.255432993174) A[2]:(0.25160497427) A[3]:(0.248200640082)\n",
      " state (12)  A[0]:(0.24342083931) A[1]:(0.256513208151) A[2]:(0.252051323652) A[3]:(0.248014599085)\n",
      " state (13)  A[0]:(0.242078438401) A[1]:(0.257592111826) A[2]:(0.252495408058) A[3]:(0.247833997011)\n",
      " state (14)  A[0]:(0.240789234638) A[1]:(0.258627206087) A[2]:(0.252919703722) A[3]:(0.247663900256)\n",
      " state (15)  A[0]:(0.2395965904) A[1]:(0.259584695101) A[2]:(0.253310531378) A[3]:(0.247508212924)\n",
      " state (0)  A[0]:(0.0143778156489)\n",
      " state (1)  A[0]:(0.012569649145)\n",
      " state (2)  A[0]:(0.0130660710856)\n",
      " state (3)  A[0]:(0.0139638129622)\n",
      " state (4)  A[0]:(0.0155616598204)\n",
      " state (5)  A[0]:(0.0183393880725)\n",
      " state (6)  A[0]:(0.0230408906937)\n",
      " state (7)  A[0]:(0.0307831801474)\n",
      " state (8)  A[0]:(0.0431621000171)\n",
      " state (9)  A[0]:(0.0622218847275)\n",
      " state (10)  A[0]:(0.0900039970875)\n",
      " state (11)  A[0]:(0.127460509539)\n",
      " state (12)  A[0]:(0.173212870955)\n",
      " state (13)  A[0]:(0.223412066698)\n",
      " state (14)  A[0]:(0.273232042789)\n",
      " state (15)  A[0]:(0.318779408932)\n",
      "Episode 45000 finished after 3 . Running score: 0.01. Policy_loss: -95675.5842349, Value_loss: 1.15657347119. Times trained:               7698. Times reached goal: 17.               Steps done: 345923.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.249895885587) A[1]:(0.250016659498) A[2]:(0.250053912401) A[3]:(0.25003349781)\n",
      " state (1)  A[0]:(0.250183552504) A[1]:(0.249940693378) A[2]:(0.249965399504) A[3]:(0.249910309911)\n",
      " state (2)  A[0]:(0.250461637974) A[1]:(0.24995996058) A[2]:(0.249882146716) A[3]:(0.249696269631)\n",
      " state (3)  A[0]:(0.250665962696) A[1]:(0.250016331673) A[2]:(0.249820485711) A[3]:(0.249497205019)\n",
      " state (4)  A[0]:(0.250781148672) A[1]:(0.250121176243) A[2]:(0.249784827232) A[3]:(0.249312832952)\n",
      " state (5)  A[0]:(0.250798344612) A[1]:(0.250283569098) A[2]:(0.249777600169) A[3]:(0.249140560627)\n",
      " state (6)  A[0]:(0.250714719296) A[1]:(0.250509858131) A[2]:(0.249799311161) A[3]:(0.248976096511)\n",
      " state (7)  A[0]:(0.250530511141) A[1]:(0.25080499053) A[2]:(0.249849662185) A[3]:(0.248814821243)\n",
      " state (8)  A[0]:(0.250246018171) A[1]:(0.251173228025) A[2]:(0.249928250909) A[3]:(0.248652517796)\n",
      " state (9)  A[0]:(0.249860525131) A[1]:(0.251618683338) A[2]:(0.25003489852) A[3]:(0.248485907912)\n",
      " state (10)  A[0]:(0.249373152852) A[1]:(0.25214445591) A[2]:(0.250169485807) A[3]:(0.248312830925)\n",
      " state (11)  A[0]:(0.2487847507) A[1]:(0.252751350403) A[2]:(0.250331550837) A[3]:(0.248132407665)\n",
      " state (12)  A[0]:(0.248099774122) A[1]:(0.253436088562) A[2]:(0.250519394875) A[3]:(0.247944757342)\n",
      " state (13)  A[0]:(0.247328191996) A[1]:(0.254190474749) A[2]:(0.250730067492) A[3]:(0.247751235962)\n",
      " state (14)  A[0]:(0.246485948563) A[1]:(0.255000919104) A[2]:(0.250959038734) A[3]:(0.247554138303)\n",
      " state (15)  A[0]:(0.245593979955) A[1]:(0.255849301815) A[2]:(0.251200348139) A[3]:(0.24735635519)\n",
      " state (0)  A[0]:(0.0142280962318)\n",
      " state (1)  A[0]:(0.0129036344588)\n",
      " state (2)  A[0]:(0.0132871009409)\n",
      " state (3)  A[0]:(0.0139557030052)\n",
      " state (4)  A[0]:(0.0151066351682)\n",
      " state (5)  A[0]:(0.0170505102724)\n",
      " state (6)  A[0]:(0.020260758698)\n",
      " state (7)  A[0]:(0.025439530611)\n",
      " state (8)  A[0]:(0.033596534282)\n",
      " state (9)  A[0]:(0.046102181077)\n",
      " state (10)  A[0]:(0.0645996555686)\n",
      " state (11)  A[0]:(0.0905897319317)\n",
      " state (12)  A[0]:(0.124624982476)\n",
      " state (13)  A[0]:(0.165514230728)\n",
      " state (14)  A[0]:(0.21030279994)\n",
      " state (15)  A[0]:(0.25527843833)\n",
      "Episode 46000 finished after 10 . Running score: 0.01. Policy_loss: -95675.2737473, Value_loss: 1.00420740177. Times trained:               7728. Times reached goal: 17.               Steps done: 353651.\n",
      " state (0)  A[0]:(0.249794587493) A[1]:(0.249986499548) A[2]:(0.250155001879) A[3]:(0.250063896179)\n",
      " state (1)  A[0]:(0.249972105026) A[1]:(0.249913722277) A[2]:(0.250095665455) A[3]:(0.250018537045)\n",
      " state (2)  A[0]:(0.250172525644) A[1]:(0.249898836017) A[2]:(0.250029116869) A[3]:(0.249899536371)\n",
      " state (3)  A[0]:(0.250322759151) A[1]:(0.249907627702) A[2]:(0.249977767467) A[3]:(0.249791786075)\n",
      " state (4)  A[0]:(0.25041231513) A[1]:(0.249947085977) A[2]:(0.249944537878) A[3]:(0.249696105719)\n",
      " state (5)  A[0]:(0.250433176756) A[1]:(0.250023066998) A[2]:(0.249931365252) A[3]:(0.249612376094)\n",
      " state (6)  A[0]:(0.25038087368) A[1]:(0.250140100718) A[2]:(0.249939292669) A[3]:(0.249539718032)\n",
      " state (7)  A[0]:(0.25025331974) A[1]:(0.250301361084) A[2]:(0.24996855855) A[3]:(0.249476760626)\n",
      " state (8)  A[0]:(0.250049412251) A[1]:(0.250509381294) A[2]:(0.250019133091) A[3]:(0.249422043562)\n",
      " state (9)  A[0]:(0.249768003821) A[1]:(0.250766605139) A[2]:(0.250091046095) A[3]:(0.249374315143)\n",
      " state (10)  A[0]:(0.249407410622) A[1]:(0.251075387001) A[2]:(0.250184446573) A[3]:(0.249332711101)\n",
      " state (11)  A[0]:(0.248966038227) A[1]:(0.251437783241) A[2]:(0.25029951334) A[3]:(0.249296680093)\n",
      " state (12)  A[0]:(0.248443305492) A[1]:(0.251854747534) A[2]:(0.250436127186) A[3]:(0.249265864491)\n",
      " state (13)  A[0]:(0.247840836644) A[1]:(0.252325475216) A[2]:(0.25059363246) A[3]:(0.249240010977)\n",
      " state (14)  A[0]:(0.247163683176) A[1]:(0.252846717834) A[2]:(0.25077059865) A[3]:(0.249218970537)\n",
      " state (15)  A[0]:(0.246420741081) A[1]:(0.253412336111) A[2]:(0.250964403152) A[3]:(0.249202474952)\n",
      " state (0)  A[0]:(0.0124524030834)\n",
      " state (1)  A[0]:(0.0107982307673)\n",
      " state (2)  A[0]:(0.0111267324537)\n",
      " state (3)  A[0]:(0.0116980522871)\n",
      " state (4)  A[0]:(0.0126793440431)\n",
      " state (5)  A[0]:(0.014333575964)\n",
      " state (6)  A[0]:(0.0170610267669)\n",
      " state (7)  A[0]:(0.0214547105134)\n",
      " state (8)  A[0]:(0.0283683240414)\n",
      " state (9)  A[0]:(0.0389692224562)\n",
      " state (10)  A[0]:(0.0546885244548)\n",
      " state (11)  A[0]:(0.0769146233797)\n",
      " state (12)  A[0]:(0.106344409287)\n",
      " state (13)  A[0]:(0.142255544662)\n",
      " state (14)  A[0]:(0.182326987386)\n",
      " state (15)  A[0]:(0.223350435495)\n",
      "Episode 47000 finished after 4 . Running score: 0.01. Policy_loss: -95674.8939363, Value_loss: 1.01391637114. Times trained:               7429. Times reached goal: 11.               Steps done: 361080.\n",
      " state (0)  A[0]:(0.249591082335) A[1]:(0.249869838357) A[2]:(0.25030964613) A[3]:(0.250229418278)\n",
      " state (1)  A[0]:(0.249808758497) A[1]:(0.249821648002) A[2]:(0.250253409147) A[3]:(0.25011613965)\n",
      " state (2)  A[0]:(0.250013023615) A[1]:(0.249839261174) A[2]:(0.250216245651) A[3]:(0.249931454659)\n",
      " state (3)  A[0]:(0.250082224607) A[1]:(0.249916985631) A[2]:(0.250219076872) A[3]:(0.249781727791)\n",
      " state (4)  A[0]:(0.249994009733) A[1]:(0.250071227551) A[2]:(0.250268727541) A[3]:(0.249666020274)\n",
      " state (5)  A[0]:(0.249735072255) A[1]:(0.250313997269) A[2]:(0.250370055437) A[3]:(0.249580815434)\n",
      " state (6)  A[0]:(0.249296337366) A[1]:(0.250654548407) A[2]:(0.250526994467) A[3]:(0.249522119761)\n",
      " state (7)  A[0]:(0.24866861105) A[1]:(0.251101046801) A[2]:(0.250743418932) A[3]:(0.249486893415)\n",
      " state (8)  A[0]:(0.247843965888) A[1]:(0.251660048962) A[2]:(0.25102263689) A[3]:(0.249473407865)\n",
      " state (9)  A[0]:(0.246822103858) A[1]:(0.252332627773) A[2]:(0.251364946365) A[3]:(0.249480366707)\n",
      " state (10)  A[0]:(0.245618775487) A[1]:(0.25310999155) A[2]:(0.25176525116) A[3]:(0.249505996704)\n",
      " state (11)  A[0]:(0.244270652533) A[1]:(0.253970533609) A[2]:(0.252211481333) A[3]:(0.249547332525)\n",
      " state (12)  A[0]:(0.242833763361) A[1]:(0.254880934954) A[2]:(0.252685248852) A[3]:(0.249600023031)\n",
      " state (13)  A[0]:(0.241374790668) A[1]:(0.255801320076) A[2]:(0.253164827824) A[3]:(0.249659061432)\n",
      " state (14)  A[0]:(0.239958941936) A[1]:(0.256692528725) A[2]:(0.253629028797) A[3]:(0.249719545245)\n",
      " state (15)  A[0]:(0.238639205694) A[1]:(0.257522493601) A[2]:(0.254060804844) A[3]:(0.249777510762)\n",
      " state (0)  A[0]:(0.0178294442594)\n",
      " state (1)  A[0]:(0.0162734072655)\n",
      " state (2)  A[0]:(0.0166492890567)\n",
      " state (3)  A[0]:(0.0173360351473)\n",
      " state (4)  A[0]:(0.0185698475689)\n",
      " state (5)  A[0]:(0.0207343064249)\n",
      " state (6)  A[0]:(0.0244303178042)\n",
      " state (7)  A[0]:(0.0305706709623)\n",
      " state (8)  A[0]:(0.0404823385179)\n",
      " state (9)  A[0]:(0.0559185966849)\n",
      " state (10)  A[0]:(0.0787459462881)\n",
      " state (11)  A[0]:(0.110077477992)\n",
      " state (12)  A[0]:(0.149145230651)\n",
      " state (13)  A[0]:(0.192947134376)\n",
      " state (14)  A[0]:(0.237326294184)\n",
      " state (15)  A[0]:(0.278648048639)\n",
      "Episode 48000 finished after 6 . Running score: 0.01. Policy_loss: -95673.9376184, Value_loss: 1.23644652896. Times trained:               7756. Times reached goal: 12.               Steps done: 368836.\n",
      " state (0)  A[0]:(0.249468952417) A[1]:(0.250029057264) A[2]:(0.250128597021) A[3]:(0.250373393297)\n",
      " state (1)  A[0]:(0.249616414309) A[1]:(0.249931335449) A[2]:(0.250077724457) A[3]:(0.25037458539)\n",
      " state (2)  A[0]:(0.249924749136) A[1]:(0.249818652868) A[2]:(0.250014662743) A[3]:(0.250241905451)\n",
      " state (3)  A[0]:(0.250184059143) A[1]:(0.249727442861) A[2]:(0.249964281917) A[3]:(0.250124245882)\n",
      " state (4)  A[0]:(0.250372201204) A[1]:(0.249668449163) A[2]:(0.249932438135) A[3]:(0.250026851892)\n",
      " state (5)  A[0]:(0.250481039286) A[1]:(0.249646529555) A[2]:(0.24992184341) A[3]:(0.249950587749)\n",
      " state (6)  A[0]:(0.25051420927) A[1]:(0.249660938978) A[2]:(0.249932155013) A[3]:(0.249892741442)\n",
      " state (7)  A[0]:(0.250480562449) A[1]:(0.249708265066) A[2]:(0.249961704016) A[3]:(0.249849513173)\n",
      " state (8)  A[0]:(0.250388801098) A[1]:(0.249784946442) A[2]:(0.250008821487) A[3]:(0.249817416072)\n",
      " state (9)  A[0]:(0.250245124102) A[1]:(0.249888420105) A[2]:(0.250072360039) A[3]:(0.249794051051)\n",
      " state (10)  A[0]:(0.25005325675) A[1]:(0.250017017126) A[2]:(0.250151723623) A[3]:(0.249778002501)\n",
      " state (11)  A[0]:(0.249815404415) A[1]:(0.250169664621) A[2]:(0.250246524811) A[3]:(0.249768450856)\n",
      " state (12)  A[0]:(0.249533489347) A[1]:(0.250345140696) A[2]:(0.250356286764) A[3]:(0.249765083194)\n",
      " state (13)  A[0]:(0.249210298061) A[1]:(0.250541895628) A[2]:(0.250480175018) A[3]:(0.249767675996)\n",
      " state (14)  A[0]:(0.248850032687) A[1]:(0.250757366419) A[2]:(0.250616669655) A[3]:(0.249775916338)\n",
      " state (15)  A[0]:(0.248458743095) A[1]:(0.250988215208) A[2]:(0.250763684511) A[3]:(0.249789372087)\n",
      " state (0)  A[0]:(0.0141841648147)\n",
      " state (1)  A[0]:(0.0120022185147)\n",
      " state (2)  A[0]:(0.0124907009304)\n",
      " state (3)  A[0]:(0.0134674366564)\n",
      " state (4)  A[0]:(0.015370878391)\n",
      " state (5)  A[0]:(0.0189599301666)\n",
      " state (6)  A[0]:(0.0255000311881)\n",
      " state (7)  A[0]:(0.037010718137)\n",
      " state (8)  A[0]:(0.0563884116709)\n",
      " state (9)  A[0]:(0.0867852419615)\n",
      " state (10)  A[0]:(0.12957829237)\n",
      " state (11)  A[0]:(0.18203638494)\n",
      " state (12)  A[0]:(0.23773406446)\n",
      " state (13)  A[0]:(0.290014356375)\n",
      " state (14)  A[0]:(0.334857821465)\n",
      " state (15)  A[0]:(0.371155083179)\n",
      "Episode 49000 finished after 3 . Running score: 0.03. Policy_loss: -95673.4748507, Value_loss: 1.17036977031. Times trained:               7712. Times reached goal: 15.               Steps done: 376548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.2501,  0.2506,  0.2499,  0.2495]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2506,  0.2499,  0.2495]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2506,  0.2499,  0.2495]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2511,  0.2499,  0.2483]])\n",
      "On state=4, selected action=1\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250101178885) A[1]:(0.250572025776) A[2]:(0.249850407243) A[3]:(0.249476328492)\n",
      " state (1)  A[0]:(0.25032132864) A[1]:(0.250562638044) A[2]:(0.249830618501) A[3]:(0.249285385013)\n",
      " state (2)  A[0]:(0.250618487597) A[1]:(0.250629723072) A[2]:(0.249824151397) A[3]:(0.24892757833)\n",
      " state (3)  A[0]:(0.250750660896) A[1]:(0.250783860683) A[2]:(0.249855920672) A[3]:(0.248609617352)\n",
      " state (4)  A[0]:(0.250683575869) A[1]:(0.251048833132) A[2]:(0.249937981367) A[3]:(0.248329609632)\n",
      " state (5)  A[0]:(0.250400692225) A[1]:(0.251443356276) A[2]:(0.250078588724) A[3]:(0.248077392578)\n",
      " state (6)  A[0]:(0.249892145395) A[1]:(0.251982241869) A[2]:(0.250283569098) A[3]:(0.247842073441)\n",
      " state (7)  A[0]:(0.249153673649) A[1]:(0.252673536539) A[2]:(0.250555902719) A[3]:(0.24761685729)\n",
      " state (8)  A[0]:(0.248197644949) A[1]:(0.253510087729) A[2]:(0.250892281532) A[3]:(0.24739998579)\n",
      " state (9)  A[0]:(0.247065499425) A[1]:(0.254461526871) A[2]:(0.251279532909) A[3]:(0.247193470597)\n",
      " state (10)  A[0]:(0.245829388499) A[1]:(0.255474686623) A[2]:(0.251694619656) A[3]:(0.247001305223)\n",
      " state (11)  A[0]:(0.244577750564) A[1]:(0.256484925747) A[2]:(0.252109706402) A[3]:(0.246827617288)\n",
      " state (12)  A[0]:(0.243392661214) A[1]:(0.25743278861) A[2]:(0.25249928236) A[3]:(0.246675297618)\n",
      " state (13)  A[0]:(0.242332085967) A[1]:(0.258276820183) A[2]:(0.25284576416) A[3]:(0.246545344591)\n",
      " state (14)  A[0]:(0.241424426436) A[1]:(0.25899758935) A[2]:(0.253141015768) A[3]:(0.246436983347)\n",
      " state (15)  A[0]:(0.24067299068) A[1]:(0.259594172239) A[2]:(0.253384739161) A[3]:(0.246348157525)\n",
      " state (0)  A[0]:(0.0154645005241)\n",
      " state (1)  A[0]:(0.0147392489016)\n",
      " state (2)  A[0]:(0.015216733329)\n",
      " state (3)  A[0]:(0.0161741804332)\n",
      " state (4)  A[0]:(0.0180423110723)\n",
      " state (5)  A[0]:(0.0215618312359)\n",
      " state (6)  A[0]:(0.027953164652)\n",
      " state (7)  A[0]:(0.039123788476)\n",
      " state (8)  A[0]:(0.0577100999653)\n",
      " state (9)  A[0]:(0.0863766148686)\n",
      " state (10)  A[0]:(0.125916942954)\n",
      " state (11)  A[0]:(0.173417434096)\n",
      " state (12)  A[0]:(0.22303815186)\n",
      " state (13)  A[0]:(0.269121974707)\n",
      " state (14)  A[0]:(0.308432519436)\n",
      " state (15)  A[0]:(0.340187966824)\n",
      "Episode 50000 finished after 4 . Running score: 0.01. Policy_loss: -95673.4685995, Value_loss: 1.0123523818. Times trained:               7544. Times reached goal: 11.               Steps done: 384092.\n",
      " state (0)  A[0]:(0.249594733119) A[1]:(0.250005215406) A[2]:(0.250011950731) A[3]:(0.250388115644)\n",
      " state (1)  A[0]:(0.249755039811) A[1]:(0.249974787235) A[2]:(0.249981865287) A[3]:(0.250288367271)\n",
      " state (2)  A[0]:(0.250130414963) A[1]:(0.250010818243) A[2]:(0.249932140112) A[3]:(0.249926567078)\n",
      " state (3)  A[0]:(0.250391453505) A[1]:(0.250088244677) A[2]:(0.249905720353) A[3]:(0.249614611268)\n",
      " state (4)  A[0]:(0.250519752502) A[1]:(0.250221252441) A[2]:(0.249907940626) A[3]:(0.249351114035)\n",
      " state (5)  A[0]:(0.25051343441) A[1]:(0.250419139862) A[2]:(0.249940738082) A[3]:(0.249126732349)\n",
      " state (6)  A[0]:(0.250377625227) A[1]:(0.250688165426) A[2]:(0.250004410744) A[3]:(0.2489297539)\n",
      " state (7)  A[0]:(0.250115543604) A[1]:(0.251034557819) A[2]:(0.25009945035) A[3]:(0.248750388622)\n",
      " state (8)  A[0]:(0.249725848436) A[1]:(0.251465022564) A[2]:(0.25022700429) A[3]:(0.248582080007)\n",
      " state (9)  A[0]:(0.249205440283) A[1]:(0.251985162497) A[2]:(0.250388294458) A[3]:(0.248421132565)\n",
      " state (10)  A[0]:(0.248554393649) A[1]:(0.252596318722) A[2]:(0.250583320856) A[3]:(0.248265996575)\n",
      " state (11)  A[0]:(0.247781142592) A[1]:(0.25329259038) A[2]:(0.25080960989) A[3]:(0.248116627336)\n",
      " state (12)  A[0]:(0.246905475855) A[1]:(0.254059016705) A[2]:(0.251061588526) A[3]:(0.247973889112)\n",
      " state (13)  A[0]:(0.245958194137) A[1]:(0.254872083664) A[2]:(0.251330673695) A[3]:(0.247839048505)\n",
      " state (14)  A[0]:(0.244977265596) A[1]:(0.255702853203) A[2]:(0.251606523991) A[3]:(0.247713401914)\n",
      " state (15)  A[0]:(0.244001954794) A[1]:(0.256521463394) A[2]:(0.251878470182) A[3]:(0.247598052025)\n",
      " state (0)  A[0]:(0.0116990264505)\n",
      " state (1)  A[0]:(0.0112641938031)\n",
      " state (2)  A[0]:(0.0116900065914)\n",
      " state (3)  A[0]:(0.0125351017341)\n",
      " state (4)  A[0]:(0.0141694806516)\n",
      " state (5)  A[0]:(0.017225606367)\n",
      " state (6)  A[0]:(0.0227410551161)\n",
      " state (7)  A[0]:(0.0323418006301)\n",
      " state (8)  A[0]:(0.0483302250504)\n",
      " state (9)  A[0]:(0.0732369422913)\n",
      " state (10)  A[0]:(0.108342476189)\n",
      " state (11)  A[0]:(0.15185777843)\n",
      " state (12)  A[0]:(0.198943063617)\n",
      " state (13)  A[0]:(0.244138941169)\n",
      " state (14)  A[0]:(0.283763378859)\n",
      " state (15)  A[0]:(0.316458374262)\n",
      "Episode 51000 finished after 4 . Running score: 0.02. Policy_loss: -95672.9915753, Value_loss: 1.01051349198. Times trained:               7765. Times reached goal: 13.               Steps done: 391857.\n",
      " state (0)  A[0]:(0.249656617641) A[1]:(0.249816596508) A[2]:(0.249836653471) A[3]:(0.250690072775)\n",
      " state (1)  A[0]:(0.249760985374) A[1]:(0.249752208591) A[2]:(0.249803423882) A[3]:(0.250683367252)\n",
      " state (2)  A[0]:(0.250207126141) A[1]:(0.249744296074) A[2]:(0.249779716134) A[3]:(0.250268816948)\n",
      " state (3)  A[0]:(0.250553905964) A[1]:(0.249766469002) A[2]:(0.249772712588) A[3]:(0.249906912446)\n",
      " state (4)  A[0]:(0.25078189373) A[1]:(0.24982996285) A[2]:(0.24978864193) A[3]:(0.249599501491)\n",
      " state (5)  A[0]:(0.250890016556) A[1]:(0.249940767884) A[2]:(0.249830648303) A[3]:(0.249338552356)\n",
      " state (6)  A[0]:(0.250888198614) A[1]:(0.250100880861) A[2]:(0.249899446964) A[3]:(0.249111458659)\n",
      " state (7)  A[0]:(0.250786811113) A[1]:(0.250311344862) A[2]:(0.249995276332) A[3]:(0.248906493187)\n",
      " state (8)  A[0]:(0.250591367483) A[1]:(0.250574439764) A[2]:(0.250119119883) A[3]:(0.248715028167)\n",
      " state (9)  A[0]:(0.250301986933) A[1]:(0.250893682241) A[2]:(0.250272750854) A[3]:(0.248531520367)\n",
      " state (10)  A[0]:(0.249915599823) A[1]:(0.251273214817) A[2]:(0.250458329916) A[3]:(0.248352795839)\n",
      " state (11)  A[0]:(0.249428838491) A[1]:(0.251716226339) A[2]:(0.250677585602) A[3]:(0.248177334666)\n",
      " state (12)  A[0]:(0.248840972781) A[1]:(0.252223372459) A[2]:(0.250930935144) A[3]:(0.248004734516)\n",
      " state (13)  A[0]:(0.248156413436) A[1]:(0.252791494131) A[2]:(0.251216709614) A[3]:(0.24783539772)\n",
      " state (14)  A[0]:(0.247386172414) A[1]:(0.253412783146) A[2]:(0.251530826092) A[3]:(0.247670188546)\n",
      " state (15)  A[0]:(0.24654789269) A[1]:(0.254075020552) A[2]:(0.25186675787) A[3]:(0.247510373592)\n",
      " state (0)  A[0]:(0.0206095445901)\n",
      " state (1)  A[0]:(0.0201999526471)\n",
      " state (2)  A[0]:(0.0206141620874)\n",
      " state (3)  A[0]:(0.0214332472533)\n",
      " state (4)  A[0]:(0.0230143722147)\n",
      " state (5)  A[0]:(0.0259711313993)\n",
      " state (6)  A[0]:(0.0313192158937)\n",
      " state (7)  A[0]:(0.0406829677522)\n",
      " state (8)  A[0]:(0.0564772300422)\n",
      " state (9)  A[0]:(0.0816796422005)\n",
      " state (10)  A[0]:(0.118537902832)\n",
      " state (11)  A[0]:(0.16636288166)\n",
      " state (12)  A[0]:(0.220556557178)\n",
      " state (13)  A[0]:(0.274651944637)\n",
      " state (14)  A[0]:(0.323469519615)\n",
      " state (15)  A[0]:(0.364535808563)\n",
      "Episode 52000 finished after 8 . Running score: 0.01. Policy_loss: -95672.7215791, Value_loss: 1.0059428402. Times trained:               7659. Times reached goal: 20.               Steps done: 399516.\n",
      " state (0)  A[0]:(0.250043839216) A[1]:(0.249690830708) A[2]:(0.249732911587) A[3]:(0.250532358885)\n",
      " state (1)  A[0]:(0.250236749649) A[1]:(0.249601528049) A[2]:(0.249684497714) A[3]:(0.250477194786)\n",
      " state (2)  A[0]:(0.250739216805) A[1]:(0.249621659517) A[2]:(0.249683454633) A[3]:(0.249955683947)\n",
      " state (3)  A[0]:(0.250992745161) A[1]:(0.249731972814) A[2]:(0.249732464552) A[3]:(0.24954277277)\n",
      " state (4)  A[0]:(0.25099042058) A[1]:(0.24995213747) A[2]:(0.249843150377) A[3]:(0.24921426177)\n",
      " state (5)  A[0]:(0.250765860081) A[1]:(0.250284194946) A[2]:(0.250016897917) A[3]:(0.248933017254)\n",
      " state (6)  A[0]:(0.250338226557) A[1]:(0.250733017921) A[2]:(0.250256717205) A[3]:(0.248672112823)\n",
      " state (7)  A[0]:(0.249702855945) A[1]:(0.251311063766) A[2]:(0.250569909811) A[3]:(0.248416185379)\n",
      " state (8)  A[0]:(0.248844027519) A[1]:(0.252033233643) A[2]:(0.250965297222) A[3]:(0.248157456517)\n",
      " state (9)  A[0]:(0.247751370072) A[1]:(0.252908080816) A[2]:(0.251447945833) A[3]:(0.247892647982)\n",
      " state (10)  A[0]:(0.24643445015) A[1]:(0.2539293468) A[2]:(0.252014249563) A[3]:(0.247621968389)\n",
      " state (11)  A[0]:(0.244931668043) A[1]:(0.25507080555) A[2]:(0.252648919821) A[3]:(0.247348561883)\n",
      " state (12)  A[0]:(0.243308648467) A[1]:(0.256287485361) A[2]:(0.253325998783) A[3]:(0.247077837586)\n",
      " state (13)  A[0]:(0.241646513343) A[1]:(0.257523626089) A[2]:(0.254013389349) A[3]:(0.246816426516)\n",
      " state (14)  A[0]:(0.240025386214) A[1]:(0.258724063635) A[2]:(0.25467979908) A[3]:(0.246570780873)\n",
      " state (15)  A[0]:(0.238510131836) A[1]:(0.259843856096) A[2]:(0.255299925804) A[3]:(0.24634604156)\n",
      " state (0)  A[0]:(0.0210208520293)\n",
      " state (1)  A[0]:(0.0197687111795)\n",
      " state (2)  A[0]:(0.020041745156)\n",
      " state (3)  A[0]:(0.020547894761)\n",
      " state (4)  A[0]:(0.0214713793248)\n",
      " state (5)  A[0]:(0.0231180787086)\n",
      " state (6)  A[0]:(0.0259796604514)\n",
      " state (7)  A[0]:(0.0308301076293)\n",
      " state (8)  A[0]:(0.0388637892902)\n",
      " state (9)  A[0]:(0.0518372952938)\n",
      " state (10)  A[0]:(0.0720404759049)\n",
      " state (11)  A[0]:(0.101743258536)\n",
      " state (12)  A[0]:(0.141914337873)\n",
      " state (13)  A[0]:(0.190894156694)\n",
      " state (14)  A[0]:(0.244439527392)\n",
      " state (15)  A[0]:(0.29746055603)\n",
      "Episode 53000 finished after 4 . Running score: 0.0. Policy_loss: -95671.8126666, Value_loss: 1.03443401291. Times trained:               7704. Times reached goal: 19.               Steps done: 407220.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.249986112118) A[1]:(0.249844059348) A[2]:(0.250237643719) A[3]:(0.249932199717)\n",
      " state (1)  A[0]:(0.250006496906) A[1]:(0.249826610088) A[2]:(0.250234514475) A[3]:(0.249932423234)\n",
      " state (2)  A[0]:(0.250296354294) A[1]:(0.249935701489) A[2]:(0.250200897455) A[3]:(0.249567002058)\n",
      " state (3)  A[0]:(0.250316023827) A[1]:(0.250161796808) A[2]:(0.250242203474) A[3]:(0.24927996099)\n",
      " state (4)  A[0]:(0.250027120113) A[1]:(0.250543475151) A[2]:(0.250371754169) A[3]:(0.249057650566)\n",
      " state (5)  A[0]:(0.249417424202) A[1]:(0.25110784173) A[2]:(0.25059774518) A[3]:(0.248876988888)\n",
      " state (6)  A[0]:(0.248465076089) A[1]:(0.251883924007) A[2]:(0.250931143761) A[3]:(0.24871981144)\n",
      " state (7)  A[0]:(0.247139364481) A[1]:(0.252900987864) A[2]:(0.251384079456) A[3]:(0.248575508595)\n",
      " state (8)  A[0]:(0.245428174734) A[1]:(0.254172116518) A[2]:(0.251961022615) A[3]:(0.248438745737)\n",
      " state (9)  A[0]:(0.243370249867) A[1]:(0.255674064159) A[2]:(0.252648711205) A[3]:(0.24830698967)\n",
      " state (10)  A[0]:(0.241070061922) A[1]:(0.257338017225) A[2]:(0.253412246704) A[3]:(0.248179659247)\n",
      " state (11)  A[0]:(0.238681048155) A[1]:(0.259060323238) A[2]:(0.254200994968) A[3]:(0.248057588935)\n",
      " state (12)  A[0]:(0.236364752054) A[1]:(0.260730087757) A[2]:(0.254962295294) A[3]:(0.247942835093)\n",
      " state (13)  A[0]:(0.234250217676) A[1]:(0.262257099152) A[2]:(0.255654633045) A[3]:(0.247838005424)\n",
      " state (14)  A[0]:(0.232413560152) A[1]:(0.263586997986) A[2]:(0.256254076958) A[3]:(0.247745350003)\n",
      " state (15)  A[0]:(0.230879515409) A[1]:(0.264701038599) A[2]:(0.256753474474) A[3]:(0.247666016221)\n",
      " state (0)  A[0]:(0.0288690533489)\n",
      " state (1)  A[0]:(0.026317499578)\n",
      " state (2)  A[0]:(0.0266776438802)\n",
      " state (3)  A[0]:(0.0273906812072)\n",
      " state (4)  A[0]:(0.0287715885788)\n",
      " state (5)  A[0]:(0.0313676595688)\n",
      " state (6)  A[0]:(0.0360993593931)\n",
      " state (7)  A[0]:(0.0444803610444)\n",
      " state (8)  A[0]:(0.0588915646076)\n",
      " state (9)  A[0]:(0.0826334953308)\n",
      " state (10)  A[0]:(0.119042254984)\n",
      " state (11)  A[0]:(0.169175311923)\n",
      " state (12)  A[0]:(0.229613393545)\n",
      " state (13)  A[0]:(0.293320596218)\n",
      " state (14)  A[0]:(0.353259176016)\n",
      " state (15)  A[0]:(0.405147194862)\n",
      "Episode 54000 finished after 16 . Running score: 0.02. Policy_loss: -95671.7095974, Value_loss: 1.02672142129. Times trained:               7409. Times reached goal: 13.               Steps done: 414629.\n",
      "action_dist \n",
      "tensor([[ 0.2493,  0.2498,  0.2504,  0.2505]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2493,  0.2498,  0.2504,  0.2505]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2494,  0.2498,  0.2504,  0.2504]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2498,  0.2500,  0.2504,  0.2498]])\n",
      "On state=2, selected action=0\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2455,  0.2535,  0.2524,  0.2486]])\n",
      "On state=6, selected action=3\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.249324262142) A[1]:(0.249814674258) A[2]:(0.250370591879) A[3]:(0.250490516424)\n",
      " state (1)  A[0]:(0.249363556504) A[1]:(0.249828204513) A[2]:(0.25039100647) A[3]:(0.250417172909)\n",
      " state (2)  A[0]:(0.249772399664) A[1]:(0.250002563) A[2]:(0.250443786383) A[3]:(0.249781265855)\n",
      " state (3)  A[0]:(0.249646544456) A[1]:(0.250392735004) A[2]:(0.250631630421) A[3]:(0.249329090118)\n",
      " state (4)  A[0]:(0.248923823237) A[1]:(0.251069307327) A[2]:(0.250990539789) A[3]:(0.249016314745)\n",
      " state (5)  A[0]:(0.247562661767) A[1]:(0.252090215683) A[2]:(0.251554638147) A[3]:(0.248792514205)\n",
      " state (6)  A[0]:(0.245502471924) A[1]:(0.253513097763) A[2]:(0.252358675003) A[3]:(0.24862575531)\n",
      " state (7)  A[0]:(0.242723733187) A[1]:(0.25536262989) A[2]:(0.253416061401) A[3]:(0.248497545719)\n",
      " state (8)  A[0]:(0.239336669445) A[1]:(0.257580310106) A[2]:(0.254688888788) A[3]:(0.24839413166)\n",
      " state (9)  A[0]:(0.235610470176) A[1]:(0.260006457567) A[2]:(0.256079345942) A[3]:(0.248303711414)\n",
      " state (10)  A[0]:(0.231899097562) A[1]:(0.262423813343) A[2]:(0.257458388805) A[3]:(0.24821870029)\n",
      " state (11)  A[0]:(0.228515252471) A[1]:(0.264635175467) A[2]:(0.258712291718) A[3]:(0.248137325048)\n",
      " state (12)  A[0]:(0.225645601749) A[1]:(0.266518831253) A[2]:(0.259773671627) A[3]:(0.248061954975)\n",
      " state (13)  A[0]:(0.223343491554) A[1]:(0.268036693335) A[2]:(0.260624051094) A[3]:(0.247995764017)\n",
      " state (14)  A[0]:(0.221570029855) A[1]:(0.269210755825) A[2]:(0.261278659105) A[3]:(0.247940599918)\n",
      " state (15)  A[0]:(0.220242112875) A[1]:(0.270092815161) A[2]:(0.261768490076) A[3]:(0.247896552086)\n",
      " state (0)  A[0]:(0.0269961673766)\n",
      " state (1)  A[0]:(0.0239383112639)\n",
      " state (2)  A[0]:(0.0243392810225)\n",
      " state (3)  A[0]:(0.0251694042236)\n",
      " state (4)  A[0]:(0.0268436409533)\n",
      " state (5)  A[0]:(0.0301090665162)\n",
      " state (6)  A[0]:(0.0362720377743)\n",
      " state (7)  A[0]:(0.0475658886135)\n",
      " state (8)  A[0]:(0.067559890449)\n",
      " state (9)  A[0]:(0.100925639272)\n",
      " state (10)  A[0]:(0.151212349534)\n",
      " state (11)  A[0]:(0.216899573803)\n",
      " state (12)  A[0]:(0.290146708488)\n",
      " state (13)  A[0]:(0.361075222492)\n",
      " state (14)  A[0]:(0.422891944647)\n",
      " state (15)  A[0]:(0.473211139441)\n",
      "Episode 55000 finished after 5 . Running score: 0.03. Policy_loss: -95671.3080431, Value_loss: 1.02920462357. Times trained:               7501. Times reached goal: 22.               Steps done: 422130.\n",
      " state (0)  A[0]:(0.249394074082) A[1]:(0.249996945262) A[2]:(0.250029176474) A[3]:(0.25057977438)\n",
      " state (1)  A[0]:(0.249020799994) A[1]:(0.250012367964) A[2]:(0.250073283911) A[3]:(0.250893592834)\n",
      " state (2)  A[0]:(0.249954208732) A[1]:(0.250177264214) A[2]:(0.250033468008) A[3]:(0.249835088849)\n",
      " state (3)  A[0]:(0.250621169806) A[1]:(0.250440984964) A[2]:(0.250053852797) A[3]:(0.248884022236)\n",
      " state (4)  A[0]:(0.250955194235) A[1]:(0.250845193863) A[2]:(0.250157237053) A[3]:(0.24804238975)\n",
      " state (5)  A[0]:(0.250900089741) A[1]:(0.251437276602) A[2]:(0.25036790967) A[3]:(0.247294723988)\n",
      " state (6)  A[0]:(0.250408709049) A[1]:(0.252266079187) A[2]:(0.250710099936) A[3]:(0.246615156531)\n",
      " state (7)  A[0]:(0.249443814158) A[1]:(0.25337472558) A[2]:(0.251204788685) A[3]:(0.245976641774)\n",
      " state (8)  A[0]:(0.247991740704) A[1]:(0.254787176847) A[2]:(0.251862794161) A[3]:(0.245358228683)\n",
      " state (9)  A[0]:(0.246085941792) A[1]:(0.256489664316) A[2]:(0.252675175667) A[3]:(0.244749188423)\n",
      " state (10)  A[0]:(0.243825152516) A[1]:(0.258417844772) A[2]:(0.253606647253) A[3]:(0.244150385261)\n",
      " state (11)  A[0]:(0.241366565228) A[1]:(0.260462015867) A[2]:(0.254598975182) A[3]:(0.243572428823)\n",
      " state (12)  A[0]:(0.238891243935) A[1]:(0.262492507696) A[2]:(0.255584955215) A[3]:(0.24303124845)\n",
      " state (13)  A[0]:(0.236560091376) A[1]:(0.264392316341) A[2]:(0.256505280733) A[3]:(0.242542311549)\n",
      " state (14)  A[0]:(0.234483540058) A[1]:(0.266080379486) A[2]:(0.257320046425) A[3]:(0.242116078734)\n",
      " state (15)  A[0]:(0.232714474201) A[1]:(0.267518043518) A[2]:(0.258011072874) A[3]:(0.241756379604)\n",
      " state (0)  A[0]:(0.0240382868797)\n",
      " state (1)  A[0]:(0.0215330421925)\n",
      " state (2)  A[0]:(0.0218588933349)\n",
      " state (3)  A[0]:(0.022509958595)\n",
      " state (4)  A[0]:(0.0237821191549)\n",
      " state (5)  A[0]:(0.0261941608042)\n",
      " state (6)  A[0]:(0.0306274201721)\n",
      " state (7)  A[0]:(0.0385505445302)\n",
      " state (8)  A[0]:(0.0523158460855)\n",
      " state (9)  A[0]:(0.0752648115158)\n",
      " state (10)  A[0]:(0.110907077789)\n",
      " state (11)  A[0]:(0.160578146577)\n",
      " state (12)  A[0]:(0.221061766148)\n",
      " state (13)  A[0]:(0.285297006369)\n",
      " state (14)  A[0]:(0.346050709486)\n",
      " state (15)  A[0]:(0.398832708597)\n",
      "Episode 56000 finished after 3 . Running score: 0.01. Policy_loss: -95671.0382024, Value_loss: 1.03740547566. Times trained:               7261. Times reached goal: 17.               Steps done: 429391.\n",
      " state (0)  A[0]:(0.25005415082) A[1]:(0.249900057912) A[2]:(0.249996483326) A[3]:(0.250049293041)\n",
      " state (1)  A[0]:(0.24993121624) A[1]:(0.249892756343) A[2]:(0.250001341105) A[3]:(0.250174671412)\n",
      " state (2)  A[0]:(0.250598609447) A[1]:(0.249899238348) A[2]:(0.250061690807) A[3]:(0.249440476298)\n",
      " state (3)  A[0]:(0.250832408667) A[1]:(0.250075012445) A[2]:(0.250214397907) A[3]:(0.248878255486)\n",
      " state (4)  A[0]:(0.250566631556) A[1]:(0.25047197938) A[2]:(0.250496804714) A[3]:(0.248464539647)\n",
      " state (5)  A[0]:(0.24977517128) A[1]:(0.251127511263) A[2]:(0.250941872597) A[3]:(0.248155459762)\n",
      " state (6)  A[0]:(0.248423859477) A[1]:(0.252078831196) A[2]:(0.251582562923) A[3]:(0.2479147017)\n",
      " state (7)  A[0]:(0.246479511261) A[1]:(0.253355801105) A[2]:(0.25244474411) A[3]:(0.24771989882)\n",
      " state (8)  A[0]:(0.243963941932) A[1]:(0.25495198369) A[2]:(0.253526449203) A[3]:(0.247557595372)\n",
      " state (9)  A[0]:(0.241004109383) A[1]:(0.256797671318) A[2]:(0.254779994488) A[3]:(0.247418254614)\n",
      " state (10)  A[0]:(0.237829506397) A[1]:(0.258761584759) A[2]:(0.256114095449) A[3]:(0.247294843197)\n",
      " state (11)  A[0]:(0.234708324075) A[1]:(0.260687559843) A[2]:(0.257420688868) A[3]:(0.247183382511)\n",
      " state (12)  A[0]:(0.231865346432) A[1]:(0.26244276762) A[2]:(0.258608818054) A[3]:(0.247083127499)\n",
      " state (13)  A[0]:(0.229433193803) A[1]:(0.263947188854) A[2]:(0.25962460041) A[3]:(0.246995061636)\n",
      " state (14)  A[0]:(0.227451682091) A[1]:(0.265175879002) A[2]:(0.260452151299) A[3]:(0.246920332313)\n",
      " state (15)  A[0]:(0.225895091891) A[1]:(0.266143471003) A[2]:(0.261102408171) A[3]:(0.246858984232)\n",
      " state (0)  A[0]:(0.0222796760499)\n",
      " state (1)  A[0]:(0.0199412833899)\n",
      " state (2)  A[0]:(0.0202027745545)\n",
      " state (3)  A[0]:(0.0207078102976)\n",
      " state (4)  A[0]:(0.0216652825475)\n",
      " state (5)  A[0]:(0.0234332401305)\n",
      " state (6)  A[0]:(0.0266057550907)\n",
      " state (7)  A[0]:(0.0321506299078)\n",
      " state (8)  A[0]:(0.0416103601456)\n",
      " state (9)  A[0]:(0.0572860836983)\n",
      " state (10)  A[0]:(0.0820734649897)\n",
      " state (11)  A[0]:(0.118394002318)\n",
      " state (12)  A[0]:(0.16625341773)\n",
      " state (13)  A[0]:(0.222048789263)\n",
      " state (14)  A[0]:(0.279842406511)\n",
      " state (15)  A[0]:(0.334112823009)\n",
      "Episode 57000 finished after 11 . Running score: 0.01. Policy_loss: -95670.5302456, Value_loss: 1.09221842685. Times trained:               7581. Times reached goal: 15.               Steps done: 436972.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250003546476) A[1]:(0.249742761254) A[2]:(0.249832376838) A[3]:(0.250421345234)\n",
      " state (1)  A[0]:(0.249652922153) A[1]:(0.249735578895) A[2]:(0.249841243029) A[3]:(0.250770270824)\n",
      " state (2)  A[0]:(0.250408768654) A[1]:(0.249700546265) A[2]:(0.249823600054) A[3]:(0.250067055225)\n",
      " state (3)  A[0]:(0.250805079937) A[1]:(0.24979019165) A[2]:(0.249880537391) A[3]:(0.249524176121)\n",
      " state (4)  A[0]:(0.250780105591) A[1]:(0.250046819448) A[2]:(0.250040024519) A[3]:(0.249133020639)\n",
      " state (5)  A[0]:(0.250317692757) A[1]:(0.250497430563) A[2]:(0.25032222271) A[3]:(0.248862653971)\n",
      " state (6)  A[0]:(0.2494020015) A[1]:(0.251167118549) A[2]:(0.250746995211) A[3]:(0.248683929443)\n",
      " state (7)  A[0]:(0.248001039028) A[1]:(0.252084493637) A[2]:(0.251336067915) A[3]:(0.248578444123)\n",
      " state (8)  A[0]:(0.246089622378) A[1]:(0.253270179033) A[2]:(0.252104878426) A[3]:(0.24853529036)\n",
      " state (9)  A[0]:(0.243689760566) A[1]:(0.254716515541) A[2]:(0.253048866987) A[3]:(0.248544886708)\n",
      " state (10)  A[0]:(0.240901216865) A[1]:(0.256371498108) A[2]:(0.254132628441) A[3]:(0.248594656587)\n",
      " state (11)  A[0]:(0.237898498774) A[1]:(0.25814050436) A[2]:(0.255291938782) A[3]:(0.248669013381)\n",
      " state (12)  A[0]:(0.234890460968) A[1]:(0.259908139706) A[2]:(0.256449133158) A[3]:(0.248752295971)\n",
      " state (13)  A[0]:(0.232065886259) A[1]:(0.261568307877) A[2]:(0.257533550262) A[3]:(0.248832210898)\n",
      " state (14)  A[0]:(0.229554995894) A[1]:(0.263046711683) A[2]:(0.258496582508) A[3]:(0.248901680112)\n",
      " state (15)  A[0]:(0.227419406176) A[1]:(0.264307141304) A[2]:(0.259315341711) A[3]:(0.248958095908)\n",
      " state (0)  A[0]:(0.019519880414)\n",
      " state (1)  A[0]:(0.0170763172209)\n",
      " state (2)  A[0]:(0.0173537991941)\n",
      " state (3)  A[0]:(0.0179075766355)\n",
      " state (4)  A[0]:(0.0189889203757)\n",
      " state (5)  A[0]:(0.021038312465)\n",
      " state (6)  A[0]:(0.0248037111014)\n",
      " state (7)  A[0]:(0.0315329805017)\n",
      " state (8)  A[0]:(0.0432417318225)\n",
      " state (9)  A[0]:(0.0628686547279)\n",
      " state (10)  A[0]:(0.0937211662531)\n",
      " state (11)  A[0]:(0.137571513653)\n",
      " state (12)  A[0]:(0.192333549261)\n",
      " state (13)  A[0]:(0.252069741488)\n",
      " state (14)  A[0]:(0.309968203306)\n",
      " state (15)  A[0]:(0.361296534538)\n",
      "Episode 58000 finished after 17 . Running score: 0.02. Policy_loss: -95670.1335824, Value_loss: 1.00848391117. Times trained:               7677. Times reached goal: 13.               Steps done: 444649.\n",
      " state (0)  A[0]:(0.249923169613) A[1]:(0.24998806417) A[2]:(0.249719932675) A[3]:(0.25036880374)\n",
      " state (1)  A[0]:(0.249256938696) A[1]:(0.250038743019) A[2]:(0.249729260802) A[3]:(0.250974982977)\n",
      " state (2)  A[0]:(0.250153809786) A[1]:(0.250092387199) A[2]:(0.249786093831) A[3]:(0.24996766448)\n",
      " state (3)  A[0]:(0.250671625137) A[1]:(0.250302523375) A[2]:(0.249926358461) A[3]:(0.249099448323)\n",
      " state (4)  A[0]:(0.250715911388) A[1]:(0.250730723143) A[2]:(0.25019082427) A[3]:(0.248362571001)\n",
      " state (5)  A[0]:(0.250205546618) A[1]:(0.251442968845) A[2]:(0.25062468648) A[3]:(0.247726768255)\n",
      " state (6)  A[0]:(0.249063938856) A[1]:(0.252507984638) A[2]:(0.251275449991) A[3]:(0.247152656317)\n",
      " state (7)  A[0]:(0.247220933437) A[1]:(0.253988206387) A[2]:(0.252185791731) A[3]:(0.246605098248)\n",
      " state (8)  A[0]:(0.244653075933) A[1]:(0.255912125111) A[2]:(0.253375023603) A[3]:(0.246059730649)\n",
      " state (9)  A[0]:(0.241441443563) A[1]:(0.258237332106) A[2]:(0.254815369844) A[3]:(0.245505794883)\n",
      " state (10)  A[0]:(0.237797215581) A[1]:(0.260833650827) A[2]:(0.256422162056) A[3]:(0.244946911931)\n",
      " state (11)  A[0]:(0.234019502997) A[1]:(0.263509124517) A[2]:(0.258072465658) A[3]:(0.244398891926)\n",
      " state (12)  A[0]:(0.230405658484) A[1]:(0.266067713499) A[2]:(0.259643167257) A[3]:(0.243883416057)\n",
      " state (13)  A[0]:(0.227173522115) A[1]:(0.268362015486) A[2]:(0.26104414463) A[3]:(0.243420317769)\n",
      " state (14)  A[0]:(0.224432781339) A[1]:(0.270315021276) A[2]:(0.262230426073) A[3]:(0.243021801114)\n",
      " state (15)  A[0]:(0.22220055759) A[1]:(0.27191221714) A[2]:(0.263195961714) A[3]:(0.242691218853)\n",
      " state (0)  A[0]:(0.0170139297843)\n",
      " state (1)  A[0]:(0.0145091116428)\n",
      " state (2)  A[0]:(0.0147199509665)\n",
      " state (3)  A[0]:(0.0151291145012)\n",
      " state (4)  A[0]:(0.0159086827189)\n",
      " state (5)  A[0]:(0.0173551402986)\n",
      " state (6)  A[0]:(0.0199629310519)\n",
      " state (7)  A[0]:(0.0245430022478)\n",
      " state (8)  A[0]:(0.0324040651321)\n",
      " state (9)  A[0]:(0.0455469936132)\n",
      " state (10)  A[0]:(0.0666145458817)\n",
      " state (11)  A[0]:(0.0980882942677)\n",
      " state (12)  A[0]:(0.140576019883)\n",
      " state (13)  A[0]:(0.191423103213)\n",
      " state (14)  A[0]:(0.24542042613)\n",
      " state (15)  A[0]:(0.297222942114)\n",
      "Episode 59000 finished after 7 . Running score: 0.0. Policy_loss: -95669.3906376, Value_loss: 1.16410704438. Times trained:               7710. Times reached goal: 14.               Steps done: 452359.\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2500,  0.2498,  0.2500]])\n",
      "On state=0, selected action=2\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2500,  0.2498,  0.2500]])\n",
      "On state=0, selected action=2\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2500,  0.2498,  0.2500]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2498,  0.2500,  0.2497,  0.2504]])\n",
      "On state=1, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2500,  0.2498,  0.2500]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2500,  0.2498,  0.2500]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2498,  0.2500]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2498,  0.2500,  0.2497,  0.2504]])\n",
      "On state=1, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2498,  0.2500,  0.2497,  0.2504]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2498,  0.2500]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2498,  0.2500]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2500,  0.2497,  0.2504]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2500,  0.2497,  0.2504]])\n",
      "On state=1, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2500,  0.2498,  0.2500]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2500,  0.2497,  0.2504]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2508,  0.2499,  0.2498,  0.2495]])\n",
      "On state=2, selected action=1\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2496,  0.2519,  0.2512,  0.2473]])\n",
      "On state=6, selected action=3\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250304788351) A[1]:(0.249977052212) A[2]:(0.249750137329) A[3]:(0.249968037009)\n",
      " state (1)  A[0]:(0.249892383814) A[1]:(0.249994903803) A[2]:(0.249729901552) A[3]:(0.250382810831)\n",
      " state (2)  A[0]:(0.25079408288) A[1]:(0.249932900071) A[2]:(0.249759092927) A[3]:(0.24951389432)\n",
      " state (3)  A[0]:(0.251273453236) A[1]:(0.250045180321) A[2]:(0.249877408147) A[3]:(0.248804003)\n",
      " state (4)  A[0]:(0.251254528761) A[1]:(0.250385731459) A[2]:(0.250124752522) A[3]:(0.248234957457)\n",
      " state (5)  A[0]:(0.250705987215) A[1]:(0.250995993614) A[2]:(0.250536233187) A[3]:(0.247761845589)\n",
      " state (6)  A[0]:(0.249589666724) A[1]:(0.251920610666) A[2]:(0.251148968935) A[3]:(0.247340798378)\n",
      " state (7)  A[0]:(0.247850000858) A[1]:(0.253208249807) A[2]:(0.252000927925) A[3]:(0.246940866113)\n",
      " state (8)  A[0]:(0.245456367731) A[1]:(0.254886627197) A[2]:(0.253113567829) A[3]:(0.246543392539)\n",
      " state (9)  A[0]:(0.242464259267) A[1]:(0.256927400827) A[2]:(0.254468291998) A[3]:(0.24614007771)\n",
      " state (10)  A[0]:(0.239048033953) A[1]:(0.259226232767) A[2]:(0.255993574858) A[3]:(0.245732098818)\n",
      " state (11)  A[0]:(0.235472917557) A[1]:(0.261619746685) A[2]:(0.257578164339) A[3]:(0.245329216123)\n",
      " state (12)  A[0]:(0.232017055154) A[1]:(0.263932615519) A[2]:(0.259104132652) A[3]:(0.244946196675)\n",
      " state (13)  A[0]:(0.228895977139) A[1]:(0.266026139259) A[2]:(0.260479927063) A[3]:(0.244597986341)\n",
      " state (14)  A[0]:(0.226227805018) A[1]:(0.267821907997) A[2]:(0.26165536046) A[3]:(0.244294926524)\n",
      " state (15)  A[0]:(0.224041357636) A[1]:(0.269298881292) A[2]:(0.26261857152) A[3]:(0.244041189551)\n",
      " state (0)  A[0]:(0.0182462260127)\n",
      " state (1)  A[0]:(0.0150882359594)\n",
      " state (2)  A[0]:(0.0153103861958)\n",
      " state (3)  A[0]:(0.0157599691302)\n",
      " state (4)  A[0]:(0.016649575904)\n",
      " state (5)  A[0]:(0.0183562040329)\n",
      " state (6)  A[0]:(0.0215272828937)\n",
      " state (7)  A[0]:(0.0272579193115)\n",
      " state (8)  A[0]:(0.0373515412211)\n",
      " state (9)  A[0]:(0.0545129999518)\n",
      " state (10)  A[0]:(0.0819299072027)\n",
      " state (11)  A[0]:(0.1215647012)\n",
      " state (12)  A[0]:(0.171859264374)\n",
      " state (13)  A[0]:(0.227466166019)\n",
      " state (14)  A[0]:(0.28192487359)\n",
      " state (15)  A[0]:(0.330567419529)\n",
      "Episode 60000 finished after 17 . Running score: 0.01. Policy_loss: -95669.1507911, Value_loss: 1.16248690565. Times trained:               7518. Times reached goal: 19.               Steps done: 459877.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250341504812) A[1]:(0.249820202589) A[2]:(0.249678313732) A[3]:(0.250159978867)\n",
      " state (1)  A[0]:(0.249678626657) A[1]:(0.24987564981) A[2]:(0.249641418457) A[3]:(0.250804305077)\n",
      " state (2)  A[0]:(0.250633776188) A[1]:(0.249740883708) A[2]:(0.249655023217) A[3]:(0.24997036159)\n",
      " state (3)  A[0]:(0.251338392496) A[1]:(0.249702975154) A[2]:(0.24971832335) A[3]:(0.249240309)\n",
      " state (4)  A[0]:(0.251739531755) A[1]:(0.249792262912) A[2]:(0.24985486269) A[3]:(0.248613357544)\n",
      " state (5)  A[0]:(0.251797914505) A[1]:(0.250037252903) A[2]:(0.250089943409) A[3]:(0.248074874282)\n",
      " state (6)  A[0]:(0.251486450434) A[1]:(0.25046351552) A[2]:(0.250449329615) A[3]:(0.247600719333)\n",
      " state (7)  A[0]:(0.250777125359) A[1]:(0.251097053289) A[2]:(0.250959783792) A[3]:(0.247166052461)\n",
      " state (8)  A[0]:(0.249639466405) A[1]:(0.251962900162) A[2]:(0.251646757126) A[3]:(0.246750816703)\n",
      " state (9)  A[0]:(0.248056605458) A[1]:(0.253075629473) A[2]:(0.252526253462) A[3]:(0.246341511607)\n",
      " state (10)  A[0]:(0.246049925685) A[1]:(0.254425257444) A[2]:(0.25359326601) A[3]:(0.245931506157)\n",
      " state (11)  A[0]:(0.243698015809) A[1]:(0.255967229605) A[2]:(0.254813820124) A[3]:(0.24552090466)\n",
      " state (12)  A[0]:(0.241133823991) A[1]:(0.257624000311) A[2]:(0.25612655282) A[3]:(0.245115682483)\n",
      " state (13)  A[0]:(0.23851852119) A[1]:(0.259300470352) A[2]:(0.257455378771) A[3]:(0.244725599885)\n",
      " state (14)  A[0]:(0.236005067825) A[1]:(0.260905712843) A[2]:(0.258727610111) A[3]:(0.244361579418)\n",
      " state (15)  A[0]:(0.23370911181) A[1]:(0.262370377779) A[2]:(0.259887784719) A[3]:(0.244032740593)\n",
      " state (0)  A[0]:(0.0171590317041)\n",
      " state (1)  A[0]:(0.0151792736724)\n",
      " state (2)  A[0]:(0.0153293292969)\n",
      " state (3)  A[0]:(0.0156125081703)\n",
      " state (4)  A[0]:(0.0161387827247)\n",
      " state (5)  A[0]:(0.0170942507684)\n",
      " state (6)  A[0]:(0.0187831688672)\n",
      " state (7)  A[0]:(0.0216934457421)\n",
      " state (8)  A[0]:(0.0265997499228)\n",
      " state (9)  A[0]:(0.0347027145326)\n",
      " state (10)  A[0]:(0.0477285422385)\n",
      " state (11)  A[0]:(0.0677597373724)\n",
      " state (12)  A[0]:(0.0964872911572)\n",
      " state (13)  A[0]:(0.133984774351)\n",
      " state (14)  A[0]:(0.17795817554)\n",
      " state (15)  A[0]:(0.224422574043)\n",
      "Episode 61000 finished after 2 . Running score: 0.0. Policy_loss: -95668.9740071, Value_loss: 1.0246608291. Times trained:               7884. Times reached goal: 14.               Steps done: 467761.\n",
      " state (0)  A[0]:(0.249755591154) A[1]:(0.250143051147) A[2]:(0.249867096543) A[3]:(0.250234246254)\n",
      " state (1)  A[0]:(0.249044761062) A[1]:(0.250259786844) A[2]:(0.249823585153) A[3]:(0.250871896744)\n",
      " state (2)  A[0]:(0.249816834927) A[1]:(0.250180512667) A[2]:(0.249941468239) A[3]:(0.250061124563)\n",
      " state (3)  A[0]:(0.250323534012) A[1]:(0.250221997499) A[2]:(0.250133037567) A[3]:(0.249321386218)\n",
      " state (4)  A[0]:(0.250523418188) A[1]:(0.250413447618) A[2]:(0.250428646803) A[3]:(0.248634472489)\n",
      " state (5)  A[0]:(0.250345557928) A[1]:(0.250799804926) A[2]:(0.250873327255) A[3]:(0.24798129499)\n",
      " state (6)  A[0]:(0.249708563089) A[1]:(0.2514334023) A[2]:(0.251521676779) A[3]:(0.247336313128)\n",
      " state (7)  A[0]:(0.248535573483) A[1]:(0.252363890409) A[2]:(0.2524279356) A[3]:(0.246672600508)\n",
      " state (8)  A[0]:(0.246784225106) A[1]:(0.253619134426) A[2]:(0.253626167774) A[3]:(0.245970398188)\n",
      " state (9)  A[0]:(0.244489759207) A[1]:(0.255179703236) A[2]:(0.255104929209) A[3]:(0.245225608349)\n",
      " state (10)  A[0]:(0.241790950298) A[1]:(0.256963610649) A[2]:(0.256791770458) A[3]:(0.244453653693)\n",
      " state (11)  A[0]:(0.23890851438) A[1]:(0.258839637041) A[2]:(0.258565574884) A[3]:(0.243686243892)\n",
      " state (12)  A[0]:(0.236080810428) A[1]:(0.260665535927) A[2]:(0.260292887688) A[3]:(0.242960765958)\n",
      " state (13)  A[0]:(0.233497679234) A[1]:(0.262327522039) A[2]:(0.261866182089) A[3]:(0.242308586836)\n",
      " state (14)  A[0]:(0.231268540025) A[1]:(0.263760268688) A[2]:(0.263223171234) A[3]:(0.241748034954)\n",
      " state (15)  A[0]:(0.229426518083) A[1]:(0.264944523573) A[2]:(0.264345228672) A[3]:(0.241283670068)\n",
      " state (0)  A[0]:(0.0143794780597)\n",
      " state (1)  A[0]:(0.0121996439993)\n",
      " state (2)  A[0]:(0.0123167168349)\n",
      " state (3)  A[0]:(0.0125303734094)\n",
      " state (4)  A[0]:(0.0129154678434)\n",
      " state (5)  A[0]:(0.0135959014297)\n",
      " state (6)  A[0]:(0.0147697404027)\n",
      " state (7)  A[0]:(0.016746705398)\n",
      " state (8)  A[0]:(0.0200059898198)\n",
      " state (9)  A[0]:(0.0252793356776)\n",
      " state (10)  A[0]:(0.0336387082934)\n",
      " state (11)  A[0]:(0.0465008877218)\n",
      " state (12)  A[0]:(0.0653755143285)\n",
      " state (13)  A[0]:(0.0912287607789)\n",
      " state (14)  A[0]:(0.123698815703)\n",
      " state (15)  A[0]:(0.160812973976)\n",
      "Episode 62000 finished after 4 . Running score: 0.0. Policy_loss: -95668.0137218, Value_loss: 1.01920300052. Times trained:               7569. Times reached goal: 5.               Steps done: 475330.\n",
      " state (0)  A[0]:(0.249849021435) A[1]:(0.250174969435) A[2]:(0.249737754464) A[3]:(0.250238269567)\n",
      " state (1)  A[0]:(0.249428778887) A[1]:(0.250143229961) A[2]:(0.249771773815) A[3]:(0.250656187534)\n",
      " state (2)  A[0]:(0.250047832727) A[1]:(0.250209659338) A[2]:(0.249674290419) A[3]:(0.250068217516)\n",
      " state (3)  A[0]:(0.250251471996) A[1]:(0.250469982624) A[2]:(0.249692842364) A[3]:(0.249585688114)\n",
      " state (4)  A[0]:(0.249996528029) A[1]:(0.250977933407) A[2]:(0.249850720167) A[3]:(0.249174848199)\n",
      " state (5)  A[0]:(0.249254032969) A[1]:(0.251789361238) A[2]:(0.250165641308) A[3]:(0.248790919781)\n",
      " state (6)  A[0]:(0.247951745987) A[1]:(0.252981156111) A[2]:(0.250667244196) A[3]:(0.248399853706)\n",
      " state (7)  A[0]:(0.246000096202) A[1]:(0.25463154912) A[2]:(0.2513884902) A[3]:(0.247979849577)\n",
      " state (8)  A[0]:(0.243370726705) A[1]:(0.256769269705) A[2]:(0.252339273691) A[3]:(0.247520729899)\n",
      " state (9)  A[0]:(0.240171894431) A[1]:(0.259321391582) A[2]:(0.253480762243) A[3]:(0.247025981545)\n",
      " state (10)  A[0]:(0.236658498645) A[1]:(0.262104779482) A[2]:(0.254723161459) A[3]:(0.246513605118)\n",
      " state (11)  A[0]:(0.233153820038) A[1]:(0.264880567789) A[2]:(0.255953907967) A[3]:(0.246011719108)\n",
      " state (12)  A[0]:(0.229938179255) A[1]:(0.26743593812) A[2]:(0.257076948881) A[3]:(0.245548948646)\n",
      " state (13)  A[0]:(0.227180033922) A[1]:(0.26963827014) A[2]:(0.258035868406) A[3]:(0.245145797729)\n",
      " state (14)  A[0]:(0.224932387471) A[1]:(0.2714420259) A[2]:(0.258814483881) A[3]:(0.244811043143)\n",
      " state (15)  A[0]:(0.223167508841) A[1]:(0.27286490798) A[2]:(0.25942414999) A[3]:(0.244543373585)\n",
      " state (0)  A[0]:(0.014287373051)\n",
      " state (1)  A[0]:(0.0111131714657)\n",
      " state (2)  A[0]:(0.0113263130188)\n",
      " state (3)  A[0]:(0.0117797991261)\n",
      " state (4)  A[0]:(0.0127182137221)\n",
      " state (5)  A[0]:(0.0145906088874)\n",
      " state (6)  A[0]:(0.0181967541575)\n",
      " state (7)  A[0]:(0.0249408539385)\n",
      " state (8)  A[0]:(0.0371780134737)\n",
      " state (9)  A[0]:(0.0582905970514)\n",
      " state (10)  A[0]:(0.0915391966701)\n",
      " state (11)  A[0]:(0.13729506731)\n",
      " state (12)  A[0]:(0.191163569689)\n",
      " state (13)  A[0]:(0.24600662291)\n",
      " state (14)  A[0]:(0.295859187841)\n",
      " state (15)  A[0]:(0.337807834148)\n",
      "Episode 63000 finished after 9 . Running score: 0.04. Policy_loss: -95667.5323505, Value_loss: 1.1829874797. Times trained:               7847. Times reached goal: 18.               Steps done: 483177.\n",
      " state (0)  A[0]:(0.25050792098) A[1]:(0.249993756413) A[2]:(0.249682471156) A[3]:(0.24981585145)\n",
      " state (1)  A[0]:(0.250183671713) A[1]:(0.249898210168) A[2]:(0.249785929918) A[3]:(0.2501321733)\n",
      " state (2)  A[0]:(0.250972121954) A[1]:(0.249815911055) A[2]:(0.249518662691) A[3]:(0.249693334103)\n",
      " state (3)  A[0]:(0.251540839672) A[1]:(0.249806657434) A[2]:(0.249317497015) A[3]:(0.249335035682)\n",
      " state (4)  A[0]:(0.251859873533) A[1]:(0.249892041087) A[2]:(0.249191790819) A[3]:(0.249056279659)\n",
      " state (5)  A[0]:(0.251953393221) A[1]:(0.25007507205) A[2]:(0.249132961035) A[3]:(0.248838543892)\n",
      " state (6)  A[0]:(0.251857995987) A[1]:(0.250353008509) A[2]:(0.24912789464) A[3]:(0.248661056161)\n",
      " state (7)  A[0]:(0.251593381166) A[1]:(0.250728607178) A[2]:(0.249168664217) A[3]:(0.248509332538)\n",
      " state (8)  A[0]:(0.251159578562) A[1]:(0.251212030649) A[2]:(0.249253302813) A[3]:(0.248375147581)\n",
      " state (9)  A[0]:(0.250543892384) A[1]:(0.251818090677) A[2]:(0.249383717775) A[3]:(0.24825425446)\n",
      " state (10)  A[0]:(0.249729275703) A[1]:(0.252562552691) A[2]:(0.249563485384) A[3]:(0.248144701123)\n",
      " state (11)  A[0]:(0.248701825738) A[1]:(0.253457307816) A[2]:(0.249795392156) A[3]:(0.248045444489)\n",
      " state (12)  A[0]:(0.247458726168) A[1]:(0.25450566411) A[2]:(0.250079721212) A[3]:(0.247955903411)\n",
      " state (13)  A[0]:(0.246014177799) A[1]:(0.25569793582) A[2]:(0.250412583351) A[3]:(0.247875347733)\n",
      " state (14)  A[0]:(0.244402557611) A[1]:(0.257009506226) A[2]:(0.250785142183) A[3]:(0.24780279398)\n",
      " state (15)  A[0]:(0.242676317692) A[1]:(0.258402049541) A[2]:(0.251184493303) A[3]:(0.24773709476)\n",
      " state (0)  A[0]:(0.0150240594521)\n",
      " state (1)  A[0]:(0.0136424079537)\n",
      " state (2)  A[0]:(0.0138687985018)\n",
      " state (3)  A[0]:(0.0143588688225)\n",
      " state (4)  A[0]:(0.0153882335871)\n",
      " state (5)  A[0]:(0.0174681190401)\n",
      " state (6)  A[0]:(0.0215183701366)\n",
      " state (7)  A[0]:(0.0291689969599)\n",
      " state (8)  A[0]:(0.04315751791)\n",
      " state (9)  A[0]:(0.067329749465)\n",
      " state (10)  A[0]:(0.105069190264)\n",
      " state (11)  A[0]:(0.156001135707)\n",
      " state (12)  A[0]:(0.214412197471)\n",
      " state (13)  A[0]:(0.27230244875)\n",
      " state (14)  A[0]:(0.323709487915)\n",
      " state (15)  A[0]:(0.366174995899)\n",
      "Episode 64000 finished after 12 . Running score: 0.02. Policy_loss: -95667.7895204, Value_loss: 1.02561186602. Times trained:               7679. Times reached goal: 15.               Steps done: 490856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.2505,  0.2492,  0.2504,  0.2499]])\n",
      "On state=0, selected action=2\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2492,  0.2504,  0.2499]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2518,  0.2493,  0.2499,  0.2490]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2492,  0.2504,  0.2500]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2518,  0.2493,  0.2499,  0.2490]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2492,  0.2504,  0.2500]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2498,  0.2490,  0.2507,  0.2506]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2498,  0.2490,  0.2507,  0.2506]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2493,  0.2504,  0.2500]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2498,  0.2490,  0.2507,  0.2506]])\n",
      "On state=1, selected action=1\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250390529633) A[1]:(0.249253839254) A[2]:(0.250379383564) A[3]:(0.249976232648)\n",
      " state (1)  A[0]:(0.249756887555) A[1]:(0.248970419168) A[2]:(0.250705212355) A[3]:(0.250567525625)\n",
      " state (2)  A[0]:(0.250656366348) A[1]:(0.249021202326) A[2]:(0.250361919403) A[3]:(0.249960586429)\n",
      " state (3)  A[0]:(0.251347362995) A[1]:(0.249123603106) A[2]:(0.250090330839) A[3]:(0.249438732862)\n",
      " state (4)  A[0]:(0.251775532961) A[1]:(0.249311953783) A[2]:(0.249901041389) A[3]:(0.249011427164)\n",
      " state (5)  A[0]:(0.251942753792) A[1]:(0.249606832862) A[2]:(0.249788448215) A[3]:(0.248662009835)\n",
      " state (6)  A[0]:(0.251872897148) A[1]:(0.25002092123) A[2]:(0.24974167347) A[3]:(0.248364523053)\n",
      " state (7)  A[0]:(0.251576393843) A[1]:(0.250570803881) A[2]:(0.249754741788) A[3]:(0.248098060489)\n",
      " state (8)  A[0]:(0.251041829586) A[1]:(0.251280575991) A[2]:(0.249828070402) A[3]:(0.247849479318)\n",
      " state (9)  A[0]:(0.250243514776) A[1]:(0.252178937197) A[2]:(0.249966233969) A[3]:(0.247611343861)\n",
      " state (10)  A[0]:(0.249154284596) A[1]:(0.253291785717) A[2]:(0.250174283981) A[3]:(0.24737970531)\n",
      " state (11)  A[0]:(0.247760608792) A[1]:(0.254632622004) A[2]:(0.250454187393) A[3]:(0.247152641416)\n",
      " state (12)  A[0]:(0.246075823903) A[1]:(0.256192952394) A[2]:(0.250801622868) A[3]:(0.246929571033)\n",
      " state (13)  A[0]:(0.244147583842) A[1]:(0.257936954498) A[2]:(0.251204520464) A[3]:(0.246710970998)\n",
      " state (14)  A[0]:(0.242054671049) A[1]:(0.259803295135) A[2]:(0.251643896103) A[3]:(0.246498093009)\n",
      " state (15)  A[0]:(0.239894270897) A[1]:(0.261715322733) A[2]:(0.252097308636) A[3]:(0.246293142438)\n",
      " state (0)  A[0]:(0.0128460964188)\n",
      " state (1)  A[0]:(0.0119088087231)\n",
      " state (2)  A[0]:(0.0121219484136)\n",
      " state (3)  A[0]:(0.0125868730247)\n",
      " state (4)  A[0]:(0.0135702723637)\n",
      " state (5)  A[0]:(0.0155698750168)\n",
      " state (6)  A[0]:(0.019487131387)\n",
      " state (7)  A[0]:(0.0269340611994)\n",
      " state (8)  A[0]:(0.0406539775431)\n",
      " state (9)  A[0]:(0.0645747408271)\n",
      " state (10)  A[0]:(0.102271467447)\n",
      " state (11)  A[0]:(0.153563022614)\n",
      " state (12)  A[0]:(0.212735146284)\n",
      " state (13)  A[0]:(0.271587729454)\n",
      " state (14)  A[0]:(0.323939979076)\n",
      " state (15)  A[0]:(0.367211997509)\n",
      "Episode 65000 finished after 10 . Running score: 0.03. Policy_loss: -95667.031985, Value_loss: 1.17031934106. Times trained:               8020. Times reached goal: 20.               Steps done: 498876.\n",
      " state (0)  A[0]:(0.249814420938) A[1]:(0.249660372734) A[2]:(0.250400871038) A[3]:(0.250124305487)\n",
      " state (1)  A[0]:(0.249195411801) A[1]:(0.249581664801) A[2]:(0.250528275967) A[3]:(0.250694692135)\n",
      " state (2)  A[0]:(0.250066190958) A[1]:(0.249595969915) A[2]:(0.250406503677) A[3]:(0.24993135035)\n",
      " state (3)  A[0]:(0.250661790371) A[1]:(0.249710157514) A[2]:(0.250361353159) A[3]:(0.249266654253)\n",
      " state (4)  A[0]:(0.250910371542) A[1]:(0.249974936247) A[2]:(0.25041565299) A[3]:(0.248698994517)\n",
      " state (5)  A[0]:(0.250774681568) A[1]:(0.250434666872) A[2]:(0.250589907169) A[3]:(0.24820074439)\n",
      " state (6)  A[0]:(0.250222533941) A[1]:(0.251134335995) A[2]:(0.250907838345) A[3]:(0.247735306621)\n",
      " state (7)  A[0]:(0.249203503132) A[1]:(0.252126097679) A[2]:(0.251399219036) A[3]:(0.247271165252)\n",
      " state (8)  A[0]:(0.247662678361) A[1]:(0.253458708525) A[2]:(0.252092212439) A[3]:(0.246786400676)\n",
      " state (9)  A[0]:(0.245581299067) A[1]:(0.255151540041) A[2]:(0.252997308969) A[3]:(0.246269851923)\n",
      " state (10)  A[0]:(0.243019908667) A[1]:(0.257166683674) A[2]:(0.254090815783) A[3]:(0.245722547174)\n",
      " state (11)  A[0]:(0.240132331848) A[1]:(0.259399533272) A[2]:(0.255310326815) A[3]:(0.245157837868)\n",
      " state (12)  A[0]:(0.237133964896) A[1]:(0.261699914932) A[2]:(0.256568044424) A[3]:(0.244598060846)\n",
      " state (13)  A[0]:(0.234241425991) A[1]:(0.263914108276) A[2]:(0.257776081562) A[3]:(0.244068369269)\n",
      " state (14)  A[0]:(0.231619536877) A[1]:(0.265922784805) A[2]:(0.25886797905) A[3]:(0.243589699268)\n",
      " state (15)  A[0]:(0.229359298944) A[1]:(0.267658442259) A[2]:(0.259807497263) A[3]:(0.243174731731)\n",
      " state (0)  A[0]:(0.0153225697577)\n",
      " state (1)  A[0]:(0.0146473729983)\n",
      " state (2)  A[0]:(0.0149443745613)\n",
      " state (3)  A[0]:(0.0156484078616)\n",
      " state (4)  A[0]:(0.0172480214387)\n",
      " state (5)  A[0]:(0.0207105912268)\n",
      " state (6)  A[0]:(0.0279063712806)\n",
      " state (7)  A[0]:(0.0423210076988)\n",
      " state (8)  A[0]:(0.0694684162736)\n",
      " state (9)  A[0]:(0.114698916674)\n",
      " state (10)  A[0]:(0.177274212241)\n",
      " state (11)  A[0]:(0.247725322843)\n",
      " state (12)  A[0]:(0.314318776131)\n",
      " state (13)  A[0]:(0.37011551857)\n",
      " state (14)  A[0]:(0.413650304079)\n",
      " state (15)  A[0]:(0.446373105049)\n",
      "Episode 66000 finished after 10 . Running score: 0.03. Policy_loss: -95667.115916, Value_loss: 1.00624632269. Times trained:               7943. Times reached goal: 16.               Steps done: 506819.\n",
      " state (0)  A[0]:(0.250058472157) A[1]:(0.250268489122) A[2]:(0.249698981643) A[3]:(0.249974116683)\n",
      " state (1)  A[0]:(0.249626874924) A[1]:(0.250228762627) A[2]:(0.249658450484) A[3]:(0.250485956669)\n",
      " state (2)  A[0]:(0.251006007195) A[1]:(0.250393986702) A[2]:(0.249689370394) A[3]:(0.248910620809)\n",
      " state (3)  A[0]:(0.251974523067) A[1]:(0.250709593296) A[2]:(0.249783247709) A[3]:(0.247532576323)\n",
      " state (4)  A[0]:(0.252430051565) A[1]:(0.251252144575) A[2]:(0.24998703599) A[3]:(0.246330738068)\n",
      " state (5)  A[0]:(0.252334624529) A[1]:(0.25209608674) A[2]:(0.250346451998) A[3]:(0.245222836733)\n",
      " state (6)  A[0]:(0.251648455858) A[1]:(0.253321170807) A[2]:(0.250909268856) A[3]:(0.244121059775)\n",
      " state (7)  A[0]:(0.250313520432) A[1]:(0.255003601313) A[2]:(0.251720994711) A[3]:(0.24296194315)\n",
      " state (8)  A[0]:(0.248303472996) A[1]:(0.257177591324) A[2]:(0.252802699804) A[3]:(0.24171628058)\n",
      " state (9)  A[0]:(0.245693519711) A[1]:(0.259787112474) A[2]:(0.254124075174) A[3]:(0.240395307541)\n",
      " state (10)  A[0]:(0.242689177394) A[1]:(0.26266759634) A[2]:(0.255594164133) A[3]:(0.239049091935)\n",
      " state (11)  A[0]:(0.239578977227) A[1]:(0.265585154295) A[2]:(0.257085114717) A[3]:(0.23775075376)\n",
      " state (12)  A[0]:(0.236640557647) A[1]:(0.268312841654) A[2]:(0.258475542068) A[3]:(0.236571013927)\n",
      " state (13)  A[0]:(0.234065130353) A[1]:(0.270693749189) A[2]:(0.259683907032) A[3]:(0.235557213426)\n",
      " state (14)  A[0]:(0.231936410069) A[1]:(0.272660166025) A[2]:(0.260677009821) A[3]:(0.23472635448)\n",
      " state (15)  A[0]:(0.230252787471) A[1]:(0.27421694994) A[2]:(0.261459559202) A[3]:(0.234070703387)\n",
      " state (0)  A[0]:(0.0154648246244)\n",
      " state (1)  A[0]:(0.015290921554)\n",
      " state (2)  A[0]:(0.0155585305765)\n",
      " state (3)  A[0]:(0.0161803327501)\n",
      " state (4)  A[0]:(0.0175665542483)\n",
      " state (5)  A[0]:(0.0205092597753)\n",
      " state (6)  A[0]:(0.0264883991331)\n",
      " state (7)  A[0]:(0.0381542481482)\n",
      " state (8)  A[0]:(0.0595712028444)\n",
      " state (9)  A[0]:(0.0947357714176)\n",
      " state (10)  A[0]:(0.143581479788)\n",
      " state (11)  A[0]:(0.199738353491)\n",
      " state (12)  A[0]:(0.254392921925)\n",
      " state (13)  A[0]:(0.301548659801)\n",
      " state (14)  A[0]:(0.339282989502)\n",
      " state (15)  A[0]:(0.368226826191)\n",
      "Episode 67000 finished after 17 . Running score: 0.01. Policy_loss: -95666.7967693, Value_loss: 1.06080368961. Times trained:               7828. Times reached goal: 20.               Steps done: 514647.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250150114298) A[1]:(0.25018119812) A[2]:(0.249751836061) A[3]:(0.249916881323)\n",
      " state (1)  A[0]:(0.249577924609) A[1]:(0.250046133995) A[2]:(0.249705478549) A[3]:(0.250670522451)\n",
      " state (2)  A[0]:(0.250772327185) A[1]:(0.250082403421) A[2]:(0.249649271369) A[3]:(0.249495998025)\n",
      " state (3)  A[0]:(0.251642465591) A[1]:(0.250213265419) A[2]:(0.249638810754) A[3]:(0.248505502939)\n",
      " state (4)  A[0]:(0.252141624689) A[1]:(0.250478208065) A[2]:(0.249696955085) A[3]:(0.247683182359)\n",
      " state (5)  A[0]:(0.252321749926) A[1]:(0.250895559788) A[2]:(0.249831914902) A[3]:(0.24695083499)\n",
      " state (6)  A[0]:(0.252220124006) A[1]:(0.251487642527) A[2]:(0.250053554773) A[3]:(0.246238708496)\n",
      " state (7)  A[0]:(0.251827329397) A[1]:(0.252291142941) A[2]:(0.250380456448) A[3]:(0.245501071215)\n",
      " state (8)  A[0]:(0.251104414463) A[1]:(0.253350049257) A[2]:(0.250836104155) A[3]:(0.244709417224)\n",
      " state (9)  A[0]:(0.250011116266) A[1]:(0.254700124264) A[2]:(0.251440316439) A[3]:(0.243848443031)\n",
      " state (10)  A[0]:(0.248535752296) A[1]:(0.256348967552) A[2]:(0.252198189497) A[3]:(0.242917120457)\n",
      " state (11)  A[0]:(0.246718481183) A[1]:(0.258259445429) A[2]:(0.253091186285) A[3]:(0.241930857301)\n",
      " state (12)  A[0]:(0.244656443596) A[1]:(0.2603469491) A[2]:(0.254076063633) A[3]:(0.240920498967)\n",
      " state (13)  A[0]:(0.242485165596) A[1]:(0.262495338917) A[2]:(0.255093544722) A[3]:(0.239925980568)\n",
      " state (14)  A[0]:(0.240345194936) A[1]:(0.264584630728) A[2]:(0.256083101034) A[3]:(0.238987073302)\n",
      " state (15)  A[0]:(0.238351657987) A[1]:(0.266516894102) A[2]:(0.256996303797) A[3]:(0.238135144114)\n",
      " state (0)  A[0]:(0.0148374857381)\n",
      " state (1)  A[0]:(0.0147537309676)\n",
      " state (2)  A[0]:(0.0149547625333)\n",
      " state (3)  A[0]:(0.0153951514512)\n",
      " state (4)  A[0]:(0.0163288936019)\n",
      " state (5)  A[0]:(0.0182268023491)\n",
      " state (6)  A[0]:(0.0219300203025)\n",
      " state (7)  A[0]:(0.02890641056)\n",
      " state (8)  A[0]:(0.0415493696928)\n",
      " state (9)  A[0]:(0.0630397871137)\n",
      " state (10)  A[0]:(0.0958392247558)\n",
      " state (11)  A[0]:(0.139060020447)\n",
      " state (12)  A[0]:(0.187684118748)\n",
      " state (13)  A[0]:(0.23531550169)\n",
      " state (14)  A[0]:(0.277410358191)\n",
      " state (15)  A[0]:(0.312172353268)\n",
      "Episode 68000 finished after 10 . Running score: 0.01. Policy_loss: -95666.4668951, Value_loss: 1.04158509434. Times trained:               7733. Times reached goal: 12.               Steps done: 522380.\n",
      " state (0)  A[0]:(0.249294057488) A[1]:(0.250458925962) A[2]:(0.250288069248) A[3]:(0.249958977103)\n",
      " state (1)  A[0]:(0.248891606927) A[1]:(0.250450134277) A[2]:(0.250253081322) A[3]:(0.25040525198)\n",
      " state (2)  A[0]:(0.24938865006) A[1]:(0.25048032403) A[2]:(0.250348806381) A[3]:(0.249782189727)\n",
      " state (3)  A[0]:(0.249416142702) A[1]:(0.250698983669) A[2]:(0.250532060862) A[3]:(0.249352753162)\n",
      " state (4)  A[0]:(0.249045282602) A[1]:(0.251113146544) A[2]:(0.250825732946) A[3]:(0.249015808105)\n",
      " state (5)  A[0]:(0.248291626573) A[1]:(0.251750260592) A[2]:(0.251263618469) A[3]:(0.248694464564)\n",
      " state (6)  A[0]:(0.247070670128) A[1]:(0.252679139376) A[2]:(0.251900345087) A[3]:(0.248349800706)\n",
      " state (7)  A[0]:(0.245256841183) A[1]:(0.253985971212) A[2]:(0.252798944712) A[3]:(0.247958302498)\n",
      " state (8)  A[0]:(0.242756217718) A[1]:(0.255734473467) A[2]:(0.254004627466) A[3]:(0.247504666448)\n",
      " state (9)  A[0]:(0.239585608244) A[1]:(0.257917881012) A[2]:(0.255511432886) A[3]:(0.246985062957)\n",
      " state (10)  A[0]:(0.235915899277) A[1]:(0.260430216789) A[2]:(0.257242530584) A[3]:(0.246411323547)\n",
      " state (11)  A[0]:(0.232036203146) A[1]:(0.2630867064) A[2]:(0.259066730738) A[3]:(0.245810389519)\n",
      " state (12)  A[0]:(0.228255793452) A[1]:(0.265684723854) A[2]:(0.260842680931) A[3]:(0.245216831565)\n",
      " state (13)  A[0]:(0.224812522531) A[1]:(0.268063932657) A[2]:(0.262461006641) A[3]:(0.244662538171)\n",
      " state (14)  A[0]:(0.221835538745) A[1]:(0.270133376122) A[2]:(0.263861745596) A[3]:(0.244169354439)\n",
      " state (15)  A[0]:(0.219357699156) A[1]:(0.271865993738) A[2]:(0.265029251575) A[3]:(0.243747040629)\n",
      " state (0)  A[0]:(0.0183267109096)\n",
      " state (1)  A[0]:(0.01816313155)\n",
      " state (2)  A[0]:(0.0184424687177)\n",
      " state (3)  A[0]:(0.0191024299711)\n",
      " state (4)  A[0]:(0.0205939691514)\n",
      " state (5)  A[0]:(0.0237935762852)\n",
      " state (6)  A[0]:(0.0303426273167)\n",
      " state (7)  A[0]:(0.0431464985013)\n",
      " state (8)  A[0]:(0.0664486140013)\n",
      " state (9)  A[0]:(0.103800803423)\n",
      " state (10)  A[0]:(0.153828695416)\n",
      " state (11)  A[0]:(0.209124878049)\n",
      " state (12)  A[0]:(0.261160254478)\n",
      " state (13)  A[0]:(0.304955303669)\n",
      " state (14)  A[0]:(0.339407384396)\n",
      " state (15)  A[0]:(0.365524560213)\n",
      "Episode 69000 finished after 18 . Running score: 0.02. Policy_loss: -95665.9560865, Value_loss: 1.20267999671. Times trained:               7610. Times reached goal: 15.               Steps done: 529990.\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2496,  0.2500]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2496,  0.2500]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2520,  0.2500,  0.2494,  0.2486]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2520,  0.2500,  0.2494,  0.2486]])\n",
      "On state=4, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2503,  0.2523,  0.2501,  0.2472]])\n",
      "On state=8, selected action=3\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2520,  0.2500,  0.2494,  0.2486]])\n",
      "On state=4, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2499,  0.2496,  0.2500]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2498,  0.2496,  0.2506]])\n",
      "On state=1, selected action=1\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2511,  0.2497,  0.2495,  0.2497]])\n",
      "On state=2, selected action=2\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2511,  0.2498,  0.2495,  0.2497]])\n",
      "On state=2, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2510,  0.2498,  0.2495,  0.2497]])\n",
      "On state=2, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2501,  0.2498,  0.2496,  0.2506]])\n",
      "On state=1, selected action=1\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2510,  0.2498,  0.2495,  0.2497]])\n",
      "On state=2, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2498,  0.2496,  0.2506]])\n",
      "On state=1, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2498,  0.2496,  0.2506]])\n",
      "On state=1, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2498,  0.2496,  0.2506]])\n",
      "On state=1, selected action=2\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2510,  0.2498,  0.2495,  0.2497]])\n",
      "On state=2, selected action=2\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2516,  0.2509,  0.2497,  0.2478]])\n",
      "On state=6, selected action=1\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.25044208765) A[1]:(0.249965906143) A[2]:(0.249598965049) A[3]:(0.249993056059)\n",
      " state (1)  A[0]:(0.250036478043) A[1]:(0.249789789319) A[2]:(0.249558717012) A[3]:(0.250615000725)\n",
      " state (2)  A[0]:(0.251029193401) A[1]:(0.24977196753) A[2]:(0.249466523528) A[3]:(0.249732255936)\n",
      " state (3)  A[0]:(0.251678496599) A[1]:(0.2498485744) A[2]:(0.249421849847) A[3]:(0.249051079154)\n",
      " state (4)  A[0]:(0.251942425966) A[1]:(0.25005787611) A[2]:(0.249443635345) A[3]:(0.248556002975)\n",
      " state (5)  A[0]:(0.251900494099) A[1]:(0.250400781631) A[2]:(0.249527126551) A[3]:(0.248171627522)\n",
      " state (6)  A[0]:(0.251611173153) A[1]:(0.250880777836) A[2]:(0.249668493867) A[3]:(0.247839510441)\n",
      " state (7)  A[0]:(0.251080304384) A[1]:(0.251518756151) A[2]:(0.249873206019) A[3]:(0.247527748346)\n",
      " state (8)  A[0]:(0.250277876854) A[1]:(0.252349555492) A[2]:(0.250153809786) A[3]:(0.24721878767)\n",
      " state (9)  A[0]:(0.249156937003) A[1]:(0.253415316343) A[2]:(0.250526368618) A[3]:(0.246901452541)\n",
      " state (10)  A[0]:(0.247670039535) A[1]:(0.254756450653) A[2]:(0.251006096601) A[3]:(0.246567428112)\n",
      " state (11)  A[0]:(0.245787009597) A[1]:(0.256399869919) A[2]:(0.251602321863) A[3]:(0.246210828424)\n",
      " state (12)  A[0]:(0.243512302637) A[1]:(0.258345991373) A[2]:(0.25231307745) A[3]:(0.245828658342)\n",
      " state (13)  A[0]:(0.240896910429) A[1]:(0.260559350252) A[2]:(0.25312179327) A[3]:(0.245421901345)\n",
      " state (14)  A[0]:(0.238037824631) A[1]:(0.262968122959) A[2]:(0.253997981548) A[3]:(0.244996055961)\n",
      " state (15)  A[0]:(0.235062941909) A[1]:(0.265474200249) A[2]:(0.254902094603) A[3]:(0.244560748339)\n",
      " state (0)  A[0]:(0.0191393718123)\n",
      " state (1)  A[0]:(0.0190891902894)\n",
      " state (2)  A[0]:(0.0193134434521)\n",
      " state (3)  A[0]:(0.0198164414614)\n",
      " state (4)  A[0]:(0.0209055524319)\n",
      " state (5)  A[0]:(0.0231599528342)\n",
      " state (6)  A[0]:(0.0276310294867)\n",
      " state (7)  A[0]:(0.0361765436828)\n",
      " state (8)  A[0]:(0.0518087372184)\n",
      " state (9)  A[0]:(0.0783165693283)\n",
      " state (10)  A[0]:(0.11799480021)\n",
      " state (11)  A[0]:(0.168504342437)\n",
      " state (12)  A[0]:(0.22303687036)\n",
      " state (13)  A[0]:(0.274413049221)\n",
      " state (14)  A[0]:(0.318378508091)\n",
      " state (15)  A[0]:(0.353796333075)\n",
      "Episode 70000 finished after 18 . Running score: 0.03. Policy_loss: -95665.2435206, Value_loss: 1.21699856212. Times trained:               7566. Times reached goal: 12.               Steps done: 537556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250037372112) A[1]:(0.249899163842) A[2]:(0.250325262547) A[3]:(0.249738186598)\n",
      " state (1)  A[0]:(0.249645531178) A[1]:(0.249776408076) A[2]:(0.250376939774) A[3]:(0.250201165676)\n",
      " state (2)  A[0]:(0.250437736511) A[1]:(0.249931454659) A[2]:(0.250366449356) A[3]:(0.249264359474)\n",
      " state (3)  A[0]:(0.250779896975) A[1]:(0.25023072958) A[2]:(0.250437885523) A[3]:(0.24855145812)\n",
      " state (4)  A[0]:(0.250610262156) A[1]:(0.250749409199) A[2]:(0.250615984201) A[3]:(0.248024329543)\n",
      " state (5)  A[0]:(0.249942421913) A[1]:(0.251540511847) A[2]:(0.250921726227) A[3]:(0.247595325112)\n",
      " state (6)  A[0]:(0.248739808798) A[1]:(0.252675145864) A[2]:(0.251387655735) A[3]:(0.247197344899)\n",
      " state (7)  A[0]:(0.246904820204) A[1]:(0.254249483347) A[2]:(0.252057462931) A[3]:(0.246788278222)\n",
      " state (8)  A[0]:(0.244328439236) A[1]:(0.256359428167) A[2]:(0.252972304821) A[3]:(0.246339783072)\n",
      " state (9)  A[0]:(0.240962207317) A[1]:(0.259055435658) A[2]:(0.254149109125) A[3]:(0.245833232999)\n",
      " state (10)  A[0]:(0.236886382103) A[1]:(0.262293368578) A[2]:(0.255558490753) A[3]:(0.245261743665)\n",
      " state (11)  A[0]:(0.232333317399) A[1]:(0.265913486481) A[2]:(0.257118761539) A[3]:(0.24463441968)\n",
      " state (12)  A[0]:(0.227637380362) A[1]:(0.269670903683) A[2]:(0.258714914322) A[3]:(0.24397675693)\n",
      " state (13)  A[0]:(0.22313670814) A[1]:(0.273305654526) A[2]:(0.260233193636) A[3]:(0.243324488401)\n",
      " state (14)  A[0]:(0.219085946679) A[1]:(0.276610821486) A[2]:(0.261590212584) A[3]:(0.242713049054)\n",
      " state (15)  A[0]:(0.215621054173) A[1]:(0.279466360807) A[2]:(0.262743890285) A[3]:(0.242168650031)\n",
      " state (0)  A[0]:(0.0198433361948)\n",
      " state (1)  A[0]:(0.0197492055595)\n",
      " state (2)  A[0]:(0.0199875682592)\n",
      " state (3)  A[0]:(0.0205337777734)\n",
      " state (4)  A[0]:(0.0217380654067)\n",
      " state (5)  A[0]:(0.0242682825774)\n",
      " state (6)  A[0]:(0.0293500646949)\n",
      " state (7)  A[0]:(0.0391552858055)\n",
      " state (8)  A[0]:(0.0571125522256)\n",
      " state (9)  A[0]:(0.0871105343103)\n",
      " state (10)  A[0]:(0.130501300097)\n",
      " state (11)  A[0]:(0.183198526502)\n",
      " state (12)  A[0]:(0.237409472466)\n",
      " state (13)  A[0]:(0.286426573992)\n",
      " state (14)  A[0]:(0.327082455158)\n",
      " state (15)  A[0]:(0.35909935832)\n",
      "Episode 71000 finished after 4 . Running score: 0.04. Policy_loss: -95665.4161456, Value_loss: 1.03757201223. Times trained:               8013. Times reached goal: 14.               Steps done: 545569.\n",
      " state (0)  A[0]:(0.249862611294) A[1]:(0.249876439571) A[2]:(0.249889671803) A[3]:(0.250371217728)\n",
      " state (1)  A[0]:(0.24911762774) A[1]:(0.249805703759) A[2]:(0.249735280871) A[3]:(0.251341372728)\n",
      " state (2)  A[0]:(0.250479221344) A[1]:(0.249809965491) A[2]:(0.249990671873) A[3]:(0.249720200896)\n",
      " state (3)  A[0]:(0.251438945532) A[1]:(0.249936491251) A[2]:(0.250270754099) A[3]:(0.248353779316)\n",
      " state (4)  A[0]:(0.251860737801) A[1]:(0.250263601542) A[2]:(0.25061750412) A[3]:(0.247258141637)\n",
      " state (5)  A[0]:(0.251708447933) A[1]:(0.250851392746) A[2]:(0.251081615686) A[3]:(0.246358513832)\n",
      " state (6)  A[0]:(0.250964969397) A[1]:(0.251757383347) A[2]:(0.251718819141) A[3]:(0.245558843017)\n",
      " state (7)  A[0]:(0.249574869871) A[1]:(0.253053694963) A[2]:(0.252590954304) A[3]:(0.244780480862)\n",
      " state (8)  A[0]:(0.247454330325) A[1]:(0.254820019007) A[2]:(0.253759145737) A[3]:(0.243966519833)\n",
      " state (9)  A[0]:(0.24454267323) A[1]:(0.257113814354) A[2]:(0.255264550447) A[3]:(0.24307897687)\n",
      " state (10)  A[0]:(0.240866750479) A[1]:(0.259929955006) A[2]:(0.257102578878) A[3]:(0.242100715637)\n",
      " state (11)  A[0]:(0.236583918333) A[1]:(0.263171195984) A[2]:(0.259205281734) A[3]:(0.241039544344)\n",
      " state (12)  A[0]:(0.231970235705) A[1]:(0.26665315032) A[2]:(0.261447876692) A[3]:(0.239928737283)\n",
      " state (13)  A[0]:(0.227352231741) A[1]:(0.270148098469) A[2]:(0.263680517673) A[3]:(0.238819152117)\n",
      " state (14)  A[0]:(0.223021194339) A[1]:(0.273444861174) A[2]:(0.265768826008) A[3]:(0.237765088677)\n",
      " state (15)  A[0]:(0.219175130129) A[1]:(0.276393055916) A[2]:(0.267621189356) A[3]:(0.236810624599)\n",
      " state (0)  A[0]:(0.018576676026)\n",
      " state (1)  A[0]:(0.0183822587132)\n",
      " state (2)  A[0]:(0.0185647457838)\n",
      " state (3)  A[0]:(0.0189604666084)\n",
      " state (4)  A[0]:(0.0197937209159)\n",
      " state (5)  A[0]:(0.0214796029031)\n",
      " state (6)  A[0]:(0.0247567296028)\n",
      " state (7)  A[0]:(0.0309152770787)\n",
      " state (8)  A[0]:(0.0421183034778)\n",
      " state (9)  A[0]:(0.061522051692)\n",
      " state (10)  A[0]:(0.0923544317484)\n",
      " state (11)  A[0]:(0.135447457433)\n",
      " state (12)  A[0]:(0.187221124768)\n",
      " state (13)  A[0]:(0.241084545851)\n",
      " state (14)  A[0]:(0.291033685207)\n",
      " state (15)  A[0]:(0.333771467209)\n",
      "Episode 72000 finished after 11 . Running score: 0.0. Policy_loss: -95642.8649793, Value_loss: 1.00812714969. Times trained:               7704. Times reached goal: 17.               Steps done: 553273.\n",
      " state (0)  A[0]:(0.250876277685) A[1]:(0.249725490808) A[2]:(0.249314785004) A[3]:(0.250083446503)\n",
      " state (1)  A[0]:(0.250162273645) A[1]:(0.249628573656) A[2]:(0.248961731791) A[3]:(0.251247435808)\n",
      " state (2)  A[0]:(0.251349359751) A[1]:(0.249425292015) A[2]:(0.249086230993) A[3]:(0.250139176846)\n",
      " state (3)  A[0]:(0.25226393342) A[1]:(0.249304950237) A[2]:(0.249208718538) A[3]:(0.249222442508)\n",
      " state (4)  A[0]:(0.252807885408) A[1]:(0.249312609434) A[2]:(0.249355897307) A[3]:(0.248523592949)\n",
      " state (5)  A[0]:(0.253021240234) A[1]:(0.249450087547) A[2]:(0.249545946717) A[3]:(0.247982695699)\n",
      " state (6)  A[0]:(0.252974152565) A[1]:(0.249706879258) A[2]:(0.24979005754) A[3]:(0.247528880835)\n",
      " state (7)  A[0]:(0.252700120211) A[1]:(0.25008431077) A[2]:(0.250101059675) A[3]:(0.247114554048)\n",
      " state (8)  A[0]:(0.252192795277) A[1]:(0.250598430634) A[2]:(0.250496834517) A[3]:(0.246711954474)\n",
      " state (9)  A[0]:(0.25142031908) A[1]:(0.251275986433) A[2]:(0.250999987125) A[3]:(0.246303737164)\n",
      " state (10)  A[0]:(0.250339031219) A[1]:(0.252148419619) A[2]:(0.251635313034) A[3]:(0.245877236128)\n",
      " state (11)  A[0]:(0.248906850815) A[1]:(0.253245234489) A[2]:(0.252425372601) A[3]:(0.245422497392)\n",
      " state (12)  A[0]:(0.247097894549) A[1]:(0.254585117102) A[2]:(0.253384500742) A[3]:(0.244932457805)\n",
      " state (13)  A[0]:(0.244916453958) A[1]:(0.256167143583) A[2]:(0.254512190819) A[3]:(0.244404152036)\n",
      " state (14)  A[0]:(0.24240629375) A[1]:(0.257964730263) A[2]:(0.255789101124) A[3]:(0.24383983016)\n",
      " state (15)  A[0]:(0.239649862051) A[1]:(0.25992551446) A[2]:(0.257177114487) A[3]:(0.2432474792)\n",
      " state (0)  A[0]:(0.0199780799448)\n",
      " state (1)  A[0]:(0.0194936357439)\n",
      " state (2)  A[0]:(0.0197009257972)\n",
      " state (3)  A[0]:(0.0201651901007)\n",
      " state (4)  A[0]:(0.0211701896042)\n",
      " state (5)  A[0]:(0.0232509300113)\n",
      " state (6)  A[0]:(0.0273777060211)\n",
      " state (7)  A[0]:(0.0352684259415)\n",
      " state (8)  A[0]:(0.0497537143528)\n",
      " state (9)  A[0]:(0.074577704072)\n",
      " state (10)  A[0]:(0.112469099462)\n",
      " state (11)  A[0]:(0.161971271038)\n",
      " state (12)  A[0]:(0.216870427132)\n",
      " state (13)  A[0]:(0.269808828831)\n",
      " state (14)  A[0]:(0.315931528807)\n",
      " state (15)  A[0]:(0.353573948145)\n",
      "Episode 73000 finished after 14 . Running score: 0.01. Policy_loss: -95642.6507966, Value_loss: 1.03441119589. Times trained:               7767. Times reached goal: 14.               Steps done: 561040.\n",
      " state (0)  A[0]:(0.250071585178) A[1]:(0.250077694654) A[2]:(0.249885246158) A[3]:(0.249965488911)\n",
      " state (1)  A[0]:(0.248837217689) A[1]:(0.250126689672) A[2]:(0.249727874994) A[3]:(0.251308202744)\n",
      " state (2)  A[0]:(0.250029534101) A[1]:(0.250139296055) A[2]:(0.249936401844) A[3]:(0.249894753098)\n",
      " state (3)  A[0]:(0.250962942839) A[1]:(0.250260978937) A[2]:(0.250191807747) A[3]:(0.248584255576)\n",
      " state (4)  A[0]:(0.251550972462) A[1]:(0.250543802977) A[2]:(0.250528216362) A[3]:(0.247376978397)\n",
      " state (5)  A[0]:(0.251694202423) A[1]:(0.251056939363) A[2]:(0.250995337963) A[3]:(0.246253475547)\n",
      " state (6)  A[0]:(0.251284629107) A[1]:(0.2518851161) A[2]:(0.251658082008) A[3]:(0.245172157884)\n",
      " state (7)  A[0]:(0.25020506978) A[1]:(0.253125786781) A[2]:(0.252592980862) A[3]:(0.244076102972)\n",
      " state (8)  A[0]:(0.24833868444) A[1]:(0.254878401756) A[2]:(0.253878176212) A[3]:(0.242904767394)\n",
      " state (9)  A[0]:(0.245603859425) A[1]:(0.257217228413) A[2]:(0.255571216345) A[3]:(0.241607680917)\n",
      " state (10)  A[0]:(0.242011487484) A[1]:(0.26015034318) A[2]:(0.25767827034) A[3]:(0.240159928799)\n",
      " state (11)  A[0]:(0.237711638212) A[1]:(0.263584524393) A[2]:(0.260129362345) A[3]:(0.238574504852)\n",
      " state (12)  A[0]:(0.232988521457) A[1]:(0.267325252295) A[2]:(0.262781381607) A[3]:(0.236904785037)\n",
      " state (13)  A[0]:(0.228192642331) A[1]:(0.271121740341) A[2]:(0.265453636646) A[3]:(0.235231950879)\n",
      " state (14)  A[0]:(0.223646834493) A[1]:(0.274733960629) A[2]:(0.267977535725) A[3]:(0.23364160955)\n",
      " state (15)  A[0]:(0.219578459859) A[1]:(0.277985692024) A[2]:(0.270233601332) A[3]:(0.232202231884)\n",
      " state (0)  A[0]:(0.0194080509245)\n",
      " state (1)  A[0]:(0.0189025048167)\n",
      " state (2)  A[0]:(0.0190634373575)\n",
      " state (3)  A[0]:(0.0194050651044)\n",
      " state (4)  A[0]:(0.0201119501144)\n",
      " state (5)  A[0]:(0.0215222835541)\n",
      " state (6)  A[0]:(0.0242312606424)\n",
      " state (7)  A[0]:(0.0292688310146)\n",
      " state (8)  A[0]:(0.0383774936199)\n",
      " state (9)  A[0]:(0.054256528616)\n",
      " state (10)  A[0]:(0.0802114456892)\n",
      " state (11)  A[0]:(0.118441417813)\n",
      " state (12)  A[0]:(0.167618736625)\n",
      " state (13)  A[0]:(0.222507655621)\n",
      " state (14)  A[0]:(0.276639819145)\n",
      " state (15)  A[0]:(0.325263768435)\n",
      "Episode 74000 finished after 6 . Running score: 0.01. Policy_loss: -95644.330417, Value_loss: 1.01368159459. Times trained:               7758. Times reached goal: 22.               Steps done: 568798.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.2507,  0.2499,  0.2501,  0.2494]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2499,  0.2500,  0.2494]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2499,  0.2500,  0.2494]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2499,  0.2500,  0.2494]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2518,  0.2507,  0.2505,  0.2471]])\n",
      "On state=4, selected action=1\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2518,  0.2507,  0.2505,  0.2471]])\n",
      "On state=4, selected action=2\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2507,  0.2499,  0.2500,  0.2494]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2518,  0.2507,  0.2505,  0.2471]])\n",
      "On state=4, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250689059496) A[1]:(0.249878376722) A[2]:(0.250046283007) A[3]:(0.249386310577)\n",
      " state (1)  A[0]:(0.249630823731) A[1]:(0.249727934599) A[2]:(0.250022321939) A[3]:(0.250618964434)\n",
      " state (2)  A[0]:(0.25061249733) A[1]:(0.249888390303) A[2]:(0.25009021163) A[3]:(0.249408870935)\n",
      " state (3)  A[0]:(0.251340866089) A[1]:(0.250183731318) A[2]:(0.250231683254) A[3]:(0.248243674636)\n",
      " state (4)  A[0]:(0.251782774925) A[1]:(0.250660330057) A[2]:(0.250473499298) A[3]:(0.247083321214)\n",
      " state (5)  A[0]:(0.251836150885) A[1]:(0.251402437687) A[2]:(0.250864088535) A[3]:(0.24589727819)\n",
      " state (6)  A[0]:(0.251354396343) A[1]:(0.252529799938) A[2]:(0.251472502947) A[3]:(0.244643285871)\n",
      " state (7)  A[0]:(0.250161081553) A[1]:(0.254190981388) A[2]:(0.252384245396) A[3]:(0.243263632059)\n",
      " state (8)  A[0]:(0.248079091311) A[1]:(0.256540387869) A[2]:(0.253686517477) A[3]:(0.241693988442)\n",
      " state (9)  A[0]:(0.244991242886) A[1]:(0.259688407183) A[2]:(0.25543692708) A[3]:(0.239883422852)\n",
      " state (10)  A[0]:(0.240921542048) A[1]:(0.263633489609) A[2]:(0.257623672485) A[3]:(0.237821295857)\n",
      " state (11)  A[0]:(0.236086681485) A[1]:(0.268214523792) A[2]:(0.260141134262) A[3]:(0.235557645559)\n",
      " state (12)  A[0]:(0.230863243341) A[1]:(0.273128658533) A[2]:(0.262807667255) A[3]:(0.233200460672)\n",
      " state (13)  A[0]:(0.225676059723) A[1]:(0.278017193079) A[2]:(0.265420764685) A[3]:(0.230886012316)\n",
      " state (14)  A[0]:(0.220877662301) A[1]:(0.282567977905) A[2]:(0.267815351486) A[3]:(0.228738993406)\n",
      " state (15)  A[0]:(0.216683328152) A[1]:(0.286578595638) A[2]:(0.269894093275) A[3]:(0.226843953133)\n",
      " state (0)  A[0]:(0.0190900918096)\n",
      " state (1)  A[0]:(0.0174007974565)\n",
      " state (2)  A[0]:(0.0175381265581)\n",
      " state (3)  A[0]:(0.017825089395)\n",
      " state (4)  A[0]:(0.0184108894318)\n",
      " state (5)  A[0]:(0.0195661745965)\n",
      " state (6)  A[0]:(0.0217615067959)\n",
      " state (7)  A[0]:(0.0257994309068)\n",
      " state (8)  A[0]:(0.0330249853432)\n",
      " state (9)  A[0]:(0.0455472320318)\n",
      " state (10)  A[0]:(0.0661189258099)\n",
      " state (11)  A[0]:(0.0970561727881)\n",
      " state (12)  A[0]:(0.138284564018)\n",
      " state (13)  A[0]:(0.186325073242)\n",
      " state (14)  A[0]:(0.235769733787)\n",
      " state (15)  A[0]:(0.281863391399)\n",
      "Episode 75000 finished after 8 . Running score: 0.01. Policy_loss: -95642.4512726, Value_loss: 1.00928840919. Times trained:               7771. Times reached goal: 19.               Steps done: 576569.\n",
      " state (0)  A[0]:(0.250274717808) A[1]:(0.249691799283) A[2]:(0.249476149678) A[3]:(0.250557303429)\n",
      " state (1)  A[0]:(0.249401643872) A[1]:(0.24937389791) A[2]:(0.249187156558) A[3]:(0.252037346363)\n",
      " state (2)  A[0]:(0.250149458647) A[1]:(0.249258086085) A[2]:(0.249162629247) A[3]:(0.251429855824)\n",
      " state (3)  A[0]:(0.250789493322) A[1]:(0.24917472899) A[2]:(0.249147951603) A[3]:(0.250887870789)\n",
      " state (4)  A[0]:(0.251272767782) A[1]:(0.249142810702) A[2]:(0.24915394187) A[3]:(0.250430434942)\n",
      " state (5)  A[0]:(0.251572668552) A[1]:(0.249177962542) A[2]:(0.249190092087) A[3]:(0.250059336424)\n",
      " state (6)  A[0]:(0.251713097095) A[1]:(0.249278992414) A[2]:(0.249257802963) A[3]:(0.24975015223)\n",
      " state (7)  A[0]:(0.251729786396) A[1]:(0.249439671636) A[2]:(0.249355509877) A[3]:(0.249474972486)\n",
      " state (8)  A[0]:(0.251646488905) A[1]:(0.249657392502) A[2]:(0.249483138323) A[3]:(0.249212920666)\n",
      " state (9)  A[0]:(0.251471400261) A[1]:(0.249935075641) A[2]:(0.249643146992) A[3]:(0.248950362206)\n",
      " state (10)  A[0]:(0.251200854778) A[1]:(0.250280499458) A[2]:(0.249840438366) A[3]:(0.248678252101)\n",
      " state (11)  A[0]:(0.250823169947) A[1]:(0.25070515275) A[2]:(0.250081807375) A[3]:(0.248389899731)\n",
      " state (12)  A[0]:(0.250321656466) A[1]:(0.251223117113) A[2]:(0.250375539064) A[3]:(0.248079687357)\n",
      " state (13)  A[0]:(0.24967700243) A[1]:(0.251849889755) A[2]:(0.250730603933) A[3]:(0.247742459178)\n",
      " state (14)  A[0]:(0.248869627714) A[1]:(0.252600967884) A[2]:(0.251155912876) A[3]:(0.247373566031)\n",
      " state (15)  A[0]:(0.247882336378) A[1]:(0.253489792347) A[2]:(0.251658946276) A[3]:(0.246968910098)\n",
      " state (0)  A[0]:(0.0211055632681)\n",
      " state (1)  A[0]:(0.0178438145667)\n",
      " state (2)  A[0]:(0.0180081296712)\n",
      " state (3)  A[0]:(0.018368050456)\n",
      " state (4)  A[0]:(0.0191349498928)\n",
      " state (5)  A[0]:(0.0207069590688)\n",
      " state (6)  A[0]:(0.0238070562482)\n",
      " state (7)  A[0]:(0.0297344774008)\n",
      " state (8)  A[0]:(0.0407708026469)\n",
      " state (9)  A[0]:(0.0605131387711)\n",
      " state (10)  A[0]:(0.0932013392448)\n",
      " state (11)  A[0]:(0.140979349613)\n",
      " state (12)  A[0]:(0.200760707259)\n",
      " state (13)  A[0]:(0.264915823936)\n",
      " state (14)  A[0]:(0.325640588999)\n",
      " state (15)  A[0]:(0.378224670887)\n",
      "Episode 76000 finished after 4 . Running score: 0.01. Policy_loss: -95642.0431624, Value_loss: 1.00496554324. Times trained:               7538. Times reached goal: 23.               Steps done: 584107.\n",
      " state (0)  A[0]:(0.251654416323) A[1]:(0.249489530921) A[2]:(0.249337896705) A[3]:(0.249518215656)\n",
      " state (1)  A[0]:(0.251136183739) A[1]:(0.249137744308) A[2]:(0.249073252082) A[3]:(0.250652849674)\n",
      " state (2)  A[0]:(0.252337932587) A[1]:(0.249045401812) A[2]:(0.249039947987) A[3]:(0.249576762319)\n",
      " state (3)  A[0]:(0.253335058689) A[1]:(0.249032899737) A[2]:(0.249041497707) A[3]:(0.248590484262)\n",
      " state (4)  A[0]:(0.254019290209) A[1]:(0.249156087637) A[2]:(0.249113082886) A[3]:(0.247711524367)\n",
      " state (5)  A[0]:(0.254326790571) A[1]:(0.249464079738) A[2]:(0.249286875129) A[3]:(0.246922194958)\n",
      " state (6)  A[0]:(0.254245847464) A[1]:(0.249992996454) A[2]:(0.24958743155) A[3]:(0.246173769236)\n",
      " state (7)  A[0]:(0.253763765097) A[1]:(0.250783860683) A[2]:(0.250041753054) A[3]:(0.24541066587)\n",
      " state (8)  A[0]:(0.252836078405) A[1]:(0.251895427704) A[2]:(0.250686913729) A[3]:(0.24458155036)\n",
      " state (9)  A[0]:(0.251387059689) A[1]:(0.253403604031) A[2]:(0.251569449902) A[3]:(0.243639871478)\n",
      " state (10)  A[0]:(0.249328762293) A[1]:(0.255389392376) A[2]:(0.252737760544) A[3]:(0.242544084787)\n",
      " state (11)  A[0]:(0.246591985226) A[1]:(0.257917910814) A[2]:(0.254228293896) A[3]:(0.241261810064)\n",
      " state (12)  A[0]:(0.243162736297) A[1]:(0.261011004448) A[2]:(0.256048589945) A[3]:(0.239777728915)\n",
      " state (13)  A[0]:(0.239111289382) A[1]:(0.264623403549) A[2]:(0.258163481951) A[3]:(0.238101795316)\n",
      " state (14)  A[0]:(0.234598278999) A[1]:(0.26863527298) A[2]:(0.260493069887) A[3]:(0.236273407936)\n",
      " state (15)  A[0]:(0.229849457741) A[1]:(0.272868156433) A[2]:(0.26292514801) A[3]:(0.234357178211)\n",
      " state (0)  A[0]:(0.0188344810158)\n",
      " state (1)  A[0]:(0.0155601426959)\n",
      " state (2)  A[0]:(0.0157170202583)\n",
      " state (3)  A[0]:(0.0160684287548)\n",
      " state (4)  A[0]:(0.0168324802071)\n",
      " state (5)  A[0]:(0.018427124247)\n",
      " state (6)  A[0]:(0.021626226604)\n",
      " state (7)  A[0]:(0.0278532728553)\n",
      " state (8)  A[0]:(0.0396619662642)\n",
      " state (9)  A[0]:(0.0610998608172)\n",
      " state (10)  A[0]:(0.0967626348138)\n",
      " state (11)  A[0]:(0.148410946131)\n",
      " state (12)  A[0]:(0.211716204882)\n",
      " state (13)  A[0]:(0.277949422598)\n",
      " state (14)  A[0]:(0.339140295982)\n",
      " state (15)  A[0]:(0.39106875658)\n",
      "Episode 77000 finished after 2 . Running score: 0.03. Policy_loss: -95641.9057361, Value_loss: 1.01465886647. Times trained:               7672. Times reached goal: 17.               Steps done: 591779.\n",
      " state (0)  A[0]:(0.249560594559) A[1]:(0.250625252724) A[2]:(0.250117599964) A[3]:(0.249696552753)\n",
      " state (1)  A[0]:(0.248754352331) A[1]:(0.250539958477) A[2]:(0.250012427568) A[3]:(0.250693202019)\n",
      " state (2)  A[0]:(0.249584376812) A[1]:(0.250739634037) A[2]:(0.250175714493) A[3]:(0.249500244856)\n",
      " state (3)  A[0]:(0.250064462423) A[1]:(0.251147627831) A[2]:(0.250444710255) A[3]:(0.248343259096)\n",
      " state (4)  A[0]:(0.250050902367) A[1]:(0.251872032881) A[2]:(0.25088378787) A[3]:(0.247193232179)\n",
      " state (5)  A[0]:(0.249359115958) A[1]:(0.253072351217) A[2]:(0.251589387655) A[3]:(0.245979204774)\n",
      " state (6)  A[0]:(0.247774794698) A[1]:(0.254955559969) A[2]:(0.252686530352) A[3]:(0.244583159685)\n",
      " state (7)  A[0]:(0.245032727718) A[1]:(0.257778465748) A[2]:(0.254325866699) A[3]:(0.242862910032)\n",
      " state (8)  A[0]:(0.240866571665) A[1]:(0.261802226305) A[2]:(0.25665128231) A[3]:(0.240679904819)\n",
      " state (9)  A[0]:(0.235151365399) A[1]:(0.267177373171) A[2]:(0.259727120399) A[3]:(0.237944111228)\n",
      " state (10)  A[0]:(0.228067949414) A[1]:(0.273802489042) A[2]:(0.26345834136) A[3]:(0.234671264887)\n",
      " state (11)  A[0]:(0.220146253705) A[1]:(0.281265854836) A[2]:(0.267572820187) A[3]:(0.231015101075)\n",
      " state (12)  A[0]:(0.212108165026) A[1]:(0.288951247931) A[2]:(0.271705150604) A[3]:(0.227235421538)\n",
      " state (13)  A[0]:(0.204612091184) A[1]:(0.296249300241) A[2]:(0.275527626276) A[3]:(0.223610952497)\n",
      " state (14)  A[0]:(0.198078230023) A[1]:(0.30272936821) A[2]:(0.278837025166) A[3]:(0.220355391502)\n",
      " state (15)  A[0]:(0.192664384842) A[1]:(0.308191150427) A[2]:(0.281563699245) A[3]:(0.217580810189)\n",
      " state (0)  A[0]:(0.0162715539336)\n",
      " state (1)  A[0]:(0.0143714556471)\n",
      " state (2)  A[0]:(0.0145271746442)\n",
      " state (3)  A[0]:(0.0148813370615)\n",
      " state (4)  A[0]:(0.0156616345048)\n",
      " state (5)  A[0]:(0.0173087865114)\n",
      " state (6)  A[0]:(0.0206477455795)\n",
      " state (7)  A[0]:(0.0272156484425)\n",
      " state (8)  A[0]:(0.0397998578846)\n",
      " state (9)  A[0]:(0.0628146976233)\n",
      " state (10)  A[0]:(0.101114861667)\n",
      " state (11)  A[0]:(0.156111910939)\n",
      " state (12)  A[0]:(0.222511053085)\n",
      " state (13)  A[0]:(0.290799587965)\n",
      " state (14)  A[0]:(0.35291108489)\n",
      " state (15)  A[0]:(0.404958367348)\n",
      "Episode 78000 finished after 7 . Running score: 0.02. Policy_loss: -95641.73955, Value_loss: 1.01538530139. Times trained:               7715. Times reached goal: 24.               Steps done: 599494.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250280439854) A[1]:(0.249489471316) A[2]:(0.250020772219) A[3]:(0.250209331512)\n",
      " state (1)  A[0]:(0.249737456441) A[1]:(0.249180331826) A[2]:(0.249932125211) A[3]:(0.251150131226)\n",
      " state (2)  A[0]:(0.250534027815) A[1]:(0.249179914594) A[2]:(0.249941959977) A[3]:(0.250344127417)\n",
      " state (3)  A[0]:(0.251082748175) A[1]:(0.249273046851) A[2]:(0.250009119511) A[3]:(0.249635085464)\n",
      " state (4)  A[0]:(0.251289099455) A[1]:(0.249519512057) A[2]:(0.250164568424) A[3]:(0.249026805162)\n",
      " state (5)  A[0]:(0.25109398365) A[1]:(0.249974563718) A[2]:(0.250441342592) A[3]:(0.248490154743)\n",
      " state (6)  A[0]:(0.250457197428) A[1]:(0.250690937042) A[2]:(0.250875383615) A[3]:(0.247976481915)\n",
      " state (7)  A[0]:(0.249304875731) A[1]:(0.251741081476) A[2]:(0.251515895128) A[3]:(0.247438088059)\n",
      " state (8)  A[0]:(0.247518286109) A[1]:(0.253223091364) A[2]:(0.252426981926) A[3]:(0.246831670403)\n",
      " state (9)  A[0]:(0.24495857954) A[1]:(0.255246698856) A[2]:(0.253677517176) A[3]:(0.24611723423)\n",
      " state (10)  A[0]:(0.241519063711) A[1]:(0.257900506258) A[2]:(0.255319416523) A[3]:(0.245261028409)\n",
      " state (11)  A[0]:(0.237191021442) A[1]:(0.261206924915) A[2]:(0.257358908653) A[3]:(0.244243189692)\n",
      " state (12)  A[0]:(0.232112571597) A[1]:(0.26508513093) A[2]:(0.259734541178) A[3]:(0.243067756295)\n",
      " state (13)  A[0]:(0.226564019918) A[1]:(0.26934799552) A[2]:(0.262319743633) A[3]:(0.241768285632)\n",
      " state (14)  A[0]:(0.220901623368) A[1]:(0.273742318153) A[2]:(0.264952540398) A[3]:(0.240403518081)\n",
      " state (15)  A[0]:(0.215465098619) A[1]:(0.278013080359) A[2]:(0.267478018999) A[3]:(0.239043757319)\n",
      " state (0)  A[0]:(0.0146214505658)\n",
      " state (1)  A[0]:(0.013769172132)\n",
      " state (2)  A[0]:(0.0139041068032)\n",
      " state (3)  A[0]:(0.0142033323646)\n",
      " state (4)  A[0]:(0.0148480795324)\n",
      " state (5)  A[0]:(0.0161824114621)\n",
      " state (6)  A[0]:(0.0188347548246)\n",
      " state (7)  A[0]:(0.0239427909255)\n",
      " state (8)  A[0]:(0.0335335768759)\n",
      " state (9)  A[0]:(0.0508967004716)\n",
      " state (10)  A[0]:(0.0801440775394)\n",
      " state (11)  A[0]:(0.123833283782)\n",
      " state (12)  A[0]:(0.179790526628)\n",
      " state (13)  A[0]:(0.241147339344)\n",
      " state (14)  A[0]:(0.300252437592)\n",
      " state (15)  A[0]:(0.352115184069)\n",
      "Episode 79000 finished after 2 . Running score: 0.01. Policy_loss: -95641.2436324, Value_loss: 1.21006309972. Times trained:               7831. Times reached goal: 17.               Steps done: 607325.\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2498,  0.2503,  0.2497]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2502,  0.2498,  0.2503,  0.2497]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2497,  0.2502,  0.2502]])\n",
      "On state=1, selected action=0\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2499,  0.2497,  0.2502,  0.2502]])\n",
      "On state=1, selected action=1\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.25017541647) A[1]:(0.249775707722) A[2]:(0.25033095479) A[3]:(0.249717950821)\n",
      " state (1)  A[0]:(0.249931648374) A[1]:(0.249657690525) A[2]:(0.250247299671) A[3]:(0.250163406134)\n",
      " state (2)  A[0]:(0.250713855028) A[1]:(0.249575957656) A[2]:(0.250479370356) A[3]:(0.249230846763)\n",
      " state (3)  A[0]:(0.250975489616) A[1]:(0.249724090099) A[2]:(0.250875651836) A[3]:(0.248424753547)\n",
      " state (4)  A[0]:(0.250494241714) A[1]:(0.250234693289) A[2]:(0.25155121088) A[3]:(0.247719794512)\n",
      " state (5)  A[0]:(0.249027833343) A[1]:(0.251260101795) A[2]:(0.252668380737) A[3]:(0.247043669224)\n",
      " state (6)  A[0]:(0.246264785528) A[1]:(0.252991914749) A[2]:(0.254442751408) A[3]:(0.246300533414)\n",
      " state (7)  A[0]:(0.241845265031) A[1]:(0.255648195744) A[2]:(0.257114380598) A[3]:(0.245392173529)\n",
      " state (8)  A[0]:(0.235561668873) A[1]:(0.259366482496) A[2]:(0.260837733746) A[3]:(0.244234085083)\n",
      " state (9)  A[0]:(0.227639436722) A[1]:(0.264046669006) A[2]:(0.26552516222) A[3]:(0.242788717151)\n",
      " state (10)  A[0]:(0.218825086951) A[1]:(0.269288003445) A[2]:(0.27078333497) A[3]:(0.241103574634)\n",
      " state (11)  A[0]:(0.210112184286) A[1]:(0.274525821209) A[2]:(0.276049226522) A[3]:(0.239312753081)\n",
      " state (12)  A[0]:(0.202320605516) A[1]:(0.279268056154) A[2]:(0.280827671289) A[3]:(0.237583696842)\n",
      " state (13)  A[0]:(0.195870012045) A[1]:(0.283240765333) A[2]:(0.284839510918) A[3]:(0.236049681902)\n",
      " state (14)  A[0]:(0.190816670656) A[1]:(0.286384940147) A[2]:(0.288021057844) A[3]:(0.23477730155)\n",
      " state (15)  A[0]:(0.187004327774) A[1]:(0.288776695728) A[2]:(0.290445566177) A[3]:(0.233773455024)\n",
      " state (0)  A[0]:(0.0143277049065)\n",
      " state (1)  A[0]:(0.0134658263996)\n",
      " state (2)  A[0]:(0.013625651598)\n",
      " state (3)  A[0]:(0.0139977568761)\n",
      " state (4)  A[0]:(0.0148338917643)\n",
      " state (5)  A[0]:(0.0166271049529)\n",
      " state (6)  A[0]:(0.0203109681606)\n",
      " state (7)  A[0]:(0.0276398509741)\n",
      " state (8)  A[0]:(0.0417622104287)\n",
      " state (9)  A[0]:(0.0673792213202)\n",
      " state (10)  A[0]:(0.10879637301)\n",
      " state (11)  A[0]:(0.16555160284)\n",
      " state (12)  A[0]:(0.230539113283)\n",
      " state (13)  A[0]:(0.294277101755)\n",
      " state (14)  A[0]:(0.350165575743)\n",
      " state (15)  A[0]:(0.395793735981)\n",
      "Episode 80000 finished after 4 . Running score: 0.03. Policy_loss: -95640.4894516, Value_loss: 1.49669754798. Times trained:               7434. Times reached goal: 16.               Steps done: 614759.\n",
      " state (0)  A[0]:(0.250489234924) A[1]:(0.250328719616) A[2]:(0.249818652868) A[3]:(0.249363422394)\n",
      " state (1)  A[0]:(0.249961167574) A[1]:(0.250268012285) A[2]:(0.249734565616) A[3]:(0.250036299229)\n",
      " state (2)  A[0]:(0.251224130392) A[1]:(0.250310003757) A[2]:(0.249782562256) A[3]:(0.248683348298)\n",
      " state (3)  A[0]:(0.252128332853) A[1]:(0.250535815954) A[2]:(0.249927103519) A[3]:(0.247408777475)\n",
      " state (4)  A[0]:(0.252522855997) A[1]:(0.251047641039) A[2]:(0.250238269567) A[3]:(0.246191203594)\n",
      " state (5)  A[0]:(0.252208024263) A[1]:(0.251992583275) A[2]:(0.25081884861) A[3]:(0.244980543852)\n",
      " state (6)  A[0]:(0.250948280096) A[1]:(0.253559827805) A[2]:(0.25180119276) A[3]:(0.243690684438)\n",
      " state (7)  A[0]:(0.248476341367) A[1]:(0.255970329046) A[2]:(0.253337115049) A[3]:(0.242216199636)\n",
      " state (8)  A[0]:(0.244565397501) A[1]:(0.259419083595) A[2]:(0.255555272102) A[3]:(0.240460202098)\n",
      " state (9)  A[0]:(0.239187285304) A[1]:(0.263958990574) A[2]:(0.258480191231) A[3]:(0.238373562694)\n",
      " state (10)  A[0]:(0.232654646039) A[1]:(0.26938945055) A[2]:(0.261961311102) A[3]:(0.235994592309)\n",
      " state (11)  A[0]:(0.225591018796) A[1]:(0.275261878967) A[2]:(0.265688538551) A[3]:(0.233458548784)\n",
      " state (12)  A[0]:(0.218706667423) A[1]:(0.281032502651) A[2]:(0.269305050373) A[3]:(0.230955794454)\n",
      " state (13)  A[0]:(0.212550252676) A[1]:(0.286253422499) A[2]:(0.272533684969) A[3]:(0.228662639856)\n",
      " state (14)  A[0]:(0.207399189472) A[1]:(0.290675282478) A[2]:(0.275234252214) A[3]:(0.226691260934)\n",
      " state (15)  A[0]:(0.203294500709) A[1]:(0.294238448143) A[2]:(0.277387142181) A[3]:(0.225079923868)\n",
      " state (0)  A[0]:(0.0125733427703)\n",
      " state (1)  A[0]:(0.0122921410948)\n",
      " state (2)  A[0]:(0.0124408332631)\n",
      " state (3)  A[0]:(0.0127812186256)\n",
      " state (4)  A[0]:(0.0135347712785)\n",
      " state (5)  A[0]:(0.0151294013485)\n",
      " state (6)  A[0]:(0.0183617547154)\n",
      " state (7)  A[0]:(0.0247005019337)\n",
      " state (8)  A[0]:(0.0367522984743)\n",
      " state (9)  A[0]:(0.0584758482873)\n",
      " state (10)  A[0]:(0.0938650369644)\n",
      " state (11)  A[0]:(0.14347422123)\n",
      " state (12)  A[0]:(0.202124387026)\n",
      " state (13)  A[0]:(0.26158246398)\n",
      " state (14)  A[0]:(0.315244376659)\n",
      " state (15)  A[0]:(0.360071510077)\n",
      "Episode 81000 finished after 4 . Running score: 0.02. Policy_loss: -95641.3201359, Value_loss: 1.0679555461. Times trained:               7655. Times reached goal: 21.               Steps done: 622414.\n",
      " state (0)  A[0]:(0.250412195921) A[1]:(0.250230640173) A[2]:(0.249931752682) A[3]:(0.249425396323)\n",
      " state (1)  A[0]:(0.249712556601) A[1]:(0.250219702721) A[2]:(0.249872088432) A[3]:(0.250195652246)\n",
      " state (2)  A[0]:(0.251147657633) A[1]:(0.25009265542) A[2]:(0.249857962132) A[3]:(0.248901754618)\n",
      " state (3)  A[0]:(0.252347588539) A[1]:(0.250070184469) A[2]:(0.249893158674) A[3]:(0.247689113021)\n",
      " state (4)  A[0]:(0.253210783005) A[1]:(0.250209212303) A[2]:(0.250015348196) A[3]:(0.24656458199)\n",
      " state (5)  A[0]:(0.253636419773) A[1]:(0.250579535961) A[2]:(0.250274986029) A[3]:(0.245509088039)\n",
      " state (6)  A[0]:(0.253533154726) A[1]:(0.251259416342) A[2]:(0.250732064247) A[3]:(0.244475364685)\n",
      " state (7)  A[0]:(0.252797573805) A[1]:(0.252340972424) A[2]:(0.251457810402) A[3]:(0.243403702974)\n",
      " state (8)  A[0]:(0.251310229301) A[1]:(0.253924548626) A[2]:(0.252528458834) A[3]:(0.242236718535)\n",
      " state (9)  A[0]:(0.248974189162) A[1]:(0.256090044975) A[2]:(0.25400352478) A[3]:(0.240932255983)\n",
      " state (10)  A[0]:(0.245784789324) A[1]:(0.258848369122) A[2]:(0.255890399218) A[3]:(0.239476397634)\n",
      " state (11)  A[0]:(0.241888552904) A[1]:(0.262100428343) A[2]:(0.258116185665) A[3]:(0.237894877791)\n",
      " state (12)  A[0]:(0.23757815361) A[1]:(0.26563808322) A[2]:(0.260531097651) A[3]:(0.236252695322)\n",
      " state (13)  A[0]:(0.233213737607) A[1]:(0.269198298454) A[2]:(0.262949943542) A[3]:(0.2346380651)\n",
      " state (14)  A[0]:(0.229117870331) A[1]:(0.272539019585) A[2]:(0.265206515789) A[3]:(0.233136624098)\n",
      " state (15)  A[0]:(0.225505724549) A[1]:(0.27549380064) A[2]:(0.267190396786) A[3]:(0.231810092926)\n",
      " state (0)  A[0]:(0.0146319428459)\n",
      " state (1)  A[0]:(0.0144745893776)\n",
      " state (2)  A[0]:(0.0146600333974)\n",
      " state (3)  A[0]:(0.0151023110375)\n",
      " state (4)  A[0]:(0.0161160044372)\n",
      " state (5)  A[0]:(0.0183248091489)\n",
      " state (6)  A[0]:(0.0229247733951)\n",
      " state (7)  A[0]:(0.0321721881628)\n",
      " state (8)  A[0]:(0.0499750077724)\n",
      " state (9)  A[0]:(0.081454500556)\n",
      " state (10)  A[0]:(0.129557311535)\n",
      " state (11)  A[0]:(0.190571978688)\n",
      " state (12)  A[0]:(0.255145847797)\n",
      " state (13)  A[0]:(0.31444388628)\n",
      " state (14)  A[0]:(0.363963991404)\n",
      " state (15)  A[0]:(0.403030395508)\n",
      "Episode 82000 finished after 6 . Running score: 0.02. Policy_loss: -95641.1860426, Value_loss: 1.07329465448. Times trained:               7648. Times reached goal: 15.               Steps done: 630062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250697374344) A[1]:(0.249927714467) A[2]:(0.249973237514) A[3]:(0.249401658773)\n",
      " state (1)  A[0]:(0.250244617462) A[1]:(0.249712616205) A[2]:(0.249973848462) A[3]:(0.250068932772)\n",
      " state (2)  A[0]:(0.251464664936) A[1]:(0.249822318554) A[2]:(0.249879688025) A[3]:(0.248833313584)\n",
      " state (3)  A[0]:(0.252358496189) A[1]:(0.250065505505) A[2]:(0.24987450242) A[3]:(0.24770142138)\n",
      " state (4)  A[0]:(0.252759605646) A[1]:(0.250549048185) A[2]:(0.250018596649) A[3]:(0.246672794223)\n",
      " state (5)  A[0]:(0.252537310123) A[1]:(0.25139605999) A[2]:(0.250378668308) A[3]:(0.245687961578)\n",
      " state (6)  A[0]:(0.251550346613) A[1]:(0.252756595612) A[2]:(0.251036733389) A[3]:(0.244656398892)\n",
      " state (7)  A[0]:(0.249601468444) A[1]:(0.254816919565) A[2]:(0.252095639706) A[3]:(0.243485942483)\n",
      " state (8)  A[0]:(0.246484994888) A[1]:(0.257761269808) A[2]:(0.253654539585) A[3]:(0.242099225521)\n",
      " state (9)  A[0]:(0.242118537426) A[1]:(0.261676698923) A[2]:(0.255750834942) A[3]:(0.240453973413)\n",
      " state (10)  A[0]:(0.236681625247) A[1]:(0.266447633505) A[2]:(0.25830167532) A[3]:(0.238569110632)\n",
      " state (11)  A[0]:(0.230635076761) A[1]:(0.271730095148) A[2]:(0.261098593473) A[3]:(0.236536219716)\n",
      " state (12)  A[0]:(0.224571615458) A[1]:(0.277054786682) A[2]:(0.263876825571) A[3]:(0.234496757388)\n",
      " state (13)  A[0]:(0.219003945589) A[1]:(0.281992077827) A[2]:(0.26641061902) A[3]:(0.232593372464)\n",
      " state (14)  A[0]:(0.214236989617) A[1]:(0.28626665473) A[2]:(0.268569141626) A[3]:(0.230927228928)\n",
      " state (15)  A[0]:(0.210364684463) A[1]:(0.289776235819) A[2]:(0.27031609416) A[3]:(0.229543045163)\n",
      " state (0)  A[0]:(0.019164249301)\n",
      " state (1)  A[0]:(0.0191403850913)\n",
      " state (2)  A[0]:(0.0194213073701)\n",
      " state (3)  A[0]:(0.0201460048556)\n",
      " state (4)  A[0]:(0.021919939667)\n",
      " state (5)  A[0]:(0.0260121375322)\n",
      " state (6)  A[0]:(0.0350102074444)\n",
      " state (7)  A[0]:(0.0538900494576)\n",
      " state (8)  A[0]:(0.0901057124138)\n",
      " state (9)  A[0]:(0.148743718863)\n",
      " state (10)  A[0]:(0.224378556013)\n",
      " state (11)  A[0]:(0.302701354027)\n",
      " state (12)  A[0]:(0.3715518713)\n",
      " state (13)  A[0]:(0.426239907742)\n",
      " state (14)  A[0]:(0.46734803915)\n",
      " state (15)  A[0]:(0.497418403625)\n",
      "Episode 83000 finished after 2 . Running score: 0.04. Policy_loss: -95640.5914556, Value_loss: 1.41605611424. Times trained:               7549. Times reached goal: 13.               Steps done: 637611.\n",
      " state (0)  A[0]:(0.249817743897) A[1]:(0.250428795815) A[2]:(0.249896794558) A[3]:(0.249856621027)\n",
      " state (1)  A[0]:(0.249607175589) A[1]:(0.250335097313) A[2]:(0.249883651733) A[3]:(0.250174075365)\n",
      " state (2)  A[0]:(0.250926494598) A[1]:(0.250708788633) A[2]:(0.2497574687) A[3]:(0.248607292771)\n",
      " state (3)  A[0]:(0.251489967108) A[1]:(0.251376986504) A[2]:(0.249788716435) A[3]:(0.247344389558)\n",
      " state (4)  A[0]:(0.251091003418) A[1]:(0.252564191818) A[2]:(0.250063031912) A[3]:(0.246281772852)\n",
      " state (5)  A[0]:(0.249524861574) A[1]:(0.254556685686) A[2]:(0.250674545765) A[3]:(0.245243906975)\n",
      " state (6)  A[0]:(0.246438711882) A[1]:(0.25772947073) A[2]:(0.25175127387) A[3]:(0.244080588222)\n",
      " state (7)  A[0]:(0.241488397121) A[1]:(0.262428790331) A[2]:(0.25340116024) A[3]:(0.242681652308)\n",
      " state (8)  A[0]:(0.234711810946) A[1]:(0.268693029881) A[2]:(0.255594909191) A[3]:(0.241000264883)\n",
      " state (9)  A[0]:(0.226781636477) A[1]:(0.276026576757) A[2]:(0.25809699297) A[3]:(0.239094778895)\n",
      " state (10)  A[0]:(0.218779563904) A[1]:(0.283532530069) A[2]:(0.260558098555) A[3]:(0.237129762769)\n",
      " state (11)  A[0]:(0.21165561676) A[1]:(0.290346592665) A[2]:(0.262695223093) A[3]:(0.235302582383)\n",
      " state (12)  A[0]:(0.205890357494) A[1]:(0.295969605446) A[2]:(0.264385968447) A[3]:(0.233754068613)\n",
      " state (13)  A[0]:(0.201527699828) A[1]:(0.300296634436) A[2]:(0.265641152859) A[3]:(0.232534468174)\n",
      " state (14)  A[0]:(0.198371425271) A[1]:(0.303469032049) A[2]:(0.266535669565) A[3]:(0.231623873115)\n",
      " state (15)  A[0]:(0.196153491735) A[1]:(0.305720686913) A[2]:(0.267157107592) A[3]:(0.230968669057)\n",
      " state (0)  A[0]:(0.0222641192377)\n",
      " state (1)  A[0]:(0.0223300848156)\n",
      " state (2)  A[0]:(0.0225900858641)\n",
      " state (3)  A[0]:(0.0232428833842)\n",
      " state (4)  A[0]:(0.0248041432351)\n",
      " state (5)  A[0]:(0.028326543048)\n",
      " state (6)  A[0]:(0.0358829759061)\n",
      " state (7)  A[0]:(0.0513612404466)\n",
      " state (8)  A[0]:(0.080760166049)\n",
      " state (9)  A[0]:(0.129244834185)\n",
      " state (10)  A[0]:(0.194663062692)\n",
      " state (11)  A[0]:(0.266271859407)\n",
      " state (12)  A[0]:(0.332497566938)\n",
      " state (13)  A[0]:(0.387239277363)\n",
      " state (14)  A[0]:(0.429616898298)\n",
      " state (15)  A[0]:(0.461296886206)\n",
      "Episode 84000 finished after 3 . Running score: 0.03. Policy_loss: -95640.0796873, Value_loss: 1.13816685501. Times trained:               7455. Times reached goal: 14.               Steps done: 645066.\n",
      "action_dist \n",
      "tensor([[ 0.2505,  0.2500,  0.2501,  0.2494]])\n",
      "On state=0, selected action=1\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2531,  0.2499,  0.2498,  0.2472]])\n",
      "On state=4, selected action=3\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250470668077) A[1]:(0.249975338578) A[2]:(0.250129431486) A[3]:(0.249424546957)\n",
      " state (1)  A[0]:(0.249929994345) A[1]:(0.249708995223) A[2]:(0.250195920467) A[3]:(0.250165134668)\n",
      " state (2)  A[0]:(0.251343756914) A[1]:(0.249724254012) A[2]:(0.249979510903) A[3]:(0.248952463269)\n",
      " state (3)  A[0]:(0.25243011117) A[1]:(0.249778866768) A[2]:(0.249827489257) A[3]:(0.247963562608)\n",
      " state (4)  A[0]:(0.253111481667) A[1]:(0.24991761148) A[2]:(0.249757930636) A[3]:(0.247212946415)\n",
      " state (5)  A[0]:(0.253461837769) A[1]:(0.250153690577) A[2]:(0.249762967229) A[3]:(0.246621474624)\n",
      " state (6)  A[0]:(0.253563225269) A[1]:(0.250491857529) A[2]:(0.24983382225) A[3]:(0.246111094952)\n",
      " state (7)  A[0]:(0.253445833921) A[1]:(0.250947296619) A[2]:(0.249972373247) A[3]:(0.245634540915)\n",
      " state (8)  A[0]:(0.253096789122) A[1]:(0.251547604799) A[2]:(0.250189989805) A[3]:(0.245165586472)\n",
      " state (9)  A[0]:(0.252477228642) A[1]:(0.252330005169) A[2]:(0.250504612923) A[3]:(0.244688183069)\n",
      " state (10)  A[0]:(0.251534998417) A[1]:(0.253336876631) A[2]:(0.250937223434) A[3]:(0.244190856814)\n",
      " state (11)  A[0]:(0.250217407942) A[1]:(0.254609316587) A[2]:(0.251508325338) A[3]:(0.243664994836)\n",
      " state (12)  A[0]:(0.248486191034) A[1]:(0.256177216768) A[2]:(0.252232044935) A[3]:(0.243104547262)\n",
      " state (13)  A[0]:(0.246334165335) A[1]:(0.258048117161) A[2]:(0.253110408783) A[3]:(0.242507368326)\n",
      " state (14)  A[0]:(0.243798032403) A[1]:(0.260197550058) A[2]:(0.254128307104) A[3]:(0.241876140237)\n",
      " state (15)  A[0]:(0.240961611271) A[1]:(0.262566149235) A[2]:(0.255252897739) A[3]:(0.241219356656)\n",
      " state (0)  A[0]:(0.0243757050484)\n",
      " state (1)  A[0]:(0.0244588237256)\n",
      " state (2)  A[0]:(0.0246946848929)\n",
      " state (3)  A[0]:(0.0252563096583)\n",
      " state (4)  A[0]:(0.0265376381576)\n",
      " state (5)  A[0]:(0.029301552102)\n",
      " state (6)  A[0]:(0.0349486917257)\n",
      " state (7)  A[0]:(0.0459184870124)\n",
      " state (8)  A[0]:(0.0658683404326)\n",
      " state (9)  A[0]:(0.0984080433846)\n",
      " state (10)  A[0]:(0.143739104271)\n",
      " state (11)  A[0]:(0.196578219533)\n",
      " state (12)  A[0]:(0.249083250761)\n",
      " state (13)  A[0]:(0.295408368111)\n",
      " state (14)  A[0]:(0.333226203918)\n",
      " state (15)  A[0]:(0.362695246935)\n",
      "Episode 85000 finished after 2 . Running score: 0.0. Policy_loss: -95640.1641043, Value_loss: 1.09775323863. Times trained:               7876. Times reached goal: 12.               Steps done: 652942.\n",
      " state (0)  A[0]:(0.250232487917) A[1]:(0.249836057425) A[2]:(0.249807313085) A[3]:(0.250124067068)\n",
      " state (1)  A[0]:(0.249677062035) A[1]:(0.249467611313) A[2]:(0.249731361866) A[3]:(0.251123964787)\n",
      " state (2)  A[0]:(0.251084297895) A[1]:(0.24968740344) A[2]:(0.249686360359) A[3]:(0.249541968107)\n",
      " state (3)  A[0]:(0.252028614283) A[1]:(0.249979838729) A[2]:(0.24969369173) A[3]:(0.248297810555)\n",
      " state (4)  A[0]:(0.25243139267) A[1]:(0.250432223082) A[2]:(0.2497921139) A[3]:(0.24734428525)\n",
      " state (5)  A[0]:(0.252347320318) A[1]:(0.251112759113) A[2]:(0.250004082918) A[3]:(0.246535897255)\n",
      " state (6)  A[0]:(0.251765936613) A[1]:(0.252107322216) A[2]:(0.250363111496) A[3]:(0.245763584971)\n",
      " state (7)  A[0]:(0.250580817461) A[1]:(0.253536581993) A[2]:(0.250921994448) A[3]:(0.244960665703)\n",
      " state (8)  A[0]:(0.24863755703) A[1]:(0.255538612604) A[2]:(0.251741856337) A[3]:(0.244081988931)\n",
      " state (9)  A[0]:(0.24581117928) A[1]:(0.258222877979) A[2]:(0.252868324518) A[3]:(0.243097603321)\n",
      " state (10)  A[0]:(0.242098808289) A[1]:(0.261603742838) A[2]:(0.254299193621) A[3]:(0.241998285055)\n",
      " state (11)  A[0]:(0.23768299818) A[1]:(0.265549480915) A[2]:(0.255963772535) A[3]:(0.240803673863)\n",
      " state (12)  A[0]:(0.232909783721) A[1]:(0.269792795181) A[2]:(0.257733792067) A[3]:(0.239563643932)\n",
      " state (13)  A[0]:(0.228181242943) A[1]:(0.274008631706) A[2]:(0.259464293718) A[3]:(0.238345816731)\n",
      " state (14)  A[0]:(0.223833665252) A[1]:(0.277912288904) A[2]:(0.261038154364) A[3]:(0.237215861678)\n",
      " state (15)  A[0]:(0.220069751143) A[1]:(0.281321197748) A[2]:(0.262388497591) A[3]:(0.236220538616)\n",
      " state (0)  A[0]:(0.0228511560708)\n",
      " state (1)  A[0]:(0.0228144191206)\n",
      " state (2)  A[0]:(0.0229881294072)\n",
      " state (3)  A[0]:(0.0233756415546)\n",
      " state (4)  A[0]:(0.0242126528174)\n",
      " state (5)  A[0]:(0.0259382817894)\n",
      " state (6)  A[0]:(0.0293257832527)\n",
      " state (7)  A[0]:(0.0356853380799)\n",
      " state (8)  A[0]:(0.0470863468945)\n",
      " state (9)  A[0]:(0.0662276148796)\n",
      " state (10)  A[0]:(0.0952564328909)\n",
      " state (11)  A[0]:(0.133695363998)\n",
      " state (12)  A[0]:(0.177641436458)\n",
      " state (13)  A[0]:(0.221693843603)\n",
      " state (14)  A[0]:(0.261601656675)\n",
      " state (15)  A[0]:(0.295313507318)\n",
      "Episode 86000 finished after 6 . Running score: 0.0. Policy_loss: -95640.3176532, Value_loss: 1.00940476632. Times trained:               7596. Times reached goal: 10.               Steps done: 660538.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250060766935) A[1]:(0.250026196241) A[2]:(0.249639585614) A[3]:(0.250273406506)\n",
      " state (1)  A[0]:(0.249242961407) A[1]:(0.249727249146) A[2]:(0.249571099877) A[3]:(0.251458734274)\n",
      " state (2)  A[0]:(0.250602334738) A[1]:(0.249962851405) A[2]:(0.249506354332) A[3]:(0.249928429723)\n",
      " state (3)  A[0]:(0.251526534557) A[1]:(0.250283569098) A[2]:(0.249494686723) A[3]:(0.248695194721)\n",
      " state (4)  A[0]:(0.25188139081) A[1]:(0.250784814358) A[2]:(0.249589279294) A[3]:(0.247744575143)\n",
      " state (5)  A[0]:(0.251640021801) A[1]:(0.25156506896) A[2]:(0.249830380082) A[3]:(0.24696457386)\n",
      " state (6)  A[0]:(0.250735163689) A[1]:(0.252747297287) A[2]:(0.250267475843) A[3]:(0.246250033379)\n",
      " state (7)  A[0]:(0.249008327723) A[1]:(0.25449231267) A[2]:(0.250970572233) A[3]:(0.245528832078)\n",
      " state (8)  A[0]:(0.246269091964) A[1]:(0.256966710091) A[2]:(0.252012342215) A[3]:(0.244751915336)\n",
      " state (9)  A[0]:(0.242420077324) A[1]:(0.260263860226) A[2]:(0.253427296877) A[3]:(0.243888825178)\n",
      " state (10)  A[0]:(0.237587690353) A[1]:(0.264310896397) A[2]:(0.255167722702) A[3]:(0.242933645844)\n",
      " state (11)  A[0]:(0.232155531645) A[1]:(0.268836468458) A[2]:(0.25709566474) A[3]:(0.241912305355)\n",
      " state (12)  A[0]:(0.226645901799) A[1]:(0.273447483778) A[2]:(0.259027719498) A[3]:(0.240878865123)\n",
      " state (13)  A[0]:(0.221531480551) A[1]:(0.277768135071) A[2]:(0.260802656412) A[3]:(0.239897683263)\n",
      " state (14)  A[0]:(0.217109486461) A[1]:(0.281545251608) A[2]:(0.262323588133) A[3]:(0.239021643996)\n",
      " state (15)  A[0]:(0.213486403227) A[1]:(0.284673452377) A[2]:(0.263560384512) A[3]:(0.238279789686)\n",
      " state (0)  A[0]:(0.0204647276551)\n",
      " state (1)  A[0]:(0.0201780684292)\n",
      " state (2)  A[0]:(0.0203148666769)\n",
      " state (3)  A[0]:(0.0206132214516)\n",
      " state (4)  A[0]:(0.0212460979819)\n",
      " state (5)  A[0]:(0.0225334912539)\n",
      " state (6)  A[0]:(0.0250361897051)\n",
      " state (7)  A[0]:(0.0297062825412)\n",
      " state (8)  A[0]:(0.0380936637521)\n",
      " state (9)  A[0]:(0.0524344630539)\n",
      " state (10)  A[0]:(0.0751190632582)\n",
      " state (11)  A[0]:(0.107163764536)\n",
      " state (12)  A[0]:(0.146694377065)\n",
      " state (13)  A[0]:(0.189360260963)\n",
      " state (14)  A[0]:(0.230528563261)\n",
      " state (15)  A[0]:(0.267082512379)\n",
      "Episode 87000 finished after 13 . Running score: 0.01. Policy_loss: -95640.1473565, Value_loss: 1.00625601915. Times trained:               7765. Times reached goal: 16.               Steps done: 668303.\n",
      " state (0)  A[0]:(0.250203222036) A[1]:(0.249938175082) A[2]:(0.249619975686) A[3]:(0.250238627195)\n",
      " state (1)  A[0]:(0.249438524246) A[1]:(0.249498978257) A[2]:(0.249653324485) A[3]:(0.251409202814)\n",
      " state (2)  A[0]:(0.250593572855) A[1]:(0.249702513218) A[2]:(0.249453350902) A[3]:(0.250250607729)\n",
      " state (3)  A[0]:(0.251350015402) A[1]:(0.249973267317) A[2]:(0.249320670962) A[3]:(0.249356105924)\n",
      " state (4)  A[0]:(0.251613467932) A[1]:(0.250388801098) A[2]:(0.249287173152) A[3]:(0.248710513115)\n",
      " state (5)  A[0]:(0.251417189837) A[1]:(0.251011788845) A[2]:(0.249354079366) A[3]:(0.248216956854)\n",
      " state (6)  A[0]:(0.250749081373) A[1]:(0.25192078948) A[2]:(0.249529182911) A[3]:(0.247800886631)\n",
      " state (7)  A[0]:(0.249504268169) A[1]:(0.253231883049) A[2]:(0.249840587378) A[3]:(0.247423201799)\n",
      " state (8)  A[0]:(0.247515887022) A[1]:(0.25509172678) A[2]:(0.250329762697) A[3]:(0.247062683105)\n",
      " state (9)  A[0]:(0.244617894292) A[1]:(0.257642865181) A[2]:(0.251035511494) A[3]:(0.246703773737)\n",
      " state (10)  A[0]:(0.240735709667) A[1]:(0.260961681604) A[2]:(0.251971036196) A[3]:(0.246331527829)\n",
      " state (11)  A[0]:(0.235972687602) A[1]:(0.26499196887) A[2]:(0.253103137016) A[3]:(0.245932251215)\n",
      " state (12)  A[0]:(0.230629593134) A[1]:(0.269520878792) A[2]:(0.254350870848) A[3]:(0.245498642325)\n",
      " state (13)  A[0]:(0.225125968456) A[1]:(0.274228334427) A[2]:(0.255609452724) A[3]:(0.245036244392)\n",
      " state (14)  A[0]:(0.219868347049) A[1]:(0.278783619404) A[2]:(0.256784707308) A[3]:(0.244563356042)\n",
      " state (15)  A[0]:(0.215148642659) A[1]:(0.282931149006) A[2]:(0.257815659046) A[3]:(0.244104549289)\n",
      " state (0)  A[0]:(0.0192213654518)\n",
      " state (1)  A[0]:(0.0184083487839)\n",
      " state (2)  A[0]:(0.0185340661556)\n",
      " state (3)  A[0]:(0.0188104044646)\n",
      " state (4)  A[0]:(0.0194009859115)\n",
      " state (5)  A[0]:(0.0206109452993)\n",
      " state (6)  A[0]:(0.0229800306261)\n",
      " state (7)  A[0]:(0.0274360980839)\n",
      " state (8)  A[0]:(0.0355134569108)\n",
      " state (9)  A[0]:(0.0494650751352)\n",
      " state (10)  A[0]:(0.071747392416)\n",
      " state (11)  A[0]:(0.103459179401)\n",
      " state (12)  A[0]:(0.142756015062)\n",
      " state (13)  A[0]:(0.185251533985)\n",
      " state (14)  A[0]:(0.226264983416)\n",
      " state (15)  A[0]:(0.262657284737)\n",
      "Episode 88000 finished after 12 . Running score: 0.02. Policy_loss: -95640.1162401, Value_loss: 1.01986660412. Times trained:               7424. Times reached goal: 15.               Steps done: 675727.\n",
      " state (0)  A[0]:(0.250170022249) A[1]:(0.24979954958) A[2]:(0.24997125566) A[3]:(0.250059127808)\n",
      " state (1)  A[0]:(0.249461814761) A[1]:(0.24938224256) A[2]:(0.249903053045) A[3]:(0.251252949238)\n",
      " state (2)  A[0]:(0.25045093894) A[1]:(0.249419972301) A[2]:(0.249851331115) A[3]:(0.250277757645)\n",
      " state (3)  A[0]:(0.251150161028) A[1]:(0.249518036842) A[2]:(0.249837741256) A[3]:(0.249494045973)\n",
      " state (4)  A[0]:(0.25146895647) A[1]:(0.249732136726) A[2]:(0.249885097146) A[3]:(0.248913764954)\n",
      " state (5)  A[0]:(0.25143635273) A[1]:(0.250090777874) A[2]:(0.250002622604) A[3]:(0.248470187187)\n",
      " state (6)  A[0]:(0.251084297895) A[1]:(0.25061994791) A[2]:(0.250200033188) A[3]:(0.248095706105)\n",
      " state (7)  A[0]:(0.250384509563) A[1]:(0.251367717981) A[2]:(0.250498592854) A[3]:(0.247749164701)\n",
      " state (8)  A[0]:(0.249252781272) A[1]:(0.25240829587) A[2]:(0.25093203783) A[3]:(0.247406855226)\n",
      " state (9)  A[0]:(0.247569829226) A[1]:(0.25383529067) A[2]:(0.251542598009) A[3]:(0.247052311897)\n",
      " state (10)  A[0]:(0.245211973786) A[1]:(0.255744874477) A[2]:(0.252371817827) A[3]:(0.246671319008)\n",
      " state (11)  A[0]:(0.242095813155) A[1]:(0.258206963539) A[2]:(0.253446340561) A[3]:(0.246250927448)\n",
      " state (12)  A[0]:(0.238228112459) A[1]:(0.261229783297) A[2]:(0.254761189222) A[3]:(0.245780915022)\n",
      " state (13)  A[0]:(0.233736768365) A[1]:(0.264735430479) A[2]:(0.256270647049) A[3]:(0.245257154107)\n",
      " state (14)  A[0]:(0.228858113289) A[1]:(0.268563389778) A[2]:(0.257893502712) A[3]:(0.244685038924)\n",
      " state (15)  A[0]:(0.223881423473) A[1]:(0.272505164146) A[2]:(0.259533047676) A[3]:(0.244080349803)\n",
      " state (0)  A[0]:(0.0174311585724)\n",
      " state (1)  A[0]:(0.0163678601384)\n",
      " state (2)  A[0]:(0.0164714809507)\n",
      " state (3)  A[0]:(0.0166965648532)\n",
      " state (4)  A[0]:(0.0171732343733)\n",
      " state (5)  A[0]:(0.0181437116116)\n",
      " state (6)  A[0]:(0.0200364496559)\n",
      " state (7)  A[0]:(0.0235906708986)\n",
      " state (8)  A[0]:(0.0300512425601)\n",
      " state (9)  A[0]:(0.041347309947)\n",
      " state (10)  A[0]:(0.0598800554872)\n",
      " state (11)  A[0]:(0.0873956605792)\n",
      " state (12)  A[0]:(0.123310022056)\n",
      " state (13)  A[0]:(0.164245963097)\n",
      " state (14)  A[0]:(0.205630525947)\n",
      " state (15)  A[0]:(0.243751361966)\n",
      "Episode 89000 finished after 6 . Running score: 0.02. Policy_loss: -95640.0585726, Value_loss: 1.03585310072. Times trained:               7935. Times reached goal: 12.               Steps done: 683662.\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2501,  0.2497,  0.2498]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2521,  0.2505,  0.2495,  0.2478]])\n",
      "On state=4, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2504,  0.2501,  0.2497,  0.2498]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2496,  0.2499,  0.2497,  0.2509]])\n",
      "On state=1, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.250369340181) A[1]:(0.250143975019) A[2]:(0.249730229378) A[3]:(0.249756455421)\n",
      " state (1)  A[0]:(0.249588012695) A[1]:(0.249851077795) A[2]:(0.249686941504) A[3]:(0.250873953104)\n",
      " state (2)  A[0]:(0.250766038895) A[1]:(0.249966964126) A[2]:(0.249582633376) A[3]:(0.249684408307)\n",
      " state (3)  A[0]:(0.251666873693) A[1]:(0.250159293413) A[2]:(0.249509990215) A[3]:(0.248663887382)\n",
      " state (4)  A[0]:(0.25215536356) A[1]:(0.25050085783) A[2]:(0.249502211809) A[3]:(0.247841492295)\n",
      " state (5)  A[0]:(0.252148151398) A[1]:(0.25107306242) A[2]:(0.249589264393) A[3]:(0.247189536691)\n",
      " state (6)  A[0]:(0.251583248377) A[1]:(0.251966297626) A[2]:(0.249799028039) A[3]:(0.246651440859)\n",
      " state (7)  A[0]:(0.250355601311) A[1]:(0.25329875946) A[2]:(0.250168323517) A[3]:(0.246177330613)\n",
      " state (8)  A[0]:(0.248305618763) A[1]:(0.255217880011) A[2]:(0.250744044781) A[3]:(0.245732426643)\n",
      " state (9)  A[0]:(0.245267525315) A[1]:(0.257870405912) A[2]:(0.251570284367) A[3]:(0.245291784406)\n",
      " state (10)  A[0]:(0.241161674261) A[1]:(0.261340200901) A[2]:(0.252663522959) A[3]:(0.244834601879)\n",
      " state (11)  A[0]:(0.23608982563) A[1]:(0.26557764411) A[2]:(0.253988802433) A[3]:(0.244343683124)\n",
      " state (12)  A[0]:(0.23036518693) A[1]:(0.270368397236) A[2]:(0.255455672741) A[3]:(0.243810757995)\n",
      " state (13)  A[0]:(0.224437057972) A[1]:(0.275377035141) A[2]:(0.256943047047) A[3]:(0.243242844939)\n",
      " state (14)  A[0]:(0.218752130866) A[1]:(0.280246645212) A[2]:(0.258338481188) A[3]:(0.242662757635)\n",
      " state (15)  A[0]:(0.213638991117) A[1]:(0.284693449736) A[2]:(0.259566664696) A[3]:(0.242100909352)\n",
      " state (0)  A[0]:(0.017282133922)\n",
      " state (1)  A[0]:(0.0152706187218)\n",
      " state (2)  A[0]:(0.0153854247183)\n",
      " state (3)  A[0]:(0.0156484693289)\n",
      " state (4)  A[0]:(0.0162321291864)\n",
      " state (5)  A[0]:(0.0174687150866)\n",
      " state (6)  A[0]:(0.0199697725475)\n",
      " state (7)  A[0]:(0.0248394198716)\n",
      " state (8)  A[0]:(0.0339883342385)\n",
      " state (9)  A[0]:(0.0502571240067)\n",
      " state (10)  A[0]:(0.0765165686607)\n",
      " state (11)  A[0]:(0.113363459706)\n",
      " state (12)  A[0]:(0.15748809278)\n",
      " state (13)  A[0]:(0.203186631203)\n",
      " state (14)  A[0]:(0.245456799865)\n",
      " state (15)  A[0]:(0.281607031822)\n",
      "Episode 90000 finished after 4 . Running score: 0.01. Policy_loss: -95639.9704922, Value_loss: 1.02357586364. Times trained:               7561. Times reached goal: 16.               Steps done: 691223.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.249915525317) A[1]:(0.249893262982) A[2]:(0.250169754028) A[3]:(0.250021457672)\n",
      " state (1)  A[0]:(0.248505309224) A[1]:(0.249560296535) A[2]:(0.250268816948) A[3]:(0.251665532589)\n",
      " state (2)  A[0]:(0.249367177486) A[1]:(0.24978120625) A[2]:(0.250209540129) A[3]:(0.250642031431)\n",
      " state (3)  A[0]:(0.250039845705) A[1]:(0.250055760145) A[2]:(0.250184953213) A[3]:(0.249719440937)\n",
      " state (4)  A[0]:(0.250502645969) A[1]:(0.250411659479) A[2]:(0.250204086304) A[3]:(0.248881533742)\n",
      " state (5)  A[0]:(0.250707656145) A[1]:(0.250892698765) A[2]:(0.250283330679) A[3]:(0.24811629951)\n",
      " state (6)  A[0]:(0.250582188368) A[1]:(0.251561611891) A[2]:(0.250446587801) A[3]:(0.247409567237)\n",
      " state (7)  A[0]:(0.25002348423) A[1]:(0.252503156662) A[2]:(0.250726759434) A[3]:(0.246746554971)\n",
      " state (8)  A[0]:(0.248900681734) A[1]:(0.253821372986) A[2]:(0.25116443634) A[3]:(0.246113449335)\n",
      " state (9)  A[0]:(0.247075080872) A[1]:(0.255625486374) A[2]:(0.251801252365) A[3]:(0.245498165488)\n",
      " state (10)  A[0]:(0.244444578886) A[1]:(0.257999151945) A[2]:(0.252665996552) A[3]:(0.244890257716)\n",
      " state (11)  A[0]:(0.241003125906) A[1]:(0.260958373547) A[2]:(0.253757059574) A[3]:(0.244281411171)\n",
      " state (12)  A[0]:(0.236884132028) A[1]:(0.26441809535) A[2]:(0.255030274391) A[3]:(0.24366748333)\n",
      " state (13)  A[0]:(0.232351601124) A[1]:(0.268193185329) A[2]:(0.25640386343) A[3]:(0.24305139482)\n",
      " state (14)  A[0]:(0.227733880281) A[1]:(0.272041499615) A[2]:(0.257779866457) A[3]:(0.242444708943)\n",
      " state (15)  A[0]:(0.223335966468) A[1]:(0.275727629662) A[2]:(0.259071290493) A[3]:(0.241865158081)\n",
      " state (0)  A[0]:(0.0154928676784)\n",
      " state (1)  A[0]:(0.0140144480392)\n",
      " state (2)  A[0]:(0.0140997515991)\n",
      " state (3)  A[0]:(0.0142839495093)\n",
      " state (4)  A[0]:(0.0146721610799)\n",
      " state (5)  A[0]:(0.0154595542699)\n",
      " state (6)  A[0]:(0.0169900879264)\n",
      " state (7)  A[0]:(0.0198546927422)\n",
      " state (8)  A[0]:(0.0250481143594)\n",
      " state (9)  A[0]:(0.0341317877173)\n",
      " state (10)  A[0]:(0.0491347797215)\n",
      " state (11)  A[0]:(0.07175270468)\n",
      " state (12)  A[0]:(0.101959779859)\n",
      " state (13)  A[0]:(0.137327536941)\n",
      " state (14)  A[0]:(0.174043595791)\n",
      " state (15)  A[0]:(0.208663702011)\n",
      "Episode 91000 finished after 15 . Running score: 0.0. Policy_loss: -95639.5308303, Value_loss: 1.00579124715. Times trained:               7798. Times reached goal: 10.               Steps done: 699021.\n",
      " state (0)  A[0]:(0.250267922878) A[1]:(0.249798119068) A[2]:(0.249916002154) A[3]:(0.250018000603)\n",
      " state (1)  A[0]:(0.249462768435) A[1]:(0.249418109655) A[2]:(0.249837040901) A[3]:(0.251282125711)\n",
      " state (2)  A[0]:(0.250038325787) A[1]:(0.24939891696) A[2]:(0.249800622463) A[3]:(0.250762164593)\n",
      " state (3)  A[0]:(0.250524193048) A[1]:(0.249393984675) A[2]:(0.249773383141) A[3]:(0.250308483839)\n",
      " state (4)  A[0]:(0.250859230757) A[1]:(0.249421268702) A[2]:(0.249763429165) A[3]:(0.249956026673)\n",
      " state (5)  A[0]:(0.251024663448) A[1]:(0.249494105577) A[2]:(0.24977594614) A[3]:(0.249705329537)\n",
      " state (6)  A[0]:(0.25104829669) A[1]:(0.249614492059) A[2]:(0.249810576439) A[3]:(0.249526634812)\n",
      " state (7)  A[0]:(0.250961452723) A[1]:(0.249782249331) A[2]:(0.24986641109) A[3]:(0.249389886856)\n",
      " state (8)  A[0]:(0.250778824091) A[1]:(0.250000834465) A[2]:(0.249944597483) A[3]:(0.249275743961)\n",
      " state (9)  A[0]:(0.250498592854) A[1]:(0.250278621912) A[2]:(0.250048756599) A[3]:(0.249173998833)\n",
      " state (10)  A[0]:(0.250107437372) A[1]:(0.250627934933) A[2]:(0.25018453598) A[3]:(0.24908015132)\n",
      " state (11)  A[0]:(0.249585166574) A[1]:(0.251063615084) A[2]:(0.250358730555) A[3]:(0.248992517591)\n",
      " state (12)  A[0]:(0.248909279704) A[1]:(0.251601219177) A[2]:(0.25057849288) A[3]:(0.248911038041)\n",
      " state (13)  A[0]:(0.248059660196) A[1]:(0.252254217863) A[2]:(0.250849992037) A[3]:(0.248836159706)\n",
      " state (14)  A[0]:(0.247023776174) A[1]:(0.253030955791) A[2]:(0.251176863909) A[3]:(0.248768404126)\n",
      " state (15)  A[0]:(0.245801344514) A[1]:(0.25393179059) A[2]:(0.251558899879) A[3]:(0.248707950115)\n",
      " state (0)  A[0]:(0.013843874447)\n",
      " state (1)  A[0]:(0.0124166272581)\n",
      " state (2)  A[0]:(0.0125022688881)\n",
      " state (3)  A[0]:(0.0126955900341)\n",
      " state (4)  A[0]:(0.0131192579865)\n",
      " state (5)  A[0]:(0.0140077406541)\n",
      " state (6)  A[0]:(0.0157878827304)\n",
      " state (7)  A[0]:(0.0192224588245)\n",
      " state (8)  A[0]:(0.0256381630898)\n",
      " state (9)  A[0]:(0.0371046476066)\n",
      " state (10)  A[0]:(0.0560580976307)\n",
      " state (11)  A[0]:(0.0838421806693)\n",
      " state (12)  A[0]:(0.119033232331)\n",
      " state (13)  A[0]:(0.157634347677)\n",
      " state (14)  A[0]:(0.195201978087)\n",
      " state (15)  A[0]:(0.228683516383)\n",
      "Episode 92000 finished after 6 . Running score: 0.02. Policy_loss: -95639.5896526, Value_loss: 1.02167458167. Times trained:               7632. Times reached goal: 19.               Steps done: 706653.\n",
      " state (0)  A[0]:(0.250343829393) A[1]:(0.249372184277) A[2]:(0.250075340271) A[3]:(0.250208675861)\n",
      " state (1)  A[0]:(0.249944016337) A[1]:(0.24907258153) A[2]:(0.249957561493) A[3]:(0.251025795937)\n",
      " state (2)  A[0]:(0.250725537539) A[1]:(0.248957663774) A[2]:(0.249970555305) A[3]:(0.250346183777)\n",
      " state (3)  A[0]:(0.251296132803) A[1]:(0.248889550567) A[2]:(0.250008881092) A[3]:(0.249805465341)\n",
      " state (4)  A[0]:(0.251520961523) A[1]:(0.24893720448) A[2]:(0.250096619129) A[3]:(0.249445140362)\n",
      " state (5)  A[0]:(0.251369774342) A[1]:(0.249133050442) A[2]:(0.250254005194) A[3]:(0.249243155122)\n",
      " state (6)  A[0]:(0.25084155798) A[1]:(0.24949246645) A[2]:(0.250502258539) A[3]:(0.24916370213)\n",
      " state (7)  A[0]:(0.249886661768) A[1]:(0.250047564507) A[2]:(0.250873744488) A[3]:(0.249192014337)\n",
      " state (8)  A[0]:(0.248393893242) A[1]:(0.250856012106) A[2]:(0.251415044069) A[3]:(0.249334990978)\n",
      " state (9)  A[0]:(0.24621142447) A[1]:(0.251993894577) A[2]:(0.252182722092) A[3]:(0.249611973763)\n",
      " state (10)  A[0]:(0.243190392852) A[1]:(0.253536701202) A[2]:(0.253230243921) A[3]:(0.250042676926)\n",
      " state (11)  A[0]:(0.239250943065) A[1]:(0.255528867245) A[2]:(0.254586517811) A[3]:(0.250633597374)\n",
      " state (12)  A[0]:(0.234448775649) A[1]:(0.25795134902) A[2]:(0.256233662367) A[3]:(0.251366138458)\n",
      " state (13)  A[0]:(0.229001447558) A[1]:(0.260706633329) A[2]:(0.25809815526) A[3]:(0.252193808556)\n",
      " state (14)  A[0]:(0.223246261477) A[1]:(0.263635367155) A[2]:(0.260065346956) A[3]:(0.253053039312)\n",
      " state (15)  A[0]:(0.217548713088) A[1]:(0.266558498144) A[2]:(0.26201120019) A[3]:(0.253881633282)\n",
      " state (0)  A[0]:(0.0125976707786)\n",
      " state (1)  A[0]:(0.011737793684)\n",
      " state (2)  A[0]:(0.0118257692084)\n",
      " state (3)  A[0]:(0.0120318029076)\n",
      " state (4)  A[0]:(0.0124981980771)\n",
      " state (5)  A[0]:(0.0135044883937)\n",
      " state (6)  A[0]:(0.0155766513199)\n",
      " state (7)  A[0]:(0.0196939203888)\n",
      " state (8)  A[0]:(0.0276204124093)\n",
      " state (9)  A[0]:(0.0421104170382)\n",
      " state (10)  A[0]:(0.0661408901215)\n",
      " state (11)  A[0]:(0.100621409714)\n",
      " state (12)  A[0]:(0.142560988665)\n",
      " state (13)  A[0]:(0.186407372355)\n",
      " state (14)  A[0]:(0.227165773511)\n",
      " state (15)  A[0]:(0.262097001076)\n",
      "Episode 93000 finished after 10 . Running score: 0.02. Policy_loss: -95638.9877055, Value_loss: 1.03337837476. Times trained:               7963. Times reached goal: 18.               Steps done: 714616.\n",
      " state (0)  A[0]:(0.249446064234) A[1]:(0.250274628401) A[2]:(0.250243663788) A[3]:(0.25003567338)\n",
      " state (1)  A[0]:(0.248566105962) A[1]:(0.250360816717) A[2]:(0.250105142593) A[3]:(0.250967890024)\n",
      " state (2)  A[0]:(0.249334082007) A[1]:(0.250247865915) A[2]:(0.250261753798) A[3]:(0.250156253576)\n",
      " state (3)  A[0]:(0.24979044497) A[1]:(0.250248968601) A[2]:(0.250478327274) A[3]:(0.249482274055)\n",
      " state (4)  A[0]:(0.249849766493) A[1]:(0.250408351421) A[2]:(0.250798732042) A[3]:(0.248943179846)\n",
      " state (5)  A[0]:(0.249352455139) A[1]:(0.250803560019) A[2]:(0.251296728849) A[3]:(0.248547315598)\n",
      " state (6)  A[0]:(0.24805688858) A[1]:(0.251549780369) A[2]:(0.252082794905) A[3]:(0.248310551047)\n",
      " state (7)  A[0]:(0.24564896524) A[1]:(0.252795130014) A[2]:(0.253300398588) A[3]:(0.248255565763)\n",
      " state (8)  A[0]:(0.24182420969) A[1]:(0.254684567451) A[2]:(0.255091398954) A[3]:(0.248399883509)\n",
      " state (9)  A[0]:(0.236465871334) A[1]:(0.257280349731) A[2]:(0.257520496845) A[3]:(0.248733326793)\n",
      " state (10)  A[0]:(0.229829400778) A[1]:(0.260476648808) A[2]:(0.260494023561) A[3]:(0.249199926853)\n",
      " state (11)  A[0]:(0.222547769547) A[1]:(0.263990402222) A[2]:(0.26375195384) A[3]:(0.249709844589)\n",
      " state (12)  A[0]:(0.215393364429) A[1]:(0.267463892698) A[2]:(0.266964554787) A[3]:(0.250178217888)\n",
      " state (13)  A[0]:(0.208981618285) A[1]:(0.270601302385) A[2]:(0.269860237837) A[3]:(0.250556826591)\n",
      " state (14)  A[0]:(0.203628346324) A[1]:(0.273241817951) A[2]:(0.272292971611) A[3]:(0.250836908817)\n",
      " state (15)  A[0]:(0.199382469058) A[1]:(0.275351375341) A[2]:(0.274233609438) A[3]:(0.251032590866)\n",
      " state (0)  A[0]:(0.0137075092643)\n",
      " state (1)  A[0]:(0.0130960829556)\n",
      " state (2)  A[0]:(0.0132251596078)\n",
      " state (3)  A[0]:(0.0135535765439)\n",
      " state (4)  A[0]:(0.0143505800515)\n",
      " state (5)  A[0]:(0.0161743145436)\n",
      " state (6)  A[0]:(0.020145688206)\n",
      " state (7)  A[0]:(0.0284693371505)\n",
      " state (8)  A[0]:(0.0449739173055)\n",
      " state (9)  A[0]:(0.0742019787431)\n",
      " state (10)  A[0]:(0.117491394281)\n",
      " state (11)  A[0]:(0.169731125236)\n",
      " state (12)  A[0]:(0.222364902496)\n",
      " state (13)  A[0]:(0.268885076046)\n",
      " state (14)  A[0]:(0.306705892086)\n",
      " state (15)  A[0]:(0.335976272821)\n",
      "Episode 94000 finished after 14 . Running score: 0.03. Policy_loss: -95638.2599349, Value_loss: 1.18543534143. Times trained:               7705. Times reached goal: 14.               Steps done: 722321.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.2495,  0.2504,  0.2499,  0.2501]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2495,  0.2504,  0.2499,  0.2501]])\n",
      "On state=0, selected action=1\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2514,  0.2502,  0.2507,  0.2478]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2496,  0.2504,  0.2499,  0.2501]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2496,  0.2504,  0.2499,  0.2501]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2496,  0.2504,  0.2499,  0.2501]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2496,  0.2504,  0.2499,  0.2501]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2514,  0.2501,  0.2507,  0.2478]])\n",
      "On state=4, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.249565154314) A[1]:(0.250420153141) A[2]:(0.249935179949) A[3]:(0.250079572201)\n",
      " state (1)  A[0]:(0.248773023486) A[1]:(0.250534534454) A[2]:(0.249684646726) A[3]:(0.251007825136)\n",
      " state (2)  A[0]:(0.250102341175) A[1]:(0.250234186649) A[2]:(0.249949201941) A[3]:(0.249714285135)\n",
      " state (3)  A[0]:(0.25101557374) A[1]:(0.250083655119) A[2]:(0.250251173973) A[3]:(0.248649612069)\n",
      " state (4)  A[0]:(0.251386195421) A[1]:(0.250138610601) A[2]:(0.250652819872) A[3]:(0.247822344303)\n",
      " state (5)  A[0]:(0.251017957926) A[1]:(0.250484585762) A[2]:(0.251255452633) A[3]:(0.247242003679)\n",
      " state (6)  A[0]:(0.249644473195) A[1]:(0.251236945391) A[2]:(0.252201318741) A[3]:(0.246917307377)\n",
      " state (7)  A[0]:(0.24694493413) A[1]:(0.252533793449) A[2]:(0.253662198782) A[3]:(0.246859103441)\n",
      " state (8)  A[0]:(0.24264344573) A[1]:(0.254495173693) A[2]:(0.255791425705) A[3]:(0.247070014477)\n",
      " state (9)  A[0]:(0.236713916063) A[1]:(0.257137954235) A[2]:(0.258630126715) A[3]:(0.247518017888)\n",
      " state (10)  A[0]:(0.22955904901) A[1]:(0.260299384594) A[2]:(0.2620228827) A[3]:(0.248118683696)\n",
      " state (11)  A[0]:(0.221948429942) A[1]:(0.263658374548) A[2]:(0.265637218952) A[3]:(0.24875600636)\n",
      " state (12)  A[0]:(0.214707687497) A[1]:(0.26686283946) A[2]:(0.269098520279) A[3]:(0.249331012368)\n",
      " state (13)  A[0]:(0.208415478468) A[1]:(0.2696595788) A[2]:(0.272131472826) A[3]:(0.249793455005)\n",
      " state (14)  A[0]:(0.2033085078) A[1]:(0.271939963102) A[2]:(0.274613559246) A[3]:(0.250137925148)\n",
      " state (15)  A[0]:(0.199360713363) A[1]:(0.273710072041) A[2]:(0.276546388865) A[3]:(0.250382810831)\n",
      " state (0)  A[0]:(0.0146069545299)\n",
      " state (1)  A[0]:(0.0143394069746)\n",
      " state (2)  A[0]:(0.0144700426608)\n",
      " state (3)  A[0]:(0.0147995585576)\n",
      " state (4)  A[0]:(0.0155944442376)\n",
      " state (5)  A[0]:(0.0174058955163)\n",
      " state (6)  A[0]:(0.0213378723711)\n",
      " state (7)  A[0]:(0.0295734796673)\n",
      " state (8)  A[0]:(0.04603035748)\n",
      " state (9)  A[0]:(0.0758117139339)\n",
      " state (10)  A[0]:(0.121414676309)\n",
      " state (11)  A[0]:(0.178445249796)\n",
      " state (12)  A[0]:(0.237646520138)\n",
      " state (13)  A[0]:(0.291103154421)\n",
      " state (14)  A[0]:(0.335191905499)\n",
      " state (15)  A[0]:(0.369648844004)\n",
      "Episode 95000 finished after 8 . Running score: 0.03. Policy_loss: -95638.5705226, Value_loss: 1.05520066239. Times trained:               7625. Times reached goal: 20.               Steps done: 729946.\n",
      " state (0)  A[0]:(0.250244408846) A[1]:(0.250071823597) A[2]:(0.250000059605) A[3]:(0.249683663249)\n",
      " state (1)  A[0]:(0.249553129077) A[1]:(0.250047296286) A[2]:(0.249761343002) A[3]:(0.250638216734)\n",
      " state (2)  A[0]:(0.25101056695) A[1]:(0.249825611711) A[2]:(0.249946281314) A[3]:(0.249217510223)\n",
      " state (3)  A[0]:(0.252187073231) A[1]:(0.249680876732) A[2]:(0.250144630671) A[3]:(0.247987478971)\n",
      " state (4)  A[0]:(0.252972245216) A[1]:(0.249657392502) A[2]:(0.250388920307) A[3]:(0.246981441975)\n",
      " state (5)  A[0]:(0.253254801035) A[1]:(0.249807581306) A[2]:(0.250731259584) A[3]:(0.24620641768)\n",
      " state (6)  A[0]:(0.252936005592) A[1]:(0.250185549259) A[2]:(0.251240879297) A[3]:(0.245637536049)\n",
      " state (7)  A[0]:(0.251890689135) A[1]:(0.250857532024) A[2]:(0.252005726099) A[3]:(0.24524602294)\n",
      " state (8)  A[0]:(0.249951303005) A[1]:(0.251904815435) A[2]:(0.253127843142) A[3]:(0.245015993714)\n",
      " state (9)  A[0]:(0.246956780553) A[1]:(0.25340282917) A[2]:(0.254699736834) A[3]:(0.244940683246)\n",
      " state (10)  A[0]:(0.242857530713) A[1]:(0.255376815796) A[2]:(0.256758660078) A[3]:(0.245006993413)\n",
      " state (11)  A[0]:(0.237816482782) A[1]:(0.257758915424) A[2]:(0.259242027998) A[3]:(0.245182588696)\n",
      " state (12)  A[0]:(0.232218116522) A[1]:(0.260382652283) A[2]:(0.26198169589) A[3]:(0.245417505503)\n",
      " state (13)  A[0]:(0.226555496454) A[1]:(0.263031005859) A[2]:(0.264753401279) A[3]:(0.245660111308)\n",
      " state (14)  A[0]:(0.221269831061) A[1]:(0.265506327152) A[2]:(0.267350316048) A[3]:(0.245873525739)\n",
      " state (15)  A[0]:(0.216645419598) A[1]:(0.267678618431) A[2]:(0.269634485245) A[3]:(0.246041446924)\n",
      " state (0)  A[0]:(0.0146471653134)\n",
      " state (1)  A[0]:(0.014226231724)\n",
      " state (2)  A[0]:(0.0143553167582)\n",
      " state (3)  A[0]:(0.0146772675216)\n",
      " state (4)  A[0]:(0.0154452454299)\n",
      " state (5)  A[0]:(0.017173755914)\n",
      " state (6)  A[0]:(0.0208661109209)\n",
      " state (7)  A[0]:(0.0284301582724)\n",
      " state (8)  A[0]:(0.0431182235479)\n",
      " state (9)  A[0]:(0.0689049437642)\n",
      " state (10)  A[0]:(0.107459694147)\n",
      " state (11)  A[0]:(0.155089169741)\n",
      " state (12)  A[0]:(0.204457074404)\n",
      " state (13)  A[0]:(0.249253854156)\n",
      " state (14)  A[0]:(0.286468446255)\n",
      " state (15)  A[0]:(0.315760761499)\n",
      "Episode 96000 finished after 5 . Running score: 0.02. Policy_loss: -95638.0663425, Value_loss: 1.06735147725. Times trained:               7408. Times reached goal: 10.               Steps done: 737354.\n",
      " state (0)  A[0]:(0.249962374568) A[1]:(0.250214964151) A[2]:(0.24980327487) A[3]:(0.250019341707)\n",
      " state (1)  A[0]:(0.249057367444) A[1]:(0.250164210796) A[2]:(0.249586760998) A[3]:(0.251191616058)\n",
      " state (2)  A[0]:(0.250627636909) A[1]:(0.250130593777) A[2]:(0.24976272881) A[3]:(0.249479070306)\n",
      " state (3)  A[0]:(0.251836597919) A[1]:(0.250188916922) A[2]:(0.249954745173) A[3]:(0.248019725084)\n",
      " state (4)  A[0]:(0.252563506365) A[1]:(0.250398546457) A[2]:(0.250209778547) A[3]:(0.246828198433)\n",
      " state (5)  A[0]:(0.252650350332) A[1]:(0.250845909119) A[2]:(0.250603616238) A[3]:(0.245900139213)\n",
      " state (6)  A[0]:(0.251901566982) A[1]:(0.251643270254) A[2]:(0.25123783946) A[3]:(0.245217293501)\n",
      " state (7)  A[0]:(0.250083208084) A[1]:(0.252922564745) A[2]:(0.252230733633) A[3]:(0.244763463736)\n",
      " state (8)  A[0]:(0.246987685561) A[1]:(0.254800915718) A[2]:(0.253685712814) A[3]:(0.244525715709)\n",
      " state (9)  A[0]:(0.242583855987) A[1]:(0.257306814194) A[2]:(0.255630552769) A[3]:(0.244478806853)\n",
      " state (10)  A[0]:(0.237156271935) A[1]:(0.260310292244) A[2]:(0.257962286472) A[3]:(0.244571134448)\n",
      " state (11)  A[0]:(0.231276497245) A[1]:(0.263532161713) A[2]:(0.260458409786) A[3]:(0.24473297596)\n",
      " state (12)  A[0]:(0.225586473942) A[1]:(0.266648083925) A[2]:(0.262863397598) A[3]:(0.244902059436)\n",
      " state (13)  A[0]:(0.220565736294) A[1]:(0.269407838583) A[2]:(0.26498401165) A[3]:(0.245042443275)\n",
      " state (14)  A[0]:(0.216438218951) A[1]:(0.271688818932) A[2]:(0.266729027033) A[3]:(0.24514387548)\n",
      " state (15)  A[0]:(0.213215515018) A[1]:(0.273479789495) A[2]:(0.268093824387) A[3]:(0.245210826397)\n",
      " state (0)  A[0]:(0.0153532382101)\n",
      " state (1)  A[0]:(0.0142796933651)\n",
      " state (2)  A[0]:(0.0144162913784)\n",
      " state (3)  A[0]:(0.0147585943341)\n",
      " state (4)  A[0]:(0.0155772771686)\n",
      " state (5)  A[0]:(0.0174200348556)\n",
      " state (6)  A[0]:(0.0213445238769)\n",
      " state (7)  A[0]:(0.0293138287961)\n",
      " state (8)  A[0]:(0.0444885455072)\n",
      " state (9)  A[0]:(0.0702579841018)\n",
      " state (10)  A[0]:(0.10719370842)\n",
      " state (11)  A[0]:(0.150973245502)\n",
      " state (12)  A[0]:(0.194868788123)\n",
      " state (13)  A[0]:(0.233778476715)\n",
      " state (14)  A[0]:(0.265600860119)\n",
      " state (15)  A[0]:(0.29038259387)\n",
      "Episode 97000 finished after 21 . Running score: 0.01. Policy_loss: -95638.0912132, Value_loss: 1.01468231141. Times trained:               7365. Times reached goal: 10.               Steps done: 744719.\n",
      " state (0)  A[0]:(0.249767795205) A[1]:(0.250076770782) A[2]:(0.249827250838) A[3]:(0.250328183174)\n",
      " state (1)  A[0]:(0.248805493116) A[1]:(0.249914258718) A[2]:(0.249707520008) A[3]:(0.251572698355)\n",
      " state (2)  A[0]:(0.250184297562) A[1]:(0.250005692244) A[2]:(0.249735698104) A[3]:(0.250074386597)\n",
      " state (3)  A[0]:(0.251246005297) A[1]:(0.25014975667) A[2]:(0.249783873558) A[3]:(0.248820319772)\n",
      " state (4)  A[0]:(0.251894146204) A[1]:(0.250395447016) A[2]:(0.249883964658) A[3]:(0.247826397419)\n",
      " state (5)  A[0]:(0.252005010843) A[1]:(0.250816583633) A[2]:(0.250085443258) A[3]:(0.247092962265)\n",
      " state (6)  A[0]:(0.251427799463) A[1]:(0.25150975585) A[2]:(0.250453323126) A[3]:(0.246609121561)\n",
      " state (7)  A[0]:(0.249971628189) A[1]:(0.252592593431) A[2]:(0.251065939665) A[3]:(0.246369898319)\n",
      " state (8)  A[0]:(0.247435152531) A[1]:(0.254184663296) A[2]:(0.252001106739) A[3]:(0.246379077435)\n",
      " state (9)  A[0]:(0.243711650372) A[1]:(0.256355255842) A[2]:(0.25330132246) A[3]:(0.246631786227)\n",
      " state (10)  A[0]:(0.238925278187) A[1]:(0.259055018425) A[2]:(0.254929751158) A[3]:(0.24708995223)\n",
      " state (11)  A[0]:(0.233478203416) A[1]:(0.262089133263) A[2]:(0.256756931543) A[3]:(0.247675687075)\n",
      " state (12)  A[0]:(0.227930516005) A[1]:(0.265173584223) A[2]:(0.258601784706) A[3]:(0.248294070363)\n",
      " state (13)  A[0]:(0.222792848945) A[1]:(0.268040031195) A[2]:(0.260300338268) A[3]:(0.248866796494)\n",
      " state (14)  A[0]:(0.218383491039) A[1]:(0.270514041185) A[2]:(0.261752039194) A[3]:(0.249350443482)\n",
      " state (15)  A[0]:(0.214810937643) A[1]:(0.27253061533) A[2]:(0.262924700975) A[3]:(0.249733790755)\n",
      " state (0)  A[0]:(0.0148373991251)\n",
      " state (1)  A[0]:(0.0143031692132)\n",
      " state (2)  A[0]:(0.0144118517637)\n",
      " state (3)  A[0]:(0.0146714355797)\n",
      " state (4)  A[0]:(0.0152687933296)\n",
      " state (5)  A[0]:(0.0165727101266)\n",
      " state (6)  A[0]:(0.0192749351263)\n",
      " state (7)  A[0]:(0.0246426034719)\n",
      " state (8)  A[0]:(0.0348542146385)\n",
      " state (9)  A[0]:(0.052953671664)\n",
      " state (10)  A[0]:(0.0814256891608)\n",
      " state (11)  A[0]:(0.119611941278)\n",
      " state (12)  A[0]:(0.162975132465)\n",
      " state (13)  A[0]:(0.205707684159)\n",
      " state (14)  A[0]:(0.243655771017)\n",
      " state (15)  A[0]:(0.275102198124)\n",
      "Episode 98000 finished after 2 . Running score: 0.01. Policy_loss: -95637.3695126, Value_loss: 1.2198647789. Times trained:               7727. Times reached goal: 19.               Steps done: 752446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.250460356474) A[1]:(0.24977350235) A[2]:(0.249977692962) A[3]:(0.249788478017)\n",
      " state (1)  A[0]:(0.249196752906) A[1]:(0.249406248331) A[2]:(0.249990925193) A[3]:(0.25140607357)\n",
      " state (2)  A[0]:(0.250200510025) A[1]:(0.249468758702) A[2]:(0.249905690551) A[3]:(0.250425070524)\n",
      " state (3)  A[0]:(0.251131922007) A[1]:(0.249529674649) A[2]:(0.249828338623) A[3]:(0.249510020018)\n",
      " state (4)  A[0]:(0.251954376698) A[1]:(0.249597415328) A[2]:(0.249764412642) A[3]:(0.248683795333)\n",
      " state (5)  A[0]:(0.252615183592) A[1]:(0.249687492847) A[2]:(0.249723285437) A[3]:(0.247974008322)\n",
      " state (6)  A[0]:(0.253078013659) A[1]:(0.249818459153) A[2]:(0.249714121222) A[3]:(0.247389361262)\n",
      " state (7)  A[0]:(0.253325074911) A[1]:(0.250008761883) A[2]:(0.24974501133) A[3]:(0.246921107173)\n",
      " state (8)  A[0]:(0.253343403339) A[1]:(0.250277727842) A[2]:(0.249824658036) A[3]:(0.246554210782)\n",
      " state (9)  A[0]:(0.253110557795) A[1]:(0.25064817071) A[2]:(0.249964386225) A[3]:(0.246276900172)\n",
      " state (10)  A[0]:(0.252588242292) A[1]:(0.251148045063) A[2]:(0.250179260969) A[3]:(0.246084436774)\n",
      " state (11)  A[0]:(0.251724630594) A[1]:(0.251809537411) A[2]:(0.250487089157) A[3]:(0.245978787541)\n",
      " state (12)  A[0]:(0.250464200974) A[1]:(0.252664625645) A[2]:(0.250905662775) A[3]:(0.245965480804)\n",
      " state (13)  A[0]:(0.248764321208) A[1]:(0.253737807274) A[2]:(0.251448303461) A[3]:(0.246049612761)\n",
      " state (14)  A[0]:(0.24661386013) A[1]:(0.255036711693) A[2]:(0.252118259668) A[3]:(0.24623118341)\n",
      " state (15)  A[0]:(0.244048446417) A[1]:(0.25654488802) A[2]:(0.252904683352) A[3]:(0.246501937509)\n",
      " state (0)  A[0]:(0.0138664701954)\n",
      " state (1)  A[0]:(0.0133846420795)\n",
      " state (2)  A[0]:(0.0134523753077)\n",
      " state (3)  A[0]:(0.0135975154117)\n",
      " state (4)  A[0]:(0.0139020131901)\n",
      " state (5)  A[0]:(0.0145182004198)\n",
      " state (6)  A[0]:(0.0157138239592)\n",
      " state (7)  A[0]:(0.0179454982281)\n",
      " state (8)  A[0]:(0.0219805836678)\n",
      " state (9)  A[0]:(0.0290545113385)\n",
      " state (10)  A[0]:(0.0409134961665)\n",
      " state (11)  A[0]:(0.0593904294074)\n",
      " state (12)  A[0]:(0.0853199288249)\n",
      " state (13)  A[0]:(0.117497116327)\n",
      " state (14)  A[0]:(0.152874141932)\n",
      " state (15)  A[0]:(0.187952488661)\n",
      "Episode 99000 finished after 14 . Running score: 0.0. Policy_loss: -95637.792744, Value_loss: 1.00768017257. Times trained:               7675. Times reached goal: 15.               Steps done: 760121.\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2498,  0.2502,  0.2501]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2494,  0.2495,  0.2501,  0.2510]])\n",
      "On state=1, selected action=0\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2494,  0.2495,  0.2501,  0.2510]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2498,  0.2502,  0.2501]])\n",
      "On state=0, selected action=2\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2498,  0.2502,  0.2501]])\n",
      "On state=0, selected action=1\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2510,  0.2498,  0.2503,  0.2490]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2464,  0.2521,  0.2524,  0.2490]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2510,  0.2498,  0.2503,  0.2489]])\n",
      "On state=4, selected action=3\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2510,  0.2498,  0.2503,  0.2489]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2500,  0.2498,  0.2502,  0.2501]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2510,  0.2498,  0.2503,  0.2489]])\n",
      "On state=4, selected action=3\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2509,  0.2498,  0.2503,  0.2489]])\n",
      "On state=4, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.249947458506) A[1]:(0.249782174826) A[2]:(0.250188648701) A[3]:(0.25008174777)\n",
      " state (1)  A[0]:(0.249337017536) A[1]:(0.249562576413) A[2]:(0.250088185072) A[3]:(0.251012265682)\n",
      " state (2)  A[0]:(0.25012499094) A[1]:(0.249590873718) A[2]:(0.250120937824) A[3]:(0.250163197517)\n",
      " state (3)  A[0]:(0.250696837902) A[1]:(0.249652534723) A[2]:(0.250191092491) A[3]:(0.249459475279)\n",
      " state (4)  A[0]:(0.250944525003) A[1]:(0.249787583947) A[2]:(0.250324875116) A[3]:(0.248943001032)\n",
      " state (5)  A[0]:(0.250755220652) A[1]:(0.250047594309) A[2]:(0.2505620718) A[3]:(0.248635068536)\n",
      " state (6)  A[0]:(0.250009059906) A[1]:(0.250492453575) A[2]:(0.250957548618) A[3]:(0.248540908098)\n",
      " state (7)  A[0]:(0.248530179262) A[1]:(0.25120049715) A[2]:(0.251587092876) A[3]:(0.248682275414)\n",
      " state (8)  A[0]:(0.246084958315) A[1]:(0.252267599106) A[2]:(0.252542972565) A[3]:(0.249104484916)\n",
      " state (9)  A[0]:(0.242459908128) A[1]:(0.253779262304) A[2]:(0.253906875849) A[3]:(0.24985396862)\n",
      " state (10)  A[0]:(0.237602502108) A[1]:(0.255758553743) A[2]:(0.255701392889) A[3]:(0.250937581062)\n",
      " state (11)  A[0]:(0.231740251184) A[1]:(0.258120894432) A[2]:(0.257848680019) A[3]:(0.252290129662)\n",
      " state (12)  A[0]:(0.225362345576) A[1]:(0.260680407286) A[2]:(0.260176897049) A[3]:(0.25378036499)\n",
      " state (13)  A[0]:(0.219048514962) A[1]:(0.263213813305) A[2]:(0.262480348349) A[3]:(0.255257278681)\n",
      " state (14)  A[0]:(0.213271647692) A[1]:(0.265536785126) A[2]:(0.264590114355) A[3]:(0.25660148263)\n",
      " state (15)  A[0]:(0.208297848701) A[1]:(0.267543464899) A[2]:(0.266410082579) A[3]:(0.257748544216)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 100000 finished after 12 . Running score: 0.01. Policy_loss: -95637.6906725, Value_loss: 1.0130347668. Times trained:               7741. Times reached goal: 14.               Steps done: 767862.\n",
      " state (0)  A[0]:(0.241669937968) A[1]:(0.250212997198) A[2]:(0.248669460416) A[3]:(0.259447664022)\n",
      " state (1)  A[0]:(0.238749489188) A[1]:(0.250395148993) A[2]:(0.248403087258) A[3]:(0.262452304363)\n",
      " state (2)  A[0]:(0.238853096962) A[1]:(0.252545028925) A[2]:(0.250438064337) A[3]:(0.258163779974)\n",
      " state (3)  A[0]:(0.234055429697) A[1]:(0.256332993507) A[2]:(0.253333032131) A[3]:(0.256278514862)\n",
      " state (4)  A[0]:(0.220275238156) A[1]:(0.263752758503) A[2]:(0.25833773613) A[3]:(0.257634222507)\n",
      " state (5)  A[0]:(0.194382399321) A[1]:(0.276855558157) A[2]:(0.266525655985) A[3]:(0.262236356735)\n",
      " state (6)  A[0]:(0.161219477654) A[1]:(0.29421544075) A[2]:(0.276418328285) A[3]:(0.268146812916)\n",
      " state (7)  A[0]:(0.132148489356) A[1]:(0.310568600893) A[2]:(0.28468182683) A[3]:(0.272601097822)\n",
      " state (8)  A[0]:(0.112549893558) A[1]:(0.322491675615) A[2]:(0.289997756481) A[3]:(0.274960637093)\n",
      " state (9)  A[0]:(0.10087671876) A[1]:(0.330064535141) A[2]:(0.293042182922) A[3]:(0.276016592979)\n",
      " state (10)  A[0]:(0.0942151769996) A[1]:(0.334584444761) A[2]:(0.294731616974) A[3]:(0.276468783617)\n",
      " state (11)  A[0]:(0.0904528722167) A[1]:(0.337212175131) A[2]:(0.295668929815) A[3]:(0.276666045189)\n",
      " state (12)  A[0]:(0.088326819241) A[1]:(0.338724285364) A[2]:(0.296193033457) A[3]:(0.276755869389)\n",
      " state (13)  A[0]:(0.0871205925941) A[1]:(0.339592128992) A[2]:(0.296488702297) A[3]:(0.276798546314)\n",
      " state (14)  A[0]:(0.0864327698946) A[1]:(0.340090841055) A[2]:(0.296656757593) A[3]:(0.276819586754)\n",
      " state (15)  A[0]:(0.0860384330153) A[1]:(0.340378284454) A[2]:(0.296753078699) A[3]:(0.276830255985)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 101000 finished after 8 . Running score: 0.0. Policy_loss: -95626.8205545, Value_loss: 1.01062219626. Times trained:               7634. Times reached goal: 15.               Steps done: 775496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.239238187671) A[1]:(0.24876844883) A[2]:(0.247011318803) A[3]:(0.264982074499)\n",
      " state (1)  A[0]:(0.235978215933) A[1]:(0.248376309872) A[2]:(0.246176615357) A[3]:(0.269468843937)\n",
      " state (2)  A[0]:(0.236708685756) A[1]:(0.251143991947) A[2]:(0.249036565423) A[3]:(0.263110756874)\n",
      " state (3)  A[0]:(0.233813479543) A[1]:(0.254841148853) A[2]:(0.252350628376) A[3]:(0.258994728327)\n",
      " state (4)  A[0]:(0.22455099225) A[1]:(0.260611742735) A[2]:(0.256872624159) A[3]:(0.257964670658)\n",
      " state (5)  A[0]:(0.205926939845) A[1]:(0.270308494568) A[2]:(0.263895481825) A[3]:(0.259869098663)\n",
      " state (6)  A[0]:(0.176604434848) A[1]:(0.285272806883) A[2]:(0.274065554142) A[3]:(0.264057189226)\n",
      " state (7)  A[0]:(0.141428261995) A[1]:(0.304099500179) A[2]:(0.28586062789) A[3]:(0.268611580133)\n",
      " state (8)  A[0]:(0.109633766115) A[1]:(0.322644561529) A[2]:(0.296302825212) A[3]:(0.271418839693)\n",
      " state (9)  A[0]:(0.0863928571343) A[1]:(0.33765861392) A[2]:(0.303808987141) A[3]:(0.272139549255)\n",
      " state (10)  A[0]:(0.0711495801806) A[1]:(0.348513752222) A[2]:(0.308670848608) A[3]:(0.271665841341)\n",
      " state (11)  A[0]:(0.0615210272372) A[1]:(0.355947941542) A[2]:(0.311719596386) A[3]:(0.270811468363)\n",
      " state (12)  A[0]:(0.0555159375072) A[1]:(0.360881924629) A[2]:(0.313613981009) A[3]:(0.269988119602)\n",
      " state (13)  A[0]:(0.0518014021218) A[1]:(0.364077568054) A[2]:(0.314782649279) A[3]:(0.269338399172)\n",
      " state (14)  A[0]:(0.0495104081929) A[1]:(0.366114914417) A[2]:(0.315501213074) A[3]:(0.268873423338)\n",
      " state (15)  A[0]:(0.0480920001864) A[1]:(0.36740642786) A[2]:(0.315944820642) A[3]:(0.268556773663)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 102000 finished after 8 . Running score: 0.02. Policy_loss: -95626.4051966, Value_loss: 1.02166095938. Times trained:               7388. Times reached goal: 27.               Steps done: 782884.\n",
      " state (0)  A[0]:(0.23746907711) A[1]:(0.243290692568) A[2]:(0.242509424686) A[3]:(0.276730775833)\n",
      " state (1)  A[0]:(0.234191805124) A[1]:(0.24231928587) A[2]:(0.241366550326) A[3]:(0.282122343779)\n",
      " state (2)  A[0]:(0.234363347292) A[1]:(0.247428655624) A[2]:(0.246331006289) A[3]:(0.271877020597)\n",
      " state (3)  A[0]:(0.2288428545) A[1]:(0.254450768232) A[2]:(0.252681314945) A[3]:(0.264025062323)\n",
      " state (4)  A[0]:(0.211544170976) A[1]:(0.265276551247) A[2]:(0.26163816452) A[3]:(0.261541157961)\n",
      " state (5)  A[0]:(0.177449330688) A[1]:(0.283164858818) A[2]:(0.275464236736) A[3]:(0.263921588659)\n",
      " state (6)  A[0]:(0.130705744028) A[1]:(0.3081163764) A[2]:(0.293385058641) A[3]:(0.267792791128)\n",
      " state (7)  A[0]:(0.0877709910274) A[1]:(0.333641290665) A[2]:(0.30997633934) A[3]:(0.268611431122)\n",
      " state (8)  A[0]:(0.0594441480935) A[1]:(0.353311449289) A[2]:(0.321366935968) A[3]:(0.265877485275)\n",
      " state (9)  A[0]:(0.0438147522509) A[1]:(0.366062223911) A[2]:(0.328015685081) A[3]:(0.262107342482)\n",
      " state (10)  A[0]:(0.0355766676366) A[1]:(0.373749166727) A[2]:(0.331721186638) A[3]:(0.258953005075)\n",
      " state (11)  A[0]:(0.0311368815601) A[1]:(0.378315657377) A[2]:(0.333809882402) A[3]:(0.256737619638)\n",
      " state (12)  A[0]:(0.0286527443677) A[1]:(0.381045222282) A[2]:(0.335017085075) A[3]:(0.255284935236)\n",
      " state (13)  A[0]:(0.0272165723145) A[1]:(0.382694512606) A[2]:(0.335730910301) A[3]:(0.254358023405)\n",
      " state (14)  A[0]:(0.0263652615249) A[1]:(0.383701741695) A[2]:(0.336160600185) A[3]:(0.253772377968)\n",
      " state (15)  A[0]:(0.0258510280401) A[1]:(0.38432314992) A[2]:(0.336422890425) A[3]:(0.253402918577)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 103000 finished after 6 . Running score: 0.01. Policy_loss: -95626.3916295, Value_loss: 1.02128721404. Times trained:               7366. Times reached goal: 17.               Steps done: 790250.\n",
      " state (0)  A[0]:(0.245904788375) A[1]:(0.243992716074) A[2]:(0.241681605577) A[3]:(0.268420875072)\n",
      " state (1)  A[0]:(0.245857268572) A[1]:(0.243242114782) A[2]:(0.240484595299) A[3]:(0.270415991545)\n",
      " state (2)  A[0]:(0.248100921512) A[1]:(0.250159472227) A[2]:(0.246032580733) A[3]:(0.25570705533)\n",
      " state (3)  A[0]:(0.243212983012) A[1]:(0.259569555521) A[2]:(0.252788752317) A[3]:(0.244428664446)\n",
      " state (4)  A[0]:(0.225066304207) A[1]:(0.274319320917) A[2]:(0.262210220098) A[3]:(0.238404199481)\n",
      " state (5)  A[0]:(0.188227787614) A[1]:(0.299930483103) A[2]:(0.2769087255) A[3]:(0.234932973981)\n",
      " state (6)  A[0]:(0.133345037699) A[1]:(0.340886831284) A[2]:(0.296794325113) A[3]:(0.228973791003)\n",
      " state (7)  A[0]:(0.0781258419156) A[1]:(0.392264127731) A[2]:(0.314931750298) A[3]:(0.214678287506)\n",
      " state (8)  A[0]:(0.0424531809986) A[1]:(0.439223259687) A[2]:(0.32405436039) A[3]:(0.194269165397)\n",
      " state (9)  A[0]:(0.0252460837364) A[1]:(0.472837895155) A[2]:(0.325920790434) A[3]:(0.175995215774)\n",
      " state (10)  A[0]:(0.0174022298306) A[1]:(0.494387537241) A[2]:(0.3250900805) A[3]:(0.163120165467)\n",
      " state (11)  A[0]:(0.0135960876942) A[1]:(0.507792949677) A[2]:(0.323810309172) A[3]:(0.15480068326)\n",
      " state (12)  A[0]:(0.0115984091535) A[1]:(0.516112029552) A[2]:(0.322735905647) A[3]:(0.149553731084)\n",
      " state (13)  A[0]:(0.0104817869142) A[1]:(0.521301746368) A[2]:(0.321961194277) A[3]:(0.146255329251)\n",
      " state (14)  A[0]:(0.00982880778611) A[1]:(0.524561405182) A[2]:(0.321435034275) A[3]:(0.144174784422)\n",
      " state (15)  A[0]:(0.00943468417972) A[1]:(0.52662307024) A[2]:(0.321087002754) A[3]:(0.142855226994)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 104000 finished after 4 . Running score: 0.02. Policy_loss: -95626.0943588, Value_loss: 1.22007871091. Times trained:               7847. Times reached goal: 21.               Steps done: 798097.\n",
      "action_dist \n",
      "tensor([[ 0.2613,  0.2402,  0.2399,  0.2587]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2646,  0.2369,  0.2369,  0.2615]])\n",
      "On state=1, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2646,  0.2369,  0.2369,  0.2615]])\n",
      "On state=1, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.261282891035) A[1]:(0.240166381001) A[2]:(0.239845469594) A[3]:(0.258705288172)\n",
      " state (1)  A[0]:(0.264590084553) A[1]:(0.236927762628) A[2]:(0.236931964755) A[3]:(0.261550217867)\n",
      " state (2)  A[0]:(0.270440906286) A[1]:(0.241350129247) A[2]:(0.240282550454) A[3]:(0.247926443815)\n",
      " state (3)  A[0]:(0.272402107716) A[1]:(0.246821150184) A[2]:(0.244201213121) A[3]:(0.23657554388)\n",
      " state (4)  A[0]:(0.267261892557) A[1]:(0.254023790359) A[2]:(0.249067395926) A[3]:(0.229646891356)\n",
      " state (5)  A[0]:(0.252690136433) A[1]:(0.265154749155) A[2]:(0.256254553795) A[3]:(0.225900560617)\n",
      " state (6)  A[0]:(0.22461386025) A[1]:(0.283722758293) A[2]:(0.267609775066) A[3]:(0.224053636193)\n",
      " state (7)  A[0]:(0.179893925786) A[1]:(0.313742488623) A[2]:(0.284302055836) A[3]:(0.222061499953)\n",
      " state (8)  A[0]:(0.12534531951) A[1]:(0.355087846518) A[2]:(0.303552597761) A[3]:(0.216014280915)\n",
      " state (9)  A[0]:(0.0786459371448) A[1]:(0.399047285318) A[2]:(0.318567454815) A[3]:(0.203739315271)\n",
      " state (10)  A[0]:(0.0489961206913) A[1]:(0.436117917299) A[2]:(0.326277732849) A[3]:(0.188608214259)\n",
      " state (11)  A[0]:(0.0327041447163) A[1]:(0.463519871235) A[2]:(0.32884785533) A[3]:(0.174928113818)\n",
      " state (12)  A[0]:(0.0239481627941) A[1]:(0.482649147511) A[2]:(0.329043567181) A[3]:(0.164359077811)\n",
      " state (13)  A[0]:(0.0190790574998) A[1]:(0.49573469162) A[2]:(0.328435242176) A[3]:(0.156751021743)\n",
      " state (14)  A[0]:(0.0162359233946) A[1]:(0.504652798176) A[2]:(0.327685832977) A[3]:(0.151425480843)\n",
      " state (15)  A[0]:(0.0144976135343) A[1]:(0.510752618313) A[2]:(0.327021747828) A[3]:(0.147727966309)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 105000 finished after 3 . Running score: 0.0. Policy_loss: -95626.0887954, Value_loss: 1.23602078436. Times trained:               7454. Times reached goal: 23.               Steps done: 805551.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.269585669041) A[1]:(0.244805917144) A[2]:(0.22768907249) A[3]:(0.257919341326)\n",
      " state (1)  A[0]:(0.274643540382) A[1]:(0.245560482144) A[2]:(0.22494186461) A[3]:(0.254854112864)\n",
      " state (2)  A[0]:(0.278736531734) A[1]:(0.257365733385) A[2]:(0.229437544942) A[3]:(0.234460219741)\n",
      " state (3)  A[0]:(0.27352720499) A[1]:(0.277565032244) A[2]:(0.236894249916) A[3]:(0.212013572454)\n",
      " state (4)  A[0]:(0.247118204832) A[1]:(0.312550485134) A[2]:(0.247437730432) A[3]:(0.192893564701)\n",
      " state (5)  A[0]:(0.189601406455) A[1]:(0.376784026623) A[2]:(0.260303944349) A[3]:(0.173310667276)\n",
      " state (6)  A[0]:(0.109747126698) A[1]:(0.481909245253) A[2]:(0.265205383301) A[3]:(0.143138229847)\n",
      " state (7)  A[0]:(0.0468468740582) A[1]:(0.604933202267) A[2]:(0.245850533247) A[3]:(0.102369427681)\n",
      " state (8)  A[0]:(0.018846925348) A[1]:(0.700533986092) A[2]:(0.211847439408) A[3]:(0.0687716156244)\n",
      " state (9)  A[0]:(0.00928891357034) A[1]:(0.757517158985) A[2]:(0.183711454272) A[3]:(0.0494824387133)\n",
      " state (10)  A[0]:(0.00580415967852) A[1]:(0.789290785789) A[2]:(0.165535867214) A[3]:(0.0393691509962)\n",
      " state (11)  A[0]:(0.0043349256739) A[1]:(0.807028532028) A[2]:(0.154633402824) A[3]:(0.0340031348169)\n",
      " state (12)  A[0]:(0.00364268687554) A[1]:(0.816941618919) A[2]:(0.148304328322) A[3]:(0.031111381948)\n",
      " state (13)  A[0]:(0.00328400731087) A[1]:(0.822623848915) A[2]:(0.144599243999) A[3]:(0.029492912814)\n",
      " state (14)  A[0]:(0.00308376131579) A[1]:(0.825997889042) A[2]:(0.142372563481) A[3]:(0.0285457633436)\n",
      " state (15)  A[0]:(0.00296620605513) A[1]:(0.828058183193) A[2]:(0.141003295779) A[3]:(0.0279722996056)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 106000 finished after 22 . Running score: 0.06. Policy_loss: -95625.9431165, Value_loss: 1.43468474135. Times trained:               7695. Times reached goal: 40.               Steps done: 813246.\n",
      " state (0)  A[0]:(0.276967376471) A[1]:(0.232299536467) A[2]:(0.223578765988) A[3]:(0.267154335976)\n",
      " state (1)  A[0]:(0.286875605583) A[1]:(0.227821797132) A[2]:(0.218409895897) A[3]:(0.266892671585)\n",
      " state (2)  A[0]:(0.299481153488) A[1]:(0.232587307692) A[2]:(0.220104604959) A[3]:(0.24782691896)\n",
      " state (3)  A[0]:(0.308154881001) A[1]:(0.238400280476) A[2]:(0.222598746419) A[3]:(0.230846121907)\n",
      " state (4)  A[0]:(0.311151385307) A[1]:(0.244467645884) A[2]:(0.225287154317) A[3]:(0.21909378469)\n",
      " state (5)  A[0]:(0.310001581907) A[1]:(0.251769483089) A[2]:(0.228388920426) A[3]:(0.209839984775)\n",
      " state (6)  A[0]:(0.304240256548) A[1]:(0.261896848679) A[2]:(0.232367128134) A[3]:(0.201495781541)\n",
      " state (7)  A[0]:(0.291125655174) A[1]:(0.277492523193) A[2]:(0.237922742963) A[3]:(0.193459048867)\n",
      " state (8)  A[0]:(0.266627192497) A[1]:(0.30238366127) A[2]:(0.245760723948) A[3]:(0.185228422284)\n",
      " state (9)  A[0]:(0.228816717863) A[1]:(0.339841634035) A[2]:(0.255498528481) A[3]:(0.175843089819)\n",
      " state (10)  A[0]:(0.181368291378) A[1]:(0.390098422766) A[2]:(0.264706969261) A[3]:(0.163826301694)\n",
      " state (11)  A[0]:(0.132396578789) A[1]:(0.449626892805) A[2]:(0.269743382931) A[3]:(0.148233160377)\n",
      " state (12)  A[0]:(0.0904319286346) A[1]:(0.511640250683) A[2]:(0.268027842045) A[3]:(0.129899919033)\n",
      " state (13)  A[0]:(0.0599934235215) A[1]:(0.568600356579) A[2]:(0.260011315346) A[3]:(0.111394926906)\n",
      " state (14)  A[0]:(0.0403530113399) A[1]:(0.615936696529) A[2]:(0.248544052243) A[3]:(0.0951662138104)\n",
      " state (15)  A[0]:(0.028371648863) A[1]:(0.652828991413) A[2]:(0.236546292901) A[3]:(0.0822530984879)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 107000 finished after 5 . Running score: 0.03. Policy_loss: -95626.0696714, Value_loss: 1.22875180407. Times trained:               7317. Times reached goal: 40.               Steps done: 820563.\n",
      " state (0)  A[0]:(0.263397723436) A[1]:(0.248473212123) A[2]:(0.222986102104) A[3]:(0.26514300704)\n",
      " state (1)  A[0]:(0.273963302374) A[1]:(0.253142565489) A[2]:(0.218500941992) A[3]:(0.25439324975)\n",
      " state (2)  A[0]:(0.283094257116) A[1]:(0.279287755489) A[2]:(0.221500784159) A[3]:(0.216117233038)\n",
      " state (3)  A[0]:(0.270162701607) A[1]:(0.328655302525) A[2]:(0.225790023804) A[3]:(0.175391927361)\n",
      " state (4)  A[0]:(0.207436159253) A[1]:(0.427099198103) A[2]:(0.226666390896) A[3]:(0.138798266649)\n",
      " state (5)  A[0]:(0.100023202598) A[1]:(0.606564462185) A[2]:(0.201955527067) A[3]:(0.0914568006992)\n",
      " state (6)  A[0]:(0.0261189807206) A[1]:(0.796404540539) A[2]:(0.135772809386) A[3]:(0.0417036786675)\n",
      " state (7)  A[0]:(0.00597949977964) A[1]:(0.8982462883) A[2]:(0.0791199207306) A[3]:(0.0166542846709)\n",
      " state (8)  A[0]:(0.00214918330312) A[1]:(0.9368019104) A[2]:(0.052550572902) A[3]:(0.00849836040288)\n",
      " state (9)  A[0]:(0.0012484745821) A[1]:(0.95100992918) A[2]:(0.0418784096837) A[3]:(0.00586317759007)\n",
      " state (10)  A[0]:(0.000947336666286) A[1]:(0.956991612911) A[2]:(0.0372235104442) A[3]:(0.00483752368018)\n",
      " state (11)  A[0]:(0.000817945343442) A[1]:(0.959887385368) A[2]:(0.0349331088364) A[3]:(0.00436157407239)\n",
      " state (12)  A[0]:(0.00075403007213) A[1]:(0.961416363716) A[2]:(0.0337136611342) A[3]:(0.00411593588069)\n",
      " state (13)  A[0]:(0.000719953619409) A[1]:(0.962264478207) A[2]:(0.0330342054367) A[3]:(0.00398137047887)\n",
      " state (14)  A[0]:(0.000700964592397) A[1]:(0.962748885155) A[2]:(0.0326451100409) A[3]:(0.00390503881499)\n",
      " state (15)  A[0]:(0.000690090295393) A[1]:(0.963030874729) A[2]:(0.0324182771146) A[3]:(0.00386077142321)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 108000 finished after 8 . Running score: 0.05. Policy_loss: -95626.0273117, Value_loss: 1.85818564641. Times trained:               7337. Times reached goal: 48.               Steps done: 827900.\n",
      " state (0)  A[0]:(0.249803543091) A[1]:(0.228676438332) A[2]:(0.218572571874) A[3]:(0.302947461605)\n",
      " state (1)  A[0]:(0.258684724569) A[1]:(0.226591020823) A[2]:(0.214112102985) A[3]:(0.300612181425)\n",
      " state (2)  A[0]:(0.273115158081) A[1]:(0.234041273594) A[2]:(0.213286295533) A[3]:(0.279557317495)\n",
      " state (3)  A[0]:(0.289164811373) A[1]:(0.245855689049) A[2]:(0.212889760733) A[3]:(0.252089709044)\n",
      " state (4)  A[0]:(0.298989087343) A[1]:(0.262611091137) A[2]:(0.213385075331) A[3]:(0.225014716387)\n",
      " state (5)  A[0]:(0.297559976578) A[1]:(0.287218630314) A[2]:(0.21458107233) A[3]:(0.200640290976)\n",
      " state (6)  A[0]:(0.279170185328) A[1]:(0.328658789396) A[2]:(0.215747863054) A[3]:(0.176423147321)\n",
      " state (7)  A[0]:(0.231626167893) A[1]:(0.407192468643) A[2]:(0.213630199432) A[3]:(0.147551193833)\n",
      " state (8)  A[0]:(0.147501498461) A[1]:(0.549212694168) A[2]:(0.195849716663) A[3]:(0.10743611306)\n",
      " state (9)  A[0]:(0.0648994296789) A[1]:(0.722642421722) A[2]:(0.150362625718) A[3]:(0.06209551543)\n",
      " state (10)  A[0]:(0.0235481224954) A[1]:(0.845093727112) A[2]:(0.100119426847) A[3]:(0.0312387086451)\n",
      " state (11)  A[0]:(0.00910370983183) A[1]:(0.908737242222) A[2]:(0.0659443512559) A[3]:(0.0162146687508)\n",
      " state (12)  A[0]:(0.00423295842484) A[1]:(0.939933300018) A[2]:(0.04637176916) A[3]:(0.00946196448058)\n",
      " state (13)  A[0]:(0.00239444407634) A[1]:(0.955922484398) A[2]:(0.0353990644217) A[3]:(0.0062839970924)\n",
      " state (14)  A[0]:(0.00159070082009) A[1]:(0.964727938175) A[2]:(0.0290266145021) A[3]:(0.00465471763164)\n",
      " state (15)  A[0]:(0.00118963688146) A[1]:(0.969927489758) A[2]:(0.0251382347196) A[3]:(0.00374465994537)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 109000 finished after 15 . Running score: 0.02. Policy_loss: -95626.0814595, Value_loss: 1.22058473661. Times trained:               7335. Times reached goal: 49.               Steps done: 835235.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.2461,  0.2300,  0.2177,  0.3062]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2461,  0.2300,  0.2177,  0.3061]])\n",
      "On state=0, selected action=1\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.3016,  0.3022,  0.2087,  0.1875]])\n",
      "On state=4, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2462,  0.2301,  0.2177,  0.3060]])\n",
      "On state=0, selected action=1\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.3017,  0.3024,  0.2087,  0.1872]])\n",
      "On state=4, selected action=3\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.24625711143) A[1]:(0.230159625411) A[2]:(0.217745766044) A[3]:(0.305837482214)\n",
      " state (1)  A[0]:(0.256010502577) A[1]:(0.22994504869) A[2]:(0.213112801313) A[3]:(0.300931662321)\n",
      " state (2)  A[0]:(0.274651318789) A[1]:(0.24289983511) A[2]:(0.211837381124) A[3]:(0.270611435175)\n",
      " state (3)  A[0]:(0.295365810394) A[1]:(0.265567004681) A[2]:(0.210390195251) A[3]:(0.228676974773)\n",
      " state (4)  A[0]:(0.301884353161) A[1]:(0.302631527185) A[2]:(0.208679810166) A[3]:(0.186804279685)\n",
      " state (5)  A[0]:(0.276624083519) A[1]:(0.368752539158) A[2]:(0.204886972904) A[3]:(0.149736389518)\n",
      " state (6)  A[0]:(0.200945407152) A[1]:(0.50030362606) A[2]:(0.189700424671) A[3]:(0.109050527215)\n",
      " state (7)  A[0]:(0.0855953171849) A[1]:(0.720321178436) A[2]:(0.137728080153) A[3]:(0.056355394423)\n",
      " state (8)  A[0]:(0.0175923202187) A[1]:(0.901792347431) A[2]:(0.0639626160264) A[3]:(0.0166527386755)\n",
      " state (9)  A[0]:(0.00338024646044) A[1]:(0.965589284897) A[2]:(0.026521967724) A[3]:(0.00450848042965)\n",
      " state (10)  A[0]:(0.00105566007551) A[1]:(0.983296275139) A[2]:(0.0138988317922) A[3]:(0.00174922181759)\n",
      " state (11)  A[0]:(0.000505325966515) A[1]:(0.989418923855) A[2]:(0.00913052260876) A[3]:(0.000945205567405)\n",
      " state (12)  A[0]:(0.000317830446875) A[1]:(0.992080450058) A[2]:(0.00696645770222) A[3]:(0.000635272066575)\n",
      " state (13)  A[0]:(0.000236523133935) A[1]:(0.993428826332) A[2]:(0.00584423961118) A[3]:(0.000490415550303)\n",
      " state (14)  A[0]:(0.000195421336684) A[1]:(0.994182646275) A[2]:(0.0052082571201) A[3]:(0.00041366086225)\n",
      " state (15)  A[0]:(0.000172480635229) A[1]:(0.994632303715) A[2]:(0.00482572102919) A[3]:(0.000369482848328)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 110000 finished after 5 . Running score: 0.06. Policy_loss: -95626.0983692, Value_loss: 1.22213954447. Times trained:               8027. Times reached goal: 45.               Steps done: 843262.\n",
      " state (0)  A[0]:(0.25202190876) A[1]:(0.225792139769) A[2]:(0.217656448483) A[3]:(0.304529517889)\n",
      " state (1)  A[0]:(0.264219582081) A[1]:(0.223607480526) A[2]:(0.212433964014) A[3]:(0.299738943577)\n",
      " state (2)  A[0]:(0.287763267756) A[1]:(0.233700200915) A[2]:(0.210485517979) A[3]:(0.268050968647)\n",
      " state (3)  A[0]:(0.315089315176) A[1]:(0.249865457416) A[2]:(0.208052486181) A[3]:(0.226992696524)\n",
      " state (4)  A[0]:(0.332236111164) A[1]:(0.272685050964) A[2]:(0.205525636673) A[3]:(0.189553245902)\n",
      " state (5)  A[0]:(0.330259293318) A[1]:(0.307898879051) A[2]:(0.202523559332) A[3]:(0.159318238497)\n",
      " state (6)  A[0]:(0.297486066818) A[1]:(0.375363111496) A[2]:(0.196600660682) A[3]:(0.130550116301)\n",
      " state (7)  A[0]:(0.20863416791) A[1]:(0.523354649544) A[2]:(0.175626590848) A[3]:(0.0923846140504)\n",
      " state (8)  A[0]:(0.0815697908401) A[1]:(0.760815620422) A[2]:(0.114991374314) A[3]:(0.042623180896)\n",
      " state (9)  A[0]:(0.0187066812068) A[1]:(0.916466653347) A[2]:(0.0519368723035) A[3]:(0.01288979128)\n",
      " state (10)  A[0]:(0.0044551496394) A[1]:(0.96870213747) A[2]:(0.0228435434401) A[3]:(0.00399917643517)\n",
      " state (11)  A[0]:(0.00145629455801) A[1]:(0.985119462013) A[2]:(0.0118377571926) A[3]:(0.00158647156786)\n",
      " state (12)  A[0]:(0.000656235672068) A[1]:(0.991204380989) A[2]:(0.00733012938872) A[3]:(0.000809235381894)\n",
      " state (13)  A[0]:(0.000378806173103) A[1]:(0.993887364864) A[2]:(0.00523057440296) A[3]:(0.000503253366333)\n",
      " state (14)  A[0]:(0.000260261032963) A[1]:(0.995242774487) A[2]:(0.00413571298122) A[3]:(0.000361233338481)\n",
      " state (15)  A[0]:(0.00020134235092) A[1]:(0.995998680592) A[2]:(0.00351324654184) A[3]:(0.000286736671114)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 111000 finished after 5 . Running score: 0.04. Policy_loss: -95626.1156325, Value_loss: 1.42952801537. Times trained:               7905. Times reached goal: 35.               Steps done: 851167.\n",
      " state (0)  A[0]:(0.25894382596) A[1]:(0.223262816668) A[2]:(0.213806912303) A[3]:(0.303986430168)\n",
      " state (1)  A[0]:(0.277813911438) A[1]:(0.222200646996) A[2]:(0.207395330071) A[3]:(0.292590111494)\n",
      " state (2)  A[0]:(0.314930468798) A[1]:(0.237530946732) A[2]:(0.204337954521) A[3]:(0.243200585246)\n",
      " state (3)  A[0]:(0.35243088007) A[1]:(0.261335164309) A[2]:(0.199534088373) A[3]:(0.18669988215)\n",
      " state (4)  A[0]:(0.365160435438) A[1]:(0.295552700758) A[2]:(0.193722113967) A[3]:(0.145564705133)\n",
      " state (5)  A[0]:(0.34028172493) A[1]:(0.357914596796) A[2]:(0.186006978154) A[3]:(0.115796692669)\n",
      " state (6)  A[0]:(0.254093497992) A[1]:(0.49655175209) A[2]:(0.166510000825) A[3]:(0.0828447341919)\n",
      " state (7)  A[0]:(0.101295091212) A[1]:(0.756708085537) A[2]:(0.105474963784) A[3]:(0.0365218669176)\n",
      " state (8)  A[0]:(0.0153390020132) A[1]:(0.943826019764) A[2]:(0.0338786058128) A[3]:(0.00695636356249)\n",
      " state (9)  A[0]:(0.00220389082097) A[1]:(0.986783742905) A[2]:(0.00977873150259) A[3]:(0.00123365723994)\n",
      " state (10)  A[0]:(0.000560258107726) A[1]:(0.995115399361) A[2]:(0.00396961160004) A[3]:(0.000354738440365)\n",
      " state (11)  A[0]:(0.000232891557971) A[1]:(0.997413337231) A[2]:(0.00219734990969) A[3]:(0.000156448484631)\n",
      " state (12)  A[0]:(0.000133661975269) A[1]:(0.998274445534) A[2]:(0.00149985402822) A[3]:(9.20660822885e-05)\n",
      " state (13)  A[0]:(9.3907576229e-05) A[1]:(0.998669087887) A[2]:(0.00117175257765) A[3]:(6.52805465506e-05)\n",
      " state (14)  A[0]:(7.48925594962e-05) A[1]:(0.998874664307) A[2]:(0.000998256960884) A[3]:(5.21933070559e-05)\n",
      " state (15)  A[0]:(6.47096676403e-05) A[1]:(0.998990952969) A[2]:(0.000899232865777) A[3]:(4.50965453638e-05)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 112000 finished after 5 . Running score: 0.0. Policy_loss: -95623.7408004, Value_loss: 1.00465924725. Times trained:               7954. Times reached goal: 45.               Steps done: 859121.\n",
      " state (0)  A[0]:(0.262186855078) A[1]:(0.205665096641) A[2]:(0.201216846704) A[3]:(0.330931186676)\n",
      " state (1)  A[0]:(0.300581157207) A[1]:(0.206326514482) A[2]:(0.192476525903) A[3]:(0.300615787506)\n",
      " state (2)  A[0]:(0.375390380621) A[1]:(0.218121603131) A[2]:(0.18069525063) A[3]:(0.225792780519)\n",
      " state (3)  A[0]:(0.44299197197) A[1]:(0.237869128585) A[2]:(0.167524367571) A[3]:(0.151614561677)\n",
      " state (4)  A[0]:(0.466778486967) A[1]:(0.269041776657) A[2]:(0.156974300742) A[3]:(0.107205450535)\n",
      " state (5)  A[0]:(0.442993074656) A[1]:(0.331786215305) A[2]:(0.146537929773) A[3]:(0.0786827802658)\n",
      " state (6)  A[0]:(0.333170264959) A[1]:(0.491583168507) A[2]:(0.125722050667) A[3]:(0.0495245456696)\n",
      " state (7)  A[0]:(0.113957263529) A[1]:(0.805280447006) A[2]:(0.0656831413507) A[3]:(0.0150791425258)\n",
      " state (8)  A[0]:(0.0130348084494) A[1]:(0.970478951931) A[2]:(0.0148593476042) A[3]:(0.00162692030426)\n",
      " state (9)  A[0]:(0.00180168519728) A[1]:(0.994331121445) A[2]:(0.00365154724568) A[3]:(0.000215630701859)\n",
      " state (10)  A[0]:(0.000490980630275) A[1]:(0.998024404049) A[2]:(0.00142838747706) A[3]:(5.6252538343e-05)\n",
      " state (11)  A[0]:(0.000219837660552) A[1]:(0.998964428902) A[2]:(0.000791552185547) A[3]:(2.41662710323e-05)\n",
      " state (12)  A[0]:(0.000133947920403) A[1]:(0.999304831028) A[2]:(0.000546963710804) A[3]:(1.42306616908e-05)\n",
      " state (13)  A[0]:(9.84696598607e-05) A[1]:(0.999457776546) A[2]:(0.000433580455137) A[3]:(1.01978021121e-05)\n",
      " state (14)  A[0]:(8.12160651549e-05) A[1]:(0.99953609705) A[2]:(0.000374427472707) A[3]:(8.26111954666e-06)\n",
      " state (15)  A[0]:(7.19284507795e-05) A[1]:(0.999579668045) A[2]:(0.000341171427863) A[3]:(7.22759386917e-06)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 113000 finished after 16 . Running score: 0.02. Policy_loss: -95623.4631331, Value_loss: 1.21834060461. Times trained:               7678. Times reached goal: 55.               Steps done: 866799.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.252057671547) A[1]:(0.196123108268) A[2]:(0.196626245975) A[3]:(0.355193018913)\n",
      " state (1)  A[0]:(0.288381308317) A[1]:(0.197562798858) A[2]:(0.189204603434) A[3]:(0.324851244688)\n",
      " state (2)  A[0]:(0.383178710938) A[1]:(0.205342844129) A[2]:(0.173859119415) A[3]:(0.237619310617)\n",
      " state (3)  A[0]:(0.497810423374) A[1]:(0.212223038077) A[2]:(0.149773389101) A[3]:(0.140193134546)\n",
      " state (4)  A[0]:(0.551332950592) A[1]:(0.228514626622) A[2]:(0.132804557681) A[3]:(0.0873478874564)\n",
      " state (5)  A[0]:(0.545459508896) A[1]:(0.270626872778) A[2]:(0.122584685683) A[3]:(0.0613289922476)\n",
      " state (6)  A[0]:(0.453221589327) A[1]:(0.394979000092) A[2]:(0.111219316721) A[3]:(0.0405800715089)\n",
      " state (7)  A[0]:(0.194804787636) A[1]:(0.719795048237) A[2]:(0.070612102747) A[3]:(0.0147880287841)\n",
      " state (8)  A[0]:(0.0255159847438) A[1]:(0.954245090485) A[2]:(0.0184797011316) A[3]:(0.00175920478068)\n",
      " state (9)  A[0]:(0.00336338579655) A[1]:(0.991880118847) A[2]:(0.00453787622973) A[3]:(0.000218598215724)\n",
      " state (10)  A[0]:(0.000832117744721) A[1]:(0.997422099113) A[2]:(0.00169443560299) A[3]:(5.1361592341e-05)\n",
      " state (11)  A[0]:(0.000343146064552) A[1]:(0.998738825321) A[2]:(0.000897781108506) A[3]:(2.02188857656e-05)\n",
      " state (12)  A[0]:(0.000197316781851) A[1]:(0.99919128418) A[2]:(0.000600179773755) A[3]:(1.11954104796e-05)\n",
      " state (13)  A[0]:(0.000139575902722) A[1]:(0.999387562275) A[2]:(0.000465141230961) A[3]:(7.69945472712e-06)\n",
      " state (14)  A[0]:(0.0001122514077) A[1]:(0.999486029148) A[2]:(0.000395674724132) A[3]:(6.07053925705e-06)\n",
      " state (15)  A[0]:(9.77847594186e-05) A[1]:(0.999540030956) A[2]:(0.000356940319762) A[3]:(5.21725405633e-06)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 114000 finished after 3 . Running score: 0.09. Policy_loss: -95623.501522, Value_loss: 1.22808824545. Times trained:               8475. Times reached goal: 46.               Steps done: 875274.\n",
      "action_dist \n",
      "tensor([[ 0.2288,  0.1880,  0.1923,  0.3909]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2288,  0.1879,  0.1923,  0.3909]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2288,  0.1879,  0.1923,  0.3909]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2289,  0.1879,  0.1923,  0.3909]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2289,  0.1879,  0.1923,  0.3910]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2604,  0.1922,  0.1876,  0.3597]])\n",
      "On state=1, selected action=1\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.3772,  0.2036,  0.1714,  0.2478]])\n",
      "On state=2, selected action=0\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2604,  0.1922,  0.1876,  0.3597]])\n",
      "On state=1, selected action=1\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.3773,  0.2035,  0.1713,  0.2478]])\n",
      "On state=2, selected action=0\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2789,  0.6333,  0.0732,  0.0145]])\n",
      "On state=6, selected action=2\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.228939965367) A[1]:(0.187866926193) A[2]:(0.192235440016) A[3]:(0.390957653522)\n",
      " state (1)  A[0]:(0.260680049658) A[1]:(0.192219004035) A[2]:(0.187572017312) A[3]:(0.359528928995)\n",
      " state (2)  A[0]:(0.378090173006) A[1]:(0.203582897782) A[2]:(0.171131789684) A[3]:(0.247195139527)\n",
      " state (3)  A[0]:(0.541789352894) A[1]:(0.208088889718) A[2]:(0.135613799095) A[3]:(0.114507943392)\n",
      " state (4)  A[0]:(0.595323443413) A[1]:(0.23268802464) A[2]:(0.112763777375) A[3]:(0.059224743396)\n",
      " state (5)  A[0]:(0.536276698112) A[1]:(0.327340900898) A[2]:(0.10065639019) A[3]:(0.0357260033488)\n",
      " state (6)  A[0]:(0.276173382998) A[1]:(0.636794269085) A[2]:(0.0726682394743) A[3]:(0.0143641401082)\n",
      " state (7)  A[0]:(0.0242215897888) A[1]:(0.959785342216) A[2]:(0.0149443205446) A[3]:(0.00104875979014)\n",
      " state (8)  A[0]:(0.0012362167472) A[1]:(0.996879816055) A[2]:(0.00183940702118) A[3]:(4.45797741122e-05)\n",
      " state (9)  A[0]:(0.000206442709896) A[1]:(0.999282419682) A[2]:(0.000504678173456) A[3]:(6.45282034384e-06)\n",
      " state (10)  A[0]:(8.07339310995e-05) A[1]:(0.99966353178) A[2]:(0.000253401201917) A[3]:(2.3060320018e-06)\n",
      " state (11)  A[0]:(4.8657246225e-05) A[1]:(0.99977594614) A[2]:(0.000174069413333) A[3]:(1.31586341467e-06)\n",
      " state (12)  A[0]:(3.66732456314e-05) A[1]:(0.999821424484) A[2]:(0.000140927091707) A[3]:(9.596470818e-07)\n",
      " state (13)  A[0]:(3.11993317155e-05) A[1]:(0.99984318018) A[2]:(0.000124826852698) A[3]:(8.00454301952e-07)\n",
      " state (14)  A[0]:(2.84053894575e-05) A[1]:(0.999854564667) A[2]:(0.00011631657253) A[3]:(7.20231980722e-07)\n",
      " state (15)  A[0]:(2.68855674221e-05) A[1]:(0.999860823154) A[2]:(0.000111592846224) A[3]:(6.7692161565e-07)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 115000 finished after 10 . Running score: 0.04. Policy_loss: -95601.7511992, Value_loss: 1.21949766671. Times trained:               8592. Times reached goal: 50.               Steps done: 883866.\n",
      " state (0)  A[0]:(0.218389555812) A[1]:(0.185317739844) A[2]:(0.193132340908) A[3]:(0.403160333633)\n",
      " state (1)  A[0]:(0.232165411115) A[1]:(0.187011271715) A[2]:(0.190459594131) A[3]:(0.390363752842)\n",
      " state (2)  A[0]:(0.291453957558) A[1]:(0.195803329349) A[2]:(0.183397129178) A[3]:(0.329345613718)\n",
      " state (3)  A[0]:(0.442841023207) A[1]:(0.197756424546) A[2]:(0.155771970749) A[3]:(0.2036306113)\n",
      " state (4)  A[0]:(0.592847883701) A[1]:(0.18867470324) A[2]:(0.118054561317) A[3]:(0.100422896445)\n",
      " state (5)  A[0]:(0.655362665653) A[1]:(0.198694244027) A[2]:(0.0933397635818) A[3]:(0.0526033490896)\n",
      " state (6)  A[0]:(0.612662494183) A[1]:(0.281577408314) A[2]:(0.0777332112193) A[3]:(0.0280268900096)\n",
      " state (7)  A[0]:(0.322656363249) A[1]:(0.615841448307) A[2]:(0.0519641526043) A[3]:(0.00953802838922)\n",
      " state (8)  A[0]:(0.0539586097002) A[1]:(0.929834485054) A[2]:(0.015001822263) A[3]:(0.00120508321561)\n",
      " state (9)  A[0]:(0.00753678567708) A[1]:(0.988702237606) A[2]:(0.00361847993918) A[3]:(0.000142523771501)\n",
      " state (10)  A[0]:(0.00166030589025) A[1]:(0.997108459473) A[2]:(0.00120321440045) A[3]:(2.80087788269e-05)\n",
      " state (11)  A[0]:(0.000587789865676) A[1]:(0.998842179775) A[2]:(0.000560911546927) A[3]:(9.10145354283e-06)\n",
      " state (12)  A[0]:(0.000296564830933) A[1]:(0.999361753464) A[2]:(0.000337400386343) A[3]:(4.30741101809e-06)\n",
      " state (13)  A[0]:(0.000189783822862) A[1]:(0.999566197395) A[2]:(0.000241396424826) A[3]:(2.63144443124e-06)\n",
      " state (14)  A[0]:(0.000141665615956) A[1]:(0.999662876129) A[2]:(0.000193536659935) A[3]:(1.90070079498e-06)\n",
      " state (15)  A[0]:(0.00011677317525) A[1]:(0.999714612961) A[2]:(0.000167108824826) A[3]:(1.53113262513e-06)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 116000 finished after 14 . Running score: 0.1. Policy_loss: -95601.8435684, Value_loss: 1.01244637579. Times trained:               8786. Times reached goal: 61.               Steps done: 892652.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.220885962248) A[1]:(0.186400324106) A[2]:(0.194981530309) A[3]:(0.397732198238)\n",
      " state (1)  A[0]:(0.23313331604) A[1]:(0.186173841357) A[2]:(0.191868454218) A[3]:(0.388824343681)\n",
      " state (2)  A[0]:(0.288325250149) A[1]:(0.193302854896) A[2]:(0.186571449041) A[3]:(0.331800460815)\n",
      " state (3)  A[0]:(0.43108895421) A[1]:(0.193165570498) A[2]:(0.162962034345) A[3]:(0.212783426046)\n",
      " state (4)  A[0]:(0.589436948299) A[1]:(0.178497523069) A[2]:(0.125005841255) A[3]:(0.107059665024)\n",
      " state (5)  A[0]:(0.668850243092) A[1]:(0.175790458918) A[2]:(0.0982662215829) A[3]:(0.0570931248367)\n",
      " state (6)  A[0]:(0.669182837009) A[1]:(0.215163618326) A[2]:(0.0825892835855) A[3]:(0.0330642834306)\n",
      " state (7)  A[0]:(0.490277588367) A[1]:(0.427066445351) A[2]:(0.0669929459691) A[3]:(0.0156630109996)\n",
      " state (8)  A[0]:(0.11291217804) A[1]:(0.859166324139) A[2]:(0.0253198295832) A[3]:(0.00260164029896)\n",
      " state (9)  A[0]:(0.0125009482726) A[1]:(0.982139885426) A[2]:(0.00512406090274) A[3]:(0.000235104802414)\n",
      " state (10)  A[0]:(0.00217092386447) A[1]:(0.99637144804) A[2]:(0.00142168602906) A[3]:(3.59530022251e-05)\n",
      " state (11)  A[0]:(0.000662382692099) A[1]:(0.99873405695) A[2]:(0.00059350009542) A[3]:(1.00449051388e-05)\n",
      " state (12)  A[0]:(0.000304925983073) A[1]:(0.999356806278) A[2]:(0.000333939213306) A[3]:(4.33941340816e-06)\n",
      " state (13)  A[0]:(0.000183858064702) A[1]:(0.999584794044) A[2]:(0.000228869190323) A[3]:(2.49938216257e-06)\n",
      " state (14)  A[0]:(0.000131752996822) A[1]:(0.999688327312) A[2]:(0.000178169342689) A[3]:(1.73350940713e-06)\n",
      " state (15)  A[0]:(0.000105475686723) A[1]:(0.999742567539) A[2]:(0.000150620253407) A[3]:(1.35604580009e-06)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 117000 finished after 11 . Running score: 0.05. Policy_loss: -95601.5510409, Value_loss: 1.42437198641. Times trained:               9452. Times reached goal: 68.               Steps done: 902104.\n",
      " state (0)  A[0]:(0.216038197279) A[1]:(0.185934141278) A[2]:(0.195688605309) A[3]:(0.402339041233)\n",
      " state (1)  A[0]:(0.224121585488) A[1]:(0.185179546475) A[2]:(0.192999154329) A[3]:(0.397699713707)\n",
      " state (2)  A[0]:(0.263138353825) A[1]:(0.192420527339) A[2]:(0.1906799227) A[3]:(0.353761166334)\n",
      " state (3)  A[0]:(0.370366543531) A[1]:(0.198696762323) A[2]:(0.177002161741) A[3]:(0.253934532404)\n",
      " state (4)  A[0]:(0.523975372314) A[1]:(0.191669762135) A[2]:(0.144881308079) A[3]:(0.139473512769)\n",
      " state (5)  A[0]:(0.622688949108) A[1]:(0.187743350863) A[2]:(0.115262642503) A[3]:(0.0743050351739)\n",
      " state (6)  A[0]:(0.642743587494) A[1]:(0.218072131276) A[2]:(0.0966158881783) A[3]:(0.0425683669746)\n",
      " state (7)  A[0]:(0.511677742004) A[1]:(0.38528239727) A[2]:(0.08147123456) A[3]:(0.0215686354786)\n",
      " state (8)  A[0]:(0.14280615747) A[1]:(0.815552532673) A[2]:(0.0372950322926) A[3]:(0.00434630410746)\n",
      " state (9)  A[0]:(0.0136284762993) A[1]:(0.978783905506) A[2]:(0.00724632386118) A[3]:(0.000341297942214)\n",
      " state (10)  A[0]:(0.00179191923235) A[1]:(0.996447265148) A[2]:(0.00172096933238) A[3]:(3.98193769797e-05)\n",
      " state (11)  A[0]:(0.000441006559413) A[1]:(0.998916387558) A[2]:(0.000633596966509) A[3]:(9.00232680578e-06)\n",
      " state (12)  A[0]:(0.000176303423359) A[1]:(0.999492585659) A[2]:(0.000327726884279) A[3]:(3.37566530106e-06)\n",
      " state (13)  A[0]:(9.72298439592e-05) A[1]:(0.999688208103) A[2]:(0.000212790575461) A[3]:(1.77447952865e-06)\n",
      " state (14)  A[0]:(6.58466815366e-05) A[1]:(0.999772965908) A[2]:(0.000160005074576) A[3]:(1.16019930374e-06)\n",
      " state (15)  A[0]:(5.08336343046e-05) A[1]:(0.999816060066) A[2]:(0.000132253146148) A[3]:(8.73235819654e-07)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 118000 finished after 15 . Running score: 0.03. Policy_loss: -95603.2075433, Value_loss: 1.2146007708. Times trained:               8919. Times reached goal: 30.               Steps done: 911023.\n",
      " state (0)  A[0]:(0.221214964986) A[1]:(0.180447399616) A[2]:(0.188839435577) A[3]:(0.409498184919)\n",
      " state (1)  A[0]:(0.258388131857) A[1]:(0.181501805782) A[2]:(0.183442130685) A[3]:(0.376667916775)\n",
      " state (2)  A[0]:(0.451948404312) A[1]:(0.17100289464) A[2]:(0.151620313525) A[3]:(0.225428387523)\n",
      " state (3)  A[0]:(0.70679038763) A[1]:(0.128281757236) A[2]:(0.0901994109154) A[3]:(0.0747284367681)\n",
      " state (4)  A[0]:(0.775351285934) A[1]:(0.124539516866) A[2]:(0.0665908455849) A[3]:(0.033518332988)\n",
      " state (5)  A[0]:(0.743754565716) A[1]:(0.176565736532) A[2]:(0.059711907059) A[3]:(0.0199678186327)\n",
      " state (6)  A[0]:(0.440210014582) A[1]:(0.502228975296) A[2]:(0.0490012578666) A[3]:(0.00855976529419)\n",
      " state (7)  A[0]:(0.018751906231) A[1]:(0.975436449051) A[2]:(0.00554060610011) A[3]:(0.000271065160632)\n",
      " state (8)  A[0]:(0.00052514456911) A[1]:(0.99907630682) A[2]:(0.000392581743654) A[3]:(5.97277812631e-06)\n",
      " state (9)  A[0]:(7.96031599748e-05) A[1]:(0.999823391438) A[2]:(9.62445337791e-05) A[3]:(7.88583633948e-07)\n",
      " state (10)  A[0]:(3.11488656735e-05) A[1]:(0.999920964241) A[2]:(4.76040950161e-05) A[3]:(2.85433799263e-07)\n",
      " state (11)  A[0]:(1.90181162907e-05) A[1]:(0.99994802475) A[2]:(3.27897978423e-05) A[3]:(1.66477832408e-07)\n",
      " state (12)  A[0]:(1.44933001138e-05) A[1]:(0.999958693981) A[2]:(2.66734059551e-05) A[3]:(1.23457951418e-07)\n",
      " state (13)  A[0]:(1.24206371765e-05) A[1]:(0.999963760376) A[2]:(2.37112053583e-05) A[3]:(1.0409211626e-07)\n",
      " state (14)  A[0]:(1.13577107186e-05) A[1]:(0.99996638298) A[2]:(2.21433583647e-05) A[3]:(9.42603364251e-08)\n",
      " state (15)  A[0]:(1.07760297396e-05) A[1]:(0.999967873096) A[2]:(2.12695158552e-05) A[3]:(8.89115270297e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 119000 finished after 33 . Running score: 0.1. Policy_loss: -95601.5138997, Value_loss: 1.43237083074. Times trained:               9533. Times reached goal: 70.               Steps done: 920556.\n",
      "action_dist \n",
      "tensor([[ 0.2090,  0.1750,  0.1862,  0.4299]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.7723,  0.1226,  0.0676,  0.0375]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3912e-04,  9.9942e-01,  3.3286e-04,  4.3803e-06]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.208964154124) A[1]:(0.174965962768) A[2]:(0.186155870557) A[3]:(0.429913967848)\n",
      " state (1)  A[0]:(0.229089856148) A[1]:(0.176588714123) A[2]:(0.183282017708) A[3]:(0.411039382219)\n",
      " state (2)  A[0]:(0.357547402382) A[1]:(0.179084718227) A[2]:(0.167302921414) A[3]:(0.296064972878)\n",
      " state (3)  A[0]:(0.657538235188) A[1]:(0.137969374657) A[2]:(0.102728523314) A[3]:(0.101763889194)\n",
      " state (4)  A[0]:(0.772235274315) A[1]:(0.12260453403) A[2]:(0.0676225945354) A[3]:(0.0375375710428)\n",
      " state (5)  A[0]:(0.743971288204) A[1]:(0.175452187657) A[2]:(0.0597445331514) A[3]:(0.0208320003003)\n",
      " state (6)  A[0]:(0.405324786901) A[1]:(0.535035908222) A[2]:(0.0507967881858) A[3]:(0.00884253904223)\n",
      " state (7)  A[0]:(0.0111574307084) A[1]:(0.983661353588) A[2]:(0.00495861144736) A[3]:(0.000222590868361)\n",
      " state (8)  A[0]:(0.000239254441112) A[1]:(0.999423384666) A[2]:(0.000332983850967) A[3]:(4.3817722144e-06)\n",
      " state (9)  A[0]:(3.42512794305e-05) A[1]:(0.999881982803) A[2]:(8.3195809566e-05) A[3]:(5.8192307506e-07)\n",
      " state (10)  A[0]:(1.3515315004e-05) A[1]:(0.999943733215) A[2]:(4.25302787335e-05) A[3]:(2.18501398308e-07)\n",
      " state (11)  A[0]:(8.40127086121e-06) A[1]:(0.99996137619) A[2]:(3.0088604035e-05) A[3]:(1.31699451345e-07)\n",
      " state (12)  A[0]:(6.49707999401e-06) A[1]:(0.999968469143) A[2]:(2.49254808296e-05) A[3]:(9.99570133331e-08)\n",
      " state (13)  A[0]:(5.62490822631e-06) A[1]:(0.999971866608) A[2]:(2.24184332183e-05) A[3]:(8.55672297462e-08)\n",
      " state (14)  A[0]:(5.17816806678e-06) A[1]:(0.999973654747) A[2]:(2.1091544113e-05) A[3]:(7.82402835853e-08)\n",
      " state (15)  A[0]:(4.93436027682e-06) A[1]:(0.999974608421) A[2]:(2.03536656045e-05) A[3]:(7.42551478083e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 120000 finished after 3 . Running score: 0.14. Policy_loss: -95601.5204147, Value_loss: 1.01517885838. Times trained:               9308. Times reached goal: 80.               Steps done: 929864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.212673306465) A[1]:(0.180106684566) A[2]:(0.188386455178) A[3]:(0.418833553791)\n",
      " state (1)  A[0]:(0.231580585241) A[1]:(0.180048674345) A[2]:(0.184763908386) A[3]:(0.403606832027)\n",
      " state (2)  A[0]:(0.354471623898) A[1]:(0.183323547244) A[2]:(0.171143010259) A[3]:(0.2910618186)\n",
      " state (3)  A[0]:(0.660218596458) A[1]:(0.136668562889) A[2]:(0.105257183313) A[3]:(0.0978556424379)\n",
      " state (4)  A[0]:(0.795759141445) A[1]:(0.105817720294) A[2]:(0.064510948956) A[3]:(0.0339121967554)\n",
      " state (5)  A[0]:(0.806876420975) A[1]:(0.120107829571) A[2]:(0.0539060346782) A[3]:(0.0191097259521)\n",
      " state (6)  A[0]:(0.689018785954) A[1]:(0.244823411107) A[2]:(0.0543280728161) A[3]:(0.0118297617882)\n",
      " state (7)  A[0]:(0.141937747598) A[1]:(0.830062150955) A[2]:(0.0260665323585) A[3]:(0.00193359167315)\n",
      " state (8)  A[0]:(0.00283154263161) A[1]:(0.995265483856) A[2]:(0.00187137210742) A[3]:(3.15992438118e-05)\n",
      " state (9)  A[0]:(0.000179156224476) A[1]:(0.999540627003) A[2]:(0.000278463179711) A[3]:(1.72624254446e-06)\n",
      " state (10)  A[0]:(4.09864151152e-05) A[1]:(0.999859035015) A[2]:(9.96248709271e-05) A[3]:(3.58836985015e-07)\n",
      " state (11)  A[0]:(1.85252665688e-05) A[1]:(0.999924361706) A[2]:(5.69696712773e-05) A[3]:(1.5250883223e-07)\n",
      " state (12)  A[0]:(1.18587850011e-05) A[1]:(0.999946534634) A[2]:(4.15081558458e-05) A[3]:(9.38614448387e-08)\n",
      " state (13)  A[0]:(9.14516567718e-06) A[1]:(0.999956309795) A[2]:(3.44759901054e-05) A[3]:(7.05958100866e-08)\n",
      " state (14)  A[0]:(7.82843289926e-06) A[1]:(0.999961256981) A[2]:(3.08367270918e-05) A[3]:(5.94811560006e-08)\n",
      " state (15)  A[0]:(7.12036035111e-06) A[1]:(0.999963998795) A[2]:(2.8802858651e-05) A[3]:(5.35613011721e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 121000 finished after 15 . Running score: 0.06. Policy_loss: -95601.6107361, Value_loss: 1.42978587967. Times trained:               9600. Times reached goal: 75.               Steps done: 939464.\n",
      " state (0)  A[0]:(0.206531494856) A[1]:(0.180013328791) A[2]:(0.187719687819) A[3]:(0.425735473633)\n",
      " state (1)  A[0]:(0.222836524248) A[1]:(0.179127067327) A[2]:(0.184120938182) A[3]:(0.413915485144)\n",
      " state (2)  A[0]:(0.347863554955) A[1]:(0.182333171368) A[2]:(0.17166377604) A[3]:(0.298139542341)\n",
      " state (3)  A[0]:(0.666629135609) A[1]:(0.134276807308) A[2]:(0.103950329125) A[3]:(0.0951437205076)\n",
      " state (4)  A[0]:(0.790086209774) A[1]:(0.111860640347) A[2]:(0.0652689859271) A[3]:(0.0327841304243)\n",
      " state (5)  A[0]:(0.759469270706) A[1]:(0.164845988154) A[2]:(0.0578537508845) A[3]:(0.0178309883922)\n",
      " state (6)  A[0]:(0.368486911058) A[1]:(0.576217889786) A[2]:(0.0487600304186) A[3]:(0.00653517525643)\n",
      " state (7)  A[0]:(0.00711712799966) A[1]:(0.988852620125) A[2]:(0.00392284756526) A[3]:(0.000107396772364)\n",
      " state (8)  A[0]:(0.000163549964782) A[1]:(0.999538898468) A[2]:(0.000295426056255) A[3]:(2.13929297388e-06)\n",
      " state (9)  A[0]:(2.81088578049e-05) A[1]:(0.99988424778) A[2]:(8.72896562214e-05) A[3]:(3.36333954465e-07)\n",
      " state (10)  A[0]:(1.23341069411e-05) A[1]:(0.999938368797) A[2]:(4.91423488711e-05) A[3]:(1.40294432072e-07)\n",
      " state (11)  A[0]:(8.04610408522e-06) A[1]:(0.99995547533) A[2]:(3.64049119526e-05) A[3]:(8.87940032612e-08)\n",
      " state (12)  A[0]:(6.34795605947e-06) A[1]:(0.999962806702) A[2]:(3.07929549308e-05) A[3]:(6.87697934154e-08)\n",
      " state (13)  A[0]:(5.53471545572e-06) A[1]:(0.999966442585) A[2]:(2.7940621294e-05) A[3]:(5.92796354226e-08)\n",
      " state (14)  A[0]:(5.10179597768e-06) A[1]:(0.999968469143) A[2]:(2.6368717954e-05) A[3]:(5.42580629315e-08)\n",
      " state (15)  A[0]:(4.85632608616e-06) A[1]:(0.999969661236) A[2]:(2.54587266681e-05) A[3]:(5.14208551294e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 122000 finished after 4 . Running score: 0.02. Policy_loss: -95601.6176625, Value_loss: 1.22092648407. Times trained:               9431. Times reached goal: 51.               Steps done: 948895.\n",
      " state (0)  A[0]:(0.211041808128) A[1]:(0.185953617096) A[2]:(0.192601144314) A[3]:(0.410403460264)\n",
      " state (1)  A[0]:(0.218898937106) A[1]:(0.181145370007) A[2]:(0.187625989318) A[3]:(0.412329673767)\n",
      " state (2)  A[0]:(0.293970644474) A[1]:(0.189479500055) A[2]:(0.185460865498) A[3]:(0.331088960171)\n",
      " state (3)  A[0]:(0.560572087765) A[1]:(0.160350471735) A[2]:(0.139170318842) A[3]:(0.139907151461)\n",
      " state (4)  A[0]:(0.785866260529) A[1]:(0.103522062302) A[2]:(0.0736268311739) A[3]:(0.0369848608971)\n",
      " state (5)  A[0]:(0.820003986359) A[1]:(0.109182976186) A[2]:(0.0543994046748) A[3]:(0.0164136439562)\n",
      " state (6)  A[0]:(0.646263837814) A[1]:(0.290474265814) A[2]:(0.0549193136394) A[3]:(0.00834255572408)\n",
      " state (7)  A[0]:(0.0498363785446) A[1]:(0.936103880405) A[2]:(0.013592694886) A[3]:(0.000467031728476)\n",
      " state (8)  A[0]:(0.000729464984033) A[1]:(0.998483538628) A[2]:(0.000781775917858) A[3]:(5.22160507899e-06)\n",
      " state (9)  A[0]:(7.63714851928e-05) A[1]:(0.999756038189) A[2]:(0.000167146979948) A[3]:(4.67304289486e-07)\n",
      " state (10)  A[0]:(2.66220285994e-05) A[1]:(0.999892175198) A[2]:(8.10255587567e-05) A[3]:(1.50037081426e-07)\n",
      " state (11)  A[0]:(1.55770958372e-05) A[1]:(0.999928355217) A[2]:(5.59587606404e-05) A[3]:(8.38508000811e-08)\n",
      " state (12)  A[0]:(1.16201572382e-05) A[1]:(0.999942660332) A[2]:(4.56643683719e-05) A[3]:(6.08916295164e-08)\n",
      " state (13)  A[0]:(9.82430628937e-06) A[1]:(0.999949514866) A[2]:(4.06289145758e-05) A[3]:(5.06572064296e-08)\n",
      " state (14)  A[0]:(8.89729108167e-06) A[1]:(0.999953150749) A[2]:(3.79151897505e-05) A[3]:(4.54285711271e-08)\n",
      " state (15)  A[0]:(8.38142022985e-06) A[1]:(0.999955236912) A[2]:(3.63661129086e-05) A[3]:(4.2537152467e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 123000 finished after 8 . Running score: 0.09. Policy_loss: -95601.6193878, Value_loss: 1.00476135115. Times trained:               9975. Times reached goal: 61.               Steps done: 958870.\n",
      " state (0)  A[0]:(0.210737302899) A[1]:(0.187041029334) A[2]:(0.191539689898) A[3]:(0.410681962967)\n",
      " state (1)  A[0]:(0.219358205795) A[1]:(0.180368021131) A[2]:(0.185195669532) A[3]:(0.41507807374)\n",
      " state (2)  A[0]:(0.313827216625) A[1]:(0.186133891344) A[2]:(0.179955720901) A[3]:(0.320083200932)\n",
      " state (3)  A[0]:(0.625419437885) A[1]:(0.139152616262) A[2]:(0.119183629751) A[3]:(0.116244271398)\n",
      " state (4)  A[0]:(0.821781814098) A[1]:(0.0844926759601) A[2]:(0.061280220747) A[3]:(0.0324453040957)\n",
      " state (5)  A[0]:(0.861495435238) A[1]:(0.0769707486033) A[2]:(0.0451499707997) A[3]:(0.01638382487)\n",
      " state (6)  A[0]:(0.831601381302) A[1]:(0.114619612694) A[2]:(0.0430074818432) A[3]:(0.0107715269551)\n",
      " state (7)  A[0]:(0.512476146221) A[1]:(0.438947290182) A[2]:(0.0435617491603) A[3]:(0.00501475902274)\n",
      " state (8)  A[0]:(0.0319558754563) A[1]:(0.959600090981) A[2]:(0.0081930346787) A[3]:(0.000250983139267)\n",
      " state (9)  A[0]:(0.00160025490914) A[1]:(0.997301578522) A[2]:(0.00108706858009) A[3]:(1.10738692456e-05)\n",
      " state (10)  A[0]:(0.000254449754721) A[1]:(0.999429881573) A[2]:(0.000314036326017) A[3]:(1.63828394761e-06)\n",
      " state (11)  A[0]:(8.68006973178e-05) A[1]:(0.999761104584) A[2]:(0.00015156295558) A[3]:(5.33020227067e-07)\n",
      " state (12)  A[0]:(4.55515255453e-05) A[1]:(0.999856472015) A[2]:(9.77000236162e-05) A[3]:(2.70660706292e-07)\n",
      " state (13)  A[0]:(3.05658795696e-05) A[1]:(0.999894917011) A[2]:(7.43478303775e-05) A[3]:(1.77482178287e-07)\n",
      " state (14)  A[0]:(2.37111489696e-05) A[1]:(0.999913692474) A[2]:(6.24350577709e-05) A[3]:(1.35487184139e-07)\n",
      " state (15)  A[0]:(2.00951781153e-05) A[1]:(0.999924063683) A[2]:(5.56990671612e-05) A[3]:(1.13551820391e-07)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 124000 finished after 7 . Running score: 0.04. Policy_loss: -95601.6247301, Value_loss: 1.01358499206. Times trained:               9681. Times reached goal: 56.               Steps done: 968551.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.2287,  0.2018,  0.1976,  0.3719]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8511,  0.0881,  0.0476,  0.0132]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.9982e-05,  9.9987e-01,  1.0981e-04,  7.2364e-08]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.4712e-06,  9.9993e-01,  5.7699e-05,  2.4345e-08]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.0634e-06,  9.9995e-01,  4.4710e-05,  1.5785e-08]])\n",
      "On state=10, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.5948e-06,  9.9996e-01,  3.5716e-05,  1.0768e-08]])\n",
      "On state=14, selected action=1\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.228718653321) A[1]:(0.201763600111) A[2]:(0.197643429041) A[3]:(0.371874302626)\n",
      " state (1)  A[0]:(0.245112448931) A[1]:(0.190546005964) A[2]:(0.187547713518) A[3]:(0.376793831587)\n",
      " state (2)  A[0]:(0.420339852571) A[1]:(0.188418850303) A[2]:(0.169382631779) A[3]:(0.221858665347)\n",
      " state (3)  A[0]:(0.770406126976) A[1]:(0.106032244861) A[2]:(0.0798319503665) A[3]:(0.0437296405435)\n",
      " state (4)  A[0]:(0.8513058424) A[1]:(0.087934538722) A[2]:(0.0475487820804) A[3]:(0.0132108461112)\n",
      " state (5)  A[0]:(0.726168692112) A[1]:(0.216038942337) A[2]:(0.0510069914162) A[3]:(0.00678536854684)\n",
      " state (6)  A[0]:(0.0789911076427) A[1]:(0.89945679903) A[2]:(0.0210145227611) A[3]:(0.000537590065505)\n",
      " state (7)  A[0]:(0.000353096693289) A[1]:(0.998940050602) A[2]:(0.000705201702658) A[3]:(1.65807921348e-06)\n",
      " state (8)  A[0]:(2.01222919713e-05) A[1]:(0.999869585037) A[2]:(0.000110211178253) A[3]:(7.28492040025e-08)\n",
      " state (9)  A[0]:(7.4950621638e-06) A[1]:(0.999934732914) A[2]:(5.77758764848e-05) A[3]:(2.4413816746e-08)\n",
      " state (10)  A[0]:(5.07100730829e-06) A[1]:(0.999950170517) A[2]:(4.47280363005e-05) A[3]:(1.58044723975e-08)\n",
      " state (11)  A[0]:(4.23636765845e-06) A[1]:(0.999956011772) A[2]:(3.97561925638e-05) A[3]:(1.29322703657e-08)\n",
      " state (12)  A[0]:(3.87051022699e-06) A[1]:(0.999958634377) A[2]:(3.74720621039e-05) A[3]:(1.16920135795e-08)\n",
      " state (13)  A[0]:(3.6905394154e-06) A[1]:(0.999960005283) A[2]:(3.63216786354e-05) A[3]:(1.10865476799e-08)\n",
      " state (14)  A[0]:(3.59641785508e-06) A[1]:(0.999960660934) A[2]:(3.57127137249e-05) A[3]:(1.07712709863e-08)\n",
      " state (15)  A[0]:(3.54542612513e-06) A[1]:(0.999961078167) A[2]:(3.53806099156e-05) A[3]:(1.06009050427e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 125000 finished after 6 . Running score: 0.06. Policy_loss: -95601.6102172, Value_loss: 1.45277701499. Times trained:               9464. Times reached goal: 63.               Steps done: 978015.\n",
      " state (0)  A[0]:(0.22734259069) A[1]:(0.207030519843) A[2]:(0.200775906444) A[3]:(0.364850968122)\n",
      " state (1)  A[0]:(0.231794640422) A[1]:(0.190756365657) A[2]:(0.18843537569) A[3]:(0.389013648033)\n",
      " state (2)  A[0]:(0.347721785307) A[1]:(0.196557089686) A[2]:(0.18198543787) A[3]:(0.273735672235)\n",
      " state (3)  A[0]:(0.664959192276) A[1]:(0.139467462897) A[2]:(0.112825624645) A[3]:(0.0827476978302)\n",
      " state (4)  A[0]:(0.815939545631) A[1]:(0.0998821556568) A[2]:(0.0617609322071) A[3]:(0.0224173553288)\n",
      " state (5)  A[0]:(0.7620703578) A[1]:(0.170191526413) A[2]:(0.0571389086545) A[3]:(0.0105991940945)\n",
      " state (6)  A[0]:(0.268149316311) A[1]:(0.679904997349) A[2]:(0.0492137223482) A[3]:(0.00273195817135)\n",
      " state (7)  A[0]:(0.00369129399769) A[1]:(0.992038965225) A[2]:(0.00423953635618) A[3]:(3.02218213619e-05)\n",
      " state (8)  A[0]:(6.86428829795e-05) A[1]:(0.999570429325) A[2]:(0.000360465433914) A[3]:(4.40593652229e-07)\n",
      " state (9)  A[0]:(1.25162669065e-05) A[1]:(0.999863922596) A[2]:(0.000123467878439) A[3]:(6.96816471191e-08)\n",
      " state (10)  A[0]:(6.43334715278e-06) A[1]:(0.999912440777) A[2]:(8.10864876257e-05) A[3]:(3.36465504347e-08)\n",
      " state (11)  A[0]:(4.68812959298e-06) A[1]:(0.999928891659) A[2]:(6.64034159854e-05) A[3]:(2.3775598379e-08)\n",
      " state (12)  A[0]:(3.92309084418e-06) A[1]:(0.999936699867) A[2]:(5.93427830609e-05) A[3]:(1.9547096386e-08)\n",
      " state (13)  A[0]:(3.50946538674e-06) A[1]:(0.999941170216) A[2]:(5.53124336875e-05) A[3]:(1.72905831874e-08)\n",
      " state (14)  A[0]:(3.25856171912e-06) A[1]:(0.999943971634) A[2]:(5.27804477315e-05) A[3]:(1.59328568117e-08)\n",
      " state (15)  A[0]:(3.0956300634e-06) A[1]:(0.999945819378) A[2]:(5.10966856382e-05) A[3]:(1.50560648393e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 126000 finished after 19 . Running score: 0.05. Policy_loss: -95601.607078, Value_loss: 1.22646718417. Times trained:               9081. Times reached goal: 57.               Steps done: 987096.\n",
      " state (0)  A[0]:(0.225103422999) A[1]:(0.205665394664) A[2]:(0.201365500689) A[3]:(0.367865711451)\n",
      " state (1)  A[0]:(0.226022139192) A[1]:(0.186121150851) A[2]:(0.186944276094) A[3]:(0.400912433863)\n",
      " state (2)  A[0]:(0.336617738008) A[1]:(0.189126044512) A[2]:(0.182344168425) A[3]:(0.291912078857)\n",
      " state (3)  A[0]:(0.644259929657) A[1]:(0.136100277305) A[2]:(0.120789200068) A[3]:(0.0988505855203)\n",
      " state (4)  A[0]:(0.809259831905) A[1]:(0.0938259512186) A[2]:(0.0685509964824) A[3]:(0.0283632557839)\n",
      " state (5)  A[0]:(0.797375798225) A[1]:(0.12887366116) A[2]:(0.0603155083954) A[3]:(0.013435059227)\n",
      " state (6)  A[0]:(0.493202269077) A[1]:(0.431333869696) A[2]:(0.0695241838694) A[3]:(0.0059396927245)\n",
      " state (7)  A[0]:(0.0297075025737) A[1]:(0.951222300529) A[2]:(0.0187677703798) A[3]:(0.000302448315779)\n",
      " state (8)  A[0]:(0.000484419113491) A[1]:(0.997830569744) A[2]:(0.00168088206556) A[3]:(4.11230575992e-06)\n",
      " state (9)  A[0]:(4.59751208837e-05) A[1]:(0.999546587467) A[2]:(0.000407072948292) A[3]:(3.34890728482e-07)\n",
      " state (10)  A[0]:(1.73300322786e-05) A[1]:(0.999757170677) A[2]:(0.000225379742915) A[3]:(1.16925072291e-07)\n",
      " state (11)  A[0]:(1.09502398118e-05) A[1]:(0.999818265438) A[2]:(0.000170725950738) A[3]:(7.11171566081e-08)\n",
      " state (12)  A[0]:(8.39563290356e-06) A[1]:(0.999846100807) A[2]:(0.00014542262943) A[3]:(5.33088631016e-08)\n",
      " state (13)  A[0]:(7.02317493051e-06) A[1]:(0.999862372875) A[2]:(0.000130566899315) A[3]:(4.39088054804e-08)\n",
      " state (14)  A[0]:(6.15867338638e-06) A[1]:(0.999873220921) A[2]:(0.000120606564451) A[3]:(3.80584914694e-08)\n",
      " state (15)  A[0]:(5.56183522349e-06) A[1]:(0.999881029129) A[2]:(0.000113397232781) A[3]:(3.40554713318e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 127000 finished after 9 . Running score: 0.03. Policy_loss: -95601.6119844, Value_loss: 1.24350969719. Times trained:               9429. Times reached goal: 55.               Steps done: 996525.\n",
      " state (0)  A[0]:(0.233934670687) A[1]:(0.208017989993) A[2]:(0.200152501464) A[3]:(0.357894837856)\n",
      " state (1)  A[0]:(0.239803642035) A[1]:(0.186723738909) A[2]:(0.183628708124) A[3]:(0.38984388113)\n",
      " state (2)  A[0]:(0.405747622252) A[1]:(0.185948103666) A[2]:(0.168158695102) A[3]:(0.240145549178)\n",
      " state (3)  A[0]:(0.753793120384) A[1]:(0.112400569022) A[2]:(0.083106726408) A[3]:(0.0506996102631)\n",
      " state (4)  A[0]:(0.824208796024) A[1]:(0.111640781164) A[2]:(0.0505786649883) A[3]:(0.0135717839003)\n",
      " state (5)  A[0]:(0.512816369534) A[1]:(0.430320590734) A[2]:(0.0522710345685) A[3]:(0.00459195068106)\n",
      " state (6)  A[0]:(0.0145644694567) A[1]:(0.97788220644) A[2]:(0.00746461376548) A[3]:(8.86972484295e-05)\n",
      " state (7)  A[0]:(0.000130817410536) A[1]:(0.999494254589) A[2]:(0.00037435698323) A[3]:(5.41363533557e-07)\n",
      " state (8)  A[0]:(1.20028580568e-05) A[1]:(0.999908864498) A[2]:(7.9102908785e-05) A[3]:(3.84556351207e-08)\n",
      " state (9)  A[0]:(5.27464453626e-06) A[1]:(0.999948561192) A[2]:(4.61468152935e-05) A[3]:(1.52753365512e-08)\n",
      " state (10)  A[0]:(3.81346330869e-06) A[1]:(0.99995881319) A[2]:(3.73356597265e-05) A[3]:(1.06030242364e-08)\n",
      " state (11)  A[0]:(3.23699555338e-06) A[1]:(0.999963223934) A[2]:(3.35580989486e-05) A[3]:(8.81769324224e-09)\n",
      " state (12)  A[0]:(2.93822654385e-06) A[1]:(0.999965548515) A[2]:(3.15108336508e-05) A[3]:(7.90752530122e-09)\n",
      " state (13)  A[0]:(2.76162086266e-06) A[1]:(0.999966979027) A[2]:(3.02665739582e-05) A[3]:(7.37467065193e-09)\n",
      " state (14)  A[0]:(2.64981372311e-06) A[1]:(0.999967873096) A[2]:(2.94643687084e-05) A[3]:(7.03938241031e-09)\n",
      " state (15)  A[0]:(2.57622241406e-06) A[1]:(0.999968469143) A[2]:(2.89298423013e-05) A[3]:(6.81963285842e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 128000 finished after 6 . Running score: 0.09. Policy_loss: -95601.6341728, Value_loss: 1.22412753021. Times trained:               9214. Times reached goal: 70.               Steps done: 1005739.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.218411952257) A[1]:(0.196280837059) A[2]:(0.194878682494) A[3]:(0.390428543091)\n",
      " state (1)  A[0]:(0.215817883611) A[1]:(0.17604957521) A[2]:(0.179257124662) A[3]:(0.428875386715)\n",
      " state (2)  A[0]:(0.316905260086) A[1]:(0.178802669048) A[2]:(0.175001472235) A[3]:(0.329290628433)\n",
      " state (3)  A[0]:(0.629410684109) A[1]:(0.129266709089) A[2]:(0.117238543928) A[3]:(0.124084040523)\n",
      " state (4)  A[0]:(0.824122726917) A[1]:(0.0786279514432) A[2]:(0.0616315342486) A[3]:(0.0356178060174)\n",
      " state (5)  A[0]:(0.863654911518) A[1]:(0.0743922591209) A[2]:(0.0454251021147) A[3]:(0.0165277030319)\n",
      " state (6)  A[0]:(0.811579167843) A[1]:(0.132169052958) A[2]:(0.0464618951082) A[3]:(0.00978988595307)\n",
      " state (7)  A[0]:(0.372952014208) A[1]:(0.576937735081) A[2]:(0.0468795597553) A[3]:(0.00323069025762)\n",
      " state (8)  A[0]:(0.00782230123878) A[1]:(0.986993432045) A[2]:(0.00513469660655) A[3]:(4.95973908983e-05)\n",
      " state (9)  A[0]:(0.000229098412092) A[1]:(0.999196588993) A[2]:(0.000573132652789) A[3]:(1.1527758943e-06)\n",
      " state (10)  A[0]:(4.23100136686e-05) A[1]:(0.999757111073) A[2]:(0.000200401060283) A[3]:(1.88824230918e-07)\n",
      " state (11)  A[0]:(1.88336125575e-05) A[1]:(0.999859988689) A[2]:(0.000121120312542) A[3]:(7.90132759221e-08)\n",
      " state (12)  A[0]:(1.20891263578e-05) A[1]:(0.99989593029) A[2]:(9.19069279917e-05) A[3]:(4.89368936485e-08)\n",
      " state (13)  A[0]:(9.20066941035e-06) A[1]:(0.999913215637) A[2]:(7.75235603214e-05) A[3]:(3.63979069107e-08)\n",
      " state (14)  A[0]:(7.65369804867e-06) A[1]:(0.999923229218) A[2]:(6.91075983923e-05) A[3]:(2.97958688833e-08)\n",
      " state (15)  A[0]:(6.69980045132e-06) A[1]:(0.999929666519) A[2]:(6.35900651105e-05) A[3]:(2.5772868284e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 129000 finished after 3 . Running score: 0.05. Policy_loss: -95601.6173382, Value_loss: 1.63958531232. Times trained:               9608. Times reached goal: 58.               Steps done: 1015347.\n",
      "action_dist \n",
      "tensor([[ 0.2096,  0.1909,  0.1934,  0.4061]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2097,  0.1909,  0.1934,  0.4060]])\n",
      "On state=0, selected action=1\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.6852,  0.1184,  0.1072,  0.0892]])\n",
      "On state=4, selected action=1\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.209777727723) A[1]:(0.191015005112) A[2]:(0.193462431431) A[3]:(0.405744820833)\n",
      " state (1)  A[0]:(0.198276847601) A[1]:(0.170109048486) A[2]:(0.176860198379) A[3]:(0.454753935337)\n",
      " state (2)  A[0]:(0.247441366315) A[1]:(0.177418857813) A[2]:(0.180038705468) A[3]:(0.395101040602)\n",
      " state (3)  A[0]:(0.426445394754) A[1]:(0.168360799551) A[2]:(0.164160743356) A[3]:(0.24103307724)\n",
      " state (4)  A[0]:(0.686657249928) A[1]:(0.118069931865) A[2]:(0.106840744615) A[3]:(0.0884320661426)\n",
      " state (5)  A[0]:(0.807642221451) A[1]:(0.0896381214261) A[2]:(0.0689814910293) A[3]:(0.0337381511927)\n",
      " state (6)  A[0]:(0.814607560635) A[1]:(0.110738322139) A[2]:(0.0582986176014) A[3]:(0.0163555219769)\n",
      " state (7)  A[0]:(0.553034901619) A[1]:(0.373975366354) A[2]:(0.0662008747458) A[3]:(0.00678890012205)\n",
      " state (8)  A[0]:(0.0271827355027) A[1]:(0.956748604774) A[2]:(0.0158310979605) A[3]:(0.000237566127907)\n",
      " state (9)  A[0]:(0.000643047096673) A[1]:(0.997537910938) A[2]:(0.00181438156869) A[3]:(4.64154163637e-06)\n",
      " state (10)  A[0]:(7.89133118815e-05) A[1]:(0.999379396439) A[2]:(0.000541155284736) A[3]:(5.16720717769e-07)\n",
      " state (11)  A[0]:(2.58427844528e-05) A[1]:(0.999689459801) A[2]:(0.000284518144326) A[3]:(1.59948911005e-07)\n",
      " state (12)  A[0]:(1.35388490889e-05) A[1]:(0.999790489674) A[2]:(0.000195864849957) A[3]:(8.07838134165e-08)\n",
      " state (13)  A[0]:(9.02181000129e-06) A[1]:(0.999836146832) A[2]:(0.00015477805573) A[3]:(5.24760572773e-08)\n",
      " state (14)  A[0]:(6.86956127538e-06) A[1]:(0.999861001968) A[2]:(0.000132061963086) A[3]:(3.92213514999e-08)\n",
      " state (15)  A[0]:(5.65863729207e-06) A[1]:(0.999876379967) A[2]:(0.000117915486044) A[3]:(3.1856647098e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 130000 finished after 3 . Running score: 0.06. Policy_loss: -95601.6358153, Value_loss: 1.43913110576. Times trained:               9879. Times reached goal: 57.               Steps done: 1025226.\n",
      " state (0)  A[0]:(0.217018708587) A[1]:(0.197270005941) A[2]:(0.197360575199) A[3]:(0.388350725174)\n",
      " state (1)  A[0]:(0.213145822287) A[1]:(0.174125343561) A[2]:(0.178486764431) A[3]:(0.434242099524)\n",
      " state (2)  A[0]:(0.348233669996) A[1]:(0.177917420864) A[2]:(0.175381705165) A[3]:(0.298467218876)\n",
      " state (3)  A[0]:(0.714444935322) A[1]:(0.109045743942) A[2]:(0.100073821843) A[3]:(0.0764354914427)\n",
      " state (4)  A[0]:(0.85467249155) A[1]:(0.0711437836289) A[2]:(0.0540432147682) A[3]:(0.0201404858381)\n",
      " state (5)  A[0]:(0.8334223032) A[1]:(0.105214782059) A[2]:(0.0512343160808) A[3]:(0.0101285828277)\n",
      " state (6)  A[0]:(0.452598869801) A[1]:(0.476478725672) A[2]:(0.0669020041823) A[3]:(0.00402041943744)\n",
      " state (7)  A[0]:(0.00599482282996) A[1]:(0.985591232777) A[2]:(0.00837046094239) A[3]:(4.35009787907e-05)\n",
      " state (8)  A[0]:(4.77247158415e-05) A[1]:(0.999395251274) A[2]:(0.000556731421966) A[3]:(2.81861133544e-07)\n",
      " state (9)  A[0]:(6.06081766819e-06) A[1]:(0.999822020531) A[2]:(0.000171881431015) A[3]:(3.15504600223e-08)\n",
      " state (10)  A[0]:(2.73277464657e-06) A[1]:(0.9998883605) A[2]:(0.000108914478915) A[3]:(1.34036595156e-08)\n",
      " state (11)  A[0]:(1.87881039437e-06) A[1]:(0.99991029501) A[2]:(8.78260107129e-05) A[3]:(8.9331315678e-09)\n",
      " state (12)  A[0]:(1.52611767135e-06) A[1]:(0.999920547009) A[2]:(7.79173351475e-05) A[3]:(7.12361281074e-09)\n",
      " state (13)  A[0]:(1.3421655467e-06) A[1]:(0.999926328659) A[2]:(7.23509074305e-05) A[3]:(6.19054274509e-09)\n",
      " state (14)  A[0]:(1.23264942431e-06) A[1]:(0.999929904938) A[2]:(6.88765212544e-05) A[3]:(5.63899327233e-09)\n",
      " state (15)  A[0]:(1.16186345167e-06) A[1]:(0.999932289124) A[2]:(6.65580300847e-05) A[3]:(5.28423749202e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 131000 finished after 15 . Running score: 0.12. Policy_loss: -95601.6310546, Value_loss: 1.01668537678. Times trained:               9963. Times reached goal: 92.               Steps done: 1035189.\n",
      " state (0)  A[0]:(0.213291093707) A[1]:(0.203121945262) A[2]:(0.201905980706) A[3]:(0.381680995226)\n",
      " state (1)  A[0]:(0.199222698808) A[1]:(0.176534727216) A[2]:(0.180064484477) A[3]:(0.444178104401)\n",
      " state (2)  A[0]:(0.291513353586) A[1]:(0.195652216673) A[2]:(0.187801748514) A[3]:(0.325032681227)\n",
      " state (3)  A[0]:(0.636684000492) A[1]:(0.15269690752) A[2]:(0.125544026494) A[3]:(0.0850750654936)\n",
      " state (4)  A[0]:(0.750831305981) A[1]:(0.161052629352) A[2]:(0.0734226480126) A[3]:(0.0146933831275)\n",
      " state (5)  A[0]:(0.104844979942) A[1]:(0.845747768879) A[2]:(0.0482275150716) A[3]:(0.00117975659668)\n",
      " state (6)  A[0]:(0.000128905026941) A[1]:(0.998456835747) A[2]:(0.00141300132964) A[3]:(1.26218549212e-06)\n",
      " state (7)  A[0]:(2.74405510936e-06) A[1]:(0.999832570553) A[2]:(0.000164678422152) A[3]:(2.2340497452e-08)\n",
      " state (8)  A[0]:(9.15726332096e-07) A[1]:(0.999911248684) A[2]:(8.78204082255e-05) A[3]:(6.81284628712e-09)\n",
      " state (9)  A[0]:(6.43148553081e-07) A[1]:(0.999927699566) A[2]:(7.16246649972e-05) A[3]:(4.62145521851e-09)\n",
      " state (10)  A[0]:(5.52471817628e-07) A[1]:(0.999933838844) A[2]:(6.55965777696e-05) A[3]:(3.90681398343e-09)\n",
      " state (11)  A[0]:(5.11288760663e-07) A[1]:(0.999936759472) A[2]:(6.27145345788e-05) A[3]:(3.58514573584e-09)\n",
      " state (12)  A[0]:(4.89739193199e-07) A[1]:(0.999938368797) A[2]:(6.11643627053e-05) A[3]:(3.41759043287e-09)\n",
      " state (13)  A[0]:(4.77592550396e-07) A[1]:(0.999939262867) A[2]:(6.0276764998e-05) A[3]:(3.32341887344e-09)\n",
      " state (14)  A[0]:(4.70439999845e-07) A[1]:(0.999939799309) A[2]:(5.97492253291e-05) A[3]:(3.2680600448e-09)\n",
      " state (15)  A[0]:(4.66107024977e-07) A[1]:(0.999940097332) A[2]:(5.94278790231e-05) A[3]:(3.23458593243e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 132000 finished after 13 . Running score: 0.06. Policy_loss: -95601.5262724, Value_loss: 1.21842602806. Times trained:               9609. Times reached goal: 66.               Steps done: 1044798.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.212503999472) A[1]:(0.200068131089) A[2]:(0.198859244585) A[3]:(0.388568609953)\n",
      " state (1)  A[0]:(0.198507130146) A[1]:(0.173060759902) A[2]:(0.177076280117) A[3]:(0.451355844736)\n",
      " state (2)  A[0]:(0.287155002356) A[1]:(0.187495902181) A[2]:(0.182482376695) A[3]:(0.342866688967)\n",
      " state (3)  A[0]:(0.635779798031) A[1]:(0.140356302261) A[2]:(0.122598417103) A[3]:(0.101265437901)\n",
      " state (4)  A[0]:(0.830038189888) A[1]:(0.0955500379205) A[2]:(0.0574535541236) A[3]:(0.016958206892)\n",
      " state (5)  A[0]:(0.448740959167) A[1]:(0.483758956194) A[2]:(0.0634439736605) A[3]:(0.00405610585585)\n",
      " state (6)  A[0]:(0.00083953991998) A[1]:(0.996853888035) A[2]:(0.0023014487233) A[3]:(5.12205679115e-06)\n",
      " state (7)  A[0]:(7.67229994381e-06) A[1]:(0.999841630459) A[2]:(0.000150661173393) A[3]:(3.58043301674e-08)\n",
      " state (8)  A[0]:(1.84793987046e-06) A[1]:(0.999933004379) A[2]:(6.5137835918e-05) A[3]:(7.69639019182e-09)\n",
      " state (9)  A[0]:(1.10228870653e-06) A[1]:(0.999951004982) A[2]:(4.7906247346e-05) A[3]:(4.36893632383e-09)\n",
      " state (10)  A[0]:(8.71570421168e-07) A[1]:(0.999957501888) A[2]:(4.16200746258e-05) A[3]:(3.3696088142e-09)\n",
      " state (11)  A[0]:(7.72530427184e-07) A[1]:(0.999960541725) A[2]:(3.87047512049e-05) A[3]:(2.94655522204e-09)\n",
      " state (12)  A[0]:(7.22923857666e-07) A[1]:(0.999962091446) A[2]:(3.71837741113e-05) A[3]:(2.73620570646e-09)\n",
      " state (13)  A[0]:(6.95890378211e-07) A[1]:(0.999962985516) A[2]:(3.63356411981e-05) A[3]:(2.62203192491e-09)\n",
      " state (14)  A[0]:(6.80378775542e-07) A[1]:(0.999963462353) A[2]:(3.58425386366e-05) A[3]:(2.55668619609e-09)\n",
      " state (15)  A[0]:(6.71176053402e-07) A[1]:(0.999963760376) A[2]:(3.55476840923e-05) A[3]:(2.51798093487e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 133000 finished after 2 . Running score: 0.08. Policy_loss: -95601.5255346, Value_loss: 1.22795255528. Times trained:               9500. Times reached goal: 59.               Steps done: 1054298.\n",
      " state (0)  A[0]:(0.201651871204) A[1]:(0.190536677837) A[2]:(0.191028043628) A[3]:(0.416783422232)\n",
      " state (1)  A[0]:(0.185833960772) A[1]:(0.162309810519) A[2]:(0.168081715703) A[3]:(0.483774542809)\n",
      " state (2)  A[0]:(0.267733633518) A[1]:(0.167291373014) A[2]:(0.17124338448) A[3]:(0.39373165369)\n",
      " state (3)  A[0]:(0.626285731792) A[1]:(0.112878747284) A[2]:(0.116769477725) A[3]:(0.144066005945)\n",
      " state (4)  A[0]:(0.87185549736) A[1]:(0.0487194061279) A[2]:(0.0489304512739) A[3]:(0.0304946787655)\n",
      " state (5)  A[0]:(0.917510867119) A[1]:(0.0360840037465) A[2]:(0.0328881330788) A[3]:(0.013517013751)\n",
      " state (6)  A[0]:(0.914375007153) A[1]:(0.0450252853334) A[2]:(0.0317947790027) A[3]:(0.00880490802228)\n",
      " state (7)  A[0]:(0.7874609828) A[1]:(0.158585757017) A[2]:(0.0482627339661) A[3]:(0.00569049268961)\n",
      " state (8)  A[0]:(0.228671684861) A[1]:(0.721722900867) A[2]:(0.0482560954988) A[3]:(0.00134932110086)\n",
      " state (9)  A[0]:(0.0141153950244) A[1]:(0.973174929619) A[2]:(0.0126352384686) A[3]:(7.44290518924e-05)\n",
      " state (10)  A[0]:(0.00105072697625) A[1]:(0.995664477348) A[2]:(0.00327962241136) A[3]:(5.19161039847e-06)\n",
      " state (11)  A[0]:(0.000158181559527) A[1]:(0.998621046543) A[2]:(0.00122002360877) A[3]:(7.44091323668e-07)\n",
      " state (12)  A[0]:(4.47097045253e-05) A[1]:(0.999327719212) A[2]:(0.000627397850621) A[3]:(2.01749898565e-07)\n",
      " state (13)  A[0]:(1.97406534426e-05) A[1]:(0.99957382679) A[2]:(0.000406333914725) A[3]:(8.60980335915e-08)\n",
      " state (14)  A[0]:(1.1650236047e-05) A[1]:(0.999682068825) A[2]:(0.000306256726617) A[3]:(4.94920193717e-08)\n",
      " state (15)  A[0]:(8.26613904792e-06) A[1]:(0.999737262726) A[2]:(0.000254439655691) A[3]:(3.44371073879e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 134000 finished after 9 . Running score: 0.03. Policy_loss: -95601.5205598, Value_loss: 1.21876596239. Times trained:               10075. Times reached goal: 54.               Steps done: 1064373.\n",
      "action_dist \n",
      "tensor([[ 0.1997,  0.1906,  0.1907,  0.4190]])\n",
      "On state=0, selected action=1\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8886,  0.0465,  0.0428,  0.0221]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1997,  0.1905,  0.1907,  0.4191]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1804,  0.1602,  0.1661,  0.4933]])\n",
      "On state=1, selected action=0\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.19963388145) A[1]:(0.190456554294) A[2]:(0.19068531692) A[3]:(0.419224232435)\n",
      " state (1)  A[0]:(0.180365473032) A[1]:(0.160194754601) A[2]:(0.166098043323) A[3]:(0.493341714144)\n",
      " state (2)  A[0]:(0.258558154106) A[1]:(0.169884502888) A[2]:(0.171704351902) A[3]:(0.399852991104)\n",
      " state (3)  A[0]:(0.634095728397) A[1]:(0.11691429466) A[2]:(0.115687958896) A[3]:(0.133302018046)\n",
      " state (4)  A[0]:(0.888521492481) A[1]:(0.0465546399355) A[2]:(0.0428427271545) A[3]:(0.0220811665058)\n",
      " state (5)  A[0]:(0.915894985199) A[1]:(0.0426580794156) A[2]:(0.0317100398242) A[3]:(0.00973692536354)\n",
      " state (6)  A[0]:(0.831335306168) A[1]:(0.11848320812) A[2]:(0.0438918732107) A[3]:(0.00628963299096)\n",
      " state (7)  A[0]:(0.0946521610022) A[1]:(0.871136248112) A[2]:(0.0336102694273) A[3]:(0.000601290608756)\n",
      " state (8)  A[0]:(0.000602419604547) A[1]:(0.996970593929) A[2]:(0.00242342846468) A[3]:(3.54183862328e-06)\n",
      " state (9)  A[0]:(3.43667379639e-05) A[1]:(0.99943369627) A[2]:(0.000531733268872) A[3]:(1.93322122755e-07)\n",
      " state (10)  A[0]:(8.58055409481e-06) A[1]:(0.999737560749) A[2]:(0.000253794132732) A[3]:(4.66417873213e-08)\n",
      " state (11)  A[0]:(4.19849038735e-06) A[1]:(0.99982303381) A[2]:(0.000172755724634) A[3]:(2.22503864222e-08)\n",
      " state (12)  A[0]:(2.83850408778e-06) A[1]:(0.999857485294) A[2]:(0.00013968575513) A[3]:(1.4782062685e-08)\n",
      " state (13)  A[0]:(2.26886231758e-06) A[1]:(0.99987411499) A[2]:(0.000123595906189) A[3]:(1.16797469474e-08)\n",
      " state (14)  A[0]:(1.98803036255e-06) A[1]:(0.999883055687) A[2]:(0.000114951224532) A[3]:(1.01579908929e-08)\n",
      " state (15)  A[0]:(1.83589827429e-06) A[1]:(0.999888122082) A[2]:(0.000110023982415) A[3]:(9.33617005927e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 135000 finished after 4 . Running score: 0.05. Policy_loss: -95601.5411404, Value_loss: 1.21765290831. Times trained:               10554. Times reached goal: 62.               Steps done: 1074927.\n",
      " state (0)  A[0]:(0.197383314371) A[1]:(0.184147745371) A[2]:(0.183625563979) A[3]:(0.43484339118)\n",
      " state (1)  A[0]:(0.182106122375) A[1]:(0.15496994555) A[2]:(0.15996979177) A[3]:(0.502954125404)\n",
      " state (2)  A[0]:(0.295025408268) A[1]:(0.158792421222) A[2]:(0.16015689075) A[3]:(0.386025249958)\n",
      " state (3)  A[0]:(0.746690392494) A[1]:(0.080719769001) A[2]:(0.0807853043079) A[3]:(0.0918045267463)\n",
      " state (4)  A[0]:(0.918180584908) A[1]:(0.0335701592267) A[2]:(0.031181672588) A[3]:(0.0170676093549)\n",
      " state (5)  A[0]:(0.934897124767) A[1]:(0.0317142382264) A[2]:(0.0247927028686) A[3]:(0.00859596021473)\n",
      " state (6)  A[0]:(0.898847579956) A[1]:(0.0642574056983) A[2]:(0.0308940690011) A[3]:(0.00600095512345)\n",
      " state (7)  A[0]:(0.363103747368) A[1]:(0.58493077755) A[2]:(0.0500361099839) A[3]:(0.00192935497034)\n",
      " state (8)  A[0]:(0.00266927108169) A[1]:(0.992782771587) A[2]:(0.00453576864675) A[3]:(1.21946704894e-05)\n",
      " state (9)  A[0]:(6.76428899169e-05) A[1]:(0.999286353588) A[2]:(0.00064572080737) A[3]:(2.88341112764e-07)\n",
      " state (10)  A[0]:(1.07111009129e-05) A[1]:(0.99974656105) A[2]:(0.000242678230279) A[3]:(4.37293863342e-08)\n",
      " state (11)  A[0]:(4.16496959588e-06) A[1]:(0.999849379063) A[2]:(0.000146426202264) A[3]:(1.64880589182e-08)\n",
      " state (12)  A[0]:(2.48832907346e-06) A[1]:(0.999886572361) A[2]:(0.000110905566544) A[3]:(9.64029212014e-09)\n",
      " state (13)  A[0]:(1.85195574431e-06) A[1]:(0.999903678894) A[2]:(9.44632483879e-05) A[3]:(7.0704975208e-09)\n",
      " state (14)  A[0]:(1.55365785304e-06) A[1]:(0.999912619591) A[2]:(8.58226558194e-05) A[3]:(5.87419135556e-09)\n",
      " state (15)  A[0]:(1.3958042473e-06) A[1]:(0.999917685986) A[2]:(8.09268458397e-05) A[3]:(5.24376897459e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 136000 finished after 9 . Running score: 0.05. Policy_loss: -95601.5442001, Value_loss: 1.43164657642. Times trained:               10340. Times reached goal: 64.               Steps done: 1085267.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.200765654445) A[1]:(0.191313013434) A[2]:(0.190203234553) A[3]:(0.417718112469)\n",
      " state (1)  A[0]:(0.17396953702) A[1]:(0.155049920082) A[2]:(0.161425068974) A[3]:(0.509555459023)\n",
      " state (2)  A[0]:(0.228546395898) A[1]:(0.161388248205) A[2]:(0.166766643524) A[3]:(0.443298697472)\n",
      " state (3)  A[0]:(0.504152595997) A[1]:(0.132066518068) A[2]:(0.139772072434) A[3]:(0.224008753896)\n",
      " state (4)  A[0]:(0.844739675522) A[1]:(0.0525885671377) A[2]:(0.057652708143) A[3]:(0.0450190752745)\n",
      " state (5)  A[0]:(0.92088675499) A[1]:(0.030836019665) A[2]:(0.032468225807) A[3]:(0.015808975324)\n",
      " state (6)  A[0]:(0.934688210487) A[1]:(0.0287582743913) A[2]:(0.0270357504487) A[3]:(0.00951777491719)\n",
      " state (7)  A[0]:(0.915455758572) A[1]:(0.0467611178756) A[2]:(0.0308777429163) A[3]:(0.00690536992624)\n",
      " state (8)  A[0]:(0.7390396595) A[1]:(0.203784614801) A[2]:(0.0526406690478) A[3]:(0.00453507667407)\n",
      " state (9)  A[0]:(0.180064335465) A[1]:(0.771815121174) A[2]:(0.0471327342093) A[3]:(0.00098783604335)\n",
      " state (10)  A[0]:(0.0117757916451) A[1]:(0.974889576435) A[2]:(0.0132714752108) A[3]:(6.31359725958e-05)\n",
      " state (11)  A[0]:(0.00091861007968) A[1]:(0.995292127132) A[2]:(0.0037842290476) A[3]:(5.01150316268e-06)\n",
      " state (12)  A[0]:(0.000126055660076) A[1]:(0.998447060585) A[2]:(0.00142617151141) A[3]:(7.01341093645e-07)\n",
      " state (13)  A[0]:(2.99951934721e-05) A[1]:(0.999266982079) A[2]:(0.000702847144566) A[3]:(1.68381362187e-07)\n",
      " state (14)  A[0]:(1.10549672172e-05) A[1]:(0.999560773373) A[2]:(0.000428135128459) A[3]:(6.20001472385e-08)\n",
      " state (15)  A[0]:(5.58151168661e-06) A[1]:(0.999690413475) A[2]:(0.000303956825519) A[3]:(3.11028749422e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 137000 finished after 7 . Running score: 0.01. Policy_loss: -95601.5342476, Value_loss: 1.01259890558. Times trained:               10599. Times reached goal: 38.               Steps done: 1095866.\n",
      " state (0)  A[0]:(0.200797453523) A[1]:(0.19203184545) A[2]:(0.190771088004) A[3]:(0.416399598122)\n",
      " state (1)  A[0]:(0.171839013696) A[1]:(0.153977811337) A[2]:(0.160019874573) A[3]:(0.514163255692)\n",
      " state (2)  A[0]:(0.227109506726) A[1]:(0.162164926529) A[2]:(0.166555806994) A[3]:(0.444169729948)\n",
      " state (3)  A[0]:(0.516779839993) A[1]:(0.133112832904) A[2]:(0.138681381941) A[3]:(0.211425945163)\n",
      " state (4)  A[0]:(0.858945608139) A[1]:(0.0512696616352) A[2]:(0.0536504872143) A[3]:(0.0361342318356)\n",
      " state (5)  A[0]:(0.919454813004) A[1]:(0.0358480885625) A[2]:(0.0325240008533) A[3]:(0.0121730975807)\n",
      " state (6)  A[0]:(0.882694602013) A[1]:(0.0713866278529) A[2]:(0.0386016815901) A[3]:(0.00731712486595)\n",
      " state (7)  A[0]:(0.244642361999) A[1]:(0.696621656418) A[2]:(0.0569597706199) A[3]:(0.0017762358766)\n",
      " state (8)  A[0]:(0.00123349018395) A[1]:(0.994217395782) A[2]:(0.00453932629898) A[3]:(9.81354514806e-06)\n",
      " state (9)  A[0]:(3.05295143335e-05) A[1]:(0.999266445637) A[2]:(0.000702766352333) A[3]:(2.64579767872e-07)\n",
      " state (10)  A[0]:(4.86841372549e-06) A[1]:(0.999717712402) A[2]:(0.000277359766187) A[3]:(4.34206945954e-08)\n",
      " state (11)  A[0]:(1.93497135115e-06) A[1]:(0.999825000763) A[2]:(0.000173047592398) A[3]:(1.73189729225e-08)\n",
      " state (12)  A[0]:(1.17852971471e-06) A[1]:(0.99986487627) A[2]:(0.000133950015879) A[3]:(1.05082724744e-08)\n",
      " state (13)  A[0]:(8.88392321485e-07) A[1]:(0.999883472919) A[2]:(0.00011561345309) A[3]:(7.88275311692e-09)\n",
      " state (14)  A[0]:(7.50775370761e-07) A[1]:(0.99989336729) A[2]:(0.000105850464024) A[3]:(6.63421229063e-09)\n",
      " state (15)  A[0]:(6.76970728364e-07) A[1]:(0.999899089336) A[2]:(0.000100237848528) A[3]:(5.96357807581e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 138000 finished after 4 . Running score: 0.04. Policy_loss: -95601.5603775, Value_loss: 1.01745469052. Times trained:               10781. Times reached goal: 50.               Steps done: 1106647.\n",
      " state (0)  A[0]:(0.196829721332) A[1]:(0.193703502417) A[2]:(0.189545795321) A[3]:(0.41992098093)\n",
      " state (1)  A[0]:(0.161215856671) A[1]:(0.150524124503) A[2]:(0.155698597431) A[3]:(0.532561421394)\n",
      " state (2)  A[0]:(0.191181257367) A[1]:(0.157826676965) A[2]:(0.162069648504) A[3]:(0.488922387362)\n",
      " state (3)  A[0]:(0.341589808464) A[1]:(0.157510563731) A[2]:(0.163261130452) A[3]:(0.33763846755)\n",
      " state (4)  A[0]:(0.731982588768) A[1]:(0.0855733901262) A[2]:(0.0926805436611) A[3]:(0.0897634476423)\n",
      " state (5)  A[0]:(0.888233482838) A[1]:(0.0453565753996) A[2]:(0.0452622063458) A[3]:(0.0211477037519)\n",
      " state (6)  A[0]:(0.877612709999) A[1]:(0.0684087052941) A[2]:(0.0441562272608) A[3]:(0.0098223676905)\n",
      " state (7)  A[0]:(0.323691040277) A[1]:(0.601415395737) A[2]:(0.0720196664333) A[3]:(0.00287394481711)\n",
      " state (8)  A[0]:(0.00141272658948) A[1]:(0.992550313473) A[2]:(0.0060229068622) A[3]:(1.4056735381e-05)\n",
      " state (9)  A[0]:(2.2823081963e-05) A[1]:(0.999195694923) A[2]:(0.000781243434176) A[3]:(2.56435299661e-07)\n",
      " state (10)  A[0]:(3.05652679344e-06) A[1]:(0.999709546566) A[2]:(0.000287344242679) A[3]:(3.58218699148e-08)\n",
      " state (11)  A[0]:(1.14459567158e-06) A[1]:(0.999823570251) A[2]:(0.00017528697208) A[3]:(1.35025777226e-08)\n",
      " state (12)  A[0]:(6.82280870024e-07) A[1]:(0.999864637852) A[2]:(0.000134668807732) A[3]:(8.01595145816e-09)\n",
      " state (13)  A[0]:(5.09660935677e-07) A[1]:(0.999883592129) A[2]:(0.000115884802653) A[3]:(5.95244031842e-09)\n",
      " state (14)  A[0]:(4.28660740681e-07) A[1]:(0.999893665314) A[2]:(0.000105925217213) A[3]:(4.98038188468e-09)\n",
      " state (15)  A[0]:(3.85323858154e-07) A[1]:(0.999899446964) A[2]:(0.000100190954981) A[3]:(4.45908376889e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 139000 finished after 7 . Running score: 0.04. Policy_loss: -95601.5756106, Value_loss: 1.01196303189. Times trained:               10478. Times reached goal: 71.               Steps done: 1117125.\n",
      "action_dist \n",
      "tensor([[ 0.2052,  0.2001,  0.1931,  0.4016]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2052,  0.2001,  0.1931,  0.4016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2052,  0.2001,  0.1931,  0.4016]])\n",
      "On state=0, selected action=1\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8784,  0.0419,  0.0454,  0.0343]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0436,  0.9324,  0.0238,  0.0002]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0437,  0.9323,  0.0238,  0.0003]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.205189585686) A[1]:(0.200051769614) A[2]:(0.19308938086) A[3]:(0.40166926384)\n",
      " state (1)  A[0]:(0.168468475342) A[1]:(0.151919528842) A[2]:(0.156429871917) A[3]:(0.523182153702)\n",
      " state (2)  A[0]:(0.227355852723) A[1]:(0.158226117492) A[2]:(0.16167460382) A[3]:(0.452743440866)\n",
      " state (3)  A[0]:(0.53780144453) A[1]:(0.122278407216) A[2]:(0.128606840968) A[3]:(0.211313322186)\n",
      " state (4)  A[0]:(0.878489792347) A[1]:(0.0418824516237) A[2]:(0.0453651100397) A[3]:(0.0342626459897)\n",
      " state (5)  A[0]:(0.934835553169) A[1]:(0.0269799306989) A[2]:(0.0265230499208) A[3]:(0.0116614531726)\n",
      " state (6)  A[0]:(0.931134164333) A[1]:(0.0354104377329) A[2]:(0.0262901261449) A[3]:(0.00716525735334)\n",
      " state (7)  A[0]:(0.786329448223) A[1]:(0.162231311202) A[2]:(0.046668164432) A[3]:(0.00477108266205)\n",
      " state (8)  A[0]:(0.0453011505306) A[1]:(0.930218756199) A[2]:(0.0242208596319) A[3]:(0.000259231805103)\n",
      " state (9)  A[0]:(0.00036070306669) A[1]:(0.997464537621) A[2]:(0.00217252643779) A[3]:(2.23040228775e-06)\n",
      " state (10)  A[0]:(1.9869328753e-05) A[1]:(0.999467730522) A[2]:(0.000512260594405) A[3]:(1.30386908381e-07)\n",
      " state (11)  A[0]:(4.3865529733e-06) A[1]:(0.999754607677) A[2]:(0.000240958179347) A[3]:(2.93550055375e-08)\n",
      " state (12)  A[0]:(1.94594849745e-06) A[1]:(0.99983805418) A[2]:(0.000160007242812) A[3]:(1.30290311873e-08)\n",
      " state (13)  A[0]:(1.22562050819e-06) A[1]:(0.999872326851) A[2]:(0.000126453436678) A[3]:(8.15911516128e-09)\n",
      " state (14)  A[0]:(9.30067074023e-07) A[1]:(0.999889314175) A[2]:(0.000109731023258) A[3]:(6.15047568431e-09)\n",
      " state (15)  A[0]:(7.83897348811e-07) A[1]:(0.999898791313) A[2]:(0.000100429286249) A[3]:(5.15402120982e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 140000 finished after 6 . Running score: 0.07. Policy_loss: -95601.5879115, Value_loss: 1.00462559165. Times trained:               10367. Times reached goal: 64.               Steps done: 1127492.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.208276450634) A[1]:(0.199047729373) A[2]:(0.187701612711) A[3]:(0.404974222183)\n",
      " state (1)  A[0]:(0.180900424719) A[1]:(0.150018289685) A[2]:(0.149371922016) A[3]:(0.519709408283)\n",
      " state (2)  A[0]:(0.404826968908) A[1]:(0.140104264021) A[2]:(0.137283578515) A[3]:(0.317785203457)\n",
      " state (3)  A[0]:(0.896304905415) A[1]:(0.0365978814662) A[2]:(0.0355700328946) A[3]:(0.0315271764994)\n",
      " state (4)  A[0]:(0.947888076305) A[1]:(0.02463497594) A[2]:(0.0193352457136) A[3]:(0.00814167689532)\n",
      " state (5)  A[0]:(0.902311980724) A[1]:(0.0658922791481) A[2]:(0.026713071391) A[3]:(0.0050826468505)\n",
      " state (6)  A[0]:(0.148217588663) A[1]:(0.818595349789) A[2]:(0.032406527549) A[3]:(0.000780536036473)\n",
      " state (7)  A[0]:(7.78664543759e-05) A[1]:(0.999269485474) A[2]:(0.000652168993838) A[3]:(4.7322618002e-07)\n",
      " state (8)  A[0]:(2.06235722544e-06) A[1]:(0.999903261662) A[2]:(9.46683794609e-05) A[3]:(1.30662662912e-08)\n",
      " state (9)  A[0]:(6.79568813666e-07) A[1]:(0.999947249889) A[2]:(5.20561734447e-05) A[3]:(4.2515786447e-09)\n",
      " state (10)  A[0]:(4.46649210062e-07) A[1]:(0.999958157539) A[2]:(4.13870911871e-05) A[3]:(2.75443801101e-09)\n",
      " state (11)  A[0]:(3.68992715494e-07) A[1]:(0.999962389469) A[2]:(3.72417707695e-05) A[3]:(2.25325913483e-09)\n",
      " state (12)  A[0]:(3.35086951964e-07) A[1]:(0.999964356422) A[2]:(3.52962379111e-05) A[3]:(2.03380690067e-09)\n",
      " state (13)  A[0]:(3.18277329825e-07) A[1]:(0.999965369701) A[2]:(3.42947641911e-05) A[3]:(1.92477100924e-09)\n",
      " state (14)  A[0]:(3.09371358753e-07) A[1]:(0.999965906143) A[2]:(3.37532728736e-05) A[3]:(1.86690551907e-09)\n",
      " state (15)  A[0]:(3.04474042423e-07) A[1]:(0.999966263771) A[2]:(3.34520846081e-05) A[3]:(1.83503767737e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 141000 finished after 11 . Running score: 0.05. Policy_loss: -95601.5412384, Value_loss: 1.0119165774. Times trained:               10900. Times reached goal: 60.               Steps done: 1138392.\n",
      " state (0)  A[0]:(0.224621474743) A[1]:(0.222837626934) A[2]:(0.202612653375) A[3]:(0.349928230047)\n",
      " state (1)  A[0]:(0.174189522862) A[1]:(0.156312346458) A[2]:(0.153214663267) A[3]:(0.516283512115)\n",
      " state (2)  A[0]:(0.296032369137) A[1]:(0.164258569479) A[2]:(0.157560765743) A[3]:(0.382148265839)\n",
      " state (3)  A[0]:(0.794395506382) A[1]:(0.0717443153262) A[2]:(0.0684073641896) A[3]:(0.0654528141022)\n",
      " state (4)  A[0]:(0.922053575516) A[1]:(0.0395493805408) A[2]:(0.0284300260246) A[3]:(0.0099670374766)\n",
      " state (5)  A[0]:(0.682595729828) A[1]:(0.26153460145) A[2]:(0.0515378154814) A[3]:(0.00433185044676)\n",
      " state (6)  A[0]:(0.000920305028558) A[1]:(0.995819926262) A[2]:(0.00325309229083) A[3]:(6.65465495331e-06)\n",
      " state (7)  A[0]:(2.07475568459e-06) A[1]:(0.999855458736) A[2]:(0.000142475444591) A[3]:(1.74719918533e-08)\n",
      " state (8)  A[0]:(3.97092662752e-07) A[1]:(0.999939441681) A[2]:(6.0140806454e-05) A[3]:(3.33559979637e-09)\n",
      " state (9)  A[0]:(2.35469499898e-07) A[1]:(0.999954223633) A[2]:(4.5521712309e-05) A[3]:(1.94171856371e-09)\n",
      " state (10)  A[0]:(1.89973363263e-07) A[1]:(0.999959290028) A[2]:(4.05186110584e-05) A[3]:(1.54590618084e-09)\n",
      " state (11)  A[0]:(1.7162373922e-07) A[1]:(0.999961495399) A[2]:(3.83223996323e-05) A[3]:(1.38540079409e-09)\n",
      " state (12)  A[0]:(1.62976022011e-07) A[1]:(0.999962568283) A[2]:(3.72436261387e-05) A[3]:(1.30951027799e-09)\n",
      " state (13)  A[0]:(1.58574977149e-07) A[1]:(0.99996316433) A[2]:(3.66828171536e-05) A[3]:(1.2708131214e-09)\n",
      " state (14)  A[0]:(1.56240758997e-07) A[1]:(0.999963462353) A[2]:(3.63820909115e-05) A[3]:(1.25026700104e-09)\n",
      " state (15)  A[0]:(1.54973022859e-07) A[1]:(0.999963641167) A[2]:(3.62178689102e-05) A[3]:(1.23910448568e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 142000 finished after 10 . Running score: 0.09. Policy_loss: -95230.0371233, Value_loss: 1.65376599003. Times trained:               10285. Times reached goal: 75.               Steps done: 1148677.\n",
      " state (0)  A[0]:(0.211471319199) A[1]:(0.213149443269) A[2]:(0.198281735182) A[3]:(0.377097517252)\n",
      " state (1)  A[0]:(0.153376892209) A[1]:(0.145132258534) A[2]:(0.146095857024) A[3]:(0.555395007133)\n",
      " state (2)  A[0]:(0.204845920205) A[1]:(0.154983088374) A[2]:(0.152963101864) A[3]:(0.487207859755)\n",
      " state (3)  A[0]:(0.558554112911) A[1]:(0.121156699955) A[2]:(0.117513284087) A[3]:(0.202775880694)\n",
      " state (4)  A[0]:(0.904876172543) A[1]:(0.0412705615163) A[2]:(0.0331964381039) A[3]:(0.0206568166614)\n",
      " state (5)  A[0]:(0.783003628254) A[1]:(0.172921568155) A[2]:(0.0383051708341) A[3]:(0.00576966442168)\n",
      " state (6)  A[0]:(0.0013577779755) A[1]:(0.996326565742) A[2]:(0.00230646203272) A[3]:(9.20547063288e-06)\n",
      " state (7)  A[0]:(4.65950552098e-06) A[1]:(0.999890863895) A[2]:(0.000104437371192) A[3]:(3.54015625703e-08)\n",
      " state (8)  A[0]:(8.08219169812e-07) A[1]:(0.999959290028) A[2]:(3.98841111746e-05) A[3]:(6.15230222323e-09)\n",
      " state (9)  A[0]:(4.26516976404e-07) A[1]:(0.999971687794) A[2]:(2.78796542261e-05) A[3]:(3.19108139912e-09)\n",
      " state (10)  A[0]:(3.22082229331e-07) A[1]:(0.999975919724) A[2]:(2.37557214859e-05) A[3]:(2.37575736861e-09)\n",
      " state (11)  A[0]:(2.80692120214e-07) A[1]:(0.999977767467) A[2]:(2.19442117668e-05) A[3]:(2.05153405375e-09)\n",
      " state (12)  A[0]:(2.6124263286e-07) A[1]:(0.999978661537) A[2]:(2.10482667171e-05) A[3]:(1.89891724567e-09)\n",
      " state (13)  A[0]:(2.51257233685e-07) A[1]:(0.999979197979) A[2]:(2.05759952223e-05) A[3]:(1.82051007602e-09)\n",
      " state (14)  A[0]:(2.45868449156e-07) A[1]:(0.999979436398) A[2]:(2.03176714422e-05) A[3]:(1.77820957958e-09)\n",
      " state (15)  A[0]:(2.42870811462e-07) A[1]:(0.999979555607) A[2]:(2.01730617846e-05) A[3]:(1.75469616615e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 143000 finished after 4 . Running score: 0.09. Policy_loss: -95230.0585886, Value_loss: 1.850334675. Times trained:               9938. Times reached goal: 71.               Steps done: 1158615.\n",
      " state (0)  A[0]:(0.216001674533) A[1]:(0.215413287282) A[2]:(0.200626209378) A[3]:(0.367958813906)\n",
      " state (1)  A[0]:(0.146363258362) A[1]:(0.138296380639) A[2]:(0.141021504998) A[3]:(0.574318885803)\n",
      " state (2)  A[0]:(0.170647934079) A[1]:(0.144900128245) A[2]:(0.146398857236) A[3]:(0.538053095341)\n",
      " state (3)  A[0]:(0.300274729729) A[1]:(0.147451177239) A[2]:(0.148806557059) A[3]:(0.403467535973)\n",
      " state (4)  A[0]:(0.72565639019) A[1]:(0.0780853256583) A[2]:(0.0815609171987) A[3]:(0.114697366953)\n",
      " state (5)  A[0]:(0.915965735912) A[1]:(0.0312308967113) A[2]:(0.0306876823306) A[3]:(0.0221156831831)\n",
      " state (6)  A[0]:(0.937273859978) A[1]:(0.0310526769608) A[2]:(0.0224978104234) A[3]:(0.00917567033321)\n",
      " state (7)  A[0]:(0.805750727654) A[1]:(0.15315040946) A[2]:(0.0357728600502) A[3]:(0.00532602192834)\n",
      " state (8)  A[0]:(0.0589092299342) A[1]:(0.923413455486) A[2]:(0.0173088461161) A[3]:(0.000368458859157)\n",
      " state (9)  A[0]:(0.000737779482733) A[1]:(0.997641801834) A[2]:(0.00161522731651) A[3]:(5.22110531165e-06)\n",
      " state (10)  A[0]:(3.87002328353e-05) A[1]:(0.999631166458) A[2]:(0.00032983062556) A[3]:(3.07584144821e-07)\n",
      " state (11)  A[0]:(6.41432188786e-06) A[1]:(0.999867975712) A[2]:(0.000125532387756) A[3]:(5.46597682671e-08)\n",
      " state (12)  A[0]:(2.10576945392e-06) A[1]:(0.999929130077) A[2]:(6.87212304911e-05) A[3]:(1.85868742619e-08)\n",
      " state (13)  A[0]:(1.03772219973e-06) A[1]:(0.999952316284) A[2]:(4.66638884973e-05) A[3]:(9.29313426212e-09)\n",
      " state (14)  A[0]:(6.56010683997e-07) A[1]:(0.99996316433) A[2]:(3.61922648153e-05) A[3]:(5.89429793862e-09)\n",
      " state (15)  A[0]:(4.84838835746e-07) A[1]:(0.99996894598) A[2]:(3.05430512526e-05) A[3]:(4.3477088596e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 144000 finished after 4 . Running score: 0.09. Policy_loss: -95230.0772295, Value_loss: 1.64820874982. Times trained:               10593. Times reached goal: 65.               Steps done: 1169208.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.1902,  0.1809,  0.1749,  0.4539]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1902,  0.1809,  0.1749,  0.4540]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1901,  0.1808,  0.1749,  0.4542]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9301,  0.0220,  0.0232,  0.0247]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1900,  0.1807,  0.1748,  0.4544]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9299,  0.0220,  0.0232,  0.0248]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9298,  0.0221,  0.0232,  0.0249]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0011,  0.9957,  0.0031,  0.0000]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0011,  0.9957,  0.0031,  0.0000]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0012,  0.9957,  0.0031,  0.0000]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.8748e-05,  9.9940e-01,  5.5881e-04,  7.6641e-07]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.5612e-07,  9.9992e-01,  7.4591e-05,  2.0984e-08]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.189839720726) A[1]:(0.180528834462) A[2]:(0.174692213535) A[3]:(0.45493927598)\n",
      " state (1)  A[0]:(0.134328052402) A[1]:(0.120928630233) A[2]:(0.125075116754) A[3]:(0.619668245316)\n",
      " state (2)  A[0]:(0.184306800365) A[1]:(0.127426326275) A[2]:(0.130181863904) A[3]:(0.558085024357)\n",
      " state (3)  A[0]:(0.578040957451) A[1]:(0.0902997106314) A[2]:(0.0950747132301) A[3]:(0.23658464849)\n",
      " state (4)  A[0]:(0.929748654366) A[1]:(0.0220690015703) A[2]:(0.0232507884502) A[3]:(0.0249315276742)\n",
      " state (5)  A[0]:(0.957013726234) A[1]:(0.0182488430291) A[2]:(0.0157222356647) A[3]:(0.00901519693434)\n",
      " state (6)  A[0]:(0.918373286724) A[1]:(0.0517158992589) A[2]:(0.023461682722) A[3]:(0.00644914852455)\n",
      " state (7)  A[0]:(0.226920232177) A[1]:(0.730877876282) A[2]:(0.0402295589447) A[3]:(0.0019722986035)\n",
      " state (8)  A[0]:(0.00114966882393) A[1]:(0.995716333389) A[2]:(0.00311759859324) A[3]:(1.64173907251e-05)\n",
      " state (9)  A[0]:(3.86792780773e-05) A[1]:(0.999402463436) A[2]:(0.000558108557016) A[3]:(7.6536269944e-07)\n",
      " state (10)  A[0]:(6.37192442809e-06) A[1]:(0.999769866467) A[2]:(0.000223600509344) A[3]:(1.49386977455e-07)\n",
      " state (11)  A[0]:(2.22840913011e-06) A[1]:(0.999866962433) A[2]:(0.000130746935611) A[3]:(5.72426941403e-08)\n",
      " state (12)  A[0]:(1.15569275749e-06) A[1]:(0.999905705452) A[2]:(9.31303657126e-05) A[3]:(3.12168602079e-08)\n",
      " state (13)  A[0]:(7.55877522352e-07) A[1]:(0.999924659729) A[2]:(7.4564559327e-05) A[3]:(2.09806838569e-08)\n",
      " state (14)  A[0]:(5.71520388348e-07) A[1]:(0.999935150146) A[2]:(6.42850427539e-05) A[3]:(1.60920468062e-08)\n",
      " state (15)  A[0]:(4.74274258977e-07) A[1]:(0.99994134903) A[2]:(5.8160196204e-05) A[3]:(1.34511104477e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 145000 finished after 12 . Running score: 0.04. Policy_loss: -95230.0428003, Value_loss: 1.66564897351. Times trained:               11446. Times reached goal: 68.               Steps done: 1180654.\n",
      " state (0)  A[0]:(0.225743725896) A[1]:(0.220218002796) A[2]:(0.203657731414) A[3]:(0.350380510092)\n",
      " state (1)  A[0]:(0.135786816478) A[1]:(0.127831861377) A[2]:(0.130529239774) A[3]:(0.605852127075)\n",
      " state (2)  A[0]:(0.159125611186) A[1]:(0.138087838888) A[2]:(0.138431385159) A[3]:(0.564355134964)\n",
      " state (3)  A[0]:(0.287585049868) A[1]:(0.151177152991) A[2]:(0.146963536739) A[3]:(0.414274245501)\n",
      " state (4)  A[0]:(0.765407025814) A[1]:(0.0743584409356) A[2]:(0.0716091766953) A[3]:(0.0886253267527)\n",
      " state (5)  A[0]:(0.93618619442) A[1]:(0.0276516862214) A[2]:(0.0234919562936) A[3]:(0.0126701314002)\n",
      " state (6)  A[0]:(0.926699280739) A[1]:(0.0441768430173) A[2]:(0.0231065861881) A[3]:(0.00601729052141)\n",
      " state (7)  A[0]:(0.4845289886) A[1]:(0.463959366083) A[2]:(0.0486233830452) A[3]:(0.00288828113116)\n",
      " state (8)  A[0]:(0.00367704196833) A[1]:(0.990459918976) A[2]:(0.00583324255422) A[3]:(2.97902042803e-05)\n",
      " state (9)  A[0]:(5.57399907848e-05) A[1]:(0.999227285385) A[2]:(0.000716387643479) A[3]:(6.09661242379e-07)\n",
      " state (10)  A[0]:(5.95936307946e-06) A[1]:(0.999758958817) A[2]:(0.000235030180193) A[3]:(7.67674421809e-08)\n",
      " state (11)  A[0]:(1.74436968337e-06) A[1]:(0.999871075153) A[2]:(0.000127160004922) A[3]:(2.44098874447e-08)\n",
      " state (12)  A[0]:(8.3617845803e-07) A[1]:(0.999911427498) A[2]:(8.77503480297e-05) A[3]:(1.22018688486e-08)\n",
      " state (13)  A[0]:(5.2465219369e-07) A[1]:(0.99993032217) A[2]:(6.91589229973e-05) A[3]:(7.81263587157e-09)\n",
      " state (14)  A[0]:(3.86660133245e-07) A[1]:(0.999940574169) A[2]:(5.90489471506e-05) A[3]:(5.80766723601e-09)\n",
      " state (15)  A[0]:(3.15335597634e-07) A[1]:(0.999946594238) A[2]:(5.30564357177e-05) A[3]:(4.74875072598e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 146000 finished after 13 . Running score: 0.03. Policy_loss: -95230.0335794, Value_loss: 1.43448600583. Times trained:               11191. Times reached goal: 65.               Steps done: 1191845.\n",
      " state (0)  A[0]:(0.217200115323) A[1]:(0.207986295223) A[2]:(0.192372515798) A[3]:(0.382441073656)\n",
      " state (1)  A[0]:(0.131598725915) A[1]:(0.120859153569) A[2]:(0.122001416981) A[3]:(0.625540673733)\n",
      " state (2)  A[0]:(0.191346675158) A[1]:(0.132765457034) A[2]:(0.130303770304) A[3]:(0.545584082603)\n",
      " state (3)  A[0]:(0.680900812149) A[1]:(0.0813157632947) A[2]:(0.0785652026534) A[3]:(0.159218221903)\n",
      " state (4)  A[0]:(0.952227950096) A[1]:(0.0184770841151) A[2]:(0.0164240188897) A[3]:(0.0128709627315)\n",
      " state (5)  A[0]:(0.963418245316) A[1]:(0.0183900613338) A[2]:(0.0126798572019) A[3]:(0.00551184592769)\n",
      " state (6)  A[0]:(0.939044475555) A[1]:(0.0398472547531) A[2]:(0.0168190244585) A[3]:(0.00428925780579)\n",
      " state (7)  A[0]:(0.68630450964) A[1]:(0.276434123516) A[2]:(0.0339768528938) A[3]:(0.00328453979455)\n",
      " state (8)  A[0]:(0.0151365557685) A[1]:(0.977051019669) A[2]:(0.00770977698267) A[3]:(0.000102646306914)\n",
      " state (9)  A[0]:(0.00010272666259) A[1]:(0.999362587929) A[2]:(0.00053362728795) A[3]:(1.05105095827e-06)\n",
      " state (10)  A[0]:(6.84148062646e-06) A[1]:(0.999867677689) A[2]:(0.000125405509607) A[3]:(8.78610109112e-08)\n",
      " state (11)  A[0]:(1.74791159679e-06) A[1]:(0.999937832355) A[2]:(6.03963635513e-05) A[3]:(2.49444642719e-08)\n",
      " state (12)  A[0]:(8.16239207779e-07) A[1]:(0.999959111214) A[2]:(4.0063088818e-05) A[3]:(1.22434586913e-08)\n",
      " state (13)  A[0]:(5.12378846906e-07) A[1]:(0.999968409538) A[2]:(3.10863615596e-05) A[3]:(7.86765053107e-09)\n",
      " state (14)  A[0]:(3.79301297926e-07) A[1]:(0.999973297119) A[2]:(2.63337369688e-05) A[3]:(5.88327164763e-09)\n",
      " state (15)  A[0]:(3.10442942464e-07) A[1]:(0.999976158142) A[2]:(2.35448405874e-05) A[3]:(4.83135442764e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 147000 finished after 9 . Running score: 0.06. Policy_loss: -95230.0409794, Value_loss: 1.43324905126. Times trained:               11391. Times reached goal: 75.               Steps done: 1203236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.206883847713) A[1]:(0.19209189713) A[2]:(0.180360764265) A[3]:(0.42066347599)\n",
      " state (1)  A[0]:(0.119563251734) A[1]:(0.110081315041) A[2]:(0.112670101225) A[3]:(0.657685339451)\n",
      " state (2)  A[0]:(0.140123724937) A[1]:(0.11587048322) A[2]:(0.117608584464) A[3]:(0.626397192478)\n",
      " state (3)  A[0]:(0.286112457514) A[1]:(0.119561687112) A[2]:(0.121517464519) A[3]:(0.472808390856)\n",
      " state (4)  A[0]:(0.830623626709) A[1]:(0.0416547991335) A[2]:(0.0454828068614) A[3]:(0.0822387561202)\n",
      " state (5)  A[0]:(0.961723268032) A[1]:(0.012359527871) A[2]:(0.0135737219825) A[3]:(0.0123434597626)\n",
      " state (6)  A[0]:(0.974578261375) A[1]:(0.0095873111859) A[2]:(0.0098634371534) A[3]:(0.00597099447623)\n",
      " state (7)  A[0]:(0.97559183836) A[1]:(0.0103137455881) A[2]:(0.00964966788888) A[3]:(0.00444472208619)\n",
      " state (8)  A[0]:(0.972209751606) A[1]:(0.0132067836821) A[2]:(0.0106976525858) A[3]:(0.00388579326682)\n",
      " state (9)  A[0]:(0.959478616714) A[1]:(0.0228887349367) A[2]:(0.013952717185) A[3]:(0.00367991859093)\n",
      " state (10)  A[0]:(0.897182404995) A[1]:(0.0746312737465) A[2]:(0.024592122063) A[3]:(0.00359422480687)\n",
      " state (11)  A[0]:(0.575479865074) A[1]:(0.376665532589) A[2]:(0.0452260300517) A[3]:(0.00262851710431)\n",
      " state (12)  A[0]:(0.124947659671) A[1]:(0.841818571091) A[2]:(0.0325505435467) A[3]:(0.000683245132677)\n",
      " state (13)  A[0]:(0.0179576463997) A[1]:(0.968176066875) A[2]:(0.0137486187741) A[3]:(0.000117663337733)\n",
      " state (14)  A[0]:(0.00348240416497) A[1]:(0.99013119936) A[2]:(0.0063597233966) A[3]:(2.66793522314e-05)\n",
      " state (15)  A[0]:(0.000960994919296) A[1]:(0.995561301708) A[2]:(0.00346932699904) A[3]:(8.36676827021e-06)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 148000 finished after 27 . Running score: 0.02. Policy_loss: -95230.0607749, Value_loss: 1.4305812509. Times trained:               11791. Times reached goal: 60.               Steps done: 1215027.\n",
      " state (0)  A[0]:(0.184403568506) A[1]:(0.170504674315) A[2]:(0.160660073161) A[3]:(0.48443171382)\n",
      " state (1)  A[0]:(0.111030727625) A[1]:(0.105375744402) A[2]:(0.106955319643) A[3]:(0.676638185978)\n",
      " state (2)  A[0]:(0.124843277037) A[1]:(0.111918441951) A[2]:(0.11061797291) A[3]:(0.652620315552)\n",
      " state (3)  A[0]:(0.244705110788) A[1]:(0.12419347465) A[2]:(0.114828251302) A[3]:(0.516273140907)\n",
      " state (4)  A[0]:(0.829104423523) A[1]:(0.0488598607481) A[2]:(0.0403757542372) A[3]:(0.0816599875689)\n",
      " state (5)  A[0]:(0.953899085522) A[1]:(0.0213284175843) A[2]:(0.0132224885747) A[3]:(0.0115499859676)\n",
      " state (6)  A[0]:(0.932972490788) A[1]:(0.0450934059918) A[2]:(0.01552031748) A[3]:(0.00641380064189)\n",
      " state (7)  A[0]:(0.673329591751) A[1]:(0.291382342577) A[2]:(0.0305880289525) A[3]:(0.00470007304102)\n",
      " state (8)  A[0]:(0.0253244824708) A[1]:(0.96482360363) A[2]:(0.00956637598574) A[3]:(0.000285552785499)\n",
      " state (9)  A[0]:(5.93397635384e-05) A[1]:(0.999555885792) A[2]:(0.000383293488994) A[3]:(1.45383182826e-06)\n",
      " state (10)  A[0]:(1.28149633838e-06) A[1]:(0.999951243401) A[2]:(4.74430526083e-05) A[3]:(5.09160251738e-08)\n",
      " state (11)  A[0]:(3.09712845592e-07) A[1]:(0.999978125095) A[2]:(2.15785948967e-05) A[3]:(1.4367308232e-08)\n",
      " state (12)  A[0]:(1.8340271879e-07) A[1]:(0.999983727932) A[2]:(1.60601666721e-05) A[3]:(8.86925377586e-09)\n",
      " state (13)  A[0]:(1.45733324075e-07) A[1]:(0.99998575449) A[2]:(1.40823885886e-05) A[3]:(7.12438996686e-09)\n",
      " state (14)  A[0]:(1.29674162963e-07) A[1]:(0.999986708164) A[2]:(1.31631850309e-05) A[3]:(6.35403418769e-09)\n",
      " state (15)  A[0]:(1.21475991932e-07) A[1]:(0.999987185001) A[2]:(1.26711156554e-05) A[3]:(5.95236393508e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 149000 finished after 19 . Running score: 0.08. Policy_loss: -95230.0574603, Value_loss: 1.43538125207. Times trained:               13417. Times reached goal: 28.               Steps done: 1228444.\n",
      "action_dist \n",
      "tensor([[ 0.2196,  0.1998,  0.1814,  0.3991]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2196,  0.1998,  0.1814,  0.3991]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2196,  0.1998,  0.1814,  0.3991]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1136,  0.1087,  0.1091,  0.6686]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1136,  0.1087,  0.1091,  0.6686]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1136,  0.1087,  0.1091,  0.6686]])\n",
      "On state=1, selected action=1\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1306,  0.1206,  0.1148,  0.6340]])\n",
      "On state=2, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1136,  0.1087,  0.1091,  0.6686]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1307,  0.1206,  0.1148,  0.6340]])\n",
      "On state=2, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.3044,  0.1485,  0.1187,  0.4284]])\n",
      "On state=3, selected action=0\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1307,  0.1206,  0.1148,  0.6340]])\n",
      "On state=2, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.3044,  0.1485,  0.1187,  0.4283]])\n",
      "On state=3, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1307,  0.1206,  0.1148,  0.6340]])\n",
      "On state=2, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.1136,  0.1087,  0.1091,  0.6686]])\n",
      "On state=1, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2196,  0.1998,  0.1814,  0.3991]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8582,  0.0726,  0.0337,  0.0354]])\n",
      "On state=4, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.4815e-08,  9.9999e-01,  7.9387e-06,  2.6029e-09]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.4403e-08,  9.9999e-01,  7.9100e-06,  2.5899e-09]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.219583705068) A[1]:(0.199856311083) A[2]:(0.181451886892) A[3]:(0.399108111858)\n",
      " state (1)  A[0]:(0.113566689193) A[1]:(0.108760789037) A[2]:(0.109061017632) A[3]:(0.668611466885)\n",
      " state (2)  A[0]:(0.130562573671) A[1]:(0.120701760054) A[2]:(0.114773117006) A[3]:(0.633962512016)\n",
      " state (3)  A[0]:(0.303256332874) A[1]:(0.149047985673) A[2]:(0.118908382952) A[3]:(0.428787261248)\n",
      " state (4)  A[0]:(0.856260299683) A[1]:(0.074053697288) A[2]:(0.0340523086488) A[3]:(0.0356336794794)\n",
      " state (5)  A[0]:(0.475267201662) A[1]:(0.487223476171) A[2]:(0.03254218027) A[3]:(0.00496716238558)\n",
      " state (6)  A[0]:(0.000366345455404) A[1]:(0.998699247837) A[2]:(0.000928045483306) A[3]:(6.36455433778e-06)\n",
      " state (7)  A[0]:(4.98412475736e-07) A[1]:(0.999974310398) A[2]:(2.51843284786e-05) A[3]:(1.69325069521e-08)\n",
      " state (8)  A[0]:(6.40365058757e-08) A[1]:(0.999992072582) A[2]:(7.88439592725e-06) A[3]:(2.57825294447e-09)\n",
      " state (9)  A[0]:(4.06661264662e-08) A[1]:(0.999993920326) A[2]:(6.04065144216e-06) A[3]:(1.64642544043e-09)\n",
      " state (10)  A[0]:(3.57859057942e-08) A[1]:(0.999994397163) A[2]:(5.59214549867e-06) A[3]:(1.43469425229e-09)\n",
      " state (11)  A[0]:(3.40709007673e-08) A[1]:(0.999994516373) A[2]:(5.42546740689e-06) A[3]:(1.35750477526e-09)\n",
      " state (12)  A[0]:(3.32725171859e-08) A[1]:(0.999994635582) A[2]:(5.34478795089e-06) A[3]:(1.32242872208e-09)\n",
      " state (13)  A[0]:(3.2820214102e-08) A[1]:(0.999994695187) A[2]:(5.29696762896e-06) A[3]:(1.30436483836e-09)\n",
      " state (14)  A[0]:(3.25236513277e-08) A[1]:(0.999994695187) A[2]:(5.26404346601e-06) A[3]:(1.29417965233e-09)\n",
      " state (15)  A[0]:(3.23113873435e-08) A[1]:(0.999994754791) A[2]:(5.23946710018e-06) A[3]:(1.28793731236e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 150000 finished after 18 . Running score: 0.05. Policy_loss: -94040.2904179, Value_loss: 1.00627601047. Times trained:               12579. Times reached goal: 61.               Steps done: 1241023.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.222396239638) A[1]:(0.195541769266) A[2]:(0.17999714613) A[3]:(0.402064859867)\n",
      " state (1)  A[0]:(0.109591342509) A[1]:(0.100735940039) A[2]:(0.103336580098) A[3]:(0.686336159706)\n",
      " state (2)  A[0]:(0.125466242433) A[1]:(0.107533022761) A[2]:(0.107812456787) A[3]:(0.659188270569)\n",
      " state (3)  A[0]:(0.312859147787) A[1]:(0.115042604506) A[2]:(0.108697794378) A[3]:(0.463400423527)\n",
      " state (4)  A[0]:(0.921313643456) A[1]:(0.0258052889258) A[2]:(0.0206422712654) A[3]:(0.0322387851775)\n",
      " state (5)  A[0]:(0.938144862652) A[1]:(0.0420394614339) A[2]:(0.0135102132335) A[3]:(0.00630548596382)\n",
      " state (6)  A[0]:(0.0498039349914) A[1]:(0.940229296684) A[2]:(0.00961764529347) A[3]:(0.000349133362761)\n",
      " state (7)  A[0]:(1.06024908746e-05) A[1]:(0.999900221825) A[2]:(8.90420924406e-05) A[3]:(1.5294078537e-07)\n",
      " state (8)  A[0]:(5.17913463227e-07) A[1]:(0.999983429909) A[2]:(1.60712716024e-05) A[3]:(9.67879376645e-09)\n",
      " state (9)  A[0]:(2.41249637156e-07) A[1]:(0.999989449978) A[2]:(1.03151733128e-05) A[3]:(4.66262939369e-09)\n",
      " state (10)  A[0]:(1.84364523648e-07) A[1]:(0.999990999699) A[2]:(8.7956450443e-06) A[3]:(3.56168894378e-09)\n",
      " state (11)  A[0]:(1.63002525255e-07) A[1]:(0.99999165535) A[2]:(8.16554256744e-06) A[3]:(3.14084425135e-09)\n",
      " state (12)  A[0]:(1.52671077558e-07) A[1]:(0.999992012978) A[2]:(7.84364237916e-06) A[3]:(2.9399283008e-09)\n",
      " state (13)  A[0]:(1.46989080463e-07) A[1]:(0.999992191792) A[2]:(7.66004632169e-06) A[3]:(2.83308332349e-09)\n",
      " state (14)  A[0]:(1.43666156305e-07) A[1]:(0.999992311001) A[2]:(7.54992424845e-06) A[3]:(2.77297695916e-09)\n",
      " state (15)  A[0]:(1.41665481124e-07) A[1]:(0.999992370605) A[2]:(7.48251659388e-06) A[3]:(2.73791211924e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 151000 finished after 15 . Running score: 0.08. Policy_loss: -94040.3066074, Value_loss: 1.22073094974. Times trained:               11903. Times reached goal: 67.               Steps done: 1252926.\n",
      " state (0)  A[0]:(0.232887625694) A[1]:(0.204666644335) A[2]:(0.18776191771) A[3]:(0.374683767557)\n",
      " state (1)  A[0]:(0.107493206859) A[1]:(0.099522806704) A[2]:(0.102668859065) A[3]:(0.690315127373)\n",
      " state (2)  A[0]:(0.122824147344) A[1]:(0.105872325599) A[2]:(0.107473626733) A[3]:(0.663829922676)\n",
      " state (3)  A[0]:(0.305832833052) A[1]:(0.111589603126) A[2]:(0.109255947173) A[3]:(0.47332161665)\n",
      " state (4)  A[0]:(0.926452517509) A[1]:(0.0207509379834) A[2]:(0.0193573720753) A[3]:(0.0334391407669)\n",
      " state (5)  A[0]:(0.978257954121) A[1]:(0.00898897927254) A[2]:(0.00688843615353) A[3]:(0.00586460949853)\n",
      " state (6)  A[0]:(0.974570393562) A[1]:(0.0145785212517) A[2]:(0.00734611600637) A[3]:(0.00350495823659)\n",
      " state (7)  A[0]:(0.887798547745) A[1]:(0.0943003967404) A[2]:(0.0150751685724) A[3]:(0.00282589183189)\n",
      " state (8)  A[0]:(0.0441811420023) A[1]:(0.948116004467) A[2]:(0.00752044469118) A[3]:(0.000182415620657)\n",
      " state (9)  A[0]:(0.000106634470285) A[1]:(0.999631166458) A[2]:(0.000261463224888) A[3]:(7.19058903087e-07)\n",
      " state (10)  A[0]:(5.16038653586e-06) A[1]:(0.999947071075) A[2]:(4.77152425447e-05) A[3]:(4.56759110534e-08)\n",
      " state (11)  A[0]:(1.40491476941e-06) A[1]:(0.99997574091) A[2]:(2.28449735005e-05) A[3]:(1.38406113237e-08)\n",
      " state (12)  A[0]:(7.41768531043e-07) A[1]:(0.999983429909) A[2]:(1.5848190742e-05) A[3]:(7.62840901558e-09)\n",
      " state (13)  A[0]:(5.20015021266e-07) A[1]:(0.999986588955) A[2]:(1.28980755107e-05) A[3]:(5.44586109541e-09)\n",
      " state (14)  A[0]:(4.19523900064e-07) A[1]:(0.99998819828) A[2]:(1.13694568427e-05) A[3]:(4.42912417853e-09)\n",
      " state (15)  A[0]:(3.65648730849e-07) A[1]:(0.999989151955) A[2]:(1.04771816041e-05) A[3]:(3.87607057561e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 152000 finished after 60 . Running score: 0.04. Policy_loss: -94040.2940603, Value_loss: 0.997322573492. Times trained:               12855. Times reached goal: 78.               Steps done: 1265781.\n",
      " state (0)  A[0]:(0.212861955166) A[1]:(0.178824037313) A[2]:(0.168764635921) A[3]:(0.439549386501)\n",
      " state (1)  A[0]:(0.0953874886036) A[1]:(0.0875460281968) A[2]:(0.0918224155903) A[3]:(0.725244104862)\n",
      " state (2)  A[0]:(0.10673237592) A[1]:(0.0921544209123) A[2]:(0.0949223488569) A[3]:(0.706190824509)\n",
      " state (3)  A[0]:(0.283721208572) A[1]:(0.0966528356075) A[2]:(0.0939061865211) A[3]:(0.525719761848)\n",
      " state (4)  A[0]:(0.942730784416) A[1]:(0.0143584376201) A[2]:(0.0123300841078) A[3]:(0.0305806901306)\n",
      " state (5)  A[0]:(0.980934619904) A[1]:(0.00776898721233) A[2]:(0.00497325183824) A[3]:(0.00632316572592)\n",
      " state (6)  A[0]:(0.96038454771) A[1]:(0.0272959228605) A[2]:(0.00776775740087) A[3]:(0.00455174641684)\n",
      " state (7)  A[0]:(0.192465037107) A[1]:(0.79146105051) A[2]:(0.0145383337513) A[3]:(0.00153558328748)\n",
      " state (8)  A[0]:(0.000211431499338) A[1]:(0.9993891716) A[2]:(0.000394353526644) A[3]:(5.04726722284e-06)\n",
      " state (9)  A[0]:(7.32425223759e-06) A[1]:(0.999929785728) A[2]:(6.25824759481e-05) A[3]:(3.07415859879e-07)\n",
      " state (10)  A[0]:(1.98688667297e-06) A[1]:(0.999967515469) A[2]:(3.03689994325e-05) A[3]:(1.02284012371e-07)\n",
      " state (11)  A[0]:(1.0943945199e-06) A[1]:(0.999977111816) A[2]:(2.17156721192e-05) A[3]:(6.10777419752e-08)\n",
      " state (12)  A[0]:(7.98674250291e-07) A[1]:(0.999980986118) A[2]:(1.81422037713e-05) A[3]:(4.62021425562e-08)\n",
      " state (13)  A[0]:(6.65404456868e-07) A[1]:(0.999982953072) A[2]:(1.6323507225e-05) A[3]:(3.91785839327e-08)\n",
      " state (14)  A[0]:(5.94632069806e-07) A[1]:(0.99998408556) A[2]:(1.52830416482e-05) A[3]:(3.5356549688e-08)\n",
      " state (15)  A[0]:(5.5300705526e-07) A[1]:(0.999984800816) A[2]:(1.46401835082e-05) A[3]:(3.30937055537e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 153000 finished after 28 . Running score: 0.03. Policy_loss: -94040.3180754, Value_loss: 1.01294437566. Times trained:               12816. Times reached goal: 67.               Steps done: 1278597.\n",
      " state (0)  A[0]:(0.240624949336) A[1]:(0.191837236285) A[2]:(0.178068578243) A[3]:(0.389469265938)\n",
      " state (1)  A[0]:(0.0897493213415) A[1]:(0.0828268155456) A[2]:(0.0874401405454) A[3]:(0.739983737469)\n",
      " state (2)  A[0]:(0.0976149514318) A[1]:(0.0874693617225) A[2]:(0.0905647426844) A[3]:(0.72435092926)\n",
      " state (3)  A[0]:(0.213393479586) A[1]:(0.0984724313021) A[2]:(0.0951173007488) A[3]:(0.593016803265)\n",
      " state (4)  A[0]:(0.917166590691) A[1]:(0.0204012561589) A[2]:(0.0165333319455) A[3]:(0.0458988435566)\n",
      " state (5)  A[0]:(0.977997422218) A[1]:(0.00966229662299) A[2]:(0.00529701123014) A[3]:(0.00704324990511)\n",
      " state (6)  A[0]:(0.939344108105) A[1]:(0.0461066588759) A[2]:(0.00922595802695) A[3]:(0.00532328197733)\n",
      " state (7)  A[0]:(0.0736974030733) A[1]:(0.916437625885) A[2]:(0.00888745952398) A[3]:(0.000977509072982)\n",
      " state (8)  A[0]:(6.36646946077e-05) A[1]:(0.999738812447) A[2]:(0.000193992833374) A[3]:(3.52474944521e-06)\n",
      " state (9)  A[0]:(2.55313648267e-06) A[1]:(0.999964952469) A[2]:(3.22293490171e-05) A[3]:(2.78354178818e-07)\n",
      " state (10)  A[0]:(8.17170928258e-07) A[1]:(0.999982237816) A[2]:(1.68265050888e-05) A[3]:(1.11261378777e-07)\n",
      " state (11)  A[0]:(5.15595388606e-07) A[1]:(0.99998652935) A[2]:(1.28722931549e-05) A[3]:(7.57792619765e-08)\n",
      " state (12)  A[0]:(4.15505951423e-07) A[1]:(0.99998819828) A[2]:(1.13309333756e-05) A[3]:(6.28980174611e-08)\n",
      " state (13)  A[0]:(3.71150804312e-07) A[1]:(0.999988973141) A[2]:(1.05908093246e-05) A[3]:(5.69233549186e-08)\n",
      " state (14)  A[0]:(3.48133454509e-07) A[1]:(0.999989390373) A[2]:(1.01886898847e-05) A[3]:(5.37712736559e-08)\n",
      " state (15)  A[0]:(3.34856480322e-07) A[1]:(0.999989688396) A[2]:(9.94957827061e-06) A[3]:(5.19782368258e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 154000 finished after 16 . Running score: 0.08. Policy_loss: -93925.467283, Value_loss: 1.42489596173. Times trained:               14585. Times reached goal: 61.               Steps done: 1293182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.2772,  0.2159,  0.1973,  0.3096]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2772,  0.2159,  0.1973,  0.3096]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0894,  0.0847,  0.0892,  0.7368]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0994,  0.0923,  0.0953,  0.7130]])\n",
      "On state=2, selected action=2\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2436,  0.1108,  0.1073,  0.5383]])\n",
      "On state=3, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0994,  0.0923,  0.0953,  0.7130]])\n",
      "On state=2, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0994,  0.0923,  0.0953,  0.7130]])\n",
      "On state=2, selected action=0\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0994,  0.0923,  0.0953,  0.7130]])\n",
      "On state=2, selected action=0\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0994,  0.0923,  0.0953,  0.7130]])\n",
      "On state=2, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2442,  0.1108,  0.1073,  0.5377]])\n",
      "On state=3, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2443,  0.1108,  0.1073,  0.5376]])\n",
      "On state=3, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2444,  0.1108,  0.1073,  0.5375]])\n",
      "On state=3, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2445,  0.1108,  0.1073,  0.5374]])\n",
      "On state=3, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.2446,  0.1108,  0.1073,  0.5373]])\n",
      "On state=3, selected action=2\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.277200222015) A[1]:(0.215882658958) A[2]:(0.197269573808) A[3]:(0.309647500515)\n",
      " state (1)  A[0]:(0.0893844813108) A[1]:(0.0846855342388) A[2]:(0.0891546607018) A[3]:(0.73677533865)\n",
      " state (2)  A[0]:(0.099400319159) A[1]:(0.0923493057489) A[2]:(0.0952648818493) A[3]:(0.71298545599)\n",
      " state (3)  A[0]:(0.244997471571) A[1]:(0.110727474093) A[2]:(0.107230618596) A[3]:(0.537044405937)\n",
      " state (4)  A[0]:(0.94399356842) A[1]:(0.0175527706742) A[2]:(0.014900572598) A[3]:(0.0235531199723)\n",
      " state (5)  A[0]:(0.980855703354) A[1]:(0.00961780454963) A[2]:(0.0057491613552) A[3]:(0.00377730024047)\n",
      " state (6)  A[0]:(0.945296347141) A[1]:(0.0411430560052) A[2]:(0.0107036149129) A[3]:(0.00285699870437)\n",
      " state (7)  A[0]:(0.169071704149) A[1]:(0.80933368206) A[2]:(0.0206579491496) A[3]:(0.000936644733883)\n",
      " state (8)  A[0]:(0.000203877454624) A[1]:(0.998982071877) A[2]:(0.000810108496808) A[3]:(3.91823186874e-06)\n",
      " state (9)  A[0]:(4.43211001766e-06) A[1]:(0.999880373478) A[2]:(0.000115026923595) A[3]:(1.8464605489e-07)\n",
      " state (10)  A[0]:(9.71365579971e-07) A[1]:(0.999947071075) A[2]:(5.19166533195e-05) A[3]:(5.51830616757e-08)\n",
      " state (11)  A[0]:(5.13205748121e-07) A[1]:(0.999962568283) A[2]:(3.68655128113e-05) A[3]:(3.29154516976e-08)\n",
      " state (12)  A[0]:(3.79413705787e-07) A[1]:(0.999968349934) A[2]:(3.12503798341e-05) A[3]:(2.55986201125e-08)\n",
      " state (13)  A[0]:(3.23957038972e-07) A[1]:(0.999971032143) A[2]:(2.86237827822e-05) A[3]:(2.23585541193e-08)\n",
      " state (14)  A[0]:(2.96378772191e-07) A[1]:(0.999972462654) A[2]:(2.72271172435e-05) A[3]:(2.06786285872e-08)\n",
      " state (15)  A[0]:(2.81135044133e-07) A[1]:(0.999973297119) A[2]:(2.64235732175e-05) A[3]:(1.97247764788e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 155000 finished after 14 . Running score: 0.09. Policy_loss: -93925.4337472, Value_loss: 1.42559444673. Times trained:               12773. Times reached goal: 63.               Steps done: 1305955.\n",
      " state (0)  A[0]:(0.238116070628) A[1]:(0.167268514633) A[2]:(0.157164648175) A[3]:(0.437450766563)\n",
      " state (1)  A[0]:(0.0775505825877) A[1]:(0.072395786643) A[2]:(0.0770184919238) A[3]:(0.773035168648)\n",
      " state (2)  A[0]:(0.0806591585279) A[1]:(0.0746434181929) A[2]:(0.0785109847784) A[3]:(0.766186475754)\n",
      " state (3)  A[0]:(0.133275732398) A[1]:(0.0821896195412) A[2]:(0.0840659067035) A[3]:(0.700468719006)\n",
      " state (4)  A[0]:(0.824263572693) A[1]:(0.029613263905) A[2]:(0.0299385916442) A[3]:(0.116184577346)\n",
      " state (5)  A[0]:(0.976950883865) A[1]:(0.00686507951468) A[2]:(0.00574435899034) A[3]:(0.0104397023097)\n",
      " state (6)  A[0]:(0.9682970047) A[1]:(0.0168547853827) A[2]:(0.0077887699008) A[3]:(0.00705946376547)\n",
      " state (7)  A[0]:(0.494610279799) A[1]:(0.465260624886) A[2]:(0.0324813835323) A[3]:(0.00764772761613)\n",
      " state (8)  A[0]:(0.00142384483479) A[1]:(0.995426416397) A[2]:(0.00303136417642) A[3]:(0.000118354975712)\n",
      " state (9)  A[0]:(1.87978075701e-05) A[1]:(0.999604105949) A[2]:(0.000371888279915) A[3]:(5.20898220202e-06)\n",
      " state (10)  A[0]:(2.51085134551e-06) A[1]:(0.999860048294) A[2]:(0.000136196249514) A[3]:(1.22596668461e-06)\n",
      " state (11)  A[0]:(9.83522909337e-07) A[1]:(0.999913990498) A[2]:(8.43812522362e-05) A[3]:(6.17102102751e-07)\n",
      " state (12)  A[0]:(6.11524853866e-07) A[1]:(0.999933063984) A[2]:(6.58963035676e-05) A[3]:(4.3058821575e-07)\n",
      " state (13)  A[0]:(4.70830087806e-07) A[1]:(0.999941766262) A[2]:(5.73978904868e-05) A[3]:(3.50436863528e-07)\n",
      " state (14)  A[0]:(4.03752494549e-07) A[1]:(0.999946415424) A[2]:(5.28716591361e-05) A[3]:(3.08999688059e-07)\n",
      " state (15)  A[0]:(3.67103297094e-07) A[1]:(0.999949097633) A[2]:(5.02249495185e-05) A[3]:(2.85093051389e-07)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 156000 finished after 20 . Running score: 0.07. Policy_loss: -93925.4735669, Value_loss: 1.21143739508. Times trained:               13701. Times reached goal: 74.               Steps done: 1319656.\n",
      " state (0)  A[0]:(0.292269408703) A[1]:(0.192523464561) A[2]:(0.176173150539) A[3]:(0.339033961296)\n",
      " state (1)  A[0]:(0.0776968076825) A[1]:(0.0730455741286) A[2]:(0.0769652202725) A[3]:(0.772292375565)\n",
      " state (2)  A[0]:(0.0851793736219) A[1]:(0.0782989710569) A[2]:(0.0805976390839) A[3]:(0.75592404604)\n",
      " state (3)  A[0]:(0.298567712307) A[1]:(0.0894944965839) A[2]:(0.0866695046425) A[3]:(0.525268316269)\n",
      " state (4)  A[0]:(0.962666869164) A[1]:(0.0113245472312) A[2]:(0.00924328435212) A[3]:(0.0167653188109)\n",
      " state (5)  A[0]:(0.965909898281) A[1]:(0.0197979025543) A[2]:(0.00836768001318) A[3]:(0.00592449260876)\n",
      " state (6)  A[0]:(0.385672092438) A[1]:(0.578839600086) A[2]:(0.0308267902583) A[3]:(0.00466156424955)\n",
      " state (7)  A[0]:(0.000198365523829) A[1]:(0.998762965202) A[2]:(0.00102085724939) A[3]:(1.78346890607e-05)\n",
      " state (8)  A[0]:(2.78755442196e-06) A[1]:(0.999880969524) A[2]:(0.000115410475701) A[3]:(8.19913907435e-07)\n",
      " state (9)  A[0]:(6.91044306222e-07) A[1]:(0.999944269657) A[2]:(5.47370545974e-05) A[3]:(2.98113491226e-07)\n",
      " state (10)  A[0]:(4.0306767346e-07) A[1]:(0.999958753586) A[2]:(4.06542530982e-05) A[3]:(1.98108992322e-07)\n",
      " state (11)  A[0]:(3.12425925131e-07) A[1]:(0.999964296818) A[2]:(3.52030874637e-05) A[3]:(1.61614735816e-07)\n",
      " state (12)  A[0]:(2.71903815019e-07) A[1]:(0.999967098236) A[2]:(3.24904394802e-05) A[3]:(1.4397964776e-07)\n",
      " state (13)  A[0]:(2.50136167779e-07) A[1]:(0.999968707561) A[2]:(3.09329443553e-05) A[3]:(1.34219348524e-07)\n",
      " state (14)  A[0]:(2.37091953181e-07) A[1]:(0.999969661236) A[2]:(2.99546009046e-05) A[3]:(1.28468258254e-07)\n",
      " state (15)  A[0]:(2.2871071792e-07) A[1]:(0.999970316887) A[2]:(2.93025568681e-05) A[3]:(1.25002287632e-07)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 157000 finished after 16 . Running score: 0.07. Policy_loss: -93816.8278209, Value_loss: 0.996877461627. Times trained:               13470. Times reached goal: 73.               Steps done: 1333126.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.279197782278) A[1]:(0.170817479491) A[2]:(0.155418202281) A[3]:(0.39456653595)\n",
      " state (1)  A[0]:(0.0667619258165) A[1]:(0.0626779198647) A[2]:(0.0649937912822) A[3]:(0.805566370487)\n",
      " state (2)  A[0]:(0.0708796456456) A[1]:(0.0652103424072) A[2]:(0.0666612833738) A[3]:(0.797248721123)\n",
      " state (3)  A[0]:(0.208629280329) A[1]:(0.0727121680975) A[2]:(0.0718129426241) A[3]:(0.646845579147)\n",
      " state (4)  A[0]:(0.963987231255) A[1]:(0.00710903294384) A[2]:(0.00680412258953) A[3]:(0.0220996346325)\n",
      " state (5)  A[0]:(0.985254585743) A[1]:(0.00473851477727) A[2]:(0.00351788778789) A[3]:(0.00648902915418)\n",
      " state (6)  A[0]:(0.968262076378) A[1]:(0.0181644335389) A[2]:(0.00678349751979) A[3]:(0.00678999535739)\n",
      " state (7)  A[0]:(0.375279784203) A[1]:(0.589690089226) A[2]:(0.0268788654357) A[3]:(0.00815124157816)\n",
      " state (8)  A[0]:(0.00126739917323) A[1]:(0.996325016022) A[2]:(0.00223040254787) A[3]:(0.000177191992407)\n",
      " state (9)  A[0]:(3.42624662153e-05) A[1]:(0.999581158161) A[2]:(0.000369087472791) A[3]:(1.5486502889e-05)\n",
      " state (10)  A[0]:(6.34010075373e-06) A[1]:(0.999832749367) A[2]:(0.000155892412295) A[3]:(5.04199124407e-06)\n",
      " state (11)  A[0]:(2.70347322839e-06) A[1]:(0.999894320965) A[2]:(0.000100137709524) A[3]:(2.85007172351e-06)\n",
      " state (12)  A[0]:(1.6592796328e-06) A[1]:(0.999918818474) A[2]:(7.75068474468e-05) A[3]:(2.04151933758e-06)\n",
      " state (13)  A[0]:(1.21037055578e-06) A[1]:(0.999931573868) A[2]:(6.55922412989e-05) A[3]:(1.63548065757e-06)\n",
      " state (14)  A[0]:(9.66590732787e-07) A[1]:(0.999939441681) A[2]:(5.81741223868e-05) A[3]:(1.39017402034e-06)\n",
      " state (15)  A[0]:(8.14477289168e-07) A[1]:(0.999944925308) A[2]:(5.30453689862e-05) A[3]:(1.22505161926e-06)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 158000 finished after 14 . Running score: 0.06. Policy_loss: -92050.6374365, Value_loss: 0.985894114954. Times trained:               15677. Times reached goal: 59.               Steps done: 1348803.\n",
      " state (0)  A[0]:(0.370559841394) A[1]:(0.176145344973) A[2]:(0.157239854336) A[3]:(0.296054959297)\n",
      " state (1)  A[0]:(0.0606468878686) A[1]:(0.0558920502663) A[2]:(0.0590283200145) A[3]:(0.824432730675)\n",
      " state (2)  A[0]:(0.063595071435) A[1]:(0.0580150894821) A[2]:(0.0605901181698) A[3]:(0.817799687386)\n",
      " state (3)  A[0]:(0.210728853941) A[1]:(0.064396046102) A[2]:(0.0666576400399) A[3]:(0.658217430115)\n",
      " state (4)  A[0]:(0.973968148232) A[1]:(0.00458125118166) A[2]:(0.00493052555248) A[3]:(0.0165200922638)\n",
      " state (5)  A[0]:(0.987924933434) A[1]:(0.00317470286973) A[2]:(0.00284565775655) A[3]:(0.00605472642928)\n",
      " state (6)  A[0]:(0.979706287384) A[1]:(0.00854563899338) A[2]:(0.00486441003159) A[3]:(0.00688368687406)\n",
      " state (7)  A[0]:(0.791577219963) A[1]:(0.167134910822) A[2]:(0.024476474151) A[3]:(0.0168113671243)\n",
      " state (8)  A[0]:(0.0264082681388) A[1]:(0.954487323761) A[2]:(0.0149606242776) A[3]:(0.00414377916604)\n",
      " state (9)  A[0]:(0.000680557452142) A[1]:(0.995818197727) A[2]:(0.00300603662618) A[3]:(0.000495217856951)\n",
      " state (10)  A[0]:(9.10130474949e-05) A[1]:(0.998544335365) A[2]:(0.00120598333888) A[3]:(0.000158668728545)\n",
      " state (11)  A[0]:(3.01427771774e-05) A[1]:(0.999159932137) A[2]:(0.000723857316189) A[3]:(8.60785148689e-05)\n",
      " state (12)  A[0]:(1.58982184075e-05) A[1]:(0.999386548996) A[2]:(0.000536860025022) A[3]:(6.07132787991e-05)\n",
      " state (13)  A[0]:(1.08383355837e-05) A[1]:(0.999491393566) A[2]:(0.000448411912657) A[3]:(4.93331426696e-05)\n",
      " state (14)  A[0]:(8.5412784756e-06) A[1]:(0.999547183514) A[2]:(0.000400883494876) A[3]:(4.33716413681e-05)\n",
      " state (15)  A[0]:(7.2917041507e-06) A[1]:(0.999580681324) A[2]:(0.000372212554794) A[3]:(3.98157390009e-05)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 159000 finished after 11 . Running score: 0.09. Policy_loss: -92050.6402913, Value_loss: 0.97840592291. Times trained:               16904. Times reached goal: 66.               Steps done: 1365707.\n",
      "action_dist \n",
      "tensor([[ 0.3929,  0.1984,  0.1774,  0.2313]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.3929,  0.1984,  0.1774,  0.2313]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0606,  0.0568,  0.0598,  0.8229]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0606,  0.0568,  0.0598,  0.8229]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.3929,  0.1984,  0.1774,  0.2312]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.3929,  0.1984,  0.1774,  0.2312]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.3930,  0.1984,  0.1774,  0.2312]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.3930,  0.1984,  0.1774,  0.2312]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0606,  0.0568,  0.0598,  0.8229]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0622,  0.0590,  0.0614,  0.8174]])\n",
      "On state=2, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0622,  0.0590,  0.0614,  0.8174]])\n",
      "On state=2, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0622,  0.0590,  0.0614,  0.8174]])\n",
      "On state=2, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0606,  0.0568,  0.0598,  0.8229]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0622,  0.0590,  0.0614,  0.8174]])\n",
      "On state=2, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0606,  0.0568,  0.0598,  0.8229]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0606,  0.0568,  0.0598,  0.8229]])\n",
      "On state=1, selected action=0\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.392934083939) A[1]:(0.198425367475) A[2]:(0.177422225475) A[3]:(0.231218323112)\n",
      " state (1)  A[0]:(0.0605610907078) A[1]:(0.0567640215158) A[2]:(0.0597944296896) A[3]:(0.822880446911)\n",
      " state (2)  A[0]:(0.0621524564922) A[1]:(0.0590034537017) A[2]:(0.0614093355834) A[3]:(0.817434728146)\n",
      " state (3)  A[0]:(0.0974657982588) A[1]:(0.067099519074) A[2]:(0.0674159154296) A[3]:(0.768018782139)\n",
      " state (4)  A[0]:(0.889629840851) A[1]:(0.0171979293227) A[2]:(0.016690980643) A[3]:(0.0764812678099)\n",
      " state (5)  A[0]:(0.986165881157) A[1]:(0.00405975710601) A[2]:(0.00303185521625) A[3]:(0.00674250815064)\n",
      " state (6)  A[0]:(0.959395885468) A[1]:(0.0243162624538) A[2]:(0.00722346222028) A[3]:(0.00906439591199)\n",
      " state (7)  A[0]:(0.13878634572) A[1]:(0.830686986446) A[2]:(0.0203054677695) A[3]:(0.010221186094)\n",
      " state (8)  A[0]:(0.000771316466853) A[1]:(0.996726095676) A[2]:(0.00197921530344) A[3]:(0.000523379654624)\n",
      " state (9)  A[0]:(4.20698415837e-05) A[1]:(0.999366104603) A[2]:(0.000489428406581) A[3]:(0.000102399550087)\n",
      " state (10)  A[0]:(9.84143662208e-06) A[1]:(0.999704241753) A[2]:(0.000239504108322) A[3]:(4.64125478175e-05)\n",
      " state (11)  A[0]:(4.45174828201e-06) A[1]:(0.999804019928) A[2]:(0.00016115247854) A[3]:(3.04039058392e-05)\n",
      " state (12)  A[0]:(2.75355341728e-06) A[1]:(0.999847054482) A[2]:(0.000126518512843) A[3]:(2.36465657508e-05)\n",
      " state (13)  A[0]:(1.99927421818e-06) A[1]:(0.999870359898) A[2]:(0.000107613261207) A[3]:(2.00457125175e-05)\n",
      " state (14)  A[0]:(1.58270938755e-06) A[1]:(0.999885082245) A[2]:(9.56033327384e-05) A[3]:(1.7758591639e-05)\n",
      " state (15)  A[0]:(1.31652166147e-06) A[1]:(0.999895513058) A[2]:(8.70762960403e-05) A[3]:(1.60905456141e-05)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 160000 finished after 16 . Running score: 0.06. Policy_loss: -92050.66367, Value_loss: 0.978442268898. Times trained:               14596. Times reached goal: 77.               Steps done: 1380303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.387752383947) A[1]:(0.180763587356) A[2]:(0.162577688694) A[3]:(0.268906325102)\n",
      " state (1)  A[0]:(0.0561742670834) A[1]:(0.0524470172822) A[2]:(0.0552964434028) A[3]:(0.836082279682)\n",
      " state (2)  A[0]:(0.0567432120442) A[1]:(0.0538838990033) A[2]:(0.0560470037162) A[3]:(0.833325862885)\n",
      " state (3)  A[0]:(0.0851876661181) A[1]:(0.0603637360036) A[2]:(0.0598789192736) A[3]:(0.794569671154)\n",
      " state (4)  A[0]:(0.904323816299) A[1]:(0.0143251437694) A[2]:(0.0125925317407) A[3]:(0.068758495152)\n",
      " state (5)  A[0]:(0.987831115723) A[1]:(0.00343687622808) A[2]:(0.0021874580998) A[3]:(0.00654452526942)\n",
      " state (6)  A[0]:(0.973990559578) A[1]:(0.013376631774) A[2]:(0.00400196900591) A[3]:(0.00863084662706)\n",
      " state (7)  A[0]:(0.54150146246) A[1]:(0.415914148092) A[2]:(0.0176989529282) A[3]:(0.0248854290694)\n",
      " state (8)  A[0]:(0.00659514497966) A[1]:(0.987190425396) A[2]:(0.00312921963632) A[3]:(0.0030851920601)\n",
      " state (9)  A[0]:(0.000207007222343) A[1]:(0.998766481876) A[2]:(0.000550433527678) A[3]:(0.000476073473692)\n",
      " state (10)  A[0]:(3.09993301926e-05) A[1]:(0.999586224556) A[2]:(0.00020797153411) A[3]:(0.000174832894118)\n",
      " state (11)  A[0]:(1.06963643702e-05) A[1]:(0.999768733978) A[2]:(0.000119746589917) A[3]:(0.000100807126728)\n",
      " state (12)  A[0]:(5.59246336707e-06) A[1]:(0.999836683273) A[2]:(8.53377277963e-05) A[3]:(7.23747143638e-05)\n",
      " state (13)  A[0]:(3.67040684068e-06) A[1]:(0.999869525433) A[2]:(6.84443293721e-05) A[3]:(5.83371038374e-05)\n",
      " state (14)  A[0]:(2.7489149943e-06) A[1]:(0.999888300896) A[2]:(5.88313123444e-05) A[3]:(5.01440808875e-05)\n",
      " state (15)  A[0]:(2.22107473746e-06) A[1]:(0.999900519848) A[2]:(5.26315852767e-05) A[3]:(4.46564736194e-05)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 161000 finished after 4 . Running score: 0.08. Policy_loss: -92050.6521681, Value_loss: 1.20134291196. Times trained:               15791. Times reached goal: 80.               Steps done: 1396094.\n",
      " state (0)  A[0]:(0.335056096315) A[1]:(0.162930443883) A[2]:(0.146853730083) A[3]:(0.355159729719)\n",
      " state (1)  A[0]:(0.0501897372305) A[1]:(0.0480815880001) A[2]:(0.0499476492405) A[3]:(0.851781010628)\n",
      " state (2)  A[0]:(0.0500841252506) A[1]:(0.0495714060962) A[2]:(0.0502328313887) A[3]:(0.850111663342)\n",
      " state (3)  A[0]:(0.0792816206813) A[1]:(0.0581088140607) A[2]:(0.0534656904638) A[3]:(0.809143841267)\n",
      " state (4)  A[0]:(0.935014128685) A[1]:(0.0124620813876) A[2]:(0.00805798824877) A[3]:(0.0444657690823)\n",
      " state (5)  A[0]:(0.979702532291) A[1]:(0.0086331712082) A[2]:(0.0027667793911) A[3]:(0.00889750570059)\n",
      " state (6)  A[0]:(0.854043781757) A[1]:(0.116813607514) A[2]:(0.00893436279148) A[3]:(0.0202082376927)\n",
      " state (7)  A[0]:(0.0239826366305) A[1]:(0.965384781361) A[2]:(0.00408547837287) A[3]:(0.00654712831602)\n",
      " state (8)  A[0]:(0.000127509003505) A[1]:(0.99921131134) A[2]:(0.000266791175818) A[3]:(0.000394372647861)\n",
      " state (9)  A[0]:(8.07731339592e-06) A[1]:(0.999837517738) A[2]:(6.03509797656e-05) A[3]:(9.40426398301e-05)\n",
      " state (10)  A[0]:(2.08706705962e-06) A[1]:(0.999922037125) A[2]:(2.87212005787e-05) A[3]:(4.71468774776e-05)\n",
      " state (11)  A[0]:(1.02225487808e-06) A[1]:(0.999946832657) A[2]:(1.93347623281e-05) A[3]:(3.28034257109e-05)\n",
      " state (12)  A[0]:(6.73392150929e-07) A[1]:(0.999957561493) A[2]:(1.53212058649e-05) A[3]:(2.64582831733e-05)\n",
      " state (13)  A[0]:(5.1513274002e-07) A[1]:(0.999963343143) A[2]:(1.31919323394e-05) A[3]:(2.292710451e-05)\n",
      " state (14)  A[0]:(4.26822225563e-07) A[1]:(0.999967098236) A[2]:(1.18753478091e-05) A[3]:(2.06097593036e-05)\n",
      " state (15)  A[0]:(3.69822629409e-07) A[1]:(0.999969780445) A[2]:(1.0958961866e-05) A[3]:(1.88997018995e-05)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 162000 finished after 38 . Running score: 0.08. Policy_loss: -92050.6900753, Value_loss: 0.978521934815. Times trained:               16191. Times reached goal: 83.               Steps done: 1412285.\n",
      " state (0)  A[0]:(0.415615826845) A[1]:(0.177757292986) A[2]:(0.151951014996) A[3]:(0.254675865173)\n",
      " state (1)  A[0]:(0.0468802824616) A[1]:(0.0444974303246) A[2]:(0.0458557605743) A[3]:(0.862766504288)\n",
      " state (2)  A[0]:(0.0462342910469) A[1]:(0.0456185229123) A[2]:(0.0458037182689) A[3]:(0.862343490124)\n",
      " state (3)  A[0]:(0.0797117128968) A[1]:(0.0538740977645) A[2]:(0.0486287400126) A[3]:(0.817785441875)\n",
      " state (4)  A[0]:(0.963203072548) A[1]:(0.00728573417291) A[2]:(0.00442693429068) A[3]:(0.0250842738897)\n",
      " state (5)  A[0]:(0.982726037502) A[1]:(0.00707822525874) A[2]:(0.00217099720612) A[3]:(0.00802472326905)\n",
      " state (6)  A[0]:(0.875161767006) A[1]:(0.0961817204952) A[2]:(0.00752789294347) A[3]:(0.0211286265403)\n",
      " state (7)  A[0]:(0.0471353232861) A[1]:(0.935262322426) A[2]:(0.0055539724417) A[3]:(0.0120484093204)\n",
      " state (8)  A[0]:(0.00054807739798) A[1]:(0.997657895088) A[2]:(0.000590516312514) A[3]:(0.00120348192286)\n",
      " state (9)  A[0]:(3.49220499629e-05) A[1]:(0.999526917934) A[2]:(0.00013966714323) A[3]:(0.000298473314615)\n",
      " state (10)  A[0]:(7.16504837328e-06) A[1]:(0.999796509743) A[2]:(5.99455488555e-05) A[3]:(0.000136404662044)\n",
      " state (11)  A[0]:(2.92352115139e-06) A[1]:(0.999871969223) A[2]:(3.69264198525e-05) A[3]:(8.81715022842e-05)\n",
      " state (12)  A[0]:(1.70492251073e-06) A[1]:(0.999902844429) A[2]:(2.75357178907e-05) A[3]:(6.79213480907e-05)\n",
      " state (13)  A[0]:(1.20571212392e-06) A[1]:(0.99991863966) A[2]:(2.27929212997e-05) A[3]:(5.73463366891e-05)\n",
      " state (14)  A[0]:(9.51672404881e-07) A[1]:(0.999928116798) A[2]:(2.00332433451e-05) A[3]:(5.09207056894e-05)\n",
      " state (15)  A[0]:(7.99813903996e-07) A[1]:(0.999934494495) A[2]:(1.82239182323e-05) A[3]:(4.64942168037e-05)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 163000 finished after 8 . Running score: 0.12. Policy_loss: -92050.6446255, Value_loss: 0.995473352827. Times trained:               16672. Times reached goal: 94.               Steps done: 1428957.\n",
      " state (0)  A[0]:(0.39240565896) A[1]:(0.184317052364) A[2]:(0.14251421392) A[3]:(0.280763059855)\n",
      " state (1)  A[0]:(0.0378684401512) A[1]:(0.0393278151751) A[2]:(0.0368381775916) A[3]:(0.885965585709)\n",
      " state (2)  A[0]:(0.0364418700337) A[1]:(0.0403034128249) A[2]:(0.0356749221683) A[3]:(0.887579798698)\n",
      " state (3)  A[0]:(0.0990957245231) A[1]:(0.0526150166988) A[2]:(0.0376802049577) A[3]:(0.810609042645)\n",
      " state (4)  A[0]:(0.981070578098) A[1]:(0.00511917797849) A[2]:(0.00178166141268) A[3]:(0.0120285609737)\n",
      " state (5)  A[0]:(0.946703791618) A[1]:(0.0357387214899) A[2]:(0.00298108696006) A[3]:(0.0145763708279)\n",
      " state (6)  A[0]:(0.0185704957694) A[1]:(0.974009037018) A[2]:(0.00171174807474) A[3]:(0.00570870097727)\n",
      " state (7)  A[0]:(2.08105539059e-05) A[1]:(0.999800622463) A[2]:(4.10161592299e-05) A[3]:(0.000137579729198)\n",
      " state (8)  A[0]:(1.43054228374e-06) A[1]:(0.999955713749) A[2]:(8.86225370778e-06) A[3]:(3.39782927767e-05)\n",
      " state (9)  A[0]:(5.42609257081e-07) A[1]:(0.99997395277) A[2]:(5.02337434227e-06) A[3]:(2.04643602046e-05)\n",
      " state (10)  A[0]:(3.6026244743e-07) A[1]:(0.999979376793) A[2]:(3.9387259676e-06) A[3]:(1.63158238138e-05)\n",
      " state (11)  A[0]:(2.92143568004e-07) A[1]:(0.999981880188) A[2]:(3.47247669197e-06) A[3]:(1.43836023199e-05)\n",
      " state (12)  A[0]:(2.58129318809e-07) A[1]:(0.999983251095) A[2]:(3.22039932144e-06) A[3]:(1.3269770534e-05)\n",
      " state (13)  A[0]:(2.38395458041e-07) A[1]:(0.999984145164) A[2]:(3.06622541757e-06) A[3]:(1.25615288198e-05)\n",
      " state (14)  A[0]:(2.25991655611e-07) A[1]:(0.999984741211) A[2]:(2.96562393487e-06) A[3]:(1.20935210361e-05)\n",
      " state (15)  A[0]:(2.17832351268e-07) A[1]:(0.999985098839) A[2]:(2.89761442218e-06) A[3]:(1.1780875866e-05)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 164000 finished after 25 . Running score: 0.11. Policy_loss: -92050.6136118, Value_loss: 1.191070988. Times trained:               17683. Times reached goal: 93.               Steps done: 1446640.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.4436,  0.2166,  0.1348,  0.2050]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0260,  0.0333,  0.0247,  0.9160]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.4437,  0.2166,  0.1348,  0.2050]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.4437,  0.2165,  0.1348,  0.2050]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9840,  0.0045,  0.0010,  0.0105]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.2469e-06,  9.9999e-01,  1.4571e-06,  7.0359e-06]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.443692028522) A[1]:(0.216531202197) A[2]:(0.134808406234) A[3]:(0.204968392849)\n",
      " state (1)  A[0]:(0.0260405968875) A[1]:(0.0332759842277) A[2]:(0.0246558003128) A[3]:(0.916027605534)\n",
      " state (2)  A[0]:(0.0243525654078) A[1]:(0.0336597189307) A[2]:(0.023356910795) A[3]:(0.91863077879)\n",
      " state (3)  A[0]:(0.0532832182944) A[1]:(0.0438457876444) A[2]:(0.0243083909154) A[3]:(0.878562569618)\n",
      " state (4)  A[0]:(0.983985126019) A[1]:(0.00450539402664) A[2]:(0.00100827158894) A[3]:(0.0105012068525)\n",
      " state (5)  A[0]:(0.960852861404) A[1]:(0.0293578803539) A[2]:(0.00117813586257) A[3]:(0.00861109886318)\n",
      " state (6)  A[0]:(0.0123157072812) A[1]:(0.985934793949) A[2]:(0.000317482685205) A[3]:(0.00143201416358)\n",
      " state (7)  A[0]:(2.43785652856e-05) A[1]:(0.999939799309) A[2]:(6.67138692734e-06) A[3]:(2.91673914035e-05)\n",
      " state (8)  A[0]:(2.24758423428e-06) A[1]:(0.999989271164) A[2]:(1.45720423461e-06) A[3]:(7.03600017005e-06)\n",
      " state (9)  A[0]:(9.58304440246e-07) A[1]:(0.999993979931) A[2]:(8.37087668515e-07) A[3]:(4.21853519583e-06)\n",
      " state (10)  A[0]:(6.74206660278e-07) A[1]:(0.999995291233) A[2]:(6.63894581976e-07) A[3]:(3.38144309353e-06)\n",
      " state (11)  A[0]:(5.6852434227e-07) A[1]:(0.999995827675) A[2]:(5.92524145304e-07) A[3]:(3.01536852021e-06)\n",
      " state (12)  A[0]:(5.18304489106e-07) A[1]:(0.999996125698) A[2]:(5.56710574529e-07) A[3]:(2.82168502963e-06)\n",
      " state (13)  A[0]:(4.91245771173e-07) A[1]:(0.999996244907) A[2]:(5.367750191e-07) A[3]:(2.70923624157e-06)\n",
      " state (14)  A[0]:(4.75642963238e-07) A[1]:(0.999996364117) A[2]:(5.25039695276e-07) A[3]:(2.64096070168e-06)\n",
      " state (15)  A[0]:(4.66295546175e-07) A[1]:(0.999996423721) A[2]:(5.17916191711e-07) A[3]:(2.59857870333e-06)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 165000 finished after 6 . Running score: 0.1. Policy_loss: -92050.6109505, Value_loss: 1.625340582. Times trained:               19217. Times reached goal: 97.               Steps done: 1465857.\n",
      " state (0)  A[0]:(0.406033366919) A[1]:(0.211844369769) A[2]:(0.109701275826) A[3]:(0.272420972586)\n",
      " state (1)  A[0]:(0.0148417428136) A[1]:(0.0237955115736) A[2]:(0.0142852775753) A[3]:(0.947077453136)\n",
      " state (2)  A[0]:(0.0140619892627) A[1]:(0.0237594246864) A[2]:(0.013589235954) A[3]:(0.948589324951)\n",
      " state (3)  A[0]:(0.0725347325206) A[1]:(0.034764289856) A[2]:(0.0162039455026) A[3]:(0.876497030258)\n",
      " state (4)  A[0]:(0.991725027561) A[1]:(0.00180388696026) A[2]:(0.000429215462646) A[3]:(0.00604186113924)\n",
      " state (5)  A[0]:(0.990460038185) A[1]:(0.0037734801881) A[2]:(0.000446122256108) A[3]:(0.00532037671655)\n",
      " state (6)  A[0]:(0.917079925537) A[1]:(0.0695155113935) A[2]:(0.00127522856928) A[3]:(0.0121293067932)\n",
      " state (7)  A[0]:(0.0213374588639) A[1]:(0.975920498371) A[2]:(0.000326453271555) A[3]:(0.00241561396979)\n",
      " state (8)  A[0]:(0.000156735870405) A[1]:(0.999717414379) A[2]:(1.43026481965e-05) A[3]:(0.000111518871563)\n",
      " state (9)  A[0]:(1.60060153576e-05) A[1]:(0.999952614307) A[2]:(3.24458915202e-06) A[3]:(2.81150441879e-05)\n",
      " state (10)  A[0]:(5.85254701946e-06) A[1]:(0.999977111816) A[2]:(1.67428845543e-06) A[3]:(1.53486780619e-05)\n",
      " state (11)  A[0]:(3.55380439032e-06) A[1]:(0.999983966351) A[2]:(1.20307436191e-06) A[3]:(1.12972857096e-05)\n",
      " state (12)  A[0]:(2.67757877737e-06) A[1]:(0.999986886978) A[2]:(9.96074845716e-07) A[3]:(9.44136081671e-06)\n",
      " state (13)  A[0]:(2.24492191592e-06) A[1]:(0.999988436699) A[2]:(8.8492379291e-07) A[3]:(8.41326800582e-06)\n",
      " state (14)  A[0]:(2.00027307073e-06) A[1]:(0.999989390373) A[2]:(8.18564330984e-07) A[3]:(7.785476555e-06)\n",
      " state (15)  A[0]:(1.85123792562e-06) A[1]:(0.99998998642) A[2]:(7.76630827204e-07) A[3]:(7.38235257813e-06)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 166000 finished after 20 . Running score: 0.09. Policy_loss: -92050.6359657, Value_loss: 0.993044283496. Times trained:               19624. Times reached goal: 100.               Steps done: 1485481.\n",
      " state (0)  A[0]:(0.501979887486) A[1]:(0.187857300043) A[2]:(0.0964353680611) A[3]:(0.213727399707)\n",
      " state (1)  A[0]:(0.0120549006388) A[1]:(0.0184354260564) A[2]:(0.0106095727533) A[3]:(0.958900094032)\n",
      " state (2)  A[0]:(0.011105899699) A[1]:(0.0183368474245) A[2]:(0.0100979562849) A[3]:(0.960459291935)\n",
      " state (3)  A[0]:(0.0161457583308) A[1]:(0.022121982649) A[2]:(0.0103299347684) A[3]:(0.951402306557)\n",
      " state (4)  A[0]:(0.982637107372) A[1]:(0.00404183613136) A[2]:(0.000587795220781) A[3]:(0.0127332331613)\n",
      " state (5)  A[0]:(0.882780730724) A[1]:(0.106064945459) A[2]:(0.000788409553934) A[3]:(0.0103659396991)\n",
      " state (6)  A[0]:(0.000410756125348) A[1]:(0.999464392662) A[2]:(1.03773872979e-05) A[3]:(0.000114489193948)\n",
      " state (7)  A[0]:(6.29750320513e-06) A[1]:(0.999984443188) A[2]:(5.86161377214e-07) A[3]:(8.69260111358e-06)\n",
      " state (8)  A[0]:(2.15204772758e-06) A[1]:(0.999992966652) A[2]:(2.73906579196e-07) A[3]:(4.58183649243e-06)\n",
      " state (9)  A[0]:(1.54163899424e-06) A[1]:(0.999994516373) A[2]:(2.15541234638e-07) A[3]:(3.74459887098e-06)\n",
      " state (10)  A[0]:(1.35469065299e-06) A[1]:(0.99999499321) A[2]:(1.96314573486e-07) A[3]:(3.45744410879e-06)\n",
      " state (11)  A[0]:(1.27845942188e-06) A[1]:(0.999995172024) A[2]:(1.88245564914e-07) A[3]:(3.33458206114e-06)\n",
      " state (12)  A[0]:(1.24290215808e-06) A[1]:(0.999995291233) A[2]:(1.84433190498e-07) A[3]:(3.27612906403e-06)\n",
      " state (13)  A[0]:(1.22513552014e-06) A[1]:(0.999995350838) A[2]:(1.82517240432e-07) A[3]:(3.24675534102e-06)\n",
      " state (14)  A[0]:(1.21592256619e-06) A[1]:(0.999995350838) A[2]:(1.81521201625e-07) A[3]:(3.23154449688e-06)\n",
      " state (15)  A[0]:(1.21104130812e-06) A[1]:(0.999995410442) A[2]:(1.80992955734e-07) A[3]:(3.22353571391e-06)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 167000 finished after 4 . Running score: 0.16. Policy_loss: -92050.6137905, Value_loss: 1.41197947888. Times trained:               21580. Times reached goal: 106.               Steps done: 1507061.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.467097729445) A[1]:(0.186178088188) A[2]:(0.0917736440897) A[3]:(0.254950523376)\n",
      " state (1)  A[0]:(0.00852480437607) A[1]:(0.0137621760368) A[2]:(0.0077315592207) A[3]:(0.969981431961)\n",
      " state (2)  A[0]:(0.00791263580322) A[1]:(0.0135129913688) A[2]:(0.00732246320695) A[3]:(0.971251904964)\n",
      " state (3)  A[0]:(0.00970256142318) A[1]:(0.0146050797775) A[2]:(0.00715058483183) A[3]:(0.968541800976)\n",
      " state (4)  A[0]:(0.984345853329) A[1]:(0.00212150486186) A[2]:(0.000437347451225) A[3]:(0.0130952978507)\n",
      " state (5)  A[0]:(0.968005776405) A[1]:(0.0269610993564) A[2]:(0.000273051438853) A[3]:(0.00476005161181)\n",
      " state (6)  A[0]:(0.00225260388106) A[1]:(0.997617125511) A[2]:(1.0809874766e-05) A[3]:(0.000119461990835)\n",
      " state (7)  A[0]:(5.8313489717e-05) A[1]:(0.999931871891) A[2]:(8.14907195945e-07) A[3]:(8.97840800462e-06)\n",
      " state (8)  A[0]:(1.8476981495e-05) A[1]:(0.999977111816) A[2]:(3.58364616204e-07) A[3]:(4.04289539802e-06)\n",
      " state (9)  A[0]:(1.14853019113e-05) A[1]:(0.999985337257) A[2]:(2.54569897606e-07) A[3]:(2.91486549031e-06)\n",
      " state (10)  A[0]:(9.18626028579e-06) A[1]:(0.999988079071) A[2]:(2.16666947495e-07) A[3]:(2.5023039143e-06)\n",
      " state (11)  A[0]:(8.22381571197e-06) A[1]:(0.999989271164) A[2]:(2.00009068863e-07) A[3]:(2.32118691201e-06)\n",
      " state (12)  A[0]:(7.77413606556e-06) A[1]:(0.999989807606) A[2]:(1.92040104707e-07) A[3]:(2.23476581596e-06)\n",
      " state (13)  A[0]:(7.55226938054e-06) A[1]:(0.999990046024) A[2]:(1.88063012274e-07) A[3]:(2.19178855332e-06)\n",
      " state (14)  A[0]:(7.43961663829e-06) A[1]:(0.999990224838) A[2]:(1.86033418004e-07) A[3]:(2.16994271796e-06)\n",
      " state (15)  A[0]:(7.38150311008e-06) A[1]:(0.999990284443) A[2]:(1.84984330076e-07) A[3]:(2.15869977183e-06)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 168000 finished after 17 . Running score: 0.16. Policy_loss: -92050.6290116, Value_loss: 1.19120696301. Times trained:               21983. Times reached goal: 112.               Steps done: 1529044.\n",
      " state (0)  A[0]:(0.493153274059) A[1]:(0.184058099985) A[2]:(0.0892394632101) A[3]:(0.233549162745)\n",
      " state (1)  A[0]:(0.00778539525345) A[1]:(0.0122486082837) A[2]:(0.00657980609685) A[3]:(0.97338616848)\n",
      " state (2)  A[0]:(0.0070672002621) A[1]:(0.0119234398007) A[2]:(0.00613644858822) A[3]:(0.974872887135)\n",
      " state (3)  A[0]:(0.00974418781698) A[1]:(0.0132704135031) A[2]:(0.00614144001156) A[3]:(0.970843970776)\n",
      " state (4)  A[0]:(0.995928168297) A[1]:(0.0007474248996) A[2]:(0.000133663837914) A[3]:(0.00319072697312)\n",
      " state (5)  A[0]:(0.992973566055) A[1]:(0.00559496274218) A[2]:(9.6897369076e-05) A[3]:(0.00133459991775)\n",
      " state (6)  A[0]:(0.181827425957) A[1]:(0.817336916924) A[2]:(0.000113320675155) A[3]:(0.000722370750736)\n",
      " state (7)  A[0]:(0.000547480129171) A[1]:(0.999443113804) A[2]:(1.90729292626e-06) A[3]:(7.49755963625e-06)\n",
      " state (8)  A[0]:(7.24245182937e-05) A[1]:(0.999925553799) A[2]:(4.48463993052e-07) A[3]:(1.58049886068e-06)\n",
      " state (9)  A[0]:(3.76814350602e-05) A[1]:(0.999961078167) A[2]:(2.81374639144e-07) A[3]:(9.6541214134e-07)\n",
      " state (10)  A[0]:(2.88129813271e-05) A[1]:(0.999970138073) A[2]:(2.32604534744e-07) A[3]:(7.91950128587e-07)\n",
      " state (11)  A[0]:(2.5420304155e-05) A[1]:(0.999973654747) A[2]:(2.12949473166e-07) A[3]:(7.23705511518e-07)\n",
      " state (12)  A[0]:(2.38970369537e-05) A[1]:(0.999975204468) A[2]:(2.03944594546e-07) A[3]:(6.93157403475e-07)\n",
      " state (13)  A[0]:(2.31591693591e-05) A[1]:(0.999975979328) A[2]:(1.99562478542e-07) A[3]:(6.7867836151e-07)\n",
      " state (14)  A[0]:(2.2785827241e-05) A[1]:(0.999976336956) A[2]:(1.97354225406e-07) A[3]:(6.71610280278e-07)\n",
      " state (15)  A[0]:(2.25910516747e-05) A[1]:(0.99997651577) A[2]:(1.96213250092e-07) A[3]:(6.68100710755e-07)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 169000 finished after 9 . Running score: 0.12. Policy_loss: -92050.6144939, Value_loss: 1.19114266415. Times trained:               21874. Times reached goal: 110.               Steps done: 1550918.\n",
      "action_dist \n",
      "tensor([[ 0.5139,  0.1825,  0.0954,  0.2082]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0001,  0.0013]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0001,  0.0013]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0001,  0.0013]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0001,  0.0013]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5138,  0.1825,  0.0954,  0.2082]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0001,  0.0013]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5138,  0.1825,  0.0954,  0.2083]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0001,  0.0013]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0001,  0.0013]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5138,  0.1825,  0.0954,  0.2083]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0001,  0.0013]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.5158e-05,  9.9996e-01,  2.0100e-07,  3.5938e-07]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4594e-05,  9.9997e-01,  1.5573e-07,  2.7458e-07]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.1607e-05,  9.9998e-01,  1.4218e-07,  2.4997e-07]])\n",
      "On state=10, selected action=1\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.51376247406) A[1]:(0.182525247335) A[2]:(0.0953931137919) A[3]:(0.208319216967)\n",
      " state (1)  A[0]:(0.00777912512422) A[1]:(0.0115720303729) A[2]:(0.00666732992977) A[3]:(0.973981499672)\n",
      " state (2)  A[0]:(0.00681909127161) A[1]:(0.0110583119094) A[2]:(0.00604796642438) A[3]:(0.976074635983)\n",
      " state (3)  A[0]:(0.0109180547297) A[1]:(0.0128465229645) A[2]:(0.00608543073758) A[3]:(0.970149993896)\n",
      " state (4)  A[0]:(0.99798566103) A[1]:(0.000642176542897) A[2]:(6.7968685471e-05) A[3]:(0.00130417500623)\n",
      " state (5)  A[0]:(0.952727079391) A[1]:(0.0461642853916) A[2]:(0.000129799038405) A[3]:(0.000978846219368)\n",
      " state (6)  A[0]:(0.0132188601419) A[1]:(0.986726224422) A[2]:(1.43811785165e-05) A[3]:(4.05196879001e-05)\n",
      " state (7)  A[0]:(0.000125406178995) A[1]:(0.999873101711) A[2]:(5.03304931954e-07) A[3]:(9.63446382229e-07)\n",
      " state (8)  A[0]:(3.51598719135e-05) A[1]:(0.999964296818) A[2]:(2.00985340371e-07) A[3]:(3.5932853848e-07)\n",
      " state (9)  A[0]:(2.45954124694e-05) A[1]:(0.999974966049) A[2]:(1.55724293904e-07) A[3]:(2.74552320434e-07)\n",
      " state (10)  A[0]:(2.16081862163e-05) A[1]:(0.999978005886) A[2]:(1.42182642549e-07) A[3]:(2.49966575439e-07)\n",
      " state (11)  A[0]:(2.04600946745e-05) A[1]:(0.999979138374) A[2]:(1.36924683147e-07) A[3]:(2.40722727085e-07)\n",
      " state (12)  A[0]:(1.99592341232e-05) A[1]:(0.999979674816) A[2]:(1.34648004746e-07) A[3]:(2.36874200255e-07)\n",
      " state (13)  A[0]:(1.97262361326e-05) A[1]:(0.999979913235) A[2]:(1.33606960162e-07) A[3]:(2.35201071064e-07)\n",
      " state (14)  A[0]:(1.96131823031e-05) A[1]:(0.999980032444) A[2]:(1.33113630341e-07) A[3]:(2.34457360193e-07)\n",
      " state (15)  A[0]:(1.95563279703e-05) A[1]:(0.999980092049) A[2]:(1.32872159497e-07) A[3]:(2.34122666143e-07)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 170000 finished after 15 . Running score: 0.08. Policy_loss: -92050.6185307, Value_loss: 1.19214997215. Times trained:               21971. Times reached goal: 113.               Steps done: 1572889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.639933288097) A[1]:(0.125302836299) A[2]:(0.072751455009) A[3]:(0.162012457848)\n",
      " state (1)  A[0]:(0.00644391681999) A[1]:(0.00869925040752) A[2]:(0.00489519070834) A[3]:(0.979961633682)\n",
      " state (2)  A[0]:(0.00539958896115) A[1]:(0.00803706236184) A[2]:(0.00430481648073) A[3]:(0.982258558273)\n",
      " state (3)  A[0]:(0.00865842308849) A[1]:(0.00913558993489) A[2]:(0.0042849611491) A[3]:(0.977921009064)\n",
      " state (4)  A[0]:(0.998733341694) A[1]:(0.000351978465915) A[2]:(3.45673761331e-05) A[3]:(0.00088008842431)\n",
      " state (5)  A[0]:(0.996084809303) A[1]:(0.00327596673742) A[2]:(3.99623604608e-05) A[3]:(0.000599275750574)\n",
      " state (6)  A[0]:(0.898728072643) A[1]:(0.100430414081) A[2]:(9.15413402254e-05) A[3]:(0.00074995670002)\n",
      " state (7)  A[0]:(0.120376728475) A[1]:(0.879411697388) A[2]:(3.77479955205e-05) A[3]:(0.00017383066006)\n",
      " state (8)  A[0]:(0.00129718112294) A[1]:(0.998697400093) A[2]:(1.37976837777e-06) A[3]:(4.06409117204e-06)\n",
      " state (9)  A[0]:(9.41998878261e-05) A[1]:(0.999905109406) A[2]:(1.9619942293e-07) A[3]:(4.85619068513e-07)\n",
      " state (10)  A[0]:(4.00882163376e-05) A[1]:(0.999959588051) A[2]:(1.03707229471e-07) A[3]:(2.45034016189e-07)\n",
      " state (11)  A[0]:(3.08078306261e-05) A[1]:(0.999968886375) A[2]:(8.52262331819e-08) A[3]:(1.98463297352e-07)\n",
      " state (12)  A[0]:(2.80867425317e-05) A[1]:(0.999971628189) A[2]:(7.95659573782e-08) A[3]:(1.84253025282e-07)\n",
      " state (13)  A[0]:(2.70587388513e-05) A[1]:(0.999972701073) A[2]:(7.7397153575e-08) A[3]:(1.7879737868e-07)\n",
      " state (14)  A[0]:(2.6614714443e-05) A[1]:(0.999973118305) A[2]:(7.64556631339e-08) A[3]:(1.76424080678e-07)\n",
      " state (15)  A[0]:(2.64090886049e-05) A[1]:(0.999973356724) A[2]:(7.60188640925e-08) A[3]:(1.75321503093e-07)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 171000 finished after 7 . Running score: 0.15. Policy_loss: -92050.6130558, Value_loss: 0.978185303591. Times trained:               22196. Times reached goal: 129.               Steps done: 1595085.\n",
      " state (0)  A[0]:(0.552140057087) A[1]:(0.136899963021) A[2]:(0.0735318362713) A[3]:(0.237428158522)\n",
      " state (1)  A[0]:(0.00468684080988) A[1]:(0.00777465291321) A[2]:(0.00412730779499) A[3]:(0.983411192894)\n",
      " state (2)  A[0]:(0.00421419460326) A[1]:(0.00743568968028) A[2]:(0.00380900851451) A[3]:(0.984541118145)\n",
      " state (3)  A[0]:(0.00665336335078) A[1]:(0.00858815852553) A[2]:(0.00386636354961) A[3]:(0.980892121792)\n",
      " state (4)  A[0]:(0.998460590839) A[1]:(0.000473688298371) A[2]:(3.71383139282e-05) A[3]:(0.00102858245373)\n",
      " state (5)  A[0]:(0.994240164757) A[1]:(0.00498226145282) A[2]:(4.57841997559e-05) A[3]:(0.000731800391804)\n",
      " state (6)  A[0]:(0.876627326012) A[1]:(0.122374661267) A[2]:(0.000101622092188) A[3]:(0.000896380399354)\n",
      " state (7)  A[0]:(0.184372097254) A[1]:(0.81527274847) A[2]:(5.65433147131e-05) A[3]:(0.000298630562611)\n",
      " state (8)  A[0]:(0.00370224635117) A[1]:(0.996282279491) A[2]:(3.51226822204e-06) A[3]:(1.19476526379e-05)\n",
      " state (9)  A[0]:(0.000170400220668) A[1]:(0.999828219414) A[2]:(3.69806969047e-07) A[3]:(9.98113250716e-07)\n",
      " state (10)  A[0]:(5.22382906638e-05) A[1]:(0.999947190285) A[2]:(1.55098973664e-07) A[3]:(3.90846906839e-07)\n",
      " state (11)  A[0]:(3.65077212336e-05) A[1]:(0.999963104725) A[2]:(1.19188911185e-07) A[3]:(2.94491144359e-07)\n",
      " state (12)  A[0]:(3.25944347424e-05) A[1]:(0.999967038631) A[2]:(1.09681735694e-07) A[3]:(2.69192611313e-07)\n",
      " state (13)  A[0]:(3.13025193464e-05) A[1]:(0.999968349934) A[2]:(1.06488144525e-07) A[3]:(2.60667292196e-07)\n",
      " state (14)  A[0]:(3.07990158035e-05) A[1]:(0.999968826771) A[2]:(1.0523793037e-07) A[3]:(2.5731281994e-07)\n",
      " state (15)  A[0]:(3.05816465698e-05) A[1]:(0.999969065189) A[2]:(1.04697988945e-07) A[3]:(2.5585717367e-07)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 172000 finished after 10 . Running score: 0.12. Policy_loss: -92050.6147422, Value_loss: 1.19329317878. Times trained:               21029. Times reached goal: 145.               Steps done: 1616114.\n",
      " state (0)  A[0]:(0.657714962959) A[1]:(0.117037177086) A[2]:(0.0750183686614) A[3]:(0.150229468942)\n",
      " state (1)  A[0]:(0.00517777726054) A[1]:(0.00749342003837) A[2]:(0.00437266007066) A[3]:(0.982956171036)\n",
      " state (2)  A[0]:(0.00411638151854) A[1]:(0.00670609530061) A[2]:(0.00364312482998) A[3]:(0.985534369946)\n",
      " state (3)  A[0]:(0.00539896963164) A[1]:(0.0072197127156) A[2]:(0.00336456671357) A[3]:(0.984016776085)\n",
      " state (4)  A[0]:(0.997986197472) A[1]:(0.000567456008866) A[2]:(4.05698374379e-05) A[3]:(0.00140575948171)\n",
      " state (5)  A[0]:(0.97970277071) A[1]:(0.0192184802145) A[2]:(6.4636238676e-05) A[3]:(0.00101408851333)\n",
      " state (6)  A[0]:(0.556198179722) A[1]:(0.442840278149) A[2]:(0.000104346247099) A[3]:(0.000857248553075)\n",
      " state (7)  A[0]:(0.0788936987519) A[1]:(0.92089676857) A[2]:(3.17486992572e-05) A[3]:(0.000177779395017)\n",
      " state (8)  A[0]:(0.00412549916655) A[1]:(0.995854854584) A[2]:(3.83642100132e-06) A[3]:(1.58149559866e-05)\n",
      " state (9)  A[0]:(0.000211035119719) A[1]:(0.999787032604) A[2]:(4.38146656734e-07) A[3]:(1.47163916608e-06)\n",
      " state (10)  A[0]:(5.00434689457e-05) A[1]:(0.999949336052) A[2]:(1.52337619852e-07) A[3]:(4.77375067476e-07)\n",
      " state (11)  A[0]:(3.20546823787e-05) A[1]:(0.999967515469) A[2]:(1.09833351303e-07) A[3]:(3.3796260368e-07)\n",
      " state (12)  A[0]:(2.840975867e-05) A[1]:(0.999971210957) A[2]:(1.00561962313e-07) A[3]:(3.07814389089e-07)\n",
      " state (13)  A[0]:(2.75285638054e-05) A[1]:(0.999972045422) A[2]:(9.83073107363e-08) A[3]:(3.00385778473e-07)\n",
      " state (14)  A[0]:(2.73166078841e-05) A[1]:(0.99997228384) A[2]:(9.77848415573e-08) A[3]:(2.98584211578e-07)\n",
      " state (15)  A[0]:(2.72771958407e-05) A[1]:(0.999972343445) A[2]:(9.77041167971e-08) A[3]:(2.98245282693e-07)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 173000 finished after 6 . Running score: 0.15. Policy_loss: -92050.613383, Value_loss: 1.19235250455. Times trained:               19644. Times reached goal: 127.               Steps done: 1635758.\n",
      " state (0)  A[0]:(0.55888915062) A[1]:(0.132535710931) A[2]:(0.0764947533607) A[3]:(0.232080355287)\n",
      " state (1)  A[0]:(0.00344359478913) A[1]:(0.00634857174009) A[2]:(0.00339996395633) A[3]:(0.986807882786)\n",
      " state (2)  A[0]:(0.00287069496699) A[1]:(0.00584738329053) A[2]:(0.00291453278624) A[3]:(0.988367378712)\n",
      " state (3)  A[0]:(0.0026924284175) A[1]:(0.00582208484411) A[2]:(0.00247970339842) A[3]:(0.989005804062)\n",
      " state (4)  A[0]:(0.991068422794) A[1]:(0.00163963448722) A[2]:(0.000127848092234) A[3]:(0.00716408528388)\n",
      " state (5)  A[0]:(0.981633901596) A[1]:(0.0171252060682) A[2]:(6.40149228275e-05) A[3]:(0.00117689499166)\n",
      " state (6)  A[0]:(0.51415681839) A[1]:(0.484864920378) A[2]:(0.000106509338366) A[3]:(0.000871729338542)\n",
      " state (7)  A[0]:(0.0378762595356) A[1]:(0.962004423141) A[2]:(2.07124085136e-05) A[3]:(9.86025188467e-05)\n",
      " state (8)  A[0]:(0.000997099909) A[1]:(0.998996496201) A[2]:(1.57500733167e-06) A[3]:(4.80982453155e-06)\n",
      " state (9)  A[0]:(6.7691275035e-05) A[1]:(0.999931514263) A[2]:(2.29487284287e-07) A[3]:(5.47951117369e-07)\n",
      " state (10)  A[0]:(2.52528734563e-05) A[1]:(0.999974370003) A[2]:(1.13097023302e-07) A[3]:(2.50182893069e-07)\n",
      " state (11)  A[0]:(1.93304331333e-05) A[1]:(0.999980390072) A[2]:(9.34576718237e-08) A[3]:(2.02491563073e-07)\n",
      " state (12)  A[0]:(1.81234845513e-05) A[1]:(0.999981582165) A[2]:(8.93595384355e-08) A[3]:(1.92427194179e-07)\n",
      " state (13)  A[0]:(1.79361395567e-05) A[1]:(0.999981760979) A[2]:(8.88197106974e-08) A[3]:(1.90854365201e-07)\n",
      " state (14)  A[0]:(1.80137249117e-05) A[1]:(0.999981701374) A[2]:(8.92083278359e-08) A[3]:(1.91504881286e-07)\n",
      " state (15)  A[0]:(1.81617233466e-05) A[1]:(0.999981582165) A[2]:(8.98353107459e-08) A[3]:(1.92738852434e-07)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 174000 finished after 49 . Running score: 0.1. Policy_loss: -92050.6341199, Value_loss: 1.19864255697. Times trained:               21415. Times reached goal: 117.               Steps done: 1657173.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.5693,  0.1365,  0.0773,  0.2170]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5693,  0.1365,  0.0773,  0.2170]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0030,  0.0058,  0.0030,  0.9882]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0030,  0.0058,  0.0030,  0.9882]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5693,  0.1364,  0.0773,  0.2170]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5693,  0.1364,  0.0773,  0.2171]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5693,  0.1364,  0.0773,  0.2170]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9964,  0.0009,  0.0001,  0.0027]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5693,  0.1364,  0.0773,  0.2171]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9964,  0.0009,  0.0001,  0.0027]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9964,  0.0009,  0.0001,  0.0027]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5693,  0.1364,  0.0773,  0.2171]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5692,  0.1363,  0.0773,  0.2171]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5692,  0.1363,  0.0773,  0.2172]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5692,  0.1363,  0.0773,  0.2172]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5692,  0.1363,  0.0773,  0.2172]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5692,  0.1363,  0.0773,  0.2172]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5692,  0.1363,  0.0773,  0.2172]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0030,  0.0058,  0.0030,  0.9882]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0024,  0.0053,  0.0026,  0.9897]])\n",
      "On state=2, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0030,  0.0058,  0.0030,  0.9882]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5698,  0.1363,  0.0773,  0.2165]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.5700,  0.1363,  0.0773,  0.2164]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9964,  0.0009,  0.0001,  0.0027]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8091e-04,  9.9982e-01,  4.6303e-07,  7.3389e-07]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.570307254791) A[1]:(0.136314928532) A[2]:(0.0773781463504) A[3]:(0.215999692678)\n",
      " state (1)  A[0]:(0.00298743881285) A[1]:(0.00578830903396) A[2]:(0.00302781257778) A[3]:(0.98819643259)\n",
      " state (2)  A[0]:(0.00243879947811) A[1]:(0.00527572073042) A[2]:(0.00255417707376) A[3]:(0.989731311798)\n",
      " state (3)  A[0]:(0.00253808498383) A[1]:(0.00539919687435) A[2]:(0.00223070476204) A[3]:(0.989832043648)\n",
      " state (4)  A[0]:(0.99641507864) A[1]:(0.000859870691784) A[2]:(6.56210177112e-05) A[3]:(0.00265940953977)\n",
      " state (5)  A[0]:(0.973416388035) A[1]:(0.0255516991019) A[2]:(7.29247694835e-05) A[3]:(0.000958983611781)\n",
      " state (6)  A[0]:(0.236686468124) A[1]:(0.762898921967) A[2]:(7.07410144969e-05) A[3]:(0.00034387037158)\n",
      " state (7)  A[0]:(0.00753637962043) A[1]:(0.992439866066) A[2]:(6.63018272462e-06) A[3]:(1.71361407411e-05)\n",
      " state (8)  A[0]:(0.000181123003131) A[1]:(0.999817669392) A[2]:(4.63386840011e-07) A[3]:(7.34366039978e-07)\n",
      " state (9)  A[0]:(1.39561534525e-05) A[1]:(0.999985873699) A[2]:(7.34878113917e-08) A[3]:(8.88609079652e-08)\n",
      " state (10)  A[0]:(5.59057161809e-06) A[1]:(0.999994337559) A[2]:(3.80128923894e-08) A[3]:(4.21768753256e-08)\n",
      " state (11)  A[0]:(4.33889636042e-06) A[1]:(0.999995589256) A[2]:(3.16619868101e-08) A[3]:(3.43130857061e-08)\n",
      " state (12)  A[0]:(4.04672391596e-06) A[1]:(0.99999588728) A[2]:(3.01079090548e-08) A[3]:(3.24067777058e-08)\n",
      " state (13)  A[0]:(3.96396399083e-06) A[1]:(0.999995946884) A[2]:(2.96600770611e-08) A[3]:(3.18526822696e-08)\n",
      " state (14)  A[0]:(3.93668096876e-06) A[1]:(0.999996006489) A[2]:(2.95095841096e-08) A[3]:(3.1662299449e-08)\n",
      " state (15)  A[0]:(3.92601486965e-06) A[1]:(0.999996006489) A[2]:(2.94484099328e-08) A[3]:(3.15819619345e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 175000 finished after 25 . Running score: 0.09. Policy_loss: -92050.613579, Value_loss: 1.19592369246. Times trained:               24103. Times reached goal: 105.               Steps done: 1681276.\n",
      " state (0)  A[0]:(0.560661911964) A[1]:(0.150492802262) A[2]:(0.0828925669193) A[3]:(0.205952733755)\n",
      " state (1)  A[0]:(0.00248359586112) A[1]:(0.00583987217396) A[2]:(0.00275503331795) A[3]:(0.988921523094)\n",
      " state (2)  A[0]:(0.00190639321227) A[1]:(0.00521652586758) A[2]:(0.00219690031372) A[3]:(0.990680158138)\n",
      " state (3)  A[0]:(0.00189813808538) A[1]:(0.0055394382216) A[2]:(0.00182030722499) A[3]:(0.990742087364)\n",
      " state (4)  A[0]:(0.995073318481) A[1]:(0.0024181259796) A[2]:(6.0794318415e-05) A[3]:(0.00244778185152)\n",
      " state (5)  A[0]:(0.63107419014) A[1]:(0.36791241169) A[2]:(9.95004666038e-05) A[3]:(0.000913936237339)\n",
      " state (6)  A[0]:(0.00920853763819) A[1]:(0.990762472153) A[2]:(6.62020738673e-06) A[3]:(2.23626211664e-05)\n",
      " state (7)  A[0]:(0.000226149044465) A[1]:(0.999772489071) A[2]:(4.76107800296e-07) A[3]:(9.10936137188e-07)\n",
      " state (8)  A[0]:(8.06185107649e-06) A[1]:(0.999991834164) A[2]:(4.37875229409e-08) A[3]:(5.61280764089e-08)\n",
      " state (9)  A[0]:(1.609464789e-06) A[1]:(0.999998390675) A[2]:(1.37585933757e-08) A[3]:(1.49237902036e-08)\n",
      " state (10)  A[0]:(1.01898899629e-06) A[1]:(0.999998986721) A[2]:(9.90337323259e-09) A[3]:(1.02650901113e-08)\n",
      " state (11)  A[0]:(9.07581693355e-07) A[1]:(0.999999046326) A[2]:(9.11249387059e-09) A[3]:(9.33346644416e-09)\n",
      " state (12)  A[0]:(8.80073741882e-07) A[1]:(0.99999910593) A[2]:(8.91338114428e-09) A[3]:(9.09809738658e-09)\n",
      " state (13)  A[0]:(8.72120551776e-07) A[1]:(0.99999910593) A[2]:(8.85557938091e-09) A[3]:(9.0286897958e-09)\n",
      " state (14)  A[0]:(8.6947386535e-07) A[1]:(0.99999910593) A[2]:(8.83627748749e-09) A[3]:(9.00499053103e-09)\n",
      " state (15)  A[0]:(8.6847444436e-07) A[1]:(0.99999910593) A[2]:(8.82896600274e-09) A[3]:(8.99575525182e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 176000 finished after 15 . Running score: 0.14. Policy_loss: -92050.6142121, Value_loss: 1.19972016358. Times trained:               24109. Times reached goal: 124.               Steps done: 1705385.\n",
      " state (0)  A[0]:(0.671262383461) A[1]:(0.124159738421) A[2]:(0.0803460180759) A[3]:(0.124231867492)\n",
      " state (1)  A[0]:(0.00264215678908) A[1]:(0.00544891832396) A[2]:(0.00269619771279) A[3]:(0.989212751389)\n",
      " state (2)  A[0]:(0.0018042603042) A[1]:(0.00452285632491) A[2]:(0.00194831390399) A[3]:(0.991724550724)\n",
      " state (3)  A[0]:(0.0043792873621) A[1]:(0.0060657938011) A[2]:(0.00189227971714) A[3]:(0.987662613392)\n",
      " state (4)  A[0]:(0.998590350151) A[1]:(0.000685076927766) A[2]:(2.1081417799e-05) A[3]:(0.00070347066503)\n",
      " state (5)  A[0]:(0.961775541306) A[1]:(0.0375599376857) A[2]:(5.03159935761e-05) A[3]:(0.000614185526501)\n",
      " state (6)  A[0]:(0.227977648377) A[1]:(0.771783828735) A[2]:(4.23432065872e-05) A[3]:(0.000196152774151)\n",
      " state (7)  A[0]:(0.0210560970008) A[1]:(0.97891241312) A[2]:(8.24439575808e-06) A[3]:(2.32723032241e-05)\n",
      " state (8)  A[0]:(0.00197881041095) A[1]:(0.99801671505) A[2]:(1.5092338117e-06) A[3]:(2.94609253615e-06)\n",
      " state (9)  A[0]:(0.000113578651508) A[1]:(0.999885976315) A[2]:(1.90052574567e-07) A[3]:(2.5872921583e-07)\n",
      " state (10)  A[0]:(1.60245526786e-05) A[1]:(0.999983906746) A[2]:(4.56607835986e-08) A[3]:(5.04622157393e-08)\n",
      " state (11)  A[0]:(7.5811658462e-06) A[1]:(0.999992370605) A[2]:(2.64586326182e-08) A[3]:(2.72079159203e-08)\n",
      " state (12)  A[0]:(6.05792411079e-06) A[1]:(0.999993920326) A[2]:(2.24764811207e-08) A[3]:(2.26325553854e-08)\n",
      " state (13)  A[0]:(5.68969562664e-06) A[1]:(0.999994277954) A[2]:(2.14833271173e-08) A[3]:(2.15025952599e-08)\n",
      " state (14)  A[0]:(5.59441195946e-06) A[1]:(0.999994337559) A[2]:(2.12301642932e-08) A[3]:(2.12113828724e-08)\n",
      " state (15)  A[0]:(5.57123075851e-06) A[1]:(0.999994397163) A[2]:(2.11734700883e-08) A[3]:(2.11429203034e-08)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 177000 finished after 18 . Running score: 0.14. Policy_loss: -92050.6138219, Value_loss: 0.988636165248. Times trained:               21634. Times reached goal: 123.               Steps done: 1727019.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.760892927647) A[1]:(0.0924325063825) A[2]:(0.0672007724643) A[3]:(0.0794738307595)\n",
      " state (1)  A[0]:(0.00251030060463) A[1]:(0.00496002053842) A[2]:(0.00238599558361) A[3]:(0.990143656731)\n",
      " state (2)  A[0]:(0.00146390974987) A[1]:(0.00379990576766) A[2]:(0.001489911112) A[3]:(0.993246257305)\n",
      " state (3)  A[0]:(0.00250352593139) A[1]:(0.00461724959314) A[2]:(0.00127865758259) A[3]:(0.991600573063)\n",
      " state (4)  A[0]:(0.998541176319) A[1]:(0.000904040585738) A[2]:(1.92481802515e-05) A[3]:(0.000535508850589)\n",
      " state (5)  A[0]:(0.873955667019) A[1]:(0.125756382942) A[2]:(4.66924684588e-05) A[3]:(0.00024128385121)\n",
      " state (6)  A[0]:(0.0489399954677) A[1]:(0.951036155224) A[2]:(9.63868933468e-06) A[3]:(1.42356338984e-05)\n",
      " state (7)  A[0]:(0.00492836954072) A[1]:(0.995068252087) A[2]:(1.80892391199e-06) A[3]:(1.5891091607e-06)\n",
      " state (8)  A[0]:(0.000708297244273) A[1]:(0.99929100275) A[2]:(4.31078177598e-07) A[3]:(2.63933060296e-07)\n",
      " state (9)  A[0]:(5.8480676671e-05) A[1]:(0.999941408634) A[2]:(6.71695374876e-08) A[3]:(2.78849885404e-08)\n",
      " state (10)  A[0]:(8.25578626973e-06) A[1]:(0.999991714954) A[2]:(1.55034811655e-08) A[3]:(4.93421614678e-09)\n",
      " state (11)  A[0]:(3.60851231562e-06) A[1]:(0.999996364117) A[2]:(8.33069435657e-09) A[3]:(2.39150499404e-09)\n",
      " state (12)  A[0]:(2.77825961348e-06) A[1]:(0.999997198582) A[2]:(6.84764422942e-09) A[3]:(1.90462934313e-09)\n",
      " state (13)  A[0]:(2.57500232692e-06) A[1]:(0.999997437) A[2]:(6.47089137829e-09) A[3]:(1.78339765178e-09)\n",
      " state (14)  A[0]:(2.52036397796e-06) A[1]:(0.999997496605) A[2]:(6.3707887854e-09) A[3]:(1.7511501138e-09)\n",
      " state (15)  A[0]:(2.50600032814e-06) A[1]:(0.999997496605) A[2]:(6.34687102874e-09) A[3]:(1.74321879154e-09)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 178000 finished after 14 . Running score: 0.11. Policy_loss: -92050.609697, Value_loss: 0.983114566007. Times trained:               17297. Times reached goal: 127.               Steps done: 1744316.\n",
      " state (0)  A[0]:(0.753332078457) A[1]:(0.0940886586905) A[2]:(0.0683936625719) A[3]:(0.084185577929)\n",
      " state (1)  A[0]:(0.00279790791683) A[1]:(0.00532301561907) A[2]:(0.00265525444411) A[3]:(0.989223837852)\n",
      " state (2)  A[0]:(0.00171045679599) A[1]:(0.00414904393256) A[2]:(0.00176853081211) A[3]:(0.992371976376)\n",
      " state (3)  A[0]:(0.00137216853909) A[1]:(0.00373769295402) A[2]:(0.00134364981204) A[3]:(0.993546485901)\n",
      " state (4)  A[0]:(0.994285643101) A[1]:(0.00122287962586) A[2]:(0.000107307852886) A[3]:(0.00438419356942)\n",
      " state (5)  A[0]:(0.99903845787) A[1]:(0.000792039034422) A[2]:(1.37315710163e-05) A[3]:(0.000155788031407)\n",
      " state (6)  A[0]:(0.958235561848) A[1]:(0.0416509918869) A[2]:(3.38552963512e-05) A[3]:(7.96043168521e-05)\n",
      " state (7)  A[0]:(0.230407446623) A[1]:(0.76955229044) A[2]:(2.6185041861e-05) A[3]:(1.40659903991e-05)\n",
      " state (8)  A[0]:(0.010900631547) A[1]:(0.989095687866) A[2]:(3.02835292132e-06) A[3]:(6.258258054e-07)\n",
      " state (9)  A[0]:(0.000402806006605) A[1]:(0.999596893787) A[2]:(2.69674103492e-07) A[3]:(2.51803804474e-08)\n",
      " state (10)  A[0]:(3.15740944643e-05) A[1]:(0.999968409538) A[2]:(4.11655527444e-08) A[3]:(2.27346785842e-09)\n",
      " state (11)  A[0]:(1.02386720755e-05) A[1]:(0.999989748001) A[2]:(1.78919723481e-08) A[3]:(7.98420329851e-10)\n",
      " state (12)  A[0]:(7.01946464687e-06) A[1]:(0.999992966652) A[2]:(1.35411308833e-08) A[3]:(5.630887423e-10)\n",
      " state (13)  A[0]:(6.22158722763e-06) A[1]:(0.999993741512) A[2]:(1.23986110268e-08) A[3]:(5.0341053548e-10)\n",
      " state (14)  A[0]:(5.97769485466e-06) A[1]:(0.999994039536) A[2]:(1.2051235565e-08) A[3]:(4.84833173608e-10)\n",
      " state (15)  A[0]:(5.89417504671e-06) A[1]:(0.99999409914) A[2]:(1.1939289557e-08) A[3]:(4.78277084603e-10)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 179000 finished after 10 . Running score: 0.13. Policy_loss: -92050.6128315, Value_loss: 1.41430117016. Times trained:               18411. Times reached goal: 127.               Steps done: 1762727.\n",
      "action_dist \n",
      "tensor([[ 0.7632,  0.0895,  0.0627,  0.0846]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9988,  0.0008,  0.0000,  0.0003]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9988,  0.0008,  0.0000,  0.0003]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.2468e-04,  9.9927e-01,  7.9404e-07,  4.3405e-08]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.2539e-04,  9.9927e-01,  7.9456e-07,  4.3441e-08]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.2605e-04,  9.9927e-01,  7.9504e-07,  4.3473e-08]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.763243675232) A[1]:(0.0894986391068) A[2]:(0.0626560524106) A[3]:(0.084601610899)\n",
      " state (1)  A[0]:(0.00230554235168) A[1]:(0.00501323631033) A[2]:(0.00227957614698) A[3]:(0.990401625633)\n",
      " state (2)  A[0]:(0.00133550283499) A[1]:(0.00388308917172) A[2]:(0.00144266348798) A[3]:(0.993338763714)\n",
      " state (3)  A[0]:(0.001313067507) A[1]:(0.00402650609612) A[2]:(0.00112055637874) A[3]:(0.993539869785)\n",
      " state (4)  A[0]:(0.998818874359) A[1]:(0.0008363078814) A[2]:(2.26121628657e-05) A[3]:(0.000322177365888)\n",
      " state (5)  A[0]:(0.972109556198) A[1]:(0.0277675315738) A[2]:(3.87538966606e-05) A[3]:(8.4172555944e-05)\n",
      " state (6)  A[0]:(0.176256626844) A[1]:(0.823699235916) A[2]:(3.35797885782e-05) A[3]:(1.05819490273e-05)\n",
      " state (7)  A[0]:(0.0101219723001) A[1]:(0.989872515202) A[2]:(4.95710901305e-06) A[3]:(5.70599695493e-07)\n",
      " state (8)  A[0]:(0.000727722421288) A[1]:(0.999271452427) A[2]:(7.96292795258e-07) A[3]:(4.35613749517e-08)\n",
      " state (9)  A[0]:(3.92827496398e-05) A[1]:(0.99996060133) A[2]:(1.02675997482e-07) A[3]:(2.76430256463e-09)\n",
      " state (10)  A[0]:(6.23002415523e-06) A[1]:(0.999993741512) A[2]:(2.80268483976e-08) A[3]:(5.02440700156e-10)\n",
      " state (11)  A[0]:(3.17340345646e-06) A[1]:(0.999996781349) A[2]:(1.74029484157e-08) A[3]:(2.70453409668e-10)\n",
      " state (12)  A[0]:(2.5895244562e-06) A[1]:(0.999997377396) A[2]:(1.50748586947e-08) A[3]:(2.24438939656e-10)\n",
      " state (13)  A[0]:(2.43824274548e-06) A[1]:(0.99999755621) A[2]:(1.44478855546e-08) A[3]:(2.12345446782e-10)\n",
      " state (14)  A[0]:(2.39300607063e-06) A[1]:(0.999997615814) A[2]:(1.42580089957e-08) A[3]:(2.08690009718e-10)\n",
      " state (15)  A[0]:(2.37770996137e-06) A[1]:(0.999997615814) A[2]:(1.4193107134e-08) A[3]:(2.07436762212e-10)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 180000 finished after 6 . Running score: 0.14. Policy_loss: -92050.61214, Value_loss: 1.00238850888. Times trained:               17445. Times reached goal: 135.               Steps done: 1780172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.82965362072) A[1]:(0.0669932290912) A[2]:(0.0514150746167) A[3]:(0.0519380904734)\n",
      " state (1)  A[0]:(0.00197948911227) A[1]:(0.00439644511789) A[2]:(0.0018105879426) A[3]:(0.991813480854)\n",
      " state (2)  A[0]:(0.000891206669621) A[1]:(0.00298584764823) A[2]:(0.000892495270818) A[3]:(0.995230436325)\n",
      " state (3)  A[0]:(0.000915804179385) A[1]:(0.00322065479122) A[2]:(0.000665658037178) A[3]:(0.995197892189)\n",
      " state (4)  A[0]:(0.998347640038) A[1]:(0.00138952326961) A[2]:(1.58491766342e-05) A[3]:(0.000247000076342)\n",
      " state (5)  A[0]:(0.803539276123) A[1]:(0.19636169076) A[2]:(4.05606915592e-05) A[3]:(5.84563786106e-05)\n",
      " state (6)  A[0]:(0.039915330708) A[1]:(0.960074722767) A[2]:(7.66655739426e-06) A[3]:(2.27999521485e-06)\n",
      " state (7)  A[0]:(0.00283700739965) A[1]:(0.997161626816) A[2]:(1.18403761462e-06) A[3]:(1.60921558745e-07)\n",
      " state (8)  A[0]:(0.000154536406626) A[1]:(0.999845325947) A[2]:(1.46516455857e-07) A[3]:(9.87311032929e-09)\n",
      " state (9)  A[0]:(1.12040897875e-05) A[1]:(0.999988794327) A[2]:(2.19784936917e-08) A[3]:(8.51867187901e-10)\n",
      " state (10)  A[0]:(3.55411657438e-06) A[1]:(0.999996423721) A[2]:(9.55944567949e-09) A[3]:(2.9568217319e-10)\n",
      " state (11)  A[0]:(2.53257962868e-06) A[1]:(0.999997437) A[2]:(7.47564588011e-09) A[3]:(2.16605428038e-10)\n",
      " state (12)  A[0]:(2.31118042393e-06) A[1]:(0.999997675419) A[2]:(6.99580926522e-09) A[3]:(1.99129615575e-10)\n",
      " state (13)  A[0]:(2.25400003728e-06) A[1]:(0.999997735023) A[2]:(6.87000500932e-09) A[3]:(1.94575425216e-10)\n",
      " state (14)  A[0]:(2.237698709e-06) A[1]:(0.999997735023) A[2]:(6.83399958845e-09) A[3]:(1.93266805337e-10)\n",
      " state (15)  A[0]:(2.23257870857e-06) A[1]:(0.999997735023) A[2]:(6.82266865226e-09) A[3]:(1.92851457026e-10)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 181000 finished after 13 . Running score: 0.15. Policy_loss: -92050.6114429, Value_loss: 1.20039599781. Times trained:               18011. Times reached goal: 120.               Steps done: 1798183.\n",
      " state (0)  A[0]:(0.8663443923) A[1]:(0.0538388639688) A[2]:(0.0472465381026) A[3]:(0.0325702130795)\n",
      " state (1)  A[0]:(0.0031652378384) A[1]:(0.00523766083643) A[2]:(0.00242388690822) A[3]:(0.989173233509)\n",
      " state (2)  A[0]:(0.00113033945672) A[1]:(0.00313717266545) A[2]:(0.000979055068456) A[3]:(0.994753420353)\n",
      " state (3)  A[0]:(0.00452033989131) A[1]:(0.00536524457857) A[2]:(0.000955770665314) A[3]:(0.989158630371)\n",
      " state (4)  A[0]:(0.999432742596) A[1]:(0.000449149782071) A[2]:(7.28865734345e-06) A[3]:(0.000110806606244)\n",
      " state (5)  A[0]:(0.99234187603) A[1]:(0.00760672753677) A[2]:(1.19691558211e-05) A[3]:(3.94418821088e-05)\n",
      " state (6)  A[0]:(0.491722583771) A[1]:(0.508246719837) A[2]:(2.08496494452e-05) A[3]:(9.89062846202e-06)\n",
      " state (7)  A[0]:(0.0421383902431) A[1]:(0.957856953144) A[2]:(3.99516784455e-06) A[3]:(6.91970285516e-07)\n",
      " state (8)  A[0]:(0.00426167994738) A[1]:(0.995737493038) A[2]:(7.35479090963e-07) A[3]:(6.59605277065e-08)\n",
      " state (9)  A[0]:(0.000257431413047) A[1]:(0.99974244833) A[2]:(9.0606860681e-08) A[3]:(4.01555677598e-09)\n",
      " state (10)  A[0]:(3.34705073328e-05) A[1]:(0.99996650219) A[2]:(1.96850304945e-08) A[3]:(5.43301847955e-10)\n",
      " state (11)  A[0]:(1.49478009916e-05) A[1]:(0.999985039234) A[2]:(1.07602602384e-08) A[3]:(2.48019105253e-10)\n",
      " state (12)  A[0]:(1.17061235869e-05) A[1]:(0.999988257885) A[2]:(8.95855301053e-09) A[3]:(1.95637131495e-10)\n",
      " state (13)  A[0]:(1.09252896436e-05) A[1]:(0.99998909235) A[2]:(8.50664783059e-09) A[3]:(1.82960910311e-10)\n",
      " state (14)  A[0]:(1.07171454147e-05) A[1]:(0.999989271164) A[2]:(8.38473912523e-09) A[3]:(1.79574854986e-10)\n",
      " state (15)  A[0]:(1.0659585314e-05) A[1]:(0.999989330769) A[2]:(8.3508560067e-09) A[3]:(1.78636105908e-10)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 182000 finished after 28 . Running score: 0.16. Policy_loss: -92050.6124774, Value_loss: 1.21457461098. Times trained:               16255. Times reached goal: 123.               Steps done: 1814438.\n",
      " state (0)  A[0]:(0.818978428841) A[1]:(0.0691211894155) A[2]:(0.0486537665129) A[3]:(0.0632466450334)\n",
      " state (1)  A[0]:(0.0013334476389) A[1]:(0.00353480316699) A[2]:(0.00127257150598) A[3]:(0.993859171867)\n",
      " state (2)  A[0]:(0.000665117404424) A[1]:(0.00253646122292) A[2]:(0.000678499927744) A[3]:(0.996119916439)\n",
      " state (3)  A[0]:(0.0140142655) A[1]:(0.00804527197033) A[2]:(0.00116949761286) A[3]:(0.976770937443)\n",
      " state (4)  A[0]:(0.999410271645) A[1]:(0.000453115586424) A[2]:(7.77274362918e-06) A[3]:(0.000128813582705)\n",
      " state (5)  A[0]:(0.995512664318) A[1]:(0.00442250305787) A[2]:(1.20791601148e-05) A[3]:(5.2727311413e-05)\n",
      " state (6)  A[0]:(0.645146310329) A[1]:(0.354809880257) A[2]:(2.87205093628e-05) A[3]:(1.50707819557e-05)\n",
      " state (7)  A[0]:(0.0453226901591) A[1]:(0.954671025276) A[2]:(5.51101584279e-06) A[3]:(7.93715230429e-07)\n",
      " state (8)  A[0]:(0.00316058145836) A[1]:(0.996838569641) A[2]:(8.04038108981e-07) A[3]:(5.07739947864e-08)\n",
      " state (9)  A[0]:(0.000166102749063) A[1]:(0.99983382225) A[2]:(9.350566188e-08) A[3]:(2.66280109074e-09)\n",
      " state (10)  A[0]:(2.35917104874e-05) A[1]:(0.999976396561) A[2]:(2.23835954216e-08) A[3]:(3.90817100904e-10)\n",
      " state (11)  A[0]:(1.13814057841e-05) A[1]:(0.999988615513) A[2]:(1.31150983407e-08) A[3]:(1.91905949465e-10)\n",
      " state (12)  A[0]:(9.17280340218e-06) A[1]:(0.999990820885) A[2]:(1.11954969739e-08) A[3]:(1.55541579616e-10)\n",
      " state (13)  A[0]:(8.63612604007e-06) A[1]:(0.999991357327) A[2]:(1.0711224796e-08) A[3]:(1.46669065781e-10)\n",
      " state (14)  A[0]:(8.49271418701e-06) A[1]:(0.999991476536) A[2]:(1.05805213479e-08) A[3]:(1.44292758297e-10)\n",
      " state (15)  A[0]:(8.45288650453e-06) A[1]:(0.99999153614) A[2]:(1.05441593234e-08) A[3]:(1.43631023741e-10)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 183000 finished after 8 . Running score: 0.09. Policy_loss: -92050.6124946, Value_loss: 1.61935324481. Times trained:               15013. Times reached goal: 115.               Steps done: 1829451.\n",
      " state (0)  A[0]:(0.780256986618) A[1]:(0.0793102011085) A[2]:(0.0490963235497) A[3]:(0.0913365185261)\n",
      " state (1)  A[0]:(0.000964497332461) A[1]:(0.00310529558919) A[2]:(0.00100375770126) A[3]:(0.994926452637)\n",
      " state (2)  A[0]:(0.000533354585059) A[1]:(0.00234778202139) A[2]:(0.000590084004216) A[3]:(0.996528804302)\n",
      " state (3)  A[0]:(0.00111069565173) A[1]:(0.00315276579931) A[2]:(0.000577584432904) A[3]:(0.995158970356)\n",
      " state (4)  A[0]:(0.999262392521) A[1]:(0.000502344977576) A[2]:(9.73714213615e-06) A[3]:(0.000225530879106)\n",
      " state (5)  A[0]:(0.996098101139) A[1]:(0.00381574197672) A[2]:(1.19278438433e-05) A[3]:(7.42185293348e-05)\n",
      " state (6)  A[0]:(0.651186227798) A[1]:(0.348762184381) A[2]:(2.92927034025e-05) A[3]:(2.22925991693e-05)\n",
      " state (7)  A[0]:(0.0400969088078) A[1]:(0.959896862507) A[2]:(5.17110629517e-06) A[3]:(1.06603783934e-06)\n",
      " state (8)  A[0]:(0.00227791094221) A[1]:(0.997721374035) A[2]:(6.52916469335e-07) A[3]:(5.75620369148e-08)\n",
      " state (9)  A[0]:(0.000109009321022) A[1]:(0.9998909235) A[2]:(7.13402528163e-08) A[3]:(2.88354318201e-09)\n",
      " state (10)  A[0]:(1.64075336215e-05) A[1]:(0.999983549118) A[2]:(1.78981078847e-08) A[3]:(4.61011590014e-10)\n",
      " state (11)  A[0]:(8.28692918731e-06) A[1]:(0.999991714954) A[2]:(1.08647268959e-08) A[3]:(2.39015668368e-10)\n",
      " state (12)  A[0]:(6.78100377627e-06) A[1]:(0.99999320507) A[2]:(9.38360589231e-09) A[3]:(1.97130395341e-10)\n",
      " state (13)  A[0]:(6.4083583311e-06) A[1]:(0.999993562698) A[2]:(9.00411656346e-09) A[3]:(1.86697046711e-10)\n",
      " state (14)  A[0]:(6.30621525488e-06) A[1]:(0.999993681908) A[2]:(8.89916584867e-09) A[3]:(1.8382469158e-10)\n",
      " state (15)  A[0]:(6.27664212516e-06) A[1]:(0.999993741512) A[2]:(8.86875017869e-09) A[3]:(1.82988277309e-10)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 184000 finished after 11 . Running score: 0.11. Policy_loss: -92050.6124862, Value_loss: 0.992827264555. Times trained:               16233. Times reached goal: 116.               Steps done: 1845684.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.8423,  0.0625,  0.0498,  0.0454]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8423,  0.0625,  0.0498,  0.0454]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9992,  0.0006,  0.0000,  0.0003]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9992,  0.0006,  0.0000,  0.0003]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8423,  0.0625,  0.0498,  0.0454]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8423,  0.0625,  0.0498,  0.0455]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9992,  0.0006,  0.0000,  0.0003]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0494e-04,  9.9989e-01,  1.1086e-07,  4.9837e-09]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0494e-04,  9.9989e-01,  1.1086e-07,  4.9837e-09]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0495e-04,  9.9989e-01,  1.1086e-07,  4.9838e-09]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8943e-06,  9.9999e-01,  1.7609e-08,  4.2982e-10]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3915e-06,  1.0000e+00,  7.5221e-09,  1.4041e-10]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3781e-06,  1.0000e+00,  7.4918e-09,  1.3965e-10]])\n",
      "On state=14, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3781e-06,  1.0000e+00,  7.4916e-09,  1.3965e-10]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3915e-06,  1.0000e+00,  7.5216e-09,  1.4041e-10]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.842287957668) A[1]:(0.062479943037) A[2]:(0.0497503727674) A[3]:(0.0454817377031)\n",
      " state (1)  A[0]:(0.00137287576217) A[1]:(0.00384826981463) A[2]:(0.00144901045132) A[3]:(0.993329823017)\n",
      " state (2)  A[0]:(0.000591430929489) A[1]:(0.00255858921446) A[2]:(0.000683382502757) A[3]:(0.996166586876)\n",
      " state (3)  A[0]:(0.00684508029372) A[1]:(0.00609751278535) A[2]:(0.000955908151809) A[3]:(0.986101508141)\n",
      " state (4)  A[0]:(0.999167978764) A[1]:(0.000558794243261) A[2]:(1.0994153854e-05) A[3]:(0.000262241868768)\n",
      " state (5)  A[0]:(0.995405435562) A[1]:(0.00447815610096) A[2]:(1.50634978127e-05) A[3]:(0.000101348741737)\n",
      " state (6)  A[0]:(0.515499413013) A[1]:(0.484438717365) A[2]:(3.62005193892e-05) A[3]:(2.56789007835e-05)\n",
      " state (7)  A[0]:(0.00698851654306) A[1]:(0.993009030819) A[2]:(2.16232388084e-06) A[3]:(2.98960827649e-07)\n",
      " state (8)  A[0]:(0.00010494448361) A[1]:(0.999894917011) A[2]:(1.10842883316e-07) A[3]:(4.98369523427e-09)\n",
      " state (9)  A[0]:(7.89428213466e-06) A[1]:(0.999992072582) A[2]:(1.76067640467e-08) A[3]:(4.29809604574e-10)\n",
      " state (10)  A[0]:(3.28884334522e-06) A[1]:(0.999996721745) A[2]:(9.43651468077e-09) A[3]:(1.89244217519e-10)\n",
      " state (11)  A[0]:(2.59579724116e-06) A[1]:(0.999997377396) A[2]:(7.97286947574e-09) A[3]:(1.5165109446e-10)\n",
      " state (12)  A[0]:(2.43553517976e-06) A[1]:(0.99999755621) A[2]:(7.61947127614e-09) A[3]:(1.42842890294e-10)\n",
      " state (13)  A[0]:(2.39154360315e-06) A[1]:(0.999997615814) A[2]:(7.5214865447e-09) A[3]:(1.40406131166e-10)\n",
      " state (14)  A[0]:(2.37805238612e-06) A[1]:(0.999997615814) A[2]:(7.49137729628e-09) A[3]:(1.39653219544e-10)\n",
      " state (15)  A[0]:(2.37351173382e-06) A[1]:(0.999997615814) A[2]:(7.4812680495e-09) A[3]:(1.39397743348e-10)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 185000 finished after 15 . Running score: 0.09. Policy_loss: -92050.6126874, Value_loss: 0.996107766751. Times trained:               16380. Times reached goal: 118.               Steps done: 1862064.\n",
      " state (0)  A[0]:(0.860456287861) A[1]:(0.055621098727) A[2]:(0.044062204659) A[3]:(0.0398603715003)\n",
      " state (1)  A[0]:(0.00166501919739) A[1]:(0.00446745799854) A[2]:(0.00172326609027) A[3]:(0.992144227028)\n",
      " state (2)  A[0]:(0.000665031024255) A[1]:(0.00291503034532) A[2]:(0.000789868878201) A[3]:(0.995630085468)\n",
      " state (3)  A[0]:(0.0017164881574) A[1]:(0.00408188765869) A[2]:(0.00072273571277) A[3]:(0.993478894234)\n",
      " state (4)  A[0]:(0.998980402946) A[1]:(0.000670755922329) A[2]:(1.25233354993e-05) A[3]:(0.000336332130246)\n",
      " state (5)  A[0]:(0.995742678642) A[1]:(0.00412303488702) A[2]:(1.42071021401e-05) A[3]:(0.000120076954772)\n",
      " state (6)  A[0]:(0.726747751236) A[1]:(0.273164719343) A[2]:(3.75266827177e-05) A[3]:(4.99954803672e-05)\n",
      " state (7)  A[0]:(0.0485067665577) A[1]:(0.951482117176) A[2]:(8.26103496365e-06) A[3]:(2.85441183223e-06)\n",
      " state (8)  A[0]:(0.00223149149679) A[1]:(0.997767388821) A[2]:(9.80963250186e-07) A[3]:(1.32089610361e-07)\n",
      " state (9)  A[0]:(8.97896316019e-05) A[1]:(0.999910116196) A[2]:(1.04360267983e-07) A[3]:(5.82450754294e-09)\n",
      " state (10)  A[0]:(9.75437797024e-06) A[1]:(0.999990224838) A[2]:(2.22149640905e-08) A[3]:(6.90976831308e-10)\n",
      " state (11)  A[0]:(4.03524018111e-06) A[1]:(0.999995946884) A[2]:(1.20164163064e-08) A[3]:(2.96889124396e-10)\n",
      " state (12)  A[0]:(3.05289222524e-06) A[1]:(0.999996960163) A[2]:(9.8968948592e-09) A[3]:(2.27305452238e-10)\n",
      " state (13)  A[0]:(2.80259564533e-06) A[1]:(0.999997198582) A[2]:(9.32572685741e-09) A[3]:(2.09396458506e-10)\n",
      " state (14)  A[0]:(2.72744273389e-06) A[1]:(0.999997258186) A[2]:(9.151493785e-09) A[3]:(2.03987896019e-10)\n",
      " state (15)  A[0]:(2.70252462542e-06) A[1]:(0.999997317791) A[2]:(9.0934664243e-09) A[3]:(2.02185532205e-10)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 186000 finished after 27 . Running score: 0.15. Policy_loss: -92050.6127406, Value_loss: 1.2026948135. Times trained:               15578. Times reached goal: 130.               Steps done: 1877642.\n",
      " state (0)  A[0]:(0.845548808575) A[1]:(0.0609716027975) A[2]:(0.0435376986861) A[3]:(0.0499418601394)\n",
      " state (1)  A[0]:(0.00135759962723) A[1]:(0.00411680340767) A[2]:(0.00140515109524) A[3]:(0.9931204319)\n",
      " state (2)  A[0]:(0.000585615984164) A[1]:(0.00282111088745) A[2]:(0.000692317087669) A[3]:(0.995900928974)\n",
      " state (3)  A[0]:(0.000938654469792) A[1]:(0.00345968315378) A[2]:(0.00057979056146) A[3]:(0.995021879673)\n",
      " state (4)  A[0]:(0.99879860878) A[1]:(0.000907048757654) A[2]:(1.18576035675e-05) A[3]:(0.000282502529444)\n",
      " state (5)  A[0]:(0.983556807041) A[1]:(0.0163289923221) A[2]:(1.91413237189e-05) A[3]:(9.50503381318e-05)\n",
      " state (6)  A[0]:(0.167345687747) A[1]:(0.832624733448) A[2]:(1.85628887266e-05) A[3]:(1.09984302981e-05)\n",
      " state (7)  A[0]:(0.0023411619477) A[1]:(0.99765765667) A[2]:(1.0539744153e-06) A[3]:(1.51057463427e-07)\n",
      " state (8)  A[0]:(5.04534364154e-05) A[1]:(0.999949455261) A[2]:(7.5872627292e-08) A[3]:(3.68329056144e-09)\n",
      " state (9)  A[0]:(3.37799565386e-06) A[1]:(0.999996602535) A[2]:(1.18704948093e-08) A[3]:(2.81308532024e-10)\n",
      " state (10)  A[0]:(1.20870890896e-06) A[1]:(0.999998807907) A[2]:(5.86856074847e-09) A[3]:(1.06321756954e-10)\n",
      " state (11)  A[0]:(8.98726625564e-07) A[1]:(0.99999910593) A[2]:(4.7909685108e-09) A[3]:(8.02954022716e-11)\n",
      " state (12)  A[0]:(8.25718245778e-07) A[1]:(0.999999165535) A[2]:(4.52144366392e-09) A[3]:(7.40793190679e-11)\n",
      " state (13)  A[0]:(8.04493993201e-07) A[1]:(0.999999165535) A[2]:(4.44188019699e-09) A[3]:(7.22570753231e-11)\n",
      " state (14)  A[0]:(7.97448194589e-07) A[1]:(0.99999922514) A[2]:(4.41538228202e-09) A[3]:(7.16478820717e-11)\n",
      " state (15)  A[0]:(7.94853008301e-07) A[1]:(0.99999922514) A[2]:(4.40563230342e-09) A[3]:(7.14222084253e-11)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 187000 finished after 5 . Running score: 0.12. Policy_loss: -92050.6120708, Value_loss: 0.988147197001. Times trained:               16242. Times reached goal: 129.               Steps done: 1893884.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.855770111084) A[1]:(0.0572237074375) A[2]:(0.0435481481254) A[3]:(0.0434580147266)\n",
      " state (1)  A[0]:(0.0018775932258) A[1]:(0.00515265809372) A[2]:(0.0018887341721) A[3]:(0.991080999374)\n",
      " state (2)  A[0]:(0.000736614165362) A[1]:(0.00344590190798) A[2]:(0.000890754628927) A[3]:(0.99492675066)\n",
      " state (3)  A[0]:(0.00111442571506) A[1]:(0.00423537474126) A[2]:(0.00067588564707) A[3]:(0.993974328041)\n",
      " state (4)  A[0]:(0.998258590698) A[1]:(0.0014168851776) A[2]:(1.19420592455e-05) A[3]:(0.000312554184347)\n",
      " state (5)  A[0]:(0.966013789177) A[1]:(0.0338690467179) A[2]:(1.81926061487e-05) A[3]:(9.89565451164e-05)\n",
      " state (6)  A[0]:(0.130468919873) A[1]:(0.869510829449) A[2]:(1.16822429845e-05) A[3]:(8.5740484792e-06)\n",
      " state (7)  A[0]:(0.00271269585937) A[1]:(0.997286319733) A[2]:(8.47915146096e-07) A[3]:(1.51706501583e-07)\n",
      " state (8)  A[0]:(7.89063196862e-05) A[1]:(0.999921023846) A[2]:(7.4690504448e-08) A[3]:(4.29875202101e-09)\n",
      " state (9)  A[0]:(3.95102688344e-06) A[1]:(0.999996066093) A[2]:(9.56577128619e-09) A[3]:(2.21558299485e-10)\n",
      " state (10)  A[0]:(9.71161284724e-07) A[1]:(0.999999046326) A[2]:(3.65843466632e-09) A[3]:(5.55907819777e-11)\n",
      " state (11)  A[0]:(6.20651348981e-07) A[1]:(0.999999403954) A[2]:(2.69363353844e-09) A[3]:(3.5760918532e-11)\n",
      " state (12)  A[0]:(5.42218060673e-07) A[1]:(0.999999463558) A[2]:(2.45641551544e-09) A[3]:(3.12919933021e-11)\n",
      " state (13)  A[0]:(5.19117463682e-07) A[1]:(0.999999463558) A[2]:(2.38471176139e-09) A[3]:(2.99682882676e-11)\n",
      " state (14)  A[0]:(5.11132157044e-07) A[1]:(0.999999463558) A[2]:(2.35977637431e-09) A[3]:(2.95081771517e-11)\n",
      " state (15)  A[0]:(5.08022935719e-07) A[1]:(0.999999463558) A[2]:(2.35006547555e-09) A[3]:(2.93281718355e-11)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 188000 finished after 8 . Running score: 0.16. Policy_loss: -92050.6130582, Value_loss: 1.2026451495. Times trained:               15974. Times reached goal: 137.               Steps done: 1909858.\n",
      " state (0)  A[0]:(0.867238759995) A[1]:(0.051934774965) A[2]:(0.04196292907) A[3]:(0.0388635508716)\n",
      " state (1)  A[0]:(0.00154824461788) A[1]:(0.00457920692861) A[2]:(0.00161295337602) A[3]:(0.99225962162)\n",
      " state (2)  A[0]:(0.00052622880321) A[1]:(0.00291193067096) A[2]:(0.000670177862048) A[3]:(0.995891690254)\n",
      " state (3)  A[0]:(0.00886788405478) A[1]:(0.00934716686606) A[2]:(0.000921097991522) A[3]:(0.98086386919)\n",
      " state (4)  A[0]:(0.997510135174) A[1]:(0.00227482174523) A[2]:(8.96957953955e-06) A[3]:(0.000206053184229)\n",
      " state (5)  A[0]:(0.819319367409) A[1]:(0.180590391159) A[2]:(2.11247370316e-05) A[3]:(6.91101595294e-05)\n",
      " state (6)  A[0]:(0.0274512227625) A[1]:(0.972544014454) A[2]:(3.13124996865e-06) A[3]:(1.65874780578e-06)\n",
      " state (7)  A[0]:(0.00113669899292) A[1]:(0.998862922192) A[2]:(3.43864059005e-07) A[3]:(6.17526438873e-08)\n",
      " state (8)  A[0]:(6.21441722615e-05) A[1]:(0.999937832355) A[2]:(4.55961064461e-08) A[3]:(3.29039506752e-09)\n",
      " state (9)  A[0]:(4.12769622926e-06) A[1]:(0.99999588728) A[2]:(6.95196833433e-09) A[3]:(2.19584322947e-10)\n",
      " state (10)  A[0]:(1.07111941361e-06) A[1]:(0.999998927116) A[2]:(2.73555511576e-09) A[3]:(5.7278560367e-11)\n",
      " state (11)  A[0]:(6.96353765761e-07) A[1]:(0.999999284744) A[2]:(2.03225680728e-09) A[3]:(3.72950663408e-11)\n",
      " state (12)  A[0]:(6.16082274973e-07) A[1]:(0.999999403954) A[2]:(1.86772686206e-09) A[3]:(3.30050674291e-11)\n",
      " state (13)  A[0]:(5.94830623868e-07) A[1]:(0.999999403954) A[2]:(1.82313553143e-09) A[3]:(3.18665961674e-11)\n",
      " state (14)  A[0]:(5.88596265061e-07) A[1]:(0.999999403954) A[2]:(1.80998660504e-09) A[3]:(3.15315863697e-11)\n",
      " state (15)  A[0]:(5.86615271914e-07) A[1]:(0.999999403954) A[2]:(1.80581072318e-09) A[3]:(3.14247725064e-11)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 189000 finished after 13 . Running score: 0.12. Policy_loss: -92050.6280167, Value_loss: 1.20021013453. Times trained:               16174. Times reached goal: 125.               Steps done: 1926032.\n",
      "action_dist \n",
      "tensor([[ 0.8672,  0.0512,  0.0420,  0.0395]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9876e-01,  9.5939e-04,  9.0535e-06,  2.7328e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9876e-01,  9.5940e-04,  9.0535e-06,  2.7326e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9876e-01,  9.5942e-04,  9.0533e-06,  2.7325e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9876e-01,  9.5943e-04,  9.0533e-06,  2.7324e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8673,  0.0512,  0.0420,  0.0395]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8673,  0.0512,  0.0420,  0.0395]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8673,  0.0512,  0.0420,  0.0395]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8673,  0.0512,  0.0420,  0.0395]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8673,  0.0512,  0.0420,  0.0395]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8673,  0.0512,  0.0420,  0.0395]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9876e-01,  9.5951e-04,  9.0527e-06,  2.7318e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9876e-01,  9.5952e-04,  9.0523e-06,  2.7318e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.4962e-03,  9.9450e-01,  8.5198e-07,  1.3038e-07]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.867340564728) A[1]:(0.051151085645) A[2]:(0.0420224107802) A[3]:(0.0394859388471)\n",
      " state (1)  A[0]:(0.00139123282861) A[1]:(0.00437874812633) A[2]:(0.00155755272135) A[3]:(0.99267244339)\n",
      " state (2)  A[0]:(0.000508167955559) A[1]:(0.00286225159653) A[2]:(0.000685818376951) A[3]:(0.995943784714)\n",
      " state (3)  A[0]:(0.00690186629072) A[1]:(0.00779650267214) A[2]:(0.000914818432648) A[3]:(0.98438680172)\n",
      " state (4)  A[0]:(0.998758375645) A[1]:(0.000959368131589) A[2]:(9.05193155631e-06) A[3]:(0.000273202778772)\n",
      " state (5)  A[0]:(0.994259119034) A[1]:(0.0056438408792) A[2]:(9.69000575424e-06) A[3]:(8.73395620147e-05)\n",
      " state (6)  A[0]:(0.783144176006) A[1]:(0.216805011034) A[2]:(1.98844736587e-05) A[3]:(3.09510505758e-05)\n",
      " state (7)  A[0]:(0.093169093132) A[1]:(0.906821966171) A[2]:(6.30259819445e-06) A[3]:(2.64091409008e-06)\n",
      " state (8)  A[0]:(0.00550747662783) A[1]:(0.994491517544) A[2]:(8.53159406233e-07) A[3]:(1.30649937091e-07)\n",
      " state (9)  A[0]:(0.000231883095694) A[1]:(0.999768018723) A[2]:(8.94490739256e-08) A[3]:(4.77889283701e-09)\n",
      " state (10)  A[0]:(2.04132702493e-05) A[1]:(0.999979555607) A[2]:(1.59958464252e-08) A[3]:(3.81913611847e-10)\n",
      " state (11)  A[0]:(7.19208901501e-06) A[1]:(0.999992787838) A[2]:(7.66084973236e-09) A[3]:(1.29125474202e-10)\n",
      " state (12)  A[0]:(5.1074316616e-06) A[1]:(0.999994874001) A[2]:(6.01958927149e-09) A[3]:(9.04322311479e-11)\n",
      " state (13)  A[0]:(4.58643808088e-06) A[1]:(0.999995410442) A[2]:(5.58081048041e-09) A[3]:(8.08355465898e-11)\n",
      " state (14)  A[0]:(4.42834880232e-06) A[1]:(0.999995589256) A[2]:(5.44499556554e-09) A[3]:(7.79245071247e-11)\n",
      " state (15)  A[0]:(4.37465405412e-06) A[1]:(0.999995648861) A[2]:(5.39860556259e-09) A[3]:(7.6933716342e-11)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 190000 finished after 14 . Running score: 0.18. Policy_loss: -92050.6183685, Value_loss: 1.4227583048. Times trained:               14942. Times reached goal: 139.               Steps done: 1940974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.882326185703) A[1]:(0.0462324805558) A[2]:(0.0390475615859) A[3]:(0.0323937609792)\n",
      " state (1)  A[0]:(0.00140065269079) A[1]:(0.00461954111233) A[2]:(0.00155835482292) A[3]:(0.992421448231)\n",
      " state (2)  A[0]:(0.000454183667898) A[1]:(0.00289043132216) A[2]:(0.000619373808149) A[3]:(0.996035993099)\n",
      " state (3)  A[0]:(0.00136263249442) A[1]:(0.00467710383236) A[2]:(0.000548492360394) A[3]:(0.993411779404)\n",
      " state (4)  A[0]:(0.998173773289) A[1]:(0.0015404560836) A[2]:(8.38754749566e-06) A[3]:(0.000277385261143)\n",
      " state (5)  A[0]:(0.982455849648) A[1]:(0.0174614414573) A[2]:(9.9017206594e-06) A[3]:(7.28329905542e-05)\n",
      " state (6)  A[0]:(0.426828593016) A[1]:(0.573142945766) A[2]:(1.2589137441e-05) A[3]:(1.58908351295e-05)\n",
      " state (7)  A[0]:(0.0210917592049) A[1]:(0.978906035423) A[2]:(1.63201320902e-06) A[3]:(5.72802548504e-07)\n",
      " state (8)  A[0]:(0.00101705314592) A[1]:(0.998982727528) A[2]:(1.84496997235e-07) A[3]:(2.26737562059e-08)\n",
      " state (9)  A[0]:(4.19455682277e-05) A[1]:(0.99995803833) A[2]:(1.87549318298e-08) A[3]:(7.93688170742e-10)\n",
      " state (10)  A[0]:(5.43907663086e-06) A[1]:(0.999994575977) A[2]:(4.36631086842e-09) A[3]:(9.33192065289e-11)\n",
      " state (11)  A[0]:(2.5289716632e-06) A[1]:(0.999997496605) A[2]:(2.53226439817e-09) A[3]:(4.1828849412e-11)\n",
      " state (12)  A[0]:(1.99368378162e-06) A[1]:(0.999998033047) A[2]:(2.13867878962e-09) A[3]:(3.25915232302e-11)\n",
      " state (13)  A[0]:(1.85228077498e-06) A[1]:(0.999998152256) A[2]:(2.02994954179e-09) A[3]:(3.01650683598e-11)\n",
      " state (14)  A[0]:(1.80825168172e-06) A[1]:(0.999998211861) A[2]:(1.99566319026e-09) A[3]:(2.94091383191e-11)\n",
      " state (15)  A[0]:(1.79296864644e-06) A[1]:(0.999998211861) A[2]:(1.98372784865e-09) A[3]:(2.91460640345e-11)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 191000 finished after 6 . Running score: 0.11. Policy_loss: -92050.6122534, Value_loss: 1.19897863978. Times trained:               15226. Times reached goal: 120.               Steps done: 1956200.\n",
      " state (0)  A[0]:(0.899326980114) A[1]:(0.0397594645619) A[2]:(0.0381332114339) A[3]:(0.022780360654)\n",
      " state (1)  A[0]:(0.00286568538286) A[1]:(0.00635451218113) A[2]:(0.00277436571196) A[3]:(0.988005459309)\n",
      " state (2)  A[0]:(0.000710742606316) A[1]:(0.00350786559284) A[2]:(0.00091786950361) A[3]:(0.994863510132)\n",
      " state (3)  A[0]:(0.000837440195028) A[1]:(0.00396580668166) A[2]:(0.000549145683181) A[3]:(0.994647622108)\n",
      " state (4)  A[0]:(0.997851192951) A[1]:(0.00185674289241) A[2]:(8.98220150702e-06) A[3]:(0.000283091532765)\n",
      " state (5)  A[0]:(0.961182415485) A[1]:(0.0387400723994) A[2]:(1.19552578326e-05) A[3]:(6.55624389765e-05)\n",
      " state (6)  A[0]:(0.150385469198) A[1]:(0.849602758884) A[2]:(6.94636355547e-06) A[3]:(4.85063355882e-06)\n",
      " state (7)  A[0]:(0.00398828834295) A[1]:(0.996011078358) A[2]:(5.47143201857e-07) A[3]:(8.9871839748e-08)\n",
      " state (8)  A[0]:(0.000133344568894) A[1]:(0.999866604805) A[2]:(4.95339271822e-08) A[3]:(2.3771158375e-09)\n",
      " state (9)  A[0]:(6.7233718255e-06) A[1]:(0.999993264675) A[2]:(6.04956262862e-09) A[3]:(1.01086528037e-10)\n",
      " state (10)  A[0]:(1.64401262737e-06) A[1]:(0.99999833107) A[2]:(2.25350782479e-09) A[3]:(2.29040363064e-11)\n",
      " state (11)  A[0]:(1.05441472442e-06) A[1]:(0.999998927116) A[2]:(1.65169578015e-09) A[3]:(1.43388122556e-11)\n",
      " state (12)  A[0]:(9.25851111333e-07) A[1]:(0.999999046326) A[2]:(1.5083965188e-09) A[3]:(1.24982134017e-11)\n",
      " state (13)  A[0]:(8.89642251423e-07) A[1]:(0.99999910593) A[2]:(1.46709444593e-09) A[3]:(1.19806664589e-11)\n",
      " state (14)  A[0]:(8.77863271853e-07) A[1]:(0.99999910593) A[2]:(1.45360201653e-09) A[3]:(1.18119203654e-11)\n",
      " state (15)  A[0]:(8.73578130722e-07) A[1]:(0.99999910593) A[2]:(1.44870015983e-09) A[3]:(1.17503506925e-11)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 192000 finished after 29 . Running score: 0.19. Policy_loss: -92050.6128705, Value_loss: 0.994442401205. Times trained:               15301. Times reached goal: 131.               Steps done: 1971501.\n",
      " state (0)  A[0]:(0.913198590279) A[1]:(0.0333746820688) A[2]:(0.0353192463517) A[3]:(0.0181074924767)\n",
      " state (1)  A[0]:(0.00357539975084) A[1]:(0.00670269550756) A[2]:(0.00340313510969) A[3]:(0.986318767071)\n",
      " state (2)  A[0]:(0.000809067219961) A[1]:(0.0034837492276) A[2]:(0.00105925113894) A[3]:(0.994647920132)\n",
      " state (3)  A[0]:(0.00290360185318) A[1]:(0.00578539911658) A[2]:(0.000850348675158) A[3]:(0.990460634232)\n",
      " state (4)  A[0]:(0.99878102541) A[1]:(0.00094727857504) A[2]:(9.86217401078e-06) A[3]:(0.000261850480456)\n",
      " state (5)  A[0]:(0.997036337852) A[1]:(0.00287283374928) A[2]:(7.80670325184e-06) A[3]:(8.29940108815e-05)\n",
      " state (6)  A[0]:(0.946878135204) A[1]:(0.0530821681023) A[2]:(1.3225994735e-05) A[3]:(2.64803729806e-05)\n",
      " state (7)  A[0]:(0.277904331684) A[1]:(0.722081124783) A[2]:(1.04945756902e-05) A[3]:(4.04902220907e-06)\n",
      " state (8)  A[0]:(0.0125887608156) A[1]:(0.987409889698) A[2]:(1.22056565033e-06) A[3]:(1.22621543142e-07)\n",
      " state (9)  A[0]:(0.000404815917136) A[1]:(0.999595105648) A[2]:(1.0429396724e-07) A[3]:(2.81946532787e-09)\n",
      " state (10)  A[0]:(2.11006863537e-05) A[1]:(0.999978899956) A[2]:(1.27223804824e-08) A[3]:(1.13212508812e-10)\n",
      " state (11)  A[0]:(4.81807319375e-06) A[1]:(0.999995172024) A[2]:(4.46374848195e-09) A[3]:(2.27610794795e-11)\n",
      " state (12)  A[0]:(2.81051825368e-06) A[1]:(0.999997198582) A[2]:(3.04851566213e-09) A[3]:(1.26676282311e-11)\n",
      " state (13)  A[0]:(2.32777279052e-06) A[1]:(0.999997675419) A[2]:(2.66863775522e-09) A[3]:(1.03150699787e-11)\n",
      " state (14)  A[0]:(2.1696944259e-06) A[1]:(0.999997854233) A[2]:(2.53957366247e-09) A[3]:(9.55106046335e-12)\n",
      " state (15)  A[0]:(2.10804819289e-06) A[1]:(0.999997913837) A[2]:(2.48859288732e-09) A[3]:(9.25335762875e-12)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 193000 finished after 6 . Running score: 0.08. Policy_loss: -92050.6132573, Value_loss: 0.994012541056. Times trained:               14072. Times reached goal: 120.               Steps done: 1985573.\n",
      " state (0)  A[0]:(0.908856093884) A[1]:(0.0352513939142) A[2]:(0.0352450571954) A[3]:(0.0206474680454)\n",
      " state (1)  A[0]:(0.0030528921634) A[1]:(0.00638932501897) A[2]:(0.00302671454847) A[3]:(0.987531065941)\n",
      " state (2)  A[0]:(0.00075481767999) A[1]:(0.00356055889279) A[2]:(0.00104115530849) A[3]:(0.994643449783)\n",
      " state (3)  A[0]:(0.000545922666788) A[1]:(0.00331892445683) A[2]:(0.000559334293939) A[3]:(0.995575845242)\n",
      " state (4)  A[0]:(0.997833907604) A[1]:(0.00176736386493) A[2]:(1.5415766029e-05) A[3]:(0.000383302831324)\n",
      " state (5)  A[0]:(0.989542484283) A[1]:(0.0103821950033) A[2]:(1.21563070934e-05) A[3]:(6.31879083812e-05)\n",
      " state (6)  A[0]:(0.682969033718) A[1]:(0.316991478205) A[2]:(2.21123191295e-05) A[3]:(1.7371326976e-05)\n",
      " state (7)  A[0]:(0.0498704276979) A[1]:(0.95012396574) A[2]:(4.87381657877e-06) A[3]:(7.62336412663e-07)\n",
      " state (8)  A[0]:(0.00216550845653) A[1]:(0.997833907604) A[2]:(5.61855586056e-07) A[3]:(2.3126633053e-08)\n",
      " state (9)  A[0]:(8.16965984995e-05) A[1]:(0.999918222427) A[2]:(5.85703183731e-08) A[3]:(6.3578020626e-10)\n",
      " state (10)  A[0]:(9.40601330512e-06) A[1]:(0.999990582466) A[2]:(1.32675950226e-08) A[3]:(6.0196368723e-11)\n",
      " state (11)  A[0]:(4.11134487877e-06) A[1]:(0.99999588728) A[2]:(7.52707052243e-09) A[3]:(2.44351639617e-11)\n",
      " state (12)  A[0]:(3.17315243592e-06) A[1]:(0.999996840954) A[2]:(6.30569640947e-09) A[3]:(1.84208100162e-11)\n",
      " state (13)  A[0]:(2.92804725177e-06) A[1]:(0.999997079372) A[2]:(5.96929528029e-09) A[3]:(1.68701025371e-11)\n",
      " state (14)  A[0]:(2.85173518932e-06) A[1]:(0.999997138977) A[2]:(5.86306869721e-09) A[3]:(1.63877817566e-11)\n",
      " state (15)  A[0]:(2.82505084215e-06) A[1]:(0.999997198582) A[2]:(5.82583670194e-09) A[3]:(1.62186548913e-11)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 194000 finished after 9 . Running score: 0.12. Policy_loss: -92050.6128806, Value_loss: 0.990502992704. Times trained:               14388. Times reached goal: 129.               Steps done: 1999961.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9177,  0.0315,  0.0329,  0.0179]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9177,  0.0315,  0.0329,  0.0179]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9177,  0.0315,  0.0329,  0.0179]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9981,  0.0015,  0.0000,  0.0003]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2037e-03,  9.9880e-01,  4.0829e-07,  1.3211e-08]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.1629e-05,  9.9994e-01,  5.7206e-08,  4.9446e-10]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2041e-03,  9.9880e-01,  4.0830e-07,  1.3216e-08]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.917661428452) A[1]:(0.0315309725702) A[2]:(0.03287095204) A[3]:(0.0179366394877)\n",
      " state (1)  A[0]:(0.00420463085175) A[1]:(0.00748334079981) A[2]:(0.00395704479888) A[3]:(0.984354972839)\n",
      " state (2)  A[0]:(0.000957946293056) A[1]:(0.00395840639248) A[2]:(0.00126468029339) A[3]:(0.993818938732)\n",
      " state (3)  A[0]:(0.00121685059275) A[1]:(0.00446361163631) A[2]:(0.000693544279784) A[3]:(0.993625998497)\n",
      " state (4)  A[0]:(0.998116791248) A[1]:(0.00154640735127) A[2]:(1.11022163765e-05) A[3]:(0.00032567823655)\n",
      " state (5)  A[0]:(0.986221373081) A[1]:(0.0136993927881) A[2]:(1.09374659587e-05) A[3]:(6.83166799718e-05)\n",
      " state (6)  A[0]:(0.650275230408) A[1]:(0.349686294794) A[2]:(1.98357320187e-05) A[3]:(1.86376437341e-05)\n",
      " state (7)  A[0]:(0.0546373687685) A[1]:(0.945356607437) A[2]:(5.1118349802e-06) A[3]:(9.40478969369e-07)\n",
      " state (8)  A[0]:(0.00120487564709) A[1]:(0.998794674873) A[2]:(4.08478115332e-07) A[3]:(1.32246595896e-08)\n",
      " state (9)  A[0]:(6.16636534687e-05) A[1]:(0.999938249588) A[2]:(5.7222212746e-08) A[3]:(4.94721819067e-10)\n",
      " state (10)  A[0]:(1.93829728232e-05) A[1]:(0.999980568886) A[2]:(2.67402473497e-08) A[3]:(1.37852312898e-10)\n",
      " state (11)  A[0]:(1.36540356834e-05) A[1]:(0.999986350536) A[2]:(2.12601030114e-08) A[3]:(9.35240634936e-11)\n",
      " state (12)  A[0]:(1.22327173813e-05) A[1]:(0.999987721443) A[2]:(1.97906349086e-08) A[3]:(8.27518678581e-11)\n",
      " state (13)  A[0]:(1.17687850434e-05) A[1]:(0.99998819828) A[2]:(1.93007299032e-08) A[3]:(7.92436533059e-11)\n",
      " state (14)  A[0]:(1.15908051157e-05) A[1]:(0.999988377094) A[2]:(1.91120008708e-08) A[3]:(7.7894080075e-11)\n",
      " state (15)  A[0]:(1.15152133731e-05) A[1]:(0.999988436699) A[2]:(1.90318996118e-08) A[3]:(7.73187208702e-11)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 195000 finished after 7 . Running score: 0.1. Policy_loss: -92050.6110633, Value_loss: 1.20929254409. Times trained:               13775. Times reached goal: 106.               Steps done: 2013736.\n",
      " state (0)  A[0]:(0.909814417362) A[1]:(0.0344405174255) A[2]:(0.0329135172069) A[3]:(0.0228315684944)\n",
      " state (1)  A[0]:(0.00155097572133) A[1]:(0.00499021820724) A[2]:(0.00190618517809) A[3]:(0.991552591324)\n",
      " state (2)  A[0]:(0.000379483215511) A[1]:(0.00273722806014) A[2]:(0.000602140207775) A[3]:(0.996281147003)\n",
      " state (3)  A[0]:(0.00235993042588) A[1]:(0.00611600326374) A[2]:(0.000566977774724) A[3]:(0.990957081318)\n",
      " state (4)  A[0]:(0.997572779655) A[1]:(0.00209278962575) A[2]:(7.53006952436e-06) A[3]:(0.000326908746501)\n",
      " state (5)  A[0]:(0.9710714221) A[1]:(0.0288489758968) A[2]:(9.43239228945e-06) A[3]:(7.01922908775e-05)\n",
      " state (6)  A[0]:(0.320238023996) A[1]:(0.679742097855) A[2]:(1.08979857032e-05) A[3]:(8.9970199042e-06)\n",
      " state (7)  A[0]:(0.00775044690818) A[1]:(0.992248356342) A[2]:(1.0445027101e-06) A[3]:(1.26621969798e-07)\n",
      " state (8)  A[0]:(0.000124428304844) A[1]:(0.999875485897) A[2]:(6.95369379855e-08) A[3]:(1.31832000871e-09)\n",
      " state (9)  A[0]:(9.83883546724e-06) A[1]:(0.999990165234) A[2]:(1.32783233298e-08) A[3]:(8.1980394695e-11)\n",
      " state (10)  A[0]:(4.27506029155e-06) A[1]:(0.999995708466) A[2]:(7.7230195572e-09) A[3]:(3.29603427884e-11)\n",
      " state (11)  A[0]:(3.39655707648e-06) A[1]:(0.999996602535) A[2]:(6.65310739834e-09) A[3]:(2.56171819868e-11)\n",
      " state (12)  A[0]:(3.17779790748e-06) A[1]:(0.999996840954) A[2]:(6.37304520268e-09) A[3]:(2.38074022302e-11)\n",
      " state (13)  A[0]:(3.11042549583e-06) A[1]:(0.999996900558) A[2]:(6.28575147488e-09) A[3]:(2.32496955715e-11)\n",
      " state (14)  A[0]:(3.08659650727e-06) A[1]:(0.999996900558) A[2]:(6.25481177963e-09) A[3]:(2.30517879091e-11)\n",
      " state (15)  A[0]:(3.07732057081e-06) A[1]:(0.999996900558) A[2]:(6.24279783423e-09) A[3]:(2.29745337338e-11)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 196000 finished after 27 . Running score: 0.1. Policy_loss: -92050.6126785, Value_loss: 0.98549530038. Times trained:               14002. Times reached goal: 98.               Steps done: 2027738.\n",
      " state (0)  A[0]:(0.883294403553) A[1]:(0.0438066832721) A[2]:(0.0357170291245) A[3]:(0.0371818467975)\n",
      " state (1)  A[0]:(0.00074381288141) A[1]:(0.00369326677173) A[2]:(0.00113486137707) A[3]:(0.994428038597)\n",
      " state (2)  A[0]:(0.000256182043813) A[1]:(0.00237130513415) A[2]:(0.000474175787531) A[3]:(0.9968983531)\n",
      " state (3)  A[0]:(0.00130780064501) A[1]:(0.00466859806329) A[2]:(0.000486056786031) A[3]:(0.993537545204)\n",
      " state (4)  A[0]:(0.997919499874) A[1]:(0.00153155555017) A[2]:(9.78626667347e-06) A[3]:(0.000539186701644)\n",
      " state (5)  A[0]:(0.99463814497) A[1]:(0.00514762988314) A[2]:(8.18972148409e-06) A[3]:(0.000206057433388)\n",
      " state (6)  A[0]:(0.870889663696) A[1]:(0.129044562578) A[2]:(1.52522115968e-05) A[3]:(5.05078751303e-05)\n",
      " state (7)  A[0]:(0.0785105079412) A[1]:(0.921481907368) A[2]:(5.69227222513e-06) A[3]:(1.88480953511e-06)\n",
      " state (8)  A[0]:(0.000680084398482) A[1]:(0.99931961298) A[2]:(2.68053867103e-07) A[3]:(8.62420712622e-09)\n",
      " state (9)  A[0]:(2.10497364606e-05) A[1]:(0.999978899956) A[2]:(2.87341599403e-08) A[3]:(1.81550482981e-10)\n",
      " state (10)  A[0]:(5.61239994568e-06) A[1]:(0.999994397163) A[2]:(1.23417809306e-08) A[3]:(4.20696359693e-11)\n",
      " state (11)  A[0]:(3.71366149921e-06) A[1]:(0.999996304512) A[2]:(9.48697032044e-09) A[3]:(2.66237275282e-11)\n",
      " state (12)  A[0]:(3.23217159348e-06) A[1]:(0.999996781349) A[2]:(8.68643112995e-09) A[3]:(2.28139243608e-11)\n",
      " state (13)  A[0]:(3.06545757667e-06) A[1]:(0.999996900558) A[2]:(8.40009040104e-09) A[3]:(2.15042584434e-11)\n",
      " state (14)  A[0]:(2.99745829579e-06) A[1]:(0.999997019768) A[2]:(8.28191915048e-09) A[3]:(2.0970966283e-11)\n",
      " state (15)  A[0]:(2.96708890346e-06) A[1]:(0.999997019768) A[2]:(8.22890289243e-09) A[3]:(2.07329344665e-11)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 197000 finished after 22 . Running score: 0.09. Policy_loss: -92050.6137614, Value_loss: 0.985822555873. Times trained:               14812. Times reached goal: 132.               Steps done: 2042550.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.906052172184) A[1]:(0.0364883430302) A[2]:(0.0327135920525) A[3]:(0.0247458610684)\n",
      " state (1)  A[0]:(0.00107933895197) A[1]:(0.00440260441974) A[2]:(0.00158019701485) A[3]:(0.992937862873)\n",
      " state (2)  A[0]:(0.000287214672426) A[1]:(0.00251026195474) A[2]:(0.000532536767423) A[3]:(0.996670007706)\n",
      " state (3)  A[0]:(0.00445880601183) A[1]:(0.00784190651029) A[2]:(0.000622681749519) A[3]:(0.987076580524)\n",
      " state (4)  A[0]:(0.997218489647) A[1]:(0.00217908620834) A[2]:(9.16833323572e-06) A[3]:(0.000593238393776)\n",
      " state (5)  A[0]:(0.954921603203) A[1]:(0.0449194684625) A[2]:(1.26648919831e-05) A[3]:(0.000146258622408)\n",
      " state (6)  A[0]:(0.107595980167) A[1]:(0.892393410206) A[2]:(6.50948186376e-06) A[3]:(4.1242478801e-06)\n",
      " state (7)  A[0]:(0.00102175248321) A[1]:(0.998977899551) A[2]:(3.14955343583e-07) A[3]:(2.09278212537e-08)\n",
      " state (8)  A[0]:(1.31637389131e-05) A[1]:(0.999986827374) A[2]:(1.8641239663e-08) A[3]:(1.81800019483e-10)\n",
      " state (9)  A[0]:(1.39978180869e-06) A[1]:(0.999998569489) A[2]:(4.38424541116e-09) A[3]:(1.6158590091e-11)\n",
      " state (10)  A[0]:(7.1337603913e-07) A[1]:(0.999999284744) A[2]:(2.84119350269e-09) A[3]:(7.80726351624e-12)\n",
      " state (11)  A[0]:(5.9241267536e-07) A[1]:(0.999999403954) A[2]:(2.5218773736e-09) A[3]:(6.38546247128e-12)\n",
      " state (12)  A[0]:(5.60179501008e-07) A[1]:(0.999999463558) A[2]:(2.43324915772e-09) A[3]:(6.00890474531e-12)\n",
      " state (13)  A[0]:(5.49633909941e-07) A[1]:(0.999999463558) A[2]:(2.40394260054e-09) A[3]:(5.8855832534e-12)\n",
      " state (14)  A[0]:(5.45705177046e-07) A[1]:(0.999999463558) A[2]:(2.39300446125e-09) A[3]:(5.8395471618e-12)\n",
      " state (15)  A[0]:(5.44118620383e-07) A[1]:(0.999999463558) A[2]:(2.38859021451e-09) A[3]:(5.82093184417e-12)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 198000 finished after 4 . Running score: 0.13. Policy_loss: -92050.6336709, Value_loss: 1.43319302994. Times trained:               15693. Times reached goal: 149.               Steps done: 2058243.\n",
      " state (0)  A[0]:(0.916097044945) A[1]:(0.0328233838081) A[2]:(0.0311680361629) A[3]:(0.0199115555733)\n",
      " state (1)  A[0]:(0.00158064241987) A[1]:(0.00523651810363) A[2]:(0.00209356565028) A[3]:(0.99108928442)\n",
      " state (2)  A[0]:(0.000370228517568) A[1]:(0.00282097072341) A[2]:(0.000646706845146) A[3]:(0.996162116528)\n",
      " state (3)  A[0]:(0.000762334093451) A[1]:(0.00402038590983) A[2]:(0.000432962639024) A[3]:(0.994784295559)\n",
      " state (4)  A[0]:(0.997487962246) A[1]:(0.0020734318532) A[2]:(6.37668972558e-06) A[3]:(0.000432230765)\n",
      " state (5)  A[0]:(0.976400613785) A[1]:(0.0234844088554) A[2]:(6.99111160429e-06) A[3]:(0.000108002263005)\n",
      " state (6)  A[0]:(0.24119643867) A[1]:(0.758792221546) A[2]:(5.75522381041e-06) A[3]:(5.54832149646e-06)\n",
      " state (7)  A[0]:(0.00336873182096) A[1]:(0.996630907059) A[2]:(3.27772198716e-07) A[3]:(3.64384824536e-08)\n",
      " state (8)  A[0]:(5.11355465278e-05) A[1]:(0.999948859215) A[2]:(1.95922762458e-08) A[3]:(3.22398163721e-10)\n",
      " state (9)  A[0]:(2.7386504371e-06) A[1]:(0.999997258186) A[2]:(2.79797696123e-09) A[3]:(1.20751160806e-11)\n",
      " state (10)  A[0]:(9.21035734791e-07) A[1]:(0.99999910593) A[2]:(1.36192768174e-09) A[3]:(3.55218697339e-12)\n",
      " state (11)  A[0]:(6.74548971347e-07) A[1]:(0.999999344349) A[2]:(1.10933440212e-09) A[3]:(2.50210941376e-12)\n",
      " state (12)  A[0]:(6.17099544797e-07) A[1]:(0.999999403954) A[2]:(1.04632180697e-09) A[3]:(2.26294395549e-12)\n",
      " state (13)  A[0]:(6.00323801336e-07) A[1]:(0.999999403954) A[2]:(1.02759800669e-09) A[3]:(2.19336636527e-12)\n",
      " state (14)  A[0]:(5.94691641709e-07) A[1]:(0.999999403954) A[2]:(1.02129071866e-09) A[3]:(2.16997643812e-12)\n",
      " state (15)  A[0]:(5.92595768012e-07) A[1]:(0.999999403954) A[2]:(1.0189422639e-09) A[3]:(2.16124449066e-12)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 199000 finished after 19 . Running score: 0.12. Policy_loss: -92050.6126321, Value_loss: 1.4203034528. Times trained:               14411. Times reached goal: 135.               Steps done: 2072654.\n",
      "action_dist \n",
      "tensor([[ 0.9287,  0.0276,  0.0281,  0.0156]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9287,  0.0276,  0.0281,  0.0156]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9287,  0.0276,  0.0281,  0.0156]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9287,  0.0276,  0.0281,  0.0156]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9287,  0.0276,  0.0281,  0.0156]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9287,  0.0276,  0.0281,  0.0156]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9287,  0.0276,  0.0281,  0.0156]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9287,  0.0276,  0.0281,  0.0156]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9287,  0.0276,  0.0281,  0.0156]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9287,  0.0276,  0.0281,  0.0156]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9767e-01,  2.2020e-03,  2.6322e-06,  1.2195e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9767e-01,  2.2020e-03,  2.6322e-06,  1.2195e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.2179e-04,  9.9958e-01,  3.5001e-08,  1.2351e-09]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.2185e-04,  9.9958e-01,  3.5004e-08,  1.2353e-09]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.928719699383) A[1]:(0.0276150591671) A[2]:(0.0280800908804) A[3]:(0.0155851310119)\n",
      " state (1)  A[0]:(0.00176290853415) A[1]:(0.00496155535802) A[2]:(0.00200670468621) A[3]:(0.99126881361)\n",
      " state (2)  A[0]:(0.000318018399412) A[1]:(0.00248668086715) A[2]:(0.000515530700795) A[3]:(0.996679782867)\n",
      " state (3)  A[0]:(0.99345344305) A[1]:(0.00339624797925) A[2]:(2.47567822953e-05) A[3]:(0.00312552414834)\n",
      " state (4)  A[0]:(0.997673988342) A[1]:(0.00220142002217) A[2]:(2.63211995843e-06) A[3]:(0.000121958561067)\n",
      " state (5)  A[0]:(0.969602406025) A[1]:(0.030367905274) A[2]:(3.5654761632e-06) A[3]:(2.61501045316e-05)\n",
      " state (6)  A[0]:(0.29067376256) A[1]:(0.709320664406) A[2]:(3.12586576001e-06) A[3]:(2.42316946242e-06)\n",
      " state (7)  A[0]:(0.0104376571253) A[1]:(0.989561975002) A[2]:(3.24638278926e-07) A[3]:(4.85245195136e-08)\n",
      " state (8)  A[0]:(0.000422052195063) A[1]:(0.99957793951) A[2]:(3.50154181206e-08) A[3]:(1.23589616319e-09)\n",
      " state (9)  A[0]:(1.74710057763e-05) A[1]:(0.999982535839) A[2]:(3.92871246646e-09) A[3]:(3.26114760196e-11)\n",
      " state (10)  A[0]:(3.030810376e-06) A[1]:(0.999996960163) A[2]:(1.19124532461e-09) A[3]:(4.39667529814e-12)\n",
      " state (11)  A[0]:(1.68387259691e-06) A[1]:(0.99999833107) A[2]:(7.99499966231e-10) A[3]:(2.24145029794e-12)\n",
      " state (12)  A[0]:(1.42076248721e-06) A[1]:(0.999998569489) A[2]:(7.12609526943e-10) A[3]:(1.84412576355e-12)\n",
      " state (13)  A[0]:(1.35261723244e-06) A[1]:(0.999998629093) A[2]:(6.89318768732e-10) A[3]:(1.74269279563e-12)\n",
      " state (14)  A[0]:(1.3325108057e-06) A[1]:(0.999998688698) A[2]:(6.82388257012e-10) A[3]:(1.71282703732e-12)\n",
      " state (15)  A[0]:(1.32597847369e-06) A[1]:(0.999998688698) A[2]:(6.80133782627e-10) A[3]:(1.70309968385e-12)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 200000 finished after 14 . Running score: 0.09. Policy_loss: -92050.6407975, Value_loss: 0.989890786156. Times trained:               14612. Times reached goal: 134.               Steps done: 2087266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.926398813725) A[1]:(0.0287580303848) A[2]:(0.0277228988707) A[3]:(0.0171202458441)\n",
      " state (1)  A[0]:(0.00168601307087) A[1]:(0.00500029139221) A[2]:(0.0019884465728) A[3]:(0.991325259209)\n",
      " state (2)  A[0]:(0.000342545972671) A[1]:(0.00260833324865) A[2]:(0.000581578060519) A[3]:(0.996467530727)\n",
      " state (3)  A[0]:(0.000316782679874) A[1]:(0.00271120783873) A[2]:(0.000326883338857) A[3]:(0.996645152569)\n",
      " state (4)  A[0]:(0.997744441032) A[1]:(0.00185300654266) A[2]:(4.55055942439e-06) A[3]:(0.000397991709178)\n",
      " state (5)  A[0]:(0.990728139877) A[1]:(0.00916904769838) A[2]:(3.32010722559e-06) A[3]:(9.9515877082e-05)\n",
      " state (6)  A[0]:(0.631523132324) A[1]:(0.368459522724) A[2]:(4.89555623062e-06) A[3]:(1.24220086946e-05)\n",
      " state (7)  A[0]:(0.0178051609546) A[1]:(0.982194185257) A[2]:(5.11923190061e-07) A[3]:(1.34789644335e-07)\n",
      " state (8)  A[0]:(0.000183890078915) A[1]:(0.999816060066) A[2]:(2.21610765294e-08) A[3]:(6.76963207713e-10)\n",
      " state (9)  A[0]:(5.67522147321e-06) A[1]:(0.999994337559) A[2]:(2.09594963607e-09) A[3]:(1.25378847929e-11)\n",
      " state (10)  A[0]:(1.39607834626e-06) A[1]:(0.999998629093) A[2]:(8.15623568684e-10) A[3]:(2.50959734764e-12)\n",
      " state (11)  A[0]:(9.02461863461e-07) A[1]:(0.99999910593) A[2]:(6.08790906576e-10) A[3]:(1.5195000206e-12)\n",
      " state (12)  A[0]:(7.83438679264e-07) A[1]:(0.99999922514) A[2]:(5.53884549337e-10) A[3]:(1.29061659363e-12)\n",
      " state (13)  A[0]:(7.43968030292e-07) A[1]:(0.999999284744) A[2]:(5.35128941159e-10) A[3]:(1.21556432454e-12)\n",
      " state (14)  A[0]:(7.28365250779e-07) A[1]:(0.999999284744) A[2]:(5.27639043568e-10) A[3]:(1.18597753146e-12)\n",
      " state (15)  A[0]:(7.21519427316e-07) A[1]:(0.999999284744) A[2]:(5.24341403629e-10) A[3]:(1.17300136618e-12)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 201000 finished after 12 . Running score: 0.14. Policy_loss: -92050.6114226, Value_loss: 0.993606016542. Times trained:               13494. Times reached goal: 121.               Steps done: 2100760.\n",
      " state (0)  A[0]:(0.942210316658) A[1]:(0.0223098695278) A[2]:(0.0230533201247) A[3]:(0.0124265169725)\n",
      " state (1)  A[0]:(0.00295105599798) A[1]:(0.00590615160763) A[2]:(0.00276128109545) A[3]:(0.988381505013)\n",
      " state (2)  A[0]:(0.000442649878096) A[1]:(0.00282639218494) A[2]:(0.000696853385307) A[3]:(0.996034085751)\n",
      " state (3)  A[0]:(0.00168240128551) A[1]:(0.0054193851538) A[2]:(0.000510879792273) A[3]:(0.992387354374)\n",
      " state (4)  A[0]:(0.998171210289) A[1]:(0.00158898090012) A[2]:(3.63386902791e-06) A[3]:(0.000236200299696)\n",
      " state (5)  A[0]:(0.995114386082) A[1]:(0.00478799361736) A[2]:(2.8303079489e-06) A[3]:(9.47637672652e-05)\n",
      " state (6)  A[0]:(0.939225316048) A[1]:(0.0607439726591) A[2]:(3.7864888327e-06) A[3]:(2.69507036137e-05)\n",
      " state (7)  A[0]:(0.380252659321) A[1]:(0.619739353657) A[2]:(3.45856369677e-06) A[3]:(4.52562699138e-06)\n",
      " state (8)  A[0]:(0.0245323237032) A[1]:(0.975466966629) A[2]:(5.78630420023e-07) A[3]:(1.52089924654e-07)\n",
      " state (9)  A[0]:(0.000683362304699) A[1]:(0.999316573143) A[2]:(4.97786167841e-08) A[3]:(2.23978036118e-09)\n",
      " state (10)  A[0]:(3.35986514983e-05) A[1]:(0.99996638298) A[2]:(6.43148823087e-09) A[3]:(6.65315372239e-11)\n",
      " state (11)  A[0]:(8.10652909422e-06) A[1]:(0.999991893768) A[2]:(2.46464737508e-09) A[3]:(1.26711861489e-11)\n",
      " state (12)  A[0]:(4.82759423903e-06) A[1]:(0.999995172024) A[2]:(1.7398763541e-09) A[3]:(6.91347369977e-12)\n",
      " state (13)  A[0]:(3.98823613068e-06) A[1]:(0.999996006489) A[2]:(1.53098889122e-09) A[3]:(5.52567453599e-12)\n",
      " state (14)  A[0]:(3.68914265891e-06) A[1]:(0.999996304512) A[2]:(1.45334577706e-09) A[3]:(5.04088697867e-12)\n",
      " state (15)  A[0]:(3.56045029548e-06) A[1]:(0.999996423721) A[2]:(1.41938405473e-09) A[3]:(4.83370331608e-12)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 202000 finished after 3 . Running score: 0.13. Policy_loss: -92050.6130727, Value_loss: 0.978787080983. Times trained:               15319. Times reached goal: 120.               Steps done: 2116079.\n",
      " state (0)  A[0]:(0.955882787704) A[1]:(0.0172000490129) A[2]:(0.0181621853262) A[3]:(0.00875500869006)\n",
      " state (1)  A[0]:(0.0108097400516) A[1]:(0.0103039890528) A[2]:(0.00642611319199) A[3]:(0.972460150719)\n",
      " state (2)  A[0]:(0.000996086979285) A[1]:(0.00409468077123) A[2]:(0.00122496089898) A[3]:(0.99368429184)\n",
      " state (3)  A[0]:(0.000493288855068) A[1]:(0.0036375594791) A[2]:(0.000405525555834) A[3]:(0.995463609695)\n",
      " state (4)  A[0]:(0.99659216404) A[1]:(0.00321122538298) A[2]:(3.79763514502e-06) A[3]:(0.00019284246082)\n",
      " state (5)  A[0]:(0.933982670307) A[1]:(0.0659937858582) A[2]:(3.57960379915e-06) A[3]:(1.99709302251e-05)\n",
      " state (6)  A[0]:(0.139605388045) A[1]:(0.860392510891) A[2]:(1.50904804741e-06) A[3]:(6.02432464802e-07)\n",
      " state (7)  A[0]:(0.00323793850839) A[1]:(0.996761977673) A[2]:(1.07113351078e-07) A[3]:(5.49380585468e-09)\n",
      " state (8)  A[0]:(7.71703125793e-05) A[1]:(0.999922811985) A[2]:(7.8297768269e-09) A[3]:(5.93662341508e-11)\n",
      " state (9)  A[0]:(3.76020784643e-06) A[1]:(0.999996244907) A[2]:(9.64717528085e-10) A[3]:(1.5598435087e-12)\n",
      " state (10)  A[0]:(1.06638401576e-06) A[1]:(0.999998927116) A[2]:(4.0510297894e-10) A[3]:(3.41827938568e-13)\n",
      " state (11)  A[0]:(7.3566525316e-07) A[1]:(0.999999284744) A[2]:(3.13977649435e-10) A[3]:(2.18408475442e-13)\n",
      " state (12)  A[0]:(6.62682737129e-07) A[1]:(0.999999344349) A[2]:(2.9230010079e-10) A[3]:(1.92478113526e-13)\n",
      " state (13)  A[0]:(6.42378722659e-07) A[1]:(0.999999344349) A[2]:(2.86153767348e-10) A[3]:(1.85342138772e-13)\n",
      " state (14)  A[0]:(6.35884930489e-07) A[1]:(0.999999344349) A[2]:(2.84182094523e-10) A[3]:(1.83061289109e-13)\n",
      " state (15)  A[0]:(6.33564752661e-07) A[1]:(0.999999344349) A[2]:(2.83477769036e-10) A[3]:(1.82243638041e-13)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 203000 finished after 51 . Running score: 0.18. Policy_loss: -92050.6134054, Value_loss: 1.19601456566. Times trained:               13135. Times reached goal: 129.               Steps done: 2129214.\n",
      " state (0)  A[0]:(0.960967242718) A[1]:(0.0148353138939) A[2]:(0.0162537060678) A[3]:(0.00794372521341)\n",
      " state (1)  A[0]:(0.0125499423593) A[1]:(0.0108582628891) A[2]:(0.00733159994707) A[3]:(0.969260215759)\n",
      " state (2)  A[0]:(0.00129536830354) A[1]:(0.00461056828499) A[2]:(0.00158374744933) A[3]:(0.992510318756)\n",
      " state (3)  A[0]:(0.00193679647055) A[1]:(0.00602845614776) A[2]:(0.000697014678735) A[3]:(0.991337716579)\n",
      " state (4)  A[0]:(0.998335719109) A[1]:(0.00152461847756) A[2]:(2.68834355666e-06) A[3]:(0.000136950795422)\n",
      " state (5)  A[0]:(0.989560604095) A[1]:(0.0104069998488) A[2]:(2.03478430194e-06) A[3]:(3.03502965835e-05)\n",
      " state (6)  A[0]:(0.714323937893) A[1]:(0.285669863224) A[2]:(2.46241916102e-06) A[3]:(3.76708953809e-06)\n",
      " state (7)  A[0]:(0.0713552683592) A[1]:(0.928644001484) A[2]:(5.6713719232e-07) A[3]:(1.37214627216e-07)\n",
      " state (8)  A[0]:(0.00279336399399) A[1]:(0.997206568718) A[2]:(5.4015092843e-08) A[3]:(2.37713559947e-09)\n",
      " state (9)  A[0]:(7.91239290265e-05) A[1]:(0.999920845032) A[2]:(4.14485379352e-09) A[3]:(2.95559896002e-11)\n",
      " state (10)  A[0]:(7.43522195989e-06) A[1]:(0.999992549419) A[2]:(7.66904595384e-10) A[3]:(1.62220974501e-12)\n",
      " state (11)  A[0]:(2.98886902783e-06) A[1]:(0.999997019768) A[2]:(4.01682964668e-10) A[3]:(5.29544561412e-13)\n",
      " state (12)  A[0]:(2.22727271648e-06) A[1]:(0.999997794628) A[2]:(3.26225046976e-10) A[3]:(3.68675738809e-13)\n",
      " state (13)  A[0]:(2.02109936254e-06) A[1]:(0.999997973442) A[2]:(3.04614500291e-10) A[3]:(3.26997706256e-13)\n",
      " state (14)  A[0]:(1.95160532712e-06) A[1]:(0.999998033047) A[2]:(2.97206453892e-10) A[3]:(3.13119159507e-13)\n",
      " state (15)  A[0]:(1.92465699911e-06) A[1]:(0.999998092651) A[2]:(2.94319874028e-10) A[3]:(3.07751274551e-13)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 204000 finished after 10 . Running score: 0.07. Policy_loss: -92050.6126769, Value_loss: 1.20066829321. Times trained:               13385. Times reached goal: 131.               Steps done: 2142599.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9641,  0.0138,  0.0140,  0.0081]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9641,  0.0138,  0.0140,  0.0081]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9587e-01,  4.0148e-03,  1.6511e-06,  1.1414e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9641,  0.0138,  0.0140,  0.0081]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9641,  0.0138,  0.0140,  0.0081]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9641,  0.0138,  0.0140,  0.0081]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9641,  0.0138,  0.0140,  0.0081]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9641,  0.0138,  0.0140,  0.0081]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9641,  0.0138,  0.0140,  0.0081]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9641,  0.0138,  0.0140,  0.0081]])\n",
      "On state=0, selected action=1\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9587e-01,  4.0132e-03,  1.6512e-06,  1.1416e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9587e-01,  4.0131e-03,  1.6512e-06,  1.1416e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.7014e-05,  9.9992e-01,  1.2036e-09,  3.3568e-11]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.7021e-05,  9.9992e-01,  1.2037e-09,  3.3572e-11]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.964115560055) A[1]:(0.013793724589) A[2]:(0.0140312500298) A[3]:(0.00805945508182)\n",
      " state (1)  A[0]:(0.00429824041203) A[1]:(0.00720168929547) A[2]:(0.00335738249123) A[3]:(0.985142707825)\n",
      " state (2)  A[0]:(0.000412289839005) A[1]:(0.00294310739264) A[2]:(0.000565407273825) A[3]:(0.996079206467)\n",
      " state (3)  A[0]:(0.0173590369523) A[1]:(0.0180139560252) A[2]:(0.00058217451442) A[3]:(0.964044809341)\n",
      " state (4)  A[0]:(0.995872437954) A[1]:(0.00401170644909) A[2]:(1.65120070506e-06) A[3]:(0.00011417841597)\n",
      " state (5)  A[0]:(0.893727004528) A[1]:(0.106262713671) A[2]:(1.39988981118e-06) A[3]:(8.88512659003e-06)\n",
      " state (6)  A[0]:(0.111558407545) A[1]:(0.888440966606) A[2]:(3.46753097347e-07) A[3]:(2.72887234587e-07)\n",
      " state (7)  A[0]:(0.00315893930383) A[1]:(0.996841013432) A[2]:(2.13517541425e-08) A[3]:(3.16957149415e-09)\n",
      " state (8)  A[0]:(7.70323167671e-05) A[1]:(0.999922990799) A[2]:(1.2038611219e-09) A[3]:(3.35771029036e-11)\n",
      " state (9)  A[0]:(3.88733542422e-06) A[1]:(0.999996125698) A[2]:(1.22192866936e-10) A[3]:(8.79507133355e-13)\n",
      " state (10)  A[0]:(1.15789441679e-06) A[1]:(0.999998867512) A[2]:(4.86088044815e-11) A[3]:(2.00604086836e-13)\n",
      " state (11)  A[0]:(8.13786186882e-07) A[1]:(0.999999165535) A[2]:(3.7195180963e-11) A[3]:(1.30349263284e-13)\n",
      " state (12)  A[0]:(7.36871584195e-07) A[1]:(0.999999284744) A[2]:(3.4501047852e-11) A[3]:(1.15417210836e-13)\n",
      " state (13)  A[0]:(7.15233738902e-07) A[1]:(0.999999284744) A[2]:(3.37328290301e-11) A[3]:(1.11264845265e-13)\n",
      " state (14)  A[0]:(7.0822966336e-07) A[1]:(0.999999284744) A[2]:(3.34834104887e-11) A[3]:(1.09922223504e-13)\n",
      " state (15)  A[0]:(7.05697971171e-07) A[1]:(0.999999284744) A[2]:(3.33934165042e-11) A[3]:(1.09436053698e-13)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 205000 finished after 14 . Running score: 0.15. Policy_loss: -92050.6127474, Value_loss: 1.19953350026. Times trained:               12562. Times reached goal: 128.               Steps done: 2155161.\n",
      " state (0)  A[0]:(0.96722894907) A[1]:(0.0124074192718) A[2]:(0.0130259674042) A[3]:(0.00733768334612)\n",
      " state (1)  A[0]:(0.00552679505199) A[1]:(0.0070972898975) A[2]:(0.00373946339823) A[3]:(0.983636438847)\n",
      " state (2)  A[0]:(0.000587266578805) A[1]:(0.00307301850989) A[2]:(0.000755674496759) A[3]:(0.995584011078)\n",
      " state (3)  A[0]:(0.57625323534) A[1]:(0.0408698283136) A[2]:(0.000870456220582) A[3]:(0.382006466389)\n",
      " state (4)  A[0]:(0.998326778412) A[1]:(0.00160494376905) A[2]:(1.40413976624e-06) A[3]:(6.68896173011e-05)\n",
      " state (5)  A[0]:(0.985815286636) A[1]:(0.0141729591414) A[2]:(1.19898936646e-06) A[3]:(1.05529843495e-05)\n",
      " state (6)  A[0]:(0.653568446636) A[1]:(0.346428573132) A[2]:(1.2499344848e-06) A[3]:(1.69089844348e-06)\n",
      " state (7)  A[0]:(0.0496085472405) A[1]:(0.950391232967) A[2]:(1.92958850675e-07) A[3]:(5.38967448449e-08)\n",
      " state (8)  A[0]:(0.00125708535779) A[1]:(0.998742878437) A[2]:(1.12766844751e-08) A[3]:(5.69174651854e-10)\n",
      " state (9)  A[0]:(3.39726502716e-05) A[1]:(0.999966025352) A[2]:(7.17099324365e-10) A[3]:(6.64440984871e-12)\n",
      " state (10)  A[0]:(5.27093561686e-06) A[1]:(0.999994754791) A[2]:(1.75339034958e-10) A[3]:(6.67243604119e-13)\n",
      " state (11)  A[0]:(2.81688858195e-06) A[1]:(0.999997198582) A[2]:(1.09431609607e-10) A[3]:(3.07511584556e-13)\n",
      " state (12)  A[0]:(2.30940236179e-06) A[1]:(0.999997675419) A[2]:(9.42831021589e-11) A[3]:(2.40352524125e-13)\n",
      " state (13)  A[0]:(2.15607656173e-06) A[1]:(0.999997854233) A[2]:(8.95613513907e-11) A[3]:(2.20641972574e-13)\n",
      " state (14)  A[0]:(2.09908284887e-06) A[1]:(0.999997913837) A[2]:(8.77897338158e-11) A[3]:(2.13372369903e-13)\n",
      " state (15)  A[0]:(2.07484345083e-06) A[1]:(0.999997913837) A[2]:(8.70339703085e-11) A[3]:(2.10286025789e-13)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 206000 finished after 9 . Running score: 0.11. Policy_loss: -92050.6125043, Value_loss: 1.2036110585. Times trained:               13147. Times reached goal: 108.               Steps done: 2168308.\n",
      " state (0)  A[0]:(0.964709103107) A[1]:(0.0136634008959) A[2]:(0.0137497773394) A[3]:(0.00787770282477)\n",
      " state (1)  A[0]:(0.0158352162689) A[1]:(0.0104383677244) A[2]:(0.00700290454552) A[3]:(0.966723501682)\n",
      " state (2)  A[0]:(0.00253981119022) A[1]:(0.00572895864025) A[2]:(0.00246740179136) A[3]:(0.989263832569)\n",
      " state (3)  A[0]:(0.00107695756014) A[1]:(0.00427145604044) A[2]:(0.00106150237843) A[3]:(0.993590056896)\n",
      " state (4)  A[0]:(0.998818576336) A[1]:(0.000959972385317) A[2]:(4.69253973279e-06) A[3]:(0.000216785294469)\n",
      " state (5)  A[0]:(0.999190747738) A[1]:(0.000761096947826) A[2]:(1.15280101909e-06) A[3]:(4.70219492854e-05)\n",
      " state (6)  A[0]:(0.998276293278) A[1]:(0.00169401091989) A[2]:(8.74173963439e-07) A[3]:(2.88122755592e-05)\n",
      " state (7)  A[0]:(0.991495192051) A[1]:(0.00849449634552) A[2]:(7.98214955466e-07) A[3]:(9.52861046244e-06)\n",
      " state (8)  A[0]:(0.941756844521) A[1]:(0.0582385957241) A[2]:(9.02397175651e-07) A[3]:(3.63303797712e-06)\n",
      " state (9)  A[0]:(0.73560410738) A[1]:(0.264393389225) A[2]:(9.68635049503e-07) A[3]:(1.56273654284e-06)\n",
      " state (10)  A[0]:(0.266225844622) A[1]:(0.733773350716) A[2]:(5.5567676327e-07) A[3]:(2.90860299401e-07)\n",
      " state (11)  A[0]:(0.0316812768579) A[1]:(0.968318581581) A[2]:(1.19401391885e-07) A[3]:(1.64854156992e-08)\n",
      " state (12)  A[0]:(0.00453528948128) A[1]:(0.995464682579) A[2]:(2.84440577758e-08) A[3]:(1.30653243779e-09)\n",
      " state (13)  A[0]:(0.00125339941587) A[1]:(0.998746573925) A[2]:(1.10959694766e-08) A[3]:(2.44685965933e-10)\n",
      " state (14)  A[0]:(0.000533082755283) A[1]:(0.999466896057) A[2]:(5.96120486307e-09) A[3]:(8.01332056266e-11)\n",
      " state (15)  A[0]:(0.000289647025056) A[1]:(0.999710321426) A[2]:(3.8338279218e-09) A[3]:(3.60938466615e-11)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 207000 finished after 14 . Running score: 0.06. Policy_loss: -92050.6134679, Value_loss: 0.978555360157. Times trained:               13179. Times reached goal: 130.               Steps done: 2181487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.961214661598) A[1]:(0.015525508672) A[2]:(0.014192215167) A[3]:(0.00906764250249)\n",
      " state (1)  A[0]:(0.00939297955483) A[1]:(0.00825935229659) A[2]:(0.00451015029103) A[3]:(0.977837502956)\n",
      " state (2)  A[0]:(0.00117327959742) A[1]:(0.00431453250349) A[2]:(0.00132751476485) A[3]:(0.993184685707)\n",
      " state (3)  A[0]:(0.0135352537036) A[1]:(0.0154783520848) A[2]:(0.00113397347741) A[3]:(0.96985244751)\n",
      " state (4)  A[0]:(0.998608112335) A[1]:(0.00129989825655) A[2]:(1.50028506596e-06) A[3]:(9.04827902559e-05)\n",
      " state (5)  A[0]:(0.992859244347) A[1]:(0.00709747895598) A[2]:(9.98123255158e-07) A[3]:(4.23027595389e-05)\n",
      " state (6)  A[0]:(0.800475120544) A[1]:(0.199519723654) A[2]:(1.11246595225e-06) A[3]:(4.06421304433e-06)\n",
      " state (7)  A[0]:(0.0632422715425) A[1]:(0.936757445335) A[2]:(2.31422518482e-07) A[3]:(5.48034151393e-08)\n",
      " state (8)  A[0]:(0.00041600028635) A[1]:(0.999584019184) A[2]:(6.44720321574e-09) A[3]:(8.61975282818e-11)\n",
      " state (9)  A[0]:(7.9807050497e-06) A[1]:(0.999992012978) A[2]:(4.01248340109e-10) A[3]:(6.07294271295e-13)\n",
      " state (10)  A[0]:(1.64380253409e-06) A[1]:(0.99999833107) A[2]:(1.33683786263e-10) A[3]:(8.40181128337e-14)\n",
      " state (11)  A[0]:(9.44016449012e-07) A[1]:(0.999999046326) A[2]:(9.10735861726e-11) A[3]:(4.18693976245e-14)\n",
      " state (12)  A[0]:(7.55654923523e-07) A[1]:(0.99999922514) A[2]:(7.81169087749e-11) A[3]:(3.16292910645e-14)\n",
      " state (13)  A[0]:(6.82151551246e-07) A[1]:(0.999999344349) A[2]:(7.28067259259e-11) A[3]:(2.77934871163e-14)\n",
      " state (14)  A[0]:(6.48103934964e-07) A[1]:(0.999999344349) A[2]:(7.02899752292e-11) A[3]:(2.6050271288e-14)\n",
      " state (15)  A[0]:(6.30976728644e-07) A[1]:(0.999999344349) A[2]:(6.90091109257e-11) A[3]:(2.51818897223e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 208000 finished after 6 . Running score: 0.14. Policy_loss: -92050.6115391, Value_loss: 1.19203225794. Times trained:               13659. Times reached goal: 118.               Steps done: 2195146.\n",
      " state (0)  A[0]:(0.956281006336) A[1]:(0.0179913789034) A[2]:(0.0145931662992) A[3]:(0.0111344316974)\n",
      " state (1)  A[0]:(0.00538642192259) A[1]:(0.00665103876963) A[2]:(0.00300060980953) A[3]:(0.984961926937)\n",
      " state (2)  A[0]:(0.000803707109299) A[1]:(0.00376770808361) A[2]:(0.000979355187155) A[3]:(0.994449257851)\n",
      " state (3)  A[0]:(0.000622327206656) A[1]:(0.00432644877583) A[2]:(0.000500616966747) A[3]:(0.994550585747)\n",
      " state (4)  A[0]:(0.997580885887) A[1]:(0.00223258230835) A[2]:(2.37710196416e-06) A[3]:(0.000184145275853)\n",
      " state (5)  A[0]:(0.984641671181) A[1]:(0.0152919190004) A[2]:(1.39813710121e-06) A[3]:(6.49939393043e-05)\n",
      " state (6)  A[0]:(0.584322988987) A[1]:(0.415673494339) A[2]:(1.13365911147e-06) A[3]:(2.3616019007e-06)\n",
      " state (7)  A[0]:(0.0175317749381) A[1]:(0.982468128204) A[2]:(1.04318246485e-07) A[3]:(1.12088844872e-08)\n",
      " state (8)  A[0]:(7.98971523182e-05) A[1]:(0.999920129776) A[2]:(2.32804620026e-09) A[3]:(1.23877045774e-11)\n",
      " state (9)  A[0]:(2.52281438406e-06) A[1]:(0.999997496605) A[2]:(2.11907658088e-10) A[3]:(1.69844823969e-13)\n",
      " state (10)  A[0]:(7.31843158519e-07) A[1]:(0.999999284744) A[2]:(9.04790409262e-11) A[3]:(3.65057749383e-14)\n",
      " state (11)  A[0]:(4.72188332878e-07) A[1]:(0.999999523163) A[2]:(6.70362584887e-11) A[3]:(2.11404244804e-14)\n",
      " state (12)  A[0]:(3.93881890659e-07) A[1]:(0.999999582767) A[2]:(5.92359564178e-11) A[3]:(1.68507921375e-14)\n",
      " state (13)  A[0]:(3.61913720326e-07) A[1]:(0.999999642372) A[2]:(5.59184573645e-11) A[3]:(1.51547899257e-14)\n",
      " state (14)  A[0]:(3.46946166019e-07) A[1]:(0.999999642372) A[2]:(5.43348595283e-11) A[3]:(1.43726989945e-14)\n",
      " state (15)  A[0]:(3.39471341704e-07) A[1]:(0.999999642372) A[2]:(5.35362656984e-11) A[3]:(1.3985100106e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 209000 finished after 8 . Running score: 0.19. Policy_loss: -92050.611212, Value_loss: 1.40899213977. Times trained:               13683. Times reached goal: 152.               Steps done: 2208829.\n",
      "action_dist \n",
      "tensor([[ 0.9578,  0.0174,  0.0148,  0.0101]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9578,  0.0174,  0.0148,  0.0101]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9864e-01,  1.2997e-03,  2.2717e-06,  5.6804e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.6961e-03,  9.9630e-01,  3.2027e-08,  2.7829e-10]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.957788527012) A[1]:(0.0173667203635) A[2]:(0.0147509733215) A[3]:(0.010093764402)\n",
      " state (1)  A[0]:(0.0133559573442) A[1]:(0.00937984045595) A[2]:(0.00563925411552) A[3]:(0.971624970436)\n",
      " state (2)  A[0]:(0.00241731130518) A[1]:(0.00573831703514) A[2]:(0.002278679749) A[3]:(0.98956567049)\n",
      " state (3)  A[0]:(0.0040519721806) A[1]:(0.00953626260161) A[2]:(0.00151060509961) A[3]:(0.984901189804)\n",
      " state (4)  A[0]:(0.998641371727) A[1]:(0.00129956798628) A[2]:(2.27181431001e-06) A[3]:(5.68026189285e-05)\n",
      " state (5)  A[0]:(0.99291485548) A[1]:(0.00705252029002) A[2]:(1.29692182327e-06) A[3]:(3.13090095005e-05)\n",
      " state (6)  A[0]:(0.778266847134) A[1]:(0.221730664372) A[2]:(1.10369740014e-06) A[3]:(1.40595795983e-06)\n",
      " state (7)  A[0]:(0.12406860292) A[1]:(0.875930964947) A[2]:(3.733845233e-07) A[3]:(3.36392673717e-08)\n",
      " state (8)  A[0]:(0.00370602379553) A[1]:(0.996293962002) A[2]:(3.20853317248e-08) A[3]:(2.79226475275e-10)\n",
      " state (9)  A[0]:(5.3100342484e-05) A[1]:(0.999946892262) A[2]:(1.69721936505e-09) A[3]:(1.12337201173e-12)\n",
      " state (10)  A[0]:(3.87999443774e-06) A[1]:(0.999996125698) A[2]:(2.84956780394e-10) A[3]:(3.8115212077e-14)\n",
      " state (11)  A[0]:(1.36122275762e-06) A[1]:(0.999998629093) A[2]:(1.40293041073e-10) A[3]:(9.80489086731e-15)\n",
      " state (12)  A[0]:(8.9613598675e-07) A[1]:(0.99999910593) A[2]:(1.05871526823e-10) A[3]:(5.69269879784e-15)\n",
      " state (13)  A[0]:(7.41996814213e-07) A[1]:(0.999999284744) A[2]:(9.32689273037e-11) A[3]:(4.45072911563e-15)\n",
      " state (14)  A[0]:(6.75175215292e-07) A[1]:(0.999999344349) A[2]:(8.75505709597e-11) A[3]:(3.93477339537e-15)\n",
      " state (15)  A[0]:(6.42158852315e-07) A[1]:(0.999999344349) A[2]:(8.46582318137e-11) A[3]:(3.68538148471e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 210000 finished after 4 . Running score: 0.11. Policy_loss: -92050.611217, Value_loss: 1.41041590124. Times trained:               13048. Times reached goal: 105.               Steps done: 2221877.\n",
      " state (0)  A[0]:(0.962639927864) A[1]:(0.0150518026203) A[2]:(0.0138589208946) A[3]:(0.0084493290633)\n",
      " state (1)  A[0]:(0.0176978372037) A[1]:(0.0103303492069) A[2]:(0.00659108581021) A[3]:(0.965380728245)\n",
      " state (2)  A[0]:(0.00272941961884) A[1]:(0.0059143807739) A[2]:(0.00237363483757) A[3]:(0.98898255825)\n",
      " state (3)  A[0]:(0.00467985449359) A[1]:(0.0102540748194) A[2]:(0.00142470409628) A[3]:(0.983641386032)\n",
      " state (4)  A[0]:(0.998664021492) A[1]:(0.0012882289011) A[2]:(1.49874745148e-06) A[3]:(4.62498319393e-05)\n",
      " state (5)  A[0]:(0.988726556301) A[1]:(0.0112498095259) A[2]:(1.0207259038e-06) A[3]:(2.25853946176e-05)\n",
      " state (6)  A[0]:(0.551222681999) A[1]:(0.448776185513) A[2]:(6.78743845128e-07) A[3]:(4.92171352562e-07)\n",
      " state (7)  A[0]:(0.0187697876245) A[1]:(0.981230139732) A[2]:(6.32137044931e-08) A[3]:(2.60775800953e-09)\n",
      " state (8)  A[0]:(0.000113104120828) A[1]:(0.999886870384) A[2]:(1.6836116945e-09) A[3]:(3.37710566976e-12)\n",
      " state (9)  A[0]:(2.47069669967e-06) A[1]:(0.99999755621) A[2]:(1.18682758066e-10) A[3]:(2.49563959992e-14)\n",
      " state (10)  A[0]:(5.4447838238e-07) A[1]:(0.999999463558) A[2]:(4.20562057402e-11) A[3]:(3.57164020649e-15)\n",
      " state (11)  A[0]:(3.12901562438e-07) A[1]:(0.999999701977) A[2]:(2.88200661253e-11) A[3]:(1.74830522577e-15)\n",
      " state (12)  A[0]:(2.47466033443e-07) A[1]:(0.999999761581) A[2]:(2.45694350282e-11) A[3]:(1.29080727261e-15)\n",
      " state (13)  A[0]:(2.21214676799e-07) A[1]:(0.999999761581) A[2]:(2.27681744985e-11) A[3]:(1.11633383794e-15)\n",
      " state (14)  A[0]:(2.08897262155e-07) A[1]:(0.999999761581) A[2]:(2.19001986684e-11) A[3]:(1.03646508964e-15)\n",
      " state (15)  A[0]:(2.02675153105e-07) A[1]:(0.999999821186) A[2]:(2.14557729222e-11) A[3]:(9.9663325942e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 211000 finished after 11 . Running score: 0.09. Policy_loss: -92050.6112082, Value_loss: 0.993844156257. Times trained:               12950. Times reached goal: 115.               Steps done: 2234827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.968573689461) A[1]:(0.0124628823251) A[2]:(0.0113722458482) A[3]:(0.00759118515998)\n",
      " state (1)  A[0]:(0.00800977647305) A[1]:(0.00802620872855) A[2]:(0.00419413531199) A[3]:(0.97976988554)\n",
      " state (2)  A[0]:(0.00108772655949) A[1]:(0.00445194076747) A[2]:(0.00131494365633) A[3]:(0.993145406246)\n",
      " state (3)  A[0]:(0.74099624157) A[1]:(0.0583060905337) A[2]:(0.000918541161809) A[3]:(0.199779123068)\n",
      " state (4)  A[0]:(0.997839570045) A[1]:(0.00211753230542) A[2]:(9.62434683061e-07) A[3]:(4.19639000029e-05)\n",
      " state (5)  A[0]:(0.94483935833) A[1]:(0.0551553964615) A[2]:(5.41322663139e-07) A[3]:(4.72513784189e-06)\n",
      " state (6)  A[0]:(0.240263432264) A[1]:(0.759736299515) A[2]:(1.66465241591e-07) A[3]:(7.01504490053e-08)\n",
      " state (7)  A[0]:(0.0117600783706) A[1]:(0.988239884377) A[2]:(1.50884122974e-08) A[3]:(9.95735494058e-10)\n",
      " state (8)  A[0]:(0.000242484253249) A[1]:(0.999757528305) A[2]:(7.25259852175e-10) A[3]:(6.32932248279e-12)\n",
      " state (9)  A[0]:(6.5185558924e-06) A[1]:(0.999993503094) A[2]:(4.50548730269e-11) A[3]:(5.96097607039e-14)\n",
      " state (10)  A[0]:(1.20535798942e-06) A[1]:(0.999998807907) A[2]:(1.24820110844e-11) A[3]:(6.74689991037e-15)\n",
      " state (11)  A[0]:(6.79417610172e-07) A[1]:(0.999999344349) A[2]:(8.08856367301e-12) A[3]:(3.21241205125e-15)\n",
      " state (12)  A[0]:(5.55466669994e-07) A[1]:(0.999999463558) A[2]:(6.94828396208e-12) A[3]:(2.47303106236e-15)\n",
      " state (13)  A[0]:(5.12789256391e-07) A[1]:(0.999999463558) A[2]:(6.54250265075e-12) A[3]:(2.22852843763e-15)\n",
      " state (14)  A[0]:(4.95008691814e-07) A[1]:(0.999999523163) A[2]:(6.37125378497e-12) A[3]:(2.12827827668e-15)\n",
      " state (15)  A[0]:(4.86812609779e-07) A[1]:(0.999999523163) A[2]:(6.29183987896e-12) A[3]:(2.08238984324e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 212000 finished after 8 . Running score: 0.11. Policy_loss: -92050.6114104, Value_loss: 1.20140328219. Times trained:               13398. Times reached goal: 144.               Steps done: 2248225.\n",
      " state (0)  A[0]:(0.969118356705) A[1]:(0.0126721039414) A[2]:(0.0106175355613) A[3]:(0.00759201683104)\n",
      " state (1)  A[0]:(0.0081067411229) A[1]:(0.00766650401056) A[2]:(0.00373461167328) A[3]:(0.980492115021)\n",
      " state (2)  A[0]:(0.00132817251142) A[1]:(0.00453244661912) A[2]:(0.00136717304122) A[3]:(0.992772221565)\n",
      " state (3)  A[0]:(0.00289063272066) A[1]:(0.00814847648144) A[2]:(0.000958260847256) A[3]:(0.98800265789)\n",
      " state (4)  A[0]:(0.998806536198) A[1]:(0.00114736671094) A[2]:(9.99657345346e-07) A[3]:(4.50972074759e-05)\n",
      " state (5)  A[0]:(0.995219886303) A[1]:(0.00473877461627) A[2]:(6.85671238898e-07) A[3]:(4.0642273234e-05)\n",
      " state (6)  A[0]:(0.837442815304) A[1]:(0.162554904819) A[2]:(3.47877005424e-07) A[3]:(1.95747566067e-06)\n",
      " state (7)  A[0]:(0.0864383652806) A[1]:(0.913561582565) A[2]:(4.96705361286e-08) A[3]:(1.4398595205e-08)\n",
      " state (8)  A[0]:(0.000792958366219) A[1]:(0.999207019806) A[2]:(1.15676279666e-09) A[3]:(2.56062827192e-11)\n",
      " state (9)  A[0]:(1.07659461719e-05) A[1]:(0.999989211559) A[2]:(4.08488728953e-11) A[3]:(9.63767233337e-14)\n",
      " state (10)  A[0]:(1.55315410666e-06) A[1]:(0.999998450279) A[2]:(9.26229665882e-12) A[3]:(7.81734838537e-15)\n",
      " state (11)  A[0]:(7.49113951315e-07) A[1]:(0.99999922514) A[2]:(5.31485792876e-12) A[3]:(3.02736309261e-15)\n",
      " state (12)  A[0]:(5.50071206362e-07) A[1]:(0.999999463558) A[2]:(4.20367820414e-12) A[3]:(2.02371036527e-15)\n",
      " state (13)  A[0]:(4.74412246376e-07) A[1]:(0.999999523163) A[2]:(3.75751643342e-12) A[3]:(1.66821868412e-15)\n",
      " state (14)  A[0]:(4.39563763166e-07) A[1]:(0.999999582767) A[2]:(3.54644113555e-12) A[3]:(1.5100622804e-15)\n",
      " state (15)  A[0]:(4.21968309183e-07) A[1]:(0.999999582767) A[2]:(3.43834509475e-12) A[3]:(1.4316629225e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 213000 finished after 7 . Running score: 0.12. Policy_loss: -92050.6114304, Value_loss: 1.42212640673. Times trained:               13011. Times reached goal: 119.               Steps done: 2261236.\n",
      " state (0)  A[0]:(0.967509388924) A[1]:(0.0136312060058) A[2]:(0.0105567807332) A[3]:(0.00830261688679)\n",
      " state (1)  A[0]:(0.00512660574168) A[1]:(0.00641371682286) A[2]:(0.00258053094149) A[3]:(0.985879123211)\n",
      " state (2)  A[0]:(0.000807386124507) A[1]:(0.00386311556213) A[2]:(0.000918286037631) A[3]:(0.994411230087)\n",
      " state (3)  A[0]:(0.0031874505803) A[1]:(0.00978281255811) A[2]:(0.000754040433094) A[3]:(0.986275672913)\n",
      " state (4)  A[0]:(0.998123109341) A[1]:(0.00182353367563) A[2]:(1.0195218465e-06) A[3]:(5.23127419001e-05)\n",
      " state (5)  A[0]:(0.988546788692) A[1]:(0.0114187365398) A[2]:(7.1559799153e-07) A[3]:(3.3788051951e-05)\n",
      " state (6)  A[0]:(0.669313430786) A[1]:(0.33068561554) A[2]:(3.21245693158e-07) A[3]:(6.69184998969e-07)\n",
      " state (7)  A[0]:(0.0632276982069) A[1]:(0.936772227287) A[2]:(4.49032171446e-08) A[3]:(6.7316072716e-09)\n",
      " state (8)  A[0]:(0.00125286763068) A[1]:(0.998747110367) A[2]:(1.97123450896e-09) A[3]:(3.43544880044e-11)\n",
      " state (9)  A[0]:(2.13612420339e-05) A[1]:(0.999978661537) A[2]:(8.30249410888e-11) A[3]:(1.78090289911e-13)\n",
      " state (10)  A[0]:(2.61840045823e-06) A[1]:(0.999997377396) A[2]:(1.65997562918e-11) A[3]:(1.1918836923e-14)\n",
      " state (11)  A[0]:(1.21401387787e-06) A[1]:(0.999998807907) A[2]:(9.24163610222e-12) A[3]:(4.4181810276e-15)\n",
      " state (12)  A[0]:(9.07325613753e-07) A[1]:(0.99999910593) A[2]:(7.40849308983e-12) A[3]:(3.03032050814e-15)\n",
      " state (13)  A[0]:(8.00962595804e-07) A[1]:(0.99999922514) A[2]:(6.74066184814e-12) A[3]:(2.57757504509e-15)\n",
      " state (14)  A[0]:(7.55108146677e-07) A[1]:(0.99999922514) A[2]:(6.44657504462e-12) A[3]:(2.38754383865e-15)\n",
      " state (15)  A[0]:(7.33068361569e-07) A[1]:(0.999999284744) A[2]:(6.30374225041e-12) A[3]:(2.29739285152e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 214000 finished after 7 . Running score: 0.11. Policy_loss: -92050.6382145, Value_loss: 1.41341203421. Times trained:               13855. Times reached goal: 115.               Steps done: 2275091.\n",
      "action_dist \n",
      "tensor([[ 0.9621,  0.0162,  0.0111,  0.0107]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9621,  0.0162,  0.0111,  0.0107]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9621,  0.0162,  0.0111,  0.0107]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9621,  0.0162,  0.0111,  0.0107]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9621,  0.0162,  0.0111,  0.0107]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9621,  0.0162,  0.0111,  0.0107]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9793e-01,  2.0157e-03,  1.1241e-06,  5.7338e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4211e-03,  9.9758e-01,  3.9619e-09,  1.0925e-10]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.962107181549) A[1]:(0.0161784086376) A[2]:(0.0110514983535) A[3]:(0.0106628825888)\n",
      " state (1)  A[0]:(0.0022550355643) A[1]:(0.00492151314393) A[2]:(0.00157292454969) A[3]:(0.991250514984)\n",
      " state (2)  A[0]:(0.000459639966721) A[1]:(0.00326595525257) A[2]:(0.000652057991829) A[3]:(0.995622336864)\n",
      " state (3)  A[0]:(0.00307076028548) A[1]:(0.0101938126609) A[2]:(0.000677378207911) A[3]:(0.986058056355)\n",
      " state (4)  A[0]:(0.997927367687) A[1]:(0.00201421277598) A[2]:(1.12412351427e-06) A[3]:(5.73189827264e-05)\n",
      " state (5)  A[0]:(0.988770782948) A[1]:(0.0111926421523) A[2]:(8.48172362566e-07) A[3]:(3.5701370507e-05)\n",
      " state (6)  A[0]:(0.679268956184) A[1]:(0.320729851723) A[2]:(3.90319087273e-07) A[3]:(7.77894797466e-07)\n",
      " state (7)  A[0]:(0.0805229023099) A[1]:(0.919477045536) A[2]:(6.45520543685e-08) A[3]:(1.17828742319e-08)\n",
      " state (8)  A[0]:(0.00243318267167) A[1]:(0.997566819191) A[2]:(3.97688770803e-09) A[3]:(1.099131966e-10)\n",
      " state (9)  A[0]:(4.16734910687e-05) A[1]:(0.999958336353) A[2]:(1.69636416025e-10) A[3]:(6.05609312698e-13)\n",
      " state (10)  A[0]:(3.8747657527e-06) A[1]:(0.999996125698) A[2]:(2.7691422072e-11) A[3]:(2.92595876456e-14)\n",
      " state (11)  A[0]:(1.49831612362e-06) A[1]:(0.999998509884) A[2]:(1.34864897028e-11) A[3]:(8.67982782545e-15)\n",
      " state (12)  A[0]:(1.01432055999e-06) A[1]:(0.999998986721) A[2]:(1.00488809981e-11) A[3]:(5.26233490987e-15)\n",
      " state (13)  A[0]:(8.46474165428e-07) A[1]:(0.999999165535) A[2]:(8.76992575782e-12) A[3]:(4.17068689426e-15)\n",
      " state (14)  A[0]:(7.72184591824e-07) A[1]:(0.99999922514) A[2]:(8.18481393772e-12) A[3]:(3.7058504594e-15)\n",
      " state (15)  A[0]:(7.35212893233e-07) A[1]:(0.999999284744) A[2]:(7.88857260764e-12) A[3]:(3.47916072579e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 215000 finished after 8 . Running score: 0.12. Policy_loss: -92050.6113851, Value_loss: 1.2013269363. Times trained:               12937. Times reached goal: 131.               Steps done: 2288028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.967438399792) A[1]:(0.0135244056582) A[2]:(0.0102443648502) A[3]:(0.00879281386733)\n",
      " state (1)  A[0]:(0.00300221866928) A[1]:(0.00527991028503) A[2]:(0.00187524175271) A[3]:(0.989842653275)\n",
      " state (2)  A[0]:(0.000577225466259) A[1]:(0.00340863317251) A[2]:(0.000778983579949) A[3]:(0.995235145092)\n",
      " state (3)  A[0]:(0.00116513925605) A[1]:(0.00584143213928) A[2]:(0.000640100042801) A[3]:(0.992353320122)\n",
      " state (4)  A[0]:(0.997722923756) A[1]:(0.00218984368257) A[2]:(1.8165299025e-06) A[3]:(8.54327445268e-05)\n",
      " state (5)  A[0]:(0.98980909586) A[1]:(0.0101367505267) A[2]:(1.01452019408e-06) A[3]:(5.31455261807e-05)\n",
      " state (6)  A[0]:(0.585521459579) A[1]:(0.414477080107) A[2]:(4.30359676784e-07) A[3]:(1.01999648905e-06)\n",
      " state (7)  A[0]:(0.0220958162099) A[1]:(0.977904140949) A[2]:(2.61687169711e-08) A[3]:(3.69181507587e-09)\n",
      " state (8)  A[0]:(0.000145968326251) A[1]:(0.999854028225) A[2]:(4.78773187762e-10) A[3]:(6.14507793609e-12)\n",
      " state (9)  A[0]:(4.65351240564e-06) A[1]:(0.999995350838) A[2]:(3.34407050662e-11) A[3]:(8.1457450919e-14)\n",
      " state (10)  A[0]:(1.04978721538e-06) A[1]:(0.999998927116) A[2]:(1.0749705813e-11) A[3]:(1.24898997598e-14)\n",
      " state (11)  A[0]:(5.2511092008e-07) A[1]:(0.999999463558) A[2]:(6.35730574086e-12) A[3]:(5.21064091261e-15)\n",
      " state (12)  A[0]:(3.62013821587e-07) A[1]:(0.999999642372) A[2]:(4.79759374589e-12) A[3]:(3.25910728357e-15)\n",
      " state (13)  A[0]:(2.91348300152e-07) A[1]:(0.999999701977) A[2]:(4.07077193135e-12) A[3]:(2.47904838442e-15)\n",
      " state (14)  A[0]:(2.55160784945e-07) A[1]:(0.999999761581) A[2]:(3.68214269839e-12) A[3]:(2.09832253298e-15)\n",
      " state (15)  A[0]:(2.34779932384e-07) A[1]:(0.999999761581) A[2]:(3.45731147704e-12) A[3]:(1.89026053619e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 216000 finished after 17 . Running score: 0.11. Policy_loss: -92050.6122459, Value_loss: 1.20018767295. Times trained:               14218. Times reached goal: 107.               Steps done: 2302246.\n",
      " state (0)  A[0]:(0.969053804874) A[1]:(0.0128325028345) A[2]:(0.00971509609371) A[3]:(0.00839860271662)\n",
      " state (1)  A[0]:(0.0030131440144) A[1]:(0.00569374160841) A[2]:(0.00208759866655) A[3]:(0.989205539227)\n",
      " state (2)  A[0]:(0.00057873991318) A[1]:(0.00348498323001) A[2]:(0.000817425374407) A[3]:(0.99511885643)\n",
      " state (3)  A[0]:(0.000464945856947) A[1]:(0.0035939165391) A[2]:(0.000541443645488) A[3]:(0.995399713516)\n",
      " state (4)  A[0]:(0.997578322887) A[1]:(0.0022644486744) A[2]:(3.11924145535e-06) A[3]:(0.000154133231263)\n",
      " state (5)  A[0]:(0.997422337532) A[1]:(0.0025222802069) A[2]:(1.00401769032e-06) A[3]:(5.44075155631e-05)\n",
      " state (6)  A[0]:(0.975632727146) A[1]:(0.0243459828198) A[2]:(7.4803415373e-07) A[3]:(2.05350115721e-05)\n",
      " state (7)  A[0]:(0.549313545227) A[1]:(0.450685679913) A[2]:(3.31519061092e-07) A[3]:(4.76316472486e-07)\n",
      " state (8)  A[0]:(0.0816422104836) A[1]:(0.918357729912) A[2]:(6.63935750822e-08) A[3]:(1.64801736702e-08)\n",
      " state (9)  A[0]:(0.00346991885453) A[1]:(0.996530056) A[2]:(5.03366504034e-09) A[3]:(2.75436062847e-10)\n",
      " state (10)  A[0]:(0.000164596451214) A[1]:(0.999835431576) A[2]:(4.45499637181e-10) A[3]:(5.959345864e-12)\n",
      " state (11)  A[0]:(2.91953165288e-05) A[1]:(0.999970793724) A[2]:(1.15242967946e-10) A[3]:(6.73139603953e-13)\n",
      " state (12)  A[0]:(1.20790009532e-05) A[1]:(0.999987900257) A[2]:(5.81332239291e-11) A[3]:(2.20376071992e-13)\n",
      " state (13)  A[0]:(7.46356818127e-06) A[1]:(0.999992549419) A[2]:(4.00800087563e-11) A[3]:(1.19742472773e-13)\n",
      " state (14)  A[0]:(5.64375523027e-06) A[1]:(0.999994337559) A[2]:(3.23100296518e-11) A[3]:(8.40368492025e-14)\n",
      " state (15)  A[0]:(4.77100093121e-06) A[1]:(0.999995231628) A[2]:(2.83870357776e-11) A[3]:(6.79383239661e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 217000 finished after 11 . Running score: 0.13. Policy_loss: -92050.6119743, Value_loss: 1.40467037525. Times trained:               13161. Times reached goal: 119.               Steps done: 2315407.\n",
      " state (0)  A[0]:(0.967908799648) A[1]:(0.0135095193982) A[2]:(0.00973468553275) A[3]:(0.0088470056653)\n",
      " state (1)  A[0]:(0.00250462139957) A[1]:(0.00550219928846) A[2]:(0.00183905451559) A[3]:(0.990154147148)\n",
      " state (2)  A[0]:(0.000465369492304) A[1]:(0.00333813228644) A[2]:(0.000689977954607) A[3]:(0.99550652504)\n",
      " state (3)  A[0]:(0.000419700780185) A[1]:(0.00372848636471) A[2]:(0.000458751397673) A[3]:(0.995393037796)\n",
      " state (4)  A[0]:(0.997398436069) A[1]:(0.00247981841676) A[2]:(2.15062777897e-06) A[3]:(0.000119568459922)\n",
      " state (5)  A[0]:(0.994991481304) A[1]:(0.00493715237826) A[2]:(1.0302977671e-06) A[3]:(7.03547484591e-05)\n",
      " state (6)  A[0]:(0.908874928951) A[1]:(0.0911143049598) A[2]:(6.70653889756e-07) A[3]:(1.00645247585e-05)\n",
      " state (7)  A[0]:(0.231695219874) A[1]:(0.768304526806) A[2]:(1.72084298811e-07) A[3]:(1.10767842898e-07)\n",
      " state (8)  A[0]:(0.0105131613091) A[1]:(0.989486813545) A[2]:(1.35209381469e-08) A[3]:(1.38398281724e-09)\n",
      " state (9)  A[0]:(0.000177421199623) A[1]:(0.999822556973) A[2]:(5.23473320246e-10) A[3]:(8.49673716802e-12)\n",
      " state (10)  A[0]:(1.32808818307e-05) A[1]:(0.999986708164) A[2]:(6.94174787097e-11) A[3]:(3.40288561218e-13)\n",
      " state (11)  A[0]:(3.97937628804e-06) A[1]:(0.999996006489) A[2]:(2.73937678097e-11) A[3]:(7.58417783328e-14)\n",
      " state (12)  A[0]:(2.20866127165e-06) A[1]:(0.999997794628) A[2]:(1.74292160565e-11) A[3]:(3.63673528141e-14)\n",
      " state (13)  A[0]:(1.61369439411e-06) A[1]:(0.999998390675) A[2]:(1.37025018068e-11) A[3]:(2.45704877885e-14)\n",
      " state (14)  A[0]:(1.35320306072e-06) A[1]:(0.999998629093) A[2]:(1.19742176244e-11) A[3]:(1.97197621866e-14)\n",
      " state (15)  A[0]:(1.22292533433e-06) A[1]:(0.999998748302) A[2]:(1.10812730519e-11) A[3]:(1.73783835494e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 218000 finished after 9 . Running score: 0.08. Policy_loss: -92050.6384962, Value_loss: 1.21425073904. Times trained:               13453. Times reached goal: 117.               Steps done: 2328860.\n",
      " state (0)  A[0]:(0.971121132374) A[1]:(0.0125237740576) A[2]:(0.00916754454374) A[3]:(0.00718756811693)\n",
      " state (1)  A[0]:(0.0119565138593) A[1]:(0.00987361557782) A[2]:(0.00458042602986) A[3]:(0.973589420319)\n",
      " state (2)  A[0]:(0.00135291635524) A[1]:(0.00533691095188) A[2]:(0.00137912132777) A[3]:(0.991931080818)\n",
      " state (3)  A[0]:(0.000458978378447) A[1]:(0.00455678533763) A[2]:(0.000568353687413) A[3]:(0.99441587925)\n",
      " state (4)  A[0]:(0.99515402317) A[1]:(0.00473578181118) A[2]:(2.07208586289e-06) A[3]:(0.000108132182504)\n",
      " state (5)  A[0]:(0.920002102852) A[1]:(0.0799866616726) A[2]:(8.43569182507e-07) A[3]:(1.0381329048e-05)\n",
      " state (6)  A[0]:(0.214042320848) A[1]:(0.78595739603) A[2]:(1.93360591538e-07) A[3]:(7.62986047675e-08)\n",
      " state (7)  A[0]:(0.0252163317055) A[1]:(0.974783658981) A[2]:(3.36598837691e-08) A[3]:(3.49725626236e-09)\n",
      " state (8)  A[0]:(0.00106447469443) A[1]:(0.998935520649) A[2]:(2.666794785e-09) A[3]:(6.57189580533e-11)\n",
      " state (9)  A[0]:(2.89132021862e-05) A[1]:(0.999971091747) A[2]:(1.57441004678e-10) A[3]:(7.44572209877e-13)\n",
      " state (10)  A[0]:(4.00248654842e-06) A[1]:(0.999996006489) A[2]:(3.40832535806e-11) A[3]:(6.33064496549e-14)\n",
      " state (11)  A[0]:(1.86711474726e-06) A[1]:(0.999998152256) A[2]:(1.8968779672e-11) A[3]:(2.4365139897e-14)\n",
      " state (12)  A[0]:(1.37727090532e-06) A[1]:(0.999998629093) A[2]:(1.50274861999e-11) A[3]:(1.66182206071e-14)\n",
      " state (13)  A[0]:(1.20068739307e-06) A[1]:(0.999998807907) A[2]:(1.35327200834e-11) A[3]:(1.39777817413e-14)\n",
      " state (14)  A[0]:(1.12171119326e-06) A[1]:(0.999998867512) A[2]:(1.2848448e-11) A[3]:(1.28270281902e-14)\n",
      " state (15)  A[0]:(1.08237168206e-06) A[1]:(0.999998927116) A[2]:(1.25034826243e-11) A[3]:(1.22615998165e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 219000 finished after 7 . Running score: 0.08. Policy_loss: -92050.6148788, Value_loss: 0.986224722526. Times trained:               12756. Times reached goal: 113.               Steps done: 2341616.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9716e-01,  2.7729e-03,  1.2790e-06,  6.3281e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9716e-01,  2.7707e-03,  1.2785e-06,  6.3258e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9716e-01,  2.7706e-03,  1.2785e-06,  6.3257e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9716e-01,  2.7705e-03,  1.2784e-06,  6.3257e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9710,  0.0123,  0.0092,  0.0075]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9716e-01,  2.7705e-03,  1.2784e-06,  6.3258e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9717e-01,  2.7705e-03,  1.2784e-06,  6.3258e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7387e-04,  9.9983e-01,  4.8219e-10,  7.6020e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7392e-04,  9.9983e-01,  4.8226e-10,  7.6045e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7397e-04,  9.9983e-01,  4.8233e-10,  7.6068e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7401e-04,  9.9983e-01,  4.8239e-10,  7.6089e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7405e-04,  9.9983e-01,  4.8245e-10,  7.6109e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.1453e-06,  9.9999e-01,  3.1210e-11,  1.0187e-13]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.5689e-07,  1.0000e+00,  4.8769e-12,  5.1279e-15]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.2744e-07,  1.0000e+00,  4.6355e-12,  4.7208e-15]])\n",
      "On state=14, selected action=1\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.971031785011) A[1]:(0.0122643541545) A[2]:(0.00916132237762) A[3]:(0.0075425519608)\n",
      " state (1)  A[0]:(0.00815913081169) A[1]:(0.00795297976583) A[2]:(0.0034222882241) A[3]:(0.980465590954)\n",
      " state (2)  A[0]:(0.000971282308456) A[1]:(0.00437819678336) A[2]:(0.00107321084943) A[3]:(0.993577301502)\n",
      " state (3)  A[0]:(0.000568791758269) A[1]:(0.00464861467481) A[2]:(0.000563935027458) A[3]:(0.99421864748)\n",
      " state (4)  A[0]:(0.997167170048) A[1]:(0.00276833330281) A[2]:(1.27746659473e-06) A[3]:(6.32152514299e-05)\n",
      " state (5)  A[0]:(0.969268500805) A[1]:(0.0307154152542) A[2]:(7.63309913054e-07) A[3]:(1.53265500558e-05)\n",
      " state (6)  A[0]:(0.29287725687) A[1]:(0.707122445107) A[2]:(1.97821776737e-07) A[3]:(1.14376319971e-07)\n",
      " state (7)  A[0]:(0.015700282529) A[1]:(0.984299719334) A[2]:(1.74462844171e-08) A[3]:(1.89375359838e-09)\n",
      " state (8)  A[0]:(0.000174017710378) A[1]:(0.999825954437) A[2]:(4.82298534443e-10) A[3]:(7.6089308465e-12)\n",
      " state (9)  A[0]:(5.14411340191e-06) A[1]:(0.999994874001) A[2]:(3.12015586978e-11) A[3]:(1.01846340335e-13)\n",
      " state (10)  A[0]:(1.19414630717e-06) A[1]:(0.999998807907) A[2]:(1.01562846674e-11) A[3]:(1.68528995555e-14)\n",
      " state (11)  A[0]:(6.71833959132e-07) A[1]:(0.999999344349) A[2]:(6.54271385334e-12) A[3]:(8.27294623188e-15)\n",
      " state (12)  A[0]:(5.19046182035e-07) A[1]:(0.999999463558) A[2]:(5.37413559834e-12) A[3]:(6.00755449686e-15)\n",
      " state (13)  A[0]:(4.56820771433e-07) A[1]:(0.999999523163) A[2]:(4.87604531405e-12) A[3]:(5.12698200951e-15)\n",
      " state (14)  A[0]:(4.2739944206e-07) A[1]:(0.999999582767) A[2]:(4.63505796244e-12) A[3]:(4.72035156121e-15)\n",
      " state (15)  A[0]:(4.12446667042e-07) A[1]:(0.999999582767) A[2]:(4.51110199542e-12) A[3]:(4.51637247498e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 220000 finished after 31 . Running score: 0.09. Policy_loss: -92050.6111728, Value_loss: 1.20467562981. Times trained:               12880. Times reached goal: 120.               Steps done: 2354496.\n",
      " state (0)  A[0]:(0.976333498955) A[1]:(0.00983856711537) A[2]:(0.00796916335821) A[3]:(0.00585875473917)\n",
      " state (1)  A[0]:(0.0319433622062) A[1]:(0.0130055695772) A[2]:(0.00816349405795) A[3]:(0.946887552738)\n",
      " state (2)  A[0]:(0.00387099990621) A[1]:(0.00746926106513) A[2]:(0.00284878117964) A[3]:(0.985810935497)\n",
      " state (3)  A[0]:(0.00309656979516) A[1]:(0.00951196439564) A[2]:(0.00126161065418) A[3]:(0.986129879951)\n",
      " state (4)  A[0]:(0.99814003706) A[1]:(0.00181909883395) A[2]:(9.36679782626e-07) A[3]:(3.99125565309e-05)\n",
      " state (5)  A[0]:(0.982379436493) A[1]:(0.017598291859) A[2]:(6.34772732155e-07) A[3]:(2.16335010919e-05)\n",
      " state (6)  A[0]:(0.459331601858) A[1]:(0.540667831898) A[2]:(2.1516001425e-07) A[3]:(2.94654512345e-07)\n",
      " state (7)  A[0]:(0.0566358081996) A[1]:(0.943364143372) A[2]:(3.5641726015e-08) A[3]:(1.06226618612e-08)\n",
      " state (8)  A[0]:(0.00167515035719) A[1]:(0.998324871063) A[2]:(1.94353422245e-09) A[3]:(1.3564710466e-10)\n",
      " state (9)  A[0]:(3.19832761306e-05) A[1]:(0.999967992306) A[2]:(7.96881102771e-11) A[3]:(1.08691094319e-12)\n",
      " state (10)  A[0]:(3.84719078284e-06) A[1]:(0.999996125698) A[2]:(1.48027978764e-11) A[3]:(8.1149381648e-14)\n",
      " state (11)  A[0]:(1.52688301114e-06) A[1]:(0.999998450279) A[2]:(7.13812559408e-12) A[3]:(2.60201050566e-14)\n",
      " state (12)  A[0]:(9.81912990028e-07) A[1]:(0.999999046326) A[2]:(5.04382473288e-12) A[3]:(1.50900274806e-14)\n",
      " state (13)  A[0]:(7.79186336786e-07) A[1]:(0.99999922514) A[2]:(4.20608296456e-12) A[3]:(1.13402168583e-14)\n",
      " state (14)  A[0]:(6.86073178713e-07) A[1]:(0.999999284744) A[2]:(3.80620491722e-12) A[3]:(9.69018651263e-15)\n",
      " state (15)  A[0]:(6.38605115455e-07) A[1]:(0.999999344349) A[2]:(3.59795570073e-12) A[3]:(8.86894148345e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 221000 finished after 15 . Running score: 0.12. Policy_loss: -92050.615537, Value_loss: 0.991884839449. Times trained:               13107. Times reached goal: 113.               Steps done: 2367603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.97478979826) A[1]:(0.010756932199) A[2]:(0.00804700609297) A[3]:(0.00640624808148)\n",
      " state (1)  A[0]:(0.0259136948735) A[1]:(0.0120328795165) A[2]:(0.00691160140559) A[3]:(0.955141842365)\n",
      " state (2)  A[0]:(0.00359336985275) A[1]:(0.00693560903892) A[2]:(0.00236090738326) A[3]:(0.987110137939)\n",
      " state (3)  A[0]:(0.00106282683555) A[1]:(0.00578621262684) A[2]:(0.00084069580771) A[3]:(0.992310285568)\n",
      " state (4)  A[0]:(0.99752497673) A[1]:(0.00240901741199) A[2]:(1.08999631721e-06) A[3]:(6.49011417408e-05)\n",
      " state (5)  A[0]:(0.981590032578) A[1]:(0.0183628406376) A[2]:(7.05060642758e-07) A[3]:(4.63972792204e-05)\n",
      " state (6)  A[0]:(0.449090391397) A[1]:(0.550908863544) A[2]:(2.35691885564e-07) A[3]:(4.91500998123e-07)\n",
      " state (7)  A[0]:(0.0341193377972) A[1]:(0.965880632401) A[2]:(2.54381440357e-08) A[3]:(6.49426867838e-09)\n",
      " state (8)  A[0]:(0.000520528177731) A[1]:(0.999479472637) A[2]:(8.29422308612e-10) A[3]:(3.69533743882e-11)\n",
      " state (9)  A[0]:(1.07864389065e-05) A[1]:(0.999989211559) A[2]:(3.76534255164e-11) A[3]:(3.2987141122e-13)\n",
      " state (10)  A[0]:(1.8391830281e-06) A[1]:(0.999998152256) A[2]:(9.3623104061e-12) A[3]:(3.78650120969e-14)\n",
      " state (11)  A[0]:(9.03274724351e-07) A[1]:(0.99999910593) A[2]:(5.37048574015e-12) A[3]:(1.57882045518e-14)\n",
      " state (12)  A[0]:(6.55230280699e-07) A[1]:(0.999999344349) A[2]:(4.18240398911e-12) A[3]:(1.06204446821e-14)\n",
      " state (13)  A[0]:(5.58097724479e-07) A[1]:(0.999999463558) A[2]:(3.69203582637e-12) A[3]:(8.7073606314e-15)\n",
      " state (14)  A[0]:(5.12735482516e-07) A[1]:(0.999999463558) A[2]:(3.4568816993e-12) A[3]:(7.83882659982e-15)\n",
      " state (15)  A[0]:(4.8966455779e-07) A[1]:(0.999999523163) A[2]:(3.33561065054e-12) A[3]:(7.40363970624e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 222000 finished after 8 . Running score: 0.09. Policy_loss: -92050.6153013, Value_loss: 0.993819658528. Times trained:               13437. Times reached goal: 128.               Steps done: 2381040.\n",
      " state (0)  A[0]:(0.979361593723) A[1]:(0.00842075236142) A[2]:(0.00709614623338) A[3]:(0.0051214969717)\n",
      " state (1)  A[0]:(0.0164138823748) A[1]:(0.0107769463211) A[2]:(0.00632337713614) A[3]:(0.966485798359)\n",
      " state (2)  A[0]:(0.00214177696034) A[1]:(0.00590955093503) A[2]:(0.00201840722002) A[3]:(0.989930272102)\n",
      " state (3)  A[0]:(0.00160543131642) A[1]:(0.00688085146248) A[2]:(0.000913914467674) A[3]:(0.990599811077)\n",
      " state (4)  A[0]:(0.998377740383) A[1]:(0.00158830045257) A[2]:(4.99438499446e-07) A[3]:(3.34842552547e-05)\n",
      " state (5)  A[0]:(0.991135179996) A[1]:(0.00884314533323) A[2]:(3.58775366749e-07) A[3]:(2.12984377868e-05)\n",
      " state (6)  A[0]:(0.645705223083) A[1]:(0.354294210672) A[2]:(1.43676587072e-07) A[3]:(4.43170819153e-07)\n",
      " state (7)  A[0]:(0.0918730571866) A[1]:(0.908126890659) A[2]:(2.33085355461e-08) A[3]:(1.32168445077e-08)\n",
      " state (8)  A[0]:(0.00470349239185) A[1]:(0.995296478271) A[2]:(1.79433823355e-09) A[3]:(3.20971610401e-10)\n",
      " state (9)  A[0]:(8.73473545653e-05) A[1]:(0.999912679195) A[2]:(6.3087202129e-11) A[3]:(2.43615023987e-12)\n",
      " state (10)  A[0]:(7.28813165551e-06) A[1]:(0.999992728233) A[2]:(8.12856639637e-12) A[3]:(1.14492074675e-13)\n",
      " state (11)  A[0]:(2.5450949579e-06) A[1]:(0.999997437) A[2]:(3.43701304396e-12) A[3]:(3.11309680291e-14)\n",
      " state (12)  A[0]:(1.61050274983e-06) A[1]:(0.999998390675) A[2]:(2.36732573648e-12) A[3]:(1.76269826372e-14)\n",
      " state (13)  A[0]:(1.29075522182e-06) A[1]:(0.999998688698) A[2]:(1.97753033566e-12) A[3]:(1.33788388151e-14)\n",
      " state (14)  A[0]:(1.15028149139e-06) A[1]:(0.999998867512) A[2]:(1.80090903128e-12) A[3]:(1.15877241206e-14)\n",
      " state (15)  A[0]:(1.08071526483e-06) A[1]:(0.999998927116) A[2]:(1.71200445313e-12) A[3]:(1.07198770328e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 223000 finished after 29 . Running score: 0.07. Policy_loss: -92050.6157503, Value_loss: 1.4157331256. Times trained:               12708. Times reached goal: 128.               Steps done: 2393748.\n",
      " state (0)  A[0]:(0.979531049728) A[1]:(0.00830885209143) A[2]:(0.00706394808367) A[3]:(0.00509614404291)\n",
      " state (1)  A[0]:(0.0162773486227) A[1]:(0.00926804635674) A[2]:(0.00524154724553) A[3]:(0.969213068485)\n",
      " state (2)  A[0]:(0.00181740242988) A[1]:(0.00497153261676) A[2]:(0.00154607486911) A[3]:(0.991665005684)\n",
      " state (3)  A[0]:(0.0953241512179) A[1]:(0.0515245385468) A[2]:(0.00150448794011) A[3]:(0.851646840572)\n",
      " state (4)  A[0]:(0.997511804104) A[1]:(0.00245839101262) A[2]:(3.94597805098e-07) A[3]:(2.93827233691e-05)\n",
      " state (5)  A[0]:(0.929297029972) A[1]:(0.0706981867552) A[2]:(2.77147591987e-07) A[3]:(4.49987373941e-06)\n",
      " state (6)  A[0]:(0.119962871075) A[1]:(0.880037069321) A[2]:(3.36950698454e-08) A[3]:(1.99619929475e-08)\n",
      " state (7)  A[0]:(0.00242631277069) A[1]:(0.997573673725) A[2]:(1.22812915393e-09) A[3]:(1.41810937992e-10)\n",
      " state (8)  A[0]:(1.23106829051e-05) A[1]:(0.999987661839) A[2]:(1.64736523034e-11) A[3]:(2.31429214985e-13)\n",
      " state (9)  A[0]:(6.56292797885e-07) A[1]:(0.999999344349) A[2]:(1.57529393176e-12) A[3]:(6.47172346976e-15)\n",
      " state (10)  A[0]:(2.34916100794e-07) A[1]:(0.999999761581) A[2]:(6.96199933641e-13) A[3]:(1.83742262094e-15)\n",
      " state (11)  A[0]:(1.59208141781e-07) A[1]:(0.999999821186) A[2]:(5.11595052346e-13) A[3]:(1.13871647182e-15)\n",
      " state (12)  A[0]:(1.34237112093e-07) A[1]:(0.999999880791) A[2]:(4.47048349944e-13) A[3]:(9.22762457299e-16)\n",
      " state (13)  A[0]:(1.23684372966e-07) A[1]:(0.999999880791) A[2]:(4.19054087221e-13) A[3]:(8.34107647996e-16)\n",
      " state (14)  A[0]:(1.18706758201e-07) A[1]:(0.999999880791) A[2]:(4.05682648932e-13) A[3]:(7.92886736678e-16)\n",
      " state (15)  A[0]:(1.16235149505e-07) A[1]:(0.999999880791) A[2]:(3.98999491216e-13) A[3]:(7.72571974927e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 224000 finished after 17 . Running score: 0.1. Policy_loss: -92050.6448968, Value_loss: 1.20729127358. Times trained:               13164. Times reached goal: 117.               Steps done: 2406912.\n",
      "action_dist \n",
      "tensor([[ 0.9808,  0.0075,  0.0068,  0.0049]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9808,  0.0075,  0.0068,  0.0049]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9808,  0.0075,  0.0068,  0.0049]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9906e-01,  9.1618e-04,  5.1929e-07,  2.6343e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9906e-01,  9.1624e-04,  5.1926e-07,  2.6343e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9808,  0.0075,  0.0068,  0.0049]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9808,  0.0075,  0.0068,  0.0049]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9906e-01,  9.1639e-04,  5.1919e-07,  2.6343e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9906e-01,  9.1643e-04,  5.1918e-07,  2.6344e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9808,  0.0075,  0.0068,  0.0049]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9808,  0.0075,  0.0068,  0.0049]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9906e-01,  9.1653e-04,  5.1913e-07,  2.6344e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9808,  0.0075,  0.0068,  0.0049]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9808,  0.0075,  0.0068,  0.0049]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9808,  0.0075,  0.0068,  0.0049]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9808,  0.0075,  0.0068,  0.0049]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9906e-01,  9.1665e-04,  5.1909e-07,  2.6344e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.9702e-03,  9.9803e-01,  8.7766e-10,  1.3451e-10]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.9693e-03,  9.9803e-01,  8.7732e-10,  1.3443e-10]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.9686e-03,  9.9803e-01,  8.7711e-10,  1.3438e-10]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.9682e-03,  9.9803e-01,  8.7699e-10,  1.3435e-10]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.980759382248) A[1]:(0.00746199907735) A[2]:(0.00682877935469) A[3]:(0.00494983512908)\n",
      " state (1)  A[0]:(0.0174784008414) A[1]:(0.0079688038677) A[2]:(0.00468273134902) A[3]:(0.969870090485)\n",
      " state (2)  A[0]:(0.00294699613005) A[1]:(0.00470294896513) A[2]:(0.00176850368734) A[3]:(0.990581572056)\n",
      " state (3)  A[0]:(0.00157067831606) A[1]:(0.00524307228625) A[2]:(0.00101070525125) A[3]:(0.992175519466)\n",
      " state (4)  A[0]:(0.999056816101) A[1]:(0.000916349817999) A[2]:(5.19162597357e-07) A[3]:(2.63419642579e-05)\n",
      " state (5)  A[0]:(0.998224675655) A[1]:(0.00174904987216) A[2]:(3.71042631286e-07) A[3]:(2.59245261987e-05)\n",
      " state (6)  A[0]:(0.969900608063) A[1]:(0.030091073364) A[2]:(2.8137424124e-07) A[3]:(8.03455986897e-06)\n",
      " state (7)  A[0]:(0.176584497094) A[1]:(0.823415398598) A[2]:(4.32321769495e-08) A[3]:(4.48142927212e-08)\n",
      " state (8)  A[0]:(0.00199037534185) A[1]:(0.998009622097) A[2]:(8.85155393426e-10) A[3]:(1.36125666295e-10)\n",
      " state (9)  A[0]:(3.07265909214e-05) A[1]:(0.999969244003) A[2]:(2.79954531779e-11) A[3]:(8.47171399872e-13)\n",
      " state (10)  A[0]:(2.85371584141e-06) A[1]:(0.999997138977) A[2]:(4.06202849135e-12) A[3]:(4.61040643054e-14)\n",
      " state (11)  A[0]:(7.79869537837e-07) A[1]:(0.99999922514) A[2]:(1.42698374818e-12) A[3]:(9.36743223138e-15)\n",
      " state (12)  A[0]:(3.70273028238e-07) A[1]:(0.999999642372) A[2]:(7.8388955483e-13) A[3]:(3.74995080628e-15)\n",
      " state (13)  A[0]:(2.38180575707e-07) A[1]:(0.999999761581) A[2]:(5.49942795296e-13) A[3]:(2.18074540325e-15)\n",
      " state (14)  A[0]:(1.82644299684e-07) A[1]:(0.999999821186) A[2]:(4.44350095998e-13) A[3]:(1.57404291407e-15)\n",
      " state (15)  A[0]:(1.55462544171e-07) A[1]:(0.999999821186) A[2]:(3.90422557565e-13) A[3]:(1.29157087281e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 225000 finished after 21 . Running score: 0.11. Policy_loss: -92050.6174115, Value_loss: 0.993580137187. Times trained:               12564. Times reached goal: 131.               Steps done: 2419476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.978278279305) A[1]:(0.00907225813717) A[2]:(0.0070585613139) A[3]:(0.00559091661125)\n",
      " state (1)  A[0]:(0.0264111757278) A[1]:(0.00983367394656) A[2]:(0.0059536844492) A[3]:(0.95780146122)\n",
      " state (2)  A[0]:(0.00522661814466) A[1]:(0.00590892648324) A[2]:(0.00222854921594) A[3]:(0.986635923386)\n",
      " state (3)  A[0]:(0.00558405416086) A[1]:(0.010348437354) A[2]:(0.00129408948123) A[3]:(0.982773423195)\n",
      " state (4)  A[0]:(0.998682975769) A[1]:(0.00129390496295) A[2]:(4.34779877878e-07) A[3]:(2.27063210332e-05)\n",
      " state (5)  A[0]:(0.991510570049) A[1]:(0.0084700640291) A[2]:(3.74770820599e-07) A[3]:(1.90207938431e-05)\n",
      " state (6)  A[0]:(0.643016278744) A[1]:(0.356983035803) A[2]:(1.76900641691e-07) A[3]:(4.94233574955e-07)\n",
      " state (7)  A[0]:(0.0984126254916) A[1]:(0.901587307453) A[2]:(3.38547039291e-08) A[3]:(1.55079451503e-08)\n",
      " state (8)  A[0]:(0.00746398838237) A[1]:(0.992536008358) A[2]:(3.8138585623e-09) A[3]:(6.35421271156e-10)\n",
      " state (9)  A[0]:(0.000383626669645) A[1]:(0.999616384506) A[2]:(3.32023436522e-10) A[3]:(1.68959724683e-11)\n",
      " state (10)  A[0]:(5.12314072694e-05) A[1]:(0.999948740005) A[2]:(6.58363294437e-11) A[3]:(1.3913419028e-12)\n",
      " state (11)  A[0]:(1.44943105624e-05) A[1]:(0.999985516071) A[2]:(2.41336586132e-11) A[3]:(2.87107197825e-13)\n",
      " state (12)  A[0]:(6.38623669147e-06) A[1]:(0.999993622303) A[2]:(1.26212052962e-11) A[3]:(1.02749399429e-13)\n",
      " state (13)  A[0]:(3.72669228454e-06) A[1]:(0.999996244907) A[2]:(8.25131196008e-12) A[3]:(5.22653277536e-14)\n",
      " state (14)  A[0]:(2.61698482973e-06) A[1]:(0.999997377396) A[2]:(6.24493512458e-12) A[3]:(3.3534681515e-14)\n",
      " state (15)  A[0]:(2.07695779864e-06) A[1]:(0.999997913837) A[2]:(5.20558466124e-12) A[3]:(2.50890379706e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 226000 finished after 7 . Running score: 0.13. Policy_loss: -92050.6106823, Value_loss: 1.20954837877. Times trained:               13144. Times reached goal: 139.               Steps done: 2432620.\n",
      " state (0)  A[0]:(0.975164890289) A[1]:(0.0115976491943) A[2]:(0.00672813365236) A[3]:(0.00650930032134)\n",
      " state (1)  A[0]:(0.0235989335924) A[1]:(0.0113306958228) A[2]:(0.00567344995216) A[3]:(0.959396898746)\n",
      " state (2)  A[0]:(0.00364746898413) A[1]:(0.00675989827141) A[2]:(0.00186481920537) A[3]:(0.987727820873)\n",
      " state (3)  A[0]:(0.0268660485744) A[1]:(0.0339208878577) A[2]:(0.00136519037187) A[3]:(0.937847852707)\n",
      " state (4)  A[0]:(0.996354877949) A[1]:(0.00361630599946) A[2]:(4.70318411772e-07) A[3]:(2.83261106233e-05)\n",
      " state (5)  A[0]:(0.952065527439) A[1]:(0.047925312072) A[2]:(3.52493401579e-07) A[3]:(8.79524122865e-06)\n",
      " state (6)  A[0]:(0.378110677004) A[1]:(0.62188911438) A[2]:(1.16146544826e-07) A[3]:(9.56753041237e-08)\n",
      " state (7)  A[0]:(0.0902348309755) A[1]:(0.909765124321) A[2]:(3.53599212133e-08) A[3]:(7.05986824556e-09)\n",
      " state (8)  A[0]:(0.00913532264531) A[1]:(0.990864694118) A[2]:(5.38677413786e-09) A[3]:(3.58056251315e-10)\n",
      " state (9)  A[0]:(0.000357231241651) A[1]:(0.999642789364) A[2]:(3.92882171241e-10) A[3]:(6.13671786998e-12)\n",
      " state (10)  A[0]:(3.58974648407e-05) A[1]:(0.999964118004) A[2]:(6.36297958767e-11) A[3]:(3.3720546971e-13)\n",
      " state (11)  A[0]:(9.24606865738e-06) A[1]:(0.99999076128) A[2]:(2.19503512588e-11) A[3]:(6.02368361354e-14)\n",
      " state (12)  A[0]:(4.03252806791e-06) A[1]:(0.999995946884) A[2]:(1.14783581945e-11) A[3]:(2.0952920185e-14)\n",
      " state (13)  A[0]:(2.39057453655e-06) A[1]:(0.999997615814) A[2]:(7.63466199982e-12) A[3]:(1.07674464031e-14)\n",
      " state (14)  A[0]:(1.71410295025e-06) A[1]:(0.999998271465) A[2]:(5.89137159196e-12) A[3]:(7.04967903714e-15)\n",
      " state (15)  A[0]:(1.38655548199e-06) A[1]:(0.999998629093) A[2]:(4.9943105207e-12) A[3]:(5.38166237038e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 227000 finished after 9 . Running score: 0.15. Policy_loss: -92050.613647, Value_loss: 1.01290734647. Times trained:               13775. Times reached goal: 134.               Steps done: 2446395.\n",
      " state (0)  A[0]:(0.976974546909) A[1]:(0.0104117328301) A[2]:(0.00666012568399) A[3]:(0.00595361553133)\n",
      " state (1)  A[0]:(0.0234677251428) A[1]:(0.0108841741458) A[2]:(0.00584692927077) A[3]:(0.959801197052)\n",
      " state (2)  A[0]:(0.00275753927417) A[1]:(0.00610182667151) A[2]:(0.00163587869611) A[3]:(0.989504754543)\n",
      " state (3)  A[0]:(0.992862701416) A[1]:(0.00666666403413) A[2]:(6.6190364123e-06) A[3]:(0.000464030483272)\n",
      " state (4)  A[0]:(0.988570332527) A[1]:(0.011404985562) A[2]:(5.09184417297e-07) A[3]:(2.41795605689e-05)\n",
      " state (5)  A[0]:(0.473147422075) A[1]:(0.526852071285) A[2]:(1.91083017853e-07) A[3]:(2.61326533746e-07)\n",
      " state (6)  A[0]:(0.0329449921846) A[1]:(0.967054963112) A[2]:(1.78958217134e-08) A[3]:(2.32877561679e-09)\n",
      " state (7)  A[0]:(0.000320833409205) A[1]:(0.999679148197) A[2]:(4.10824707586e-10) A[3]:(7.59492468916e-12)\n",
      " state (8)  A[0]:(2.16601642933e-06) A[1]:(0.999997854233) A[2]:(7.84268483489e-12) A[3]:(1.61252406556e-14)\n",
      " state (9)  A[0]:(2.80332301372e-07) A[1]:(0.999999701977) A[2]:(1.58715022461e-12) A[3]:(1.28056177383e-15)\n",
      " state (10)  A[0]:(1.37046669124e-07) A[1]:(0.999999880791) A[2]:(9.10011649369e-13) A[3]:(5.25519569267e-16)\n",
      " state (11)  A[0]:(1.02065733643e-07) A[1]:(0.999999880791) A[2]:(7.24165465007e-13) A[3]:(3.63753005242e-16)\n",
      " state (12)  A[0]:(8.90015883215e-08) A[1]:(0.999999940395) A[2]:(6.51321769955e-13) A[3]:(3.06495430893e-16)\n",
      " state (13)  A[0]:(8.31870892171e-08) A[1]:(0.999999940395) A[2]:(6.18160524938e-13) A[3]:(2.8164467536e-16)\n",
      " state (14)  A[0]:(8.0386151069e-08) A[1]:(0.999999940395) A[2]:(6.02006617299e-13) A[3]:(2.69821710073e-16)\n",
      " state (15)  A[0]:(7.89841010373e-08) A[1]:(0.999999940395) A[2]:(5.93875101006e-13) A[3]:(2.63941554414e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 228000 finished after 8 . Running score: 0.16. Policy_loss: -92050.6132093, Value_loss: 0.995388173216. Times trained:               13280. Times reached goal: 114.               Steps done: 2459675.\n",
      " state (0)  A[0]:(0.979815900326) A[1]:(0.00869468506426) A[2]:(0.00635541602969) A[3]:(0.00513399252668)\n",
      " state (1)  A[0]:(0.040065780282) A[1]:(0.0121239051223) A[2]:(0.0080464174971) A[3]:(0.939763903618)\n",
      " state (2)  A[0]:(0.00957081560045) A[1]:(0.00780520122498) A[2]:(0.00336059322581) A[3]:(0.979263365269)\n",
      " state (3)  A[0]:(0.0904581621289) A[1]:(0.0398626103997) A[2]:(0.0027113603428) A[3]:(0.866967856884)\n",
      " state (4)  A[0]:(0.998478412628) A[1]:(0.00150813465007) A[2]:(4.16310598439e-07) A[3]:(1.30531807372e-05)\n",
      " state (5)  A[0]:(0.97814053297) A[1]:(0.0218490157276) A[2]:(4.13449271264e-07) A[3]:(1.00537763501e-05)\n",
      " state (6)  A[0]:(0.373275250196) A[1]:(0.626724541187) A[2]:(1.25721271615e-07) A[3]:(7.43677901482e-08)\n",
      " state (7)  A[0]:(0.038781054318) A[1]:(0.961218953133) A[2]:(1.81611241601e-08) A[3]:(1.89076376778e-09)\n",
      " state (8)  A[0]:(0.000666552048642) A[1]:(0.999333441257) A[2]:(6.68140154314e-10) A[3]:(1.14617655644e-11)\n",
      " state (9)  A[0]:(1.22085466501e-05) A[1]:(0.999987781048) A[2]:(2.76459098669e-11) A[3]:(7.7257820909e-14)\n",
      " state (10)  A[0]:(1.96175096789e-06) A[1]:(0.999998033047) A[2]:(6.55320893037e-12) A[3]:(7.77013222779e-15)\n",
      " state (11)  A[0]:(9.26195525608e-07) A[1]:(0.999999046326) A[2]:(3.64072747644e-12) A[3]:(3.01415679017e-15)\n",
      " state (12)  A[0]:(6.55851067677e-07) A[1]:(0.999999344349) A[2]:(2.78067196005e-12) A[3]:(1.94745283606e-15)\n",
      " state (13)  A[0]:(5.51132984583e-07) A[1]:(0.999999463558) A[2]:(2.42809938822e-12) A[3]:(1.56206713874e-15)\n",
      " state (14)  A[0]:(5.02634179611e-07) A[1]:(0.999999523163) A[2]:(2.26006279663e-12) A[3]:(1.38979260161e-15)\n",
      " state (15)  A[0]:(4.78121251035e-07) A[1]:(0.999999523163) A[2]:(2.17384595567e-12) A[3]:(1.30433481229e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 229000 finished after 9 . Running score: 0.12. Policy_loss: -92050.6121197, Value_loss: 1.21118385589. Times trained:               12806. Times reached goal: 119.               Steps done: 2472481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9824,  0.0072,  0.0061,  0.0043]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9824,  0.0072,  0.0061,  0.0043]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9765e-01,  2.3390e-03,  3.9638e-07,  1.3253e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9824,  0.0072,  0.0061,  0.0043]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9765e-01,  2.3375e-03,  3.9634e-07,  1.3248e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4572e-05,  9.9998e-01,  4.6178e-11,  1.6365e-13]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4587e-05,  9.9998e-01,  4.6201e-11,  1.6376e-13]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.982362985611) A[1]:(0.00718957511708) A[2]:(0.00611119391397) A[3]:(0.00433626538143)\n",
      " state (1)  A[0]:(0.047564547509) A[1]:(0.012738850899) A[2]:(0.00913309492171) A[3]:(0.930563509464)\n",
      " state (2)  A[0]:(0.0118707437068) A[1]:(0.00835913885385) A[2]:(0.00382974371314) A[3]:(0.975940346718)\n",
      " state (3)  A[0]:(0.214741080999) A[1]:(0.057798486203) A[2]:(0.00277153309435) A[3]:(0.724688947201)\n",
      " state (4)  A[0]:(0.997652947903) A[1]:(0.00233342521824) A[2]:(3.96225857457e-07) A[3]:(1.32352370201e-05)\n",
      " state (5)  A[0]:(0.87247556448) A[1]:(0.127522692084) A[2]:(3.02709082689e-07) A[3]:(1.46588070038e-06)\n",
      " state (6)  A[0]:(0.113381557167) A[1]:(0.886618375778) A[2]:(4.31063718054e-08) A[3]:(7.49436512848e-09)\n",
      " state (7)  A[0]:(0.00469305273145) A[1]:(0.995306968689) A[2]:(3.14556891645e-09) A[3]:(1.17821391532e-10)\n",
      " state (8)  A[0]:(2.46028139372e-05) A[1]:(0.999975383282) A[2]:(4.62235839882e-11) A[3]:(1.63860637841e-13)\n",
      " state (9)  A[0]:(9.93616254163e-07) A[1]:(0.999998986721) A[2]:(3.64258840105e-12) A[3]:(2.89301900849e-15)\n",
      " state (10)  A[0]:(3.03340812025e-07) A[1]:(0.999999701977) A[2]:(1.43261596162e-12) A[3]:(6.46301332409e-16)\n",
      " state (11)  A[0]:(1.89259722561e-07) A[1]:(0.999999821186) A[2]:(9.89980992148e-13) A[3]:(3.55340221071e-16)\n",
      " state (12)  A[0]:(1.52522673602e-07) A[1]:(0.999999821186) A[2]:(8.3637106572e-13) A[3]:(2.70077064037e-16)\n",
      " state (13)  A[0]:(1.37004988687e-07) A[1]:(0.999999880791) A[2]:(7.69241278748e-13) A[3]:(2.3558519421e-16)\n",
      " state (14)  A[0]:(1.29598348053e-07) A[1]:(0.999999880791) A[2]:(7.36674393362e-13) A[3]:(2.19468963021e-16)\n",
      " state (15)  A[0]:(1.25846284504e-07) A[1]:(0.999999880791) A[2]:(7.20048695148e-13) A[3]:(2.11390796568e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 230000 finished after 7 . Running score: 0.2. Policy_loss: -92050.6133354, Value_loss: 1.41181889858. Times trained:               12654. Times reached goal: 121.               Steps done: 2485135.\n",
      " state (0)  A[0]:(0.983799517155) A[1]:(0.00628993660212) A[2]:(0.00575957074761) A[3]:(0.00415098806843)\n",
      " state (1)  A[0]:(0.0372410081327) A[1]:(0.010323013179) A[2]:(0.00738212792203) A[3]:(0.945053875446)\n",
      " state (2)  A[0]:(0.013691008091) A[1]:(0.0072667747736) A[2]:(0.00383183313534) A[3]:(0.975210368633)\n",
      " state (3)  A[0]:(0.00372390495613) A[1]:(0.00556528195739) A[2]:(0.00191253237426) A[3]:(0.988798260689)\n",
      " state (4)  A[0]:(0.997639715672) A[1]:(0.0022008374799) A[2]:(4.22075027018e-06) A[3]:(0.00015525125491)\n",
      " state (5)  A[0]:(0.998246431351) A[1]:(0.00173826736864) A[2]:(4.22948176038e-07) A[3]:(1.48709250425e-05)\n",
      " state (6)  A[0]:(0.958864629269) A[1]:(0.0411304906011) A[2]:(3.53538609943e-07) A[3]:(4.49805020253e-06)\n",
      " state (7)  A[0]:(0.278627663851) A[1]:(0.721372187138) A[2]:(8.77205863503e-08) A[3]:(3.45918280686e-08)\n",
      " state (8)  A[0]:(0.0303617529571) A[1]:(0.969638228416) A[2]:(1.33166002669e-08) A[3]:(1.35288802383e-09)\n",
      " state (9)  A[0]:(0.000862897140905) A[1]:(0.999137103558) A[2]:(7.0952288489e-10) A[3]:(1.56222725728e-11)\n",
      " state (10)  A[0]:(4.09191925428e-05) A[1]:(0.999959051609) A[2]:(6.06972597184e-11) A[3]:(3.38586797488e-13)\n",
      " state (11)  A[0]:(7.1985186878e-06) A[1]:(0.999992787838) A[2]:(1.51794722625e-11) A[3]:(3.75213030676e-14)\n",
      " state (12)  A[0]:(2.71566204901e-06) A[1]:(0.999997258186) A[2]:(7.00080141428e-12) A[3]:(1.0879456346e-14)\n",
      " state (13)  A[0]:(1.53150597271e-06) A[1]:(0.999998450279) A[2]:(4.44650696471e-12) A[3]:(5.25322634107e-15)\n",
      " state (14)  A[0]:(1.08347785499e-06) A[1]:(0.999998927116) A[2]:(3.38063583204e-12) A[3]:(3.38383034395e-15)\n",
      " state (15)  A[0]:(8.76559909102e-07) A[1]:(0.99999910593) A[2]:(2.8584901373e-12) A[3]:(2.58507806293e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 231000 finished after 3 . Running score: 0.19. Policy_loss: -92050.6090106, Value_loss: 0.990766931141. Times trained:               13036. Times reached goal: 135.               Steps done: 2498171.\n",
      " state (0)  A[0]:(0.983876943588) A[1]:(0.00635808380321) A[2]:(0.00568284466863) A[3]:(0.00408212793991)\n",
      " state (1)  A[0]:(0.0370027683675) A[1]:(0.0108730867505) A[2]:(0.00768948392943) A[3]:(0.944434642792)\n",
      " state (2)  A[0]:(0.00948323495686) A[1]:(0.00691522378474) A[2]:(0.00313490140252) A[3]:(0.980466663837)\n",
      " state (3)  A[0]:(0.120949134231) A[1]:(0.0407996922731) A[2]:(0.00276985554956) A[3]:(0.835481286049)\n",
      " state (4)  A[0]:(0.998586237431) A[1]:(0.00140230264515) A[2]:(4.41181896349e-07) A[3]:(1.10299897642e-05)\n",
      " state (5)  A[0]:(0.976592838764) A[1]:(0.0234019029886) A[2]:(4.73252498523e-07) A[3]:(4.79978825751e-06)\n",
      " state (6)  A[0]:(0.33780246973) A[1]:(0.66219741106) A[2]:(1.31525411007e-07) A[3]:(3.4889371392e-08)\n",
      " state (7)  A[0]:(0.0315127372742) A[1]:(0.968487262726) A[2]:(1.70683644996e-08) A[3]:(1.08156494871e-09)\n",
      " state (8)  A[0]:(0.000350057613105) A[1]:(0.999649941921) A[2]:(4.5081960387e-10) A[3]:(3.96557829976e-12)\n",
      " state (9)  A[0]:(5.38154245078e-06) A[1]:(0.999994635582) A[2]:(1.64907740241e-11) A[3]:(2.16021404459e-14)\n",
      " state (10)  A[0]:(8.52690106967e-07) A[1]:(0.999999165535) A[2]:(3.87919557587e-12) A[3]:(2.13994382619e-15)\n",
      " state (11)  A[0]:(3.92660354009e-07) A[1]:(0.999999582767) A[2]:(2.11468581361e-12) A[3]:(8.05563220008e-16)\n",
      " state (12)  A[0]:(2.71000402563e-07) A[1]:(0.999999701977) A[2]:(1.58314117024e-12) A[3]:(5.04283606247e-16)\n",
      " state (13)  A[0]:(2.2328950422e-07) A[1]:(0.999999761581) A[2]:(1.3612771934e-12) A[3]:(3.94720582734e-16)\n",
      " state (14)  A[0]:(2.00874040956e-07) A[1]:(0.999999821186) A[2]:(1.25357471799e-12) A[3]:(3.45241523697e-16)\n",
      " state (15)  A[0]:(1.89344746104e-07) A[1]:(0.999999821186) A[2]:(1.19718503774e-12) A[3]:(3.20336875693e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 232000 finished after 9 . Running score: 0.1. Policy_loss: -92050.6120858, Value_loss: 0.993680856878. Times trained:               12928. Times reached goal: 135.               Steps done: 2511099.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.983109593391) A[1]:(0.00677252188325) A[2]:(0.0057707009837) A[3]:(0.00434720981866)\n",
      " state (1)  A[0]:(0.0328298993409) A[1]:(0.010382104665) A[2]:(0.00689669139683) A[3]:(0.949891328812)\n",
      " state (2)  A[0]:(0.00854657962918) A[1]:(0.00657510291785) A[2]:(0.00271546537988) A[3]:(0.982162833214)\n",
      " state (3)  A[0]:(0.023005021736) A[1]:(0.0186696648598) A[2]:(0.0018143003108) A[3]:(0.95651102066)\n",
      " state (4)  A[0]:(0.998112976551) A[1]:(0.00187093089335) A[2]:(4.54231582125e-07) A[3]:(1.56321657414e-05)\n",
      " state (5)  A[0]:(0.962305128574) A[1]:(0.0376878418028) A[2]:(4.99258192121e-07) A[3]:(6.50811807645e-06)\n",
      " state (6)  A[0]:(0.284078329802) A[1]:(0.715921521187) A[2]:(1.16755984436e-07) A[3]:(3.07366043728e-08)\n",
      " state (7)  A[0]:(0.0206664465368) A[1]:(0.979333519936) A[2]:(1.27974217889e-08) A[3]:(6.4803939992e-10)\n",
      " state (8)  A[0]:(0.000112130335765) A[1]:(0.999887883663) A[2]:(2.00882296908e-10) A[3]:(9.87714575927e-13)\n",
      " state (9)  A[0]:(1.87416139852e-06) A[1]:(0.999998152256) A[2]:(8.06067799314e-12) A[3]:(6.18256675036e-15)\n",
      " state (10)  A[0]:(3.98725575224e-07) A[1]:(0.999999582767) A[2]:(2.40443906105e-12) A[3]:(9.01303830493e-16)\n",
      " state (11)  A[0]:(2.24187800768e-07) A[1]:(0.999999761581) A[2]:(1.53549005108e-12) A[3]:(4.38982448211e-16)\n",
      " state (12)  A[0]:(1.7519093376e-07) A[1]:(0.999999821186) A[2]:(1.26790646125e-12) A[3]:(3.22203365732e-16)\n",
      " state (13)  A[0]:(1.55815058633e-07) A[1]:(0.999999821186) A[2]:(1.15787327643e-12) A[3]:(2.78039703137e-16)\n",
      " state (14)  A[0]:(1.46871116158e-07) A[1]:(0.999999880791) A[2]:(1.10616203697e-12) A[3]:(2.5807632771e-16)\n",
      " state (15)  A[0]:(1.42416737958e-07) A[1]:(0.999999880791) A[2]:(1.08018336029e-12) A[3]:(2.48233116649e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 233000 finished after 11 . Running score: 0.15. Policy_loss: -92050.6111049, Value_loss: 1.00000318978. Times trained:               12747. Times reached goal: 135.               Steps done: 2523846.\n",
      " state (0)  A[0]:(0.984554111958) A[1]:(0.00592675292864) A[2]:(0.00546982325613) A[3]:(0.00404928904027)\n",
      " state (1)  A[0]:(0.0282914061099) A[1]:(0.00893745012581) A[2]:(0.00598335731775) A[3]:(0.956787765026)\n",
      " state (2)  A[0]:(0.0103606162593) A[1]:(0.00615480123088) A[2]:(0.00296329474077) A[3]:(0.980521261692)\n",
      " state (3)  A[0]:(0.00354360439815) A[1]:(0.00516541814432) A[2]:(0.00155462778639) A[3]:(0.989736378193)\n",
      " state (4)  A[0]:(0.997521340847) A[1]:(0.00234761834145) A[2]:(3.3405844988e-06) A[3]:(0.000127699342556)\n",
      " state (5)  A[0]:(0.992571532726) A[1]:(0.00740727689117) A[2]:(5.51544189875e-07) A[3]:(2.062342719e-05)\n",
      " state (6)  A[0]:(0.480463057756) A[1]:(0.519536554813) A[2]:(2.05405498832e-07) A[3]:(1.50696948253e-07)\n",
      " state (7)  A[0]:(0.0419089086354) A[1]:(0.958091080189) A[2]:(2.35822650296e-08) A[3]:(1.81402082244e-09)\n",
      " state (8)  A[0]:(0.000629815622233) A[1]:(0.999370157719) A[2]:(7.50944029182e-10) A[3]:(9.67098102989e-12)\n",
      " state (9)  A[0]:(1.04239052234e-05) A[1]:(0.999989569187) A[2]:(2.78702842055e-11) A[3]:(5.90351877626e-14)\n",
      " state (10)  A[0]:(1.42575879636e-06) A[1]:(0.999998569489) A[2]:(5.74289270652e-12) A[3]:(4.86816219863e-15)\n",
      " state (11)  A[0]:(5.69202882161e-07) A[1]:(0.999999403954) A[2]:(2.7796484732e-12) A[3]:(1.5304171174e-15)\n",
      " state (12)  A[0]:(3.55867115331e-07) A[1]:(0.999999642372) A[2]:(1.91927636978e-12) A[3]:(8.45773462201e-16)\n",
      " state (13)  A[0]:(2.75417448847e-07) A[1]:(0.999999701977) A[2]:(1.56846486753e-12) A[3]:(6.11831167801e-16)\n",
      " state (14)  A[0]:(2.38437849021e-07) A[1]:(0.999999761581) A[2]:(1.40023051747e-12) A[3]:(5.09908910868e-16)\n",
      " state (15)  A[0]:(2.19594511464e-07) A[1]:(0.999999761581) A[2]:(1.3124776873e-12) A[3]:(4.59505950644e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 234000 finished after 21 . Running score: 0.14. Policy_loss: -92050.6109108, Value_loss: 0.998379435277. Times trained:               13307. Times reached goal: 129.               Steps done: 2537153.\n",
      "action_dist \n",
      "tensor([[ 0.9842,  0.0060,  0.0056,  0.0042]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9936e-01,  6.2734e-04,  6.8085e-07,  1.0971e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9936e-01,  6.2739e-04,  6.8019e-07,  1.0962e-05]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7404e-02,  9.8260e-01,  1.5130e-08,  5.1838e-10]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.9335e-04,  9.9971e-01,  5.8709e-10,  3.1420e-12]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7173e-02,  9.8283e-01,  1.4972e-08,  5.0994e-10]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.984151542187) A[1]:(0.0059916828759) A[2]:(0.00561763159931) A[3]:(0.00423914939165)\n",
      " state (1)  A[0]:(0.0267514009029) A[1]:(0.00865042675287) A[2]:(0.00588483409956) A[3]:(0.95871335268)\n",
      " state (2)  A[0]:(0.0108491322026) A[1]:(0.00616350956261) A[2]:(0.00310879060999) A[3]:(0.979878544807)\n",
      " state (3)  A[0]:(0.0105454465374) A[1]:(0.00825096014887) A[2]:(0.00222504232079) A[3]:(0.978978574276)\n",
      " state (4)  A[0]:(0.999360918999) A[1]:(0.000627463276032) A[2]:(6.78115384289e-07) A[3]:(1.09328575491e-05)\n",
      " state (5)  A[0]:(0.997914612293) A[1]:(0.00207076291554) A[2]:(5.55633334898e-07) A[3]:(1.40459869726e-05)\n",
      " state (6)  A[0]:(0.943069756031) A[1]:(0.0569244399667) A[2]:(5.88576767768e-07) A[3]:(5.24385677636e-06)\n",
      " state (7)  A[0]:(0.225682601333) A[1]:(0.774317264557) A[2]:(1.26993924709e-07) A[3]:(2.28281340497e-08)\n",
      " state (8)  A[0]:(0.0172159802169) A[1]:(0.982784032822) A[2]:(1.50013637068e-08) A[3]:(5.11500564127e-10)\n",
      " state (9)  A[0]:(0.000291624339297) A[1]:(0.999708354473) A[2]:(5.8442367612e-10) A[3]:(3.11959941386e-12)\n",
      " state (10)  A[0]:(1.32859358928e-05) A[1]:(0.999986708164) A[2]:(5.20698900663e-11) A[3]:(6.52924844182e-14)\n",
      " state (11)  A[0]:(2.69727206614e-06) A[1]:(0.999997317791) A[2]:(1.50966461554e-11) A[3]:(8.76282265472e-15)\n",
      " state (12)  A[0]:(1.15931197797e-06) A[1]:(0.999998867512) A[2]:(7.85616710575e-12) A[3]:(3.01369642776e-15)\n",
      " state (13)  A[0]:(7.16927331723e-07) A[1]:(0.999999284744) A[2]:(5.42111104271e-12) A[3]:(1.63993223061e-15)\n",
      " state (14)  A[0]:(5.39105826647e-07) A[1]:(0.999999463558) A[2]:(4.3517507306e-12) A[3]:(1.14269318563e-15)\n",
      " state (15)  A[0]:(4.5367616508e-07) A[1]:(0.999999523163) A[2]:(3.81062239055e-12) A[3]:(9.18102823177e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 235000 finished after 6 . Running score: 0.16. Policy_loss: -92050.6116333, Value_loss: 1.20577644879. Times trained:               13231. Times reached goal: 147.               Steps done: 2550384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.983711600304) A[1]:(0.00618910370395) A[2]:(0.00565715273842) A[3]:(0.00444215675816)\n",
      " state (1)  A[0]:(0.0191078446805) A[1]:(0.00774240680039) A[2]:(0.00457361759618) A[3]:(0.968576133251)\n",
      " state (2)  A[0]:(0.00674968352541) A[1]:(0.00539467390627) A[2]:(0.00217753648758) A[3]:(0.985678136349)\n",
      " state (3)  A[0]:(0.0381605215371) A[1]:(0.0198025256395) A[2]:(0.00202047894709) A[3]:(0.940016448498)\n",
      " state (4)  A[0]:(0.998170137405) A[1]:(0.00181355525274) A[2]:(4.50953848485e-07) A[3]:(1.58363272931e-05)\n",
      " state (5)  A[0]:(0.917636930943) A[1]:(0.0823540836573) A[2]:(5.86883800224e-07) A[3]:(8.39024323795e-06)\n",
      " state (6)  A[0]:(0.105914905667) A[1]:(0.894085049629) A[2]:(6.72044464523e-08) A[3]:(9.02695340699e-09)\n",
      " state (7)  A[0]:(0.00440748268738) A[1]:(0.995592534542) A[2]:(5.15095699427e-09) A[3]:(1.22790527746e-10)\n",
      " state (8)  A[0]:(2.08241381188e-05) A[1]:(0.999979197979) A[2]:(7.89291826342e-11) A[3]:(1.69365200033e-13)\n",
      " state (9)  A[0]:(7.09680591626e-07) A[1]:(0.999999284744) A[2]:(5.87530805257e-12) A[3]:(2.58068407952e-15)\n",
      " state (10)  A[0]:(2.018397538e-07) A[1]:(0.999999821186) A[2]:(2.25096026887e-12) A[3]:(5.38476685201e-16)\n",
      " state (11)  A[0]:(1.21907874018e-07) A[1]:(0.999999880791) A[2]:(1.53562492583e-12) A[3]:(2.86203274333e-16)\n",
      " state (12)  A[0]:(9.6421985063e-08) A[1]:(0.999999880791) A[2]:(1.28665535257e-12) A[3]:(2.12998372458e-16)\n",
      " state (13)  A[0]:(8.55645723163e-08) A[1]:(0.999999940395) A[2]:(1.17638765482e-12) A[3]:(1.83120833725e-16)\n",
      " state (14)  A[0]:(8.02680233392e-08) A[1]:(0.999999940395) A[2]:(1.12168434263e-12) A[3]:(1.68834656923e-16)\n",
      " state (15)  A[0]:(7.74909878487e-08) A[1]:(0.999999940395) A[2]:(1.0928620205e-12) A[3]:(1.61402114864e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 236000 finished after 9 . Running score: 0.09. Policy_loss: -92050.6594571, Value_loss: 1.0030650889. Times trained:               13253. Times reached goal: 116.               Steps done: 2563637.\n",
      " state (0)  A[0]:(0.984014987946) A[1]:(0.00599769130349) A[2]:(0.0056215734221) A[3]:(0.00436573429033)\n",
      " state (1)  A[0]:(0.0237365867943) A[1]:(0.00808780174702) A[2]:(0.00530875008553) A[3]:(0.962866842747)\n",
      " state (2)  A[0]:(0.0106402197853) A[1]:(0.00589589960873) A[2]:(0.00289775058627) A[3]:(0.98056614399)\n",
      " state (3)  A[0]:(0.184823855758) A[1]:(0.0278603509068) A[2]:(0.00317166000605) A[3]:(0.784144163132)\n",
      " state (4)  A[0]:(0.999593436718) A[1]:(0.000401955592679) A[2]:(2.50905571875e-07) A[3]:(4.36822210759e-06)\n",
      " state (5)  A[0]:(0.997047662735) A[1]:(0.00294012809172) A[2]:(3.89042554616e-07) A[3]:(1.18272701002e-05)\n",
      " state (6)  A[0]:(0.880101442337) A[1]:(0.119895711541) A[2]:(4.84904717268e-07) A[3]:(2.38907091443e-06)\n",
      " state (7)  A[0]:(0.146744370461) A[1]:(0.85325551033) A[2]:(8.67502905066e-08) A[3]:(1.03840909205e-08)\n",
      " state (8)  A[0]:(0.00610322877765) A[1]:(0.993896782398) A[2]:(6.91738666347e-09) A[3]:(1.38453526422e-10)\n",
      " state (9)  A[0]:(6.36183976894e-05) A[1]:(0.999936401844) A[2]:(2.03082994865e-10) A[3]:(4.95475568126e-13)\n",
      " state (10)  A[0]:(4.22235825681e-06) A[1]:(0.99999576807) A[2]:(2.56651852548e-11) A[3]:(1.71170671089e-14)\n",
      " state (11)  A[0]:(1.2961260154e-06) A[1]:(0.999998688698) A[2]:(1.05102046857e-11) A[3]:(3.91751721666e-15)\n",
      " state (12)  A[0]:(7.50803337723e-07) A[1]:(0.99999922514) A[2]:(6.97698105886e-12) A[3]:(1.974554714e-15)\n",
      " state (13)  A[0]:(5.68964594549e-07) A[1]:(0.999999403954) A[2]:(5.67489718412e-12) A[3]:(1.3922173393e-15)\n",
      " state (14)  A[0]:(4.89542230753e-07) A[1]:(0.999999523163) A[2]:(5.07847279854e-12) A[3]:(1.15109342312e-15)\n",
      " state (15)  A[0]:(4.49632494792e-07) A[1]:(0.999999523163) A[2]:(4.77241293359e-12) A[3]:(1.03321989466e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 237000 finished after 8 . Running score: 0.12. Policy_loss: -92050.611664, Value_loss: 1.63645925585. Times trained:               13076. Times reached goal: 133.               Steps done: 2576713.\n",
      " state (0)  A[0]:(0.983315110207) A[1]:(0.00638215010986) A[2]:(0.00550915813074) A[3]:(0.00479356106371)\n",
      " state (1)  A[0]:(0.0186440385878) A[1]:(0.00727784447372) A[2]:(0.00425299024209) A[3]:(0.969825148582)\n",
      " state (2)  A[0]:(0.00764694623649) A[1]:(0.00518008740619) A[2]:(0.00215351954103) A[3]:(0.985019445419)\n",
      " state (3)  A[0]:(0.0400375165045) A[1]:(0.0150799844414) A[2]:(0.00197789655067) A[3]:(0.94290459156)\n",
      " state (4)  A[0]:(0.999330341816) A[1]:(0.000661352125462) A[2]:(2.64661309757e-07) A[3]:(8.01398800832e-06)\n",
      " state (5)  A[0]:(0.989949285984) A[1]:(0.0100316042081) A[2]:(4.33847816339e-07) A[3]:(1.86802171811e-05)\n",
      " state (6)  A[0]:(0.473753720522) A[1]:(0.526245832443) A[2]:(2.4054989467e-07) A[3]:(2.44577222475e-07)\n",
      " state (7)  A[0]:(0.0264016054571) A[1]:(0.973598361015) A[2]:(1.97532372681e-08) A[3]:(1.29903610091e-09)\n",
      " state (8)  A[0]:(0.000153271001182) A[1]:(0.999846756458) A[2]:(3.50801609983e-10) A[3]:(2.46141106629e-12)\n",
      " state (9)  A[0]:(2.42317491939e-06) A[1]:(0.99999755621) A[2]:(1.44084553316e-11) A[3]:(1.62353481624e-14)\n",
      " state (10)  A[0]:(4.54131111383e-07) A[1]:(0.999999523163) A[2]:(4.02303104025e-12) A[3]:(2.10753126343e-15)\n",
      " state (11)  A[0]:(2.3125356563e-07) A[1]:(0.999999761581) A[2]:(2.41576160118e-12) A[3]:(9.21415145517e-16)\n",
      " state (12)  A[0]:(1.69595182342e-07) A[1]:(0.999999821186) A[2]:(1.91491787704e-12) A[3]:(6.28870929601e-16)\n",
      " state (13)  A[0]:(1.45008513641e-07) A[1]:(0.999999880791) A[2]:(1.70490227838e-12) A[3]:(5.18059432463e-16)\n",
      " state (14)  A[0]:(1.33342950903e-07) A[1]:(0.999999880791) A[2]:(1.60335275872e-12) A[3]:(4.66719018525e-16)\n",
      " state (15)  A[0]:(1.27255418647e-07) A[1]:(0.999999880791) A[2]:(1.55031619053e-12) A[3]:(4.4017642056e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 238000 finished after 15 . Running score: 0.05. Policy_loss: -92050.6115179, Value_loss: 1.42862661505. Times trained:               13141. Times reached goal: 105.               Steps done: 2589854.\n",
      " state (0)  A[0]:(0.982411146164) A[1]:(0.00704558519647) A[2]:(0.0054857833311) A[3]:(0.00505746016279)\n",
      " state (1)  A[0]:(0.0163148958236) A[1]:(0.00721699139103) A[2]:(0.00391899701208) A[3]:(0.972549140453)\n",
      " state (2)  A[0]:(0.00572794117033) A[1]:(0.00509975710884) A[2]:(0.0018728566356) A[3]:(0.987299442291)\n",
      " state (3)  A[0]:(0.998643100262) A[1]:(0.00128999247681) A[2]:(2.43231284003e-06) A[3]:(6.44752726657e-05)\n",
      " state (4)  A[0]:(0.998086512089) A[1]:(0.00190017651767) A[2]:(4.75848253245e-07) A[3]:(1.28391766339e-05)\n",
      " state (5)  A[0]:(0.965848684311) A[1]:(0.0341378934681) A[2]:(7.1799519219e-07) A[3]:(1.26930008264e-05)\n",
      " state (6)  A[0]:(0.34025272727) A[1]:(0.659746944904) A[2]:(2.39531971147e-07) A[3]:(1.02914185618e-07)\n",
      " state (7)  A[0]:(0.0381770804524) A[1]:(0.961822867393) A[2]:(3.34833636373e-08) A[3]:(2.00797689587e-09)\n",
      " state (8)  A[0]:(0.00076787424041) A[1]:(0.999232113361) A[2]:(1.52873713688e-09) A[3]:(1.53605721892e-11)\n",
      " state (9)  A[0]:(8.16315423435e-06) A[1]:(0.999991834164) A[2]:(4.47776538071e-11) A[3]:(5.95250099753e-14)\n",
      " state (10)  A[0]:(9.9495230188e-07) A[1]:(0.999998986721) A[2]:(8.85424806391e-12) A[3]:(4.5022126252e-15)\n",
      " state (11)  A[0]:(4.56348743683e-07) A[1]:(0.999999523163) A[2]:(4.87409244909e-12) A[3]:(1.72179754144e-15)\n",
      " state (12)  A[0]:(3.32752080112e-07) A[1]:(0.999999642372) A[2]:(3.83189053405e-12) A[3]:(1.16368075623e-15)\n",
      " state (13)  A[0]:(2.88560414674e-07) A[1]:(0.999999701977) A[2]:(3.44000500828e-12) A[3]:(9.74162851758e-16)\n",
      " state (14)  A[0]:(2.69305644451e-07) A[1]:(0.999999701977) A[2]:(3.26590794339e-12) A[3]:(8.9331143936e-16)\n",
      " state (15)  A[0]:(2.60074074276e-07) A[1]:(0.999999761581) A[2]:(3.1820188845e-12) A[3]:(8.54833008968e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 239000 finished after 6 . Running score: 0.08. Policy_loss: -92050.6184761, Value_loss: 0.998357483196. Times trained:               13443. Times reached goal: 119.               Steps done: 2603297.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9807,  0.0077,  0.0058,  0.0058]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9848e-01,  1.3809e-03,  4.3490e-06,  1.3248e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9848e-01,  1.3811e-03,  4.3509e-06,  1.3254e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9848e-01,  1.3812e-03,  4.3526e-06,  1.3259e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9807,  0.0077,  0.0058,  0.0058]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9807,  0.0077,  0.0058,  0.0058]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9807,  0.0077,  0.0058,  0.0058]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9807,  0.0077,  0.0058,  0.0058]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9848e-01,  1.3817e-03,  4.3593e-06,  1.3280e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9807,  0.0077,  0.0058,  0.0058]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9807,  0.0077,  0.0058,  0.0058]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9848e-01,  1.3820e-03,  4.3620e-06,  1.3289e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9848e-01,  1.3820e-03,  4.3628e-06,  1.3292e-04]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2565e-03,  9.9874e-01,  2.1546e-09,  2.9161e-11]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2569e-03,  9.9874e-01,  2.1552e-09,  2.9172e-11]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8337e-05,  9.9998e-01,  9.3829e-11,  1.9444e-13]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2403e-07,  1.0000e+00,  2.5355e-12,  4.5439e-16]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.4165e-08,  1.0000e+00,  2.0855e-12,  3.2496e-16]])\n",
      "On state=14, selected action=1\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.980682194233) A[1]:(0.00769010279328) A[2]:(0.00579789467156) A[3]:(0.00582978129387)\n",
      " state (1)  A[0]:(0.0205884631723) A[1]:(0.00780717423186) A[2]:(0.00477330433205) A[3]:(0.966831088066)\n",
      " state (2)  A[0]:(0.0132110547274) A[1]:(0.00653607118875) A[2]:(0.00341146579012) A[3]:(0.976841390133)\n",
      " state (3)  A[0]:(0.0269325301051) A[1]:(0.0100267194211) A[2]:(0.00306866155006) A[3]:(0.959972083569)\n",
      " state (4)  A[0]:(0.998485147953) A[1]:(0.00137837731745) A[2]:(4.3419472604e-06) A[3]:(0.00013213379134)\n",
      " state (5)  A[0]:(0.998928248882) A[1]:(0.00106475537177) A[2]:(2.14786993524e-07) A[3]:(6.7926457632e-06)\n",
      " state (6)  A[0]:(0.931217849255) A[1]:(0.0687734857202) A[2]:(3.73096582962e-07) A[3]:(8.29327746033e-06)\n",
      " state (7)  A[0]:(0.0960225462914) A[1]:(0.903977394104) A[2]:(5.82348604894e-08) A[3]:(7.2426322717e-09)\n",
      " state (8)  A[0]:(0.00125388952438) A[1]:(0.998746097088) A[2]:(2.15137907489e-09) A[3]:(2.90909761558e-11)\n",
      " state (9)  A[0]:(1.83008360182e-05) A[1]:(0.999981701374) A[2]:(9.3695391723e-11) A[3]:(1.93985791514e-13)\n",
      " state (10)  A[0]:(1.58812508744e-06) A[1]:(0.999998390675) A[2]:(1.58168200759e-11) A[3]:(1.0140973212e-14)\n",
      " state (11)  A[0]:(4.17969630462e-07) A[1]:(0.999999582767) A[2]:(6.04417080621e-12) A[3]:(1.99808232466e-15)\n",
      " state (12)  A[0]:(1.94882559867e-07) A[1]:(0.999999821186) A[2]:(3.49909294248e-12) A[3]:(7.88143193355e-16)\n",
      " state (13)  A[0]:(1.23962578868e-07) A[1]:(0.999999880791) A[2]:(2.5345183851e-12) A[3]:(4.54105797968e-16)\n",
      " state (14)  A[0]:(9.41436439916e-08) A[1]:(0.999999880791) A[2]:(2.08517599888e-12) A[3]:(3.24876681123e-16)\n",
      " state (15)  A[0]:(7.937494928e-08) A[1]:(0.999999940395) A[2]:(1.84875227106e-12) A[3]:(2.64042271925e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 240000 finished after 18 . Running score: 0.13. Policy_loss: -92050.6181982, Value_loss: 1.20707543401. Times trained:               12512. Times reached goal: 129.               Steps done: 2615809.\n",
      " state (0)  A[0]:(0.978324234486) A[1]:(0.00882160197943) A[2]:(0.0059246304445) A[3]:(0.0069295479916)\n",
      " state (1)  A[0]:(0.0128596462309) A[1]:(0.0065487883985) A[2]:(0.00329856202006) A[3]:(0.977293014526)\n",
      " state (2)  A[0]:(0.00669056084007) A[1]:(0.00517724594101) A[2]:(0.00199354742654) A[3]:(0.986138641834)\n",
      " state (3)  A[0]:(0.152761802077) A[1]:(0.0276146028191) A[2]:(0.00252554053441) A[3]:(0.817098081112)\n",
      " state (4)  A[0]:(0.99936234951) A[1]:(0.000630415568594) A[2]:(2.50288877623e-07) A[3]:(6.98909570929e-06)\n",
      " state (5)  A[0]:(0.987344324589) A[1]:(0.0126290954649) A[2]:(4.20500555265e-07) A[3]:(2.61568529822e-05)\n",
      " state (6)  A[0]:(0.303238928318) A[1]:(0.696760714054) A[2]:(2.15794500491e-07) A[3]:(1.81204583782e-07)\n",
      " state (7)  A[0]:(0.00381119758822) A[1]:(0.996188819408) A[2]:(6.59216992105e-09) A[3]:(1.61744700966e-10)\n",
      " state (8)  A[0]:(9.25874883251e-06) A[1]:(0.99999076128) A[2]:(7.23283100079e-11) A[3]:(1.33459893527e-13)\n",
      " state (9)  A[0]:(3.65980156403e-07) A[1]:(0.999999642372) A[2]:(6.7107873078e-12) A[3]:(2.8516694011e-15)\n",
      " state (10)  A[0]:(9.67773985394e-08) A[1]:(0.999999880791) A[2]:(2.54479250172e-12) A[3]:(5.7854806693e-16)\n",
      " state (11)  A[0]:(5.23203382841e-08) A[1]:(0.999999940395) A[2]:(1.6293041135e-12) A[3]:(2.76186633276e-16)\n",
      " state (12)  A[0]:(3.82408664734e-08) A[1]:(0.999999940395) A[2]:(1.29925714284e-12) A[3]:(1.89417226669e-16)\n",
      " state (13)  A[0]:(3.23084705656e-08) A[1]:(0.999999940395) A[2]:(1.15084536952e-12) A[3]:(1.54653869548e-16)\n",
      " state (14)  A[0]:(2.94262925138e-08) A[1]:(1.0) A[2]:(1.07625746423e-12) A[3]:(1.38220014238e-16)\n",
      " state (15)  A[0]:(2.7912484768e-08) A[1]:(1.0) A[2]:(1.03638582091e-12) A[3]:(1.29719932733e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 241000 finished after 13 . Running score: 0.18. Policy_loss: -92050.611846, Value_loss: 1.21860466219. Times trained:               12907. Times reached goal: 127.               Steps done: 2628716.\n",
      " state (0)  A[0]:(0.98419624567) A[1]:(0.00573302898556) A[2]:(0.00536422291771) A[3]:(0.00470652524382)\n",
      " state (1)  A[0]:(0.0233434345573) A[1]:(0.00749280164018) A[2]:(0.0051567251794) A[3]:(0.964007019997)\n",
      " state (2)  A[0]:(0.0127569204196) A[1]:(0.00586242461577) A[2]:(0.0032548843883) A[3]:(0.978125751019)\n",
      " state (3)  A[0]:(0.0477665625513) A[1]:(0.0120340604335) A[2]:(0.00312204216607) A[3]:(0.937077343464)\n",
      " state (4)  A[0]:(0.999766528606) A[1]:(0.000230431745877) A[2]:(2.1419901941e-07) A[3]:(2.83543749902e-06)\n",
      " state (5)  A[0]:(0.997612595558) A[1]:(0.00238104956225) A[2]:(2.52461916261e-07) A[3]:(6.08825530435e-06)\n",
      " state (6)  A[0]:(0.582724869251) A[1]:(0.417274683714) A[2]:(2.68954352123e-07) A[3]:(2.24141018634e-07)\n",
      " state (7)  A[0]:(0.0227358229458) A[1]:(0.977264165878) A[2]:(2.07315355993e-08) A[3]:(9.24822107784e-10)\n",
      " state (8)  A[0]:(0.000193414452951) A[1]:(0.999806582928) A[2]:(5.37517530486e-10) A[3]:(3.74243084439e-12)\n",
      " state (9)  A[0]:(5.33882757736e-06) A[1]:(0.999994635582) A[2]:(3.59046126164e-11) A[3]:(5.40730451409e-14)\n",
      " state (10)  A[0]:(1.0839925153e-06) A[1]:(0.999998927116) A[2]:(1.09882675875e-11) A[3]:(8.00264361885e-15)\n",
      " state (11)  A[0]:(5.21525066688e-07) A[1]:(0.999999463558) A[2]:(6.41652226144e-12) A[3]:(3.31296968517e-15)\n",
      " state (12)  A[0]:(3.58699224989e-07) A[1]:(0.999999642372) A[2]:(4.8829421409e-12) A[3]:(2.10831138077e-15)\n",
      " state (13)  A[0]:(2.92377876576e-07) A[1]:(0.999999701977) A[2]:(4.21033910861e-12) A[3]:(1.64716176269e-15)\n",
      " state (14)  A[0]:(2.60433694166e-07) A[1]:(0.999999761581) A[2]:(3.87381489733e-12) A[3]:(1.43264124556e-15)\n",
      " state (15)  A[0]:(2.43576465664e-07) A[1]:(0.999999761581) A[2]:(3.69291749958e-12) A[3]:(1.32175065672e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 242000 finished after 7 . Running score: 0.15. Policy_loss: -92050.6096867, Value_loss: 1.84088519632. Times trained:               13056. Times reached goal: 141.               Steps done: 2641772.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.982499837875) A[1]:(0.00655089179054) A[2]:(0.00553840119392) A[3]:(0.00541088869795)\n",
      " state (1)  A[0]:(0.0202971901745) A[1]:(0.00740310596302) A[2]:(0.00472194002941) A[3]:(0.967577755451)\n",
      " state (2)  A[0]:(0.0104599492624) A[1]:(0.00571626517922) A[2]:(0.00281471363269) A[3]:(0.981009066105)\n",
      " state (3)  A[0]:(0.028904736042) A[1]:(0.0102292550728) A[2]:(0.00247441278771) A[3]:(0.958391606808)\n",
      " state (4)  A[0]:(0.999700307846) A[1]:(0.000295948964776) A[2]:(2.14556905576e-07) A[3]:(3.54214625986e-06)\n",
      " state (5)  A[0]:(0.99543774128) A[1]:(0.00455171009526) A[2]:(2.92994059237e-07) A[3]:(1.02840485852e-05)\n",
      " state (6)  A[0]:(0.515311062336) A[1]:(0.484688401222) A[2]:(2.82571562593e-07) A[3]:(2.16966526523e-07)\n",
      " state (7)  A[0]:(0.0285087097436) A[1]:(0.971491277218) A[2]:(3.23272146829e-08) A[3]:(1.42325495833e-09)\n",
      " state (8)  A[0]:(0.000377120421035) A[1]:(0.999622881413) A[2]:(1.24254873057e-09) A[3]:(1.005581729e-11)\n",
      " state (9)  A[0]:(1.18753487186e-05) A[1]:(0.999988138676) A[2]:(9.41036207291e-11) A[3]:(1.79287587923e-13)\n",
      " state (10)  A[0]:(2.477078624e-06) A[1]:(0.999997496605) A[2]:(2.97421670625e-11) A[3]:(2.81240603587e-14)\n",
      " state (11)  A[0]:(1.21191078506e-06) A[1]:(0.999998807907) A[2]:(1.76980669703e-11) A[3]:(1.20265481737e-14)\n",
      " state (12)  A[0]:(8.42912754706e-07) A[1]:(0.999999165535) A[2]:(1.36330903164e-11) A[3]:(7.80826310998e-15)\n",
      " state (13)  A[0]:(6.91673108122e-07) A[1]:(0.999999284744) A[2]:(1.1844092547e-11) A[3]:(6.17324134113e-15)\n",
      " state (14)  A[0]:(6.18315993961e-07) A[1]:(0.999999403954) A[2]:(1.09462265641e-11) A[3]:(5.40514127664e-15)\n",
      " state (15)  A[0]:(5.79217839913e-07) A[1]:(0.999999403954) A[2]:(1.04613921692e-11) A[3]:(5.00380393722e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 243000 finished after 6 . Running score: 0.21. Policy_loss: -92050.6096828, Value_loss: 1.63499275657. Times trained:               13160. Times reached goal: 137.               Steps done: 2654932.\n",
      " state (0)  A[0]:(0.983864843845) A[1]:(0.00608139671385) A[2]:(0.00550598837435) A[3]:(0.00454775476828)\n",
      " state (1)  A[0]:(0.0318769663572) A[1]:(0.00919958204031) A[2]:(0.0068688262254) A[3]:(0.952054619789)\n",
      " state (2)  A[0]:(0.0158733651042) A[1]:(0.00702599389479) A[2]:(0.00404200097546) A[3]:(0.973058640957)\n",
      " state (3)  A[0]:(0.0270545072854) A[1]:(0.00998487137258) A[2]:(0.00298167392612) A[3]:(0.959978938103)\n",
      " state (4)  A[0]:(0.999700725079) A[1]:(0.000296138518024) A[2]:(2.34721582615e-07) A[3]:(2.89897366201e-06)\n",
      " state (5)  A[0]:(0.991545200348) A[1]:(0.00844772346318) A[2]:(3.64385499552e-07) A[3]:(6.73171189192e-06)\n",
      " state (6)  A[0]:(0.22186923027) A[1]:(0.778130531311) A[2]:(1.85299384725e-07) A[3]:(2.3926492787e-08)\n",
      " state (7)  A[0]:(0.00214426312596) A[1]:(0.997855722904) A[2]:(6.11467854128e-09) A[3]:(6.04178582164e-11)\n",
      " state (8)  A[0]:(1.05672461359e-05) A[1]:(0.999989449978) A[2]:(1.27678881356e-10) A[3]:(1.31469596494e-13)\n",
      " state (9)  A[0]:(8.13402891708e-07) A[1]:(0.999999165535) A[2]:(2.04808427162e-11) A[3]:(6.36174555892e-15)\n",
      " state (10)  A[0]:(2.82938685814e-07) A[1]:(0.999999701977) A[2]:(9.75161357913e-12) A[3]:(1.8117257589e-15)\n",
      " state (11)  A[0]:(1.72103682416e-07) A[1]:(0.999999821186) A[2]:(6.90912908483e-12) A[3]:(1.00311359086e-15)\n",
      " state (12)  A[0]:(1.32894427907e-07) A[1]:(0.999999880791) A[2]:(5.79103431875e-12) A[3]:(7.38457087702e-16)\n",
      " state (13)  A[0]:(1.15222647423e-07) A[1]:(0.999999880791) A[2]:(5.26300053885e-12) A[3]:(6.2425089427e-16)\n",
      " state (14)  A[0]:(1.06121859744e-07) A[1]:(0.999999880791) A[2]:(4.98739678029e-12) A[3]:(5.67052823984e-16)\n",
      " state (15)  A[0]:(1.01002164854e-07) A[1]:(0.999999880791) A[2]:(4.83362785561e-12) A[3]:(5.35528904665e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 244000 finished after 19 . Running score: 0.19. Policy_loss: -92050.6096902, Value_loss: 1.4180111342. Times trained:               12756. Times reached goal: 123.               Steps done: 2667688.\n",
      "action_dist \n",
      "tensor([[ 0.9855,  0.0054,  0.0051,  0.0040]])\n",
      "On state=0, selected action=1\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9855,  0.0054,  0.0051,  0.0040]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9855,  0.0054,  0.0051,  0.0040]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9985e-01,  1.4745e-04,  1.4779e-07,  1.5477e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9855,  0.0054,  0.0051,  0.0040]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9985e-01,  1.4742e-04,  1.4778e-07,  1.5475e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9985e-01,  1.4741e-04,  1.4778e-07,  1.5474e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9985e-01,  1.4741e-04,  1.4778e-07,  1.5473e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.5509e-03,  9.9545e-01,  7.9168e-09,  1.1344e-10]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.2900e-04,  9.9977e-01,  8.1860e-10,  3.5159e-12]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.8182e-05,  9.9995e-01,  2.5592e-10,  5.5061e-13]])\n",
      "On state=10, selected action=1\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.985504388809) A[1]:(0.00541124306619) A[2]:(0.00509977107868) A[3]:(0.00398457050323)\n",
      " state (1)  A[0]:(0.0278865229338) A[1]:(0.00812965724617) A[2]:(0.0058960467577) A[3]:(0.958087742329)\n",
      " state (2)  A[0]:(0.0143825095147) A[1]:(0.00615506386384) A[2]:(0.0035295386333) A[3]:(0.975932896137)\n",
      " state (3)  A[0]:(0.0743666291237) A[1]:(0.0140858506784) A[2]:(0.00340136722662) A[3]:(0.90814614296)\n",
      " state (4)  A[0]:(0.999850869179) A[1]:(0.000147420127178) A[2]:(1.47749389612e-07) A[3]:(1.54702104282e-06)\n",
      " state (5)  A[0]:(0.998109161854) A[1]:(0.00188647524919) A[2]:(2.32834850067e-07) A[3]:(4.14505939261e-06)\n",
      " state (6)  A[0]:(0.67226678133) A[1]:(0.327732801437) A[2]:(2.98222943229e-07) A[3]:(1.37486594554e-07)\n",
      " state (7)  A[0]:(0.0781482979655) A[1]:(0.921851634979) A[2]:(6.63396377831e-08) A[3]:(2.79522782698e-09)\n",
      " state (8)  A[0]:(0.00453892303631) A[1]:(0.995461046696) A[2]:(7.90144838447e-09) A[3]:(1.13103061639e-10)\n",
      " state (9)  A[0]:(0.000228577278904) A[1]:(0.999771416187) A[2]:(8.17500123151e-10) A[3]:(3.50846413553e-12)\n",
      " state (10)  A[0]:(4.81471615785e-05) A[1]:(0.999951839447) A[2]:(2.55789917025e-10) A[3]:(5.50159798361e-13)\n",
      " state (11)  A[0]:(2.41988345806e-05) A[1]:(0.999975800514) A[2]:(1.54584539613e-10) A[3]:(2.41077746958e-13)\n",
      " state (12)  A[0]:(1.73821954377e-05) A[1]:(0.999982595444) A[2]:(1.21910925799e-10) A[3]:(1.62064331681e-13)\n",
      " state (13)  A[0]:(1.46067213791e-05) A[1]:(0.999985396862) A[2]:(1.07903214142e-10) A[3]:(1.31622848471e-13)\n",
      " state (14)  A[0]:(1.32568775371e-05) A[1]:(0.999986767769) A[2]:(1.00997987751e-10) A[3]:(1.17303085648e-13)\n",
      " state (15)  A[0]:(1.25308924908e-05) A[1]:(0.999987483025) A[2]:(9.73271255256e-11) A[3]:(1.09780965514e-13)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 245000 finished after 11 . Running score: 0.14. Policy_loss: -92050.6101697, Value_loss: 1.41676346084. Times trained:               13219. Times reached goal: 115.               Steps done: 2680907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.986406505108) A[1]:(0.00504759093747) A[2]:(0.00479833316058) A[3]:(0.00374757382087)\n",
      " state (1)  A[0]:(0.0260864123702) A[1]:(0.0079768197611) A[2]:(0.00555872917175) A[3]:(0.960378050804)\n",
      " state (2)  A[0]:(0.0113968411461) A[1]:(0.005662036594) A[2]:(0.00289190607145) A[3]:(0.980049192905)\n",
      " state (3)  A[0]:(0.998692035675) A[1]:(0.00110585079528) A[2]:(7.31884256311e-06) A[3]:(0.000194784326595)\n",
      " state (4)  A[0]:(0.99959141016) A[1]:(0.000405937666073) A[2]:(1.67003207707e-07) A[3]:(2.4934324756e-06)\n",
      " state (5)  A[0]:(0.94889074564) A[1]:(0.0511077456176) A[2]:(3.59719876997e-07) A[3]:(1.15183274829e-06)\n",
      " state (6)  A[0]:(0.0876344516873) A[1]:(0.912365496159) A[2]:(6.286266796e-08) A[3]:(3.12870973573e-09)\n",
      " state (7)  A[0]:(0.00170330051333) A[1]:(0.998296678066) A[2]:(3.01410096881e-09) A[3]:(3.44542658293e-11)\n",
      " state (8)  A[0]:(1.49057514136e-05) A[1]:(0.999985098839) A[2]:(7.67958197034e-11) A[3]:(1.45142334566e-13)\n",
      " state (9)  A[0]:(1.68708413639e-06) A[1]:(0.99999833107) A[2]:(1.45793099815e-11) A[3]:(1.11275294263e-14)\n",
      " state (10)  A[0]:(8.36190451992e-07) A[1]:(0.999999165535) A[2]:(8.61249613293e-12) A[3]:(4.8380256134e-15)\n",
      " state (11)  A[0]:(6.4973892222e-07) A[1]:(0.999999344349) A[2]:(7.15852897792e-12) A[3]:(3.58509913651e-15)\n",
      " state (12)  A[0]:(5.83949486099e-07) A[1]:(0.999999403954) A[2]:(6.63526004974e-12) A[3]:(3.1589428847e-15)\n",
      " state (13)  A[0]:(5.54957296117e-07) A[1]:(0.999999463558) A[2]:(6.40808326541e-12) A[3]:(2.97476065252e-15)\n",
      " state (14)  A[0]:(5.40517930858e-07) A[1]:(0.999999463558) A[2]:(6.29889976983e-12) A[3]:(2.88422723001e-15)\n",
      " state (15)  A[0]:(5.32717308488e-07) A[1]:(0.999999463558) A[2]:(6.24292327903e-12) A[3]:(2.83586334279e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 246000 finished after 4 . Running score: 0.18. Policy_loss: -92050.6118416, Value_loss: 1.41816531248. Times trained:               12862. Times reached goal: 134.               Steps done: 2693769.\n",
      " state (0)  A[0]:(0.985230505466) A[1]:(0.00548442313448) A[2]:(0.00496477400884) A[3]:(0.00432028621435)\n",
      " state (1)  A[0]:(0.0182130094618) A[1]:(0.00677598593757) A[2]:(0.00433560553938) A[3]:(0.97067540884)\n",
      " state (2)  A[0]:(0.0101295290515) A[1]:(0.00529706384987) A[2]:(0.00274731148966) A[3]:(0.981826066971)\n",
      " state (3)  A[0]:(0.00806130748242) A[1]:(0.00504387263209) A[2]:(0.00191837386228) A[3]:(0.98497647047)\n",
      " state (4)  A[0]:(0.999574244022) A[1]:(0.000408166844863) A[2]:(8.05443903573e-07) A[3]:(1.67667894857e-05)\n",
      " state (5)  A[0]:(0.99883544445) A[1]:(0.00115716806613) A[2]:(1.96335435021e-07) A[3]:(7.17169041309e-06)\n",
      " state (6)  A[0]:(0.740289926529) A[1]:(0.259708613157) A[2]:(3.22310370393e-07) A[3]:(1.14114857297e-06)\n",
      " state (7)  A[0]:(0.0194223895669) A[1]:(0.980577588081) A[2]:(2.00067518108e-08) A[3]:(8.29909474476e-10)\n",
      " state (8)  A[0]:(0.000171377309016) A[1]:(0.999828636646) A[2]:(5.46430733994e-10) A[3]:(3.86817617867e-12)\n",
      " state (9)  A[0]:(7.68763675296e-06) A[1]:(0.999992311001) A[2]:(5.33409913461e-11) A[3]:(1.11382128835e-13)\n",
      " state (10)  A[0]:(1.84294560768e-06) A[1]:(0.999998152256) A[2]:(1.86336727298e-11) A[3]:(2.1292429625e-14)\n",
      " state (11)  A[0]:(9.13883013709e-07) A[1]:(0.99999910593) A[2]:(1.11970892627e-11) A[3]:(9.42129420946e-15)\n",
      " state (12)  A[0]:(6.27629731298e-07) A[1]:(0.999999344349) A[2]:(8.55428141516e-12) A[3]:(6.09039220152e-15)\n",
      " state (13)  A[0]:(5.07761853896e-07) A[1]:(0.999999463558) A[2]:(7.36623869541e-12) A[3]:(4.7661103986e-15)\n",
      " state (14)  A[0]:(4.48720072654e-07) A[1]:(0.999999523163) A[2]:(6.76253020596e-12) A[3]:(4.13405017819e-15)\n",
      " state (15)  A[0]:(4.1665111894e-07) A[1]:(0.999999582767) A[2]:(6.43279656973e-12) A[3]:(3.79787508341e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 247000 finished after 10 . Running score: 0.21. Policy_loss: -92050.6118434, Value_loss: 1.42293324046. Times trained:               12958. Times reached goal: 126.               Steps done: 2706727.\n",
      " state (0)  A[0]:(0.986882090569) A[1]:(0.00476727122441) A[2]:(0.00474533298984) A[3]:(0.00360533501953)\n",
      " state (1)  A[0]:(0.0282062925398) A[1]:(0.00815876852721) A[2]:(0.0060165701434) A[3]:(0.957618355751)\n",
      " state (2)  A[0]:(0.0140633629635) A[1]:(0.0060611651279) A[2]:(0.00356306973845) A[3]:(0.976312398911)\n",
      " state (3)  A[0]:(0.0243828650564) A[1]:(0.00821494404227) A[2]:(0.00283209816553) A[3]:(0.964570105076)\n",
      " state (4)  A[0]:(0.999870479107) A[1]:(0.000127611856442) A[2]:(1.7370253147e-07) A[3]:(1.72454429048e-06)\n",
      " state (5)  A[0]:(0.999332070351) A[1]:(0.000663920829538) A[2]:(2.05480347404e-07) A[3]:(3.79413404517e-06)\n",
      " state (6)  A[0]:(0.850077092648) A[1]:(0.149921640754) A[2]:(3.94984454033e-07) A[3]:(8.43092436753e-07)\n",
      " state (7)  A[0]:(0.024985069409) A[1]:(0.975014925003) A[2]:(2.82539112106e-08) A[3]:(9.35595045881e-10)\n",
      " state (8)  A[0]:(0.000211739752558) A[1]:(0.999788284302) A[2]:(7.25848214866e-10) A[3]:(4.26163618283e-12)\n",
      " state (9)  A[0]:(8.62687375047e-06) A[1]:(0.999991357327) A[2]:(6.44343051137e-11) A[3]:(1.08464534012e-13)\n",
      " state (10)  A[0]:(2.05770743378e-06) A[1]:(0.999997913837) A[2]:(2.21428049507e-11) A[3]:(2.04070887657e-14)\n",
      " state (11)  A[0]:(1.0556975667e-06) A[1]:(0.999998927116) A[2]:(1.35437225671e-11) A[3]:(9.33350856184e-15)\n",
      " state (12)  A[0]:(7.53295637423e-07) A[1]:(0.99999922514) A[2]:(1.05925017022e-11) A[3]:(6.2820914276e-15)\n",
      " state (13)  A[0]:(6.29327701063e-07) A[1]:(0.999999344349) A[2]:(9.30874301253e-12) A[3]:(5.08956729368e-15)\n",
      " state (14)  A[0]:(5.70049223825e-07) A[1]:(0.999999403954) A[2]:(8.68057622311e-12) A[3]:(4.53498221235e-15)\n",
      " state (15)  A[0]:(5.39143343303e-07) A[1]:(0.999999463558) A[2]:(8.35293553075e-12) A[3]:(4.25111436667e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 248000 finished after 3 . Running score: 0.07. Policy_loss: -92050.6118445, Value_loss: 1.20523956587. Times trained:               12839. Times reached goal: 121.               Steps done: 2719566.\n",
      " state (0)  A[0]:(0.98766797781) A[1]:(0.00435736915097) A[2]:(0.00455671316013) A[3]:(0.00341793405823)\n",
      " state (1)  A[0]:(0.0307188872248) A[1]:(0.00840213056654) A[2]:(0.00640735309571) A[3]:(0.954471647739)\n",
      " state (2)  A[0]:(0.0163722727448) A[1]:(0.00641963584349) A[2]:(0.00401036161929) A[3]:(0.973197758198)\n",
      " state (3)  A[0]:(0.0253891889006) A[1]:(0.00814028736204) A[2]:(0.00322725297883) A[3]:(0.963243246078)\n",
      " state (4)  A[0]:(0.999879777431) A[1]:(0.00011816512415) A[2]:(2.31696873243e-07) A[3]:(1.84325301689e-06)\n",
      " state (5)  A[0]:(0.999647080898) A[1]:(0.000350269663613) A[2]:(1.95093804223e-07) A[3]:(2.48035621553e-06)\n",
      " state (6)  A[0]:(0.950527608395) A[1]:(0.0494700223207) A[2]:(4.36686889316e-07) A[3]:(1.92436937141e-06)\n",
      " state (7)  A[0]:(0.0561820827425) A[1]:(0.943817853928) A[2]:(6.07485546311e-08) A[3]:(2.61898791543e-09)\n",
      " state (8)  A[0]:(0.000694933056366) A[1]:(0.999305069447) A[2]:(2.11147188622e-09) A[3]:(1.77999021772e-11)\n",
      " state (9)  A[0]:(1.97252848011e-05) A[1]:(0.999980270863) A[2]:(1.42037867579e-10) A[3]:(3.1717572904e-13)\n",
      " state (10)  A[0]:(3.19056994158e-06) A[1]:(0.999996781349) A[2]:(3.64839547462e-11) A[3]:(3.86809216656e-14)\n",
      " state (11)  A[0]:(1.27133307615e-06) A[1]:(0.999998748302) A[2]:(1.84875118686e-11) A[3]:(1.32434414455e-14)\n",
      " state (12)  A[0]:(7.7518393482e-07) A[1]:(0.99999922514) A[2]:(1.28634845831e-11) A[3]:(7.43165023877e-15)\n",
      " state (13)  A[0]:(5.87908459693e-07) A[1]:(0.999999403954) A[2]:(1.05193770361e-11) A[3]:(5.38100930798e-15)\n",
      " state (14)  A[0]:(5.02020043314e-07) A[1]:(0.999999523163) A[2]:(9.38753588753e-12) A[3]:(4.47637303811e-15)\n",
      " state (15)  A[0]:(4.58139652437e-07) A[1]:(0.999999523163) A[2]:(8.79534032411e-12) A[3]:(4.02495106404e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 249000 finished after 5 . Running score: 0.11. Policy_loss: -92050.6117498, Value_loss: 1.20701187347. Times trained:               12657. Times reached goal: 145.               Steps done: 2732223.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9858,  0.0050,  0.0049,  0.0042]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0139,  0.0060,  0.0036,  0.9765]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0066,  0.0045,  0.0020,  0.9869]])\n",
      "On state=2, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0778,  0.0161,  0.0025,  0.9035]])\n",
      "On state=3, selected action=1\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0711,  0.0155,  0.0025,  0.9109]])\n",
      "On state=3, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0668,  0.0150,  0.0025,  0.9158]])\n",
      "On state=3, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0067,  0.0045,  0.0020,  0.9868]])\n",
      "On state=2, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0637,  0.0147,  0.0025,  0.9192]])\n",
      "On state=3, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0633,  0.0146,  0.0025,  0.9197]])\n",
      "On state=3, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0067,  0.0045,  0.0021,  0.9868]])\n",
      "On state=2, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0662,  0.0149,  0.0025,  0.9165]])\n",
      "On state=3, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0684,  0.0151,  0.0025,  0.9139]])\n",
      "On state=3, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0717,  0.0155,  0.0025,  0.9104]])\n",
      "On state=3, selected action=3\n",
      "new state=3, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0760,  0.0159,  0.0025,  0.9056]])\n",
      "On state=3, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0067,  0.0045,  0.0021,  0.9868]])\n",
      "On state=2, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0067,  0.0045,  0.0020,  0.9868]])\n",
      "On state=2, selected action=0\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.9231e-01,  7.0769e-01,  3.2425e-07,  6.8393e-08]])\n",
      "On state=6, selected action=0\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.98596996069) A[1]:(0.00500385509804) A[2]:(0.00490106176585) A[3]:(0.00412512850016)\n",
      " state (1)  A[0]:(0.014326796867) A[1]:(0.00613091140985) A[2]:(0.00370031222701) A[3]:(0.975841999054)\n",
      " state (2)  A[0]:(0.00667701615021) A[1]:(0.00448562065139) A[2]:(0.00204865238629) A[3]:(0.98678869009)\n",
      " state (3)  A[0]:(0.133010625839) A[1]:(0.0206106435508) A[2]:(0.00273475819267) A[3]:(0.843643963337)\n",
      " state (4)  A[0]:(0.999738037586) A[1]:(0.000258739164565) A[2]:(1.97239842237e-07) A[3]:(3.02108674077e-06)\n",
      " state (5)  A[0]:(0.993283748627) A[1]:(0.00670489203185) A[2]:(4.64602806005e-07) A[3]:(1.08684571387e-05)\n",
      " state (6)  A[0]:(0.230389997363) A[1]:(0.769609689713) A[2]:(2.75608158518e-07) A[3]:(4.32234372738e-08)\n",
      " state (7)  A[0]:(0.00410284474492) A[1]:(0.995897114277) A[2]:(1.3379343855e-08) A[3]:(1.8556697845e-10)\n",
      " state (8)  A[0]:(2.38696593442e-05) A[1]:(0.999976158142) A[2]:(2.77494138778e-10) A[3]:(6.02417313082e-13)\n",
      " state (9)  A[0]:(1.44896284837e-06) A[1]:(0.999998569489) A[2]:(3.4805460597e-11) A[3]:(2.48880641573e-14)\n",
      " state (10)  A[0]:(5.08733194238e-07) A[1]:(0.999999463558) A[2]:(1.61769816293e-11) A[3]:(7.46908231878e-15)\n",
      " state (11)  A[0]:(3.30833614726e-07) A[1]:(0.999999642372) A[2]:(1.18427420648e-11) A[3]:(4.54303453106e-15)\n",
      " state (12)  A[0]:(2.7112886869e-07) A[1]:(0.999999701977) A[2]:(1.02662565948e-11) A[3]:(3.60854924365e-15)\n",
      " state (13)  A[0]:(2.45684049105e-07) A[1]:(0.999999761581) A[2]:(9.57239842947e-12) A[3]:(3.21988478767e-15)\n",
      " state (14)  A[0]:(2.33559262597e-07) A[1]:(0.999999761581) A[2]:(9.23882498283e-12) A[3]:(3.03717363996e-15)\n",
      " state (15)  A[0]:(2.27434156841e-07) A[1]:(0.999999761581) A[2]:(9.07117957138e-12) A[3]:(2.94568412902e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 250000 finished after 17 . Running score: 0.17. Policy_loss: -92050.6153264, Value_loss: 1.20691438697. Times trained:               12808. Times reached goal: 124.               Steps done: 2745031.\n",
      " state (0)  A[0]:(0.983926177025) A[1]:(0.00586442835629) A[2]:(0.00510073360056) A[3]:(0.00510865403339)\n",
      " state (1)  A[0]:(0.00910593569279) A[1]:(0.00509808305651) A[2]:(0.0026132108178) A[3]:(0.983182787895)\n",
      " state (2)  A[0]:(0.00420952215791) A[1]:(0.00382847734727) A[2]:(0.00143410800956) A[3]:(0.990527868271)\n",
      " state (3)  A[0]:(0.0579828359187) A[1]:(0.0166021008044) A[2]:(0.00182531611063) A[3]:(0.923589766026)\n",
      " state (4)  A[0]:(0.999292075634) A[1]:(0.000700003700331) A[2]:(2.78843089063e-07) A[3]:(7.61352657719e-06)\n",
      " state (5)  A[0]:(0.926331341267) A[1]:(0.0736610814929) A[2]:(7.53458948566e-07) A[3]:(6.79437516737e-06)\n",
      " state (6)  A[0]:(0.0858747214079) A[1]:(0.914125144482) A[2]:(1.56124443151e-07) A[3]:(8.41158520615e-09)\n",
      " state (7)  A[0]:(0.0109746996313) A[1]:(0.989025235176) A[2]:(3.46079147562e-08) A[3]:(6.88276879934e-10)\n",
      " state (8)  A[0]:(0.000506445183419) A[1]:(0.999493539333) A[2]:(3.24864468659e-09) A[3]:(2.34571338048e-11)\n",
      " state (9)  A[0]:(2.76098689937e-05) A[1]:(0.999972403049) A[2]:(3.56197404905e-10) A[3]:(8.77723295521e-13)\n",
      " state (10)  A[0]:(7.73346255301e-06) A[1]:(0.999992251396) A[2]:(1.37293176827e-10) A[3]:(2.0374781235e-13)\n",
      " state (11)  A[0]:(4.93720517625e-06) A[1]:(0.999995052814) A[2]:(9.84432119155e-11) A[3]:(1.21192864229e-13)\n",
      " state (12)  A[0]:(4.1592843445e-06) A[1]:(0.999995827675) A[2]:(8.68053337544e-11) A[3]:(9.92633167503e-14)\n",
      " state (13)  A[0]:(3.8639759623e-06) A[1]:(0.999996125698) A[2]:(8.22819659629e-11) A[3]:(9.10809988086e-14)\n",
      " state (14)  A[0]:(3.73349757865e-06) A[1]:(0.999996244907) A[2]:(8.0272816172e-11) A[3]:(8.74929062577e-14)\n",
      " state (15)  A[0]:(3.67129223378e-06) A[1]:(0.999996304512) A[2]:(7.93177884484e-11) A[3]:(8.57891841875e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 251000 finished after 9 . Running score: 0.16. Policy_loss: -92050.6109309, Value_loss: 1.22298651. Times trained:               12781. Times reached goal: 125.               Steps done: 2757812.\n",
      " state (0)  A[0]:(0.986990213394) A[1]:(0.00478385481983) A[2]:(0.00464295689017) A[3]:(0.00358297605999)\n",
      " state (1)  A[0]:(0.0210106410086) A[1]:(0.00733560929075) A[2]:(0.00481211300939) A[3]:(0.966841638088)\n",
      " state (2)  A[0]:(0.00908788759261) A[1]:(0.00520343799144) A[2]:(0.00256282230839) A[3]:(0.983145833015)\n",
      " state (3)  A[0]:(0.0127330878749) A[1]:(0.00682716537267) A[2]:(0.00187247630674) A[3]:(0.978567242622)\n",
      " state (4)  A[0]:(0.999754607677) A[1]:(0.000242019174038) A[2]:(1.92037063584e-07) A[3]:(3.16223463415e-06)\n",
      " state (5)  A[0]:(0.996010184288) A[1]:(0.00397737789899) A[2]:(3.79351206448e-07) A[3]:(1.20782033264e-05)\n",
      " state (6)  A[0]:(0.412140101194) A[1]:(0.587859272957) A[2]:(4.0755398345e-07) A[3]:(1.80363727509e-07)\n",
      " state (7)  A[0]:(0.032083388418) A[1]:(0.967916548252) A[2]:(6.250436968e-08) A[3]:(1.93982430119e-09)\n",
      " state (8)  A[0]:(0.00211857957765) A[1]:(0.997881412506) A[2]:(7.80628450769e-09) A[3]:(9.11694469918e-11)\n",
      " state (9)  A[0]:(6.76260242471e-05) A[1]:(0.999932348728) A[2]:(5.54378043471e-10) A[3]:(1.83620154671e-12)\n",
      " state (10)  A[0]:(9.37137156143e-06) A[1]:(0.999990642071) A[2]:(1.24473237273e-10) A[3]:(1.86367482319e-13)\n",
      " state (11)  A[0]:(4.15740441895e-06) A[1]:(0.999995827675) A[2]:(6.78289299727e-11) A[3]:(7.19993251759e-14)\n",
      " state (12)  A[0]:(2.9301199902e-06) A[1]:(0.999997079372) A[2]:(5.23678322928e-11) A[3]:(4.7698664815e-14)\n",
      " state (13)  A[0]:(2.47909679274e-06) A[1]:(0.999997496605) A[2]:(4.63333572898e-11) A[3]:(3.91590853168e-14)\n",
      " state (14)  A[0]:(2.2753815756e-06) A[1]:(0.999997735023) A[2]:(4.35397065623e-11) A[3]:(3.53870781961e-14)\n",
      " state (15)  A[0]:(2.17365618482e-06) A[1]:(0.999997854233) A[2]:(4.21345840163e-11) A[3]:(3.35263196057e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 252000 finished after 12 . Running score: 0.1. Policy_loss: -92050.6108413, Value_loss: 1.21408595398. Times trained:               12942. Times reached goal: 115.               Steps done: 2770754.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.987714767456) A[1]:(0.0043897908181) A[2]:(0.00444138562307) A[3]:(0.00345403817482)\n",
      " state (1)  A[0]:(0.0205404255539) A[1]:(0.00710711209103) A[2]:(0.00470796879381) A[3]:(0.967644512653)\n",
      " state (2)  A[0]:(0.00829353369772) A[1]:(0.00489422352985) A[2]:(0.00237915152684) A[3]:(0.984433114529)\n",
      " state (3)  A[0]:(0.046503406018) A[1]:(0.0128747858107) A[2]:(0.00234343204647) A[3]:(0.938278377056)\n",
      " state (4)  A[0]:(0.999694943428) A[1]:(0.000301481486531) A[2]:(1.96168855382e-07) A[3]:(3.37072151524e-06)\n",
      " state (5)  A[0]:(0.987395942211) A[1]:(0.0125932293013) A[2]:(5.13923225753e-07) A[3]:(1.0341066627e-05)\n",
      " state (6)  A[0]:(0.118267729878) A[1]:(0.881732106209) A[2]:(1.67890348735e-07) A[3]:(1.59897819429e-08)\n",
      " state (7)  A[0]:(0.00326433079317) A[1]:(0.99673563242) A[2]:(1.08356097428e-08) A[3]:(1.69045513698e-10)\n",
      " state (8)  A[0]:(2.59504013229e-05) A[1]:(0.99997407198) A[2]:(2.58881915904e-10) A[3]:(7.71634329783e-13)\n",
      " state (9)  A[0]:(1.40137399285e-06) A[1]:(0.999998569489) A[2]:(2.82479439145e-11) A[3]:(2.75171205944e-14)\n",
      " state (10)  A[0]:(4.71490864129e-07) A[1]:(0.999999523163) A[2]:(1.24847346003e-11) A[3]:(7.81264650548e-15)\n",
      " state (11)  A[0]:(3.05170857473e-07) A[1]:(0.999999701977) A[2]:(9.04186534673e-12) A[3]:(4.71234625282e-15)\n",
      " state (12)  A[0]:(2.50132956126e-07) A[1]:(0.999999761581) A[2]:(7.81453721177e-12) A[3]:(3.73806820458e-15)\n",
      " state (13)  A[0]:(2.26571671647e-07) A[1]:(0.999999761581) A[2]:(7.27427276401e-12) A[3]:(3.33113028332e-15)\n",
      " state (14)  A[0]:(2.15193182385e-07) A[1]:(0.999999761581) A[2]:(7.0121512763e-12) A[3]:(3.13713602147e-15)\n",
      " state (15)  A[0]:(2.09335013324e-07) A[1]:(0.999999761581) A[2]:(6.87859665061e-12) A[3]:(3.03797323906e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 253000 finished after 35 . Running score: 0.11. Policy_loss: -92050.6616908, Value_loss: 1.20432510696. Times trained:               12738. Times reached goal: 124.               Steps done: 2783492.\n",
      " state (0)  A[0]:(0.987486600876) A[1]:(0.00446215784177) A[2]:(0.0044935089536) A[3]:(0.00355772068724)\n",
      " state (1)  A[0]:(0.0241980906576) A[1]:(0.00732817174867) A[2]:(0.00545762106776) A[3]:(0.963016092777)\n",
      " state (2)  A[0]:(0.0165666379035) A[1]:(0.00616015307605) A[2]:(0.00415711198002) A[3]:(0.973116099834)\n",
      " state (3)  A[0]:(0.0186274796724) A[1]:(0.00643155025318) A[2]:(0.00366037455387) A[3]:(0.971280574799)\n",
      " state (4)  A[0]:(0.99936491251) A[1]:(0.000530255259946) A[2]:(5.9894732658e-06) A[3]:(9.88371975836e-05)\n",
      " state (5)  A[0]:(0.99994379282) A[1]:(5.5362135754e-05) A[2]:(1.11074328402e-07) A[3]:(7.22998152014e-07)\n",
      " state (6)  A[0]:(0.999936759472) A[1]:(6.24399472144e-05) A[2]:(9.68399831436e-08) A[3]:(7.00543807852e-07)\n",
      " state (7)  A[0]:(0.999900817871) A[1]:(9.79891556199e-05) A[2]:(1.18646333647e-07) A[3]:(1.09822315153e-06)\n",
      " state (8)  A[0]:(0.999728500843) A[1]:(0.000268717441941) A[2]:(1.69248679072e-07) A[3]:(2.58608179138e-06)\n",
      " state (9)  A[0]:(0.996614038944) A[1]:(0.00337747926824) A[2]:(2.9406319868e-07) A[3]:(8.17434101918e-06)\n",
      " state (10)  A[0]:(0.847236454487) A[1]:(0.152761265635) A[2]:(4.48933832331e-07) A[3]:(1.84092345989e-06)\n",
      " state (11)  A[0]:(0.266332864761) A[1]:(0.73366689682) A[2]:(2.18025519416e-07) A[3]:(4.72081040925e-08)\n",
      " state (12)  A[0]:(0.0838451683521) A[1]:(0.916154742241) A[2]:(9.27906000925e-08) A[3]:(5.71833957963e-09)\n",
      " state (13)  A[0]:(0.0353062786162) A[1]:(0.964693665504) A[2]:(4.84332502992e-08) A[3]:(1.70238567687e-09)\n",
      " state (14)  A[0]:(0.0164723526686) A[1]:(0.983527600765) A[2]:(2.70647433354e-08) A[3]:(6.62112142891e-10)\n",
      " state (15)  A[0]:(0.00791135523468) A[1]:(0.992088615894) A[2]:(1.5400562603e-08) A[3]:(2.78110978691e-10)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 254000 finished after 23 . Running score: 0.0. Policy_loss: -92050.6275037, Value_loss: 0.986481716308. Times trained:               15048. Times reached goal: 76.               Steps done: 2798540.\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0053,  0.0044,  0.0042]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9979e-01,  2.0339e-04,  1.5977e-07,  3.5446e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9979e-01,  2.0334e-04,  1.5976e-07,  3.5439e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9979e-01,  2.0330e-04,  1.5975e-07,  3.5433e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9979e-01,  2.0326e-04,  1.5975e-07,  3.5427e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0053,  0.0044,  0.0042]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9979e-01,  2.0320e-04,  1.5973e-07,  3.5417e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9979e-01,  2.0317e-04,  1.5973e-07,  3.5413e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0053,  0.0044,  0.0042]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0053,  0.0044,  0.0042]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0053,  0.0044,  0.0042]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0053,  0.0044,  0.0042]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0053,  0.0044,  0.0042]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0053,  0.0044,  0.0042]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9979e-01,  2.0305e-04,  1.5971e-07,  3.5395e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.9538e-03,  9.9405e-01,  2.2351e-08,  1.3889e-09]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.9567e-03,  9.9404e-01,  2.2360e-08,  1.3897e-09]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.9594e-03,  9.9404e-01,  2.2369e-08,  1.3904e-09]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.986137270927) A[1]:(0.00532238883898) A[2]:(0.00438416330144) A[3]:(0.00415619602427)\n",
      " state (1)  A[0]:(0.01203199476) A[1]:(0.00580948265269) A[2]:(0.00314663373865) A[3]:(0.979011893272)\n",
      " state (2)  A[0]:(0.00569395162165) A[1]:(0.00426716217771) A[2]:(0.00177609710954) A[3]:(0.98826277256)\n",
      " state (3)  A[0]:(0.155584082007) A[1]:(0.0216959211975) A[2]:(0.00242032832466) A[3]:(0.820299685001)\n",
      " state (4)  A[0]:(0.999793350697) A[1]:(0.000202941300813) A[2]:(1.59695730417e-07) A[3]:(3.53780001205e-06)\n",
      " state (5)  A[0]:(0.998966217041) A[1]:(0.00101865828037) A[2]:(2.98820793887e-07) A[3]:(1.48202088894e-05)\n",
      " state (6)  A[0]:(0.972129225731) A[1]:(0.0278187394142) A[2]:(6.23185712811e-07) A[3]:(5.13907434652e-05)\n",
      " state (7)  A[0]:(0.201102018356) A[1]:(0.798897325993) A[2]:(3.0814626939e-07) A[3]:(3.32340761133e-07)\n",
      " state (8)  A[0]:(0.00597846508026) A[1]:(0.99402153492) A[2]:(2.24263931869e-08) A[3]:(1.39549294342e-09)\n",
      " state (9)  A[0]:(5.75278099859e-05) A[1]:(0.999942481518) A[2]:(6.17570938832e-10) A[3]:(7.00648263366e-12)\n",
      " state (10)  A[0]:(2.36543792198e-06) A[1]:(0.999997615814) A[2]:(5.31508517754e-11) A[3]:(1.89153881902e-13)\n",
      " state (11)  A[0]:(5.43389035101e-07) A[1]:(0.999999463558) A[2]:(1.72741265736e-11) A[3]:(3.53756669682e-14)\n",
      " state (12)  A[0]:(2.70701804084e-07) A[1]:(0.999999701977) A[2]:(1.0160983166e-11) A[3]:(1.59286815779e-14)\n",
      " state (13)  A[0]:(1.88917269384e-07) A[1]:(0.999999821186) A[2]:(7.73090879508e-12) A[3]:(1.05394090401e-14)\n",
      " state (14)  A[0]:(1.55247860789e-07) A[1]:(0.999999821186) A[2]:(6.66144830269e-12) A[3]:(8.40999616274e-15)\n",
      " state (15)  A[0]:(1.39026795409e-07) A[1]:(0.999999880791) A[2]:(6.12734038855e-12) A[3]:(7.40700412111e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 255000 finished after 18 . Running score: 0.19. Policy_loss: -92050.61143, Value_loss: 1.840492005. Times trained:               15815. Times reached goal: 69.               Steps done: 2814355.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.985018372536) A[1]:(0.00589264044538) A[2]:(0.00446584820747) A[3]:(0.00462311599404)\n",
      " state (1)  A[0]:(0.0102255269885) A[1]:(0.00553245842457) A[2]:(0.00273370952345) A[3]:(0.98150831461)\n",
      " state (2)  A[0]:(0.00441249785945) A[1]:(0.0039265952073) A[2]:(0.0014609224163) A[3]:(0.99019998312)\n",
      " state (3)  A[0]:(0.0105105387047) A[1]:(0.00658518821001) A[2]:(0.00127557537053) A[3]:(0.981628715992)\n",
      " state (4)  A[0]:(0.999711334705) A[1]:(0.000280436652247) A[2]:(2.11872190903e-07) A[3]:(8.02297108748e-06)\n",
      " state (5)  A[0]:(0.998865544796) A[1]:(0.00110675732139) A[2]:(3.35395498041e-07) A[3]:(2.73714449577e-05)\n",
      " state (6)  A[0]:(0.983410656452) A[1]:(0.0164538174868) A[2]:(6.74951536439e-07) A[3]:(0.000134864661959)\n",
      " state (7)  A[0]:(0.536588311195) A[1]:(0.463401287794) A[2]:(8.5386847104e-07) A[3]:(9.53202925302e-06)\n",
      " state (8)  A[0]:(0.0957891717553) A[1]:(0.90421038866) A[2]:(3.14553631142e-07) A[3]:(1.22381692336e-07)\n",
      " state (9)  A[0]:(0.0215492174029) A[1]:(0.978450655937) A[2]:(1.11185904927e-07) A[3]:(1.54807207053e-08)\n",
      " state (10)  A[0]:(0.00196266151033) A[1]:(0.998037338257) A[2]:(1.80512405024e-08) A[3]:(1.15631926256e-09)\n",
      " state (11)  A[0]:(0.000182350617251) A[1]:(0.999817669392) A[2]:(2.96766167196e-09) A[3]:(9.19348694395e-11)\n",
      " state (12)  A[0]:(4.16006660089e-05) A[1]:(0.999958395958) A[2]:(9.71996816368e-10) A[3]:(1.88463515627e-11)\n",
      " state (13)  A[0]:(1.800663631e-05) A[1]:(0.999981999397) A[2]:(5.1770698839e-10) A[3]:(7.63790766745e-12)\n",
      " state (14)  A[0]:(1.09974807856e-05) A[1]:(0.999988973141) A[2]:(3.5755379213e-10) A[3]:(4.47901438161e-12)\n",
      " state (15)  A[0]:(8.13091082819e-06) A[1]:(0.999991893768) A[2]:(2.85098028519e-10) A[3]:(3.22817206994e-12)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 256000 finished after 10 . Running score: 0.1. Policy_loss: -92050.6262968, Value_loss: 1.20261865937. Times trained:               13185. Times reached goal: 99.               Steps done: 2827540.\n",
      " state (0)  A[0]:(0.987376391888) A[1]:(0.00468560215086) A[2]:(0.00415816903114) A[3]:(0.00377983483486)\n",
      " state (1)  A[0]:(0.0143046341836) A[1]:(0.00607611332089) A[2]:(0.00345967221074) A[3]:(0.976159572601)\n",
      " state (2)  A[0]:(0.00616617547348) A[1]:(0.00435511674732) A[2]:(0.00186262582429) A[3]:(0.987616062164)\n",
      " state (3)  A[0]:(0.00310113397427) A[1]:(0.0034580854699) A[2]:(0.0010229946347) A[3]:(0.992417812347)\n",
      " state (4)  A[0]:(0.999340116978) A[1]:(0.000613706710283) A[2]:(7.04530350504e-07) A[3]:(4.54982691735e-05)\n",
      " state (5)  A[0]:(0.997587502003) A[1]:(0.00236746203154) A[2]:(3.01052693885e-07) A[3]:(4.47436323157e-05)\n",
      " state (6)  A[0]:(0.690935730934) A[1]:(0.309041857719) A[2]:(8.83153120412e-07) A[3]:(2.15621694224e-05)\n",
      " state (7)  A[0]:(0.0341600254178) A[1]:(0.965839743614) A[2]:(1.76726771883e-07) A[3]:(4.49250698864e-08)\n",
      " state (8)  A[0]:(0.000834845181089) A[1]:(0.999165117741) A[2]:(1.14901874682e-08) A[3]:(8.49485259913e-10)\n",
      " state (9)  A[0]:(1.36082908284e-05) A[1]:(0.999986410141) A[2]:(5.11525544145e-10) A[3]:(1.24353053896e-11)\n",
      " state (10)  A[0]:(1.47030050357e-06) A[1]:(0.999998509884) A[2]:(9.68901556209e-11) A[3]:(1.21738003792e-12)\n",
      " state (11)  A[0]:(5.03899855175e-07) A[1]:(0.999999523163) A[2]:(4.37812806842e-11) A[3]:(3.93575390377e-13)\n",
      " state (12)  A[0]:(2.86993071086e-07) A[1]:(0.999999701977) A[2]:(2.8899826976e-11) A[3]:(2.16771736737e-13)\n",
      " state (13)  A[0]:(2.09257166262e-07) A[1]:(0.999999761581) A[2]:(2.29138583108e-11) A[3]:(1.54981848752e-13)\n",
      " state (14)  A[0]:(1.74211280068e-07) A[1]:(0.999999821186) A[2]:(2.00384205756e-11) A[3]:(1.27523751108e-13)\n",
      " state (15)  A[0]:(1.56336625423e-07) A[1]:(0.999999821186) A[2]:(1.85199650754e-11) A[3]:(1.13632017749e-13)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 257000 finished after 14 . Running score: 0.17. Policy_loss: -92050.6314127, Value_loss: 1.64011253133. Times trained:               13244. Times reached goal: 130.               Steps done: 2840784.\n",
      " state (0)  A[0]:(0.988332033157) A[1]:(0.00428578117862) A[2]:(0.00402500992641) A[3]:(0.00335716060363)\n",
      " state (1)  A[0]:(0.0143829844892) A[1]:(0.0060334103182) A[2]:(0.00345235876739) A[3]:(0.976131260395)\n",
      " state (2)  A[0]:(0.00461917882785) A[1]:(0.0038306300994) A[2]:(0.00147617340554) A[3]:(0.990074038506)\n",
      " state (3)  A[0]:(0.126493126154) A[1]:(0.0222067628056) A[2]:(0.00149132346269) A[3]:(0.849808752537)\n",
      " state (4)  A[0]:(0.999410033226) A[1]:(0.000575829762965) A[2]:(1.94887448401e-07) A[3]:(1.39400581247e-05)\n",
      " state (5)  A[0]:(0.986067473888) A[1]:(0.0138714211062) A[2]:(5.75136141379e-07) A[3]:(6.05277782597e-05)\n",
      " state (6)  A[0]:(0.248806491494) A[1]:(0.751192033291) A[2]:(6.79688128002e-07) A[3]:(8.28730776448e-07)\n",
      " state (7)  A[0]:(0.0207713972777) A[1]:(0.979228436947) A[2]:(1.37413138646e-07) A[3]:(1.947691608e-08)\n",
      " state (8)  A[0]:(0.000766232202295) A[1]:(0.999233782291) A[2]:(1.11612967757e-08) A[3]:(6.32317809224e-10)\n",
      " state (9)  A[0]:(1.29967129396e-05) A[1]:(0.999987006187) A[2]:(4.94217722302e-10) A[3]:(9.28391131333e-12)\n",
      " state (10)  A[0]:(1.63976608292e-06) A[1]:(0.99999833107) A[2]:(1.02687545966e-10) A[3]:(1.06409846212e-12)\n",
      " state (11)  A[0]:(7.42458325931e-07) A[1]:(0.999999284744) A[2]:(5.64449008367e-11) A[3]:(4.61726522901e-13)\n",
      " state (12)  A[0]:(5.35719152595e-07) A[1]:(0.999999463558) A[2]:(4.41616222757e-11) A[3]:(3.26770620111e-13)\n",
      " state (13)  A[0]:(4.61141013375e-07) A[1]:(0.999999523163) A[2]:(3.94700452877e-11) A[3]:(2.78624020557e-13)\n",
      " state (14)  A[0]:(4.28261984098e-07) A[1]:(0.999999582767) A[2]:(3.73481107152e-11) A[3]:(2.57502325669e-13)\n",
      " state (15)  A[0]:(4.12317660903e-07) A[1]:(0.999999582767) A[2]:(3.63071100962e-11) A[3]:(2.47280738637e-13)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 258000 finished after 8 . Running score: 0.09. Policy_loss: -92050.6118007, Value_loss: 1.22270231264. Times trained:               12650. Times reached goal: 113.               Steps done: 2853434.\n",
      " state (0)  A[0]:(0.988122284412) A[1]:(0.00436650728807) A[2]:(0.00409321766347) A[3]:(0.00341799180023)\n",
      " state (1)  A[0]:(0.0204145908356) A[1]:(0.00681722769514) A[2]:(0.00448900274932) A[3]:(0.968279182911)\n",
      " state (2)  A[0]:(0.0086106620729) A[1]:(0.00478387065232) A[2]:(0.00239809183404) A[3]:(0.984207391739)\n",
      " state (3)  A[0]:(0.0249245837331) A[1]:(0.00849020294845) A[2]:(0.00193520949688) A[3]:(0.9646499753)\n",
      " state (4)  A[0]:(0.999850869179) A[1]:(0.000145600541146) A[2]:(1.10488457494e-07) A[3]:(3.41514987667e-06)\n",
      " state (5)  A[0]:(0.999229073524) A[1]:(0.000756488239858) A[2]:(1.80991762022e-07) A[3]:(1.42730068546e-05)\n",
      " state (6)  A[0]:(0.968137323856) A[1]:(0.031811915338) A[2]:(5.31677017079e-07) A[3]:(5.0216094678e-05)\n",
      " state (7)  A[0]:(0.21110920608) A[1]:(0.788889765739) A[2]:(6.33392119198e-07) A[3]:(4.11089104091e-07)\n",
      " state (8)  A[0]:(0.014355847612) A[1]:(0.985644042492) A[2]:(1.23850171008e-07) A[3]:(9.61644097686e-09)\n",
      " state (9)  A[0]:(0.000246747513302) A[1]:(0.999753236771) A[2]:(6.30661389778e-09) A[3]:(1.41727449221e-10)\n",
      " state (10)  A[0]:(9.75517832558e-06) A[1]:(0.999990224838) A[2]:(5.8201515829e-10) A[3]:(4.95812592372e-12)\n",
      " state (11)  A[0]:(2.06843833439e-06) A[1]:(0.999997913837) A[2]:(1.86420323622e-10) A[3]:(9.78189968262e-13)\n",
      " state (12)  A[0]:(9.95230379885e-07) A[1]:(0.999998986721) A[2]:(1.09250990199e-10) A[3]:(4.53020406561e-13)\n",
      " state (13)  A[0]:(6.84081214786e-07) A[1]:(0.999999344349) A[2]:(8.31881785679e-11) A[3]:(3.04926629736e-13)\n",
      " state (14)  A[0]:(5.57829025638e-07) A[1]:(0.999999463558) A[2]:(7.17715401e-11) A[3]:(2.45706666818e-13)\n",
      " state (15)  A[0]:(4.9732534535e-07) A[1]:(0.999999523163) A[2]:(6.60789548079e-11) A[3]:(2.17547076816e-13)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 259000 finished after 10 . Running score: 0.14. Policy_loss: -92050.6114363, Value_loss: 1.21098182217. Times trained:               12867. Times reached goal: 141.               Steps done: 2866301.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9877,  0.0047,  0.0043,  0.0034]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9877,  0.0047,  0.0043,  0.0034]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9996e-01,  4.4033e-05,  1.2729e-07,  6.4038e-07]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9996e-01,  4.4020e-05,  1.2717e-07,  6.3985e-07]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9082e-01,  9.1554e-03,  3.1828e-07,  2.2335e-05]])\n",
      "On state=8, selected action=0\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.987680912018) A[1]:(0.00466937106103) A[2]:(0.00425275787711) A[3]:(0.00339696533047)\n",
      " state (1)  A[0]:(0.0539098381996) A[1]:(0.0104326158762) A[2]:(0.0093493424356) A[3]:(0.926308214664)\n",
      " state (2)  A[0]:(0.036774776876) A[1]:(0.00918949116021) A[2]:(0.00766439409927) A[3]:(0.94637131691)\n",
      " state (3)  A[0]:(0.81493806839) A[1]:(0.0173434708267) A[2]:(0.00331098935567) A[3]:(0.164407476783)\n",
      " state (4)  A[0]:(0.999955236912) A[1]:(4.39902432845e-05) A[2]:(1.26883278995e-07) A[3]:(6.3856413135e-07)\n",
      " state (5)  A[0]:(0.999958574772) A[1]:(4.09051281167e-05) A[2]:(6.38292547706e-08) A[3]:(4.31888480534e-07)\n",
      " state (6)  A[0]:(0.999917685986) A[1]:(8.1213991507e-05) A[2]:(7.94302081886e-08) A[3]:(1.00619365639e-06)\n",
      " state (7)  A[0]:(0.99964004755) A[1]:(0.000355518684955) A[2]:(1.29389334802e-07) A[3]:(4.28517887485e-06)\n",
      " state (8)  A[0]:(0.990755736828) A[1]:(0.00922160875052) A[2]:(3.18952174894e-07) A[3]:(2.23538991122e-05)\n",
      " state (9)  A[0]:(0.599160373211) A[1]:(0.40083694458) A[2]:(9.18932755667e-07) A[3]:(1.75700222371e-06)\n",
      " state (10)  A[0]:(0.0744199082255) A[1]:(0.925579607487) A[2]:(4.4964011181e-07) A[3]:(3.89572001325e-08)\n",
      " state (11)  A[0]:(0.00689482642338) A[1]:(0.993105053902) A[2]:(9.84732437814e-08) A[3]:(2.59274979264e-09)\n",
      " state (12)  A[0]:(0.000726758269593) A[1]:(0.999273240566) A[2]:(2.08953938596e-08) A[3]:(2.37422276284e-10)\n",
      " state (13)  A[0]:(0.000124920014059) A[1]:(0.999875068665) A[2]:(6.14839379409e-09) A[3]:(3.65145275127e-11)\n",
      " state (14)  A[0]:(3.45856351487e-05) A[1]:(0.999965429306) A[2]:(2.51865750478e-09) A[3]:(9.25985416816e-12)\n",
      " state (15)  A[0]:(1.38465857162e-05) A[1]:(0.999986171722) A[2]:(1.33349886688e-09) A[3]:(3.47239164505e-12)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 260000 finished after 5 . Running score: 0.0. Policy_loss: -92050.6114043, Value_loss: 0.979304318686. Times trained:               14576. Times reached goal: 95.               Steps done: 2880877.\n",
      " state (0)  A[0]:(0.983847379684) A[1]:(0.00669598253444) A[2]:(0.00457771122456) A[3]:(0.00487890280783)\n",
      " state (1)  A[0]:(0.0158488675952) A[1]:(0.00712121790275) A[2]:(0.00420743087307) A[3]:(0.972822487354)\n",
      " state (2)  A[0]:(0.00648973835632) A[1]:(0.00504468381405) A[2]:(0.00205264310353) A[3]:(0.98641294241)\n",
      " state (3)  A[0]:(0.999842584133) A[1]:(0.000154853609274) A[2]:(1.45570368204e-07) A[3]:(2.41378666033e-06)\n",
      " state (4)  A[0]:(0.999300479889) A[1]:(0.000690811895765) A[2]:(2.46897570833e-07) A[3]:(8.46168768476e-06)\n",
      " state (5)  A[0]:(0.984710514545) A[1]:(0.0152476392686) A[2]:(7.61049989251e-07) A[3]:(4.10584289057e-05)\n",
      " state (6)  A[0]:(0.342835277319) A[1]:(0.657161414623) A[2]:(1.76474350155e-06) A[3]:(1.51048743646e-06)\n",
      " state (7)  A[0]:(0.0239841807634) A[1]:(0.976015329361) A[2]:(4.84998111006e-07) A[3]:(2.18080700165e-08)\n",
      " state (8)  A[0]:(0.000578416569624) A[1]:(0.999421536922) A[2]:(3.71973705171e-08) A[3]:(4.15088935446e-10)\n",
      " state (9)  A[0]:(4.90499223815e-06) A[1]:(0.999995112419) A[2]:(1.29122135206e-09) A[3]:(2.95806695111e-12)\n",
      " state (10)  A[0]:(4.67492526468e-07) A[1]:(0.999999523163) A[2]:(2.46977549523e-10) A[3]:(2.5729806198e-13)\n",
      " state (11)  A[0]:(1.97446212269e-07) A[1]:(0.999999821186) A[2]:(1.3483401895e-10) A[3]:(1.04768610779e-13)\n",
      " state (12)  A[0]:(1.40311627206e-07) A[1]:(0.999999880791) A[2]:(1.06177171222e-10) A[3]:(7.32646839789e-14)\n",
      " state (13)  A[0]:(1.2052277043e-07) A[1]:(0.999999880791) A[2]:(9.55119386359e-11) A[3]:(6.24406329051e-14)\n",
      " state (14)  A[0]:(1.11998545549e-07) A[1]:(0.999999880791) A[2]:(9.07769276415e-11) A[3]:(5.7789351375e-14)\n",
      " state (15)  A[0]:(1.07918317838e-07) A[1]:(0.999999880791) A[2]:(8.8481479088e-11) A[3]:(5.55625458024e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 261000 finished after 7 . Running score: 0.22. Policy_loss: -92050.6109666, Value_loss: 1.20805395156. Times trained:               12954. Times reached goal: 106.               Steps done: 2893831.\n",
      " state (0)  A[0]:(0.985855460167) A[1]:(0.00575428362936) A[2]:(0.00444313045591) A[3]:(0.0039471485652)\n",
      " state (1)  A[0]:(0.0319116227329) A[1]:(0.00968597363681) A[2]:(0.00733384303749) A[3]:(0.951068580151)\n",
      " state (2)  A[0]:(0.0260930135846) A[1]:(0.00928242504597) A[2]:(0.00493742711842) A[3]:(0.959687113762)\n",
      " state (3)  A[0]:(0.999914586544) A[1]:(8.46152397571e-05) A[2]:(1.21289389199e-07) A[3]:(6.5034095087e-07)\n",
      " state (4)  A[0]:(0.999710857868) A[1]:(0.000286956579657) A[2]:(1.55894724685e-07) A[3]:(2.05623427973e-06)\n",
      " state (5)  A[0]:(0.992444634438) A[1]:(0.00753418449312) A[2]:(5.36186860245e-07) A[3]:(2.06502900255e-05)\n",
      " state (6)  A[0]:(0.326055884361) A[1]:(0.67394143343) A[2]:(1.86327281426e-06) A[3]:(7.93808396793e-07)\n",
      " state (7)  A[0]:(0.0110817151144) A[1]:(0.988917946815) A[2]:(3.59114125104e-07) A[3]:(4.10632683412e-09)\n",
      " state (8)  A[0]:(6.8247965828e-05) A[1]:(0.999931752682) A[2]:(1.11330846764e-08) A[3]:(1.81998791038e-11)\n",
      " state (9)  A[0]:(1.06938023237e-06) A[1]:(0.999998927116) A[2]:(6.34353680695e-10) A[3]:(2.25922850306e-13)\n",
      " state (10)  A[0]:(2.01586416892e-07) A[1]:(0.999999821186) A[2]:(2.01502689534e-10) A[3]:(3.84277807871e-14)\n",
      " state (11)  A[0]:(1.03820063657e-07) A[1]:(0.999999880791) A[2]:(1.28020136159e-10) A[3]:(1.89239374632e-14)\n",
      " state (12)  A[0]:(7.68104087001e-08) A[1]:(0.999999940395) A[2]:(1.04352165797e-10) A[3]:(1.36924434464e-14)\n",
      " state (13)  A[0]:(6.61158949811e-08) A[1]:(0.999999940395) A[2]:(9.43570777068e-11) A[3]:(1.1643522416e-14)\n",
      " state (14)  A[0]:(6.11321624433e-08) A[1]:(0.999999940395) A[2]:(8.95856305805e-11) A[3]:(1.06897108014e-14)\n",
      " state (15)  A[0]:(5.86146668979e-08) A[1]:(0.999999940395) A[2]:(8.71748645492e-11) A[3]:(1.02048640602e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 262000 finished after 11 . Running score: 0.13. Policy_loss: -92050.6111104, Value_loss: 1.2179132817. Times trained:               12977. Times reached goal: 131.               Steps done: 2906808.\n",
      " state (0)  A[0]:(0.989560902119) A[1]:(0.00389558332972) A[2]:(0.00378744420595) A[3]:(0.00275606033392)\n",
      " state (1)  A[0]:(0.0369342155755) A[1]:(0.00961808115244) A[2]:(0.00805373489857) A[3]:(0.945393979549)\n",
      " state (2)  A[0]:(0.930013239384) A[1]:(0.0126484679058) A[2]:(0.00145519326907) A[3]:(0.0558830685914)\n",
      " state (3)  A[0]:(0.999951899052) A[1]:(4.77572648379e-05) A[2]:(7.07100369368e-08) A[3]:(2.57325865505e-07)\n",
      " state (4)  A[0]:(0.99986743927) A[1]:(0.000131637440063) A[2]:(1.1490453744e-07) A[3]:(8.38029677652e-07)\n",
      " state (5)  A[0]:(0.998861312866) A[1]:(0.00113363133278) A[2]:(2.92189042739e-07) A[3]:(4.76375043945e-06)\n",
      " state (6)  A[0]:(0.851545214653) A[1]:(0.148450091481) A[2]:(1.72471845872e-06) A[3]:(2.94740971185e-06)\n",
      " state (7)  A[0]:(0.0540621057153) A[1]:(0.945936918259) A[2]:(9.38023333674e-07) A[3]:(1.57516435451e-08)\n",
      " state (8)  A[0]:(0.00156803638674) A[1]:(0.998431861401) A[2]:(8.50442845035e-08) A[3]:(2.99221258881e-10)\n",
      " state (9)  A[0]:(1.84676009667e-05) A[1]:(0.99998152256) A[2]:(3.46965967069e-09) A[3]:(2.49807379643e-12)\n",
      " state (10)  A[0]:(1.71280714767e-06) A[1]:(0.999998271465) A[2]:(6.28397056612e-10) A[3]:(1.89322570208e-13)\n",
      " state (11)  A[0]:(6.32693456737e-07) A[1]:(0.999999344349) A[2]:(3.08125691628e-10) A[3]:(6.38628418811e-14)\n",
      " state (12)  A[0]:(4.02851611625e-07) A[1]:(0.999999582767) A[2]:(2.23490226325e-10) A[3]:(3.89221464846e-14)\n",
      " state (13)  A[0]:(3.22023879562e-07) A[1]:(0.999999701977) A[2]:(1.90811644263e-10) A[3]:(3.04058298992e-14)\n",
      " state (14)  A[0]:(2.86426796947e-07) A[1]:(0.999999701977) A[2]:(1.75822814641e-10) A[3]:(2.67007197466e-14)\n",
      " state (15)  A[0]:(2.68963532335e-07) A[1]:(0.999999701977) A[2]:(1.68377825571e-10) A[3]:(2.48869545442e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 263000 finished after 15 . Running score: 0.09. Policy_loss: -92050.6111291, Value_loss: 1.42641038271. Times trained:               13194. Times reached goal: 115.               Steps done: 2920002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.989539861679) A[1]:(0.00400108518079) A[2]:(0.00374386599287) A[3]:(0.00271519715898)\n",
      " state (1)  A[0]:(0.0499725267291) A[1]:(0.011240365915) A[2]:(0.0099096223712) A[3]:(0.928877472878)\n",
      " state (2)  A[0]:(0.996287703514) A[1]:(0.00231086136773) A[2]:(9.22310355236e-05) A[3]:(0.00130918272771)\n",
      " state (3)  A[0]:(0.999955058098) A[1]:(4.46955964435e-05) A[2]:(5.16296267961e-08) A[3]:(1.79123603061e-07)\n",
      " state (4)  A[0]:(0.999797642231) A[1]:(0.000201383430976) A[2]:(9.1812260905e-08) A[3]:(8.66295351898e-07)\n",
      " state (5)  A[0]:(0.98570650816) A[1]:(0.0142897162586) A[2]:(4.85166140152e-07) A[3]:(3.28320084009e-06)\n",
      " state (6)  A[0]:(0.167348921299) A[1]:(0.832649290562) A[2]:(1.76080948222e-06) A[3]:(2.34301626989e-08)\n",
      " state (7)  A[0]:(0.0151338325813) A[1]:(0.984865665436) A[2]:(5.05365107983e-07) A[3]:(1.41807832144e-09)\n",
      " state (8)  A[0]:(0.000457054411527) A[1]:(0.999542891979) A[2]:(4.38730971553e-08) A[3]:(3.5526616371e-11)\n",
      " state (9)  A[0]:(8.65043602971e-06) A[1]:(0.999991357327) A[2]:(2.62396748774e-09) A[3]:(5.01714772158e-13)\n",
      " state (10)  A[0]:(1.31874105591e-06) A[1]:(0.999998688698) A[2]:(6.92731205731e-10) A[3]:(6.54616064046e-14)\n",
      " state (11)  A[0]:(6.8716150281e-07) A[1]:(0.999999284744) A[2]:(4.37888947324e-10) A[3]:(3.21940587524e-14)\n",
      " state (12)  A[0]:(5.41301517387e-07) A[1]:(0.999999463558) A[2]:(3.71003366917e-10) A[3]:(2.47889291152e-14)\n",
      " state (13)  A[0]:(4.90567572342e-07) A[1]:(0.999999523163) A[2]:(3.47011086532e-10) A[3]:(2.2236044657e-14)\n",
      " state (14)  A[0]:(4.6931464226e-07) A[1]:(0.999999523163) A[2]:(3.3713043468e-10) A[3]:(2.11621695102e-14)\n",
      " state (15)  A[0]:(4.59701681166e-07) A[1]:(0.999999523163) A[2]:(3.32966432204e-10) A[3]:(2.06676903104e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 264000 finished after 9 . Running score: 0.14. Policy_loss: -92050.610854, Value_loss: 1.84864991868. Times trained:               13184. Times reached goal: 120.               Steps done: 2933186.\n",
      "action_dist \n",
      "tensor([[ 0.9898,  0.0037,  0.0037,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9898,  0.0037,  0.0037,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9898,  0.0037,  0.0037,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9898,  0.0037,  0.0037,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9898,  0.0037,  0.0037,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9898,  0.0037,  0.0037,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9898,  0.0037,  0.0037,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9959e-01,  4.0497e-04,  9.0642e-08,  1.7514e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9898,  0.0037,  0.0037,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9898,  0.0037,  0.0037,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9959e-01,  4.0496e-04,  9.0645e-08,  1.7513e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9959e-01,  4.0496e-04,  9.0646e-08,  1.7513e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9959e-01,  4.0496e-04,  9.0647e-08,  1.7514e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9959e-01,  4.0496e-04,  9.0649e-08,  1.7514e-06]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.4685e-06,  9.9999e-01,  2.1246e-09,  3.3348e-13]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.4687e-06,  9.9999e-01,  2.1247e-09,  3.3347e-13]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.4688e-06,  9.9999e-01,  2.1249e-09,  3.3346e-13]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.989793896675) A[1]:(0.00374403246678) A[2]:(0.00373159907758) A[3]:(0.00273045431823)\n",
      " state (1)  A[0]:(0.0298459958285) A[1]:(0.00868676509708) A[2]:(0.00677785556763) A[3]:(0.954689383507)\n",
      " state (2)  A[0]:(0.999705851078) A[1]:(0.000279634463368) A[2]:(1.71104443325e-06) A[3]:(1.27878020066e-05)\n",
      " state (3)  A[0]:(0.999934315681) A[1]:(6.53020033496e-05) A[2]:(5.35814734803e-08) A[3]:(3.25863425132e-07)\n",
      " state (4)  A[0]:(0.999593436718) A[1]:(0.000404696533224) A[2]:(9.06384656219e-08) A[3]:(1.7503049321e-06)\n",
      " state (5)  A[0]:(0.946448981762) A[1]:(0.0535474941134) A[2]:(5.77435230298e-07) A[3]:(2.91488686344e-06)\n",
      " state (6)  A[0]:(0.0803346037865) A[1]:(0.91966432333) A[2]:(1.03957097508e-06) A[3]:(6.56652821007e-09)\n",
      " state (7)  A[0]:(0.00143249763642) A[1]:(0.998567402363) A[2]:(9.57686481229e-08) A[3]:(9.81437986436e-11)\n",
      " state (8)  A[0]:(5.46899491383e-06) A[1]:(0.999994516373) A[2]:(2.12503747932e-09) A[3]:(3.33458710948e-13)\n",
      " state (9)  A[0]:(3.13983662181e-07) A[1]:(0.999999701977) A[2]:(2.97019936424e-10) A[3]:(1.71330743375e-14)\n",
      " state (10)  A[0]:(1.31441268536e-07) A[1]:(0.999999880791) A[2]:(1.6410012238e-10) A[3]:(6.91597488642e-15)\n",
      " state (11)  A[0]:(9.96038806989e-08) A[1]:(0.999999880791) A[2]:(1.36562428033e-10) A[3]:(5.18417494466e-15)\n",
      " state (12)  A[0]:(8.97519996101e-08) A[1]:(0.999999880791) A[2]:(1.27916413573e-10) A[3]:(4.65639845611e-15)\n",
      " state (13)  A[0]:(8.58033430973e-08) A[1]:(0.999999940395) A[2]:(1.24661378442e-10) A[3]:(4.4484916781e-15)\n",
      " state (14)  A[0]:(8.39992750912e-08) A[1]:(0.999999940395) A[2]:(1.23385177075e-10) A[3]:(4.35599779785e-15)\n",
      " state (15)  A[0]:(8.31241209198e-08) A[1]:(0.999999940395) A[2]:(1.22955493009e-10) A[3]:(4.31307355621e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 265000 finished after 17 . Running score: 0.14. Policy_loss: -92050.6109444, Value_loss: 1.19798006764. Times trained:               12990. Times reached goal: 130.               Steps done: 2946176.\n",
      " state (0)  A[0]:(0.99079477787) A[1]:(0.00322142243385) A[2]:(0.00357310497202) A[3]:(0.00241069146432)\n",
      " state (1)  A[0]:(0.0347790829837) A[1]:(0.00888195633888) A[2]:(0.00742063950747) A[3]:(0.94891834259)\n",
      " state (2)  A[0]:(0.999481499195) A[1]:(0.000465154793346) A[2]:(5.70272322875e-06) A[3]:(4.7637931857e-05)\n",
      " state (3)  A[0]:(0.99996304512) A[1]:(3.67440516129e-05) A[2]:(4.29064535012e-08) A[3]:(1.64414814208e-07)\n",
      " state (4)  A[0]:(0.999918162823) A[1]:(8.1373276771e-05) A[2]:(5.41547393595e-08) A[3]:(4.37816481735e-07)\n",
      " state (5)  A[0]:(0.9992390275) A[1]:(0.000759028305765) A[2]:(1.08772603369e-07) A[3]:(1.85616352155e-06)\n",
      " state (6)  A[0]:(0.829531788826) A[1]:(0.170466586947) A[2]:(1.31993374453e-06) A[3]:(2.7953086601e-07)\n",
      " state (7)  A[0]:(0.0781911611557) A[1]:(0.921807467937) A[2]:(1.34439119392e-06) A[3]:(2.84691870078e-09)\n",
      " state (8)  A[0]:(0.00221878429875) A[1]:(0.997781038284) A[2]:(1.5934330122e-07) A[3]:(8.05478531096e-11)\n",
      " state (9)  A[0]:(3.07749542117e-05) A[1]:(0.999969244003) A[2]:(9.29158971985e-09) A[3]:(1.04679773964e-12)\n",
      " state (10)  A[0]:(3.34121455126e-06) A[1]:(0.99999666214) A[2]:(2.11396100624e-09) A[3]:(1.05451515846e-13)\n",
      " state (11)  A[0]:(1.50112794017e-06) A[1]:(0.999998509884) A[2]:(1.24963128734e-09) A[3]:(4.60511349106e-14)\n",
      " state (12)  A[0]:(1.11063127406e-06) A[1]:(0.999998867512) A[2]:(1.03136554852e-09) A[3]:(3.37917526263e-14)\n",
      " state (13)  A[0]:(9.74809836407e-07) A[1]:(0.999999046326) A[2]:(9.52773748786e-10) A[3]:(2.96192615519e-14)\n",
      " state (14)  A[0]:(9.1437118499e-07) A[1]:(0.99999910593) A[2]:(9.1856938722e-10) A[3]:(2.78083813436e-14)\n",
      " state (15)  A[0]:(8.83440748112e-07) A[1]:(0.99999910593) A[2]:(9.0193047475e-10) A[3]:(2.69081716679e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 266000 finished after 9 . Running score: 0.16. Policy_loss: -92050.6111193, Value_loss: 1.63727357474. Times trained:               13274. Times reached goal: 136.               Steps done: 2959450.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.990910351276) A[1]:(0.00317943375558) A[2]:(0.00350826582871) A[3]:(0.00240195333026)\n",
      " state (1)  A[0]:(0.0323124714196) A[1]:(0.00854131672531) A[2]:(0.00694199558347) A[3]:(0.952204227448)\n",
      " state (2)  A[0]:(0.999544560909) A[1]:(0.000417710340116) A[2]:(7.14906809662e-06) A[3]:(3.05989997287e-05)\n",
      " state (3)  A[0]:(0.999962449074) A[1]:(3.73665752704e-05) A[2]:(8.18395093916e-08) A[3]:(1.23892704096e-07)\n",
      " state (4)  A[0]:(0.999918162823) A[1]:(8.14131708466e-05) A[2]:(1.0412112772e-07) A[3]:(3.27337005501e-07)\n",
      " state (5)  A[0]:(0.999307334423) A[1]:(0.000691062421538) A[2]:(2.17434887873e-07) A[3]:(1.37173651638e-06)\n",
      " state (6)  A[0]:(0.83987480402) A[1]:(0.160120815039) A[2]:(4.1184366637e-06) A[3]:(2.47567328415e-07)\n",
      " state (7)  A[0]:(0.0618889331818) A[1]:(0.938105404377) A[2]:(5.67755660086e-06) A[3]:(1.72818548361e-09)\n",
      " state (8)  A[0]:(0.000979606760666) A[1]:(0.999019861221) A[2]:(5.3833940683e-07) A[3]:(2.70704361705e-11)\n",
      " state (9)  A[0]:(1.39096637213e-05) A[1]:(0.999986052513) A[2]:(3.82598912552e-08) A[3]:(3.57403559872e-13)\n",
      " state (10)  A[0]:(2.06333947972e-06) A[1]:(0.999997913837) A[2]:(1.17165157576e-08) A[3]:(4.97801059813e-14)\n",
      " state (11)  A[0]:(1.0421939578e-06) A[1]:(0.999998927116) A[2]:(7.72603492294e-09) A[3]:(2.45496999059e-14)\n",
      " state (12)  A[0]:(7.96550523319e-07) A[1]:(0.99999922514) A[2]:(6.59002807879e-09) A[3]:(1.86166118871e-14)\n",
      " state (13)  A[0]:(7.0614970582e-07) A[1]:(0.999999284744) A[2]:(6.15506001722e-09) A[3]:(1.64716822132e-14)\n",
      " state (14)  A[0]:(6.65376035158e-07) A[1]:(0.999999344349) A[2]:(5.9618785464e-09) A[3]:(1.55239692357e-14)\n",
      " state (15)  A[0]:(6.4486221163e-07) A[1]:(0.999999344349) A[2]:(5.86907855649e-09) A[3]:(1.50589210427e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 267000 finished after 12 . Running score: 0.14. Policy_loss: -92050.6111871, Value_loss: 1.19976317742. Times trained:               12839. Times reached goal: 128.               Steps done: 2972289.\n",
      " state (0)  A[0]:(0.990825653076) A[1]:(0.003191371914) A[2]:(0.00349439773709) A[3]:(0.00248858937994)\n",
      " state (1)  A[0]:(0.0278043709695) A[1]:(0.00783720891923) A[2]:(0.0062209572643) A[3]:(0.958137452602)\n",
      " state (2)  A[0]:(0.997192084789) A[1]:(0.00187978660688) A[2]:(0.000104910235677) A[3]:(0.000823228096124)\n",
      " state (3)  A[0]:(0.999966979027) A[1]:(3.27798479702e-05) A[2]:(1.34232692517e-07) A[3]:(1.04318836236e-07)\n",
      " state (4)  A[0]:(0.999948501587) A[1]:(5.11464058945e-05) A[2]:(1.45260301565e-07) A[3]:(1.94093189521e-07)\n",
      " state (5)  A[0]:(0.99985742569) A[1]:(0.000141808093758) A[2]:(2.01190516691e-07) A[3]:(5.4770572433e-07)\n",
      " state (6)  A[0]:(0.996500551701) A[1]:(0.00349680474028) A[2]:(7.53386075303e-07) A[3]:(1.91987624021e-06)\n",
      " state (7)  A[0]:(0.418864786625) A[1]:(0.581113636494) A[2]:(2.15152140299e-05) A[3]:(3.03347285069e-08)\n",
      " state (8)  A[0]:(0.00952164735645) A[1]:(0.990472018719) A[2]:(6.3633233367e-06) A[3]:(2.47175446777e-10)\n",
      " state (9)  A[0]:(9.67072701314e-05) A[1]:(0.999902844429) A[2]:(4.52162510101e-07) A[3]:(2.54489983774e-12)\n",
      " state (10)  A[0]:(7.00654982211e-06) A[1]:(0.999992907047) A[2]:(9.72259428522e-08) A[3]:(1.776558501e-13)\n",
      " state (11)  A[0]:(2.27448549595e-06) A[1]:(0.999997675419) A[2]:(5.06803274902e-08) A[3]:(5.62107259068e-14)\n",
      " state (12)  A[0]:(1.36219671276e-06) A[1]:(0.999998629093) A[2]:(3.78595110817e-08) A[3]:(3.32615100013e-14)\n",
      " state (13)  A[0]:(1.05388085103e-06) A[1]:(0.999998927116) A[2]:(3.28316254183e-08) A[3]:(2.56162736287e-14)\n",
      " state (14)  A[0]:(9.19001877264e-07) A[1]:(0.999999046326) A[2]:(3.05028535763e-08) A[3]:(2.23204226911e-14)\n",
      " state (15)  A[0]:(8.5168642272e-07) A[1]:(0.99999910593) A[2]:(2.93333553003e-08) A[3]:(2.07055594594e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 268000 finished after 11 . Running score: 0.15. Policy_loss: -92050.6151542, Value_loss: 1.61951952584. Times trained:               12803. Times reached goal: 126.               Steps done: 2985092.\n",
      " state (0)  A[0]:(0.991331160069) A[1]:(0.00314598041587) A[2]:(0.00324825849384) A[3]:(0.00227462104522)\n",
      " state (1)  A[0]:(0.0336976200342) A[1]:(0.00902657862753) A[2]:(0.00699532730505) A[3]:(0.950280487537)\n",
      " state (2)  A[0]:(0.99995803833) A[1]:(4.15053073084e-05) A[2]:(3.62432302836e-07) A[3]:(7.355952647e-08)\n",
      " state (3)  A[0]:(0.999941289425) A[1]:(5.83496403124e-05) A[2]:(2.60389668938e-07) A[3]:(9.18867684163e-08)\n",
      " state (4)  A[0]:(0.999730885029) A[1]:(0.000268300413154) A[2]:(4.50679266351e-07) A[3]:(3.88644679106e-07)\n",
      " state (5)  A[0]:(0.977930128574) A[1]:(0.0220641233027) A[2]:(5.19163495483e-06) A[3]:(5.63646267437e-07)\n",
      " state (6)  A[0]:(0.172826275229) A[1]:(0.827091395855) A[2]:(8.2293387095e-05) A[3]:(2.66289390538e-09)\n",
      " state (7)  A[0]:(0.0149421663955) A[1]:(0.985022723675) A[2]:(3.51362032234e-05) A[3]:(1.80393547322e-10)\n",
      " state (8)  A[0]:(0.000419561023591) A[1]:(0.999576032162) A[2]:(4.42388727606e-06) A[3]:(4.93043626759e-12)\n",
      " state (9)  A[0]:(1.27478833747e-05) A[1]:(0.999986708164) A[2]:(5.3615900697e-07) A[3]:(1.28376232171e-13)\n",
      " state (10)  A[0]:(2.91309834211e-06) A[1]:(0.999996840954) A[2]:(2.20736708911e-07) A[3]:(2.70287840774e-14)\n",
      " state (11)  A[0]:(1.85062606306e-06) A[1]:(0.999997973442) A[2]:(1.69230048641e-07) A[3]:(1.67518383605e-14)\n",
      " state (12)  A[0]:(1.60599017818e-06) A[1]:(0.999998211861) A[2]:(1.56638265025e-07) A[3]:(1.44527029504e-14)\n",
      " state (13)  A[0]:(1.52915470153e-06) A[1]:(0.99999833107) A[2]:(1.53178405071e-07) A[3]:(1.37579207873e-14)\n",
      " state (14)  A[0]:(1.50101470808e-06) A[1]:(0.99999833107) A[2]:(1.52420795985e-07) A[3]:(1.35243074064e-14)\n",
      " state (15)  A[0]:(1.49003972183e-06) A[1]:(0.99999833107) A[2]:(1.52578309098e-07) A[3]:(1.34513228125e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 269000 finished after 13 . Running score: 0.11. Policy_loss: -92050.6112978, Value_loss: 1.41200575. Times trained:               13003. Times reached goal: 123.               Steps done: 2998095.\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0032,  0.0032,  0.0023]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9965e-01,  3.4888e-04,  6.5686e-07,  2.9922e-07]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9965e-01,  3.4876e-04,  6.5691e-07,  2.9915e-07]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.4467e-04,  9.9955e-01,  8.0097e-06,  4.1278e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.4643e-04,  9.9955e-01,  8.0346e-06,  4.1443e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.4803e-04,  9.9954e-01,  8.0573e-06,  4.1594e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5563e-05,  9.9998e-01,  9.8471e-07,  1.2620e-13]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.4978e-04,  9.9954e-01,  8.0820e-06,  4.1758e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5598e-05,  9.9998e-01,  9.8656e-07,  1.2649e-13]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8160e-06,  1.0000e+00,  2.6297e-07,  1.3181e-14]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.991395294666) A[1]:(0.00315729319118) A[2]:(0.0031694797799) A[3]:(0.0022779093124)\n",
      " state (1)  A[0]:(0.035055488348) A[1]:(0.00916255638003) A[2]:(0.0071095013991) A[3]:(0.948672473431)\n",
      " state (2)  A[0]:(0.999934732914) A[1]:(6.42118611722e-05) A[2]:(9.30238854835e-07) A[3]:(1.46995930095e-07)\n",
      " state (3)  A[0]:(0.999942600727) A[1]:(5.70307784074e-05) A[2]:(3.32986076046e-07) A[3]:(5.98714819944e-08)\n",
      " state (4)  A[0]:(0.9996509552) A[1]:(0.000348102475982) A[2]:(6.5679665795e-07) A[3]:(2.98786147823e-07)\n",
      " state (5)  A[0]:(0.921789705753) A[1]:(0.0781893581152) A[2]:(2.07434004551e-05) A[3]:(1.7220432369e-07)\n",
      " state (6)  A[0]:(0.0992656499147) A[1]:(0.900591492653) A[2]:(0.000142885415698) A[3]:(9.03832786392e-10)\n",
      " state (7)  A[0]:(0.0113969249651) A[1]:(0.988546013832) A[2]:(5.7078908867e-05) A[3]:(1.06057392035e-10)\n",
      " state (8)  A[0]:(0.000449900690001) A[1]:(0.99954199791) A[2]:(8.08029471955e-06) A[3]:(4.17685677712e-12)\n",
      " state (9)  A[0]:(1.55931138579e-05) A[1]:(0.999983429909) A[2]:(9.85813016996e-07) A[3]:(1.26449415175e-13)\n",
      " state (10)  A[0]:(3.48227945324e-06) A[1]:(0.999996125698) A[2]:(3.87319744277e-07) A[3]:(2.6081508169e-14)\n",
      " state (11)  A[0]:(2.19199296225e-06) A[1]:(0.999997496605) A[2]:(2.92039743499e-07) A[3]:(1.60169051416e-14)\n",
      " state (12)  A[0]:(1.90331149952e-06) A[1]:(0.999997854233) A[2]:(2.69207589554e-07) A[3]:(1.38234625027e-14)\n",
      " state (13)  A[0]:(1.81577513558e-06) A[1]:(0.999997913837) A[2]:(2.62907491333e-07) A[3]:(1.31793261336e-14)\n",
      " state (14)  A[0]:(1.78500408765e-06) A[1]:(0.999997973442) A[2]:(2.61337362417e-07) A[3]:(1.29677754198e-14)\n",
      " state (15)  A[0]:(1.77364336196e-06) A[1]:(0.999997973442) A[2]:(2.61307945948e-07) A[3]:(1.29020998732e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 270000 finished after 10 . Running score: 0.12. Policy_loss: -92050.6111844, Value_loss: 1.42706098806. Times trained:               12595. Times reached goal: 135.               Steps done: 3010690.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.991278409958) A[1]:(0.00318389688618) A[2]:(0.00319166132249) A[3]:(0.00234600785188)\n",
      " state (1)  A[0]:(0.0339019782841) A[1]:(0.0087984027341) A[2]:(0.00739553943276) A[3]:(0.949904084206)\n",
      " state (2)  A[0]:(0.996321797371) A[1]:(0.00243839900941) A[2]:(0.000393141497625) A[3]:(0.000846632756293)\n",
      " state (3)  A[0]:(0.99996638298) A[1]:(3.2910698792e-05) A[2]:(6.44351587198e-07) A[3]:(3.34029444105e-08)\n",
      " state (4)  A[0]:(0.999938905239) A[1]:(6.02979562245e-05) A[2]:(7.44926921925e-07) A[3]:(7.52713376073e-08)\n",
      " state (5)  A[0]:(0.9997164011) A[1]:(0.000282066990621) A[2]:(1.24430573578e-06) A[3]:(3.1324159977e-07)\n",
      " state (6)  A[0]:(0.979642689228) A[1]:(0.0203426983207) A[2]:(1.39573157867e-05) A[3]:(6.56272959532e-07)\n",
      " state (7)  A[0]:(0.292656570673) A[1]:(0.706758856773) A[2]:(0.000584600842558) A[3]:(3.95580990187e-09)\n",
      " state (8)  A[0]:(0.0226357281208) A[1]:(0.977026820183) A[2]:(0.000337479315931) A[3]:(1.65421232268e-10)\n",
      " state (9)  A[0]:(0.000450799503596) A[1]:(0.999512732029) A[2]:(3.6497367546e-05) A[3]:(3.36398313719e-12)\n",
      " state (10)  A[0]:(2.03884646908e-05) A[1]:(0.999973833561) A[2]:(5.75230569666e-06) A[3]:(1.45196639542e-13)\n",
      " state (11)  A[0]:(5.2931159189e-06) A[1]:(0.999992132187) A[2]:(2.59012085735e-06) A[3]:(3.65314264841e-14)\n",
      " state (12)  A[0]:(3.10972245643e-06) A[1]:(0.99999499321) A[2]:(1.90830064639e-06) A[3]:(2.1274176065e-14)\n",
      " state (13)  A[0]:(2.46716786023e-06) A[1]:(0.999995827675) A[2]:(1.68311385096e-06) A[3]:(1.69027457504e-14)\n",
      " state (14)  A[0]:(2.20352626457e-06) A[1]:(0.999996185303) A[2]:(1.59217051987e-06) A[3]:(1.51815578609e-14)\n",
      " state (15)  A[0]:(2.07494713322e-06) A[1]:(0.999996364117) A[2]:(1.55294787874e-06) A[3]:(1.43990213904e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 271000 finished after 8 . Running score: 0.15. Policy_loss: -92050.6227017, Value_loss: 2.26606119772. Times trained:               12879. Times reached goal: 124.               Steps done: 3023569.\n",
      " state (0)  A[0]:(0.991511106491) A[1]:(0.00305195641704) A[2]:(0.00315933395177) A[3]:(0.00227761059068)\n",
      " state (1)  A[0]:(0.0275610871613) A[1]:(0.00796423666179) A[2]:(0.00734976818785) A[3]:(0.957124888897)\n",
      " state (2)  A[0]:(0.999784588814) A[1]:(0.000194800217287) A[2]:(1.85090448213e-05) A[3]:(2.1009198008e-06)\n",
      " state (3)  A[0]:(0.999950349331) A[1]:(4.78136062156e-05) A[2]:(1.76468279278e-06) A[3]:(5.3431289615e-08)\n",
      " state (4)  A[0]:(0.999861061573) A[1]:(0.000135864291224) A[2]:(2.87841999125e-06) A[3]:(1.75737369545e-07)\n",
      " state (5)  A[0]:(0.997257292271) A[1]:(0.00272491341457) A[2]:(1.70135499502e-05) A[3]:(7.66794528317e-07)\n",
      " state (6)  A[0]:(0.440448284149) A[1]:(0.556272745132) A[2]:(0.00327899307013) A[3]:(2.00472829448e-08)\n",
      " state (7)  A[0]:(0.01555857528) A[1]:(0.980917215347) A[2]:(0.00352423312142) A[3]:(2.0135190737e-10)\n",
      " state (8)  A[0]:(0.000127510036691) A[1]:(0.999576866627) A[2]:(0.000295644713333) A[3]:(1.72930203143e-12)\n",
      " state (9)  A[0]:(2.60526462625e-06) A[1]:(0.999963402748) A[2]:(3.4008284274e-05) A[3]:(3.30802144574e-14)\n",
      " state (10)  A[0]:(6.11539746842e-07) A[1]:(0.999984204769) A[2]:(1.52127058755e-05) A[3]:(7.46350037166e-15)\n",
      " state (11)  A[0]:(3.83738438359e-07) A[1]:(0.999987840652) A[2]:(1.17987856356e-05) A[3]:(4.62135755011e-15)\n",
      " state (12)  A[0]:(3.25884599306e-07) A[1]:(0.999988853931) A[2]:(1.08263693619e-05) A[3]:(3.90916377773e-15)\n",
      " state (13)  A[0]:(3.06067988731e-07) A[1]:(0.999989211559) A[2]:(1.04954133349e-05) A[3]:(3.66862463192e-15)\n",
      " state (14)  A[0]:(2.98543170629e-07) A[1]:(0.999989330769) A[2]:(1.03803131424e-05) A[3]:(3.57936514697e-15)\n",
      " state (15)  A[0]:(2.95770774983e-07) A[1]:(0.999989330769) A[2]:(1.03468782982e-05) A[3]:(3.54786759155e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 272000 finished after 11 . Running score: 0.1. Policy_loss: -92050.6116537, Value_loss: 1.62470486989. Times trained:               12437. Times reached goal: 121.               Steps done: 3036006.\n",
      " state (0)  A[0]:(0.991072416306) A[1]:(0.00323626073077) A[2]:(0.00329560856335) A[3]:(0.00239573186263)\n",
      " state (1)  A[0]:(0.0284075178206) A[1]:(0.00827919691801) A[2]:(0.00805001985282) A[3]:(0.955263257027)\n",
      " state (2)  A[0]:(0.999003887177) A[1]:(0.000794654362835) A[2]:(0.000152106193127) A[3]:(4.93472798553e-05)\n",
      " state (3)  A[0]:(0.999950230122) A[1]:(4.74033404316e-05) A[2]:(2.34617778005e-06) A[3]:(4.64755913754e-08)\n",
      " state (4)  A[0]:(0.999853551388) A[1]:(0.00014261015167) A[2]:(3.64512789019e-06) A[3]:(1.77957019787e-07)\n",
      " state (5)  A[0]:(0.997893810272) A[1]:(0.00209274142981) A[2]:(1.20799440992e-05) A[3]:(1.34566494125e-06)\n",
      " state (6)  A[0]:(0.758919775486) A[1]:(0.240335285664) A[2]:(0.000744600431062) A[3]:(3.61008858363e-07)\n",
      " state (7)  A[0]:(0.021945476532) A[1]:(0.973744392395) A[2]:(0.00431011756882) A[3]:(2.31660982064e-10)\n",
      " state (8)  A[0]:(4.9389036576e-05) A[1]:(0.999756753445) A[2]:(0.00019388164219) A[3]:(4.16278231503e-13)\n",
      " state (9)  A[0]:(7.86788916685e-07) A[1]:(0.999980568886) A[2]:(1.86170964298e-05) A[3]:(6.39386377773e-15)\n",
      " state (10)  A[0]:(1.94983115875e-07) A[1]:(0.999991238117) A[2]:(8.54605968925e-06) A[3]:(1.55547775181e-15)\n",
      " state (11)  A[0]:(1.24362344422e-07) A[1]:(0.99999320507) A[2]:(6.69134351483e-06) A[3]:(9.89229344428e-16)\n",
      " state (12)  A[0]:(1.0555133656e-07) A[1]:(0.999993741512) A[2]:(6.14048894931e-06) A[3]:(8.40872211931e-16)\n",
      " state (13)  A[0]:(9.86835075878e-08) A[1]:(0.999993979931) A[2]:(5.94052698943e-06) A[3]:(7.88234831732e-16)\n",
      " state (14)  A[0]:(9.57722150474e-08) A[1]:(0.999994039536) A[2]:(5.86331680097e-06) A[3]:(7.67204803596e-16)\n",
      " state (15)  A[0]:(9.44515576862e-08) A[1]:(0.999994039536) A[2]:(5.83638211538e-06) A[3]:(7.58842417885e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 273000 finished after 27 . Running score: 0.16. Policy_loss: -92050.6113767, Value_loss: 1.19307270565. Times trained:               13241. Times reached goal: 116.               Steps done: 3049247.\n",
      " state (0)  A[0]:(0.991351187229) A[1]:(0.00311889825389) A[2]:(0.00323628191836) A[3]:(0.00229364447296)\n",
      " state (1)  A[0]:(0.0389984510839) A[1]:(0.00955124106258) A[2]:(0.0102008478716) A[3]:(0.94124943018)\n",
      " state (2)  A[0]:(0.999532818794) A[1]:(0.000383223639801) A[2]:(7.45128782e-05) A[3]:(9.43209761317e-06)\n",
      " state (3)  A[0]:(0.999963343143) A[1]:(3.43640676874e-05) A[2]:(2.26716338148e-06) A[3]:(2.58925840768e-08)\n",
      " state (4)  A[0]:(0.999913036823) A[1]:(8.36631370476e-05) A[2]:(3.24065695168e-06) A[3]:(8.35159283952e-08)\n",
      " state (5)  A[0]:(0.998974204063) A[1]:(0.00101552845445) A[2]:(9.59937824518e-06) A[3]:(6.97757457147e-07)\n",
      " state (6)  A[0]:(0.825508356094) A[1]:(0.173803761601) A[2]:(0.000687522173394) A[3]:(3.43068222719e-07)\n",
      " state (7)  A[0]:(0.0315274707973) A[1]:(0.962025225163) A[2]:(0.00644729705527) A[3]:(2.61391547296e-10)\n",
      " state (8)  A[0]:(9.27206419874e-05) A[1]:(0.999563097954) A[2]:(0.000344154512277) A[3]:(6.28672786572e-13)\n",
      " state (9)  A[0]:(1.26875659134e-06) A[1]:(0.999967992306) A[2]:(3.07200207317e-05) A[3]:(8.27960052271e-15)\n",
      " state (10)  A[0]:(2.69835283007e-07) A[1]:(0.99998652935) A[2]:(1.31766719278e-05) A[3]:(1.7352593302e-15)\n",
      " state (11)  A[0]:(1.57766763209e-07) A[1]:(0.99998986721) A[2]:(9.9995495475e-06) A[3]:(1.02330177413e-15)\n",
      " state (12)  A[0]:(1.26987075078e-07) A[1]:(0.999990820885) A[2]:(9.04791977518e-06) A[3]:(8.35934856882e-16)\n",
      " state (13)  A[0]:(1.14794104888e-07) A[1]:(0.999991178513) A[2]:(8.70746225701e-06) A[3]:(7.6738696862e-16)\n",
      " state (14)  A[0]:(1.09036932372e-07) A[1]:(0.999991297722) A[2]:(8.59151987243e-06) A[3]:(7.39434510783e-16)\n",
      " state (15)  A[0]:(1.06072377548e-07) A[1]:(0.999991297722) A[2]:(8.57213944983e-06) A[3]:(7.28662475156e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 274000 finished after 8 . Running score: 0.12. Policy_loss: -92050.611382, Value_loss: 1.2066340205. Times trained:               12905. Times reached goal: 138.               Steps done: 3062152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9922,  0.0028,  0.0030,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9922,  0.0028,  0.0030,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9982e-01,  1.7143e-04,  6.1764e-06,  1.3123e-07]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8828e-04,  9.9899e-01,  8.1764e-04,  1.9562e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8151e-04,  9.9902e-01,  8.0004e-04,  1.8904e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.0188e-06,  9.9994e-01,  5.5922e-05,  2.8536e-14]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.5765e-07,  9.9999e-01,  1.1338e-05,  2.1744e-15]])\n",
      "On state=10, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.8445e-06,  9.9994e-01,  5.3939e-05,  2.6939e-14]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6188e-04,  9.9909e-01,  7.4712e-04,  1.6987e-12]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.99215734005) A[1]:(0.00276053510606) A[2]:(0.0030432799831) A[3]:(0.00203886278905)\n",
      " state (1)  A[0]:(0.0366543196142) A[1]:(0.00909534282982) A[2]:(0.00983262807131) A[3]:(0.944417715073)\n",
      " state (2)  A[0]:(0.999931871891) A[1]:(6.13866650383e-05) A[2]:(6.61156900605e-06) A[3]:(1.16963008168e-07)\n",
      " state (3)  A[0]:(0.999952852726) A[1]:(4.42991913587e-05) A[2]:(2.83146914626e-06) A[3]:(3.5606511517e-08)\n",
      " state (4)  A[0]:(0.999806821346) A[1]:(0.000186509380001) A[2]:(6.52463995721e-06) A[3]:(1.38668156069e-07)\n",
      " state (5)  A[0]:(0.929489374161) A[1]:(0.0694780647755) A[2]:(0.00103245861828) A[3]:(1.08053470171e-07)\n",
      " state (6)  A[0]:(0.0490878857672) A[1]:(0.934058427811) A[2]:(0.0168537143618) A[3]:(4.25746521371e-10)\n",
      " state (7)  A[0]:(0.00379934231751) A[1]:(0.990816712379) A[2]:(0.00538391573355) A[3]:(3.74971685646e-11)\n",
      " state (8)  A[0]:(0.000158419454237) A[1]:(0.999103784561) A[2]:(0.000737770285923) A[3]:(1.66476977603e-12)\n",
      " state (9)  A[0]:(2.7120543109e-06) A[1]:(0.999944865704) A[2]:(5.24233182659e-05) A[3]:(2.57227456701e-14)\n",
      " state (10)  A[0]:(2.44991127829e-07) A[1]:(0.999988734722) A[2]:(1.10140199467e-05) A[3]:(2.07347376268e-15)\n",
      " state (11)  A[0]:(1.08551802214e-07) A[1]:(0.999993383884) A[2]:(6.50667971058e-06) A[3]:(8.7684347774e-16)\n",
      " state (12)  A[0]:(8.53948591839e-08) A[1]:(0.999994337559) A[2]:(5.56872328161e-06) A[3]:(6.79026926782e-16)\n",
      " state (13)  A[0]:(7.94478367538e-08) A[1]:(0.999994635582) A[2]:(5.30739362148e-06) A[3]:(6.2781060317e-16)\n",
      " state (14)  A[0]:(7.76639978994e-08) A[1]:(0.999994695187) A[2]:(5.22005530001e-06) A[3]:(6.11611045114e-16)\n",
      " state (15)  A[0]:(7.71243193753e-08) A[1]:(0.999994754791) A[2]:(5.18549222761e-06) A[3]:(6.05831474617e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 275000 finished after 9 . Running score: 0.14. Policy_loss: -92050.6112464, Value_loss: 1.42073194665. Times trained:               12757. Times reached goal: 136.               Steps done: 3074909.\n",
      " state (0)  A[0]:(0.99264395237) A[1]:(0.00262190354988) A[2]:(0.00289187044837) A[3]:(0.00184224999975)\n",
      " state (1)  A[0]:(0.0427470728755) A[1]:(0.00967662315816) A[2]:(0.0109822703525) A[3]:(0.936594009399)\n",
      " state (2)  A[0]:(0.999840795994) A[1]:(0.000134931586217) A[2]:(2.38038956013e-05) A[3]:(4.81650033635e-07)\n",
      " state (3)  A[0]:(0.999960422516) A[1]:(3.65410960512e-05) A[2]:(3.03729393636e-06) A[3]:(1.30801787179e-08)\n",
      " state (4)  A[0]:(0.999850332737) A[1]:(0.000142613629578) A[2]:(7.02776696926e-06) A[3]:(5.0676533192e-08)\n",
      " state (5)  A[0]:(0.969904899597) A[1]:(0.0293172188103) A[2]:(0.000777842360549) A[3]:(3.98491906139e-08)\n",
      " state (6)  A[0]:(0.113427355886) A[1]:(0.851182758808) A[2]:(0.0353898517787) A[3]:(2.17279347292e-10)\n",
      " state (7)  A[0]:(0.0159823913127) A[1]:(0.96492344141) A[2]:(0.0190941859037) A[3]:(3.13721201795e-11)\n",
      " state (8)  A[0]:(0.00248297932558) A[1]:(0.990992069244) A[2]:(0.00652496563271) A[3]:(5.15733636353e-12)\n",
      " state (9)  A[0]:(0.000131999826408) A[1]:(0.998860776424) A[2]:(0.00100719602779) A[3]:(2.51026657143e-13)\n",
      " state (10)  A[0]:(5.70453903492e-06) A[1]:(0.999861657619) A[2]:(0.000132629254949) A[3]:(8.89268635807e-15)\n",
      " state (11)  A[0]:(1.15757211461e-06) A[1]:(0.999951303005) A[2]:(4.75429296785e-05) A[3]:(1.58846703813e-15)\n",
      " state (12)  A[0]:(6.71676218644e-07) A[1]:(0.999965786934) A[2]:(3.35372642439e-05) A[3]:(8.79147036779e-16)\n",
      " state (13)  A[0]:(5.67407028029e-07) A[1]:(0.999969363213) A[2]:(3.00980846077e-05) A[3]:(7.31294682979e-16)\n",
      " state (14)  A[0]:(5.3785873888e-07) A[1]:(0.999970376492) A[2]:(2.90721127385e-05) A[3]:(6.89437861676e-16)\n",
      " state (15)  A[0]:(5.28124701304e-07) A[1]:(0.999970734119) A[2]:(2.87155871774e-05) A[3]:(6.75324757528e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 276000 finished after 16 . Running score: 0.09. Policy_loss: -92050.61117, Value_loss: 1.86451210953. Times trained:               12714. Times reached goal: 120.               Steps done: 3087623.\n",
      " state (0)  A[0]:(0.992728412151) A[1]:(0.00255188462324) A[2]:(0.00286189327016) A[3]:(0.00185782497283)\n",
      " state (1)  A[0]:(0.0430349707603) A[1]:(0.00957187078893) A[2]:(0.0107379443944) A[3]:(0.93665522337)\n",
      " state (2)  A[0]:(0.999894857407) A[1]:(9.14234115044e-05) A[2]:(1.35486325235e-05) A[3]:(1.90047018123e-07)\n",
      " state (3)  A[0]:(0.999962091446) A[1]:(3.52320930688e-05) A[2]:(2.64216737378e-06) A[3]:(1.15503944187e-08)\n",
      " state (4)  A[0]:(0.999785423279) A[1]:(0.000207028002478) A[2]:(7.47167814552e-06) A[3]:(5.75863623453e-08)\n",
      " state (5)  A[0]:(0.86724460125) A[1]:(0.130297258496) A[2]:(0.00245809624903) A[3]:(1.47064742606e-08)\n",
      " state (6)  A[0]:(0.050423014909) A[1]:(0.933065235615) A[2]:(0.0165117513388) A[3]:(7.27480922724e-11)\n",
      " state (7)  A[0]:(0.00375233590603) A[1]:(0.991861283779) A[2]:(0.00438636029139) A[3]:(5.46529575068e-12)\n",
      " state (8)  A[0]:(0.000115905393614) A[1]:(0.999426305294) A[2]:(0.000457796122646) A[3]:(1.58145117011e-13)\n",
      " state (9)  A[0]:(2.40150302488e-06) A[1]:(0.999962985516) A[2]:(3.46075939888e-05) A[3]:(2.60456342061e-15)\n",
      " state (10)  A[0]:(3.6987967178e-07) A[1]:(0.999989569187) A[2]:(1.00457418739e-05) A[3]:(3.46076592304e-16)\n",
      " state (11)  A[0]:(2.0978802695e-07) A[1]:(0.999992847443) A[2]:(6.93470155966e-06) A[3]:(1.87271268697e-16)\n",
      " state (12)  A[0]:(1.79064400641e-07) A[1]:(0.999993562698) A[2]:(6.26465634923e-06) A[3]:(1.57907812789e-16)\n",
      " state (13)  A[0]:(1.708273345e-07) A[1]:(0.999993741512) A[2]:(6.08382606515e-06) A[3]:(1.50192944062e-16)\n",
      " state (14)  A[0]:(1.68124714151e-07) A[1]:(0.999993801117) A[2]:(6.02621457801e-06) A[3]:(1.47703196592e-16)\n",
      " state (15)  A[0]:(1.67067412349e-07) A[1]:(0.999993801117) A[2]:(6.00427210884e-06) A[3]:(1.46733330633e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 277000 finished after 7 . Running score: 0.16. Policy_loss: -92050.6111949, Value_loss: 1.19918423366. Times trained:               12564. Times reached goal: 128.               Steps done: 3100187.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.992953360081) A[1]:(0.00245715165511) A[2]:(0.00276127108373) A[3]:(0.00182822789066)\n",
      " state (1)  A[0]:(0.0317056253552) A[1]:(0.00821183808148) A[2]:(0.00871009659022) A[3]:(0.95137244463)\n",
      " state (2)  A[0]:(0.999964594841) A[1]:(3.23525200656e-05) A[2]:(3.01259819935e-06) A[3]:(1.36429925135e-08)\n",
      " state (3)  A[0]:(0.999945163727) A[1]:(5.13946215506e-05) A[2]:(3.41107397617e-06) A[3]:(2.06682848614e-08)\n",
      " state (4)  A[0]:(0.999315500259) A[1]:(0.000662652018946) A[2]:(2.17746292037e-05) A[3]:(7.73694566192e-08)\n",
      " state (5)  A[0]:(0.415698766708) A[1]:(0.565598428249) A[2]:(0.0187028255314) A[3]:(1.17509224573e-09)\n",
      " state (6)  A[0]:(0.0238438118249) A[1]:(0.962190628052) A[2]:(0.0139655731618) A[3]:(3.84936874043e-11)\n",
      " state (7)  A[0]:(0.00329001573846) A[1]:(0.992541372776) A[2]:(0.00416863290593) A[3]:(5.3522563985e-12)\n",
      " state (8)  A[0]:(0.000187274272321) A[1]:(0.999251663685) A[2]:(0.000561073247809) A[3]:(2.62301167115e-13)\n",
      " state (9)  A[0]:(5.23438711753e-06) A[1]:(0.999950706959) A[2]:(4.40568801423e-05) A[3]:(5.32053539071e-15)\n",
      " state (10)  A[0]:(8.17892328087e-07) A[1]:(0.999987304211) A[2]:(1.18514226415e-05) A[3]:(6.77376800721e-16)\n",
      " state (11)  A[0]:(4.6284066002e-07) A[1]:(0.999991595745) A[2]:(7.94475272414e-06) A[3]:(3.58616385692e-16)\n",
      " state (12)  A[0]:(3.97006573394e-07) A[1]:(0.999992489815) A[2]:(7.14076713848e-06) A[3]:(3.02267915923e-16)\n",
      " state (13)  A[0]:(3.80506065767e-07) A[1]:(0.999992668629) A[2]:(6.93797483109e-06) A[3]:(2.8849762189e-16)\n",
      " state (14)  A[0]:(3.75485313953e-07) A[1]:(0.999992728233) A[2]:(6.87936972099e-06) A[3]:(2.84483585692e-16)\n",
      " state (15)  A[0]:(3.73520634867e-07) A[1]:(0.999992787838) A[2]:(6.85912073095e-06) A[3]:(2.83045006111e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 278000 finished after 10 . Running score: 0.15. Policy_loss: -92050.6111901, Value_loss: 1.20594125099. Times trained:               12861. Times reached goal: 140.               Steps done: 3113048.\n",
      " state (0)  A[0]:(0.993197739124) A[1]:(0.0023459289223) A[2]:(0.00264003500342) A[3]:(0.0018162681954)\n",
      " state (1)  A[0]:(0.0405656732619) A[1]:(0.00889483373612) A[2]:(0.0100873364136) A[3]:(0.940452158451)\n",
      " state (2)  A[0]:(0.999928116798) A[1]:(6.21237195446e-05) A[2]:(9.68863514572e-06) A[3]:(9.70606706119e-08)\n",
      " state (3)  A[0]:(0.99997574091) A[1]:(2.21610134759e-05) A[2]:(2.09334893952e-06) A[3]:(6.47237996532e-09)\n",
      " state (4)  A[0]:(0.999948859215) A[1]:(4.78962319903e-05) A[2]:(3.2339964946e-06) A[3]:(1.74630283567e-08)\n",
      " state (5)  A[0]:(0.998843193054) A[1]:(0.00112889939919) A[2]:(2.7852387575e-05) A[3]:(6.63066117568e-08)\n",
      " state (6)  A[0]:(0.505090653896) A[1]:(0.482498317957) A[2]:(0.0124110765755) A[3]:(1.02131891833e-09)\n",
      " state (7)  A[0]:(0.0271746311337) A[1]:(0.964556992054) A[2]:(0.00826839357615) A[3]:(1.93116946046e-11)\n",
      " state (8)  A[0]:(0.000381866790121) A[1]:(0.999147117138) A[2]:(0.000471035105875) A[3]:(2.07205875179e-13)\n",
      " state (9)  A[0]:(8.91887793841e-06) A[1]:(0.999957978725) A[2]:(3.30951443175e-05) A[3]:(3.48098184663e-15)\n",
      " state (10)  A[0]:(2.08582332561e-06) A[1]:(0.999985873699) A[2]:(1.20135746329e-05) A[3]:(7.03859815213e-16)\n",
      " state (11)  A[0]:(1.35478433094e-06) A[1]:(0.999989688396) A[2]:(8.98584039533e-06) A[3]:(4.40299557974e-16)\n",
      " state (12)  A[0]:(1.17648130527e-06) A[1]:(0.999990582466) A[2]:(8.24062681204e-06) A[3]:(3.80500275978e-16)\n",
      " state (13)  A[0]:(1.10849339308e-06) A[1]:(0.999990880489) A[2]:(7.9964820543e-06) A[3]:(3.60037574629e-16)\n",
      " state (14)  A[0]:(1.07178334474e-06) A[1]:(0.999991059303) A[2]:(7.89453770267e-06) A[3]:(3.50449214605e-16)\n",
      " state (15)  A[0]:(1.04663706679e-06) A[1]:(0.999991118908) A[2]:(7.84110670793e-06) A[3]:(3.44660829672e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 279000 finished after 16 . Running score: 0.14. Policy_loss: -92050.6113891, Value_loss: 1.83985901196. Times trained:               12697. Times reached goal: 145.               Steps done: 3125745.\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0024,  0.0027,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0024,  0.0027,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0024,  0.0027,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9994e-01,  5.9024e-05,  4.4181e-06,  2.3171e-08]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9994e-01,  5.9032e-05,  4.4177e-06,  2.3174e-08]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.7817e-04,  9.9740e-01,  1.7239e-03,  6.0730e-13]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.7685e-04,  9.9740e-01,  1.7214e-03,  6.0634e-13]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.7572e-04,  9.9741e-01,  1.7192e-03,  6.0552e-13]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.992962598801) A[1]:(0.0024263067171) A[2]:(0.00273271277547) A[3]:(0.00187840254512)\n",
      " state (1)  A[0]:(0.0349679328501) A[1]:(0.00833099894226) A[2]:(0.00954053085297) A[3]:(0.947160542011)\n",
      " state (2)  A[0]:(0.999943673611) A[1]:(4.85797645524e-05) A[2]:(7.71827035351e-06) A[3]:(5.0293767373e-08)\n",
      " state (3)  A[0]:(0.999972224236) A[1]:(2.50641624007e-05) A[2]:(2.68195299213e-06) A[3]:(7.92068011179e-09)\n",
      " state (4)  A[0]:(0.999936521053) A[1]:(5.90270901739e-05) A[2]:(4.417803666e-06) A[3]:(2.31719177179e-08)\n",
      " state (5)  A[0]:(0.998432159424) A[1]:(0.00152313068975) A[2]:(4.46216508863e-05) A[3]:(8.18095529098e-08)\n",
      " state (6)  A[0]:(0.490790128708) A[1]:(0.489747583866) A[2]:(0.0194622892886) A[3]:(1.19418119837e-09)\n",
      " state (7)  A[0]:(0.0342245437205) A[1]:(0.948602974415) A[2]:(0.0171724930406) A[3]:(2.91085593129e-11)\n",
      " state (8)  A[0]:(0.000880271662027) A[1]:(0.997391521931) A[2]:(0.00172818556894) A[3]:(6.08822562677e-13)\n",
      " state (9)  A[0]:(1.71432548086e-05) A[1]:(0.999864399433) A[2]:(0.000118451243907) A[3]:(8.7499342014e-15)\n",
      " state (10)  A[0]:(2.78392940345e-06) A[1]:(0.999962568283) A[2]:(3.46354463545e-05) A[3]:(1.20099223992e-15)\n",
      " state (11)  A[0]:(1.59767841978e-06) A[1]:(0.999974370003) A[2]:(2.40278241108e-05) A[3]:(6.57251774351e-16)\n",
      " state (12)  A[0]:(1.34974027333e-06) A[1]:(0.999976992607) A[2]:(2.16728421947e-05) A[3]:(5.51061318396e-16)\n",
      " state (13)  A[0]:(1.2688632296e-06) A[1]:(0.999977707863) A[2]:(2.09989048017e-05) A[3]:(5.19704635145e-16)\n",
      " state (14)  A[0]:(1.23108281969e-06) A[1]:(0.999978005886) A[2]:(2.07636294363e-05) A[3]:(5.0714514824e-16)\n",
      " state (15)  A[0]:(1.20761819744e-06) A[1]:(0.999978125095) A[2]:(2.06620425161e-05) A[3]:(5.00472540319e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 280000 finished after 8 . Running score: 0.11. Policy_loss: -92050.6113213, Value_loss: 1.41100625993. Times trained:               12736. Times reached goal: 131.               Steps done: 3138481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993301689625) A[1]:(0.00232802750543) A[2]:(0.00257973652333) A[3]:(0.00179056974594)\n",
      " state (1)  A[0]:(0.0317276790738) A[1]:(0.00795498024672) A[2]:(0.00890974607319) A[3]:(0.95140761137)\n",
      " state (2)  A[0]:(0.999970912933) A[1]:(2.59580665443e-05) A[2]:(3.14677095048e-06) A[3]:(8.63285620767e-09)\n",
      " state (3)  A[0]:(0.999949872494) A[1]:(4.60811315861e-05) A[2]:(4.04499451179e-06) A[3]:(1.58493875801e-08)\n",
      " state (4)  A[0]:(0.99865603447) A[1]:(0.00129783072043) A[2]:(4.60597475467e-05) A[3]:(7.37251113492e-08)\n",
      " state (5)  A[0]:(0.357323855162) A[1]:(0.607374668121) A[2]:(0.0353014431894) A[3]:(5.228866784e-10)\n",
      " state (6)  A[0]:(0.0446725524962) A[1]:(0.924026548862) A[2]:(0.0313008725643) A[3]:(3.36882119423e-11)\n",
      " state (7)  A[0]:(0.0138042923063) A[1]:(0.969619870186) A[2]:(0.0165758505464) A[3]:(1.03495250564e-11)\n",
      " state (8)  A[0]:(0.00366624444723) A[1]:(0.989287734032) A[2]:(0.00704602291808) A[3]:(2.65200361671e-12)\n",
      " state (9)  A[0]:(0.000267992989393) A[1]:(0.998542487621) A[2]:(0.00118950440083) A[3]:(1.65637360598e-13)\n",
      " state (10)  A[0]:(1.35706486617e-05) A[1]:(0.999832391739) A[2]:(0.000154028995894) A[3]:(6.49510073207e-15)\n",
      " state (11)  A[0]:(2.92627782983e-06) A[1]:(0.999942839146) A[2]:(5.42418601981e-05) A[3]:(1.20464856352e-15)\n",
      " state (12)  A[0]:(1.74553474608e-06) A[1]:(0.999959886074) A[2]:(3.83594888262e-05) A[3]:(6.83681955162e-16)\n",
      " state (13)  A[0]:(1.49408640482e-06) A[1]:(0.999963819981) A[2]:(3.46782544511e-05) A[3]:(5.78209465509e-16)\n",
      " state (14)  A[0]:(1.42389683333e-06) A[1]:(0.999964892864) A[2]:(3.3697240724e-05) A[3]:(5.50350869512e-16)\n",
      " state (15)  A[0]:(1.40001543514e-06) A[1]:(0.999965190887) A[2]:(3.34172291332e-05) A[3]:(5.41812459766e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 281000 finished after 4 . Running score: 0.09. Policy_loss: -92050.611326, Value_loss: 1.20932306027. Times trained:               12865. Times reached goal: 129.               Steps done: 3151346.\n",
      " state (0)  A[0]:(0.993389427662) A[1]:(0.00229964405298) A[2]:(0.00257475348189) A[3]:(0.00173615780659)\n",
      " state (1)  A[0]:(0.0380377992988) A[1]:(0.00891094002873) A[2]:(0.0104827946052) A[3]:(0.942568480968)\n",
      " state (2)  A[0]:(0.999971687794) A[1]:(2.49501063081e-05) A[2]:(3.36208313456e-06) A[3]:(5.31758903577e-09)\n",
      " state (3)  A[0]:(0.999958753586) A[1]:(3.71835049009e-05) A[2]:(4.02570412916e-06) A[3]:(7.96749954901e-09)\n",
      " state (4)  A[0]:(0.999633073807) A[1]:(0.000347174238414) A[2]:(1.97162153199e-05) A[3]:(3.32776828316e-08)\n",
      " state (5)  A[0]:(0.582130074501) A[1]:(0.397518277168) A[2]:(0.0203516408801) A[3]:(1.27472010725e-09)\n",
      " state (6)  A[0]:(0.0130660096183) A[1]:(0.96708637476) A[2]:(0.0198476035148) A[3]:(6.39701529595e-12)\n",
      " state (7)  A[0]:(0.000512915314175) A[1]:(0.996547102928) A[2]:(0.0029400086496) A[3]:(2.15567269438e-13)\n",
      " state (8)  A[0]:(6.64884737489e-06) A[1]:(0.999825716019) A[2]:(0.000167616293766) A[3]:(1.95028912589e-15)\n",
      " state (9)  A[0]:(2.37928489355e-07) A[1]:(0.999981284142) A[2]:(1.84713844646e-05) A[3]:(4.86889030286e-17)\n",
      " state (10)  A[0]:(7.62935883358e-08) A[1]:(0.999991178513) A[2]:(8.75405567058e-06) A[3]:(1.36505803187e-17)\n",
      " state (11)  A[0]:(5.70643443609e-08) A[1]:(0.999992728233) A[2]:(7.24333494873e-06) A[3]:(9.85649392764e-18)\n",
      " state (12)  A[0]:(5.31398427484e-08) A[1]:(0.999993026257) A[2]:(6.91418517818e-06) A[3]:(9.09354140811e-18)\n",
      " state (13)  A[0]:(5.22131138325e-08) A[1]:(0.999993145466) A[2]:(6.8321019171e-06) A[3]:(8.90606505818e-18)\n",
      " state (14)  A[0]:(5.20042995333e-08) A[1]:(0.999993145466) A[2]:(6.80889388605e-06) A[3]:(8.8541015721e-18)\n",
      " state (15)  A[0]:(5.19897191964e-08) A[1]:(0.999993145466) A[2]:(6.80106722939e-06) A[3]:(8.83813946782e-18)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 282000 finished after 25 . Running score: 0.11. Policy_loss: -92050.6113712, Value_loss: 1.20258153542. Times trained:               12328. Times reached goal: 134.               Steps done: 3163674.\n",
      " state (0)  A[0]:(0.993321180344) A[1]:(0.00232987874188) A[2]:(0.00260642939247) A[3]:(0.00174253247678)\n",
      " state (1)  A[0]:(0.0388020463288) A[1]:(0.00905422959477) A[2]:(0.0109782088548) A[3]:(0.94116550684)\n",
      " state (2)  A[0]:(0.99997484684) A[1]:(2.15846994251e-05) A[2]:(3.58583133675e-06) A[3]:(5.89615467561e-09)\n",
      " state (3)  A[0]:(0.999976038933) A[1]:(2.10124908335e-05) A[2]:(2.94405504064e-06) A[3]:(4.42722081218e-09)\n",
      " state (4)  A[0]:(0.999952614307) A[1]:(4.26859223808e-05) A[2]:(4.67784821012e-06) A[3]:(9.93045290443e-09)\n",
      " state (5)  A[0]:(0.999278724194) A[1]:(0.000676101539284) A[2]:(4.51800588053e-05) A[3]:(2.25770619977e-08)\n",
      " state (6)  A[0]:(0.671355724335) A[1]:(0.298905909061) A[2]:(0.0297383740544) A[3]:(3.65654756473e-10)\n",
      " state (7)  A[0]:(0.0784505233169) A[1]:(0.879721224308) A[2]:(0.0418282262981) A[3]:(1.28134759747e-11)\n",
      " state (8)  A[0]:(0.00538776488975) A[1]:(0.986556231976) A[2]:(0.00805599614978) A[3]:(6.57832512421e-13)\n",
      " state (9)  A[0]:(0.000109667133074) A[1]:(0.999330937862) A[2]:(0.000559386389796) A[3]:(8.24892437749e-15)\n",
      " state (10)  A[0]:(6.06767753197e-06) A[1]:(0.999916493893) A[2]:(7.74287982495e-05) A[3]:(3.0026259189e-16)\n",
      " state (11)  A[0]:(1.99923488253e-06) A[1]:(0.999961316586) A[2]:(3.67003849533e-05) A[3]:(8.38103300931e-17)\n",
      " state (12)  A[0]:(1.41465045544e-06) A[1]:(0.999969303608) A[2]:(2.92770509986e-05) A[3]:(5.65533855682e-17)\n",
      " state (13)  A[0]:(1.26458371597e-06) A[1]:(0.999971389771) A[2]:(2.73448385997e-05) A[3]:(5.00119684922e-17)\n",
      " state (14)  A[0]:(1.21134689834e-06) A[1]:(0.999972045422) A[2]:(2.67394989351e-05) A[3]:(4.78912031331e-17)\n",
      " state (15)  A[0]:(1.18613013456e-06) A[1]:(0.99997228384) A[2]:(2.65150774794e-05) A[3]:(4.70108017199e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 283000 finished after 24 . Running score: 0.16. Policy_loss: -92050.612649, Value_loss: 1.20807586493. Times trained:               12809. Times reached goal: 167.               Steps done: 3176483.\n",
      " state (0)  A[0]:(0.993235826492) A[1]:(0.00234389910474) A[2]:(0.00262418622151) A[3]:(0.00179605872836)\n",
      " state (1)  A[0]:(0.0292009208351) A[1]:(0.00782164372504) A[2]:(0.00931343622506) A[3]:(0.953664004803)\n",
      " state (2)  A[0]:(0.99997574091) A[1]:(2.07026623684e-05) A[2]:(3.5443017623e-06) A[3]:(5.09613595767e-09)\n",
      " state (3)  A[0]:(0.999970138073) A[1]:(2.60776178038e-05) A[2]:(3.80377218789e-06) A[3]:(6.42886366364e-09)\n",
      " state (4)  A[0]:(0.999918878078) A[1]:(7.37154623494e-05) A[2]:(7.38570042813e-06) A[3]:(1.8922209577e-08)\n",
      " state (5)  A[0]:(0.996628463268) A[1]:(0.00318979029544) A[2]:(0.000181734794751) A[3]:(2.94939663803e-08)\n",
      " state (6)  A[0]:(0.434412360191) A[1]:(0.504752874374) A[2]:(0.0608347281814) A[3]:(1.82316869934e-10)\n",
      " state (7)  A[0]:(0.0496445521712) A[1]:(0.905981719494) A[2]:(0.0443737097085) A[3]:(8.29384824708e-12)\n",
      " state (8)  A[0]:(0.00210883188993) A[1]:(0.992287099361) A[2]:(0.00560409110039) A[3]:(2.31501612585e-13)\n",
      " state (9)  A[0]:(3.44807885995e-05) A[1]:(0.999637722969) A[2]:(0.000327780726366) A[3]:(2.07855786619e-15)\n",
      " state (10)  A[0]:(3.2900916267e-06) A[1]:(0.999930918217) A[2]:(6.58101853332e-05) A[3]:(1.36097309968e-16)\n",
      " state (11)  A[0]:(1.51447352437e-06) A[1]:(0.999959409237) A[2]:(3.90862151107e-05) A[3]:(5.52794612504e-17)\n",
      " state (12)  A[0]:(1.20195647924e-06) A[1]:(0.999965131283) A[2]:(3.36482444254e-05) A[3]:(4.24273443149e-17)\n",
      " state (13)  A[0]:(1.11412600745e-06) A[1]:(0.999966740608) A[2]:(3.21673178405e-05) A[3]:(3.90433451203e-17)\n",
      " state (14)  A[0]:(1.08030576484e-06) A[1]:(0.999967217445) A[2]:(3.16813457175e-05) A[3]:(3.78587298388e-17)\n",
      " state (15)  A[0]:(1.06290656277e-06) A[1]:(0.999967455864) A[2]:(3.14877288474e-05) A[3]:(3.7320450327e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 284000 finished after 3 . Running score: 0.09. Policy_loss: -92050.6114987, Value_loss: 1.63040409096. Times trained:               12754. Times reached goal: 127.               Steps done: 3189237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9935,  0.0023,  0.0025,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9935,  0.0023,  0.0025,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9995e-01,  4.2883e-05,  5.2664e-06,  6.7974e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9995e-01,  4.2857e-05,  5.2643e-06,  6.7940e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9935,  0.0023,  0.0025,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9995e-01,  4.2812e-05,  5.2606e-06,  6.7882e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9935,  0.0023,  0.0025,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9995e-01,  4.2775e-05,  5.2577e-06,  6.7835e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9995e-01,  4.2760e-05,  5.2564e-06,  6.7815e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.7624e-03,  9.8091e-01,  1.2329e-02,  5.6193e-13]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.0485e-04,  9.9824e-01,  1.4558e-03,  1.6006e-14]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4594e-06,  9.9996e-01,  3.7934e-05,  3.0161e-17]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993522942066) A[1]:(0.00230159540661) A[2]:(0.00250969175249) A[3]:(0.00166578020435)\n",
      " state (1)  A[0]:(0.0447932705283) A[1]:(0.00968008767813) A[2]:(0.0123230228201) A[3]:(0.9332036376)\n",
      " state (2)  A[0]:(0.999925076962) A[1]:(5.98367332714e-05) A[2]:(1.50292080434e-05) A[3]:(6.63613448637e-08)\n",
      " state (3)  A[0]:(0.999977827072) A[1]:(1.90186274267e-05) A[2]:(3.13412715514e-06) A[3]:(3.28336069444e-09)\n",
      " state (4)  A[0]:(0.999951779842) A[1]:(4.29250248999e-05) A[2]:(5.26872872797e-06) A[3]:(6.80177425494e-09)\n",
      " state (5)  A[0]:(0.992878377438) A[1]:(0.00671033887193) A[2]:(0.000411297252867) A[3]:(1.03887343172e-08)\n",
      " state (6)  A[0]:(0.269459754229) A[1]:(0.661425232887) A[2]:(0.0691150426865) A[3]:(5.05205634271e-11)\n",
      " state (7)  A[0]:(0.0438437834382) A[1]:(0.915762424469) A[2]:(0.0403938107193) A[3]:(4.66699161955e-12)\n",
      " state (8)  A[0]:(0.00664377398789) A[1]:(0.98117429018) A[2]:(0.012181930244) A[3]:(5.51650955818e-13)\n",
      " state (9)  A[0]:(0.000297675054753) A[1]:(0.998270273209) A[2]:(0.00143202557229) A[3]:(1.55913185195e-14)\n",
      " state (10)  A[0]:(1.27680486912e-05) A[1]:(0.999824106693) A[2]:(0.000163108721608) A[3]:(3.88804084067e-16)\n",
      " state (11)  A[0]:(2.83534382106e-06) A[1]:(0.999938488007) A[2]:(5.86697642575e-05) A[3]:(6.56285892093e-17)\n",
      " state (12)  A[0]:(1.71079432221e-06) A[1]:(0.9999563694) A[2]:(4.19461757701e-05) A[3]:(3.62309244201e-17)\n",
      " state (13)  A[0]:(1.45342096403e-06) A[1]:(0.999960720539) A[2]:(3.78303120669e-05) A[3]:(3.00418929026e-17)\n",
      " state (14)  A[0]:(1.37008362344e-06) A[1]:(0.999962031841) A[2]:(3.65766500181e-05) A[3]:(2.81716606986e-17)\n",
      " state (15)  A[0]:(1.33407752401e-06) A[1]:(0.999962568283) A[2]:(3.61162856279e-05) A[3]:(2.74350166585e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 285000 finished after 12 . Running score: 0.05. Policy_loss: -92050.611291, Value_loss: 0.986536122119. Times trained:               13277. Times reached goal: 121.               Steps done: 3202514.\n",
      " state (0)  A[0]:(0.993446588516) A[1]:(0.0023562500719) A[2]:(0.00249247509055) A[3]:(0.00170470483135)\n",
      " state (1)  A[0]:(0.0466488115489) A[1]:(0.0100704533979) A[2]:(0.0122006339952) A[3]:(0.931080102921)\n",
      " state (2)  A[0]:(0.999976456165) A[1]:(2.0403358576e-05) A[2]:(3.10960535899e-06) A[3]:(3.72584185726e-09)\n",
      " state (3)  A[0]:(0.999973118305) A[1]:(2.39812306972e-05) A[2]:(2.88244291369e-06) A[3]:(3.58340268569e-09)\n",
      " state (4)  A[0]:(0.999744236469) A[1]:(0.000245097762672) A[2]:(1.06748439066e-05) A[3]:(1.80545765005e-08)\n",
      " state (5)  A[0]:(0.58456492424) A[1]:(0.408118098974) A[2]:(0.00731701916084) A[3]:(1.05543518369e-09)\n",
      " state (6)  A[0]:(0.01498765219) A[1]:(0.977937757969) A[2]:(0.0070745870471) A[3]:(1.50774423328e-12)\n",
      " state (7)  A[0]:(0.000705646234564) A[1]:(0.99826413393) A[2]:(0.00103022204712) A[3]:(3.53688805403e-14)\n",
      " state (8)  A[0]:(1.19966780403e-05) A[1]:(0.999929785728) A[2]:(5.82342836424e-05) A[3]:(2.66574079874e-16)\n",
      " state (9)  A[0]:(6.6614370553e-07) A[1]:(0.999991714954) A[2]:(7.60188822824e-06) A[3]:(7.89951032979e-18)\n",
      " state (10)  A[0]:(2.60254381601e-07) A[1]:(0.99999576807) A[2]:(3.96655741497e-06) A[3]:(2.50028016687e-18)\n",
      " state (11)  A[0]:(2.04583798791e-07) A[1]:(0.999996423721) A[2]:(3.37397386829e-06) A[3]:(1.86748823904e-18)\n",
      " state (12)  A[0]:(1.91808950945e-07) A[1]:(0.999996542931) A[2]:(3.24250754602e-06) A[3]:(1.73249608538e-18)\n",
      " state (13)  A[0]:(1.87594565659e-07) A[1]:(0.999996602535) A[2]:(3.20823869515e-06) A[3]:(1.69300214024e-18)\n",
      " state (14)  A[0]:(1.85379150253e-07) A[1]:(0.999996602535) A[2]:(3.19688024319e-06) A[3]:(1.67557758064e-18)\n",
      " state (15)  A[0]:(1.83681990507e-07) A[1]:(0.999996602535) A[2]:(3.19134232996e-06) A[3]:(1.66382603247e-18)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 286000 finished after 10 . Running score: 0.08. Policy_loss: -92050.6112083, Value_loss: 1.20100256122. Times trained:               12698. Times reached goal: 121.               Steps done: 3215212.\n",
      " state (0)  A[0]:(0.993281185627) A[1]:(0.00230835890397) A[2]:(0.00252022407949) A[3]:(0.00189020950347)\n",
      " state (1)  A[0]:(0.0334287136793) A[1]:(0.00833511352539) A[2]:(0.00992170069367) A[3]:(0.948314487934)\n",
      " state (2)  A[0]:(0.999978423119) A[1]:(1.86054476217e-05) A[2]:(2.97821748063e-06) A[3]:(3.4386187231e-09)\n",
      " state (3)  A[0]:(0.999975144863) A[1]:(2.17756441998e-05) A[2]:(3.08906760438e-06) A[3]:(3.65502050848e-09)\n",
      " state (4)  A[0]:(0.999910593033) A[1]:(8.09349876363e-05) A[2]:(8.48378749652e-06) A[3]:(7.48948192353e-09)\n",
      " state (5)  A[0]:(0.946439504623) A[1]:(0.049094453454) A[2]:(0.00446606986225) A[3]:(9.91227655511e-10)\n",
      " state (6)  A[0]:(0.0451111644506) A[1]:(0.931557416916) A[2]:(0.0233314111829) A[3]:(6.37211117205e-12)\n",
      " state (7)  A[0]:(0.00171949481592) A[1]:(0.9949837327) A[2]:(0.00329677341506) A[3]:(1.75142289994e-13)\n",
      " state (8)  A[0]:(2.86796985165e-05) A[1]:(0.999773919582) A[2]:(0.000197427885723) A[3]:(1.75628512317e-15)\n",
      " state (9)  A[0]:(9.33689932481e-07) A[1]:(0.999980449677) A[2]:(1.85956942005e-05) A[3]:(3.42544094337e-17)\n",
      " state (10)  A[0]:(2.63563094904e-07) A[1]:(0.999991834164) A[2]:(7.87936551205e-06) A[3]:(7.92927973286e-18)\n",
      " state (11)  A[0]:(1.88249927646e-07) A[1]:(0.999993503094) A[2]:(6.29011719866e-06) A[3]:(5.37744036443e-18)\n",
      " state (12)  A[0]:(1.72762725015e-07) A[1]:(0.999993860722) A[2]:(5.94311632085e-06) A[3]:(4.87273942241e-18)\n",
      " state (13)  A[0]:(1.68855677884e-07) A[1]:(0.999993979931) A[2]:(5.85499583394e-06) A[3]:(4.74698853055e-18)\n",
      " state (14)  A[0]:(1.67726128097e-07) A[1]:(0.999993979931) A[2]:(5.82941720495e-06) A[3]:(4.71053219942e-18)\n",
      " state (15)  A[0]:(1.67342847135e-07) A[1]:(0.999994039536) A[2]:(5.82042957831e-06) A[3]:(4.69782753239e-18)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 287000 finished after 9 . Running score: 0.14. Policy_loss: -92050.6112148, Value_loss: 1.19879894966. Times trained:               12937. Times reached goal: 123.               Steps done: 3228149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993157804012) A[1]:(0.00236213742755) A[2]:(0.00253337714821) A[3]:(0.0019466727972)\n",
      " state (1)  A[0]:(0.0282850358635) A[1]:(0.00784687884152) A[2]:(0.00903908722103) A[3]:(0.954828977585)\n",
      " state (2)  A[0]:(0.999975502491) A[1]:(2.12833347177e-05) A[2]:(3.20643380292e-06) A[3]:(3.90682020068e-09)\n",
      " state (3)  A[0]:(0.999968707561) A[1]:(2.77158978861e-05) A[2]:(3.56852706318e-06) A[3]:(4.76419659279e-09)\n",
      " state (4)  A[0]:(0.999868631363) A[1]:(0.000120836499264) A[2]:(1.05251983769e-05) A[3]:(1.17639045172e-08)\n",
      " state (5)  A[0]:(0.967280387878) A[1]:(0.0307499114424) A[2]:(0.00196971138939) A[3]:(3.09500558515e-09)\n",
      " state (6)  A[0]:(0.0306381322443) A[1]:(0.953881561756) A[2]:(0.0154803078622) A[3]:(5.97593762669e-12)\n",
      " state (7)  A[0]:(0.000171831590706) A[1]:(0.99924993515) A[2]:(0.00057825690601) A[3]:(1.64702676682e-14)\n",
      " state (8)  A[0]:(1.15348268537e-06) A[1]:(0.999981880188) A[2]:(1.69733229995e-05) A[3]:(5.16648606446e-17)\n",
      " state (9)  A[0]:(1.44070455121e-07) A[1]:(0.99999588728) A[2]:(3.9856577132e-06) A[3]:(4.53283188355e-18)\n",
      " state (10)  A[0]:(8.62814886204e-08) A[1]:(0.999997138977) A[2]:(2.79904043055e-06) A[3]:(2.48062759629e-18)\n",
      " state (11)  A[0]:(7.69160592995e-08) A[1]:(0.999997317791) A[2]:(2.58183081314e-06) A[3]:(2.16148084324e-18)\n",
      " state (12)  A[0]:(7.50503730274e-08) A[1]:(0.999997377396) A[2]:(2.53219809565e-06) A[3]:(2.09292618203e-18)\n",
      " state (13)  A[0]:(7.47676125457e-08) A[1]:(0.999997377396) A[2]:(2.51901565207e-06) A[3]:(2.07667870044e-18)\n",
      " state (14)  A[0]:(7.48493818037e-08) A[1]:(0.999997437) A[2]:(2.51471692536e-06) A[3]:(2.07293488098e-18)\n",
      " state (15)  A[0]:(7.50113073877e-08) A[1]:(0.999997437) A[2]:(2.51287815445e-06) A[3]:(2.0724604929e-18)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 288000 finished after 19 . Running score: 0.2. Policy_loss: -92050.6114483, Value_loss: 1.20004812585. Times trained:               12495. Times reached goal: 144.               Steps done: 3240644.\n",
      " state (0)  A[0]:(0.993154883385) A[1]:(0.00236155604944) A[2]:(0.00252800341696) A[3]:(0.00195553153753)\n",
      " state (1)  A[0]:(0.0293835867196) A[1]:(0.00808877125382) A[2]:(0.00932561606169) A[3]:(0.953202009201)\n",
      " state (2)  A[0]:(0.999974310398) A[1]:(2.23636943701e-05) A[2]:(3.31951741828e-06) A[3]:(3.76259068346e-09)\n",
      " state (3)  A[0]:(0.999965667725) A[1]:(3.04928053083e-05) A[2]:(3.81313316211e-06) A[3]:(4.76334660604e-09)\n",
      " state (4)  A[0]:(0.999791324139) A[1]:(0.000193455227418) A[2]:(1.51975773406e-05) A[3]:(1.23621246573e-08)\n",
      " state (5)  A[0]:(0.799474656582) A[1]:(0.190727591515) A[2]:(0.00979776866734) A[3]:(1.04086472774e-09)\n",
      " state (6)  A[0]:(0.00307612679899) A[1]:(0.992670118809) A[2]:(0.00425374088809) A[3]:(5.07216013981e-13)\n",
      " state (7)  A[0]:(5.41854387848e-06) A[1]:(0.999940276146) A[2]:(5.43116984772e-05) A[3]:(4.16546239504e-16)\n",
      " state (8)  A[0]:(9.32679569132e-08) A[1]:(0.999996840954) A[2]:(3.08843073071e-06) A[3]:(3.94411999697e-18)\n",
      " state (9)  A[0]:(3.11248662399e-08) A[1]:(0.999998509884) A[2]:(1.44250043377e-06) A[3]:(1.10899629096e-18)\n",
      " state (10)  A[0]:(2.47603999526e-08) A[1]:(0.999998748302) A[2]:(1.22851508877e-06) A[3]:(8.47476626255e-19)\n",
      " state (11)  A[0]:(2.36671766629e-08) A[1]:(0.999998807907) A[2]:(1.18572961583e-06) A[3]:(7.99673651861e-19)\n",
      " state (12)  A[0]:(2.35129817838e-08) A[1]:(0.999998807907) A[2]:(1.17509148367e-06) A[3]:(7.89055754723e-19)\n",
      " state (13)  A[0]:(2.35553994088e-08) A[1]:(0.999998807907) A[2]:(1.17159470392e-06) A[3]:(7.86471383995e-19)\n",
      " state (14)  A[0]:(2.3638438762e-08) A[1]:(0.999998807907) A[2]:(1.16995340704e-06) A[3]:(7.85877571713e-19)\n",
      " state (15)  A[0]:(2.3726251186e-08) A[1]:(0.999998807907) A[2]:(1.16892181268e-06) A[3]:(7.85841589356e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 289000 finished after 28 . Running score: 0.15. Policy_loss: -92050.6112085, Value_loss: 1.42313521026. Times trained:               12553. Times reached goal: 140.               Steps done: 3253197.\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0024,  0.0026,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0024,  0.0026,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0024,  0.0026,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0024,  0.0026,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9996e-01,  3.4581e-05,  3.6028e-06,  5.5006e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.0074e-08,  1.0000e+00,  1.0473e-06,  4.0321e-18]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.0113e-08,  1.0000e+00,  1.0478e-06,  4.0344e-18]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.0149e-08,  1.0000e+00,  1.0481e-06,  4.0365e-18]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4532e-08,  1.0000e+00,  4.9290e-07,  1.2054e-18]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7398e-08,  1.0000e+00,  3.8157e-07,  7.9744e-19]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7390e-08,  1.0000e+00,  3.8021e-07,  7.9411e-19]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7398e-08,  1.0000e+00,  3.8129e-07,  7.9734e-19]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7398e-08,  1.0000e+00,  3.8119e-07,  7.9731e-19]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7398e-08,  1.0000e+00,  3.8111e-07,  7.9729e-19]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7398e-08,  1.0000e+00,  3.8104e-07,  7.9726e-19]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7398e-08,  1.0000e+00,  3.8097e-07,  7.9724e-19]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993046700954) A[1]:(0.00237634871155) A[2]:(0.00256614596583) A[3]:(0.00201078620739)\n",
      " state (1)  A[0]:(0.0268730055541) A[1]:(0.00752640049905) A[2]:(0.00865117646754) A[3]:(0.956949412823)\n",
      " state (2)  A[0]:(0.999960541725) A[1]:(3.37991186825e-05) A[2]:(5.62461355003e-06) A[3]:(1.47874779088e-08)\n",
      " state (3)  A[0]:(0.999977767467) A[1]:(1.9616991267e-05) A[2]:(2.60378919847e-06) A[3]:(3.29792815279e-09)\n",
      " state (4)  A[0]:(0.999961972237) A[1]:(3.44512081938e-05) A[2]:(3.59333353117e-06) A[3]:(5.4855284759e-09)\n",
      " state (5)  A[0]:(0.998773097992) A[1]:(0.00118158571422) A[2]:(4.53021712019e-05) A[3]:(1.47362069214e-08)\n",
      " state (6)  A[0]:(0.0358755923808) A[1]:(0.957953810692) A[2]:(0.00617061555386) A[3]:(1.47547321583e-11)\n",
      " state (7)  A[0]:(5.83698783885e-06) A[1]:(0.999968111515) A[2]:(2.60413289652e-05) A[3]:(6.25174689578e-16)\n",
      " state (8)  A[0]:(7.02712839029e-08) A[1]:(0.999998867512) A[2]:(1.04762796127e-06) A[3]:(4.04300861202e-18)\n",
      " state (9)  A[0]:(2.45477771443e-08) A[1]:(0.999999463558) A[2]:(4.92275773922e-07) A[3]:(1.20589657061e-18)\n",
      " state (10)  A[0]:(1.91417299789e-08) A[1]:(0.999999582767) A[2]:(4.11487633301e-07) A[3]:(9.01832111872e-19)\n",
      " state (11)  A[0]:(1.78713097654e-08) A[1]:(0.999999582767) A[2]:(3.90726853539e-07) A[3]:(8.29204620114e-19)\n",
      " state (12)  A[0]:(1.75022307758e-08) A[1]:(0.999999582767) A[2]:(3.83790876413e-07) A[3]:(8.05896945199e-19)\n",
      " state (13)  A[0]:(1.74006089537e-08) A[1]:(0.999999582767) A[2]:(3.81077001066e-07) A[3]:(7.97331851848e-19)\n",
      " state (14)  A[0]:(1.73925123192e-08) A[1]:(0.999999582767) A[2]:(3.79894459002e-07) A[3]:(7.94059783839e-19)\n",
      " state (15)  A[0]:(1.74195360358e-08) A[1]:(0.999999582767) A[2]:(3.79329691214e-07) A[3]:(7.92891391223e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 290000 finished after 16 . Running score: 0.1. Policy_loss: -92050.6112084, Value_loss: 1.42371854212. Times trained:               12457. Times reached goal: 124.               Steps done: 3265654.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993111550808) A[1]:(0.00232737977058) A[2]:(0.00248462380841) A[3]:(0.00207643443719)\n",
      " state (1)  A[0]:(0.0264509730041) A[1]:(0.00716229341924) A[2]:(0.008205819875) A[3]:(0.958180904388)\n",
      " state (2)  A[0]:(0.999867796898) A[1]:(0.000107916166598) A[2]:(2.38412976614e-05) A[3]:(4.20073092755e-07)\n",
      " state (3)  A[0]:(0.999983787537) A[1]:(1.44729174281e-05) A[2]:(1.7492693587e-06) A[3]:(2.80063572333e-09)\n",
      " state (4)  A[0]:(0.999971687794) A[1]:(2.59647968051e-05) A[2]:(2.35588299802e-06) A[3]:(4.74956696195e-09)\n",
      " state (5)  A[0]:(0.996041953564) A[1]:(0.00388684449717) A[2]:(7.1207388828e-05) A[3]:(1.50379317887e-08)\n",
      " state (6)  A[0]:(0.00392537051812) A[1]:(0.995258748531) A[2]:(0.000815889856312) A[3]:(1.91234268004e-12)\n",
      " state (7)  A[0]:(4.87884733502e-07) A[1]:(0.999998390675) A[2]:(1.12701309263e-06) A[3]:(1.00371650627e-16)\n",
      " state (8)  A[0]:(1.61815503219e-08) A[1]:(0.999999880791) A[2]:(7.91747538642e-08) A[3]:(2.24498513967e-18)\n",
      " state (9)  A[0]:(7.14095271803e-09) A[1]:(0.999999940395) A[2]:(4.19041583655e-08) A[3]:(8.84274686383e-19)\n",
      " state (10)  A[0]:(5.47008527363e-09) A[1]:(0.999999940395) A[2]:(3.39188410692e-08) A[3]:(6.4619204696e-19)\n",
      " state (11)  A[0]:(4.85544227047e-09) A[1]:(0.999999940395) A[2]:(3.07703302838e-08) A[3]:(5.58846582262e-19)\n",
      " state (12)  A[0]:(4.55795223786e-09) A[1]:(0.999999940395) A[2]:(2.91679036479e-08) A[3]:(5.16080258919e-19)\n",
      " state (13)  A[0]:(4.39647696027e-09) A[1]:(0.999999940395) A[2]:(2.82535221885e-08) A[3]:(4.92313860655e-19)\n",
      " state (14)  A[0]:(4.30464597301e-09) A[1]:(0.999999940395) A[2]:(2.77004978955e-08) A[3]:(4.78245741483e-19)\n",
      " state (15)  A[0]:(4.25185886499e-09) A[1]:(0.999999940395) A[2]:(2.73542362095e-08) A[3]:(4.69624037959e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 291000 finished after 8 . Running score: 0.1. Policy_loss: -92050.6112092, Value_loss: 1.4300031948. Times trained:               12682. Times reached goal: 130.               Steps done: 3278336.\n",
      " state (0)  A[0]:(0.992483973503) A[1]:(0.00248502939939) A[2]:(0.00264891283587) A[3]:(0.00238210451789)\n",
      " state (1)  A[0]:(0.0264683384448) A[1]:(0.00708326278254) A[2]:(0.00906543433666) A[3]:(0.957382977009)\n",
      " state (2)  A[0]:(0.998899519444) A[1]:(0.000665852567181) A[2]:(0.000382927508326) A[3]:(5.16715044796e-05)\n",
      " state (3)  A[0]:(0.999988257885) A[1]:(9.36296055443e-06) A[2]:(2.39418636738e-06) A[3]:(2.34007813127e-09)\n",
      " state (4)  A[0]:(0.999987900257) A[1]:(9.75177863438e-06) A[2]:(2.36576465795e-06) A[3]:(2.32566943481e-09)\n",
      " state (5)  A[0]:(0.999981641769) A[1]:(1.53379332914e-05) A[2]:(3.04333138956e-06) A[3]:(3.82683884581e-09)\n",
      " state (6)  A[0]:(0.999656975269) A[1]:(0.000321319588693) A[2]:(2.16788866965e-05) A[3]:(2.52398351108e-08)\n",
      " state (7)  A[0]:(0.773389160633) A[1]:(0.213299274445) A[2]:(0.0133115407079) A[3]:(2.43622833018e-09)\n",
      " state (8)  A[0]:(0.0202995240688) A[1]:(0.961929857731) A[2]:(0.0177706163377) A[3]:(1.06438790073e-11)\n",
      " state (9)  A[0]:(0.000205336022191) A[1]:(0.998922049999) A[2]:(0.000872640230227) A[3]:(6.59238627771e-14)\n",
      " state (10)  A[0]:(1.59665996762e-05) A[1]:(0.99984639883) A[2]:(0.000137627765071) A[3]:(3.64059631169e-15)\n",
      " state (11)  A[0]:(4.87417673867e-06) A[1]:(0.999937534332) A[2]:(5.75950471102e-05) A[3]:(9.09235235252e-16)\n",
      " state (12)  A[0]:(2.53512598647e-06) A[1]:(0.999961912632) A[2]:(3.55261981895e-05) A[3]:(4.17033812844e-16)\n",
      " state (13)  A[0]:(1.65843357536e-06) A[1]:(0.999972403049) A[2]:(2.59333373833e-05) A[3]:(2.50018609162e-16)\n",
      " state (14)  A[0]:(1.22240805922e-06) A[1]:(0.999978125095) A[2]:(2.06763106689e-05) A[3]:(1.72688392135e-16)\n",
      " state (15)  A[0]:(9.67073219726e-07) A[1]:(0.999981641769) A[2]:(1.73740245373e-05) A[3]:(1.29831423445e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 292000 finished after 42 . Running score: 0.13. Policy_loss: -92050.6114396, Value_loss: 0.993595402648. Times trained:               13105. Times reached goal: 125.               Steps done: 3291441.\n",
      " state (0)  A[0]:(0.992439091206) A[1]:(0.00251791952178) A[2]:(0.00257848319598) A[3]:(0.00246447673999)\n",
      " state (1)  A[0]:(0.0199852753431) A[1]:(0.00612598611042) A[2]:(0.00740727782249) A[3]:(0.96648144722)\n",
      " state (2)  A[0]:(0.999879598618) A[1]:(9.00249069673e-05) A[2]:(2.99607909255e-05) A[3]:(4.26533006248e-07)\n",
      " state (3)  A[0]:(0.999987006187) A[1]:(1.06784946183e-05) A[2]:(2.33498735724e-06) A[3]:(2.66614175182e-09)\n",
      " state (4)  A[0]:(0.999981462955) A[1]:(1.56896239787e-05) A[2]:(2.85438136416e-06) A[3]:(4.1166621223e-09)\n",
      " state (5)  A[0]:(0.999752521515) A[1]:(0.000233321567066) A[2]:(1.41314321809e-05) A[3]:(3.05782101861e-08)\n",
      " state (6)  A[0]:(0.480494767427) A[1]:(0.504064381123) A[2]:(0.0154408011585) A[3]:(1.77590675499e-09)\n",
      " state (7)  A[0]:(0.000862293352839) A[1]:(0.997485280037) A[2]:(0.00165242468938) A[3]:(4.57189082599e-13)\n",
      " state (8)  A[0]:(2.11157362173e-06) A[1]:(0.999977052212) A[2]:(2.08170240512e-05) A[3]:(5.37514191075e-16)\n",
      " state (9)  A[0]:(2.16308379208e-07) A[1]:(0.999995946884) A[2]:(3.81778590963e-06) A[3]:(3.78814872319e-17)\n",
      " state (10)  A[0]:(1.03882229041e-07) A[1]:(0.999997675419) A[2]:(2.2084648208e-06) A[3]:(1.57497488192e-17)\n",
      " state (11)  A[0]:(7.57001146212e-08) A[1]:(0.999998211861) A[2]:(1.74109777618e-06) A[3]:(1.06961624859e-17)\n",
      " state (12)  A[0]:(6.3577239473e-08) A[1]:(0.999998390675) A[2]:(1.52598613568e-06) A[3]:(8.61627473828e-18)\n",
      " state (13)  A[0]:(5.69916878135e-08) A[1]:(0.999998509884) A[2]:(1.40451413699e-06) A[3]:(7.51691199953e-18)\n",
      " state (14)  A[0]:(5.29908525948e-08) A[1]:(0.999998629093) A[2]:(1.32883053539e-06) A[3]:(6.86106199853e-18)\n",
      " state (15)  A[0]:(5.04088042419e-08) A[1]:(0.999998688698) A[2]:(1.27909686398e-06) A[3]:(6.44276627534e-18)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 293000 finished after 20 . Running score: 0.1. Policy_loss: -92050.61258, Value_loss: 1.19961357002. Times trained:               12873. Times reached goal: 126.               Steps done: 3304314.\n",
      " state (0)  A[0]:(0.992410004139) A[1]:(0.00253783888184) A[2]:(0.00256364583038) A[3]:(0.00248851533979)\n",
      " state (1)  A[0]:(0.01891618222) A[1]:(0.00599708827212) A[2]:(0.00696612522006) A[3]:(0.968120634556)\n",
      " state (2)  A[0]:(0.999976694584) A[1]:(1.91727431229e-05) A[2]:(4.15044360125e-06) A[3]:(9.02983110507e-09)\n",
      " state (3)  A[0]:(0.999985039234) A[1]:(1.26682980408e-05) A[2]:(2.28800581681e-06) A[3]:(2.7202617936e-09)\n",
      " state (4)  A[0]:(0.999947845936) A[1]:(4.74916450912e-05) A[2]:(4.64737104267e-06) A[3]:(8.35930613619e-09)\n",
      " state (5)  A[0]:(0.765852868557) A[1]:(0.230837672949) A[2]:(0.00330945150927) A[3]:(6.84025147635e-09)\n",
      " state (6)  A[0]:(7.42538322811e-05) A[1]:(0.999738931656) A[2]:(0.000186827106518) A[3]:(4.15566933892e-14)\n",
      " state (7)  A[0]:(6.33102033021e-08) A[1]:(0.999998867512) A[2]:(1.05037406684e-06) A[3]:(1.64006754412e-17)\n",
      " state (8)  A[0]:(1.04026938175e-08) A[1]:(0.999999701977) A[2]:(2.77742259414e-07) A[3]:(2.1072630832e-18)\n",
      " state (9)  A[0]:(6.88149404127e-09) A[1]:(0.999999761581) A[2]:(2.05131897246e-07) A[3]:(1.30723715579e-18)\n",
      " state (10)  A[0]:(6.01284888546e-09) A[1]:(0.999999821186) A[2]:(1.85598565849e-07) A[3]:(1.11431547589e-18)\n",
      " state (11)  A[0]:(5.67292790521e-09) A[1]:(0.999999821186) A[2]:(1.77667786261e-07) A[3]:(1.03882263049e-18)\n",
      " state (12)  A[0]:(5.51075851618e-09) A[1]:(0.999999821186) A[2]:(1.73805204895e-07) A[3]:(1.00269510365e-18)\n",
      " state (13)  A[0]:(5.42632605516e-09) A[1]:(0.999999821186) A[2]:(1.71759680256e-07) A[3]:(9.83755149161e-19)\n",
      " state (14)  A[0]:(5.38055600074e-09) A[1]:(0.999999821186) A[2]:(1.70627117768e-07) A[3]:(9.73351698597e-19)\n",
      " state (15)  A[0]:(5.35538946522e-09) A[1]:(0.999999821186) A[2]:(1.69982172338e-07) A[3]:(9.67480577405e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 294000 finished after 19 . Running score: 0.09. Policy_loss: -92050.6112208, Value_loss: 1.41403658763. Times trained:               12135. Times reached goal: 120.               Steps done: 3316449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9928,  0.0024,  0.0025,  0.0024]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9998e-01,  1.6396e-05,  2.2965e-06,  3.4799e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9928,  0.0024,  0.0025,  0.0024]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9928,  0.0024,  0.0025,  0.0024]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9998e-01,  1.6389e-05,  2.2941e-06,  3.4782e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.4173e-09,  1.0000e+00,  4.8508e-08,  8.3213e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.992798984051) A[1]:(0.00239662569948) A[2]:(0.00244972156361) A[3]:(0.00235468265601)\n",
      " state (1)  A[0]:(0.0213818699121) A[1]:(0.0061245393008) A[2]:(0.00720695219934) A[3]:(0.965286612511)\n",
      " state (2)  A[0]:(0.999954760075) A[1]:(3.58982942998e-05) A[2]:(9.29730231292e-06) A[3]:(5.76961696197e-08)\n",
      " state (3)  A[0]:(0.999989151955) A[1]:(9.13607709663e-06) A[2]:(1.73623686806e-06) A[3]:(2.13184625508e-09)\n",
      " state (4)  A[0]:(0.999981343746) A[1]:(1.6385136405e-05) A[2]:(2.2927658847e-06) A[3]:(3.47721829108e-09)\n",
      " state (5)  A[0]:(0.994860649109) A[1]:(0.00504471641034) A[2]:(9.46364743868e-05) A[3]:(1.53715351559e-08)\n",
      " state (6)  A[0]:(0.000145019235788) A[1]:(0.99975001812) A[2]:(0.000104975915747) A[3]:(7.81927148898e-14)\n",
      " state (7)  A[0]:(3.05724761063e-08) A[1]:(0.999999761581) A[2]:(2.08040205507e-07) A[3]:(7.03629938412e-18)\n",
      " state (8)  A[0]:(4.41754188785e-09) A[1]:(0.999999940395) A[2]:(4.84797659794e-08) A[3]:(8.32183607693e-19)\n",
      " state (9)  A[0]:(2.69226352323e-09) A[1]:(0.999999940395) A[2]:(3.37430066111e-08) A[3]:(4.85684369937e-19)\n",
      " state (10)  A[0]:(2.22172857889e-09) A[1]:(0.999999940395) A[2]:(2.94840454274e-08) A[3]:(3.96305772841e-19)\n",
      " state (11)  A[0]:(2.01841321434e-09) A[1]:(1.0) A[2]:(2.76940017585e-08) A[3]:(3.59944179734e-19)\n",
      " state (12)  A[0]:(1.9093593373e-09) A[1]:(1.0) A[2]:(2.68214748189e-08) A[3]:(3.42131232983e-19)\n",
      " state (13)  A[0]:(1.84209225651e-09) A[1]:(1.0) A[2]:(2.63687542912e-08) A[3]:(3.32524615843e-19)\n",
      " state (14)  A[0]:(1.79560433189e-09) A[1]:(1.0) A[2]:(2.61287933512e-08) A[3]:(3.26997343291e-19)\n",
      " state (15)  A[0]:(1.76016901055e-09) A[1]:(1.0) A[2]:(2.60030095234e-08) A[3]:(3.23633096342e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 295000 finished after 6 . Running score: 0.15. Policy_loss: -92050.6115475, Value_loss: 1.21216131301. Times trained:               13015. Times reached goal: 146.               Steps done: 3329464.\n",
      " state (0)  A[0]:(0.993073880672) A[1]:(0.00227496749721) A[2]:(0.00230188504793) A[3]:(0.00234925188124)\n",
      " state (1)  A[0]:(0.021479383111) A[1]:(0.00572997052222) A[2]:(0.00663552293554) A[3]:(0.96615511179)\n",
      " state (2)  A[0]:(0.999990105629) A[1]:(8.44973146741e-06) A[2]:(1.45919136685e-06) A[3]:(3.92156263018e-09)\n",
      " state (3)  A[0]:(0.999992728233) A[1]:(6.30874774288e-06) A[2]:(9.40244092362e-07) A[3]:(1.71893954626e-09)\n",
      " state (4)  A[0]:(0.999983429909) A[1]:(1.51464791998e-05) A[2]:(1.4138207689e-06) A[3]:(3.09630299178e-09)\n",
      " state (5)  A[0]:(0.944578647614) A[1]:(0.0550757311285) A[2]:(0.000345637032297) A[3]:(3.57326812583e-09)\n",
      " state (6)  A[0]:(3.55413976649e-06) A[1]:(0.999994397163) A[2]:(2.0585441689e-06) A[3]:(8.56529033626e-16)\n",
      " state (7)  A[0]:(1.06118571708e-08) A[1]:(0.999999940395) A[2]:(2.14600639481e-08) A[3]:(1.49732067562e-18)\n",
      " state (8)  A[0]:(3.8746348352e-09) A[1]:(1.0) A[2]:(9.82018732998e-09) A[3]:(5.05785737701e-19)\n",
      " state (9)  A[0]:(3.03252378764e-09) A[1]:(1.0) A[2]:(8.15668688148e-09) A[3]:(3.89813516557e-19)\n",
      " state (10)  A[0]:(2.78385670072e-09) A[1]:(1.0) A[2]:(7.65687335758e-09) A[3]:(3.56334725432e-19)\n",
      " state (11)  A[0]:(2.68213584675e-09) A[1]:(1.0) A[2]:(7.45954231718e-09) A[3]:(3.43144193191e-19)\n",
      " state (12)  A[0]:(2.63234656295e-09) A[1]:(1.0) A[2]:(7.37219973956e-09) A[3]:(3.37172802199e-19)\n",
      " state (13)  A[0]:(2.60425059295e-09) A[1]:(1.0) A[2]:(7.33173033396e-09) A[3]:(3.34245306613e-19)\n",
      " state (14)  A[0]:(2.58599563985e-09) A[1]:(1.0) A[2]:(7.31288984923e-09) A[3]:(3.32711119222e-19)\n",
      " state (15)  A[0]:(2.5724034014e-09) A[1]:(1.0) A[2]:(7.30452587305e-09) A[3]:(3.31847930403e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 296000 finished after 5 . Running score: 0.13. Policy_loss: -92050.6112092, Value_loss: 1.02037799965. Times trained:               12981. Times reached goal: 136.               Steps done: 3342445.\n",
      " state (0)  A[0]:(0.993730485439) A[1]:(0.00204343022779) A[2]:(0.0021099045407) A[3]:(0.00211616978049)\n",
      " state (1)  A[0]:(0.0253439266235) A[1]:(0.00600551534444) A[2]:(0.0069709061645) A[3]:(0.961679637432)\n",
      " state (2)  A[0]:(0.999996542931) A[1]:(2.97217115985e-06) A[2]:(4.66546936195e-07) A[3]:(2.37606023745e-09)\n",
      " state (3)  A[0]:(0.999997913837) A[1]:(1.83482552529e-06) A[2]:(2.50706165161e-07) A[3]:(8.06902156203e-10)\n",
      " state (4)  A[0]:(0.999997258186) A[1]:(2.46253580372e-06) A[2]:(2.80720342971e-07) A[3]:(9.88576220884e-10)\n",
      " state (5)  A[0]:(0.999910235405) A[1]:(8.80475781742e-05) A[2]:(1.73717137386e-06) A[3]:(2.18303686239e-09)\n",
      " state (6)  A[0]:(6.05561253906e-05) A[1]:(0.99993789196) A[2]:(1.53002940806e-06) A[3]:(5.67043877199e-15)\n",
      " state (7)  A[0]:(5.04249264566e-08) A[1]:(0.999999940395) A[2]:(7.05568536929e-09) A[3]:(3.02579028477e-18)\n",
      " state (8)  A[0]:(1.3352511985e-08) A[1]:(1.0) A[2]:(2.50199083673e-09) A[3]:(7.97469060432e-19)\n",
      " state (9)  A[0]:(8.93195650775e-09) A[1]:(1.0) A[2]:(1.85804549524e-09) A[3]:(5.44183565134e-19)\n",
      " state (10)  A[0]:(7.56811591174e-09) A[1]:(1.0) A[2]:(1.66129932033e-09) A[3]:(4.6996276842e-19)\n",
      " state (11)  A[0]:(6.97138302641e-09) A[1]:(1.0) A[2]:(1.58428892227e-09) A[3]:(4.40431128082e-19)\n",
      " state (12)  A[0]:(6.65424693125e-09) A[1]:(1.0) A[2]:(1.55112322986e-09) A[3]:(4.26888604641e-19)\n",
      " state (13)  A[0]:(6.46208908606e-09) A[1]:(1.0) A[2]:(1.53641555034e-09) A[3]:(4.20121259104e-19)\n",
      " state (14)  A[0]:(6.33441832321e-09) A[1]:(1.0) A[2]:(1.52998491654e-09) A[3]:(4.16502162978e-19)\n",
      " state (15)  A[0]:(6.2440914661e-09) A[1]:(1.0) A[2]:(1.52736656656e-09) A[3]:(4.14444938946e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 297000 finished after 9 . Running score: 0.13. Policy_loss: -92050.6112121, Value_loss: 1.6329739034. Times trained:               13074. Times reached goal: 138.               Steps done: 3355519.\n",
      " state (0)  A[0]:(0.993692934513) A[1]:(0.00207149307244) A[2]:(0.0021874580998) A[3]:(0.00204812875018)\n",
      " state (1)  A[0]:(0.0300760474056) A[1]:(0.00649000750855) A[2]:(0.00807751435786) A[3]:(0.955356419086)\n",
      " state (2)  A[0]:(0.999922811985) A[1]:(5.31131918251e-05) A[2]:(2.24397936108e-05) A[3]:(1.66184554473e-06)\n",
      " state (3)  A[0]:(0.999998450279) A[1]:(1.25982819554e-06) A[2]:(2.96241807973e-07) A[3]:(7.24020565723e-10)\n",
      " state (4)  A[0]:(0.999998629093) A[1]:(1.13424300707e-06) A[2]:(2.60808462826e-07) A[3]:(5.86665438451e-10)\n",
      " state (5)  A[0]:(0.999998629093) A[1]:(1.13031239835e-06) A[2]:(2.57320010633e-07) A[3]:(5.80554326834e-10)\n",
      " state (6)  A[0]:(0.999998569489) A[1]:(1.16633691505e-06) A[2]:(2.61179565086e-07) A[3]:(6.02903005298e-10)\n",
      " state (7)  A[0]:(0.99999833107) A[1]:(1.36985636345e-06) A[2]:(2.8318069667e-07) A[3]:(6.93476498448e-10)\n",
      " state (8)  A[0]:(0.999996304512) A[1]:(3.25248538502e-06) A[2]:(4.56236335822e-07) A[3]:(1.10629050365e-09)\n",
      " state (9)  A[0]:(0.999933183193) A[1]:(6.28952475381e-05) A[2]:(3.9159717744e-06) A[3]:(1.19218757089e-09)\n",
      " state (10)  A[0]:(0.988920092583) A[1]:(0.010811958462) A[2]:(0.000267947383691) A[3]:(2.46379416868e-10)\n",
      " state (11)  A[0]:(0.478834271431) A[1]:(0.517731189728) A[2]:(0.00343456841074) A[3]:(4.06300063038e-11)\n",
      " state (12)  A[0]:(0.0313710682094) A[1]:(0.967391490936) A[2]:(0.00123743934091) A[3]:(1.63594886336e-12)\n",
      " state (13)  A[0]:(0.00275738933124) A[1]:(0.996982693672) A[2]:(0.000259939959506) A[3]:(1.07524063167e-13)\n",
      " state (14)  A[0]:(0.000459805014543) A[1]:(0.999469459057) A[2]:(7.0710957516e-05) A[3]:(1.4507323023e-14)\n",
      " state (15)  A[0]:(0.00012682013039) A[1]:(0.999846875668) A[2]:(2.62864268734e-05) A[3]:(3.40987766587e-15)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 298000 finished after 34 . Running score: 0.0. Policy_loss: -92050.6111931, Value_loss: 0.97967535182. Times trained:               14435. Times reached goal: 71.               Steps done: 3369954.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993782520294) A[1]:(0.00202737515792) A[2]:(0.00229047844186) A[3]:(0.00189961306751)\n",
      " state (1)  A[0]:(0.0309354141355) A[1]:(0.00683703646064) A[2]:(0.010042396374) A[3]:(0.952185153961)\n",
      " state (2)  A[0]:(0.999997138977) A[1]:(1.88821024949e-06) A[2]:(9.90263743006e-07) A[3]:(1.20955956362e-09)\n",
      " state (3)  A[0]:(0.999998033047) A[1]:(1.32407080855e-06) A[2]:(6.62948025365e-07) A[3]:(5.72471958726e-10)\n",
      " state (4)  A[0]:(0.999997854233) A[1]:(1.45885746861e-06) A[2]:(7.14215047992e-07) A[3]:(6.23582685488e-10)\n",
      " state (5)  A[0]:(0.999996125698) A[1]:(2.56606585936e-06) A[2]:(1.28811302602e-06) A[3]:(5.78627090686e-10)\n",
      " state (6)  A[0]:(0.998048782349) A[1]:(0.000948766246438) A[2]:(0.00100243999623) A[3]:(2.86249895315e-11)\n",
      " state (7)  A[0]:(0.0311502404511) A[1]:(0.933053255081) A[2]:(0.0357964709401) A[3]:(7.06079346467e-13)\n",
      " state (8)  A[0]:(0.0001702010195) A[1]:(0.998605132103) A[2]:(0.00122467929032) A[3]:(2.13531457937e-15)\n",
      " state (9)  A[0]:(1.72697400558e-05) A[1]:(0.999740540981) A[2]:(0.000242190100835) A[3]:(1.54034463471e-16)\n",
      " state (10)  A[0]:(6.17656041868e-06) A[1]:(0.999878048897) A[2]:(0.000115746763186) A[3]:(4.68038245871e-17)\n",
      " state (11)  A[0]:(3.75007380171e-06) A[1]:(0.999915421009) A[2]:(8.08388795122e-05) A[3]:(2.62112161771e-17)\n",
      " state (12)  A[0]:(2.89324657388e-06) A[1]:(0.999929964542) A[2]:(6.711329479e-05) A[3]:(1.93861423255e-17)\n",
      " state (13)  A[0]:(2.50869402407e-06) A[1]:(0.999936878681) A[2]:(6.06236135354e-05) A[3]:(1.64267051607e-17)\n",
      " state (14)  A[0]:(2.31122021432e-06) A[1]:(0.99994045496) A[2]:(5.72086246393e-05) A[3]:(1.49388338862e-17)\n",
      " state (15)  A[0]:(2.20107267523e-06) A[1]:(0.999942541122) A[2]:(5.52859310119e-05) A[3]:(1.41210301931e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 299000 finished after 14 . Running score: 0.03. Policy_loss: -92050.6124949, Value_loss: 0.979835137156. Times trained:               15686. Times reached goal: 54.               Steps done: 3385640.\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0020,  0.0023,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0020,  0.0023,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0020,  0.0023,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0020,  0.0023,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1288e-06,  6.2421e-07,  5.5402e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0020,  0.0023,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0020,  0.0023,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1288e-06,  6.2422e-07,  5.5402e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1288e-06,  6.2422e-07,  5.5402e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9993e-01,  1.6991e-05,  5.5231e-05,  7.6181e-12]])\n",
      "On state=8, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1288e-06,  6.2422e-07,  5.5402e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1288e-06,  6.2422e-07,  5.5402e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0020,  0.0023,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0020,  0.0023,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1288e-06,  6.2422e-07,  5.5402e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9993e-01,  1.6992e-05,  5.5236e-05,  7.6173e-12]])\n",
      "On state=8, selected action=0\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993745326996) A[1]:(0.00204251054674) A[2]:(0.00228355498984) A[3]:(0.00192862574477)\n",
      " state (1)  A[0]:(0.0281005669385) A[1]:(0.00627098092809) A[2]:(0.00934501364827) A[3]:(0.956283450127)\n",
      " state (2)  A[0]:(0.999976634979) A[1]:(1.37677252496e-05) A[2]:(9.49096920522e-06) A[3]:(9.66239070976e-08)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(1.17432557545e-06) A[2]:(6.52237588383e-07) A[3]:(6.04199024146e-10)\n",
      " state (4)  A[0]:(0.999998271465) A[1]:(1.12884481496e-06) A[2]:(6.24217591394e-07) A[3]:(5.5402354926e-10)\n",
      " state (5)  A[0]:(0.999998211861) A[1]:(1.14927195227e-06) A[2]:(6.35890842204e-07) A[3]:(5.55947898828e-10)\n",
      " state (6)  A[0]:(0.999998092651) A[1]:(1.22866197216e-06) A[2]:(6.9526271318e-07) A[3]:(5.39104205721e-10)\n",
      " state (7)  A[0]:(0.99999666214) A[1]:(1.84372697731e-06) A[2]:(1.48895628627e-06) A[3]:(2.30425192815e-10)\n",
      " state (8)  A[0]:(0.999927759171) A[1]:(1.69925369846e-05) A[2]:(5.52383135073e-05) A[3]:(7.61705368918e-12)\n",
      " state (9)  A[0]:(0.990965366364) A[1]:(0.00211852439679) A[2]:(0.00691612018272) A[3]:(8.7144189434e-12)\n",
      " state (10)  A[0]:(0.784591495991) A[1]:(0.0982750505209) A[2]:(0.117133446038) A[3]:(1.61066056326e-11)\n",
      " state (11)  A[0]:(0.302655279636) A[1]:(0.52040374279) A[2]:(0.176940962672) A[3]:(8.25350118111e-12)\n",
      " state (12)  A[0]:(0.0636807754636) A[1]:(0.856507182121) A[2]:(0.0798120051622) A[3]:(1.67656242622e-12)\n",
      " state (13)  A[0]:(0.0131682809442) A[1]:(0.959306538105) A[2]:(0.0275251846761) A[3]:(2.97453657842e-13)\n",
      " state (14)  A[0]:(0.00359259685501) A[1]:(0.985625207424) A[2]:(0.0107822110876) A[3]:(6.81634246659e-14)\n",
      " state (15)  A[0]:(0.00133497465868) A[1]:(0.993482112885) A[2]:(0.00518294237554) A[3]:(2.16702893287e-14)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 300000 finished after 16 . Running score: 0.0. Policy_loss: -92050.6112293, Value_loss: 0.979715165466. Times trained:               14436. Times reached goal: 112.               Steps done: 3400076.\n",
      " state (0)  A[0]:(0.993328154087) A[1]:(0.00213934946805) A[2]:(0.00240899133496) A[3]:(0.00212347856723)\n",
      " state (1)  A[0]:(0.0204345975071) A[1]:(0.00532607687637) A[2]:(0.00797556526959) A[3]:(0.966263771057)\n",
      " state (2)  A[0]:(0.999995112419) A[1]:(2.98698842016e-06) A[2]:(1.89821253116e-06) A[3]:(3.7576133316e-09)\n",
      " state (3)  A[0]:(0.999997973442) A[1]:(1.26795384858e-06) A[2]:(7.83030657203e-07) A[3]:(6.20685003394e-10)\n",
      " state (4)  A[0]:(0.999997854233) A[1]:(1.31483443511e-06) A[2]:(8.41972450871e-07) A[3]:(5.7130139508e-10)\n",
      " state (5)  A[0]:(0.999997317791) A[1]:(1.51918902702e-06) A[2]:(1.16502599212e-06) A[3]:(3.80280501533e-10)\n",
      " state (6)  A[0]:(0.999989628792) A[1]:(3.11712869916e-06) A[2]:(7.23439325157e-06) A[3]:(2.72732669782e-11)\n",
      " state (7)  A[0]:(0.998961627483) A[1]:(5.42678608326e-05) A[2]:(0.000984109705314) A[3]:(1.44617130771e-12)\n",
      " state (8)  A[0]:(0.85154902935) A[1]:(0.0136198541149) A[2]:(0.134831115603) A[3]:(5.89939729012e-12)\n",
      " state (9)  A[0]:(0.292093575001) A[1]:(0.365457683802) A[2]:(0.342448711395) A[3]:(9.9528900746e-12)\n",
      " state (10)  A[0]:(0.0313936881721) A[1]:(0.877269089222) A[2]:(0.0913372412324) A[3]:(1.27187149701e-12)\n",
      " state (11)  A[0]:(0.00294960220344) A[1]:(0.981392741203) A[2]:(0.0156576763839) A[3]:(9.88136777806e-14)\n",
      " state (12)  A[0]:(0.00049245468108) A[1]:(0.995560824871) A[2]:(0.0039467071183) A[3]:(1.32193171001e-14)\n",
      " state (13)  A[0]:(0.000145452402649) A[1]:(0.998325049877) A[2]:(0.00152950407937) A[3]:(3.27749255721e-15)\n",
      " state (14)  A[0]:(6.48416826152e-05) A[1]:(0.999121487141) A[2]:(0.00081366347149) A[3]:(1.28934889363e-15)\n",
      " state (15)  A[0]:(3.78610493499e-05) A[1]:(0.999428451061) A[2]:(0.000533679267392) A[3]:(6.89908494357e-16)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 301000 finished after 39 . Running score: 0.1. Policy_loss: -92050.6127942, Value_loss: 0.985733181187. Times trained:               14290. Times reached goal: 91.               Steps done: 3414366.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993303894997) A[1]:(0.00216386769898) A[2]:(0.00241379393265) A[3]:(0.00211844802834)\n",
      " state (1)  A[0]:(0.0185003299266) A[1]:(0.00523090781644) A[2]:(0.00775196962059) A[3]:(0.968516767025)\n",
      " state (2)  A[0]:(0.999996840954) A[1]:(1.96614269043e-06) A[2]:(1.21690356991e-06) A[3]:(1.11866149677e-09)\n",
      " state (3)  A[0]:(0.999996781349) A[1]:(1.87568764431e-06) A[2]:(1.3504763956e-06) A[3]:(5.44735700991e-10)\n",
      " state (4)  A[0]:(0.999990284443) A[1]:(3.54463736585e-06) A[2]:(6.16000534137e-06) A[3]:(7.77311132127e-11)\n",
      " state (5)  A[0]:(0.999372899532) A[1]:(3.0516375773e-05) A[2]:(0.000596594589297) A[3]:(1.43046783186e-12)\n",
      " state (6)  A[0]:(0.831969976425) A[1]:(0.00585991796106) A[2]:(0.162170127034) A[3]:(2.31027111504e-12)\n",
      " state (7)  A[0]:(0.0389470718801) A[1]:(0.793222010136) A[2]:(0.167830914259) A[3]:(1.72870344341e-12)\n",
      " state (8)  A[0]:(0.000133066670969) A[1]:(0.997387945652) A[2]:(0.00247897068039) A[3]:(4.92323077515e-15)\n",
      " state (9)  A[0]:(6.98480926076e-06) A[1]:(0.99973577261) A[2]:(0.000257244159002) A[3]:(1.9708730853e-16)\n",
      " state (10)  A[0]:(2.00082536139e-06) A[1]:(0.999900221825) A[2]:(9.77841846179e-05) A[3]:(4.95855601952e-17)\n",
      " state (11)  A[0]:(1.15468139938e-06) A[1]:(0.999935209751) A[2]:(6.36226905044e-05) A[3]:(2.68342486145e-17)\n",
      " state (12)  A[0]:(8.92092145932e-07) A[1]:(0.999947249889) A[2]:(5.18329070474e-05) A[3]:(2.00177493171e-17)\n",
      " state (13)  A[0]:(7.85070710663e-07) A[1]:(0.999952495098) A[2]:(4.6728473535e-05) A[3]:(1.72591003199e-17)\n",
      " state (14)  A[0]:(7.35052083201e-07) A[1]:(0.999955058098) A[2]:(4.4233052904e-05) A[3]:(1.59571081095e-17)\n",
      " state (15)  A[0]:(7.09892447048e-07) A[1]:(0.9999563694) A[2]:(4.29213723692e-05) A[3]:(1.5286810572e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 302000 finished after 5 . Running score: 0.11. Policy_loss: -92050.6111613, Value_loss: 1.42176375873. Times trained:               13022. Times reached goal: 129.               Steps done: 3427388.\n",
      " state (0)  A[0]:(0.99301725626) A[1]:(0.00222508469597) A[2]:(0.00248841918074) A[3]:(0.00226926058531)\n",
      " state (1)  A[0]:(0.0173795428127) A[1]:(0.0050461771898) A[2]:(0.00749892462045) A[3]:(0.970075368881)\n",
      " state (2)  A[0]:(0.999997019768) A[1]:(1.7978275082e-06) A[2]:(1.1711534853e-06) A[3]:(9.63238155904e-10)\n",
      " state (3)  A[0]:(0.999996304512) A[1]:(1.98866155188e-06) A[2]:(1.6920193957e-06) A[3]:(4.39364267191e-10)\n",
      " state (4)  A[0]:(0.999977052212) A[1]:(5.21379888596e-06) A[2]:(1.77436850208e-05) A[3]:(2.68581389767e-11)\n",
      " state (5)  A[0]:(0.995632231236) A[1]:(7.38053713576e-05) A[2]:(0.00429397076368) A[3]:(5.31473790758e-13)\n",
      " state (6)  A[0]:(0.773659825325) A[1]:(0.00749809946865) A[2]:(0.218842044473) A[3]:(2.77317708727e-12)\n",
      " state (7)  A[0]:(0.149557694793) A[1]:(0.495959609747) A[2]:(0.354482680559) A[3]:(5.49315627707e-12)\n",
      " state (8)  A[0]:(0.00254236976616) A[1]:(0.979057312012) A[2]:(0.0184002965689) A[3]:(8.21727533023e-14)\n",
      " state (9)  A[0]:(0.000111562214443) A[1]:(0.998316407204) A[2]:(0.00157201883849) A[3]:(2.55960947628e-15)\n",
      " state (10)  A[0]:(2.27701948461e-05) A[1]:(0.999531447887) A[2]:(0.000445810670499) A[3]:(4.31221852939e-16)\n",
      " state (11)  A[0]:(1.06541074274e-05) A[1]:(0.999746203423) A[2]:(0.00024313427275) A[3]:(1.83049934421e-16)\n",
      " state (12)  A[0]:(7.35742059987e-06) A[1]:(0.999812304974) A[2]:(0.000180350878509) A[3]:(1.19992751951e-16)\n",
      " state (13)  A[0]:(6.10372353549e-06) A[1]:(0.999839127064) A[2]:(0.000154742505401) A[3]:(9.66230864663e-17)\n",
      " state (14)  A[0]:(5.53701647732e-06) A[1]:(0.999851882458) A[2]:(0.000142601202242) A[3]:(8.60788894667e-17)\n",
      " state (15)  A[0]:(5.25515542904e-06) A[1]:(0.999858438969) A[2]:(0.000136294256663) A[3]:(8.07547249104e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 303000 finished after 29 . Running score: 0.08. Policy_loss: -92050.613155, Value_loss: 0.978323792382. Times trained:               13675. Times reached goal: 97.               Steps done: 3441063.\n",
      " state (0)  A[0]:(0.992137312889) A[1]:(0.00234385230578) A[2]:(0.00263263517991) A[3]:(0.00288622966036)\n",
      " state (1)  A[0]:(0.0175332650542) A[1]:(0.00500427139923) A[2]:(0.0071869683452) A[3]:(0.970275521278)\n",
      " state (2)  A[0]:(0.99999755621) A[1]:(1.54149961418e-06) A[2]:(8.79579829416e-07) A[3]:(6.78838263379e-10)\n",
      " state (3)  A[0]:(0.999996542931) A[1]:(1.93848450181e-06) A[2]:(1.524813797e-06) A[3]:(3.49131196176e-10)\n",
      " state (4)  A[0]:(0.999951720238) A[1]:(7.53099993744e-06) A[2]:(4.07271145377e-05) A[3]:(8.80005443515e-12)\n",
      " state (5)  A[0]:(0.990125715733) A[1]:(0.00014465591812) A[2]:(0.00972960889339) A[3]:(2.61764785195e-13)\n",
      " state (6)  A[0]:(0.744899570942) A[1]:(0.0315086133778) A[2]:(0.223591804504) A[3]:(3.79157165734e-12)\n",
      " state (7)  A[0]:(0.0171848647296) A[1]:(0.948174893856) A[2]:(0.0346402451396) A[3]:(3.34562239024e-13)\n",
      " state (8)  A[0]:(0.000151185158757) A[1]:(0.999107182026) A[2]:(0.000741657044273) A[3]:(1.96784007207e-15)\n",
      " state (9)  A[0]:(1.7357075194e-05) A[1]:(0.999857246876) A[2]:(0.000125403850689) A[3]:(1.78287941661e-16)\n",
      " state (10)  A[0]:(7.1907597885e-06) A[1]:(0.999931931496) A[2]:(6.08694645052e-05) A[3]:(6.70503207764e-17)\n",
      " state (11)  A[0]:(4.97536257171e-06) A[1]:(0.999950051308) A[2]:(4.49869330623e-05) A[3]:(4.45084513268e-17)\n",
      " state (12)  A[0]:(4.23717301601e-06) A[1]:(0.9999563694) A[2]:(3.94091075577e-05) A[3]:(3.71829067348e-17)\n",
      " state (13)  A[0]:(3.93961499867e-06) A[1]:(0.9999589324) A[2]:(3.70992820535e-05) A[3]:(3.42458332251e-17)\n",
      " state (14)  A[0]:(3.80720007342e-06) A[1]:(0.999960124493) A[2]:(3.60535304935e-05) A[3]:(3.29336832368e-17)\n",
      " state (15)  A[0]:(3.74461501451e-06) A[1]:(0.999960720539) A[2]:(3.55524571205e-05) A[3]:(3.23081659489e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 304000 finished after 11 . Running score: 0.18. Policy_loss: -92050.6130296, Value_loss: 0.978857106434. Times trained:               12821. Times reached goal: 134.               Steps done: 3453884.\n",
      "action_dist \n",
      "tensor([[ 0.9929,  0.0022,  0.0025,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  3.8584e-06,  8.4753e-06,  7.4711e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  3.8576e-06,  8.4714e-06,  7.4755e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9929,  0.0022,  0.0025,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9929,  0.0022,  0.0025,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  3.8557e-06,  8.4617e-06,  7.4860e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.5095e-04,  9.9774e-01,  1.9102e-03,  4.0060e-15]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.992850065231) A[1]:(0.00217942916788) A[2]:(0.00249536545016) A[3]:(0.00247515644878)\n",
      " state (1)  A[0]:(0.0178898219019) A[1]:(0.00500774942338) A[2]:(0.00744997058064) A[3]:(0.969652473927)\n",
      " state (2)  A[0]:(0.999996244907) A[1]:(2.27233135774e-06) A[2]:(1.4673471469e-06) A[3]:(1.70688674306e-09)\n",
      " state (3)  A[0]:(0.999997079372) A[1]:(1.67702739873e-06) A[2]:(1.245856879e-06) A[3]:(4.98174057562e-10)\n",
      " state (4)  A[0]:(0.999987721443) A[1]:(3.8538746594e-06) A[2]:(8.45292743179e-06) A[3]:(7.49596912319e-11)\n",
      " state (5)  A[0]:(0.997423171997) A[1]:(5.11378930241e-05) A[2]:(0.00252568349242) A[3]:(4.61760566849e-13)\n",
      " state (6)  A[0]:(0.854781508446) A[1]:(0.00488452892751) A[2]:(0.140333965421) A[3]:(1.32042326292e-12)\n",
      " state (7)  A[0]:(0.0737522989511) A[1]:(0.792131185532) A[2]:(0.134116500616) A[3]:(1.23387194559e-12)\n",
      " state (8)  A[0]:(0.000351779628545) A[1]:(0.997734189034) A[2]:(0.00191405613441) A[3]:(4.01666072907e-15)\n",
      " state (9)  A[0]:(2.44331167778e-05) A[1]:(0.999761223793) A[2]:(0.000214324885746) A[3]:(2.05435162332e-16)\n",
      " state (10)  A[0]:(8.19885281089e-06) A[1]:(0.999904692173) A[2]:(8.7130596512e-05) A[3]:(6.07340688806e-17)\n",
      " state (11)  A[0]:(5.1880524552e-06) A[1]:(0.999935209751) A[2]:(5.96050776949e-05) A[3]:(3.63479638594e-17)\n",
      " state (12)  A[0]:(4.24063182436e-06) A[1]:(0.99994546175) A[2]:(5.03134106111e-05) A[3]:(2.8904678379e-17)\n",
      " state (13)  A[0]:(3.86503825212e-06) A[1]:(0.99994969368) A[2]:(4.64620752609e-05) A[3]:(2.59552037775e-17)\n",
      " state (14)  A[0]:(3.69843610315e-06) A[1]:(0.999951601028) A[2]:(4.46786070825e-05) A[3]:(2.46204800304e-17)\n",
      " state (15)  A[0]:(3.6201806779e-06) A[1]:(0.999952614307) A[2]:(4.37921407865e-05) A[3]:(2.39675599031e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 305000 finished after 7 . Running score: 0.06. Policy_loss: -92050.611738, Value_loss: 1.62874504246. Times trained:               12691. Times reached goal: 111.               Steps done: 3466575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993470191956) A[1]:(0.00208007334732) A[2]:(0.00234807026573) A[3]:(0.00210163835436)\n",
      " state (1)  A[0]:(0.0218686982989) A[1]:(0.00562302116305) A[2]:(0.00826549530029) A[3]:(0.964242815971)\n",
      " state (2)  A[0]:(0.999994516373) A[1]:(3.40889437211e-06) A[2]:(2.07928542295e-06) A[3]:(4.05386613167e-09)\n",
      " state (3)  A[0]:(0.999997794628) A[1]:(1.3974798776e-06) A[2]:(8.11954009805e-07) A[3]:(5.42188405284e-10)\n",
      " state (4)  A[0]:(0.999996781349) A[1]:(1.87552848274e-06) A[2]:(1.33471223762e-06) A[3]:(4.58228177624e-10)\n",
      " state (5)  A[0]:(0.99996560812) A[1]:(7.82640654506e-06) A[2]:(2.65780399786e-05) A[3]:(4.18657469803e-11)\n",
      " state (6)  A[0]:(0.973096549511) A[1]:(0.000698464224115) A[2]:(0.0262050032616) A[3]:(5.48576429508e-13)\n",
      " state (7)  A[0]:(0.311748355627) A[1]:(0.453080266714) A[2]:(0.235171362758) A[3]:(2.7418876631e-12)\n",
      " state (8)  A[0]:(0.00143190193921) A[1]:(0.994597911835) A[2]:(0.00397020252421) A[3]:(1.0766569724e-14)\n",
      " state (9)  A[0]:(5.36113002454e-05) A[1]:(0.999683439732) A[2]:(0.000262945541181) A[3]:(2.63983879591e-16)\n",
      " state (10)  A[0]:(1.28892488647e-05) A[1]:(0.999906659126) A[2]:(8.04779483587e-05) A[3]:(5.25354053735e-17)\n",
      " state (11)  A[0]:(6.93061429047e-06) A[1]:(0.999944984913) A[2]:(4.80669550598e-05) A[3]:(2.60365950411e-17)\n",
      " state (12)  A[0]:(5.21433503309e-06) A[1]:(0.999956846237) A[2]:(3.79426055588e-05) A[3]:(1.88551089158e-17)\n",
      " state (13)  A[0]:(4.54521386928e-06) A[1]:(0.999961614609) A[2]:(3.38435056619e-05) A[3]:(1.61226633461e-17)\n",
      " state (14)  A[0]:(4.24091194873e-06) A[1]:(0.999963819981) A[2]:(3.19433274854e-05) A[3]:(1.48908210148e-17)\n",
      " state (15)  A[0]:(4.08960795539e-06) A[1]:(0.999964892864) A[2]:(3.09886927425e-05) A[3]:(1.42790862102e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 306000 finished after 17 . Running score: 0.14. Policy_loss: -92050.6114367, Value_loss: 1.21797125981. Times trained:               12738. Times reached goal: 137.               Steps done: 3479313.\n",
      " state (0)  A[0]:(0.993388950825) A[1]:(0.00210833083838) A[2]:(0.00235631898977) A[3]:(0.00214639538899)\n",
      " state (1)  A[0]:(0.0195076838136) A[1]:(0.00538512971252) A[2]:(0.00784805417061) A[3]:(0.96725910902)\n",
      " state (2)  A[0]:(0.999996900558) A[1]:(1.96746509573e-06) A[2]:(1.15742284379e-06) A[3]:(1.08985043212e-09)\n",
      " state (3)  A[0]:(0.999996721745) A[1]:(1.97930398826e-06) A[2]:(1.30231103412e-06) A[3]:(6.33759711377e-10)\n",
      " state (4)  A[0]:(0.999979674816) A[1]:(6.82922200212e-06) A[2]:(1.34964539029e-05) A[3]:(1.66967314974e-10)\n",
      " state (5)  A[0]:(0.990539669991) A[1]:(0.000244745548116) A[2]:(0.00921559333801) A[3]:(1.22448785052e-12)\n",
      " state (6)  A[0]:(0.657513260841) A[1]:(0.0569244287908) A[2]:(0.285562336445) A[3]:(5.06284727683e-12)\n",
      " state (7)  A[0]:(0.0294031873345) A[1]:(0.915961205959) A[2]:(0.0546356141567) A[3]:(3.35337850153e-13)\n",
      " state (8)  A[0]:(0.000401195487939) A[1]:(0.998060584068) A[2]:(0.0015382329002) A[3]:(2.3509617562e-15)\n",
      " state (9)  A[0]:(4.20451833634e-05) A[1]:(0.999729514122) A[2]:(0.000228449804126) A[3]:(1.72940106504e-16)\n",
      " state (10)  A[0]:(1.55900161189e-05) A[1]:(0.999885797501) A[2]:(9.86016821116e-05) A[3]:(5.537078199e-17)\n",
      " state (11)  A[0]:(1.01255127447e-05) A[1]:(0.999921500683) A[2]:(6.83466641931e-05) A[3]:(3.37602252659e-17)\n",
      " state (12)  A[0]:(8.36200615595e-06) A[1]:(0.999933600426) A[2]:(5.80201412959e-05) A[3]:(2.70709893602e-17)\n",
      " state (13)  A[0]:(7.66746234149e-06) A[1]:(0.999938547611) A[2]:(5.38059139217e-05) A[3]:(2.44565344873e-17)\n",
      " state (14)  A[0]:(7.3661794886e-06) A[1]:(0.999940693378) A[2]:(5.19120148965e-05) A[3]:(2.33065681474e-17)\n",
      " state (15)  A[0]:(7.22877121007e-06) A[1]:(0.999941766262) A[2]:(5.10049503646e-05) A[3]:(2.27634843706e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 307000 finished after 17 . Running score: 0.09. Policy_loss: -92050.6121901, Value_loss: 1.18895906632. Times trained:               12867. Times reached goal: 122.               Steps done: 3492180.\n",
      " state (0)  A[0]:(0.993341863155) A[1]:(0.00212870864198) A[2]:(0.00232296064496) A[3]:(0.00220647477545)\n",
      " state (1)  A[0]:(0.0160963032395) A[1]:(0.00483215041459) A[2]:(0.00671688094735) A[3]:(0.972354650497)\n",
      " state (2)  A[0]:(0.999992072582) A[1]:(5.0641301641e-06) A[2]:(2.87130364995e-06) A[3]:(8.57528803522e-09)\n",
      " state (3)  A[0]:(0.999996960163) A[1]:(1.97595318241e-06) A[2]:(1.07476409994e-06) A[3]:(8.05692623729e-10)\n",
      " state (4)  A[0]:(0.99998664856) A[1]:(6.5303579504e-06) A[2]:(6.82108793626e-06) A[3]:(4.87086593282e-10)\n",
      " state (5)  A[0]:(0.993618130684) A[1]:(0.000389737950172) A[2]:(0.00599215505645) A[3]:(4.70043665449e-12)\n",
      " state (6)  A[0]:(0.605563044548) A[1]:(0.157289162278) A[2]:(0.237147822976) A[3]:(7.23470025207e-12)\n",
      " state (7)  A[0]:(0.0102530019358) A[1]:(0.977702856064) A[2]:(0.012044146657) A[3]:(8.85066014076e-14)\n",
      " state (8)  A[0]:(0.000183039752301) A[1]:(0.999466180801) A[2]:(0.000350808288204) A[3]:(7.80168642854e-16)\n",
      " state (9)  A[0]:(2.80457807094e-05) A[1]:(0.999905526638) A[2]:(6.64330946165e-05) A[3]:(8.69488452431e-17)\n",
      " state (10)  A[0]:(1.30448015625e-05) A[1]:(0.999953269958) A[2]:(3.36750672432e-05) A[3]:(3.57294345932e-17)\n",
      " state (11)  A[0]:(9.52201935434e-06) A[1]:(0.999965012074) A[2]:(2.5453948183e-05) A[3]:(2.47912068001e-17)\n",
      " state (12)  A[0]:(8.33458125271e-06) A[1]:(0.999969065189) A[2]:(2.25964213314e-05) A[3]:(2.12222831076e-17)\n",
      " state (13)  A[0]:(7.86564669397e-06) A[1]:(0.999970674515) A[2]:(2.14445135498e-05) A[3]:(1.982125918e-17)\n",
      " state (14)  A[0]:(7.6653823271e-06) A[1]:(0.999971389771) A[2]:(2.09417357837e-05) A[3]:(1.92174471114e-17)\n",
      " state (15)  A[0]:(7.57595717005e-06) A[1]:(0.999971687794) A[2]:(2.07103403227e-05) A[3]:(1.89418288769e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 308000 finished after 11 . Running score: 0.2. Policy_loss: -92050.6118078, Value_loss: 1.84836154391. Times trained:               12688. Times reached goal: 121.               Steps done: 3504868.\n",
      " state (0)  A[0]:(0.993079900742) A[1]:(0.00221141241491) A[2]:(0.00241126539186) A[3]:(0.00229739886709)\n",
      " state (1)  A[0]:(0.0139279859141) A[1]:(0.0045916242525) A[2]:(0.00611373456195) A[3]:(0.975366652012)\n",
      " state (2)  A[0]:(0.999974608421) A[1]:(1.61092302733e-05) A[2]:(9.1712727226e-06) A[3]:(8.97189238458e-08)\n",
      " state (3)  A[0]:(0.999995946884) A[1]:(2.73124987871e-06) A[2]:(1.331417252e-06) A[3]:(1.19688770006e-09)\n",
      " state (4)  A[0]:(0.999956071377) A[1]:(1.96435230464e-05) A[2]:(2.43026843236e-05) A[3]:(5.20552101424e-10)\n",
      " state (5)  A[0]:(0.96361374855) A[1]:(0.00358036835678) A[2]:(0.0328058712184) A[3]:(8.12072891571e-12)\n",
      " state (6)  A[0]:(0.55357336998) A[1]:(0.240119159222) A[2]:(0.206307411194) A[3]:(8.1607125571e-12)\n",
      " state (7)  A[0]:(0.0676538348198) A[1]:(0.889969944954) A[2]:(0.0423762127757) A[3]:(6.48860576814e-13)\n",
      " state (8)  A[0]:(0.00277030817233) A[1]:(0.994904339314) A[2]:(0.00232537905686) A[3]:(1.30314883911e-14)\n",
      " state (9)  A[0]:(0.000253327569226) A[1]:(0.999492287636) A[2]:(0.000254362996202) A[3]:(7.06352050841e-16)\n",
      " state (10)  A[0]:(6.97682553437e-05) A[1]:(0.999853312969) A[2]:(7.69479374867e-05) A[3]:(1.48587154882e-16)\n",
      " state (11)  A[0]:(3.70739289792e-05) A[1]:(0.999920070171) A[2]:(4.28545754403e-05) A[3]:(6.95255297193e-17)\n",
      " state (12)  A[0]:(2.73440400633e-05) A[1]:(0.999940335751) A[2]:(3.23475687765e-05) A[3]:(4.82982554038e-17)\n",
      " state (13)  A[0]:(2.36291180045e-05) A[1]:(0.999948084354) A[2]:(2.82694563793e-05) A[3]:(4.05646096761e-17)\n",
      " state (14)  A[0]:(2.20276269829e-05) A[1]:(0.999951481819) A[2]:(2.64971840807e-05) A[3]:(3.73002108718e-17)\n",
      " state (15)  A[0]:(2.12925406231e-05) A[1]:(0.99995303154) A[2]:(2.56803268712e-05) A[3]:(3.58149882296e-17)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 309000 finished after 13 . Running score: 0.13. Policy_loss: -92050.611836, Value_loss: 1.1998803967. Times trained:               12639. Times reached goal: 127.               Steps done: 3517507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9932,  0.0022,  0.0023,  0.0023]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9996e-01,  3.3574e-05,  7.9627e-06,  5.7090e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9932,  0.0022,  0.0023,  0.0023]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9933,  0.0022,  0.0023,  0.0023]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9933,  0.0022,  0.0023,  0.0023]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9995e-01,  3.9490e-05,  8.9708e-06,  6.1149e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9995e-01,  4.0768e-05,  9.1831e-06,  6.1963e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0281e-05,  9.9998e-01,  9.3514e-06,  2.0492e-17]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0171e-05,  9.9998e-01,  9.2108e-06,  2.0242e-17]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0073e-05,  9.9998e-01,  9.0863e-06,  2.0021e-17]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993255674839) A[1]:(0.00215948815458) A[2]:(0.00233298679814) A[3]:(0.00225184950978)\n",
      " state (1)  A[0]:(0.0156287942082) A[1]:(0.00494235614315) A[2]:(0.0063742431812) A[3]:(0.973054587841)\n",
      " state (2)  A[0]:(0.999991297722) A[1]:(6.07265837971e-06) A[2]:(2.62738171841e-06) A[3]:(9.22131526693e-09)\n",
      " state (3)  A[0]:(0.999995231628) A[1]:(3.61126240023e-06) A[2]:(1.16644309855e-06) A[3]:(1.6864845076e-09)\n",
      " state (4)  A[0]:(0.999945044518) A[1]:(4.50452243967e-05) A[2]:(9.87712428469e-06) A[3]:(6.45717568304e-09)\n",
      " state (5)  A[0]:(0.893176436424) A[1]:(0.079896427691) A[2]:(0.0269271209836) A[3]:(1.15133756695e-10)\n",
      " state (6)  A[0]:(0.00900430604815) A[1]:(0.986518144608) A[2]:(0.00447752792388) A[3]:(7.35805662819e-14)\n",
      " state (7)  A[0]:(8.89378716238e-05) A[1]:(0.999842941761) A[2]:(6.81259625708e-05) A[3]:(2.6719429328e-16)\n",
      " state (8)  A[0]:(9.98631912807e-06) A[1]:(0.999981045723) A[2]:(8.97603331396e-06) A[3]:(1.98255472843e-17)\n",
      " state (9)  A[0]:(4.37101152784e-06) A[1]:(0.999991416931) A[2]:(4.2136225602e-06) A[3]:(7.56541787065e-18)\n",
      " state (10)  A[0]:(3.21517268276e-06) A[1]:(0.999993562698) A[2]:(3.20295544043e-06) A[3]:(5.32804692851e-18)\n",
      " state (11)  A[0]:(2.85858027382e-06) A[1]:(0.999994218349) A[2]:(2.89467698167e-06) A[3]:(4.67464290059e-18)\n",
      " state (12)  A[0]:(2.72888701147e-06) A[1]:(0.999994456768) A[2]:(2.78756215266e-06) A[3]:(4.44745939971e-18)\n",
      " state (13)  A[0]:(2.67758650807e-06) A[1]:(0.999994575977) A[2]:(2.74886360785e-06) A[3]:(4.36276975347e-18)\n",
      " state (14)  A[0]:(2.65605035565e-06) A[1]:(0.999994635582) A[2]:(2.73504656434e-06) A[3]:(4.33032152599e-18)\n",
      " state (15)  A[0]:(2.64648792836e-06) A[1]:(0.999994635582) A[2]:(2.73045202448e-06) A[3]:(4.31785095107e-18)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 310000 finished after 10 . Running score: 0.16. Policy_loss: -92050.6128089, Value_loss: 1.21773691195. Times trained:               12278. Times reached goal: 138.               Steps done: 3529785.\n",
      " state (0)  A[0]:(0.994179189205) A[1]:(0.00189441000111) A[2]:(0.00211262353696) A[3]:(0.00181378889829)\n",
      " state (1)  A[0]:(0.0277119595557) A[1]:(0.00656737713143) A[2]:(0.00893359165639) A[3]:(0.95678704977)\n",
      " state (2)  A[0]:(0.999997138977) A[1]:(2.0439799755e-06) A[2]:(7.86734403846e-07) A[3]:(7.41344763355e-10)\n",
      " state (3)  A[0]:(0.999997138977) A[1]:(2.1726475552e-06) A[2]:(7.09167579771e-07) A[3]:(6.02649263826e-10)\n",
      " state (4)  A[0]:(0.999986290932) A[1]:(1.16470146168e-05) A[2]:(2.05529295272e-06) A[3]:(2.12652428999e-09)\n",
      " state (5)  A[0]:(0.940536022186) A[1]:(0.057553935796) A[2]:(0.00191004760563) A[3]:(9.37070532281e-10)\n",
      " state (6)  A[0]:(0.000722626282368) A[1]:(0.999028623104) A[2]:(0.00024872776703) A[3]:(2.62425968949e-15)\n",
      " state (7)  A[0]:(8.00520501798e-06) A[1]:(0.999987244606) A[2]:(4.73621457786e-06) A[3]:(9.96669175603e-18)\n",
      " state (8)  A[0]:(1.64199263963e-06) A[1]:(0.999997258186) A[2]:(1.12846487355e-06) A[3]:(1.57323807147e-18)\n",
      " state (9)  A[0]:(9.66000811786e-07) A[1]:(0.99999833107) A[2]:(7.10218159838e-07) A[3]:(8.70690209376e-19)\n",
      " state (10)  A[0]:(8.0039706063e-07) A[1]:(0.999998569489) A[2]:(6.08536140589e-07) A[3]:(7.13303846695e-19)\n",
      " state (11)  A[0]:(7.45463978546e-07) A[1]:(0.999998688698) A[2]:(5.77066600727e-07) A[3]:(6.64956070469e-19)\n",
      " state (12)  A[0]:(7.24256608464e-07) A[1]:(0.999998688698) A[2]:(5.66555286241e-07) A[3]:(6.48228100339e-19)\n",
      " state (13)  A[0]:(7.15084638614e-07) A[1]:(0.999998748302) A[2]:(5.63028436318e-07) A[3]:(6.42104895855e-19)\n",
      " state (14)  A[0]:(7.10676488325e-07) A[1]:(0.999998748302) A[2]:(5.6191800013e-07) A[3]:(6.39791943766e-19)\n",
      " state (15)  A[0]:(7.08338006916e-07) A[1]:(0.999998748302) A[2]:(5.61636227303e-07) A[3]:(6.38891971259e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 311000 finished after 11 . Running score: 0.14. Policy_loss: -92050.611844, Value_loss: 1.63654790017. Times trained:               12484. Times reached goal: 118.               Steps done: 3542269.\n",
      " state (0)  A[0]:(0.994501829147) A[1]:(0.00183099508286) A[2]:(0.00197886046953) A[3]:(0.00168833509088)\n",
      " state (1)  A[0]:(0.0302332304418) A[1]:(0.00680644670501) A[2]:(0.00878615304828) A[3]:(0.954174160957)\n",
      " state (2)  A[0]:(0.999985933304) A[1]:(1.01578898466e-05) A[2]:(3.90496370528e-06) A[3]:(2.59390908752e-08)\n",
      " state (3)  A[0]:(0.999998092651) A[1]:(1.50719813519e-06) A[2]:(4.03797230319e-07) A[3]:(3.72986669328e-10)\n",
      " state (4)  A[0]:(0.999997377396) A[1]:(2.12665418076e-06) A[2]:(4.91597290875e-07) A[3]:(4.78233952439e-10)\n",
      " state (5)  A[0]:(0.999946475029) A[1]:(4.95779422636e-05) A[2]:(3.93869777326e-06) A[3]:(1.24365517884e-09)\n",
      " state (6)  A[0]:(0.156069755554) A[1]:(0.841742277145) A[2]:(0.0021879717242) A[3]:(4.41164075757e-12)\n",
      " state (7)  A[0]:(7.10331441951e-05) A[1]:(0.999918341637) A[2]:(1.06042907646e-05) A[3]:(1.97240647963e-16)\n",
      " state (8)  A[0]:(1.49608251832e-06) A[1]:(0.999998152256) A[2]:(3.5002440768e-07) A[3]:(2.62133722234e-18)\n",
      " state (9)  A[0]:(3.04701472942e-07) A[1]:(0.999999582767) A[2]:(8.86515323373e-08) A[3]:(4.95838167052e-19)\n",
      " state (10)  A[0]:(1.58039227927e-07) A[1]:(0.999999761581) A[2]:(5.20653848923e-08) A[3]:(2.60153601774e-19)\n",
      " state (11)  A[0]:(1.18692938145e-07) A[1]:(0.999999821186) A[2]:(4.19559782472e-08) A[3]:(1.9974688931e-19)\n",
      " state (12)  A[0]:(1.04058358374e-07) A[1]:(0.999999880791) A[2]:(3.83279079585e-08) A[3]:(1.78452584931e-19)\n",
      " state (13)  A[0]:(9.76131104835e-08) A[1]:(0.999999880791) A[2]:(3.6877874976e-08) A[3]:(1.69824612928e-19)\n",
      " state (14)  A[0]:(9.44353502064e-08) A[1]:(0.999999880791) A[2]:(3.62713201696e-08) A[3]:(1.66064753927e-19)\n",
      " state (15)  A[0]:(9.27192687072e-08) A[1]:(0.999999880791) A[2]:(3.60135672395e-08) A[3]:(1.64343675415e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 312000 finished after 28 . Running score: 0.12. Policy_loss: -92050.6120836, Value_loss: 1.40835564279. Times trained:               13025. Times reached goal: 133.               Steps done: 3555294.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994609475136) A[1]:(0.00180482363794) A[2]:(0.00195773458108) A[3]:(0.00162797875237)\n",
      " state (1)  A[0]:(0.0223478619009) A[1]:(0.00571124535054) A[2]:(0.00789417512715) A[3]:(0.96404671669)\n",
      " state (2)  A[0]:(0.99997651577) A[1]:(1.43604438563e-05) A[2]:(9.05815068108e-06) A[3]:(5.68226319331e-08)\n",
      " state (3)  A[0]:(0.999997675419) A[1]:(1.57952342761e-06) A[2]:(7.21763967704e-07) A[3]:(4.13693995727e-10)\n",
      " state (4)  A[0]:(0.999996423721) A[1]:(2.6227789931e-06) A[2]:(9.78169964583e-07) A[3]:(6.15973660967e-10)\n",
      " state (5)  A[0]:(0.999798476696) A[1]:(0.000183865631698) A[2]:(1.76530447789e-05) A[3]:(2.48189691021e-09)\n",
      " state (6)  A[0]:(0.229480803013) A[1]:(0.762308657169) A[2]:(0.00821055844426) A[3]:(1.52732791692e-11)\n",
      " state (7)  A[0]:(0.000483777141199) A[1]:(0.999125480652) A[2]:(0.00039075728273) A[3]:(1.42098109589e-15)\n",
      " state (8)  A[0]:(9.60186298471e-06) A[1]:(0.999972522259) A[2]:(1.7903981643e-05) A[3]:(1.17185792297e-17)\n",
      " state (9)  A[0]:(1.29847467178e-06) A[1]:(0.999995231628) A[2]:(3.48701519215e-06) A[3]:(1.23021506024e-18)\n",
      " state (10)  A[0]:(4.87240527036e-07) A[1]:(0.999997913837) A[2]:(1.62613446264e-06) A[3]:(4.38783384302e-19)\n",
      " state (11)  A[0]:(2.95628552749e-07) A[1]:(0.999998569489) A[2]:(1.13282465009e-06) A[3]:(2.68896177065e-19)\n",
      " state (12)  A[0]:(2.28009227499e-07) A[1]:(0.999998807907) A[2]:(9.53682274485e-07) A[3]:(2.12392529247e-19)\n",
      " state (13)  A[0]:(1.9877801094e-07) A[1]:(0.999998927116) A[2]:(8.79409071786e-07) A[3]:(1.89604698573e-19)\n",
      " state (14)  A[0]:(1.84472924047e-07) A[1]:(0.999998986721) A[2]:(8.47018270633e-07) A[3]:(1.79552489576e-19)\n",
      " state (15)  A[0]:(1.7678756592e-07) A[1]:(0.999998986721) A[2]:(8.32801617889e-07) A[3]:(1.74916090544e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 313000 finished after 13 . Running score: 0.08. Policy_loss: -92050.6330521, Value_loss: 0.986535718439. Times trained:               12920. Times reached goal: 122.               Steps done: 3568214.\n",
      " state (0)  A[0]:(0.994704604149) A[1]:(0.00180287251715) A[2]:(0.00189587124623) A[3]:(0.00159667467233)\n",
      " state (1)  A[0]:(0.0211801566184) A[1]:(0.00558970822021) A[2]:(0.00757625559345) A[3]:(0.965653896332)\n",
      " state (2)  A[0]:(0.999996066093) A[1]:(2.64745085587e-06) A[2]:(1.29801014737e-06) A[3]:(1.27401245109e-09)\n",
      " state (3)  A[0]:(0.999997437) A[1]:(1.77015306235e-06) A[2]:(7.68624317971e-07) A[3]:(4.24139778854e-10)\n",
      " state (4)  A[0]:(0.999993979931) A[1]:(4.5473211685e-06) A[2]:(1.46759430208e-06) A[3]:(7.22492954353e-10)\n",
      " state (5)  A[0]:(0.995256483555) A[1]:(0.00435913400725) A[2]:(0.000384356884751) A[3]:(6.47405462573e-10)\n",
      " state (6)  A[0]:(0.00991273950785) A[1]:(0.986694931984) A[2]:(0.00339233316481) A[3]:(5.75435898475e-14)\n",
      " state (7)  A[0]:(3.72654176317e-05) A[1]:(0.999901890755) A[2]:(6.08549271419e-05) A[3]:(4.74668065393e-17)\n",
      " state (8)  A[0]:(1.62002447723e-06) A[1]:(0.999993801117) A[2]:(4.58562999484e-06) A[3]:(1.31874416869e-18)\n",
      " state (9)  A[0]:(3.72894476186e-07) A[1]:(0.999998211861) A[2]:(1.41246289331e-06) A[3]:(2.72957685572e-19)\n",
      " state (10)  A[0]:(1.89134880202e-07) A[1]:(0.999998986721) A[2]:(8.44641874664e-07) A[3]:(1.37407430572e-19)\n",
      " state (11)  A[0]:(1.37302848202e-07) A[1]:(0.999999165535) A[2]:(6.70996939789e-07) A[3]:(1.00923291949e-19)\n",
      " state (12)  A[0]:(1.17988079751e-07) A[1]:(0.999999284744) A[2]:(6.04831313922e-07) A[3]:(8.77268401971e-20)\n",
      " state (13)  A[0]:(1.09792232195e-07) A[1]:(0.999999284744) A[2]:(5.77172613703e-07) A[3]:(8.23083806699e-20)\n",
      " state (14)  A[0]:(1.06064732108e-07) A[1]:(0.999999344349) A[2]:(5.65134769204e-07) A[3]:(7.99499788836e-20)\n",
      " state (15)  A[0]:(1.04291572711e-07) A[1]:(0.999999344349) A[2]:(5.59803481792e-07) A[3]:(7.88929325336e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 314000 finished after 6 . Running score: 0.14. Policy_loss: -92050.6114012, Value_loss: 1.41124104583. Times trained:               12693. Times reached goal: 141.               Steps done: 3580907.\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0018,  0.0019,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0018,  0.0019,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0018,  0.0019,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0018,  0.0019,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0018,  0.0019,  0.0016]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0245,  0.0062,  0.0083,  0.9610]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0245,  0.0062,  0.0083,  0.9610]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0018,  0.0019,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0018,  0.0019,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0018,  0.0019,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0018,  0.0019,  0.0016]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0245,  0.0062,  0.0083,  0.9610]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.7848e-06,  1.3419e-06,  1.3257e-09]])\n",
      "On state=2, selected action=0\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.7843e-06,  1.3417e-06,  1.3254e-09]])\n",
      "On state=2, selected action=0\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.8370e-02,  9.6639e-01,  5.2374e-03,  9.0102e-14]])\n",
      "On state=6, selected action=1\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994634568691) A[1]:(0.00183636182919) A[2]:(0.0019313772209) A[3]:(0.00159771682229)\n",
      " state (1)  A[0]:(0.0246154516935) A[1]:(0.00616970891133) A[2]:(0.00836480502039) A[3]:(0.960850059986)\n",
      " state (2)  A[0]:(0.99999576807) A[1]:(2.86392764792e-06) A[2]:(1.39496978591e-06) A[3]:(1.42669531744e-09)\n",
      " state (3)  A[0]:(0.999997735023) A[1]:(1.58119030402e-06) A[2]:(6.90506794854e-07) A[3]:(3.42917971796e-10)\n",
      " state (4)  A[0]:(0.999996364117) A[1]:(2.62718458544e-06) A[2]:(9.9820010746e-07) A[3]:(4.23226981239e-10)\n",
      " state (5)  A[0]:(0.999367833138) A[1]:(0.000534961232916) A[2]:(9.71946064965e-05) A[3]:(2.51673321072e-10)\n",
      " state (6)  A[0]:(0.0467079207301) A[1]:(0.946171045303) A[2]:(0.00712106423452) A[3]:(1.68795695737e-13)\n",
      " state (7)  A[0]:(8.62015294842e-05) A[1]:(0.999840915203) A[2]:(7.28666782379e-05) A[3]:(6.62750076969e-17)\n",
      " state (8)  A[0]:(2.53152074947e-06) A[1]:(0.999994039536) A[2]:(3.42849375556e-06) A[3]:(1.14384106747e-18)\n",
      " state (9)  A[0]:(4.78583899621e-07) A[1]:(0.999998688698) A[2]:(8.23003972528e-07) A[3]:(1.86285248624e-19)\n",
      " state (10)  A[0]:(2.11684167084e-07) A[1]:(0.999999344349) A[2]:(4.25127296921e-07) A[3]:(8.08547658397e-20)\n",
      " state (11)  A[0]:(1.390402673e-07) A[1]:(0.999999523163) A[2]:(3.0890825542e-07) A[3]:(5.39410842859e-20)\n",
      " state (12)  A[0]:(1.11812397563e-07) A[1]:(0.999999642372) A[2]:(2.64534151029e-07) A[3]:(4.4247681036e-20)\n",
      " state (13)  A[0]:(9.98496147986e-08) A[1]:(0.999999642372) A[2]:(2.45522414843e-07) A[3]:(4.01783886308e-20)\n",
      " state (14)  A[0]:(9.40662658877e-08) A[1]:(0.999999642372) A[2]:(2.3695393736e-07) A[3]:(3.83384869169e-20)\n",
      " state (15)  A[0]:(9.10623185746e-08) A[1]:(0.999999701977) A[2]:(2.33014176843e-07) A[3]:(3.74726743848e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 315000 finished after 15 . Running score: 0.15. Policy_loss: -92050.6141406, Value_loss: 1.84356862546. Times trained:               12780. Times reached goal: 131.               Steps done: 3593687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994647681713) A[1]:(0.00184447830543) A[2]:(0.00193840172142) A[3]:(0.00156946247444)\n",
      " state (1)  A[0]:(0.0253881812096) A[1]:(0.00640105595812) A[2]:(0.00881789810956) A[3]:(0.959392845631)\n",
      " state (2)  A[0]:(0.999996364117) A[1]:(2.37675249082e-06) A[2]:(1.28658359699e-06) A[3]:(8.67273142191e-10)\n",
      " state (3)  A[0]:(0.999997615814) A[1]:(1.60407580552e-06) A[2]:(8.02238730557e-07) A[3]:(3.20689863553e-10)\n",
      " state (4)  A[0]:(0.999996185303) A[1]:(2.62838966592e-06) A[2]:(1.15664965961e-06) A[3]:(3.78791026323e-10)\n",
      " state (5)  A[0]:(0.999432325363) A[1]:(0.000457953865407) A[2]:(0.000109731197881) A[3]:(1.62251517777e-10)\n",
      " state (6)  A[0]:(0.0432065613568) A[1]:(0.946352481842) A[2]:(0.0104409279302) A[3]:(8.30278025694e-14)\n",
      " state (7)  A[0]:(6.98615622241e-05) A[1]:(0.999816536903) A[2]:(0.000113579102617) A[3]:(2.92376119581e-17)\n",
      " state (8)  A[0]:(1.84854229701e-06) A[1]:(0.999992847443) A[2]:(5.31074101673e-06) A[3]:(4.42168620958e-19)\n",
      " state (9)  A[0]:(3.58972499725e-07) A[1]:(0.999998271465) A[2]:(1.35623338338e-06) A[3]:(7.39139256315e-20)\n",
      " state (10)  A[0]:(1.74658964625e-07) A[1]:(0.999999046326) A[2]:(7.7345362115e-07) A[3]:(3.55206412302e-20)\n",
      " state (11)  A[0]:(1.25053730926e-07) A[1]:(0.999999284744) A[2]:(6.08757659393e-07) A[3]:(2.59296322773e-20)\n",
      " state (12)  A[0]:(1.06306153214e-07) A[1]:(0.999999344349) A[2]:(5.48325488126e-07) A[3]:(2.25510130653e-20)\n",
      " state (13)  A[0]:(9.78052128175e-08) A[1]:(0.999999403954) A[2]:(5.23779533523e-07) A[3]:(2.11746184534e-20)\n",
      " state (14)  A[0]:(9.34081256787e-08) A[1]:(0.999999403954) A[2]:(5.13523787049e-07) A[3]:(2.0574565158e-20)\n",
      " state (15)  A[0]:(9.08551456291e-08) A[1]:(0.999999403954) A[2]:(5.09332721776e-07) A[3]:(2.03032418392e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 316000 finished after 4 . Running score: 0.22. Policy_loss: -92050.6114094, Value_loss: 1.21210377731. Times trained:               13020. Times reached goal: 126.               Steps done: 3606707.\n",
      " state (0)  A[0]:(0.994410157204) A[1]:(0.00188471528236) A[2]:(0.00206234003417) A[3]:(0.00164279562887)\n",
      " state (1)  A[0]:(0.0234838463366) A[1]:(0.00600088620558) A[2]:(0.00882151443511) A[3]:(0.961693763733)\n",
      " state (2)  A[0]:(0.999996364117) A[1]:(2.1321229724e-06) A[2]:(1.48245953824e-06) A[3]:(8.00110144805e-10)\n",
      " state (3)  A[0]:(0.99999755621) A[1]:(1.46136915191e-06) A[2]:(9.61689920587e-07) A[3]:(3.09151787503e-10)\n",
      " state (4)  A[0]:(0.999996125698) A[1]:(2.40496092374e-06) A[2]:(1.49137849803e-06) A[3]:(3.77146563979e-10)\n",
      " state (5)  A[0]:(0.999610364437) A[1]:(0.000228321950999) A[2]:(0.000161287462106) A[3]:(1.78591919031e-10)\n",
      " state (6)  A[0]:(0.563883304596) A[1]:(0.343169510365) A[2]:(0.0929471328855) A[3]:(2.62151108113e-12)\n",
      " state (7)  A[0]:(0.013849420473) A[1]:(0.965833544731) A[2]:(0.0203170366585) A[3]:(1.35883702023e-14)\n",
      " state (8)  A[0]:(0.000295213016216) A[1]:(0.998652398586) A[2]:(0.00105237658136) A[3]:(1.19347114322e-16)\n",
      " state (9)  A[0]:(1.76707835635e-05) A[1]:(0.999884486198) A[2]:(9.78366588242e-05) A[3]:(4.2384916567e-18)\n",
      " state (10)  A[0]:(3.43276997228e-06) A[1]:(0.999972105026) A[2]:(2.44722050411e-05) A[3]:(6.53220083637e-19)\n",
      " state (11)  A[0]:(1.36519304306e-06) A[1]:(0.999987185001) A[2]:(1.14522044896e-05) A[3]:(2.37987203459e-19)\n",
      " state (12)  A[0]:(8.00454131422e-07) A[1]:(0.999991714954) A[2]:(7.48152751839e-06) A[3]:(1.35521484624e-19)\n",
      " state (13)  A[0]:(5.88327338846e-07) A[1]:(0.999993503094) A[2]:(5.89241744819e-06) A[3]:(9.88691956928e-20)\n",
      " state (14)  A[0]:(4.94552182317e-07) A[1]:(0.999994337559) A[2]:(5.16512591275e-06) A[3]:(8.30779752946e-20)\n",
      " state (15)  A[0]:(4.49303286132e-07) A[1]:(0.999994754791) A[2]:(4.80848666484e-06) A[3]:(7.55787817106e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 317000 finished after 13 . Running score: 0.16. Policy_loss: -92050.6114096, Value_loss: 1.85295091782. Times trained:               12748. Times reached goal: 128.               Steps done: 3619455.\n",
      " state (0)  A[0]:(0.9944409132) A[1]:(0.00188469002023) A[2]:(0.00203774729744) A[3]:(0.00163665541913)\n",
      " state (1)  A[0]:(0.0249553974718) A[1]:(0.00612447783351) A[2]:(0.00921296793967) A[3]:(0.959707140923)\n",
      " state (2)  A[0]:(0.999996244907) A[1]:(2.08774577004e-06) A[2]:(1.66767529208e-06) A[3]:(9.22695087002e-10)\n",
      " state (3)  A[0]:(0.999997913837) A[1]:(1.19826700029e-06) A[2]:(9.01645250906e-07) A[3]:(2.59303717387e-10)\n",
      " state (4)  A[0]:(0.999997437) A[1]:(1.50591267811e-06) A[2]:(1.06508969111e-06) A[3]:(2.92795038215e-10)\n",
      " state (5)  A[0]:(0.999987065792) A[1]:(8.37702918943e-06) A[2]:(4.56233192381e-06) A[3]:(4.00177863069e-10)\n",
      " state (6)  A[0]:(0.97411441803) A[1]:(0.0184050966054) A[2]:(0.0074804876931) A[3]:(2.01030355557e-11)\n",
      " state (7)  A[0]:(0.0870017111301) A[1]:(0.863762915134) A[2]:(0.0492353700101) A[3]:(7.78668443743e-14)\n",
      " state (8)  A[0]:(0.00139463809319) A[1]:(0.995660364628) A[2]:(0.0029449716676) A[3]:(3.34839112071e-16)\n",
      " state (9)  A[0]:(7.1991024015e-05) A[1]:(0.999681413174) A[2]:(0.000246621581027) A[3]:(8.3745295499e-18)\n",
      " state (10)  A[0]:(1.3179378584e-05) A[1]:(0.999930679798) A[2]:(5.61616761843e-05) A[3]:(1.0656460298e-18)\n",
      " state (11)  A[0]:(5.1422443903e-06) A[1]:(0.999970018864) A[2]:(2.48122014455e-05) A[3]:(3.50056502131e-19)\n",
      " state (12)  A[0]:(2.98245845443e-06) A[1]:(0.999981403351) A[2]:(1.56245205289e-05) A[3]:(1.87326694865e-19)\n",
      " state (13)  A[0]:(2.16726721192e-06) A[1]:(0.999985814095) A[2]:(1.20160721053e-05) A[3]:(1.31407282128e-19)\n",
      " state (14)  A[0]:(1.80056144927e-06) A[1]:(0.999987840652) A[2]:(1.03805059553e-05) A[3]:(1.07776276529e-19)\n",
      " state (15)  A[0]:(1.61801222021e-06) A[1]:(0.999988794327) A[2]:(9.58483724389e-06) A[3]:(9.66491915627e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 318000 finished after 19 . Running score: 0.1. Policy_loss: -92050.613805, Value_loss: 0.993189908791. Times trained:               12888. Times reached goal: 125.               Steps done: 3632343.\n",
      " state (0)  A[0]:(0.994454860687) A[1]:(0.00189884274732) A[2]:(0.00201947521418) A[3]:(0.00162680773064)\n",
      " state (1)  A[0]:(0.0240140948445) A[1]:(0.0061471988447) A[2]:(0.00897407718003) A[3]:(0.960864603519)\n",
      " state (2)  A[0]:(0.999997377396) A[1]:(1.53289545324e-06) A[2]:(1.06829804736e-06) A[3]:(4.02080951867e-10)\n",
      " state (3)  A[0]:(0.999997675419) A[1]:(1.40340557664e-06) A[2]:(9.34452543788e-07) A[3]:(2.80379469642e-10)\n",
      " state (4)  A[0]:(0.99999588728) A[1]:(2.58208547166e-06) A[2]:(1.51652477598e-06) A[3]:(3.52488566113e-10)\n",
      " state (5)  A[0]:(0.999502658844) A[1]:(0.000335687422194) A[2]:(0.000161663512699) A[3]:(1.63620436644e-10)\n",
      " state (6)  A[0]:(0.111884631217) A[1]:(0.84216183424) A[2]:(0.0459535680711) A[3]:(1.29573543735e-13)\n",
      " state (7)  A[0]:(0.000381218880648) A[1]:(0.998765528202) A[2]:(0.000853248347994) A[3]:(7.51343569029e-17)\n",
      " state (8)  A[0]:(1.14032764031e-05) A[1]:(0.999946653843) A[2]:(4.19196148869e-05) A[3]:(1.01415879296e-18)\n",
      " state (9)  A[0]:(2.10448251892e-06) A[1]:(0.99998831749) A[2]:(9.59105091169e-06) A[3]:(1.37405453093e-19)\n",
      " state (10)  A[0]:(9.95522441372e-07) A[1]:(0.999993920326) A[2]:(5.09662049808e-06) A[3]:(5.89281658925e-20)\n",
      " state (11)  A[0]:(7.06553180407e-07) A[1]:(0.999995410442) A[2]:(3.86909869121e-06) A[3]:(4.07395369723e-20)\n",
      " state (12)  A[0]:(6.00877910983e-07) A[1]:(0.999996006489) A[2]:(3.42200019077e-06) A[3]:(3.45257917538e-20)\n",
      " state (13)  A[0]:(5.55549092951e-07) A[1]:(0.999996185303) A[2]:(3.23844574268e-06) A[3]:(3.20255608041e-20)\n",
      " state (14)  A[0]:(5.342118925e-07) A[1]:(0.999996304512) A[2]:(3.15946886076e-06) A[3]:(3.09438896751e-20)\n",
      " state (15)  A[0]:(5.23416417764e-07) A[1]:(0.999996364117) A[2]:(3.12514771394e-06) A[3]:(3.04604349188e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 319000 finished after 10 . Running score: 0.13. Policy_loss: -92050.6114501, Value_loss: 1.63218615178. Times trained:               12654. Times reached goal: 135.               Steps done: 3644997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9944,  0.0019,  0.0020,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0019,  0.0020,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0019,  0.0020,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0019,  0.0020,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0019,  0.0020,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.9781e-06,  2.7248e-06,  4.4901e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.9785e-06,  2.7249e-06,  4.4903e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.9789e-06,  2.7249e-06,  4.4904e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0019,  0.0020,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0019,  0.0020,  0.0017]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0019,  0.0020,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.9768e-06,  2.7242e-06,  4.4843e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0019,  0.0020,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.9747e-06,  2.7235e-06,  4.4792e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.9739e-06,  2.7232e-06,  4.4771e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0019,  0.0020,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0019,  0.0020,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.9717e-06,  2.7226e-06,  4.4718e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.6913e-05,  9.9979e-01,  1.6031e-04,  6.5790e-18]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994396805763) A[1]:(0.00193017639685) A[2]:(0.00202516722493) A[3]:(0.00164785597008)\n",
      " state (1)  A[0]:(0.0240033864975) A[1]:(0.00624531833455) A[2]:(0.00901883561164) A[3]:(0.960732460022)\n",
      " state (2)  A[0]:(0.99999755621) A[1]:(1.47262574046e-06) A[2]:(9.7454153547e-07) A[3]:(3.03581937366e-10)\n",
      " state (3)  A[0]:(0.999997258186) A[1]:(1.68670908351e-06) A[2]:(1.06215281903e-06) A[3]:(3.04327979483e-10)\n",
      " state (4)  A[0]:(0.999992311001) A[1]:(4.96417897011e-06) A[2]:(2.72056604445e-06) A[3]:(4.46800985099e-10)\n",
      " state (5)  A[0]:(0.995803713799) A[1]:(0.00260024820454) A[2]:(0.00159603077918) A[3]:(9.3207691565e-11)\n",
      " state (6)  A[0]:(0.105772458017) A[1]:(0.834246456623) A[2]:(0.0599811114371) A[3]:(1.50592591782e-13)\n",
      " state (7)  A[0]:(0.00149748544209) A[1]:(0.995441675186) A[2]:(0.00306082493626) A[3]:(4.89969543531e-16)\n",
      " state (8)  A[0]:(4.77589674119e-05) A[1]:(0.999789178371) A[2]:(0.000163050601259) A[3]:(6.70729449933e-18)\n",
      " state (9)  A[0]:(5.15332976647e-06) A[1]:(0.999971926212) A[2]:(2.29130710068e-05) A[3]:(4.65812389696e-19)\n",
      " state (10)  A[0]:(1.62065327913e-06) A[1]:(0.99998998642) A[2]:(8.41239852889e-06) A[3]:(1.23079951504e-19)\n",
      " state (11)  A[0]:(9.04821888525e-07) A[1]:(0.999993920326) A[2]:(5.16686804986e-06) A[3]:(6.46683469792e-20)\n",
      " state (12)  A[0]:(6.72145858971e-07) A[1]:(0.999995291233) A[2]:(4.06335448133e-06) A[3]:(4.70963521251e-20)\n",
      " state (13)  A[0]:(5.77723540118e-07) A[1]:(0.999995827675) A[2]:(3.60719627679e-06) A[3]:(4.0239283341e-20)\n",
      " state (14)  A[0]:(5.35124968337e-07) A[1]:(0.999996066093) A[2]:(3.40060023518e-06) A[3]:(3.72166813722e-20)\n",
      " state (15)  A[0]:(5.14853468303e-07) A[1]:(0.999996185303) A[2]:(3.30280954586e-06) A[3]:(3.58030910977e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 320000 finished after 19 . Running score: 0.14. Policy_loss: -92050.6117044, Value_loss: 1.21374469551. Times trained:               13150. Times reached goal: 138.               Steps done: 3658147.\n",
      " state (0)  A[0]:(0.994383633137) A[1]:(0.00194195180666) A[2]:(0.00203489954583) A[3]:(0.00163951993454)\n",
      " state (1)  A[0]:(0.0288061890751) A[1]:(0.00697044655681) A[2]:(0.0100800814107) A[3]:(0.954143285751)\n",
      " state (2)  A[0]:(0.999997675419) A[1]:(1.39400947319e-06) A[2]:(9.58348209679e-07) A[3]:(2.44329861898e-10)\n",
      " state (3)  A[0]:(0.999996840954) A[1]:(1.87148339137e-06) A[2]:(1.25824601582e-06) A[3]:(2.7056587526e-10)\n",
      " state (4)  A[0]:(0.999984562397) A[1]:(8.56802762428e-06) A[2]:(6.85232998876e-06) A[3]:(2.60861360291e-10)\n",
      " state (5)  A[0]:(0.986919224262) A[1]:(0.00418998021632) A[2]:(0.00889079365879) A[3]:(1.10017567972e-11)\n",
      " state (6)  A[0]:(0.160357356071) A[1]:(0.712378263474) A[2]:(0.127264395356) A[3]:(9.66014919966e-14)\n",
      " state (7)  A[0]:(0.00586961442605) A[1]:(0.98252004385) A[2]:(0.0116103328764) A[3]:(1.22980152453e-15)\n",
      " state (8)  A[0]:(0.000294012890663) A[1]:(0.998807191849) A[2]:(0.000898817495909) A[3]:(2.79457179687e-17)\n",
      " state (9)  A[0]:(2.59477092186e-05) A[1]:(0.999871313572) A[2]:(0.000102746816992) A[3]:(1.38237938001e-18)\n",
      " state (10)  A[0]:(5.90467470829e-06) A[1]:(0.999966740608) A[2]:(2.73676432698e-05) A[3]:(2.32434029815e-19)\n",
      " state (11)  A[0]:(2.61711693383e-06) A[1]:(0.999983966351) A[2]:(1.34340634759e-05) A[3]:(9.00363672415e-20)\n",
      " state (12)  A[0]:(1.67011137364e-06) A[1]:(0.999989151955) A[2]:(9.17066336115e-06) A[3]:(5.4222748978e-20)\n",
      " state (13)  A[0]:(1.30300463752e-06) A[1]:(0.999991238117) A[2]:(7.46676914787e-06) A[3]:(4.127103929e-20)\n",
      " state (14)  A[0]:(1.13797477752e-06) A[1]:(0.999992191792) A[2]:(6.69002884024e-06) A[3]:(3.56655432403e-20)\n",
      " state (15)  A[0]:(1.05794947558e-06) A[1]:(0.999992609024) A[2]:(6.31185821476e-06) A[3]:(3.30082804524e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 321000 finished after 17 . Running score: 0.21. Policy_loss: -92050.6347329, Value_loss: 1.627177488. Times trained:               13063. Times reached goal: 132.               Steps done: 3671210.\n",
      " state (0)  A[0]:(0.994572162628) A[1]:(0.00187972746789) A[2]:(0.00198645028286) A[3]:(0.00156167312525)\n",
      " state (1)  A[0]:(0.0271638408303) A[1]:(0.0067924130708) A[2]:(0.00942206569016) A[3]:(0.956621706486)\n",
      " state (2)  A[0]:(0.999997913837) A[1]:(1.27757687096e-06) A[2]:(8.00859254468e-07) A[3]:(2.29821939257e-10)\n",
      " state (3)  A[0]:(0.999997735023) A[1]:(1.41733073633e-06) A[2]:(8.67882079092e-07) A[3]:(2.11089215552e-10)\n",
      " state (4)  A[0]:(0.999995231628) A[1]:(2.89522381536e-06) A[2]:(1.84468638054e-06) A[3]:(1.98769778414e-10)\n",
      " state (5)  A[0]:(0.999902248383) A[1]:(4.37401831732e-05) A[2]:(5.40006367373e-05) A[3]:(4.26578425361e-11)\n",
      " state (6)  A[0]:(0.888490140438) A[1]:(0.0556628704071) A[2]:(0.0558470189571) A[3]:(4.94993694471e-13)\n",
      " state (7)  A[0]:(0.00879250559956) A[1]:(0.984683930874) A[2]:(0.0065235439688) A[3]:(6.10304285035e-16)\n",
      " state (8)  A[0]:(0.00011510407785) A[1]:(0.999747991562) A[2]:(0.000136923670652) A[3]:(2.37842384814e-18)\n",
      " state (9)  A[0]:(1.23342342704e-05) A[1]:(0.999970555305) A[2]:(1.71352676261e-05) A[3]:(1.40044107548e-19)\n",
      " state (10)  A[0]:(4.30755972047e-06) A[1]:(0.999989211559) A[2]:(6.46821990813e-06) A[3]:(3.78946140453e-20)\n",
      " state (11)  A[0]:(2.62405660578e-06) A[1]:(0.999993264675) A[2]:(4.13657153331e-06) A[3]:(2.08367474849e-20)\n",
      " state (12)  A[0]:(2.06152890314e-06) A[1]:(0.999994575977) A[2]:(3.35417917086e-06) A[3]:(1.57294419617e-20)\n",
      " state (13)  A[0]:(1.82853182196e-06) A[1]:(0.999995112419) A[2]:(3.03675915347e-06) A[3]:(1.37551040112e-20)\n",
      " state (14)  A[0]:(1.72093984929e-06) A[1]:(0.999995410442) A[2]:(2.89710828838e-06) A[3]:(1.28998622657e-20)\n",
      " state (15)  A[0]:(1.66773452293e-06) A[1]:(0.999995470047) A[2]:(2.83388067146e-06) A[3]:(1.25102755453e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 322000 finished after 4 . Running score: 0.15. Policy_loss: -92050.6115632, Value_loss: 1.41602734763. Times trained:               12522. Times reached goal: 157.               Steps done: 3683732.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994738817215) A[1]:(0.00177755241748) A[2]:(0.00193282263353) A[3]:(0.00155078584794)\n",
      " state (1)  A[0]:(0.0223565455526) A[1]:(0.00600352603942) A[2]:(0.00864828471094) A[3]:(0.962991654873)\n",
      " state (2)  A[0]:(0.999996721745) A[1]:(1.84648968116e-06) A[2]:(1.43524448504e-06) A[3]:(5.52870971227e-10)\n",
      " state (3)  A[0]:(0.999996960163) A[1]:(1.71805959326e-06) A[2]:(1.30332398385e-06) A[3]:(2.62917104754e-10)\n",
      " state (4)  A[0]:(0.999985516071) A[1]:(7.20245634511e-06) A[2]:(7.28050690668e-06) A[3]:(1.90133214728e-10)\n",
      " state (5)  A[0]:(0.99611479044) A[1]:(0.000842116191052) A[2]:(0.00304308440536) A[3]:(7.9262698835e-12)\n",
      " state (6)  A[0]:(0.235415279865) A[1]:(0.585119783878) A[2]:(0.179464951158) A[3]:(7.91243630403e-14)\n",
      " state (7)  A[0]:(0.00118168443441) A[1]:(0.995806753635) A[2]:(0.00301156868227) A[3]:(8.1719283679e-17)\n",
      " state (8)  A[0]:(4.0812898078e-05) A[1]:(0.999807059765) A[2]:(0.00015211731079) A[3]:(1.15030827569e-18)\n",
      " state (9)  A[0]:(6.62592537992e-06) A[1]:(0.999963760376) A[2]:(2.96150465147e-05) A[3]:(1.20807660512e-19)\n",
      " state (10)  A[0]:(2.84814291263e-06) A[1]:(0.999983072281) A[2]:(1.40645533975e-05) A[3]:(4.37982376201e-20)\n",
      " state (11)  A[0]:(1.93910182134e-06) A[1]:(0.999987959862) A[2]:(1.01288123915e-05) A[3]:(2.80174829954e-20)\n",
      " state (12)  A[0]:(1.6227437527e-06) A[1]:(0.999989628792) A[2]:(8.7380185505e-06) A[3]:(2.29089754754e-20)\n",
      " state (13)  A[0]:(1.49298932683e-06) A[1]:(0.999990344048) A[2]:(8.16729152575e-06) A[3]:(2.08902492684e-20)\n",
      " state (14)  A[0]:(1.43556837884e-06) A[1]:(0.999990642071) A[2]:(7.91651928012e-06) A[3]:(2.0017441244e-20)\n",
      " state (15)  A[0]:(1.40915460634e-06) A[1]:(0.99999076128) A[2]:(7.80262143962e-06) A[3]:(1.96232331366e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 323000 finished after 10 . Running score: 0.1. Policy_loss: -92050.6115662, Value_loss: 1.42418066561. Times trained:               13135. Times reached goal: 123.               Steps done: 3696867.\n",
      " state (0)  A[0]:(0.99473887682) A[1]:(0.00178954482544) A[2]:(0.00192617799621) A[3]:(0.00154540885706)\n",
      " state (1)  A[0]:(0.0248034335673) A[1]:(0.00652473652735) A[2]:(0.0091296909377) A[3]:(0.959542155266)\n",
      " state (2)  A[0]:(0.999987185001) A[1]:(7.20266052667e-06) A[2]:(5.61068964089e-06) A[3]:(9.79467795759e-09)\n",
      " state (3)  A[0]:(0.999997138977) A[1]:(1.76827791165e-06) A[2]:(1.06324318949e-06) A[3]:(2.93744778501e-10)\n",
      " state (4)  A[0]:(0.999989271164) A[1]:(7.65462755226e-06) A[2]:(3.07796335619e-06) A[3]:(4.91162832628e-10)\n",
      " state (5)  A[0]:(0.994214653969) A[1]:(0.00459234882146) A[2]:(0.00119298184291) A[3]:(7.4718092824e-11)\n",
      " state (6)  A[0]:(0.00846449658275) A[1]:(0.983044266701) A[2]:(0.00849126651883) A[3]:(1.80204269001e-15)\n",
      " state (7)  A[0]:(3.86212268495e-05) A[1]:(0.999838232994) A[2]:(0.000123161415104) A[3]:(1.59927730356e-18)\n",
      " state (8)  A[0]:(2.79058440356e-06) A[1]:(0.999984204769) A[2]:(1.30104172058e-05) A[3]:(6.69984953778e-20)\n",
      " state (9)  A[0]:(8.79534809428e-07) A[1]:(0.999994039536) A[2]:(5.06926699018e-06) A[3]:(1.84417671903e-20)\n",
      " state (10)  A[0]:(5.36926393124e-07) A[1]:(0.999995946884) A[2]:(3.52141864823e-06) A[3]:(1.11908109927e-20)\n",
      " state (11)  A[0]:(4.30526029049e-07) A[1]:(0.999996542931) A[2]:(3.04718992084e-06) A[3]:(9.15478346313e-21)\n",
      " state (12)  A[0]:(3.87795296319e-07) A[1]:(0.999996721745) A[2]:(2.8729305086e-06) A[3]:(8.42069863579e-21)\n",
      " state (13)  A[0]:(3.67670907053e-07) A[1]:(0.999996840954) A[2]:(2.80458675661e-06) A[3]:(8.12620618185e-21)\n",
      " state (14)  A[0]:(3.5683225974e-07) A[1]:(0.999996840954) A[2]:(2.77764661405e-06) A[3]:(8.00153374611e-21)\n",
      " state (15)  A[0]:(3.50235012547e-07) A[1]:(0.999996900558) A[2]:(2.76755940831e-06) A[3]:(7.94681057871e-21)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 324000 finished after 6 . Running score: 0.15. Policy_loss: -92050.6112163, Value_loss: 1.40814578144. Times trained:               12729. Times reached goal: 137.               Steps done: 3709596.\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0018,  0.0018,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0018,  0.0018,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0018,  0.0018,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0018,  0.0018,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0018,  0.0018,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0154e-06,  1.1431e-06,  2.8407e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0154e-06,  1.1431e-06,  2.8407e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0154e-06,  1.1431e-06,  2.8407e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.8233e-05,  9.9987e-01,  6.4411e-05,  7.8958e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.8232e-05,  9.9987e-01,  6.4411e-05,  7.8961e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.7799e-06,  9.9999e-01,  8.6670e-06,  3.9331e-20]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4500e-06,  1.0000e+00,  2.7413e-06,  7.9973e-21]])\n",
      "On state=10, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.7797e-06,  9.9999e-01,  8.6668e-06,  3.9334e-20]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4499e-06,  1.0000e+00,  2.7413e-06,  7.9979e-21]])\n",
      "On state=10, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.4097e-07,  1.0000e+00,  1.0885e-06,  2.2669e-21]])\n",
      "On state=14, selected action=1\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.994974553585) A[1]:(0.00175337842666) A[2]:(0.00180678488687) A[3]:(0.00146530009806)\n",
      " state (1)  A[0]:(0.0306878983974) A[1]:(0.00743481935933) A[2]:(0.0101419091225) A[3]:(0.951735377312)\n",
      " state (2)  A[0]:(0.999986946583) A[1]:(7.6095411714e-06) A[2]:(5.42992393093e-06) A[3]:(1.07658850723e-08)\n",
      " state (3)  A[0]:(0.999997794628) A[1]:(1.4181852066e-06) A[2]:(7.67617336805e-07) A[3]:(2.10401862599e-10)\n",
      " state (4)  A[0]:(0.999995827675) A[1]:(3.01547788695e-06) A[2]:(1.14311183097e-06) A[3]:(2.8407973196e-10)\n",
      " state (5)  A[0]:(0.999843418598) A[1]:(0.000143819881487) A[2]:(1.27523780975e-05) A[3]:(3.51776663354e-10)\n",
      " state (6)  A[0]:(0.351199239492) A[1]:(0.643745243549) A[2]:(0.00505555653945) A[3]:(4.43869171019e-13)\n",
      " state (7)  A[0]:(0.00217957771383) A[1]:(0.997127711773) A[2]:(0.000692686531693) A[3]:(8.40434362072e-17)\n",
      " state (8)  A[0]:(6.82237878209e-05) A[1]:(0.999867379665) A[2]:(6.44033498247e-05) A[3]:(7.89686738434e-19)\n",
      " state (9)  A[0]:(5.77937225898e-06) A[1]:(0.999985575676) A[2]:(8.66631398821e-06) A[3]:(3.93361345903e-20)\n",
      " state (10)  A[0]:(1.44989598994e-06) A[1]:(0.999995827675) A[2]:(2.74120998256e-06) A[3]:(7.99826056657e-21)\n",
      " state (11)  A[0]:(7.46504724702e-07) A[1]:(0.999997615814) A[2]:(1.61290233791e-06) A[3]:(3.88088101809e-21)\n",
      " state (12)  A[0]:(5.46955959635e-07) A[1]:(0.999998152256) A[2]:(1.27431724195e-06) A[3]:(2.81339575381e-21)\n",
      " state (13)  A[0]:(4.7240848744e-07) A[1]:(0.999998390675) A[2]:(1.14463875889e-06) A[3]:(2.42883613469e-21)\n",
      " state (14)  A[0]:(4.40967312443e-07) A[1]:(0.999998450279) A[2]:(1.08851486402e-06) A[3]:(2.26698358639e-21)\n",
      " state (15)  A[0]:(4.27140633974e-07) A[1]:(0.999998509884) A[2]:(1.06259801669e-06) A[3]:(2.19331685206e-21)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 325000 finished after 15 . Running score: 0.15. Policy_loss: -92050.6386958, Value_loss: 1.19793348262. Times trained:               13681. Times reached goal: 99.               Steps done: 3723277.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994729459286) A[1]:(0.00182569341268) A[2]:(0.00188491318841) A[3]:(0.00155991874635)\n",
      " state (1)  A[0]:(0.032490439713) A[1]:(0.00775071186945) A[2]:(0.0106970891356) A[3]:(0.949061751366)\n",
      " state (2)  A[0]:(0.999997377396) A[1]:(1.60940487604e-06) A[2]:(1.00013835436e-06) A[3]:(3.28033739061e-10)\n",
      " state (3)  A[0]:(0.99999755621) A[1]:(1.57461704475e-06) A[2]:(8.4233039388e-07) A[3]:(2.04266714654e-10)\n",
      " state (4)  A[0]:(0.99999320507) A[1]:(5.13530176249e-06) A[2]:(1.67966197751e-06) A[3]:(3.1447019988e-10)\n",
      " state (5)  A[0]:(0.998878896236) A[1]:(0.00104655744508) A[2]:(7.45576471672e-05) A[3]:(1.8686066583e-10)\n",
      " state (6)  A[0]:(0.22447052598) A[1]:(0.766604185104) A[2]:(0.00892530009151) A[3]:(1.12904247041e-13)\n",
      " state (7)  A[0]:(0.00821074657142) A[1]:(0.989328503609) A[2]:(0.00246075144969) A[3]:(3.82419573227e-16)\n",
      " state (8)  A[0]:(0.000757330679335) A[1]:(0.998694181442) A[2]:(0.000548492651433) A[3]:(1.34297123418e-17)\n",
      " state (9)  A[0]:(7.20182797522e-05) A[1]:(0.999840557575) A[2]:(8.74312245287e-05) A[3]:(7.00817555348e-19)\n",
      " state (10)  A[0]:(9.51096717472e-06) A[1]:(0.999974727631) A[2]:(1.57657595992e-05) A[3]:(6.2393865251e-20)\n",
      " state (11)  A[0]:(2.54120163845e-06) A[1]:(0.999992370605) A[2]:(5.10766267325e-06) A[3]:(1.35993807985e-20)\n",
      " state (12)  A[0]:(1.22814390124e-06) A[1]:(0.999995946884) A[2]:(2.79560367744e-06) A[3]:(6.07327718743e-21)\n",
      " state (13)  A[0]:(8.36945844185e-07) A[1]:(0.999997079372) A[2]:(2.06131994673e-06) A[3]:(4.03938788739e-21)\n",
      " state (14)  A[0]:(6.83799555645e-07) A[1]:(0.99999755621) A[2]:(1.76492267201e-06) A[3]:(3.27953121408e-21)\n",
      " state (15)  A[0]:(6.1473548385e-07) A[1]:(0.999997735023) A[2]:(1.62833941886e-06) A[3]:(2.94268170056e-21)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 326000 finished after 13 . Running score: 0.11. Policy_loss: -92050.6283075, Value_loss: 1.20162060766. Times trained:               12681. Times reached goal: 132.               Steps done: 3735958.\n",
      " state (0)  A[0]:(0.994738042355) A[1]:(0.00183128670324) A[2]:(0.00184320216067) A[3]:(0.00158747029491)\n",
      " state (1)  A[0]:(0.0299438294023) A[1]:(0.00726654659957) A[2]:(0.0100564202294) A[3]:(0.95273321867)\n",
      " state (2)  A[0]:(0.999997913837) A[1]:(1.25693884456e-06) A[2]:(8.12600148947e-07) A[3]:(2.07207875857e-10)\n",
      " state (3)  A[0]:(0.999997496605) A[1]:(1.58804175499e-06) A[2]:(8.87657733983e-07) A[3]:(1.89801771522e-10)\n",
      " state (4)  A[0]:(0.999990463257) A[1]:(6.99587644704e-06) A[2]:(2.53602070188e-06) A[3]:(2.38120301255e-10)\n",
      " state (5)  A[0]:(0.996875941753) A[1]:(0.00263188011013) A[2]:(0.000492176855914) A[3]:(1.62020188932e-11)\n",
      " state (6)  A[0]:(0.172766923904) A[1]:(0.808319509029) A[2]:(0.0189135503024) A[3]:(1.53113300847e-14)\n",
      " state (7)  A[0]:(0.00800386816263) A[1]:(0.988567352295) A[2]:(0.00342880212702) A[3]:(1.61810080343e-16)\n",
      " state (8)  A[0]:(0.000655464769807) A[1]:(0.998801767826) A[2]:(0.000542756635696) A[3]:(6.557295635e-18)\n",
      " state (9)  A[0]:(5.80465966777e-05) A[1]:(0.999870657921) A[2]:(7.12970140739e-05) A[3]:(3.46234281466e-19)\n",
      " state (10)  A[0]:(9.46371073951e-06) A[1]:(0.99997574091) A[2]:(1.47899809235e-05) A[3]:(4.0575884458e-20)\n",
      " state (11)  A[0]:(3.30228726853e-06) A[1]:(0.999990642071) A[2]:(6.08163190918e-06) A[3]:(1.22878237372e-20)\n",
      " state (12)  A[0]:(1.88416470337e-06) A[1]:(0.999994218349) A[2]:(3.90658988181e-06) A[3]:(6.76535271033e-21)\n",
      " state (13)  A[0]:(1.39429630508e-06) A[1]:(0.999995470047) A[2]:(3.14140015689e-06) A[3]:(5.02954650663e-21)\n",
      " state (14)  A[0]:(1.18184254916e-06) A[1]:(0.999996006489) A[2]:(2.81725624518e-06) A[3]:(4.3302545599e-21)\n",
      " state (15)  A[0]:(1.0767104186e-06) A[1]:(0.999996244907) A[2]:(2.66676033789e-06) A[3]:(4.01143701828e-21)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 327000 finished after 14 . Running score: 0.17. Policy_loss: -92050.6110791, Value_loss: 2.06192377257. Times trained:               12592. Times reached goal: 119.               Steps done: 3748550.\n",
      " state (0)  A[0]:(0.994837164879) A[1]:(0.00178874295671) A[2]:(0.00184700102545) A[3]:(0.00152711465489)\n",
      " state (1)  A[0]:(0.0243367291987) A[1]:(0.00642442470416) A[2]:(0.00951140932739) A[3]:(0.959727406502)\n",
      " state (2)  A[0]:(0.99999755621) A[1]:(1.28952706291e-06) A[2]:(1.17064053029e-06) A[3]:(2.06706485262e-10)\n",
      " state (3)  A[0]:(0.999996602535) A[1]:(1.78882919499e-06) A[2]:(1.58436000675e-06) A[3]:(2.00240782289e-10)\n",
      " state (4)  A[0]:(0.999982655048) A[1]:(8.25382267067e-06) A[2]:(9.06216337171e-06) A[3]:(1.40768438572e-10)\n",
      " state (5)  A[0]:(0.994619548321) A[1]:(0.00116692937445) A[2]:(0.00421350076795) A[3]:(4.59396106225e-12)\n",
      " state (6)  A[0]:(0.234551429749) A[1]:(0.477563768625) A[2]:(0.287884771824) A[3]:(3.6977006472e-14)\n",
      " state (7)  A[0]:(0.00946890935302) A[1]:(0.947186291218) A[2]:(0.0433447994292) A[3]:(4.90814882413e-16)\n",
      " state (8)  A[0]:(0.000719394069165) A[1]:(0.993004798889) A[2]:(0.00627578562126) A[3]:(2.02200264097e-17)\n",
      " state (9)  A[0]:(5.13538143423e-05) A[1]:(0.999163627625) A[2]:(0.000784993986599) A[3]:(8.50767564322e-19)\n",
      " state (10)  A[0]:(5.66177914152e-06) A[1]:(0.999854445457) A[2]:(0.000139881565701) A[3]:(6.75456042671e-20)\n",
      " state (11)  A[0]:(1.43483794091e-06) A[1]:(0.999949038029) A[2]:(4.95070089528e-05) A[3]:(1.50817450018e-20)\n",
      " state (12)  A[0]:(6.88978332164e-07) A[1]:(0.999970376492) A[2]:(2.89612544293e-05) A[3]:(6.98676165688e-21)\n",
      " state (13)  A[0]:(4.7207163334e-07) A[1]:(0.99997740984) A[2]:(2.21214813791e-05) A[3]:(4.74840566866e-21)\n",
      " state (14)  A[0]:(3.88360348325e-07) A[1]:(0.999980330467) A[2]:(1.92920506379e-05) A[3]:(3.90238407895e-21)\n",
      " state (15)  A[0]:(3.50893287759e-07) A[1]:(0.999981641769) A[2]:(1.79804428626e-05) A[3]:(3.52751939627e-21)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 328000 finished after 13 . Running score: 0.12. Policy_loss: -92050.6108966, Value_loss: 1.83888642127. Times trained:               12712. Times reached goal: 133.               Steps done: 3761262.\n",
      " state (0)  A[0]:(0.994977116585) A[1]:(0.00176009524148) A[2]:(0.00180244829971) A[3]:(0.00146035628859)\n",
      " state (1)  A[0]:(0.0262925568968) A[1]:(0.00685218395665) A[2]:(0.00986009929329) A[3]:(0.95699518919)\n",
      " state (2)  A[0]:(0.999997437) A[1]:(1.4302472664e-06) A[2]:(1.10751591365e-06) A[3]:(1.93063745546e-10)\n",
      " state (3)  A[0]:(0.999994277954) A[1]:(3.44310501532e-06) A[2]:(2.27610075854e-06) A[3]:(2.45163667145e-10)\n",
      " state (4)  A[0]:(0.999882638454) A[1]:(6.69501750963e-05) A[2]:(5.03919291077e-05) A[3]:(1.27563251628e-10)\n",
      " state (5)  A[0]:(0.935625433922) A[1]:(0.0253122393042) A[2]:(0.0390623398125) A[3]:(1.62760777078e-12)\n",
      " state (6)  A[0]:(0.0361520685256) A[1]:(0.896026551723) A[2]:(0.0678213611245) A[3]:(1.99794192895e-15)\n",
      " state (7)  A[0]:(0.00131537939887) A[1]:(0.993109345436) A[2]:(0.0055753053166) A[3]:(2.09284023796e-17)\n",
      " state (8)  A[0]:(0.000115419410577) A[1]:(0.999116718769) A[2]:(0.000767878605984) A[3]:(9.22252202667e-19)\n",
      " state (9)  A[0]:(1.08904632725e-05) A[1]:(0.999878585339) A[2]:(0.000110544489871) A[3]:(5.22229461431e-20)\n",
      " state (10)  A[0]:(1.92074844563e-06) A[1]:(0.99996984005) A[2]:(2.82312212221e-05) A[3]:(7.31368459916e-21)\n",
      " state (11)  A[0]:(7.17806187822e-07) A[1]:(0.999985575676) A[2]:(1.36801445478e-05) A[3]:(2.59953200307e-21)\n",
      " state (12)  A[0]:(4.33110159292e-07) A[1]:(0.999989926815) A[2]:(9.63515412877e-06) A[3]:(1.57574079769e-21)\n",
      " state (13)  A[0]:(3.35612554636e-07) A[1]:(0.99999153614) A[2]:(8.13094629848e-06) A[3]:(1.23580581567e-21)\n",
      " state (14)  A[0]:(2.94927218647e-07) A[1]:(0.999992251396) A[2]:(7.47884951124e-06) A[3]:(1.09609266761e-21)\n",
      " state (15)  A[0]:(2.7611847031e-07) A[1]:(0.999992549419) A[2]:(7.17322245691e-06) A[3]:(1.03224042316e-21)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 329000 finished after 6 . Running score: 0.16. Policy_loss: -92050.3396187, Value_loss: 1.41694905325. Times trained:               12857. Times reached goal: 132.               Steps done: 3774119.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9951,  0.0017,  0.0018,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.9564e-06,  4.6834e-06,  1.3115e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.1072e-04,  9.9964e-01,  2.4871e-04,  3.1695e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995065391064) A[1]:(0.00174192117993) A[2]:(0.00178090308327) A[3]:(0.00141179130878)\n",
      " state (1)  A[0]:(0.0296914950013) A[1]:(0.00759232835844) A[2]:(0.0106463767588) A[3]:(0.952069818974)\n",
      " state (2)  A[0]:(0.999997675419) A[1]:(1.37488643759e-06) A[2]:(9.6913561265e-07) A[3]:(1.57716603666e-10)\n",
      " state (3)  A[0]:(0.999996900558) A[1]:(1.8701115323e-06) A[2]:(1.22096878385e-06) A[3]:(1.54468146607e-10)\n",
      " state (4)  A[0]:(0.999987363815) A[1]:(7.95592495706e-06) A[2]:(4.68320195068e-06) A[3]:(1.31150187932e-10)\n",
      " state (5)  A[0]:(0.98349070549) A[1]:(0.0105950310826) A[2]:(0.00591427646577) A[3]:(1.61977972268e-12)\n",
      " state (6)  A[0]:(0.0428016111255) A[1]:(0.935344874859) A[2]:(0.021853517741) A[3]:(1.04756852691e-15)\n",
      " state (7)  A[0]:(0.00146560475696) A[1]:(0.996530473232) A[2]:(0.00200390140526) A[3]:(9.48949208782e-18)\n",
      " state (8)  A[0]:(0.000110787885205) A[1]:(0.999640345573) A[2]:(0.000248854950769) A[3]:(3.17201224242e-19)\n",
      " state (9)  A[0]:(1.06367469925e-05) A[1]:(0.999954998493) A[2]:(3.43467763742e-05) A[3]:(1.6222659711e-20)\n",
      " state (10)  A[0]:(1.81129519206e-06) A[1]:(0.999990046024) A[2]:(8.16606734588e-06) A[3]:(1.98119248281e-21)\n",
      " state (11)  A[0]:(6.19472814378e-07) A[1]:(0.999995589256) A[2]:(3.77914034289e-06) A[3]:(6.39215632331e-22)\n",
      " state (12)  A[0]:(3.47161375203e-07) A[1]:(0.999997019768) A[2]:(2.63736319539e-06) A[3]:(3.74226778195e-22)\n",
      " state (13)  A[0]:(2.57538914639e-07) A[1]:(0.999997496605) A[2]:(2.23448228098e-06) A[3]:(2.91440458383e-22)\n",
      " state (14)  A[0]:(2.21249891297e-07) A[1]:(0.999997735023) A[2]:(2.06629283639e-06) A[3]:(2.58692002308e-22)\n",
      " state (15)  A[0]:(2.04810518767e-07) A[1]:(0.999997794628) A[2]:(1.9896149297e-06) A[3]:(2.44109460402e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 330000 finished after 3 . Running score: 0.11. Policy_loss: -92050.6296163, Value_loss: 0.99427303083. Times trained:               13686. Times reached goal: 101.               Steps done: 3787805.\n",
      " state (0)  A[0]:(0.995179355145) A[1]:(0.00169441313483) A[2]:(0.00175520754419) A[3]:(0.00137101544533)\n",
      " state (1)  A[0]:(0.0365642979741) A[1]:(0.00844124611467) A[2]:(0.0120485797524) A[3]:(0.942945897579)\n",
      " state (2)  A[0]:(0.999997973442) A[1]:(1.19056915082e-06) A[2]:(8.30548231079e-07) A[3]:(1.23264731755e-10)\n",
      " state (3)  A[0]:(0.999997496605) A[1]:(1.51238361923e-06) A[2]:(9.92806803879e-07) A[3]:(1.23350288317e-10)\n",
      " state (4)  A[0]:(0.999995112419) A[1]:(3.04502145809e-06) A[2]:(1.83262750397e-06) A[3]:(1.15752005203e-10)\n",
      " state (5)  A[0]:(0.999944150448) A[1]:(3.48657631548e-05) A[2]:(2.09892114071e-05) A[3]:(4.05050437635e-11)\n",
      " state (6)  A[0]:(0.906882405281) A[1]:(0.0701279938221) A[2]:(0.0229895990342) A[3]:(1.4170809708e-13)\n",
      " state (7)  A[0]:(0.0184157211334) A[1]:(0.973625063896) A[2]:(0.00795919634402) A[3]:(1.0298285468e-16)\n",
      " state (8)  A[0]:(0.000511951278895) A[1]:(0.999045789242) A[2]:(0.000442282675067) A[3]:(7.14191721685e-19)\n",
      " state (9)  A[0]:(3.4686516301e-05) A[1]:(0.999923586845) A[2]:(4.17375449615e-05) A[3]:(1.97273641897e-20)\n",
      " state (10)  A[0]:(5.32237299922e-06) A[1]:(0.999986231327) A[2]:(8.43328416522e-06) A[3]:(1.88536110468e-21)\n",
      " state (11)  A[0]:(1.76695527898e-06) A[1]:(0.999994575977) A[2]:(3.6851488403e-06) A[3]:(5.58836385888e-22)\n",
      " state (12)  A[0]:(9.66115180745e-07) A[1]:(0.999996542931) A[2]:(2.51398591899e-06) A[3]:(3.16061021805e-22)\n",
      " state (13)  A[0]:(6.99301210716e-07) A[1]:(0.999997198582) A[2]:(2.1069270133e-06) A[3]:(2.41833855437e-22)\n",
      " state (14)  A[0]:(5.88512591548e-07) A[1]:(0.999997496605) A[2]:(1.93764708456e-06) A[3]:(2.12567822672e-22)\n",
      " state (15)  A[0]:(5.36264565199e-07) A[1]:(0.999997615814) A[2]:(1.86075703823e-06) A[3]:(1.9950171124e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 331000 finished after 20 . Running score: 0.12. Policy_loss: -92050.6295801, Value_loss: 1.00408608658. Times trained:               12914. Times reached goal: 120.               Steps done: 3800719.\n",
      " state (0)  A[0]:(0.994835853577) A[1]:(0.00173047755379) A[2]:(0.00198654690757) A[3]:(0.00144714757334)\n",
      " state (1)  A[0]:(0.0327294804156) A[1]:(0.00818148814142) A[2]:(0.0115011213347) A[3]:(0.947587907314)\n",
      " state (2)  A[0]:(0.999997735023) A[1]:(1.30857961267e-06) A[2]:(9.47037790411e-07) A[3]:(1.31340299747e-10)\n",
      " state (3)  A[0]:(0.99999666214) A[1]:(1.9700669327e-06) A[2]:(1.35308414428e-06) A[3]:(1.34482494585e-10)\n",
      " state (4)  A[0]:(0.999989926815) A[1]:(5.90896343056e-06) A[2]:(4.15580234403e-06) A[3]:(1.07444490804e-10)\n",
      " state (5)  A[0]:(0.999717533588) A[1]:(0.000131307897391) A[2]:(0.000151153930346) A[3]:(1.54837236782e-11)\n",
      " state (6)  A[0]:(0.804021298885) A[1]:(0.115529343486) A[2]:(0.0804493501782) A[3]:(8.6483358181e-14)\n",
      " state (7)  A[0]:(0.028174854815) A[1]:(0.95187908411) A[2]:(0.0199460685253) A[3]:(2.36644250092e-16)\n",
      " state (8)  A[0]:(0.00158331741113) A[1]:(0.996598899364) A[2]:(0.00181780813728) A[3]:(3.98622597169e-18)\n",
      " state (9)  A[0]:(0.000149900428369) A[1]:(0.999626994133) A[2]:(0.000223094873945) A[3]:(1.55473597987e-19)\n",
      " state (10)  A[0]:(2.04458974622e-05) A[1]:(0.99994212389) A[2]:(3.74241244572e-05) A[3]:(1.09791715016e-20)\n",
      " state (11)  A[0]:(5.18434853802e-06) A[1]:(0.999983072281) A[2]:(1.17356512419e-05) A[3]:(2.00366085698e-21)\n",
      " state (12)  A[0]:(2.25176449931e-06) A[1]:(0.999991476536) A[2]:(6.27013014309e-06) A[3]:(7.95868615388e-22)\n",
      " state (13)  A[0]:(1.37549534429e-06) A[1]:(0.99999409914) A[2]:(4.5314509407e-06) A[3]:(4.90705309403e-22)\n",
      " state (14)  A[0]:(1.03045590549e-06) A[1]:(0.999995172024) A[2]:(3.81850031772e-06) A[3]:(3.79234341003e-22)\n",
      " state (15)  A[0]:(8.71883003128e-07) A[1]:(0.999995648861) A[2]:(3.48217145074e-06) A[3]:(3.29713490393e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 332000 finished after 25 . Running score: 0.17. Policy_loss: -92050.6303117, Value_loss: 1.40273002524. Times trained:               12783. Times reached goal: 119.               Steps done: 3813502.\n",
      " state (0)  A[0]:(0.994648635387) A[1]:(0.00177769304719) A[2]:(0.00208160770126) A[3]:(0.00149206176866)\n",
      " state (1)  A[0]:(0.0264036022127) A[1]:(0.00763970892876) A[2]:(0.0112206256017) A[3]:(0.954736053944)\n",
      " state (2)  A[0]:(0.999996602535) A[1]:(1.76173762156e-06) A[2]:(1.64354730714e-06) A[3]:(1.9932129558e-10)\n",
      " state (3)  A[0]:(0.999994397163) A[1]:(2.79191726804e-06) A[2]:(2.78553420685e-06) A[3]:(1.94689112054e-10)\n",
      " state (4)  A[0]:(0.999974668026) A[1]:(1.03093370853e-05) A[2]:(1.50357163875e-05) A[3]:(1.3945676558e-10)\n",
      " state (5)  A[0]:(0.998772203922) A[1]:(0.000210773447179) A[2]:(0.00101705209818) A[3]:(1.90426181768e-11)\n",
      " state (6)  A[0]:(0.607766449451) A[1]:(0.0378495380282) A[2]:(0.35438400507) A[3]:(4.53307936977e-13)\n",
      " state (7)  A[0]:(0.0630197450519) A[1]:(0.544939815998) A[2]:(0.392040401697) A[3]:(1.13299406546e-14)\n",
      " state (8)  A[0]:(0.006191378925) A[1]:(0.91590565443) A[2]:(0.07790299505) A[3]:(4.29966285413e-16)\n",
      " state (9)  A[0]:(0.00080724369036) A[1]:(0.983019590378) A[2]:(0.0161731764674) A[3]:(2.96996849744e-17)\n",
      " state (10)  A[0]:(9.15914206416e-05) A[1]:(0.996969997883) A[2]:(0.00293843448162) A[3]:(1.91672843975e-18)\n",
      " state (11)  A[0]:(9.94614401861e-06) A[1]:(0.999479591846) A[2]:(0.000510447425768) A[3]:(1.26317846614e-19)\n",
      " state (12)  A[0]:(1.62504522905e-06) A[1]:(0.999876260757) A[2]:(0.000122103345348) A[3]:(1.43825899761e-20)\n",
      " state (13)  A[0]:(4.94882613111e-07) A[1]:(0.999951720238) A[2]:(4.78104338981e-05) A[3]:(3.52628347211e-21)\n",
      " state (14)  A[0]:(2.46770810008e-07) A[1]:(0.999972105026) A[2]:(2.7652575227e-05) A[3]:(1.559281095e-21)\n",
      " state (15)  A[0]:(1.66072211982e-07) A[1]:(0.999979555607) A[2]:(2.0262603357e-05) A[3]:(9.82016564956e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 333000 finished after 3 . Running score: 0.13. Policy_loss: -92050.6666774, Value_loss: 0.999587431858. Times trained:               13159. Times reached goal: 111.               Steps done: 3826661.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994603931904) A[1]:(0.00178958359174) A[2]:(0.00211231317371) A[3]:(0.00149415328633)\n",
      " state (1)  A[0]:(0.0214385800064) A[1]:(0.00679163867608) A[2]:(0.0097154956311) A[3]:(0.962054312229)\n",
      " state (2)  A[0]:(0.999995708466) A[1]:(2.20841002374e-06) A[2]:(2.06666709346e-06) A[3]:(3.98134830659e-10)\n",
      " state (3)  A[0]:(0.999995172024) A[1]:(2.38224788518e-06) A[2]:(2.42576334131e-06) A[3]:(1.76592337975e-10)\n",
      " state (4)  A[0]:(0.999976992607) A[1]:(8.64282355906e-06) A[2]:(1.43742345244e-05) A[3]:(9.50909559427e-11)\n",
      " state (5)  A[0]:(0.998730480671) A[1]:(0.000173790569534) A[2]:(0.00109571509529) A[3]:(8.33463766753e-12)\n",
      " state (6)  A[0]:(0.709788739681) A[1]:(0.0257510654628) A[2]:(0.264460206032) A[3]:(3.03540179103e-13)\n",
      " state (7)  A[0]:(0.0479651689529) A[1]:(0.732933402061) A[2]:(0.219101458788) A[3]:(4.64874466639e-15)\n",
      " state (8)  A[0]:(0.000578011968173) A[1]:(0.993293464184) A[2]:(0.00612849416211) A[3]:(1.14012380052e-17)\n",
      " state (9)  A[0]:(1.90289792954e-05) A[1]:(0.999616086483) A[2]:(0.000364903564332) A[3]:(1.43571515873e-19)\n",
      " state (10)  A[0]:(1.68138535628e-06) A[1]:(0.999944865704) A[2]:(5.34522805538e-05) A[3]:(7.8765309228e-21)\n",
      " state (11)  A[0]:(4.3326340915e-07) A[1]:(0.999980330467) A[2]:(1.92607076315e-05) A[3]:(1.70622419743e-21)\n",
      " state (12)  A[0]:(2.26480793231e-07) A[1]:(0.999987721443) A[2]:(1.20413315017e-05) A[3]:(8.44017201433e-22)\n",
      " state (13)  A[0]:(1.67584460087e-07) A[1]:(0.999990105629) A[2]:(9.743861483e-06) A[3]:(6.13983745884e-22)\n",
      " state (14)  A[0]:(1.45286691122e-07) A[1]:(0.999990999699) A[2]:(8.8415636128e-06) A[3]:(5.30140226241e-22)\n",
      " state (15)  A[0]:(1.35491518449e-07) A[1]:(0.999991416931) A[2]:(8.44984697324e-06) A[3]:(4.94756394141e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 334000 finished after 8 . Running score: 0.12. Policy_loss: -92050.6112649, Value_loss: 1.20050657104. Times trained:               12800. Times reached goal: 129.               Steps done: 3839461.\n",
      "action_dist \n",
      "tensor([[ 0.9945,  0.0018,  0.0021,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9945,  0.0018,  0.0021,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9945,  0.0018,  0.0021,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9945,  0.0018,  0.0021,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9994e-01,  4.0931e-05,  1.9464e-05,  7.5750e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9994e-01,  4.0928e-05,  1.9463e-05,  7.5748e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9994e-01,  4.0926e-05,  1.9462e-05,  7.5747e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9994e-01,  4.0925e-05,  1.9462e-05,  7.5746e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.4390e-05,  9.9970e-01,  2.6696e-04,  1.7386e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7571e-06,  9.9996e-01,  3.3219e-05,  7.2359e-21]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.4392e-05,  9.9970e-01,  2.6696e-04,  1.7388e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994530558586) A[1]:(0.00182396511082) A[2]:(0.00213023484685) A[3]:(0.00151521223597)\n",
      " state (1)  A[0]:(0.0197007246315) A[1]:(0.00663441652432) A[2]:(0.00903607066721) A[3]:(0.964628815651)\n",
      " state (2)  A[0]:(0.99838411808) A[1]:(0.000707368715666) A[2]:(0.000815528212115) A[3]:(9.29673697101e-05)\n",
      " state (3)  A[0]:(0.999993145466) A[1]:(4.21823369834e-06) A[2]:(2.66272672889e-06) A[3]:(3.89220516928e-10)\n",
      " state (4)  A[0]:(0.999939620495) A[1]:(4.09080967074e-05) A[2]:(1.94551812456e-05) A[3]:(7.5732137228e-10)\n",
      " state (5)  A[0]:(0.994341433048) A[1]:(0.0033852821216) A[2]:(0.00227329437621) A[3]:(2.51926146611e-10)\n",
      " state (6)  A[0]:(0.246592581272) A[1]:(0.532228946686) A[2]:(0.221178486943) A[3]:(1.89319371811e-13)\n",
      " state (7)  A[0]:(0.00125431688502) A[1]:(0.992917597294) A[2]:(0.00582805834711) A[3]:(2.64859079693e-17)\n",
      " state (8)  A[0]:(3.44012951246e-05) A[1]:(0.999698579311) A[2]:(0.000267016526777) A[3]:(1.73930827962e-19)\n",
      " state (9)  A[0]:(2.75784896075e-06) A[1]:(0.999963998795) A[2]:(3.3224492654e-05) A[3]:(7.23798222503e-21)\n",
      " state (10)  A[0]:(5.96563211275e-07) A[1]:(0.999988734722) A[2]:(1.06806019176e-05) A[3]:(1.30638819222e-21)\n",
      " state (11)  A[0]:(2.78865286418e-07) A[1]:(0.999993264675) A[2]:(6.48301192996e-06) A[3]:(6.11871971551e-22)\n",
      " state (12)  A[0]:(1.96563448185e-07) A[1]:(0.999994516373) A[2]:(5.26342500962e-06) A[3]:(4.44312664624e-22)\n",
      " state (13)  A[0]:(1.67134061257e-07) A[1]:(0.99999499321) A[2]:(4.81856841361e-06) A[3]:(3.87274991951e-22)\n",
      " state (14)  A[0]:(1.54388445139e-07) A[1]:(0.999995231628) A[2]:(4.63829701403e-06) A[3]:(3.64473857321e-22)\n",
      " state (15)  A[0]:(1.47959553942e-07) A[1]:(0.999995291233) A[2]:(4.56203497379e-06) A[3]:(3.54540015931e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 335000 finished after 11 . Running score: 0.14. Policy_loss: -92050.6112064, Value_loss: 1.19011453575. Times trained:               12650. Times reached goal: 137.               Steps done: 3852111.\n",
      " state (0)  A[0]:(0.99469691515) A[1]:(0.00174679653719) A[2]:(0.00202992069535) A[3]:(0.00152639567386)\n",
      " state (1)  A[0]:(0.0199418440461) A[1]:(0.00632490962744) A[2]:(0.00852170307189) A[3]:(0.965211570263)\n",
      " state (2)  A[0]:(0.981715142727) A[1]:(0.00480415998027) A[2]:(0.00622348999605) A[3]:(0.00725721661001)\n",
      " state (3)  A[0]:(0.999995589256) A[1]:(2.73236400972e-06) A[2]:(1.7034992652e-06) A[3]:(2.65604760408e-10)\n",
      " state (4)  A[0]:(0.99997407198) A[1]:(1.78725167643e-05) A[2]:(8.04405954113e-06) A[3]:(3.99464961109e-10)\n",
      " state (5)  A[0]:(0.997908353806) A[1]:(0.00135818507988) A[2]:(0.000733454944566) A[3]:(1.31357924538e-10)\n",
      " state (6)  A[0]:(0.402982026339) A[1]:(0.461411982775) A[2]:(0.135605975986) A[3]:(1.03941242549e-13)\n",
      " state (7)  A[0]:(0.00119874882512) A[1]:(0.997290670872) A[2]:(0.00151060032658) A[3]:(4.52112148762e-18)\n",
      " state (8)  A[0]:(3.06403380819e-05) A[1]:(0.999922811985) A[2]:(4.65652046842e-05) A[3]:(1.994907929e-20)\n",
      " state (9)  A[0]:(4.77009598399e-06) A[1]:(0.999986708164) A[2]:(8.54474910739e-06) A[3]:(1.5794668465e-21)\n",
      " state (10)  A[0]:(1.9812678147e-06) A[1]:(0.999993741512) A[2]:(4.26007045462e-06) A[3]:(5.55783027179e-22)\n",
      " state (11)  A[0]:(1.32623461013e-06) A[1]:(0.999995410442) A[2]:(3.26223675984e-06) A[3]:(3.69862092848e-22)\n",
      " state (12)  A[0]:(1.10293137823e-06) A[1]:(0.999995946884) A[2]:(2.93682819574e-06) A[3]:(3.1405855204e-22)\n",
      " state (13)  A[0]:(1.00973909412e-06) A[1]:(0.999996185303) A[2]:(2.8127249152e-06) A[3]:(2.93129277052e-22)\n",
      " state (14)  A[0]:(9.63960360423e-07) A[1]:(0.999996304512) A[2]:(2.76100672636e-06) A[3]:(2.84178949574e-22)\n",
      " state (15)  A[0]:(9.37008792334e-07) A[1]:(0.999996304512) A[2]:(2.73757746072e-06) A[3]:(2.79818000272e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 336000 finished after 11 . Running score: 0.14. Policy_loss: -92050.6112058, Value_loss: 1.41084202546. Times trained:               12763. Times reached goal: 128.               Steps done: 3864874.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994983553886) A[1]:(0.0016177969519) A[2]:(0.00195756158791) A[3]:(0.00144109432586)\n",
      " state (1)  A[0]:(0.0212195068598) A[1]:(0.0063597522676) A[2]:(0.00857327040285) A[3]:(0.963847458363)\n",
      " state (2)  A[0]:(0.998805582523) A[1]:(0.000532894919161) A[2]:(0.00059437926393) A[3]:(6.71644011163e-05)\n",
      " state (3)  A[0]:(0.999996364117) A[1]:(2.30443174587e-06) A[2]:(1.31900219458e-06) A[3]:(1.98404861984e-10)\n",
      " state (4)  A[0]:(0.999976754189) A[1]:(1.73331100086e-05) A[2]:(5.9310054894e-06) A[3]:(2.49529286123e-10)\n",
      " state (5)  A[0]:(0.987049520016) A[1]:(0.0103497747332) A[2]:(0.002600713633) A[3]:(1.17471084943e-11)\n",
      " state (6)  A[0]:(0.00391118600965) A[1]:(0.993209302425) A[2]:(0.00287949340418) A[3]:(3.13023374851e-17)\n",
      " state (7)  A[0]:(1.27911735035e-05) A[1]:(0.999970376492) A[2]:(1.68569331436e-05) A[3]:(7.41142600517e-21)\n",
      " state (8)  A[0]:(1.48967239966e-06) A[1]:(0.99999588728) A[2]:(2.61122954726e-06) A[3]:(4.65140965416e-22)\n",
      " state (9)  A[0]:(5.81543929457e-07) A[1]:(0.999998033047) A[2]:(1.41081352467e-06) A[3]:(1.84408366121e-22)\n",
      " state (10)  A[0]:(3.7376793216e-07) A[1]:(0.999998450279) A[2]:(1.15376803933e-06) A[3]:(1.3438448731e-22)\n",
      " state (11)  A[0]:(2.99937454429e-07) A[1]:(0.999998629093) A[2]:(1.07623463919e-06) A[3]:(1.19490313792e-22)\n",
      " state (12)  A[0]:(2.65542666966e-07) A[1]:(0.999998688698) A[2]:(1.04924049538e-06) A[3]:(1.13862857773e-22)\n",
      " state (13)  A[0]:(2.46307592988e-07) A[1]:(0.999998688698) A[2]:(1.03916579519e-06) A[3]:(1.11365753288e-22)\n",
      " state (14)  A[0]:(2.34261037235e-07) A[1]:(0.999998748302) A[2]:(1.03525371742e-06) A[3]:(1.10095182359e-22)\n",
      " state (15)  A[0]:(2.2621692608e-07) A[1]:(0.999998748302) A[2]:(1.03371269233e-06) A[3]:(1.093739363e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 337000 finished after 4 . Running score: 0.14. Policy_loss: -92050.611199, Value_loss: 1.83974280468. Times trained:               12350. Times reached goal: 119.               Steps done: 3877224.\n",
      " state (0)  A[0]:(0.994964957237) A[1]:(0.00161105627194) A[2]:(0.0019942088984) A[3]:(0.00142978678923)\n",
      " state (1)  A[0]:(0.0218002833426) A[1]:(0.00644423486665) A[2]:(0.0086448919028) A[3]:(0.963110566139)\n",
      " state (2)  A[0]:(0.999453485012) A[1]:(0.00026039237855) A[2]:(0.000271903787507) A[3]:(1.42338376463e-05)\n",
      " state (3)  A[0]:(0.999997138977) A[1]:(1.84404086667e-06) A[2]:(1.03983927602e-06) A[3]:(1.61286137224e-10)\n",
      " state (4)  A[0]:(0.999991178513) A[1]:(6.78668311593e-06) A[2]:(2.01620150619e-06) A[3]:(2.04173483676e-10)\n",
      " state (5)  A[0]:(0.991844415665) A[1]:(0.00789453089237) A[2]:(0.000261068315012) A[3]:(2.99698842132e-11)\n",
      " state (6)  A[0]:(0.000404475053074) A[1]:(0.999368011951) A[2]:(0.000227484895731) A[3]:(1.82651322701e-18)\n",
      " state (7)  A[0]:(2.57862302533e-06) A[1]:(0.999992907047) A[2]:(4.51520463685e-06) A[3]:(1.54432499906e-21)\n",
      " state (8)  A[0]:(3.64004023368e-07) A[1]:(0.999998509884) A[2]:(1.14708223009e-06) A[3]:(1.90642172186e-22)\n",
      " state (9)  A[0]:(1.57363245989e-07) A[1]:(0.99999910593) A[2]:(7.5319132975e-07) A[3]:(9.97503886144e-23)\n",
      " state (10)  A[0]:(1.1107646003e-07) A[1]:(0.99999922514) A[2]:(6.59507009004e-07) A[3]:(8.06300915896e-23)\n",
      " state (11)  A[0]:(9.52618677275e-08) A[1]:(0.999999284744) A[2]:(6.30585645922e-07) A[3]:(7.47135906242e-23)\n",
      " state (12)  A[0]:(8.83100952365e-08) A[1]:(0.999999284744) A[2]:(6.20821197117e-07) A[3]:(7.2551209287e-23)\n",
      " state (13)  A[0]:(8.46877483696e-08) A[1]:(0.999999284744) A[2]:(6.17533316927e-07) A[3]:(7.16821685594e-23)\n",
      " state (14)  A[0]:(8.25577757269e-08) A[1]:(0.999999284744) A[2]:(6.16555894339e-07) A[3]:(7.13071630177e-23)\n",
      " state (15)  A[0]:(8.11924607547e-08) A[1]:(0.999999284744) A[2]:(6.16404179254e-07) A[3]:(7.11343709251e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 338000 finished after 18 . Running score: 0.13. Policy_loss: -92050.6111982, Value_loss: 1.43393786004. Times trained:               13103. Times reached goal: 132.               Steps done: 3890327.\n",
      " state (0)  A[0]:(0.994886696339) A[1]:(0.00162318418734) A[2]:(0.00206993869506) A[3]:(0.00142017891631)\n",
      " state (1)  A[0]:(0.0220610070974) A[1]:(0.00631413515657) A[2]:(0.00949044339359) A[3]:(0.962134420872)\n",
      " state (2)  A[0]:(0.999602019787) A[1]:(0.000158598093549) A[2]:(0.000233093640418) A[3]:(6.26076507615e-06)\n",
      " state (3)  A[0]:(0.999997437) A[1]:(1.24302096083e-06) A[2]:(1.29437648866e-06) A[3]:(1.29402211169e-10)\n",
      " state (4)  A[0]:(0.999996542931) A[1]:(1.87893306247e-06) A[2]:(1.5783816707e-06) A[3]:(1.32273331177e-10)\n",
      " state (5)  A[0]:(0.999976038933) A[1]:(1.77414840437e-05) A[2]:(6.20411719865e-06) A[3]:(1.44959752535e-10)\n",
      " state (6)  A[0]:(0.900322854519) A[1]:(0.0945026949048) A[2]:(0.00517443753779) A[3]:(1.25311783519e-12)\n",
      " state (7)  A[0]:(0.0117155015469) A[1]:(0.978887915611) A[2]:(0.00939658749849) A[3]:(1.07423656516e-16)\n",
      " state (8)  A[0]:(0.000260902830632) A[1]:(0.998459458351) A[2]:(0.00127962313127) A[3]:(3.07232301994e-19)\n",
      " state (9)  A[0]:(1.53819728439e-05) A[1]:(0.999811291695) A[2]:(0.000173335080035) A[3]:(7.08916483957e-21)\n",
      " state (10)  A[0]:(2.24550194616e-06) A[1]:(0.999952077866) A[2]:(4.56706329715e-05) A[3]:(7.97974331269e-22)\n",
      " state (11)  A[0]:(7.61054479881e-07) A[1]:(0.999975383282) A[2]:(2.38841239479e-05) A[3]:(2.83839322866e-22)\n",
      " state (12)  A[0]:(4.46105389074e-07) A[1]:(0.99998152256) A[2]:(1.80495753739e-05) A[3]:(1.81273041608e-22)\n",
      " state (13)  A[0]:(3.44860410451e-07) A[1]:(0.999983668327) A[2]:(1.60069539561e-05) A[3]:(1.49169482479e-22)\n",
      " state (14)  A[0]:(3.02973376165e-07) A[1]:(0.999984502792) A[2]:(1.51939630086e-05) A[3]:(1.36769851226e-22)\n",
      " state (15)  A[0]:(2.82489622805e-07) A[1]:(0.99998486042) A[2]:(1.48550570884e-05) A[3]:(1.31482665626e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 339000 finished after 10 . Running score: 0.17. Policy_loss: -92050.6112088, Value_loss: 1.206112404. Times trained:               12493. Times reached goal: 129.               Steps done: 3902820.\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0016,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0016,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0016,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.5709e-06,  2.8200e-06,  1.3616e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0016,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.5709e-06,  2.8200e-06,  1.3616e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.5710e-06,  2.8200e-06,  1.3616e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0016,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.5710e-06,  2.8200e-06,  1.3616e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.5710e-06,  2.8200e-06,  1.3616e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.5710e-06,  2.8200e-06,  1.3617e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0016,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.5710e-06,  2.8201e-06,  1.3617e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0016,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0016,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0016,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0016,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.5711e-06,  2.8201e-06,  1.3618e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6347e-06,  9.9997e-01,  2.8295e-05,  7.7502e-22]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6347e-06,  9.9997e-01,  2.8295e-05,  7.7511e-22]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.3039e-07,  9.9999e-01,  1.2293e-05,  2.1373e-22]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4286e-07,  9.9999e-01,  9.1704e-06,  1.3508e-22]])\n",
      "On state=10, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.3038e-07,  9.9999e-01,  1.2293e-05,  2.1377e-22]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6035e-07,  9.9999e-01,  7.8784e-06,  1.0515e-22]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6034e-07,  9.9999e-01,  7.8784e-06,  1.0516e-22]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6034e-07,  9.9999e-01,  7.8783e-06,  1.0516e-22]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5451e-07,  9.9999e-01,  7.8492e-06,  1.0419e-22]])\n",
      "On state=14, selected action=1\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.994872152805) A[1]:(0.00164760625921) A[2]:(0.00204602442682) A[3]:(0.00143420905806)\n",
      " state (1)  A[0]:(0.0214808359742) A[1]:(0.00624355208129) A[2]:(0.00927051715553) A[3]:(0.963005065918)\n",
      " state (2)  A[0]:(0.999977767467) A[1]:(1.01860359791e-05) A[2]:(1.20233344205e-05) A[3]:(1.43770622074e-08)\n",
      " state (3)  A[0]:(0.999997079372) A[1]:(1.51370181811e-06) A[2]:(1.39744952321e-06) A[3]:(1.33893882093e-10)\n",
      " state (4)  A[0]:(0.999992609024) A[1]:(4.57129044662e-06) A[2]:(2.82010660158e-06) A[3]:(1.36192584987e-10)\n",
      " state (5)  A[0]:(0.997370541096) A[1]:(0.00204683141783) A[2]:(0.000582642154768) A[3]:(4.60668482527e-12)\n",
      " state (6)  A[0]:(0.00500900344923) A[1]:(0.986166715622) A[2]:(0.008824265562) A[3]:(3.16750154383e-17)\n",
      " state (7)  A[0]:(2.42483893089e-05) A[1]:(0.999763250351) A[2]:(0.00021247335826) A[3]:(1.92363711038e-20)\n",
      " state (8)  A[0]:(1.63413267273e-06) A[1]:(0.999970078468) A[2]:(2.82899291051e-05) A[3]:(7.75443050177e-22)\n",
      " state (9)  A[0]:(4.3023391072e-07) A[1]:(0.999987304211) A[2]:(1.22918254419e-05) A[3]:(2.13815611298e-22)\n",
      " state (10)  A[0]:(2.42786654781e-07) A[1]:(0.999990582466) A[2]:(9.16926728678e-06) A[3]:(1.35123643667e-22)\n",
      " state (11)  A[0]:(1.90580138337e-07) A[1]:(0.99999153614) A[2]:(8.27411895443e-06) A[3]:(1.14564363377e-22)\n",
      " state (12)  A[0]:(1.70267824728e-07) A[1]:(0.999991834164) A[2]:(7.97727898316e-06) A[3]:(1.07703797857e-22)\n",
      " state (13)  A[0]:(1.60310548836e-07) A[1]:(0.999991953373) A[2]:(7.87763747212e-06) A[3]:(1.05168727074e-22)\n",
      " state (14)  A[0]:(1.54486329507e-07) A[1]:(0.999992012978) A[2]:(7.84858821135e-06) A[3]:(1.0418960554e-22)\n",
      " state (15)  A[0]:(1.50617950112e-07) A[1]:(0.999992012978) A[2]:(7.84510848462e-06) A[3]:(1.03803606432e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 340000 finished after 27 . Running score: 0.15. Policy_loss: -92050.6053113, Value_loss: 1.22243387035. Times trained:               13414. Times reached goal: 119.               Steps done: 3916234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995096087456) A[1]:(0.00160680781119) A[2]:(0.00191756826825) A[3]:(0.00137955544051)\n",
      " state (1)  A[0]:(0.0224144980311) A[1]:(0.00643052067608) A[2]:(0.00926599185914) A[3]:(0.961888968945)\n",
      " state (2)  A[0]:(0.999978840351) A[1]:(1.03984457382e-05) A[2]:(1.07558089439e-05) A[3]:(1.3739862581e-08)\n",
      " state (3)  A[0]:(0.999997198582) A[1]:(1.5924181298e-06) A[2]:(1.20327149489e-06) A[3]:(1.20913695723e-10)\n",
      " state (4)  A[0]:(0.999992072582) A[1]:(5.5759701354e-06) A[2]:(2.3713309929e-06) A[3]:(1.04784542154e-10)\n",
      " state (5)  A[0]:(0.992785334587) A[1]:(0.00660253036767) A[2]:(0.000612107862253) A[3]:(1.56756432921e-12)\n",
      " state (6)  A[0]:(0.000545818416867) A[1]:(0.998744249344) A[2]:(0.000709937652573) A[3]:(1.00442029221e-18)\n",
      " state (7)  A[0]:(4.30297268394e-06) A[1]:(0.999975681305) A[2]:(2.00065314857e-05) A[3]:(1.56895260543e-21)\n",
      " state (8)  A[0]:(6.23295136393e-07) A[1]:(0.999994695187) A[2]:(4.69172528028e-06) A[3]:(1.74142774011e-22)\n",
      " state (9)  A[0]:(2.65991047854e-07) A[1]:(0.999996900558) A[2]:(2.86141653305e-06) A[3]:(8.27021010215e-23)\n",
      " state (10)  A[0]:(1.8205066965e-07) A[1]:(0.999997377396) A[2]:(2.43954696089e-06) A[3]:(6.44807646407e-23)\n",
      " state (11)  A[0]:(1.51587883579e-07) A[1]:(0.99999755621) A[2]:(2.31443800658e-06) A[3]:(5.90829239433e-23)\n",
      " state (12)  A[0]:(1.36497490644e-07) A[1]:(0.999997615814) A[2]:(2.27541818276e-06) A[3]:(5.71961453629e-23)\n",
      " state (13)  A[0]:(1.27054320842e-07) A[1]:(0.999997615814) A[2]:(2.26498491429e-06) A[3]:(5.64681529651e-23)\n",
      " state (14)  A[0]:(1.20215545962e-07) A[1]:(0.999997615814) A[2]:(2.26445149565e-06) A[3]:(5.61575310951e-23)\n",
      " state (15)  A[0]:(1.14871504309e-07) A[1]:(0.999997615814) A[2]:(2.26723250307e-06) A[3]:(5.60062780605e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 341000 finished after 9 . Running score: 0.1. Policy_loss: -92050.6075941, Value_loss: 0.993721055526. Times trained:               12752. Times reached goal: 92.               Steps done: 3928986.\n",
      " state (0)  A[0]:(0.994971692562) A[1]:(0.00167759985197) A[2]:(0.00195548660122) A[3]:(0.00139522145037)\n",
      " state (1)  A[0]:(0.0224527604878) A[1]:(0.00640767207369) A[2]:(0.00944216921926) A[3]:(0.961697399616)\n",
      " state (2)  A[0]:(0.999986231327) A[1]:(6.70941244607e-06) A[2]:(7.08191737431e-06) A[3]:(6.37838892814e-09)\n",
      " state (3)  A[0]:(0.999997794628) A[1]:(1.16271883144e-06) A[2]:(1.02671640434e-06) A[3]:(1.07735709243e-10)\n",
      " state (4)  A[0]:(0.999997019768) A[1]:(1.70278042333e-06) A[2]:(1.28166618651e-06) A[3]:(1.06314262949e-10)\n",
      " state (5)  A[0]:(0.999982595444) A[1]:(1.18323596325e-05) A[2]:(5.54949838261e-06) A[3]:(5.88696869031e-11)\n",
      " state (6)  A[0]:(0.940189003944) A[1]:(0.0505695231259) A[2]:(0.00924144964665) A[3]:(1.47589975181e-13)\n",
      " state (7)  A[0]:(0.0237152334303) A[1]:(0.963562548161) A[2]:(0.0127222398296) A[3]:(1.09089293868e-16)\n",
      " state (8)  A[0]:(0.000494802021421) A[1]:(0.998626768589) A[2]:(0.000878429913428) A[3]:(5.40533527283e-19)\n",
      " state (9)  A[0]:(2.39387809415e-05) A[1]:(0.999904692173) A[2]:(7.13963381713e-05) A[3]:(1.05094694308e-20)\n",
      " state (10)  A[0]:(2.7114065233e-06) A[1]:(0.999985396862) A[2]:(1.19126698337e-05) A[3]:(7.63511282861e-22)\n",
      " state (11)  A[0]:(7.37396078421e-07) A[1]:(0.999994635582) A[2]:(4.65517405246e-06) A[3]:(1.95242796363e-22)\n",
      " state (12)  A[0]:(3.71606290628e-07) A[1]:(0.999996602535) A[2]:(3.05342405227e-06) A[3]:(1.05117072461e-22)\n",
      " state (13)  A[0]:(2.63308493231e-07) A[1]:(0.999997198582) A[2]:(2.53772168435e-06) A[3]:(7.97632533616e-23)\n",
      " state (14)  A[0]:(2.20687581987e-07) A[1]:(0.999997437) A[2]:(2.33622563428e-06) A[3]:(7.03223995508e-23)\n",
      " state (15)  A[0]:(2.00519906457e-07) A[1]:(0.99999755621) A[2]:(2.25010785471e-06) A[3]:(6.62897426033e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 342000 finished after 11 . Running score: 0.14. Policy_loss: -92050.6074662, Value_loss: 0.999110868207. Times trained:               12612. Times reached goal: 131.               Steps done: 3941598.\n",
      " state (0)  A[0]:(0.994915962219) A[1]:(0.00169232580811) A[2]:(0.001946935663) A[3]:(0.00144474813715)\n",
      " state (1)  A[0]:(0.0234059598297) A[1]:(0.00642210105434) A[2]:(0.00979654770344) A[3]:(0.960375368595)\n",
      " state (2)  A[0]:(0.999992132187) A[1]:(3.69798272004e-06) A[2]:(4.15918157159e-06) A[3]:(1.88873472418e-09)\n",
      " state (3)  A[0]:(0.999997794628) A[1]:(1.10989094537e-06) A[2]:(1.09365964818e-06) A[3]:(1.05694841768e-10)\n",
      " state (4)  A[0]:(0.999996840954) A[1]:(1.73644070856e-06) A[2]:(1.44025477766e-06) A[3]:(1.19174003999e-10)\n",
      " state (5)  A[0]:(0.999982774258) A[1]:(1.17121780931e-05) A[2]:(5.53513291379e-06) A[3]:(1.57352186836e-10)\n",
      " state (6)  A[0]:(0.97173500061) A[1]:(0.0238650199026) A[2]:(0.00439996877685) A[3]:(4.62168107604e-12)\n",
      " state (7)  A[0]:(0.0574280396104) A[1]:(0.909066557884) A[2]:(0.0335054323077) A[3]:(1.22848756467e-15)\n",
      " state (8)  A[0]:(0.00169419753365) A[1]:(0.994630038738) A[2]:(0.00367575371638) A[3]:(4.64369972823e-18)\n",
      " state (9)  A[0]:(8.88178692549e-05) A[1]:(0.999589562416) A[2]:(0.000321612140397) A[3]:(7.86158141037e-20)\n",
      " state (10)  A[0]:(8.61425087351e-06) A[1]:(0.999946117401) A[2]:(4.52797867183e-05) A[3]:(4.27221943888e-21)\n",
      " state (11)  A[0]:(1.93459914044e-06) A[1]:(0.999983370304) A[2]:(1.4702775843e-05) A[3]:(8.39594834577e-22)\n",
      " state (12)  A[0]:(8.50583319334e-07) A[1]:(0.999990582466) A[2]:(8.59122610564e-06) A[3]:(3.84825711369e-22)\n",
      " state (13)  A[0]:(5.55542953862e-07) A[1]:(0.999992728233) A[2]:(6.70318195262e-06) A[3]:(2.67221292633e-22)\n",
      " state (14)  A[0]:(4.45155393436e-07) A[1]:(0.999993562698) A[2]:(5.96285417487e-06) A[3]:(2.24371655682e-22)\n",
      " state (15)  A[0]:(3.95190170366e-07) A[1]:(0.999993979931) A[2]:(5.63490630157e-06) A[3]:(2.0579849996e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 343000 finished after 12 . Running score: 0.12. Policy_loss: -92050.611454, Value_loss: 1.01496346396. Times trained:               12854. Times reached goal: 129.               Steps done: 3954452.\n",
      " state (0)  A[0]:(0.99503827095) A[1]:(0.00167065975256) A[2]:(0.00189908547327) A[3]:(0.00139200931881)\n",
      " state (1)  A[0]:(0.0281273853034) A[1]:(0.00713583268225) A[2]:(0.0110744461417) A[3]:(0.953662335873)\n",
      " state (2)  A[0]:(0.999997198582) A[1]:(1.41332918702e-06) A[2]:(1.39370990837e-06) A[3]:(1.72363470718e-10)\n",
      " state (3)  A[0]:(0.999996900558) A[1]:(1.70239707131e-06) A[2]:(1.41510236062e-06) A[3]:(1.24002211277e-10)\n",
      " state (4)  A[0]:(0.999988555908) A[1]:(7.48578622733e-06) A[2]:(3.967153134e-06) A[3]:(1.81013884437e-10)\n",
      " state (5)  A[0]:(0.993487536907) A[1]:(0.00516532966867) A[2]:(0.00134716136381) A[3]:(1.89388608962e-11)\n",
      " state (6)  A[0]:(0.00685895327479) A[1]:(0.979930639267) A[2]:(0.013210398145) A[3]:(5.37894310345e-17)\n",
      " state (7)  A[0]:(3.80608835258e-05) A[1]:(0.999743878841) A[2]:(0.000218040731852) A[3]:(3.29932748791e-20)\n",
      " state (8)  A[0]:(1.74093474925e-06) A[1]:(0.999979972839) A[2]:(1.82739477168e-05) A[3]:(8.20775062003e-22)\n",
      " state (9)  A[0]:(3.593667941e-07) A[1]:(0.999993562698) A[2]:(6.0908728301e-06) A[3]:(1.67321222395e-22)\n",
      " state (10)  A[0]:(1.88404072787e-07) A[1]:(0.999995708466) A[2]:(4.08084315495e-06) A[3]:(9.35008242639e-23)\n",
      " state (11)  A[0]:(1.46036072124e-07) A[1]:(0.999996304512) A[2]:(3.52699930772e-06) A[3]:(7.54670032758e-23)\n",
      " state (12)  A[0]:(1.31316042484e-07) A[1]:(0.999996542931) A[2]:(3.33694379151e-06) A[3]:(6.94617081269e-23)\n",
      " state (13)  A[0]:(1.25172576304e-07) A[1]:(0.999996602535) A[2]:(3.26533449879e-06) A[3]:(6.71683758795e-23)\n",
      " state (14)  A[0]:(1.22253254631e-07) A[1]:(0.99999666214) A[2]:(3.23723656948e-06) A[3]:(6.62237181009e-23)\n",
      " state (15)  A[0]:(1.20699752415e-07) A[1]:(0.99999666214) A[2]:(3.22602159031e-06) A[3]:(6.58096986543e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 344000 finished after 22 . Running score: 0.19. Policy_loss: -92050.6111169, Value_loss: 0.992670696779. Times trained:               12818. Times reached goal: 125.               Steps done: 3967270.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9951,  0.0016,  0.0019,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9951,  0.0016,  0.0019,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  9.4919e-06,  4.9826e-06,  1.9187e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9951,  0.0016,  0.0019,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9951,  0.0016,  0.0019,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  9.4697e-06,  4.9803e-06,  1.9185e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8279e-04,  9.9909e-01,  7.2897e-04,  1.0695e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995124161243) A[1]:(0.00160149100702) A[2]:(0.00193455256522) A[3]:(0.00133977690712)\n",
      " state (1)  A[0]:(0.0309195313603) A[1]:(0.00735795497894) A[2]:(0.0118296695873) A[3]:(0.949892818928)\n",
      " state (2)  A[0]:(0.999996960163) A[1]:(1.49890558987e-06) A[2]:(1.56583791977e-06) A[3]:(2.09202391521e-10)\n",
      " state (3)  A[0]:(0.999996840954) A[1]:(1.68306416981e-06) A[2]:(1.44768125665e-06) A[3]:(1.21410742571e-10)\n",
      " state (4)  A[0]:(0.999985575676) A[1]:(9.43905160966e-06) A[2]:(4.97797418575e-06) A[3]:(1.91819726769e-10)\n",
      " state (5)  A[0]:(0.9873239398) A[1]:(0.00947703048587) A[2]:(0.00319903763011) A[3]:(7.82893021245e-12)\n",
      " state (6)  A[0]:(0.0333826690912) A[1]:(0.925708174706) A[2]:(0.040909178555) A[3]:(2.88859252019e-16)\n",
      " state (7)  A[0]:(0.00158294546418) A[1]:(0.993789732456) A[2]:(0.00462730647996) A[3]:(2.31112939658e-18)\n",
      " state (8)  A[0]:(0.000188397825696) A[1]:(0.999059259892) A[2]:(0.000752363004722) A[3]:(1.11329657032e-19)\n",
      " state (9)  A[0]:(2.26725205721e-05) A[1]:(0.999860703945) A[2]:(0.000116624512884) A[3]:(6.82083358809e-21)\n",
      " state (10)  A[0]:(3.66457038581e-06) A[1]:(0.999970912933) A[2]:(2.54462593148e-05) A[3]:(7.72517827723e-22)\n",
      " state (11)  A[0]:(1.1173456187e-06) A[1]:(0.999988436699) A[2]:(1.04536993604e-05) A[3]:(2.20008939332e-22)\n",
      " state (12)  A[0]:(5.92907099417e-07) A[1]:(0.999992549419) A[2]:(6.83907319399e-06) A[3]:(1.20652526787e-22)\n",
      " state (13)  A[0]:(4.34274937788e-07) A[1]:(0.999993920326) A[2]:(5.65041455047e-06) A[3]:(9.18777587287e-23)\n",
      " state (14)  A[0]:(3.7294890376e-07) A[1]:(0.999994456768) A[2]:(5.18412116435e-06) A[3]:(8.11391277545e-23)\n",
      " state (15)  A[0]:(3.45296001569e-07) A[1]:(0.999994695187) A[2]:(4.98402914673e-06) A[3]:(7.65793223957e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 345000 finished after 7 . Running score: 0.12. Policy_loss: -92050.6111282, Value_loss: 1.01361350655. Times trained:               12791. Times reached goal: 140.               Steps done: 3980061.\n",
      " state (0)  A[0]:(0.995082616806) A[1]:(0.00163676694501) A[2]:(0.00191815872677) A[3]:(0.00136247288901)\n",
      " state (1)  A[0]:(0.0271814856678) A[1]:(0.00700636906549) A[2]:(0.0110307205468) A[3]:(0.954781413078)\n",
      " state (2)  A[0]:(0.999995648861) A[1]:(2.11223323276e-06) A[2]:(2.23218012252e-06) A[3]:(4.2337602868e-10)\n",
      " state (3)  A[0]:(0.999996483326) A[1]:(1.91486265066e-06) A[2]:(1.61702348578e-06) A[3]:(1.42477030174e-10)\n",
      " state (4)  A[0]:(0.999976456165) A[1]:(1.60491035786e-05) A[2]:(7.47746889829e-06) A[3]:(3.01305591588e-10)\n",
      " state (5)  A[0]:(0.945192873478) A[1]:(0.0450830832124) A[2]:(0.00972404889762) A[3]:(9.67127853496e-12)\n",
      " state (6)  A[0]:(0.0206646602601) A[1]:(0.945715606213) A[2]:(0.0336197167635) A[3]:(2.27550001445e-16)\n",
      " state (7)  A[0]:(0.00160473713186) A[1]:(0.992597520351) A[2]:(0.0057977149263) A[3]:(3.207587833e-18)\n",
      " state (8)  A[0]:(0.000221679350943) A[1]:(0.998691856861) A[2]:(0.00108647649176) A[3]:(1.81500060479e-19)\n",
      " state (9)  A[0]:(2.69718912023e-05) A[1]:(0.999803960323) A[2]:(0.000169094084413) A[3]:(1.11324220581e-20)\n",
      " state (10)  A[0]:(4.00571479986e-06) A[1]:(0.999962091446) A[2]:(3.39094949595e-05) A[3]:(1.13722016441e-21)\n",
      " state (11)  A[0]:(1.103903287e-06) A[1]:(0.999986112118) A[2]:(1.27551957121e-05) A[3]:(2.91042746269e-22)\n",
      " state (12)  A[0]:(5.4454204701e-07) A[1]:(0.99999153614) A[2]:(7.89006389823e-06) A[3]:(1.48975031421e-22)\n",
      " state (13)  A[0]:(3.81471693345e-07) A[1]:(0.99999332428) A[2]:(6.31874672763e-06) A[3]:(1.09076680889e-22)\n",
      " state (14)  A[0]:(3.19276296068e-07) A[1]:(0.999993979931) A[2]:(5.69931762584e-06) A[3]:(9.4233278459e-23)\n",
      " state (15)  A[0]:(2.91385049422e-07) A[1]:(0.999994277954) A[2]:(5.42831776329e-06) A[3]:(8.78525233846e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 346000 finished after 14 . Running score: 0.15. Policy_loss: -92050.611129, Value_loss: 1.00938322927. Times trained:               12561. Times reached goal: 126.               Steps done: 3992622.\n",
      " state (0)  A[0]:(0.995150864124) A[1]:(0.00161913794) A[2]:(0.00186629127711) A[3]:(0.00136370770633)\n",
      " state (1)  A[0]:(0.0308194067329) A[1]:(0.00751660810784) A[2]:(0.0120282359421) A[3]:(0.949635744095)\n",
      " state (2)  A[0]:(0.999995052814) A[1]:(2.43189197136e-06) A[2]:(2.53253028859e-06) A[3]:(4.86520546072e-10)\n",
      " state (3)  A[0]:(0.999995589256) A[1]:(2.58504155681e-06) A[2]:(1.80907136382e-06) A[3]:(1.27404531369e-10)\n",
      " state (4)  A[0]:(0.999882519245) A[1]:(9.89749096334e-05) A[2]:(1.85139288078e-05) A[3]:(1.912935782e-10)\n",
      " state (5)  A[0]:(0.15365716815) A[1]:(0.824269711971) A[2]:(0.0220731310546) A[3]:(1.13018632064e-14)\n",
      " state (6)  A[0]:(0.000643145060167) A[1]:(0.997840285301) A[2]:(0.00151655927766) A[3]:(5.43999310652e-19)\n",
      " state (7)  A[0]:(4.39300310973e-05) A[1]:(0.999791026115) A[2]:(0.000165050951182) A[3]:(1.07918328643e-20)\n",
      " state (8)  A[0]:(5.65160144106e-06) A[1]:(0.999970257282) A[2]:(2.41036013904e-05) A[3]:(6.65722923847e-22)\n",
      " state (9)  A[0]:(1.68875635609e-06) A[1]:(0.999990522861) A[2]:(7.76442084316e-06) A[3]:(1.39880108894e-22)\n",
      " state (10)  A[0]:(9.64461378317e-07) A[1]:(0.999994218349) A[2]:(4.82470841234e-06) A[3]:(7.27856335045e-23)\n",
      " state (11)  A[0]:(7.48785282667e-07) A[1]:(0.999995231628) A[2]:(4.02341720473e-06) A[3]:(5.65068008386e-23)\n",
      " state (12)  A[0]:(6.63979619731e-07) A[1]:(0.999995589256) A[2]:(3.75136892217e-06) A[3]:(5.11186473531e-23)\n",
      " state (13)  A[0]:(6.23841685865e-07) A[1]:(0.999995708466) A[2]:(3.65004325431e-06) A[3]:(4.90640307264e-23)\n",
      " state (14)  A[0]:(6.0123483081e-07) A[1]:(0.99999576807) A[2]:(3.61161573892e-06) A[3]:(4.82195993036e-23)\n",
      " state (15)  A[0]:(5.86132841818e-07) A[1]:(0.999995827675) A[2]:(3.59790601578e-06) A[3]:(4.78532996309e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 347000 finished after 6 . Running score: 0.15. Policy_loss: -92050.6111117, Value_loss: 1.21323447643. Times trained:               13240. Times reached goal: 116.               Steps done: 4005862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995271742344) A[1]:(0.00160011474509) A[2]:(0.0017904615961) A[3]:(0.00133766129147)\n",
      " state (1)  A[0]:(0.0264281406999) A[1]:(0.00686686579138) A[2]:(0.0108646806329) A[3]:(0.955840289593)\n",
      " state (2)  A[0]:(0.999980986118) A[1]:(8.52182438393e-06) A[2]:(1.05026356323e-05) A[3]:(9.57446122385e-09)\n",
      " state (3)  A[0]:(0.999996602535) A[1]:(1.75133823177e-06) A[2]:(1.63110246376e-06) A[3]:(1.08847847402e-10)\n",
      " state (4)  A[0]:(0.999954640865) A[1]:(2.80400709016e-05) A[2]:(1.73099469976e-05) A[3]:(6.58854568125e-11)\n",
      " state (5)  A[0]:(0.5439453125) A[1]:(0.347446203232) A[2]:(0.108608424664) A[3]:(2.63833449716e-14)\n",
      " state (6)  A[0]:(0.0134136993438) A[1]:(0.955518722534) A[2]:(0.0310675874352) A[3]:(3.29284918513e-17)\n",
      " state (7)  A[0]:(0.00387468351983) A[1]:(0.982164680958) A[2]:(0.0139606650919) A[3]:(5.43004739703e-18)\n",
      " state (8)  A[0]:(0.00211538700387) A[1]:(0.98868983984) A[2]:(0.00919477827847) A[3]:(2.44870793722e-18)\n",
      " state (9)  A[0]:(0.00130379199982) A[1]:(0.992333114147) A[2]:(0.00636306544766) A[3]:(1.32532066815e-18)\n",
      " state (10)  A[0]:(0.000747761863749) A[1]:(0.995232343674) A[2]:(0.00401991698891) A[3]:(6.52185539185e-19)\n",
      " state (11)  A[0]:(0.000355927622877) A[1]:(0.997531950474) A[2]:(0.00211209710687) A[3]:(2.51262573412e-19)\n",
      " state (12)  A[0]:(0.000132516070153) A[1]:(0.998991191387) A[2]:(0.000876277976204) A[3]:(7.08857014195e-20)\n",
      " state (13)  A[0]:(3.9757185732e-05) A[1]:(0.999663472176) A[2]:(0.000296790181892) A[3]:(1.55839034103e-20)\n",
      " state (14)  A[0]:(1.11406798169e-05) A[1]:(0.999893128872) A[2]:(9.57325610216e-05) A[3]:(3.32558493897e-21)\n",
      " state (15)  A[0]:(3.56801047019e-06) A[1]:(0.999960362911) A[2]:(3.60695667041e-05) A[3]:(8.98111753976e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 348000 finished after 19 . Running score: 0.11. Policy_loss: -92050.6047357, Value_loss: 0.98884725403. Times trained:               12958. Times reached goal: 120.               Steps done: 4018820.\n",
      " state (0)  A[0]:(0.995099723339) A[1]:(0.00169899256434) A[2]:(0.00182534044143) A[3]:(0.00137597275898)\n",
      " state (1)  A[0]:(0.0240175966173) A[1]:(0.00684096012264) A[2]:(0.0100232940167) A[3]:(0.959118127823)\n",
      " state (2)  A[0]:(0.99755012989) A[1]:(0.000889691000339) A[2]:(0.00134577206336) A[3]:(0.000214432104258)\n",
      " state (3)  A[0]:(0.999996781349) A[1]:(1.86192346519e-06) A[2]:(1.36301275688e-06) A[3]:(1.07027560425e-10)\n",
      " state (4)  A[0]:(0.999985396862) A[1]:(1.05048857222e-05) A[2]:(4.07020979765e-06) A[3]:(7.80282713442e-11)\n",
      " state (5)  A[0]:(0.999599635601) A[1]:(0.000331681512762) A[2]:(6.86886924086e-05) A[3]:(1.71070813765e-11)\n",
      " state (6)  A[0]:(0.966768920422) A[1]:(0.0274587143213) A[2]:(0.00577235873789) A[3]:(2.98459336672e-13)\n",
      " state (7)  A[0]:(0.0713096112013) A[1]:(0.891115665436) A[2]:(0.0375746935606) A[3]:(2.82184314757e-16)\n",
      " state (8)  A[0]:(0.000234615450609) A[1]:(0.999205231667) A[2]:(0.000560144486371) A[3]:(9.03631682069e-20)\n",
      " state (9)  A[0]:(1.08895019366e-05) A[1]:(0.999954283237) A[2]:(3.48303219653e-05) A[3]:(1.71470764547e-21)\n",
      " state (10)  A[0]:(2.01877037398e-06) A[1]:(0.999990344048) A[2]:(7.62919307817e-06) A[3]:(2.32235298865e-22)\n",
      " state (11)  A[0]:(6.49146102205e-07) A[1]:(0.999996304512) A[2]:(3.02429771182e-06) A[3]:(7.03008668035e-23)\n",
      " state (12)  A[0]:(3.11351612936e-07) A[1]:(0.999997854233) A[2]:(1.82692815542e-06) A[3]:(3.6427447746e-23)\n",
      " state (13)  A[0]:(2.01910907549e-07) A[1]:(0.999998390675) A[2]:(1.42879810028e-06) A[3]:(2.62457289051e-23)\n",
      " state (14)  A[0]:(1.58821535479e-07) A[1]:(0.999998569489) A[2]:(1.27341513689e-06) A[3]:(2.24228916035e-23)\n",
      " state (15)  A[0]:(1.39193389259e-07) A[1]:(0.999998629093) A[2]:(1.20676452298e-06) A[3]:(2.07902997839e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 349000 finished after 18 . Running score: 0.1. Policy_loss: -92050.6045841, Value_loss: 1.22206754341. Times trained:               12605. Times reached goal: 131.               Steps done: 4031425.\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0016,  0.0017,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  8.0708e-06,  2.7768e-06,  7.9764e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0016,  0.0017,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  8.0708e-06,  2.7767e-06,  7.9765e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  8.0708e-06,  2.7767e-06,  7.9765e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0016,  0.0017,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  8.0709e-06,  2.7767e-06,  7.9766e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.9214e-05,  9.9985e-01,  1.0110e-04,  6.6798e-21]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0119e-06,  9.9999e-01,  1.0348e-05,  2.5551e-22]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0719e-06,  1.0000e+00,  3.2677e-06,  5.5180e-23]])\n",
      "On state=10, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0118e-06,  9.9999e-01,  1.0346e-05,  2.5551e-22]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.9210e-05,  9.9985e-01,  1.0105e-04,  6.6797e-21]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0116e-06,  9.9999e-01,  1.0343e-05,  2.5551e-22]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0719e-06,  1.0000e+00,  3.2659e-06,  5.5181e-23]])\n",
      "On state=10, selected action=1\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995313227177) A[1]:(0.00160855916329) A[2]:(0.00174049416091) A[3]:(0.00133770878892)\n",
      " state (1)  A[0]:(0.0286047197878) A[1]:(0.00723998341709) A[2]:(0.0109950853512) A[3]:(0.953160226345)\n",
      " state (2)  A[0]:(0.997217714787) A[1]:(0.000969738874119) A[2]:(0.00153726583812) A[3]:(0.000275306229014)\n",
      " state (3)  A[0]:(0.999997019768) A[1]:(1.74344006609e-06) A[2]:(1.2662512745e-06) A[3]:(1.00362433642e-10)\n",
      " state (4)  A[0]:(0.999989151955) A[1]:(8.07106061984e-06) A[2]:(2.77620597444e-06) A[3]:(7.97673524455e-11)\n",
      " state (5)  A[0]:(0.99975079298) A[1]:(0.00022403744515) A[2]:(2.51937599387e-05) A[3]:(3.61789140313e-11)\n",
      " state (6)  A[0]:(0.964636862278) A[1]:(0.0335429869592) A[2]:(0.00182014633901) A[3]:(1.09499410407e-12)\n",
      " state (7)  A[0]:(0.0146608641371) A[1]:(0.978400051594) A[2]:(0.00693908613175) A[3]:(2.97452857872e-17)\n",
      " state (8)  A[0]:(4.92054787173e-05) A[1]:(0.999849796295) A[2]:(0.000101006233308) A[3]:(6.67917042636e-21)\n",
      " state (9)  A[0]:(4.01142506234e-06) A[1]:(0.999985635281) A[2]:(1.03396196209e-05) A[3]:(2.55502479896e-22)\n",
      " state (10)  A[0]:(1.07184348508e-06) A[1]:(0.999995648861) A[2]:(3.26539702655e-06) A[3]:(5.51810096468e-23)\n",
      " state (11)  A[0]:(4.79578488921e-07) A[1]:(0.999997675419) A[2]:(1.81521545528e-06) A[3]:(2.53285077088e-23)\n",
      " state (12)  A[0]:(2.97842319696e-07) A[1]:(0.99999833107) A[2]:(1.38537609473e-06) A[3]:(1.75249552172e-23)\n",
      " state (13)  A[0]:(2.28114444667e-07) A[1]:(0.999998569489) A[2]:(1.2301187553e-06) A[3]:(1.48120925408e-23)\n",
      " state (14)  A[0]:(1.96839266664e-07) A[1]:(0.999998629093) A[2]:(1.16751550649e-06) A[3]:(1.37108648052e-23)\n",
      " state (15)  A[0]:(1.80748259027e-07) A[1]:(0.999998688698) A[2]:(1.14083672997e-06) A[3]:(1.3220313229e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 350000 finished after 14 . Running score: 0.15. Policy_loss: -92050.6046395, Value_loss: 1.00435238332. Times trained:               12504. Times reached goal: 132.               Steps done: 4043929.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995110273361) A[1]:(0.00163448194508) A[2]:(0.00177382084075) A[3]:(0.00148143561091)\n",
      " state (1)  A[0]:(0.0229144599289) A[1]:(0.00655903667212) A[2]:(0.00948846153915) A[3]:(0.961038053036)\n",
      " state (2)  A[0]:(0.999964892864) A[1]:(1.73708340299e-05) A[2]:(1.77198289748e-05) A[3]:(2.81112164657e-08)\n",
      " state (3)  A[0]:(0.999994039536) A[1]:(3.97379380956e-06) A[2]:(1.97830809157e-06) A[3]:(1.06066842809e-10)\n",
      " state (4)  A[0]:(0.999816596508) A[1]:(0.000161883086548) A[2]:(2.15289255721e-05) A[3]:(6.99505592339e-11)\n",
      " state (5)  A[0]:(0.964586675167) A[1]:(0.0333327427506) A[2]:(0.00208058906719) A[3]:(2.81060049366e-12)\n",
      " state (6)  A[0]:(0.0307346563786) A[1]:(0.952595710754) A[2]:(0.0166696421802) A[3]:(2.98422942207e-16)\n",
      " state (7)  A[0]:(2.87407128781e-05) A[1]:(0.999863982201) A[2]:(0.000107293672045) A[3]:(1.38416720166e-20)\n",
      " state (8)  A[0]:(1.05202639133e-06) A[1]:(0.999993383884) A[2]:(5.54220105187e-06) A[3]:(2.44041328063e-22)\n",
      " state (9)  A[0]:(2.5856130037e-07) A[1]:(0.999997794628) A[2]:(1.94045651369e-06) A[3]:(6.28114592346e-23)\n",
      " state (10)  A[0]:(1.2239921432e-07) A[1]:(0.999998569489) A[2]:(1.29213333366e-06) A[3]:(3.64743313273e-23)\n",
      " state (11)  A[0]:(8.455648981e-08) A[1]:(0.999998807907) A[2]:(1.10562621103e-06) A[3]:(2.93096346843e-23)\n",
      " state (12)  A[0]:(7.1292447501e-08) A[1]:(0.999998867512) A[2]:(1.04121738786e-06) A[3]:(2.68417085858e-23)\n",
      " state (13)  A[0]:(6.58173817669e-08) A[1]:(0.999998927116) A[2]:(1.01700936739e-06) A[3]:(2.58861960812e-23)\n",
      " state (14)  A[0]:(6.323875823e-08) A[1]:(0.999998927116) A[2]:(1.00755630683e-06) A[3]:(2.54875909767e-23)\n",
      " state (15)  A[0]:(6.18724200763e-08) A[1]:(0.999998927116) A[2]:(1.00380646018e-06) A[3]:(2.53093178785e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 351000 finished after 23 . Running score: 0.18. Policy_loss: -92050.6046259, Value_loss: 1.44018312375. Times trained:               12915. Times reached goal: 126.               Steps done: 4056844.\n",
      " state (0)  A[0]:(0.995040595531) A[1]:(0.00166052544955) A[2]:(0.00177339219954) A[3]:(0.00152549659833)\n",
      " state (1)  A[0]:(0.0210650470108) A[1]:(0.00632336037233) A[2]:(0.00885461084545) A[3]:(0.963756978512)\n",
      " state (2)  A[0]:(0.999985694885) A[1]:(7.84776057117e-06) A[2]:(6.4749115154e-06) A[3]:(3.92124643867e-09)\n",
      " state (3)  A[0]:(0.999987602234) A[1]:(9.31452359509e-06) A[2]:(3.0645815059e-06) A[3]:(1.36561539854e-10)\n",
      " state (4)  A[0]:(0.999083995819) A[1]:(0.000850281270687) A[2]:(6.57526325085e-05) A[3]:(8.25798457393e-11)\n",
      " state (5)  A[0]:(0.937220692635) A[1]:(0.0597933866084) A[2]:(0.00298593356274) A[3]:(4.83183068209e-12)\n",
      " state (6)  A[0]:(0.168821588159) A[1]:(0.797220647335) A[2]:(0.0339577496052) A[3]:(1.09476136991e-14)\n",
      " state (7)  A[0]:(0.000751102867071) A[1]:(0.99754858017) A[2]:(0.00170031003654) A[3]:(1.87738462789e-18)\n",
      " state (8)  A[0]:(5.55989799977e-06) A[1]:(0.999974250793) A[2]:(2.01935108635e-05) A[3]:(2.95824586311e-21)\n",
      " state (9)  A[0]:(6.36232471152e-07) A[1]:(0.999996423721) A[2]:(2.92774961963e-06) A[3]:(2.54527548791e-22)\n",
      " state (10)  A[0]:(2.23759670348e-07) A[1]:(0.999998390675) A[2]:(1.39108351505e-06) A[3]:(9.99502859677e-23)\n",
      " state (11)  A[0]:(1.27637179048e-07) A[1]:(0.999998867512) A[2]:(1.0189655768e-06) A[3]:(6.67575244983e-23)\n",
      " state (12)  A[0]:(9.66347357689e-08) A[1]:(0.999998986721) A[2]:(8.95151856639e-07) A[3]:(5.61104960525e-23)\n",
      " state (13)  A[0]:(8.49222772104e-08) A[1]:(0.999999046326) A[2]:(8.48188790314e-07) A[3]:(5.20992203909e-23)\n",
      " state (14)  A[0]:(8.00745212359e-08) A[1]:(0.99999910593) A[2]:(8.29236967093e-07) A[3]:(5.04687616513e-23)\n",
      " state (15)  A[0]:(7.7959391831e-08) A[1]:(0.99999910593) A[2]:(8.21351477498e-07) A[3]:(4.97788964788e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 352000 finished after 12 . Running score: 0.11. Policy_loss: -92050.6044104, Value_loss: 1.42966840315. Times trained:               12916. Times reached goal: 137.               Steps done: 4069760.\n",
      " state (0)  A[0]:(0.995071470737) A[1]:(0.00163654109929) A[2]:(0.00175471184775) A[3]:(0.00153729564045)\n",
      " state (1)  A[0]:(0.0199298672378) A[1]:(0.00599547475576) A[2]:(0.00842543412) A[3]:(0.965649247169)\n",
      " state (2)  A[0]:(0.900757491589) A[1]:(0.0115825170651) A[2]:(0.019555836916) A[3]:(0.068104185164)\n",
      " state (3)  A[0]:(0.999996542931) A[1]:(2.05017136068e-06) A[2]:(1.40663325965e-06) A[3]:(1.56427884535e-10)\n",
      " state (4)  A[0]:(0.999988853931) A[1]:(8.61215357872e-06) A[2]:(2.51423739428e-06) A[3]:(1.43769884886e-10)\n",
      " state (5)  A[0]:(0.999708354473) A[1]:(0.000272962905001) A[2]:(1.86901124835e-05) A[3]:(1.65653352147e-10)\n",
      " state (6)  A[0]:(0.981133639812) A[1]:(0.0183607470244) A[2]:(0.000505629577674) A[3]:(3.20460012693e-11)\n",
      " state (7)  A[0]:(0.257414549589) A[1]:(0.726050674915) A[2]:(0.0165347903967) A[3]:(5.48066976459e-14)\n",
      " state (8)  A[0]:(0.000952864065766) A[1]:(0.997706830502) A[2]:(0.00134029681794) A[3]:(2.10086008487e-18)\n",
      " state (9)  A[0]:(1.00650468084e-05) A[1]:(0.999968707561) A[2]:(2.12190461752e-05) A[3]:(3.6906173603e-21)\n",
      " state (10)  A[0]:(1.564597369e-06) A[1]:(0.99999499321) A[2]:(3.41836312145e-06) A[3]:(3.56108170436e-22)\n",
      " state (11)  A[0]:(7.5057283766e-07) A[1]:(0.999997437) A[2]:(1.80619917955e-06) A[3]:(1.60650589337e-22)\n",
      " state (12)  A[0]:(5.22448260654e-07) A[1]:(0.999998033047) A[2]:(1.41583836921e-06) A[3]:(1.17536349506e-22)\n",
      " state (13)  A[0]:(4.27748716447e-07) A[1]:(0.999998271465) A[2]:(1.28239662445e-06) A[3]:(1.02729619642e-22)\n",
      " state (14)  A[0]:(3.80271330869e-07) A[1]:(0.999998390675) A[2]:(1.23011625419e-06) A[3]:(9.65648420613e-23)\n",
      " state (15)  A[0]:(3.52994277364e-07) A[1]:(0.999998450279) A[2]:(1.20847460039e-06) A[3]:(9.36788733257e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 353000 finished after 18 . Running score: 0.18. Policy_loss: -92050.6044934, Value_loss: 1.43368027628. Times trained:               12372. Times reached goal: 123.               Steps done: 4082132.\n",
      " state (0)  A[0]:(0.995044708252) A[1]:(0.0016760478029) A[2]:(0.00175312510692) A[3]:(0.00152610102668)\n",
      " state (1)  A[0]:(0.0206745360047) A[1]:(0.00620722351596) A[2]:(0.00832813326269) A[3]:(0.96479010582)\n",
      " state (2)  A[0]:(0.996395587921) A[1]:(0.0013985313708) A[2]:(0.00173990032636) A[3]:(0.000465970369987)\n",
      " state (3)  A[0]:(0.999995350838) A[1]:(3.40950509781e-06) A[2]:(1.22963274407e-06) A[3]:(1.11368185507e-10)\n",
      " state (4)  A[0]:(0.999922156334) A[1]:(7.40350005799e-05) A[2]:(3.79696098207e-06) A[3]:(1.3251853781e-10)\n",
      " state (5)  A[0]:(0.99544519186) A[1]:(0.00452880840749) A[2]:(2.60227971012e-05) A[3]:(9.02632205091e-11)\n",
      " state (6)  A[0]:(0.806443214417) A[1]:(0.193279355764) A[2]:(0.000277394399745) A[3]:(9.14445949518e-12)\n",
      " state (7)  A[0]:(0.0150025319308) A[1]:(0.984517097473) A[2]:(0.000480391725432) A[3]:(3.3808197771e-16)\n",
      " state (8)  A[0]:(4.15314571001e-05) A[1]:(0.99994134903) A[2]:(1.71492847585e-05) A[3]:(6.80906080465e-21)\n",
      " state (9)  A[0]:(3.24373399962e-06) A[1]:(0.999995172024) A[2]:(1.60840397712e-06) A[3]:(2.01590476078e-22)\n",
      " state (10)  A[0]:(1.48843537318e-06) A[1]:(0.999997735023) A[2]:(7.49771800201e-07) A[3]:(7.70093020762e-23)\n",
      " state (11)  A[0]:(1.10644748474e-06) A[1]:(0.99999833107) A[2]:(5.74992043312e-07) A[3]:(5.55001385929e-23)\n",
      " state (12)  A[0]:(9.66444986261e-07) A[1]:(0.999998509884) A[2]:(5.19480920502e-07) A[3]:(4.88289060005e-23)\n",
      " state (13)  A[0]:(9.02517285795e-07) A[1]:(0.999998569489) A[2]:(4.98802080529e-07) A[3]:(4.62687431296e-23)\n",
      " state (14)  A[0]:(8.67907715474e-07) A[1]:(0.999998629093) A[2]:(4.90566662847e-07) A[3]:(4.5154363505e-23)\n",
      " state (15)  A[0]:(8.45255669901e-07) A[1]:(0.999998688698) A[2]:(4.87191698539e-07) A[3]:(4.46012426372e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 354000 finished after 8 . Running score: 0.22. Policy_loss: -92050.6116326, Value_loss: 1.2074759161. Times trained:               13480. Times reached goal: 122.               Steps done: 4095612.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  1.1728e-05,  2.2939e-06,  1.4664e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  1.1740e-05,  2.2952e-06,  1.4666e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  1.1779e-05,  2.2992e-06,  1.4671e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  1.1808e-05,  2.3024e-06,  1.4676e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  1.1813e-05,  2.3029e-06,  1.4676e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  1.1817e-05,  2.3033e-06,  1.4677e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  1.1820e-05,  2.3037e-06,  1.4678e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0016,  0.0016,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  1.1829e-05,  2.3046e-06,  1.4679e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  1.1831e-05,  2.3048e-06,  1.4679e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.8960e-01,  1.0328e-02,  6.9348e-05,  1.0474e-10]])\n",
      "On state=8, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.8960e-01,  1.0331e-02,  6.9365e-05,  1.0472e-10]])\n",
      "On state=8, selected action=0\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995192289352) A[1]:(0.00157995603513) A[2]:(0.00164337921888) A[3]:(0.0015843694564)\n",
      " state (1)  A[0]:(0.0201519075781) A[1]:(0.00558066321537) A[2]:(0.00788585282862) A[3]:(0.966381549835)\n",
      " state (2)  A[0]:(0.93547141552) A[1]:(0.00848714821041) A[2]:(0.0148750888184) A[3]:(0.0411663353443)\n",
      " state (3)  A[0]:(0.999996542931) A[1]:(2.1031214601e-06) A[2]:(1.3415686908e-06) A[3]:(1.51207324439e-10)\n",
      " state (4)  A[0]:(0.999985814095) A[1]:(1.18542302516e-05) A[2]:(2.30729688155e-06) A[3]:(1.4685187788e-10)\n",
      " state (5)  A[0]:(0.999823689461) A[1]:(0.00016906435485) A[2]:(7.23516541257e-06) A[3]:(1.98945401819e-10)\n",
      " state (6)  A[0]:(0.998933553696) A[1]:(0.00104903650936) A[2]:(1.74089436769e-05) A[3]:(1.86509113709e-10)\n",
      " state (7)  A[0]:(0.996652305126) A[1]:(0.00331467064098) A[2]:(3.30366092385e-05) A[3]:(1.5306259038e-10)\n",
      " state (8)  A[0]:(0.989567041397) A[1]:(0.0103634586558) A[2]:(6.95270573488e-05) A[3]:(1.0461311678e-10)\n",
      " state (9)  A[0]:(0.938737034798) A[1]:(0.061002727598) A[2]:(0.00026022491511) A[3]:(3.26394640482e-11)\n",
      " state (10)  A[0]:(0.386874884367) A[1]:(0.611409664154) A[2]:(0.00171547825448) A[3]:(4.61157913072e-13)\n",
      " state (11)  A[0]:(0.00999824795872) A[1]:(0.988750636578) A[2]:(0.00125113804825) A[3]:(6.61139192357e-17)\n",
      " state (12)  A[0]:(0.000435657304479) A[1]:(0.999341666698) A[2]:(0.00022270239424) A[3]:(2.00786952608e-19)\n",
      " state (13)  A[0]:(7.58962487453e-05) A[1]:(0.999870121479) A[2]:(5.39917418791e-05) A[3]:(1.50920653724e-20)\n",
      " state (14)  A[0]:(2.80375443253e-05) A[1]:(0.999950349331) A[2]:(2.16380030906e-05) A[3]:(4.06870513931e-21)\n",
      " state (15)  A[0]:(1.46282436617e-05) A[1]:(0.999973833561) A[2]:(1.15384154924e-05) A[3]:(1.81399011966e-21)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 355000 finished after 26 . Running score: 0.0. Policy_loss: -92050.6113634, Value_loss: 0.978290914133. Times trained:               12951. Times reached goal: 118.               Steps done: 4108563.\n",
      " state (0)  A[0]:(0.99510884285) A[1]:(0.00161597819533) A[2]:(0.00165491155349) A[3]:(0.00162026053295)\n",
      " state (1)  A[0]:(0.0191600248218) A[1]:(0.00573568930849) A[2]:(0.00852569472045) A[3]:(0.966578602791)\n",
      " state (2)  A[0]:(0.999787628651) A[1]:(9.07221110538e-05) A[2]:(0.000120714074001) A[3]:(9.35504431254e-07)\n",
      " state (3)  A[0]:(0.99998909235) A[1]:(7.43305508877e-06) A[2]:(3.4693505313e-06) A[3]:(1.82202086751e-10)\n",
      " state (4)  A[0]:(0.999315440655) A[1]:(0.00064295437187) A[2]:(4.15758986492e-05) A[3]:(3.31282168364e-10)\n",
      " state (5)  A[0]:(0.937544941902) A[1]:(0.0611419640481) A[2]:(0.00131311593577) A[3]:(5.33837765659e-11)\n",
      " state (6)  A[0]:(0.311956375837) A[1]:(0.668580114841) A[2]:(0.0194634795189) A[3]:(2.74075494288e-13)\n",
      " state (7)  A[0]:(0.017554236576) A[1]:(0.957538127899) A[2]:(0.0249076168984) A[3]:(2.90758749873e-16)\n",
      " state (8)  A[0]:(0.000438121060142) A[1]:(0.997213184834) A[2]:(0.00234870426357) A[3]:(8.94406408116e-19)\n",
      " state (9)  A[0]:(1.02336780401e-05) A[1]:(0.999917685986) A[2]:(7.20834723325e-05) A[3]:(7.17387572756e-21)\n",
      " state (10)  A[0]:(1.42393332681e-06) A[1]:(0.999986708164) A[2]:(1.1886696484e-05) A[3]:(7.9666363572e-22)\n",
      " state (11)  A[0]:(6.08237257893e-07) A[1]:(0.999992787838) A[2]:(6.59485476717e-06) A[3]:(3.87302204497e-22)\n",
      " state (12)  A[0]:(3.91069249872e-07) A[1]:(0.999994277954) A[2]:(5.34769242222e-06) A[3]:(2.94128946835e-22)\n",
      " state (13)  A[0]:(3.07939075128e-07) A[1]:(0.999994754791) A[2]:(4.91804939884e-06) A[3]:(2.60993810077e-22)\n",
      " state (14)  A[0]:(2.70746483011e-07) A[1]:(0.99999499321) A[2]:(4.74795524497e-06) A[3]:(2.47088956487e-22)\n",
      " state (15)  A[0]:(2.52114745081e-07) A[1]:(0.999995052814) A[2]:(4.67709651275e-06) A[3]:(2.40738032976e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 356000 finished after 26 . Running score: 0.1. Policy_loss: -92050.6259991, Value_loss: 1.19911252022. Times trained:               14100. Times reached goal: 88.               Steps done: 4122663.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995282888412) A[1]:(0.00158345245291) A[2]:(0.00157887139358) A[3]:(0.00155479798559)\n",
      " state (1)  A[0]:(0.0211135335267) A[1]:(0.00585396494716) A[2]:(0.00879173632711) A[3]:(0.964240789413)\n",
      " state (2)  A[0]:(0.959476590157) A[1]:(0.00675664097071) A[2]:(0.0130196744576) A[3]:(0.0207470692694)\n",
      " state (3)  A[0]:(0.999995410442) A[1]:(2.51359210779e-06) A[2]:(2.09623294722e-06) A[3]:(1.72329303605e-10)\n",
      " state (4)  A[0]:(0.999983072281) A[1]:(1.3547683011e-05) A[2]:(3.38912764164e-06) A[3]:(1.3786265185e-10)\n",
      " state (5)  A[0]:(0.999279558659) A[1]:(0.000700768083334) A[2]:(1.965096817e-05) A[3]:(2.03628530704e-10)\n",
      " state (6)  A[0]:(0.956509828568) A[1]:(0.043291117996) A[2]:(0.000199081478058) A[3]:(7.78247327693e-11)\n",
      " state (7)  A[0]:(0.278540581465) A[1]:(0.719876110554) A[2]:(0.00158333475702) A[3]:(6.61446266682e-13)\n",
      " state (8)  A[0]:(0.00361179420725) A[1]:(0.995185852051) A[2]:(0.00120238179807) A[3]:(2.33607319339e-17)\n",
      " state (9)  A[0]:(4.63341493742e-05) A[1]:(0.999864757061) A[2]:(8.88870708877e-05) A[3]:(1.35476303115e-20)\n",
      " state (10)  A[0]:(5.03932506035e-06) A[1]:(0.999982774258) A[2]:(1.21687198771e-05) A[3]:(7.59123954051e-22)\n",
      " state (11)  A[0]:(2.41774273491e-06) A[1]:(0.99999153614) A[2]:(6.07094580118e-06) A[3]:(3.26418323729e-22)\n",
      " state (12)  A[0]:(1.84878649634e-06) A[1]:(0.999993383884) A[2]:(4.77251660413e-06) A[3]:(2.46647825468e-22)\n",
      " state (13)  A[0]:(1.6429335119e-06) A[1]:(0.999994039536) A[2]:(4.34114963355e-06) A[3]:(2.20679256531e-22)\n",
      " state (14)  A[0]:(1.55141265168e-06) A[1]:(0.999994277954) A[2]:(4.17037517764e-06) A[3]:(2.10117103208e-22)\n",
      " state (15)  A[0]:(1.50588800807e-06) A[1]:(0.999994397163) A[2]:(4.09714630223e-06) A[3]:(2.05241942814e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 357000 finished after 10 . Running score: 0.11. Policy_loss: -92050.6125742, Value_loss: 1.00782080803. Times trained:               12959. Times reached goal: 126.               Steps done: 4135622.\n",
      " state (0)  A[0]:(0.995582044125) A[1]:(0.00150217325427) A[2]:(0.00147120351903) A[3]:(0.00144458247814)\n",
      " state (1)  A[0]:(0.0237012840807) A[1]:(0.00620681466535) A[2]:(0.00930230971426) A[3]:(0.960789561272)\n",
      " state (2)  A[0]:(0.998212993145) A[1]:(0.000708951265551) A[2]:(0.000987832667306) A[3]:(9.02408937691e-05)\n",
      " state (3)  A[0]:(0.999990880489) A[1]:(6.70640520184e-06) A[2]:(2.42984674514e-06) A[3]:(1.4546654159e-10)\n",
      " state (4)  A[0]:(0.999476611614) A[1]:(0.000508907018229) A[2]:(1.44635496326e-05) A[3]:(2.41618169916e-10)\n",
      " state (5)  A[0]:(0.937306046486) A[1]:(0.0625124275684) A[2]:(0.000181554438313) A[3]:(8.3636368664e-11)\n",
      " state (6)  A[0]:(0.257486820221) A[1]:(0.741512179375) A[2]:(0.00100100610871) A[3]:(8.53297521617e-13)\n",
      " state (7)  A[0]:(0.010912806727) A[1]:(0.987889528275) A[2]:(0.00119765300769) A[3]:(3.14650664109e-16)\n",
      " state (8)  A[0]:(0.000336439377861) A[1]:(0.999345123768) A[2]:(0.000318428210448) A[3]:(3.2637832785e-19)\n",
      " state (9)  A[0]:(1.37642891787e-05) A[1]:(0.999962568283) A[2]:(2.36742107518e-05) A[3]:(3.38107773169e-21)\n",
      " state (10)  A[0]:(2.64034360953e-06) A[1]:(0.999992489815) A[2]:(4.85481632495e-06) A[3]:(4.62176817409e-22)\n",
      " state (11)  A[0]:(1.48708443248e-06) A[1]:(0.999995648861) A[2]:(2.85174724013e-06) A[3]:(2.51140142199e-22)\n",
      " state (12)  A[0]:(1.17612796657e-06) A[1]:(0.999996423721) A[2]:(2.37729227592e-06) A[3]:(2.03820794116e-22)\n",
      " state (13)  A[0]:(1.0455106576e-06) A[1]:(0.999996721745) A[2]:(2.21451796278e-06) A[3]:(1.87062927243e-22)\n",
      " state (14)  A[0]:(9.80233267001e-07) A[1]:(0.999996840954) A[2]:(2.14978945223e-06) A[3]:(1.79840419717e-22)\n",
      " state (15)  A[0]:(9.44165776673e-07) A[1]:(0.999996960163) A[2]:(2.1223886506e-06) A[3]:(1.7633511313e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 358000 finished after 17 . Running score: 0.16. Policy_loss: -92050.6122699, Value_loss: 1.62119137325. Times trained:               13984. Times reached goal: 117.               Steps done: 4149606.\n",
      " state (0)  A[0]:(0.995710492134) A[1]:(0.00145328277722) A[2]:(0.00145228451584) A[3]:(0.00138396024704)\n",
      " state (1)  A[0]:(0.0285367704928) A[1]:(0.00676682312042) A[2]:(0.0103872250766) A[3]:(0.954309165478)\n",
      " state (2)  A[0]:(0.999934852123) A[1]:(3.12559786835e-05) A[2]:(3.3836276998e-05) A[3]:(7.77396778062e-08)\n",
      " state (3)  A[0]:(0.999993264675) A[1]:(4.84150859847e-06) A[2]:(1.89622244307e-06) A[3]:(9.26045212735e-11)\n",
      " state (4)  A[0]:(0.999696969986) A[1]:(0.000293947174214) A[2]:(9.08272431843e-06) A[3]:(1.01897164317e-10)\n",
      " state (5)  A[0]:(0.834973573685) A[1]:(0.164775744081) A[2]:(0.000250675220741) A[3]:(1.687751848e-11)\n",
      " state (6)  A[0]:(0.0180944185704) A[1]:(0.981355130672) A[2]:(0.000550428347196) A[3]:(1.40486851753e-15)\n",
      " state (7)  A[0]:(0.000260099564912) A[1]:(0.999574780464) A[2]:(0.000165111589013) A[3]:(2.37302659804e-19)\n",
      " state (8)  A[0]:(8.13729002402e-06) A[1]:(0.99997895956) A[2]:(1.28888286781e-05) A[3]:(1.38476931079e-21)\n",
      " state (9)  A[0]:(1.29703778384e-06) A[1]:(0.999996244907) A[2]:(2.46850095209e-06) A[3]:(1.59316316313e-22)\n",
      " state (10)  A[0]:(6.70745862408e-07) A[1]:(0.999997794628) A[2]:(1.55748421093e-06) A[3]:(9.09304125121e-23)\n",
      " state (11)  A[0]:(4.8634558425e-07) A[1]:(0.999998152256) A[2]:(1.38385871651e-06) A[3]:(7.71357596347e-23)\n",
      " state (12)  A[0]:(4.04778148777e-07) A[1]:(0.999998271465) A[2]:(1.33624143928e-06) A[3]:(7.24051501127e-23)\n",
      " state (13)  A[0]:(3.6328680153e-07) A[1]:(0.99999833107) A[2]:(1.321589707e-06) A[3]:(7.0356806508e-23)\n",
      " state (14)  A[0]:(3.3979350178e-07) A[1]:(0.99999833107) A[2]:(1.31732281261e-06) A[3]:(6.93297411638e-23)\n",
      " state (15)  A[0]:(3.25153507674e-07) A[1]:(0.99999833107) A[2]:(1.31652404889e-06) A[3]:(6.87466593589e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 359000 finished after 6 . Running score: 0.14. Policy_loss: -92050.6117251, Value_loss: 1.00263294259. Times trained:               12858. Times reached goal: 125.               Steps done: 4162464.\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9983e-01,  1.6686e-04,  6.7391e-06,  6.9202e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9983e-01,  1.6685e-04,  6.7390e-06,  6.9202e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0012,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9983e-01,  1.6681e-04,  6.7384e-06,  6.9206e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9983e-01,  1.6681e-04,  6.7384e-06,  6.9207e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.1977e-04,  9.9980e-01,  7.9234e-05,  1.9707e-20]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.5422e-06,  9.9998e-01,  1.1513e-05,  4.9873e-22]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.1977e-04,  9.9980e-01,  7.9231e-05,  1.9707e-20]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.5413e-06,  9.9998e-01,  1.1512e-05,  4.9870e-22]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.0715e-07,  1.0000e+00,  1.7397e-06,  4.2368e-23]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.0712e-07,  1.0000e+00,  1.7397e-06,  4.2368e-23]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996315538883) A[1]:(0.00124733604025) A[2]:(0.00134729361162) A[3]:(0.00108985754196)\n",
      " state (1)  A[0]:(0.0390456542373) A[1]:(0.00743902428076) A[2]:(0.0126859191805) A[3]:(0.940829396248)\n",
      " state (2)  A[0]:(0.999925792217) A[1]:(3.40849946951e-05) A[2]:(4.00327990064e-05) A[3]:(1.04746590068e-07)\n",
      " state (3)  A[0]:(0.999994456768) A[1]:(3.82522239306e-06) A[2]:(1.73949945292e-06) A[3]:(7.63580726426e-11)\n",
      " state (4)  A[0]:(0.999826490879) A[1]:(0.000166756712133) A[2]:(6.73735894452e-06) A[3]:(6.92077992137e-11)\n",
      " state (5)  A[0]:(0.889313280582) A[1]:(0.110512338579) A[2]:(0.000174405577127) A[3]:(1.07303393601e-11)\n",
      " state (6)  A[0]:(0.0378713235259) A[1]:(0.961651563644) A[2]:(0.000477131485241) A[3]:(2.52689642858e-15)\n",
      " state (7)  A[0]:(0.00159279070795) A[1]:(0.998138964176) A[2]:(0.000268251576927) A[3]:(2.30504300164e-18)\n",
      " state (8)  A[0]:(0.000119712560263) A[1]:(0.9998010993) A[2]:(7.92040591477e-05) A[3]:(1.96957002975e-20)\n",
      " state (9)  A[0]:(9.53771086643e-06) A[1]:(0.99997895956) A[2]:(1.15078582894e-05) A[3]:(4.98508847695e-22)\n",
      " state (10)  A[0]:(2.21952200263e-06) A[1]:(0.999994575977) A[2]:(3.19573382512e-06) A[3]:(8.91077841005e-23)\n",
      " state (11)  A[0]:(1.2629535604e-06) A[1]:(0.99999666214) A[2]:(2.07900006899e-06) A[3]:(5.34515046873e-23)\n",
      " state (12)  A[0]:(9.59327167038e-07) A[1]:(0.999997198582) A[2]:(1.82085466349e-06) A[3]:(4.53450406521e-23)\n",
      " state (13)  A[0]:(8.07106175671e-07) A[1]:(0.999997437) A[2]:(1.73962087047e-06) A[3]:(4.23677723107e-23)\n",
      " state (14)  A[0]:(7.18349610906e-07) A[1]:(0.99999755621) A[2]:(1.71079466327e-06) A[3]:(4.09720943536e-23)\n",
      " state (15)  A[0]:(6.63442961013e-07) A[1]:(0.999997615814) A[2]:(1.70045927916e-06) A[3]:(4.02181168776e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 360000 finished after 22 . Running score: 0.14. Policy_loss: -92050.6117506, Value_loss: 1.01659779676. Times trained:               12889. Times reached goal: 134.               Steps done: 4175353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996985435486) A[1]:(0.0010050367564) A[2]:(0.00122083723545) A[3]:(0.000788691570051)\n",
      " state (1)  A[0]:(0.0576578825712) A[1]:(0.0082416664809) A[2]:(0.0166297256947) A[3]:(0.917470693588)\n",
      " state (2)  A[0]:(0.999960422516) A[1]:(1.68816713995e-05) A[2]:(2.26986012422e-05) A[3]:(2.6393021102e-08)\n",
      " state (3)  A[0]:(0.999995291233) A[1]:(2.88828005068e-06) A[2]:(1.82241478797e-06) A[3]:(6.94057589179e-11)\n",
      " state (4)  A[0]:(0.999902963638) A[1]:(8.98604266695e-05) A[2]:(7.15613441571e-06) A[3]:(6.28003829495e-11)\n",
      " state (5)  A[0]:(0.95487087965) A[1]:(0.044893398881) A[2]:(0.000235737854382) A[3]:(7.57194133783e-12)\n",
      " state (6)  A[0]:(0.116018615663) A[1]:(0.882033705711) A[2]:(0.00194767583162) A[3]:(4.23867060564e-15)\n",
      " state (7)  A[0]:(0.00539118191227) A[1]:(0.993501245975) A[2]:(0.00110758468509) A[3]:(4.88562730994e-18)\n",
      " state (8)  A[0]:(0.000299479201203) A[1]:(0.999458193779) A[2]:(0.000242341615376) A[3]:(2.7000645378e-20)\n",
      " state (9)  A[0]:(1.87035675481e-05) A[1]:(0.999954879284) A[2]:(2.64199370577e-05) A[3]:(4.82423052448e-22)\n",
      " state (10)  A[0]:(4.41930478701e-06) A[1]:(0.999988257885) A[2]:(7.33563501853e-06) A[3]:(8.64714993077e-23)\n",
      " state (11)  A[0]:(2.44108832703e-06) A[1]:(0.999992907047) A[2]:(4.65384300696e-06) A[3]:(5.01171663458e-23)\n",
      " state (12)  A[0]:(1.74679053089e-06) A[1]:(0.999994337559) A[2]:(3.92534184357e-06) A[3]:(4.06678527906e-23)\n",
      " state (13)  A[0]:(1.38080758916e-06) A[1]:(0.999994933605) A[2]:(3.66141989616e-06) A[3]:(3.68653128801e-23)\n",
      " state (14)  A[0]:(1.16337480449e-06) A[1]:(0.999995291233) A[2]:(3.55654356099e-06) A[3]:(3.50096690618e-23)\n",
      " state (15)  A[0]:(1.02733497442e-06) A[1]:(0.999995470047) A[2]:(3.51450921698e-06) A[3]:(3.39999176368e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 361000 finished after 9 . Running score: 0.18. Policy_loss: -92050.6117126, Value_loss: 1.83128655395. Times trained:               12635. Times reached goal: 123.               Steps done: 4187988.\n",
      " state (0)  A[0]:(0.996242582798) A[1]:(0.0013186361175) A[2]:(0.0013661577832) A[3]:(0.00107261014637)\n",
      " state (1)  A[0]:(0.0439265705645) A[1]:(0.00759853050113) A[2]:(0.0139220990241) A[3]:(0.934552788734)\n",
      " state (2)  A[0]:(0.999981701374) A[1]:(7.65526965552e-06) A[2]:(1.06313718788e-05) A[3]:(3.88983956157e-09)\n",
      " state (3)  A[0]:(0.999992787838) A[1]:(4.69269662062e-06) A[2]:(2.48976357398e-06) A[3]:(5.38874951606e-11)\n",
      " state (4)  A[0]:(0.999218463898) A[1]:(0.000752044201363) A[2]:(2.95079516945e-05) A[3]:(2.26199736025e-11)\n",
      " state (5)  A[0]:(0.539413690567) A[1]:(0.45723465085) A[2]:(0.00335166975856) A[3]:(6.19779428517e-14)\n",
      " state (6)  A[0]:(0.0144019592553) A[1]:(0.981687307358) A[2]:(0.00391074968502) A[3]:(1.08678107348e-17)\n",
      " state (7)  A[0]:(0.000914343283512) A[1]:(0.997951209545) A[2]:(0.00113446975593) A[3]:(7.18645081157e-20)\n",
      " state (8)  A[0]:(5.3472838772e-05) A[1]:(0.999818265438) A[2]:(0.000128280793433) A[3]:(1.109657844e-21)\n",
      " state (9)  A[0]:(7.37442223908e-06) A[1]:(0.999970912933) A[2]:(2.16866246774e-05) A[3]:(9.93052628045e-23)\n",
      " state (10)  A[0]:(3.22916071127e-06) A[1]:(0.999985039234) A[2]:(1.17364797916e-05) A[3]:(4.70523439103e-23)\n",
      " state (11)  A[0]:(2.06582762985e-06) A[1]:(0.999988138676) A[2]:(9.79008837021e-06) A[3]:(3.70142908397e-23)\n",
      " state (12)  A[0]:(1.52038126089e-06) A[1]:(0.999989271164) A[2]:(9.21770515561e-06) A[3]:(3.34174732315e-23)\n",
      " state (13)  A[0]:(1.22608821584e-06) A[1]:(0.999989748001) A[2]:(9.02441843209e-06) A[3]:(3.17260607874e-23)\n",
      " state (14)  A[0]:(1.05720039301e-06) A[1]:(0.99998998642) A[2]:(8.95834273251e-06) A[3]:(3.08152482971e-23)\n",
      " state (15)  A[0]:(9.56850612965e-07) A[1]:(0.999990105629) A[2]:(8.9375907919e-06) A[3]:(3.02866294491e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 362000 finished after 13 . Running score: 0.11. Policy_loss: -92050.6117425, Value_loss: 1.64719322207. Times trained:               13122. Times reached goal: 122.               Steps done: 4201110.\n",
      " state (0)  A[0]:(0.996478617191) A[1]:(0.0012857200345) A[2]:(0.00128286867402) A[3]:(0.000952776230406)\n",
      " state (1)  A[0]:(0.0353729203343) A[1]:(0.00679808249697) A[2]:(0.0119492653757) A[3]:(0.945879757404)\n",
      " state (2)  A[0]:(0.995083034039) A[1]:(0.00136180373374) A[2]:(0.00288228970021) A[3]:(0.000672899477649)\n",
      " state (3)  A[0]:(0.999996542931) A[1]:(1.54586894041e-06) A[2]:(1.90681726053e-06) A[3]:(7.81302592068e-11)\n",
      " state (4)  A[0]:(0.999989926815) A[1]:(7.05329784978e-06) A[2]:(3.0059677556e-06) A[3]:(3.65688382353e-11)\n",
      " state (5)  A[0]:(0.99833214283) A[1]:(0.00160314107779) A[2]:(6.46991320536e-05) A[3]:(5.41672045759e-12)\n",
      " state (6)  A[0]:(0.516393899918) A[1]:(0.477608591318) A[2]:(0.00599754182622) A[3]:(9.86993198624e-15)\n",
      " state (7)  A[0]:(0.0219973642379) A[1]:(0.972780764103) A[2]:(0.00522188283503) A[3]:(8.28284567799e-18)\n",
      " state (8)  A[0]:(0.00104848656338) A[1]:(0.997832119465) A[2]:(0.00111936463509) A[3]:(4.09244732316e-20)\n",
      " state (9)  A[0]:(5.79592124268e-05) A[1]:(0.999826073647) A[2]:(0.000115966569865) A[3]:(6.10138295095e-22)\n",
      " state (10)  A[0]:(1.18165353342e-05) A[1]:(0.999960362911) A[2]:(2.78280986095e-05) A[3]:(8.7435645106e-23)\n",
      " state (11)  A[0]:(6.41114775135e-06) A[1]:(0.999976754189) A[2]:(1.68353526533e-05) A[3]:(4.77262330717e-23)\n",
      " state (12)  A[0]:(4.81366032545e-06) A[1]:(0.999981045723) A[2]:(1.41157352118e-05) A[3]:(3.86392875065e-23)\n",
      " state (13)  A[0]:(4.06314075008e-06) A[1]:(0.999982774258) A[2]:(1.31731412694e-05) A[3]:(3.53341843501e-23)\n",
      " state (14)  A[0]:(3.61322213394e-06) A[1]:(0.999983608723) A[2]:(1.2800361219e-05) A[3]:(3.38255162679e-23)\n",
      " state (15)  A[0]:(3.28292276208e-06) A[1]:(0.99998408556) A[2]:(1.26461000036e-05) A[3]:(3.29884401841e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 363000 finished after 9 . Running score: 0.16. Policy_loss: -92050.6117073, Value_loss: 1.42136661453. Times trained:               12946. Times reached goal: 132.               Steps done: 4214056.\n",
      " state (0)  A[0]:(0.996540725231) A[1]:(0.00122961844318) A[2]:(0.00134402897675) A[3]:(0.000885629677214)\n",
      " state (1)  A[0]:(0.0430748425424) A[1]:(0.00717408256605) A[2]:(0.0143909966573) A[3]:(0.935360074043)\n",
      " state (2)  A[0]:(0.999044060707) A[1]:(0.000279745057924) A[2]:(0.000654313887935) A[3]:(2.18595851038e-05)\n",
      " state (3)  A[0]:(0.999996304512) A[1]:(1.37093570629e-06) A[2]:(2.29613715419e-06) A[3]:(5.99187019446e-11)\n",
      " state (4)  A[0]:(0.999976038933) A[1]:(1.07999549073e-05) A[2]:(1.31628939926e-05) A[3]:(1.39929821202e-11)\n",
      " state (5)  A[0]:(0.978532314301) A[1]:(0.00733835157007) A[2]:(0.0141293499619) A[3]:(2.46165053944e-14)\n",
      " state (6)  A[0]:(0.213327854872) A[1]:(0.506983041763) A[2]:(0.279689073563) A[3]:(8.70923114485e-17)\n",
      " state (7)  A[0]:(0.0134810525924) A[1]:(0.928060293198) A[2]:(0.0584586448967) A[3]:(1.0081325754e-18)\n",
      " state (8)  A[0]:(0.000555616221391) A[1]:(0.995900154114) A[2]:(0.00354422791861) A[3]:(1.21381581685e-20)\n",
      " state (9)  A[0]:(2.91392716463e-05) A[1]:(0.999754488468) A[2]:(0.000216400279896) A[3]:(3.42318225481e-22)\n",
      " state (10)  A[0]:(6.05866171099e-06) A[1]:(0.999930679798) A[2]:(6.32381561445e-05) A[3]:(8.07353634997e-23)\n",
      " state (11)  A[0]:(2.70525220003e-06) A[1]:(0.999955117702) A[2]:(4.21871154686e-05) A[3]:(4.93390623479e-23)\n",
      " state (12)  A[0]:(1.75380614564e-06) A[1]:(0.999961912632) A[2]:(3.63289873349e-05) A[3]:(4.04871342235e-23)\n",
      " state (13)  A[0]:(1.40258555348e-06) A[1]:(0.999964416027) A[2]:(3.41796694556e-05) A[3]:(3.71296538585e-23)\n",
      " state (14)  A[0]:(1.2501116089e-06) A[1]:(0.999965429306) A[2]:(3.33005482389e-05) A[3]:(3.569420469e-23)\n",
      " state (15)  A[0]:(1.17521199172e-06) A[1]:(0.999965906143) A[2]:(3.29234608216e-05) A[3]:(3.5034956787e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 364000 finished after 13 . Running score: 0.11. Policy_loss: -92050.6122479, Value_loss: 1.6318972452. Times trained:               13224. Times reached goal: 136.               Steps done: 4227280.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9987e-01,  5.1597e-05,  7.3968e-05,  5.5233e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9987e-01,  5.1595e-05,  7.3953e-05,  5.5247e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.3624e-05,  9.9899e-01,  9.3026e-04,  1.7313e-21]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.7919e-06,  9.9989e-01,  1.0802e-04,  1.3223e-22]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.3573e-05,  9.9899e-01,  9.2952e-04,  1.7304e-21]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.3552e-05,  9.9899e-01,  9.2922e-04,  1.7300e-21]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996613681316) A[1]:(0.00120834831614) A[2]:(0.00132214068435) A[3]:(0.000855835329276)\n",
      " state (1)  A[0]:(0.0473032854497) A[1]:(0.00744800409302) A[2]:(0.0155617631972) A[3]:(0.929686963558)\n",
      " state (2)  A[0]:(0.999849081039) A[1]:(4.67957906949e-05) A[2]:(0.000103777150798) A[3]:(3.64043387435e-07)\n",
      " state (3)  A[0]:(0.999995231628) A[1]:(1.80103847924e-06) A[2]:(2.95222571367e-06) A[3]:(5.28284152534e-11)\n",
      " state (4)  A[0]:(0.999874651432) A[1]:(5.15387291671e-05) A[2]:(7.38007584005e-05) A[3]:(5.53697756048e-12)\n",
      " state (5)  A[0]:(0.746999680996) A[1]:(0.0817967057228) A[2]:(0.17120359838) A[3]:(3.0396670932e-15)\n",
      " state (6)  A[0]:(0.0471141189337) A[1]:(0.749171793461) A[2]:(0.203714072704) A[3]:(8.91204722837e-18)\n",
      " state (7)  A[0]:(0.00246564880945) A[1]:(0.976303458214) A[2]:(0.0212308801711) A[3]:(1.24298659965e-19)\n",
      " state (8)  A[0]:(8.35873797769e-05) A[1]:(0.998986721039) A[2]:(0.000929696019739) A[3]:(1.73101457421e-21)\n",
      " state (9)  A[0]:(6.791220585e-06) A[1]:(0.999885201454) A[2]:(0.000108001033368) A[3]:(1.32258803782e-22)\n",
      " state (10)  A[0]:(2.03094441531e-06) A[1]:(0.999945878983) A[2]:(5.21194415342e-05) A[3]:(5.60121408748e-23)\n",
      " state (11)  A[0]:(1.16495323255e-06) A[1]:(0.999957799911) A[2]:(4.10099310102e-05) A[3]:(4.15893149031e-23)\n",
      " state (12)  A[0]:(9.08673598587e-07) A[1]:(0.999961674213) A[2]:(3.74454575649e-05) A[3]:(3.69822599315e-23)\n",
      " state (13)  A[0]:(8.12764824332e-07) A[1]:(0.999963104725) A[2]:(3.6091721995e-05) A[3]:(3.52185688958e-23)\n",
      " state (14)  A[0]:(7.71334327965e-07) A[1]:(0.999963700771) A[2]:(3.55438496626e-05) A[3]:(3.44863198046e-23)\n",
      " state (15)  A[0]:(7.51403547383e-07) A[1]:(0.99996393919) A[2]:(3.53161922249e-05) A[3]:(3.41661748057e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 365000 finished after 13 . Running score: 0.09. Policy_loss: -92050.6119708, Value_loss: 1.85783360073. Times trained:               12811. Times reached goal: 110.               Steps done: 4240091.\n",
      " state (0)  A[0]:(0.997094511986) A[1]:(0.00104017404374) A[2]:(0.00119035597891) A[3]:(0.00067497062264)\n",
      " state (1)  A[0]:(0.054624479264) A[1]:(0.00781609863043) A[2]:(0.0161832347512) A[3]:(0.921376168728)\n",
      " state (2)  A[0]:(0.995248556137) A[1]:(0.001274791779) A[2]:(0.0028785248287) A[3]:(0.000598113110755)\n",
      " state (3)  A[0]:(0.999996721745) A[1]:(1.45375611282e-06) A[2]:(1.82359076462e-06) A[3]:(4.79007562781e-11)\n",
      " state (4)  A[0]:(0.999977231026) A[1]:(1.64699176821e-05) A[2]:(6.28038878858e-06) A[3]:(1.1638657052e-11)\n",
      " state (5)  A[0]:(0.977413415909) A[1]:(0.0200984347612) A[2]:(0.00248814211227) A[3]:(3.84279603581e-14)\n",
      " state (6)  A[0]:(0.0473019629717) A[1]:(0.917612373829) A[2]:(0.0350856520236) A[3]:(4.66919467548e-18)\n",
      " state (7)  A[0]:(0.000636431796011) A[1]:(0.997244954109) A[2]:(0.00211859680712) A[3]:(9.27541450766e-21)\n",
      " state (8)  A[0]:(2.16526314034e-05) A[1]:(0.999881327152) A[2]:(9.70399923972e-05) A[3]:(1.6489347456e-22)\n",
      " state (9)  A[0]:(4.13471343563e-06) A[1]:(0.999968409538) A[2]:(2.74488484138e-05) A[3]:(3.84947681887e-23)\n",
      " state (10)  A[0]:(1.95704819816e-06) A[1]:(0.999978244305) A[2]:(1.97741574084e-05) A[3]:(2.56738331478e-23)\n",
      " state (11)  A[0]:(1.40069823829e-06) A[1]:(0.999980688095) A[2]:(1.79211929208e-05) A[3]:(2.241522072e-23)\n",
      " state (12)  A[0]:(1.21597997804e-06) A[1]:(0.999981462955) A[2]:(1.73211465153e-05) A[3]:(2.13127797164e-23)\n",
      " state (13)  A[0]:(1.14611827939e-06) A[1]:(0.999981760979) A[2]:(1.71053015947e-05) A[3]:(2.09010369223e-23)\n",
      " state (14)  A[0]:(1.11807912617e-06) A[1]:(0.999981880188) A[2]:(1.7024405679e-05) A[3]:(2.07409975551e-23)\n",
      " state (15)  A[0]:(1.10679548015e-06) A[1]:(0.999981880188) A[2]:(1.69933919096e-05) A[3]:(2.06781142969e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 366000 finished after 7 . Running score: 0.17. Policy_loss: -92050.6117215, Value_loss: 1.21716835401. Times trained:               13047. Times reached goal: 130.               Steps done: 4253138.\n",
      " state (0)  A[0]:(0.997301995754) A[1]:(0.000927028653678) A[2]:(0.00120610021986) A[3]:(0.000564853427932)\n",
      " state (1)  A[0]:(0.0566588565707) A[1]:(0.00746905757114) A[2]:(0.0164114832878) A[3]:(0.919460594654)\n",
      " state (2)  A[0]:(0.991140902042) A[1]:(0.00186152872629) A[2]:(0.00494476826862) A[3]:(0.00205278373323)\n",
      " state (3)  A[0]:(0.99999755621) A[1]:(8.22169965886e-07) A[2]:(1.5920087435e-06) A[3]:(5.00767170786e-11)\n",
      " state (4)  A[0]:(0.999995589256) A[1]:(1.73257274128e-06) A[2]:(2.65096150542e-06) A[3]:(1.33162413388e-11)\n",
      " state (5)  A[0]:(0.999866724014) A[1]:(4.36894770246e-05) A[2]:(8.96007986739e-05) A[3]:(1.05413074103e-13)\n",
      " state (6)  A[0]:(0.932899534702) A[1]:(0.0212740115821) A[2]:(0.0458264835179) A[3]:(5.67158769277e-17)\n",
      " state (7)  A[0]:(0.068464666605) A[1]:(0.872429251671) A[2]:(0.059106066823) A[3]:(2.42814991406e-19)\n",
      " state (8)  A[0]:(0.000611936207861) A[1]:(0.998603224754) A[2]:(0.000784831761848) A[3]:(7.41363804688e-22)\n",
      " state (9)  A[0]:(6.39929858153e-05) A[1]:(0.999842941761) A[2]:(9.30687674554e-05) A[3]:(8.08114727998e-23)\n",
      " state (10)  A[0]:(2.69357569778e-05) A[1]:(0.999922513962) A[2]:(5.05368298036e-05) A[3]:(4.44792563423e-23)\n",
      " state (11)  A[0]:(1.75520744961e-05) A[1]:(0.999940872192) A[2]:(4.16046204919e-05) A[3]:(3.63928735502e-23)\n",
      " state (12)  A[0]:(1.36763901537e-05) A[1]:(0.999947607517) A[2]:(3.87038089684e-05) A[3]:(3.35143800605e-23)\n",
      " state (13)  A[0]:(1.15493348858e-05) A[1]:(0.999950885773) A[2]:(3.75375821022e-05) A[3]:(3.22047605173e-23)\n",
      " state (14)  A[0]:(1.01755476862e-05) A[1]:(0.999952852726) A[2]:(3.69826266251e-05) A[3]:(3.14828107941e-23)\n",
      " state (15)  A[0]:(9.24299729377e-06) A[1]:(0.999954104424) A[2]:(3.66774802387e-05) A[3]:(3.10304811065e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 367000 finished after 18 . Running score: 0.18. Policy_loss: -92050.6124844, Value_loss: 1.6306877825. Times trained:               13463. Times reached goal: 123.               Steps done: 4266601.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997537493706) A[1]:(0.000808158249129) A[2]:(0.00115623371676) A[3]:(0.000498117005918)\n",
      " state (1)  A[0]:(0.0650092586875) A[1]:(0.00758722191676) A[2]:(0.0187836140394) A[3]:(0.908619940281)\n",
      " state (2)  A[0]:(0.999416053295) A[1]:(0.000154268665938) A[2]:(0.000421292206738) A[3]:(8.37364314066e-06)\n",
      " state (3)  A[0]:(0.999997019768) A[1]:(9.06320451577e-07) A[2]:(2.0848224267e-06) A[3]:(4.07741514163e-11)\n",
      " state (4)  A[0]:(0.999987244606) A[1]:(3.59533009942e-06) A[2]:(9.14261454454e-06) A[3]:(6.61292982179e-12)\n",
      " state (5)  A[0]:(0.997439265251) A[1]:(0.000269353273325) A[2]:(0.0022913988214) A[3]:(1.04267943156e-14)\n",
      " state (6)  A[0]:(0.647108912468) A[1]:(0.0507494285703) A[2]:(0.302141666412) A[3]:(4.42432506049e-17)\n",
      " state (7)  A[0]:(0.101644843817) A[1]:(0.568756580353) A[2]:(0.32959857583) A[3]:(1.63860777714e-18)\n",
      " state (8)  A[0]:(0.00352370389737) A[1]:(0.974479436874) A[2]:(0.0219968501478) A[3]:(2.62112016497e-20)\n",
      " state (9)  A[0]:(0.000111516412289) A[1]:(0.998890161514) A[2]:(0.000998324831016) A[3]:(8.36446812559e-22)\n",
      " state (10)  A[0]:(1.60919971677e-05) A[1]:(0.999754965305) A[2]:(0.00022894286667) A[3]:(1.86999868858e-22)\n",
      " state (11)  A[0]:(6.43158819003e-06) A[1]:(0.999863028526) A[2]:(0.000130562999402) A[3]:(1.05572775318e-22)\n",
      " state (12)  A[0]:(4.231982075e-06) A[1]:(0.999892055988) A[2]:(0.000103732629213) A[3]:(8.33394564567e-23)\n",
      " state (13)  A[0]:(3.49613810613e-06) A[1]:(0.999902546406) A[2]:(9.39670790103e-05) A[3]:(7.52500854595e-23)\n",
      " state (14)  A[0]:(3.19278501593e-06) A[1]:(0.99990683794) A[2]:(8.9951492555e-05) A[3]:(7.19138096756e-23)\n",
      " state (15)  A[0]:(3.04997206513e-06) A[1]:(0.999908745289) A[2]:(8.81935047801e-05) A[3]:(7.0437510134e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 368000 finished after 6 . Running score: 0.16. Policy_loss: -92050.6126071, Value_loss: 1.41684370274. Times trained:               12811. Times reached goal: 135.               Steps done: 4279412.\n",
      " state (0)  A[0]:(0.997152328491) A[1]:(0.000785032578278) A[2]:(0.00113918120041) A[3]:(0.000923446728848)\n",
      " state (1)  A[0]:(0.0657327026129) A[1]:(0.00749523099512) A[2]:(0.0181070417166) A[3]:(0.908665001392)\n",
      " state (2)  A[0]:(0.999950587749) A[1]:(1.49640018208e-05) A[2]:(3.43801184499e-05) A[3]:(4.11734575323e-08)\n",
      " state (3)  A[0]:(0.999996483326) A[1]:(1.23256688767e-06) A[2]:(2.2829253794e-06) A[3]:(2.3009738212e-11)\n",
      " state (4)  A[0]:(0.999935150146) A[1]:(1.87191344594e-05) A[2]:(4.61301387986e-05) A[3]:(4.07497413634e-13)\n",
      " state (5)  A[0]:(0.962983548641) A[1]:(0.00575668690726) A[2]:(0.0312597714365) A[3]:(1.31541860898e-16)\n",
      " state (6)  A[0]:(0.332089155912) A[1]:(0.360174894333) A[2]:(0.307735919952) A[3]:(2.91927568486e-18)\n",
      " state (7)  A[0]:(0.018721383065) A[1]:(0.937685132027) A[2]:(0.0435934960842) A[3]:(6.46831457574e-20)\n",
      " state (8)  A[0]:(0.000413123139879) A[1]:(0.998479425907) A[2]:(0.00110743416008) A[3]:(9.65466692304e-22)\n",
      " state (9)  A[0]:(3.22370651702e-05) A[1]:(0.999854326248) A[2]:(0.000113459864224) A[3]:(1.06909244531e-22)\n",
      " state (10)  A[0]:(1.00658926385e-05) A[1]:(0.999934494495) A[2]:(5.54348443984e-05) A[3]:(5.32613387221e-23)\n",
      " state (11)  A[0]:(5.90773561271e-06) A[1]:(0.999949932098) A[2]:(4.41459269496e-05) A[3]:(4.19142120001e-23)\n",
      " state (12)  A[0]:(4.61810668639e-06) A[1]:(0.999954760075) A[2]:(4.06331200793e-05) A[3]:(3.81911798025e-23)\n",
      " state (13)  A[0]:(4.1031416913e-06) A[1]:(0.999956607819) A[2]:(3.93161717511e-05) A[3]:(3.67323708849e-23)\n",
      " state (14)  A[0]:(3.86600504498e-06) A[1]:(0.999957382679) A[2]:(3.8774604036e-05) A[3]:(3.61025411827e-23)\n",
      " state (15)  A[0]:(3.74610772269e-06) A[1]:(0.999957740307) A[2]:(3.85383464163e-05) A[3]:(3.581053643e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 369000 finished after 4 . Running score: 0.14. Policy_loss: -92050.6117537, Value_loss: 0.995699850384. Times trained:               13120. Times reached goal: 142.               Steps done: 4292532.\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9514e-01,  9.1655e-04,  3.9389e-03,  2.8495e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9515e-01,  9.1531e-04,  3.9330e-03,  2.8542e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9515e-01,  9.1481e-04,  3.9306e-03,  2.8562e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9516e-01,  9.1330e-04,  3.9233e-03,  2.8620e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9516e-01,  9.1303e-04,  3.9220e-03,  2.8631e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9517e-01,  9.1241e-04,  3.9190e-03,  2.8655e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0008,  0.0012,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9517e-01,  9.1212e-04,  3.9177e-03,  2.8667e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7176e-04,  9.9879e-01,  9.3993e-04,  1.9370e-21]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996990323067) A[1]:(0.000797805550974) A[2]:(0.00118082656991) A[3]:(0.0010310158832)\n",
      " state (1)  A[0]:(0.0668068826199) A[1]:(0.00758384680375) A[2]:(0.0177143737674) A[3]:(0.907894909382)\n",
      " state (2)  A[0]:(0.999993145466) A[1]:(2.40256053985e-06) A[2]:(4.45176237918e-06) A[3]:(5.1247234234e-10)\n",
      " state (3)  A[0]:(0.999991714954) A[1]:(3.31455498781e-06) A[2]:(4.96604207001e-06) A[3]:(9.56805034508e-12)\n",
      " state (4)  A[0]:(0.995187103748) A[1]:(0.000909273396246) A[2]:(0.00390361365862) A[3]:(2.87785733049e-15)\n",
      " state (5)  A[0]:(0.46807962656) A[1]:(0.222028970718) A[2]:(0.309891432524) A[3]:(1.19216272547e-17)\n",
      " state (6)  A[0]:(0.068268224597) A[1]:(0.790247738361) A[2]:(0.141484051943) A[3]:(7.51456932064e-19)\n",
      " state (7)  A[0]:(0.00744113558903) A[1]:(0.971265435219) A[2]:(0.0212934352458) A[3]:(5.67976265156e-20)\n",
      " state (8)  A[0]:(0.0002721658675) A[1]:(0.998786747456) A[2]:(0.000941065140069) A[3]:(1.93929909674e-21)\n",
      " state (9)  A[0]:(1.4084540453e-05) A[1]:(0.999914467335) A[2]:(7.14553389116e-05) A[3]:(1.4524884378e-22)\n",
      " state (10)  A[0]:(4.37563676314e-06) A[1]:(0.999964654446) A[2]:(3.09856404783e-05) A[3]:(6.20279184074e-23)\n",
      " state (11)  A[0]:(3.0929222703e-06) A[1]:(0.999971926212) A[2]:(2.49556796916e-05) A[3]:(4.9522043368e-23)\n",
      " state (12)  A[0]:(2.76707078228e-06) A[1]:(0.999973833561) A[2]:(2.34249400819e-05) A[3]:(4.6315983276e-23)\n",
      " state (13)  A[0]:(2.65809967459e-06) A[1]:(0.999974370003) A[2]:(2.29499764828e-05) A[3]:(4.53018173854e-23)\n",
      " state (14)  A[0]:(2.61527611656e-06) A[1]:(0.999974608421) A[2]:(2.27878299484e-05) A[3]:(4.49429424715e-23)\n",
      " state (15)  A[0]:(2.59595867647e-06) A[1]:(0.999974668026) A[2]:(2.27286218433e-05) A[3]:(4.48034339981e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 370000 finished after 24 . Running score: 0.11. Policy_loss: -92050.6116909, Value_loss: 1.21921799901. Times trained:               12502. Times reached goal: 119.               Steps done: 4305034.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997304439545) A[1]:(0.000702850520611) A[2]:(0.00114720896818) A[3]:(0.000845505273901)\n",
      " state (1)  A[0]:(0.0673936977983) A[1]:(0.00761341070756) A[2]:(0.0176448635757) A[3]:(0.907348036766)\n",
      " state (2)  A[0]:(0.999958693981) A[1]:(1.49607558342e-05) A[2]:(2.63476231339e-05) A[3]:(2.59060488617e-08)\n",
      " state (3)  A[0]:(0.999990463257) A[1]:(5.45385773876e-06) A[2]:(4.08016785514e-06) A[3]:(1.1058820526e-11)\n",
      " state (4)  A[0]:(0.993096888065) A[1]:(0.00413367897272) A[2]:(0.00276940572076) A[3]:(9.13691390885e-15)\n",
      " state (5)  A[0]:(0.278445422649) A[1]:(0.518534839153) A[2]:(0.203019723296) A[3]:(6.0740161893e-18)\n",
      " state (6)  A[0]:(0.0337168872356) A[1]:(0.901129603386) A[2]:(0.0651535317302) A[3]:(2.29871527675e-19)\n",
      " state (7)  A[0]:(0.00340996892191) A[1]:(0.988516628742) A[2]:(0.00807338580489) A[3]:(1.56709706321e-20)\n",
      " state (8)  A[0]:(0.000119221003843) A[1]:(0.999571859837) A[2]:(0.000308933318593) A[3]:(5.69930521762e-22)\n",
      " state (9)  A[0]:(1.09285911094e-05) A[1]:(0.99994790554) A[2]:(4.119226287e-05) A[3]:(8.07900536485e-23)\n",
      " state (10)  A[0]:(4.76802097182e-06) A[1]:(0.999971270561) A[2]:(2.39384571614e-05) A[3]:(4.68066168937e-23)\n",
      " state (11)  A[0]:(3.72440149476e-06) A[1]:(0.999975442886) A[2]:(2.08288074646e-05) A[3]:(4.05178114464e-23)\n",
      " state (12)  A[0]:(3.44123850482e-06) A[1]:(0.999976575375) A[2]:(1.99970108952e-05) A[3]:(3.88100285608e-23)\n",
      " state (13)  A[0]:(3.3483831885e-06) A[1]:(0.999976933002) A[2]:(1.97392946575e-05) A[3]:(3.8271801387e-23)\n",
      " state (14)  A[0]:(3.31468004333e-06) A[1]:(0.999977052212) A[2]:(1.96538294404e-05) A[3]:(3.80887320144e-23)\n",
      " state (15)  A[0]:(3.30142370331e-06) A[1]:(0.999977052212) A[2]:(1.96242745005e-05) A[3]:(3.80228211081e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 371000 finished after 5 . Running score: 0.19. Policy_loss: -92050.5923775, Value_loss: 0.979058555921. Times trained:               12264. Times reached goal: 136.               Steps done: 4317298.\n",
      " state (0)  A[0]:(0.996618211269) A[1]:(0.000874231511261) A[2]:(0.00131880072877) A[3]:(0.00118874886539)\n",
      " state (1)  A[0]:(0.0381802655756) A[1]:(0.00613394472748) A[2]:(0.0126358224079) A[3]:(0.943049967289)\n",
      " state (2)  A[0]:(0.795027673244) A[1]:(0.0119037916884) A[2]:(0.0331076532602) A[3]:(0.159960895777)\n",
      " state (3)  A[0]:(0.999997377396) A[1]:(9.06721936644e-07) A[2]:(1.72741010829e-06) A[3]:(4.99254491915e-11)\n",
      " state (4)  A[0]:(0.999992489815) A[1]:(3.20536605614e-06) A[2]:(4.27960685556e-06) A[3]:(1.12150124257e-11)\n",
      " state (5)  A[0]:(0.998408675194) A[1]:(0.000421273551183) A[2]:(0.00117004918866) A[3]:(2.4317551892e-14)\n",
      " state (6)  A[0]:(0.601810336113) A[1]:(0.0875196829438) A[2]:(0.310670018196) A[3]:(2.97882330046e-17)\n",
      " state (7)  A[0]:(0.162401676178) A[1]:(0.445937514305) A[2]:(0.391660809517) A[3]:(1.9369283971e-18)\n",
      " state (8)  A[0]:(0.033146712929) A[1]:(0.841669380665) A[2]:(0.125183910131) A[3]:(2.1200969972e-19)\n",
      " state (9)  A[0]:(0.00201744050719) A[1]:(0.988803446293) A[2]:(0.00917912647128) A[3]:(9.86163999272e-21)\n",
      " state (10)  A[0]:(0.000110321088869) A[1]:(0.999279677868) A[2]:(0.000610004703049) A[3]:(6.44087182898e-22)\n",
      " state (11)  A[0]:(2.44599541475e-05) A[1]:(0.999798536301) A[2]:(0.000176989720785) A[3]:(1.90922792098e-22)\n",
      " state (12)  A[0]:(1.34945930768e-05) A[1]:(0.999871373177) A[2]:(0.000115116308734) A[3]:(1.24527209605e-22)\n",
      " state (13)  A[0]:(1.06758507172e-05) A[1]:(0.999890983105) A[2]:(9.8332966445e-05) A[3]:(1.06308826719e-22)\n",
      " state (14)  A[0]:(9.69435950537e-06) A[1]:(0.999897897243) A[2]:(9.24371634028e-05) A[3]:(9.98594470568e-23)\n",
      " state (15)  A[0]:(9.30245641939e-06) A[1]:(0.999900579453) A[2]:(9.01312523638e-05) A[3]:(9.73145502221e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 372000 finished after 17 . Running score: 0.14. Policy_loss: -92050.5938882, Value_loss: 1.21621269808. Times trained:               12764. Times reached goal: 139.               Steps done: 4330062.\n",
      " state (0)  A[0]:(0.9968547225) A[1]:(0.000812562589999) A[2]:(0.00124959845562) A[3]:(0.00108314363752)\n",
      " state (1)  A[0]:(0.0393738262355) A[1]:(0.00613470980898) A[2]:(0.0123864877969) A[3]:(0.942104995251)\n",
      " state (2)  A[0]:(0.559845149517) A[1]:(0.0146085564047) A[2]:(0.0397256426513) A[3]:(0.38582059741)\n",
      " state (3)  A[0]:(0.99999755621) A[1]:(9.30232488372e-07) A[2]:(1.533085765e-06) A[3]:(4.80111055079e-11)\n",
      " state (4)  A[0]:(0.999993383884) A[1]:(3.79944458473e-06) A[2]:(2.83191343442e-06) A[3]:(7.75401531178e-12)\n",
      " state (5)  A[0]:(0.999208033085) A[1]:(0.000545082788449) A[2]:(0.000246868527029) A[3]:(5.21683797506e-14)\n",
      " state (6)  A[0]:(0.845304250717) A[1]:(0.0871529653668) A[2]:(0.0675427764654) A[3]:(6.01398157111e-17)\n",
      " state (7)  A[0]:(0.347865015268) A[1]:(0.44571390748) A[2]:(0.20642106235) A[3]:(2.1967245276e-18)\n",
      " state (8)  A[0]:(0.104045048356) A[1]:(0.773736059666) A[2]:(0.122218906879) A[3]:(2.50991878556e-19)\n",
      " state (9)  A[0]:(0.0139901004732) A[1]:(0.966577649117) A[2]:(0.0194322802126) A[3]:(1.93111323985e-20)\n",
      " state (10)  A[0]:(0.000933500472456) A[1]:(0.997842371464) A[2]:(0.00122412713245) A[3]:(1.23461835913e-21)\n",
      " state (11)  A[0]:(0.000140090458444) A[1]:(0.999647378922) A[2]:(0.000212512779399) A[3]:(2.54790334136e-22)\n",
      " state (12)  A[0]:(5.56459926884e-05) A[1]:(0.999836623669) A[2]:(0.000107740655949) A[3]:(1.36882197641e-22)\n",
      " state (13)  A[0]:(3.66670974472e-05) A[1]:(0.999879777431) A[2]:(8.35789323901e-05) A[3]:(1.0753204075e-22)\n",
      " state (14)  A[0]:(3.04475288431e-05) A[1]:(0.999894022942) A[2]:(7.55218861741e-05) A[3]:(9.73970966272e-23)\n",
      " state (15)  A[0]:(2.80246786133e-05) A[1]:(0.999899566174) A[2]:(7.23880366422e-05) A[3]:(9.33806271055e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 373000 finished after 9 . Running score: 0.11. Policy_loss: -92050.6138325, Value_loss: 0.995867983207. Times trained:               12504. Times reached goal: 113.               Steps done: 4342566.\n",
      " state (0)  A[0]:(0.996029138565) A[1]:(0.00108593376353) A[2]:(0.00140559766442) A[3]:(0.00147930707317)\n",
      " state (1)  A[0]:(0.0276854801923) A[1]:(0.00566997658461) A[2]:(0.00976424105465) A[3]:(0.956880271435)\n",
      " state (2)  A[0]:(0.232574403286) A[1]:(0.0127206258476) A[2]:(0.0319479331374) A[3]:(0.722757041454)\n",
      " state (3)  A[0]:(0.999996423721) A[1]:(1.52261941366e-06) A[2]:(2.03667786991e-06) A[3]:(1.20896070932e-10)\n",
      " state (4)  A[0]:(0.999994039536) A[1]:(3.97198255087e-06) A[2]:(1.9891899683e-06) A[3]:(1.48911768333e-11)\n",
      " state (5)  A[0]:(0.999116718769) A[1]:(0.00080866535427) A[2]:(7.46237055864e-05) A[3]:(3.69402777681e-13)\n",
      " state (6)  A[0]:(0.365300059319) A[1]:(0.577982842922) A[2]:(0.0567171052098) A[3]:(2.25878904702e-17)\n",
      " state (7)  A[0]:(0.00434733647853) A[1]:(0.990930438042) A[2]:(0.00472223944962) A[3]:(1.80387137433e-20)\n",
      " state (8)  A[0]:(6.66476844344e-05) A[1]:(0.999849796295) A[2]:(8.35573009681e-05) A[3]:(2.63117070224e-22)\n",
      " state (9)  A[0]:(9.1531273938e-06) A[1]:(0.999972105026) A[2]:(1.87707555597e-05) A[3]:(6.19208100291e-23)\n",
      " state (10)  A[0]:(4.80308472106e-06) A[1]:(0.999981701374) A[2]:(1.35159434649e-05) A[3]:(4.37996116517e-23)\n",
      " state (11)  A[0]:(3.85701014238e-06) A[1]:(0.999983727932) A[2]:(1.24041589515e-05) A[3]:(3.96909747772e-23)\n",
      " state (12)  A[0]:(3.51108360519e-06) A[1]:(0.999984443188) A[2]:(1.20597151181e-05) A[3]:(3.82836248342e-23)\n",
      " state (13)  A[0]:(3.33215666615e-06) A[1]:(0.999984741211) A[2]:(1.19153537526e-05) A[3]:(3.76122568722e-23)\n",
      " state (14)  A[0]:(3.21380844071e-06) A[1]:(0.99998497963) A[2]:(1.18315647342e-05) A[3]:(3.71825801144e-23)\n",
      " state (15)  A[0]:(3.12291672344e-06) A[1]:(0.999985098839) A[2]:(1.17683775898e-05) A[3]:(3.6847516178e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 374000 finished after 7 . Running score: 0.08. Policy_loss: -92050.611317, Value_loss: 1.01187143214. Times trained:               12810. Times reached goal: 130.               Steps done: 4355376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9959,  0.0011,  0.0014,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0011,  0.0014,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0011,  0.0014,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0011,  0.0014,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0011,  0.0014,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0011,  0.0014,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0011,  0.0014,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0011,  0.0014,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.4646e-06,  1.5625e-06,  3.2737e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0011,  0.0014,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.4646e-06,  1.5625e-06,  3.2738e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2053e-05,  9.9980e-01,  1.3455e-04,  5.2792e-22]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.6584e-06,  9.9997e-01,  2.2005e-05,  9.1502e-23]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2043e-05,  9.9980e-01,  1.3453e-04,  5.2791e-22]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2039e-05,  9.9980e-01,  1.3452e-04,  5.2790e-22]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.6571e-06,  9.9997e-01,  2.2002e-05,  9.1501e-23]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8497e-06,  9.9999e-01,  1.2829e-05,  5.1362e-23]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8495e-06,  9.9999e-01,  1.2829e-05,  5.1361e-23]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8494e-06,  9.9999e-01,  1.2828e-05,  5.1361e-23]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995882093906) A[1]:(0.0010559738148) A[2]:(0.00138849427458) A[3]:(0.00167343800422)\n",
      " state (1)  A[0]:(0.0246489848942) A[1]:(0.00502735283226) A[2]:(0.0088721299544) A[3]:(0.961451530457)\n",
      " state (2)  A[0]:(0.200811758637) A[1]:(0.0111149121076) A[2]:(0.0287781469524) A[3]:(0.759295165539)\n",
      " state (3)  A[0]:(0.999995052814) A[1]:(1.89382581084e-06) A[2]:(3.06232550429e-06) A[3]:(3.25098614695e-10)\n",
      " state (4)  A[0]:(0.999996960163) A[1]:(1.46475326801e-06) A[2]:(1.56255316597e-06) A[3]:(3.27373371467e-11)\n",
      " state (5)  A[0]:(0.999976456165) A[1]:(1.70917428477e-05) A[2]:(6.42948407403e-06) A[3]:(5.89231051104e-12)\n",
      " state (6)  A[0]:(0.96218085289) A[1]:(0.0290830954909) A[2]:(0.00873602740467) A[3]:(3.31749368814e-15)\n",
      " state (7)  A[0]:(0.0130949364975) A[1]:(0.967481017113) A[2]:(0.0194240454584) A[3]:(1.4908831942e-19)\n",
      " state (8)  A[0]:(6.20002829237e-05) A[1]:(0.999803543091) A[2]:(0.000134465139126) A[3]:(5.27759002267e-22)\n",
      " state (9)  A[0]:(5.65441132494e-06) A[1]:(0.999972343445) A[2]:(2.19977118832e-05) A[3]:(9.1492453819e-23)\n",
      " state (10)  A[0]:(2.72912711807e-06) A[1]:(0.999982297421) A[2]:(1.49863271872e-05) A[3]:(6.13753852883e-23)\n",
      " state (11)  A[0]:(2.1488810944e-06) A[1]:(0.999984323978) A[2]:(1.35125255838e-05) A[3]:(5.47263228428e-23)\n",
      " state (12)  A[0]:(1.94704580281e-06) A[1]:(0.999985039234) A[2]:(1.30330799948e-05) A[3]:(5.24274433255e-23)\n",
      " state (13)  A[0]:(1.84930411251e-06) A[1]:(0.999985337257) A[2]:(1.2827745195e-05) A[3]:(5.1360493174e-23)\n",
      " state (14)  A[0]:(1.78938944373e-06) A[1]:(0.999985516071) A[2]:(1.27148550746e-05) A[3]:(5.07269053384e-23)\n",
      " state (15)  A[0]:(1.74632555172e-06) A[1]:(0.999985635281) A[2]:(1.26377881315e-05) A[3]:(5.02734333802e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 375000 finished after 19 . Running score: 0.17. Policy_loss: -92050.6113063, Value_loss: 1.43635048847. Times trained:               12724. Times reached goal: 142.               Steps done: 4368100.\n",
      " state (0)  A[0]:(0.997249782085) A[1]:(0.000781869865023) A[2]:(0.00110833370127) A[3]:(0.000859996594954)\n",
      " state (1)  A[0]:(0.0389420986176) A[1]:(0.00583787821233) A[2]:(0.0115786939859) A[3]:(0.94364130497)\n",
      " state (2)  A[0]:(0.491680294275) A[1]:(0.0139173353091) A[2]:(0.0363209582865) A[3]:(0.458081454039)\n",
      " state (3)  A[0]:(0.999996483326) A[1]:(1.33093556087e-06) A[2]:(2.19114212996e-06) A[3]:(1.7643465855e-10)\n",
      " state (4)  A[0]:(0.999997794628) A[1]:(9.78252501227e-07) A[2]:(1.22242386169e-06) A[3]:(3.21500048805e-11)\n",
      " state (5)  A[0]:(0.999994456768) A[1]:(3.52324764208e-06) A[2]:(2.049408522e-06) A[3]:(1.23397377386e-11)\n",
      " state (6)  A[0]:(0.999634802341) A[1]:(0.000307167705614) A[2]:(5.80228879699e-05) A[3]:(2.76498306672e-13)\n",
      " state (7)  A[0]:(0.272274911404) A[1]:(0.663813233376) A[2]:(0.0639118477702) A[3]:(6.76845664741e-18)\n",
      " state (8)  A[0]:(0.000574750942178) A[1]:(0.998831391335) A[2]:(0.000593871052843) A[3]:(1.45920284317e-21)\n",
      " state (9)  A[0]:(1.98547040782e-05) A[1]:(0.999951124191) A[2]:(2.90012158075e-05) A[3]:(7.51622189765e-23)\n",
      " state (10)  A[0]:(5.67582765143e-06) A[1]:(0.999980032444) A[2]:(1.42835697261e-05) A[3]:(3.70454918662e-23)\n",
      " state (11)  A[0]:(3.51032099388e-06) A[1]:(0.999984622002) A[2]:(1.18802463476e-05) A[3]:(3.01499798076e-23)\n",
      " state (12)  A[0]:(2.85738224193e-06) A[1]:(0.999985992908) A[2]:(1.11627941806e-05) A[3]:(2.79392602296e-23)\n",
      " state (13)  A[0]:(2.57537203652e-06) A[1]:(0.99998652935) A[2]:(1.08787253339e-05) A[3]:(2.69923179099e-23)\n",
      " state (14)  A[0]:(2.42182522925e-06) A[1]:(0.999986827374) A[2]:(1.07396072053e-05) A[3]:(2.64874185315e-23)\n",
      " state (15)  A[0]:(2.32358479479e-06) A[1]:(0.999987006187) A[2]:(1.06564602902e-05) A[3]:(2.61644604547e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 376000 finished after 5 . Running score: 0.07. Policy_loss: -92050.6113401, Value_loss: 1.20970806843. Times trained:               12520. Times reached goal: 132.               Steps done: 4380620.\n",
      " state (0)  A[0]:(0.997427761555) A[1]:(0.000709069834556) A[2]:(0.00113522051834) A[3]:(0.000727976788767)\n",
      " state (1)  A[0]:(0.0499398931861) A[1]:(0.00638515735045) A[2]:(0.0152170620859) A[3]:(0.928457915783)\n",
      " state (2)  A[0]:(0.992171168327) A[1]:(0.00171794707421) A[2]:(0.00461784517393) A[3]:(0.00149306096137)\n",
      " state (3)  A[0]:(0.999996960163) A[1]:(1.12433951927e-06) A[2]:(1.88858825823e-06) A[3]:(3.52277720883e-11)\n",
      " state (4)  A[0]:(0.999989569187) A[1]:(6.68073471388e-06) A[2]:(3.74324486074e-06) A[3]:(9.54424300009e-12)\n",
      " state (5)  A[0]:(0.998751282692) A[1]:(0.00112722662743) A[2]:(0.00012147436064) A[3]:(3.48978685576e-13)\n",
      " state (6)  A[0]:(0.318571567535) A[1]:(0.562034785748) A[2]:(0.119393609464) A[3]:(3.10982257058e-17)\n",
      " state (7)  A[0]:(0.00414803437889) A[1]:(0.966308534145) A[2]:(0.0295434165746) A[3]:(1.56003210068e-20)\n",
      " state (8)  A[0]:(6.6340478952e-05) A[1]:(0.999243140221) A[2]:(0.00069050618913) A[3]:(2.66180930249e-22)\n",
      " state (9)  A[0]:(6.42222994429e-06) A[1]:(0.999870896339) A[2]:(0.000122667013784) A[3]:(5.05810070917e-23)\n",
      " state (10)  A[0]:(2.85964915747e-06) A[1]:(0.999915003777) A[2]:(8.21432040539e-05) A[3]:(3.32585745571e-23)\n",
      " state (11)  A[0]:(2.1845773972e-06) A[1]:(0.999923706055) A[2]:(7.40879477235e-05) A[3]:(2.95957166693e-23)\n",
      " state (12)  A[0]:(1.95918187273e-06) A[1]:(0.999926388264) A[2]:(7.16596914572e-05) A[3]:(2.84047291845e-23)\n",
      " state (13)  A[0]:(1.8556664827e-06) A[1]:(0.999927461147) A[2]:(7.0684691309e-05) A[3]:(2.78832163955e-23)\n",
      " state (14)  A[0]:(1.79732126071e-06) A[1]:(0.999927997589) A[2]:(7.01762692188e-05) A[3]:(2.75937265314e-23)\n",
      " state (15)  A[0]:(1.75987963758e-06) A[1]:(0.999928414822) A[2]:(6.9852998422e-05) A[3]:(2.74044977328e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 377000 finished after 9 . Running score: 0.15. Policy_loss: -92050.6113242, Value_loss: 1.41898484611. Times trained:               12940. Times reached goal: 126.               Steps done: 4393560.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997347772121) A[1]:(0.000709549349267) A[2]:(0.00116546056233) A[3]:(0.00077724235598)\n",
      " state (1)  A[0]:(0.0468908399343) A[1]:(0.00644658971578) A[2]:(0.0147540820763) A[3]:(0.931908488274)\n",
      " state (2)  A[0]:(0.660482883453) A[1]:(0.0151199363172) A[2]:(0.0418238155544) A[3]:(0.282573401928)\n",
      " state (3)  A[0]:(0.999996244907) A[1]:(1.46911975207e-06) A[2]:(2.27678901865e-06) A[3]:(5.20170781448e-11)\n",
      " state (4)  A[0]:(0.999988913536) A[1]:(7.5596321949e-06) A[2]:(3.50527784576e-06) A[3]:(9.74475795196e-12)\n",
      " state (5)  A[0]:(0.998330175877) A[1]:(0.00157742586453) A[2]:(9.23713014345e-05) A[3]:(5.36211374781e-13)\n",
      " state (6)  A[0]:(0.311319291592) A[1]:(0.606835365295) A[2]:(0.0818453133106) A[3]:(6.56250290239e-17)\n",
      " state (7)  A[0]:(0.00839130952954) A[1]:(0.927318096161) A[2]:(0.0642905980349) A[3]:(5.09577378533e-20)\n",
      " state (8)  A[0]:(0.000254217506154) A[1]:(0.995760917664) A[2]:(0.00398485735059) A[3]:(1.5732602656e-21)\n",
      " state (9)  A[0]:(9.0748853836e-06) A[1]:(0.999718546867) A[2]:(0.000272351724561) A[3]:(1.1912284345e-22)\n",
      " state (10)  A[0]:(2.27085342885e-06) A[1]:(0.999897778034) A[2]:(9.99264666461e-05) A[3]:(4.53443369882e-23)\n",
      " state (11)  A[0]:(1.53235737343e-06) A[1]:(0.999921858311) A[2]:(7.66023440519e-05) A[3]:(3.50212558508e-23)\n",
      " state (12)  A[0]:(1.34961453568e-06) A[1]:(0.999927699566) A[2]:(7.09524247213e-05) A[3]:(3.24480483759e-23)\n",
      " state (13)  A[0]:(1.27943064854e-06) A[1]:(0.999929666519) A[2]:(6.90833985573e-05) A[3]:(3.15490435557e-23)\n",
      " state (14)  A[0]:(1.24142968616e-06) A[1]:(0.999930500984) A[2]:(6.82347963448e-05) A[3]:(3.11098689125e-23)\n",
      " state (15)  A[0]:(1.21498908356e-06) A[1]:(0.999931097031) A[2]:(6.76994386595e-05) A[3]:(3.08174318641e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 378000 finished after 13 . Running score: 0.1. Policy_loss: -92050.6112661, Value_loss: 1.01075232507. Times trained:               12732. Times reached goal: 124.               Steps done: 4406292.\n",
      " state (0)  A[0]:(0.997770905495) A[1]:(0.000595384917688) A[2]:(0.00102957652416) A[3]:(0.000604149827268)\n",
      " state (1)  A[0]:(0.0675952881575) A[1]:(0.00724797556177) A[2]:(0.0178180951625) A[3]:(0.907338619232)\n",
      " state (2)  A[0]:(0.742171287537) A[1]:(0.013706440106) A[2]:(0.0370247587562) A[3]:(0.207097515464)\n",
      " state (3)  A[0]:(0.999996185303) A[1]:(1.49307209085e-06) A[2]:(2.32905290432e-06) A[3]:(7.70843111564e-11)\n",
      " state (4)  A[0]:(0.999994874001) A[1]:(3.12757879328e-06) A[2]:(1.97090162146e-06) A[3]:(1.17547395428e-11)\n",
      " state (5)  A[0]:(0.999918222427) A[1]:(7.57971865823e-05) A[2]:(5.96691006649e-06) A[3]:(1.64684162574e-12)\n",
      " state (6)  A[0]:(0.978199362755) A[1]:(0.0215695444494) A[2]:(0.000231097335927) A[3]:(9.64851232222e-14)\n",
      " state (7)  A[0]:(0.0844498723745) A[1]:(0.89588791132) A[2]:(0.0196622330695) A[3]:(4.25607379421e-18)\n",
      " state (8)  A[0]:(0.000929406436626) A[1]:(0.996268808842) A[2]:(0.00280179874972) A[3]:(1.44435297208e-21)\n",
      " state (9)  A[0]:(4.31300577475e-05) A[1]:(0.999808371067) A[2]:(0.000148494684254) A[3]:(6.76246340944e-23)\n",
      " state (10)  A[0]:(1.33007879413e-05) A[1]:(0.999931931496) A[2]:(5.47695890418e-05) A[3]:(2.79533114201e-23)\n",
      " state (11)  A[0]:(8.01411442808e-06) A[1]:(0.999950528145) A[2]:(4.14606874983e-05) A[3]:(2.14893536082e-23)\n",
      " state (12)  A[0]:(5.86274973102e-06) A[1]:(0.999956488609) A[2]:(3.76464922738e-05) A[3]:(1.9276718676e-23)\n",
      " state (13)  A[0]:(4.60668252344e-06) A[1]:(0.999959349632) A[2]:(3.60278245353e-05) A[3]:(1.81150720836e-23)\n",
      " state (14)  A[0]:(3.79317839361e-06) A[1]:(0.999961078167) A[2]:(3.51173330273e-05) A[3]:(1.73510224316e-23)\n",
      " state (15)  A[0]:(3.26591680277e-06) A[1]:(0.999962210655) A[2]:(3.45167572959e-05) A[3]:(1.68127163717e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 379000 finished after 3 . Running score: 0.1. Policy_loss: -92050.6113976, Value_loss: 1.22506636932. Times trained:               12813. Times reached goal: 125.               Steps done: 4419105.\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9996e-01,  3.0138e-05,  5.6177e-06,  2.5335e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9996e-01,  3.0134e-05,  5.6175e-06,  2.5338e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9996e-01,  3.0130e-05,  5.6173e-06,  2.5340e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9977,  0.0006,  0.0010,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9996e-01,  3.0111e-05,  5.6162e-06,  2.5352e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.2436e-05,  9.9911e-01,  8.0294e-04,  1.1788e-22]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.997742891312) A[1]:(0.000601789914072) A[2]:(0.0010447986424) A[3]:(0.000610535265878)\n",
      " state (1)  A[0]:(0.077076561749) A[1]:(0.0076966136694) A[2]:(0.0205654539168) A[3]:(0.89466136694)\n",
      " state (2)  A[0]:(0.999649405479) A[1]:(0.000104022845335) A[2]:(0.000244800030487) A[3]:(1.79674498213e-06)\n",
      " state (3)  A[0]:(0.999996006489) A[1]:(1.82993267117e-06) A[2]:(2.16064609049e-06) A[3]:(1.92323171277e-11)\n",
      " state (4)  A[0]:(0.999964296818) A[1]:(3.01030995615e-05) A[2]:(5.61571050639e-06) A[3]:(2.53564161855e-12)\n",
      " state (5)  A[0]:(0.991782367229) A[1]:(0.00803174823523) A[2]:(0.000185873359442) A[3]:(1.13020493843e-13)\n",
      " state (6)  A[0]:(0.205587908626) A[1]:(0.748811841011) A[2]:(0.0456002540886) A[3]:(1.58609797962e-17)\n",
      " state (7)  A[0]:(0.00407609483227) A[1]:(0.970292270184) A[2]:(0.0256316103041) A[3]:(6.22649422055e-21)\n",
      " state (8)  A[0]:(8.24670569273e-05) A[1]:(0.999114274979) A[2]:(0.000803243834525) A[3]:(1.17923572925e-22)\n",
      " state (9)  A[0]:(7.87303360994e-06) A[1]:(0.999855518341) A[2]:(0.000136629358167) A[3]:(2.40918357113e-23)\n",
      " state (10)  A[0]:(3.21331913256e-06) A[1]:(0.999908983707) A[2]:(8.77909842529e-05) A[3]:(1.58363795169e-23)\n",
      " state (11)  A[0]:(2.23038273361e-06) A[1]:(0.999920547009) A[2]:(7.72378043621e-05) A[3]:(1.38816705462e-23)\n",
      " state (12)  A[0]:(1.85806084119e-06) A[1]:(0.999924659729) A[2]:(7.34586137696e-05) A[3]:(1.31442102398e-23)\n",
      " state (13)  A[0]:(1.67639927895e-06) A[1]:(0.999926686287) A[2]:(7.16147551429e-05) A[3]:(1.27788169682e-23)\n",
      " state (14)  A[0]:(1.57689078151e-06) A[1]:(0.99992787838) A[2]:(7.05361671862e-05) A[3]:(1.25653669845e-23)\n",
      " state (15)  A[0]:(1.51814333549e-06) A[1]:(0.99992865324) A[2]:(6.98336953064e-05) A[3]:(1.24261890439e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 380000 finished after 21 . Running score: 0.07. Policy_loss: -92050.6110983, Value_loss: 1.4238514955. Times trained:               12495. Times reached goal: 124.               Steps done: 4431600.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997414708138) A[1]:(0.000674401642755) A[2]:(0.00113025761675) A[3]:(0.000780609145295)\n",
      " state (1)  A[0]:(0.0397033654153) A[1]:(0.0060178944841) A[2]:(0.0132963145152) A[3]:(0.940982401371)\n",
      " state (2)  A[0]:(0.167909577489) A[1]:(0.010029557161) A[2]:(0.0306730158627) A[3]:(0.791387856007)\n",
      " state (3)  A[0]:(0.999884665012) A[1]:(3.16726582241e-05) A[2]:(8.3478174929e-05) A[3]:(1.96453214585e-07)\n",
      " state (4)  A[0]:(0.999997317791) A[1]:(8.7567434548e-07) A[2]:(1.82414328265e-06) A[3]:(3.28775201508e-11)\n",
      " state (5)  A[0]:(0.999993741512) A[1]:(2.60909814642e-06) A[2]:(3.65476989828e-06) A[3]:(1.12055321619e-11)\n",
      " state (6)  A[0]:(0.998369991779) A[1]:(0.000534882186912) A[2]:(0.00109514512587) A[3]:(2.52830542553e-14)\n",
      " state (7)  A[0]:(0.24024553597) A[1]:(0.358963727951) A[2]:(0.400790721178) A[3]:(1.75784979154e-18)\n",
      " state (8)  A[0]:(0.00325999921188) A[1]:(0.982866108418) A[2]:(0.0138738714159) A[3]:(3.06120218245e-21)\n",
      " state (9)  A[0]:(7.29083913029e-05) A[1]:(0.999537825584) A[2]:(0.000389259745134) A[3]:(8.81416503727e-23)\n",
      " state (10)  A[0]:(1.5155796973e-05) A[1]:(0.999860227108) A[2]:(0.000124598111142) A[3]:(3.04820429171e-23)\n",
      " state (11)  A[0]:(8.62914021127e-06) A[1]:(0.999901115894) A[2]:(9.02777173906e-05) A[3]:(2.23405392581e-23)\n",
      " state (12)  A[0]:(6.83073176333e-06) A[1]:(0.9999127388) A[2]:(8.04359297035e-05) A[3]:(1.99016463965e-23)\n",
      " state (13)  A[0]:(6.07833453614e-06) A[1]:(0.999917566776) A[2]:(7.63555653975e-05) A[3]:(1.88488452541e-23)\n",
      " state (14)  A[0]:(5.67338247492e-06) A[1]:(0.999920129776) A[2]:(7.42041374906e-05) A[3]:(1.82702993994e-23)\n",
      " state (15)  A[0]:(5.41309236723e-06) A[1]:(0.999921739101) A[2]:(7.28460217942e-05) A[3]:(1.78916082996e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 381000 finished after 7 . Running score: 0.09. Policy_loss: -92050.6113856, Value_loss: 1.86022951424. Times trained:               11972. Times reached goal: 114.               Steps done: 4443572.\n",
      " state (0)  A[0]:(0.997270524502) A[1]:(0.00070585636422) A[2]:(0.00117727019824) A[3]:(0.000846349401399)\n",
      " state (1)  A[0]:(0.0448290593922) A[1]:(0.00637438101694) A[2]:(0.0147378621623) A[3]:(0.934058666229)\n",
      " state (2)  A[0]:(0.918990910053) A[1]:(0.00788644701242) A[2]:(0.0234395749867) A[3]:(0.0496830902994)\n",
      " state (3)  A[0]:(0.999997198582) A[1]:(8.9063547648e-07) A[2]:(1.88542685464e-06) A[3]:(3.62412252986e-11)\n",
      " state (4)  A[0]:(0.999994933605) A[1]:(2.18342074731e-06) A[2]:(2.86891418e-06) A[3]:(1.31373930831e-11)\n",
      " state (5)  A[0]:(0.999604284763) A[1]:(0.000208436948014) A[2]:(0.000187283920241) A[3]:(1.05115406396e-13)\n",
      " state (6)  A[0]:(0.448940962553) A[1]:(0.224508911371) A[2]:(0.326550126076) A[3]:(5.76923983824e-18)\n",
      " state (7)  A[0]:(0.0194064900279) A[1]:(0.878973841667) A[2]:(0.101619638503) A[3]:(1.97633255437e-20)\n",
      " state (8)  A[0]:(0.000293965596939) A[1]:(0.997768759727) A[2]:(0.0019372664392) A[3]:(2.81833470466e-22)\n",
      " state (9)  A[0]:(1.47134469444e-05) A[1]:(0.999818205833) A[2]:(0.000167103018612) A[3]:(3.09977528452e-23)\n",
      " state (10)  A[0]:(5.56712484467e-06) A[1]:(0.999907553196) A[2]:(8.68925199029e-05) A[3]:(1.70960972969e-23)\n",
      " state (11)  A[0]:(4.13058978666e-06) A[1]:(0.999923288822) A[2]:(7.25822101231e-05) A[3]:(1.4474172928e-23)\n",
      " state (12)  A[0]:(3.67421193914e-06) A[1]:(0.999928236008) A[2]:(6.808454782e-05) A[3]:(1.36213306702e-23)\n",
      " state (13)  A[0]:(3.46261936102e-06) A[1]:(0.999930500984) A[2]:(6.60574587528e-05) A[3]:(1.32228912265e-23)\n",
      " state (14)  A[0]:(3.33633010996e-06) A[1]:(0.999931812286) A[2]:(6.48361819913e-05) A[3]:(1.29764573906e-23)\n",
      " state (15)  A[0]:(3.24737288793e-06) A[1]:(0.999932825565) A[2]:(6.39363352093e-05) A[3]:(1.27929920098e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 382000 finished after 23 . Running score: 0.17. Policy_loss: -92050.6113305, Value_loss: 1.41610026838. Times trained:               12631. Times reached goal: 132.               Steps done: 4456203.\n",
      " state (0)  A[0]:(0.996869921684) A[1]:(0.000792459060904) A[2]:(0.00127555045765) A[3]:(0.00106209795922)\n",
      " state (1)  A[0]:(0.0332073867321) A[1]:(0.00565958023071) A[2]:(0.0123562617227) A[3]:(0.948776781559)\n",
      " state (2)  A[0]:(0.950131654739) A[1]:(0.00592797668651) A[2]:(0.0178959704936) A[3]:(0.0260443966836)\n",
      " state (3)  A[0]:(0.999997317791) A[1]:(8.27708220186e-07) A[2]:(1.84605755749e-06) A[3]:(3.19186586883e-11)\n",
      " state (4)  A[0]:(0.999994337559) A[1]:(2.25383541874e-06) A[2]:(3.39270354743e-06) A[3]:(1.0341155883e-11)\n",
      " state (5)  A[0]:(0.999178111553) A[1]:(0.000272027275059) A[2]:(0.000549886841327) A[3]:(2.05689482916e-14)\n",
      " state (6)  A[0]:(0.427782297134) A[1]:(0.165215209126) A[2]:(0.407002478838) A[3]:(3.12865660459e-18)\n",
      " state (7)  A[0]:(0.0407242253423) A[1]:(0.786043345928) A[2]:(0.17323243618) A[3]:(3.88101931235e-20)\n",
      " state (8)  A[0]:(0.00101317791268) A[1]:(0.994608521461) A[2]:(0.00437832484022) A[3]:(7.38561518317e-22)\n",
      " state (9)  A[0]:(3.24719694618e-05) A[1]:(0.999754309654) A[2]:(0.000213191815419) A[3]:(5.21490984883e-23)\n",
      " state (10)  A[0]:(9.64322225627e-06) A[1]:(0.999902844429) A[2]:(8.75308614923e-05) A[3]:(2.40201124778e-23)\n",
      " state (11)  A[0]:(6.82281370246e-06) A[1]:(0.999923944473) A[2]:(6.92138346494e-05) A[3]:(1.9553074006e-23)\n",
      " state (12)  A[0]:(6.03182616032e-06) A[1]:(0.999929904938) A[2]:(6.40413563815e-05) A[3]:(1.82456127862e-23)\n",
      " state (13)  A[0]:(5.69255144001e-06) A[1]:(0.999932408333) A[2]:(6.1907878262e-05) A[3]:(1.76869967134e-23)\n",
      " state (14)  A[0]:(5.49740616407e-06) A[1]:(0.99993377924) A[2]:(6.0705056967e-05) A[3]:(1.73622668549e-23)\n",
      " state (15)  A[0]:(5.36137031304e-06) A[1]:(0.999934792519) A[2]:(5.98536935286e-05) A[3]:(1.71287466721e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 383000 finished after 26 . Running score: 0.1. Policy_loss: -92050.60935, Value_loss: 1.42611940274. Times trained:               12263. Times reached goal: 123.               Steps done: 4468466.\n",
      " state (0)  A[0]:(0.996826410294) A[1]:(0.000755145854782) A[2]:(0.00139110710006) A[3]:(0.00102736172266)\n",
      " state (1)  A[0]:(0.0360916294158) A[1]:(0.00591512024403) A[2]:(0.012600613758) A[3]:(0.945392668247)\n",
      " state (2)  A[0]:(0.896147727966) A[1]:(0.00940784439445) A[2]:(0.0260217729956) A[3]:(0.0684226825833)\n",
      " state (3)  A[0]:(0.999997079372) A[1]:(1.13427586257e-06) A[2]:(1.77034667104e-06) A[3]:(2.90790100332e-11)\n",
      " state (4)  A[0]:(0.999987542629) A[1]:(9.22205981624e-06) A[2]:(3.22989558299e-06) A[3]:(3.985118225e-12)\n",
      " state (5)  A[0]:(0.993962883949) A[1]:(0.00575370714068) A[2]:(0.000283379486063) A[3]:(2.84973223675e-14)\n",
      " state (6)  A[0]:(0.411933273077) A[1]:(0.52519261837) A[2]:(0.0628741383553) A[3]:(3.77295937103e-18)\n",
      " state (7)  A[0]:(0.0432907752693) A[1]:(0.90883398056) A[2]:(0.0478752180934) A[3]:(1.83823798228e-20)\n",
      " state (8)  A[0]:(0.0024597235024) A[1]:(0.995081067085) A[2]:(0.00245922408067) A[3]:(4.82397606951e-22)\n",
      " state (9)  A[0]:(0.000162832977367) A[1]:(0.999710559845) A[2]:(0.000126632934553) A[3]:(4.04883837792e-23)\n",
      " state (10)  A[0]:(4.46587291663e-05) A[1]:(0.999909043312) A[2]:(4.62839561806e-05) A[3]:(1.81041432047e-23)\n",
      " state (11)  A[0]:(2.42540154431e-05) A[1]:(0.999940812588) A[2]:(3.49355759681e-05) A[3]:(1.40798686931e-23)\n",
      " state (12)  A[0]:(1.72860509338e-05) A[1]:(0.999951124191) A[2]:(3.16138903145e-05) A[3]:(1.26841323604e-23)\n",
      " state (13)  A[0]:(1.40263173307e-05) A[1]:(0.999955773354) A[2]:(3.01786985801e-05) A[3]:(1.19964681098e-23)\n",
      " state (14)  A[0]:(1.22300607472e-05) A[1]:(0.999958395958) A[2]:(2.93876182695e-05) A[3]:(1.1580793633e-23)\n",
      " state (15)  A[0]:(1.11178933366e-05) A[1]:(0.999960005283) A[2]:(2.88796200039e-05) A[3]:(1.12942604196e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 384000 finished after 8 . Running score: 0.1. Policy_loss: -92050.6140302, Value_loss: 1.41691147354. Times trained:               12810. Times reached goal: 119.               Steps done: 4481276.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9973,  0.0007,  0.0013,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9973,  0.0007,  0.0013,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9973,  0.0007,  0.0013,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9973,  0.0007,  0.0013,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9973,  0.0007,  0.0013,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9973,  0.0007,  0.0013,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9973,  0.0007,  0.0013,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.0398e-06,  3.4316e-06,  2.7962e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.0371e-06,  3.4311e-06,  2.7970e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.0347e-06,  3.4306e-06,  2.7977e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0551e-03,  9.8839e-01,  7.5512e-03,  4.8655e-22]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0581e-03,  9.8839e-01,  7.5568e-03,  4.8688e-22]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0610e-03,  9.8838e-01,  7.5621e-03,  4.8719e-22]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3085e-04,  9.9960e-01,  2.7333e-04,  2.7411e-23]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.7037e-06,  9.9994e-01,  4.6531e-05,  6.1027e-24]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.997298836708) A[1]:(0.000707651139237) A[2]:(0.00127914838959) A[3]:(0.000714381341822)\n",
      " state (1)  A[0]:(0.0517959631979) A[1]:(0.00677378848195) A[2]:(0.0165166780353) A[3]:(0.924913585186)\n",
      " state (2)  A[0]:(0.996699988842) A[1]:(0.000811980222352) A[2]:(0.00225855293684) A[3]:(0.000229462864809)\n",
      " state (3)  A[0]:(0.999997317791) A[1]:(9.93789967652e-07) A[2]:(1.69984264176e-06) A[3]:(1.7869537447e-11)\n",
      " state (4)  A[0]:(0.999989509583) A[1]:(7.03295609128e-06) A[2]:(3.43021815752e-06) A[3]:(2.79834597335e-12)\n",
      " state (5)  A[0]:(0.997995197773) A[1]:(0.00180313573219) A[2]:(0.000201683229534) A[3]:(1.69300439282e-14)\n",
      " state (6)  A[0]:(0.531090557575) A[1]:(0.358455359936) A[2]:(0.11045409739) A[3]:(2.45078333338e-18)\n",
      " state (7)  A[0]:(0.0622110627592) A[1]:(0.820094645023) A[2]:(0.117694325745) A[3]:(1.47190569993e-20)\n",
      " state (8)  A[0]:(0.00405163923278) A[1]:(0.988401412964) A[2]:(0.00754696968943) A[3]:(4.86345041677e-22)\n",
      " state (9)  A[0]:(0.000130503438413) A[1]:(0.999596476555) A[2]:(0.000272999488516) A[3]:(2.73826557521e-23)\n",
      " state (10)  A[0]:(2.14299125219e-05) A[1]:(0.999900877476) A[2]:(7.76974266046e-05) A[3]:(9.47272142485e-24)\n",
      " state (11)  A[0]:(1.18440602819e-05) A[1]:(0.999933362007) A[2]:(5.47889467271e-05) A[3]:(7.03130341941e-24)\n",
      " state (12)  A[0]:(9.5632576631e-06) A[1]:(0.999941647053) A[2]:(4.87962715852e-05) A[3]:(6.36438463292e-24)\n",
      " state (13)  A[0]:(8.70032636158e-06) A[1]:(0.999944806099) A[2]:(4.65228586108e-05) A[3]:(6.10193810393e-24)\n",
      " state (14)  A[0]:(8.28181237011e-06) A[1]:(0.999946296215) A[2]:(4.54246437585e-05) A[3]:(5.96972067748e-24)\n",
      " state (15)  A[0]:(8.04450337455e-06) A[1]:(0.999947190285) A[2]:(4.47915081168e-05) A[3]:(5.89073085175e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 385000 finished after 15 . Running score: 0.14. Policy_loss: -92050.611369, Value_loss: 1.42271410605. Times trained:               13007. Times reached goal: 132.               Steps done: 4494283.\n",
      " state (0)  A[0]:(0.963807463646) A[1]:(0.00266184541397) A[2]:(0.00373917422257) A[3]:(0.029791502282)\n",
      " state (1)  A[0]:(0.0206632409245) A[1]:(0.00476185046136) A[2]:(0.00839372631162) A[3]:(0.966181159019)\n",
      " state (2)  A[0]:(0.999954760075) A[1]:(1.56794685608e-05) A[2]:(2.95258378173e-05) A[3]:(2.43280293688e-08)\n",
      " state (3)  A[0]:(0.999995410442) A[1]:(2.80776180261e-06) A[2]:(1.76440221367e-06) A[3]:(9.07487800184e-12)\n",
      " state (4)  A[0]:(0.999182581902) A[1]:(0.00078970938921) A[2]:(2.76888804365e-05) A[3]:(1.11024579287e-13)\n",
      " state (5)  A[0]:(0.299412220716) A[1]:(0.662928342819) A[2]:(0.0376594513655) A[3]:(2.48916244585e-18)\n",
      " state (6)  A[0]:(0.00906094443053) A[1]:(0.980298399925) A[2]:(0.0106406416744) A[3]:(4.58908939021e-21)\n",
      " state (7)  A[0]:(0.000213082545088) A[1]:(0.999586939812) A[2]:(0.000199969130335) A[3]:(1.2008849755e-22)\n",
      " state (8)  A[0]:(1.08088797788e-05) A[1]:(0.999968409538) A[2]:(2.08030323847e-05) A[3]:(1.9931855036e-23)\n",
      " state (9)  A[0]:(4.43524731963e-06) A[1]:(0.999982476234) A[2]:(1.30732787511e-05) A[3]:(1.34629053123e-23)\n",
      " state (10)  A[0]:(3.58779857379e-06) A[1]:(0.999984502792) A[2]:(1.18841307994e-05) A[3]:(1.23966753908e-23)\n",
      " state (11)  A[0]:(3.35122149409e-06) A[1]:(0.999985039234) A[2]:(1.16039391287e-05) A[3]:(1.21284216623e-23)\n",
      " state (12)  A[0]:(3.25501855514e-06) A[1]:(0.999985218048) A[2]:(1.15221182568e-05) A[3]:(1.20415704432e-23)\n",
      " state (13)  A[0]:(3.20502499562e-06) A[1]:(0.999985277653) A[2]:(1.14939357445e-05) A[3]:(1.20074443204e-23)\n",
      " state (14)  A[0]:(3.17410854223e-06) A[1]:(0.999985337257) A[2]:(1.14829254017e-05) A[3]:(1.19919731804e-23)\n",
      " state (15)  A[0]:(3.15238276016e-06) A[1]:(0.999985396862) A[2]:(1.14784152174e-05) A[3]:(1.19842447101e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 386000 finished after 21 . Running score: 0.09. Policy_loss: -92050.6119589, Value_loss: 0.997275948474. Times trained:               12960. Times reached goal: 100.               Steps done: 4507243.\n",
      " state (0)  A[0]:(0.993297874928) A[1]:(0.00124561076518) A[2]:(0.00193425803445) A[3]:(0.00352227198891)\n",
      " state (1)  A[0]:(0.0368358679116) A[1]:(0.00573031511158) A[2]:(0.0115421647206) A[3]:(0.945891678333)\n",
      " state (2)  A[0]:(0.999988079071) A[1]:(4.28708790423e-06) A[2]:(7.63072603149e-06) A[3]:(1.45208967073e-09)\n",
      " state (3)  A[0]:(0.999996602535) A[1]:(2.01703937819e-06) A[2]:(1.35133939239e-06) A[3]:(7.40156807372e-12)\n",
      " state (4)  A[0]:(0.999762773514) A[1]:(0.000228242672165) A[2]:(8.98145481187e-06) A[3]:(9.44773502053e-14)\n",
      " state (5)  A[0]:(0.616121649742) A[1]:(0.369941562414) A[2]:(0.0139367701486) A[3]:(5.82875672409e-18)\n",
      " state (6)  A[0]:(0.0197858698666) A[1]:(0.967188417912) A[2]:(0.0130257075652) A[3]:(5.05978908608e-21)\n",
      " state (7)  A[0]:(0.00083621009253) A[1]:(0.998674452305) A[2]:(0.000489343947265) A[3]:(1.55126265837e-22)\n",
      " state (8)  A[0]:(2.67887080554e-05) A[1]:(0.999950051308) A[2]:(2.3133958166e-05) A[3]:(1.37066428217e-23)\n",
      " state (9)  A[0]:(5.31832210982e-06) A[1]:(0.999985873699) A[2]:(8.82206222741e-06) A[3]:(6.32492383829e-24)\n",
      " state (10)  A[0]:(3.62513605978e-06) A[1]:(0.999989211559) A[2]:(7.19302352081e-06) A[3]:(5.37752358479e-24)\n",
      " state (11)  A[0]:(3.25976816384e-06) A[1]:(0.99998986721) A[2]:(6.86307930664e-06) A[3]:(5.18083728972e-24)\n",
      " state (12)  A[0]:(3.13057512358e-06) A[1]:(0.999990105629) A[2]:(6.78259948472e-06) A[3]:(5.13180209028e-24)\n",
      " state (13)  A[0]:(3.06719152832e-06) A[1]:(0.999990165234) A[2]:(6.76168792779e-06) A[3]:(5.11895667373e-24)\n",
      " state (14)  A[0]:(3.02844705402e-06) A[1]:(0.999990224838) A[2]:(6.75655701343e-06) A[3]:(5.11614596233e-24)\n",
      " state (15)  A[0]:(3.0013623018e-06) A[1]:(0.999990224838) A[2]:(6.75586761645e-06) A[3]:(5.11630215679e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 387000 finished after 3 . Running score: 0.13. Policy_loss: -92050.6116604, Value_loss: 1.41268772908. Times trained:               13028. Times reached goal: 145.               Steps done: 4520271.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996280372143) A[1]:(0.000978170661256) A[2]:(0.00149124779273) A[3]:(0.00125018402468)\n",
      " state (1)  A[0]:(0.0370906442404) A[1]:(0.00553175481036) A[2]:(0.0116680571809) A[3]:(0.945709526539)\n",
      " state (2)  A[0]:(0.999953508377) A[1]:(1.40003048728e-05) A[2]:(3.24648790411e-05) A[3]:(3.66850656519e-08)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(6.80094899508e-07) A[2]:(1.15088744224e-06) A[3]:(1.65449893369e-11)\n",
      " state (4)  A[0]:(0.999996066093) A[1]:(2.36093569583e-06) A[2]:(1.58487591762e-06) A[3]:(4.11593068655e-12)\n",
      " state (5)  A[0]:(0.99991106987) A[1]:(7.74945729063e-05) A[2]:(1.1429217011e-05) A[3]:(4.54170765395e-14)\n",
      " state (6)  A[0]:(0.938659608364) A[1]:(0.048965793103) A[2]:(0.0123746125028) A[3]:(5.55257145731e-18)\n",
      " state (7)  A[0]:(0.0827504247427) A[1]:(0.852093577385) A[2]:(0.0651559755206) A[3]:(2.09430741209e-20)\n",
      " state (8)  A[0]:(0.00408041290939) A[1]:(0.992215394974) A[2]:(0.00370419910178) A[3]:(6.86638163421e-22)\n",
      " state (9)  A[0]:(0.000132600835059) A[1]:(0.999725520611) A[2]:(0.000141898563015) A[3]:(4.4378641867e-23)\n",
      " state (10)  A[0]:(1.60197614605e-05) A[1]:(0.999955654144) A[2]:(2.83466652036e-05) A[3]:(1.20567962475e-23)\n",
      " state (11)  A[0]:(8.44769965624e-06) A[1]:(0.999973714352) A[2]:(1.78302725544e-05) A[3]:(8.36003232917e-24)\n",
      " state (12)  A[0]:(7.01339331499e-06) A[1]:(0.99997729063) A[2]:(1.56985242938e-05) A[3]:(7.57265053815e-24)\n",
      " state (13)  A[0]:(6.56652491671e-06) A[1]:(0.999978303909) A[2]:(1.51162566908e-05) A[3]:(7.3550038144e-24)\n",
      " state (14)  A[0]:(6.37708581053e-06) A[1]:(0.999978721142) A[2]:(1.49311372297e-05) A[3]:(7.28591932062e-24)\n",
      " state (15)  A[0]:(6.27533336228e-06) A[1]:(0.999978840351) A[2]:(1.48633425852e-05) A[3]:(7.26111516718e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 388000 finished after 3 . Running score: 0.14. Policy_loss: -92050.6117272, Value_loss: 1.43795964905. Times trained:               12505. Times reached goal: 117.               Steps done: 4532776.\n",
      " state (0)  A[0]:(0.997072517872) A[1]:(0.000810427882243) A[2]:(0.00129906414077) A[3]:(0.000817995460238)\n",
      " state (1)  A[0]:(0.0852526947856) A[1]:(0.00778832146898) A[2]:(0.0195009466261) A[3]:(0.887458026409)\n",
      " state (2)  A[0]:(0.999998211861) A[1]:(6.6259229925e-07) A[2]:(1.13902387966e-06) A[3]:(1.79182571558e-11)\n",
      " state (3)  A[0]:(0.999995827675) A[1]:(2.78479478766e-06) A[2]:(1.39151120493e-06) A[3]:(3.08546547687e-12)\n",
      " state (4)  A[0]:(0.999816536903) A[1]:(0.000176250949153) A[2]:(7.22956883692e-06) A[3]:(2.63550879525e-14)\n",
      " state (5)  A[0]:(0.946599781513) A[1]:(0.0507792122662) A[2]:(0.00262098759413) A[3]:(1.09237926643e-17)\n",
      " state (6)  A[0]:(0.151403576136) A[1]:(0.792348325253) A[2]:(0.0562481246889) A[3]:(2.77869419426e-20)\n",
      " state (7)  A[0]:(0.0174780488014) A[1]:(0.96794706583) A[2]:(0.0145748918876) A[3]:(1.46826204608e-21)\n",
      " state (8)  A[0]:(0.00151777837891) A[1]:(0.997249126434) A[2]:(0.00123311695643) A[3]:(1.51926115891e-22)\n",
      " state (9)  A[0]:(8.05562303867e-05) A[1]:(0.999830722809) A[2]:(8.86948764673e-05) A[3]:(1.76887748059e-23)\n",
      " state (10)  A[0]:(1.45548974615e-05) A[1]:(0.999964475632) A[2]:(2.09642712434e-05) A[3]:(5.65565779617e-24)\n",
      " state (11)  A[0]:(8.59175816004e-06) A[1]:(0.999978065491) A[2]:(1.33629919219e-05) A[3]:(4.0048290902e-24)\n",
      " state (12)  A[0]:(7.43073906051e-06) A[1]:(0.9999807477) A[2]:(1.18166071843e-05) A[3]:(3.65003889807e-24)\n",
      " state (13)  A[0]:(7.12287601345e-06) A[1]:(0.999981462955) A[2]:(1.14237882372e-05) A[3]:(3.55891165846e-24)\n",
      " state (14)  A[0]:(7.02824718246e-06) A[1]:(0.999981641769) A[2]:(1.13172518468e-05) A[3]:(3.53428894302e-24)\n",
      " state (15)  A[0]:(6.99520160197e-06) A[1]:(0.999981701374) A[2]:(1.12880625238e-05) A[3]:(3.52763529572e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 389000 finished after 9 . Running score: 0.12. Policy_loss: -92050.6116223, Value_loss: 1.23611064789. Times trained:               12546. Times reached goal: 128.               Steps done: 4545322.\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0009,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0009,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0009,  0.0014,  0.0011]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0009,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0009,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9990e-01,  9.1816e-05,  3.4291e-06,  1.0203e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.0850e-05,  9.9989e-01,  3.5886e-05,  9.7815e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996687889099) A[1]:(0.000873743731063) A[2]:(0.00140452326741) A[3]:(0.00103383557871)\n",
      " state (1)  A[0]:(0.0914867594838) A[1]:(0.00840244535357) A[2]:(0.0198828745633) A[3]:(0.880227923393)\n",
      " state (2)  A[0]:(0.999998211861) A[1]:(7.07060735294e-07) A[2]:(1.07525033854e-06) A[3]:(1.67575883725e-11)\n",
      " state (3)  A[0]:(0.999996423721) A[1]:(2.35313223129e-06) A[2]:(1.21683694942e-06) A[3]:(4.01451788479e-12)\n",
      " state (4)  A[0]:(0.999904811382) A[1]:(9.1782756499e-05) A[2]:(3.42850830748e-06) A[3]:(1.01622174759e-13)\n",
      " state (5)  A[0]:(0.906789362431) A[1]:(0.0916841104627) A[2]:(0.00152654189151) A[3]:(5.82795196007e-17)\n",
      " state (6)  A[0]:(0.0433733016253) A[1]:(0.938017070293) A[2]:(0.0186096336693) A[3]:(5.36466411074e-21)\n",
      " state (7)  A[0]:(0.00219645118341) A[1]:(0.996789693832) A[2]:(0.00101388350595) A[3]:(1.42431587984e-22)\n",
      " state (8)  A[0]:(7.08595034666e-05) A[1]:(0.999893248081) A[2]:(3.58868092007e-05) A[3]:(9.71683758741e-24)\n",
      " state (9)  A[0]:(1.01263076431e-05) A[1]:(0.999980986118) A[2]:(8.89050716069e-06) A[3]:(3.23486633404e-24)\n",
      " state (10)  A[0]:(6.08370100963e-06) A[1]:(0.999987483025) A[2]:(6.42113536742e-06) A[3]:(2.50883129738e-24)\n",
      " state (11)  A[0]:(5.31703881279e-06) A[1]:(0.999988734722) A[2]:(5.9591752688e-06) A[3]:(2.36747373406e-24)\n",
      " state (12)  A[0]:(5.07662889504e-06) A[1]:(0.99998909235) A[2]:(5.8527289184e-06) A[3]:(2.33435616996e-24)\n",
      " state (13)  A[0]:(4.97176324643e-06) A[1]:(0.999989211559) A[2]:(5.82777192903e-06) A[3]:(2.32659555359e-24)\n",
      " state (14)  A[0]:(4.91453874929e-06) A[1]:(0.999989271164) A[2]:(5.82311122344e-06) A[3]:(2.32530892146e-24)\n",
      " state (15)  A[0]:(4.87909892399e-06) A[1]:(0.999989271164) A[2]:(5.82348911848e-06) A[3]:(2.32567258633e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 390000 finished after 7 . Running score: 0.16. Policy_loss: -92050.6118201, Value_loss: 1.63738273393. Times trained:               12769. Times reached goal: 140.               Steps done: 4558091.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995404839516) A[1]:(0.000920045480598) A[2]:(0.00157006108202) A[3]:(0.0021050372161)\n",
      " state (1)  A[0]:(0.0605245381594) A[1]:(0.00658298283815) A[2]:(0.0145025057718) A[3]:(0.918389976025)\n",
      " state (2)  A[0]:(0.999998152256) A[1]:(6.77134664784e-07) A[2]:(1.1961619748e-06) A[3]:(3.81232025115e-11)\n",
      " state (3)  A[0]:(0.999998450279) A[1]:(7.83459825016e-07) A[2]:(7.95333107817e-07) A[3]:(7.63411556193e-12)\n",
      " state (4)  A[0]:(0.999989748001) A[1]:(9.11774259293e-06) A[2]:(1.12642521799e-06) A[3]:(4.78519784561e-13)\n",
      " state (5)  A[0]:(0.99640005827) A[1]:(0.0035602862481) A[2]:(3.96314026148e-05) A[3]:(1.08824103734e-15)\n",
      " state (6)  A[0]:(0.319282114506) A[1]:(0.665008425713) A[2]:(0.0157094281167) A[3]:(5.58878118523e-20)\n",
      " state (7)  A[0]:(0.0231502782553) A[1]:(0.974195241928) A[2]:(0.00265449751168) A[3]:(6.42765481161e-22)\n",
      " state (8)  A[0]:(0.00103787356056) A[1]:(0.998868942261) A[2]:(9.31891117943e-05) A[3]:(3.68450202221e-23)\n",
      " state (9)  A[0]:(7.40825562389e-05) A[1]:(0.999913871288) A[2]:(1.20568975035e-05) A[3]:(7.8564393045e-24)\n",
      " state (10)  A[0]:(3.16240839311e-05) A[1]:(0.999961614609) A[2]:(6.7389760261e-06) A[3]:(5.21302439238e-24)\n",
      " state (11)  A[0]:(2.54183105426e-05) A[1]:(0.999968707561) A[2]:(5.84996996622e-06) A[3]:(4.74750613372e-24)\n",
      " state (12)  A[0]:(2.36912292166e-05) A[1]:(0.999970674515) A[2]:(5.63879029869e-06) A[3]:(4.64238923481e-24)\n",
      " state (13)  A[0]:(2.29925190069e-05) A[1]:(0.999971449375) A[2]:(5.58103693038e-06) A[3]:(4.61995639725e-24)\n",
      " state (14)  A[0]:(2.26238880714e-05) A[1]:(0.999971807003) A[2]:(5.56365057491e-06) A[3]:(4.61860113421e-24)\n",
      " state (15)  A[0]:(2.23946772167e-05) A[1]:(0.999972045422) A[2]:(5.55788165002e-06) A[3]:(4.62239161086e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 391000 finished after 9 . Running score: 0.13. Policy_loss: -92050.6115895, Value_loss: 0.98893732999. Times trained:               12883. Times reached goal: 124.               Steps done: 4570974.\n",
      " state (0)  A[0]:(0.994934439659) A[1]:(0.000916294637136) A[2]:(0.00161706388462) A[3]:(0.00253219367005)\n",
      " state (1)  A[0]:(0.114004410803) A[1]:(0.00849633384496) A[2]:(0.0205685477704) A[3]:(0.856930732727)\n",
      " state (2)  A[0]:(0.999998807907) A[1]:(4.5990043418e-07) A[2]:(7.18930323274e-07) A[3]:(1.15450296892e-11)\n",
      " state (3)  A[0]:(0.999998092651) A[1]:(1.21755988403e-06) A[2]:(6.99722079389e-07) A[3]:(2.98585743803e-12)\n",
      " state (4)  A[0]:(0.999978363514) A[1]:(2.07465946005e-05) A[2]:(9.15179271033e-07) A[3]:(9.81010316926e-14)\n",
      " state (5)  A[0]:(0.9974822402) A[1]:(0.00250684353523) A[2]:(1.09223519758e-05) A[3]:(1.03461326386e-15)\n",
      " state (6)  A[0]:(0.659271121025) A[1]:(0.337020844221) A[2]:(0.0037080461625) A[3]:(3.41967425373e-19)\n",
      " state (7)  A[0]:(0.0843601971865) A[1]:(0.910465657711) A[2]:(0.00517417816445) A[3]:(1.47716766723e-21)\n",
      " state (8)  A[0]:(0.00449576834217) A[1]:(0.995300471783) A[2]:(0.000203764808248) A[3]:(5.03822709416e-23)\n",
      " state (9)  A[0]:(0.000214979183511) A[1]:(0.999769032001) A[2]:(1.60071213031e-05) A[3]:(7.15468047612e-24)\n",
      " state (10)  A[0]:(6.76251293044e-05) A[1]:(0.999924778938) A[2]:(7.57051748224e-06) A[3]:(4.20127044395e-24)\n",
      " state (11)  A[0]:(4.98349290865e-05) A[1]:(0.999943852425) A[2]:(6.30522754363e-06) A[3]:(3.72746165162e-24)\n",
      " state (12)  A[0]:(4.50837760582e-05) A[1]:(0.999948918819) A[2]:(6.01213878326e-06) A[3]:(3.62706292978e-24)\n",
      " state (13)  A[0]:(4.31690023106e-05) A[1]:(0.999950885773) A[2]:(5.93619370193e-06) A[3]:(3.61145492234e-24)\n",
      " state (14)  A[0]:(4.21509430453e-05) A[1]:(0.999951958656) A[2]:(5.91793150306e-06) A[3]:(3.61760843183e-24)\n",
      " state (15)  A[0]:(4.15047652496e-05) A[1]:(0.999952554703) A[2]:(5.91595971855e-06) A[3]:(3.62883352808e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 392000 finished after 24 . Running score: 0.09. Policy_loss: -92050.6123625, Value_loss: 1.4215909618. Times trained:               12597. Times reached goal: 138.               Steps done: 4583571.\n",
      " state (0)  A[0]:(0.9969368577) A[1]:(0.000771598308347) A[2]:(0.00131241464987) A[3]:(0.000979120144621)\n",
      " state (1)  A[0]:(0.12557014823) A[1]:(0.008814628236) A[2]:(0.0222720932215) A[3]:(0.843343138695)\n",
      " state (2)  A[0]:(0.999998807907) A[1]:(4.67874883725e-07) A[2]:(7.52725100028e-07) A[3]:(1.10284333749e-11)\n",
      " state (3)  A[0]:(0.999998092651) A[1]:(1.17172669434e-06) A[2]:(7.31644092866e-07) A[3]:(2.94356075972e-12)\n",
      " state (4)  A[0]:(0.999981999397) A[1]:(1.70661842276e-05) A[2]:(9.40206177802e-07) A[3]:(1.05462039384e-13)\n",
      " state (5)  A[0]:(0.998408198357) A[1]:(0.00158375070896) A[2]:(8.03691818874e-06) A[3]:(1.10701467558e-15)\n",
      " state (6)  A[0]:(0.721929013729) A[1]:(0.27549430728) A[2]:(0.00257672299631) A[3]:(5.08897797961e-19)\n",
      " state (7)  A[0]:(0.0879413336515) A[1]:(0.906407475471) A[2]:(0.00565116526559) A[3]:(1.22327017218e-21)\n",
      " state (8)  A[0]:(0.00548354908824) A[1]:(0.994251132011) A[2]:(0.000265330367256) A[3]:(3.67306890334e-23)\n",
      " state (9)  A[0]:(0.000271077995421) A[1]:(0.999710440636) A[2]:(1.84734963113e-05) A[3]:(4.562154981e-24)\n",
      " state (10)  A[0]:(6.54182513244e-05) A[1]:(0.999926686287) A[2]:(7.91516777099e-06) A[3]:(2.42784308976e-24)\n",
      " state (11)  A[0]:(4.38020433648e-05) A[1]:(0.999949812889) A[2]:(6.40106645733e-06) A[3]:(2.09111248756e-24)\n",
      " state (12)  A[0]:(3.83729384339e-05) A[1]:(0.99995559454) A[2]:(6.04702518103e-06) A[3]:(2.01413958195e-24)\n",
      " state (13)  A[0]:(3.6279532651e-05) A[1]:(0.999957740307) A[2]:(5.95057463215e-06) A[3]:(1.99600110594e-24)\n",
      " state (14)  A[0]:(3.52351344191e-05) A[1]:(0.99995881319) A[2]:(5.9242001953e-06) A[3]:(1.99385737643e-24)\n",
      " state (15)  A[0]:(3.46216656908e-05) A[1]:(0.999959468842) A[2]:(5.91857360632e-06) A[3]:(1.9962026599e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 393000 finished after 7 . Running score: 0.13. Policy_loss: -92050.611297, Value_loss: 1.4474297089. Times trained:               12453. Times reached goal: 132.               Steps done: 4596024.\n",
      " state (0)  A[0]:(0.997384607792) A[1]:(0.00066390982829) A[2]:(0.00125431967899) A[3]:(0.000697148730978)\n",
      " state (1)  A[0]:(0.135009393096) A[1]:(0.00874105934054) A[2]:(0.0265985317528) A[3]:(0.829650998116)\n",
      " state (2)  A[0]:(0.999998390675) A[1]:(4.60229728105e-07) A[2]:(1.15665886824e-06) A[3]:(1.17214883633e-11)\n",
      " state (3)  A[0]:(0.999997794628) A[1]:(9.40907227687e-07) A[2]:(1.23912150229e-06) A[3]:(3.87958198553e-12)\n",
      " state (4)  A[0]:(0.999987363815) A[1]:(1.04162209027e-05) A[2]:(2.20180118049e-06) A[3]:(1.6602011107e-13)\n",
      " state (5)  A[0]:(0.998780727386) A[1]:(0.00116818910465) A[2]:(5.10585232405e-05) A[3]:(4.30897968716e-16)\n",
      " state (6)  A[0]:(0.537923395634) A[1]:(0.39343008399) A[2]:(0.0686464831233) A[3]:(6.32298734317e-20)\n",
      " state (7)  A[0]:(0.0236279144883) A[1]:(0.951357722282) A[2]:(0.0250143818557) A[3]:(2.35171628481e-22)\n",
      " state (8)  A[0]:(0.000254188780673) A[1]:(0.999281287193) A[2]:(0.000464537210064) A[3]:(6.06168568738e-24)\n",
      " state (9)  A[0]:(3.61847487511e-05) A[1]:(0.999863266945) A[2]:(0.000100550299976) A[3]:(1.76584640089e-24)\n",
      " state (10)  A[0]:(2.45104547503e-05) A[1]:(0.999903142452) A[2]:(7.23279808881e-05) A[3]:(1.37500349046e-24)\n",
      " state (11)  A[0]:(2.22212038352e-05) A[1]:(0.999910950661) A[2]:(6.68205393595e-05) A[3]:(1.29759326009e-24)\n",
      " state (12)  A[0]:(2.15276359086e-05) A[1]:(0.999913036823) A[2]:(6.54131144984e-05) A[3]:(1.27871207153e-24)\n",
      " state (13)  A[0]:(2.12570084841e-05) A[1]:(0.999913692474) A[2]:(6.50209549349e-05) A[3]:(1.27421862121e-24)\n",
      " state (14)  A[0]:(2.11289152503e-05) A[1]:(0.999913930893) A[2]:(6.49135326967e-05) A[3]:(1.27358230628e-24)\n",
      " state (15)  A[0]:(2.10588732443e-05) A[1]:(0.999914050102) A[2]:(6.48902714602e-05) A[3]:(1.27393709648e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 394000 finished after 11 . Running score: 0.15. Policy_loss: -92050.6076296, Value_loss: 1.01210632248. Times trained:               12617. Times reached goal: 119.               Steps done: 4608641.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9978,  0.0006,  0.0011,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  3.6754e-06,  1.4043e-06,  5.2916e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9978,  0.0006,  0.0011,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9978,  0.0006,  0.0011,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9978,  0.0006,  0.0011,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9978,  0.0006,  0.0011,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.3140e-06,  1.3730e-06,  5.9640e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.2653e-06,  1.3686e-06,  6.0672e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.2222e-06,  1.3647e-06,  6.1614e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.1840e-06,  1.3612e-06,  6.2474e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.9147e-02,  9.7375e-01,  7.1061e-03,  8.2096e-23]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.4197e-04,  9.9899e-01,  2.6500e-04,  4.9911e-24]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.1679e-04,  9.9980e-01,  8.2670e-05,  2.0059e-24]])\n",
      "On state=10, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.3610e-04,  9.9900e-01,  2.6366e-04,  4.9715e-24]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.1606e-04,  9.9980e-01,  8.2469e-05,  2.0020e-24]])\n",
      "On state=10, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.4610e-05,  9.9990e-01,  5.3703e-05,  1.4501e-24]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.6767e-05,  9.9990e-01,  5.4219e-05,  1.4568e-24]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.997761070728) A[1]:(0.000588457274716) A[2]:(0.00112269155215) A[3]:(0.000527794880327)\n",
      " state (1)  A[0]:(0.119787253439) A[1]:(0.00782896671444) A[2]:(0.024082897231) A[3]:(0.848300874233)\n",
      " state (2)  A[0]:(0.999995827675) A[1]:(1.14849012789e-06) A[2]:(3.010834007e-06) A[3]:(1.32650196383e-10)\n",
      " state (3)  A[0]:(0.999998390675) A[1]:(5.83148050737e-07) A[2]:(1.0237506558e-06) A[3]:(5.98897754306e-12)\n",
      " state (4)  A[0]:(0.999995410442) A[1]:(3.22157507071e-06) A[2]:(1.36455082611e-06) A[3]:(6.15817076152e-13)\n",
      " state (5)  A[0]:(0.999856054783) A[1]:(0.000137652852573) A[2]:(6.27580175205e-06) A[3]:(7.80259053033e-15)\n",
      " state (6)  A[0]:(0.965119421482) A[1]:(0.033317565918) A[2]:(0.0015630312264) A[3]:(6.88009914674e-18)\n",
      " state (7)  A[0]:(0.279077112675) A[1]:(0.651110231876) A[2]:(0.0698126703501) A[3]:(5.99870491651e-21)\n",
      " state (8)  A[0]:(0.0186068359762) A[1]:(0.974479079247) A[2]:(0.00691406568512) A[3]:(7.99618001851e-23)\n",
      " state (9)  A[0]:(0.000716078502592) A[1]:(0.999023318291) A[2]:(0.000260601460468) A[3]:(4.92609162089e-24)\n",
      " state (10)  A[0]:(0.000114075155579) A[1]:(0.999803841114) A[2]:(8.20972636575e-05) A[3]:(1.99459811682e-24)\n",
      " state (11)  A[0]:(6.28856723779e-05) A[1]:(0.999876260757) A[2]:(6.0865033447e-05) A[3]:(1.58431649039e-24)\n",
      " state (12)  A[0]:(5.11331818416e-05) A[1]:(0.999893069267) A[2]:(5.57728199055e-05) A[3]:(1.48501221445e-24)\n",
      " state (13)  A[0]:(4.66760102427e-05) A[1]:(0.999899148941) A[2]:(5.41883018741e-05) A[3]:(1.4563260765e-24)\n",
      " state (14)  A[0]:(4.44244869868e-05) A[1]:(0.999901950359) A[2]:(5.3627161833e-05) A[3]:(1.44898227451e-24)\n",
      " state (15)  A[0]:(4.30567706644e-05) A[1]:(0.999903559685) A[2]:(5.34127721039e-05) A[3]:(1.4491725872e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 395000 finished after 17 . Running score: 0.15. Policy_loss: -92050.6113715, Value_loss: 1.21889609199. Times trained:               12795. Times reached goal: 118.               Steps done: 4621436.\n",
      " state (0)  A[0]:(0.996608376503) A[1]:(0.000659526500385) A[2]:(0.00139985082205) A[3]:(0.00133222050499)\n",
      " state (1)  A[0]:(0.146562427282) A[1]:(0.00922621507198) A[2]:(0.0265952162445) A[3]:(0.81761610508)\n",
      " state (2)  A[0]:(0.999998509884) A[1]:(5.56006170882e-07) A[2]:(9.19571220948e-07) A[3]:(9.13035966277e-12)\n",
      " state (3)  A[0]:(0.999996781349) A[1]:(2.25318331104e-06) A[2]:(9.81111384135e-07) A[3]:(1.21979520668e-12)\n",
      " state (4)  A[0]:(0.999878704548) A[1]:(0.000118753130664) A[2]:(2.5162773909e-06) A[3]:(8.41809295069e-15)\n",
      " state (5)  A[0]:(0.88658863306) A[1]:(0.111840777099) A[2]:(0.00157060171477) A[3]:(1.0090288256e-18)\n",
      " state (6)  A[0]:(0.0228436794132) A[1]:(0.972756326199) A[2]:(0.00439998321235) A[3]:(1.75164494033e-22)\n",
      " state (7)  A[0]:(0.000132957255119) A[1]:(0.999825358391) A[2]:(4.1661751311e-05) A[3]:(1.88572387342e-24)\n",
      " state (8)  A[0]:(1.29100080812e-05) A[1]:(0.999980568886) A[2]:(6.53599454381e-06) A[3]:(4.70296537068e-25)\n",
      " state (9)  A[0]:(9.08096899366e-06) A[1]:(0.999986171722) A[2]:(4.7642147365e-06) A[3]:(3.80164593393e-25)\n",
      " state (10)  A[0]:(8.44786882226e-06) A[1]:(0.999987065792) A[2]:(4.47474167231e-06) A[3]:(3.65673563691e-25)\n",
      " state (11)  A[0]:(8.27200074127e-06) A[1]:(0.999987304211) A[2]:(4.40878102381e-06) A[3]:(3.62952560561e-25)\n",
      " state (12)  A[0]:(8.20393506729e-06) A[1]:(0.99998742342) A[2]:(4.3911704779e-06) A[3]:(3.62650895221e-25)\n",
      " state (13)  A[0]:(8.17003365228e-06) A[1]:(0.99998742342) A[2]:(4.38585084339e-06) A[3]:(3.62851537062e-25)\n",
      " state (14)  A[0]:(8.15017483546e-06) A[1]:(0.999987483025) A[2]:(4.38404003944e-06) A[3]:(3.63116051984e-25)\n",
      " state (15)  A[0]:(8.13745282358e-06) A[1]:(0.999987483025) A[2]:(4.38334200226e-06) A[3]:(3.63339127057e-25)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 396000 finished after 3 . Running score: 0.14. Policy_loss: -92050.6114382, Value_loss: 1.21247536587. Times trained:               12643. Times reached goal: 138.               Steps done: 4634079.\n",
      " state (0)  A[0]:(0.996342897415) A[1]:(0.000738428381737) A[2]:(0.00152071693446) A[3]:(0.00139793928247)\n",
      " state (1)  A[0]:(0.109889686108) A[1]:(0.007765264716) A[2]:(0.0255518592894) A[3]:(0.856793165207)\n",
      " state (2)  A[0]:(0.999988377094) A[1]:(2.9519239888e-06) A[2]:(8.69600171427e-06) A[3]:(1.02846320349e-09)\n",
      " state (3)  A[0]:(0.999998271465) A[1]:(5.18958415796e-07) A[2]:(1.22216920317e-06) A[3]:(8.86559662489e-12)\n",
      " state (4)  A[0]:(0.999997079372) A[1]:(1.44698731219e-06) A[2]:(1.47713865317e-06) A[3]:(2.40648863684e-12)\n",
      " state (5)  A[0]:(0.999959468842) A[1]:(3.55684205715e-05) A[2]:(4.97900282426e-06) A[3]:(4.88199602543e-14)\n",
      " state (6)  A[0]:(0.975571632385) A[1]:(0.0219282470644) A[2]:(0.00250010215677) A[3]:(8.68372635415e-18)\n",
      " state (7)  A[0]:(0.176913708448) A[1]:(0.719474315643) A[2]:(0.103611953557) A[3]:(4.72908324654e-21)\n",
      " state (8)  A[0]:(0.0114033324644) A[1]:(0.978609144688) A[2]:(0.00998753029853) A[3]:(1.02192626246e-22)\n",
      " state (9)  A[0]:(0.000293664750643) A[1]:(0.999397575855) A[2]:(0.0003087563382) A[3]:(5.34453381616e-24)\n",
      " state (10)  A[0]:(3.11759358738e-05) A[1]:(0.999919831753) A[2]:(4.89728772664e-05) A[3]:(1.32792062316e-24)\n",
      " state (11)  A[0]:(1.65491856023e-05) A[1]:(0.999954938889) A[2]:(2.85263358819e-05) A[3]:(9.087857094e-25)\n",
      " state (12)  A[0]:(1.37568767968e-05) A[1]:(0.999961793423) A[2]:(2.44514212682e-05) A[3]:(8.20086041306e-25)\n",
      " state (13)  A[0]:(1.28356941786e-05) A[1]:(0.99996393919) A[2]:(2.32431029872e-05) A[3]:(7.94995876481e-25)\n",
      " state (14)  A[0]:(1.24082071125e-05) A[1]:(0.99996483326) A[2]:(2.27761356655e-05) A[3]:(7.86898416504e-25)\n",
      " state (15)  A[0]:(1.21557131934e-05) A[1]:(0.999965310097) A[2]:(2.25453004532e-05) A[3]:(7.84219740693e-25)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 397000 finished after 17 . Running score: 0.09. Policy_loss: -92050.6112532, Value_loss: 1.00361426115. Times trained:               12569. Times reached goal: 124.               Steps done: 4646648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995468139648) A[1]:(0.000741625437513) A[2]:(0.00167568260804) A[3]:(0.00211453763768)\n",
      " state (1)  A[0]:(0.123248808086) A[1]:(0.00823510717601) A[2]:(0.0273082591593) A[3]:(0.841207802296)\n",
      " state (2)  A[0]:(0.999996066093) A[1]:(1.06067875549e-06) A[2]:(2.87204079541e-06) A[3]:(8.08860270429e-11)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(6.12811675182e-07) A[2]:(1.24335861074e-06) A[3]:(7.71226658924e-12)\n",
      " state (4)  A[0]:(0.999996006489) A[1]:(2.44604575528e-06) A[2]:(1.57202043738e-06) A[3]:(1.40392374375e-12)\n",
      " state (5)  A[0]:(0.999840259552) A[1]:(0.000151827276568) A[2]:(7.93768958829e-06) A[3]:(1.20319835841e-14)\n",
      " state (6)  A[0]:(0.715145230293) A[1]:(0.269203424454) A[2]:(0.0156513862312) A[3]:(5.2837133564e-19)\n",
      " state (7)  A[0]:(0.0150319905952) A[1]:(0.971370756626) A[2]:(0.0135972481221) A[3]:(2.04093437067e-22)\n",
      " state (8)  A[0]:(0.00015084221377) A[1]:(0.999614953995) A[2]:(0.000234223305597) A[3]:(3.71839803425e-24)\n",
      " state (9)  A[0]:(1.05519711724e-05) A[1]:(0.999965786934) A[2]:(2.36504129134e-05) A[3]:(6.32346710802e-25)\n",
      " state (10)  A[0]:(5.84429153605e-06) A[1]:(0.999980807304) A[2]:(1.3327443412e-05) A[3]:(4.21116335135e-25)\n",
      " state (11)  A[0]:(5.06972537551e-06) A[1]:(0.999983370304) A[2]:(1.1588945199e-05) A[3]:(3.83057518896e-25)\n",
      " state (12)  A[0]:(4.8445772336e-06) A[1]:(0.999984025955) A[2]:(1.11243234642e-05) A[3]:(3.73286046774e-25)\n",
      " state (13)  A[0]:(4.75256138088e-06) A[1]:(0.999984264374) A[2]:(1.09613256427e-05) A[3]:(3.70323282429e-25)\n",
      " state (14)  A[0]:(4.70406939712e-06) A[1]:(0.999984383583) A[2]:(1.08886870294e-05) A[3]:(3.69337181646e-25)\n",
      " state (15)  A[0]:(4.6739037316e-06) A[1]:(0.999984502792) A[2]:(1.08492367872e-05) A[3]:(3.69007684307e-25)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 398000 finished after 5 . Running score: 0.07. Policy_loss: -92050.6113645, Value_loss: 1.00137289312. Times trained:               12522. Times reached goal: 116.               Steps done: 4659170.\n",
      " state (0)  A[0]:(0.987156331539) A[1]:(0.000818533124402) A[2]:(0.00224259356037) A[3]:(0.00978255923837)\n",
      " state (1)  A[0]:(0.102225035429) A[1]:(0.00736321741715) A[2]:(0.0241821426898) A[3]:(0.866229593754)\n",
      " state (2)  A[0]:(0.999983131886) A[1]:(4.4996336328e-06) A[2]:(1.23375884868e-05) A[3]:(2.36486075167e-09)\n",
      " state (3)  A[0]:(0.999998092651) A[1]:(6.65304185077e-07) A[2]:(1.25637700421e-06) A[3]:(8.37251101782e-12)\n",
      " state (4)  A[0]:(0.999993562698) A[1]:(4.68377174911e-06) A[2]:(1.73519845248e-06) A[3]:(8.26700741283e-13)\n",
      " state (5)  A[0]:(0.99877679348) A[1]:(0.00119463552255) A[2]:(2.85814476229e-05) A[3]:(1.40887191287e-15)\n",
      " state (6)  A[0]:(0.250874131918) A[1]:(0.716446816921) A[2]:(0.0326790362597) A[3]:(4.97681455037e-20)\n",
      " state (7)  A[0]:(0.00796657521278) A[1]:(0.986485004425) A[2]:(0.00554841710255) A[3]:(2.04284644328e-22)\n",
      " state (8)  A[0]:(0.00022545956017) A[1]:(0.999589622021) A[2]:(0.000184899210581) A[3]:(1.10676964121e-23)\n",
      " state (9)  A[0]:(1.6585127014e-05) A[1]:(0.99996137619) A[2]:(2.20371966861e-05) A[3]:(2.29811037795e-24)\n",
      " state (10)  A[0]:(7.95019514044e-06) A[1]:(0.999980211258) A[2]:(1.18185162137e-05) A[3]:(1.50378335691e-24)\n",
      " state (11)  A[0]:(6.63232640363e-06) A[1]:(0.999983251095) A[2]:(1.01394343801e-05) A[3]:(1.36378795915e-24)\n",
      " state (12)  A[0]:(6.2619151322e-06) A[1]:(0.999984025955) A[2]:(9.72711768554e-06) A[3]:(1.33234938689e-24)\n",
      " state (13)  A[0]:(6.10455208516e-06) A[1]:(0.999984323978) A[2]:(9.59816861723e-06) A[3]:(1.32590557658e-24)\n",
      " state (14)  A[0]:(6.01193914918e-06) A[1]:(0.999984443188) A[2]:(9.54517054197e-06) A[3]:(1.32600181761e-24)\n",
      " state (15)  A[0]:(5.94587845626e-06) A[1]:(0.999984562397) A[2]:(9.51617676037e-06) A[3]:(1.32786471264e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 399000 finished after 26 . Running score: 0.17. Policy_loss: -92050.6113508, Value_loss: 1.22604599659. Times trained:               12413. Times reached goal: 115.               Steps done: 4671583.\n",
      "action_dist \n",
      "tensor([[ 0.9928,  0.0008,  0.0019,  0.0045]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9928,  0.0008,  0.0019,  0.0044]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9929,  0.0008,  0.0019,  0.0044]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6457e-06,  1.7671e-06,  2.8936e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8587e-04,  9.9948e-01,  3.2965e-04,  8.7226e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8596e-04,  9.9948e-01,  3.2976e-04,  8.6983e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3177e-05,  9.9994e-01,  4.4682e-05,  1.8368e-24]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8603e-04,  9.9948e-01,  3.2982e-04,  8.6544e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993150949478) A[1]:(0.000782996125054) A[2]:(0.00190581509378) A[3]:(0.00416026823223)\n",
      " state (1)  A[0]:(0.0939777866006) A[1]:(0.00716293696314) A[2]:(0.0242002755404) A[3]:(0.874659001827)\n",
      " state (2)  A[0]:(0.999290406704) A[1]:(0.000161802527145) A[2]:(0.000539376982488) A[3]:(8.41918244987e-06)\n",
      " state (3)  A[0]:(0.999997973442) A[1]:(5.51661116788e-07) A[2]:(1.48898936914e-06) A[3]:(1.12938547403e-11)\n",
      " state (4)  A[0]:(0.999996602535) A[1]:(1.64486925769e-06) A[2]:(1.76687785824e-06) A[3]:(2.88937146778e-12)\n",
      " state (5)  A[0]:(0.999901294708) A[1]:(9.15751370485e-05) A[2]:(7.11127540853e-06) A[3]:(2.97115305449e-14)\n",
      " state (6)  A[0]:(0.733518898487) A[1]:(0.25186163187) A[2]:(0.0146194482222) A[3]:(1.44212001821e-18)\n",
      " state (7)  A[0]:(0.0116248959675) A[1]:(0.973378241062) A[2]:(0.0149968918413) A[3]:(3.53433995077e-22)\n",
      " state (8)  A[0]:(0.000186216668226) A[1]:(0.999483704567) A[2]:(0.000330049690092) A[3]:(8.63965196563e-24)\n",
      " state (9)  A[0]:(1.31857914312e-05) A[1]:(0.99994212389) A[2]:(4.46997910331e-05) A[3]:(1.82817765367e-24)\n",
      " state (10)  A[0]:(6.85196027916e-06) A[1]:(0.999965846539) A[2]:(2.72972647508e-05) A[3]:(1.27948722598e-24)\n",
      " state (11)  A[0]:(5.82410939387e-06) A[1]:(0.999969959259) A[2]:(2.42278129008e-05) A[3]:(1.17929488988e-24)\n",
      " state (12)  A[0]:(5.50552340428e-06) A[1]:(0.999971091747) A[2]:(2.3409980713e-05) A[3]:(1.15472759192e-24)\n",
      " state (13)  A[0]:(5.35728941031e-06) A[1]:(0.99997150898) A[2]:(2.31222129514e-05) A[3]:(1.14844184962e-24)\n",
      " state (14)  A[0]:(5.26634266862e-06) A[1]:(0.999971747398) A[2]:(2.2988053388e-05) A[3]:(1.14745242083e-24)\n",
      " state (15)  A[0]:(5.20120738656e-06) A[1]:(0.999971866608) A[2]:(2.29081324505e-05) A[3]:(1.14817501742e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 400000 finished after 8 . Running score: 0.15. Policy_loss: -92050.6115584, Value_loss: 1.22081502347. Times trained:               12905. Times reached goal: 128.               Steps done: 4684488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994697749615) A[1]:(0.00090195244411) A[2]:(0.00174226181116) A[3]:(0.00265802466311)\n",
      " state (1)  A[0]:(0.0892706438899) A[1]:(0.00681104743853) A[2]:(0.023919345811) A[3]:(0.879998981953)\n",
      " state (2)  A[0]:(0.985561549664) A[1]:(0.00222141528502) A[2]:(0.00850214995444) A[3]:(0.00371487415396)\n",
      " state (3)  A[0]:(0.999997675419) A[1]:(5.04885917962e-07) A[2]:(1.7992042558e-06) A[3]:(1.61207401594e-11)\n",
      " state (4)  A[0]:(0.99999755621) A[1]:(7.35417756914e-07) A[2]:(1.68688507074e-06) A[3]:(5.81237792271e-12)\n",
      " state (5)  A[0]:(0.999990820885) A[1]:(6.33894433122e-06) A[2]:(2.83613053398e-06) A[3]:(4.70326631374e-13)\n",
      " state (6)  A[0]:(0.998608291149) A[1]:(0.00133290805388) A[2]:(5.87855938647e-05) A[3]:(2.0841703065e-15)\n",
      " state (7)  A[0]:(0.45282959938) A[1]:(0.480515360832) A[2]:(0.0666550472379) A[3]:(2.03444063531e-19)\n",
      " state (8)  A[0]:(0.0407319702208) A[1]:(0.887688696384) A[2]:(0.0715793371201) A[3]:(9.00558156794e-22)\n",
      " state (9)  A[0]:(0.00610911939293) A[1]:(0.983584523201) A[2]:(0.0103063723072) A[3]:(8.26579058781e-23)\n",
      " state (10)  A[0]:(0.000697325274814) A[1]:(0.998310089111) A[2]:(0.000992582179606) A[3]:(1.23911296987e-23)\n",
      " state (11)  A[0]:(0.000116490853543) A[1]:(0.999656200409) A[2]:(0.000227280266699) A[3]:(4.10103067759e-24)\n",
      " state (12)  A[0]:(4.89013436891e-05) A[1]:(0.999819993973) A[2]:(0.000131094624521) A[3]:(2.69459403828e-24)\n",
      " state (13)  A[0]:(3.48485446011e-05) A[1]:(0.999855577946) A[2]:(0.000109600252472) A[3]:(2.33073470676e-24)\n",
      " state (14)  A[0]:(3.01195741486e-05) A[1]:(0.999866724014) A[2]:(0.0001031444408) A[3]:(2.2028508877e-24)\n",
      " state (15)  A[0]:(2.79221385426e-05) A[1]:(0.999871253967) A[2]:(0.000100812656456) A[3]:(2.14503744132e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 401000 finished after 9 . Running score: 0.19. Policy_loss: -92050.6097111, Value_loss: 1.44699887736. Times trained:               13064. Times reached goal: 147.               Steps done: 4697552.\n",
      " state (0)  A[0]:(0.993351519108) A[1]:(0.000889326503966) A[2]:(0.00185230083298) A[3]:(0.00390688050538)\n",
      " state (1)  A[0]:(0.0821824446321) A[1]:(0.00603403942659) A[2]:(0.0231174845248) A[3]:(0.888666033745)\n",
      " state (2)  A[0]:(0.961025357246) A[1]:(0.00385653972626) A[2]:(0.0171339754015) A[3]:(0.0179841332138)\n",
      " state (3)  A[0]:(0.99999755621) A[1]:(4.1064632228e-07) A[2]:(2.03283980227e-06) A[3]:(1.96998840851e-11)\n",
      " state (4)  A[0]:(0.999997615814) A[1]:(4.78477488741e-07) A[2]:(1.9061956209e-06) A[3]:(9.31146132421e-12)\n",
      " state (5)  A[0]:(0.999993383884) A[1]:(2.44140392169e-06) A[2]:(4.19503476223e-06) A[3]:(1.2531125226e-12)\n",
      " state (6)  A[0]:(0.998022913933) A[1]:(0.000702195742633) A[2]:(0.00127491448075) A[3]:(3.21573755554e-16)\n",
      " state (7)  A[0]:(0.334314882755) A[1]:(0.108000062406) A[2]:(0.55768507719) A[3]:(1.09239630118e-19)\n",
      " state (8)  A[0]:(0.10019377619) A[1]:(0.265374839306) A[2]:(0.634431362152) A[3]:(6.07665336064e-21)\n",
      " state (9)  A[0]:(0.0675286650658) A[1]:(0.445988535881) A[2]:(0.486482828856) A[3]:(2.82806912201e-21)\n",
      " state (10)  A[0]:(0.049242220819) A[1]:(0.587609350681) A[2]:(0.363148421049) A[3]:(1.89676977518e-21)\n",
      " state (11)  A[0]:(0.0320679582655) A[1]:(0.726541101933) A[2]:(0.241390958428) A[3]:(1.24747469667e-21)\n",
      " state (12)  A[0]:(0.0137637266889) A[1]:(0.877346396446) A[2]:(0.108889855444) A[3]:(6.14063767934e-22)\n",
      " state (13)  A[0]:(0.00309522333555) A[1]:(0.968846738338) A[2]:(0.0280580185354) A[3]:(1.99769065667e-22)\n",
      " state (14)  A[0]:(0.00063969619805) A[1]:(0.992596268654) A[2]:(0.00676404638216) A[3]:(6.57880712438e-23)\n",
      " state (15)  A[0]:(0.000239125831285) A[1]:(0.997047364712) A[2]:(0.00271348957904) A[3]:(3.35592315362e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 402000 finished after 11 . Running score: 0.12. Policy_loss: -92050.5693951, Value_loss: 1.20817122321. Times trained:               14004. Times reached goal: 100.               Steps done: 4711556.\n",
      " state (0)  A[0]:(0.995857656002) A[1]:(0.000878856691997) A[2]:(0.00152925646398) A[3]:(0.00173420296051)\n",
      " state (1)  A[0]:(0.0956566780806) A[1]:(0.00685291970149) A[2]:(0.0251971967518) A[3]:(0.872293174267)\n",
      " state (2)  A[0]:(0.999996066093) A[1]:(8.68621782502e-07) A[2]:(3.08270205096e-06) A[3]:(5.49599463162e-11)\n",
      " state (3)  A[0]:(0.999996483326) A[1]:(1.33391029067e-06) A[2]:(2.21187497118e-06) A[3]:(5.08313833733e-12)\n",
      " state (4)  A[0]:(0.999846875668) A[1]:(0.000139642317663) A[2]:(1.35071895784e-05) A[3]:(2.6912029056e-14)\n",
      " state (5)  A[0]:(0.266598582268) A[1]:(0.562422633171) A[2]:(0.170978814363) A[3]:(1.19900088284e-19)\n",
      " state (6)  A[0]:(0.00541357044131) A[1]:(0.923136532307) A[2]:(0.0714498981833) A[3]:(8.71259080224e-22)\n",
      " state (7)  A[0]:(0.000830327975564) A[1]:(0.976471960545) A[2]:(0.0226976983249) A[3]:(2.56041379179e-22)\n",
      " state (8)  A[0]:(0.000352157338057) A[1]:(0.987420499325) A[2]:(0.0122273312882) A[3]:(1.50623435169e-22)\n",
      " state (9)  A[0]:(0.000150873995153) A[1]:(0.994257450104) A[2]:(0.0055916630663) A[3]:(8.15023003826e-23)\n",
      " state (10)  A[0]:(4.26086007792e-05) A[1]:(0.998558342457) A[2]:(0.00139902695082) A[3]:(2.87015522995e-23)\n",
      " state (11)  A[0]:(1.20307013276e-05) A[1]:(0.99968534708) A[2]:(0.000302641245071) A[3]:(9.32377896391e-24)\n",
      " state (12)  A[0]:(6.14571536062e-06) A[1]:(0.999865055084) A[2]:(0.00012881787552) A[3]:(5.02658106173e-24)\n",
      " state (13)  A[0]:(4.79950858789e-06) A[1]:(0.999901652336) A[2]:(9.35703064897e-05) A[3]:(3.99882940858e-24)\n",
      " state (14)  A[0]:(4.41528891315e-06) A[1]:(0.999911606312) A[2]:(8.40070424601e-05) A[3]:(3.7052548227e-24)\n",
      " state (15)  A[0]:(4.29207057095e-06) A[1]:(0.999914646149) A[2]:(8.1034253526e-05) A[3]:(3.61391498507e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 403000 finished after 12 . Running score: 0.16. Policy_loss: -92050.6268161, Value_loss: 1.41746529155. Times trained:               13097. Times reached goal: 123.               Steps done: 4724653.\n",
      " state (0)  A[0]:(0.99524307251) A[1]:(0.000934736104682) A[2]:(0.00151723006275) A[3]:(0.00230495701544)\n",
      " state (1)  A[0]:(0.0863914266229) A[1]:(0.00669147260487) A[2]:(0.0220757238567) A[3]:(0.884841382504)\n",
      " state (2)  A[0]:(0.130125075579) A[1]:(0.00768398307264) A[2]:(0.0281907096505) A[3]:(0.834000229836)\n",
      " state (3)  A[0]:(0.999948382378) A[1]:(1.17217659863e-05) A[2]:(3.98552729166e-05) A[3]:(2.91679569386e-08)\n",
      " state (4)  A[0]:(0.999998092651) A[1]:(5.40063240351e-07) A[2]:(1.38113205139e-06) A[3]:(1.06711705444e-11)\n",
      " state (5)  A[0]:(0.999979496002) A[1]:(1.51986405399e-05) A[2]:(5.30958186573e-06) A[3]:(3.46776020442e-13)\n",
      " state (6)  A[0]:(0.63034594059) A[1]:(0.299315661192) A[2]:(0.0703383758664) A[3]:(1.03804280597e-18)\n",
      " state (7)  A[0]:(0.0274454969913) A[1]:(0.928296148777) A[2]:(0.0442583672702) A[3]:(2.77270416262e-21)\n",
      " state (8)  A[0]:(0.00259266234934) A[1]:(0.990278601646) A[2]:(0.00712871691212) A[3]:(2.38224356101e-22)\n",
      " state (9)  A[0]:(0.0001603395649) A[1]:(0.999118864536) A[2]:(0.000720767769963) A[3]:(2.82542145237e-23)\n",
      " state (10)  A[0]:(1.7790684069e-05) A[1]:(0.99987667799) A[2]:(0.000105520623038) A[3]:(5.87205378095e-24)\n",
      " state (11)  A[0]:(7.82259940024e-06) A[1]:(0.999944388866) A[2]:(4.7775945859e-05) A[3]:(3.20673575137e-24)\n",
      " state (12)  A[0]:(6.12299209024e-06) A[1]:(0.999956429005) A[2]:(3.74474693672e-05) A[3]:(2.68151787992e-24)\n",
      " state (13)  A[0]:(5.65046366319e-06) A[1]:(0.999959647655) A[2]:(3.47087516275e-05) A[3]:(2.54219084456e-24)\n",
      " state (14)  A[0]:(5.47526542505e-06) A[1]:(0.999960720539) A[2]:(3.38161516993e-05) A[3]:(2.50037569456e-24)\n",
      " state (15)  A[0]:(5.39276697964e-06) A[1]:(0.999961137772) A[2]:(3.34719516104e-05) A[3]:(2.48742890938e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 404000 finished after 4 . Running score: 0.12. Policy_loss: -92050.6464085, Value_loss: 1.21914646189. Times trained:               12781. Times reached goal: 123.               Steps done: 4737434.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9961,  0.0009,  0.0014,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.5909e-07,  1.4174e-06,  6.1813e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.5849e-07,  1.4172e-06,  6.1847e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.5795e-07,  1.4170e-06,  6.1877e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0009,  0.0014,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.5703e-07,  1.4166e-06,  6.1929e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0009,  0.0014,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0009,  0.0014,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.5596e-07,  1.4162e-06,  6.1989e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4802e-04,  9.9892e-01,  8.2701e-04,  4.2988e-23]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8277e-05,  9.9989e-01,  8.7944e-05,  6.6579e-24]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4844e-04,  9.9892e-01,  8.2781e-04,  4.3025e-23]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8286e-05,  9.9989e-01,  8.7971e-05,  6.6596e-24]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4839e-04,  9.9892e-01,  8.2773e-04,  4.3022e-23]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996138751507) A[1]:(0.000907576002646) A[2]:(0.00139842345379) A[3]:(0.00155523512512)\n",
      " state (1)  A[0]:(0.0791318267584) A[1]:(0.00649679498747) A[2]:(0.0211199130863) A[3]:(0.893251478672)\n",
      " state (2)  A[0]:(0.152179166675) A[1]:(0.00843998137861) A[2]:(0.0303193442523) A[3]:(0.809061527252)\n",
      " state (3)  A[0]:(0.999996900558) A[1]:(7.60510033615e-07) A[2]:(2.33319155996e-06) A[3]:(5.32513720619e-11)\n",
      " state (4)  A[0]:(0.999997735023) A[1]:(8.55091229823e-07) A[2]:(1.41589055147e-06) A[3]:(6.20377750907e-12)\n",
      " state (5)  A[0]:(0.999590992928) A[1]:(0.000376519921701) A[2]:(3.24828688463e-05) A[3]:(1.52814806436e-14)\n",
      " state (6)  A[0]:(0.108730979264) A[1]:(0.82829195261) A[2]:(0.062977053225) A[3]:(2.65337003066e-20)\n",
      " state (7)  A[0]:(0.00437457859516) A[1]:(0.986775636673) A[2]:(0.0088497614488) A[3]:(4.29377268391e-22)\n",
      " state (8)  A[0]:(0.000249033997534) A[1]:(0.998921990395) A[2]:(0.000828949559946) A[3]:(4.30761820254e-23)\n",
      " state (9)  A[0]:(1.83195661521e-05) A[1]:(0.999893605709) A[2]:(8.80681109265e-05) A[3]:(6.66535163434e-24)\n",
      " state (10)  A[0]:(6.58510680296e-06) A[1]:(0.999960243702) A[2]:(3.31984592776e-05) A[3]:(3.11424812112e-24)\n",
      " state (11)  A[0]:(5.02074226461e-06) A[1]:(0.999969661236) A[2]:(2.53088583122e-05) A[3]:(2.5376140708e-24)\n",
      " state (12)  A[0]:(4.64793902211e-06) A[1]:(0.999971807003) A[2]:(2.35193783737e-05) A[3]:(2.40386230989e-24)\n",
      " state (13)  A[0]:(4.52762105851e-06) A[1]:(0.999972462654) A[2]:(2.30328350881e-05) A[3]:(2.36844679398e-24)\n",
      " state (14)  A[0]:(4.47781076218e-06) A[1]:(0.999972641468) A[2]:(2.28895441978e-05) A[3]:(2.35900669274e-24)\n",
      " state (15)  A[0]:(4.45234672952e-06) A[1]:(0.999972701073) A[2]:(2.28475837503e-05) A[3]:(2.35701896048e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 405000 finished after 14 . Running score: 0.1. Policy_loss: -92050.6244444, Value_loss: 1.20621491781. Times trained:               12736. Times reached goal: 116.               Steps done: 4750170.\n",
      " state (0)  A[0]:(0.996514499187) A[1]:(0.000878622813616) A[2]:(0.00133604102302) A[3]:(0.00127084425185)\n",
      " state (1)  A[0]:(0.0853461250663) A[1]:(0.00667204521596) A[2]:(0.0218634083867) A[3]:(0.886118412018)\n",
      " state (2)  A[0]:(0.984484910965) A[1]:(0.00246762600727) A[2]:(0.00829724036157) A[3]:(0.00475023547187)\n",
      " state (3)  A[0]:(0.999998450279) A[1]:(4.61427362097e-07) A[2]:(1.06762797714e-06) A[3]:(7.45968998378e-12)\n",
      " state (4)  A[0]:(0.999991357327) A[1]:(7.03418982084e-06) A[2]:(1.61751336236e-06) A[3]:(1.84015170181e-13)\n",
      " state (5)  A[0]:(0.807418167591) A[1]:(0.178082838655) A[2]:(0.014498972334) A[3]:(1.74352586677e-19)\n",
      " state (6)  A[0]:(0.0246170572937) A[1]:(0.964136481285) A[2]:(0.0112464595586) A[3]:(3.49575215694e-22)\n",
      " state (7)  A[0]:(0.00279977405444) A[1]:(0.995564937592) A[2]:(0.00163529429119) A[3]:(5.00149205062e-23)\n",
      " state (8)  A[0]:(0.000208966244827) A[1]:(0.999608457088) A[2]:(0.000182560033863) A[3]:(8.22882819455e-24)\n",
      " state (9)  A[0]:(2.7153726478e-05) A[1]:(0.999938845634) A[2]:(3.39715952578e-05) A[3]:(2.21582922731e-24)\n",
      " state (10)  A[0]:(1.4362118236e-05) A[1]:(0.999966263771) A[2]:(1.93503183255e-05) A[3]:(1.4608529548e-24)\n",
      " state (11)  A[0]:(1.22111132441e-05) A[1]:(0.999971032143) A[2]:(1.67826456163e-05) A[3]:(1.3210564486e-24)\n",
      " state (12)  A[0]:(1.16168557724e-05) A[1]:(0.999972224236) A[2]:(1.6173815311e-05) A[3]:(1.29015055466e-24)\n",
      " state (13)  A[0]:(1.13758851512e-05) A[1]:(0.999972641468) A[2]:(1.60005802172e-05) A[3]:(1.28383256767e-24)\n",
      " state (14)  A[0]:(1.1239709238e-05) A[1]:(0.999972820282) A[2]:(1.59372320923e-05) A[3]:(1.28346555013e-24)\n",
      " state (15)  A[0]:(1.11457347884e-05) A[1]:(0.999972939491) A[2]:(1.59048613568e-05) A[3]:(1.28447460184e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 406000 finished after 8 . Running score: 0.2. Policy_loss: -92050.6244135, Value_loss: 1.85349518552. Times trained:               12649. Times reached goal: 136.               Steps done: 4762819.\n",
      " state (0)  A[0]:(0.996946036816) A[1]:(0.000829576165415) A[2]:(0.00125445891172) A[3]:(0.000969937071204)\n",
      " state (1)  A[0]:(0.0877182111144) A[1]:(0.00652103172615) A[2]:(0.0230207964778) A[3]:(0.882739961147)\n",
      " state (2)  A[0]:(0.938896656036) A[1]:(0.00543514499441) A[2]:(0.0208891462535) A[3]:(0.0347790569067)\n",
      " state (3)  A[0]:(0.999998390675) A[1]:(3.45946915559e-07) A[2]:(1.2459250911e-06) A[3]:(9.83205791089e-12)\n",
      " state (4)  A[0]:(0.999997913837) A[1]:(7.74741749865e-07) A[2]:(1.33541152536e-06) A[3]:(2.36958803274e-12)\n",
      " state (5)  A[0]:(0.999892890453) A[1]:(8.42828812893e-05) A[2]:(2.28302105825e-05) A[3]:(5.58832369231e-16)\n",
      " state (6)  A[0]:(0.584985494614) A[1]:(0.233556434512) A[2]:(0.181458026171) A[3]:(5.67921084777e-21)\n",
      " state (7)  A[0]:(0.0688540041447) A[1]:(0.842501342297) A[2]:(0.0886446535587) A[3]:(5.85825728214e-22)\n",
      " state (8)  A[0]:(0.00514876935631) A[1]:(0.984635055065) A[2]:(0.0102161606774) A[3]:(8.994426065e-23)\n",
      " state (9)  A[0]:(0.000244210503297) A[1]:(0.999052643776) A[2]:(0.000703153549694) A[3]:(1.1405445631e-23)\n",
      " state (10)  A[0]:(5.72177341382e-05) A[1]:(0.999778330326) A[2]:(0.000164426717674) A[3]:(3.9464817822e-24)\n",
      " state (11)  A[0]:(3.78276308766e-05) A[1]:(0.99985653162) A[2]:(0.000105637314846) A[3]:(2.89038773724e-24)\n",
      " state (12)  A[0]:(3.35552322213e-05) A[1]:(0.999873042107) A[2]:(9.34107156354e-05) A[3]:(2.66154471066e-24)\n",
      " state (13)  A[0]:(3.21750740113e-05) A[1]:(0.999877750874) A[2]:(9.00614904822e-05) A[3]:(2.60520387038e-24)\n",
      " state (14)  A[0]:(3.15544893965e-05) A[1]:(0.999879479408) A[2]:(8.89451912371e-05) A[3]:(2.5927065387e-24)\n",
      " state (15)  A[0]:(3.11823787342e-05) A[1]:(0.999880373478) A[2]:(8.84698310983e-05) A[3]:(2.5920563201e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 407000 finished after 28 . Running score: 0.12. Policy_loss: -92050.6153379, Value_loss: 1.20113775769. Times trained:               12636. Times reached goal: 127.               Steps done: 4775455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997241914272) A[1]:(0.000828671327326) A[2]:(0.00117174140178) A[3]:(0.000757687259465)\n",
      " state (1)  A[0]:(0.0968841612339) A[1]:(0.00697226217017) A[2]:(0.0248076934367) A[3]:(0.871335864067)\n",
      " state (2)  A[0]:(0.99910736084) A[1]:(0.000183203650522) A[2]:(0.000695447844919) A[3]:(1.4001298041e-05)\n",
      " state (3)  A[0]:(0.99999833107) A[1]:(4.017691424e-07) A[2]:(1.2683570958e-06) A[3]:(6.31553013011e-12)\n",
      " state (4)  A[0]:(0.99999666214) A[1]:(1.74594856617e-06) A[2]:(1.61358855166e-06) A[3]:(5.32606510978e-13)\n",
      " state (5)  A[0]:(0.999693870544) A[1]:(0.000248236552579) A[2]:(5.79006664339e-05) A[3]:(5.69482088007e-17)\n",
      " state (6)  A[0]:(0.460079580545) A[1]:(0.278024375439) A[2]:(0.261896073818) A[3]:(1.42863674239e-21)\n",
      " state (7)  A[0]:(0.0671410188079) A[1]:(0.840026319027) A[2]:(0.092832647264) A[3]:(2.01454249266e-22)\n",
      " state (8)  A[0]:(0.00657618558034) A[1]:(0.984935164452) A[2]:(0.00848867092282) A[3]:(3.11565442346e-23)\n",
      " state (9)  A[0]:(0.00032785863732) A[1]:(0.999087035656) A[2]:(0.000585120404139) A[3]:(4.01731991377e-24)\n",
      " state (10)  A[0]:(7.27995211491e-05) A[1]:(0.999756217003) A[2]:(0.000170969360624) A[3]:(1.5532375401e-24)\n",
      " state (11)  A[0]:(4.7446192184e-05) A[1]:(0.999831438065) A[2]:(0.00012112136028) A[3]:(1.19691912718e-24)\n",
      " state (12)  A[0]:(4.14678252127e-05) A[1]:(0.99984806776) A[2]:(0.000110494002001) A[3]:(1.1191992689e-24)\n",
      " state (13)  A[0]:(3.91692992707e-05) A[1]:(0.99985319376) A[2]:(0.000107659659989) A[3]:(1.10011041272e-24)\n",
      " state (14)  A[0]:(3.79147713829e-05) A[1]:(0.999855339527) A[2]:(0.000106763007352) A[3]:(1.09582808129e-24)\n",
      " state (15)  A[0]:(3.70710804418e-05) A[1]:(0.99985653162) A[2]:(0.000106401079393) A[3]:(1.09554093593e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 408000 finished after 11 . Running score: 0.09. Policy_loss: -92050.6156668, Value_loss: 1.62981282775. Times trained:               12396. Times reached goal: 117.               Steps done: 4787851.\n",
      " state (0)  A[0]:(0.996116280556) A[1]:(0.000952708243858) A[2]:(0.00139996712096) A[3]:(0.001531043672)\n",
      " state (1)  A[0]:(0.070162512362) A[1]:(0.0064031179063) A[2]:(0.0199800450355) A[3]:(0.903454303741)\n",
      " state (2)  A[0]:(0.999967157841) A[1]:(7.86428427091e-06) A[2]:(2.49505392276e-05) A[3]:(9.17166254055e-09)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(6.11131440564e-07) A[2]:(1.26387953969e-06) A[3]:(5.4037512312e-12)\n",
      " state (4)  A[0]:(0.999984800816) A[1]:(1.272533882e-05) A[2]:(2.456692755e-06) A[3]:(6.93554304157e-14)\n",
      " state (5)  A[0]:(0.853049635887) A[1]:(0.122777253389) A[2]:(0.024173092097) A[3]:(9.23889529141e-20)\n",
      " state (6)  A[0]:(0.0201670769602) A[1]:(0.958460211754) A[2]:(0.0213727112859) A[3]:(2.00850688632e-22)\n",
      " state (7)  A[0]:(0.00105002964847) A[1]:(0.997840702534) A[2]:(0.00110924476758) A[3]:(1.72479782647e-23)\n",
      " state (8)  A[0]:(4.45273508376e-05) A[1]:(0.999884009361) A[2]:(7.14610723662e-05) A[3]:(2.02515858829e-24)\n",
      " state (9)  A[0]:(1.12073303171e-05) A[1]:(0.999964177608) A[2]:(2.46006620728e-05) A[3]:(8.63025860441e-25)\n",
      " state (10)  A[0]:(8.15823386802e-06) A[1]:(0.999972581863) A[2]:(1.92839361262e-05) A[3]:(7.12043454511e-25)\n",
      " state (11)  A[0]:(7.4813037827e-06) A[1]:(0.999974250793) A[2]:(1.82504536497e-05) A[3]:(6.81451970949e-25)\n",
      " state (12)  A[0]:(7.2281136454e-06) A[1]:(0.999974787235) A[2]:(1.80073111551e-05) A[3]:(6.73718767495e-25)\n",
      " state (13)  A[0]:(7.08883817424e-06) A[1]:(0.999974966049) A[2]:(1.79446396942e-05) A[3]:(6.7126126856e-25)\n",
      " state (14)  A[0]:(6.99395422998e-06) A[1]:(0.999975085258) A[2]:(1.79299986485e-05) A[3]:(6.70235305649e-25)\n",
      " state (15)  A[0]:(6.9229095061e-06) A[1]:(0.999975144863) A[2]:(1.79313337867e-05) A[3]:(6.69706325108e-25)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 409000 finished after 7 . Running score: 0.14. Policy_loss: -92050.6112777, Value_loss: 1.64165916392. Times trained:               12901. Times reached goal: 133.               Steps done: 4800752.\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0010,  0.0015,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0010,  0.0015,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0010,  0.0015,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0010,  0.0015,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0010,  0.0015,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0010,  0.0015,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0010,  0.0015,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0010,  0.0015,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0010,  0.0015,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1230e-06,  1.4925e-06,  1.9068e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2434e-03,  9.9754e-01,  1.2161e-03,  1.4066e-23]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995524585247) A[1]:(0.000989817432128) A[2]:(0.00150527723599) A[3]:(0.00198034662753)\n",
      " state (1)  A[0]:(0.0522410161793) A[1]:(0.00591874821112) A[2]:(0.0168103612959) A[3]:(0.925029873848)\n",
      " state (2)  A[0]:(0.815192401409) A[1]:(0.0104108443484) A[2]:(0.0362382680178) A[3]:(0.138158515096)\n",
      " state (3)  A[0]:(0.999998092651) A[1]:(4.90580475798e-07) A[2]:(1.43401643982e-06) A[3]:(1.26643539405e-11)\n",
      " state (4)  A[0]:(0.999997377396) A[1]:(1.12272516617e-06) A[2]:(1.49242191583e-06) A[3]:(1.90728769583e-12)\n",
      " state (5)  A[0]:(0.995391249657) A[1]:(0.00247998232953) A[2]:(0.00212878012098) A[3]:(4.60270548426e-18)\n",
      " state (6)  A[0]:(0.166776001453) A[1]:(0.7228987813) A[2]:(0.110325187445) A[3]:(1.42901791998e-21)\n",
      " state (7)  A[0]:(0.0168399009854) A[1]:(0.968199670315) A[2]:(0.01496041473) A[3]:(1.1344303231e-22)\n",
      " state (8)  A[0]:(0.00124513427727) A[1]:(0.997537732124) A[2]:(0.00121715874411) A[3]:(1.40753706083e-23)\n",
      " state (9)  A[0]:(5.34741921001e-05) A[1]:(0.999864518642) A[2]:(8.202164463e-05) A[3]:(1.63781575509e-24)\n",
      " state (10)  A[0]:(1.35391446747e-05) A[1]:(0.999962985516) A[2]:(2.34464332607e-05) A[3]:(6.196366332e-25)\n",
      " state (11)  A[0]:(9.45195097302e-06) A[1]:(0.999974131584) A[2]:(1.6398180378e-05) A[3]:(4.73282868633e-25)\n",
      " state (12)  A[0]:(8.49864136399e-06) A[1]:(0.999976754189) A[2]:(1.47731925608e-05) A[3]:(4.37889933867e-25)\n",
      " state (13)  A[0]:(8.16292867967e-06) A[1]:(0.999977588654) A[2]:(1.4242823454e-05) A[3]:(4.2620897462e-25)\n",
      " state (14)  A[0]:(8.00454199634e-06) A[1]:(0.999978005886) A[2]:(1.40133515742e-05) A[3]:(4.21179394704e-25)\n",
      " state (15)  A[0]:(7.91353704699e-06) A[1]:(0.9999781847) A[2]:(1.38890081871e-05) A[3]:(4.18480948066e-25)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 410000 finished after 11 . Running score: 0.11. Policy_loss: -92050.6057439, Value_loss: 1.42138563129. Times trained:               12964. Times reached goal: 120.               Steps done: 4813716.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996243774891) A[1]:(0.000922478211578) A[2]:(0.00135837157723) A[3]:(0.00147538387682)\n",
      " state (1)  A[0]:(0.0772770866752) A[1]:(0.00700638489798) A[2]:(0.0215475596488) A[3]:(0.894168972969)\n",
      " state (2)  A[0]:(0.999241828918) A[1]:(0.000176836532773) A[2]:(0.000572223274503) A[3]:(9.10413109523e-06)\n",
      " state (3)  A[0]:(0.999998390675) A[1]:(4.44386415666e-07) A[2]:(1.15870250283e-06) A[3]:(6.46635956586e-12)\n",
      " state (4)  A[0]:(0.999996423721) A[1]:(1.89368029169e-06) A[2]:(1.71145359218e-06) A[3]:(6.79952188304e-13)\n",
      " state (5)  A[0]:(0.980286836624) A[1]:(0.0103723034263) A[2]:(0.00934086646885) A[3]:(8.24384289718e-20)\n",
      " state (6)  A[0]:(0.0396389774978) A[1]:(0.932149708271) A[2]:(0.0282113142312) A[3]:(1.155146694e-22)\n",
      " state (7)  A[0]:(0.00107737921644) A[1]:(0.998012065887) A[2]:(0.000910565780941) A[3]:(4.92974838562e-24)\n",
      " state (8)  A[0]:(2.8644430131e-05) A[1]:(0.9999319911) A[2]:(3.93874106521e-05) A[3]:(3.66670289444e-25)\n",
      " state (9)  A[0]:(7.04365720594e-06) A[1]:(0.999982118607) A[2]:(1.08439980977e-05) A[3]:(1.29378993979e-25)\n",
      " state (10)  A[0]:(5.22721256857e-06) A[1]:(0.999986767769) A[2]:(8.01318219601e-06) A[3]:(1.01971229771e-25)\n",
      " state (11)  A[0]:(4.85864211441e-06) A[1]:(0.999987721443) A[2]:(7.44948056308e-06) A[3]:(9.62947715861e-26)\n",
      " state (12)  A[0]:(4.74833814224e-06) A[1]:(0.999987959862) A[2]:(7.29748626327e-06) A[3]:(9.47266886699e-26)\n",
      " state (13)  A[0]:(4.70422173748e-06) A[1]:(0.999988079071) A[2]:(7.24571236788e-06) A[3]:(9.41765198182e-26)\n",
      " state (14)  A[0]:(4.68223606731e-06) A[1]:(0.999988079071) A[2]:(7.22385448171e-06) A[3]:(9.39368416885e-26)\n",
      " state (15)  A[0]:(4.66969686386e-06) A[1]:(0.999988138676) A[2]:(7.21299193174e-06) A[3]:(9.38136561278e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 411000 finished after 8 . Running score: 0.15. Policy_loss: -92050.6029835, Value_loss: 1.21731033815. Times trained:               12730. Times reached goal: 143.               Steps done: 4826446.\n",
      " state (0)  A[0]:(0.995747387409) A[1]:(0.00091204396449) A[2]:(0.00150962907355) A[3]:(0.00183094956446)\n",
      " state (1)  A[0]:(0.054758887738) A[1]:(0.00601846026257) A[2]:(0.0175868365914) A[3]:(0.921635806561)\n",
      " state (2)  A[0]:(0.208392471075) A[1]:(0.0099058393389) A[2]:(0.0364161506295) A[3]:(0.745285511017)\n",
      " state (3)  A[0]:(0.999967753887) A[1]:(7.08215111445e-06) A[2]:(2.51516157732e-05) A[3]:(7.44491712723e-09)\n",
      " state (4)  A[0]:(0.999998390675) A[1]:(4.00185257377e-07) A[2]:(1.21625578231e-06) A[3]:(5.91843631395e-12)\n",
      " state (5)  A[0]:(0.999995172024) A[1]:(1.91895696844e-06) A[2]:(2.93103403237e-06) A[3]:(3.37109815974e-13)\n",
      " state (6)  A[0]:(0.964476585388) A[1]:(0.00589659810066) A[2]:(0.0296268239617) A[3]:(7.16761306559e-20)\n",
      " state (7)  A[0]:(0.255622923374) A[1]:(0.54956561327) A[2]:(0.194811448455) A[3]:(7.17198660133e-22)\n",
      " state (8)  A[0]:(0.0232629291713) A[1]:(0.95731908083) A[2]:(0.0194180086255) A[3]:(5.11318718173e-23)\n",
      " state (9)  A[0]:(0.000820194021799) A[1]:(0.998496949673) A[2]:(0.000682878657244) A[3]:(3.14581892619e-24)\n",
      " state (10)  A[0]:(6.60971709294e-05) A[1]:(0.999850511551) A[2]:(8.33892263472e-05) A[3]:(5.52201055933e-25)\n",
      " state (11)  A[0]:(2.83051431325e-05) A[1]:(0.999929845333) A[2]:(4.18357194576e-05) A[3]:(3.12761679897e-25)\n",
      " state (12)  A[0]:(2.16540847759e-05) A[1]:(0.999944627285) A[2]:(3.37344390573e-05) A[3]:(2.62191751568e-25)\n",
      " state (13)  A[0]:(1.93980704353e-05) A[1]:(0.999949514866) A[2]:(3.10781870212e-05) A[3]:(2.45035826468e-25)\n",
      " state (14)  A[0]:(1.83000975085e-05) A[1]:(0.999951839447) A[2]:(2.98607155855e-05) A[3]:(2.3701774562e-25)\n",
      " state (15)  A[0]:(1.76444646058e-05) A[1]:(0.999953210354) A[2]:(2.91617470793e-05) A[3]:(2.3237253818e-25)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 412000 finished after 15 . Running score: 0.17. Policy_loss: -92050.6033967, Value_loss: 1.40826610753. Times trained:               12492. Times reached goal: 121.               Steps done: 4838938.\n",
      " state (0)  A[0]:(0.995354056358) A[1]:(0.000912819930818) A[2]:(0.00154221383855) A[3]:(0.00219091726467)\n",
      " state (1)  A[0]:(0.0817277282476) A[1]:(0.00705204717815) A[2]:(0.0225314497948) A[3]:(0.888688743114)\n",
      " state (2)  A[0]:(0.974514722824) A[1]:(0.0035393172875) A[2]:(0.0126336254179) A[3]:(0.00931231956929)\n",
      " state (3)  A[0]:(0.999998390675) A[1]:(3.95464155645e-07) A[2]:(1.21802008834e-06) A[3]:(7.40273901206e-12)\n",
      " state (4)  A[0]:(0.999998033047) A[1]:(6.9011201731e-07) A[2]:(1.28047645376e-06) A[3]:(2.09652976403e-12)\n",
      " state (5)  A[0]:(0.999760627747) A[1]:(0.000109630607767) A[2]:(0.000129741383716) A[3]:(4.51928638743e-17)\n",
      " state (6)  A[0]:(0.35466787219) A[1]:(0.484911262989) A[2]:(0.160420894623) A[3]:(4.98863620532e-22)\n",
      " state (7)  A[0]:(0.0102970320731) A[1]:(0.981552243233) A[2]:(0.00815072003752) A[3]:(1.44219235148e-23)\n",
      " state (8)  A[0]:(0.000158259834279) A[1]:(0.99965852499) A[2]:(0.000183229902177) A[3]:(6.0180433383e-25)\n",
      " state (9)  A[0]:(1.77323418029e-05) A[1]:(0.999956488609) A[2]:(2.58075924648e-05) A[3]:(1.21131280652e-25)\n",
      " state (10)  A[0]:(1.07557834781e-05) A[1]:(0.999973595142) A[2]:(1.5627259927e-05) A[3]:(8.13272637341e-26)\n",
      " state (11)  A[0]:(9.49726290855e-06) A[1]:(0.999976754189) A[2]:(1.3777197637e-05) A[3]:(7.36554127046e-26)\n",
      " state (12)  A[0]:(9.1226065706e-06) A[1]:(0.999977588654) A[2]:(1.32652412503e-05) A[3]:(7.1494260462e-26)\n",
      " state (13)  A[0]:(8.97037443792e-06) A[1]:(0.999977946281) A[2]:(1.30792559503e-05) A[3]:(7.07031439076e-26)\n",
      " state (14)  A[0]:(8.89367856871e-06) A[1]:(0.999978125095) A[2]:(1.29953823489e-05) A[3]:(7.03453461833e-26)\n",
      " state (15)  A[0]:(8.84973451321e-06) A[1]:(0.9999781847) A[2]:(1.29515601657e-05) A[3]:(7.01582998671e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 413000 finished after 9 . Running score: 0.17. Policy_loss: -92050.6113151, Value_loss: 1.6335753354. Times trained:               12911. Times reached goal: 129.               Steps done: 4851849.\n",
      " state (0)  A[0]:(0.994374930859) A[1]:(0.00101584498771) A[2]:(0.00167240318842) A[3]:(0.00293680559844)\n",
      " state (1)  A[0]:(0.0717703551054) A[1]:(0.00690710684285) A[2]:(0.0210941825062) A[3]:(0.900228381157)\n",
      " state (2)  A[0]:(0.99818944931) A[1]:(0.000407766259741) A[2]:(0.00134732434526) A[3]:(5.54649050173e-05)\n",
      " state (3)  A[0]:(0.999998271465) A[1]:(5.01936597175e-07) A[2]:(1.24261191559e-06) A[3]:(5.38723492899e-12)\n",
      " state (4)  A[0]:(0.999992370605) A[1]:(5.27809106643e-06) A[2]:(2.3385566692e-06) A[3]:(1.39566526738e-13)\n",
      " state (5)  A[0]:(0.8598305583) A[1]:(0.0936769917607) A[2]:(0.0464924611151) A[3]:(1.38221864202e-20)\n",
      " state (6)  A[0]:(0.0500178001821) A[1]:(0.891839325428) A[2]:(0.058142863214) A[3]:(9.5432902393e-23)\n",
      " state (7)  A[0]:(0.0054621649906) A[1]:(0.986275732517) A[2]:(0.00826210062951) A[3]:(1.56616278937e-23)\n",
      " state (8)  A[0]:(0.000267987110419) A[1]:(0.999090254307) A[2]:(0.000641730846837) A[3]:(2.01504677199e-24)\n",
      " state (9)  A[0]:(1.83973752428e-05) A[1]:(0.999929964542) A[2]:(5.16085201525e-05) A[3]:(2.75443258921e-25)\n",
      " state (10)  A[0]:(7.33265596864e-06) A[1]:(0.99997407198) A[2]:(1.85995122592e-05) A[3]:(1.25677572331e-25)\n",
      " state (11)  A[0]:(5.80468258704e-06) A[1]:(0.999980032444) A[2]:(1.41368718687e-05) A[3]:(1.02067057882e-25)\n",
      " state (12)  A[0]:(5.44541489944e-06) A[1]:(0.999981403351) A[2]:(1.31230308398e-05) A[3]:(9.65081892759e-26)\n",
      " state (13)  A[0]:(5.33665206603e-06) A[1]:(0.999981820583) A[2]:(1.28320561998e-05) A[3]:(9.49091312431e-26)\n",
      " state (14)  A[0]:(5.29610224476e-06) A[1]:(0.999981999397) A[2]:(1.27314942802e-05) A[3]:(9.43640221947e-26)\n",
      " state (15)  A[0]:(5.27775000592e-06) A[1]:(0.999982059002) A[2]:(1.26895292851e-05) A[3]:(9.41411135221e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 414000 finished after 4 . Running score: 0.19. Policy_loss: -92050.612442, Value_loss: 1.19779848908. Times trained:               12657. Times reached goal: 115.               Steps done: 4864506.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9962,  0.0009,  0.0014,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9998e-01,  2.1611e-05,  2.7356e-06,  1.0749e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0009,  0.0014,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9998e-01,  2.1613e-05,  2.7355e-06,  1.0749e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9998e-01,  2.1614e-05,  2.7354e-06,  1.0749e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3367e-05,  9.9996e-01,  2.1058e-05,  1.2313e-25]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2761e-06,  9.9999e-01,  6.4341e-06,  4.6717e-26]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3354e-05,  9.9996e-01,  2.1046e-05,  1.2309e-25]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3348e-05,  9.9996e-01,  2.1041e-05,  1.2307e-25]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996226966381) A[1]:(0.000937678501941) A[2]:(0.00138824223541) A[3]:(0.00144709064625)\n",
      " state (1)  A[0]:(0.0895718038082) A[1]:(0.00752159347758) A[2]:(0.0230910014361) A[3]:(0.879815578461)\n",
      " state (2)  A[0]:(0.999991476536) A[1]:(2.30453929362e-06) A[2]:(6.22865582045e-06) A[3]:(3.7578054557e-10)\n",
      " state (3)  A[0]:(0.99999833107) A[1]:(6.06077946941e-07) A[2]:(1.04508774257e-06) A[3]:(3.22194939999e-12)\n",
      " state (4)  A[0]:(0.999975681305) A[1]:(2.16121334233e-05) A[2]:(2.73475734502e-06) A[3]:(1.07542377713e-14)\n",
      " state (5)  A[0]:(0.568288624287) A[1]:(0.379941999912) A[2]:(0.0517693981528) A[3]:(1.24240387353e-21)\n",
      " state (6)  A[0]:(0.0115214018151) A[1]:(0.982183635235) A[2]:(0.00629496667534) A[3]:(1.33426560891e-23)\n",
      " state (7)  A[0]:(0.000594297249336) A[1]:(0.999054610729) A[2]:(0.000351108901668) A[3]:(1.2381788205e-24)\n",
      " state (8)  A[0]:(2.33495520661e-05) A[1]:(0.99995559454) A[2]:(2.10415582842e-05) A[3]:(1.23077215616e-25)\n",
      " state (9)  A[0]:(6.27383087703e-06) A[1]:(0.999987304211) A[2]:(6.43130124445e-06) A[3]:(4.67077701078e-26)\n",
      " state (10)  A[0]:(4.72082501801e-06) A[1]:(0.999990403652) A[2]:(4.87832267027e-06) A[3]:(3.74176418054e-26)\n",
      " state (11)  A[0]:(4.41193515144e-06) A[1]:(0.999990999699) A[2]:(4.58483464172e-06) A[3]:(3.55892810127e-26)\n",
      " state (12)  A[0]:(4.32227898273e-06) A[1]:(0.999991178513) A[2]:(4.51990536021e-06) A[3]:(3.51600359097e-26)\n",
      " state (13)  A[0]:(4.28486555393e-06) A[1]:(0.999991238117) A[2]:(4.50651987194e-06) A[3]:(3.50510313564e-26)\n",
      " state (14)  A[0]:(4.26322185376e-06) A[1]:(0.999991238117) A[2]:(4.50585821454e-06) A[3]:(3.50237632698e-26)\n",
      " state (15)  A[0]:(4.24805921284e-06) A[1]:(0.999991238117) A[2]:(4.50842844657e-06) A[3]:(3.50197573355e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 415000 finished after 9 . Running score: 0.12. Policy_loss: -92050.6113151, Value_loss: 1.19987028172. Times trained:               13189. Times reached goal: 119.               Steps done: 4877695.\n",
      " state (0)  A[0]:(0.996080636978) A[1]:(0.00108023395296) A[2]:(0.00147381203715) A[3]:(0.0013653045753)\n",
      " state (1)  A[0]:(0.0836855396628) A[1]:(0.00743056926876) A[2]:(0.024419458583) A[3]:(0.88446444273)\n",
      " state (2)  A[0]:(0.999490976334) A[1]:(0.000113418202091) A[2]:(0.000392690359149) A[3]:(2.94266897072e-06)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(4.83791097849e-07) A[2]:(1.38953714668e-06) A[3]:(5.57374285615e-12)\n",
      " state (4)  A[0]:(0.999994277954) A[1]:(2.98697727885e-06) A[2]:(2.75881916423e-06) A[3]:(2.48966103809e-13)\n",
      " state (5)  A[0]:(0.913681745529) A[1]:(0.0240520909429) A[2]:(0.0622661858797) A[3]:(5.10913356002e-21)\n",
      " state (6)  A[0]:(0.0662108212709) A[1]:(0.771793782711) A[2]:(0.161995381117) A[3]:(8.42475931308e-23)\n",
      " state (7)  A[0]:(0.00791721045971) A[1]:(0.963668227196) A[2]:(0.0284145511687) A[3]:(1.70458089919e-23)\n",
      " state (8)  A[0]:(0.000619784346782) A[1]:(0.996021509171) A[2]:(0.00335869309492) A[3]:(2.99196738385e-24)\n",
      " state (9)  A[0]:(3.52173374267e-05) A[1]:(0.999747276306) A[2]:(0.000217489476199) A[3]:(3.2495175025e-25)\n",
      " state (10)  A[0]:(8.7344315034e-06) A[1]:(0.999946117401) A[2]:(4.51303676527e-05) A[3]:(9.25598973044e-26)\n",
      " state (11)  A[0]:(5.85759880778e-06) A[1]:(0.999966323376) A[2]:(2.78051047644e-05) A[3]:(6.31214017475e-26)\n",
      " state (12)  A[0]:(5.25873656443e-06) A[1]:(0.999970376492) A[2]:(2.43633203354e-05) A[3]:(5.68820543291e-26)\n",
      " state (13)  A[0]:(5.09860092279e-06) A[1]:(0.999971449375) A[2]:(2.34760318563e-05) A[3]:(5.5243824422e-26)\n",
      " state (14)  A[0]:(5.0492653827e-06) A[1]:(0.999971747398) A[2]:(2.32179845625e-05) A[3]:(5.47637717463e-26)\n",
      " state (15)  A[0]:(5.03173987454e-06) A[1]:(0.999971807003) A[2]:(2.31346148212e-05) A[3]:(5.46077313615e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 416000 finished after 15 . Running score: 0.15. Policy_loss: -92050.611277, Value_loss: 1.84877169355. Times trained:               12778. Times reached goal: 127.               Steps done: 4890473.\n",
      " state (0)  A[0]:(0.996773183346) A[1]:(0.00101666001137) A[2]:(0.00134701363277) A[3]:(0.000863166816998)\n",
      " state (1)  A[0]:(0.0772641450167) A[1]:(0.00726909795776) A[2]:(0.0236130934209) A[3]:(0.891853690147)\n",
      " state (2)  A[0]:(0.994180798531) A[1]:(0.0011447337456) A[2]:(0.00408175960183) A[3]:(0.000592727737967)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(4.383123553e-07) A[2]:(1.40998781717e-06) A[3]:(7.09536552776e-12)\n",
      " state (4)  A[0]:(0.999997437) A[1]:(8.23795289762e-07) A[2]:(1.75995398877e-06) A[3]:(2.00436368891e-12)\n",
      " state (5)  A[0]:(0.999058544636) A[1]:(0.000145120022353) A[2]:(0.000796319451183) A[3]:(3.30331330939e-18)\n",
      " state (6)  A[0]:(0.298916459084) A[1]:(0.371203094721) A[2]:(0.329880416393) A[3]:(2.37508070399e-22)\n",
      " state (7)  A[0]:(0.0134574165568) A[1]:(0.953323245049) A[2]:(0.0332193635404) A[3]:(1.84664307308e-23)\n",
      " state (8)  A[0]:(0.000250961747952) A[1]:(0.998799741268) A[2]:(0.000949279114138) A[3]:(9.98810417296e-25)\n",
      " state (9)  A[0]:(1.5117685507e-05) A[1]:(0.999924004078) A[2]:(6.08843110967e-05) A[3]:(1.08073303066e-25)\n",
      " state (10)  A[0]:(6.82004520058e-06) A[1]:(0.999968230724) A[2]:(2.49528275162e-05) A[3]:(5.32510631678e-26)\n",
      " state (11)  A[0]:(5.60308990316e-06) A[1]:(0.999974548817) A[2]:(1.98594116227e-05) A[3]:(4.4508829579e-26)\n",
      " state (12)  A[0]:(5.29749513589e-06) A[1]:(0.999976038933) A[2]:(1.86353790923e-05) A[3]:(4.2354521321e-26)\n",
      " state (13)  A[0]:(5.19657123732e-06) A[1]:(0.99997651577) A[2]:(1.82587446034e-05) A[3]:(4.16934373935e-26)\n",
      " state (14)  A[0]:(5.1552469813e-06) A[1]:(0.999976754189) A[2]:(1.81189388968e-05) A[3]:(4.14520798529e-26)\n",
      " state (15)  A[0]:(5.13525674251e-06) A[1]:(0.999976813793) A[2]:(1.80583920155e-05) A[3]:(4.13505325003e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 417000 finished after 15 . Running score: 0.13. Policy_loss: -92050.6273819, Value_loss: 1.4127778231. Times trained:               13381. Times reached goal: 128.               Steps done: 4903854.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997005403042) A[1]:(0.00107032083906) A[2]:(0.00126673735213) A[3]:(0.000657556171063)\n",
      " state (1)  A[0]:(0.0837508291006) A[1]:(0.00752138532698) A[2]:(0.0245846733451) A[3]:(0.88414311409)\n",
      " state (2)  A[0]:(0.998192608356) A[1]:(0.000388820946682) A[2]:(0.00136821577325) A[3]:(5.0342270697e-05)\n",
      " state (3)  A[0]:(0.999998271465) A[1]:(4.17717245682e-07) A[2]:(1.32585023493e-06) A[3]:(6.31395196543e-12)\n",
      " state (4)  A[0]:(0.999997854233) A[1]:(7.12424537141e-07) A[2]:(1.45217120462e-06) A[3]:(2.19001878264e-12)\n",
      " state (5)  A[0]:(0.999939143658) A[1]:(3.21490115311e-05) A[2]:(2.86967497232e-05) A[3]:(5.43184864899e-16)\n",
      " state (6)  A[0]:(0.590043663979) A[1]:(0.17465659976) A[2]:(0.235299751163) A[3]:(2.89519348576e-22)\n",
      " state (7)  A[0]:(0.0343709215522) A[1]:(0.910715401173) A[2]:(0.0549136847258) A[3]:(1.93294745378e-23)\n",
      " state (8)  A[0]:(0.00108167133294) A[1]:(0.997071266174) A[2]:(0.00184703827836) A[3]:(1.21078463949e-24)\n",
      " state (9)  A[0]:(3.74687115254e-05) A[1]:(0.999871194363) A[2]:(9.13332260097e-05) A[3]:(1.0024899234e-25)\n",
      " state (10)  A[0]:(1.15512839329e-05) A[1]:(0.999957501888) A[2]:(3.0945873732e-05) A[3]:(4.07964963813e-26)\n",
      " state (11)  A[0]:(8.58165731188e-06) A[1]:(0.99996805191) A[2]:(2.33536284213e-05) A[3]:(3.23717885513e-26)\n",
      " state (12)  A[0]:(7.84880830906e-06) A[1]:(0.999970495701) A[2]:(2.162797864e-05) A[3]:(3.0392813874e-26)\n",
      " state (13)  A[0]:(7.58254600441e-06) A[1]:(0.999971270561) A[2]:(2.11383103306e-05) A[3]:(2.98188590172e-26)\n",
      " state (14)  A[0]:(7.45362422094e-06) A[1]:(0.999971568584) A[2]:(2.0985622541e-05) A[3]:(2.96323365539e-26)\n",
      " state (15)  A[0]:(7.37736172596e-06) A[1]:(0.999971687794) A[2]:(2.0939964088e-05) A[3]:(2.9570121313e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 418000 finished after 6 . Running score: 0.16. Policy_loss: -92050.6193358, Value_loss: 0.979779182295. Times trained:               12525. Times reached goal: 141.               Steps done: 4916379.\n",
      " state (0)  A[0]:(0.996680557728) A[1]:(0.00120272662025) A[2]:(0.00133964745328) A[3]:(0.000777095498051)\n",
      " state (1)  A[0]:(0.0584353804588) A[1]:(0.0067589757964) A[2]:(0.0199511609972) A[3]:(0.914854466915)\n",
      " state (2)  A[0]:(0.331289023161) A[1]:(0.0131732048467) A[2]:(0.0479512214661) A[3]:(0.607586562634)\n",
      " state (3)  A[0]:(0.999981760979) A[1]:(4.13505267716e-06) A[2]:(1.41207137858e-05) A[3]:(1.31415189841e-09)\n",
      " state (4)  A[0]:(0.999997973442) A[1]:(5.7316111679e-07) A[2]:(1.46046136251e-06) A[3]:(3.5670450968e-12)\n",
      " state (5)  A[0]:(0.999983608723) A[1]:(9.37995173445e-06) A[2]:(7.01497583577e-06) A[3]:(1.1425973015e-14)\n",
      " state (6)  A[0]:(0.888471901417) A[1]:(0.0298895388842) A[2]:(0.0816385596991) A[3]:(1.25241667674e-21)\n",
      " state (7)  A[0]:(0.330413967371) A[1]:(0.412021756172) A[2]:(0.257564306259) A[3]:(6.44817743827e-23)\n",
      " state (8)  A[0]:(0.132130533457) A[1]:(0.756000101566) A[2]:(0.11186940968) A[3]:(2.19317326044e-23)\n",
      " state (9)  A[0]:(0.0281090401113) A[1]:(0.9554002285) A[2]:(0.0164907611907) A[3]:(4.82044579074e-24)\n",
      " state (10)  A[0]:(0.0027919090353) A[1]:(0.996207177639) A[2]:(0.00100093288347) A[3]:(5.87382428065e-25)\n",
      " state (11)  A[0]:(0.000525020237546) A[1]:(0.999295115471) A[2]:(0.000179884591489) A[3]:(1.59218335647e-25)\n",
      " state (12)  A[0]:(0.000245747651206) A[1]:(0.999658226967) A[2]:(9.60220131674e-05) A[3]:(9.70517267655e-26)\n",
      " state (13)  A[0]:(0.000180785908015) A[1]:(0.999741971493) A[2]:(7.72148778196e-05) A[3]:(8.11578188769e-26)\n",
      " state (14)  A[0]:(0.000157683214638) A[1]:(0.999771416187) A[2]:(7.09117157385e-05) A[3]:(7.54107761301e-26)\n",
      " state (15)  A[0]:(0.000146706675878) A[1]:(0.999785065651) A[2]:(6.82039899402e-05) A[3]:(7.27530914145e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 419000 finished after 8 . Running score: 0.07. Policy_loss: -92050.6347967, Value_loss: 1.19317250484. Times trained:               12794. Times reached goal: 119.               Steps done: 4929173.\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0013,  0.0014,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0013,  0.0014,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9994e-01,  5.0506e-05,  9.7137e-06,  1.4048e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7300e-03,  9.9615e-01,  1.1172e-03,  1.2053e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996497869492) A[1]:(0.0012809636537) A[2]:(0.00136352085974) A[3]:(0.000857637496665)\n",
      " state (1)  A[0]:(0.0580135881901) A[1]:(0.00697130523622) A[2]:(0.019593693316) A[3]:(0.915421426296)\n",
      " state (2)  A[0]:(0.999699950218) A[1]:(7.51867482904e-05) A[2]:(0.000224040792091) A[3]:(8.04712840363e-07)\n",
      " state (3)  A[0]:(0.999997913837) A[1]:(7.36935930945e-07) A[2]:(1.3741029079e-06) A[3]:(3.20396790696e-12)\n",
      " state (4)  A[0]:(0.999940097332) A[1]:(5.02123330079e-05) A[2]:(9.67086089076e-06) A[3]:(1.42094117946e-15)\n",
      " state (5)  A[0]:(0.433488339186) A[1]:(0.421856075525) A[2]:(0.144655570388) A[3]:(1.66416695548e-22)\n",
      " state (6)  A[0]:(0.0529076121747) A[1]:(0.912023365498) A[2]:(0.03506905213) A[3]:(1.56842808235e-23)\n",
      " state (7)  A[0]:(0.014720801264) A[1]:(0.977279186249) A[2]:(0.00799998920411) A[3]:(5.03000629578e-24)\n",
      " state (8)  A[0]:(0.00277758389711) A[1]:(0.996085584164) A[2]:(0.00113684358075) A[3]:(1.21958625644e-24)\n",
      " state (9)  A[0]:(0.000210810903809) A[1]:(0.999705374241) A[2]:(8.3791630459e-05) A[3]:(1.72837513811e-25)\n",
      " state (10)  A[0]:(3.36556004186e-05) A[1]:(0.999947190285) A[2]:(1.9172724933e-05) A[3]:(5.42610701345e-26)\n",
      " state (11)  A[0]:(1.82185431186e-05) A[1]:(0.999969542027) A[2]:(1.22551709865e-05) A[3]:(3.8036259748e-26)\n",
      " state (12)  A[0]:(1.50789301188e-05) A[1]:(0.99997407198) A[2]:(1.08547128548e-05) A[3]:(3.45415289007e-26)\n",
      " state (13)  A[0]:(1.39843195939e-05) A[1]:(0.999975502491) A[2]:(1.04912860479e-05) A[3]:(3.36114156711e-26)\n",
      " state (14)  A[0]:(1.34093834276e-05) A[1]:(0.999976217747) A[2]:(1.03841275632e-05) A[3]:(3.33371570841e-26)\n",
      " state (15)  A[0]:(1.30140415422e-05) A[1]:(0.999976634979) A[2]:(1.03477923403e-05) A[3]:(3.32548628679e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 420000 finished after 4 . Running score: 0.14. Policy_loss: -92050.611442, Value_loss: 1.62114459049. Times trained:               12993. Times reached goal: 142.               Steps done: 4942166.\n",
      " state (0)  A[0]:(0.996460914612) A[1]:(0.00126504804939) A[2]:(0.00142319675069) A[3]:(0.000850815267768)\n",
      " state (1)  A[0]:(0.0499492138624) A[1]:(0.00649457518011) A[2]:(0.0188384372741) A[3]:(0.924717783928)\n",
      " state (2)  A[0]:(0.99828350544) A[1]:(0.000364488689229) A[2]:(0.00131624389905) A[3]:(3.57344797521e-05)\n",
      " state (3)  A[0]:(0.999997794628) A[1]:(5.24238942035e-07) A[2]:(1.67716768829e-06) A[3]:(5.79586248786e-12)\n",
      " state (4)  A[0]:(0.999994874001) A[1]:(2.09512654692e-06) A[2]:(3.02684702547e-06) A[3]:(3.62098724485e-13)\n",
      " state (5)  A[0]:(0.983021438122) A[1]:(0.00336034270003) A[2]:(0.0136182438582) A[3]:(2.36287631644e-20)\n",
      " state (6)  A[0]:(0.212900653481) A[1]:(0.443920314312) A[2]:(0.343179017305) A[3]:(6.07376385781e-23)\n",
      " state (7)  A[0]:(0.0578832291067) A[1]:(0.839908897877) A[2]:(0.102207839489) A[3]:(1.94288189471e-23)\n",
      " state (8)  A[0]:(0.00828965194523) A[1]:(0.979878723621) A[2]:(0.0118316486478) A[3]:(3.99345332151e-24)\n",
      " state (9)  A[0]:(0.000338697514962) A[1]:(0.999103009701) A[2]:(0.000558288942557) A[3]:(3.90591954053e-25)\n",
      " state (10)  A[0]:(5.02082511957e-05) A[1]:(0.99984639883) A[2]:(0.00010337553249) A[3]:(1.05409085596e-25)\n",
      " state (11)  A[0]:(2.88136925519e-05) A[1]:(0.999909043312) A[2]:(6.21298240731e-05) A[3]:(7.1466724286e-26)\n",
      " state (12)  A[0]:(2.45303654083e-05) A[1]:(0.999921679497) A[2]:(5.38180902367e-05) A[3]:(6.41889031156e-26)\n",
      " state (13)  A[0]:(2.31651119975e-05) A[1]:(0.999925374985) A[2]:(5.1470757171e-05) A[3]:(6.21791382111e-26)\n",
      " state (14)  A[0]:(2.25435287575e-05) A[1]:(0.999926805496) A[2]:(5.06337237312e-05) A[3]:(6.15481912354e-26)\n",
      " state (15)  A[0]:(2.21587652049e-05) A[1]:(0.999927580357) A[2]:(5.025352948e-05) A[3]:(6.13438392831e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 421000 finished after 4 . Running score: 0.09. Policy_loss: -92050.619976, Value_loss: 1.41083229638. Times trained:               13082. Times reached goal: 120.               Steps done: 4955248.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.99571955204) A[1]:(0.00132219085936) A[2]:(0.00162484589964) A[3]:(0.00133340246975)\n",
      " state (1)  A[0]:(0.0533794723451) A[1]:(0.00648742122576) A[2]:(0.0205013416708) A[3]:(0.919631779194)\n",
      " state (2)  A[0]:(0.999838769436) A[1]:(3.1662897527e-05) A[2]:(0.000129447493237) A[3]:(1.3856009673e-07)\n",
      " state (3)  A[0]:(0.999997437) A[1]:(5.30617626282e-07) A[2]:(2.04076286536e-06) A[3]:(5.15050242039e-12)\n",
      " state (4)  A[0]:(0.999990880489) A[1]:(2.67285940936e-06) A[2]:(6.46628768664e-06) A[3]:(1.84466997883e-13)\n",
      " state (5)  A[0]:(0.934070289135) A[1]:(0.00320680579171) A[2]:(0.0627229213715) A[3]:(3.87426114981e-21)\n",
      " state (6)  A[0]:(0.127322584391) A[1]:(0.27260684967) A[2]:(0.600070595741) A[3]:(6.49103530462e-23)\n",
      " state (7)  A[0]:(0.0359005331993) A[1]:(0.688503146172) A[2]:(0.275596350431) A[3]:(3.97802328109e-23)\n",
      " state (8)  A[0]:(0.004541228991) A[1]:(0.938813388348) A[2]:(0.0566453970969) A[3]:(1.31973337109e-23)\n",
      " state (9)  A[0]:(0.000238460677792) A[1]:(0.996181666851) A[2]:(0.00357989780605) A[3]:(1.54257381699e-24)\n",
      " state (10)  A[0]:(4.22581215389e-05) A[1]:(0.999491333961) A[2]:(0.000466400873847) A[3]:(3.11717104799e-25)\n",
      " state (11)  A[0]:(2.42505357164e-05) A[1]:(0.999747812748) A[2]:(0.000227935393923) A[3]:(1.78203690815e-25)\n",
      " state (12)  A[0]:(2.07305602089e-05) A[1]:(0.999793708324) A[2]:(0.00018556768191) A[3]:(1.51896350592e-25)\n",
      " state (13)  A[0]:(1.97916633624e-05) A[1]:(0.999805450439) A[2]:(0.000174760862137) A[3]:(1.45041765623e-25)\n",
      " state (14)  A[0]:(1.9494593289e-05) A[1]:(0.999809026718) A[2]:(0.000171500330907) A[3]:(1.42995300197e-25)\n",
      " state (15)  A[0]:(1.93846062757e-05) A[1]:(0.999810218811) A[2]:(0.000170376035385) A[3]:(1.42313601116e-25)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 422000 finished after 10 . Running score: 0.09. Policy_loss: -92050.6169312, Value_loss: 1.00451743149. Times trained:               13014. Times reached goal: 122.               Steps done: 4968262.\n",
      " state (0)  A[0]:(0.996179163456) A[1]:(0.00145369081292) A[2]:(0.00142972986214) A[3]:(0.000937400211114)\n",
      " state (1)  A[0]:(0.0471224449575) A[1]:(0.00631976267323) A[2]:(0.0184434000403) A[3]:(0.928114414215)\n",
      " state (2)  A[0]:(0.997549951077) A[1]:(0.000506620795932) A[2]:(0.00186463736463) A[3]:(7.87844037404e-05)\n",
      " state (3)  A[0]:(0.999997794628) A[1]:(4.93060099416e-07) A[2]:(1.7158636183e-06) A[3]:(7.21180146851e-12)\n",
      " state (4)  A[0]:(0.999996840954) A[1]:(1.0413483551e-06) A[2]:(2.13554267248e-06) A[3]:(2.01879658823e-12)\n",
      " state (5)  A[0]:(0.998528063297) A[1]:(0.000910474569537) A[2]:(0.000561441993341) A[3]:(3.06967850284e-17)\n",
      " state (6)  A[0]:(0.161260381341) A[1]:(0.52202641964) A[2]:(0.316713213921) A[3]:(1.96053379342e-22)\n",
      " state (7)  A[0]:(0.0201725102961) A[1]:(0.915202915668) A[2]:(0.0646245852113) A[3]:(2.51960185198e-23)\n",
      " state (8)  A[0]:(0.000820138899144) A[1]:(0.995670437813) A[2]:(0.0035094236955) A[3]:(2.44067430682e-24)\n",
      " state (9)  A[0]:(3.89622800867e-05) A[1]:(0.999778211117) A[2]:(0.000182801508345) A[3]:(2.27886853205e-25)\n",
      " state (10)  A[0]:(1.41241589517e-05) A[1]:(0.999927222729) A[2]:(5.86331916566e-05) A[3]:(9.30250355786e-26)\n",
      " state (11)  A[0]:(1.09119255285e-05) A[1]:(0.999945759773) A[2]:(4.33531167801e-05) A[3]:(7.35924702325e-26)\n",
      " state (12)  A[0]:(1.01395417005e-05) A[1]:(0.999949932098) A[2]:(3.99389282393e-05) A[3]:(6.91456181689e-26)\n",
      " state (13)  A[0]:(9.88432657323e-06) A[1]:(0.999951124191) A[2]:(3.89675333281e-05) A[3]:(6.7929983514e-26)\n",
      " state (14)  A[0]:(9.7736110547e-06) A[1]:(0.999951601028) A[2]:(3.86435131077e-05) A[3]:(6.75738682821e-26)\n",
      " state (15)  A[0]:(9.71334429778e-06) A[1]:(0.999951779842) A[2]:(3.85206258215e-05) A[3]:(6.7477029443e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 423000 finished after 16 . Running score: 0.17. Policy_loss: -92050.6113647, Value_loss: 1.2096884702. Times trained:               12759. Times reached goal: 139.               Steps done: 4981021.\n",
      " state (0)  A[0]:(0.991042733192) A[1]:(0.00165453297086) A[2]:(0.00201372010633) A[3]:(0.00528901442885)\n",
      " state (1)  A[0]:(0.0401166565716) A[1]:(0.00592700717971) A[2]:(0.0172230824828) A[3]:(0.93673324585)\n",
      " state (2)  A[0]:(0.994907796383) A[1]:(0.000975105911493) A[2]:(0.00376687594689) A[3]:(0.000350225047441)\n",
      " state (3)  A[0]:(0.999997496605) A[1]:(5.29933458893e-07) A[2]:(1.97283043235e-06) A[3]:(7.81553294305e-12)\n",
      " state (4)  A[0]:(0.99999576807) A[1]:(1.39783560371e-06) A[2]:(2.83929989564e-06) A[3]:(1.58521676688e-12)\n",
      " state (5)  A[0]:(0.989889860153) A[1]:(0.00411855243146) A[2]:(0.00599156692624) A[3]:(2.11488369139e-18)\n",
      " state (6)  A[0]:(0.0758648738265) A[1]:(0.571006000042) A[2]:(0.353129088879) A[3]:(1.35234311386e-22)\n",
      " state (7)  A[0]:(0.00621786247939) A[1]:(0.949867546558) A[2]:(0.0439146012068) A[3]:(1.85046526194e-23)\n",
      " state (8)  A[0]:(0.000123387522763) A[1]:(0.998626410961) A[2]:(0.00125022185966) A[3]:(1.06452884159e-24)\n",
      " state (9)  A[0]:(1.38860714287e-05) A[1]:(0.999862134457) A[2]:(0.000123951846035) A[3]:(1.67622847405e-25)\n",
      " state (10)  A[0]:(8.02088652563e-06) A[1]:(0.999927282333) A[2]:(6.46972257528e-05) A[3]:(1.00554127436e-25)\n",
      " state (11)  A[0]:(7.00226382833e-06) A[1]:(0.999937951565) A[2]:(5.50242548343e-05) A[3]:(8.8663855042e-26)\n",
      " state (12)  A[0]:(6.7283676799e-06) A[1]:(0.999940633774) A[2]:(5.26200456079e-05) A[3]:(8.56978612979e-26)\n",
      " state (13)  A[0]:(6.63116361466e-06) A[1]:(0.999941468239) A[2]:(5.187914212e-05) A[3]:(8.48191565311e-26)\n",
      " state (14)  A[0]:(6.58772387396e-06) A[1]:(0.999941825867) A[2]:(5.16115978826e-05) A[3]:(8.45310620633e-26)\n",
      " state (15)  A[0]:(6.56467091176e-06) A[1]:(0.999941945076) A[2]:(5.15035280841e-05) A[3]:(8.44356838495e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 424000 finished after 5 . Running score: 0.05. Policy_loss: -92050.6114569, Value_loss: 0.992305201336. Times trained:               12582. Times reached goal: 113.               Steps done: 4993603.\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0015,  0.0016,  0.0022]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0015,  0.0016,  0.0022]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0015,  0.0016,  0.0022]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0015,  0.0016,  0.0022]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0015,  0.0016,  0.0022]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4961e-07,  1.8128e-06,  6.2147e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4950e-07,  1.8126e-06,  6.2168e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.1297e-03,  9.7556e-01,  1.6311e-02,  8.5779e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994722425938) A[1]:(0.00145376566797) A[2]:(0.00161499797832) A[3]:(0.00220878096297)\n",
      " state (1)  A[0]:(0.0325744971633) A[1]:(0.00535398442298) A[2]:(0.0147563442588) A[3]:(0.94731515646)\n",
      " state (2)  A[0]:(0.52618175745) A[1]:(0.0140608148649) A[2]:(0.0549836046994) A[3]:(0.404773771763)\n",
      " state (3)  A[0]:(0.999995172024) A[1]:(9.32084503802e-07) A[2]:(3.91305547964e-06) A[3]:(4.98476017408e-11)\n",
      " state (4)  A[0]:(0.999997735023) A[1]:(4.49073297659e-07) A[2]:(1.81222321771e-06) A[3]:(6.22419824015e-12)\n",
      " state (5)  A[0]:(0.999995291233) A[1]:(1.13887290354e-06) A[2]:(3.59820955964e-06) A[3]:(1.1062582057e-12)\n",
      " state (6)  A[0]:(0.974765658379) A[1]:(0.00124337733723) A[2]:(0.0239909775555) A[3]:(2.2188809739e-19)\n",
      " state (7)  A[0]:(0.183934807777) A[1]:(0.391771674156) A[2]:(0.424293518066) A[3]:(1.95654001154e-22)\n",
      " state (8)  A[0]:(0.00829436723143) A[1]:(0.975059628487) A[2]:(0.0166459828615) A[3]:(8.70742931689e-24)\n",
      " state (9)  A[0]:(0.000239285334828) A[1]:(0.99929702282) A[2]:(0.00046366994502) A[3]:(5.13293943049e-25)\n",
      " state (10)  A[0]:(5.55041588086e-05) A[1]:(0.999811768532) A[2]:(0.000132713990752) A[3]:(1.86106400756e-25)\n",
      " state (11)  A[0]:(3.55280535587e-05) A[1]:(0.999871969223) A[2]:(9.2503898486e-05) A[3]:(1.38615630733e-25)\n",
      " state (12)  A[0]:(3.00670744764e-05) A[1]:(0.999888420105) A[2]:(8.15302482806e-05) A[3]:(1.25013671923e-25)\n",
      " state (13)  A[0]:(2.77343751804e-05) A[1]:(0.999895095825) A[2]:(7.71991617512e-05) A[3]:(1.19567573449e-25)\n",
      " state (14)  A[0]:(2.64447353402e-05) A[1]:(0.999898433685) A[2]:(7.51053448766e-05) A[3]:(1.16948875718e-25)\n",
      " state (15)  A[0]:(2.56072235061e-05) A[1]:(0.999900400639) A[2]:(7.39707174944e-05) A[3]:(1.15564572741e-25)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 425000 finished after 8 . Running score: 0.11. Policy_loss: -92050.611793, Value_loss: 1.41398845103. Times trained:               12477. Times reached goal: 131.               Steps done: 5006080.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.991065442562) A[1]:(0.00166388379876) A[2]:(0.00200617453083) A[3]:(0.00526449503377)\n",
      " state (1)  A[0]:(0.0345320478082) A[1]:(0.00556988455355) A[2]:(0.0153459431604) A[3]:(0.944552123547)\n",
      " state (2)  A[0]:(0.999300718307) A[1]:(0.000144974925206) A[2]:(0.000549670425244) A[3]:(4.64782397103e-06)\n",
      " state (3)  A[0]:(0.999997794628) A[1]:(4.69496939104e-07) A[2]:(1.75504544586e-06) A[3]:(7.2773809548e-12)\n",
      " state (4)  A[0]:(0.999997258186) A[1]:(6.97272753314e-07) A[2]:(2.04610796573e-06) A[3]:(3.42859247937e-12)\n",
      " state (5)  A[0]:(0.999944269657) A[1]:(2.23901679419e-05) A[2]:(3.33584539476e-05) A[3]:(3.16864077667e-15)\n",
      " state (6)  A[0]:(0.419408380985) A[1]:(0.187817692757) A[2]:(0.392773926258) A[3]:(4.29487380751e-22)\n",
      " state (7)  A[0]:(0.0165173392743) A[1]:(0.945612728596) A[2]:(0.0378699116409) A[3]:(1.70930696488e-23)\n",
      " state (8)  A[0]:(0.000179409296834) A[1]:(0.999401628971) A[2]:(0.000418991461629) A[3]:(5.03232416634e-25)\n",
      " state (9)  A[0]:(2.19876637857e-05) A[1]:(0.99991607666) A[2]:(6.19075217401e-05) A[3]:(1.09365136754e-25)\n",
      " state (10)  A[0]:(1.35572245199e-05) A[1]:(0.999947071075) A[2]:(3.93966438423e-05) A[3]:(7.65714616926e-26)\n",
      " state (11)  A[0]:(1.18899370136e-05) A[1]:(0.999953091145) A[2]:(3.50210648321e-05) A[3]:(6.98931563213e-26)\n",
      " state (12)  A[0]:(1.13170781333e-05) A[1]:(0.999954938889) A[2]:(3.37379351549e-05) A[3]:(6.79719780312e-26)\n",
      " state (13)  A[0]:(1.1027970686e-05) A[1]:(0.999955713749) A[2]:(3.32650524797e-05) A[3]:(6.73212294142e-26)\n",
      " state (14)  A[0]:(1.08382764665e-05) A[1]:(0.999956071377) A[2]:(3.30659313477e-05) A[3]:(6.71017843341e-26)\n",
      " state (15)  A[0]:(1.0693657714e-05) A[1]:(0.999956309795) A[2]:(3.29778849846e-05) A[3]:(6.70549765327e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 426000 finished after 6 . Running score: 0.18. Policy_loss: -92050.6114984, Value_loss: 1.21226305393. Times trained:               13307. Times reached goal: 129.               Steps done: 5019387.\n",
      " state (0)  A[0]:(0.992590248585) A[1]:(0.00165150151588) A[2]:(0.00195545633323) A[3]:(0.00380279775709)\n",
      " state (1)  A[0]:(0.0458121709526) A[1]:(0.0064299874939) A[2]:(0.0182210206985) A[3]:(0.929536819458)\n",
      " state (2)  A[0]:(0.999995410442) A[1]:(1.18589514386e-06) A[2]:(3.42353541782e-06) A[3]:(4.59794424756e-11)\n",
      " state (3)  A[0]:(0.999997437) A[1]:(8.23607535949e-07) A[2]:(1.72039187873e-06) A[3]:(4.79925821306e-12)\n",
      " state (4)  A[0]:(0.99996137619) A[1]:(3.05113790091e-05) A[2]:(8.09103494248e-06) A[3]:(1.51655743492e-14)\n",
      " state (5)  A[0]:(0.214139565825) A[1]:(0.65205603838) A[2]:(0.133804365993) A[3]:(3.00864329596e-22)\n",
      " state (6)  A[0]:(0.000925269036088) A[1]:(0.997531056404) A[2]:(0.00154369906522) A[3]:(2.10583480142e-24)\n",
      " state (7)  A[0]:(7.03408932168e-06) A[1]:(0.999977052212) A[2]:(1.59085975611e-05) A[3]:(5.42428215631e-26)\n",
      " state (8)  A[0]:(1.45499620885e-06) A[1]:(0.999994933605) A[2]:(3.61086881639e-06) A[3]:(1.63818383882e-26)\n",
      " state (9)  A[0]:(1.12963175525e-06) A[1]:(0.999996066093) A[2]:(2.78845209323e-06) A[3]:(1.33605920251e-26)\n",
      " state (10)  A[0]:(1.07282994577e-06) A[1]:(0.999996244907) A[2]:(2.65400353783e-06) A[3]:(1.28590051415e-26)\n",
      " state (11)  A[0]:(1.05721926502e-06) A[1]:(0.999996304512) A[2]:(2.62496541836e-06) A[3]:(1.27550927469e-26)\n",
      " state (12)  A[0]:(1.05090032321e-06) A[1]:(0.999996304512) A[2]:(2.61739614871e-06) A[3]:(1.27316611128e-26)\n",
      " state (13)  A[0]:(1.04750836272e-06) A[1]:(0.999996364117) A[2]:(2.61502623289e-06) A[3]:(1.27267576952e-26)\n",
      " state (14)  A[0]:(1.04540470147e-06) A[1]:(0.999996364117) A[2]:(2.61418836089e-06) A[3]:(1.27266120949e-26)\n",
      " state (15)  A[0]:(1.0440078313e-06) A[1]:(0.999996364117) A[2]:(2.61385434897e-06) A[3]:(1.27276806008e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 427000 finished after 13 . Running score: 0.14. Policy_loss: -92050.6112993, Value_loss: 1.2150599341. Times trained:               12883. Times reached goal: 127.               Steps done: 5032270.\n",
      " state (0)  A[0]:(0.996020197868) A[1]:(0.00142056692857) A[2]:(0.00143253011629) A[3]:(0.00112672057003)\n",
      " state (1)  A[0]:(0.0476651228964) A[1]:(0.00676121562719) A[2]:(0.0176526699215) A[3]:(0.927920997143)\n",
      " state (2)  A[0]:(0.167008921504) A[1]:(0.0110332127661) A[2]:(0.0358706787229) A[3]:(0.786087214947)\n",
      " state (3)  A[0]:(0.99999332428) A[1]:(2.12041845771e-06) A[2]:(4.57207806903e-06) A[3]:(1.375470432e-10)\n",
      " state (4)  A[0]:(0.999997794628) A[1]:(8.66024322477e-07) A[2]:(1.35107416099e-06) A[3]:(6.36983304644e-12)\n",
      " state (5)  A[0]:(0.999941706657) A[1]:(5.25181785633e-05) A[2]:(5.76596130486e-06) A[3]:(1.20807674303e-13)\n",
      " state (6)  A[0]:(0.0199031215161) A[1]:(0.974582314491) A[2]:(0.00551455654204) A[3]:(2.98223677669e-22)\n",
      " state (7)  A[0]:(1.22353703773e-05) A[1]:(0.999981403351) A[2]:(6.34616117168e-06) A[3]:(4.50150718204e-26)\n",
      " state (8)  A[0]:(3.46345700564e-07) A[1]:(0.999999344349) A[2]:(3.24582543954e-07) A[3]:(3.16250427265e-27)\n",
      " state (9)  A[0]:(1.54629745452e-07) A[1]:(0.999999642372) A[2]:(1.83954085742e-07) A[3]:(1.89900833266e-27)\n",
      " state (10)  A[0]:(1.24553395153e-07) A[1]:(0.999999701977) A[2]:(1.58634620107e-07) A[3]:(1.66304301069e-27)\n",
      " state (11)  A[0]:(1.13777225863e-07) A[1]:(0.999999761581) A[2]:(1.49801437033e-07) A[3]:(1.57952486606e-27)\n",
      " state (12)  A[0]:(1.0833716857e-07) A[1]:(0.999999761581) A[2]:(1.45691316789e-07) A[3]:(1.54042396225e-27)\n",
      " state (13)  A[0]:(1.051041707e-07) A[1]:(0.999999761581) A[2]:(1.43519059748e-07) A[3]:(1.51966908191e-27)\n",
      " state (14)  A[0]:(1.02994242468e-07) A[1]:(0.999999761581) A[2]:(1.42303889561e-07) A[3]:(1.50797522047e-27)\n",
      " state (15)  A[0]:(1.015261617e-07) A[1]:(0.999999761581) A[2]:(1.41606960824e-07) A[3]:(1.50115675439e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 428000 finished after 12 . Running score: 0.13. Policy_loss: -92050.6112961, Value_loss: 1.41431211503. Times trained:               12583. Times reached goal: 139.               Steps done: 5044853.\n",
      " state (0)  A[0]:(0.992833852768) A[1]:(0.00156846910249) A[2]:(0.00188818480819) A[3]:(0.0037095118314)\n",
      " state (1)  A[0]:(0.0537025146186) A[1]:(0.00699915504083) A[2]:(0.0196262747049) A[3]:(0.919672071934)\n",
      " state (2)  A[0]:(0.266992628574) A[1]:(0.0133859142661) A[2]:(0.0447984486818) A[3]:(0.674823045731)\n",
      " state (3)  A[0]:(0.999997079372) A[1]:(9.65550043475e-07) A[2]:(1.9570643417e-06) A[3]:(2.56339584975e-11)\n",
      " state (4)  A[0]:(0.999998092651) A[1]:(7.0998640922e-07) A[2]:(1.17084323392e-06) A[3]:(6.60635825558e-12)\n",
      " state (5)  A[0]:(0.999988555908) A[1]:(8.70449912327e-06) A[2]:(2.7167418466e-06) A[3]:(4.577135654e-13)\n",
      " state (6)  A[0]:(0.459242969751) A[1]:(0.514702618122) A[2]:(0.0260543841869) A[3]:(6.35559506141e-20)\n",
      " state (7)  A[0]:(0.00232895975932) A[1]:(0.99606949091) A[2]:(0.001601557713) A[3]:(1.30907357256e-23)\n",
      " state (8)  A[0]:(5.2244788094e-05) A[1]:(0.999907612801) A[2]:(4.01130528189e-05) A[3]:(2.66508965435e-25)\n",
      " state (9)  A[0]:(1.01529690255e-06) A[1]:(0.999997913837) A[2]:(1.06156323909e-06) A[3]:(1.10904614218e-26)\n",
      " state (10)  A[0]:(1.75180701945e-07) A[1]:(0.999999582767) A[2]:(2.18188546341e-07) A[3]:(2.88389077281e-27)\n",
      " state (11)  A[0]:(1.12756879389e-07) A[1]:(0.999999761581) A[2]:(1.4269609494e-07) A[3]:(2.02359808891e-27)\n",
      " state (12)  A[0]:(9.95683535621e-08) A[1]:(0.999999761581) A[2]:(1.26732203398e-07) A[3]:(1.83307103953e-27)\n",
      " state (13)  A[0]:(9.52061469661e-08) A[1]:(0.999999761581) A[2]:(1.21941113207e-07) A[3]:(1.77415376104e-27)\n",
      " state (14)  A[0]:(9.32600414671e-08) A[1]:(0.999999761581) A[2]:(1.2015780726e-07) A[3]:(1.75135845422e-27)\n",
      " state (15)  A[0]:(9.21795120234e-08) A[1]:(0.999999761581) A[2]:(1.19377233432e-07) A[3]:(1.74078124696e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 429000 finished after 5 . Running score: 0.12. Policy_loss: -92050.6113018, Value_loss: 1.21490523431. Times trained:               12454. Times reached goal: 115.               Steps done: 5057307.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9941,  0.0015,  0.0018,  0.0026]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.8717e-07,  1.4716e-06,  6.6464e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.8689e-07,  1.4717e-06,  6.6472e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9941,  0.0015,  0.0018,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0015,  0.0018,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0015,  0.0018,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.8601e-07,  1.4722e-06,  6.6497e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3467e-03,  9.9023e-01,  7.4225e-03,  5.6569e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.9982e-05,  9.9966e-01,  2.4903e-04,  3.1049e-25]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3535e-03,  9.9020e-01,  7.4480e-03,  5.6549e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.0033e-05,  9.9966e-01,  2.4916e-04,  3.0967e-25]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.6029e-06,  9.9998e-01,  1.1842e-05,  2.4600e-26]])\n",
      "On state=10, selected action=1\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994218289852) A[1]:(0.00153701053932) A[2]:(0.00179164367728) A[3]:(0.00245304824784)\n",
      " state (1)  A[0]:(0.0513124801219) A[1]:(0.00665871985257) A[2]:(0.0203836597502) A[3]:(0.92164516449)\n",
      " state (2)  A[0]:(0.318293869495) A[1]:(0.0136069767177) A[2]:(0.0515254363418) A[3]:(0.616573750973)\n",
      " state (3)  A[0]:(0.999996960163) A[1]:(7.9664897612e-07) A[2]:(2.24954442274e-06) A[3]:(2.15253526809e-11)\n",
      " state (4)  A[0]:(0.999997913837) A[1]:(5.85875284287e-07) A[2]:(1.47219941482e-06) A[3]:(6.64557254712e-12)\n",
      " state (5)  A[0]:(0.999992489815) A[1]:(3.21353218169e-06) A[2]:(4.30383261119e-06) A[3]:(6.28572281031e-13)\n",
      " state (6)  A[0]:(0.70328271389) A[1]:(0.0985567867756) A[2]:(0.198160499334) A[3]:(5.85282434534e-20)\n",
      " state (7)  A[0]:(0.0279052313417) A[1]:(0.889959931374) A[2]:(0.0821348205209) A[3]:(9.35153329937e-23)\n",
      " state (8)  A[0]:(0.00234520155936) A[1]:(0.990238249302) A[2]:(0.00741653423756) A[3]:(5.61525865149e-24)\n",
      " state (9)  A[0]:(8.97130958037e-05) A[1]:(0.999662101269) A[2]:(0.000248194322921) A[3]:(3.07988874205e-25)\n",
      " state (10)  A[0]:(3.59849695997e-06) A[1]:(0.999984562397) A[2]:(1.18264151752e-05) A[3]:(2.45466227129e-26)\n",
      " state (11)  A[0]:(9.61393539001e-07) A[1]:(0.999995529652) A[2]:(3.51189260073e-06) A[3]:(8.94274446648e-27)\n",
      " state (12)  A[0]:(6.47143792776e-07) A[1]:(0.999996900558) A[2]:(2.4412051971e-06) A[3]:(6.61970779723e-27)\n",
      " state (13)  A[0]:(5.66048981909e-07) A[1]:(0.999997258186) A[2]:(2.18094123738e-06) A[3]:(6.02908517231e-27)\n",
      " state (14)  A[0]:(5.34648222583e-07) A[1]:(0.999997377396) A[2]:(2.09793893191e-06) A[3]:(5.83581579127e-27)\n",
      " state (15)  A[0]:(5.18242359249e-07) A[1]:(0.999997437) A[2]:(2.0663946998e-06) A[3]:(5.75988985507e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 430000 finished after 12 . Running score: 0.1. Policy_loss: -92050.6122803, Value_loss: 1.83910836828. Times trained:               12592. Times reached goal: 124.               Steps done: 5069899.\n",
      " state (0)  A[0]:(0.99359357357) A[1]:(0.00160048401449) A[2]:(0.00184895109851) A[3]:(0.00295697478577)\n",
      " state (1)  A[0]:(0.039019908756) A[1]:(0.00593554088846) A[2]:(0.0169134512544) A[3]:(0.938131093979)\n",
      " state (2)  A[0]:(0.364981561899) A[1]:(0.0140081103891) A[2]:(0.0522322021425) A[3]:(0.56877809763)\n",
      " state (3)  A[0]:(0.999996840954) A[1]:(8.03249406545e-07) A[2]:(2.35306129071e-06) A[3]:(2.4241240959e-11)\n",
      " state (4)  A[0]:(0.999998033047) A[1]:(5.21622837368e-07) A[2]:(1.44396972246e-06) A[3]:(7.0741758805e-12)\n",
      " state (5)  A[0]:(0.999996721745) A[1]:(1.00508293599e-06) A[2]:(2.26749648391e-06) A[3]:(2.53500844448e-12)\n",
      " state (6)  A[0]:(0.978085815907) A[1]:(0.00105782155879) A[2]:(0.0208563394845) A[3]:(1.19141367206e-18)\n",
      " state (7)  A[0]:(0.0708910748363) A[1]:(0.752747297287) A[2]:(0.176361650229) A[3]:(5.47024474942e-22)\n",
      " state (8)  A[0]:(0.000701569020748) A[1]:(0.997231125832) A[2]:(0.00206733471714) A[3]:(3.31237684836e-24)\n",
      " state (9)  A[0]:(8.015930689e-06) A[1]:(0.99996727705) A[2]:(2.47124298767e-05) A[3]:(7.12465889526e-26)\n",
      " state (10)  A[0]:(1.29961563289e-06) A[1]:(0.999994337559) A[2]:(4.335106496e-06) A[3]:(1.62810906817e-26)\n",
      " state (11)  A[0]:(7.97559380317e-07) A[1]:(0.999996542931) A[2]:(2.67785731012e-06) A[3]:(1.0855047301e-26)\n",
      " state (12)  A[0]:(6.81304754835e-07) A[1]:(0.999997019768) A[2]:(2.29791976381e-06) A[3]:(9.53419832278e-27)\n",
      " state (13)  A[0]:(6.37501159417e-07) A[1]:(0.999997198582) A[2]:(2.16459193325e-06) A[3]:(9.05252401406e-27)\n",
      " state (14)  A[0]:(6.15312842456e-07) A[1]:(0.999997258186) A[2]:(2.10540679291e-06) A[3]:(8.82937421512e-27)\n",
      " state (15)  A[0]:(6.01718795679e-07) A[1]:(0.999997317791) A[2]:(2.07545576814e-06) A[3]:(8.70964839494e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 431000 finished after 13 . Running score: 0.1. Policy_loss: -92050.6121134, Value_loss: 1.62333812451. Times trained:               12276. Times reached goal: 117.               Steps done: 5082175.\n",
      " state (0)  A[0]:(0.994931817055) A[1]:(0.00148814881686) A[2]:(0.00163284281734) A[3]:(0.00194718118291)\n",
      " state (1)  A[0]:(0.0360121317208) A[1]:(0.00559822749346) A[2]:(0.0159261766821) A[3]:(0.942463457584)\n",
      " state (2)  A[0]:(0.267757236958) A[1]:(0.0122035034001) A[2]:(0.0467729754746) A[3]:(0.673266291618)\n",
      " state (3)  A[0]:(0.999996960163) A[1]:(7.31201737381e-07) A[2]:(2.31683929997e-06) A[3]:(2.31847770149e-11)\n",
      " state (4)  A[0]:(0.999998033047) A[1]:(4.90164211442e-07) A[2]:(1.44859041029e-06) A[3]:(6.53860082397e-12)\n",
      " state (5)  A[0]:(0.999995112419) A[1]:(1.39856012993e-06) A[2]:(3.50539653482e-06) A[3]:(1.00869551427e-12)\n",
      " state (6)  A[0]:(0.882090866566) A[1]:(0.00676078721881) A[2]:(0.111148357391) A[3]:(8.59977483735e-20)\n",
      " state (7)  A[0]:(0.0774580985308) A[1]:(0.710065424442) A[2]:(0.212476477027) A[3]:(2.525220498e-22)\n",
      " state (8)  A[0]:(0.00330032082275) A[1]:(0.988317549229) A[2]:(0.00838211923838) A[3]:(7.50379323631e-24)\n",
      " state (9)  A[0]:(7.45257420931e-05) A[1]:(0.999799728394) A[2]:(0.000125742109958) A[3]:(2.47643258279e-25)\n",
      " state (10)  A[0]:(7.08759762347e-06) A[1]:(0.999979257584) A[2]:(1.36713979373e-05) A[3]:(4.03246990117e-26)\n",
      " state (11)  A[0]:(3.1821000448e-06) A[1]:(0.999989807606) A[2]:(7.01012413629e-06) A[3]:(2.30949291358e-26)\n",
      " state (12)  A[0]:(2.42345231527e-06) A[1]:(0.999991834164) A[2]:(5.72805947741e-06) A[3]:(1.94646189834e-26)\n",
      " state (13)  A[0]:(2.1463365556e-06) A[1]:(0.999992549419) A[2]:(5.33124602953e-06) A[3]:(1.82828899399e-26)\n",
      " state (14)  A[0]:(1.99801320377e-06) A[1]:(0.999992847443) A[2]:(5.17114631293e-06) A[3]:(1.77768541582e-26)\n",
      " state (15)  A[0]:(1.89910315385e-06) A[1]:(0.999993026257) A[2]:(5.09443407282e-06) A[3]:(1.75136103496e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 432000 finished after 5 . Running score: 0.11. Policy_loss: -92050.6113906, Value_loss: 1.41321078263. Times trained:               12841. Times reached goal: 141.               Steps done: 5095016.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995046675205) A[1]:(0.00150372623466) A[2]:(0.00160928594414) A[3]:(0.00184028956573)\n",
      " state (1)  A[0]:(0.057423196733) A[1]:(0.00664481846616) A[2]:(0.0218217689544) A[3]:(0.914110183716)\n",
      " state (2)  A[0]:(0.999887108803) A[1]:(2.56651674135e-05) A[2]:(8.71454321896e-05) A[3]:(7.81929401228e-08)\n",
      " state (3)  A[0]:(0.999997973442) A[1]:(4.90368961437e-07) A[2]:(1.512198196e-06) A[3]:(6.60969716459e-12)\n",
      " state (4)  A[0]:(0.999996960163) A[1]:(9.22668959902e-07) A[2]:(2.12549775824e-06) A[3]:(2.41304242213e-12)\n",
      " state (5)  A[0]:(0.984796702862) A[1]:(0.000834364094771) A[2]:(0.0143689103425) A[3]:(4.02935289409e-19)\n",
      " state (6)  A[0]:(0.0752892941236) A[1]:(0.606473326683) A[2]:(0.318237394094) A[3]:(1.86421147876e-22)\n",
      " state (7)  A[0]:(0.00793558638543) A[1]:(0.94409686327) A[2]:(0.047967556864) A[3]:(1.83475257266e-23)\n",
      " state (8)  A[0]:(0.000671295099892) A[1]:(0.995276510715) A[2]:(0.00405218172818) A[3]:(2.31165926121e-24)\n",
      " state (9)  A[0]:(1.8873517547e-05) A[1]:(0.999852716923) A[2]:(0.000128435291117) A[3]:(1.32839825378e-25)\n",
      " state (10)  A[0]:(2.72282500191e-06) A[1]:(0.999979317188) A[2]:(1.79481412488e-05) A[3]:(2.61051976982e-26)\n",
      " state (11)  A[0]:(1.57782267252e-06) A[1]:(0.999988496304) A[2]:(9.93150661088e-06) A[3]:(1.60859770301e-26)\n",
      " state (12)  A[0]:(1.3610527958e-06) A[1]:(0.999990165234) A[2]:(8.47992396302e-06) A[3]:(1.41331164221e-26)\n",
      " state (13)  A[0]:(1.30164994516e-06) A[1]:(0.999990582466) A[2]:(8.12479174783e-06) A[3]:(1.36386947683e-26)\n",
      " state (14)  A[0]:(1.28090300677e-06) A[1]:(0.999990701675) A[2]:(8.0285126387e-06) A[3]:(1.34977598372e-26)\n",
      " state (15)  A[0]:(1.27171574604e-06) A[1]:(0.999990701675) A[2]:(8.0015452113e-06) A[3]:(1.34536021154e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 433000 finished after 12 . Running score: 0.08. Policy_loss: -92050.6114485, Value_loss: 1.43023439411. Times trained:               12846. Times reached goal: 133.               Steps done: 5107862.\n",
      " state (0)  A[0]:(0.995523571968) A[1]:(0.00146290124394) A[2]:(0.00151736172847) A[3]:(0.00149614689872)\n",
      " state (1)  A[0]:(0.0603793822229) A[1]:(0.0067819324322) A[2]:(0.0227564815432) A[3]:(0.910082221031)\n",
      " state (2)  A[0]:(0.999980330467) A[1]:(4.54880273537e-06) A[2]:(1.51193580677e-05) A[3]:(1.43251932538e-09)\n",
      " state (3)  A[0]:(0.999997913837) A[1]:(5.19290949796e-07) A[2]:(1.53997052621e-06) A[3]:(5.85544243301e-12)\n",
      " state (4)  A[0]:(0.999990165234) A[1]:(3.34167089022e-06) A[2]:(6.49144931231e-06) A[3]:(2.12692561588e-13)\n",
      " state (5)  A[0]:(0.38585793972) A[1]:(0.140473768115) A[2]:(0.473668307066) A[3]:(1.56781462624e-21)\n",
      " state (6)  A[0]:(0.0257925335318) A[1]:(0.821333408356) A[2]:(0.152874097228) A[3]:(3.68455661139e-23)\n",
      " state (7)  A[0]:(0.00694277090952) A[1]:(0.947767734528) A[2]:(0.0452894791961) A[3]:(1.16473821709e-23)\n",
      " state (8)  A[0]:(0.00143598462455) A[1]:(0.988694369793) A[2]:(0.009869675152) A[3]:(3.38996092395e-24)\n",
      " state (9)  A[0]:(7.12991168257e-05) A[1]:(0.99935489893) A[2]:(0.000573829980567) A[3]:(3.25911473497e-25)\n",
      " state (10)  A[0]:(6.04123169978e-06) A[1]:(0.999946892262) A[2]:(4.70775521535e-05) A[3]:(4.08543112575e-26)\n",
      " state (11)  A[0]:(2.56624321082e-06) A[1]:(0.999979317188) A[2]:(1.80926708708e-05) A[3]:(1.86272446732e-26)\n",
      " state (12)  A[0]:(2.02187538889e-06) A[1]:(0.999984204769) A[2]:(1.37613842526e-05) A[3]:(1.48967384006e-26)\n",
      " state (13)  A[0]:(1.89042577858e-06) A[1]:(0.999985337257) A[2]:(1.27677458295e-05) A[3]:(1.40131725867e-26)\n",
      " state (14)  A[0]:(1.85271346709e-06) A[1]:(0.999985635281) A[2]:(1.25101041704e-05) A[3]:(1.3781720489e-26)\n",
      " state (15)  A[0]:(1.84029522643e-06) A[1]:(0.999985694885) A[2]:(1.24420002976e-05) A[3]:(1.37202432644e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 434000 finished after 56 . Running score: 0.22. Policy_loss: -92050.611259, Value_loss: 1.40747537714. Times trained:               12329. Times reached goal: 145.               Steps done: 5120191.\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.7343e-06,  3.3025e-06,  2.0106e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.5083e-06,  3.2258e-06,  2.1343e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.4918e-06,  3.2201e-06,  2.1439e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0015,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.4316e-06,  3.1995e-06,  2.1797e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.4230e-06,  3.1965e-06,  2.1849e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.4153e-06,  3.1938e-06,  2.1896e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.4083e-06,  3.1915e-06,  2.1938e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.4021e-06,  3.1893e-06,  2.1976e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.0246e-04,  9.9743e-01,  1.7647e-03,  1.0231e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.0309e-04,  9.9743e-01,  1.7655e-03,  1.0235e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.1995e-05,  9.9990e-01,  6.8181e-05,  7.4419e-26]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.1299e-06,  9.9999e-01,  8.1187e-06,  1.2773e-26]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995302438736) A[1]:(0.00143976253457) A[2]:(0.00151568627916) A[3]:(0.00174212595448)\n",
      " state (1)  A[0]:(0.0592065677047) A[1]:(0.00668737711385) A[2]:(0.0220974311233) A[3]:(0.91200864315)\n",
      " state (2)  A[0]:(0.999990999699) A[1]:(2.14746955862e-06) A[2]:(6.83872349327e-06) A[3]:(2.77744466315e-10)\n",
      " state (3)  A[0]:(0.999998092651) A[1]:(5.11610380727e-07) A[2]:(1.37672918754e-06) A[3]:(5.11722261787e-12)\n",
      " state (4)  A[0]:(0.99999243021) A[1]:(4.38755478172e-06) A[2]:(3.18466777571e-06) A[3]:(2.20734224627e-13)\n",
      " state (5)  A[0]:(0.650067687035) A[1]:(0.183012351394) A[2]:(0.166919976473) A[3]:(5.49932531783e-21)\n",
      " state (6)  A[0]:(0.0345083959401) A[1]:(0.864378511906) A[2]:(0.10111310333) A[3]:(2.71310091233e-23)\n",
      " state (7)  A[0]:(0.00747347483411) A[1]:(0.971550107002) A[2]:(0.0209764372557) A[3]:(6.91174760492e-24)\n",
      " state (8)  A[0]:(0.000799955858383) A[1]:(0.9974386096) A[2]:(0.0017614181852) A[3]:(1.02174990077e-24)\n",
      " state (9)  A[0]:(3.18656529998e-05) A[1]:(0.999900043011) A[2]:(6.80623779772e-05) A[3]:(7.43092859504e-26)\n",
      " state (10)  A[0]:(5.99016311753e-06) A[1]:(0.999979674816) A[2]:(1.43154620673e-05) A[3]:(2.04681039809e-26)\n",
      " state (11)  A[0]:(3.74227488464e-06) A[1]:(0.999986946583) A[2]:(9.29811631067e-06) A[3]:(1.43169148501e-26)\n",
      " state (12)  A[0]:(3.27713746628e-06) A[1]:(0.999988377094) A[2]:(8.33901867736e-06) A[3]:(1.30718689338e-26)\n",
      " state (13)  A[0]:(3.12706765726e-06) A[1]:(0.999988734722) A[2]:(8.11749578133e-06) A[3]:(1.27715008996e-26)\n",
      " state (14)  A[0]:(3.05461776406e-06) A[1]:(0.999988853931) A[2]:(8.06864136393e-06) A[3]:(1.26982924506e-26)\n",
      " state (15)  A[0]:(3.00621672977e-06) A[1]:(0.999988913536) A[2]:(8.063995665e-06) A[3]:(1.26857053429e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 435000 finished after 30 . Running score: 0.16. Policy_loss: -92050.6112993, Value_loss: 1.63656842388. Times trained:               12782. Times reached goal: 142.               Steps done: 5132973.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996710896492) A[1]:(0.00126071111299) A[2]:(0.00121485441923) A[3]:(0.000813565391582)\n",
      " state (1)  A[0]:(0.054388795048) A[1]:(0.00642075529322) A[2]:(0.0207621213049) A[3]:(0.918428301811)\n",
      " state (2)  A[0]:(0.999935865402) A[1]:(1.48649296534e-05) A[2]:(4.92307408422e-05) A[3]:(2.73993734368e-08)\n",
      " state (3)  A[0]:(0.999998271465) A[1]:(4.56221982859e-07) A[2]:(1.24484063235e-06) A[3]:(4.81540935598e-12)\n",
      " state (4)  A[0]:(0.999993622303) A[1]:(3.85996645491e-06) A[2]:(2.53880421042e-06) A[3]:(2.10021358486e-13)\n",
      " state (5)  A[0]:(0.744373381138) A[1]:(0.153716817498) A[2]:(0.101909779012) A[3]:(8.1616109661e-21)\n",
      " state (6)  A[0]:(0.0599986054003) A[1]:(0.833103477955) A[2]:(0.106897898018) A[3]:(2.88943877701e-23)\n",
      " state (7)  A[0]:(0.0135017456487) A[1]:(0.96399641037) A[2]:(0.0225018467754) A[3]:(7.38836789752e-24)\n",
      " state (8)  A[0]:(0.00125685741659) A[1]:(0.99714243412) A[2]:(0.00160071917344) A[3]:(9.56833353593e-25)\n",
      " state (9)  A[0]:(4.90511629323e-05) A[1]:(0.999888300896) A[2]:(6.26491455478e-05) A[3]:(6.89188453106e-26)\n",
      " state (10)  A[0]:(1.08540070869e-05) A[1]:(0.99997395277) A[2]:(1.51977446876e-05) A[3]:(2.12640245749e-26)\n",
      " state (11)  A[0]:(7.16916974852e-06) A[1]:(0.99998241663) A[2]:(1.04151758933e-05) A[3]:(1.55300673665e-26)\n",
      " state (12)  A[0]:(6.32687761026e-06) A[1]:(0.999984204769) A[2]:(9.48867909756e-06) A[3]:(1.43527479323e-26)\n",
      " state (13)  A[0]:(6.01343162998e-06) A[1]:(0.999984681606) A[2]:(9.28542340262e-06) A[3]:(1.40748793821e-26)\n",
      " state (14)  A[0]:(5.84262170378e-06) A[1]:(0.999984920025) A[2]:(9.25372478378e-06) A[3]:(1.40172817508e-26)\n",
      " state (15)  A[0]:(5.72559520151e-06) A[1]:(0.99998497963) A[2]:(9.2664067779e-06) A[3]:(1.40204371944e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 436000 finished after 8 . Running score: 0.16. Policy_loss: -92050.6115795, Value_loss: 1.62728006987. Times trained:               13193. Times reached goal: 146.               Steps done: 5146166.\n",
      " state (0)  A[0]:(0.996565759182) A[1]:(0.0013037318131) A[2]:(0.00127519713715) A[3]:(0.000855295686051)\n",
      " state (1)  A[0]:(0.0485339462757) A[1]:(0.00619981344789) A[2]:(0.0193904414773) A[3]:(0.925875782967)\n",
      " state (2)  A[0]:(0.999990999699) A[1]:(2.20009428631e-06) A[2]:(6.81039227857e-06) A[3]:(3.19875875787e-10)\n",
      " state (3)  A[0]:(0.999998033047) A[1]:(6.85998429617e-07) A[2]:(1.27553289531e-06) A[3]:(2.83731350047e-12)\n",
      " state (4)  A[0]:(0.999862909317) A[1]:(0.000128105573822) A[2]:(8.97241352504e-06) A[3]:(1.88609821629e-15)\n",
      " state (5)  A[0]:(0.148805975914) A[1]:(0.715186834335) A[2]:(0.136007219553) A[3]:(1.44444735771e-22)\n",
      " state (6)  A[0]:(0.0189994908869) A[1]:(0.940106332302) A[2]:(0.0408941507339) A[3]:(1.4099481353e-23)\n",
      " state (7)  A[0]:(0.00677646696568) A[1]:(0.976922094822) A[2]:(0.0163014475256) A[3]:(6.58607111336e-24)\n",
      " state (8)  A[0]:(0.00139662856236) A[1]:(0.994747817516) A[2]:(0.0038555462379) A[3]:(2.04000633103e-24)\n",
      " state (9)  A[0]:(7.35223075026e-05) A[1]:(0.999690830708) A[2]:(0.00023565403535) A[3]:(1.96921991916e-25)\n",
      " state (10)  A[0]:(9.38024913921e-06) A[1]:(0.999965727329) A[2]:(2.48790074693e-05) A[3]:(3.04020614192e-26)\n",
      " state (11)  A[0]:(4.81551160192e-06) A[1]:(0.99998408556) A[2]:(1.11103927338e-05) A[3]:(1.57031499202e-26)\n",
      " state (12)  A[0]:(4.01839952247e-06) A[1]:(0.999987125397) A[2]:(8.88180693437e-06) A[3]:(1.30842765448e-26)\n",
      " state (13)  A[0]:(3.82518237529e-06) A[1]:(0.999987781048) A[2]:(8.36726212583e-06) A[3]:(1.24646964076e-26)\n",
      " state (14)  A[0]:(3.77231322091e-06) A[1]:(0.999988019466) A[2]:(8.23755635793e-06) A[3]:(1.23080204659e-26)\n",
      " state (15)  A[0]:(3.75635954697e-06) A[1]:(0.999988019466) A[2]:(8.2052538346e-06) A[3]:(1.22693454815e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 437000 finished after 12 . Running score: 0.14. Policy_loss: -92050.6114436, Value_loss: 1.63087848896. Times trained:               12962. Times reached goal: 121.               Steps done: 5159128.\n",
      " state (0)  A[0]:(0.996574759483) A[1]:(0.00124871032313) A[2]:(0.00132568355184) A[3]:(0.000850850774441)\n",
      " state (1)  A[0]:(0.0447688959539) A[1]:(0.00579543365166) A[2]:(0.0184846948832) A[3]:(0.93095099926)\n",
      " state (2)  A[0]:(0.998762905598) A[1]:(0.000252272002399) A[2]:(0.000962558959145) A[3]:(2.22793532885e-05)\n",
      " state (3)  A[0]:(0.999998450279) A[1]:(3.46386997307e-07) A[2]:(1.1850734154e-06) A[3]:(5.23608847253e-12)\n",
      " state (4)  A[0]:(0.999997913837) A[1]:(7.04101751126e-07) A[2]:(1.3817693798e-06) A[3]:(1.56396911481e-12)\n",
      " state (5)  A[0]:(0.999580562115) A[1]:(0.000233563478105) A[2]:(0.000185844735825) A[3]:(1.58732535021e-17)\n",
      " state (6)  A[0]:(0.291038066149) A[1]:(0.43474856019) A[2]:(0.274213403463) A[3]:(7.7426388614e-23)\n",
      " state (7)  A[0]:(0.0487372241914) A[1]:(0.878740012646) A[2]:(0.0725227445364) A[3]:(1.35086513562e-23)\n",
      " state (8)  A[0]:(0.00398819567636) A[1]:(0.990562975407) A[2]:(0.00544880516827) A[3]:(1.74694706854e-24)\n",
      " state (9)  A[0]:(0.00012004916789) A[1]:(0.999706029892) A[2]:(0.000173891661689) A[3]:(1.05301060956e-25)\n",
      " state (10)  A[0]:(2.30524765357e-05) A[1]:(0.999943554401) A[2]:(3.33649186359e-05) A[3]:(2.69922502157e-26)\n",
      " state (11)  A[0]:(1.48397966768e-05) A[1]:(0.999964118004) A[2]:(2.10547459574e-05) A[3]:(1.85095225906e-26)\n",
      " state (12)  A[0]:(1.32049090098e-05) A[1]:(0.999968111515) A[2]:(1.86714150914e-05) A[3]:(1.67734569831e-26)\n",
      " state (13)  A[0]:(1.27544208226e-05) A[1]:(0.999969184399) A[2]:(1.80757251655e-05) A[3]:(1.63300062208e-26)\n",
      " state (14)  A[0]:(1.25951355585e-05) A[1]:(0.999969482422) A[2]:(1.79089147423e-05) A[3]:(1.62034248604e-26)\n",
      " state (15)  A[0]:(1.2520384189e-05) A[1]:(0.999969601631) A[2]:(1.78580576176e-05) A[3]:(1.61641620822e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 438000 finished after 19 . Running score: 0.12. Policy_loss: -92050.6221638, Value_loss: 1.2060076291. Times trained:               12704. Times reached goal: 140.               Steps done: 5171832.\n",
      " state (0)  A[0]:(0.996918439865) A[1]:(0.00118828855921) A[2]:(0.00121399434283) A[3]:(0.000679291319102)\n",
      " state (1)  A[0]:(0.0530095994473) A[1]:(0.00609402125701) A[2]:(0.0201792828739) A[3]:(0.920717120171)\n",
      " state (2)  A[0]:(0.999941170216) A[1]:(1.32720715555e-05) A[2]:(4.55088775198e-05) A[3]:(2.49659670715e-08)\n",
      " state (3)  A[0]:(0.999998390675) A[1]:(4.00182216254e-07) A[2]:(1.20088623135e-06) A[3]:(5.35782572822e-12)\n",
      " state (4)  A[0]:(0.999997198582) A[1]:(1.25087876768e-06) A[2]:(1.5599642893e-06) A[3]:(8.68035461218e-13)\n",
      " state (5)  A[0]:(0.993927240372) A[1]:(0.00328117981553) A[2]:(0.00279159331694) A[3]:(2.09479341301e-19)\n",
      " state (6)  A[0]:(0.0748480260372) A[1]:(0.818440616131) A[2]:(0.106711342931) A[3]:(4.03108742983e-23)\n",
      " state (7)  A[0]:(0.00199584313668) A[1]:(0.994339346886) A[2]:(0.00366480322555) A[3]:(2.02231198371e-24)\n",
      " state (8)  A[0]:(2.58273230429e-05) A[1]:(0.999921619892) A[2]:(5.25619689142e-05) A[3]:(6.06177051458e-26)\n",
      " state (9)  A[0]:(5.30135503141e-06) A[1]:(0.999985337257) A[2]:(9.36139804253e-06) A[3]:(1.47705129521e-26)\n",
      " state (10)  A[0]:(3.77826108888e-06) A[1]:(0.99998986721) A[2]:(6.33610397927e-06) A[3]:(1.07771588421e-26)\n",
      " state (11)  A[0]:(3.48648723048e-06) A[1]:(0.999990701675) A[2]:(5.80851019549e-06) A[3]:(1.00544728898e-26)\n",
      " state (12)  A[0]:(3.40311339642e-06) A[1]:(0.999990880489) A[2]:(5.69454596189e-06) A[3]:(9.90267340271e-27)\n",
      " state (13)  A[0]:(3.36818447977e-06) A[1]:(0.999990940094) A[2]:(5.67295137444e-06) A[3]:(9.87973326596e-27)\n",
      " state (14)  A[0]:(3.3476446788e-06) A[1]:(0.999990999699) A[2]:(5.67502956983e-06) A[3]:(9.88965026443e-27)\n",
      " state (15)  A[0]:(3.33292132382e-06) A[1]:(0.999990999699) A[2]:(5.68363066122e-06) A[3]:(9.90815305859e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 439000 finished after 8 . Running score: 0.14. Policy_loss: -92050.6113798, Value_loss: 1.43552652328. Times trained:               12911. Times reached goal: 123.               Steps done: 5184743.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9969,  0.0012,  0.0012,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0012,  0.0012,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0012,  0.0012,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3293e-06,  2.1287e-06,  8.5038e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0012,  0.0012,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0012,  0.0012,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0012,  0.0012,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0012,  0.0012,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0012,  0.0012,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0012,  0.0012,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3255e-06,  2.1271e-06,  8.5425e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3251e-06,  2.1269e-06,  8.5462e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3248e-06,  2.1268e-06,  8.5494e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3245e-06,  2.1267e-06,  8.5523e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.2292e-03,  9.8801e-01,  9.7567e-03,  5.1485e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.6370e-05,  9.9954e-01,  3.8528e-04,  3.5945e-25]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.8416e-06,  9.9997e-01,  2.4116e-05,  3.7152e-26]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996934711933) A[1]:(0.00117501604836) A[2]:(0.00122170161922) A[3]:(0.000668558757752)\n",
      " state (1)  A[0]:(0.0442541949451) A[1]:(0.00555453868583) A[2]:(0.0184672512114) A[3]:(0.931724011898)\n",
      " state (2)  A[0]:(0.999750673771) A[1]:(5.1419086958e-05) A[2]:(0.000197278757696) A[3]:(6.03020680501e-07)\n",
      " state (3)  A[0]:(0.999998211861) A[1]:(4.01362029834e-07) A[2]:(1.38513416914e-06) A[3]:(5.98561911841e-12)\n",
      " state (4)  A[0]:(0.999996542931) A[1]:(1.32516152007e-06) A[2]:(2.12690611079e-06) A[3]:(8.54632608172e-13)\n",
      " state (5)  A[0]:(0.983685672283) A[1]:(0.00417790934443) A[2]:(0.0121363932267) A[3]:(7.03943561356e-20)\n",
      " state (6)  A[0]:(0.0997401922941) A[1]:(0.614980101585) A[2]:(0.285279691219) A[3]:(8.84538436536e-23)\n",
      " state (7)  A[0]:(0.0191722195596) A[1]:(0.905306041241) A[2]:(0.0755217522383) A[3]:(2.5680008351e-23)\n",
      " state (8)  A[0]:(0.00222225440666) A[1]:(0.988054394722) A[2]:(0.00972334388644) A[3]:(5.13611558171e-24)\n",
      " state (9)  A[0]:(7.61517076171e-05) A[1]:(0.999539792538) A[2]:(0.000384030456189) A[3]:(3.5858128507e-25)\n",
      " state (10)  A[0]:(1.19392070701e-05) A[1]:(0.999934196472) A[2]:(5.3855117585e-05) A[3]:(7.12292833165e-26)\n",
      " state (11)  A[0]:(7.02711486156e-06) A[1]:(0.999963521957) A[2]:(2.9435255783e-05) A[3]:(4.36168435993e-26)\n",
      " state (12)  A[0]:(6.09418475506e-06) A[1]:(0.999968826771) A[2]:(2.51060391747e-05) A[3]:(3.83670328234e-26)\n",
      " state (13)  A[0]:(5.83993914915e-06) A[1]:(0.999970078468) A[2]:(2.410520392e-05) A[3]:(3.71431243734e-26)\n",
      " state (14)  A[0]:(5.74644082008e-06) A[1]:(0.999970376492) A[2]:(2.38763295783e-05) A[3]:(3.68741967604e-26)\n",
      " state (15)  A[0]:(5.69753683521e-06) A[1]:(0.999970436096) A[2]:(2.38466564042e-05) A[3]:(3.68525431448e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 440000 finished after 17 . Running score: 0.12. Policy_loss: -92050.6112233, Value_loss: 1.64451063183. Times trained:               13005. Times reached goal: 130.               Steps done: 5197748.\n",
      " state (0)  A[0]:(0.988967001438) A[1]:(0.00159578211606) A[2]:(0.00225041341037) A[3]:(0.00718678534031)\n",
      " state (1)  A[0]:(0.0217267051339) A[1]:(0.00408770143986) A[2]:(0.0112459445372) A[3]:(0.962939679623)\n",
      " state (2)  A[0]:(0.99999076128) A[1]:(1.96101268557e-06) A[2]:(7.27515225663e-06) A[3]:(3.95729116143e-10)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(4.15818107058e-07) A[2]:(1.4173880345e-06) A[3]:(6.03820509218e-12)\n",
      " state (4)  A[0]:(0.999993264675) A[1]:(2.23624442697e-06) A[2]:(4.48664877695e-06) A[3]:(2.85428798652e-13)\n",
      " state (5)  A[0]:(0.91715413332) A[1]:(0.00802352000028) A[2]:(0.0748223513365) A[3]:(6.36714198577e-21)\n",
      " state (6)  A[0]:(0.095655567944) A[1]:(0.651963829994) A[2]:(0.25238057971) A[3]:(1.53195841161e-22)\n",
      " state (7)  A[0]:(0.0132595291361) A[1]:(0.947821080685) A[2]:(0.0389194041491) A[3]:(3.20093722929e-23)\n",
      " state (8)  A[0]:(0.000661663711071) A[1]:(0.997544407845) A[2]:(0.00179393240251) A[3]:(2.70068286118e-24)\n",
      " state (9)  A[0]:(2.84221514448e-05) A[1]:(0.999893248081) A[2]:(7.83459545346e-05) A[3]:(1.98543002468e-25)\n",
      " state (10)  A[0]:(9.60123088589e-06) A[1]:(0.999965786934) A[2]:(2.46297167905e-05) A[3]:(7.63576310835e-26)\n",
      " state (11)  A[0]:(7.35065123081e-06) A[1]:(0.999974191189) A[2]:(1.84331020137e-05) A[3]:(6.02304175821e-26)\n",
      " state (12)  A[0]:(6.8417652983e-06) A[1]:(0.999975979328) A[2]:(1.72026884684e-05) A[3]:(5.69114578867e-26)\n",
      " state (13)  A[0]:(6.67506174068e-06) A[1]:(0.999976396561) A[2]:(1.69340546563e-05) A[3]:(5.6167605194e-26)\n",
      " state (14)  A[0]:(6.59333272779e-06) A[1]:(0.99997651577) A[2]:(1.68873521034e-05) A[3]:(5.60253144082e-26)\n",
      " state (15)  A[0]:(6.53831875752e-06) A[1]:(0.999976575375) A[2]:(1.68941187439e-05) A[3]:(5.60304420041e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 441000 finished after 36 . Running score: 0.14. Policy_loss: -92050.6118806, Value_loss: 1.41340776191. Times trained:               13334. Times reached goal: 136.               Steps done: 5211082.\n",
      " state (0)  A[0]:(0.993186712265) A[1]:(0.00132366036996) A[2]:(0.00181381264701) A[3]:(0.00367581075989)\n",
      " state (1)  A[0]:(0.0371413975954) A[1]:(0.00469175633043) A[2]:(0.0150264045224) A[3]:(0.94314044714)\n",
      " state (2)  A[0]:(0.99999499321) A[1]:(1.08553888367e-06) A[2]:(3.89183787775e-06) A[3]:(1.26591764849e-10)\n",
      " state (3)  A[0]:(0.999998271465) A[1]:(5.79963398195e-07) A[2]:(1.16869034628e-06) A[3]:(2.52973770404e-12)\n",
      " state (4)  A[0]:(0.999788999557) A[1]:(0.000185793105629) A[2]:(2.52298505075e-05) A[3]:(5.20048795219e-16)\n",
      " state (5)  A[0]:(0.256262928247) A[1]:(0.538399755955) A[2]:(0.205337300897) A[3]:(2.48677381562e-22)\n",
      " state (6)  A[0]:(0.0640962272882) A[1]:(0.862807273865) A[2]:(0.0730965286493) A[3]:(5.55508528827e-23)\n",
      " state (7)  A[0]:(0.0298431236297) A[1]:(0.939769625664) A[2]:(0.0303872395307) A[3]:(2.84243244893e-23)\n",
      " state (8)  A[0]:(0.00749900192022) A[1]:(0.986915171146) A[2]:(0.00558585347608) A[3]:(7.78162846941e-24)\n",
      " state (9)  A[0]:(0.000516196305398) A[1]:(0.999216139317) A[2]:(0.000267650553724) A[3]:(6.70654683828e-25)\n",
      " state (10)  A[0]:(6.54243267491e-05) A[1]:(0.999897480011) A[2]:(3.71166061086e-05) A[3]:(1.26370278487e-25)\n",
      " state (11)  A[0]:(3.22045889334e-05) A[1]:(0.999947845936) A[2]:(1.99379010155e-05) A[3]:(7.40663352805e-26)\n",
      " state (12)  A[0]:(2.60934448306e-05) A[1]:(0.999956965446) A[2]:(1.69219747477e-05) A[3]:(6.41338985564e-26)\n",
      " state (13)  A[0]:(2.41730012931e-05) A[1]:(0.999959588051) A[2]:(1.62169799296e-05) A[3]:(6.16112877819e-26)\n",
      " state (14)  A[0]:(2.32477286772e-05) A[1]:(0.999960720539) A[2]:(1.60391664394e-05) A[3]:(6.08311043466e-26)\n",
      " state (15)  A[0]:(2.26339252549e-05) A[1]:(0.99996137619) A[2]:(1.59949995577e-05) A[3]:(6.05175259738e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 442000 finished after 14 . Running score: 0.15. Policy_loss: -92050.611387, Value_loss: 2.05603001889. Times trained:               13053. Times reached goal: 161.               Steps done: 5224135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996218025684) A[1]:(0.00116955582052) A[2]:(0.00141315988731) A[3]:(0.0011992668733)\n",
      " state (1)  A[0]:(0.0272776465863) A[1]:(0.004290192388) A[2]:(0.0127797937021) A[3]:(0.955652356148)\n",
      " state (2)  A[0]:(0.984252512455) A[1]:(0.00228125043213) A[2]:(0.00915223266929) A[3]:(0.00431400584057)\n",
      " state (3)  A[0]:(0.999998390675) A[1]:(3.63252837587e-07) A[2]:(1.27115083615e-06) A[3]:(6.54466758565e-12)\n",
      " state (4)  A[0]:(0.999996483326) A[1]:(1.72103523255e-06) A[2]:(1.78162395059e-06) A[3]:(7.40476853011e-13)\n",
      " state (5)  A[0]:(0.965794801712) A[1]:(0.0236086528748) A[2]:(0.0105965342373) A[3]:(5.88087210121e-19)\n",
      " state (6)  A[0]:(0.0877603292465) A[1]:(0.726566135883) A[2]:(0.185673490167) A[3]:(1.37819162448e-22)\n",
      " state (7)  A[0]:(0.022805467248) A[1]:(0.926463484764) A[2]:(0.0507310405374) A[3]:(3.48639348982e-23)\n",
      " state (8)  A[0]:(0.0035345335491) A[1]:(0.990646600723) A[2]:(0.00581888621673) A[3]:(5.88584267515e-24)\n",
      " state (9)  A[0]:(0.000163836666616) A[1]:(0.99960321188) A[2]:(0.000232945472817) A[3]:(3.89461516376e-25)\n",
      " state (10)  A[0]:(2.56521580013e-05) A[1]:(0.999933183193) A[2]:(4.11662076658e-05) A[3]:(8.69568216728e-26)\n",
      " state (11)  A[0]:(1.49208235598e-05) A[1]:(0.999960064888) A[2]:(2.50381508522e-05) A[3]:(5.65362127779e-26)\n",
      " state (12)  A[0]:(1.27726270875e-05) A[1]:(0.999965131283) A[2]:(2.20683214138e-05) A[3]:(5.05748926062e-26)\n",
      " state (13)  A[0]:(1.20787199194e-05) A[1]:(0.99996650219) A[2]:(2.13947241718e-05) A[3]:(4.91025915787e-26)\n",
      " state (14)  A[0]:(1.17537965707e-05) A[1]:(0.999966979027) A[2]:(2.12512895814e-05) A[3]:(4.86993110928e-26)\n",
      " state (15)  A[0]:(1.15547454698e-05) A[1]:(0.999967217445) A[2]:(2.12392205867e-05) A[3]:(4.8579832562e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 443000 finished after 12 . Running score: 0.15. Policy_loss: -92050.6114368, Value_loss: 1.40850144733. Times trained:               12441. Times reached goal: 120.               Steps done: 5236576.\n",
      " state (0)  A[0]:(0.995524287224) A[1]:(0.00122600723989) A[2]:(0.00152315339074) A[3]:(0.00172655319329)\n",
      " state (1)  A[0]:(0.0239440333098) A[1]:(0.00407940987498) A[2]:(0.0118200471625) A[3]:(0.96015650034)\n",
      " state (2)  A[0]:(0.856254160404) A[1]:(0.00842367485166) A[2]:(0.0347272232175) A[3]:(0.100594945252)\n",
      " state (3)  A[0]:(0.999998390675) A[1]:(3.18312743275e-07) A[2]:(1.27754572077e-06) A[3]:(7.0113125375e-12)\n",
      " state (4)  A[0]:(0.999998033047) A[1]:(5.45345415048e-07) A[2]:(1.42462852182e-06) A[3]:(3.19683841031e-12)\n",
      " state (5)  A[0]:(0.999772012234) A[1]:(0.000125826671137) A[2]:(0.000102156554931) A[3]:(4.06601966824e-16)\n",
      " state (6)  A[0]:(0.256614536047) A[1]:(0.363486588001) A[2]:(0.379898905754) A[3]:(4.78803480936e-22)\n",
      " state (7)  A[0]:(0.048028524965) A[1]:(0.834016263485) A[2]:(0.117955207825) A[3]:(6.8076152833e-23)\n",
      " state (8)  A[0]:(0.00479565840214) A[1]:(0.986080348492) A[2]:(0.00912402290851) A[3]:(8.13010777429e-24)\n",
      " state (9)  A[0]:(0.000161507210578) A[1]:(0.999561488628) A[2]:(0.000277019833447) A[3]:(4.33540352051e-25)\n",
      " state (10)  A[0]:(2.93557550322e-05) A[1]:(0.999917864799) A[2]:(5.27725242137e-05) A[3]:(1.05712612155e-25)\n",
      " state (11)  A[0]:(1.83964457392e-05) A[1]:(0.999948203564) A[2]:(3.33999669238e-05) A[3]:(7.17388504834e-26)\n",
      " state (12)  A[0]:(1.61023581313e-05) A[1]:(0.999954223633) A[2]:(2.96481957776e-05) A[3]:(6.47813807963e-26)\n",
      " state (13)  A[0]:(1.53468736244e-05) A[1]:(0.999955892563) A[2]:(2.87312395812e-05) A[3]:(6.29807564763e-26)\n",
      " state (14)  A[0]:(1.49824254549e-05) A[1]:(0.999956488609) A[2]:(2.85122914647e-05) A[3]:(6.24747083686e-26)\n",
      " state (15)  A[0]:(1.47459850268e-05) A[1]:(0.999956786633) A[2]:(2.84897942038e-05) A[3]:(6.2343313724e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 444000 finished after 6 . Running score: 0.18. Policy_loss: -92050.6118697, Value_loss: 1.62969221897. Times trained:               12450. Times reached goal: 136.               Steps done: 5249026.\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0011,  0.0013,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.0182e-06,  2.7548e-06,  2.7638e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.0228e-05,  9.9997e-01,  9.2821e-06,  2.6775e-26]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0043e-05,  9.9999e-01,  4.5906e-06,  1.4897e-26]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.3227e-06,  9.9999e-01,  3.9077e-06,  1.3025e-26]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.2799e-06,  9.9999e-01,  3.9153e-06,  1.3043e-26]])\n",
      "On state=14, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.2795e-06,  9.9999e-01,  3.9151e-06,  1.3043e-26]])\n",
      "On state=14, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.2792e-06,  9.9999e-01,  3.9149e-06,  1.3043e-26]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.3213e-06,  9.9999e-01,  3.9068e-06,  1.3026e-26]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.3212e-06,  9.9999e-01,  3.9067e-06,  1.3026e-26]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.2785e-06,  9.9999e-01,  3.9144e-06,  1.3044e-26]])\n",
      "On state=14, selected action=1\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.996218919754) A[1]:(0.00111715169623) A[2]:(0.00132691673934) A[3]:(0.00133703823667)\n",
      " state (1)  A[0]:(0.0495517365634) A[1]:(0.00510498508811) A[2]:(0.017510574311) A[3]:(0.927832722664)\n",
      " state (2)  A[0]:(0.999996066093) A[1]:(8.67543064942e-07) A[2]:(3.04742661683e-06) A[3]:(7.6487344236e-11)\n",
      " state (3)  A[0]:(0.999998569489) A[1]:(3.83230911893e-07) A[2]:(1.02057288132e-06) A[3]:(3.79718131938e-12)\n",
      " state (4)  A[0]:(0.999990224838) A[1]:(7.01867611497e-06) A[2]:(2.754609568e-06) A[3]:(2.76444584455e-14)\n",
      " state (5)  A[0]:(0.667321562767) A[1]:(0.217332974076) A[2]:(0.115345433354) A[3]:(4.37214986448e-22)\n",
      " state (6)  A[0]:(0.0265976712108) A[1]:(0.95891982317) A[2]:(0.0144824879244) A[3]:(1.17548005715e-23)\n",
      " state (7)  A[0]:(0.000497047847603) A[1]:(0.999312222004) A[2]:(0.000190713777556) A[3]:(3.44346437947e-25)\n",
      " state (8)  A[0]:(2.02187948162e-05) A[1]:(0.999970495701) A[2]:(9.27656401473e-06) A[3]:(2.67769928777e-26)\n",
      " state (9)  A[0]:(1.00389670479e-05) A[1]:(0.999985396862) A[2]:(4.58834347228e-06) A[3]:(1.48982914705e-26)\n",
      " state (10)  A[0]:(8.77769525687e-06) A[1]:(0.999987185001) A[2]:(4.01565694119e-06) A[3]:(1.33432571148e-26)\n",
      " state (11)  A[0]:(8.48689251143e-06) A[1]:(0.999987602234) A[2]:(3.91550474887e-06) A[3]:(1.30621406764e-26)\n",
      " state (12)  A[0]:(8.38013420434e-06) A[1]:(0.999987721443) A[2]:(3.90186232835e-06) A[3]:(1.30186639635e-26)\n",
      " state (13)  A[0]:(8.32017121866e-06) A[1]:(0.999987781048) A[2]:(3.90609193346e-06) A[3]:(1.30255202741e-26)\n",
      " state (14)  A[0]:(8.27779695101e-06) A[1]:(0.999987781048) A[2]:(3.91399726141e-06) A[3]:(1.30435207857e-26)\n",
      " state (15)  A[0]:(8.24500693852e-06) A[1]:(0.999987840652) A[2]:(3.92220317735e-06) A[3]:(1.30631406193e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 445000 finished after 11 . Running score: 0.17. Policy_loss: -92050.6112907, Value_loss: 2.0606617364. Times trained:               12623. Times reached goal: 138.               Steps done: 5261649.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996708273888) A[1]:(0.0010318615241) A[2]:(0.00111565680709) A[3]:(0.00114419113379)\n",
      " state (1)  A[0]:(0.0520187057555) A[1]:(0.00500761065632) A[2]:(0.0162072665989) A[3]:(0.926766395569)\n",
      " state (2)  A[0]:(0.99958139658) A[1]:(9.73834175966e-05) A[2]:(0.000317289610393) A[3]:(3.95649021812e-06)\n",
      " state (3)  A[0]:(0.999998867512) A[1]:(2.9041734706e-07) A[2]:(8.28854069823e-07) A[3]:(5.71648024056e-12)\n",
      " state (4)  A[0]:(0.999998688698) A[1]:(4.62413368041e-07) A[2]:(8.30980638966e-07) A[3]:(3.04454096482e-12)\n",
      " state (5)  A[0]:(0.999962508678) A[1]:(3.06046422338e-05) A[2]:(6.85871054884e-06) A[3]:(4.49119018546e-15)\n",
      " state (6)  A[0]:(0.448377460241) A[1]:(0.483525216579) A[2]:(0.0680973604321) A[3]:(7.46600023563e-22)\n",
      " state (7)  A[0]:(0.0120262391865) A[1]:(0.985145747662) A[2]:(0.00282803736627) A[3]:(9.70442091676e-24)\n",
      " state (8)  A[0]:(0.000170997358509) A[1]:(0.999801635742) A[2]:(2.73952664429e-05) A[3]:(2.07758771386e-25)\n",
      " state (9)  A[0]:(1.39332814797e-05) A[1]:(0.99998319149) A[2]:(2.87325042336e-06) A[3]:(3.09638166654e-26)\n",
      " state (10)  A[0]:(7.69498183217e-06) A[1]:(0.999990642071) A[2]:(1.69132420069e-06) A[3]:(1.9830111183e-26)\n",
      " state (11)  A[0]:(6.66305459163e-06) A[1]:(0.999991834164) A[2]:(1.49611878442e-06) A[3]:(1.78791210382e-26)\n",
      " state (12)  A[0]:(6.35704827801e-06) A[1]:(0.999992191792) A[2]:(1.44957959947e-06) A[3]:(1.73892384161e-26)\n",
      " state (13)  A[0]:(6.21836989012e-06) A[1]:(0.999992370605) A[2]:(1.43666238728e-06) A[3]:(1.72379311966e-26)\n",
      " state (14)  A[0]:(6.13156908003e-06) A[1]:(0.99999243021) A[2]:(1.43296517763e-06) A[3]:(1.71831130675e-26)\n",
      " state (15)  A[0]:(6.06615958532e-06) A[1]:(0.999992489815) A[2]:(1.43222746374e-06) A[3]:(1.71612992145e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 446000 finished after 6 . Running score: 0.13. Policy_loss: -92050.6112972, Value_loss: 1.8347242255. Times trained:               12451. Times reached goal: 127.               Steps done: 5274100.\n",
      " state (0)  A[0]:(0.996608614922) A[1]:(0.00107115064748) A[2]:(0.00115338584874) A[3]:(0.00116683193482)\n",
      " state (1)  A[0]:(0.0405279621482) A[1]:(0.00458095688373) A[2]:(0.0143922585994) A[3]:(0.940498828888)\n",
      " state (2)  A[0]:(0.999868452549) A[1]:(2.81501288555e-05) A[2]:(0.000103172758827) A[3]:(2.44095105018e-07)\n",
      " state (3)  A[0]:(0.999998629093) A[1]:(3.23102938182e-07) A[2]:(1.03083107206e-06) A[3]:(4.91861629551e-12)\n",
      " state (4)  A[0]:(0.999996185303) A[1]:(2.20109609472e-06) A[2]:(1.61104298968e-06) A[3]:(2.36382284189e-13)\n",
      " state (5)  A[0]:(0.889219403267) A[1]:(0.0437563471496) A[2]:(0.0670242831111) A[3]:(3.04499945908e-21)\n",
      " state (6)  A[0]:(0.110445104539) A[1]:(0.745605111122) A[2]:(0.143949791789) A[3]:(6.47647293231e-23)\n",
      " state (7)  A[0]:(0.0326307155192) A[1]:(0.923621296883) A[2]:(0.0437480024993) A[3]:(2.16577990773e-23)\n",
      " state (8)  A[0]:(0.00767487240955) A[1]:(0.983806431293) A[2]:(0.0085186753422) A[3]:(5.91755646126e-24)\n",
      " state (9)  A[0]:(0.000456189387478) A[1]:(0.999122679234) A[2]:(0.00042111671064) A[3]:(5.1641531774e-25)\n",
      " state (10)  A[0]:(3.36067641911e-05) A[1]:(0.999929428101) A[2]:(3.69888839487e-05) A[3]:(6.64583080285e-26)\n",
      " state (11)  A[0]:(1.33870789796e-05) A[1]:(0.999970853329) A[2]:(1.57506547112e-05) A[3]:(3.23489208295e-26)\n",
      " state (12)  A[0]:(1.04000737338e-05) A[1]:(0.999977111816) A[2]:(1.24634843814e-05) A[3]:(2.65807514016e-26)\n",
      " state (13)  A[0]:(9.62827562034e-06) A[1]:(0.999978661537) A[2]:(1.17014569696e-05) A[3]:(2.51925041198e-26)\n",
      " state (14)  A[0]:(9.3443750302e-06) A[1]:(0.999979138374) A[2]:(1.15055772767e-05) A[3]:(2.48145288127e-26)\n",
      " state (15)  A[0]:(9.19355989026e-06) A[1]:(0.999979376793) A[2]:(1.14566373668e-05) A[3]:(2.47037462408e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 447000 finished after 6 . Running score: 0.13. Policy_loss: -92050.6114231, Value_loss: 1.42287422118. Times trained:               12887. Times reached goal: 132.               Steps done: 5286987.\n",
      " state (0)  A[0]:(0.996244430542) A[1]:(0.00112042867113) A[2]:(0.00117917591706) A[3]:(0.00145594775677)\n",
      " state (1)  A[0]:(0.0352323427796) A[1]:(0.00447862222791) A[2]:(0.0128064183518) A[3]:(0.947482645512)\n",
      " state (2)  A[0]:(0.998235702515) A[1]:(0.000382987549528) A[2]:(0.00130071293097) A[3]:(8.061677363e-05)\n",
      " state (3)  A[0]:(0.999998629093) A[1]:(3.68866380995e-07) A[2]:(1.00732654573e-06) A[3]:(5.28789729018e-12)\n",
      " state (4)  A[0]:(0.999993681908) A[1]:(4.70603390568e-06) A[2]:(1.63888921634e-06) A[3]:(1.96948292635e-13)\n",
      " state (5)  A[0]:(0.58073592186) A[1]:(0.334195405245) A[2]:(0.0850686579943) A[3]:(5.68273727059e-21)\n",
      " state (6)  A[0]:(0.021481314674) A[1]:(0.948746502399) A[2]:(0.0297721903771) A[3]:(2.96821726691e-23)\n",
      " state (7)  A[0]:(0.00242173695005) A[1]:(0.99429744482) A[2]:(0.00328080612235) A[3]:(3.82077285265e-24)\n",
      " state (8)  A[0]:(6.94335249136e-05) A[1]:(0.999827623367) A[2]:(0.000102931306174) A[3]:(1.9479703483e-25)\n",
      " state (9)  A[0]:(6.24604081167e-06) A[1]:(0.999984443188) A[2]:(9.3148137239e-06) A[3]:(2.53966326643e-26)\n",
      " state (10)  A[0]:(3.44185218637e-06) A[1]:(0.999991774559) A[2]:(4.79387517771e-06) A[3]:(1.46843160315e-26)\n",
      " state (11)  A[0]:(3.01111208501e-06) A[1]:(0.999992847443) A[2]:(4.12076860812e-06) A[3]:(1.29750023613e-26)\n",
      " state (12)  A[0]:(2.91170408673e-06) A[1]:(0.999993085861) A[2]:(3.98089014197e-06) A[3]:(1.26116456328e-26)\n",
      " state (13)  A[0]:(2.88264368464e-06) A[1]:(0.999993145466) A[2]:(3.95067945647e-06) A[3]:(1.2530793553e-26)\n",
      " state (14)  A[0]:(2.87136708721e-06) A[1]:(0.99999320507) A[2]:(3.94524295189e-06) A[3]:(1.25146480971e-26)\n",
      " state (15)  A[0]:(2.86550744022e-06) A[1]:(0.99999320507) A[2]:(3.94527341996e-06) A[3]:(1.25132637386e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 448000 finished after 8 . Running score: 0.1. Policy_loss: -92050.6112883, Value_loss: 1.83859588061. Times trained:               13102. Times reached goal: 99.               Steps done: 5300089.\n",
      " state (0)  A[0]:(0.996859848499) A[1]:(0.00109979498666) A[2]:(0.00109106360469) A[3]:(0.000949281733483)\n",
      " state (1)  A[0]:(0.0496236532927) A[1]:(0.00509964860976) A[2]:(0.0161239933223) A[3]:(0.929152727127)\n",
      " state (2)  A[0]:(0.99999576807) A[1]:(9.8169255125e-07) A[2]:(3.250471309e-06) A[3]:(7.68378138893e-11)\n",
      " state (3)  A[0]:(0.999998271465) A[1]:(4.88264504384e-07) A[2]:(1.2240623164e-06) A[3]:(4.9390049342e-12)\n",
      " state (4)  A[0]:(0.999983549118) A[1]:(1.22531218949e-05) A[2]:(4.16855664298e-06) A[3]:(5.80500071298e-14)\n",
      " state (5)  A[0]:(0.280149668455) A[1]:(0.442037165165) A[2]:(0.277813196182) A[3]:(9.86955616772e-22)\n",
      " state (6)  A[0]:(0.00886281300336) A[1]:(0.937224268913) A[2]:(0.0539129078388) A[3]:(2.54472517855e-23)\n",
      " state (7)  A[0]:(0.00209453399293) A[1]:(0.980660557747) A[2]:(0.0172448903322) A[3]:(8.44421562067e-24)\n",
      " state (8)  A[0]:(0.000605271256063) A[1]:(0.993460297585) A[2]:(0.00593443447724) A[3]:(3.24985745225e-24)\n",
      " state (9)  A[0]:(6.96274728398e-05) A[1]:(0.999212741852) A[2]:(0.000717613147572) A[3]:(5.24723058452e-25)\n",
      " state (10)  A[0]:(6.4006935645e-06) A[1]:(0.999948859215) A[2]:(4.4762971811e-05) A[3]:(5.2038725705e-26)\n",
      " state (11)  A[0]:(2.02292426366e-06) A[1]:(0.999987900257) A[2]:(1.00589832073e-05) A[3]:(1.54445468325e-26)\n",
      " state (12)  A[0]:(1.41587622693e-06) A[1]:(0.999992370605) A[2]:(6.23045525572e-06) A[3]:(1.04985376386e-26)\n",
      " state (13)  A[0]:(1.27700570829e-06) A[1]:(0.99999332428) A[2]:(5.41711551705e-06) A[3]:(9.38266230289e-27)\n",
      " state (14)  A[0]:(1.23904897009e-06) A[1]:(0.999993562698) A[2]:(5.19971308677e-06) A[3]:(9.07929289954e-27)\n",
      " state (15)  A[0]:(1.2280777355e-06) A[1]:(0.999993622303) A[2]:(5.13738086738e-06) A[3]:(8.99184951709e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 449000 finished after 10 . Running score: 0.18. Policy_loss: -92050.6079904, Value_loss: 1.83607912222. Times trained:               13374. Times reached goal: 133.               Steps done: 5313463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9971,  0.0010,  0.0010,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.2579e-06,  9.2743e-07,  8.0583e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0010,  0.0010,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0010,  0.0010,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.2559e-06,  9.2716e-07,  8.0715e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.2553e-06,  9.2708e-07,  8.0751e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5853e-04,  9.9966e-01,  1.8107e-04,  3.5376e-25]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0236e-05,  9.9998e-01,  9.0304e-06,  2.8462e-26]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5862e-04,  9.9966e-01,  1.8115e-04,  3.5393e-25]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0233e-05,  9.9998e-01,  9.0282e-06,  2.8459e-26]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0417e-06,  9.9999e-01,  2.7771e-06,  1.0940e-26]])\n",
      "On state=10, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0224e-05,  9.9998e-01,  9.0213e-06,  2.8443e-26]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5820e-04,  9.9966e-01,  1.8078e-04,  3.5334e-25]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0212e-05,  9.9998e-01,  9.0112e-06,  2.8419e-26]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0362e-06,  9.9999e-01,  2.7742e-06,  1.0932e-26]])\n",
      "On state=10, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0195e-05,  9.9998e-01,  8.9980e-06,  2.8387e-26]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0329e-06,  9.9999e-01,  2.7725e-06,  1.0928e-26]])\n",
      "On state=10, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.9952e-06,  1.0000e+00,  1.8678e-06,  7.9650e-27]])\n",
      "On state=14, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.9942e-06,  1.0000e+00,  1.8674e-06,  7.9642e-27]])\n",
      "On state=14, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.9934e-06,  1.0000e+00,  1.8672e-06,  7.9636e-27]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.0043e-06,  1.0000e+00,  1.8761e-06,  7.9941e-27]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.0036e-06,  1.0000e+00,  1.8759e-06,  7.9936e-27]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.9913e-06,  1.0000e+00,  1.8666e-06,  7.9620e-27]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.0025e-06,  1.0000e+00,  1.8755e-06,  7.9927e-27]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.997142732143) A[1]:(0.00100949860644) A[2]:(0.000983879901469) A[3]:(0.000863905705046)\n",
      " state (1)  A[0]:(0.0493333041668) A[1]:(0.00486430060118) A[2]:(0.0146256024018) A[3]:(0.931176781654)\n",
      " state (2)  A[0]:(0.992343723774) A[1]:(0.00139672693331) A[2]:(0.00459702592343) A[3]:(0.00166250253096)\n",
      " state (3)  A[0]:(0.999998807907) A[1]:(3.21515727819e-07) A[2]:(8.43718282795e-07) A[3]:(5.06961399943e-12)\n",
      " state (4)  A[0]:(0.999997794628) A[1]:(1.25926044348e-06) A[2]:(9.27691871766e-07) A[3]:(8.05194290149e-13)\n",
      " state (5)  A[0]:(0.997532069683) A[1]:(0.00225094961934) A[2]:(0.000217002379941) A[3]:(5.49864424446e-18)\n",
      " state (6)  A[0]:(0.0709462985396) A[1]:(0.889111399651) A[2]:(0.0399422831833) A[3]:(7.45728389061e-23)\n",
      " state (7)  A[0]:(0.00377762154676) A[1]:(0.992757737637) A[2]:(0.00346466316842) A[3]:(4.85424926939e-24)\n",
      " state (8)  A[0]:(0.00015629621339) A[1]:(0.999664604664) A[2]:(0.000179109410965) A[3]:(3.50589877581e-25)\n",
      " state (9)  A[0]:(1.01440919025e-05) A[1]:(0.999980926514) A[2]:(8.9589111667e-06) A[3]:(2.82912822595e-26)\n",
      " state (10)  A[0]:(4.02267232857e-06) A[1]:(0.99999320507) A[2]:(2.76766263596e-06) A[3]:(1.09143459136e-26)\n",
      " state (11)  A[0]:(3.21803554471e-06) A[1]:(0.999994754791) A[2]:(2.05688888855e-06) A[3]:(8.60477380575e-27)\n",
      " state (12)  A[0]:(3.04541049445e-06) A[1]:(0.999995052814) A[2]:(1.91086473933e-06) A[3]:(8.11306540202e-27)\n",
      " state (13)  A[0]:(3.00223564409e-06) A[1]:(0.999995112419) A[2]:(1.87540717889e-06) A[3]:(7.99243516514e-27)\n",
      " state (14)  A[0]:(2.99062253362e-06) A[1]:(0.999995172024) A[2]:(1.86634008514e-06) A[3]:(7.96139764853e-27)\n",
      " state (15)  A[0]:(2.98720215142e-06) A[1]:(0.999995172024) A[2]:(1.86394584034e-06) A[3]:(7.95311075716e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 450000 finished after 24 . Running score: 0.09. Policy_loss: -92050.6111424, Value_loss: 1.62721765012. Times trained:               12928. Times reached goal: 125.               Steps done: 5326391.\n",
      " state (0)  A[0]:(0.997123301029) A[1]:(0.0010186949512) A[2]:(0.000986380851828) A[3]:(0.000871599826496)\n",
      " state (1)  A[0]:(0.0648793205619) A[1]:(0.00525761488825) A[2]:(0.0176647659391) A[3]:(0.91219830513)\n",
      " state (2)  A[0]:(0.999996304512) A[1]:(8.65919048465e-07) A[2]:(2.82928135675e-06) A[3]:(7.51589068759e-11)\n",
      " state (3)  A[0]:(0.999998688698) A[1]:(4.11347201634e-07) A[2]:(8.72147666087e-07) A[3]:(2.73918930073e-12)\n",
      " state (4)  A[0]:(0.999987244606) A[1]:(1.06543902803e-05) A[2]:(2.07556149689e-06) A[3]:(1.36871257736e-14)\n",
      " state (5)  A[0]:(0.73457467556) A[1]:(0.185933724046) A[2]:(0.0794915929437) A[3]:(4.41290002071e-22)\n",
      " state (6)  A[0]:(0.0335159301758) A[1]:(0.946281194687) A[2]:(0.0202028527856) A[3]:(9.86705326984e-24)\n",
      " state (7)  A[0]:(0.00315381470136) A[1]:(0.995344877243) A[2]:(0.00150131783448) A[3]:(1.08092640019e-24)\n",
      " state (8)  A[0]:(9.98494288069e-05) A[1]:(0.999849379063) A[2]:(5.07946242578e-05) A[3]:(5.88058518839e-26)\n",
      " state (9)  A[0]:(1.22588098748e-05) A[1]:(0.999980330467) A[2]:(7.39380811865e-06) A[3]:(1.09629841188e-26)\n",
      " state (10)  A[0]:(7.62422041589e-06) A[1]:(0.999987781048) A[2]:(4.57186979474e-06) A[3]:(7.28658569238e-27)\n",
      " state (11)  A[0]:(6.84171072862e-06) A[1]:(0.999989032745) A[2]:(4.10783650295e-06) A[3]:(6.65258958436e-27)\n",
      " state (12)  A[0]:(6.6257666731e-06) A[1]:(0.999989390373) A[2]:(4.00698991143e-06) A[3]:(6.50639763489e-27)\n",
      " state (13)  A[0]:(6.53496226732e-06) A[1]:(0.999989509583) A[2]:(3.98252404921e-06) A[3]:(6.46456335501e-27)\n",
      " state (14)  A[0]:(6.48043487672e-06) A[1]:(0.999989569187) A[2]:(3.97573967348e-06) A[3]:(6.4479148462e-27)\n",
      " state (15)  A[0]:(6.44092824587e-06) A[1]:(0.999989569187) A[2]:(3.97334406443e-06) A[3]:(6.43832949286e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 451000 finished after 28 . Running score: 0.07. Policy_loss: -92050.6111231, Value_loss: 1.84951201134. Times trained:               12754. Times reached goal: 122.               Steps done: 5339145.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997258901596) A[1]:(0.00100777845364) A[2]:(0.000954744813498) A[3]:(0.0007785744383)\n",
      " state (1)  A[0]:(0.0713181123137) A[1]:(0.00537236593664) A[2]:(0.018025977537) A[3]:(0.90528357029)\n",
      " state (2)  A[0]:(0.999996066093) A[1]:(9.70254859567e-07) A[2]:(2.94671508527e-06) A[3]:(1.01594004043e-10)\n",
      " state (3)  A[0]:(0.999998867512) A[1]:(4.06325483482e-07) A[2]:(7.29262239929e-07) A[3]:(2.37390597631e-12)\n",
      " state (4)  A[0]:(0.999988555908) A[1]:(1.01848145277e-05) A[2]:(1.28559486257e-06) A[3]:(1.48587569928e-14)\n",
      " state (5)  A[0]:(0.823237478733) A[1]:(0.139607176185) A[2]:(0.0371553488076) A[3]:(5.29947365527e-22)\n",
      " state (6)  A[0]:(0.0677002668381) A[1]:(0.913028478622) A[2]:(0.0192712210119) A[3]:(9.88692546491e-24)\n",
      " state (7)  A[0]:(0.00906441640109) A[1]:(0.989089727402) A[2]:(0.00184586632531) A[3]:(1.35604085202e-24)\n",
      " state (8)  A[0]:(0.000466540019261) A[1]:(0.999465346336) A[2]:(6.8086788815e-05) A[3]:(8.76683618834e-26)\n",
      " state (9)  A[0]:(3.52251518052e-05) A[1]:(0.999957621098) A[2]:(7.17180955689e-06) A[3]:(1.21548874649e-26)\n",
      " state (10)  A[0]:(1.65235105669e-05) A[1]:(0.999979615211) A[2]:(3.86043529943e-06) A[3]:(7.00856075673e-27)\n",
      " state (11)  A[0]:(1.37938204716e-05) A[1]:(0.999982833862) A[2]:(3.34969331561e-06) A[3]:(6.17508067693e-27)\n",
      " state (12)  A[0]:(1.30209864437e-05) A[1]:(0.999983727932) A[2]:(3.23575636685e-06) A[3]:(5.97620106196e-27)\n",
      " state (13)  A[0]:(1.26562226797e-05) A[1]:(0.999984145164) A[2]:(3.20441654367e-06) A[3]:(5.91117704481e-27)\n",
      " state (14)  A[0]:(1.24086154756e-05) A[1]:(0.999984383583) A[2]:(3.19252717418e-06) A[3]:(5.87895354054e-27)\n",
      " state (15)  A[0]:(1.22116289276e-05) A[1]:(0.999984622002) A[2]:(3.18577599501e-06) A[3]:(5.85645906397e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 452000 finished after 10 . Running score: 0.12. Policy_loss: -92050.6111415, Value_loss: 2.05777646446. Times trained:               12542. Times reached goal: 120.               Steps done: 5351687.\n",
      " state (0)  A[0]:(0.996668219566) A[1]:(0.00105965288822) A[2]:(0.00111073709559) A[3]:(0.00116138055455)\n",
      " state (1)  A[0]:(0.0686888322234) A[1]:(0.00521323597059) A[2]:(0.019722988829) A[3]:(0.906374931335)\n",
      " state (2)  A[0]:(0.99999666214) A[1]:(6.1365642523e-07) A[2]:(2.70351779363e-06) A[3]:(3.84461906755e-11)\n",
      " state (3)  A[0]:(0.999998509884) A[1]:(3.63759738775e-07) A[2]:(1.1158479083e-06) A[3]:(2.73927235062e-12)\n",
      " state (4)  A[0]:(0.999993145466) A[1]:(4.52898439107e-06) A[2]:(2.30093496612e-06) A[3]:(5.48497404722e-14)\n",
      " state (5)  A[0]:(0.891730725765) A[1]:(0.0313051864505) A[2]:(0.0769640877843) A[3]:(2.17717027034e-21)\n",
      " state (6)  A[0]:(0.136351451278) A[1]:(0.568551659584) A[2]:(0.295096874237) A[3]:(2.57285075194e-23)\n",
      " state (7)  A[0]:(0.0446793064475) A[1]:(0.856310963631) A[2]:(0.0990097373724) A[3]:(9.44536136207e-24)\n",
      " state (8)  A[0]:(0.00903110858053) A[1]:(0.97328722477) A[2]:(0.0176816470921) A[3]:(2.2883851035e-24)\n",
      " state (9)  A[0]:(0.000386931293178) A[1]:(0.998782038689) A[2]:(0.000831024255604) A[3]:(1.65810772276e-25)\n",
      " state (10)  A[0]:(4.126058775e-05) A[1]:(0.999858021736) A[2]:(0.000100735975138) A[3]:(2.64927564328e-26)\n",
      " state (11)  A[0]:(2.16108655877e-05) A[1]:(0.999926626682) A[2]:(5.17572007084e-05) A[3]:(1.50290990916e-26)\n",
      " state (12)  A[0]:(1.81847535714e-05) A[1]:(0.999938488007) A[2]:(4.33365967183e-05) A[3]:(1.29287939093e-26)\n",
      " state (13)  A[0]:(1.72168984136e-05) A[1]:(0.999941468239) A[2]:(4.12894587498e-05) A[3]:(1.23973528005e-26)\n",
      " state (14)  A[0]:(1.68285150721e-05) A[1]:(0.999942481518) A[2]:(4.07115912822e-05) A[3]:(1.22354629812e-26)\n",
      " state (15)  A[0]:(1.66118552443e-05) A[1]:(0.99994289875) A[2]:(4.051763608e-05) A[3]:(1.21722947901e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 453000 finished after 18 . Running score: 0.11. Policy_loss: -92050.6108801, Value_loss: 2.04525423281. Times trained:               12867. Times reached goal: 141.               Steps done: 5364554.\n",
      " state (0)  A[0]:(0.996872127056) A[1]:(0.00107816711534) A[2]:(0.00111837836448) A[3]:(0.000931337825023)\n",
      " state (1)  A[0]:(0.0814940184355) A[1]:(0.0056140772067) A[2]:(0.0234252754599) A[3]:(0.889466643333)\n",
      " state (2)  A[0]:(0.99999755621) A[1]:(4.13919536868e-07) A[2]:(2.04158254746e-06) A[3]:(1.36306504278e-11)\n",
      " state (3)  A[0]:(0.99999833107) A[1]:(3.43312677842e-07) A[2]:(1.307245725e-06) A[3]:(3.07510917771e-12)\n",
      " state (4)  A[0]:(0.999996542931) A[1]:(1.57863610184e-06) A[2]:(1.90593368643e-06) A[3]:(2.78796462912e-13)\n",
      " state (5)  A[0]:(0.998821616173) A[1]:(0.000486923468998) A[2]:(0.000691477267537) A[3]:(1.86683187122e-18)\n",
      " state (6)  A[0]:(0.220091000199) A[1]:(0.218249171972) A[2]:(0.561659872532) A[3]:(3.90674559623e-23)\n",
      " state (7)  A[0]:(0.0595800802112) A[1]:(0.647106170654) A[2]:(0.293313801289) A[3]:(1.39008004231e-23)\n",
      " state (8)  A[0]:(0.0183346699923) A[1]:(0.878055095673) A[2]:(0.103610210121) A[3]:(5.79409815187e-24)\n",
      " state (9)  A[0]:(0.00180501292925) A[1]:(0.985075712204) A[2]:(0.013119250536) A[3]:(9.6055036757e-25)\n",
      " state (10)  A[0]:(9.47604494286e-05) A[1]:(0.999170422554) A[2]:(0.000734821136575) A[3]:(7.84556128239e-26)\n",
      " state (11)  A[0]:(2.39641376538e-05) A[1]:(0.999826550484) A[2]:(0.000149515137309) A[3]:(2.05181242334e-26)\n",
      " state (12)  A[0]:(1.58415277838e-05) A[1]:(0.999894917011) A[2]:(8.92151874723e-05) A[3]:(1.33723941238e-26)\n",
      " state (13)  A[0]:(1.4034589185e-05) A[1]:(0.999909281731) A[2]:(7.6659845945e-05) A[3]:(1.17968108659e-26)\n",
      " state (14)  A[0]:(1.35162554216e-05) A[1]:(0.999913215637) A[2]:(7.32894186513e-05) A[3]:(1.13644619342e-26)\n",
      " state (15)  A[0]:(1.33414605443e-05) A[1]:(0.999914348125) A[2]:(7.22882468835e-05) A[3]:(1.12333592607e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 454000 finished after 26 . Running score: 0.11. Policy_loss: -92050.6043618, Value_loss: 1.62175784632. Times trained:               12723. Times reached goal: 114.               Steps done: 5377277.\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0011,  0.0011,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0011,  0.0011,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0011,  0.0011,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0011,  0.0011,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0011,  0.0011,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.1176e-06,  2.4134e-06,  2.0405e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.1057e-06,  2.4096e-06,  2.0581e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.1584e-03,  9.0720e-01,  8.3637e-02,  3.0986e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.997103571892) A[1]:(0.00106394267641) A[2]:(0.00108070508577) A[3]:(0.000751798099373)\n",
      " state (1)  A[0]:(0.0711400508881) A[1]:(0.00544532714412) A[2]:(0.0224140975624) A[3]:(0.901000499725)\n",
      " state (2)  A[0]:(0.999988257885) A[1]:(1.99751639229e-06) A[2]:(9.76469982561e-06) A[3]:(4.78076411792e-10)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(3.67557476011e-07) A[2]:(1.45104195326e-06) A[3]:(3.29454497537e-12)\n",
      " state (4)  A[0]:(0.999995529652) A[1]:(2.07561629395e-06) A[2]:(2.39958035309e-06) A[3]:(2.10383414249e-13)\n",
      " state (5)  A[0]:(0.996297240257) A[1]:(0.00105008541141) A[2]:(0.00265268213116) A[3]:(2.46383267623e-19)\n",
      " state (6)  A[0]:(0.140081033111) A[1]:(0.238611504436) A[2]:(0.621307432652) A[3]:(2.22824002094e-23)\n",
      " state (7)  A[0]:(0.037121180445) A[1]:(0.668145239353) A[2]:(0.294733583927) A[3]:(8.94660326588e-24)\n",
      " state (8)  A[0]:(0.00929787475616) A[1]:(0.905694782734) A[2]:(0.0850073322654) A[3]:(3.13810248604e-24)\n",
      " state (9)  A[0]:(0.00057408073917) A[1]:(0.992497503757) A[2]:(0.00692839827389) A[3]:(3.41905603699e-25)\n",
      " state (10)  A[0]:(3.94044509449e-05) A[1]:(0.999508321285) A[2]:(0.000452265929198) A[3]:(3.15260772756e-26)\n",
      " state (11)  A[0]:(1.45734729813e-05) A[1]:(0.999846577644) A[2]:(0.000138845382025) A[3]:(1.1582750686e-26)\n",
      " state (12)  A[0]:(1.10227574623e-05) A[1]:(0.999890983105) A[2]:(9.80156619335e-05) A[3]:(8.64988755914e-27)\n",
      " state (13)  A[0]:(1.01777768577e-05) A[1]:(0.999900996685) A[2]:(8.88399445103e-05) A[3]:(7.96596364324e-27)\n",
      " state (14)  A[0]:(9.93161120277e-06) A[1]:(0.999903678894) A[2]:(8.63746463438e-05) A[3]:(7.77890037886e-27)\n",
      " state (15)  A[0]:(9.84743564914e-06) A[1]:(0.999904513359) A[2]:(8.56607148307e-05) A[3]:(7.72337890005e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 455000 finished after 8 . Running score: 0.16. Policy_loss: -92050.6058943, Value_loss: 2.05891492386. Times trained:               12969. Times reached goal: 137.               Steps done: 5390246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996816754341) A[1]:(0.00108155561611) A[2]:(0.00111561373342) A[3]:(0.0009861048311)\n",
      " state (1)  A[0]:(0.0756618306041) A[1]:(0.00555202970281) A[2]:(0.0229782108217) A[3]:(0.895807921886)\n",
      " state (2)  A[0]:(0.999996960163) A[1]:(5.30978809365e-07) A[2]:(2.49980007538e-06) A[3]:(1.87427590809e-11)\n",
      " state (3)  A[0]:(0.999997913837) A[1]:(4.93823449688e-07) A[2]:(1.57594865868e-06) A[3]:(3.10116602537e-12)\n",
      " state (4)  A[0]:(0.999986827374) A[1]:(8.35969240143e-06) A[2]:(4.80186326968e-06) A[3]:(5.52107764075e-14)\n",
      " state (5)  A[0]:(0.667927742004) A[1]:(0.0759059786797) A[2]:(0.256166309118) A[3]:(2.16938152477e-21)\n",
      " state (6)  A[0]:(0.0270456932485) A[1]:(0.682672202587) A[2]:(0.290282070637) A[3]:(1.52144179134e-23)\n",
      " state (7)  A[0]:(0.00333053921349) A[1]:(0.947764992714) A[2]:(0.0489044971764) A[3]:(2.877054213e-24)\n",
      " state (8)  A[0]:(0.000141950411489) A[1]:(0.997057676315) A[2]:(0.00280035356991) A[3]:(2.27282339233e-25)\n",
      " state (9)  A[0]:(9.8142254501e-06) A[1]:(0.999839425087) A[2]:(0.000150733700139) A[3]:(1.87105912175e-26)\n",
      " state (10)  A[0]:(4.14835994889e-06) A[1]:(0.99994468689) A[2]:(5.11365033162e-05) A[3]:(7.63664734131e-27)\n",
      " state (11)  A[0]:(3.37466940437e-06) A[1]:(0.999957501888) A[2]:(3.91028406739e-05) A[3]:(6.12663660548e-27)\n",
      " state (12)  A[0]:(3.20822277899e-06) A[1]:(0.999960184097) A[2]:(3.66204876627e-05) A[3]:(5.80582328425e-27)\n",
      " state (13)  A[0]:(3.16665341416e-06) A[1]:(0.999960780144) A[2]:(3.60240337614e-05) A[3]:(5.72792981802e-27)\n",
      " state (14)  A[0]:(3.15518673233e-06) A[1]:(0.999960958958) A[2]:(3.58714132744e-05) A[3]:(5.70779807231e-27)\n",
      " state (15)  A[0]:(3.15154238706e-06) A[1]:(0.999961018562) A[2]:(3.58295656042e-05) A[3]:(5.70216203092e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 456000 finished after 7 . Running score: 0.15. Policy_loss: -92050.6114223, Value_loss: 1.61425579212. Times trained:               12830. Times reached goal: 127.               Steps done: 5403076.\n",
      " state (0)  A[0]:(0.996820688248) A[1]:(0.00105606985744) A[2]:(0.00108477915637) A[3]:(0.00103849044535)\n",
      " state (1)  A[0]:(0.0584656447172) A[1]:(0.00495924521238) A[2]:(0.0186455082148) A[3]:(0.917929589748)\n",
      " state (2)  A[0]:(0.999971270561) A[1]:(5.60532680538e-06) A[2]:(2.31263747992e-05) A[3]:(5.43173150902e-09)\n",
      " state (3)  A[0]:(0.999998450279) A[1]:(3.81544225547e-07) A[2]:(1.14481122182e-06) A[3]:(2.98430984785e-12)\n",
      " state (4)  A[0]:(0.999993681908) A[1]:(4.52105905424e-06) A[2]:(1.81579332548e-06) A[3]:(8.67585015873e-14)\n",
      " state (5)  A[0]:(0.892790615559) A[1]:(0.072825729847) A[2]:(0.0343836434186) A[3]:(2.60586611437e-20)\n",
      " state (6)  A[0]:(0.0442152321339) A[1]:(0.850446164608) A[2]:(0.105338610709) A[3]:(1.34940022092e-23)\n",
      " state (7)  A[0]:(0.00372595200315) A[1]:(0.989750444889) A[2]:(0.00652359006926) A[3]:(1.06382083893e-24)\n",
      " state (8)  A[0]:(8.6419058789e-05) A[1]:(0.999752044678) A[2]:(0.000161517047673) A[3]:(4.12394849204e-26)\n",
      " state (9)  A[0]:(1.20772519949e-05) A[1]:(0.999961256981) A[2]:(2.6660021831e-05) A[3]:(8.40949221311e-27)\n",
      " state (10)  A[0]:(8.00718680694e-06) A[1]:(0.999974429607) A[2]:(1.75847671926e-05) A[3]:(5.87994886113e-27)\n",
      " state (11)  A[0]:(7.28160966901e-06) A[1]:(0.999976694584) A[2]:(1.60146300914e-05) A[3]:(5.42241569908e-27)\n",
      " state (12)  A[0]:(7.073686902e-06) A[1]:(0.99997729063) A[2]:(1.56489440997e-05) A[3]:(5.30971567134e-27)\n",
      " state (13)  A[0]:(6.98667781762e-06) A[1]:(0.999977469444) A[2]:(1.55471079779e-05) A[3]:(5.27473038353e-27)\n",
      " state (14)  A[0]:(6.93758511261e-06) A[1]:(0.999977529049) A[2]:(1.55116831593e-05) A[3]:(5.26040377586e-27)\n",
      " state (15)  A[0]:(6.90492606736e-06) A[1]:(0.999977588654) A[2]:(1.54954814207e-05) A[3]:(5.25272393761e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 457000 finished after 5 . Running score: 0.14. Policy_loss: -92050.6113241, Value_loss: 1.834589065. Times trained:               12615. Times reached goal: 142.               Steps done: 5415691.\n",
      " state (0)  A[0]:(0.995342314243) A[1]:(0.00112369039562) A[2]:(0.00128134828992) A[3]:(0.0022526497487)\n",
      " state (1)  A[0]:(0.0566300749779) A[1]:(0.00484573189169) A[2]:(0.0182104986161) A[3]:(0.920313715935)\n",
      " state (2)  A[0]:(0.99998652935) A[1]:(2.60309707301e-06) A[2]:(1.08432177512e-05) A[3]:(9.45302280897e-10)\n",
      " state (3)  A[0]:(0.999998390675) A[1]:(4.81495305849e-07) A[2]:(1.14519889394e-06) A[3]:(2.07615630417e-12)\n",
      " state (4)  A[0]:(0.99994045496) A[1]:(5.4171836382e-05) A[2]:(5.38586118637e-06) A[3]:(3.8878426879e-15)\n",
      " state (5)  A[0]:(0.22561404109) A[1]:(0.501908004284) A[2]:(0.272478014231) A[3]:(1.59844739523e-22)\n",
      " state (6)  A[0]:(0.0320727601647) A[1]:(0.891179919243) A[2]:(0.0767472907901) A[3]:(9.70896948874e-24)\n",
      " state (7)  A[0]:(0.00712986942381) A[1]:(0.979831874371) A[2]:(0.0130382673815) A[3]:(2.02421944938e-24)\n",
      " state (8)  A[0]:(0.000347832858097) A[1]:(0.999075829983) A[2]:(0.00057636119891) A[3]:(1.34276168523e-25)\n",
      " state (9)  A[0]:(2.46358740696e-05) A[1]:(0.999925911427) A[2]:(4.9478298024e-05) A[3]:(1.55440650254e-26)\n",
      " state (10)  A[0]:(1.226735003e-05) A[1]:(0.999963462353) A[2]:(2.42612732109e-05) A[3]:(8.4679696092e-27)\n",
      " state (11)  A[0]:(1.04888367787e-05) A[1]:(0.99996894598) A[2]:(2.05811611522e-05) A[3]:(7.37017413346e-27)\n",
      " state (12)  A[0]:(1.00581328297e-05) A[1]:(0.999970138073) A[2]:(1.97957524506e-05) A[3]:(7.12690452958e-27)\n",
      " state (13)  A[0]:(9.91247907223e-06) A[1]:(0.999970495701) A[2]:(1.9610115487e-05) A[3]:(7.06454214761e-27)\n",
      " state (14)  A[0]:(9.84290363704e-06) A[1]:(0.99997061491) A[2]:(1.95650973183e-05) A[3]:(7.04583828636e-27)\n",
      " state (15)  A[0]:(9.7996507975e-06) A[1]:(0.999970674515) A[2]:(1.95549146156e-05) A[3]:(7.03885409401e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 458000 finished after 11 . Running score: 0.1. Policy_loss: -92050.6116818, Value_loss: 1.40201508577. Times trained:               12643. Times reached goal: 133.               Steps done: 5428334.\n",
      " state (0)  A[0]:(0.996586918831) A[1]:(0.00107884639874) A[2]:(0.00110102060717) A[3]:(0.00123321521096)\n",
      " state (1)  A[0]:(0.043047323823) A[1]:(0.0045915869996) A[2]:(0.015511113219) A[3]:(0.936849951744)\n",
      " state (2)  A[0]:(0.999896645546) A[1]:(2.19453595491e-05) A[2]:(8.1315440184e-05) A[3]:(1.06803099698e-07)\n",
      " state (3)  A[0]:(0.999998271465) A[1]:(5.81315077852e-07) A[2]:(1.13831708859e-06) A[3]:(2.34847276175e-12)\n",
      " state (4)  A[0]:(0.999927759171) A[1]:(6.81353776599e-05) A[2]:(4.09904623666e-06) A[3]:(5.64377332778e-15)\n",
      " state (5)  A[0]:(0.216254428029) A[1]:(0.650283098221) A[2]:(0.133462458849) A[3]:(3.28775465357e-22)\n",
      " state (6)  A[0]:(0.012192921713) A[1]:(0.966721117496) A[2]:(0.0210859701037) A[3]:(4.72829145422e-24)\n",
      " state (7)  A[0]:(0.00115938857198) A[1]:(0.997407853603) A[2]:(0.00143276294693) A[3]:(4.3752873418e-25)\n",
      " state (8)  A[0]:(3.32773124683e-05) A[1]:(0.999916017056) A[2]:(5.06972464791e-05) A[3]:(2.28240940818e-26)\n",
      " state (9)  A[0]:(7.12333348929e-06) A[1]:(0.999980866909) A[2]:(1.20231770779e-05) A[3]:(6.49540673788e-27)\n",
      " state (10)  A[0]:(5.26044414073e-06) A[1]:(0.999985992908) A[2]:(8.7624630396e-06) A[3]:(4.96868894497e-27)\n",
      " state (11)  A[0]:(4.91740092912e-06) A[1]:(0.999986886978) A[2]:(8.18288026494e-06) A[3]:(4.68794421105e-27)\n",
      " state (12)  A[0]:(4.8246647566e-06) A[1]:(0.999987125397) A[2]:(8.05500440038e-06) A[3]:(4.62300108295e-27)\n",
      " state (13)  A[0]:(4.78880883747e-06) A[1]:(0.999987185001) A[2]:(8.02317208581e-06) A[3]:(4.60492546005e-27)\n",
      " state (14)  A[0]:(4.76945660921e-06) A[1]:(0.999987244606) A[2]:(8.0136296674e-06) A[3]:(4.59830796476e-27)\n",
      " state (15)  A[0]:(4.75671095046e-06) A[1]:(0.999987244606) A[2]:(8.0097333921e-06) A[3]:(4.59494143922e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 459000 finished after 10 . Running score: 0.12. Policy_loss: -92050.6113136, Value_loss: 2.05381476012. Times trained:               12317. Times reached goal: 118.               Steps done: 5440651.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9958,  0.0011,  0.0012,  0.0018]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9958,  0.0011,  0.0012,  0.0018]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9958,  0.0011,  0.0012,  0.0018]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9958,  0.0011,  0.0012,  0.0018]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9958,  0.0011,  0.0012,  0.0018]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9958,  0.0011,  0.0012,  0.0018]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9991e-01,  8.4616e-05,  7.2841e-06,  4.2835e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.6088e-05,  9.9992e-01,  5.4430e-05,  2.3976e-26]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.6088e-05,  9.9992e-01,  5.4431e-05,  2.3982e-26]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.6088e-05,  9.9992e-01,  5.4431e-05,  2.3988e-26]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.5410e-06,  9.9998e-01,  1.3627e-05,  7.3299e-27]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.6088e-05,  9.9992e-01,  5.4429e-05,  2.3997e-26]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995788991451) A[1]:(0.00112335174344) A[2]:(0.00122972135432) A[3]:(0.00185791682452)\n",
      " state (1)  A[0]:(0.0461193732917) A[1]:(0.00464741885662) A[2]:(0.0166096780449) A[3]:(0.932623505592)\n",
      " state (2)  A[0]:(0.999996185303) A[1]:(7.58975602366e-07) A[2]:(3.05195567307e-06) A[3]:(4.90448931467e-11)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(6.03820865308e-07) A[2]:(1.23211884784e-06) A[3]:(2.40952223451e-12)\n",
      " state (4)  A[0]:(0.999908566475) A[1]:(8.41779328766e-05) A[2]:(7.25873314877e-06) A[3]:(4.31656375547e-15)\n",
      " state (5)  A[0]:(0.153591245413) A[1]:(0.659852683544) A[2]:(0.186556085944) A[3]:(1.56608731116e-22)\n",
      " state (6)  A[0]:(0.0108945164829) A[1]:(0.965934574604) A[2]:(0.0231709238142) A[3]:(5.06472366979e-24)\n",
      " state (7)  A[0]:(0.000937815522775) A[1]:(0.99749392271) A[2]:(0.00156824942678) A[3]:(4.64474151942e-25)\n",
      " state (8)  A[0]:(2.60888828052e-05) A[1]:(0.999919474125) A[2]:(5.4432424804e-05) A[3]:(2.40015136644e-26)\n",
      " state (9)  A[0]:(6.54114637655e-06) A[1]:(0.99997985363) A[2]:(1.36275129989e-05) A[3]:(7.33256380313e-27)\n",
      " state (10)  A[0]:(4.99478119309e-06) A[1]:(0.999984920025) A[2]:(1.00898514575e-05) A[3]:(5.71098394562e-27)\n",
      " state (11)  A[0]:(4.70517079521e-06) A[1]:(0.999985814095) A[2]:(9.45447845879e-06) A[3]:(5.40984130247e-27)\n",
      " state (12)  A[0]:(4.62812749902e-06) A[1]:(0.999986052513) A[2]:(9.31227077672e-06) A[3]:(5.33996740853e-27)\n",
      " state (13)  A[0]:(4.59894863525e-06) A[1]:(0.999986112118) A[2]:(9.27509881876e-06) A[3]:(5.32024473034e-27)\n",
      " state (14)  A[0]:(4.58325666841e-06) A[1]:(0.999986171722) A[2]:(9.26256507228e-06) A[3]:(5.31272089242e-27)\n",
      " state (15)  A[0]:(4.5727779252e-06) A[1]:(0.999986171722) A[2]:(9.25657786865e-06) A[3]:(5.30870956553e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 460000 finished after 12 . Running score: 0.13. Policy_loss: -92050.6113126, Value_loss: 2.05080968355. Times trained:               12299. Times reached goal: 123.               Steps done: 5452950.\n",
      " state (0)  A[0]:(0.995384097099) A[1]:(0.00115921534598) A[2]:(0.00139735010453) A[3]:(0.00205936422572)\n",
      " state (1)  A[0]:(0.0244256295264) A[1]:(0.00380951003172) A[2]:(0.0115308761597) A[3]:(0.960233986378)\n",
      " state (2)  A[0]:(0.0862187445164) A[1]:(0.00585820013657) A[2]:(0.0240273531526) A[3]:(0.88389569521)\n",
      " state (3)  A[0]:(0.999996960163) A[1]:(5.96633526584e-07) A[2]:(2.45722162617e-06) A[3]:(2.96200911021e-11)\n",
      " state (4)  A[0]:(0.999997913837) A[1]:(7.19609488442e-07) A[2]:(1.35639334076e-06) A[3]:(2.21421188676e-12)\n",
      " state (5)  A[0]:(0.99559289217) A[1]:(0.0032984726131) A[2]:(0.00110861368012) A[3]:(1.6952199769e-17)\n",
      " state (6)  A[0]:(0.085593432188) A[1]:(0.735724389553) A[2]:(0.178682163358) A[3]:(5.33482143958e-23)\n",
      " state (7)  A[0]:(0.011607340537) A[1]:(0.967737078667) A[2]:(0.0206555724144) A[3]:(4.39750117185e-24)\n",
      " state (8)  A[0]:(0.00052300497191) A[1]:(0.998844921589) A[2]:(0.000632044742815) A[3]:(2.09368318803e-25)\n",
      " state (9)  A[0]:(2.30915229622e-05) A[1]:(0.999944150448) A[2]:(3.27495727106e-05) A[3]:(1.58562551878e-26)\n",
      " state (10)  A[0]:(9.20387719816e-06) A[1]:(0.999977588654) A[2]:(1.32033474074e-05) A[3]:(7.30524179057e-27)\n",
      " state (11)  A[0]:(7.47502781451e-06) A[1]:(0.999981880188) A[2]:(1.06411926026e-05) A[3]:(6.09338542499e-27)\n",
      " state (12)  A[0]:(7.06023320163e-06) A[1]:(0.999982893467) A[2]:(1.00685301732e-05) A[3]:(5.81311331427e-27)\n",
      " state (13)  A[0]:(6.9163002081e-06) A[1]:(0.99998319149) A[2]:(9.91018532659e-06) A[3]:(5.73146775133e-27)\n",
      " state (14)  A[0]:(6.84505221216e-06) A[1]:(0.999983310699) A[2]:(9.85790939012e-06) A[3]:(5.70159311121e-27)\n",
      " state (15)  A[0]:(6.79852337271e-06) A[1]:(0.999983370304) A[2]:(9.83651716524e-06) A[3]:(5.68732158514e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 461000 finished after 12 . Running score: 0.09. Policy_loss: -92050.6117673, Value_loss: 2.06488203769. Times trained:               12830. Times reached goal: 122.               Steps done: 5465780.\n",
      " state (0)  A[0]:(0.996445596218) A[1]:(0.00111816101708) A[2]:(0.00124041631352) A[3]:(0.00119581783656)\n",
      " state (1)  A[0]:(0.0343599691987) A[1]:(0.00429254304618) A[2]:(0.0146971382201) A[3]:(0.946650326252)\n",
      " state (2)  A[0]:(0.706681787968) A[1]:(0.0104512078688) A[2]:(0.0460603162646) A[3]:(0.236806690693)\n",
      " state (3)  A[0]:(0.99999833107) A[1]:(3.3889867268e-07) A[2]:(1.35570462589e-06) A[3]:(4.53794380545e-12)\n",
      " state (4)  A[0]:(0.99999243021) A[1]:(4.80146445625e-06) A[2]:(2.77905451185e-06) A[3]:(1.91976304103e-13)\n",
      " state (5)  A[0]:(0.420815676451) A[1]:(0.295370668173) A[2]:(0.283813655376) A[3]:(1.13510717838e-21)\n",
      " state (6)  A[0]:(0.0388690717518) A[1]:(0.814495384693) A[2]:(0.146635517478) A[3]:(8.73079379918e-24)\n",
      " state (7)  A[0]:(0.00988704152405) A[1]:(0.962330162525) A[2]:(0.0277827866375) A[3]:(1.77523522617e-24)\n",
      " state (8)  A[0]:(0.000741167285014) A[1]:(0.997735500336) A[2]:(0.00152335455641) A[3]:(1.40119248922e-25)\n",
      " state (9)  A[0]:(3.14402168442e-05) A[1]:(0.999894440174) A[2]:(7.41307230783e-05) A[3]:(9.80796156065e-27)\n",
      " state (10)  A[0]:(1.0735633623e-05) A[1]:(0.99996393919) A[2]:(2.53168709605e-05) A[3]:(3.88884467705e-27)\n",
      " state (11)  A[0]:(8.39047970658e-06) A[1]:(0.999972105026) A[2]:(1.94771801034e-05) A[3]:(3.11454168369e-27)\n",
      " state (12)  A[0]:(7.86581495049e-06) A[1]:(0.999973893166) A[2]:(1.82403528015e-05) A[3]:(2.9450731375e-27)\n",
      " state (13)  A[0]:(7.70152200857e-06) A[1]:(0.999974370003) A[2]:(1.79243434104e-05) A[3]:(2.89981359121e-27)\n",
      " state (14)  A[0]:(7.62854187997e-06) A[1]:(0.999974548817) A[2]:(1.78331611096e-05) A[3]:(2.88533714619e-27)\n",
      " state (15)  A[0]:(7.58365649745e-06) A[1]:(0.999974608421) A[2]:(1.78020327439e-05) A[3]:(2.87932285216e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 462000 finished after 9 . Running score: 0.17. Policy_loss: -92050.6113003, Value_loss: 1.84368419726. Times trained:               12616. Times reached goal: 158.               Steps done: 5478396.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996637165546) A[1]:(0.00110229535494) A[2]:(0.00121545698494) A[3]:(0.00104511133395)\n",
      " state (1)  A[0]:(0.0354009568691) A[1]:(0.00441135000437) A[2]:(0.0151333976537) A[3]:(0.945054292679)\n",
      " state (2)  A[0]:(0.960440635681) A[1]:(0.00405860925093) A[2]:(0.0178298708051) A[3]:(0.017670892179)\n",
      " state (3)  A[0]:(0.999998211861) A[1]:(3.41154958505e-07) A[2]:(1.43690090226e-06) A[3]:(4.33288648016e-12)\n",
      " state (4)  A[0]:(0.999995291233) A[1]:(2.32135607803e-06) A[2]:(2.37213498622e-06) A[3]:(3.22880963502e-13)\n",
      " state (5)  A[0]:(0.783917605877) A[1]:(0.0717799067497) A[2]:(0.144302472472) A[3]:(2.73818997078e-21)\n",
      " state (6)  A[0]:(0.035475011915) A[1]:(0.739160120487) A[2]:(0.225364893675) A[3]:(7.96009404972e-24)\n",
      " state (7)  A[0]:(0.00856692157686) A[1]:(0.940631330013) A[2]:(0.0508017204702) A[3]:(1.76700227933e-24)\n",
      " state (8)  A[0]:(0.000816606741864) A[1]:(0.994729161263) A[2]:(0.00445421831682) A[3]:(1.99346407996e-25)\n",
      " state (9)  A[0]:(2.83669542114e-05) A[1]:(0.99980711937) A[2]:(0.00016451018746) A[3]:(1.08469075506e-26)\n",
      " state (10)  A[0]:(6.96469714967e-06) A[1]:(0.999958276749) A[2]:(3.47487111867e-05) A[3]:(2.89044586797e-27)\n",
      " state (11)  A[0]:(4.94450887345e-06) A[1]:(0.999972105026) A[2]:(2.29544111789e-05) A[3]:(2.04706708603e-27)\n",
      " state (12)  A[0]:(4.53601614936e-06) A[1]:(0.99997484684) A[2]:(2.06382428587e-05) A[3]:(1.87439071092e-27)\n",
      " state (13)  A[0]:(4.43111957793e-06) A[1]:(0.999975502491) A[2]:(2.00590275199e-05) A[3]:(1.83055269353e-27)\n",
      " state (14)  A[0]:(4.40089979747e-06) A[1]:(0.999975681305) A[2]:(1.99001588044e-05) A[3]:(1.81839429779e-27)\n",
      " state (15)  A[0]:(4.39113182438e-06) A[1]:(0.99997574091) A[2]:(1.98535653908e-05) A[3]:(1.81476314947e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 463000 finished after 10 . Running score: 0.15. Policy_loss: -92050.611297, Value_loss: 1.62781952232. Times trained:               12090. Times reached goal: 126.               Steps done: 5490486.\n",
      " state (0)  A[0]:(0.99679338932) A[1]:(0.00110084016342) A[2]:(0.00117397762369) A[3]:(0.000931769725867)\n",
      " state (1)  A[0]:(0.0280359685421) A[1]:(0.00414850376546) A[2]:(0.0131194805726) A[3]:(0.954696059227)\n",
      " state (2)  A[0]:(0.916705667973) A[1]:(0.00650673918426) A[2]:(0.0271508209407) A[3]:(0.0496367588639)\n",
      " state (3)  A[0]:(0.999998092651) A[1]:(3.9518840822e-07) A[2]:(1.49049708398e-06) A[3]:(4.58999808647e-12)\n",
      " state (4)  A[0]:(0.999994277954) A[1]:(3.29608201355e-06) A[2]:(2.44305283559e-06) A[3]:(3.03477430902e-13)\n",
      " state (5)  A[0]:(0.756520390511) A[1]:(0.133721888065) A[2]:(0.109757706523) A[3]:(4.15421251175e-21)\n",
      " state (6)  A[0]:(0.0146489031613) A[1]:(0.912731468678) A[2]:(0.0726196542382) A[3]:(3.88265701851e-24)\n",
      " state (7)  A[0]:(0.000764392490964) A[1]:(0.996318519115) A[2]:(0.00291709997691) A[3]:(2.01241658647e-25)\n",
      " state (8)  A[0]:(1.4059145542e-05) A[1]:(0.999927461147) A[2]:(5.84708250244e-05) A[3]:(6.46182006039e-27)\n",
      " state (9)  A[0]:(3.20008757626e-06) A[1]:(0.999984681606) A[2]:(1.20945924209e-05) A[3]:(1.69267710215e-27)\n",
      " state (10)  A[0]:(2.40141434915e-06) A[1]:(0.999988973141) A[2]:(8.61841635924e-06) A[3]:(1.27840735629e-27)\n",
      " state (11)  A[0]:(2.25471717386e-06) A[1]:(0.999989748001) A[2]:(7.99369718152e-06) A[3]:(1.20140713637e-27)\n",
      " state (12)  A[0]:(2.21846630666e-06) A[1]:(0.999989926815) A[2]:(7.84740404924e-06) A[3]:(1.1830971278e-27)\n",
      " state (13)  A[0]:(2.20736160372e-06) A[1]:(0.99998998642) A[2]:(7.80778100307e-06) A[3]:(1.17803520612e-27)\n",
      " state (14)  A[0]:(2.20302513299e-06) A[1]:(0.99998998642) A[2]:(7.79522179073e-06) A[3]:(1.17637370636e-27)\n",
      " state (15)  A[0]:(2.20090828407e-06) A[1]:(0.99998998642) A[2]:(7.79046513344e-06) A[3]:(1.17571426795e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 464000 finished after 20 . Running score: 0.11. Policy_loss: -92050.6113171, Value_loss: 1.61606697912. Times trained:               12626. Times reached goal: 125.               Steps done: 5503112.\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0010,  0.0012,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0010,  0.0012,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.2801e-07,  1.5472e-06,  1.7442e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.8789e-02,  2.9211e-01,  6.1910e-01,  4.3695e-24]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.1358e-02,  3.6637e-01,  5.5227e-01,  3.9012e-24]])\n",
      "On state=9, selected action=0\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.7934e-03,  9.8262e-01,  1.3590e-02,  1.7126e-25]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.9866e-03,  9.8202e-01,  1.3992e-02,  1.7647e-25]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996968626976) A[1]:(0.00102799537126) A[2]:(0.00116133911069) A[3]:(0.000842015724629)\n",
      " state (1)  A[0]:(0.0289408117533) A[1]:(0.00389702990651) A[2]:(0.0139099918306) A[3]:(0.953252196312)\n",
      " state (2)  A[0]:(0.387132316828) A[1]:(0.0102384798229) A[2]:(0.0507012493908) A[3]:(0.551927924156)\n",
      " state (3)  A[0]:(0.999998033047) A[1]:(2.56132864251e-07) A[2]:(1.7173881588e-06) A[3]:(5.3526350019e-12)\n",
      " state (4)  A[0]:(0.999998152256) A[1]:(3.17793364957e-07) A[2]:(1.52633811012e-06) A[3]:(1.79118558093e-12)\n",
      " state (5)  A[0]:(0.9999807477) A[1]:(9.45985993894e-06) A[2]:(9.78969910648e-06) A[3]:(3.28525836702e-15)\n",
      " state (6)  A[0]:(0.468295961618) A[1]:(0.0467026382685) A[2]:(0.485001415014) A[3]:(6.84006144788e-23)\n",
      " state (7)  A[0]:(0.115161962807) A[1]:(0.182157173753) A[2]:(0.702680826187) A[3]:(5.73178800112e-24)\n",
      " state (8)  A[0]:(0.0929473116994) A[1]:(0.285199671984) A[2]:(0.62185305357) A[3]:(4.41815078934e-24)\n",
      " state (9)  A[0]:(0.0851437523961) A[1]:(0.359910726547) A[2]:(0.554945528507) A[3]:(3.94450016361e-24)\n",
      " state (10)  A[0]:(0.0775189101696) A[1]:(0.446956694126) A[2]:(0.475524365902) A[3]:(3.48998888117e-24)\n",
      " state (11)  A[0]:(0.0608354434371) A[1]:(0.61216211319) A[2]:(0.327002495527) A[3]:(2.6116492584e-24)\n",
      " state (12)  A[0]:(0.0264856014401) A[1]:(0.862525105476) A[2]:(0.110989309847) A[3]:(1.0690687361e-24)\n",
      " state (13)  A[0]:(0.0042628897354) A[1]:(0.98104506731) A[2]:(0.0146920327097) A[3]:(1.84932131995e-25)\n",
      " state (14)  A[0]:(0.000682547281031) A[1]:(0.996908962727) A[2]:(0.00240848981775) A[3]:(3.73595896547e-26)\n",
      " state (15)  A[0]:(0.000250828277785) A[1]:(0.998828530312) A[2]:(0.00092065759236) A[3]:(1.59567009085e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 465000 finished after 7 . Running score: 0.08. Policy_loss: -92050.5647248, Value_loss: 1.40910294744. Times trained:               13594. Times reached goal: 112.               Steps done: 5516706.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.941998064518) A[1]:(0.00270076817833) A[2]:(0.00348116573878) A[3]:(0.051819998771)\n",
      " state (1)  A[0]:(0.0130106480792) A[1]:(0.00323509192094) A[2]:(0.00809608399868) A[3]:(0.975658178329)\n",
      " state (2)  A[0]:(0.999996960163) A[1]:(6.77471689414e-07) A[2]:(2.35154971051e-06) A[3]:(1.26005855056e-11)\n",
      " state (3)  A[0]:(0.999994337559) A[1]:(3.62553851119e-06) A[2]:(2.03902322937e-06) A[3]:(3.8158953536e-13)\n",
      " state (4)  A[0]:(0.984458625317) A[1]:(0.0153962690383) A[2]:(0.00014508064487) A[3]:(9.12171369085e-17)\n",
      " state (5)  A[0]:(0.0140426568687) A[1]:(0.886053860188) A[2]:(0.0999034717679) A[3]:(9.74016026007e-24)\n",
      " state (6)  A[0]:(0.00199032295495) A[1]:(0.976996421814) A[2]:(0.0210132729262) A[3]:(1.20887027129e-24)\n",
      " state (7)  A[0]:(0.00081141066039) A[1]:(0.98949187994) A[2]:(0.00969669688493) A[3]:(5.81658551944e-25)\n",
      " state (8)  A[0]:(0.00037796550896) A[1]:(0.994729757309) A[2]:(0.00489230360836) A[3]:(3.12061122109e-25)\n",
      " state (9)  A[0]:(9.2219801445e-05) A[1]:(0.998662352562) A[2]:(0.00124543532729) A[3]:(9.38710087808e-26)\n",
      " state (10)  A[0]:(1.13256937766e-05) A[1]:(0.999865233898) A[2]:(0.000123414589325) A[3]:(1.33226604496e-26)\n",
      " state (11)  A[0]:(2.61172795035e-06) A[1]:(0.999977886677) A[2]:(1.95151496882e-05) A[3]:(2.96492446781e-27)\n",
      " state (12)  A[0]:(1.52113636886e-06) A[1]:(0.999989032745) A[2]:(9.45667306951e-06) A[3]:(1.66263837281e-27)\n",
      " state (13)  A[0]:(1.29098441448e-06) A[1]:(0.999991178513) A[2]:(7.55567043598e-06) A[3]:(1.39185214114e-27)\n",
      " state (14)  A[0]:(1.22766334698e-06) A[1]:(0.999991714954) A[2]:(7.05029742676e-06) A[3]:(1.3177749051e-27)\n",
      " state (15)  A[0]:(1.20845515994e-06) A[1]:(0.999991893768) A[2]:(6.89866647008e-06) A[3]:(1.29535197683e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 466000 finished after 23 . Running score: 0.15. Policy_loss: -92050.6213358, Value_loss: 1.42133345007. Times trained:               13122. Times reached goal: 122.               Steps done: 5529828.\n",
      " state (0)  A[0]:(0.990944862366) A[1]:(0.00159369246103) A[2]:(0.00191963673569) A[3]:(0.00554182194173)\n",
      " state (1)  A[0]:(0.0146725345403) A[1]:(0.00329729425721) A[2]:(0.00879001803696) A[3]:(0.9732401371)\n",
      " state (2)  A[0]:(0.999915599823) A[1]:(1.698406777e-05) A[2]:(6.73834802001e-05) A[3]:(4.03211153355e-08)\n",
      " state (3)  A[0]:(0.999997854233) A[1]:(6.16067211467e-07) A[2]:(1.51119320435e-06) A[3]:(1.90605213904e-12)\n",
      " state (4)  A[0]:(0.999807953835) A[1]:(0.000185403681826) A[2]:(6.66470214128e-06) A[3]:(2.2408447202e-15)\n",
      " state (5)  A[0]:(0.105212502182) A[1]:(0.723230481148) A[2]:(0.171557024121) A[3]:(8.88347751184e-23)\n",
      " state (6)  A[0]:(0.0135706644505) A[1]:(0.909824550152) A[2]:(0.0766047909856) A[3]:(1.81246117786e-24)\n",
      " state (7)  A[0]:(0.00312688294798) A[1]:(0.978383362293) A[2]:(0.0184897482395) A[3]:(4.51172231447e-25)\n",
      " state (8)  A[0]:(0.000219002598897) A[1]:(0.998361945152) A[2]:(0.00141907809302) A[3]:(4.47958270371e-26)\n",
      " state (9)  A[0]:(1.35719164973e-05) A[1]:(0.999920129776) A[2]:(6.62771271891e-05) A[3]:(3.20517344316e-27)\n",
      " state (10)  A[0]:(5.12589622303e-06) A[1]:(0.99997574091) A[2]:(1.91239669221e-05) A[3]:(1.14370146041e-27)\n",
      " state (11)  A[0]:(4.05328046327e-06) A[1]:(0.999981999397) A[2]:(1.39632484206e-05) A[3]:(8.84234689404e-28)\n",
      " state (12)  A[0]:(3.82274356525e-06) A[1]:(0.999983251095) A[2]:(1.29011323224e-05) A[3]:(8.28965892604e-28)\n",
      " state (13)  A[0]:(3.76390948986e-06) A[1]:(0.999983608723) A[2]:(1.26353761516e-05) A[3]:(8.1499731531e-28)\n",
      " state (14)  A[0]:(3.74743399334e-06) A[1]:(0.999983668327) A[2]:(1.25628757814e-05) A[3]:(8.11163751756e-28)\n",
      " state (15)  A[0]:(3.74233422917e-06) A[1]:(0.999983727932) A[2]:(1.25414499053e-05) A[3]:(8.10025912345e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 467000 finished after 27 . Running score: 0.15. Policy_loss: -92050.6173289, Value_loss: 1.19039471248. Times trained:               12658. Times reached goal: 136.               Steps done: 5542486.\n",
      " state (0)  A[0]:(0.990692138672) A[1]:(0.00154162000399) A[2]:(0.00192398880608) A[3]:(0.00584224099293)\n",
      " state (1)  A[0]:(0.0155539931729) A[1]:(0.00329712755047) A[2]:(0.00911091361195) A[3]:(0.97203797102)\n",
      " state (2)  A[0]:(0.999992489815) A[1]:(1.35594359563e-06) A[2]:(6.16554962107e-06) A[3]:(1.43499170879e-10)\n",
      " state (3)  A[0]:(0.999997973442) A[1]:(5.03846194988e-07) A[2]:(1.50392429532e-06) A[3]:(2.16557826359e-12)\n",
      " state (4)  A[0]:(0.999930918217) A[1]:(6.36678159935e-05) A[2]:(5.41770805285e-06) A[3]:(3.72445977325e-15)\n",
      " state (5)  A[0]:(0.16388040781) A[1]:(0.555973887444) A[2]:(0.280145734549) A[3]:(1.04995827696e-22)\n",
      " state (6)  A[0]:(0.0280498061329) A[1]:(0.806994378567) A[2]:(0.164955839515) A[3]:(3.44484000498e-24)\n",
      " state (7)  A[0]:(0.0114706763998) A[1]:(0.929034531116) A[2]:(0.0594948157668) A[3]:(1.27663362026e-24)\n",
      " state (8)  A[0]:(0.00194109498989) A[1]:(0.989571928978) A[2]:(0.00848694611341) A[3]:(2.29951351498e-25)\n",
      " state (9)  A[0]:(6.81682067807e-05) A[1]:(0.999622702599) A[2]:(0.000309124094201) A[3]:(1.2432987897e-26)\n",
      " state (10)  A[0]:(1.09872098619e-05) A[1]:(0.999943494797) A[2]:(4.55352492281e-05) A[3]:(2.3930311264e-27)\n",
      " state (11)  A[0]:(6.8406848186e-06) A[1]:(0.999966859818) A[2]:(2.63166239165e-05) A[3]:(1.51075847813e-27)\n",
      " state (12)  A[0]:(6.05854893365e-06) A[1]:(0.999971151352) A[2]:(2.28045773838e-05) A[3]:(1.34065572321e-27)\n",
      " state (13)  A[0]:(5.85225097893e-06) A[1]:(0.999972224236) A[2]:(2.19363682845e-05) A[3]:(1.29764604828e-27)\n",
      " state (14)  A[0]:(5.78352046432e-06) A[1]:(0.999972522259) A[2]:(2.16960306716e-05) A[3]:(1.28535447449e-27)\n",
      " state (15)  A[0]:(5.75183821638e-06) A[1]:(0.999972641468) A[2]:(2.16227872443e-05) A[3]:(1.28128652526e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 468000 finished after 7 . Running score: 0.09. Policy_loss: -92050.6113371, Value_loss: 0.996551010344. Times trained:               13042. Times reached goal: 149.               Steps done: 5555528.\n",
      " state (0)  A[0]:(0.993341088295) A[1]:(0.00146749941632) A[2]:(0.00169081729837) A[3]:(0.00350057403557)\n",
      " state (1)  A[0]:(0.0149885173887) A[1]:(0.00335624418221) A[2]:(0.0088905589655) A[3]:(0.972764670849)\n",
      " state (2)  A[0]:(0.999553620815) A[1]:(9.23226180021e-05) A[2]:(0.000352045230102) A[3]:(1.99896862796e-06)\n",
      " state (3)  A[0]:(0.999998033047) A[1]:(4.37097583017e-07) A[2]:(1.52117627295e-06) A[3]:(4.01124836472e-12)\n",
      " state (4)  A[0]:(0.999992907047) A[1]:(4.88588193548e-06) A[2]:(2.19669982471e-06) A[3]:(2.80348281482e-13)\n",
      " state (5)  A[0]:(0.880631148815) A[1]:(0.112578965724) A[2]:(0.00678985286504) A[3]:(6.47115128825e-19)\n",
      " state (6)  A[0]:(0.0163168963045) A[1]:(0.900116086006) A[2]:(0.0835670083761) A[3]:(2.6169894523e-24)\n",
      " state (7)  A[0]:(0.00176246359479) A[1]:(0.990479171276) A[2]:(0.00775838410482) A[3]:(2.0198619543e-25)\n",
      " state (8)  A[0]:(5.47692579858e-05) A[1]:(0.999706566334) A[2]:(0.000238653985434) A[3]:(8.74026066622e-27)\n",
      " state (9)  A[0]:(4.67529253001e-06) A[1]:(0.999974548817) A[2]:(2.07976045203e-05) A[3]:(1.02352969115e-27)\n",
      " state (10)  A[0]:(2.59822127191e-06) A[1]:(0.99998664856) A[2]:(1.07264631879e-05) A[3]:(5.83817736657e-28)\n",
      " state (11)  A[0]:(2.28698195315e-06) A[1]:(0.999988496304) A[2]:(9.23040079215e-06) A[3]:(5.14756633301e-28)\n",
      " state (12)  A[0]:(2.21518280341e-06) A[1]:(0.999988853931) A[2]:(8.90319370228e-06) A[3]:(4.99349723377e-28)\n",
      " state (13)  A[0]:(2.19394041778e-06) A[1]:(0.999988973141) A[2]:(8.82314907358e-06) A[3]:(4.95458189331e-28)\n",
      " state (14)  A[0]:(2.1850369194e-06) A[1]:(0.999989032745) A[2]:(8.80282732396e-06) A[3]:(4.94366909277e-28)\n",
      " state (15)  A[0]:(2.17953424908e-06) A[1]:(0.999989032745) A[2]:(8.79795970832e-06) A[3]:(4.94006856673e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 469000 finished after 11 . Running score: 0.07. Policy_loss: -92050.6111251, Value_loss: 1.41933176373. Times trained:               12708. Times reached goal: 119.               Steps done: 5568236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9934,  0.0014,  0.0017,  0.0036]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.5014e-07,  1.7711e-06,  1.8531e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9934,  0.0014,  0.0017,  0.0036]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.5340e-07,  1.7718e-06,  1.8451e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5418e-02,  8.8331e-01,  1.0127e-01,  1.2125e-24]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5219e-03,  9.8871e-01,  9.7718e-03,  1.4307e-25]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.6765e-05,  9.9957e-01,  3.7587e-04,  7.9496e-27]])\n",
      "On state=10, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5012e-03,  9.8888e-01,  9.6217e-03,  1.4126e-25]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.6156e-05,  9.9957e-01,  3.7092e-04,  7.8670e-27]])\n",
      "On state=10, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4832e-03,  9.8902e-01,  9.4918e-03,  1.3969e-25]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.4898e-06,  9.9996e-01,  3.4671e-05,  1.0595e-27]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.1983e-06,  9.9996e-01,  3.2946e-05,  1.0161e-27]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.4700e-06,  9.9996e-01,  3.4525e-05,  1.0567e-27]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.1828e-06,  9.9996e-01,  3.2831e-05,  1.0139e-27]])\n",
      "On state=14, selected action=1\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.993368625641) A[1]:(0.00135116477031) A[2]:(0.00168644823134) A[3]:(0.00359377171844)\n",
      " state (1)  A[0]:(0.0155932148919) A[1]:(0.00326824141666) A[2]:(0.00922675710171) A[3]:(0.971911787987)\n",
      " state (2)  A[0]:(0.994343280792) A[1]:(0.000978532014415) A[2]:(0.0041121491231) A[3]:(0.000566047965549)\n",
      " state (3)  A[0]:(0.999998092651) A[1]:(3.17138187711e-07) A[2]:(1.5818832253e-06) A[3]:(4.49720339094e-12)\n",
      " state (4)  A[0]:(0.999997437) A[1]:(7.67830840687e-07) A[2]:(1.7747304355e-06) A[3]:(1.81085398412e-12)\n",
      " state (5)  A[0]:(0.999754190445) A[1]:(0.000221706155571) A[2]:(2.41104153247e-05) A[3]:(1.66562866913e-15)\n",
      " state (6)  A[0]:(0.216116786003) A[1]:(0.419359833002) A[2]:(0.364523380995) A[3]:(1.42214960469e-22)\n",
      " state (7)  A[0]:(0.0415210910141) A[1]:(0.678243219852) A[2]:(0.28023570776) A[3]:(4.01251377871e-24)\n",
      " state (8)  A[0]:(0.0149291753769) A[1]:(0.887985527515) A[2]:(0.0970852896571) A[3]:(1.17205728829e-24)\n",
      " state (9)  A[0]:(0.00142778002191) A[1]:(0.989467561245) A[2]:(0.00910465419292) A[3]:(1.34911127596e-25)\n",
      " state (10)  A[0]:(5.41225454072e-05) A[1]:(0.999590992928) A[2]:(0.000354872085154) A[3]:(7.59296570942e-27)\n",
      " state (11)  A[0]:(1.26887925944e-05) A[1]:(0.999920129776) A[2]:(6.72057576594e-05) A[3]:(1.84092710036e-27)\n",
      " state (12)  A[0]:(8.40529901325e-06) A[1]:(0.999951481819) A[2]:(4.01273646276e-05) A[3]:(1.19775066207e-27)\n",
      " state (13)  A[0]:(7.45385113987e-06) A[1]:(0.999958157539) A[2]:(3.44068139384e-05) A[3]:(1.05440870373e-27)\n",
      " state (14)  A[0]:(7.176082363e-06) A[1]:(0.999960064888) A[2]:(3.27813395415e-05) A[3]:(1.01294208388e-27)\n",
      " state (15)  A[0]:(7.08319521436e-06) A[1]:(0.999960660934) A[2]:(3.22555315506e-05) A[3]:(9.99401640811e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 470000 finished after 14 . Running score: 0.13. Policy_loss: -92050.6064902, Value_loss: 1.62833702282. Times trained:               12444. Times reached goal: 120.               Steps done: 5580680.\n",
      " state (0)  A[0]:(0.994828462601) A[1]:(0.00124596129172) A[2]:(0.00148334354162) A[3]:(0.00244225328788)\n",
      " state (1)  A[0]:(0.0191632285714) A[1]:(0.00355782569386) A[2]:(0.00998313724995) A[3]:(0.967295825481)\n",
      " state (2)  A[0]:(0.999912500381) A[1]:(1.75607528945e-05) A[2]:(6.98767398717e-05) A[3]:(5.29704209384e-08)\n",
      " state (3)  A[0]:(0.999998211861) A[1]:(4.39865488033e-07) A[2]:(1.32729996949e-06) A[3]:(3.38358789771e-12)\n",
      " state (4)  A[0]:(0.999978303909) A[1]:(1.89387264982e-05) A[2]:(2.76386799669e-06) A[3]:(5.82524480042e-14)\n",
      " state (5)  A[0]:(0.236265942454) A[1]:(0.694781899452) A[2]:(0.0689521953464) A[3]:(6.84023891003e-22)\n",
      " state (6)  A[0]:(0.00906740687788) A[1]:(0.968854963779) A[2]:(0.0220775995404) A[3]:(6.62488001306e-25)\n",
      " state (7)  A[0]:(0.000852826633491) A[1]:(0.997441053391) A[2]:(0.0017060933169) A[3]:(5.20668720155e-26)\n",
      " state (8)  A[0]:(2.07767006941e-05) A[1]:(0.999930262566) A[2]:(4.89596532134e-05) A[3]:(2.11423273728e-27)\n",
      " state (9)  A[0]:(3.56511122845e-06) A[1]:(0.999988555908) A[2]:(7.89543719293e-06) A[3]:(4.33151225994e-28)\n",
      " state (10)  A[0]:(2.50008633884e-06) A[1]:(0.999992311001) A[2]:(5.17880243933e-06) A[3]:(3.0390664151e-28)\n",
      " state (11)  A[0]:(2.31773606174e-06) A[1]:(0.999992966652) A[2]:(4.71792964163e-06) A[3]:(2.81190129297e-28)\n",
      " state (12)  A[0]:(2.27581381296e-06) A[1]:(0.999993085861) A[2]:(4.6139080041e-06) A[3]:(2.76009305524e-28)\n",
      " state (13)  A[0]:(2.26478050536e-06) A[1]:(0.999993145466) A[2]:(4.58764407085e-06) A[3]:(2.746910787e-28)\n",
      " state (14)  A[0]:(2.26143515647e-06) A[1]:(0.999993145466) A[2]:(4.58032582173e-06) A[3]:(2.74318290885e-28)\n",
      " state (15)  A[0]:(2.26021916205e-06) A[1]:(0.999993145466) A[2]:(4.57800251752e-06) A[3]:(2.74197992486e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 471000 finished after 23 . Running score: 0.1. Policy_loss: -92050.6095437, Value_loss: 1.21620913247. Times trained:               12881. Times reached goal: 132.               Steps done: 5593561.\n",
      " state (0)  A[0]:(0.993876934052) A[1]:(0.00118278700393) A[2]:(0.00150136137381) A[3]:(0.0034388974309)\n",
      " state (1)  A[0]:(0.0231245886534) A[1]:(0.0036139509175) A[2]:(0.0105603244156) A[3]:(0.962701141834)\n",
      " state (2)  A[0]:(0.99941444397) A[1]:(0.000114628986921) A[2]:(0.000465581775643) A[3]:(5.35517301614e-06)\n",
      " state (3)  A[0]:(0.999998748302) A[1]:(2.72874132179e-07) A[2]:(1.00830595784e-06) A[3]:(3.21678035771e-12)\n",
      " state (4)  A[0]:(0.999997973442) A[1]:(9.56724079515e-07) A[2]:(1.0721128092e-06) A[3]:(7.04645814354e-13)\n",
      " state (5)  A[0]:(0.998509168625) A[1]:(0.00132952840067) A[2]:(0.000161316565936) A[3]:(7.62557292633e-18)\n",
      " state (6)  A[0]:(0.102174215019) A[1]:(0.829410731792) A[2]:(0.0684150308371) A[3]:(5.43600499914e-24)\n",
      " state (7)  A[0]:(0.0069619724527) A[1]:(0.988613963127) A[2]:(0.00442404998466) A[3]:(1.61221154878e-25)\n",
      " state (8)  A[0]:(0.000192051258637) A[1]:(0.999704957008) A[2]:(0.000102965212136) A[3]:(5.1677287973e-27)\n",
      " state (9)  A[0]:(1.26149161588e-05) A[1]:(0.999978542328) A[2]:(8.86862926563e-06) A[3]:(5.65935188234e-28)\n",
      " state (10)  A[0]:(6.80609673509e-06) A[1]:(0.999988496304) A[2]:(4.72379770144e-06) A[3]:(3.27395259038e-28)\n",
      " state (11)  A[0]:(5.98405813435e-06) A[1]:(0.999989926815) A[2]:(4.10258599004e-06) A[3]:(2.90217011095e-28)\n",
      " state (12)  A[0]:(5.80129926675e-06) A[1]:(0.999990224838) A[2]:(3.96546511183e-06) A[3]:(2.81900364112e-28)\n",
      " state (13)  A[0]:(5.7539041336e-06) A[1]:(0.999990344048) A[2]:(3.93125310438e-06) A[3]:(2.79802665291e-28)\n",
      " state (14)  A[0]:(5.74025671085e-06) A[1]:(0.999990344048) A[2]:(3.92207812183e-06) A[3]:(2.79230086319e-28)\n",
      " state (15)  A[0]:(5.73583520236e-06) A[1]:(0.999990344048) A[2]:(3.9194310375e-06) A[3]:(2.79061832264e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 472000 finished after 5 . Running score: 0.12. Policy_loss: -92050.6096097, Value_loss: 1.00099705623. Times trained:               12686. Times reached goal: 148.               Steps done: 5606247.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.990906119347) A[1]:(0.00124614674132) A[2]:(0.00163055490702) A[3]:(0.00621719751507)\n",
      " state (1)  A[0]:(0.0423069782555) A[1]:(0.00444648461416) A[2]:(0.014510653913) A[3]:(0.938735902309)\n",
      " state (2)  A[0]:(0.999998748302) A[1]:(2.74925923804e-07) A[2]:(9.95778918877e-07) A[3]:(4.63029137601e-12)\n",
      " state (3)  A[0]:(0.999998688698) A[1]:(5.88856323702e-07) A[2]:(7.23838184058e-07) A[3]:(7.89285357992e-13)\n",
      " state (4)  A[0]:(0.999797463417) A[1]:(0.000197968372959) A[2]:(4.54642758996e-06) A[3]:(1.73008597059e-16)\n",
      " state (5)  A[0]:(0.239538043737) A[1]:(0.722044467926) A[2]:(0.0384174622595) A[3]:(5.57507881015e-24)\n",
      " state (6)  A[0]:(0.0354401133955) A[1]:(0.956879854202) A[2]:(0.00768003193662) A[3]:(2.30032382304e-25)\n",
      " state (7)  A[0]:(0.0129341566935) A[1]:(0.984990000725) A[2]:(0.0020758593455) A[3]:(6.56379296649e-26)\n",
      " state (8)  A[0]:(0.00231195916422) A[1]:(0.997451364994) A[2]:(0.000236685664277) A[3]:(9.86442135253e-27)\n",
      " state (9)  A[0]:(0.000128466344904) A[1]:(0.999858021736) A[2]:(1.34839165185e-05) A[3]:(7.75404095662e-28)\n",
      " state (10)  A[0]:(2.58654636127e-05) A[1]:(0.999970555305) A[2]:(3.56879559149e-06) A[3]:(2.30767598164e-28)\n",
      " state (11)  A[0]:(1.75785589818e-05) A[1]:(0.999979913235) A[2]:(2.53606162914e-06) A[3]:(1.70032422366e-28)\n",
      " state (12)  A[0]:(1.60301733558e-05) A[1]:(0.999981641769) A[2]:(2.3284726467e-06) A[3]:(1.57665412303e-28)\n",
      " state (13)  A[0]:(1.56513760885e-05) A[1]:(0.999982059002) A[2]:(2.27713462664e-06) A[3]:(1.54590003108e-28)\n",
      " state (14)  A[0]:(1.55506822921e-05) A[1]:(0.999982178211) A[2]:(2.26360702982e-06) A[3]:(1.5377599678e-28)\n",
      " state (15)  A[0]:(1.55228262884e-05) A[1]:(0.999982237816) A[2]:(2.25994858738e-06) A[3]:(1.53553250945e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 473000 finished after 16 . Running score: 0.12. Policy_loss: -92050.6097533, Value_loss: 0.979444644292. Times trained:               13041. Times reached goal: 152.               Steps done: 5619288.\n",
      " state (0)  A[0]:(0.996099472046) A[1]:(0.00119583623018) A[2]:(0.00117092090659) A[3]:(0.00153379666153)\n",
      " state (1)  A[0]:(0.0379539690912) A[1]:(0.00439581368119) A[2]:(0.0137763153762) A[3]:(0.943873882294)\n",
      " state (2)  A[0]:(0.999998271465) A[1]:(3.28937346694e-07) A[2]:(1.38505106406e-06) A[3]:(6.51819397068e-12)\n",
      " state (3)  A[0]:(0.999998629093) A[1]:(3.95484988758e-07) A[2]:(9.94611355054e-07) A[3]:(1.86649393857e-12)\n",
      " state (4)  A[0]:(0.999990344048) A[1]:(7.41295298212e-06) A[2]:(2.26355837185e-06) A[3]:(2.48649892858e-14)\n",
      " state (5)  A[0]:(0.864355742931) A[1]:(0.0788555145264) A[2]:(0.0567887201905) A[3]:(2.00016578665e-22)\n",
      " state (6)  A[0]:(0.0361913442612) A[1]:(0.920647740364) A[2]:(0.0431608967483) A[3]:(3.51045494058e-25)\n",
      " state (7)  A[0]:(0.00237890216522) A[1]:(0.995841622353) A[2]:(0.00177944661118) A[3]:(1.59329734516e-26)\n",
      " state (8)  A[0]:(5.6189532188e-05) A[1]:(0.999896347523) A[2]:(4.74715416203e-05) A[3]:(5.77373478766e-28)\n",
      " state (9)  A[0]:(1.02206022348e-05) A[1]:(0.999979972839) A[2]:(9.78187108558e-06) A[3]:(1.37852034851e-28)\n",
      " state (10)  A[0]:(7.40958876122e-06) A[1]:(0.999985635281) A[2]:(6.95791686667e-06) A[3]:(1.02216324386e-28)\n",
      " state (11)  A[0]:(6.93725087331e-06) A[1]:(0.999986588955) A[2]:(6.46747457722e-06) A[3]:(9.59201945824e-29)\n",
      " state (12)  A[0]:(6.83461348672e-06) A[1]:(0.999986827374) A[2]:(6.3605075411e-06) A[3]:(9.45398023503e-29)\n",
      " state (13)  A[0]:(6.81059145791e-06) A[1]:(0.999986827374) A[2]:(6.3355405473e-06) A[3]:(9.42172211217e-29)\n",
      " state (14)  A[0]:(6.80478751747e-06) A[1]:(0.999986886978) A[2]:(6.32955016044e-06) A[3]:(9.41389079953e-29)\n",
      " state (15)  A[0]:(6.80330822433e-06) A[1]:(0.999986886978) A[2]:(6.3280294853e-06) A[3]:(9.4118800083e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 474000 finished after 3 . Running score: 0.14. Policy_loss: -92050.609694, Value_loss: 1.21033994626. Times trained:               12888. Times reached goal: 129.               Steps done: 5632176.\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.7040e-07,  9.6603e-07,  1.0898e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.7039e-07,  9.6604e-07,  1.0898e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.7039e-07,  9.6604e-07,  1.0898e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.0963e-04,  9.9899e-01,  3.9948e-04,  2.3393e-27]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996862053871) A[1]:(0.00114373711403) A[2]:(0.00106522860005) A[3]:(0.000928971974645)\n",
      " state (1)  A[0]:(0.0324023365974) A[1]:(0.00433980440721) A[2]:(0.012839557603) A[3]:(0.950418293476)\n",
      " state (2)  A[0]:(0.994523584843) A[1]:(0.00100131286308) A[2]:(0.00384560390376) A[3]:(0.000629500020295)\n",
      " state (3)  A[0]:(0.999998688698) A[1]:(2.78755123873e-07) A[2]:(1.00513375401e-06) A[3]:(2.70537216761e-12)\n",
      " state (4)  A[0]:(0.999998450279) A[1]:(5.70343388517e-07) A[2]:(9.66034917838e-07) A[3]:(1.08989195707e-12)\n",
      " state (5)  A[0]:(0.999986290932) A[1]:(1.14629237942e-05) A[2]:(2.27189502766e-06) A[3]:(8.46576057683e-15)\n",
      " state (6)  A[0]:(0.940590918064) A[1]:(0.0385285653174) A[2]:(0.0208805222064) A[3]:(2.28648796402e-22)\n",
      " state (7)  A[0]:(0.0340577661991) A[1]:(0.932688117027) A[2]:(0.0332540981472) A[3]:(1.75687212202e-25)\n",
      " state (8)  A[0]:(0.000610036193393) A[1]:(0.998990178108) A[2]:(0.000399757263949) A[3]:(2.34077294329e-27)\n",
      " state (9)  A[0]:(1.68806836882e-05) A[1]:(0.999966800213) A[2]:(1.63034692378e-05) A[3]:(1.17710888197e-28)\n",
      " state (10)  A[0]:(6.79491358824e-06) A[1]:(0.999986290932) A[2]:(6.89192029313e-06) A[3]:(5.36792305205e-29)\n",
      " state (11)  A[0]:(5.66629796594e-06) A[1]:(0.999988615513) A[2]:(5.69331632505e-06) A[3]:(4.52688118147e-29)\n",
      " state (12)  A[0]:(5.42233465239e-06) A[1]:(0.999989151955) A[2]:(5.42976476936e-06) A[3]:(4.34016287521e-29)\n",
      " state (13)  A[0]:(5.3581411521e-06) A[1]:(0.999989271164) A[2]:(5.36094194103e-06) A[3]:(4.2911046247e-29)\n",
      " state (14)  A[0]:(5.33955289939e-06) A[1]:(0.999989330769) A[2]:(5.34134596819e-06) A[3]:(4.27708295193e-29)\n",
      " state (15)  A[0]:(5.3337917052e-06) A[1]:(0.999989330769) A[2]:(5.33536876901e-06) A[3]:(4.27274539658e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 475000 finished after 15 . Running score: 0.14. Policy_loss: -92050.6096635, Value_loss: 1.21829085416. Times trained:               12832. Times reached goal: 143.               Steps done: 5645008.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994738042355) A[1]:(0.00131189450622) A[2]:(0.00135031517129) A[3]:(0.00259971967898)\n",
      " state (1)  A[0]:(0.0242083109915) A[1]:(0.0039115822874) A[2]:(0.0105482004583) A[3]:(0.961331903934)\n",
      " state (2)  A[0]:(0.99998408556) A[1]:(3.58407987733e-06) A[2]:(1.23256395455e-05) A[3]:(1.21173282519e-09)\n",
      " state (3)  A[0]:(0.999998629093) A[1]:(3.94782205149e-07) A[2]:(9.7512645425e-07) A[3]:(2.59548567534e-12)\n",
      " state (4)  A[0]:(0.999996423721) A[1]:(2.47574394052e-06) A[2]:(1.1123394188e-06) A[3]:(2.49686149577e-13)\n",
      " state (5)  A[0]:(0.998512744904) A[1]:(0.00139787746593) A[2]:(8.93960459507e-05) A[3]:(3.7407568162e-18)\n",
      " state (6)  A[0]:(0.069487221539) A[1]:(0.886155605316) A[2]:(0.0443572029471) A[3]:(1.52541342989e-24)\n",
      " state (7)  A[0]:(0.00175745144952) A[1]:(0.997219741344) A[2]:(0.00102278625127) A[3]:(2.2325772805e-26)\n",
      " state (8)  A[0]:(3.70668021787e-05) A[1]:(0.999945163727) A[2]:(1.7747604943e-05) A[3]:(6.12758637783e-28)\n",
      " state (9)  A[0]:(3.56502846444e-06) A[1]:(0.999993979931) A[2]:(2.48469291364e-06) A[3]:(1.02553843609e-28)\n",
      " state (10)  A[0]:(2.22299559027e-06) A[1]:(0.999996185303) A[2]:(1.59791738952e-06) A[3]:(6.94736616238e-29)\n",
      " state (11)  A[0]:(2.01514740183e-06) A[1]:(0.999996542931) A[2]:(1.44818761783e-06) A[3]:(6.37745279732e-29)\n",
      " state (12)  A[0]:(1.96626774596e-06) A[1]:(0.999996602535) A[2]:(1.4128257817e-06) A[3]:(6.24123237421e-29)\n",
      " state (13)  A[0]:(1.95293182514e-06) A[1]:(0.99999666214) A[2]:(1.4033480511e-06) A[3]:(6.20429725085e-29)\n",
      " state (14)  A[0]:(1.94897256733e-06) A[1]:(0.99999666214) A[2]:(1.40060444664e-06) A[3]:(6.19341935778e-29)\n",
      " state (15)  A[0]:(1.94771291717e-06) A[1]:(0.99999666214) A[2]:(1.39974986268e-06) A[3]:(6.19001828587e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 476000 finished after 8 . Running score: 0.13. Policy_loss: -92050.6095996, Value_loss: 1.20637788181. Times trained:               13195. Times reached goal: 126.               Steps done: 5658203.\n",
      " state (0)  A[0]:(0.996260285378) A[1]:(0.00119161931798) A[2]:(0.00119616219308) A[3]:(0.00135192635935)\n",
      " state (1)  A[0]:(0.033490113914) A[1]:(0.00424624374136) A[2]:(0.0129316886887) A[3]:(0.94933193922)\n",
      " state (2)  A[0]:(0.999995410442) A[1]:(9.38082678204e-07) A[2]:(3.65802225133e-06) A[3]:(6.72551181413e-11)\n",
      " state (3)  A[0]:(0.999998688698) A[1]:(4.15687111399e-07) A[2]:(9.24985954498e-07) A[3]:(1.73685924584e-12)\n",
      " state (4)  A[0]:(0.999991834164) A[1]:(6.9356469794e-06) A[2]:(1.22536960134e-06) A[3]:(2.98816012082e-14)\n",
      " state (5)  A[0]:(0.98155850172) A[1]:(0.0174298025668) A[2]:(0.00101170339622) A[3]:(7.57530971e-20)\n",
      " state (6)  A[0]:(0.0860220491886) A[1]:(0.831788301468) A[2]:(0.0821896567941) A[3]:(9.99084250638e-25)\n",
      " state (7)  A[0]:(0.018195990473) A[1]:(0.965885221958) A[2]:(0.0159187577665) A[3]:(1.33578951069e-25)\n",
      " state (8)  A[0]:(0.00330789410509) A[1]:(0.994637072086) A[2]:(0.00205500773154) A[3]:(2.13473464673e-26)\n",
      " state (9)  A[0]:(0.000149622836034) A[1]:(0.999762713909) A[2]:(8.76861886354e-05) A[3]:(1.29579638517e-27)\n",
      " state (10)  A[0]:(1.61778989423e-05) A[1]:(0.999972879887) A[2]:(1.09623506432e-05) A[3]:(2.03027731026e-28)\n",
      " state (11)  A[0]:(8.75206842466e-06) A[1]:(0.999985456467) A[2]:(5.81667291044e-06) A[3]:(1.17096841545e-28)\n",
      " state (12)  A[0]:(7.47099466025e-06) A[1]:(0.999987661839) A[2]:(4.89412468596e-06) A[3]:(1.00961713435e-28)\n",
      " state (13)  A[0]:(7.13154258847e-06) A[1]:(0.99998819828) A[2]:(4.65060429633e-06) A[3]:(9.66269506866e-29)\n",
      " state (14)  A[0]:(7.02517172613e-06) A[1]:(0.999988377094) A[2]:(4.57561509393e-06) A[3]:(9.52756219412e-29)\n",
      " state (15)  A[0]:(6.98872690919e-06) A[1]:(0.999988436699) A[2]:(4.550453923e-06) A[3]:(9.48187793213e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 477000 finished after 8 . Running score: 0.17. Policy_loss: -92050.6097262, Value_loss: 1.19523753677. Times trained:               12528. Times reached goal: 130.               Steps done: 5670731.\n",
      " state (0)  A[0]:(0.995844602585) A[1]:(0.00124287931249) A[2]:(0.00123472872656) A[3]:(0.0016778166173)\n",
      " state (1)  A[0]:(0.0231724213809) A[1]:(0.0036398882512) A[2]:(0.0102719226852) A[3]:(0.96291577816)\n",
      " state (2)  A[0]:(0.9977196455) A[1]:(0.000438202056102) A[2]:(0.00172726681922) A[3]:(0.000114880516776)\n",
      " state (3)  A[0]:(0.999998807907) A[1]:(2.49342491543e-07) A[2]:(9.21153173294e-07) A[3]:(2.87165929057e-12)\n",
      " state (4)  A[0]:(0.999998569489) A[1]:(5.48005630208e-07) A[2]:(8.99625661077e-07) A[3]:(1.01825015433e-12)\n",
      " state (5)  A[0]:(0.999974668026) A[1]:(1.99858386623e-05) A[2]:(5.31934801984e-06) A[3]:(1.57145353435e-15)\n",
      " state (6)  A[0]:(0.768598496914) A[1]:(0.118767932057) A[2]:(0.112633541226) A[3]:(3.28399544737e-23)\n",
      " state (7)  A[0]:(0.0668027997017) A[1]:(0.884528040886) A[2]:(0.0486691370606) A[3]:(5.90070274384e-25)\n",
      " state (8)  A[0]:(0.00265198200941) A[1]:(0.996064722538) A[2]:(0.00128332013264) A[3]:(2.04359363286e-26)\n",
      " state (9)  A[0]:(5.48690004507e-05) A[1]:(0.999912381172) A[2]:(3.27585294144e-05) A[3]:(7.65074177812e-28)\n",
      " state (10)  A[0]:(1.61060070241e-05) A[1]:(0.999974787235) A[2]:(9.0928069767e-06) A[3]:(2.52407659983e-28)\n",
      " state (11)  A[0]:(1.23941445054e-05) A[1]:(0.999980926514) A[2]:(6.69509745421e-06) A[3]:(1.94955966036e-28)\n",
      " state (12)  A[0]:(1.16126730063e-05) A[1]:(0.999982178211) A[2]:(6.19437059868e-06) A[3]:(1.82621889375e-28)\n",
      " state (13)  A[0]:(1.14042404675e-05) A[1]:(0.999982535839) A[2]:(6.06388948654e-06) A[3]:(1.79365250172e-28)\n",
      " state (14)  A[0]:(1.1341681784e-05) A[1]:(0.999982655048) A[2]:(6.02634827374e-06) A[3]:(1.78416031525e-28)\n",
      " state (15)  A[0]:(1.13208461698e-05) A[1]:(0.999982655048) A[2]:(6.01469309913e-06) A[3]:(1.78115454046e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 478000 finished after 17 . Running score: 0.11. Policy_loss: -92050.6098044, Value_loss: 1.41950726932. Times trained:               12625. Times reached goal: 134.               Steps done: 5683356.\n",
      " state (0)  A[0]:(0.996390402317) A[1]:(0.00121514790226) A[2]:(0.00118535989895) A[3]:(0.00120911828708)\n",
      " state (1)  A[0]:(0.0220290552825) A[1]:(0.00362279242836) A[2]:(0.0100779207423) A[3]:(0.964270234108)\n",
      " state (2)  A[0]:(0.999665677547) A[1]:(6.76589479554e-05) A[2]:(0.000265143928118) A[3]:(1.5086179701e-06)\n",
      " state (3)  A[0]:(0.999998748302) A[1]:(2.75079884204e-07) A[2]:(9.49153616148e-07) A[3]:(2.75928867428e-12)\n",
      " state (4)  A[0]:(0.999998033047) A[1]:(9.03085947357e-07) A[2]:(1.07693006157e-06) A[3]:(5.38260842148e-13)\n",
      " state (5)  A[0]:(0.999650299549) A[1]:(0.000218531160499) A[2]:(0.000131143518956) A[3]:(4.04525440739e-18)\n",
      " state (6)  A[0]:(0.319347262383) A[1]:(0.48737847805) A[2]:(0.19327428937) A[3]:(4.18709619121e-24)\n",
      " state (7)  A[0]:(0.0185145325959) A[1]:(0.96817946434) A[2]:(0.0133060198277) A[3]:(1.57948368197e-25)\n",
      " state (8)  A[0]:(0.000306605012156) A[1]:(0.999496877193) A[2]:(0.000196507709916) A[3]:(3.53812667121e-27)\n",
      " state (9)  A[0]:(1.88700450963e-05) A[1]:(0.999968230724) A[2]:(1.28762076201e-05) A[3]:(3.18714659444e-28)\n",
      " state (10)  A[0]:(1.03118445622e-05) A[1]:(0.999983251095) A[2]:(6.42922714178e-06) A[3]:(1.76879861653e-28)\n",
      " state (11)  A[0]:(9.07050616661e-06) A[1]:(0.999985456467) A[2]:(5.49879177925e-06) A[3]:(1.55180180266e-28)\n",
      " state (12)  A[0]:(8.79212802829e-06) A[1]:(0.999985933304) A[2]:(5.29283079231e-06) A[3]:(1.50296876558e-28)\n",
      " state (13)  A[0]:(8.72006512509e-06) A[1]:(0.999986052513) A[2]:(5.24063580087e-06) A[3]:(1.49047658211e-28)\n",
      " state (14)  A[0]:(8.69961513672e-06) A[1]:(0.999986052513) A[2]:(5.22638129041e-06) A[3]:(1.48702375083e-28)\n",
      " state (15)  A[0]:(8.69327868713e-06) A[1]:(0.999986112118) A[2]:(5.22220670973e-06) A[3]:(1.48599181349e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 479000 finished after 29 . Running score: 0.1. Policy_loss: -92050.6095957, Value_loss: 1.20035000146. Times trained:               12488. Times reached goal: 113.               Steps done: 5695844.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9947,  0.0013,  0.0014,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.7552e-07,  1.4284e-06,  8.2636e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.7571e-07,  1.4284e-06,  8.2614e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.7588e-07,  1.4285e-06,  8.2593e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.9145e-03,  9.7732e-01,  1.6770e-02,  1.2675e-25]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994686067104) A[1]:(0.00133270269725) A[2]:(0.00144901010208) A[3]:(0.00253221718594)\n",
      " state (1)  A[0]:(0.0179038550705) A[1]:(0.00328940385953) A[2]:(0.00929827149957) A[3]:(0.969508469105)\n",
      " state (2)  A[0]:(0.997575879097) A[1]:(0.000434759800555) A[2]:(0.00188102584798) A[3]:(0.000108306165203)\n",
      " state (3)  A[0]:(0.999998509884) A[1]:(2.71747751412e-07) A[2]:(1.22994970297e-06) A[3]:(3.17057686529e-12)\n",
      " state (4)  A[0]:(0.999997794628) A[1]:(7.74121986069e-07) A[2]:(1.42788587709e-06) A[3]:(8.28307799954e-13)\n",
      " state (5)  A[0]:(0.999850988388) A[1]:(9.67788291746e-05) A[2]:(5.2220912039e-05) A[3]:(1.08643754537e-16)\n",
      " state (6)  A[0]:(0.313380777836) A[1]:(0.212005212903) A[2]:(0.474614024162) A[3]:(1.2449248237e-23)\n",
      " state (7)  A[0]:(0.0578824579716) A[1]:(0.74096685648) A[2]:(0.201150670648) A[3]:(1.21938411083e-24)\n",
      " state (8)  A[0]:(0.00596609339118) A[1]:(0.977106332779) A[2]:(0.0169275570661) A[3]:(1.277505139e-25)\n",
      " state (9)  A[0]:(0.000125692633446) A[1]:(0.999476850033) A[2]:(0.000397453113692) A[3]:(4.5578642063e-27)\n",
      " state (10)  A[0]:(2.06434469874e-05) A[1]:(0.999923706055) A[2]:(5.5629399867e-05) A[3]:(8.48913615706e-28)\n",
      " state (11)  A[0]:(1.32437226057e-05) A[1]:(0.999954342842) A[2]:(3.24012180499e-05) A[3]:(5.41869393576e-28)\n",
      " state (12)  A[0]:(1.18256293717e-05) A[1]:(0.999960064888) A[2]:(2.8110051062e-05) A[3]:(4.81949524108e-28)\n",
      " state (13)  A[0]:(1.1458661902e-05) A[1]:(0.999961495399) A[2]:(2.70184682449e-05) A[3]:(4.6644376583e-28)\n",
      " state (14)  A[0]:(1.13520509331e-05) A[1]:(0.999961912632) A[2]:(2.67060477199e-05) A[3]:(4.61969156493e-28)\n",
      " state (15)  A[0]:(1.13187134048e-05) A[1]:(0.999962091446) A[2]:(2.66104598268e-05) A[3]:(4.60587879537e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 480000 finished after 5 . Running score: 0.19. Policy_loss: -92050.6179561, Value_loss: 1.62108033212. Times trained:               12997. Times reached goal: 119.               Steps done: 5708841.\n",
      " state (0)  A[0]:(0.994581639767) A[1]:(0.00127806409728) A[2]:(0.00150089140516) A[3]:(0.00263938540593)\n",
      " state (1)  A[0]:(0.0191435739398) A[1]:(0.00326458131894) A[2]:(0.0098771257326) A[3]:(0.967714726925)\n",
      " state (2)  A[0]:(0.991859138012) A[1]:(0.00120474444702) A[2]:(0.0056617949158) A[3]:(0.00127429515123)\n",
      " state (3)  A[0]:(0.999998450279) A[1]:(2.33301392427e-07) A[2]:(1.34036019972e-06) A[3]:(3.22344971895e-12)\n",
      " state (4)  A[0]:(0.999997913837) A[1]:(4.8387590823e-07) A[2]:(1.63058973612e-06) A[3]:(1.17281184764e-12)\n",
      " state (5)  A[0]:(0.999922215939) A[1]:(2.44988405029e-05) A[2]:(5.33031416126e-05) A[3]:(3.47991283811e-16)\n",
      " state (6)  A[0]:(0.404400110245) A[1]:(0.0569027252495) A[2]:(0.538697183132) A[3]:(1.3114661088e-23)\n",
      " state (7)  A[0]:(0.0919313803315) A[1]:(0.404418259859) A[2]:(0.503650426865) A[3]:(1.74505636616e-24)\n",
      " state (8)  A[0]:(0.0216408390552) A[1]:(0.857314109802) A[2]:(0.121045053005) A[3]:(4.49695434529e-25)\n",
      " state (9)  A[0]:(0.000578918436076) A[1]:(0.995653688908) A[2]:(0.00376736931503) A[3]:(2.019398252e-26)\n",
      " state (10)  A[0]:(5.16652035003e-05) A[1]:(0.999702095985) A[2]:(0.000246241746936) A[3]:(1.96996151774e-27)\n",
      " state (11)  A[0]:(2.55177219515e-05) A[1]:(0.999874293804) A[2]:(0.00010020792979) A[3]:(9.37191792492e-28)\n",
      " state (12)  A[0]:(2.11683636735e-05) A[1]:(0.999900460243) A[2]:(7.83680443419e-05) A[3]:(7.66285972933e-28)\n",
      " state (13)  A[0]:(2.00836802833e-05) A[1]:(0.99990683794) A[2]:(7.30974134058e-05) A[3]:(7.23859180706e-28)\n",
      " state (14)  A[0]:(1.97719709831e-05) A[1]:(0.99990862608) A[2]:(7.16028152965e-05) A[3]:(7.11708970169e-28)\n",
      " state (15)  A[0]:(1.96754481294e-05) A[1]:(0.999909162521) A[2]:(7.11451721145e-05) A[3]:(7.07964432726e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 481000 finished after 22 . Running score: 0.11. Policy_loss: -92050.6155078, Value_loss: 1.62250530644. Times trained:               13586. Times reached goal: 127.               Steps done: 5722427.\n",
      " state (0)  A[0]:(0.996040284634) A[1]:(0.00117346714251) A[2]:(0.00131430232432) A[3]:(0.00147195579484)\n",
      " state (1)  A[0]:(0.0207984615117) A[1]:(0.003372810781) A[2]:(0.00993842165917) A[3]:(0.965890288353)\n",
      " state (2)  A[0]:(0.998680770397) A[1]:(0.000238428430748) A[2]:(0.0010509461863) A[3]:(2.98519535136e-05)\n",
      " state (3)  A[0]:(0.999998509884) A[1]:(2.79965689742e-07) A[2]:(1.20269373838e-06) A[3]:(2.99257493785e-12)\n",
      " state (4)  A[0]:(0.999996304512) A[1]:(1.47056925925e-06) A[2]:(2.22350877266e-06) A[3]:(3.07152578111e-13)\n",
      " state (5)  A[0]:(0.98073643446) A[1]:(0.00311870197766) A[2]:(0.0161448474973) A[3]:(8.81471448835e-21)\n",
      " state (6)  A[0]:(0.122494444251) A[1]:(0.502322494984) A[2]:(0.375183105469) A[3]:(2.74951907045e-24)\n",
      " state (7)  A[0]:(0.0110046928748) A[1]:(0.95697003603) A[2]:(0.0320252552629) A[3]:(2.28361303736e-25)\n",
      " state (8)  A[0]:(0.000129565218231) A[1]:(0.999452888966) A[2]:(0.000417517934693) A[3]:(4.79627892598e-27)\n",
      " state (9)  A[0]:(1.59026913025e-05) A[1]:(0.999945223331) A[2]:(3.88504922739e-05) A[3]:(6.43563405761e-28)\n",
      " state (10)  A[0]:(1.02361018435e-05) A[1]:(0.999967575073) A[2]:(2.21803711611e-05) A[3]:(4.06452588806e-28)\n",
      " state (11)  A[0]:(9.30691021495e-06) A[1]:(0.999971091747) A[2]:(1.95907614398e-05) A[3]:(3.67368729479e-28)\n",
      " state (12)  A[0]:(9.09945993044e-06) A[1]:(0.999971866608) A[2]:(1.90222508536e-05) A[3]:(3.58661840942e-28)\n",
      " state (13)  A[0]:(9.04780154087e-06) A[1]:(0.999972045422) A[2]:(1.88825761143e-05) A[3]:(3.56509397562e-28)\n",
      " state (14)  A[0]:(9.03397267393e-06) A[1]:(0.999972105026) A[2]:(1.88458761841e-05) A[3]:(3.55938696371e-28)\n",
      " state (15)  A[0]:(9.02987176232e-06) A[1]:(0.999972105026) A[2]:(1.88353824342e-05) A[3]:(3.55775786772e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 482000 finished after 11 . Running score: 0.17. Policy_loss: -92050.6159456, Value_loss: 1.4132557771. Times trained:               12979. Times reached goal: 144.               Steps done: 5735406.\n",
      " state (0)  A[0]:(0.9967674613) A[1]:(0.001075549284) A[2]:(0.00114277377725) A[3]:(0.00101423705928)\n",
      " state (1)  A[0]:(0.022209091112) A[1]:(0.0034756436944) A[2]:(0.00996558181942) A[3]:(0.964349687099)\n",
      " state (2)  A[0]:(0.988185882568) A[1]:(0.0018183554057) A[2]:(0.00712605752051) A[3]:(0.00286968634464)\n",
      " state (3)  A[0]:(0.999998569489) A[1]:(3.13058478696e-07) A[2]:(1.13015175884e-06) A[3]:(3.42952554376e-12)\n",
      " state (4)  A[0]:(0.999997377396) A[1]:(1.31978617901e-06) A[2]:(1.2762610595e-06) A[3]:(7.0421945185e-13)\n",
      " state (5)  A[0]:(0.991700947285) A[1]:(0.00629293685779) A[2]:(0.00200614030473) A[3]:(5.13363539293e-19)\n",
      " state (6)  A[0]:(0.0410629734397) A[1]:(0.836768388748) A[2]:(0.122168660164) A[3]:(2.25321374004e-24)\n",
      " state (7)  A[0]:(0.00298211537302) A[1]:(0.990038573742) A[2]:(0.0069793225266) A[3]:(1.08006742927e-25)\n",
      " state (8)  A[0]:(5.36466541234e-05) A[1]:(0.999826729298) A[2]:(0.000119599913887) A[3]:(2.98531159183e-27)\n",
      " state (9)  A[0]:(3.31624323735e-06) A[1]:(0.999986886978) A[2]:(9.78322623268e-06) A[3]:(3.28548241073e-28)\n",
      " state (10)  A[0]:(1.82057578968e-06) A[1]:(0.999992847443) A[2]:(5.30368106411e-06) A[3]:(1.95473619894e-28)\n",
      " state (11)  A[0]:(1.60504600899e-06) A[1]:(0.999993741512) A[2]:(4.63139485873e-06) A[3]:(1.74520477101e-28)\n",
      " state (12)  A[0]:(1.55580073624e-06) A[1]:(0.999993979931) A[2]:(4.48205901193e-06) A[3]:(1.69777501464e-28)\n",
      " state (13)  A[0]:(1.54203962666e-06) A[1]:(0.999994039536) A[2]:(4.44382158094e-06) A[3]:(1.68534072943e-28)\n",
      " state (14)  A[0]:(1.53737016717e-06) A[1]:(0.999994039536) A[2]:(4.43281669504e-06) A[3]:(1.68161598092e-28)\n",
      " state (15)  A[0]:(1.53540679548e-06) A[1]:(0.999994039536) A[2]:(4.42909777121e-06) A[3]:(1.68027577442e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 483000 finished after 22 . Running score: 0.16. Policy_loss: -92050.6159554, Value_loss: 1.40377187932. Times trained:               12633. Times reached goal: 143.               Steps done: 5748039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996869683266) A[1]:(0.00108205503784) A[2]:(0.00114426680375) A[3]:(0.00090400892077)\n",
      " state (1)  A[0]:(0.0280525851995) A[1]:(0.00382342841476) A[2]:(0.0120662497357) A[3]:(0.956057727337)\n",
      " state (2)  A[0]:(0.999970316887) A[1]:(5.48510161025e-06) A[2]:(2.41985399043e-05) A[3]:(3.84580545187e-09)\n",
      " state (3)  A[0]:(0.999998271465) A[1]:(4.61579105604e-07) A[2]:(1.28239912556e-06) A[3]:(2.26665626372e-12)\n",
      " state (4)  A[0]:(0.999953866005) A[1]:(3.89959677705e-05) A[2]:(7.1603080869e-06) A[3]:(6.25693370203e-15)\n",
      " state (5)  A[0]:(0.132742449641) A[1]:(0.482300847769) A[2]:(0.384956717491) A[3]:(9.07853236367e-24)\n",
      " state (6)  A[0]:(0.0164192207158) A[1]:(0.88887566328) A[2]:(0.0947051346302) A[3]:(4.57363409046e-25)\n",
      " state (7)  A[0]:(0.00454274984077) A[1]:(0.973846912384) A[2]:(0.0216103307903) A[3]:(1.16529620799e-25)\n",
      " state (8)  A[0]:(0.000420534721343) A[1]:(0.997797012329) A[2]:(0.0017824441893) A[3]:(1.29193768823e-26)\n",
      " state (9)  A[0]:(1.2194232113e-05) A[1]:(0.999920904636) A[2]:(6.68969587423e-05) A[3]:(7.06931075015e-28)\n",
      " state (10)  A[0]:(2.91492574434e-06) A[1]:(0.999981164932) A[2]:(1.59327446454e-05) A[3]:(2.06263654444e-28)\n",
      " state (11)  A[0]:(2.09562540476e-06) A[1]:(0.999986827374) A[2]:(1.10630344352e-05) A[3]:(1.52029238506e-28)\n",
      " state (12)  A[0]:(1.93225514522e-06) A[1]:(0.999987959862) A[2]:(1.00888228189e-05) A[3]:(1.40821072527e-28)\n",
      " state (13)  A[0]:(1.8910552626e-06) A[1]:(0.999988257885) A[2]:(9.84489906841e-06) A[3]:(1.37984695313e-28)\n",
      " state (14)  A[0]:(1.87973489574e-06) A[1]:(0.99998831749) A[2]:(9.77876243269e-06) A[3]:(1.37211013143e-28)\n",
      " state (15)  A[0]:(1.87645730421e-06) A[1]:(0.999988377094) A[2]:(9.76009141596e-06) A[3]:(1.36991360833e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 484000 finished after 5 . Running score: 0.09. Policy_loss: -92050.6165492, Value_loss: 0.979411528698. Times trained:               12689. Times reached goal: 137.               Steps done: 5760728.\n",
      "action_dist \n",
      "tensor([[ 0.9934,  0.0014,  0.0018,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9934,  0.0014,  0.0018,  0.0033]])\n",
      "On state=0, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5457e-07,  1.2738e-06,  2.1376e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2485e-04,  9.9720e-01,  2.1706e-03,  9.5527e-27]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4502e-05,  9.9992e-01,  6.5404e-05,  4.2047e-28]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.5910e-06,  9.9998e-01,  1.8997e-05,  1.4672e-28]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.99349629879) A[1]:(0.00142854382284) A[2]:(0.00179448188283) A[3]:(0.00328070204705)\n",
      " state (1)  A[0]:(0.0222608521581) A[1]:(0.00347459968179) A[2]:(0.010660963133) A[3]:(0.963603615761)\n",
      " state (2)  A[0]:(0.895658314228) A[1]:(0.00667175324634) A[2]:(0.0289314147085) A[3]:(0.0687385126948)\n",
      " state (3)  A[0]:(0.999998450279) A[1]:(2.460131725e-07) A[2]:(1.31205194975e-06) A[3]:(3.58667457713e-12)\n",
      " state (4)  A[0]:(0.999998390675) A[1]:(3.54526235924e-07) A[2]:(1.27367968616e-06) A[3]:(2.13778300617e-12)\n",
      " state (5)  A[0]:(0.999988853931) A[1]:(5.65144227949e-06) A[2]:(5.49874221178e-06) A[3]:(2.39112979498e-14)\n",
      " state (6)  A[0]:(0.590374290943) A[1]:(0.066722407937) A[2]:(0.342903286219) A[3]:(3.59824670866e-23)\n",
      " state (7)  A[0]:(0.0412146151066) A[1]:(0.790801525116) A[2]:(0.167983859777) A[3]:(5.44260036934e-25)\n",
      " state (8)  A[0]:(0.000625341839623) A[1]:(0.997202515602) A[2]:(0.00217215460725) A[3]:(9.55941490872e-27)\n",
      " state (9)  A[0]:(1.45050462379e-05) A[1]:(0.999920070171) A[2]:(6.54160685372e-05) A[3]:(4.20560362686e-28)\n",
      " state (10)  A[0]:(5.95198980591e-06) A[1]:(0.999968588352) A[2]:(2.54813148786e-05) A[3]:(1.87818590032e-28)\n",
      " state (11)  A[0]:(4.90480761073e-06) A[1]:(0.999974608421) A[2]:(2.0474075427e-05) A[3]:(1.56257220476e-28)\n",
      " state (12)  A[0]:(4.66223582407e-06) A[1]:(0.999976038933) A[2]:(1.93262367247e-05) A[3]:(1.48864345791e-28)\n",
      " state (13)  A[0]:(4.59105103801e-06) A[1]:(0.999976396561) A[2]:(1.89973179658e-05) A[3]:(1.46722037618e-28)\n",
      " state (14)  A[0]:(4.56657153336e-06) A[1]:(0.99997651577) A[2]:(1.88886388059e-05) A[3]:(1.46006277791e-28)\n",
      " state (15)  A[0]:(4.55690542367e-06) A[1]:(0.999976575375) A[2]:(1.88480444194e-05) A[3]:(1.45735841116e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 485000 finished after 6 . Running score: 0.15. Policy_loss: -92050.6384022, Value_loss: 1.62384817074. Times trained:               12663. Times reached goal: 132.               Steps done: 5773391.\n",
      " state (0)  A[0]:(0.993659973145) A[1]:(0.00124589074403) A[2]:(0.00166481395718) A[3]:(0.00342932669446)\n",
      " state (1)  A[0]:(0.0300242248923) A[1]:(0.00371430325322) A[2]:(0.0125554678962) A[3]:(0.953706026077)\n",
      " state (2)  A[0]:(0.999984383583) A[1]:(2.53583584708e-06) A[2]:(1.3078279153e-05) A[3]:(9.18273901362e-10)\n",
      " state (3)  A[0]:(0.999998450279) A[1]:(2.79074271248e-07) A[2]:(1.25318956634e-06) A[3]:(2.67837531667e-12)\n",
      " state (4)  A[0]:(0.999996185303) A[1]:(1.82338635568e-06) A[2]:(2.01323337023e-06) A[3]:(2.31908378135e-13)\n",
      " state (5)  A[0]:(0.908319175243) A[1]:(0.0189521852881) A[2]:(0.0727286562324) A[3]:(7.36416523474e-22)\n",
      " state (6)  A[0]:(0.0803277269006) A[1]:(0.48261860013) A[2]:(0.43705368042) A[3]:(1.34573122885e-24)\n",
      " state (7)  A[0]:(0.031319797039) A[1]:(0.80728918314) A[2]:(0.161391019821) A[3]:(4.46012489481e-25)\n",
      " state (8)  A[0]:(0.00593240233138) A[1]:(0.968868196011) A[2]:(0.0251994282007) A[3]:(8.67811152321e-26)\n",
      " state (9)  A[0]:(0.000161861404194) A[1]:(0.999066948891) A[2]:(0.000771167280618) A[3]:(4.00068574004e-27)\n",
      " state (10)  A[0]:(1.458038696e-05) A[1]:(0.9999127388) A[2]:(7.26777143427e-05) A[3]:(5.18171162648e-28)\n",
      " state (11)  A[0]:(7.5771322372e-06) A[1]:(0.999957025051) A[2]:(3.54017320205e-05) A[3]:(2.83835394076e-28)\n",
      " state (12)  A[0]:(6.40124517304e-06) A[1]:(0.999964416027) A[2]:(2.91748128802e-05) A[3]:(2.41863000945e-28)\n",
      " state (13)  A[0]:(6.10473261986e-06) A[1]:(0.999966263771) A[2]:(2.76207338175e-05) A[3]:(2.31169250854e-28)\n",
      " state (14)  A[0]:(6.01841929893e-06) A[1]:(0.999966800213) A[2]:(2.71744847851e-05) A[3]:(2.28071248094e-28)\n",
      " state (15)  A[0]:(5.99120721745e-06) A[1]:(0.999966979027) A[2]:(2.70367618214e-05) A[3]:(2.27108475715e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 486000 finished after 9 . Running score: 0.14. Policy_loss: -92050.6077911, Value_loss: 0.989466956409. Times trained:               12622. Times reached goal: 120.               Steps done: 5786013.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.990531563759) A[1]:(0.00118345837109) A[2]:(0.00173223332968) A[3]:(0.00655277259648)\n",
      " state (1)  A[0]:(0.0325254276395) A[1]:(0.0036050407216) A[2]:(0.0122135737911) A[3]:(0.951655983925)\n",
      " state (2)  A[0]:(0.99999588728) A[1]:(6.98942869803e-07) A[2]:(3.3843584788e-06) A[3]:(5.80151690577e-11)\n",
      " state (3)  A[0]:(0.999998688698) A[1]:(3.61654628023e-07) A[2]:(9.36675064622e-07) A[3]:(1.4025919098e-12)\n",
      " state (4)  A[0]:(0.999942958355) A[1]:(5.1648436056e-05) A[2]:(5.40819200978e-06) A[3]:(6.41668538643e-16)\n",
      " state (5)  A[0]:(0.34385061264) A[1]:(0.355001151562) A[2]:(0.3011482656) A[3]:(4.80968217812e-24)\n",
      " state (6)  A[0]:(0.0907873436809) A[1]:(0.790367841721) A[2]:(0.118844799697) A[3]:(6.48399685096e-25)\n",
      " state (7)  A[0]:(0.0274338647723) A[1]:(0.946449398994) A[2]:(0.0261167325079) A[3]:(1.75350097424e-25)\n",
      " state (8)  A[0]:(0.00156912731472) A[1]:(0.997315824032) A[2]:(0.00111502758227) A[3]:(1.20128439686e-26)\n",
      " state (9)  A[0]:(5.72700555495e-05) A[1]:(0.999891281128) A[2]:(5.14565363119e-05) A[3]:(8.2847555084e-28)\n",
      " state (10)  A[0]:(2.14487881749e-05) A[1]:(0.999959230423) A[2]:(1.93369742192e-05) A[3]:(3.62355244945e-28)\n",
      " state (11)  A[0]:(1.73198113771e-05) A[1]:(0.999967336655) A[2]:(1.5368052118e-05) A[3]:(2.99677540093e-28)\n",
      " state (12)  A[0]:(1.64397461049e-05) A[1]:(0.999969065189) A[2]:(1.45248604895e-05) A[3]:(2.86049707955e-28)\n",
      " state (13)  A[0]:(1.62130581884e-05) A[1]:(0.999969482422) A[2]:(1.43146889968e-05) A[3]:(2.82605134101e-28)\n",
      " state (14)  A[0]:(1.61478801601e-05) A[1]:(0.999969601631) A[2]:(1.42582839544e-05) A[3]:(2.81662343245e-28)\n",
      " state (15)  A[0]:(1.61268872034e-05) A[1]:(0.999969601631) A[2]:(1.42420849443e-05) A[3]:(2.81384214887e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 487000 finished after 7 . Running score: 0.16. Policy_loss: -92050.6165721, Value_loss: 1.63351204392. Times trained:               12393. Times reached goal: 139.               Steps done: 5798406.\n",
      " state (0)  A[0]:(0.991893351078) A[1]:(0.00117267598398) A[2]:(0.00156774255447) A[3]:(0.00536622665823)\n",
      " state (1)  A[0]:(0.0518306382) A[1]:(0.00426043989137) A[2]:(0.0154224783182) A[3]:(0.928486466408)\n",
      " state (2)  A[0]:(0.999998867512) A[1]:(2.14240216678e-07) A[2]:(8.96225458291e-07) A[3]:(2.93484008797e-12)\n",
      " state (3)  A[0]:(0.999998748302) A[1]:(5.36446748356e-07) A[2]:(7.44158512589e-07) A[3]:(7.19393078094e-13)\n",
      " state (4)  A[0]:(0.999930620193) A[1]:(6.72446112731e-05) A[2]:(2.13826047002e-06) A[3]:(3.76890618603e-16)\n",
      " state (5)  A[0]:(0.46628922224) A[1]:(0.435410946608) A[2]:(0.0982998460531) A[3]:(8.90482361462e-24)\n",
      " state (6)  A[0]:(0.0547207742929) A[1]:(0.911130547523) A[2]:(0.0341486819088) A[3]:(2.77495431262e-25)\n",
      " state (7)  A[0]:(0.00820559728891) A[1]:(0.988230526447) A[2]:(0.0035638846457) A[3]:(3.71971859973e-26)\n",
      " state (8)  A[0]:(0.000203599702218) A[1]:(0.999706923962) A[2]:(8.94783806871e-05) A[3]:(1.52501893011e-27)\n",
      " state (9)  A[0]:(2.04093848879e-05) A[1]:(0.999968528748) A[2]:(1.10412456706e-05) A[3]:(2.47311745647e-28)\n",
      " state (10)  A[0]:(1.2666634575e-05) A[1]:(0.999980568886) A[2]:(6.76319632475e-06) A[3]:(1.64101075731e-28)\n",
      " state (11)  A[0]:(1.14724825835e-05) A[1]:(0.999982476234) A[2]:(6.07790025242e-06) A[3]:(1.50247067195e-28)\n",
      " state (12)  A[0]:(1.1212242498e-05) A[1]:(0.999982833862) A[2]:(5.92954847889e-06) A[3]:(1.47211861788e-28)\n",
      " state (13)  A[0]:(1.11488234324e-05) A[1]:(0.999982953072) A[2]:(5.8945024648e-06) A[3]:(1.46488121389e-28)\n",
      " state (14)  A[0]:(1.11320787255e-05) A[1]:(0.999982953072) A[2]:(5.88578404859e-06) A[3]:(1.46304941377e-28)\n",
      " state (15)  A[0]:(1.11271974674e-05) A[1]:(0.999983012676) A[2]:(5.88349485042e-06) A[3]:(1.46255854238e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 488000 finished after 18 . Running score: 0.17. Policy_loss: -92050.6150507, Value_loss: 1.20113021157. Times trained:               12773. Times reached goal: 132.               Steps done: 5811179.\n",
      " state (0)  A[0]:(0.991621434689) A[1]:(0.00117344444152) A[2]:(0.00164329749532) A[3]:(0.00556181836873)\n",
      " state (1)  A[0]:(0.0314233452082) A[1]:(0.00364375626668) A[2]:(0.0122413784266) A[3]:(0.952691495419)\n",
      " state (2)  A[0]:(0.999955236912) A[1]:(7.71114900999e-06) A[2]:(3.70138732251e-05) A[3]:(1.43883971404e-08)\n",
      " state (3)  A[0]:(0.999998807907) A[1]:(2.38365998939e-07) A[2]:(9.65891558735e-07) A[3]:(2.22054514533e-12)\n",
      " state (4)  A[0]:(0.99999755621) A[1]:(1.30223770611e-06) A[2]:(1.1174474821e-06) A[3]:(2.27233718495e-13)\n",
      " state (5)  A[0]:(0.997913122177) A[1]:(0.00149932911154) A[2]:(0.000587573624216) A[3]:(4.57328612076e-20)\n",
      " state (6)  A[0]:(0.171655729413) A[1]:(0.595248639584) A[2]:(0.233095601201) A[3]:(1.0907728634e-24)\n",
      " state (7)  A[0]:(0.0134626468644) A[1]:(0.969616174698) A[2]:(0.0169211495668) A[3]:(6.75664542221e-26)\n",
      " state (8)  A[0]:(0.00017607708287) A[1]:(0.999584019184) A[2]:(0.000239929082454) A[3]:(1.5612061908e-27)\n",
      " state (9)  A[0]:(1.70168932527e-05) A[1]:(0.999958157539) A[2]:(2.48273408943e-05) A[3]:(2.1968880653e-28)\n",
      " state (10)  A[0]:(1.06632605821e-05) A[1]:(0.999974548817) A[2]:(1.47940836541e-05) A[3]:(1.42865531423e-28)\n",
      " state (11)  A[0]:(9.67450341705e-06) A[1]:(0.999977111816) A[2]:(1.32207542265e-05) A[3]:(1.3025666067e-28)\n",
      " state (12)  A[0]:(9.45995725488e-06) A[1]:(0.999977648258) A[2]:(1.28804067572e-05) A[3]:(1.27497814173e-28)\n",
      " state (13)  A[0]:(9.40863719734e-06) A[1]:(0.999977767467) A[2]:(1.27996390802e-05) A[3]:(1.26840036875e-28)\n",
      " state (14)  A[0]:(9.39565416047e-06) A[1]:(0.999977827072) A[2]:(1.27794901346e-05) A[3]:(1.2667563468e-28)\n",
      " state (15)  A[0]:(9.39221445151e-06) A[1]:(0.999977827072) A[2]:(1.27742760014e-05) A[3]:(1.26633119776e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 489000 finished after 6 . Running score: 0.11. Policy_loss: -92050.6150428, Value_loss: 1.22273481434. Times trained:               12809. Times reached goal: 138.               Steps done: 5823988.\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0011,  0.0014,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  5.5656e-06,  1.4201e-06,  2.7005e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.7758e-04,  9.9828e-01,  8.3774e-04,  3.7177e-27]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.8906e-05,  9.9993e-01,  3.6940e-05,  2.3216e-28]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.7623e-06,  9.9998e-01,  1.1682e-05,  8.5709e-29]])\n",
      "On state=10, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2380e-06,  9.9999e-01,  8.1269e-06,  6.3157e-29]])\n",
      "On state=14, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2411e-06,  9.9999e-01,  8.1305e-06,  6.3176e-29]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2689e-06,  9.9999e-01,  8.1693e-06,  6.3424e-29]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2465e-06,  9.9999e-01,  8.1366e-06,  6.3207e-29]])\n",
      "On state=14, selected action=1\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.994888722897) A[1]:(0.00108718313277) A[2]:(0.00135458167642) A[3]:(0.0026695090346)\n",
      " state (1)  A[0]:(0.0326291956007) A[1]:(0.00378376338631) A[2]:(0.0124274129048) A[3]:(0.951159596443)\n",
      " state (2)  A[0]:(0.999985396862) A[1]:(2.64134723693e-06) A[2]:(1.19716978588e-05) A[3]:(1.03394770523e-09)\n",
      " state (3)  A[0]:(0.999998748302) A[1]:(3.07943821554e-07) A[2]:(9.23960612909e-07) A[3]:(1.73067875135e-12)\n",
      " state (4)  A[0]:(0.999993026257) A[1]:(5.53497329747e-06) A[2]:(1.41816633459e-06) A[3]:(2.71703571642e-14)\n",
      " state (5)  A[0]:(0.921094655991) A[1]:(0.054260302335) A[2]:(0.0246450453997) A[3]:(2.08537033748e-22)\n",
      " state (6)  A[0]:(0.0713046938181) A[1]:(0.824720144272) A[2]:(0.103975132108) A[3]:(3.29111167284e-25)\n",
      " state (7)  A[0]:(0.0132784964517) A[1]:(0.971017122269) A[2]:(0.0157043728977) A[3]:(4.9386590749e-26)\n",
      " state (8)  A[0]:(0.000923542713281) A[1]:(0.998196959496) A[2]:(0.000879504194017) A[3]:(3.87929322009e-27)\n",
      " state (9)  A[0]:(2.98078921332e-05) A[1]:(0.999932229519) A[2]:(3.79571247322e-05) A[3]:(2.37699958925e-28)\n",
      " state (10)  A[0]:(8.82725635165e-06) A[1]:(0.999979436398) A[2]:(1.17651807159e-05) A[3]:(8.62050155153e-29)\n",
      " state (11)  A[0]:(6.7797795964e-06) A[1]:(0.999984323978) A[2]:(8.89721195563e-06) A[3]:(6.81140393425e-29)\n",
      " state (12)  A[0]:(6.37076982457e-06) A[1]:(0.999985337257) A[2]:(8.31358374853e-06) A[3]:(6.43554546483e-29)\n",
      " state (13)  A[0]:(6.27214467386e-06) A[1]:(0.999985575676) A[2]:(8.17265208752e-06) A[3]:(6.34433462637e-29)\n",
      " state (14)  A[0]:(6.24702533969e-06) A[1]:(0.999985635281) A[2]:(8.13690985524e-06) A[3]:(6.32109527233e-29)\n",
      " state (15)  A[0]:(6.24047515885e-06) A[1]:(0.999985635281) A[2]:(8.12769667391e-06) A[3]:(6.31511826911e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 490000 finished after 9 . Running score: 0.13. Policy_loss: -92050.6150271, Value_loss: 1.84214686339. Times trained:               12402. Times reached goal: 126.               Steps done: 5836390.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.992791235447) A[1]:(0.0011531310156) A[2]:(0.0015590187395) A[3]:(0.00449658557773)\n",
      " state (1)  A[0]:(0.0424200035632) A[1]:(0.00416215695441) A[2]:(0.0153221273795) A[3]:(0.93809568882)\n",
      " state (2)  A[0]:(0.999995112419) A[1]:(7.91982756709e-07) A[2]:(4.08189953305e-06) A[3]:(4.34624281009e-11)\n",
      " state (3)  A[0]:(0.999998033047) A[1]:(5.68267012113e-07) A[2]:(1.38554617024e-06) A[3]:(1.22531390416e-12)\n",
      " state (4)  A[0]:(0.999873399734) A[1]:(0.000112814217573) A[2]:(1.37711758725e-05) A[3]:(5.18341759133e-16)\n",
      " state (5)  A[0]:(0.114639870822) A[1]:(0.478335350752) A[2]:(0.407024770975) A[3]:(1.47096072819e-24)\n",
      " state (6)  A[0]:(0.00865222048014) A[1]:(0.930169820786) A[2]:(0.0611779429018) A[3]:(4.6720459675e-26)\n",
      " state (7)  A[0]:(0.000733035500161) A[1]:(0.995101213455) A[2]:(0.00416578073055) A[3]:(3.93998890229e-27)\n",
      " state (8)  A[0]:(1.6102061636e-05) A[1]:(0.999859511852) A[2]:(0.000124406753457) A[3]:(1.63401000196e-28)\n",
      " state (9)  A[0]:(2.84993120658e-06) A[1]:(0.999972820282) A[2]:(2.43357917498e-05) A[3]:(3.86092349565e-29)\n",
      " state (10)  A[0]:(2.03905619856e-06) A[1]:(0.999980926514) A[2]:(1.70417042682e-05) A[3]:(2.84777992339e-29)\n",
      " state (11)  A[0]:(1.90412424672e-06) A[1]:(0.999982297421) A[2]:(1.57994873007e-05) A[3]:(2.67118689289e-29)\n",
      " state (12)  A[0]:(1.87547675523e-06) A[1]:(0.999982595444) A[2]:(1.553473885e-05) A[3]:(2.63334868894e-29)\n",
      " state (13)  A[0]:(1.86902047972e-06) A[1]:(0.999982655048) A[2]:(1.54750614456e-05) A[3]:(2.62480387944e-29)\n",
      " state (14)  A[0]:(1.86755232789e-06) A[1]:(0.999982655048) A[2]:(1.5461431758e-05) A[3]:(2.62286230132e-29)\n",
      " state (15)  A[0]:(1.86721752016e-06) A[1]:(0.999982655048) A[2]:(1.54583649419e-05) A[3]:(2.62242204577e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 491000 finished after 10 . Running score: 0.18. Policy_loss: -92050.6150507, Value_loss: 1.19459632927. Times trained:               13048. Times reached goal: 135.               Steps done: 5849438.\n",
      " state (0)  A[0]:(0.995570719242) A[1]:(0.00100773083977) A[2]:(0.00124840857461) A[3]:(0.00217315065674)\n",
      " state (1)  A[0]:(0.0521410852671) A[1]:(0.00437660422176) A[2]:(0.0163997337222) A[3]:(0.927082598209)\n",
      " state (2)  A[0]:(0.999997913837) A[1]:(3.64136440112e-07) A[2]:(1.74929573404e-06) A[3]:(8.14605327637e-12)\n",
      " state (3)  A[0]:(0.999998509884) A[1]:(4.86251167331e-07) A[2]:(1.00479155662e-06) A[3]:(9.69133519095e-13)\n",
      " state (4)  A[0]:(0.99989938736) A[1]:(9.4011600595e-05) A[2]:(6.57282316752e-06) A[3]:(7.28576024855e-16)\n",
      " state (5)  A[0]:(0.111415073276) A[1]:(0.701922893524) A[2]:(0.186662018299) A[3]:(1.21151837874e-24)\n",
      " state (6)  A[0]:(0.00660417927429) A[1]:(0.972530663013) A[2]:(0.0208651833236) A[3]:(2.12793719254e-26)\n",
      " state (7)  A[0]:(0.000826085684821) A[1]:(0.997040450573) A[2]:(0.00213344604708) A[3]:(2.47633106317e-27)\n",
      " state (8)  A[0]:(2.71487915597e-05) A[1]:(0.999882936478) A[2]:(8.99054284673e-05) A[3]:(1.38186123511e-28)\n",
      " state (9)  A[0]:(3.21867810271e-06) A[1]:(0.999982714653) A[2]:(1.40780084621e-05) A[3]:(2.58399222021e-29)\n",
      " state (10)  A[0]:(2.04433763429e-06) A[1]:(0.999988973141) A[2]:(9.01137354958e-06) A[3]:(1.75156497409e-29)\n",
      " state (11)  A[0]:(1.86313991435e-06) A[1]:(0.999989926815) A[2]:(8.18173430162e-06) A[3]:(1.61215442858e-29)\n",
      " state (12)  A[0]:(1.82456869879e-06) A[1]:(0.999990165234) A[2]:(8.00303678261e-06) A[3]:(1.5819745045e-29)\n",
      " state (13)  A[0]:(1.81569532742e-06) A[1]:(0.999990224838) A[2]:(7.96183758212e-06) A[3]:(1.57500173532e-29)\n",
      " state (14)  A[0]:(1.81361167506e-06) A[1]:(0.999990224838) A[2]:(7.95215601102e-06) A[3]:(1.57336830599e-29)\n",
      " state (15)  A[0]:(1.81312054792e-06) A[1]:(0.999990224838) A[2]:(7.94988045527e-06) A[3]:(1.57298417324e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 492000 finished after 16 . Running score: 0.1. Policy_loss: -92050.6152885, Value_loss: 1.20152529394. Times trained:               12706. Times reached goal: 128.               Steps done: 5862144.\n",
      " state (0)  A[0]:(0.996174097061) A[1]:(0.000983999343589) A[2]:(0.00122337602079) A[3]:(0.00161850592121)\n",
      " state (1)  A[0]:(0.0561594367027) A[1]:(0.00450597610325) A[2]:(0.0172320641577) A[3]:(0.922102510929)\n",
      " state (2)  A[0]:(0.999939858913) A[1]:(1.0724385902e-05) A[2]:(4.94163650728e-05) A[3]:(2.38359785243e-08)\n",
      " state (3)  A[0]:(0.999998867512) A[1]:(2.42681693408e-07) A[2]:(8.77189393123e-07) A[3]:(1.51463271179e-12)\n",
      " state (4)  A[0]:(0.999995827675) A[1]:(2.98368490803e-06) A[2]:(1.18349930744e-06) A[3]:(6.58628154186e-14)\n",
      " state (5)  A[0]:(0.676141321659) A[1]:(0.270788043737) A[2]:(0.0530706122518) A[3]:(1.56467531325e-22)\n",
      " state (6)  A[0]:(0.0182628259063) A[1]:(0.949265480042) A[2]:(0.0324716717005) A[3]:(3.94675030306e-26)\n",
      " state (7)  A[0]:(0.00377200893126) A[1]:(0.990201771259) A[2]:(0.00602620467544) A[3]:(5.38931589668e-27)\n",
      " state (8)  A[0]:(0.000409986794693) A[1]:(0.999057710171) A[2]:(0.000532307836693) A[3]:(5.54461027256e-28)\n",
      " state (9)  A[0]:(1.59886767506e-05) A[1]:(0.999953150749) A[2]:(3.08606286126e-05) A[3]:(3.99951311352e-29)\n",
      " state (10)  A[0]:(3.54979852091e-06) A[1]:(0.999987185001) A[2]:(9.29116868065e-06) A[3]:(1.31667223798e-29)\n",
      " state (11)  A[0]:(2.5336660201e-06) A[1]:(0.999990522861) A[2]:(6.92552566761e-06) A[3]:(1.01042477608e-29)\n",
      " state (12)  A[0]:(2.34134768107e-06) A[1]:(0.999991238117) A[2]:(6.44590409138e-06) A[3]:(9.47839199893e-30)\n",
      " state (13)  A[0]:(2.2950289349e-06) A[1]:(0.999991357327) A[2]:(6.32904766462e-06) A[3]:(9.32539492044e-30)\n",
      " state (14)  A[0]:(2.28304020311e-06) A[1]:(0.999991416931) A[2]:(6.2990366132e-06) A[3]:(9.28606306754e-30)\n",
      " state (15)  A[0]:(2.27983741752e-06) A[1]:(0.999991416931) A[2]:(6.29118403594e-06) A[3]:(9.27565401804e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 493000 finished after 7 . Running score: 0.12. Policy_loss: -92050.6150868, Value_loss: 1.1927445291. Times trained:               12599. Times reached goal: 139.               Steps done: 5874743.\n",
      " state (0)  A[0]:(0.996311604977) A[1]:(0.0011672108667) A[2]:(0.00117438356392) A[3]:(0.00134680210613)\n",
      " state (1)  A[0]:(0.054539360106) A[1]:(0.00436608912423) A[2]:(0.0164248906076) A[3]:(0.92466968298)\n",
      " state (2)  A[0]:(0.999991059303) A[1]:(1.54630117777e-06) A[2]:(7.37490154279e-06) A[3]:(2.85116263932e-10)\n",
      " state (3)  A[0]:(0.999998927116) A[1]:(2.26752845833e-07) A[2]:(8.57363886553e-07) A[3]:(1.55721388475e-12)\n",
      " state (4)  A[0]:(0.999997735023) A[1]:(1.26741440454e-06) A[2]:(9.70671180767e-07) A[3]:(1.92534342961e-13)\n",
      " state (5)  A[0]:(0.977601408958) A[1]:(0.0188913531601) A[2]:(0.00350722926669) A[3]:(1.77995373772e-20)\n",
      " state (6)  A[0]:(0.031511105597) A[1]:(0.915855228901) A[2]:(0.0526336543262) A[3]:(8.6641221844e-26)\n",
      " state (7)  A[0]:(0.00413807667792) A[1]:(0.989615261555) A[2]:(0.00624668318778) A[3]:(6.37715232818e-27)\n",
      " state (8)  A[0]:(0.000367520027794) A[1]:(0.999207437038) A[2]:(0.000425021891715) A[3]:(5.26443657686e-28)\n",
      " state (9)  A[0]:(1.48343751789e-05) A[1]:(0.999959230423) A[2]:(2.595382648e-05) A[3]:(4.02301066255e-29)\n",
      " state (10)  A[0]:(3.72184695152e-06) A[1]:(0.99998742342) A[2]:(8.87760961632e-06) A[3]:(1.48549985876e-29)\n",
      " state (11)  A[0]:(2.74709123005e-06) A[1]:(0.999990403652) A[2]:(6.87868487148e-06) A[3]:(1.17912706095e-29)\n",
      " state (12)  A[0]:(2.55950772043e-06) A[1]:(0.999990999699) A[2]:(6.46683747618e-06) A[3]:(1.11566902345e-29)\n",
      " state (13)  A[0]:(2.51443975685e-06) A[1]:(0.999991118908) A[2]:(6.36677350485e-06) A[3]:(1.10020899698e-29)\n",
      " state (14)  A[0]:(2.50287962444e-06) A[1]:(0.999991178513) A[2]:(6.341299013e-06) A[3]:(1.09626264615e-29)\n",
      " state (15)  A[0]:(2.49981667366e-06) A[1]:(0.999991178513) A[2]:(6.3346983552e-06) A[3]:(1.09523445535e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 494000 finished after 7 . Running score: 0.14. Policy_loss: -92050.6150723, Value_loss: 1.20202648148. Times trained:               12709. Times reached goal: 134.               Steps done: 5887452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9965,  0.0011,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0011,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0011,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0011,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0011,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0011,  0.0013,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9998e-01,  1.4058e-05,  2.0152e-06,  4.4384e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9998e-01,  1.4047e-05,  2.0146e-06,  4.4427e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9998e-01,  1.4037e-05,  2.0141e-06,  4.4465e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.9068e-04,  9.9896e-01,  5.4556e-04,  3.8695e-28]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996501386166) A[1]:(0.00109377119225) A[2]:(0.00134998967405) A[3]:(0.00105485238601)\n",
      " state (1)  A[0]:(0.0628459528089) A[1]:(0.00449906475842) A[2]:(0.0175844058394) A[3]:(0.915070593357)\n",
      " state (2)  A[0]:(0.999998748302) A[1]:(2.06493069754e-07) A[2]:(1.05558967789e-06) A[3]:(3.00345187089e-12)\n",
      " state (3)  A[0]:(0.999998927116) A[1]:(2.73559976449e-07) A[2]:(7.74772047407e-07) A[3]:(9.79867771124e-13)\n",
      " state (4)  A[0]:(0.999983966351) A[1]:(1.40026413646e-05) A[2]:(2.01218199436e-06) A[3]:(4.45988977696e-15)\n",
      " state (5)  A[0]:(0.346647769213) A[1]:(0.498660832644) A[2]:(0.154691383243) A[3]:(3.99022530269e-24)\n",
      " state (6)  A[0]:(0.0276173315942) A[1]:(0.929770946503) A[2]:(0.0426116958261) A[3]:(2.62962869266e-26)\n",
      " state (7)  A[0]:(0.0064581069164) A[1]:(0.98542112112) A[2]:(0.00812079384923) A[3]:(4.66488813331e-27)\n",
      " state (8)  A[0]:(0.000491868180688) A[1]:(0.998961269855) A[2]:(0.000546873430721) A[3]:(3.87752555344e-28)\n",
      " state (9)  A[0]:(2.08768306038e-05) A[1]:(0.999942958355) A[2]:(3.61683814845e-05) A[3]:(3.17140747364e-29)\n",
      " state (10)  A[0]:(7.21941887605e-06) A[1]:(0.999978542328) A[2]:(1.42226117532e-05) A[3]:(1.36608467905e-29)\n",
      " state (11)  A[0]:(5.76127331442e-06) A[1]:(0.999982774258) A[2]:(1.14464319267e-05) A[3]:(1.12850090786e-29)\n",
      " state (12)  A[0]:(5.46708452021e-06) A[1]:(0.999983668327) A[2]:(1.08667491077e-05) A[3]:(1.07842725593e-29)\n",
      " state (13)  A[0]:(5.39712891623e-06) A[1]:(0.999983847141) A[2]:(1.0727987501e-05) A[3]:(1.0664083246e-29)\n",
      " state (14)  A[0]:(5.37973983228e-06) A[1]:(0.999983906746) A[2]:(1.06935040094e-05) A[3]:(1.06342674431e-29)\n",
      " state (15)  A[0]:(5.3753501561e-06) A[1]:(0.999983966351) A[2]:(1.0684901099e-05) A[3]:(1.0626724719e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 495000 finished after 10 . Running score: 0.13. Policy_loss: -92050.615001, Value_loss: 1.63508962078. Times trained:               12610. Times reached goal: 131.               Steps done: 5900062.\n",
      " state (0)  A[0]:(0.993444621563) A[1]:(0.00117658276577) A[2]:(0.0017135128146) A[3]:(0.00366525771096)\n",
      " state (1)  A[0]:(0.0406747423112) A[1]:(0.00393781857565) A[2]:(0.0132666220888) A[3]:(0.942120790482)\n",
      " state (2)  A[0]:(0.999902665615) A[1]:(1.80019451363e-05) A[2]:(7.92504943092e-05) A[3]:(1.0400141548e-07)\n",
      " state (3)  A[0]:(0.99999910593) A[1]:(1.85832575994e-07) A[2]:(7.05602303697e-07) A[3]:(1.45397973175e-12)\n",
      " state (4)  A[0]:(0.999998152256) A[1]:(1.10102735107e-06) A[2]:(7.5390926213e-07) A[3]:(1.89606576967e-13)\n",
      " state (5)  A[0]:(0.963817059994) A[1]:(0.0331725478172) A[2]:(0.00301036890596) A[3]:(1.07440106773e-20)\n",
      " state (6)  A[0]:(0.0505683347583) A[1]:(0.913403630257) A[2]:(0.0360280647874) A[3]:(5.12739928501e-26)\n",
      " state (7)  A[0]:(0.00805830676109) A[1]:(0.987216174603) A[2]:(0.00472550280392) A[3]:(4.56058400457e-27)\n",
      " state (8)  A[0]:(0.000771550869104) A[1]:(0.998877763748) A[2]:(0.000350709451595) A[3]:(4.15878908474e-28)\n",
      " state (9)  A[0]:(3.35218319378e-05) A[1]:(0.999943375587) A[2]:(2.31281337619e-05) A[3]:(3.42652126982e-29)\n",
      " state (10)  A[0]:(9.08715719561e-06) A[1]:(0.999982714653) A[2]:(8.17194359115e-06) A[3]:(1.31491949127e-29)\n",
      " state (11)  A[0]:(6.84627730152e-06) A[1]:(0.999986767769) A[2]:(6.38340407022e-06) A[3]:(1.05410005239e-29)\n",
      " state (12)  A[0]:(6.41054475636e-06) A[1]:(0.999987602234) A[2]:(6.01287592872e-06) A[3]:(9.9976302351e-30)\n",
      " state (13)  A[0]:(6.30603426544e-06) A[1]:(0.999987781048) A[2]:(5.92268270339e-06) A[3]:(9.86518719255e-30)\n",
      " state (14)  A[0]:(6.27948475085e-06) A[1]:(0.999987840652) A[2]:(5.89970522924e-06) A[3]:(9.8313765896e-30)\n",
      " state (15)  A[0]:(6.27258987151e-06) A[1]:(0.999987840652) A[2]:(5.8937216636e-06) A[3]:(9.82260458056e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 496000 finished after 18 . Running score: 0.13. Policy_loss: -92050.6154364, Value_loss: 1.20253916361. Times trained:               12586. Times reached goal: 121.               Steps done: 5912648.\n",
      " state (0)  A[0]:(0.993608415127) A[1]:(0.00115197058767) A[2]:(0.0016606587451) A[3]:(0.00357893481851)\n",
      " state (1)  A[0]:(0.0499978736043) A[1]:(0.00414396263659) A[2]:(0.0144481547177) A[3]:(0.931410014629)\n",
      " state (2)  A[0]:(0.999991297722) A[1]:(1.61944888077e-06) A[2]:(7.10290760253e-06) A[3]:(4.04434402634e-10)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.86218812814e-07) A[2]:(6.13194686139e-07) A[3]:(1.19620058216e-12)\n",
      " state (4)  A[0]:(0.999998152256) A[1]:(1.2429270555e-06) A[2]:(6.00570388087e-07) A[3]:(1.16991160683e-13)\n",
      " state (5)  A[0]:(0.982631266117) A[1]:(0.0169006884098) A[2]:(0.000468023237772) A[3]:(6.06274662893e-20)\n",
      " state (6)  A[0]:(0.0739976838231) A[1]:(0.902092039585) A[2]:(0.0239102505147) A[3]:(5.22050336075e-26)\n",
      " state (7)  A[0]:(0.0137597564608) A[1]:(0.98169106245) A[2]:(0.0045491848141) A[3]:(4.30857376023e-27)\n",
      " state (8)  A[0]:(0.00241464236751) A[1]:(0.997012853622) A[2]:(0.000572526711039) A[3]:(6.11239175353e-28)\n",
      " state (9)  A[0]:(0.000155181303853) A[1]:(0.999805033207) A[2]:(3.97706280637e-05) A[3]:(5.52313856109e-29)\n",
      " state (10)  A[0]:(2.06950971915e-05) A[1]:(0.999969780445) A[2]:(9.53393100644e-06) A[3]:(1.41704869742e-29)\n",
      " state (11)  A[0]:(1.2076458006e-05) A[1]:(0.999981403351) A[2]:(6.50807896818e-06) A[3]:(9.8760438703e-30)\n",
      " state (12)  A[0]:(1.06443758341e-05) A[1]:(0.999983429909) A[2]:(5.91929119764e-06) A[3]:(9.04365318675e-30)\n",
      " state (13)  A[0]:(1.03007641883e-05) A[1]:(0.999983906746) A[2]:(5.77342416364e-06) A[3]:(8.83719500135e-30)\n",
      " state (14)  A[0]:(1.02079020508e-05) A[1]:(0.99998408556) A[2]:(5.7343768276e-06) A[3]:(8.78161311454e-30)\n",
      " state (15)  A[0]:(1.01814575828e-05) A[1]:(0.99998408556) A[2]:(5.72353746975e-06) A[3]:(8.76608379973e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 497000 finished after 8 . Running score: 0.07. Policy_loss: -92050.6150736, Value_loss: 1.63118082329. Times trained:               12428. Times reached goal: 119.               Steps done: 5925076.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.991000831127) A[1]:(0.00120198761579) A[2]:(0.00193250691518) A[3]:(0.00586465233937)\n",
      " state (1)  A[0]:(0.0360525622964) A[1]:(0.00367401703261) A[2]:(0.0119377495721) A[3]:(0.948335647583)\n",
      " state (2)  A[0]:(0.98923099041) A[1]:(0.00146979826968) A[2]:(0.00633338466287) A[3]:(0.00296585215256)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.31011177018e-07) A[2]:(6.53957329178e-07) A[3]:(1.41024323295e-12)\n",
      " state (4)  A[0]:(0.999999284744) A[1]:(1.66025770909e-07) A[2]:(5.66334676932e-07) A[3]:(9.5121458453e-13)\n",
      " state (5)  A[0]:(0.999998629093) A[1]:(8.08143852282e-07) A[2]:(5.76426316456e-07) A[3]:(1.18119476331e-13)\n",
      " state (6)  A[0]:(0.998682975769) A[1]:(0.00119450862985) A[2]:(0.000122539859149) A[3]:(2.7010300773e-19)\n",
      " state (7)  A[0]:(0.249334663153) A[1]:(0.701694667339) A[2]:(0.0489706508815) A[3]:(1.53209643761e-25)\n",
      " state (8)  A[0]:(0.00745102902874) A[1]:(0.991177082062) A[2]:(0.00137188646477) A[3]:(1.21836620139e-27)\n",
      " state (9)  A[0]:(0.000187167068361) A[1]:(0.99976593256) A[2]:(4.6903060138e-05) A[3]:(4.91327599231e-29)\n",
      " state (10)  A[0]:(3.60669037036e-05) A[1]:(0.999948561192) A[2]:(1.53438631969e-05) A[3]:(1.64501259899e-29)\n",
      " state (11)  A[0]:(2.54713486356e-05) A[1]:(0.999962627888) A[2]:(1.19008045658e-05) A[3]:(1.29156774116e-29)\n",
      " state (12)  A[0]:(2.33317514358e-05) A[1]:(0.999965488911) A[2]:(1.11497774924e-05) A[3]:(1.21418034011e-29)\n",
      " state (13)  A[0]:(2.26994070545e-05) A[1]:(0.99996638298) A[2]:(1.09372176667e-05) A[3]:(1.19179667072e-29)\n",
      " state (14)  A[0]:(2.24692157644e-05) A[1]:(0.999966681004) A[2]:(1.08662325147e-05) A[3]:(1.1840752715e-29)\n",
      " state (15)  A[0]:(2.23724018724e-05) A[1]:(0.999966800213) A[2]:(1.08388276203e-05) A[3]:(1.18100792714e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 498000 finished after 11 . Running score: 0.04. Policy_loss: -92050.6154599, Value_loss: 0.988349194576. Times trained:               12701. Times reached goal: 119.               Steps done: 5937777.\n",
      " state (0)  A[0]:(0.995351552963) A[1]:(0.00116294319741) A[2]:(0.00152779719792) A[3]:(0.00195770664141)\n",
      " state (1)  A[0]:(0.0536542348564) A[1]:(0.00433632172644) A[2]:(0.0151622071862) A[3]:(0.926847219467)\n",
      " state (2)  A[0]:(0.999996900558) A[1]:(5.00969917994e-07) A[2]:(2.62226490122e-06) A[3]:(2.44910862424e-11)\n",
      " state (3)  A[0]:(0.99999910593) A[1]:(1.76994745971e-07) A[2]:(7.19339311672e-07) A[3]:(1.03064659671e-12)\n",
      " state (4)  A[0]:(0.999998748302) A[1]:(5.8146616766e-07) A[2]:(6.90565457262e-07) A[3]:(2.19542740649e-13)\n",
      " state (5)  A[0]:(0.999917209148) A[1]:(7.88473480497e-05) A[2]:(3.96323594032e-06) A[3]:(1.24381375234e-16)\n",
      " state (6)  A[0]:(0.370353460312) A[1]:(0.551286399364) A[2]:(0.0783601254225) A[3]:(1.06245315133e-24)\n",
      " state (7)  A[0]:(0.0079645588994) A[1]:(0.985460519791) A[2]:(0.00657490082085) A[3]:(1.37979851599e-27)\n",
      " state (8)  A[0]:(0.000161174044479) A[1]:(0.999682307243) A[2]:(0.000156501540914) A[3]:(3.58898549769e-29)\n",
      " state (9)  A[0]:(1.60410290846e-05) A[1]:(0.999952971935) A[2]:(3.10077739414e-05) A[3]:(7.38899047503e-30)\n",
      " state (10)  A[0]:(1.05413237179e-05) A[1]:(0.999967157841) A[2]:(2.2276179152e-05) A[3]:(5.42150783978e-30)\n",
      " state (11)  A[0]:(9.70059409156e-06) A[1]:(0.999969542027) A[2]:(2.07712564588e-05) A[3]:(5.08384642923e-30)\n",
      " state (12)  A[0]:(9.50959838519e-06) A[1]:(0.999970078468) A[2]:(2.04334100999e-05) A[3]:(5.00736143168e-30)\n",
      " state (13)  A[0]:(9.45938063523e-06) A[1]:(0.999970197678) A[2]:(2.03498693736e-05) A[3]:(4.98806865415e-30)\n",
      " state (14)  A[0]:(9.44474231801e-06) A[1]:(0.999970257282) A[2]:(2.03275249078e-05) A[3]:(4.98281974274e-30)\n",
      " state (15)  A[0]:(9.44013117987e-06) A[1]:(0.999970257282) A[2]:(2.03212457563e-05) A[3]:(4.98122332737e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 499000 finished after 8 . Running score: 0.1. Policy_loss: -92050.615124, Value_loss: 0.998256185758. Times trained:               12601. Times reached goal: 122.               Steps done: 5950378.\n",
      "action_dist \n",
      "tensor([[ 0.9933,  0.0013,  0.0019,  0.0035]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3852e-07,  9.9112e-07,  1.2578e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3798e-07,  9.9158e-07,  1.2617e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9933,  0.0013,  0.0019,  0.0035]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3708e-07,  9.9238e-07,  1.2681e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3174e-01,  1.0441e-01,  7.6385e-01,  1.7640e-25]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993291556835) A[1]:(0.00134625216015) A[2]:(0.0018758897204) A[3]:(0.00348630547523)\n",
      " state (1)  A[0]:(0.0379927195609) A[1]:(0.00373178557493) A[2]:(0.0135054849088) A[3]:(0.944770038128)\n",
      " state (2)  A[0]:(0.999258637428) A[1]:(0.000109068969323) A[2]:(0.000624397827778) A[3]:(7.86690816312e-06)\n",
      " state (3)  A[0]:(0.999998867512) A[1]:(1.2807797134e-07) A[2]:(1.03287914044e-06) A[3]:(1.53722932821e-12)\n",
      " state (4)  A[0]:(0.999998867512) A[1]:(1.36417355634e-07) A[2]:(9.92740069705e-07) A[3]:(1.27259097357e-12)\n",
      " state (5)  A[0]:(0.99999833107) A[1]:(2.87637391239e-07) A[2]:(1.36175719945e-06) A[3]:(5.33719932399e-13)\n",
      " state (6)  A[0]:(0.999160289764) A[1]:(5.16951986356e-05) A[2]:(0.0007880438352) A[3]:(1.36017253774e-17)\n",
      " state (7)  A[0]:(0.328265637159) A[1]:(0.0243857335299) A[2]:(0.647348642349) A[3]:(5.75631920869e-24)\n",
      " state (8)  A[0]:(0.132310152054) A[1]:(0.102830417454) A[2]:(0.764859437943) A[3]:(1.80140652486e-25)\n",
      " state (9)  A[0]:(0.104856960475) A[1]:(0.210815697908) A[2]:(0.684327363968) A[3]:(6.71292712063e-26)\n",
      " state (10)  A[0]:(0.0872473716736) A[1]:(0.378833532333) A[2]:(0.533919036388) A[3]:(3.71690057903e-26)\n",
      " state (11)  A[0]:(0.0506592951715) A[1]:(0.698919296265) A[2]:(0.250421434641) A[3]:(1.54489626047e-26)\n",
      " state (12)  A[0]:(0.00931924488395) A[1]:(0.955766558647) A[2]:(0.0349142067134) A[3]:(2.34728297169e-27)\n",
      " state (13)  A[0]:(0.000766009965446) A[1]:(0.9959846735) A[2]:(0.00324932509102) A[3]:(2.54939993045e-28)\n",
      " state (14)  A[0]:(0.000141307784361) A[1]:(0.99905860424) A[2]:(0.000800065405201) A[3]:(6.83329694289e-29)\n",
      " state (15)  A[0]:(7.08071165718e-05) A[1]:(0.999479949474) A[2]:(0.000449228595244) A[3]:(3.99768167451e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 500000 finished after 6 . Running score: 0.09. Policy_loss: -92050.5770445, Value_loss: 1.20414625426. Times trained:               12742. Times reached goal: 132.               Steps done: 5963120.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994971036911) A[1]:(0.00134780455846) A[2]:(0.00165641040076) A[3]:(0.00202474393882)\n",
      " state (1)  A[0]:(0.0373264886439) A[1]:(0.00399601366371) A[2]:(0.0127353016287) A[3]:(0.945942163467)\n",
      " state (2)  A[0]:(0.999994814396) A[1]:(8.74847444265e-07) A[2]:(4.29986312156e-06) A[3]:(7.70453978394e-11)\n",
      " state (3)  A[0]:(0.999998927116) A[1]:(2.07830012755e-07) A[2]:(8.56504186686e-07) A[3]:(1.40682008144e-12)\n",
      " state (4)  A[0]:(0.999997735023) A[1]:(1.06195386707e-06) A[2]:(1.18342131827e-06) A[3]:(2.11782943071e-13)\n",
      " state (5)  A[0]:(0.98475009203) A[1]:(0.00881451461464) A[2]:(0.00643536960706) A[3]:(8.19567158185e-21)\n",
      " state (6)  A[0]:(0.0452243834734) A[1]:(0.867007374763) A[2]:(0.0877682194114) A[3]:(3.34560501321e-26)\n",
      " state (7)  A[0]:(0.00581212015823) A[1]:(0.984457135201) A[2]:(0.00973075907677) A[3]:(2.3044069563e-27)\n",
      " state (8)  A[0]:(0.000656348478515) A[1]:(0.998521625996) A[2]:(0.000822037283797) A[3]:(2.28462982245e-28)\n",
      " state (9)  A[0]:(2.82022956526e-05) A[1]:(0.999917447567) A[2]:(5.43205096619e-05) A[3]:(1.83217717933e-29)\n",
      " state (10)  A[0]:(5.81451695325e-06) A[1]:(0.999977171421) A[2]:(1.70038492797e-05) A[3]:(6.09237130551e-30)\n",
      " state (11)  A[0]:(4.12376311942e-06) A[1]:(0.999983012676) A[2]:(1.28397587105e-05) A[3]:(4.70917631877e-30)\n",
      " state (12)  A[0]:(3.81056088372e-06) A[1]:(0.999984204769) A[2]:(1.19899195852e-05) A[3]:(4.42697341514e-30)\n",
      " state (13)  A[0]:(3.7344507291e-06) A[1]:(0.999984502792) A[2]:(1.17792960737e-05) A[3]:(4.35697752257e-30)\n",
      " state (14)  A[0]:(3.71439000446e-06) A[1]:(0.999984562397) A[2]:(1.17238887469e-05) A[3]:(4.33846903487e-30)\n",
      " state (15)  A[0]:(3.70886800738e-06) A[1]:(0.999984562397) A[2]:(1.17088266052e-05) A[3]:(4.33344055216e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 501000 finished after 11 . Running score: 0.12. Policy_loss: -92050.5528169, Value_loss: 1.44165356134. Times trained:               13302. Times reached goal: 116.               Steps done: 5976422.\n",
      " state (0)  A[0]:(0.988811671734) A[1]:(0.00150905444752) A[2]:(0.00220371037722) A[3]:(0.00747558195144)\n",
      " state (1)  A[0]:(0.035316888243) A[1]:(0.00385716860183) A[2]:(0.0123868053779) A[3]:(0.948439121246)\n",
      " state (2)  A[0]:(0.999990582466) A[1]:(1.54520819251e-06) A[2]:(7.89860769146e-06) A[3]:(3.17480902678e-10)\n",
      " state (3)  A[0]:(0.999998927116) A[1]:(1.76094303583e-07) A[2]:(8.9125978775e-07) A[3]:(1.57003197335e-12)\n",
      " state (4)  A[0]:(0.999998629093) A[1]:(4.11981204707e-07) A[2]:(9.59581029747e-07) A[3]:(6.68863185324e-13)\n",
      " state (5)  A[0]:(0.998774170876) A[1]:(0.000590608746279) A[2]:(0.000635210948531) A[3]:(1.41069987284e-18)\n",
      " state (6)  A[0]:(0.127073720098) A[1]:(0.611115455627) A[2]:(0.261810779572) A[3]:(2.31780596678e-25)\n",
      " state (7)  A[0]:(0.0161981023848) A[1]:(0.943056285381) A[2]:(0.0407456420362) A[3]:(9.91783586398e-27)\n",
      " state (8)  A[0]:(0.00142800470348) A[1]:(0.995514750481) A[2]:(0.00305722793564) A[3]:(7.26972350164e-28)\n",
      " state (9)  A[0]:(3.58896650141e-05) A[1]:(0.999840259552) A[2]:(0.00012382955174) A[3]:(3.56123796295e-29)\n",
      " state (10)  A[0]:(6.80415450915e-06) A[1]:(0.999964475632) A[2]:(2.87443144771e-05) A[3]:(9.43556939801e-30)\n",
      " state (11)  A[0]:(4.75017168355e-06) A[1]:(0.999975264072) A[2]:(1.99779988179e-05) A[3]:(6.86944980767e-30)\n",
      " state (12)  A[0]:(4.36260461356e-06) A[1]:(0.99997740984) A[2]:(1.8249709683e-05) A[3]:(6.35404010958e-30)\n",
      " state (13)  A[0]:(4.26580845669e-06) A[1]:(0.999977946281) A[2]:(1.78156806214e-05) A[3]:(6.22373364557e-30)\n",
      " state (14)  A[0]:(4.23907613367e-06) A[1]:(0.999978065491) A[2]:(1.76964713319e-05) A[3]:(6.18775110521e-30)\n",
      " state (15)  A[0]:(4.23124038207e-06) A[1]:(0.999978125095) A[2]:(1.76618759724e-05) A[3]:(6.177233346e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 502000 finished after 9 . Running score: 0.14. Policy_loss: -92050.5527936, Value_loss: 1.2233954177. Times trained:               12759. Times reached goal: 130.               Steps done: 5989181.\n",
      " state (0)  A[0]:(0.994461297989) A[1]:(0.00135298306122) A[2]:(0.00170792185236) A[3]:(0.00247780350037)\n",
      " state (1)  A[0]:(0.0282743480057) A[1]:(0.00359541689977) A[2]:(0.0108169233426) A[3]:(0.957313299179)\n",
      " state (2)  A[0]:(0.999836564064) A[1]:(2.91480027954e-05) A[2]:(0.00013397395378) A[3]:(3.03564775095e-07)\n",
      " state (3)  A[0]:(0.999999046326) A[1]:(1.6099562572e-07) A[2]:(8.14324835119e-07) A[3]:(1.5175420599e-12)\n",
      " state (4)  A[0]:(0.999998748302) A[1]:(3.79052380595e-07) A[2]:(8.65118011006e-07) A[3]:(5.60653032246e-13)\n",
      " state (5)  A[0]:(0.998928368092) A[1]:(0.000440762087237) A[2]:(0.000630847353023) A[3]:(4.77534676349e-19)\n",
      " state (6)  A[0]:(0.179271996021) A[1]:(0.565140664577) A[2]:(0.255587339401) A[3]:(1.62855095353e-25)\n",
      " state (7)  A[0]:(0.0203944463283) A[1]:(0.951507389545) A[2]:(0.0280981790274) A[3]:(6.85283699477e-27)\n",
      " state (8)  A[0]:(0.000818066066131) A[1]:(0.998260855675) A[2]:(0.000921057944652) A[3]:(2.62265580552e-28)\n",
      " state (9)  A[0]:(2.82080800389e-05) A[1]:(0.999916553497) A[2]:(5.52302153665e-05) A[3]:(1.89446416114e-29)\n",
      " state (10)  A[0]:(1.14438171295e-05) A[1]:(0.999964058399) A[2]:(2.44772818405e-05) A[3]:(9.11357271921e-30)\n",
      " state (11)  A[0]:(9.53284325078e-06) A[1]:(0.999970078468) A[2]:(2.03895742743e-05) A[3]:(7.76739583634e-30)\n",
      " state (12)  A[0]:(9.12697123567e-06) A[1]:(0.999971389771) A[2]:(1.95015891222e-05) A[3]:(7.47200783575e-30)\n",
      " state (13)  A[0]:(9.02342890186e-06) A[1]:(0.999971687794) A[2]:(1.92763072846e-05) A[3]:(7.39657382419e-30)\n",
      " state (14)  A[0]:(8.99452788872e-06) A[1]:(0.999971807003) A[2]:(1.92144925677e-05) A[3]:(7.37561203277e-30)\n",
      " state (15)  A[0]:(8.98578218766e-06) A[1]:(0.999971807003) A[2]:(1.91961044038e-05) A[3]:(7.36942498282e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 503000 finished after 13 . Running score: 0.14. Policy_loss: -92050.5678714, Value_loss: 1.00674483393. Times trained:               12745. Times reached goal: 141.               Steps done: 6001926.\n",
      " state (0)  A[0]:(0.99528503418) A[1]:(0.00130081840325) A[2]:(0.00159610935953) A[3]:(0.00181802082807)\n",
      " state (1)  A[0]:(0.0222783107311) A[1]:(0.00329690566286) A[2]:(0.00957330130041) A[3]:(0.964851498604)\n",
      " state (2)  A[0]:(0.653897345066) A[1]:(0.00976969860494) A[2]:(0.0393754988909) A[3]:(0.296957463026)\n",
      " state (3)  A[0]:(0.999998867512) A[1]:(1.59163803914e-07) A[2]:(9.70189944383e-07) A[3]:(2.2503656924e-12)\n",
      " state (4)  A[0]:(0.999998986721) A[1]:(1.71250860603e-07) A[2]:(8.45997817578e-07) A[3]:(1.31339329603e-12)\n",
      " state (5)  A[0]:(0.999995231628) A[1]:(1.41776922646e-06) A[2]:(3.33189291268e-06) A[3]:(4.1157740871e-14)\n",
      " state (6)  A[0]:(0.738503575325) A[1]:(0.0227010250092) A[2]:(0.238795399666) A[3]:(2.0237145784e-23)\n",
      " state (7)  A[0]:(0.123960256577) A[1]:(0.619757533073) A[2]:(0.256282240152) A[3]:(7.18798778592e-26)\n",
      " state (8)  A[0]:(0.00798713043332) A[1]:(0.980435609818) A[2]:(0.0115772690624) A[3]:(2.27384379624e-27)\n",
      " state (9)  A[0]:(0.000123165125842) A[1]:(0.999629020691) A[2]:(0.00024780392414) A[3]:(6.0401616769e-29)\n",
      " state (10)  A[0]:(2.16740645556e-05) A[1]:(0.999924659729) A[2]:(5.36954175914e-05) A[3]:(1.48908299124e-29)\n",
      " state (11)  A[0]:(1.49885836436e-05) A[1]:(0.999947845936) A[2]:(3.71485039068e-05) A[3]:(1.07593964657e-29)\n",
      " state (12)  A[0]:(1.36838161779e-05) A[1]:(0.999952554703) A[2]:(3.37824785674e-05) A[3]:(9.90254271031e-30)\n",
      " state (13)  A[0]:(1.33314561026e-05) A[1]:(0.9999538064) A[2]:(3.28700443788e-05) A[3]:(9.66814575133e-30)\n",
      " state (14)  A[0]:(1.32199975269e-05) A[1]:(0.999954223633) A[2]:(3.25825494656e-05) A[3]:(9.59378980915e-30)\n",
      " state (15)  A[0]:(1.31802189571e-05) A[1]:(0.999954342842) A[2]:(3.24799257214e-05) A[3]:(9.56718489253e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 504000 finished after 4 . Running score: 0.15. Policy_loss: -92050.5515791, Value_loss: 1.42368435081. Times trained:               13171. Times reached goal: 134.               Steps done: 6015097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.6248e-07,  8.7383e-07,  2.0272e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.6249e-07,  8.7383e-07,  2.0272e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.6249e-07,  8.7382e-07,  2.0272e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.6250e-07,  8.7382e-07,  2.0272e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.6250e-07,  8.7382e-07,  2.0273e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0012,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.6250e-07,  8.7382e-07,  2.0274e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.1910e-05,  9.9991e-01,  2.6100e-05,  8.2447e-30]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.1910e-05,  9.9991e-01,  2.6100e-05,  8.2449e-30]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996587872505) A[1]:(0.00119302619714) A[2]:(0.00130571669433) A[3]:(0.000913402473088)\n",
      " state (1)  A[0]:(0.0317179597914) A[1]:(0.00370219536126) A[2]:(0.0111368624493) A[3]:(0.95344299078)\n",
      " state (2)  A[0]:(0.999975323677) A[1]:(4.40965732196e-06) A[2]:(2.0283534468e-05) A[3]:(4.39055369839e-09)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(1.50283284484e-07) A[2]:(6.72346573083e-07) A[3]:(1.23484068023e-12)\n",
      " state (4)  A[0]:(0.999998569489) A[1]:(5.62503430501e-07) A[2]:(8.73820113156e-07) A[3]:(2.0274079182e-13)\n",
      " state (5)  A[0]:(0.976337313652) A[1]:(0.00696846377105) A[2]:(0.0166942365468) A[3]:(1.58911700088e-22)\n",
      " state (6)  A[0]:(0.080893009901) A[1]:(0.872519671917) A[2]:(0.0465872883797) A[3]:(1.15653826497e-26)\n",
      " state (7)  A[0]:(0.00260975328274) A[1]:(0.996673464775) A[2]:(0.00071679498069) A[3]:(1.79651539659e-28)\n",
      " state (8)  A[0]:(6.1911989178e-05) A[1]:(0.99991196394) A[2]:(2.61010318354e-05) A[3]:(8.2453183613e-30)\n",
      " state (9)  A[0]:(1.78858299478e-05) A[1]:(0.999971032143) A[2]:(1.10697392302e-05) A[3]:(3.59826043024e-30)\n",
      " state (10)  A[0]:(1.45475851241e-05) A[1]:(0.999975979328) A[2]:(9.4671968327e-06) A[3]:(3.10638563369e-30)\n",
      " state (11)  A[0]:(1.39539379234e-05) A[1]:(0.999976873398) A[2]:(9.16487624636e-06) A[3]:(3.01364157443e-30)\n",
      " state (12)  A[0]:(1.38209707075e-05) A[1]:(0.999977052212) A[2]:(9.09747632249e-06) A[3]:(2.9928373933e-30)\n",
      " state (13)  A[0]:(1.37875858854e-05) A[1]:(0.999977111816) A[2]:(9.08097263164e-06) A[3]:(2.98770415052e-30)\n",
      " state (14)  A[0]:(1.37781753438e-05) A[1]:(0.999977171421) A[2]:(9.07650610316e-06) A[3]:(2.98626891894e-30)\n",
      " state (15)  A[0]:(1.37751794682e-05) A[1]:(0.999977171421) A[2]:(9.07515550352e-06) A[3]:(2.98585871843e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 505000 finished after 28 . Running score: 0.11. Policy_loss: -92050.5513164, Value_loss: 1.63302710747. Times trained:               12561. Times reached goal: 138.               Steps done: 6027658.\n",
      " state (0)  A[0]:(0.975980997086) A[1]:(0.00196213158779) A[2]:(0.00263994769193) A[3]:(0.0194169282913)\n",
      " state (1)  A[0]:(0.0133766140789) A[1]:(0.00271706678905) A[2]:(0.00655156327412) A[3]:(0.977354764938)\n",
      " state (2)  A[0]:(0.999945998192) A[1]:(1.28261272039e-05) A[2]:(4.11750224885e-05) A[3]:(2.6655303742e-08)\n",
      " state (3)  A[0]:(0.99999910593) A[1]:(1.89575331433e-07) A[2]:(6.98618578099e-07) A[3]:(1.58948190981e-12)\n",
      " state (4)  A[0]:(0.999997675419) A[1]:(1.11574797756e-06) A[2]:(1.19417245514e-06) A[3]:(1.14843884728e-13)\n",
      " state (5)  A[0]:(0.901028096676) A[1]:(0.0380088239908) A[2]:(0.0609630867839) A[3]:(2.48924159341e-23)\n",
      " state (6)  A[0]:(0.0588552765548) A[1]:(0.90706294775) A[2]:(0.0340817980468) A[3]:(2.92076736234e-26)\n",
      " state (7)  A[0]:(0.0068773124367) A[1]:(0.990734517574) A[2]:(0.00238815043122) A[3]:(2.11487850159e-27)\n",
      " state (8)  A[0]:(0.000323198502883) A[1]:(0.999602437019) A[2]:(7.43747295928e-05) A[3]:(1.0369360895e-28)\n",
      " state (9)  A[0]:(1.69010054378e-05) A[1]:(0.999976038933) A[2]:(7.05695401848e-06) A[3]:(1.21073352736e-29)\n",
      " state (10)  A[0]:(7.78578851168e-06) A[1]:(0.999988377094) A[2]:(3.86512556361e-06) A[3]:(7.00690026808e-30)\n",
      " state (11)  A[0]:(6.68148277327e-06) A[1]:(0.999989926815) A[2]:(3.39338112099e-06) A[3]:(6.24607029518e-30)\n",
      " state (12)  A[0]:(6.44907049718e-06) A[1]:(0.999990284443) A[2]:(3.29057252202e-06) A[3]:(6.07954318268e-30)\n",
      " state (13)  A[0]:(6.39183326712e-06) A[1]:(0.999990344048) A[2]:(3.26545114149e-06) A[3]:(6.03863259385e-30)\n",
      " state (14)  A[0]:(6.3766610765e-06) A[1]:(0.999990344048) A[2]:(3.25900509779e-06) A[3]:(6.02795346277e-30)\n",
      " state (15)  A[0]:(6.37245420876e-06) A[1]:(0.999990344048) A[2]:(3.2572775126e-06) A[3]:(6.02505666853e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 506000 finished after 20 . Running score: 0.08. Policy_loss: -92050.551404, Value_loss: 1.02049079333. Times trained:               13017. Times reached goal: 136.               Steps done: 6040675.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993908464909) A[1]:(0.00145446206443) A[2]:(0.00171164027415) A[3]:(0.00292544951662)\n",
      " state (1)  A[0]:(0.0148231592029) A[1]:(0.00281124515459) A[2]:(0.00703711016104) A[3]:(0.975328505039)\n",
      " state (2)  A[0]:(0.999988615513) A[1]:(2.33946684602e-06) A[2]:(9.04727858142e-06) A[3]:(7.03461233709e-10)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(1.83900326078e-07) A[2]:(6.78533581322e-07) A[3]:(1.29318908013e-12)\n",
      " state (4)  A[0]:(0.999996304512) A[1]:(2.09526342587e-06) A[2]:(1.61957427736e-06) A[3]:(2.65140760367e-14)\n",
      " state (5)  A[0]:(0.728492379189) A[1]:(0.123940050602) A[2]:(0.147567540407) A[3]:(2.32167799192e-24)\n",
      " state (6)  A[0]:(0.075280956924) A[1]:(0.876572370529) A[2]:(0.0481466501951) A[3]:(1.93341765186e-26)\n",
      " state (7)  A[0]:(0.0123601853848) A[1]:(0.982284486294) A[2]:(0.00535531202331) A[3]:(2.22911426587e-27)\n",
      " state (8)  A[0]:(0.000457434682176) A[1]:(0.999370872974) A[2]:(0.000171679086634) A[3]:(1.02843984955e-28)\n",
      " state (9)  A[0]:(2.21789632633e-05) A[1]:(0.999965131283) A[2]:(1.26873301269e-05) A[3]:(9.80762972292e-30)\n",
      " state (10)  A[0]:(1.09026968858e-05) A[1]:(0.999982774258) A[2]:(6.33442778053e-06) A[3]:(5.38494451117e-30)\n",
      " state (11)  A[0]:(9.44913517742e-06) A[1]:(0.999985098839) A[2]:(5.43459236724e-06) A[3]:(4.73270614217e-30)\n",
      " state (12)  A[0]:(9.13821531867e-06) A[1]:(0.999985635281) A[2]:(5.23993367096e-06) A[3]:(4.58996915454e-30)\n",
      " state (13)  A[0]:(9.06150035007e-06) A[1]:(0.99998575449) A[2]:(5.19231889484e-06) A[3]:(4.55487547599e-30)\n",
      " state (14)  A[0]:(9.04136959434e-06) A[1]:(0.99998575449) A[2]:(5.18007300343e-06) A[3]:(4.54574536435e-30)\n",
      " state (15)  A[0]:(9.03575073607e-06) A[1]:(0.999985814095) A[2]:(5.17677426615e-06) A[3]:(4.54328378514e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 507000 finished after 9 . Running score: 0.17. Policy_loss: -92050.5515441, Value_loss: 1.21851386158. Times trained:               12648. Times reached goal: 138.               Steps done: 6053323.\n",
      " state (0)  A[0]:(0.995962262154) A[1]:(0.00128406577278) A[2]:(0.00139373319689) A[3]:(0.00135994283482)\n",
      " state (1)  A[0]:(0.0170608703047) A[1]:(0.00283520994708) A[2]:(0.00774241844192) A[3]:(0.972361505032)\n",
      " state (2)  A[0]:(0.99999499321) A[1]:(8.4956803903e-07) A[2]:(4.17372029915e-06) A[3]:(8.54146545159e-11)\n",
      " state (3)  A[0]:(0.999998986721) A[1]:(2.1713722731e-07) A[2]:(8.22370282094e-07) A[3]:(1.34796991277e-12)\n",
      " state (4)  A[0]:(0.999992907047) A[1]:(4.76932927995e-06) A[2]:(2.32011552725e-06) A[3]:(1.36823222497e-14)\n",
      " state (5)  A[0]:(0.478802114725) A[1]:(0.226623505354) A[2]:(0.294574350119) A[3]:(1.55219673674e-24)\n",
      " state (6)  A[0]:(0.0324365533888) A[1]:(0.90618366003) A[2]:(0.0613797791302) A[3]:(1.50144050165e-26)\n",
      " state (7)  A[0]:(0.00313944113441) A[1]:(0.99261879921) A[2]:(0.00424176175147) A[3]:(1.19901522768e-27)\n",
      " state (8)  A[0]:(6.00131388637e-05) A[1]:(0.999824762344) A[2]:(0.000115230883239) A[3]:(4.53101109749e-29)\n",
      " state (9)  A[0]:(8.76001558936e-06) A[1]:(0.999969899654) A[2]:(2.13587136386e-05) A[3]:(1.00322194855e-29)\n",
      " state (10)  A[0]:(6.19581533101e-06) A[1]:(0.999978840351) A[2]:(1.49457691805e-05) A[3]:(7.40119906532e-30)\n",
      " state (11)  A[0]:(5.77829496251e-06) A[1]:(0.999980390072) A[2]:(1.38560162668e-05) A[3]:(6.94494776613e-30)\n",
      " state (12)  A[0]:(5.68777932131e-06) A[1]:(0.999980688095) A[2]:(1.36189491968e-05) A[3]:(6.84510710642e-30)\n",
      " state (13)  A[0]:(5.66638345845e-06) A[1]:(0.9999807477) A[2]:(1.35634227263e-05) A[3]:(6.82159496245e-30)\n",
      " state (14)  A[0]:(5.66113340028e-06) A[1]:(0.999980807304) A[2]:(1.35499258249e-05) A[3]:(6.81587284403e-30)\n",
      " state (15)  A[0]:(5.65979462408e-06) A[1]:(0.999980807304) A[2]:(1.3546567061e-05) A[3]:(6.81441711183e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 508000 finished after 20 . Running score: 0.11. Policy_loss: -92050.5515333, Value_loss: 1.00588596167. Times trained:               12513. Times reached goal: 114.               Steps done: 6065836.\n",
      " state (0)  A[0]:(0.996934711933) A[1]:(0.00110286998097) A[2]:(0.00113254436292) A[3]:(0.000829902652185)\n",
      " state (1)  A[0]:(0.0264934897423) A[1]:(0.0030525024049) A[2]:(0.0092880744487) A[3]:(0.961165904999)\n",
      " state (2)  A[0]:(0.99998819828) A[1]:(1.94266181097e-06) A[2]:(9.85444148682e-06) A[3]:(9.80328040967e-10)\n",
      " state (3)  A[0]:(0.999999284744) A[1]:(1.11696188299e-07) A[2]:(5.73996601361e-07) A[3]:(1.06017137343e-12)\n",
      " state (4)  A[0]:(0.99999910593) A[1]:(2.81872246433e-07) A[2]:(6.21619108188e-07) A[3]:(2.15872147089e-13)\n",
      " state (5)  A[0]:(0.997727334499) A[1]:(0.000224790695938) A[2]:(0.00204784865491) A[3]:(2.14151830127e-22)\n",
      " state (6)  A[0]:(0.571908473969) A[1]:(0.277649879456) A[2]:(0.150441676378) A[3]:(4.20363669387e-26)\n",
      " state (7)  A[0]:(0.118647634983) A[1]:(0.853027164936) A[2]:(0.0283251665533) A[3]:(5.20877498665e-27)\n",
      " state (8)  A[0]:(0.00536305690184) A[1]:(0.993752777576) A[2]:(0.000884160806891) A[3]:(2.27710401045e-28)\n",
      " state (9)  A[0]:(0.000156150592375) A[1]:(0.999798595905) A[2]:(4.52681852039e-05) A[3]:(1.47988065722e-29)\n",
      " state (10)  A[0]:(5.90334711887e-05) A[1]:(0.999921619892) A[2]:(1.93441392184e-05) A[3]:(6.90581151549e-30)\n",
      " state (11)  A[0]:(4.87008874188e-05) A[1]:(0.999935269356) A[2]:(1.60247836902e-05) A[3]:(5.8649475671e-30)\n",
      " state (12)  A[0]:(4.65403863927e-05) A[1]:(0.999938130379) A[2]:(1.53148739628e-05) A[3]:(5.63980748508e-30)\n",
      " state (13)  A[0]:(4.5991717343e-05) A[1]:(0.999938845634) A[2]:(1.51385402205e-05) A[3]:(5.58321373273e-30)\n",
      " state (14)  A[0]:(4.58377689938e-05) A[1]:(0.999939084053) A[2]:(1.50916093844e-05) A[3]:(5.56794396708e-30)\n",
      " state (15)  A[0]:(4.57900641777e-05) A[1]:(0.999939143658) A[2]:(1.50783744175e-05) A[3]:(5.56348574018e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 509000 finished after 3 . Running score: 0.09. Policy_loss: -92050.5542013, Value_loss: 1.00618588795. Times trained:               12923. Times reached goal: 102.               Steps done: 6078759.\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9969,  0.0011,  0.0011,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  4.3864e-06,  4.3359e-06,  1.2287e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.4965e-04,  9.9932e-01,  2.3358e-04,  6.1801e-29]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.0548e-05,  9.9996e-01,  1.9312e-05,  6.1913e-30]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0350e-05,  9.9998e-01,  1.0293e-05,  3.5539e-30]])\n",
      "On state=10, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.6228e-06,  9.9998e-01,  8.5805e-06,  3.0373e-30]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.6491e-06,  9.9998e-01,  8.6009e-06,  3.0443e-30]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996851682663) A[1]:(0.00114430999383) A[2]:(0.00114410859533) A[3]:(0.000859885767568)\n",
      " state (1)  A[0]:(0.0283929631114) A[1]:(0.00322179845534) A[2]:(0.00985961966217) A[3]:(0.958525598049)\n",
      " state (2)  A[0]:(0.999997258186) A[1]:(4.54625393331e-07) A[2]:(2.28438625527e-06) A[3]:(2.39242167743e-11)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(1.75411159375e-07) A[2]:(6.66177868425e-07) A[3]:(9.83753768551e-13)\n",
      " state (4)  A[0]:(0.999991297722) A[1]:(4.38667530034e-06) A[2]:(4.33344393969e-06) A[3]:(1.22984144095e-15)\n",
      " state (5)  A[0]:(0.604140102863) A[1]:(0.171535134315) A[2]:(0.224324792624) A[3]:(2.00729207208e-25)\n",
      " state (6)  A[0]:(0.0798610672355) A[1]:(0.860948801041) A[2]:(0.0591901168227) A[3]:(1.0235279193e-26)\n",
      " state (7)  A[0]:(0.0136088412255) A[1]:(0.978804171085) A[2]:(0.00758698303252) A[3]:(1.40442432293e-27)\n",
      " state (8)  A[0]:(0.000449289131211) A[1]:(0.999317288399) A[2]:(0.000233411134104) A[3]:(6.17770076017e-29)\n",
      " state (9)  A[0]:(2.05412143259e-05) A[1]:(0.999960124493) A[2]:(1.93055202544e-05) A[3]:(6.19099321267e-30)\n",
      " state (10)  A[0]:(1.034853085e-05) A[1]:(0.999979376793) A[2]:(1.02917629192e-05) A[3]:(3.55406334728e-30)\n",
      " state (11)  A[0]:(9.02551892068e-06) A[1]:(0.999981999397) A[2]:(8.96823803487e-06) A[3]:(3.15708103749e-30)\n",
      " state (12)  A[0]:(8.730036825e-06) A[1]:(0.999982595444) A[2]:(8.67509425007e-06) A[3]:(3.06770998875e-30)\n",
      " state (13)  A[0]:(8.64885987539e-06) A[1]:(0.999982774258) A[2]:(8.60066029418e-06) A[3]:(3.04434849614e-30)\n",
      " state (14)  A[0]:(8.62217711983e-06) A[1]:(0.999982774258) A[2]:(8.57988470671e-06) A[3]:(3.03743489665e-30)\n",
      " state (15)  A[0]:(8.61142871145e-06) A[1]:(0.999982833862) A[2]:(8.57344002725e-06) A[3]:(3.03507224704e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 510000 finished after 11 . Running score: 0.1. Policy_loss: -92050.5513055, Value_loss: 1.22487183565. Times trained:               12771. Times reached goal: 135.               Steps done: 6091530.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993386983871) A[1]:(0.00130254065152) A[2]:(0.00159996503498) A[3]:(0.00371049717069)\n",
      " state (1)  A[0]:(0.0272048339248) A[1]:(0.00313065317459) A[2]:(0.0101590296254) A[3]:(0.959505498409)\n",
      " state (2)  A[0]:(0.999996006489) A[1]:(5.8840288375e-07) A[2]:(3.39852522302e-06) A[3]:(3.78707655513e-11)\n",
      " state (3)  A[0]:(0.999998807907) A[1]:(1.80094957614e-07) A[2]:(1.00423608274e-06) A[3]:(1.62742638376e-12)\n",
      " state (4)  A[0]:(0.999995589256) A[1]:(8.82780739175e-07) A[2]:(3.55372026206e-06) A[3]:(9.26666173808e-14)\n",
      " state (5)  A[0]:(0.557493388653) A[1]:(0.0163534972817) A[2]:(0.426153093576) A[3]:(5.96073515734e-24)\n",
      " state (6)  A[0]:(0.068283252418) A[1]:(0.560298383236) A[2]:(0.371418356895) A[3]:(5.86866660944e-26)\n",
      " state (7)  A[0]:(0.0184976942837) A[1]:(0.888797104359) A[2]:(0.0927051827312) A[3]:(1.08891270165e-26)\n",
      " state (8)  A[0]:(0.0017023839755) A[1]:(0.990254580975) A[2]:(0.00804306194186) A[3]:(1.08867523449e-27)\n",
      " state (9)  A[0]:(2.96665166388e-05) A[1]:(0.999755680561) A[2]:(0.000214635030716) A[3]:(3.98848596088e-29)\n",
      " state (10)  A[0]:(5.15039482707e-06) A[1]:(0.999958276749) A[2]:(3.65719788533e-05) A[3]:(8.65930452601e-30)\n",
      " state (11)  A[0]:(3.46539059137e-06) A[1]:(0.999973356724) A[2]:(2.3188546038e-05) A[3]:(5.93471513165e-30)\n",
      " state (12)  A[0]:(3.14185490424e-06) A[1]:(0.999976217747) A[2]:(2.06438216992e-05) A[3]:(5.39366724349e-30)\n",
      " state (13)  A[0]:(3.05784578813e-06) A[1]:(0.999976933002) A[2]:(1.99935839191e-05) A[3]:(5.25303335631e-30)\n",
      " state (14)  A[0]:(3.03303181681e-06) A[1]:(0.999977171421) A[2]:(1.98078269023e-05) A[3]:(5.21215511708e-30)\n",
      " state (15)  A[0]:(3.02492071569e-06) A[1]:(0.999977231026) A[2]:(1.97506360564e-05) A[3]:(5.19924724872e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 511000 finished after 6 . Running score: 0.14. Policy_loss: -92050.5557491, Value_loss: 1.63607619947. Times trained:               13318. Times reached goal: 127.               Steps done: 6104848.\n",
      " state (0)  A[0]:(0.992705464363) A[1]:(0.0013705593301) A[2]:(0.00167462020181) A[3]:(0.00424937158823)\n",
      " state (1)  A[0]:(0.0230371076614) A[1]:(0.00300035858527) A[2]:(0.00934414193034) A[3]:(0.964618384838)\n",
      " state (2)  A[0]:(0.999993681908) A[1]:(9.56400640462e-07) A[2]:(5.33478123543e-06) A[3]:(1.16503154346e-10)\n",
      " state (3)  A[0]:(0.999998807907) A[1]:(1.86774713029e-07) A[2]:(1.01599141544e-06) A[3]:(1.71879329346e-12)\n",
      " state (4)  A[0]:(0.999995350838) A[1]:(1.14895522074e-06) A[2]:(3.49545939571e-06) A[3]:(7.3652733489e-14)\n",
      " state (5)  A[0]:(0.498640060425) A[1]:(0.0300739370286) A[2]:(0.471285998821) A[3]:(2.63207188116e-24)\n",
      " state (6)  A[0]:(0.0564323812723) A[1]:(0.645979762077) A[2]:(0.297587841749) A[3]:(4.20941818149e-26)\n",
      " state (7)  A[0]:(0.00947978254408) A[1]:(0.94358843565) A[2]:(0.0469317510724) A[3]:(5.7429790346e-27)\n",
      " state (8)  A[0]:(0.000229035736993) A[1]:(0.998282313347) A[2]:(0.00148866581731) A[3]:(2.33292877507e-28)\n",
      " state (9)  A[0]:(8.59180636326e-06) A[1]:(0.999925434589) A[2]:(6.59518773318e-05) A[3]:(1.48460339856e-29)\n",
      " state (10)  A[0]:(3.77481615033e-06) A[1]:(0.999970316887) A[2]:(2.58845084318e-05) A[3]:(6.82339826483e-30)\n",
      " state (11)  A[0]:(3.16087198371e-06) A[1]:(0.999975919724) A[2]:(2.08944093174e-05) A[3]:(5.72936247517e-30)\n",
      " state (12)  A[0]:(3.02649027617e-06) A[1]:(0.999977171421) A[2]:(1.98171237571e-05) A[3]:(5.48760447653e-30)\n",
      " state (13)  A[0]:(2.99169937534e-06) A[1]:(0.999977469444) A[2]:(1.95416978386e-05) A[3]:(5.42508059029e-30)\n",
      " state (14)  A[0]:(2.98186682812e-06) A[1]:(0.999977529049) A[2]:(1.94654403458e-05) A[3]:(5.40755989401e-30)\n",
      " state (15)  A[0]:(2.97884275824e-06) A[1]:(0.999977588654) A[2]:(1.94428794202e-05) A[3]:(5.40228201841e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 512000 finished after 6 . Running score: 0.11. Policy_loss: -92050.5589084, Value_loss: 0.998853712903. Times trained:               12332. Times reached goal: 119.               Steps done: 6117180.\n",
      " state (0)  A[0]:(0.994531214237) A[1]:(0.00134457368404) A[2]:(0.00151629373431) A[3]:(0.00260793138295)\n",
      " state (1)  A[0]:(0.0425633303821) A[1]:(0.00382109661587) A[2]:(0.0130434278399) A[3]:(0.940572142601)\n",
      " state (2)  A[0]:(0.999998688698) A[1]:(1.93849487573e-07) A[2]:(1.11396195734e-06) A[3]:(1.94513806104e-12)\n",
      " state (3)  A[0]:(0.999998450279) A[1]:(3.83928437486e-07) A[2]:(1.13629664611e-06) A[3]:(6.78304634683e-13)\n",
      " state (4)  A[0]:(0.999865472317) A[1]:(9.00914819795e-05) A[2]:(4.44528704975e-05) A[3]:(2.98781210674e-17)\n",
      " state (5)  A[0]:(0.102623961866) A[1]:(0.32398685813) A[2]:(0.573389232159) A[3]:(8.23703782069e-26)\n",
      " state (6)  A[0]:(0.0234906226397) A[1]:(0.775094509125) A[2]:(0.201414883137) A[3]:(1.05170766402e-26)\n",
      " state (7)  A[0]:(0.0099103488028) A[1]:(0.908208727837) A[2]:(0.0818809345365) A[3]:(4.25101772903e-27)\n",
      " state (8)  A[0]:(0.00185608596075) A[1]:(0.982663929462) A[2]:(0.0154799940065) A[3]:(9.1485890145e-28)\n",
      " state (9)  A[0]:(5.05658281327e-05) A[1]:(0.999351978302) A[2]:(0.000597481965087) A[3]:(4.72619506956e-29)\n",
      " state (10)  A[0]:(5.08010725753e-06) A[1]:(0.999934911728) A[2]:(5.99840459472e-05) A[3]:(6.43514357742e-30)\n",
      " state (11)  A[0]:(2.7793028039e-06) A[1]:(0.99996727705) A[2]:(2.9934968552e-05) A[3]:(3.61676967025e-30)\n",
      " state (12)  A[0]:(2.39206769947e-06) A[1]:(0.999972641468) A[2]:(2.49890381383e-05) A[3]:(3.12029427092e-30)\n",
      " state (13)  A[0]:(2.29675788432e-06) A[1]:(0.999973893166) A[2]:(2.3783872166e-05) A[3]:(2.99705506703e-30)\n",
      " state (14)  A[0]:(2.27032978728e-06) A[1]:(0.999974250793) A[2]:(2.34511699091e-05) A[3]:(2.96283990594e-30)\n",
      " state (15)  A[0]:(2.26261795433e-06) A[1]:(0.999974370003) A[2]:(2.33544888033e-05) A[3]:(2.95286588339e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 513000 finished after 9 . Running score: 0.09. Policy_loss: -92050.5586711, Value_loss: 1.21096311057. Times trained:               12997. Times reached goal: 133.               Steps done: 6130177.\n",
      " state (0)  A[0]:(0.99675732851) A[1]:(0.00114905252121) A[2]:(0.00116986199282) A[3]:(0.000923765182961)\n",
      " state (1)  A[0]:(0.026255549863) A[1]:(0.00313260708936) A[2]:(0.0100849149749) A[3]:(0.960526943207)\n",
      " state (2)  A[0]:(0.999994516373) A[1]:(7.57558439091e-07) A[2]:(4.75275965073e-06) A[3]:(7.26723542455e-11)\n",
      " state (3)  A[0]:(0.999998807907) A[1]:(1.57172578952e-07) A[2]:(1.01359034943e-06) A[3]:(1.440019653e-12)\n",
      " state (4)  A[0]:(0.999998390675) A[1]:(3.41260431469e-07) A[2]:(1.25229348669e-06) A[3]:(4.48284909627e-13)\n",
      " state (5)  A[0]:(0.998862206936) A[1]:(0.000119820702821) A[2]:(0.0010179982055) A[3]:(2.23744833522e-19)\n",
      " state (6)  A[0]:(0.141242861748) A[1]:(0.120290853083) A[2]:(0.738466322422) A[3]:(7.54629149056e-26)\n",
      " state (7)  A[0]:(0.0495454221964) A[1]:(0.583073616028) A[2]:(0.367380976677) A[3]:(1.35196075865e-26)\n",
      " state (8)  A[0]:(0.00355825992301) A[1]:(0.969537734985) A[2]:(0.0269039813429) A[3]:(1.0896041105e-27)\n",
      " state (9)  A[0]:(4.83701514895e-05) A[1]:(0.999426782131) A[2]:(0.000524863076862) A[3]:(2.98573127604e-29)\n",
      " state (10)  A[0]:(1.071562383e-05) A[1]:(0.999887347221) A[2]:(0.00010196412768) A[3]:(7.32720573964e-30)\n",
      " state (11)  A[0]:(7.62501804275e-06) A[1]:(0.999924480915) A[2]:(6.79024742567e-05) A[3]:(5.22218913686e-30)\n",
      " state (12)  A[0]:(7.00610780768e-06) A[1]:(0.999931752682) A[2]:(6.12449075561e-05) A[3]:(4.7943061839e-30)\n",
      " state (13)  A[0]:(6.84740189172e-06) A[1]:(0.999933600426) A[2]:(5.95569108555e-05) A[3]:(4.68450560757e-30)\n",
      " state (14)  A[0]:(6.80254697727e-06) A[1]:(0.999934136868) A[2]:(5.90855779592e-05) A[3]:(4.65362377231e-30)\n",
      " state (15)  A[0]:(6.7890155151e-06) A[1]:(0.999934256077) A[2]:(5.89455557929e-05) A[3]:(4.64436689535e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 514000 finished after 9 . Running score: 0.1. Policy_loss: -92050.5652199, Value_loss: 1.63174948185. Times trained:               12851. Times reached goal: 138.               Steps done: 6143028.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0014,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0014,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  1.8854e-06,  3.4328e-06,  3.1640e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0014,  0.0014,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  1.8848e-06,  3.4321e-06,  3.1658e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3500e-05,  9.9978e-01,  1.9876e-04,  2.2875e-29]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3507e-05,  9.9978e-01,  1.9881e-04,  2.2880e-29]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3512e-05,  9.9978e-01,  1.9887e-04,  2.2885e-29]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995327651501) A[1]:(0.00138860754669) A[2]:(0.00138045486528) A[3]:(0.00190328445751)\n",
      " state (1)  A[0]:(0.0270424261689) A[1]:(0.00313703319989) A[2]:(0.0100549608469) A[3]:(0.959765553474)\n",
      " state (2)  A[0]:(0.999998092651) A[1]:(2.65457174464e-07) A[2]:(1.62973037732e-06) A[3]:(5.60985589579e-12)\n",
      " state (3)  A[0]:(0.999998748302) A[1]:(2.06167996453e-07) A[2]:(1.049604748e-06) A[3]:(1.40464809922e-12)\n",
      " state (4)  A[0]:(0.999994695187) A[1]:(1.88337867257e-06) A[2]:(3.43044530382e-06) A[3]:(3.1701441331e-14)\n",
      " state (5)  A[0]:(0.510173201561) A[1]:(0.0352602787316) A[2]:(0.454566508532) A[3]:(2.07347789645e-24)\n",
      " state (6)  A[0]:(0.0455516427755) A[1]:(0.64143872261) A[2]:(0.313009649515) A[3]:(2.170758473e-26)\n",
      " state (7)  A[0]:(0.00282677426003) A[1]:(0.98143941164) A[2]:(0.0157337952405) A[3]:(1.209428442e-27)\n",
      " state (8)  A[0]:(2.35263851209e-05) A[1]:(0.999777495861) A[2]:(0.000198995272513) A[3]:(2.28969342329e-29)\n",
      " state (9)  A[0]:(4.5633632908e-06) A[1]:(0.999958455563) A[2]:(3.69716181012e-05) A[3]:(5.38830698925e-30)\n",
      " state (10)  A[0]:(3.40193355441e-06) A[1]:(0.999970316887) A[2]:(2.62990506599e-05) A[3]:(4.06394351995e-30)\n",
      " state (11)  A[0]:(3.19948662764e-06) A[1]:(0.999972343445) A[2]:(2.44530328928e-05) A[3]:(3.8276747978e-30)\n",
      " state (12)  A[0]:(3.15423744723e-06) A[1]:(0.999972820282) A[2]:(2.40455756284e-05) A[3]:(3.77495208942e-30)\n",
      " state (13)  A[0]:(3.14303133564e-06) A[1]:(0.999972879887) A[2]:(2.39473556576e-05) A[3]:(3.76210064478e-30)\n",
      " state (14)  A[0]:(3.13999953505e-06) A[1]:(0.999972939491) A[2]:(2.39221571974e-05) A[3]:(3.75871559721e-30)\n",
      " state (15)  A[0]:(3.13910118166e-06) A[1]:(0.999972939491) A[2]:(2.39152213908e-05) A[3]:(3.75779777122e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 515000 finished after 8 . Running score: 0.13. Policy_loss: -92050.5588512, Value_loss: 1.00165627345. Times trained:               12561. Times reached goal: 118.               Steps done: 6155589.\n",
      " state (0)  A[0]:(0.996006667614) A[1]:(0.00138624326792) A[2]:(0.00128756393678) A[3]:(0.00131950248033)\n",
      " state (1)  A[0]:(0.045052472502) A[1]:(0.00384580669925) A[2]:(0.0135046355426) A[3]:(0.937597095966)\n",
      " state (2)  A[0]:(0.999998629093) A[1]:(1.9636135562e-07) A[2]:(1.16793876259e-06) A[3]:(2.0539423027e-12)\n",
      " state (3)  A[0]:(0.999998509884) A[1]:(3.34764251875e-07) A[2]:(1.16900389457e-06) A[3]:(8.76389510008e-13)\n",
      " state (4)  A[0]:(0.999953627586) A[1]:(3.31572337018e-05) A[2]:(1.32324767037e-05) A[3]:(4.67875694954e-16)\n",
      " state (5)  A[0]:(0.124396458268) A[1]:(0.216200068593) A[2]:(0.659403502941) A[3]:(1.32327336961e-25)\n",
      " state (6)  A[0]:(0.0256712324917) A[1]:(0.706783473492) A[2]:(0.267545282841) A[3]:(1.023755102e-26)\n",
      " state (7)  A[0]:(0.00959620438516) A[1]:(0.90170788765) A[2]:(0.0886959433556) A[3]:(3.50862874299e-27)\n",
      " state (8)  A[0]:(0.00140635459684) A[1]:(0.986175656319) A[2]:(0.0124180028215) A[3]:(5.91640575201e-28)\n",
      " state (9)  A[0]:(3.22317173413e-05) A[1]:(0.999565422535) A[2]:(0.000402353325626) A[3]:(2.67539565167e-29)\n",
      " state (10)  A[0]:(4.26517226515e-06) A[1]:(0.999943137169) A[2]:(5.26160656591e-05) A[3]:(4.63814072495e-30)\n",
      " state (11)  A[0]:(2.60455749412e-06) A[1]:(0.999967515469) A[2]:(2.98855702567e-05) A[3]:(2.90697740512e-30)\n",
      " state (12)  A[0]:(2.3063143999e-06) A[1]:(0.999971866608) A[2]:(2.58544496319e-05) A[3]:(2.58284903785e-30)\n",
      " state (13)  A[0]:(2.23207825911e-06) A[1]:(0.999972879887) A[2]:(2.48584092333e-05) A[3]:(2.501590969e-30)\n",
      " state (14)  A[0]:(2.21153436541e-06) A[1]:(0.99997317791) A[2]:(2.45843712037e-05) A[3]:(2.47909595679e-30)\n",
      " state (15)  A[0]:(2.20556125896e-06) A[1]:(0.999973297119) A[2]:(2.45052560786e-05) A[3]:(2.47259857633e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 516000 finished after 29 . Running score: 0.14. Policy_loss: -92050.5599497, Value_loss: 0.99408717715. Times trained:               13566. Times reached goal: 120.               Steps done: 6169155.\n",
      " state (0)  A[0]:(0.996559858322) A[1]:(0.00129466643557) A[2]:(0.00116843625437) A[3]:(0.00097701291088)\n",
      " state (1)  A[0]:(0.0452502444386) A[1]:(0.0037157270126) A[2]:(0.0131650883704) A[3]:(0.937868952751)\n",
      " state (2)  A[0]:(0.999998688698) A[1]:(1.93188597564e-07) A[2]:(1.08894073492e-06) A[3]:(2.08108877353e-12)\n",
      " state (3)  A[0]:(0.999998569489) A[1]:(3.82662648235e-07) A[2]:(1.02373610389e-06) A[3]:(5.90993996363e-13)\n",
      " state (4)  A[0]:(0.999914526939) A[1]:(7.54192878958e-05) A[2]:(1.00507650131e-05) A[3]:(1.12986571867e-16)\n",
      " state (5)  A[0]:(0.146312117577) A[1]:(0.379443973303) A[2]:(0.474243909121) A[3]:(8.88348468062e-26)\n",
      " state (6)  A[0]:(0.0242124050856) A[1]:(0.836982667446) A[2]:(0.138804912567) A[3]:(6.00967565033e-27)\n",
      " state (7)  A[0]:(0.00562010146677) A[1]:(0.966625034809) A[2]:(0.0277548581362) A[3]:(1.33577127598e-27)\n",
      " state (8)  A[0]:(0.000203298026463) A[1]:(0.998621702194) A[2]:(0.00117499346379) A[3]:(7.69664980909e-29)\n",
      " state (9)  A[0]:(8.03950752015e-06) A[1]:(0.999932527542) A[2]:(5.94127559452e-05) A[3]:(5.48338360546e-30)\n",
      " state (10)  A[0]:(3.49642004949e-06) A[1]:(0.999972641468) A[2]:(2.38634529524e-05) A[3]:(2.55876569767e-30)\n",
      " state (11)  A[0]:(2.92971981253e-06) A[1]:(0.999977707863) A[2]:(1.93825271708e-05) A[3]:(2.15875531444e-30)\n",
      " state (12)  A[0]:(2.80876247416e-06) A[1]:(0.999978780746) A[2]:(1.8431499484e-05) A[3]:(2.07216426282e-30)\n",
      " state (13)  A[0]:(2.77882668342e-06) A[1]:(0.999979019165) A[2]:(1.81976010936e-05) A[3]:(2.05074374648e-30)\n",
      " state (14)  A[0]:(2.77098297374e-06) A[1]:(0.99997907877) A[2]:(1.81368232006e-05) A[3]:(2.04515046224e-30)\n",
      " state (15)  A[0]:(2.76880632555e-06) A[1]:(0.999979138374) A[2]:(1.81204341061e-05) A[3]:(2.04362194342e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 517000 finished after 14 . Running score: 0.1. Policy_loss: -92050.5623179, Value_loss: 1.43898254417. Times trained:               12419. Times reached goal: 136.               Steps done: 6181574.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996633768082) A[1]:(0.00121130328625) A[2]:(0.00109065591823) A[3]:(0.00106425967533)\n",
      " state (1)  A[0]:(0.0283646564931) A[1]:(0.00309849111363) A[2]:(0.00934657268226) A[3]:(0.959190249443)\n",
      " state (2)  A[0]:(0.999998807907) A[1]:(2.05626847105e-07) A[2]:(1.00129932434e-06) A[3]:(3.14582626441e-12)\n",
      " state (3)  A[0]:(0.999999046326) A[1]:(3.16248133458e-07) A[2]:(6.38532981156e-07) A[3]:(3.7141332219e-13)\n",
      " state (4)  A[0]:(0.999936401844) A[1]:(6.11773648416e-05) A[2]:(2.43221347773e-06) A[3]:(5.93292779725e-17)\n",
      " state (5)  A[0]:(0.320969223976) A[1]:(0.52877509594) A[2]:(0.150255724788) A[3]:(9.70778454571e-26)\n",
      " state (6)  A[0]:(0.0414244756103) A[1]:(0.918765664101) A[2]:(0.0398098789155) A[3]:(3.0531172296e-27)\n",
      " state (7)  A[0]:(0.00796995870769) A[1]:(0.986290574074) A[2]:(0.00573945138603) A[3]:(5.07163461903e-28)\n",
      " state (8)  A[0]:(0.000242292546318) A[1]:(0.999575138092) A[2]:(0.000182592266356) A[3]:(2.38160331413e-29)\n",
      " state (9)  A[0]:(1.57706435857e-05) A[1]:(0.999966919422) A[2]:(1.73324297066e-05) A[3]:(2.89153611133e-30)\n",
      " state (10)  A[0]:(8.81492542248e-06) A[1]:(0.99998152256) A[2]:(9.68798849499e-06) A[3]:(1.76448755025e-30)\n",
      " state (11)  A[0]:(7.83622635936e-06) A[1]:(0.999983608723) A[2]:(8.53263009049e-06) A[3]:(1.58806409186e-30)\n",
      " state (12)  A[0]:(7.62369791119e-06) A[1]:(0.99998408556) A[2]:(8.28011889098e-06) A[3]:(1.54912476003e-30)\n",
      " state (13)  A[0]:(7.57141469876e-06) A[1]:(0.999984204769) A[2]:(8.21869252832e-06) A[3]:(1.53958106246e-30)\n",
      " state (14)  A[0]:(7.55776591177e-06) A[1]:(0.999984264374) A[2]:(8.20303284854e-06) A[3]:(1.53712822893e-30)\n",
      " state (15)  A[0]:(7.55399059926e-06) A[1]:(0.999984264374) A[2]:(8.19887191028e-06) A[3]:(1.5364716448e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 518000 finished after 6 . Running score: 0.1. Policy_loss: -92050.5597814, Value_loss: 1.21112805005. Times trained:               12530. Times reached goal: 133.               Steps done: 6194104.\n",
      " state (0)  A[0]:(0.990441143513) A[1]:(0.00137568730861) A[2]:(0.00156835536472) A[3]:(0.00661482755095)\n",
      " state (1)  A[0]:(0.0280459094793) A[1]:(0.00303246476687) A[2]:(0.00899826455861) A[3]:(0.959923386574)\n",
      " state (2)  A[0]:(0.999996602535) A[1]:(6.28659506674e-07) A[2]:(2.77782601188e-06) A[3]:(5.3869613631e-11)\n",
      " state (3)  A[0]:(0.999999344349) A[1]:(1.82101913992e-07) A[2]:(4.56477494026e-07) A[3]:(5.04898260787e-13)\n",
      " state (4)  A[0]:(0.99999320507) A[1]:(6.2723602241e-06) A[2]:(5.49985429643e-07) A[3]:(9.27053737968e-16)\n",
      " state (5)  A[0]:(0.80383515358) A[1]:(0.180592224002) A[2]:(0.0155726475641) A[3]:(7.53299943082e-25)\n",
      " state (6)  A[0]:(0.0750625431538) A[1]:(0.913891077042) A[2]:(0.0110463928431) A[3]:(1.5276528319e-27)\n",
      " state (7)  A[0]:(0.00722136162221) A[1]:(0.992197096348) A[2]:(0.000581550935749) A[3]:(9.91971041974e-29)\n",
      " state (8)  A[0]:(0.000210347032407) A[1]:(0.999770402908) A[2]:(1.92655170395e-05) A[3]:(4.9107658887e-30)\n",
      " state (9)  A[0]:(3.51926973963e-05) A[1]:(0.999959111214) A[2]:(5.72136468691e-06) A[3]:(1.53251116923e-30)\n",
      " state (10)  A[0]:(2.57747251453e-05) A[1]:(0.999969661236) A[2]:(4.53581924376e-06) A[3]:(1.23659833393e-30)\n",
      " state (11)  A[0]:(2.41453653871e-05) A[1]:(0.99997150898) A[2]:(4.31962280345e-06) A[3]:(1.18204495766e-30)\n",
      " state (12)  A[0]:(2.37095482589e-05) A[1]:(0.999972045422) A[2]:(4.26995165981e-06) A[3]:(1.16866670347e-30)\n",
      " state (13)  A[0]:(2.35456918745e-05) A[1]:(0.999972224236) A[2]:(4.25653524871e-06) A[3]:(1.16446595686e-30)\n",
      " state (14)  A[0]:(2.34623967117e-05) A[1]:(0.99997228384) A[2]:(4.25202415499e-06) A[3]:(1.16269932991e-30)\n",
      " state (15)  A[0]:(2.34109920711e-05) A[1]:(0.999972343445) A[2]:(4.24996505899e-06) A[3]:(1.16172404576e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 519000 finished after 17 . Running score: 0.06. Policy_loss: -92050.5597919, Value_loss: 0.999582034781. Times trained:               12404. Times reached goal: 120.               Steps done: 6206508.\n",
      "action_dist \n",
      "tensor([[ 0.9918,  0.0014,  0.0015,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.4279e-07,  4.9601e-07,  5.6393e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.6430e-04,  9.9969e-01,  4.7823e-05,  5.9966e-30]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.991838157177) A[1]:(0.00136317475699) A[2]:(0.00147147418465) A[3]:(0.00532720983028)\n",
      " state (1)  A[0]:(0.023693272844) A[1]:(0.00293458905071) A[2]:(0.0082422439009) A[3]:(0.9651299119)\n",
      " state (2)  A[0]:(0.999631226063) A[1]:(7.35891953809e-05) A[2]:(0.000291970005492) A[3]:(3.2158227441e-06)\n",
      " state (3)  A[0]:(0.999999344349) A[1]:(1.46513457366e-07) A[2]:(5.30370243723e-07) A[3]:(8.55816069054e-13)\n",
      " state (4)  A[0]:(0.999998748302) A[1]:(7.42739757698e-07) A[2]:(4.9600487273e-07) A[3]:(5.63998006013e-14)\n",
      " state (5)  A[0]:(0.998439848423) A[1]:(0.00141394173261) A[2]:(0.00014623037714) A[3]:(2.97316156938e-21)\n",
      " state (6)  A[0]:(0.194605067372) A[1]:(0.754002511501) A[2]:(0.0513923987746) A[3]:(6.23072503006e-27)\n",
      " state (7)  A[0]:(0.0117978397757) A[1]:(0.986025691032) A[2]:(0.00217648642138) A[3]:(1.82643857013e-28)\n",
      " state (8)  A[0]:(0.000264392787358) A[1]:(0.999687790871) A[2]:(4.78344336443e-05) A[3]:(5.99847018366e-30)\n",
      " state (9)  A[0]:(3.08031740133e-05) A[1]:(0.9999589324) A[2]:(1.02922194856e-05) A[3]:(1.38546970937e-30)\n",
      " state (10)  A[0]:(2.10724138014e-05) A[1]:(0.999971330166) A[2]:(7.60849297876e-06) A[3]:(1.05101166198e-30)\n",
      " state (11)  A[0]:(1.95203847397e-05) A[1]:(0.999973356724) A[2]:(7.14044699635e-06) A[3]:(9.92456526783e-31)\n",
      " state (12)  A[0]:(1.91443687072e-05) A[1]:(0.999973833561) A[2]:(7.03551540937e-06) A[3]:(9.78853612116e-31)\n",
      " state (13)  A[0]:(1.90270075109e-05) A[1]:(0.99997395277) A[2]:(7.00926420905e-06) A[3]:(9.75134253951e-31)\n",
      " state (14)  A[0]:(1.89793099707e-05) A[1]:(0.999974012375) A[2]:(7.00188866176e-06) A[3]:(9.73900078922e-31)\n",
      " state (15)  A[0]:(1.89548554772e-05) A[1]:(0.99997407198) A[2]:(6.99940528648e-06) A[3]:(9.73365276012e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 520000 finished after 3 . Running score: 0.19. Policy_loss: -92050.5597021, Value_loss: 1.42640728871. Times trained:               13282. Times reached goal: 127.               Steps done: 6219790.\n",
      " state (0)  A[0]:(0.994800031185) A[1]:(0.00122435379308) A[2]:(0.001300198026) A[3]:(0.00267543597147)\n",
      " state (1)  A[0]:(0.0274578779936) A[1]:(0.0029832997825) A[2]:(0.00930098816752) A[3]:(0.960257828236)\n",
      " state (2)  A[0]:(0.999976217747) A[1]:(3.84415807275e-06) A[2]:(1.9907098249e-05) A[3]:(3.96186550233e-09)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.30544478338e-07) A[2]:(6.3330003286e-07) A[3]:(8.1554500566e-13)\n",
      " state (4)  A[0]:(0.999998867512) A[1]:(5.00491410094e-07) A[2]:(6.45493742013e-07) A[3]:(7.82987498622e-14)\n",
      " state (5)  A[0]:(0.999399244785) A[1]:(0.00046156955068) A[2]:(0.000139212745125) A[3]:(1.36656053307e-20)\n",
      " state (6)  A[0]:(0.369994729757) A[1]:(0.377846717834) A[2]:(0.252158582211) A[3]:(1.07591513972e-26)\n",
      " state (7)  A[0]:(0.0837672650814) A[1]:(0.849252939224) A[2]:(0.0669797584414) A[3]:(1.1014887353e-27)\n",
      " state (8)  A[0]:(0.00518793519586) A[1]:(0.991678476334) A[2]:(0.00313356122933) A[3]:(6.31100520498e-29)\n",
      " state (9)  A[0]:(0.000115604569146) A[1]:(0.999766528606) A[2]:(0.000117886003864) A[3]:(3.04403835371e-30)\n",
      " state (10)  A[0]:(3.61424608855e-05) A[1]:(0.999924063683) A[2]:(3.97890216846e-05) A[3]:(1.16185419649e-30)\n",
      " state (11)  A[0]:(2.86388767563e-05) A[1]:(0.999940276146) A[2]:(3.10690666083e-05) A[3]:(9.40755464048e-31)\n",
      " state (12)  A[0]:(2.71020126092e-05) A[1]:(0.999943673611) A[2]:(2.9250206353e-05) A[3]:(8.93905713518e-31)\n",
      " state (13)  A[0]:(2.6713938496e-05) A[1]:(0.999944508076) A[2]:(2.8796857805e-05) A[3]:(8.82118044247e-31)\n",
      " state (14)  A[0]:(2.6607156542e-05) A[1]:(0.99994468689) A[2]:(2.8676171496e-05) A[3]:(8.78940730036e-31)\n",
      " state (15)  A[0]:(2.65755115834e-05) A[1]:(0.999944806099) A[2]:(2.86426111415e-05) A[3]:(8.78035975544e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 521000 finished after 17 . Running score: 0.12. Policy_loss: -92050.5601673, Value_loss: 1.41450455093. Times trained:               12887. Times reached goal: 134.               Steps done: 6232677.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995842516422) A[1]:(0.00119710224681) A[2]:(0.00116565823555) A[3]:(0.00179470330477)\n",
      " state (1)  A[0]:(0.0365902930498) A[1]:(0.0033424848225) A[2]:(0.0105012431741) A[3]:(0.94956600666)\n",
      " state (2)  A[0]:(0.999998927116) A[1]:(1.87377409588e-07) A[2]:(8.68612687555e-07) A[3]:(2.38759229401e-12)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(2.79795841607e-07) A[2]:(5.66508560951e-07) A[3]:(4.23763834353e-13)\n",
      " state (4)  A[0]:(0.999966323376) A[1]:(3.20371909766e-05) A[2]:(1.63399408848e-06) A[3]:(1.41109256856e-16)\n",
      " state (5)  A[0]:(0.319739222527) A[1]:(0.599927425385) A[2]:(0.0803333371878) A[3]:(3.02300250306e-26)\n",
      " state (6)  A[0]:(0.0194170158356) A[1]:(0.971305131912) A[2]:(0.00927787087858) A[3]:(3.11980886135e-28)\n",
      " state (7)  A[0]:(0.00188253959641) A[1]:(0.997593522072) A[2]:(0.000523950264324) A[3]:(2.27129206545e-29)\n",
      " state (8)  A[0]:(5.76381426072e-05) A[1]:(0.999919712543) A[2]:(2.26595038839e-05) A[3]:(1.30453918026e-30)\n",
      " state (9)  A[0]:(1.17984509416e-05) A[1]:(0.999980449677) A[2]:(7.74476029619e-06) A[3]:(4.59465523926e-31)\n",
      " state (10)  A[0]:(9.0514367912e-06) A[1]:(0.999984622002) A[2]:(6.3319266701e-06) A[3]:(3.80877026255e-31)\n",
      " state (11)  A[0]:(8.59592637426e-06) A[1]:(0.999985337257) A[2]:(6.07673518971e-06) A[3]:(3.66772669208e-31)\n",
      " state (12)  A[0]:(8.4968187366e-06) A[1]:(0.999985456467) A[2]:(6.02202180744e-06) A[3]:(3.63718617335e-31)\n",
      " state (13)  A[0]:(8.47260980663e-06) A[1]:(0.999985516071) A[2]:(6.00958446739e-06) A[3]:(3.63003399552e-31)\n",
      " state (14)  A[0]:(8.46592138259e-06) A[1]:(0.999985516071) A[2]:(6.00665089223e-06) A[3]:(3.62826205534e-31)\n",
      " state (15)  A[0]:(8.4637576947e-06) A[1]:(0.999985516071) A[2]:(6.00591738475e-06) A[3]:(3.62776364573e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 522000 finished after 14 . Running score: 0.09. Policy_loss: -92050.5588359, Value_loss: 1.21688779707. Times trained:               12692. Times reached goal: 143.               Steps done: 6245369.\n",
      " state (0)  A[0]:(0.995305418968) A[1]:(0.00115997565445) A[2]:(0.00120898068417) A[3]:(0.00232561444864)\n",
      " state (1)  A[0]:(0.0266745239496) A[1]:(0.00297381496057) A[2]:(0.00870004389435) A[3]:(0.961651623249)\n",
      " state (2)  A[0]:(0.999893248081) A[1]:(2.09821755561e-05) A[2]:(8.56115657371e-05) A[3]:(1.58336305844e-07)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.95887878363e-07) A[2]:(5.82295513141e-07) A[3]:(8.00906704238e-13)\n",
      " state (4)  A[0]:(0.999991238117) A[1]:(7.76757406129e-06) A[2]:(1.01610191905e-06) A[3]:(4.11063268182e-15)\n",
      " state (5)  A[0]:(0.400135934353) A[1]:(0.53420060873) A[2]:(0.0656634122133) A[3]:(2.45577823214e-25)\n",
      " state (6)  A[0]:(0.0280128121376) A[1]:(0.953878164291) A[2]:(0.0181090049446) A[3]:(8.50926212498e-28)\n",
      " state (7)  A[0]:(0.0103081986308) A[1]:(0.98420548439) A[2]:(0.00548628764227) A[3]:(2.28866296049e-28)\n",
      " state (8)  A[0]:(0.00276375515386) A[1]:(0.996194303036) A[2]:(0.00104192027356) A[3]:(5.11160705428e-29)\n",
      " state (9)  A[0]:(0.000221984941163) A[1]:(0.999708116055) A[2]:(6.99213051121e-05) A[3]:(4.78892072706e-30)\n",
      " state (10)  A[0]:(1.77236452146e-05) A[1]:(0.999971330166) A[2]:(1.09564807644e-05) A[3]:(8.2326969374e-31)\n",
      " state (11)  A[0]:(8.10648543847e-06) A[1]:(0.999985516071) A[2]:(6.36382355879e-06) A[3]:(4.89134531143e-31)\n",
      " state (12)  A[0]:(6.80189896229e-06) A[1]:(0.999987661839) A[2]:(5.56541544938e-06) A[3]:(4.32046177797e-31)\n",
      " state (13)  A[0]:(6.50278707326e-06) A[1]:(0.999988138676) A[2]:(5.37110918231e-06) A[3]:(4.18214088751e-31)\n",
      " state (14)  A[0]:(6.42237137072e-06) A[1]:(0.999988257885) A[2]:(5.31860860065e-06) A[3]:(4.14469151829e-31)\n",
      " state (15)  A[0]:(6.3994325501e-06) A[1]:(0.99998831749) A[2]:(5.30389934283e-06) A[3]:(4.13411206913e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 523000 finished after 38 . Running score: 0.13. Policy_loss: -92050.5588713, Value_loss: 1.00793823479. Times trained:               12782. Times reached goal: 112.               Steps done: 6258151.\n",
      " state (0)  A[0]:(0.995631575584) A[1]:(0.00110112549737) A[2]:(0.00114077341277) A[3]:(0.00212654727511)\n",
      " state (1)  A[0]:(0.0305854957551) A[1]:(0.00298777176067) A[2]:(0.00903679896146) A[3]:(0.957389950752)\n",
      " state (2)  A[0]:(0.999793350697) A[1]:(3.90796849388e-05) A[2]:(0.000166721147252) A[3]:(8.63767297687e-07)\n",
      " state (3)  A[0]:(0.999999403954) A[1]:(1.26491883634e-07) A[2]:(4.87527131554e-07) A[3]:(7.26715237466e-13)\n",
      " state (4)  A[0]:(0.999998986721) A[1]:(5.82108839353e-07) A[2]:(4.56762592194e-07) A[3]:(6.49623584092e-14)\n",
      " state (5)  A[0]:(0.997432768345) A[1]:(0.00231147510931) A[2]:(0.000255734368693) A[3]:(1.09105829615e-21)\n",
      " state (6)  A[0]:(0.181332141161) A[1]:(0.7763915658) A[2]:(0.0422762744129) A[3]:(1.72373175954e-27)\n",
      " state (7)  A[0]:(0.0223580859601) A[1]:(0.973739981651) A[2]:(0.00390191748738) A[3]:(9.28717784996e-29)\n",
      " state (8)  A[0]:(0.00164160353597) A[1]:(0.9981944561) A[2]:(0.000163924996741) A[3]:(5.44523702702e-30)\n",
      " state (9)  A[0]:(8.72182572493e-05) A[1]:(0.999895632267) A[2]:(1.71532119566e-05) A[3]:(6.32365508525e-31)\n",
      " state (10)  A[0]:(3.21580373566e-05) A[1]:(0.999958515167) A[2]:(9.33075261855e-06) A[3]:(3.4050178106e-31)\n",
      " state (11)  A[0]:(2.64971986326e-05) A[1]:(0.999965310097) A[2]:(8.20259538159e-06) A[3]:(2.9997155869e-31)\n",
      " state (12)  A[0]:(2.53275684372e-05) A[1]:(0.999966740608) A[2]:(7.95846244728e-06) A[3]:(2.91200137394e-31)\n",
      " state (13)  A[0]:(2.50176090049e-05) A[1]:(0.999967098236) A[2]:(7.89846035332e-06) A[3]:(2.88969378254e-31)\n",
      " state (14)  A[0]:(2.49188433372e-05) A[1]:(0.999967217445) A[2]:(7.88262877904e-06) A[3]:(2.88332965613e-31)\n",
      " state (15)  A[0]:(2.48802807619e-05) A[1]:(0.999967217445) A[2]:(7.87805947766e-06) A[3]:(2.88119660408e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 524000 finished after 12 . Running score: 0.16. Policy_loss: -92050.5588041, Value_loss: 1.6518759247. Times trained:               12514. Times reached goal: 117.               Steps done: 6270665.\n",
      "action_dist \n",
      "tensor([[ 0.9932,  0.0014,  0.0013,  0.0041]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9932,  0.0014,  0.0013,  0.0041]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9932,  0.0014,  0.0013,  0.0041]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9932,  0.0014,  0.0013,  0.0041]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.9771e-07,  7.6852e-07,  4.7346e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0596e-02,  9.6352e-01,  2.5882e-02,  2.3803e-28]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.9017e-04,  9.9800e-01,  1.5084e-03,  1.6191e-29]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0568e-02,  9.6362e-01,  2.5809e-02,  2.3753e-28]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.8726e-04,  9.9801e-01,  1.5002e-03,  1.6119e-29]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.6977e-05,  9.9987e-01,  1.0063e-04,  1.4129e-30]])\n",
      "On state=10, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.4138e-06,  9.9997e-01,  2.6292e-05,  4.6047e-31]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.5502e-06,  9.9996e-01,  2.6828e-05,  4.6820e-31]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993187904358) A[1]:(0.00139308476355) A[2]:(0.00132060493343) A[3]:(0.00409840838984)\n",
      " state (1)  A[0]:(0.0203111451119) A[1]:(0.00266592064872) A[2]:(0.00763278827071) A[3]:(0.969390153885)\n",
      " state (2)  A[0]:(0.155647277832) A[1]:(0.00572290271521) A[2]:(0.0215919632465) A[3]:(0.817037820816)\n",
      " state (3)  A[0]:(0.999999046326) A[1]:(1.49060994659e-07) A[2]:(8.2014992131e-07) A[3]:(1.42939425487e-12)\n",
      " state (4)  A[0]:(0.999998927116) A[1]:(2.98609535321e-07) A[2]:(7.68788936512e-07) A[3]:(4.72154216766e-13)\n",
      " state (5)  A[0]:(0.999279499054) A[1]:(0.000487261917442) A[2]:(0.000233247425058) A[3]:(3.50739417275e-19)\n",
      " state (6)  A[0]:(0.168066129088) A[1]:(0.453709304333) A[2]:(0.378224521875) A[3]:(1.26444881309e-26)\n",
      " state (7)  A[0]:(0.0449011027813) A[1]:(0.834573686123) A[2]:(0.120525173843) A[3]:(1.19258108092e-27)\n",
      " state (8)  A[0]:(0.010470142588) A[1]:(0.963978230953) A[2]:(0.0255516394973) A[3]:(2.35610613825e-28)\n",
      " state (9)  A[0]:(0.000480984308524) A[1]:(0.998036623001) A[2]:(0.00148239533883) A[3]:(1.59592380444e-29)\n",
      " state (10)  A[0]:(2.68459516519e-05) A[1]:(0.999873042107) A[2]:(0.000100091659988) A[3]:(1.40743527892e-30)\n",
      " state (11)  A[0]:(1.12089401227e-05) A[1]:(0.99995136261) A[2]:(3.74084484065e-05) A[3]:(6.14714097274e-31)\n",
      " state (12)  A[0]:(9.05254455574e-06) A[1]:(0.999962151051) A[2]:(2.88093451672e-05) A[3]:(4.96261271273e-31)\n",
      " state (13)  A[0]:(8.54748668644e-06) A[1]:(0.999964654446) A[2]:(2.68169878836e-05) A[3]:(4.68141518591e-31)\n",
      " state (14)  A[0]:(8.40833308757e-06) A[1]:(0.999965310097) A[2]:(2.62708435912e-05) A[3]:(4.60370842655e-31)\n",
      " state (15)  A[0]:(8.36792150949e-06) A[1]:(0.999965548515) A[2]:(2.61126861005e-05) A[3]:(4.58114551778e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 525000 finished after 12 . Running score: 0.14. Policy_loss: -92050.5597277, Value_loss: 0.998838609627. Times trained:               12659. Times reached goal: 126.               Steps done: 6283324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995919764042) A[1]:(0.00130003504455) A[2]:(0.00109605351463) A[3]:(0.00168417242821)\n",
      " state (1)  A[0]:(0.0212530829012) A[1]:(0.00277313613333) A[2]:(0.0078187007457) A[3]:(0.96815508604)\n",
      " state (2)  A[0]:(0.875042021275) A[1]:(0.00620157783851) A[2]:(0.0236570220441) A[3]:(0.0950993895531)\n",
      " state (3)  A[0]:(0.99999910593) A[1]:(1.60239096658e-07) A[2]:(7.48222078073e-07) A[3]:(1.10913578669e-12)\n",
      " state (4)  A[0]:(0.999998748302) A[1]:(4.35083734374e-07) A[2]:(8.04978697033e-07) A[3]:(3.72009443649e-13)\n",
      " state (5)  A[0]:(0.998246431351) A[1]:(0.00139238557313) A[2]:(0.000361164711649) A[3]:(9.5402972884e-20)\n",
      " state (6)  A[0]:(0.0688806995749) A[1]:(0.768338084221) A[2]:(0.162781238556) A[3]:(3.47819211652e-27)\n",
      " state (7)  A[0]:(0.00492989225313) A[1]:(0.982883632183) A[2]:(0.0121864825487) A[3]:(1.03353369351e-28)\n",
      " state (8)  A[0]:(0.000103354977909) A[1]:(0.999546468258) A[2]:(0.000350172631443) A[3]:(3.37608635556e-30)\n",
      " state (9)  A[0]:(6.89630451234e-06) A[1]:(0.999966084957) A[2]:(2.70303444267e-05) A[3]:(3.34214085288e-31)\n",
      " state (10)  A[0]:(3.83953283745e-06) A[1]:(0.99998241663) A[2]:(1.37456463563e-05) A[3]:(1.88858343092e-31)\n",
      " state (11)  A[0]:(3.40833435075e-06) A[1]:(0.999984741211) A[2]:(1.18706439025e-05) A[3]:(1.67276219791e-31)\n",
      " state (12)  A[0]:(3.31489673044e-06) A[1]:(0.999985218048) A[2]:(1.14658641905e-05) A[3]:(1.62558302684e-31)\n",
      " state (13)  A[0]:(3.29206295646e-06) A[1]:(0.999985337257) A[2]:(1.13671803774e-05) A[3]:(1.61404073026e-31)\n",
      " state (14)  A[0]:(3.28624128088e-06) A[1]:(0.999985396862) A[2]:(1.13421447168e-05) A[3]:(1.61110034869e-31)\n",
      " state (15)  A[0]:(3.28469968736e-06) A[1]:(0.999985396862) A[2]:(1.13354835776e-05) A[3]:(1.61031394297e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 526000 finished after 13 . Running score: 0.17. Policy_loss: -92050.5561533, Value_loss: 1.64569572286. Times trained:               12603. Times reached goal: 136.               Steps done: 6295927.\n",
      " state (0)  A[0]:(0.99614149332) A[1]:(0.00124978739768) A[2]:(0.00108347670175) A[3]:(0.00152525596786)\n",
      " state (1)  A[0]:(0.0243250634521) A[1]:(0.00283801346086) A[2]:(0.00842331536114) A[3]:(0.964413583279)\n",
      " state (2)  A[0]:(0.999882638454) A[1]:(2.04311563721e-05) A[2]:(9.67623273027e-05) A[3]:(1.53489821741e-07)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(1.39630742524e-07) A[2]:(6.94852474226e-07) A[3]:(9.1157441838e-13)\n",
      " state (4)  A[0]:(0.999998927116) A[1]:(3.27909646103e-07) A[2]:(7.57751081437e-07) A[3]:(3.48996330966e-13)\n",
      " state (5)  A[0]:(0.999647438526) A[1]:(0.000196803652216) A[2]:(0.000155743313371) A[3]:(2.50905644977e-19)\n",
      " state (6)  A[0]:(0.162782207131) A[1]:(0.521147549152) A[2]:(0.316070258617) A[3]:(5.52748788868e-27)\n",
      " state (7)  A[0]:(0.00899933464825) A[1]:(0.972311556339) A[2]:(0.0186890885234) A[3]:(1.03621434725e-28)\n",
      " state (8)  A[0]:(0.000102802623587) A[1]:(0.999597191811) A[2]:(0.000299989944324) A[3]:(1.95778828632e-30)\n",
      " state (9)  A[0]:(1.03999700514e-05) A[1]:(0.999959468842) A[2]:(3.01504169329e-05) A[3]:(2.52723268555e-31)\n",
      " state (10)  A[0]:(6.718136774e-06) A[1]:(0.999975383282) A[2]:(1.79139733518e-05) A[3]:(1.62991354803e-31)\n",
      " state (11)  A[0]:(6.14550572209e-06) A[1]:(0.999977827072) A[2]:(1.60326198966e-05) A[3]:(1.4863003469e-31)\n",
      " state (12)  A[0]:(6.01982037551e-06) A[1]:(0.999978363514) A[2]:(1.56215537572e-05) A[3]:(1.45459843976e-31)\n",
      " state (13)  A[0]:(5.98940232521e-06) A[1]:(0.999978482723) A[2]:(1.55220568558e-05) A[3]:(1.44691717192e-31)\n",
      " state (14)  A[0]:(5.98166207055e-06) A[1]:(0.999978542328) A[2]:(1.5496972992e-05) A[3]:(1.44495362616e-31)\n",
      " state (15)  A[0]:(5.97967755311e-06) A[1]:(0.999978542328) A[2]:(1.54904719238e-05) A[3]:(1.44445756754e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 527000 finished after 6 . Running score: 0.1. Policy_loss: -92050.5561824, Value_loss: 1.21786752408. Times trained:               12781. Times reached goal: 123.               Steps done: 6308708.\n",
      " state (0)  A[0]:(0.993661642075) A[1]:(0.00135013402905) A[2]:(0.00130685931072) A[3]:(0.00368137913756)\n",
      " state (1)  A[0]:(0.0216590277851) A[1]:(0.00266698654741) A[2]:(0.00758953625336) A[3]:(0.968084454536)\n",
      " state (2)  A[0]:(0.999913990498) A[1]:(1.54319677677e-05) A[2]:(7.04933627276e-05) A[3]:(8.44099403707e-08)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.35504464538e-07) A[2]:(6.30122144685e-07) A[3]:(8.96225639275e-13)\n",
      " state (4)  A[0]:(0.999998927116) A[1]:(3.38531918942e-07) A[2]:(7.05833713255e-07) A[3]:(2.95302004421e-13)\n",
      " state (5)  A[0]:(0.999628305435) A[1]:(0.000172567801201) A[2]:(0.00019912654534) A[3]:(8.06485975344e-20)\n",
      " state (6)  A[0]:(0.250918626785) A[1]:(0.410124868155) A[2]:(0.33895650506) A[3]:(9.33010521545e-27)\n",
      " state (7)  A[0]:(0.0345936976373) A[1]:(0.921565175056) A[2]:(0.0438411012292) A[3]:(4.06641955868e-28)\n",
      " state (8)  A[0]:(0.0015049141366) A[1]:(0.997089982033) A[2]:(0.00140507984906) A[3]:(1.64163680491e-29)\n",
      " state (9)  A[0]:(3.63091276085e-05) A[1]:(0.999909162521) A[2]:(5.45039620192e-05) A[3]:(8.20021661789e-31)\n",
      " state (10)  A[0]:(1.23080017147e-05) A[1]:(0.999968230724) A[2]:(1.94563490368e-05) A[3]:(3.30878502518e-31)\n",
      " state (11)  A[0]:(9.95086065814e-06) A[1]:(0.999974548817) A[2]:(1.54748759087e-05) A[3]:(2.72555221827e-31)\n",
      " state (12)  A[0]:(9.47474745772e-06) A[1]:(0.999975860119) A[2]:(1.46522879731e-05) A[3]:(2.60354272252e-31)\n",
      " state (13)  A[0]:(9.35940170166e-06) A[1]:(0.999976217747) A[2]:(1.44525602082e-05) A[3]:(2.57378085125e-31)\n",
      " state (14)  A[0]:(9.32992315938e-06) A[1]:(0.999976277351) A[2]:(1.44016539707e-05) A[3]:(2.56617328691e-31)\n",
      " state (15)  A[0]:(9.32216698857e-06) A[1]:(0.999976277351) A[2]:(1.43883107739e-05) A[3]:(2.56417706241e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 528000 finished after 7 . Running score: 0.08. Policy_loss: -92050.5562154, Value_loss: 1.21530936952. Times trained:               12949. Times reached goal: 143.               Steps done: 6321657.\n",
      " state (0)  A[0]:(0.995434582233) A[1]:(0.00116905965842) A[2]:(0.00114140997175) A[3]:(0.00225491984747)\n",
      " state (1)  A[0]:(0.0237847398967) A[1]:(0.0026286568027) A[2]:(0.00766324158758) A[3]:(0.965923368931)\n",
      " state (2)  A[0]:(0.999912500381) A[1]:(1.51717440531e-05) A[2]:(7.22211334505e-05) A[3]:(1.12202137359e-07)\n",
      " state (3)  A[0]:(0.999999403954) A[1]:(1.00922974866e-07) A[2]:(4.91742298436e-07) A[3]:(6.60028021821e-13)\n",
      " state (4)  A[0]:(0.999999165535) A[1]:(2.80659321561e-07) A[2]:(5.26115286448e-07) A[3]:(1.34532142371e-13)\n",
      " state (5)  A[0]:(0.999362826347) A[1]:(0.00028765029856) A[2]:(0.000349536479916) A[3]:(1.38872860998e-21)\n",
      " state (6)  A[0]:(0.48433342576) A[1]:(0.281785905361) A[2]:(0.23388068378) A[3]:(3.84013598283e-27)\n",
      " state (7)  A[0]:(0.16551630199) A[1]:(0.763584077358) A[2]:(0.0708995908499) A[3]:(5.14031328084e-28)\n",
      " state (8)  A[0]:(0.0214066207409) A[1]:(0.973546683788) A[2]:(0.00504668289796) A[3]:(4.55179970568e-29)\n",
      " state (9)  A[0]:(0.000626287306659) A[1]:(0.999215066433) A[2]:(0.000158654962434) A[3]:(2.01270343273e-30)\n",
      " state (10)  A[0]:(8.79304716364e-05) A[1]:(0.999876320362) A[2]:(3.57423086825e-05) A[3]:(4.90766916638e-31)\n",
      " state (11)  A[0]:(5.72588942305e-05) A[1]:(0.999917626381) A[2]:(2.50911107287e-05) A[3]:(3.54979075445e-31)\n",
      " state (12)  A[0]:(5.18412998645e-05) A[1]:(0.999925136566) A[2]:(2.3030956072e-05) A[3]:(3.2857258227e-31)\n",
      " state (13)  A[0]:(5.04750169057e-05) A[1]:(0.99992698431) A[2]:(2.25267704081e-05) A[3]:(3.2198454769e-31)\n",
      " state (14)  A[0]:(5.00639071106e-05) A[1]:(0.999927520752) A[2]:(2.23932111112e-05) A[3]:(3.20152445705e-31)\n",
      " state (15)  A[0]:(4.9915171985e-05) A[1]:(0.999927699566) A[2]:(2.23557435675e-05) A[3]:(3.19583882597e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 529000 finished after 10 . Running score: 0.13. Policy_loss: -92050.5648932, Value_loss: 1.64776288325. Times trained:               12419. Times reached goal: 116.               Steps done: 6334076.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9967,  0.0011,  0.0010,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8208e-07,  1.0423e-06,  1.9821e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8173e-07,  1.0421e-06,  1.9837e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0011,  0.0010,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0011,  0.0010,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0011,  0.0010,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8066e-07,  1.0414e-06,  1.9888e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0011,  0.0010,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0011,  0.0010,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0011,  0.0010,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0011,  0.0010,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.7982e-07,  1.0408e-06,  1.9929e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4470e-03,  9.9758e-01,  9.7560e-04,  4.9825e-30]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4478e-03,  9.9758e-01,  9.7601e-04,  4.9847e-30]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.5358e-05,  9.9985e-01,  7.6656e-05,  4.8036e-31]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4476e-03,  9.9758e-01,  9.7593e-04,  4.9847e-30]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996670365334) A[1]:(0.00111178809311) A[2]:(0.00102305191103) A[3]:(0.00119478383567)\n",
      " state (1)  A[0]:(0.0222066882998) A[1]:(0.00261819851585) A[2]:(0.00778015609831) A[3]:(0.967394948006)\n",
      " state (2)  A[0]:(0.999959290028) A[1]:(6.44873580313e-06) A[2]:(3.42341045325e-05) A[3]:(1.52080232851e-08)\n",
      " state (3)  A[0]:(0.999999284744) A[1]:(1.14125690232e-07) A[2]:(5.81779318054e-07) A[3]:(6.73533819863e-13)\n",
      " state (4)  A[0]:(0.999998152256) A[1]:(7.7927319353e-07) A[2]:(1.04046125671e-06) A[3]:(1.99551340079e-14)\n",
      " state (5)  A[0]:(0.845449030399) A[1]:(0.015107373707) A[2]:(0.139443561435) A[3]:(1.01714745206e-25)\n",
      " state (6)  A[0]:(0.254032373428) A[1]:(0.459069371223) A[2]:(0.286898255348) A[3]:(1.09224003452e-27)\n",
      " state (7)  A[0]:(0.0585196912289) A[1]:(0.897730767727) A[2]:(0.0437495447695) A[3]:(1.58512339996e-28)\n",
      " state (8)  A[0]:(0.00144990847912) A[1]:(0.997572958469) A[2]:(0.000977157847956) A[3]:(4.99067242116e-30)\n",
      " state (9)  A[0]:(7.54072825657e-05) A[1]:(0.999847888947) A[2]:(7.66971061239e-05) A[3]:(4.8062249443e-31)\n",
      " state (10)  A[0]:(4.11938963225e-05) A[1]:(0.99991697073) A[2]:(4.18357412855e-05) A[3]:(2.83560623118e-31)\n",
      " state (11)  A[0]:(3.6553665268e-05) A[1]:(0.999926686287) A[2]:(3.67416978406e-05) A[3]:(2.53934521444e-31)\n",
      " state (12)  A[0]:(3.55699412466e-05) A[1]:(0.999928772449) A[2]:(3.56544369424e-05) A[3]:(2.47555442741e-31)\n",
      " state (13)  A[0]:(3.53353134415e-05) A[1]:(0.999929249287) A[2]:(3.5398050386e-05) A[3]:(2.46041711645e-31)\n",
      " state (14)  A[0]:(3.52761926479e-05) A[1]:(0.999929368496) A[2]:(3.53353170794e-05) A[3]:(2.45666587888e-31)\n",
      " state (15)  A[0]:(3.52605893568e-05) A[1]:(0.999929428101) A[2]:(3.53195537173e-05) A[3]:(2.45572900988e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 530000 finished after 16 . Running score: 0.17. Policy_loss: -92050.5562915, Value_loss: 1.22385904484. Times trained:               13173. Times reached goal: 120.               Steps done: 6347249.\n",
      " state (0)  A[0]:(0.995765268803) A[1]:(0.00120545574464) A[2]:(0.0011165071046) A[3]:(0.00191274809185)\n",
      " state (1)  A[0]:(0.0201665572822) A[1]:(0.00253637181595) A[2]:(0.00736884539947) A[3]:(0.969928205013)\n",
      " state (2)  A[0]:(0.999992907047) A[1]:(1.0940360653e-06) A[2]:(5.993945706e-06) A[3]:(2.15000003911e-10)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.50656106257e-07) A[2]:(6.32994499483e-07) A[3]:(6.13556623043e-13)\n",
      " state (4)  A[0]:(0.999991059303) A[1]:(5.02037119077e-06) A[2]:(3.90206878365e-06) A[3]:(4.3576025547e-16)\n",
      " state (5)  A[0]:(0.467523217201) A[1]:(0.0891198515892) A[2]:(0.44335693121) A[3]:(9.14031483427e-27)\n",
      " state (6)  A[0]:(0.163421496749) A[1]:(0.576572835445) A[2]:(0.260005682707) A[3]:(9.86755503314e-28)\n",
      " state (7)  A[0]:(0.0562780424953) A[1]:(0.879826366901) A[2]:(0.063895560801) A[3]:(2.59962505376e-28)\n",
      " state (8)  A[0]:(0.0034698031377) A[1]:(0.993836164474) A[2]:(0.00269406149164) A[3]:(1.60894640106e-29)\n",
      " state (9)  A[0]:(8.9444743935e-05) A[1]:(0.999796271324) A[2]:(0.000114295566163) A[3]:(9.00976359058e-31)\n",
      " state (10)  A[0]:(3.06370347971e-05) A[1]:(0.999925673008) A[2]:(4.36702684965e-05) A[3]:(3.83525485557e-31)\n",
      " state (11)  A[0]:(2.48088999797e-05) A[1]:(0.999939918518) A[2]:(3.52925817424e-05) A[3]:(3.19719487626e-31)\n",
      " state (12)  A[0]:(2.36284977291e-05) A[1]:(0.999942839146) A[2]:(3.35467593686e-05) A[3]:(3.0628274084e-31)\n",
      " state (13)  A[0]:(2.33414793911e-05) A[1]:(0.999943554401) A[2]:(3.3123207686e-05) A[3]:(3.03003487759e-31)\n",
      " state (14)  A[0]:(2.326716276e-05) A[1]:(0.999943733215) A[2]:(3.30153561663e-05) A[3]:(3.02160940428e-31)\n",
      " state (15)  A[0]:(2.32469374168e-05) A[1]:(0.99994379282) A[2]:(3.2986907172e-05) A[3]:(3.01939735901e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 531000 finished after 16 . Running score: 0.14. Policy_loss: -92050.5648798, Value_loss: 1.42710756884. Times trained:               12315. Times reached goal: 130.               Steps done: 6359564.\n",
      " state (0)  A[0]:(0.99639749527) A[1]:(0.00118012225721) A[2]:(0.00103613489773) A[3]:(0.00138625770342)\n",
      " state (1)  A[0]:(0.0179450400174) A[1]:(0.00249891821295) A[2]:(0.00686217239127) A[3]:(0.972693860531)\n",
      " state (2)  A[0]:(0.919238269329) A[1]:(0.0048540038988) A[2]:(0.0182379018515) A[3]:(0.0576698035002)\n",
      " state (3)  A[0]:(0.999999344349) A[1]:(1.08023904488e-07) A[2]:(5.53103745915e-07) A[3]:(7.79037695898e-13)\n",
      " state (4)  A[0]:(0.99999922514) A[1]:(2.11273714967e-07) A[2]:(5.77479795538e-07) A[3]:(2.58204563416e-13)\n",
      " state (5)  A[0]:(0.999867796898) A[1]:(4.81883143948e-05) A[2]:(8.40403881739e-05) A[3]:(1.47337733825e-19)\n",
      " state (6)  A[0]:(0.535032093525) A[1]:(0.130495518446) A[2]:(0.334472417831) A[3]:(3.41581702341e-27)\n",
      " state (7)  A[0]:(0.207618057728) A[1]:(0.66005551815) A[2]:(0.132326468825) A[3]:(4.59006016977e-28)\n",
      " state (8)  A[0]:(0.0210585128516) A[1]:(0.972652256489) A[2]:(0.00628923391923) A[3]:(2.92188338819e-29)\n",
      " state (9)  A[0]:(0.000551346340217) A[1]:(0.999286592007) A[2]:(0.000162080512382) A[3]:(1.11995938975e-30)\n",
      " state (10)  A[0]:(8.48541458254e-05) A[1]:(0.999873936176) A[2]:(4.12130284531e-05) A[3]:(3.02515845683e-31)\n",
      " state (11)  A[0]:(5.60822627449e-05) A[1]:(0.999913811684) A[2]:(3.00839001284e-05) A[3]:(2.25118288359e-31)\n",
      " state (12)  A[0]:(5.05892458023e-05) A[1]:(0.999921560287) A[2]:(2.78606603388e-05) A[3]:(2.0933922802e-31)\n",
      " state (13)  A[0]:(4.89248923259e-05) A[1]:(0.999923765659) A[2]:(2.72955967375e-05) A[3]:(2.05001160158e-31)\n",
      " state (14)  A[0]:(4.81998795294e-05) A[1]:(0.999924659729) A[2]:(2.71303561021e-05) A[3]:(2.03466552283e-31)\n",
      " state (15)  A[0]:(4.77711982967e-05) A[1]:(0.999925136566) A[2]:(2.70723721769e-05) A[3]:(2.02733702085e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 532000 finished after 6 . Running score: 0.12. Policy_loss: -92050.5556003, Value_loss: 1.85407518975. Times trained:               12767. Times reached goal: 156.               Steps done: 6372331.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996039152145) A[1]:(0.00122133723926) A[2]:(0.00113635754678) A[3]:(0.0016031753039)\n",
      " state (1)  A[0]:(0.0206689760089) A[1]:(0.00261658732779) A[2]:(0.00735654169694) A[3]:(0.969357907772)\n",
      " state (2)  A[0]:(0.99998986721) A[1]:(1.72201214355e-06) A[2]:(8.41501332616e-06) A[3]:(5.57257573419e-10)\n",
      " state (3)  A[0]:(0.999999344349) A[1]:(1.2145650885e-07) A[2]:(5.52522749331e-07) A[3]:(7.14098810466e-13)\n",
      " state (4)  A[0]:(0.999999046326) A[1]:(3.00404479958e-07) A[2]:(6.34062587324e-07) A[3]:(1.70790736154e-13)\n",
      " state (5)  A[0]:(0.99978518486) A[1]:(8.81172018126e-05) A[2]:(0.000126716564409) A[3]:(3.06721349945e-20)\n",
      " state (6)  A[0]:(0.44737559557) A[1]:(0.247610583901) A[2]:(0.30501383543) A[3]:(2.10141586609e-27)\n",
      " state (7)  A[0]:(0.0390601344407) A[1]:(0.940553307533) A[2]:(0.0203865580261) A[3]:(7.30328102537e-29)\n",
      " state (8)  A[0]:(0.000370464811567) A[1]:(0.999432742596) A[2]:(0.000196804176085) A[3]:(1.06952231258e-30)\n",
      " state (9)  A[0]:(3.89652559534e-05) A[1]:(0.999930799007) A[2]:(3.02242879116e-05) A[3]:(1.90117873534e-31)\n",
      " state (10)  A[0]:(2.68473595497e-05) A[1]:(0.99995213747) A[2]:(2.10319376492e-05) A[3]:(1.38534865696e-31)\n",
      " state (11)  A[0]:(2.49524146057e-05) A[1]:(0.999955534935) A[2]:(1.95091670321e-05) A[3]:(1.29870132267e-31)\n",
      " state (12)  A[0]:(2.45393566729e-05) A[1]:(0.999956250191) A[2]:(1.91810941033e-05) A[3]:(1.27982676252e-31)\n",
      " state (13)  A[0]:(2.44371658482e-05) A[1]:(0.999956429005) A[2]:(1.91045674001e-05) A[3]:(1.27532379631e-31)\n",
      " state (14)  A[0]:(2.44089369517e-05) A[1]:(0.999956488609) A[2]:(1.90859191207e-05) A[3]:(1.27417616118e-31)\n",
      " state (15)  A[0]:(2.43995345954e-05) A[1]:(0.999956548214) A[2]:(1.90811169887e-05) A[3]:(1.27384584727e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 533000 finished after 4 . Running score: 0.11. Policy_loss: -92050.5554713, Value_loss: 1.43727159469. Times trained:               12440. Times reached goal: 124.               Steps done: 6384771.\n",
      " state (0)  A[0]:(0.99441832304) A[1]:(0.0014110065531) A[2]:(0.00138681847602) A[3]:(0.00278384867124)\n",
      " state (1)  A[0]:(0.0213695783168) A[1]:(0.00266080442816) A[2]:(0.00743777723983) A[3]:(0.968531847)\n",
      " state (2)  A[0]:(0.99999910593) A[1]:(1.43323774182e-07) A[2]:(7.43547957427e-07) A[3]:(1.45095730136e-12)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.91713823483e-07) A[2]:(5.84527697356e-07) A[3]:(3.84647364958e-13)\n",
      " state (4)  A[0]:(0.999988377094) A[1]:(8.60784803081e-06) A[2]:(3.03576871374e-06) A[3]:(1.09817967195e-16)\n",
      " state (5)  A[0]:(0.620914101601) A[1]:(0.094311453402) A[2]:(0.284774452448) A[3]:(5.25825482322e-27)\n",
      " state (6)  A[0]:(0.164246037602) A[1]:(0.680020391941) A[2]:(0.155733570457) A[3]:(3.99165086563e-28)\n",
      " state (7)  A[0]:(0.0352634191513) A[1]:(0.943396151066) A[2]:(0.0213404241949) A[3]:(6.48762802721e-29)\n",
      " state (8)  A[0]:(0.00118845619727) A[1]:(0.998280704021) A[2]:(0.00053083372768) A[3]:(2.56923173513e-30)\n",
      " state (9)  A[0]:(5.1342023653e-05) A[1]:(0.999906420708) A[2]:(4.22099255957e-05) A[3]:(2.47317687253e-31)\n",
      " state (10)  A[0]:(2.58224717982e-05) A[1]:(0.999951183796) A[2]:(2.30182049563e-05) A[3]:(1.44267798664e-31)\n",
      " state (11)  A[0]:(2.26083720918e-05) A[1]:(0.99995714426) A[2]:(2.02430674108e-05) A[3]:(1.29125397821e-31)\n",
      " state (12)  A[0]:(2.19104749704e-05) A[1]:(0.999958455563) A[2]:(1.96512901312e-05) A[3]:(1.25835447741e-31)\n",
      " state (13)  A[0]:(2.1722396923e-05) A[1]:(0.999958753586) A[2]:(1.95104221348e-05) A[3]:(1.25015399372e-31)\n",
      " state (14)  A[0]:(2.16592670768e-05) A[1]:(0.999958872795) A[2]:(1.94749554794e-05) A[3]:(1.247829101e-31)\n",
      " state (15)  A[0]:(2.16316911974e-05) A[1]:(0.9999589324) A[2]:(1.94651529455e-05) A[3]:(1.24702024333e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 534000 finished after 13 . Running score: 0.13. Policy_loss: -92050.5563946, Value_loss: 1.21467661253. Times trained:               12760. Times reached goal: 129.               Steps done: 6397531.\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0016,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.3947e-06,  7.3459e-07,  8.1155e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.3914e-06,  7.3438e-07,  8.1268e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0016,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0016,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.3721e-06,  7.3314e-07,  8.1953e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.3674e-06,  7.3286e-07,  8.2144e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.3653e-06,  7.3276e-07,  8.2243e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9914,  0.0017,  0.0016,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.3638e-06,  7.3271e-07,  8.2333e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.2649e-04,  9.9964e-01,  3.2653e-05,  4.4450e-31]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.2649e-04,  9.9964e-01,  3.2653e-05,  4.4455e-31]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.7931e-05,  9.9995e-01,  8.5903e-06,  1.1546e-31]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.2621e-04,  9.9964e-01,  3.2634e-05,  4.4440e-31]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.7908e-05,  9.9995e-01,  8.5871e-06,  1.1544e-31]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.1318e-05,  9.9997e-01,  6.1548e-06,  8.1878e-32]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.1110e-05,  9.9997e-01,  6.1431e-06,  8.1594e-32]])\n",
      "On state=14, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.1104e-05,  9.9997e-01,  6.1413e-06,  8.1587e-32]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.1300e-05,  9.9997e-01,  6.1498e-06,  8.1858e-32]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.1296e-05,  9.9997e-01,  6.1486e-06,  8.1853e-32]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.991382360458) A[1]:(0.00165229896083) A[2]:(0.00161758938339) A[3]:(0.00534773664549)\n",
      " state (1)  A[0]:(0.01779984124) A[1]:(0.00254063284956) A[2]:(0.00642152410001) A[3]:(0.973237991333)\n",
      " state (2)  A[0]:(0.99999409914) A[1]:(1.22557685245e-06) A[2]:(4.68437701784e-06) A[3]:(2.03937824961e-10)\n",
      " state (3)  A[0]:(0.999999403954) A[1]:(1.7351415238e-07) A[2]:(4.50997674761e-07) A[3]:(5.03253255041e-13)\n",
      " state (4)  A[0]:(0.999991893768) A[1]:(7.3796286415e-06) A[2]:(7.33211834358e-07) A[3]:(8.20402337272e-16)\n",
      " state (5)  A[0]:(0.705605745316) A[1]:(0.267335951328) A[2]:(0.0270583070815) A[3]:(6.56610531502e-26)\n",
      " state (6)  A[0]:(0.0680622383952) A[1]:(0.916339337826) A[2]:(0.0155984461308) A[3]:(1.22690524513e-28)\n",
      " state (7)  A[0]:(0.00775130698457) A[1]:(0.991302371025) A[2]:(0.000946306507103) A[3]:(8.4974218387e-30)\n",
      " state (8)  A[0]:(0.000324310909491) A[1]:(0.999643206596) A[2]:(3.25010478264e-05) A[3]:(4.43037661235e-31)\n",
      " state (9)  A[0]:(3.77994219889e-05) A[1]:(0.999953627586) A[2]:(8.57054055814e-06) A[3]:(1.15313186384e-31)\n",
      " state (10)  A[0]:(2.43866252276e-05) A[1]:(0.999969005585) A[2]:(6.5978119892e-06) A[3]:(8.83469463082e-32)\n",
      " state (11)  A[0]:(2.2231199182e-05) A[1]:(0.99997150898) A[2]:(6.25137090537e-06) A[3]:(8.35758851019e-32)\n",
      " state (12)  A[0]:(2.15970994759e-05) A[1]:(0.999972224236) A[2]:(6.1716896198e-06) A[3]:(8.23329702726e-32)\n",
      " state (13)  A[0]:(2.12938502955e-05) A[1]:(0.999972581863) A[2]:(6.14847522229e-06) A[3]:(8.18532392731e-32)\n",
      " state (14)  A[0]:(2.10930320463e-05) A[1]:(0.999972760677) A[2]:(6.13877364231e-06) A[3]:(8.15776916423e-32)\n",
      " state (15)  A[0]:(2.09410281968e-05) A[1]:(0.999972939491) A[2]:(6.13294650975e-06) A[3]:(8.13812665363e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 535000 finished after 41 . Running score: 0.16. Policy_loss: -92050.5562665, Value_loss: 1.43573052563. Times trained:               12345. Times reached goal: 137.               Steps done: 6409876.\n",
      " state (0)  A[0]:(0.992027401924) A[1]:(0.00167729740497) A[2]:(0.00165318406653) A[3]:(0.00464213918895)\n",
      " state (1)  A[0]:(0.019063571468) A[1]:(0.00267733470537) A[2]:(0.00686922995374) A[3]:(0.971389889717)\n",
      " state (2)  A[0]:(0.999975323677) A[1]:(5.32782496521e-06) A[2]:(1.93255236809e-05) A[3]:(5.0472155344e-09)\n",
      " state (3)  A[0]:(0.999999344349) A[1]:(1.90559447333e-07) A[2]:(4.8268378805e-07) A[3]:(4.64422174763e-13)\n",
      " state (4)  A[0]:(0.999971866608) A[1]:(2.71317912848e-05) A[2]:(1.03098932414e-06) A[3]:(1.32444361269e-16)\n",
      " state (5)  A[0]:(0.49210575223) A[1]:(0.483175188303) A[2]:(0.0247190520167) A[3]:(1.65231184459e-26)\n",
      " state (6)  A[0]:(0.0509207993746) A[1]:(0.937188446522) A[2]:(0.0118907615542) A[3]:(6.29837953049e-29)\n",
      " state (7)  A[0]:(0.00408311188221) A[1]:(0.995198011398) A[2]:(0.0007188586751) A[3]:(3.45184010159e-30)\n",
      " state (8)  A[0]:(6.90197775839e-05) A[1]:(0.999906718731) A[2]:(2.42822570726e-05) A[3]:(1.4095612545e-31)\n",
      " state (9)  A[0]:(1.99126479856e-05) A[1]:(0.999971807003) A[2]:(8.30630779092e-06) A[3]:(5.33132885026e-32)\n",
      " state (10)  A[0]:(1.64932025655e-05) A[1]:(0.999976694584) A[2]:(6.81098981659e-06) A[3]:(4.50197027211e-32)\n",
      " state (11)  A[0]:(1.58529037435e-05) A[1]:(0.999977648258) A[2]:(6.52829430692e-06) A[3]:(4.3426070336e-32)\n",
      " state (12)  A[0]:(1.56820278789e-05) A[1]:(0.999977886677) A[2]:(6.45849377179e-06) A[3]:(4.30194844724e-32)\n",
      " state (13)  A[0]:(1.56208334374e-05) A[1]:(0.999977946281) A[2]:(6.43650628263e-06) A[3]:(4.28844760075e-32)\n",
      " state (14)  A[0]:(1.55926718435e-05) A[1]:(0.999978005886) A[2]:(6.42745271762e-06) A[3]:(4.28262772822e-32)\n",
      " state (15)  A[0]:(1.55774541781e-05) A[1]:(0.999978005886) A[2]:(6.42291797703e-06) A[3]:(4.27962316466e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 536000 finished after 4 . Running score: 0.15. Policy_loss: -92050.5562829, Value_loss: 1.43880176151. Times trained:               12714. Times reached goal: 123.               Steps done: 6422590.\n",
      " state (0)  A[0]:(0.99393171072) A[1]:(0.00159483402967) A[2]:(0.0015098080039) A[3]:(0.00296367495321)\n",
      " state (1)  A[0]:(0.0221744664013) A[1]:(0.00294943782501) A[2]:(0.00744614377618) A[3]:(0.967429935932)\n",
      " state (2)  A[0]:(0.999987065792) A[1]:(2.70006762548e-06) A[2]:(1.02432932181e-05) A[3]:(9.21121567909e-10)\n",
      " state (3)  A[0]:(0.999999344349) A[1]:(1.6572252548e-07) A[2]:(5.17140904321e-07) A[3]:(4.74244666974e-13)\n",
      " state (4)  A[0]:(0.999997317791) A[1]:(2.00924228011e-06) A[2]:(6.91519403517e-07) A[3]:(4.184048839e-15)\n",
      " state (5)  A[0]:(0.943879246712) A[1]:(0.0490801781416) A[2]:(0.00704055232927) A[3]:(1.38314049209e-24)\n",
      " state (6)  A[0]:(0.16637288034) A[1]:(0.74387294054) A[2]:(0.0897541716695) A[3]:(2.77750249897e-28)\n",
      " state (7)  A[0]:(0.0618782117963) A[1]:(0.9005189538) A[2]:(0.0376028306782) A[3]:(5.21813565618e-29)\n",
      " state (8)  A[0]:(0.0169138927013) A[1]:(0.974505543709) A[2]:(0.00858054030687) A[3]:(1.1149477025e-29)\n",
      " state (9)  A[0]:(0.000898227095604) A[1]:(0.998631000519) A[2]:(0.00047074307804) A[3]:(7.61238527708e-31)\n",
      " state (10)  A[0]:(4.03140220442e-05) A[1]:(0.999925136566) A[2]:(3.45358603226e-05) A[3]:(6.82990547242e-32)\n",
      " state (11)  A[0]:(1.62645374076e-05) A[1]:(0.999969542027) A[2]:(1.42128301377e-05) A[3]:(3.14833122178e-32)\n",
      " state (12)  A[0]:(1.3220580513e-05) A[1]:(0.999975442886) A[2]:(1.13122177936e-05) A[3]:(2.59787542916e-32)\n",
      " state (13)  A[0]:(1.25240330817e-05) A[1]:(0.999976813793) A[2]:(1.06370798676e-05) A[3]:(2.4678363666e-32)\n",
      " state (14)  A[0]:(1.23348572743e-05) A[1]:(0.999977231026) A[2]:(1.04544496935e-05) A[3]:(2.43241504839e-32)\n",
      " state (15)  A[0]:(1.22801611724e-05) A[1]:(0.99997729063) A[2]:(1.04024165921e-05) A[3]:(2.42226671179e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 537000 finished after 26 . Running score: 0.09. Policy_loss: -92050.5565025, Value_loss: 1.84509164502. Times trained:               12062. Times reached goal: 98.               Steps done: 6434652.\n",
      " state (0)  A[0]:(0.99544519186) A[1]:(0.00152872293256) A[2]:(0.00141901173629) A[3]:(0.00160708336625)\n",
      " state (1)  A[0]:(0.0239382311702) A[1]:(0.00318092852831) A[2]:(0.00859431643039) A[3]:(0.964286506176)\n",
      " state (2)  A[0]:(0.999997794628) A[1]:(3.60459466719e-07) A[2]:(1.86152055903e-06) A[3]:(6.44944384356e-12)\n",
      " state (3)  A[0]:(0.999998927116) A[1]:(2.54199960636e-07) A[2]:(8.37938500808e-07) A[3]:(3.44007943876e-13)\n",
      " state (4)  A[0]:(0.999947309494) A[1]:(4.47528291261e-05) A[2]:(7.95642608864e-06) A[3]:(1.92280565299e-17)\n",
      " state (5)  A[0]:(0.15370477736) A[1]:(0.370871245861) A[2]:(0.47542399168) A[3]:(6.56982033759e-28)\n",
      " state (6)  A[0]:(0.0293238088489) A[1]:(0.808224856853) A[2]:(0.162451356649) A[3]:(3.0501289567e-29)\n",
      " state (7)  A[0]:(0.00611972389743) A[1]:(0.962474942207) A[2]:(0.0314053073525) A[3]:(5.44692936273e-30)\n",
      " state (8)  A[0]:(0.000155528599862) A[1]:(0.998676836491) A[2]:(0.00116765440907) A[3]:(2.3760502412e-31)\n",
      " state (9)  A[0]:(1.02007388705e-05) A[1]:(0.999900043011) A[2]:(8.97555073607e-05) A[3]:(2.32109543949e-32)\n",
      " state (10)  A[0]:(5.64566335015e-06) A[1]:(0.999948978424) A[2]:(4.53804750578e-05) A[3]:(1.29680610251e-32)\n",
      " state (11)  A[0]:(5.00288342664e-06) A[1]:(0.999955832958) A[2]:(3.91376961488e-05) A[3]:(1.14536973669e-32)\n",
      " state (12)  A[0]:(4.86608951178e-06) A[1]:(0.999957323074) A[2]:(3.78158583771e-05) A[3]:(1.11293057418e-32)\n",
      " state (13)  A[0]:(4.83380699734e-06) A[1]:(0.999957680702) A[2]:(3.75047056878e-05) A[3]:(1.10526466124e-32)\n",
      " state (14)  A[0]:(4.82595896756e-06) A[1]:(0.999957740307) A[2]:(3.74292394554e-05) A[3]:(1.10341112705e-32)\n",
      " state (15)  A[0]:(4.82398945678e-06) A[1]:(0.999957740307) A[2]:(3.74106821255e-05) A[3]:(1.10293982729e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 538000 finished after 11 . Running score: 0.13. Policy_loss: -92050.5561451, Value_loss: 1.42352543808. Times trained:               12387. Times reached goal: 121.               Steps done: 6447039.\n",
      " state (0)  A[0]:(0.989094078541) A[1]:(0.00164680555463) A[2]:(0.00225170212798) A[3]:(0.00700741540641)\n",
      " state (1)  A[0]:(0.0201003011316) A[1]:(0.00283275893889) A[2]:(0.00770098622888) A[3]:(0.969365954399)\n",
      " state (2)  A[0]:(0.999905824661) A[1]:(1.74033011717e-05) A[2]:(7.67000165069e-05) A[3]:(7.09334457838e-08)\n",
      " state (3)  A[0]:(0.99999910593) A[1]:(1.60599697097e-07) A[2]:(7.31848558644e-07) A[3]:(6.12418969703e-13)\n",
      " state (4)  A[0]:(0.999997913837) A[1]:(9.86488544186e-07) A[2]:(1.09611460175e-06) A[3]:(2.5056757545e-14)\n",
      " state (5)  A[0]:(0.930976867676) A[1]:(0.0210717022419) A[2]:(0.0479514524341) A[3]:(9.22263003535e-25)\n",
      " state (6)  A[0]:(0.0799651145935) A[1]:(0.632323086262) A[2]:(0.287711769342) A[3]:(9.39317381186e-29)\n",
      " state (7)  A[0]:(0.0111509719864) A[1]:(0.95597243309) A[2]:(0.0328765846789) A[3]:(7.59634090767e-30)\n",
      " state (8)  A[0]:(0.000206900222111) A[1]:(0.998999655247) A[2]:(0.000793450337369) A[3]:(2.22900154029e-31)\n",
      " state (9)  A[0]:(1.29199806906e-05) A[1]:(0.999917268753) A[2]:(6.98095900589e-05) A[3]:(2.37090980441e-32)\n",
      " state (10)  A[0]:(7.62290892453e-06) A[1]:(0.99995303154) A[2]:(3.93323061871e-05) A[3]:(1.44466801042e-32)\n",
      " state (11)  A[0]:(6.86829025653e-06) A[1]:(0.999958276749) A[2]:(3.48264766217e-05) A[3]:(1.30315568218e-32)\n",
      " state (12)  A[0]:(6.70653298585e-06) A[1]:(0.999959409237) A[2]:(3.3866839658e-05) A[3]:(1.27267819934e-32)\n",
      " state (13)  A[0]:(6.66748064759e-06) A[1]:(0.99995970726) A[2]:(3.36422854161e-05) A[3]:(1.26545566817e-32)\n",
      " state (14)  A[0]:(6.65731522531e-06) A[1]:(0.999959766865) A[2]:(3.35880504281e-05) A[3]:(1.26366127605e-32)\n",
      " state (15)  A[0]:(6.65439529257e-06) A[1]:(0.999959766865) A[2]:(3.35747245117e-05) A[3]:(1.26319857208e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 539000 finished after 8 . Running score: 0.11. Policy_loss: -92050.5561484, Value_loss: 1.84997062323. Times trained:               12880. Times reached goal: 145.               Steps done: 6459919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9861,  0.0018,  0.0024,  0.0098]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0018,  0.0024,  0.0098]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0018,  0.0024,  0.0098]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0018,  0.0024,  0.0098]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9861,  0.0018,  0.0024,  0.0098]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8699e-07,  1.2609e-06,  2.9060e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.3901e-05,  9.9891e-01,  1.0355e-03,  3.3941e-31]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.986044287682) A[1]:(0.001776706893) A[2]:(0.00240935245529) A[3]:(0.0097696473822)\n",
      " state (1)  A[0]:(0.014678388834) A[1]:(0.00261374888942) A[2]:(0.00669016363099) A[3]:(0.976017713547)\n",
      " state (2)  A[0]:(0.0324829034507) A[1]:(0.0035963091068) A[2]:(0.0100125623867) A[3]:(0.953908205032)\n",
      " state (3)  A[0]:(0.999997258186) A[1]:(4.53992811345e-07) A[2]:(2.30225032283e-06) A[3]:(9.02373488432e-12)\n",
      " state (4)  A[0]:(0.999998271465) A[1]:(4.86900205487e-07) A[2]:(1.26083511987e-06) A[3]:(2.90681460022e-13)\n",
      " state (5)  A[0]:(0.889376997948) A[1]:(0.0241647753865) A[2]:(0.0864582061768) A[3]:(3.85420903766e-23)\n",
      " state (6)  A[0]:(0.0339394882321) A[1]:(0.48955732584) A[2]:(0.476503193378) A[3]:(2.18948936468e-28)\n",
      " state (7)  A[0]:(0.00453042285517) A[1]:(0.937793135643) A[2]:(0.0576764158905) A[3]:(1.59912175093e-29)\n",
      " state (8)  A[0]:(5.39520187886e-05) A[1]:(0.998909652233) A[2]:(0.00103642418981) A[3]:(3.39693370085e-31)\n",
      " state (9)  A[0]:(3.72183217223e-06) A[1]:(0.999918580055) A[2]:(7.7711250924e-05) A[3]:(3.29462619572e-32)\n",
      " state (10)  A[0]:(2.20898323278e-06) A[1]:(0.999955773354) A[2]:(4.20156939072e-05) A[3]:(1.95498052403e-32)\n",
      " state (11)  A[0]:(1.98559928322e-06) A[1]:(0.999961197376) A[2]:(3.68299697584e-05) A[3]:(1.75078554728e-32)\n",
      " state (12)  A[0]:(1.93622690858e-06) A[1]:(0.999962389469) A[2]:(3.56971468136e-05) A[3]:(1.7055526716e-32)\n",
      " state (13)  A[0]:(1.92371157937e-06) A[1]:(0.999962687492) A[2]:(3.54144940502e-05) A[3]:(1.69415375594e-32)\n",
      " state (14)  A[0]:(1.92023640011e-06) A[1]:(0.999962747097) A[2]:(3.53381146851e-05) A[3]:(1.69102870421e-32)\n",
      " state (15)  A[0]:(1.91913818526e-06) A[1]:(0.999962747097) A[2]:(3.53152063326e-05) A[3]:(1.69006156623e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 540000 finished after 7 . Running score: 0.15. Policy_loss: -92050.6251901, Value_loss: 1.83907372412. Times trained:               13170. Times reached goal: 129.               Steps done: 6473089.\n",
      " state (0)  A[0]:(0.993181526661) A[1]:(0.00158872967586) A[2]:(0.00191569945309) A[3]:(0.00331404618919)\n",
      " state (1)  A[0]:(0.0157737042755) A[1]:(0.00272045028396) A[2]:(0.00693731335923) A[3]:(0.974568545818)\n",
      " state (2)  A[0]:(0.0452887564898) A[1]:(0.00424901582301) A[2]:(0.0118499211967) A[3]:(0.938612282276)\n",
      " state (3)  A[0]:(0.99999833107) A[1]:(2.7715080364e-07) A[2]:(1.39991232118e-06) A[3]:(2.57769456821e-12)\n",
      " state (4)  A[0]:(0.999997079372) A[1]:(1.21373511774e-06) A[2]:(1.68191729699e-06) A[3]:(4.72335895169e-14)\n",
      " state (5)  A[0]:(0.434093385935) A[1]:(0.120451442897) A[2]:(0.445455163717) A[3]:(1.23800305243e-25)\n",
      " state (6)  A[0]:(0.043925575912) A[1]:(0.554532825947) A[2]:(0.401541590691) A[3]:(9.03571158453e-29)\n",
      " state (7)  A[0]:(0.0132965035737) A[1]:(0.896891176701) A[2]:(0.0898123309016) A[3]:(1.57962171024e-29)\n",
      " state (8)  A[0]:(0.000919826212339) A[1]:(0.994840145111) A[2]:(0.00424004672095) A[3]:(9.44226933965e-31)\n",
      " state (9)  A[0]:(2.05996129807e-05) A[1]:(0.999808430672) A[2]:(0.000170949599124) A[3]:(4.60089517469e-32)\n",
      " state (10)  A[0]:(6.0207498791e-06) A[1]:(0.999934434891) A[2]:(5.95231176703e-05) A[3]:(1.73840024493e-32)\n",
      " state (11)  A[0]:(4.76985223941e-06) A[1]:(0.99994802475) A[2]:(4.72160863865e-05) A[3]:(1.4161674152e-32)\n",
      " state (12)  A[0]:(4.51577625427e-06) A[1]:(0.999950766563) A[2]:(4.46904596174e-05) A[3]:(1.34899202728e-32)\n",
      " state (13)  A[0]:(4.44541274192e-06) A[1]:(0.999951481819) A[2]:(4.40770891146e-05) A[3]:(1.33190430671e-32)\n",
      " state (14)  A[0]:(4.41878455604e-06) A[1]:(0.999951660633) A[2]:(4.3922173063e-05) A[3]:(1.32688391728e-32)\n",
      " state (15)  A[0]:(4.4039088607e-06) A[1]:(0.999951720238) A[2]:(4.38853348896e-05) A[3]:(1.32499239993e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 541000 finished after 5 . Running score: 0.08. Policy_loss: -92050.6251661, Value_loss: 1.42376020738. Times trained:               12555. Times reached goal: 127.               Steps done: 6485644.\n",
      " state (0)  A[0]:(0.993631184101) A[1]:(0.00170864013489) A[2]:(0.0018504189793) A[3]:(0.00280974106863)\n",
      " state (1)  A[0]:(0.0187588203698) A[1]:(0.00308366585523) A[2]:(0.00749237881973) A[3]:(0.970665156841)\n",
      " state (2)  A[0]:(0.998231172562) A[1]:(0.000390617962694) A[2]:(0.00131447962485) A[3]:(6.37438861304e-05)\n",
      " state (3)  A[0]:(0.999998986721) A[1]:(2.11027398223e-07) A[2]:(8.22458105176e-07) A[3]:(5.0283377722e-13)\n",
      " state (4)  A[0]:(0.999987900257) A[1]:(8.8952419901e-06) A[2]:(3.219014161e-06) A[3]:(9.68814897488e-16)\n",
      " state (5)  A[0]:(0.213824734092) A[1]:(0.292474538088) A[2]:(0.493700742722) A[3]:(2.46813546089e-27)\n",
      " state (6)  A[0]:(0.0175647288561) A[1]:(0.886850357056) A[2]:(0.0955849215388) A[3]:(1.37592557921e-29)\n",
      " state (7)  A[0]:(0.00133082654793) A[1]:(0.9940751791) A[2]:(0.00459400843829) A[3]:(7.4308085455e-31)\n",
      " state (8)  A[0]:(2.04846455745e-05) A[1]:(0.999861359596) A[2]:(0.00011815517064) A[3]:(2.36672915875e-32)\n",
      " state (9)  A[0]:(4.3100953917e-06) A[1]:(0.999964594841) A[2]:(3.11089606839e-05) A[3]:(6.85614573894e-33)\n",
      " state (10)  A[0]:(3.37799133376e-06) A[1]:(0.999972462654) A[2]:(2.41765410465e-05) A[3]:(5.4884286744e-33)\n",
      " state (11)  A[0]:(3.2175078104e-06) A[1]:(0.999973833561) A[2]:(2.29717097682e-05) A[3]:(5.24670038343e-33)\n",
      " state (12)  A[0]:(3.17862895827e-06) A[1]:(0.99997407198) A[2]:(2.27228010772e-05) A[3]:(5.19400958384e-33)\n",
      " state (13)  A[0]:(3.1655119983e-06) A[1]:(0.999974131584) A[2]:(2.26739666687e-05) A[3]:(5.18114710425e-33)\n",
      " state (14)  A[0]:(3.15873307954e-06) A[1]:(0.999974191189) A[2]:(2.26689517149e-05) A[3]:(5.17735393092e-33)\n",
      " state (15)  A[0]:(3.1540373584e-06) A[1]:(0.999974191189) A[2]:(2.26736210607e-05) A[3]:(5.17577399304e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 542000 finished after 7 . Running score: 0.11. Policy_loss: -92050.612474, Value_loss: 1.19909503135. Times trained:               12457. Times reached goal: 116.               Steps done: 6498101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995870351791) A[1]:(0.00147080875468) A[2]:(0.00144896050915) A[3]:(0.00120988488197)\n",
      " state (1)  A[0]:(0.0200319699943) A[1]:(0.00290618557483) A[2]:(0.00707830814645) A[3]:(0.969983518124)\n",
      " state (2)  A[0]:(0.264190524817) A[1]:(0.00893963128328) A[2]:(0.0247559826821) A[3]:(0.702113866806)\n",
      " state (3)  A[0]:(0.999999403954) A[1]:(1.01279141518e-07) A[2]:(4.68670975806e-07) A[3]:(4.91806465344e-13)\n",
      " state (4)  A[0]:(0.999999403954) A[1]:(1.64140672609e-07) A[2]:(4.17174675249e-07) A[3]:(1.32845936942e-13)\n",
      " state (5)  A[0]:(0.999960005283) A[1]:(3.15669021802e-05) A[2]:(8.42475947138e-06) A[3]:(4.85503837768e-19)\n",
      " state (6)  A[0]:(0.715943038464) A[1]:(0.145642533898) A[2]:(0.138414382935) A[3]:(3.58609479722e-28)\n",
      " state (7)  A[0]:(0.319911599159) A[1]:(0.573176443577) A[2]:(0.106911979616) A[3]:(2.94106805785e-29)\n",
      " state (8)  A[0]:(0.181301221251) A[1]:(0.764898121357) A[2]:(0.053800649941) A[3]:(1.14967312218e-29)\n",
      " state (9)  A[0]:(0.0827154219151) A[1]:(0.89835780859) A[2]:(0.0189267359674) A[3]:(4.14007455264e-30)\n",
      " state (10)  A[0]:(0.0161422602832) A[1]:(0.981572926044) A[2]:(0.00228483532555) A[3]:(6.17560721414e-31)\n",
      " state (11)  A[0]:(0.00141189212445) A[1]:(0.998421669006) A[2]:(0.000166447789525) A[3]:(5.81151065186e-32)\n",
      " state (12)  A[0]:(0.000182015675819) A[1]:(0.999780654907) A[2]:(3.73164111807e-05) A[3]:(1.35410351753e-32)\n",
      " state (13)  A[0]:(8.18486951175e-05) A[1]:(0.999895751476) A[2]:(2.24091618293e-05) A[3]:(8.10103980686e-33)\n",
      " state (14)  A[0]:(6.46154876449e-05) A[1]:(0.999916195869) A[2]:(1.91594099306e-05) A[3]:(6.93265940091e-33)\n",
      " state (15)  A[0]:(6.00849307375e-05) A[1]:(0.999921679497) A[2]:(1.82235780812e-05) A[3]:(6.60000184647e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 543000 finished after 8 . Running score: 0.09. Policy_loss: -92050.6307642, Value_loss: 0.994558920942. Times trained:               12520. Times reached goal: 124.               Steps done: 6510621.\n",
      " state (0)  A[0]:(0.995770037174) A[1]:(0.00148650282063) A[2]:(0.0015772349434) A[3]:(0.00116620433982)\n",
      " state (1)  A[0]:(0.0225448086858) A[1]:(0.00313959433697) A[2]:(0.00737547269091) A[3]:(0.966940104961)\n",
      " state (2)  A[0]:(0.999944269657) A[1]:(1.31461420096e-05) A[2]:(4.25379694207e-05) A[3]:(3.09130925302e-08)\n",
      " state (3)  A[0]:(0.999999403954) A[1]:(1.48634768493e-07) A[2]:(4.18696828319e-07) A[3]:(3.26441049756e-13)\n",
      " state (4)  A[0]:(0.99999666214) A[1]:(2.83418125946e-06) A[2]:(5.17434045832e-07) A[3]:(1.47181747228e-15)\n",
      " state (5)  A[0]:(0.723165810108) A[1]:(0.258123368025) A[2]:(0.0187108088285) A[3]:(5.71282975688e-27)\n",
      " state (6)  A[0]:(0.0332573801279) A[1]:(0.95958173275) A[2]:(0.00716087501496) A[3]:(3.14829228941e-30)\n",
      " state (7)  A[0]:(0.0104384506121) A[1]:(0.98769813776) A[2]:(0.0018634183798) A[3]:(7.13330832263e-31)\n",
      " state (8)  A[0]:(0.00331685622223) A[1]:(0.996221542358) A[2]:(0.000461623538285) A[3]:(1.98683042591e-31)\n",
      " state (9)  A[0]:(0.000359102326911) A[1]:(0.999592602253) A[2]:(4.82696013933e-05) A[3]:(2.55141754143e-32)\n",
      " state (10)  A[0]:(2.74330359389e-05) A[1]:(0.999964416027) A[2]:(8.14850955067e-06) A[3]:(4.41872991717e-33)\n",
      " state (11)  A[0]:(1.23164973047e-05) A[1]:(0.999983012676) A[2]:(4.66838173452e-06) A[3]:(2.56358530386e-33)\n",
      " state (12)  A[0]:(1.04368436951e-05) A[1]:(0.999985456467) A[2]:(4.0783120312e-06) A[3]:(2.25984307373e-33)\n",
      " state (13)  A[0]:(9.99602161755e-06) A[1]:(0.999986052513) A[2]:(3.93518939745e-06) A[3]:(2.18584239827e-33)\n",
      " state (14)  A[0]:(9.85754377325e-06) A[1]:(0.999986231327) A[2]:(3.89614478991e-06) A[3]:(2.16456760356e-33)\n",
      " state (15)  A[0]:(9.80036475084e-06) A[1]:(0.999986290932) A[2]:(3.88520675187e-06) A[3]:(2.15757690192e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 544000 finished after 11 . Running score: 0.15. Policy_loss: -92050.6124177, Value_loss: 1.64630289389. Times trained:               12903. Times reached goal: 129.               Steps done: 6523524.\n",
      "action_dist \n",
      "tensor([[ 0.9941,  0.0015,  0.0019,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.6027e-07,  5.0546e-07,  8.7901e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.6027e-07,  5.0546e-07,  8.7902e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9941,  0.0015,  0.0019,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.6026e-07,  5.0545e-07,  8.7904e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.5622e-04,  9.9900e-01,  2.4265e-04,  5.8904e-32]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994111895561) A[1]:(0.00151208043098) A[2]:(0.00186935719103) A[3]:(0.00250664772466)\n",
      " state (1)  A[0]:(0.0244146212935) A[1]:(0.00308400089853) A[2]:(0.0078024812974) A[3]:(0.964698910713)\n",
      " state (2)  A[0]:(0.999986469746) A[1]:(2.75596016763e-06) A[2]:(1.0760087207e-05) A[3]:(9.20169329621e-10)\n",
      " state (3)  A[0]:(0.999999403954) A[1]:(1.31895376398e-07) A[2]:(4.58218693211e-07) A[3]:(3.54907780682e-13)\n",
      " state (4)  A[0]:(0.999998509884) A[1]:(9.60178681453e-07) A[2]:(5.05454408994e-07) A[3]:(8.79136004175e-15)\n",
      " state (5)  A[0]:(0.971167623997) A[1]:(0.0235994048417) A[2]:(0.00523299910128) A[3]:(8.2908707982e-26)\n",
      " state (6)  A[0]:(0.0684674456716) A[1]:(0.909388780594) A[2]:(0.0221437457949) A[3]:(6.24358125643e-30)\n",
      " state (7)  A[0]:(0.0101271858439) A[1]:(0.986881673336) A[2]:(0.00299114780501) A[3]:(6.30861392973e-31)\n",
      " state (8)  A[0]:(0.000757072644774) A[1]:(0.999000072479) A[2]:(0.000242883092142) A[3]:(5.89568897781e-32)\n",
      " state (9)  A[0]:(3.30291913997e-05) A[1]:(0.999946832657) A[2]:(2.01096299861e-05) A[3]:(5.47097993013e-33)\n",
      " state (10)  A[0]:(1.34329056891e-05) A[1]:(0.999977827072) A[2]:(8.71269548952e-06) A[3]:(2.56325928785e-33)\n",
      " state (11)  A[0]:(1.13495443657e-05) A[1]:(0.999981403351) A[2]:(7.24520805306e-06) A[3]:(2.18333216682e-33)\n",
      " state (12)  A[0]:(1.09237962533e-05) A[1]:(0.999982118607) A[2]:(6.9356719905e-06) A[3]:(2.10276947782e-33)\n",
      " state (13)  A[0]:(1.08197446025e-05) A[1]:(0.999982297421) A[2]:(6.85984059601e-06) A[3]:(2.08297047953e-33)\n",
      " state (14)  A[0]:(1.07929099613e-05) A[1]:(0.999982357025) A[2]:(6.8405302045e-06) A[3]:(2.07789142558e-33)\n",
      " state (15)  A[0]:(1.07858304546e-05) A[1]:(0.999982357025) A[2]:(6.83557391312e-06) A[3]:(2.07659195331e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 545000 finished after 6 . Running score: 0.13. Policy_loss: -92050.6398395, Value_loss: 1.40695606766. Times trained:               12881. Times reached goal: 136.               Steps done: 6536405.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996155261993) A[1]:(0.00139700295404) A[2]:(0.00155644060578) A[3]:(0.000891272095032)\n",
      " state (1)  A[0]:(0.0282077211887) A[1]:(0.00344557757489) A[2]:(0.00857735052705) A[3]:(0.959769368172)\n",
      " state (2)  A[0]:(0.999998450279) A[1]:(2.9869397622e-07) A[2]:(1.25650683458e-06) A[3]:(4.09710329899e-12)\n",
      " state (3)  A[0]:(0.999999344349) A[1]:(1.85058340207e-07) A[2]:(4.89617264066e-07) A[3]:(2.21342421388e-13)\n",
      " state (4)  A[0]:(0.999977886677) A[1]:(2.08449346246e-05) A[2]:(1.25108374505e-06) A[3]:(2.47894449054e-17)\n",
      " state (5)  A[0]:(0.266834557056) A[1]:(0.669870734215) A[2]:(0.0632946789265) A[3]:(9.13749397283e-29)\n",
      " state (6)  A[0]:(0.0179508812726) A[1]:(0.974202930927) A[2]:(0.00784616917372) A[3]:(9.65258220613e-31)\n",
      " state (7)  A[0]:(0.00392097607255) A[1]:(0.994663834572) A[2]:(0.00141520565376) A[3]:(1.78653418189e-31)\n",
      " state (8)  A[0]:(0.000244720751652) A[1]:(0.999647915363) A[2]:(0.000107367515739) A[3]:(1.60345099553e-32)\n",
      " state (9)  A[0]:(1.68535079865e-05) A[1]:(0.999969244003) A[2]:(1.39073335959e-05) A[3]:(2.25097911164e-33)\n",
      " state (10)  A[0]:(9.38545417739e-06) A[1]:(0.999982535839) A[2]:(8.08308504929e-06) A[3]:(1.37540544044e-33)\n",
      " state (11)  A[0]:(8.4270213847e-06) A[1]:(0.999984383583) A[2]:(7.20823345546e-06) A[3]:(1.2437820319e-33)\n",
      " state (12)  A[0]:(8.22399761091e-06) A[1]:(0.999984741211) A[2]:(7.01902536093e-06) A[3]:(1.2152113662e-33)\n",
      " state (13)  A[0]:(8.17429918243e-06) A[1]:(0.99998486042) A[2]:(6.9738152888e-06) A[3]:(1.20831479595e-33)\n",
      " state (14)  A[0]:(8.16118154034e-06) A[1]:(0.99998486042) A[2]:(6.96280994816e-06) A[3]:(1.20660132926e-33)\n",
      " state (15)  A[0]:(8.15738439996e-06) A[1]:(0.99998486042) A[2]:(6.96015467838e-06) A[3]:(1.20615950869e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 546000 finished after 9 . Running score: 0.15. Policy_loss: -92050.6125036, Value_loss: 1.40990397977. Times trained:               12808. Times reached goal: 129.               Steps done: 6549213.\n",
      " state (0)  A[0]:(0.992552280426) A[1]:(0.00160147098359) A[2]:(0.00205052620731) A[3]:(0.00379570829682)\n",
      " state (1)  A[0]:(0.0254787709564) A[1]:(0.0031834859401) A[2]:(0.00830143224448) A[3]:(0.963036298752)\n",
      " state (2)  A[0]:(0.999950349331) A[1]:(9.22733761399e-06) A[2]:(4.03772974096e-05) A[3]:(1.79613266482e-08)\n",
      " state (3)  A[0]:(0.999999344349) A[1]:(9.92753257378e-08) A[2]:(5.32792455488e-07) A[3]:(4.1521343655e-13)\n",
      " state (4)  A[0]:(0.999999344349) A[1]:(1.33567766625e-07) A[2]:(5.18572619512e-07) A[3]:(2.40417115469e-13)\n",
      " state (5)  A[0]:(0.999998629093) A[1]:(6.8929483632e-07) A[2]:(6.70704878303e-07) A[3]:(7.42478842487e-15)\n",
      " state (6)  A[0]:(0.99854105711) A[1]:(0.000700275239069) A[2]:(0.00075865117833) A[3]:(7.54106296978e-24)\n",
      " state (7)  A[0]:(0.408766806126) A[1]:(0.3561629951) A[2]:(0.235070168972) A[3]:(3.87137076282e-29)\n",
      " state (8)  A[0]:(0.0518246926367) A[1]:(0.914513409138) A[2]:(0.0336619243026) A[3]:(2.02805444856e-30)\n",
      " state (9)  A[0]:(0.00139704148751) A[1]:(0.997550308704) A[2]:(0.00105266936589) A[3]:(7.08813865915e-32)\n",
      " state (10)  A[0]:(6.69650762575e-05) A[1]:(0.999851465225) A[2]:(8.15735838842e-05) A[3]:(6.28705072014e-33)\n",
      " state (11)  A[0]:(3.15630823025e-05) A[1]:(0.999930500984) A[2]:(3.796299643e-05) A[3]:(3.17124057812e-33)\n",
      " state (12)  A[0]:(2.67054110736e-05) A[1]:(0.999941885471) A[2]:(3.14126918965e-05) A[3]:(2.68882100263e-33)\n",
      " state (13)  A[0]:(2.55724316958e-05) A[1]:(0.99994456768) A[2]:(2.98698414554e-05) A[3]:(2.57401083672e-33)\n",
      " state (14)  A[0]:(2.52649770118e-05) A[1]:(0.999945282936) A[2]:(2.94553865388e-05) A[3]:(2.54293774558e-33)\n",
      " state (15)  A[0]:(2.51765650319e-05) A[1]:(0.99994546175) A[2]:(2.93395478366e-05) A[3]:(2.53416470064e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 547000 finished after 12 . Running score: 0.11. Policy_loss: -92050.6199691, Value_loss: 1.20820299125. Times trained:               12869. Times reached goal: 127.               Steps done: 6562082.\n",
      " state (0)  A[0]:(0.990583658218) A[1]:(0.00172497425228) A[2]:(0.0023480609525) A[3]:(0.00534329051152)\n",
      " state (1)  A[0]:(0.0233073867857) A[1]:(0.00310090300627) A[2]:(0.00838932674378) A[3]:(0.965202391148)\n",
      " state (2)  A[0]:(0.999995291233) A[1]:(7.26417908936e-07) A[2]:(3.99148666475e-06) A[3]:(4.21850575305e-11)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.14557501263e-07) A[2]:(6.40796315565e-07) A[3]:(4.13937899799e-13)\n",
      " state (4)  A[0]:(0.99999910593) A[1]:(2.16428134081e-07) A[2]:(6.83792109157e-07) A[3]:(1.23712915997e-13)\n",
      " state (5)  A[0]:(0.999982953072) A[1]:(1.02274771052e-05) A[2]:(6.80420271237e-06) A[3]:(3.74092390668e-18)\n",
      " state (6)  A[0]:(0.549474537373) A[1]:(0.0791037678719) A[2]:(0.371421664953) A[3]:(2.12523961951e-28)\n",
      " state (7)  A[0]:(0.0707487463951) A[1]:(0.807415723801) A[2]:(0.121835522354) A[3]:(4.81635732944e-30)\n",
      " state (8)  A[0]:(0.000998395611532) A[1]:(0.996703743935) A[2]:(0.00229786848649) A[3]:(9.87058609705e-32)\n",
      " state (9)  A[0]:(4.20243159169e-05) A[1]:(0.999840438366) A[2]:(0.000117538627819) A[3]:(6.44590114923e-33)\n",
      " state (10)  A[0]:(2.16205444303e-05) A[1]:(0.999924242496) A[2]:(5.41241715837e-05) A[3]:(3.30202901833e-33)\n",
      " state (11)  A[0]:(1.89281799976e-05) A[1]:(0.999935209751) A[2]:(4.58519316453e-05) A[3]:(2.86892767566e-33)\n",
      " state (12)  A[0]:(1.83586053026e-05) A[1]:(0.999937534332) A[2]:(4.4116506615e-05) A[3]:(2.77689289648e-33)\n",
      " state (13)  A[0]:(1.82229050552e-05) A[1]:(0.999938070774) A[2]:(4.37053058704e-05) A[3]:(2.75499013065e-33)\n",
      " state (14)  A[0]:(1.81895011337e-05) A[1]:(0.999938189983) A[2]:(4.36047266703e-05) A[3]:(2.74961499906e-33)\n",
      " state (15)  A[0]:(1.81811083166e-05) A[1]:(0.999938249588) A[2]:(4.35801157437e-05) A[3]:(2.74829366994e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 548000 finished after 6 . Running score: 0.12. Policy_loss: -92050.6863467, Value_loss: 1.19576760289. Times trained:               12740. Times reached goal: 111.               Steps done: 6574822.\n",
      " state (0)  A[0]:(0.99466830492) A[1]:(0.00168475799728) A[2]:(0.00184454559349) A[3]:(0.00180241302587)\n",
      " state (1)  A[0]:(0.0233465973288) A[1]:(0.00347079336643) A[2]:(0.00827073305845) A[3]:(0.964911878109)\n",
      " state (2)  A[0]:(0.999996840954) A[1]:(5.59099930797e-07) A[2]:(2.61683385361e-06) A[3]:(1.64803084374e-11)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.44311442796e-07) A[2]:(6.25404538823e-07) A[3]:(3.73439262369e-13)\n",
      " state (4)  A[0]:(0.999998688698) A[1]:(5.52651499675e-07) A[2]:(7.68477207203e-07) A[3]:(2.70956251414e-14)\n",
      " state (5)  A[0]:(0.998557388783) A[1]:(0.000713725574315) A[2]:(0.000728874991182) A[3]:(2.61919175473e-23)\n",
      " state (6)  A[0]:(0.13564170897) A[1]:(0.694868087769) A[2]:(0.169490218163) A[3]:(9.98993403849e-30)\n",
      " state (7)  A[0]:(0.00284922658466) A[1]:(0.99411469698) A[2]:(0.00303608831018) A[3]:(1.22631755438e-31)\n",
      " state (8)  A[0]:(3.18970487569e-05) A[1]:(0.999900519848) A[2]:(6.75616029184e-05) A[3]:(3.23156988694e-33)\n",
      " state (9)  A[0]:(1.01880095826e-05) A[1]:(0.999967396259) A[2]:(2.24006798817e-05) A[3]:(1.18792832599e-33)\n",
      " state (10)  A[0]:(8.56714450492e-06) A[1]:(0.999973058701) A[2]:(1.83801384992e-05) A[3]:(9.99807536542e-34)\n",
      " state (11)  A[0]:(8.28085740068e-06) A[1]:(0.99997407198) A[2]:(1.7664699044e-05) A[3]:(9.65925289412e-34)\n",
      " state (12)  A[0]:(8.21889352665e-06) A[1]:(0.999974250793) A[2]:(1.75157383637e-05) A[3]:(9.58759641154e-34)\n",
      " state (13)  A[0]:(8.20401510282e-06) A[1]:(0.999974310398) A[2]:(1.74842316483e-05) A[3]:(9.5717373397e-34)\n",
      " state (14)  A[0]:(8.19982233224e-06) A[1]:(0.999974310398) A[2]:(1.7478096197e-05) A[3]:(9.56808687873e-34)\n",
      " state (15)  A[0]:(8.19835167931e-06) A[1]:(0.999974310398) A[2]:(1.74772285391e-05) A[3]:(9.5672107681e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 549000 finished after 14 . Running score: 0.16. Policy_loss: -92050.6125507, Value_loss: 1.20726442477. Times trained:               12652. Times reached goal: 131.               Steps done: 6587474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9956,  0.0017,  0.0016,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0017,  0.0016,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0017,  0.0016,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0017,  0.0016,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0017,  0.0016,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0017,  0.0016,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0017,  0.0016,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0017,  0.0016,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  5.7547e-06,  1.0036e-06,  8.1520e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.8522e-06,  9.9999e-01,  4.5422e-06,  4.0599e-34]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.8524e-06,  9.9999e-01,  4.5426e-06,  4.0603e-34]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995575368404) A[1]:(0.00166497007012) A[2]:(0.00163774297107) A[3]:(0.00112190807704)\n",
      " state (1)  A[0]:(0.0348842702806) A[1]:(0.00426750723273) A[2]:(0.00973086152226) A[3]:(0.95111733675)\n",
      " state (2)  A[0]:(0.999999165535) A[1]:(1.69146517237e-07) A[2]:(6.54269797451e-07) A[3]:(6.6406992752e-13)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(2.32546753409e-07) A[2]:(5.63930370845e-07) A[3]:(2.64965214483e-13)\n",
      " state (4)  A[0]:(0.999993264675) A[1]:(5.74304431211e-06) A[2]:(1.0030064459e-06) A[3]:(8.18962010684e-16)\n",
      " state (5)  A[0]:(0.468381971121) A[1]:(0.465366780758) A[2]:(0.0662512630224) A[3]:(9.56961559667e-28)\n",
      " state (6)  A[0]:(0.00302393222228) A[1]:(0.994863271713) A[2]:(0.00211280141957) A[3]:(1.53110619488e-31)\n",
      " state (7)  A[0]:(3.93629015889e-05) A[1]:(0.999925673008) A[2]:(3.49527072103e-05) A[3]:(2.87545222032e-33)\n",
      " state (8)  A[0]:(2.85301166514e-06) A[1]:(0.999992609024) A[2]:(4.54359542346e-06) A[3]:(4.06098859898e-34)\n",
      " state (9)  A[0]:(2.01684338208e-06) A[1]:(0.999994814396) A[2]:(3.19596006193e-06) A[3]:(2.9649978443e-34)\n",
      " state (10)  A[0]:(1.91084291146e-06) A[1]:(0.999995052814) A[2]:(3.01113595924e-06) A[3]:(2.81376544512e-34)\n",
      " state (11)  A[0]:(1.8904696617e-06) A[1]:(0.999995112419) A[2]:(2.97714541375e-06) A[3]:(2.78537978202e-34)\n",
      " state (12)  A[0]:(1.88567287296e-06) A[1]:(0.999995172024) A[2]:(2.97056590171e-06) A[3]:(2.77945754046e-34)\n",
      " state (13)  A[0]:(1.88417732261e-06) A[1]:(0.999995172024) A[2]:(2.96935377264e-06) A[3]:(2.77807931925e-34)\n",
      " state (14)  A[0]:(1.88350179542e-06) A[1]:(0.999995172024) A[2]:(2.96918392451e-06) A[3]:(2.77765549844e-34)\n",
      " state (15)  A[0]:(1.88310661997e-06) A[1]:(0.999995172024) A[2]:(2.9692064345e-06) A[3]:(2.77744358803e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 550000 finished after 11 . Running score: 0.15. Policy_loss: -92050.6125144, Value_loss: 0.987057255586. Times trained:               12504. Times reached goal: 129.               Steps done: 6599978.\n",
      " state (0)  A[0]:(0.994317054749) A[1]:(0.00209596171044) A[2]:(0.00193786004093) A[3]:(0.00164914969355)\n",
      " state (1)  A[0]:(0.0268804524094) A[1]:(0.00414718035609) A[2]:(0.00948932953179) A[3]:(0.959483027458)\n",
      " state (2)  A[0]:(0.999995172024) A[1]:(8.736871564e-07) A[2]:(3.95224014937e-06) A[3]:(2.72636913046e-11)\n",
      " state (3)  A[0]:(0.999998927116) A[1]:(1.80913858117e-07) A[2]:(8.77011245848e-07) A[3]:(5.83648960325e-13)\n",
      " state (4)  A[0]:(0.999998807907) A[1]:(2.4495855655e-07) A[2]:(9.4980839549e-07) A[3]:(3.79159350401e-13)\n",
      " state (5)  A[0]:(0.999984323978) A[1]:(2.91594119517e-06) A[2]:(1.27630701172e-05) A[3]:(5.80455638067e-16)\n",
      " state (6)  A[0]:(0.338352113962) A[1]:(0.0355727858841) A[2]:(0.626075088978) A[3]:(2.84387399301e-27)\n",
      " state (7)  A[0]:(0.0221042912453) A[1]:(0.785883903503) A[2]:(0.19201181829) A[3]:(6.09161372291e-30)\n",
      " state (8)  A[0]:(0.00126250553876) A[1]:(0.988699197769) A[2]:(0.0100383125246) A[3]:(2.23669350512e-31)\n",
      " state (9)  A[0]:(2.06510212593e-05) A[1]:(0.99972987175) A[2]:(0.000249480333878) A[3]:(6.28599791801e-33)\n",
      " state (10)  A[0]:(2.12184727388e-06) A[1]:(0.999970734119) A[2]:(2.7140507882e-05) A[3]:(8.23534995468e-34)\n",
      " state (11)  A[0]:(1.25870019474e-06) A[1]:(0.999983966351) A[2]:(1.47673836182e-05) A[3]:(4.83381001309e-34)\n",
      " state (12)  A[0]:(1.11437930173e-06) A[1]:(0.999986171722) A[2]:(1.27155062728e-05) A[3]:(4.24689991823e-34)\n",
      " state (13)  A[0]:(1.07930327431e-06) A[1]:(0.999986708164) A[2]:(1.22218598335e-05) A[3]:(4.10388404088e-34)\n",
      " state (14)  A[0]:(1.06971197056e-06) A[1]:(0.999986827374) A[2]:(1.20889690152e-05) A[3]:(4.06505645228e-34)\n",
      " state (15)  A[0]:(1.06692459667e-06) A[1]:(0.999986886978) A[2]:(1.20517652249e-05) A[3]:(4.05396915283e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 551000 finished after 16 . Running score: 0.16. Policy_loss: -92050.6171174, Value_loss: 1.40707324846. Times trained:               13116. Times reached goal: 125.               Steps done: 6613094.\n",
      " state (0)  A[0]:(0.994598805904) A[1]:(0.00207404582761) A[2]:(0.00181822071318) A[3]:(0.00150894222315)\n",
      " state (1)  A[0]:(0.02633619681) A[1]:(0.00412505632266) A[2]:(0.00894780177623) A[3]:(0.960590958595)\n",
      " state (2)  A[0]:(0.999997437) A[1]:(4.9058149898e-07) A[2]:(2.09966628972e-06) A[3]:(7.34557119991e-12)\n",
      " state (3)  A[0]:(0.999999046326) A[1]:(1.73216150756e-07) A[2]:(7.53013182475e-07) A[3]:(5.30302743992e-13)\n",
      " state (4)  A[0]:(0.999998867512) A[1]:(2.79803231251e-07) A[2]:(8.78990647379e-07) A[3]:(2.57952920092e-13)\n",
      " state (5)  A[0]:(0.999791562557) A[1]:(1.75218610821e-05) A[2]:(0.000190915932762) A[3]:(5.91206921801e-19)\n",
      " state (6)  A[0]:(0.233157396317) A[1]:(0.218012452126) A[2]:(0.548830151558) A[3]:(2.32630044643e-28)\n",
      " state (7)  A[0]:(0.00917931459844) A[1]:(0.961397886276) A[2]:(0.0294228121638) A[3]:(1.04917572788e-30)\n",
      " state (8)  A[0]:(0.000218807646888) A[1]:(0.999207913876) A[2]:(0.000573251163587) A[3]:(2.12557766817e-32)\n",
      " state (9)  A[0]:(5.18946990269e-06) A[1]:(0.999970912933) A[2]:(2.38970260398e-05) A[3]:(1.03088163787e-33)\n",
      " state (10)  A[0]:(2.00886097446e-06) A[1]:(0.999988675117) A[2]:(9.31634531298e-06) A[3]:(4.39910669202e-34)\n",
      " state (11)  A[0]:(1.68260442024e-06) A[1]:(0.999990701675) A[2]:(7.6145638559e-06) A[3]:(3.68627015771e-34)\n",
      " state (12)  A[0]:(1.61519767516e-06) A[1]:(0.999991118908) A[2]:(7.26055077394e-06) A[3]:(3.53605759171e-34)\n",
      " state (13)  A[0]:(1.59805586009e-06) A[1]:(0.999991238117) A[2]:(7.17358261682e-06) A[3]:(3.49848975653e-34)\n",
      " state (14)  A[0]:(1.59307091963e-06) A[1]:(0.999991238117) A[2]:(7.15071473678e-06) A[3]:(3.48822874695e-34)\n",
      " state (15)  A[0]:(1.59133378475e-06) A[1]:(0.999991238117) A[2]:(7.14422549208e-06) A[3]:(3.48506340699e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 552000 finished after 8 . Running score: 0.16. Policy_loss: -92050.6375145, Value_loss: 0.986702009772. Times trained:               12659. Times reached goal: 128.               Steps done: 6625753.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994315385818) A[1]:(0.00208421703428) A[2]:(0.00190240703523) A[3]:(0.00169798044953)\n",
      " state (1)  A[0]:(0.0336769483984) A[1]:(0.00449999701232) A[2]:(0.0100372219458) A[3]:(0.951785862446)\n",
      " state (2)  A[0]:(0.999999165535) A[1]:(1.57621144581e-07) A[2]:(6.66026039653e-07) A[3]:(7.33110783451e-13)\n",
      " state (3)  A[0]:(0.999999344349) A[1]:(1.41579079127e-07) A[2]:(5.02509976741e-07) A[3]:(2.971633085e-13)\n",
      " state (4)  A[0]:(0.999998807907) A[1]:(4.54142053741e-07) A[2]:(7.1655102829e-07) A[3]:(1.85657204535e-14)\n",
      " state (5)  A[0]:(0.990421414375) A[1]:(0.000741357391234) A[2]:(0.00883722025901) A[3]:(3.58591484795e-26)\n",
      " state (6)  A[0]:(0.302154541016) A[1]:(0.638595700264) A[2]:(0.0592497438192) A[3]:(2.99446691058e-30)\n",
      " state (7)  A[0]:(0.0252798367292) A[1]:(0.97198933363) A[2]:(0.00273080542684) A[3]:(9.72771533815e-32)\n",
      " state (8)  A[0]:(0.00121567514725) A[1]:(0.998700141907) A[2]:(8.41543296701e-05) A[3]:(4.02954100269e-33)\n",
      " state (9)  A[0]:(6.13450683886e-05) A[1]:(0.999929189682) A[2]:(9.4921397249e-06) A[3]:(4.79838263374e-34)\n",
      " state (10)  A[0]:(3.05816865875e-05) A[1]:(0.999963760376) A[2]:(5.67364122617e-06) A[3]:(2.92110024755e-34)\n",
      " state (11)  A[0]:(2.70153432211e-05) A[1]:(0.999967873096) A[2]:(5.10835570822e-06) A[3]:(2.6489496577e-34)\n",
      " state (12)  A[0]:(2.62190751528e-05) A[1]:(0.999968767166) A[2]:(4.98528788739e-06) A[3]:(2.58855679871e-34)\n",
      " state (13)  A[0]:(2.5970126444e-05) A[1]:(0.999969065189) A[2]:(4.95433050673e-06) A[3]:(2.57217816756e-34)\n",
      " state (14)  A[0]:(2.58599029621e-05) A[1]:(0.999969184399) A[2]:(4.94528649142e-06) A[3]:(2.56645474985e-34)\n",
      " state (15)  A[0]:(2.57945848716e-05) A[1]:(0.999969244003) A[2]:(4.94187361255e-06) A[3]:(2.56371506741e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 553000 finished after 14 . Running score: 0.08. Policy_loss: -92050.6112921, Value_loss: 1.1953519153. Times trained:               12750. Times reached goal: 129.               Steps done: 6638503.\n",
      " state (0)  A[0]:(0.995372474194) A[1]:(0.00212903530337) A[2]:(0.00154640874825) A[3]:(0.000952064525336)\n",
      " state (1)  A[0]:(0.0298963990062) A[1]:(0.00465836003423) A[2]:(0.00913174450397) A[3]:(0.956313490868)\n",
      " state (2)  A[0]:(0.999998927116) A[1]:(2.30478377716e-07) A[2]:(8.71589975304e-07) A[3]:(1.46419031413e-12)\n",
      " state (3)  A[0]:(0.999999344349) A[1]:(1.74947246023e-07) A[2]:(5.02802379287e-07) A[3]:(2.73909042149e-13)\n",
      " state (4)  A[0]:(0.999997735023) A[1]:(1.61953653333e-06) A[2]:(6.2811608359e-07) A[3]:(3.15728714704e-15)\n",
      " state (5)  A[0]:(0.927460730076) A[1]:(0.0539365187287) A[2]:(0.0186027400196) A[3]:(2.89932710131e-27)\n",
      " state (6)  A[0]:(0.0613938309252) A[1]:(0.924892306328) A[2]:(0.0137138729915) A[3]:(4.350919777e-31)\n",
      " state (7)  A[0]:(0.00820688251406) A[1]:(0.990710616112) A[2]:(0.00108252919745) A[3]:(3.38137180134e-32)\n",
      " state (8)  A[0]:(0.000527593947481) A[1]:(0.99941688776) A[2]:(5.55024525966e-05) A[3]:(2.26744980788e-33)\n",
      " state (9)  A[0]:(3.23278218275e-05) A[1]:(0.999960005283) A[2]:(7.66223274695e-06) A[3]:(3.25980448065e-34)\n",
      " state (10)  A[0]:(1.72978088813e-05) A[1]:(0.999977946281) A[2]:(4.77186949865e-06) A[3]:(2.07285411086e-34)\n",
      " state (11)  A[0]:(1.54999543156e-05) A[1]:(0.999980151653) A[2]:(4.33215200246e-06) A[3]:(1.8964960939e-34)\n",
      " state (12)  A[0]:(1.51132071551e-05) A[1]:(0.99998062849) A[2]:(4.23675874117e-06) A[3]:(1.85795744273e-34)\n",
      " state (13)  A[0]:(1.50072719407e-05) A[1]:(0.999980807304) A[2]:(4.21366257797e-06) A[3]:(1.84823091574e-34)\n",
      " state (14)  A[0]:(1.49703355419e-05) A[1]:(0.999980807304) A[2]:(4.20759079134e-06) A[3]:(1.84541294354e-34)\n",
      " state (15)  A[0]:(1.49534416778e-05) A[1]:(0.999980866909) A[2]:(4.20585820393e-06) A[3]:(1.84441377334e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 554000 finished after 12 . Running score: 0.13. Policy_loss: -92050.611218, Value_loss: 1.4122295182. Times trained:               12837. Times reached goal: 129.               Steps done: 6651340.\n",
      "action_dist \n",
      "tensor([[ 0.9915,  0.0024,  0.0020,  0.0042]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9915,  0.0024,  0.0020,  0.0042]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6404e-07,  4.5875e-07,  3.2289e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5131e-04,  9.9983e-01,  1.4537e-05,  1.0610e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5124e-04,  9.9983e-01,  1.4534e-05,  1.0607e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.991513431072) A[1]:(0.00236669555306) A[2]:(0.00195857370272) A[3]:(0.00416130851954)\n",
      " state (1)  A[0]:(0.0217677094042) A[1]:(0.00405540224165) A[2]:(0.00749804638326) A[3]:(0.966678857803)\n",
      " state (2)  A[0]:(0.262776434422) A[1]:(0.0117989573628) A[2]:(0.0254562981427) A[3]:(0.699968338013)\n",
      " state (3)  A[0]:(0.999999344349) A[1]:(1.49104977254e-07) A[2]:(5.31851242158e-07) A[3]:(5.40615675056e-13)\n",
      " state (4)  A[0]:(0.999999403954) A[1]:(1.64035824923e-07) A[2]:(4.58753874e-07) A[3]:(3.22895844177e-13)\n",
      " state (5)  A[0]:(0.999998807907) A[1]:(7.22886284166e-07) A[2]:(4.97294195156e-07) A[3]:(2.24978828698e-14)\n",
      " state (6)  A[0]:(0.993717253208) A[1]:(0.00439057173207) A[2]:(0.0018922011368) A[3]:(2.17766556652e-25)\n",
      " state (7)  A[0]:(0.0223361831158) A[1]:(0.975602984428) A[2]:(0.00206083105877) A[3]:(1.7032781488e-31)\n",
      " state (8)  A[0]:(0.000151471846038) A[1]:(0.999834001064) A[2]:(1.45447984323e-05) A[3]:(1.06159537671e-33)\n",
      " state (9)  A[0]:(1.80940314749e-05) A[1]:(0.9999781847) A[2]:(3.71326132154e-06) A[3]:(2.58648843375e-34)\n",
      " state (10)  A[0]:(1.3669733562e-05) A[1]:(0.999983370304) A[2]:(2.98305621982e-06) A[3]:(2.09041443526e-34)\n",
      " state (11)  A[0]:(1.29698328237e-05) A[1]:(0.999984204769) A[2]:(2.85462078864e-06) A[3]:(2.0040089424e-34)\n",
      " state (12)  A[0]:(1.27941584651e-05) A[1]:(0.999984383583) A[2]:(2.82504720417e-06) A[3]:(1.98335353219e-34)\n",
      " state (13)  A[0]:(1.27343664644e-05) A[1]:(0.999984443188) A[2]:(2.81718030237e-06) A[3]:(1.97727999165e-34)\n",
      " state (14)  A[0]:(1.27069497466e-05) A[1]:(0.999984502792) A[2]:(2.81468851426e-06) A[3]:(1.97498869602e-34)\n",
      " state (15)  A[0]:(1.26912545966e-05) A[1]:(0.999984502792) A[2]:(2.81370080302e-06) A[3]:(1.9738738131e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 555000 finished after 5 . Running score: 0.13. Policy_loss: -92050.6112313, Value_loss: 1.22138533063. Times trained:               12712. Times reached goal: 154.               Steps done: 6664052.\n",
      " state (0)  A[0]:(0.990397632122) A[1]:(0.00223418371752) A[2]:(0.00207026256248) A[3]:(0.00529791833833)\n",
      " state (1)  A[0]:(0.0219828058034) A[1]:(0.0038673800882) A[2]:(0.0074439458549) A[3]:(0.966705858707)\n",
      " state (2)  A[0]:(0.999993622303) A[1]:(1.5559367057e-06) A[2]:(4.81936149299e-06) A[3]:(1.22394094859e-10)\n",
      " state (3)  A[0]:(0.999999403954) A[1]:(1.51901318191e-07) A[2]:(4.59960773469e-07) A[3]:(3.61875270418e-13)\n",
      " state (4)  A[0]:(0.99999910593) A[1]:(4.68766927497e-07) A[2]:(4.3824169893e-07) A[3]:(5.14390809949e-14)\n",
      " state (5)  A[0]:(0.999382257462) A[1]:(0.0005764082307) A[2]:(4.13062152802e-05) A[3]:(1.26638312459e-21)\n",
      " state (6)  A[0]:(0.14308975637) A[1]:(0.837764799595) A[2]:(0.0191454701126) A[3]:(2.09768941735e-30)\n",
      " state (7)  A[0]:(0.00874707009643) A[1]:(0.99054646492) A[2]:(0.0007064511301) A[3]:(4.5068870711e-32)\n",
      " state (8)  A[0]:(0.000576711492613) A[1]:(0.999393284321) A[2]:(3.00078536384e-05) A[3]:(2.60059427412e-33)\n",
      " state (9)  A[0]:(3.39630387316e-05) A[1]:(0.999961555004) A[2]:(4.45861951448e-06) A[3]:(3.9002011756e-34)\n",
      " state (10)  A[0]:(1.62536198332e-05) A[1]:(0.999980926514) A[2]:(2.83475287688e-06) A[3]:(2.46161993921e-34)\n",
      " state (11)  A[0]:(1.43172865137e-05) A[1]:(0.999983072281) A[2]:(2.58917361862e-06) A[3]:(2.25447235025e-34)\n",
      " state (12)  A[0]:(1.39042958835e-05) A[1]:(0.999983549118) A[2]:(2.53567918662e-06) A[3]:(2.20913132908e-34)\n",
      " state (13)  A[0]:(1.37868873935e-05) A[1]:(0.999983668327) A[2]:(2.52242352872e-06) A[3]:(2.19729786623e-34)\n",
      " state (14)  A[0]:(1.37430952236e-05) A[1]:(0.999983727932) A[2]:(2.51876031143e-06) A[3]:(2.19357967658e-34)\n",
      " state (15)  A[0]:(1.37214610731e-05) A[1]:(0.999983787537) A[2]:(2.51755955105e-06) A[3]:(2.19209056401e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 556000 finished after 25 . Running score: 0.15. Policy_loss: -92050.6112908, Value_loss: 0.985900300103. Times trained:               12881. Times reached goal: 130.               Steps done: 6676933.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994524240494) A[1]:(0.0020816361066) A[2]:(0.00173364416696) A[3]:(0.0016604878474)\n",
      " state (1)  A[0]:(0.0225847493857) A[1]:(0.0041571254842) A[2]:(0.00814031995833) A[3]:(0.965117812157)\n",
      " state (2)  A[0]:(0.999998867512) A[1]:(2.34732041804e-07) A[2]:(8.91883018994e-07) A[3]:(1.05646079992e-12)\n",
      " state (3)  A[0]:(0.99999910593) A[1]:(2.79885767895e-07) A[2]:(6.24715369213e-07) A[3]:(1.95299315999e-13)\n",
      " state (4)  A[0]:(0.999917209148) A[1]:(7.80953923822e-05) A[2]:(4.70098211736e-06) A[3]:(2.73075253786e-18)\n",
      " state (5)  A[0]:(0.113721445203) A[1]:(0.818833470345) A[2]:(0.0674450993538) A[3]:(2.96753247939e-30)\n",
      " state (6)  A[0]:(0.00966443680227) A[1]:(0.985667765141) A[2]:(0.0046678041108) A[3]:(7.54633989188e-32)\n",
      " state (7)  A[0]:(0.00315587804653) A[1]:(0.995656073093) A[2]:(0.00118807493709) A[3]:(2.06744330118e-32)\n",
      " state (8)  A[0]:(0.000589772069361) A[1]:(0.999215781689) A[2]:(0.000194432781427) A[3]:(4.02734797104e-33)\n",
      " state (9)  A[0]:(3.16084006045e-05) A[1]:(0.999950051308) A[2]:(1.83631073014e-05) A[3]:(4.31520302767e-34)\n",
      " state (10)  A[0]:(8.15679959487e-06) A[1]:(0.999985575676) A[2]:(6.24228459856e-06) A[3]:(1.56202444402e-34)\n",
      " state (11)  A[0]:(6.3425177359e-06) A[1]:(0.999988794327) A[2]:(4.85459895572e-06) A[3]:(1.2492410093e-34)\n",
      " state (12)  A[0]:(6.00568910158e-06) A[1]:(0.999989449978) A[2]:(4.57359556094e-06) A[3]:(1.18602948935e-34)\n",
      " state (13)  A[0]:(5.92352944295e-06) A[1]:(0.999989569187) A[2]:(4.5036854317e-06) A[3]:(1.17029979078e-34)\n",
      " state (14)  A[0]:(5.90183162785e-06) A[1]:(0.999989628792) A[2]:(4.48516948381e-06) A[3]:(1.1661375765e-34)\n",
      " state (15)  A[0]:(5.89598130318e-06) A[1]:(0.999989628792) A[2]:(4.48019318355e-06) A[3]:(1.16500811469e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 557000 finished after 9 . Running score: 0.15. Policy_loss: -92050.6298506, Value_loss: 1.40864357772. Times trained:               13077. Times reached goal: 120.               Steps done: 6690010.\n",
      " state (0)  A[0]:(0.994587957859) A[1]:(0.00225783721544) A[2]:(0.00172816903796) A[3]:(0.00142601958942)\n",
      " state (1)  A[0]:(0.0175757221878) A[1]:(0.00391751527786) A[2]:(0.00749428803101) A[3]:(0.971012473106)\n",
      " state (2)  A[0]:(0.981106579304) A[1]:(0.00386890023947) A[2]:(0.00922978390008) A[3]:(0.00579476682469)\n",
      " state (3)  A[0]:(0.999998986721) A[1]:(1.81692456636e-07) A[2]:(8.06587308944e-07) A[3]:(5.29568847541e-13)\n",
      " state (4)  A[0]:(0.999998986721) A[1]:(2.61676063928e-07) A[2]:(7.75259195507e-07) A[3]:(2.4216403622e-13)\n",
      " state (5)  A[0]:(0.999975800514) A[1]:(1.74710730789e-05) A[2]:(6.75171895637e-06) A[3]:(2.12874550137e-17)\n",
      " state (6)  A[0]:(0.313658386469) A[1]:(0.449437141418) A[2]:(0.236904487014) A[3]:(2.2409079627e-29)\n",
      " state (7)  A[0]:(0.0063087712042) A[1]:(0.98657232523) A[2]:(0.00711892126128) A[3]:(5.72831856566e-32)\n",
      " state (8)  A[0]:(8.49632488098e-05) A[1]:(0.999778032303) A[2]:(0.000136980932439) A[3]:(1.17911707747e-33)\n",
      " state (9)  A[0]:(7.47073136154e-06) A[1]:(0.999975264072) A[2]:(1.72428863152e-05) A[3]:(1.66512310461e-34)\n",
      " state (10)  A[0]:(5.05730713485e-06) A[1]:(0.999983847141) A[2]:(1.11157523861e-05) A[3]:(1.13075290065e-34)\n",
      " state (11)  A[0]:(4.69942779091e-06) A[1]:(0.999985158443) A[2]:(1.01597806861e-05) A[3]:(1.04603224477e-34)\n",
      " state (12)  A[0]:(4.62036314275e-06) A[1]:(0.999985456467) A[2]:(9.94804668153e-06) A[3]:(1.02716314975e-34)\n",
      " state (13)  A[0]:(4.6006298362e-06) A[1]:(0.999985516071) A[2]:(9.89558884612e-06) A[3]:(1.02246415701e-34)\n",
      " state (14)  A[0]:(4.59538568975e-06) A[1]:(0.999985516071) A[2]:(9.88185820461e-06) A[3]:(1.02123229862e-34)\n",
      " state (15)  A[0]:(4.59394823338e-06) A[1]:(0.999985516071) A[2]:(9.87812654785e-06) A[3]:(1.02089732865e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 558000 finished after 6 . Running score: 0.11. Policy_loss: -92050.6813091, Value_loss: 0.99648462141. Times trained:               12878. Times reached goal: 128.               Steps done: 6702888.\n",
      " state (0)  A[0]:(0.9936876297) A[1]:(0.0023637029808) A[2]:(0.00187421415467) A[3]:(0.00207444652915)\n",
      " state (1)  A[0]:(0.0170664917678) A[1]:(0.0039512780495) A[2]:(0.00761886686087) A[3]:(0.97136336565)\n",
      " state (2)  A[0]:(0.999407470226) A[1]:(0.000151372878463) A[2]:(0.000438377377577) A[3]:(2.79467803921e-06)\n",
      " state (3)  A[0]:(0.999998867512) A[1]:(2.03305091873e-07) A[2]:(9.20694787965e-07) A[3]:(4.99728242728e-13)\n",
      " state (4)  A[0]:(0.999997437) A[1]:(1.11658334845e-06) A[2]:(1.44875525621e-06) A[3]:(2.21028029863e-14)\n",
      " state (5)  A[0]:(0.762162804604) A[1]:(0.0523286834359) A[2]:(0.185508549213) A[3]:(4.65367074694e-27)\n",
      " state (6)  A[0]:(0.0328955315053) A[1]:(0.786730647087) A[2]:(0.18037378788) A[3]:(7.8823267402e-31)\n",
      " state (7)  A[0]:(0.00476005487144) A[1]:(0.967880904675) A[2]:(0.0273590162396) A[3]:(9.91154384671e-32)\n",
      " state (8)  A[0]:(0.000395358365495) A[1]:(0.996692597866) A[2]:(0.00291202357039) A[3]:(1.15209679698e-32)\n",
      " state (9)  A[0]:(1.81522900675e-05) A[1]:(0.999835908413) A[2]:(0.000145951897139) A[3]:(7.43839049876e-34)\n",
      " state (10)  A[0]:(4.85230066261e-06) A[1]:(0.999965310097) A[2]:(2.98185459542e-05) A[3]:(1.88186738813e-34)\n",
      " state (11)  A[0]:(3.55068254976e-06) A[1]:(0.999976754189) A[2]:(1.97102090169e-05) A[3]:(1.32623772599e-34)\n",
      " state (12)  A[0]:(3.29547196998e-06) A[1]:(0.999978899956) A[2]:(1.78166483238e-05) A[3]:(1.21831816104e-34)\n",
      " state (13)  A[0]:(3.23199242303e-06) A[1]:(0.999979436398) A[2]:(1.73513017216e-05) A[3]:(1.19155924896e-34)\n",
      " state (14)  A[0]:(3.21502329825e-06) A[1]:(0.999979555607) A[2]:(1.72271138581e-05) A[3]:(1.18439917971e-34)\n",
      " state (15)  A[0]:(3.21035417983e-06) A[1]:(0.999979615211) A[2]:(1.71931733348e-05) A[3]:(1.18243998419e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 559000 finished after 5 . Running score: 0.07. Policy_loss: -92050.6318261, Value_loss: 0.986792334365. Times trained:               12940. Times reached goal: 121.               Steps done: 6715828.\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0022,  0.0016,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0022,  0.0016,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0022,  0.0016,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0022,  0.0016,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0022,  0.0016,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0022,  0.0016,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.3764e-06,  1.9479e-06,  6.8906e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  7.3740e-06,  1.9477e-06,  6.8942e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3253e-04,  9.9945e-01,  4.1621e-04,  2.9886e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3267e-04,  9.9945e-01,  4.1659e-04,  2.9911e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994869947433) A[1]:(0.00224168412387) A[2]:(0.0015986826038) A[3]:(0.00128966977354)\n",
      " state (1)  A[0]:(0.0170566923916) A[1]:(0.00400888128206) A[2]:(0.00729569373652) A[3]:(0.971638739109)\n",
      " state (2)  A[0]:(0.999949634075) A[1]:(1.28402116388e-05) A[2]:(3.75004310627e-05) A[3]:(7.80679787482e-09)\n",
      " state (3)  A[0]:(0.999998986721) A[1]:(2.26174051932e-07) A[2]:(7.75581611379e-07) A[3]:(3.88102039655e-13)\n",
      " state (4)  A[0]:(0.999990701675) A[1]:(7.34865579943e-06) A[2]:(1.94565723177e-06) A[3]:(6.92860933574e-16)\n",
      " state (5)  A[0]:(0.201903641224) A[1]:(0.582467317581) A[2]:(0.215629070997) A[3]:(2.70905789783e-29)\n",
      " state (6)  A[0]:(0.00801271945238) A[1]:(0.973743379116) A[2]:(0.0182438846678) A[3]:(1.20497868788e-31)\n",
      " state (7)  A[0]:(0.00145801412873) A[1]:(0.99505096674) A[2]:(0.00349099584855) A[3]:(2.23704832809e-32)\n",
      " state (8)  A[0]:(0.000134488102049) A[1]:(0.999444127083) A[2]:(0.000421410542913) A[3]:(3.022055835e-33)\n",
      " state (9)  A[0]:(8.78056380316e-06) A[1]:(0.999959170818) A[2]:(3.20362269122e-05) A[3]:(2.87386948147e-34)\n",
      " state (10)  A[0]:(3.33840284839e-06) A[1]:(0.999986410141) A[2]:(1.02812400655e-05) A[3]:(1.07628630103e-34)\n",
      " state (11)  A[0]:(2.70645978162e-06) A[1]:(0.999989449978) A[2]:(7.82169809099e-06) A[3]:(8.54493188921e-35)\n",
      " state (12)  A[0]:(2.57645251622e-06) A[1]:(0.999990105629) A[2]:(7.32449871066e-06) A[3]:(8.08666644969e-35)\n",
      " state (13)  A[0]:(2.54403812505e-06) A[1]:(0.999990284443) A[2]:(7.20113030184e-06) A[3]:(7.97241907466e-35)\n",
      " state (14)  A[0]:(2.53549296758e-06) A[1]:(0.999990284443) A[2]:(7.16865179129e-06) A[3]:(7.9422465224e-35)\n",
      " state (15)  A[0]:(2.53319217336e-06) A[1]:(0.999990284443) A[2]:(7.15990654498e-06) A[3]:(7.9341311344e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 560000 finished after 10 . Running score: 0.14. Policy_loss: -92050.6317557, Value_loss: 1.19521072734. Times trained:               12594. Times reached goal: 128.               Steps done: 6728422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995327293873) A[1]:(0.00205380725674) A[2]:(0.00152741186321) A[3]:(0.00109149317723)\n",
      " state (1)  A[0]:(0.0197384953499) A[1]:(0.00413969671354) A[2]:(0.00800890941173) A[3]:(0.968112885952)\n",
      " state (2)  A[0]:(0.999997854233) A[1]:(4.17991202539e-07) A[2]:(1.72658246811e-06) A[3]:(3.51728650545e-12)\n",
      " state (3)  A[0]:(0.999999046326) A[1]:(1.91295924878e-07) A[2]:(7.32864123165e-07) A[3]:(3.19183183573e-13)\n",
      " state (4)  A[0]:(0.999994814396) A[1]:(3.4242093534e-06) A[2]:(1.77984793481e-06) A[3]:(9.0843447148e-16)\n",
      " state (5)  A[0]:(0.455677211285) A[1]:(0.241522297263) A[2]:(0.302800506353) A[3]:(5.06765251812e-29)\n",
      " state (6)  A[0]:(0.0270277112722) A[1]:(0.919782996178) A[2]:(0.0531892888248) A[3]:(1.98037225995e-31)\n",
      " state (7)  A[0]:(0.00269766640849) A[1]:(0.991936862469) A[2]:(0.00536548253149) A[3]:(1.7998950564e-32)\n",
      " state (8)  A[0]:(5.97999714955e-05) A[1]:(0.999753057957) A[2]:(0.000187166791875) A[3]:(7.41460189101e-34)\n",
      " state (9)  A[0]:(7.55170276534e-06) A[1]:(0.999971330166) A[2]:(2.1146821382e-05) A[3]:(1.05689546561e-34)\n",
      " state (10)  A[0]:(4.96277698403e-06) A[1]:(0.999982595444) A[2]:(1.24157431856e-05) A[3]:(6.69999235765e-35)\n",
      " state (11)  A[0]:(4.55770577901e-06) A[1]:(0.999984323978) A[2]:(1.10946184577e-05) A[3]:(6.09042449438e-35)\n",
      " state (12)  A[0]:(4.46972399004e-06) A[1]:(0.999984741211) A[2]:(1.08104477476e-05) A[3]:(5.95810905946e-35)\n",
      " state (13)  A[0]:(4.44886882178e-06) A[1]:(0.999984800816) A[2]:(1.07433143057e-05) A[3]:(5.92678167583e-35)\n",
      " state (14)  A[0]:(4.44383113063e-06) A[1]:(0.999984800816) A[2]:(1.07270570879e-05) A[3]:(5.91923451996e-35)\n",
      " state (15)  A[0]:(4.44256056653e-06) A[1]:(0.99998486042) A[2]:(1.07230471258e-05) A[3]:(5.91733869094e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 561000 finished after 15 . Running score: 0.14. Policy_loss: -92050.63176, Value_loss: 1.21668879346. Times trained:               12154. Times reached goal: 119.               Steps done: 6740576.\n",
      " state (0)  A[0]:(0.995487213135) A[1]:(0.00204683165066) A[2]:(0.00151991820894) A[3]:(0.00094603333855)\n",
      " state (1)  A[0]:(0.0202565379441) A[1]:(0.00423309300095) A[2]:(0.00845065806061) A[3]:(0.967059731483)\n",
      " state (2)  A[0]:(0.999995589256) A[1]:(8.04540263744e-07) A[2]:(3.61799607163e-06) A[3]:(1.78330752942e-11)\n",
      " state (3)  A[0]:(0.999999046326) A[1]:(1.62856423458e-07) A[2]:(7.70905671743e-07) A[3]:(3.08715726594e-13)\n",
      " state (4)  A[0]:(0.999997675419) A[1]:(1.14783210847e-06) A[2]:(1.14774229587e-06) A[3]:(7.6566128722e-15)\n",
      " state (5)  A[0]:(0.893154203892) A[1]:(0.0442094244063) A[2]:(0.0626363530755) A[3]:(1.04850807809e-26)\n",
      " state (6)  A[0]:(0.0960564762354) A[1]:(0.666942179203) A[2]:(0.237001374364) A[3]:(4.52927847524e-31)\n",
      " state (7)  A[0]:(0.0284036248922) A[1]:(0.90555948019) A[2]:(0.0660368800163) A[3]:(8.56839637735e-32)\n",
      " state (8)  A[0]:(0.00443337345496) A[1]:(0.985416889191) A[2]:(0.0101497191936) A[3]:(1.37644819577e-32)\n",
      " state (9)  A[0]:(0.000132038199808) A[1]:(0.99942368269) A[2]:(0.000444290140877) A[3]:(7.10984533783e-34)\n",
      " state (10)  A[0]:(1.9456480004e-05) A[1]:(0.99991774559) A[2]:(6.2781618908e-05) A[3]:(1.21297586064e-34)\n",
      " state (11)  A[0]:(1.2531878383e-05) A[1]:(0.999950826168) A[2]:(3.66292297258e-05) A[3]:(7.59879841195e-35)\n",
      " state (12)  A[0]:(1.13072592285e-05) A[1]:(0.999956607819) A[2]:(3.2081396057e-05) A[3]:(6.7812759553e-35)\n",
      " state (13)  A[0]:(1.10095006676e-05) A[1]:(0.999957978725) A[2]:(3.09833339998e-05) A[3]:(6.5820813203e-35)\n",
      " state (14)  A[0]:(1.09301645352e-05) A[1]:(0.999958395958) A[2]:(3.06916153932e-05) A[3]:(6.52896883507e-35)\n",
      " state (15)  A[0]:(1.09085058284e-05) A[1]:(0.999958455563) A[2]:(3.06118708977e-05) A[3]:(6.5144398856e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 562000 finished after 8 . Running score: 0.15. Policy_loss: -92050.6430917, Value_loss: 1.19071157486. Times trained:               12641. Times reached goal: 122.               Steps done: 6753217.\n",
      " state (0)  A[0]:(0.995315551758) A[1]:(0.00202462612651) A[2]:(0.00151983648539) A[3]:(0.00113999983296)\n",
      " state (1)  A[0]:(0.0212366655469) A[1]:(0.00421029888093) A[2]:(0.00842669513077) A[3]:(0.966126322746)\n",
      " state (2)  A[0]:(0.999995112419) A[1]:(8.75574698966e-07) A[2]:(4.00315457227e-06) A[3]:(2.56931368542e-11)\n",
      " state (3)  A[0]:(0.99999910593) A[1]:(1.52927611907e-07) A[2]:(7.2404355933e-07) A[3]:(2.93711262993e-13)\n",
      " state (4)  A[0]:(0.999998211861) A[1]:(8.58329485709e-07) A[2]:(9.42827171002e-07) A[3]:(1.03482447153e-14)\n",
      " state (5)  A[0]:(0.976350545883) A[1]:(0.0115199312568) A[2]:(0.0121295172721) A[3]:(7.964749537e-26)\n",
      " state (6)  A[0]:(0.135233074427) A[1]:(0.618403613567) A[2]:(0.246363341808) A[3]:(4.97534754833e-31)\n",
      " state (7)  A[0]:(0.040165796876) A[1]:(0.885975718498) A[2]:(0.0738584548235) A[3]:(9.38582809594e-32)\n",
      " state (8)  A[0]:(0.00921724271029) A[1]:(0.974312365055) A[2]:(0.0164704211056) A[3]:(2.14229672432e-32)\n",
      " state (9)  A[0]:(0.000407017796533) A[1]:(0.998572707176) A[2]:(0.00102026865352) A[3]:(1.53241107809e-33)\n",
      " state (10)  A[0]:(3.3295724279e-05) A[1]:(0.999870240688) A[2]:(9.64878126979e-05) A[3]:(1.76142272423e-34)\n",
      " state (11)  A[0]:(1.71027204487e-05) A[1]:(0.999938488007) A[2]:(4.44337383669e-05) A[3]:(8.9579717907e-35)\n",
      " state (12)  A[0]:(1.46287711686e-05) A[1]:(0.999948859215) A[2]:(3.64960978914e-05) A[3]:(7.56765756918e-35)\n",
      " state (13)  A[0]:(1.40414722409e-05) A[1]:(0.99995136261) A[2]:(3.46252818417e-05) A[3]:(7.23535484413e-35)\n",
      " state (14)  A[0]:(1.38825535032e-05) A[1]:(0.999952018261) A[2]:(3.41204940923e-05) A[3]:(7.14533883874e-35)\n",
      " state (15)  A[0]:(1.38378254633e-05) A[1]:(0.999952197075) A[2]:(3.39785328833e-05) A[3]:(7.12003586367e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 563000 finished after 27 . Running score: 0.06. Policy_loss: -92050.6223191, Value_loss: 1.00401963162. Times trained:               12885. Times reached goal: 95.               Steps done: 6766102.\n",
      " state (0)  A[0]:(0.991943180561) A[1]:(0.00225830986165) A[2]:(0.00183354585897) A[3]:(0.00396498246118)\n",
      " state (1)  A[0]:(0.0181916840374) A[1]:(0.00377561408095) A[2]:(0.00714769540355) A[3]:(0.970884978771)\n",
      " state (2)  A[0]:(0.999998033047) A[1]:(3.66538813523e-07) A[2]:(1.59415469625e-06) A[3]:(3.44752676927e-12)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(1.86523081425e-07) A[2]:(6.63462969896e-07) A[3]:(2.64763146303e-13)\n",
      " state (4)  A[0]:(0.99999320507) A[1]:(5.54297685085e-06) A[2]:(1.2712168882e-06) A[3]:(3.47446932794e-16)\n",
      " state (5)  A[0]:(0.584583222866) A[1]:(0.267066478729) A[2]:(0.148350328207) A[3]:(9.32951460496e-29)\n",
      " state (6)  A[0]:(0.0460984781384) A[1]:(0.900377750397) A[2]:(0.0535237416625) A[3]:(2.17314087324e-31)\n",
      " state (7)  A[0]:(0.00922492984682) A[1]:(0.981173396111) A[2]:(0.00960167218) A[3]:(3.73644955954e-32)\n",
      " state (8)  A[0]:(0.000453785440186) A[1]:(0.998941421509) A[2]:(0.000604804663453) A[3]:(2.84618185998e-33)\n",
      " state (9)  A[0]:(2.48327487498e-05) A[1]:(0.999928057194) A[2]:(4.71050443593e-05) A[3]:(2.7280319585e-34)\n",
      " state (10)  A[0]:(1.25325404952e-05) A[1]:(0.999965429306) A[2]:(2.2040647309e-05) A[3]:(1.41754183901e-34)\n",
      " state (11)  A[0]:(1.09645388875e-05) A[1]:(0.999970316887) A[2]:(1.86969009519e-05) A[3]:(1.23442863254e-34)\n",
      " state (12)  A[0]:(1.06341412902e-05) A[1]:(0.999971389771) A[2]:(1.79901890078e-05) A[3]:(1.19530097146e-34)\n",
      " state (13)  A[0]:(1.05547305793e-05) A[1]:(0.999971628189) A[2]:(1.78204609256e-05) A[3]:(1.18587268024e-34)\n",
      " state (14)  A[0]:(1.05349799924e-05) A[1]:(0.999971687794) A[2]:(1.77782276296e-05) A[3]:(1.18353167865e-34)\n",
      " state (15)  A[0]:(1.05299977804e-05) A[1]:(0.999971687794) A[2]:(1.77677193278e-05) A[3]:(1.18294496463e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 564000 finished after 9 . Running score: 0.16. Policy_loss: -92050.6252109, Value_loss: 1.20434645285. Times trained:               12687. Times reached goal: 124.               Steps done: 6778789.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9943,  0.0020,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9943,  0.0020,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9943,  0.0020,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.6146e-07,  8.6085e-07,  7.0108e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.6146e-07,  8.6085e-07,  7.0108e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.0760e-04,  9.9890e-01,  5.9341e-04,  1.0058e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.0755e-04,  9.9890e-01,  5.9335e-04,  1.0058e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.0751e-04,  9.9890e-01,  5.9330e-04,  1.0057e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994252800941) A[1]:(0.00202193506993) A[2]:(0.0016355906846) A[3]:(0.00208965549245)\n",
      " state (1)  A[0]:(0.0201664175838) A[1]:(0.00385801703669) A[2]:(0.00775760458782) A[3]:(0.968217968941)\n",
      " state (2)  A[0]:(0.999996900558) A[1]:(5.29961425855e-07) A[2]:(2.5662857297e-06) A[3]:(9.48571256265e-12)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(1.40814762517e-07) A[2]:(6.71646091632e-07) A[3]:(2.78207578502e-13)\n",
      " state (4)  A[0]:(0.999998271465) A[1]:(8.61385785811e-07) A[2]:(8.60845375428e-07) A[3]:(7.01183572264e-15)\n",
      " state (5)  A[0]:(0.96525734663) A[1]:(0.0112881334499) A[2]:(0.0234545338899) A[3]:(3.37494994555e-27)\n",
      " state (6)  A[0]:(0.133451610804) A[1]:(0.731201887131) A[2]:(0.135346502066) A[3]:(2.22485651211e-31)\n",
      " state (7)  A[0]:(0.0172514691949) A[1]:(0.968023478985) A[2]:(0.0147250369191) A[3]:(2.0405666514e-32)\n",
      " state (8)  A[0]:(0.000508191413246) A[1]:(0.998897790909) A[2]:(0.000593993230723) A[3]:(1.00679594229e-33)\n",
      " state (9)  A[0]:(3.66284366464e-05) A[1]:(0.999898672104) A[2]:(6.47154956823e-05) A[3]:(1.28266890031e-34)\n",
      " state (10)  A[0]:(2.2183647161e-05) A[1]:(0.999939858913) A[2]:(3.79443845304e-05) A[3]:(8.04889910189e-35)\n",
      " state (11)  A[0]:(2.01583825401e-05) A[1]:(0.999945878983) A[2]:(3.39386751875e-05) A[3]:(7.31730826693e-35)\n",
      " state (12)  A[0]:(1.97277549887e-05) A[1]:(0.999947190285) A[2]:(3.30796392518e-05) A[3]:(7.15954693794e-35)\n",
      " state (13)  A[0]:(1.96262008103e-05) A[1]:(0.999947488308) A[2]:(3.28769820044e-05) A[3]:(7.12222958408e-35)\n",
      " state (14)  A[0]:(1.96015134861e-05) A[1]:(0.999947547913) A[2]:(3.28281093971e-05) A[3]:(7.11321593013e-35)\n",
      " state (15)  A[0]:(1.95954580704e-05) A[1]:(0.999947607517) A[2]:(3.28160895151e-05) A[3]:(7.11099178921e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 565000 finished after 8 . Running score: 0.12. Policy_loss: -92050.625185, Value_loss: 1.62244795908. Times trained:               12799. Times reached goal: 122.               Steps done: 6791588.\n",
      " state (0)  A[0]:(0.995081841946) A[1]:(0.00196545384824) A[2]:(0.00147276953794) A[3]:(0.00147990998812)\n",
      " state (1)  A[0]:(0.0280307307839) A[1]:(0.00435905018821) A[2]:(0.00915166828781) A[3]:(0.958458542824)\n",
      " state (2)  A[0]:(0.999998807907) A[1]:(1.91626554624e-07) A[2]:(1.01322984847e-06) A[3]:(9.13784889769e-13)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(1.53329224872e-07) A[2]:(6.76583454151e-07) A[3]:(2.3628516678e-13)\n",
      " state (4)  A[0]:(0.999996185303) A[1]:(2.240101594e-06) A[2]:(1.57895624397e-06) A[3]:(6.32436355974e-16)\n",
      " state (5)  A[0]:(0.576358675957) A[1]:(0.0887828394771) A[2]:(0.334858506918) A[3]:(8.72772017802e-30)\n",
      " state (6)  A[0]:(0.0629822462797) A[1]:(0.857290387154) A[2]:(0.0797273516655) A[3]:(7.37135286408e-32)\n",
      " state (7)  A[0]:(0.00535736419261) A[1]:(0.989956259727) A[2]:(0.00468639656901) A[3]:(4.94532934582e-33)\n",
      " state (8)  A[0]:(9.80577606242e-05) A[1]:(0.999759554863) A[2]:(0.000142373086419) A[3]:(1.86884775504e-34)\n",
      " state (9)  A[0]:(1.92858187802e-05) A[1]:(0.99994623661) A[2]:(3.44727523043e-05) A[3]:(5.05098902289e-35)\n",
      " state (10)  A[0]:(1.49596153278e-05) A[1]:(0.999958753586) A[2]:(2.62713729171e-05) A[3]:(3.98077115404e-35)\n",
      " state (11)  A[0]:(1.42481012517e-05) A[1]:(0.999960899353) A[2]:(2.48609576374e-05) A[3]:(3.79521294228e-35)\n",
      " state (12)  A[0]:(1.40980209835e-05) A[1]:(0.999961316586) A[2]:(2.45613027801e-05) A[3]:(3.75569526733e-35)\n",
      " state (13)  A[0]:(1.40643442137e-05) A[1]:(0.999961435795) A[2]:(2.44941256824e-05) A[3]:(3.74682338443e-35)\n",
      " state (14)  A[0]:(1.40567271956e-05) A[1]:(0.999961435795) A[2]:(2.4479086278e-05) A[3]:(3.74485179112e-35)\n",
      " state (15)  A[0]:(1.40549045682e-05) A[1]:(0.999961495399) A[2]:(2.44755392487e-05) A[3]:(3.74436621093e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 566000 finished after 13 . Running score: 0.16. Policy_loss: -92050.625205, Value_loss: 1.42275751564. Times trained:               12994. Times reached goal: 137.               Steps done: 6804582.\n",
      " state (0)  A[0]:(0.994016766548) A[1]:(0.00203597499058) A[2]:(0.00163435202558) A[3]:(0.00231289886869)\n",
      " state (1)  A[0]:(0.0240406449884) A[1]:(0.0041407989338) A[2]:(0.00848767347634) A[3]:(0.963330864906)\n",
      " state (2)  A[0]:(0.999998986721) A[1]:(1.62625312328e-07) A[2]:(8.7666262516e-07) A[3]:(5.06700204798e-13)\n",
      " state (3)  A[0]:(0.999998986721) A[1]:(2.10965225733e-07) A[2]:(8.11821848856e-07) A[3]:(2.11785612918e-13)\n",
      " state (4)  A[0]:(0.999981939793) A[1]:(1.18514490168e-05) A[2]:(6.19819275016e-06) A[3]:(1.94035759841e-17)\n",
      " state (5)  A[0]:(0.225788682699) A[1]:(0.246125653386) A[2]:(0.528085649014) A[3]:(2.0611228914e-30)\n",
      " state (6)  A[0]:(0.0249343141913) A[1]:(0.901227712631) A[2]:(0.0738380029798) A[3]:(5.56670043511e-32)\n",
      " state (7)  A[0]:(0.00265006162226) A[1]:(0.991039514542) A[2]:(0.00631043920293) A[3]:(5.43948219165e-33)\n",
      " state (8)  A[0]:(5.16471154697e-05) A[1]:(0.99974668026) A[2]:(0.000201651433599) A[3]:(2.1779028982e-34)\n",
      " state (9)  A[0]:(8.84811925062e-06) A[1]:(0.999952077866) A[2]:(3.90553759644e-05) A[3]:(4.92265637864e-35)\n",
      " state (10)  A[0]:(6.58323233438e-06) A[1]:(0.999965488911) A[2]:(2.79248761217e-05) A[3]:(3.68734268151e-35)\n",
      " state (11)  A[0]:(6.21442222837e-06) A[1]:(0.999967694283) A[2]:(2.6065568818e-05) A[3]:(3.47745007217e-35)\n",
      " state (12)  A[0]:(6.13607699051e-06) A[1]:(0.99996817112) A[2]:(2.56689145317e-05) A[3]:(3.43248310846e-35)\n",
      " state (13)  A[0]:(6.11831319475e-06) A[1]:(0.999968290329) A[2]:(2.55791837844e-05) A[3]:(3.42231175329e-35)\n",
      " state (14)  A[0]:(6.11427731201e-06) A[1]:(0.999968349934) A[2]:(2.55587019637e-05) A[3]:(3.41998888921e-35)\n",
      " state (15)  A[0]:(6.11329824096e-06) A[1]:(0.999968349934) A[2]:(2.55537306657e-05) A[3]:(3.41941491736e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 567000 finished after 15 . Running score: 0.13. Policy_loss: -92050.6111948, Value_loss: 1.626653601. Times trained:               12729. Times reached goal: 122.               Steps done: 6817311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995768249035) A[1]:(0.00175582675729) A[2]:(0.00130046298727) A[3]:(0.00117544457316)\n",
      " state (1)  A[0]:(0.0176427457482) A[1]:(0.0035025626421) A[2]:(0.00677381642163) A[3]:(0.972080886364)\n",
      " state (2)  A[0]:(0.999997973442) A[1]:(3.5728965031e-07) A[2]:(1.6660089841e-06) A[3]:(3.51694476493e-12)\n",
      " state (3)  A[0]:(0.99999910593) A[1]:(1.7002776076e-07) A[2]:(7.19577428754e-07) A[3]:(3.37845609778e-13)\n",
      " state (4)  A[0]:(0.999996423721) A[1]:(2.30488899433e-06) A[2]:(1.25679912344e-06) A[3]:(2.5569042648e-15)\n",
      " state (5)  A[0]:(0.643695950508) A[1]:(0.141153857112) A[2]:(0.215150207281) A[3]:(3.12228825541e-28)\n",
      " state (6)  A[0]:(0.0358329899609) A[1]:(0.871568322182) A[2]:(0.0925986543298) A[3]:(1.53106787376e-31)\n",
      " state (7)  A[0]:(0.00236607389525) A[1]:(0.992381513119) A[2]:(0.00525238923728) A[3]:(9.3696134738e-33)\n",
      " state (8)  A[0]:(3.74758492399e-05) A[1]:(0.999825060368) A[2]:(0.000137488386827) A[3]:(3.18187035313e-34)\n",
      " state (9)  A[0]:(8.62228080223e-06) A[1]:(0.999960482121) A[2]:(3.09207171085e-05) A[3]:(8.56125909248e-35)\n",
      " state (10)  A[0]:(6.74810189594e-06) A[1]:(0.999970197678) A[2]:(2.30613568419e-05) A[3]:(6.68698902536e-35)\n",
      " state (11)  A[0]:(6.42988970867e-06) A[1]:(0.999971866608) A[2]:(2.17201140913e-05) A[3]:(6.36080369231e-35)\n",
      " state (12)  A[0]:(6.36257391307e-06) A[1]:(0.999972224236) A[2]:(2.14363881241e-05) A[3]:(6.29149888719e-35)\n",
      " state (13)  A[0]:(6.34756815998e-06) A[1]:(0.99997228384) A[2]:(2.13731091208e-05) A[3]:(6.27606191425e-35)\n",
      " state (14)  A[0]:(6.34417892798e-06) A[1]:(0.99997228384) A[2]:(2.13588464248e-05) A[3]:(6.27251935999e-35)\n",
      " state (15)  A[0]:(6.34342904959e-06) A[1]:(0.99997228384) A[2]:(2.13556686504e-05) A[3]:(6.27175368154e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 568000 finished after 13 . Running score: 0.13. Policy_loss: -92050.6111853, Value_loss: 1.21729407276. Times trained:               12638. Times reached goal: 124.               Steps done: 6829949.\n",
      " state (0)  A[0]:(0.994139075279) A[1]:(0.00167257676367) A[2]:(0.00138647004496) A[3]:(0.00280185765587)\n",
      " state (1)  A[0]:(0.0176274944097) A[1]:(0.0032154538203) A[2]:(0.0062710987404) A[3]:(0.972885966301)\n",
      " state (2)  A[0]:(0.999998748302) A[1]:(2.05964411748e-07) A[2]:(1.02959234027e-06) A[3]:(1.37717235619e-12)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(1.72469142967e-07) A[2]:(6.58802321141e-07) A[3]:(2.89801521539e-13)\n",
      " state (4)  A[0]:(0.999975860119) A[1]:(2.07903340197e-05) A[2]:(3.3788824112e-06) A[3]:(4.03116710796e-17)\n",
      " state (5)  A[0]:(0.226534128189) A[1]:(0.325577586889) A[2]:(0.447888284922) A[3]:(4.73795994364e-30)\n",
      " state (6)  A[0]:(0.0664581358433) A[1]:(0.755515515804) A[2]:(0.17802631855) A[3]:(3.42019062149e-31)\n",
      " state (7)  A[0]:(0.0434974655509) A[1]:(0.847184121609) A[2]:(0.109318420291) A[3]:(2.01875849817e-31)\n",
      " state (8)  A[0]:(0.0317541733384) A[1]:(0.893211841583) A[2]:(0.0750339627266) A[3]:(1.42420944225e-31)\n",
      " state (9)  A[0]:(0.0156226595864) A[1]:(0.951711058617) A[2]:(0.0326662957668) A[3]:(6.80185406298e-32)\n",
      " state (10)  A[0]:(0.00240819714963) A[1]:(0.992892324924) A[2]:(0.0046995067969) A[3]:(1.20730668097e-32)\n",
      " state (11)  A[0]:(0.000110455417598) A[1]:(0.999571382999) A[2]:(0.000318174250424) A[3]:(1.03111471636e-33)\n",
      " state (12)  A[0]:(2.0810357455e-05) A[1]:(0.999912321568) A[2]:(6.68797947583e-05) A[3]:(2.58217767556e-34)\n",
      " state (13)  A[0]:(1.30073040054e-05) A[1]:(0.999947071075) A[2]:(3.99016571464e-05) A[3]:(1.66726264208e-34)\n",
      " state (14)  A[0]:(1.13622791105e-05) A[1]:(0.999954581261) A[2]:(3.40332226187e-05) A[3]:(1.4607616898e-34)\n",
      " state (15)  A[0]:(1.08886979433e-05) A[1]:(0.999956786633) A[2]:(3.23347485391e-05) A[3]:(1.40025621754e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 569000 finished after 15 . Running score: 0.12. Policy_loss: -92050.6094491, Value_loss: 1.20474175298. Times trained:               12588. Times reached goal: 126.               Steps done: 6842537.\n",
      "action_dist \n",
      "tensor([[ 0.9958,  0.0017,  0.0012,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9958,  0.0017,  0.0012,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9998e-01,  1.4194e-05,  1.1234e-06,  4.9963e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.4689e-04,  9.9916e-01,  4.9125e-04,  2.1116e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995818436146) A[1]:(0.00167157023679) A[2]:(0.00118195975665) A[3]:(0.00132800906431)\n",
      " state (1)  A[0]:(0.0173472445458) A[1]:(0.00343885621987) A[2]:(0.00611417554319) A[3]:(0.973099708557)\n",
      " state (2)  A[0]:(0.999997496605) A[1]:(5.34459104529e-07) A[2]:(1.97150438908e-06) A[3]:(7.96256983959e-12)\n",
      " state (3)  A[0]:(0.99999910593) A[1]:(2.36835205669e-07) A[2]:(6.32988417237e-07) A[3]:(3.33626274393e-13)\n",
      " state (4)  A[0]:(0.999984681606) A[1]:(1.41887767313e-05) A[2]:(1.12345639991e-06) A[3]:(4.99731810007e-16)\n",
      " state (5)  A[0]:(0.192887023091) A[1]:(0.775121331215) A[2]:(0.0319916121662) A[3]:(1.52926156118e-28)\n",
      " state (6)  A[0]:(0.005794018507) A[1]:(0.984739899635) A[2]:(0.00946605950594) A[3]:(3.63803952348e-32)\n",
      " state (7)  A[0]:(0.00177934963722) A[1]:(0.995597422123) A[2]:(0.00262323603965) A[3]:(9.66276486717e-33)\n",
      " state (8)  A[0]:(0.000347592955222) A[1]:(0.99916023016) A[2]:(0.000492157647386) A[3]:(2.11506551607e-33)\n",
      " state (9)  A[0]:(1.63822405739e-05) A[1]:(0.999943971634) A[2]:(3.96588875446e-05) A[3]:(2.00714168076e-34)\n",
      " state (10)  A[0]:(3.35478216584e-06) A[1]:(0.999986469746) A[2]:(1.01860659925e-05) A[3]:(5.78058186183e-35)\n",
      " state (11)  A[0]:(2.44126363214e-06) A[1]:(0.999990284443) A[2]:(7.26317648514e-06) A[3]:(4.32092581401e-35)\n",
      " state (12)  A[0]:(2.27615191761e-06) A[1]:(0.999990999699) A[2]:(6.69963583277e-06) A[3]:(4.03677703138e-35)\n",
      " state (13)  A[0]:(2.23593701776e-06) A[1]:(0.999991178513) A[2]:(6.56043766867e-06) A[3]:(3.96640865642e-35)\n",
      " state (14)  A[0]:(2.22528365157e-06) A[1]:(0.999991238117) A[2]:(6.52337939755e-06) A[3]:(3.94766101385e-35)\n",
      " state (15)  A[0]:(2.22238236347e-06) A[1]:(0.999991238117) A[2]:(6.51335858493e-06) A[3]:(3.94260460883e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 570000 finished after 4 . Running score: 0.15. Policy_loss: -92050.6112169, Value_loss: 1.63628747531. Times trained:               12802. Times reached goal: 136.               Steps done: 6855339.\n",
      " state (0)  A[0]:(0.995521068573) A[1]:(0.00156698108185) A[2]:(0.00124154740479) A[3]:(0.00167040899396)\n",
      " state (1)  A[0]:(0.0174165703356) A[1]:(0.00317934760824) A[2]:(0.00605602236465) A[3]:(0.973348081112)\n",
      " state (2)  A[0]:(0.999997079372) A[1]:(5.60425007734e-07) A[2]:(2.38898451244e-06) A[3]:(1.37474580331e-11)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.70242785202e-07) A[2]:(6.1428573872e-07) A[3]:(3.58197195863e-13)\n",
      " state (4)  A[0]:(0.99999243021) A[1]:(6.42939085083e-06) A[2]:(1.16739909117e-06) A[3]:(7.95853522515e-16)\n",
      " state (5)  A[0]:(0.413265407085) A[1]:(0.43236219883) A[2]:(0.154372394085) A[3]:(5.43700314989e-29)\n",
      " state (6)  A[0]:(0.0329071357846) A[1]:(0.918804585934) A[2]:(0.0482882745564) A[3]:(1.36180021373e-31)\n",
      " state (7)  A[0]:(0.00810336042196) A[1]:(0.982046127319) A[2]:(0.00985050387681) A[3]:(2.87217850529e-32)\n",
      " state (8)  A[0]:(0.000556406739634) A[1]:(0.99875330925) A[2]:(0.000690280925483) A[3]:(2.60051584661e-33)\n",
      " state (9)  A[0]:(2.16142962017e-05) A[1]:(0.999932706356) A[2]:(4.56727102573e-05) A[3]:(2.11809709763e-34)\n",
      " state (10)  A[0]:(9.1266583695e-06) A[1]:(0.999971449375) A[2]:(1.94388303498e-05) A[3]:(1.00562324893e-34)\n",
      " state (11)  A[0]:(7.75459920987e-06) A[1]:(0.999976098537) A[2]:(1.61560565175e-05) A[3]:(8.61084968643e-35)\n",
      " state (12)  A[0]:(7.4742220022e-06) A[1]:(0.999977052212) A[2]:(1.54716108227e-05) A[3]:(8.30667158209e-35)\n",
      " state (13)  A[0]:(7.40726591175e-06) A[1]:(0.99997729063) A[2]:(1.53073560796e-05) A[3]:(8.23354584636e-35)\n",
      " state (14)  A[0]:(7.39058577892e-06) A[1]:(0.999977350235) A[2]:(1.52663615154e-05) A[3]:(8.21534979074e-35)\n",
      " state (15)  A[0]:(7.38638618714e-06) A[1]:(0.999977350235) A[2]:(1.52561151481e-05) A[3]:(8.21071267215e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 571000 finished after 15 . Running score: 0.08. Policy_loss: -92050.6111765, Value_loss: 1.42511775408. Times trained:               12713. Times reached goal: 113.               Steps done: 6868052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994137227535) A[1]:(0.00162239687052) A[2]:(0.00150501914322) A[3]:(0.0027353297919)\n",
      " state (1)  A[0]:(0.0151823423803) A[1]:(0.00299197505228) A[2]:(0.00551230506971) A[3]:(0.976313352585)\n",
      " state (2)  A[0]:(0.999991893768) A[1]:(1.78083428182e-06) A[2]:(6.34146044831e-06) A[3]:(1.59627755458e-10)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(1.87822934095e-07) A[2]:(6.29195540114e-07) A[3]:(4.42051316342e-13)\n",
      " state (4)  A[0]:(0.999974548817) A[1]:(2.34502822423e-05) A[2]:(2.01329567062e-06) A[3]:(2.63438972708e-16)\n",
      " state (5)  A[0]:(0.102186694741) A[1]:(0.76915115118) A[2]:(0.12866216898) A[3]:(3.065794027e-30)\n",
      " state (6)  A[0]:(0.00704048154876) A[1]:(0.982300758362) A[2]:(0.0106587773189) A[3]:(4.88778985244e-32)\n",
      " state (7)  A[0]:(0.000543662754353) A[1]:(0.998715281487) A[2]:(0.000741035270039) A[3]:(4.26660919323e-33)\n",
      " state (8)  A[0]:(1.33104113047e-05) A[1]:(0.999953269958) A[2]:(3.34211654263e-05) A[3]:(2.43142649437e-34)\n",
      " state (9)  A[0]:(4.67247991764e-06) A[1]:(0.999983131886) A[2]:(1.21659504657e-05) A[3]:(1.00411703199e-34)\n",
      " state (10)  A[0]:(3.97356416215e-06) A[1]:(0.999985933304) A[2]:(1.01114974314e-05) A[3]:(8.60751892778e-35)\n",
      " state (11)  A[0]:(3.85087196264e-06) A[1]:(0.999986410141) A[2]:(9.74232898443e-06) A[3]:(8.34772721462e-35)\n",
      " state (12)  A[0]:(3.82519237974e-06) A[1]:(0.99998652935) A[2]:(9.66470815911e-06) A[3]:(8.29300933012e-35)\n",
      " state (13)  A[0]:(3.81960762752e-06) A[1]:(0.99998652935) A[2]:(9.64772698353e-06) A[3]:(8.28112294706e-35)\n",
      " state (14)  A[0]:(3.8183839024e-06) A[1]:(0.99998652935) A[2]:(9.64404716797e-06) A[3]:(8.27847004916e-35)\n",
      " state (15)  A[0]:(3.81810696126e-06) A[1]:(0.99998652935) A[2]:(9.64323771768e-06) A[3]:(8.27790181703e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 572000 finished after 18 . Running score: 0.12. Policy_loss: -92050.6114409, Value_loss: 1.63080645539. Times trained:               13074. Times reached goal: 146.               Steps done: 6881126.\n",
      " state (0)  A[0]:(0.995496034622) A[1]:(0.00144569808617) A[2]:(0.00135646236595) A[3]:(0.00170178338885)\n",
      " state (1)  A[0]:(0.0156301036477) A[1]:(0.00293488614261) A[2]:(0.00558371981606) A[3]:(0.975851297379)\n",
      " state (2)  A[0]:(0.994552314281) A[1]:(0.00128311396111) A[2]:(0.00331369321793) A[3]:(0.000850894080941)\n",
      " state (3)  A[0]:(0.999999284744) A[1]:(1.32178314516e-07) A[2]:(6.00355178904e-07) A[3]:(5.21380681684e-13)\n",
      " state (4)  A[0]:(0.999999046326) A[1]:(3.44056928725e-07) A[2]:(5.98902545335e-07) A[3]:(1.22547330898e-13)\n",
      " state (5)  A[0]:(0.989226102829) A[1]:(0.00782765354961) A[2]:(0.00294623500668) A[3]:(4.10394906851e-24)\n",
      " state (6)  A[0]:(0.0944743007421) A[1]:(0.767379879951) A[2]:(0.138145834208) A[3]:(5.54518113023e-31)\n",
      " state (7)  A[0]:(0.0215458739549) A[1]:(0.950583696365) A[2]:(0.0278704147786) A[3]:(7.46482876261e-32)\n",
      " state (8)  A[0]:(0.00308719812892) A[1]:(0.993668198586) A[2]:(0.00324458861724) A[3]:(1.03232846938e-32)\n",
      " state (9)  A[0]:(7.7484153735e-05) A[1]:(0.999803423882) A[2]:(0.000119104501209) A[3]:(5.04909491578e-34)\n",
      " state (10)  A[0]:(1.07783716885e-05) A[1]:(0.999969601631) A[2]:(1.96259625227e-05) A[3]:(1.01121465311e-34)\n",
      " state (11)  A[0]:(7.13396275387e-06) A[1]:(0.999980390072) A[2]:(1.24556108858e-05) A[3]:(6.89243536176e-35)\n",
      " state (12)  A[0]:(6.49858338875e-06) A[1]:(0.999982357025) A[2]:(1.11609861051e-05) A[3]:(6.29396294834e-35)\n",
      " state (13)  A[0]:(6.34652133158e-06) A[1]:(0.999982833862) A[2]:(1.084917767e-05) A[3]:(6.14877562069e-35)\n",
      " state (14)  A[0]:(6.30689373793e-06) A[1]:(0.999982953072) A[2]:(1.07678297354e-05) A[3]:(6.11080279097e-35)\n",
      " state (15)  A[0]:(6.29629266768e-06) A[1]:(0.999982953072) A[2]:(1.07460818981e-05) A[3]:(6.10064750702e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 573000 finished after 12 . Running score: 0.09. Policy_loss: -92050.6125259, Value_loss: 1.41580160292. Times trained:               12863. Times reached goal: 117.               Steps done: 6893989.\n",
      " state (0)  A[0]:(0.993005633354) A[1]:(0.00155291392002) A[2]:(0.00160216738004) A[3]:(0.00383926392533)\n",
      " state (1)  A[0]:(0.0152806183323) A[1]:(0.00285667716525) A[2]:(0.00540217803791) A[3]:(0.976460516453)\n",
      " state (2)  A[0]:(0.999930679798) A[1]:(1.67332891579e-05) A[2]:(5.25294235558e-05) A[3]:(3.2336135547e-08)\n",
      " state (3)  A[0]:(0.999999284744) A[1]:(1.38951421036e-07) A[2]:(5.74859939206e-07) A[3]:(4.88419580388e-13)\n",
      " state (4)  A[0]:(0.999998807907) A[1]:(5.81307631364e-07) A[2]:(5.93812330862e-07) A[3]:(5.8403843285e-14)\n",
      " state (5)  A[0]:(0.961547613144) A[1]:(0.0295868944377) A[2]:(0.00886551849544) A[3]:(1.80973504713e-25)\n",
      " state (6)  A[0]:(0.0495450869203) A[1]:(0.894330501556) A[2]:(0.056124381721) A[3]:(2.55213506318e-31)\n",
      " state (7)  A[0]:(0.00678034313023) A[1]:(0.987291872501) A[2]:(0.00592781137675) A[3]:(2.37544515549e-32)\n",
      " state (8)  A[0]:(0.000261961453361) A[1]:(0.99950158596) A[2]:(0.000236432329984) A[3]:(1.31917658653e-33)\n",
      " state (9)  A[0]:(1.22466535686e-05) A[1]:(0.999968588352) A[2]:(1.91639228433e-05) A[3]:(1.31167628972e-34)\n",
      " state (10)  A[0]:(6.37686071059e-06) A[1]:(0.999983549118) A[2]:(1.00817005659e-05) A[3]:(7.53663266872e-35)\n",
      " state (11)  A[0]:(5.6469248193e-06) A[1]:(0.999985516071) A[2]:(8.81449068402e-06) A[3]:(6.73835663617e-35)\n",
      " state (12)  A[0]:(5.49565947949e-06) A[1]:(0.999985933304) A[2]:(8.54662539496e-06) A[3]:(6.56861995848e-35)\n",
      " state (13)  A[0]:(5.46024011783e-06) A[1]:(0.999986052513) A[2]:(8.48367471917e-06) A[3]:(6.52865142863e-35)\n",
      " state (14)  A[0]:(5.45166540178e-06) A[1]:(0.999986052513) A[2]:(8.46841339808e-06) A[3]:(6.51894613861e-35)\n",
      " state (15)  A[0]:(5.44962813365e-06) A[1]:(0.999986112118) A[2]:(8.46476450533e-06) A[3]:(6.51660892523e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 574000 finished after 6 . Running score: 0.13. Policy_loss: -92050.6112692, Value_loss: 1.20955701003. Times trained:               12926. Times reached goal: 129.               Steps done: 6906915.\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0015,  0.0015,  0.0035]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  6.0847e-06,  1.0339e-06,  4.2026e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  6.0846e-06,  1.0339e-06,  4.2026e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0015,  0.0015,  0.0035]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0015,  0.0015,  0.0035]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  6.0845e-06,  1.0339e-06,  4.2028e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7354e-04,  9.9959e-01,  1.3759e-04,  7.1207e-34]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7357e-04,  9.9959e-01,  1.3760e-04,  7.1213e-34]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5931e-05,  9.9997e-01,  1.5677e-05,  9.4782e-35]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8363e-06,  9.9998e-01,  8.4439e-06,  5.4900e-35]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8261e-06,  9.9998e-01,  8.4333e-06,  5.4843e-35]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8355e-06,  9.9998e-01,  8.4434e-06,  5.4900e-35]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8352e-06,  9.9998e-01,  8.4432e-06,  5.4900e-35]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8349e-06,  9.9998e-01,  8.4431e-06,  5.4900e-35]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8248e-06,  9.9998e-01,  8.4326e-06,  5.4843e-35]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8343e-06,  9.9998e-01,  8.4428e-06,  5.4900e-35]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8341e-06,  9.9998e-01,  8.4426e-06,  5.4900e-35]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8339e-06,  9.9998e-01,  8.4425e-06,  5.4899e-35]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8338e-06,  9.9998e-01,  8.4424e-06,  5.4900e-35]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993586599827) A[1]:(0.00146084045991) A[2]:(0.00149103871081) A[3]:(0.00346152437851)\n",
      " state (1)  A[0]:(0.0172694455832) A[1]:(0.00291848555207) A[2]:(0.00558104459196) A[3]:(0.974231004715)\n",
      " state (2)  A[0]:(0.999998867512) A[1]:(2.08323911011e-07) A[2]:(8.98360497104e-07) A[3]:(1.4675676039e-12)\n",
      " state (3)  A[0]:(0.999999284744) A[1]:(1.69046700194e-07) A[2]:(5.23851326761e-07) A[3]:(2.90187605933e-13)\n",
      " state (4)  A[0]:(0.999992907047) A[1]:(6.08509162703e-06) A[2]:(1.03394415873e-06) A[3]:(4.20352937918e-16)\n",
      " state (5)  A[0]:(0.431039869785) A[1]:(0.383120417595) A[2]:(0.185839667916) A[3]:(1.2845379464e-29)\n",
      " state (6)  A[0]:(0.0366390347481) A[1]:(0.929811477661) A[2]:(0.0335494801402) A[3]:(8.62421413885e-32)\n",
      " state (7)  A[0]:(0.00671937176958) A[1]:(0.98931646347) A[2]:(0.00396415311843) A[3]:(1.2820971417e-32)\n",
      " state (8)  A[0]:(0.000272975245025) A[1]:(0.999589622021) A[2]:(0.000137409355375) A[3]:(7.11330478097e-34)\n",
      " state (9)  A[0]:(1.59155933943e-05) A[1]:(0.999968409538) A[2]:(1.56683672685e-05) A[3]:(9.47478982065e-35)\n",
      " state (10)  A[0]:(8.95483208296e-06) A[1]:(0.999981462955) A[2]:(9.59654698818e-06) A[3]:(6.12638440481e-35)\n",
      " state (11)  A[0]:(8.05858417152e-06) A[1]:(0.999983251095) A[2]:(8.67980088515e-06) A[3]:(5.62091035452e-35)\n",
      " state (12)  A[0]:(7.87533372204e-06) A[1]:(0.999983608723) A[2]:(8.4865023382e-06) A[3]:(5.51429737909e-35)\n",
      " state (13)  A[0]:(7.8337470768e-06) A[1]:(0.999983727932) A[2]:(8.44239639264e-06) A[3]:(5.48995064111e-35)\n",
      " state (14)  A[0]:(7.82398092269e-06) A[1]:(0.999983727932) A[2]:(8.43212910695e-06) A[3]:(5.48429931427e-35)\n",
      " state (15)  A[0]:(7.82177266956e-06) A[1]:(0.999983727932) A[2]:(8.42974895932e-06) A[3]:(5.48300213789e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 575000 finished after 19 . Running score: 0.12. Policy_loss: -92050.6112425, Value_loss: 1.21389100275. Times trained:               13119. Times reached goal: 121.               Steps done: 6920034.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995579779148) A[1]:(0.00132726295851) A[2]:(0.00133702810854) A[3]:(0.00175595073961)\n",
      " state (1)  A[0]:(0.0183549579233) A[1]:(0.00301916687749) A[2]:(0.00617237435654) A[3]:(0.972453474998)\n",
      " state (2)  A[0]:(0.999998986721) A[1]:(1.63731613156e-07) A[2]:(8.65163599428e-07) A[3]:(8.95859666832e-13)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.86382806078e-07) A[2]:(6.16337160864e-07) A[3]:(2.13627563989e-13)\n",
      " state (4)  A[0]:(0.999934673309) A[1]:(5.79756233492e-05) A[2]:(7.36754100217e-06) A[3]:(1.68470924101e-18)\n",
      " state (5)  A[0]:(0.169619545341) A[1]:(0.480517417192) A[2]:(0.349863022566) A[3]:(5.08985527423e-31)\n",
      " state (6)  A[0]:(0.0445866510272) A[1]:(0.864576756954) A[2]:(0.0908366143703) A[3]:(5.82709770695e-32)\n",
      " state (7)  A[0]:(0.0153510542586) A[1]:(0.960660099983) A[2]:(0.0239888150245) A[3]:(1.75255701727e-32)\n",
      " state (8)  A[0]:(0.00116883043665) A[1]:(0.997450530529) A[2]:(0.00138062739279) A[3]:(1.48789079076e-33)\n",
      " state (9)  A[0]:(3.38852842106e-05) A[1]:(0.999893546104) A[2]:(7.25779682398e-05) A[3]:(1.01380280698e-34)\n",
      " state (10)  A[0]:(1.22431629279e-05) A[1]:(0.999958157539) A[2]:(2.9571729101e-05) A[3]:(4.56714337989e-35)\n",
      " state (11)  A[0]:(1.01257646747e-05) A[1]:(0.999965488911) A[2]:(2.43955655606e-05) A[3]:(3.87717755753e-35)\n",
      " state (12)  A[0]:(9.70734345174e-06) A[1]:(0.999966979027) A[2]:(2.33336559177e-05) A[3]:(3.73468646265e-35)\n",
      " state (13)  A[0]:(9.6097091955e-06) A[1]:(0.99996727705) A[2]:(2.3083997803e-05) A[3]:(3.70113149427e-35)\n",
      " state (14)  A[0]:(9.58598502621e-06) A[1]:(0.999967396259) A[2]:(2.3023407266e-05) A[3]:(3.69298023303e-35)\n",
      " state (15)  A[0]:(9.58020973485e-06) A[1]:(0.999967396259) A[2]:(2.30086588999e-05) A[3]:(3.6909805151e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 576000 finished after 39 . Running score: 0.04. Policy_loss: -92050.6115712, Value_loss: 1.2156219482. Times trained:               13095. Times reached goal: 117.               Steps done: 6933129.\n",
      " state (0)  A[0]:(0.992724657059) A[1]:(0.00144352507778) A[2]:(0.00150044821203) A[3]:(0.00433136196807)\n",
      " state (1)  A[0]:(0.0172343980521) A[1]:(0.00296842888929) A[2]:(0.00586603535339) A[3]:(0.973931133747)\n",
      " state (2)  A[0]:(0.99999833107) A[1]:(2.87247047481e-07) A[2]:(1.38235122904e-06) A[3]:(2.94324699161e-12)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.51050585373e-07) A[2]:(6.19418983661e-07) A[3]:(3.41524361959e-13)\n",
      " state (4)  A[0]:(0.999997496605) A[1]:(1.5383493519e-06) A[2]:(9.35313607897e-07) A[3]:(5.46753118244e-15)\n",
      " state (5)  A[0]:(0.881784796715) A[1]:(0.050611898303) A[2]:(0.0676033347845) A[3]:(3.19100110248e-27)\n",
      " state (6)  A[0]:(0.0675781667233) A[1]:(0.75888389349) A[2]:(0.173537954688) A[3]:(1.13685303032e-31)\n",
      " state (7)  A[0]:(0.00881003774703) A[1]:(0.976117432117) A[2]:(0.0150725496933) A[3]:(1.07246322606e-32)\n",
      " state (8)  A[0]:(0.000210254831472) A[1]:(0.999404609203) A[2]:(0.000385163148167) A[3]:(4.16894581649e-34)\n",
      " state (9)  A[0]:(1.34686169986e-05) A[1]:(0.999947249889) A[2]:(3.92855945393e-05) A[3]:(5.2376372072e-35)\n",
      " state (10)  A[0]:(8.09997800388e-06) A[1]:(0.999968290329) A[2]:(2.3609636628e-05) A[3]:(3.38938930189e-35)\n",
      " state (11)  A[0]:(7.35788216844e-06) A[1]:(0.999971389771) A[2]:(2.12538179767e-05) A[3]:(3.10533264173e-35)\n",
      " state (12)  A[0]:(7.20426851331e-06) A[1]:(0.999972045422) A[2]:(2.07589509955e-05) A[3]:(3.04534167784e-35)\n",
      " state (13)  A[0]:(7.1695335464e-06) A[1]:(0.999972164631) A[2]:(2.06465738302e-05) A[3]:(3.03173424018e-35)\n",
      " state (14)  A[0]:(7.16144359103e-06) A[1]:(0.999972224236) A[2]:(2.06205204449e-05) A[3]:(3.0285673505e-35)\n",
      " state (15)  A[0]:(7.15953137842e-06) A[1]:(0.999972224236) A[2]:(2.06143868127e-05) A[3]:(3.02782778777e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 577000 finished after 7 . Running score: 0.13. Policy_loss: -92050.6112538, Value_loss: 1.41436255411. Times trained:               12734. Times reached goal: 129.               Steps done: 6945863.\n",
      " state (0)  A[0]:(0.992162644863) A[1]:(0.00161003344692) A[2]:(0.0015719429357) A[3]:(0.00465540401638)\n",
      " state (1)  A[0]:(0.0158293191344) A[1]:(0.00292940600775) A[2]:(0.0057307719253) A[3]:(0.97551047802)\n",
      " state (2)  A[0]:(0.999998450279) A[1]:(2.65172616309e-07) A[2]:(1.28435988245e-06) A[3]:(1.98929891657e-12)\n",
      " state (3)  A[0]:(0.999999046326) A[1]:(1.98047089839e-07) A[2]:(7.26489247427e-07) A[3]:(3.35560599489e-13)\n",
      " state (4)  A[0]:(0.999989688396) A[1]:(7.97469147074e-06) A[2]:(2.3532643354e-06) A[3]:(3.5773033629e-16)\n",
      " state (5)  A[0]:(0.236887738109) A[1]:(0.231867671013) A[2]:(0.531244635582) A[3]:(6.98481451598e-30)\n",
      " state (6)  A[0]:(0.0252129752189) A[1]:(0.849414885044) A[2]:(0.125372126698) A[3]:(7.52639939346e-32)\n",
      " state (7)  A[0]:(0.00424230610952) A[1]:(0.980260014534) A[2]:(0.0154976565391) A[3]:(1.13226370936e-32)\n",
      " state (8)  A[0]:(0.000117966381367) A[1]:(0.9993724823) A[2]:(0.000509580480866) A[3]:(5.51865026681e-34)\n",
      " state (9)  A[0]:(7.30205965738e-06) A[1]:(0.999946951866) A[2]:(4.57595124317e-05) A[3]:(6.39890279583e-35)\n",
      " state (10)  A[0]:(4.2318361011e-06) A[1]:(0.999970138073) A[2]:(2.56554558291e-05) A[3]:(3.93927413716e-35)\n",
      " state (11)  A[0]:(3.80927031074e-06) A[1]:(0.999973475933) A[2]:(2.27236541832e-05) A[3]:(3.56703158087e-35)\n",
      " state (12)  A[0]:(3.72120166503e-06) A[1]:(0.999974191189) A[2]:(2.21073642024e-05) A[3]:(3.48812767051e-35)\n",
      " state (13)  A[0]:(3.70101452063e-06) A[1]:(0.999974310398) A[2]:(2.196580499e-05) A[3]:(3.46994596419e-35)\n",
      " state (14)  A[0]:(3.69628810404e-06) A[1]:(0.999974370003) A[2]:(2.19326502702e-05) A[3]:(3.46571320877e-35)\n",
      " state (15)  A[0]:(3.69518829757e-06) A[1]:(0.999974370003) A[2]:(2.19248704525e-05) A[3]:(3.46470875803e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 578000 finished after 4 . Running score: 0.16. Policy_loss: -92050.6059135, Value_loss: 1.22725839323. Times trained:               12693. Times reached goal: 123.               Steps done: 6958556.\n",
      " state (0)  A[0]:(0.995392680168) A[1]:(0.00148742890451) A[2]:(0.00124129967298) A[3]:(0.00187860953156)\n",
      " state (1)  A[0]:(0.0159723013639) A[1]:(0.00304410536774) A[2]:(0.00557025475428) A[3]:(0.975413322449)\n",
      " state (2)  A[0]:(0.999995827675) A[1]:(8.76530293681e-07) A[2]:(3.29615954797e-06) A[3]:(2.61068736074e-11)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(1.92376631958e-07) A[2]:(6.49427192911e-07) A[3]:(4.81502533158e-13)\n",
      " state (4)  A[0]:(0.999995529652) A[1]:(3.50581240127e-06) A[2]:(9.36344349611e-07) A[3]:(8.16537304758e-15)\n",
      " state (5)  A[0]:(0.248060598969) A[1]:(0.599788486958) A[2]:(0.152150928974) A[3]:(5.10457740998e-29)\n",
      " state (6)  A[0]:(0.00547421909869) A[1]:(0.976690471172) A[2]:(0.0178352911025) A[3]:(1.50166420501e-32)\n",
      " state (7)  A[0]:(0.000414888752857) A[1]:(0.998530745506) A[2]:(0.00105433922727) A[3]:(1.07252077018e-33)\n",
      " state (8)  A[0]:(7.87762655818e-06) A[1]:(0.999954283237) A[2]:(3.78156037186e-05) A[3]:(5.01730720673e-35)\n",
      " state (9)  A[0]:(2.16350736082e-06) A[1]:(0.999986112118) A[2]:(1.17460658657e-05) A[3]:(1.77511069014e-35)\n",
      " state (10)  A[0]:(1.77558854375e-06) A[1]:(0.999988734722) A[2]:(9.468690223e-06) A[3]:(1.48075611402e-35)\n",
      " state (11)  A[0]:(1.70958162471e-06) A[1]:(0.999989211559) A[2]:(9.06765126274e-06) A[3]:(1.42848047975e-35)\n",
      " state (12)  A[0]:(1.69587667642e-06) A[1]:(0.999989330769) A[2]:(8.98384951142e-06) A[3]:(1.41753713244e-35)\n",
      " state (13)  A[0]:(1.69290331087e-06) A[1]:(0.999989330769) A[2]:(8.96556775842e-06) A[3]:(1.41514912255e-35)\n",
      " state (14)  A[0]:(1.69224472302e-06) A[1]:(0.999989330769) A[2]:(8.96149867913e-06) A[3]:(1.414620064e-35)\n",
      " state (15)  A[0]:(1.69209624801e-06) A[1]:(0.999989330769) A[2]:(8.9606101028e-06) A[3]:(1.41451215729e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 579000 finished after 20 . Running score: 0.09. Policy_loss: -92050.6112466, Value_loss: 1.23021331305. Times trained:               12646. Times reached goal: 125.               Steps done: 6971202.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9933,  0.0015,  0.0013,  0.0040]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9933,  0.0015,  0.0013,  0.0040]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.0229e-06,  5.8516e-07,  1.4275e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.0221e-06,  5.8512e-07,  1.4280e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9933,  0.0015,  0.0013,  0.0040]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9933,  0.0015,  0.0013,  0.0040]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.0201e-06,  5.8504e-07,  1.4290e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9933,  0.0015,  0.0013,  0.0040]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.0192e-06,  5.8500e-07,  1.4296e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.9847e-06,  9.9999e-01,  3.2405e-06,  3.2424e-35]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.9850e-06,  9.9999e-01,  3.2408e-06,  3.2429e-35]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993252158165) A[1]:(0.00146032916382) A[2]:(0.00125944742467) A[3]:(0.00402807816863)\n",
      " state (1)  A[0]:(0.0147801488638) A[1]:(0.00275713368319) A[2]:(0.00468200305477) A[3]:(0.97778069973)\n",
      " state (2)  A[0]:(0.999995350838) A[1]:(1.22826384086e-06) A[2]:(3.43552233062e-06) A[3]:(5.43905163963e-11)\n",
      " state (3)  A[0]:(0.999999284744) A[1]:(1.96533463281e-07) A[2]:(5.01756687754e-07) A[3]:(5.95235557892e-13)\n",
      " state (4)  A[0]:(0.999995410442) A[1]:(4.01509987569e-06) A[2]:(5.84865631481e-07) A[3]:(1.4318106027e-14)\n",
      " state (5)  A[0]:(0.0973906219006) A[1]:(0.881312608719) A[2]:(0.0212967395782) A[3]:(4.03627099114e-29)\n",
      " state (6)  A[0]:(0.00142531387974) A[1]:(0.997196853161) A[2]:(0.00137784588151) A[3]:(8.05646138766e-33)\n",
      " state (7)  A[0]:(7.76093293098e-05) A[1]:(0.999861180782) A[2]:(6.11969153397e-05) A[3]:(4.8051265734e-34)\n",
      " state (8)  A[0]:(1.98579482458e-06) A[1]:(0.999994754791) A[2]:(3.24190796164e-06) A[3]:(3.24402519091e-35)\n",
      " state (9)  A[0]:(8.45214458423e-07) A[1]:(0.999997675419) A[2]:(1.48502829234e-06) A[3]:(1.64442318288e-35)\n",
      " state (10)  A[0]:(7.45697377624e-07) A[1]:(0.999997973442) A[2]:(1.29651573388e-06) A[3]:(1.47022186515e-35)\n",
      " state (11)  A[0]:(7.28148336293e-07) A[1]:(0.999998033047) A[2]:(1.26218469632e-06) A[3]:(1.43840129621e-35)\n",
      " state (12)  A[0]:(7.24540768715e-07) A[1]:(0.999998033047) A[2]:(1.25507870052e-06) A[3]:(1.43182099592e-35)\n",
      " state (13)  A[0]:(7.23775599454e-07) A[1]:(0.999998033047) A[2]:(1.25356200442e-06) A[3]:(1.430412469e-35)\n",
      " state (14)  A[0]:(7.23609957731e-07) A[1]:(0.999998033047) A[2]:(1.25323686007e-06) A[3]:(1.43010697248e-35)\n",
      " state (15)  A[0]:(7.23571304206e-07) A[1]:(0.999998033047) A[2]:(1.25316034882e-06) A[3]:(1.4300413962e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 580000 finished after 11 . Running score: 0.17. Policy_loss: -92050.6112479, Value_loss: 1.02348089144. Times trained:               12506. Times reached goal: 127.               Steps done: 6983708.\n",
      " state (0)  A[0]:(0.995254278183) A[1]:(0.00131405575667) A[2]:(0.00115307851229) A[3]:(0.00227857776918)\n",
      " state (1)  A[0]:(0.0146272210404) A[1]:(0.00258481036872) A[2]:(0.00478343246505) A[3]:(0.978004515171)\n",
      " state (2)  A[0]:(0.0239075645804) A[1]:(0.00319289811887) A[2]:(0.00616442319006) A[3]:(0.966735124588)\n",
      " state (3)  A[0]:(0.999998629093) A[1]:(2.65118046627e-07) A[2]:(1.11131339509e-06) A[3]:(2.63918596179e-12)\n",
      " state (4)  A[0]:(0.999999344349) A[1]:(1.23817656572e-07) A[2]:(5.49180072085e-07) A[3]:(5.84014499087e-13)\n",
      " state (5)  A[0]:(0.999999046326) A[1]:(3.36718557037e-07) A[2]:(6.4545878331e-07) A[3]:(1.06321615195e-13)\n",
      " state (6)  A[0]:(0.914317786694) A[1]:(0.00625799875706) A[2]:(0.0794242396951) A[3]:(1.78765341291e-26)\n",
      " state (7)  A[0]:(0.0977822095156) A[1]:(0.710776805878) A[2]:(0.191440954804) A[3]:(4.30224067514e-31)\n",
      " state (8)  A[0]:(0.00369649403729) A[1]:(0.989992916584) A[2]:(0.00631060823798) A[3]:(9.47255004474e-33)\n",
      " state (9)  A[0]:(3.66622261936e-05) A[1]:(0.999867081642) A[2]:(9.62683916441e-05) A[3]:(2.09165490322e-34)\n",
      " state (10)  A[0]:(7.25074824004e-06) A[1]:(0.999975204468) A[2]:(1.75662098627e-05) A[3]:(4.97753153143e-35)\n",
      " state (11)  A[0]:(5.2325294746e-06) A[1]:(0.999982953072) A[2]:(1.18348125397e-05) A[3]:(3.62100014511e-35)\n",
      " state (12)  A[0]:(4.8497095122e-06) A[1]:(0.999984383583) A[2]:(1.07594687506e-05) A[3]:(3.35627456992e-35)\n",
      " state (13)  A[0]:(4.75601837024e-06) A[1]:(0.999984741211) A[2]:(1.04970877146e-05) A[3]:(3.29100794375e-35)\n",
      " state (14)  A[0]:(4.73101135867e-06) A[1]:(0.99998486042) A[2]:(1.04270075099e-05) A[3]:(3.27350409819e-35)\n",
      " state (15)  A[0]:(4.72399642604e-06) A[1]:(0.99998486042) A[2]:(1.04072587419e-05) A[3]:(3.26856306151e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 581000 finished after 18 . Running score: 0.12. Policy_loss: -92050.611272, Value_loss: 1.86293900791. Times trained:               12467. Times reached goal: 148.               Steps done: 6996175.\n",
      " state (0)  A[0]:(0.993516743183) A[1]:(0.00139509036671) A[2]:(0.00140277575701) A[3]:(0.00368541735224)\n",
      " state (1)  A[0]:(0.0146728577092) A[1]:(0.00259578344412) A[2]:(0.00499361427501) A[3]:(0.977737724781)\n",
      " state (2)  A[0]:(0.0711052715778) A[1]:(0.00570952845737) A[2]:(0.0119094997644) A[3]:(0.911275684834)\n",
      " state (3)  A[0]:(0.999998927116) A[1]:(1.81605656735e-07) A[2]:(8.90202215942e-07) A[3]:(1.11068522001e-12)\n",
      " state (4)  A[0]:(0.99999922514) A[1]:(1.31639396272e-07) A[2]:(6.17365515154e-07) A[3]:(4.96459318967e-13)\n",
      " state (5)  A[0]:(0.99999755621) A[1]:(7.84015924182e-07) A[2]:(1.6663976794e-06) A[3]:(1.34055931977e-14)\n",
      " state (6)  A[0]:(0.438110917807) A[1]:(0.0397365801036) A[2]:(0.522152483463) A[3]:(1.00910586013e-28)\n",
      " state (7)  A[0]:(0.0419863276184) A[1]:(0.786928832531) A[2]:(0.171084865928) A[3]:(1.15149616345e-31)\n",
      " state (8)  A[0]:(0.000665174389724) A[1]:(0.996457517147) A[2]:(0.00287730898708) A[3]:(1.97647252805e-33)\n",
      " state (9)  A[0]:(1.32465502247e-05) A[1]:(0.999916017056) A[2]:(7.07211002009e-05) A[3]:(7.51023742521e-35)\n",
      " state (10)  A[0]:(5.38061021871e-06) A[1]:(0.999970257282) A[2]:(2.43668509938e-05) A[3]:(3.16056623994e-35)\n",
      " state (11)  A[0]:(4.48415084975e-06) A[1]:(0.999976217747) A[2]:(1.93076721189e-05) A[3]:(2.62858645656e-35)\n",
      " state (12)  A[0]:(4.29875490227e-06) A[1]:(0.99997740984) A[2]:(1.82786243386e-05) A[3]:(2.51749707466e-35)\n",
      " state (13)  A[0]:(4.25399366577e-06) A[1]:(0.999977707863) A[2]:(1.8031527361e-05) A[3]:(2.49063720093e-35)\n",
      " state (14)  A[0]:(4.24248810305e-06) A[1]:(0.999977767467) A[2]:(1.79684247996e-05) A[3]:(2.48373002368e-35)\n",
      " state (15)  A[0]:(4.2394144657e-06) A[1]:(0.999977827072) A[2]:(1.79514354386e-05) A[3]:(2.48187379871e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 582000 finished after 6 . Running score: 0.13. Policy_loss: -92050.6111891, Value_loss: 1.20306693601. Times trained:               13083. Times reached goal: 106.               Steps done: 7009258.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994800388813) A[1]:(0.0014448055299) A[2]:(0.00127355440054) A[3]:(0.00248125591315)\n",
      " state (1)  A[0]:(0.0167138502002) A[1]:(0.00264112651348) A[2]:(0.00525761861354) A[3]:(0.975387394428)\n",
      " state (2)  A[0]:(0.0244316402823) A[1]:(0.00297531648539) A[2]:(0.00628467462957) A[3]:(0.966308355331)\n",
      " state (3)  A[0]:(0.999996602535) A[1]:(6.0141371705e-07) A[2]:(2.81229972643e-06) A[3]:(2.318371016e-11)\n",
      " state (4)  A[0]:(0.999999344349) A[1]:(9.63980255619e-08) A[2]:(5.37290873126e-07) A[3]:(4.45933329325e-13)\n",
      " state (5)  A[0]:(0.999999344349) A[1]:(1.43160832522e-07) A[2]:(5.31667694759e-07) A[3]:(1.93194229061e-13)\n",
      " state (6)  A[0]:(0.998998105526) A[1]:(5.67551032873e-05) A[2]:(0.000945156498346) A[3]:(1.316077878e-21)\n",
      " state (7)  A[0]:(0.34999063611) A[1]:(0.136541157961) A[2]:(0.513468205929) A[3]:(1.45649862638e-30)\n",
      " state (8)  A[0]:(0.0992942154408) A[1]:(0.765509068966) A[2]:(0.135196685791) A[3]:(5.94514084966e-32)\n",
      " state (9)  A[0]:(0.00414890563115) A[1]:(0.992530465126) A[2]:(0.00332060293294) A[3]:(1.89473820179e-33)\n",
      " state (10)  A[0]:(9.66252118815e-05) A[1]:(0.999778807163) A[2]:(0.000124583340948) A[3]:(9.69712011302e-35)\n",
      " state (11)  A[0]:(2.93462926493e-05) A[1]:(0.999927341938) A[2]:(4.32952983829e-05) A[3]:(3.81092771759e-35)\n",
      " state (12)  A[0]:(2.2375914341e-05) A[1]:(0.999944448471) A[2]:(3.31938208546e-05) A[3]:(3.03646491618e-35)\n",
      " state (13)  A[0]:(2.07682805922e-05) A[1]:(0.999948322773) A[2]:(3.09081951855e-05) A[3]:(2.85509698592e-35)\n",
      " state (14)  A[0]:(2.0249552108e-05) A[1]:(0.999949455261) A[2]:(3.02863682009e-05) A[3]:(2.80209986903e-35)\n",
      " state (15)  A[0]:(2.00200611289e-05) A[1]:(0.999949872494) A[2]:(3.01011714328e-05) A[3]:(2.78326641767e-35)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 583000 finished after 4 . Running score: 0.15. Policy_loss: -92050.612746, Value_loss: 1.41828443605. Times trained:               12389. Times reached goal: 114.               Steps done: 7021647.\n",
      " state (0)  A[0]:(0.995534420013) A[1]:(0.00143465667497) A[2]:(0.00181331054773) A[3]:(0.00121761334594)\n",
      " state (1)  A[0]:(0.0181650426239) A[1]:(0.00286787259392) A[2]:(0.00594093976542) A[3]:(0.973026156425)\n",
      " state (2)  A[0]:(0.99989849329) A[1]:(2.08396213566e-05) A[2]:(8.05781310191e-05) A[3]:(6.36905568285e-08)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.13253101119e-07) A[2]:(6.83272048718e-07) A[3]:(4.48879079523e-13)\n",
      " state (4)  A[0]:(0.999999165535) A[1]:(1.63964926969e-07) A[2]:(6.6591360337e-07) A[3]:(2.24420146095e-13)\n",
      " state (5)  A[0]:(0.999718606472) A[1]:(6.37282864773e-05) A[2]:(0.000217665408854) A[3]:(6.24726671291e-20)\n",
      " state (6)  A[0]:(0.115168295801) A[1]:(0.221557855606) A[2]:(0.663273870945) A[3]:(2.12116403953e-31)\n",
      " state (7)  A[0]:(0.0302461311221) A[1]:(0.821194350719) A[2]:(0.14855954051) A[3]:(1.43634374717e-32)\n",
      " state (8)  A[0]:(0.00111152313184) A[1]:(0.994875609875) A[2]:(0.00401286967099) A[3]:(5.40513792014e-34)\n",
      " state (9)  A[0]:(2.18880322791e-05) A[1]:(0.999853014946) A[2]:(0.00012507782958) A[3]:(2.40539778929e-35)\n",
      " state (10)  A[0]:(8.3558870756e-06) A[1]:(0.999945342541) A[2]:(4.6316501539e-05) A[3]:(1.03705463625e-35)\n",
      " state (11)  A[0]:(6.92417506798e-06) A[1]:(0.999955713749) A[2]:(3.73464426957e-05) A[3]:(8.68983194395e-36)\n",
      " state (12)  A[0]:(6.62841466692e-06) A[1]:(0.999957859516) A[2]:(3.55285264959e-05) A[3]:(8.34051411037e-36)\n",
      " state (13)  A[0]:(6.55274425299e-06) A[1]:(0.999958336353) A[2]:(3.51015005435e-05) A[3]:(8.25581021446e-36)\n",
      " state (14)  A[0]:(6.52931430523e-06) A[1]:(0.999958455563) A[2]:(3.49952133547e-05) A[3]:(8.23304075113e-36)\n",
      " state (15)  A[0]:(6.51993150314e-06) A[1]:(0.999958515167) A[2]:(3.49670590367e-05) A[3]:(8.22588403961e-36)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 584000 finished after 9 . Running score: 0.13. Policy_loss: -92050.6343362, Value_loss: 1.61713656. Times trained:               12688. Times reached goal: 125.               Steps done: 7034335.\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0013,  0.0016,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.7958e-07,  6.7971e-07,  1.5485e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.7519e-03,  9.6928e-01,  2.3971e-02,  1.6721e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.7774e-03,  9.6915e-01,  2.4070e-02,  1.6780e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.8008e-03,  9.6904e-01,  2.4161e-02,  1.6833e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.4151e-05,  9.9944e-01,  4.6953e-04,  5.0180e-35]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.8195e-03,  9.6895e-01,  2.4233e-02,  1.6876e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.8279e-03,  9.6891e-01,  2.4266e-02,  1.6895e-33]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.4357e-05,  9.9944e-01,  4.7052e-04,  5.0272e-35]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4976e-05,  9.9991e-01,  7.6439e-05,  1.0565e-35]])\n",
      "On state=10, selected action=1\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996458649635) A[1]:(0.00128957536072) A[2]:(0.00161787227262) A[3]:(0.000633904244751)\n",
      " state (1)  A[0]:(0.0172643233091) A[1]:(0.00281393085606) A[2]:(0.00575909810141) A[3]:(0.974162638187)\n",
      " state (2)  A[0]:(0.999956667423) A[1]:(8.92586103873e-06) A[2]:(3.44137879438e-05) A[3]:(6.92932333735e-09)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(1.13618966679e-07) A[2]:(6.8553293886e-07) A[3]:(4.51890857633e-13)\n",
      " state (4)  A[0]:(0.999999165535) A[1]:(1.79228905495e-07) A[2]:(6.79515324009e-07) A[3]:(1.5526756313e-13)\n",
      " state (5)  A[0]:(0.981805622578) A[1]:(0.00201686890796) A[2]:(0.0161775294691) A[3]:(1.133266631e-24)\n",
      " state (6)  A[0]:(0.108338989317) A[1]:(0.243291273713) A[2]:(0.648369729519) A[3]:(5.59093560214e-32)\n",
      " state (7)  A[0]:(0.0535535365343) A[1]:(0.681049704552) A[2]:(0.265396714211) A[3]:(1.44226485915e-32)\n",
      " state (8)  A[0]:(0.00681655341759) A[1]:(0.968961656094) A[2]:(0.0242217872292) A[3]:(1.68709318586e-33)\n",
      " state (9)  A[0]:(9.4242997875e-05) A[1]:(0.999435782433) A[2]:(0.000469975435408) A[3]:(5.02255560533e-35)\n",
      " state (10)  A[0]:(1.49735951709e-05) A[1]:(0.99990862608) A[2]:(7.64219730627e-05) A[3]:(1.0563329158e-35)\n",
      " state (11)  A[0]:(1.02122885437e-05) A[1]:(0.999940574169) A[2]:(4.92380167998e-05) A[3]:(7.36484447214e-36)\n",
      " state (12)  A[0]:(9.34371200856e-06) A[1]:(0.99994635582) A[2]:(4.42777418357e-05) A[3]:(6.75695662448e-36)\n",
      " state (13)  A[0]:(9.13487201615e-06) A[1]:(0.999947786331) A[2]:(4.31010739703e-05) A[3]:(6.61077245121e-36)\n",
      " state (14)  A[0]:(9.07942830963e-06) A[1]:(0.999948143959) A[2]:(4.27996201324e-05) A[3]:(6.57285515327e-36)\n",
      " state (15)  A[0]:(9.06333752937e-06) A[1]:(0.999948203564) A[2]:(4.27192098869e-05) A[3]:(6.56248348193e-36)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 585000 finished after 10 . Running score: 0.06. Policy_loss: -92050.611985, Value_loss: 1.20490445458. Times trained:               12243. Times reached goal: 117.               Steps done: 7046578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996127486229) A[1]:(0.00128765136469) A[2]:(0.00163809186779) A[3]:(0.000946785847191)\n",
      " state (1)  A[0]:(0.0173314847052) A[1]:(0.00269917654805) A[2]:(0.00539525737986) A[3]:(0.97457408905)\n",
      " state (2)  A[0]:(0.999994754791) A[1]:(1.00068530173e-06) A[2]:(4.25711004937e-06) A[3]:(4.52774484572e-11)\n",
      " state (3)  A[0]:(0.999999284744) A[1]:(1.06438839964e-07) A[2]:(5.80253924909e-07) A[3]:(4.01655678118e-13)\n",
      " state (4)  A[0]:(0.99999922514) A[1]:(2.02828175588e-07) A[2]:(5.94111043029e-07) A[3]:(1.17471570665e-13)\n",
      " state (5)  A[0]:(0.980788886547) A[1]:(0.00268929451704) A[2]:(0.0165218189359) A[3]:(1.7004058282e-25)\n",
      " state (6)  A[0]:(0.122321426868) A[1]:(0.527675926685) A[2]:(0.350002676249) A[3]:(4.03179398455e-32)\n",
      " state (7)  A[0]:(0.0129346391186) A[1]:(0.964663624763) A[2]:(0.0224017165601) A[3]:(2.40818844043e-33)\n",
      " state (8)  A[0]:(0.000124866724946) A[1]:(0.999621093273) A[2]:(0.000254038663115) A[3]:(4.63375310018e-35)\n",
      " state (9)  A[0]:(1.2849621271e-05) A[1]:(0.999953567982) A[2]:(3.3584121411e-05) A[3]:(7.83174761404e-36)\n",
      " state (10)  A[0]:(8.8298220362e-06) A[1]:(0.999968647957) A[2]:(2.25183430302e-05) A[3]:(5.62578043393e-36)\n",
      " state (11)  A[0]:(8.20136028779e-06) A[1]:(0.999971032143) A[2]:(2.07397770282e-05) A[3]:(5.26028520373e-36)\n",
      " state (12)  A[0]:(8.0632562458e-06) A[1]:(0.999971568584) A[2]:(2.03620875254e-05) A[3]:(5.18118506675e-36)\n",
      " state (13)  A[0]:(8.02704289526e-06) A[1]:(0.999971687794) A[2]:(2.02737319341e-05) A[3]:(5.16193153974e-36)\n",
      " state (14)  A[0]:(8.01535497885e-06) A[1]:(0.999971747398) A[2]:(2.02513165277e-05) A[3]:(5.15657853476e-36)\n",
      " state (15)  A[0]:(8.01052556199e-06) A[1]:(0.999971747398) A[2]:(2.02450628422e-05) A[3]:(5.1547690885e-36)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 586000 finished after 14 . Running score: 0.12. Policy_loss: -92050.6113014, Value_loss: 1.42145683064. Times trained:               12621. Times reached goal: 122.               Steps done: 7059199.\n",
      " state (0)  A[0]:(0.996487498283) A[1]:(0.0012017696863) A[2]:(0.00150353438221) A[3]:(0.000807224889286)\n",
      " state (1)  A[0]:(0.0178075134754) A[1]:(0.00254163960926) A[2]:(0.00520603684708) A[3]:(0.974444806576)\n",
      " state (2)  A[0]:(0.99996984005) A[1]:(6.12363510299e-06) A[2]:(2.4014749215e-05) A[3]:(4.40630154586e-09)\n",
      " state (3)  A[0]:(0.999999403954) A[1]:(8.94762521852e-08) A[2]:(5.26493352027e-07) A[3]:(3.97621172099e-13)\n",
      " state (4)  A[0]:(0.999999403954) A[1]:(1.03738265977e-07) A[2]:(4.74267892514e-07) A[3]:(2.2997454093e-13)\n",
      " state (5)  A[0]:(0.999985814095) A[1]:(5.97206144448e-06) A[2]:(8.19028809929e-06) A[3]:(6.07021405361e-18)\n",
      " state (6)  A[0]:(0.400252699852) A[1]:(0.138973131776) A[2]:(0.460774153471) A[3]:(2.55594107879e-31)\n",
      " state (7)  A[0]:(0.0632103905082) A[1]:(0.874175131321) A[2]:(0.0626145005226) A[3]:(7.06266319864e-33)\n",
      " state (8)  A[0]:(0.000695375667419) A[1]:(0.998763561249) A[2]:(0.000541034620255) A[3]:(1.06394693939e-34)\n",
      " state (9)  A[0]:(3.47722780134e-05) A[1]:(0.999919116497) A[2]:(4.6134191507e-05) A[3]:(1.14962881158e-35)\n",
      " state (10)  A[0]:(2.04220450541e-05) A[1]:(0.999951601028) A[2]:(2.80052445305e-05) A[3]:(7.49433036933e-36)\n",
      " state (11)  A[0]:(1.84013697435e-05) A[1]:(0.9999563694) A[2]:(2.52400204772e-05) A[3]:(6.86777910947e-36)\n",
      " state (12)  A[0]:(1.79472353921e-05) A[1]:(0.999957442284) A[2]:(2.46338331635e-05) A[3]:(6.72760800881e-36)\n",
      " state (13)  A[0]:(1.78191239684e-05) A[1]:(0.999957680702) A[2]:(2.44802995439e-05) A[3]:(6.69055023371e-36)\n",
      " state (14)  A[0]:(1.77732363227e-05) A[1]:(0.999957799911) A[2]:(2.443617268e-05) A[3]:(6.67887205894e-36)\n",
      " state (15)  A[0]:(1.77523652383e-05) A[1]:(0.999957799911) A[2]:(2.44213551923e-05) A[3]:(6.67433839878e-36)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 587000 finished after 20 . Running score: 0.12. Policy_loss: -92050.6112094, Value_loss: 1.41831191722. Times trained:               12968. Times reached goal: 121.               Steps done: 7072167.\n",
      " state (0)  A[0]:(0.996083498001) A[1]:(0.00109513395) A[2]:(0.00176417187322) A[3]:(0.00105719070416)\n",
      " state (1)  A[0]:(0.0174021888524) A[1]:(0.00224223942496) A[2]:(0.00592094333842) A[3]:(0.974434614182)\n",
      " state (2)  A[0]:(0.0200432315469) A[1]:(0.00227090483531) A[2]:(0.00626976275817) A[3]:(0.971416115761)\n",
      " state (3)  A[0]:(0.0452890321612) A[1]:(0.00289599178359) A[2]:(0.0093163587153) A[3]:(0.942498624325)\n",
      " state (4)  A[0]:(0.999980986118) A[1]:(1.88891465314e-06) A[2]:(1.71074589161e-05) A[3]:(1.01063746261e-09)\n",
      " state (5)  A[0]:(0.999999046326) A[1]:(6.80957654708e-08) A[2]:(8.69755808708e-07) A[3]:(3.14080114997e-13)\n",
      " state (6)  A[0]:(0.999634027481) A[1]:(1.06231914287e-06) A[2]:(0.000364892184734) A[3]:(1.13643747012e-17)\n",
      " state (7)  A[0]:(0.353876620531) A[1]:(4.36893387814e-05) A[2]:(0.646079659462) A[3]:(1.14786509369e-25)\n",
      " state (8)  A[0]:(0.0848191231489) A[1]:(9.38964585657e-05) A[2]:(0.915086984634) A[3]:(1.32126509942e-28)\n",
      " state (9)  A[0]:(0.0473374091089) A[1]:(0.000159073446412) A[2]:(0.952503502369) A[3]:(4.85655171308e-30)\n",
      " state (10)  A[0]:(0.0356665030122) A[1]:(0.000249451230047) A[2]:(0.964084029198) A[3]:(6.42919472965e-31)\n",
      " state (11)  A[0]:(0.0309886708856) A[1]:(0.000389007967897) A[2]:(0.968622326851) A[3]:(1.62324426328e-31)\n",
      " state (12)  A[0]:(0.0294571761042) A[1]:(0.000650580332149) A[2]:(0.969892263412) A[3]:(5.98676040264e-32)\n",
      " state (13)  A[0]:(0.0305735263973) A[1]:(0.00134926964529) A[2]:(0.96807718277) A[3]:(2.85448819693e-32)\n",
      " state (14)  A[0]:(0.0363990888) A[1]:(0.00463419593871) A[2]:(0.958966732025) A[3]:(1.76169648585e-32)\n",
      " state (15)  A[0]:(0.0511648729444) A[1]:(0.036029484123) A[2]:(0.912805616856) A[3]:(1.48214835392e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 588000 finished after 6 . Running score: 0.08. Policy_loss: -92050.6379296, Value_loss: 1.20125071146. Times trained:               14697. Times reached goal: 114.               Steps done: 7086864.\n",
      " state (0)  A[0]:(0.996224224567) A[1]:(0.00110613217112) A[2]:(0.00193308177404) A[3]:(0.00073655199958)\n",
      " state (1)  A[0]:(0.0132495909929) A[1]:(0.00197332026437) A[2]:(0.00626371521503) A[3]:(0.978513360023)\n",
      " state (2)  A[0]:(0.0143829276785) A[1]:(0.0019845420029) A[2]:(0.0064917341806) A[3]:(0.977140784264)\n",
      " state (3)  A[0]:(0.0282126255333) A[1]:(0.00246386742219) A[2]:(0.00922158453614) A[3]:(0.960101902485)\n",
      " state (4)  A[0]:(0.999989271164) A[1]:(5.81880556183e-07) A[2]:(1.01565474324e-05) A[3]:(4.23577457831e-11)\n",
      " state (5)  A[0]:(0.996459424496) A[1]:(8.25995721243e-06) A[2]:(0.00353233912028) A[3]:(3.49315262482e-17)\n",
      " state (6)  A[0]:(0.00078559742542) A[1]:(1.43426314025e-05) A[2]:(0.999200046062) A[3]:(1.04346028627e-30)\n",
      " state (7)  A[0]:(0.000262648623902) A[1]:(1.93082105397e-05) A[2]:(0.99971807003) A[3]:(2.16092772204e-32)\n",
      " state (8)  A[0]:(0.000205521369935) A[1]:(2.41927773459e-05) A[2]:(0.999770283699) A[3]:(8.59510742935e-33)\n",
      " state (9)  A[0]:(0.000189786282135) A[1]:(2.89713007078e-05) A[2]:(0.999781250954) A[3]:(5.9422411255e-33)\n",
      " state (10)  A[0]:(0.000184638352948) A[1]:(3.35423828801e-05) A[2]:(0.999781847) A[3]:(4.93374117558e-33)\n",
      " state (11)  A[0]:(0.000183002572157) A[1]:(3.81232457585e-05) A[2]:(0.999778866768) A[3]:(4.41669704663e-33)\n",
      " state (12)  A[0]:(0.000182798496098) A[1]:(4.39785435447e-05) A[2]:(0.999773204327) A[3]:(4.0647330995e-33)\n",
      " state (13)  A[0]:(0.000183625234058) A[1]:(5.49700671399e-05) A[2]:(0.999761402607) A[3]:(3.74048032967e-33)\n",
      " state (14)  A[0]:(0.000184958626051) A[1]:(8.53320161696e-05) A[2]:(0.999729692936) A[3]:(3.39653031527e-33)\n",
      " state (15)  A[0]:(0.00017819974164) A[1]:(0.000217427266762) A[2]:(0.999604344368) A[3]:(3.08074385983e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 589000 finished after 21 . Running score: 0.06. Policy_loss: -92050.6112201, Value_loss: 0.992377562914. Times trained:               16743. Times reached goal: 107.               Steps done: 7103607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9952,  0.0011,  0.0021,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  6.3460e-07,  1.2513e-05,  5.0502e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.9879e-06,  8.9037e-06,  9.9998e-01,  6.8838e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  6.3453e-07,  1.2512e-05,  5.0488e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.9908e-06,  8.9048e-06,  9.9998e-01,  6.8841e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  6.3443e-07,  1.2510e-05,  5.0470e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.9943e-06,  8.9059e-06,  9.9998e-01,  6.8843e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  6.3431e-07,  1.2508e-05,  5.0448e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.9981e-06,  8.9069e-06,  9.9998e-01,  6.8846e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.9228e-06,  1.0064e-05,  9.9998e-01,  4.3744e-33]])\n",
      "On state=9, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.4813e-06,  1.1058e-05,  9.9998e-01,  3.4907e-33]])\n",
      "On state=10, selected action=2\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995224475861) A[1]:(0.00110140652396) A[2]:(0.00207275734283) A[3]:(0.0016013860004)\n",
      " state (1)  A[0]:(0.0131361708045) A[1]:(0.00191463099327) A[2]:(0.0061224619858) A[3]:(0.978826761246)\n",
      " state (2)  A[0]:(0.0142086371779) A[1]:(0.00192457286175) A[2]:(0.00633329013363) A[3]:(0.977533519268)\n",
      " state (3)  A[0]:(0.0263692028821) A[1]:(0.00231578201056) A[2]:(0.00870074983686) A[3]:(0.962614238262)\n",
      " state (4)  A[0]:(0.999986886978) A[1]:(6.34073614947e-07) A[2]:(1.25031310745e-05) A[3]:(5.0404094093e-11)\n",
      " state (5)  A[0]:(0.980601608753) A[1]:(1.96551318368e-05) A[2]:(0.0193787664175) A[3]:(4.65225467828e-17)\n",
      " state (6)  A[0]:(5.14093298989e-05) A[1]:(6.25911025054e-06) A[2]:(0.999942302704) A[3]:(1.46656452556e-30)\n",
      " state (7)  A[0]:(1.16600640467e-05) A[1]:(7.60409648137e-06) A[2]:(0.9999807477) A[3]:(2.08394591943e-32)\n",
      " state (8)  A[0]:(8.00221732788e-06) A[1]:(8.9083396233e-06) A[2]:(0.999983072281) A[3]:(6.88489245329e-33)\n",
      " state (9)  A[0]:(6.92536741553e-06) A[1]:(1.00649795058e-05) A[2]:(0.999983012676) A[3]:(4.37456806378e-33)\n",
      " state (10)  A[0]:(6.4833561737e-06) A[1]:(1.10587661766e-05) A[2]:(0.999982476234) A[3]:(3.49082222728e-33)\n",
      " state (11)  A[0]:(6.26891642241e-06) A[1]:(1.18528269013e-05) A[2]:(0.999981880188) A[3]:(3.08510696425e-33)\n",
      " state (12)  A[0]:(6.14831833445e-06) A[1]:(1.24646549011e-05) A[2]:(0.999981403351) A[3]:(2.86910399982e-33)\n",
      " state (13)  A[0]:(6.06455114394e-06) A[1]:(1.29628169816e-05) A[2]:(0.999980986118) A[3]:(2.73767528253e-33)\n",
      " state (14)  A[0]:(5.98525775786e-06) A[1]:(1.34503761728e-05) A[2]:(0.999980568886) A[3]:(2.64098352533e-33)\n",
      " state (15)  A[0]:(5.88235570831e-06) A[1]:(1.40749098136e-05) A[2]:(0.999980032444) A[3]:(2.5487951972e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 590000 finished after 11 . Running score: 0.05. Policy_loss: -92050.6080516, Value_loss: 0.987088935325. Times trained:               16509. Times reached goal: 93.               Steps done: 7120116.\n",
      " state (0)  A[0]:(0.99490070343) A[1]:(0.00150174868759) A[2]:(0.00199532904662) A[3]:(0.00160224491265)\n",
      " state (1)  A[0]:(0.0129018500447) A[1]:(0.0020453853067) A[2]:(0.00574299134314) A[3]:(0.979309797287)\n",
      " state (2)  A[0]:(0.013841887936) A[1]:(0.00206546951085) A[2]:(0.00591978849843) A[3]:(0.978172838688)\n",
      " state (3)  A[0]:(0.0221298895776) A[1]:(0.00237148115411) A[2]:(0.00748343765736) A[3]:(0.968015193939)\n",
      " state (4)  A[0]:(0.999967396259) A[1]:(1.71610759025e-06) A[2]:(3.08627131744e-05) A[3]:(6.84897805137e-10)\n",
      " state (5)  A[0]:(0.998901307583) A[1]:(2.99581324725e-06) A[2]:(0.00109570496716) A[3]:(6.69496069708e-15)\n",
      " state (6)  A[0]:(4.70875147585e-06) A[1]:(3.26593806221e-07) A[2]:(0.99999499321) A[3]:(8.92967048365e-30)\n",
      " state (7)  A[0]:(2.70480313702e-07) A[1]:(2.06099741717e-07) A[2]:(0.999999523163) A[3]:(1.39815031941e-32)\n",
      " state (8)  A[0]:(1.29442184971e-07) A[1]:(1.99687889335e-07) A[2]:(0.999999642372) A[3]:(2.55685008857e-33)\n",
      " state (9)  A[0]:(9.6300112773e-08) A[1]:(2.01285558887e-07) A[2]:(0.999999701977) A[3]:(1.30530364097e-33)\n",
      " state (10)  A[0]:(8.33573352566e-08) A[1]:(2.03756158612e-07) A[2]:(0.999999701977) A[3]:(9.47042901212e-34)\n",
      " state (11)  A[0]:(7.70991093191e-08) A[1]:(2.05889193694e-07) A[2]:(0.999999701977) A[3]:(8.00080117914e-34)\n",
      " state (12)  A[0]:(7.36780165767e-08) A[1]:(2.07515086004e-07) A[2]:(0.999999701977) A[3]:(7.28053446441e-34)\n",
      " state (13)  A[0]:(7.16002119816e-08) A[1]:(2.08737745311e-07) A[2]:(0.999999701977) A[3]:(6.8860279992e-34)\n",
      " state (14)  A[0]:(7.01510813883e-08) A[1]:(2.09736271017e-07) A[2]:(0.999999701977) A[3]:(6.64753856251e-34)\n",
      " state (15)  A[0]:(6.89105945639e-08) A[1]:(2.10709018234e-07) A[2]:(0.999999701977) A[3]:(6.48310343322e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 591000 finished after 6 . Running score: 0.07. Policy_loss: -92050.6114365, Value_loss: 0.992384469129. Times trained:               15625. Times reached goal: 116.               Steps done: 7135741.\n",
      " state (0)  A[0]:(0.994237482548) A[1]:(0.0015199019108) A[2]:(0.00194560422096) A[3]:(0.00229700119235)\n",
      " state (1)  A[0]:(0.0121757732704) A[1]:(0.00195007072762) A[2]:(0.00521638337523) A[3]:(0.980657756329)\n",
      " state (2)  A[0]:(0.012736977078) A[1]:(0.00196305010468) A[2]:(0.00531342159957) A[3]:(0.979986548424)\n",
      " state (3)  A[0]:(0.019592307508) A[1]:(0.0022811209783) A[2]:(0.00662085460499) A[3]:(0.971505701542)\n",
      " state (4)  A[0]:(0.999991714954) A[1]:(4.90485660976e-07) A[2]:(7.80520349508e-06) A[3]:(6.64113833371e-11)\n",
      " state (5)  A[0]:(0.998501598835) A[1]:(1.53709152073e-06) A[2]:(0.00149689184036) A[3]:(2.72384450607e-15)\n",
      " state (6)  A[0]:(4.0944569264e-06) A[1]:(2.66404196481e-08) A[2]:(0.99999588728) A[3]:(7.07342549947e-29)\n",
      " state (7)  A[0]:(7.40482661854e-08) A[1]:(8.00805022294e-09) A[2]:(0.999999940395) A[3]:(4.15374417661e-32)\n",
      " state (8)  A[0]:(2.09746620072e-08) A[1]:(6.30234975318e-09) A[2]:(1.0) A[3]:(4.5771413726e-33)\n",
      " state (9)  A[0]:(1.22500098954e-08) A[1]:(5.90305493375e-09) A[2]:(1.0) A[3]:(1.91006095432e-33)\n",
      " state (10)  A[0]:(9.33148225357e-09) A[1]:(5.79614933827e-09) A[2]:(1.0) A[3]:(1.26953380705e-33)\n",
      " state (11)  A[0]:(7.9700477329e-09) A[1]:(5.77954573089e-09) A[2]:(1.0) A[3]:(1.02309205925e-33)\n",
      " state (12)  A[0]:(7.16551173952e-09) A[1]:(5.80010839357e-09) A[2]:(1.0) A[3]:(9.00777004767e-34)\n",
      " state (13)  A[0]:(6.55823351181e-09) A[1]:(5.84684789473e-09) A[2]:(1.0) A[3]:(8.26252683306e-34)\n",
      " state (14)  A[0]:(5.96881166715e-09) A[1]:(5.92538995647e-09) A[2]:(1.0) A[3]:(7.68702224764e-34)\n",
      " state (15)  A[0]:(5.29402832683e-09) A[1]:(6.05074612636e-09) A[2]:(1.0) A[3]:(7.11736528744e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 592000 finished after 33 . Running score: 0.15. Policy_loss: -92050.626176, Value_loss: 1.23566805732. Times trained:               16172. Times reached goal: 121.               Steps done: 7151913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995713472366) A[1]:(0.00131993577816) A[2]:(0.00166486599483) A[3]:(0.00130171712954)\n",
      " state (1)  A[0]:(0.0116835646331) A[1]:(0.00177003699355) A[2]:(0.00507888337597) A[3]:(0.981467485428)\n",
      " state (2)  A[0]:(0.0120825106278) A[1]:(0.00178031623363) A[2]:(0.00514284707606) A[3]:(0.980994343758)\n",
      " state (3)  A[0]:(0.0170594751835) A[1]:(0.0019994575996) A[2]:(0.00610209954903) A[3]:(0.974838972092)\n",
      " state (4)  A[0]:(0.999995529652) A[1]:(2.92103237598e-07) A[2]:(4.20014021074e-06) A[3]:(3.49828568891e-11)\n",
      " state (5)  A[0]:(0.428798496723) A[1]:(3.44547402165e-06) A[2]:(0.571198046207) A[3]:(3.9094274734e-19)\n",
      " state (6)  A[0]:(8.34642435166e-07) A[1]:(3.44693096288e-09) A[2]:(0.999999165535) A[3]:(1.3019084239e-30)\n",
      " state (7)  A[0]:(7.68477761426e-08) A[1]:(1.65750746461e-09) A[2]:(0.999999940395) A[3]:(1.59210380162e-32)\n",
      " state (8)  A[0]:(3.39569048435e-08) A[1]:(1.44404188607e-09) A[2]:(0.999999940395) A[3]:(4.4172689981e-33)\n",
      " state (9)  A[0]:(2.28466916496e-08) A[1]:(1.40821976302e-09) A[2]:(1.0) A[3]:(2.65764964751e-33)\n",
      " state (10)  A[0]:(1.76816481456e-08) A[1]:(1.41912381846e-09) A[2]:(1.0) A[3]:(2.05974509888e-33)\n",
      " state (11)  A[0]:(1.38908795577e-08) A[1]:(1.45928702455e-09) A[2]:(1.0) A[3]:(1.73275985569e-33)\n",
      " state (12)  A[0]:(1.01903854244e-08) A[1]:(1.5385533958e-09) A[2]:(1.0) A[3]:(1.46428227242e-33)\n",
      " state (13)  A[0]:(6.95541890749e-09) A[1]:(1.65904157079e-09) A[2]:(1.0) A[3]:(1.20802891205e-33)\n",
      " state (14)  A[0]:(5.11887066068e-09) A[1]:(1.78346681867e-09) A[2]:(1.0) A[3]:(9.99971738409e-34)\n",
      " state (15)  A[0]:(4.41917702432e-09) A[1]:(1.92466864668e-09) A[2]:(1.0) A[3]:(8.5715027681e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 593000 finished after 12 . Running score: 0.17. Policy_loss: -92050.6112067, Value_loss: 0.979596206516. Times trained:               16219. Times reached goal: 97.               Steps done: 7168132.\n",
      " state (0)  A[0]:(0.994945287704) A[1]:(0.00133700971492) A[2]:(0.00162329128943) A[3]:(0.00209441804327)\n",
      " state (1)  A[0]:(0.0135132139549) A[1]:(0.00187696143985) A[2]:(0.00512698106468) A[3]:(0.979482829571)\n",
      " state (2)  A[0]:(0.0253899972886) A[1]:(0.00233385222964) A[2]:(0.00698730535805) A[3]:(0.965288817883)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(4.55303350577e-08) A[2]:(7.03473119756e-07) A[3]:(1.19095076682e-12)\n",
      " state (4)  A[0]:(0.999999344349) A[1]:(3.168530327e-08) A[2]:(6.01610395279e-07) A[3]:(3.3060541609e-13)\n",
      " state (5)  A[0]:(0.0030015327502) A[1]:(2.52585664384e-07) A[2]:(0.99699819088) A[3]:(2.4433590963e-25)\n",
      " state (6)  A[0]:(8.82073010189e-08) A[1]:(2.2304327274e-09) A[2]:(0.999999880791) A[3]:(7.9218761007e-33)\n",
      " state (7)  A[0]:(2.88474488741e-08) A[1]:(1.83882398197e-09) A[2]:(0.999999940395) A[3]:(2.05815836518e-33)\n",
      " state (8)  A[0]:(2.10083737073e-08) A[1]:(1.79307857451e-09) A[2]:(1.0) A[3]:(1.47183133388e-33)\n",
      " state (9)  A[0]:(1.79700307967e-08) A[1]:(1.79262715783e-09) A[2]:(1.0) A[3]:(1.28034045723e-33)\n",
      " state (10)  A[0]:(1.59347219864e-08) A[1]:(1.80834869301e-09) A[2]:(1.0) A[3]:(1.17570447044e-33)\n",
      " state (11)  A[0]:(1.39449269909e-08) A[1]:(1.83660886499e-09) A[2]:(1.0) A[3]:(1.08580697875e-33)\n",
      " state (12)  A[0]:(1.16881047063e-08) A[1]:(1.87899407145e-09) A[2]:(1.0) A[3]:(9.81408937736e-34)\n",
      " state (13)  A[0]:(9.39215727414e-09) A[1]:(1.93336657794e-09) A[2]:(1.0) A[3]:(8.5694107555e-34)\n",
      " state (14)  A[0]:(7.65758922938e-09) A[1]:(1.99435024051e-09) A[2]:(1.0) A[3]:(7.31957878478e-34)\n",
      " state (15)  A[0]:(6.6861289838e-09) A[1]:(2.09680117713e-09) A[2]:(1.0) A[3]:(6.28579900232e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 594000 finished after 6 . Running score: 0.13. Policy_loss: -92050.6111983, Value_loss: 1.20334972609. Times trained:               15845. Times reached goal: 127.               Steps done: 7183977.\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0012,  0.0016,  0.0030]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0012,  0.0016,  0.0030]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0012,  0.0016,  0.0030]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6668e-08,  4.3809e-07,  3.1322e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0012,  0.0016,  0.0030]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0012,  0.0016,  0.0030]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6668e-08,  4.3808e-07,  3.1326e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.9222e-08,  6.7863e-10,  1.0000e+00,  1.0055e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6667e-08,  4.3801e-07,  3.1331e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.9223e-08,  6.7863e-10,  1.0000e+00,  1.0056e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7111e-08,  6.5476e-10,  1.0000e+00,  8.2828e-32]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6041e-08,  6.4308e-10,  1.0000e+00,  7.4033e-32]])\n",
      "On state=13, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6040e-08,  6.4308e-10,  1.0000e+00,  7.4035e-32]])\n",
      "On state=13, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6040e-08,  6.4308e-10,  1.0000e+00,  7.4036e-32]])\n",
      "On state=13, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6039e-08,  6.4308e-10,  1.0000e+00,  7.4037e-32]])\n",
      "On state=13, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7108e-08,  6.5477e-10,  1.0000e+00,  8.2836e-32]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6039e-08,  6.4308e-10,  1.0000e+00,  7.4039e-32]])\n",
      "On state=13, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7108e-08,  6.5477e-10,  1.0000e+00,  8.2838e-32]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994211494923) A[1]:(0.00122216809541) A[2]:(0.00158620334696) A[3]:(0.00298013933934)\n",
      " state (1)  A[0]:(0.0133562106639) A[1]:(0.00166315061506) A[2]:(0.00468081096187) A[3]:(0.980299830437)\n",
      " state (2)  A[0]:(0.999998688698) A[1]:(7.63861933706e-08) A[2]:(1.25016038055e-06) A[3]:(7.23503678843e-12)\n",
      " state (3)  A[0]:(0.999999642372) A[1]:(1.39819151812e-08) A[2]:(3.23930464674e-07) A[3]:(3.96000100116e-13)\n",
      " state (4)  A[0]:(0.999999523163) A[1]:(1.6658908919e-08) A[2]:(4.37652573737e-07) A[3]:(3.13495513186e-13)\n",
      " state (5)  A[0]:(0.598050475121) A[1]:(1.38746338507e-06) A[2]:(0.401948094368) A[3]:(2.31398684031e-19)\n",
      " state (6)  A[0]:(3.58792618727e-07) A[1]:(1.82122450454e-09) A[2]:(0.999999642372) A[3]:(9.61635704374e-30)\n",
      " state (7)  A[0]:(3.07450456205e-08) A[1]:(7.88414833419e-10) A[2]:(0.999999940395) A[3]:(2.14191245522e-31)\n",
      " state (8)  A[0]:(1.9220687264e-08) A[1]:(6.78639255902e-10) A[2]:(1.0) A[3]:(1.0057340411e-31)\n",
      " state (9)  A[0]:(1.71085083878e-08) A[1]:(6.54768128605e-10) A[2]:(1.0) A[3]:(8.28386973297e-32)\n",
      " state (10)  A[0]:(1.64931535096e-08) A[1]:(6.47821518651e-10) A[2]:(1.0) A[3]:(7.77574349192e-32)\n",
      " state (11)  A[0]:(1.62500288781e-08) A[1]:(6.45207332006e-10) A[2]:(1.0) A[3]:(7.57397635182e-32)\n",
      " state (12)  A[0]:(1.61245239383e-08) A[1]:(6.43926245658e-10) A[2]:(1.0) A[3]:(7.47090254192e-32)\n",
      " state (13)  A[0]:(1.60397402027e-08) A[1]:(6.43081810026e-10) A[2]:(1.0) A[3]:(7.40400280743e-32)\n",
      " state (14)  A[0]:(1.59649609088e-08) A[1]:(6.423389598e-10) A[2]:(1.0) A[3]:(7.34795406128e-32)\n",
      " state (15)  A[0]:(1.58824402519e-08) A[1]:(6.41507624799e-10) A[2]:(1.0) A[3]:(7.2888249322e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 595000 finished after 18 . Running score: 0.08. Policy_loss: -92050.6111957, Value_loss: 1.21124464836. Times trained:               15062. Times reached goal: 124.               Steps done: 7199039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996232748032) A[1]:(0.0010401395848) A[2]:(0.00133943243418) A[3]:(0.00138769764453)\n",
      " state (1)  A[0]:(0.0130566377193) A[1]:(0.00156877015252) A[2]:(0.00470800697803) A[3]:(0.980666577816)\n",
      " state (2)  A[0]:(0.999998152256) A[1]:(9.86357449051e-08) A[2]:(1.73907972112e-06) A[3]:(1.46732660744e-11)\n",
      " state (3)  A[0]:(0.999999642372) A[1]:(1.15749116958e-08) A[2]:(3.21887824839e-07) A[3]:(3.93646757985e-13)\n",
      " state (4)  A[0]:(0.999999642372) A[1]:(1.12896039184e-08) A[2]:(3.3548758438e-07) A[3]:(3.29261330657e-13)\n",
      " state (5)  A[0]:(0.999971628189) A[1]:(6.41762980536e-08) A[2]:(2.83147492155e-05) A[3]:(4.2608510104e-15)\n",
      " state (6)  A[0]:(2.92565528071e-06) A[1]:(2.23662510734e-09) A[2]:(0.999997079372) A[3]:(1.39738194342e-28)\n",
      " state (7)  A[0]:(8.81668960062e-09) A[1]:(2.89176599333e-10) A[2]:(1.0) A[3]:(4.25435679521e-32)\n",
      " state (8)  A[0]:(3.26621907298e-09) A[1]:(2.19063572837e-10) A[2]:(1.0) A[3]:(1.11001005846e-32)\n",
      " state (9)  A[0]:(2.61521138079e-09) A[1]:(2.06518219192e-10) A[2]:(1.0) A[3]:(8.19995177901e-33)\n",
      " state (10)  A[0]:(2.45662779008e-09) A[1]:(2.03261393827e-10) A[2]:(1.0) A[3]:(7.52854614002e-33)\n",
      " state (11)  A[0]:(2.38668795838e-09) A[1]:(2.01967539915e-10) A[2]:(1.0) A[3]:(7.2475927046e-33)\n",
      " state (12)  A[0]:(2.3180803943e-09) A[1]:(2.00871874689e-10) A[2]:(1.0) A[3]:(6.98911178243e-33)\n",
      " state (13)  A[0]:(2.2165649316e-09) A[1]:(1.99291944059e-10) A[2]:(1.0) A[3]:(6.6177136076e-33)\n",
      " state (14)  A[0]:(2.05680228405e-09) A[1]:(1.96678798248e-10) A[2]:(1.0) A[3]:(6.04082432657e-33)\n",
      " state (15)  A[0]:(1.81883674788e-09) A[1]:(1.92369883911e-10) A[2]:(1.0) A[3]:(5.19244066623e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 596000 finished after 26 . Running score: 0.07. Policy_loss: -92050.611195, Value_loss: 1.22503450397. Times trained:               15133. Times reached goal: 110.               Steps done: 7214172.\n",
      " state (0)  A[0]:(0.995671570301) A[1]:(0.00105515727773) A[2]:(0.00138661835808) A[3]:(0.00188666605391)\n",
      " state (1)  A[0]:(0.0138200810179) A[1]:(0.00161389925051) A[2]:(0.0047408817336) A[3]:(0.979825139046)\n",
      " state (2)  A[0]:(0.999997913837) A[1]:(1.22196070151e-07) A[2]:(1.94091398953e-06) A[3]:(2.27906825356e-11)\n",
      " state (3)  A[0]:(0.999999701977) A[1]:(1.11784626e-08) A[2]:(2.86273859729e-07) A[3]:(3.59444326517e-13)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(9.96619675675e-09) A[2]:(2.6750464599e-07) A[3]:(3.04951729016e-13)\n",
      " state (5)  A[0]:(0.999999642372) A[1]:(1.2305553021e-08) A[2]:(3.7346549675e-07) A[3]:(2.352025367e-13)\n",
      " state (6)  A[0]:(0.999977588654) A[1]:(6.67141009103e-08) A[2]:(2.23638107855e-05) A[3]:(4.04245034121e-15)\n",
      " state (7)  A[0]:(0.000476070010336) A[1]:(3.18037152169e-08) A[2]:(0.999523878098) A[3]:(2.6519605437e-25)\n",
      " state (8)  A[0]:(8.13135514477e-08) A[1]:(7.51278261824e-10) A[2]:(0.999999940395) A[3]:(6.64189761124e-31)\n",
      " state (9)  A[0]:(1.11667457503e-08) A[1]:(3.92147980754e-10) A[2]:(1.0) A[3]:(4.14108762893e-32)\n",
      " state (10)  A[0]:(6.61577503891e-09) A[1]:(3.34615529729e-10) A[2]:(1.0) A[3]:(1.97954673965e-32)\n",
      " state (11)  A[0]:(5.62206503574e-09) A[1]:(3.18984366698e-10) A[2]:(1.0) A[3]:(1.57146785136e-32)\n",
      " state (12)  A[0]:(5.27920551718e-09) A[1]:(3.13436970822e-10) A[2]:(1.0) A[3]:(1.43814842487e-32)\n",
      " state (13)  A[0]:(5.07456165977e-09) A[1]:(3.10382664015e-10) A[2]:(1.0) A[3]:(1.36227658281e-32)\n",
      " state (14)  A[0]:(4.8529797958e-09) A[1]:(3.07287778556e-10) A[2]:(1.0) A[3]:(1.28326999119e-32)\n",
      " state (15)  A[0]:(4.54088455726e-09) A[1]:(3.02860903023e-10) A[2]:(1.0) A[3]:(1.17457988969e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 597000 finished after 27 . Running score: 0.09. Policy_loss: -92050.6111933, Value_loss: 1.19288964546. Times trained:               14971. Times reached goal: 102.               Steps done: 7229143.\n",
      " state (0)  A[0]:(0.995285272598) A[1]:(0.00101598689798) A[2]:(0.00143535609823) A[3]:(0.00226339395158)\n",
      " state (1)  A[0]:(0.0140005480498) A[1]:(0.00158159586135) A[2]:(0.00479731010273) A[3]:(0.979620575905)\n",
      " state (2)  A[0]:(0.999999463558) A[1]:(2.42583624299e-08) A[2]:(5.12419660481e-07) A[3]:(1.01234266196e-12)\n",
      " state (3)  A[0]:(0.999999701977) A[1]:(1.01104058459e-08) A[2]:(2.75937196648e-07) A[3]:(3.3991285806e-13)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(1.00419104143e-08) A[2]:(2.83243196009e-07) A[3]:(3.0777818987e-13)\n",
      " state (5)  A[0]:(0.999999582767) A[1]:(1.2661397264e-08) A[2]:(4.30254800676e-07) A[3]:(1.97551420751e-13)\n",
      " state (6)  A[0]:(0.999997138977) A[1]:(2.64078874324e-08) A[2]:(2.81325083051e-06) A[3]:(2.10286940584e-14)\n",
      " state (7)  A[0]:(0.963079154491) A[1]:(4.6784637675e-07) A[2]:(0.0369203910232) A[3]:(5.45177215844e-19)\n",
      " state (8)  A[0]:(3.75078889192e-05) A[1]:(5.55133672364e-09) A[2]:(0.999962508678) A[3]:(9.59464787113e-28)\n",
      " state (9)  A[0]:(1.74829580146e-07) A[1]:(7.43995698382e-10) A[2]:(0.999999821186) A[3]:(5.94508854016e-31)\n",
      " state (10)  A[0]:(3.62918726182e-08) A[1]:(4.56155946349e-10) A[2]:(0.999999940395) A[3]:(7.52145879071e-32)\n",
      " state (11)  A[0]:(2.2162533142e-08) A[1]:(3.9448105893e-10) A[2]:(1.0) A[3]:(3.91834055953e-32)\n",
      " state (12)  A[0]:(1.86100450605e-08) A[1]:(3.75242281692e-10) A[2]:(1.0) A[3]:(3.10780840503e-32)\n",
      " state (13)  A[0]:(1.72513114904e-08) A[1]:(3.67612607022e-10) A[2]:(1.0) A[3]:(2.81234437347e-32)\n",
      " state (14)  A[0]:(1.6396452196e-08) A[1]:(3.63101188006e-10) A[2]:(1.0) A[3]:(2.63344059849e-32)\n",
      " state (15)  A[0]:(1.54942085828e-08) A[1]:(3.58567148195e-10) A[2]:(1.0) A[3]:(2.45007126741e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 598000 finished after 47 . Running score: 0.1. Policy_loss: -92050.6111828, Value_loss: 1.01134860551. Times trained:               16284. Times reached goal: 79.               Steps done: 7245427.\n",
      " state (0)  A[0]:(0.995614647865) A[1]:(0.0010434781434) A[2]:(0.00138011889067) A[3]:(0.00196174765006)\n",
      " state (1)  A[0]:(0.0139205846936) A[1]:(0.00163035979494) A[2]:(0.00463656755164) A[3]:(0.979812502861)\n",
      " state (2)  A[0]:(0.999999523163) A[1]:(2.3826309814e-08) A[2]:(4.43465694389e-07) A[3]:(9.30075352672e-13)\n",
      " state (3)  A[0]:(0.999999761581) A[1]:(1.02667625512e-08) A[2]:(2.44528848725e-07) A[3]:(3.39853877462e-13)\n",
      " state (4)  A[0]:(0.999999761581) A[1]:(1.02652739642e-08) A[2]:(2.53588808619e-07) A[3]:(3.03008838723e-13)\n",
      " state (5)  A[0]:(0.999999582767) A[1]:(1.24054251316e-08) A[2]:(3.83615599731e-07) A[3]:(1.81143728489e-13)\n",
      " state (6)  A[0]:(0.999998211861) A[1]:(1.97690734893e-08) A[2]:(1.7598327986e-06) A[3]:(2.29628768528e-14)\n",
      " state (7)  A[0]:(0.997621774673) A[1]:(1.19923981856e-07) A[2]:(0.00237810984254) A[3]:(3.16160217441e-18)\n",
      " state (8)  A[0]:(0.00457023549825) A[1]:(3.33702132593e-08) A[2]:(0.995429754257) A[3]:(3.79272021931e-25)\n",
      " state (9)  A[0]:(1.03011971078e-05) A[1]:(3.16233395026e-09) A[2]:(0.999989688396) A[3]:(4.70854572718e-29)\n",
      " state (10)  A[0]:(9.24778078115e-07) A[1]:(1.54532209251e-09) A[2]:(0.999999046326) A[3]:(2.10390750034e-30)\n",
      " state (11)  A[0]:(4.04235521501e-07) A[1]:(1.22641652389e-09) A[2]:(0.999999582767) A[3]:(7.38113262443e-31)\n",
      " state (12)  A[0]:(3.02665512208e-07) A[1]:(1.13265674617e-09) A[2]:(0.999999701977) A[3]:(5.11697675009e-31)\n",
      " state (13)  A[0]:(2.7260230695e-07) A[1]:(1.10070408343e-09) A[2]:(0.999999701977) A[3]:(4.48115561731e-31)\n",
      " state (14)  A[0]:(2.62109608684e-07) A[1]:(1.08901410112e-09) A[2]:(0.999999761581) A[3]:(4.26330971283e-31)\n",
      " state (15)  A[0]:(2.5797854164e-07) A[1]:(1.0844043441e-09) A[2]:(0.999999761581) A[3]:(4.17826692833e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 599000 finished after 10 . Running score: 0.11. Policy_loss: -92050.6111852, Value_loss: 0.993674150353. Times trained:               14923. Times reached goal: 100.               Steps done: 7260350.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9962,  0.0012,  0.0013,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0012,  0.0013,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1893e-08,  2.9398e-07,  2.4481e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.1125e-03,  1.0793e-08,  9.9889e-01,  1.1336e-26]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.2030e-05,  4.1300e-09,  9.9995e-01,  1.4900e-28]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.9998e-06,  2.5007e-09,  9.9999e-01,  1.5678e-29]])\n",
      "On state=13, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.8766e-06,  2.4898e-09,  9.9999e-01,  1.5409e-29]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.8763e-06,  2.4886e-09,  9.9999e-01,  1.5408e-29]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.8759e-06,  2.4877e-09,  9.9999e-01,  1.5407e-29]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.8757e-06,  2.4868e-09,  9.9999e-01,  1.5407e-29]])\n",
      "On state=14, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5870e-05,  2.9295e-09,  9.9998e-01,  3.2088e-29]])\n",
      "On state=10, selected action=2\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996198713779) A[1]:(0.0011853154283) A[2]:(0.00125942646991) A[3]:(0.00135652825702)\n",
      " state (1)  A[0]:(0.0147529337555) A[1]:(0.00177751237061) A[2]:(0.004658466205) A[3]:(0.978811085224)\n",
      " state (2)  A[0]:(0.999999701977) A[1]:(1.66046323358e-08) A[2]:(3.04166121623e-07) A[3]:(6.02654482307e-13)\n",
      " state (3)  A[0]:(0.999999761581) A[1]:(1.10083275828e-08) A[2]:(2.27453455182e-07) A[3]:(3.62481231012e-13)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(1.18886811507e-08) A[2]:(2.93985863209e-07) A[3]:(2.44817892087e-13)\n",
      " state (5)  A[0]:(0.999998807907) A[1]:(1.55594630513e-08) A[2]:(1.18719151487e-06) A[3]:(2.64386951866e-14)\n",
      " state (6)  A[0]:(0.999796867371) A[1]:(3.9344712377e-08) A[2]:(0.000203083560336) A[3]:(2.46974673843e-17)\n",
      " state (7)  A[0]:(0.252884596586) A[1]:(1.18648209479e-07) A[2]:(0.747115314007) A[3]:(2.12051364904e-22)\n",
      " state (8)  A[0]:(0.00111213838682) A[1]:(1.07756727985e-08) A[2]:(0.998887836933) A[3]:(1.13318220439e-26)\n",
      " state (9)  A[0]:(5.20151224919e-05) A[1]:(4.12237000091e-09) A[2]:(0.999947965145) A[3]:(1.48954455239e-28)\n",
      " state (10)  A[0]:(1.58695256687e-05) A[1]:(2.92996893414e-09) A[2]:(0.999984145164) A[3]:(3.20872748206e-29)\n",
      " state (11)  A[0]:(1.0676056263e-05) A[1]:(2.61983634786e-09) A[2]:(0.999989330769) A[3]:(1.94366414806e-29)\n",
      " state (12)  A[0]:(9.38390076044e-06) A[1]:(2.52651100041e-09) A[2]:(0.999990642071) A[3]:(1.65246564695e-29)\n",
      " state (13)  A[0]:(8.99818405742e-06) A[1]:(2.49686760156e-09) A[2]:(0.999990999699) A[3]:(1.56753439335e-29)\n",
      " state (14)  A[0]:(8.87544138095e-06) A[1]:(2.48723308616e-09) A[2]:(0.999991118908) A[3]:(1.54067940676e-29)\n",
      " state (15)  A[0]:(8.83515895111e-06) A[1]:(2.48403764225e-09) A[2]:(0.999991178513) A[3]:(1.5318771546e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 600000 finished after 11 . Running score: 0.09. Policy_loss: -92050.6063709, Value_loss: 0.98720573491. Times trained:               16652. Times reached goal: 75.               Steps done: 7277002.\n",
      " state (0)  A[0]:(0.99232262373) A[1]:(0.00129237072542) A[2]:(0.00169594038744) A[3]:(0.00468908762559)\n",
      " state (1)  A[0]:(0.0131601588801) A[1]:(0.00166433537379) A[2]:(0.00451224436983) A[3]:(0.980663239956)\n",
      " state (2)  A[0]:(0.999999642372) A[1]:(9.86715953388e-09) A[2]:(3.52509943014e-07) A[3]:(7.83099008816e-13)\n",
      " state (3)  A[0]:(0.999999761581) A[1]:(5.99506400079e-09) A[2]:(2.48605942943e-07) A[3]:(4.26438967688e-13)\n",
      " state (4)  A[0]:(0.999999642372) A[1]:(6.46360343026e-09) A[2]:(3.55098222826e-07) A[3]:(2.44583135212e-13)\n",
      " state (5)  A[0]:(0.999996423721) A[1]:(8.96390783822e-09) A[2]:(3.57167323273e-06) A[3]:(6.71243244568e-15)\n",
      " state (6)  A[0]:(0.992900073528) A[1]:(3.01046050311e-08) A[2]:(0.00709992367774) A[3]:(5.43520011186e-19)\n",
      " state (7)  A[0]:(0.0286760888994) A[1]:(1.00131991587e-08) A[2]:(0.971323907375) A[3]:(3.39834375436e-24)\n",
      " state (8)  A[0]:(0.000240997163928) A[1]:(1.25447052746e-09) A[2]:(0.999759018421) A[3]:(1.48912190706e-27)\n",
      " state (9)  A[0]:(2.27661676035e-05) A[1]:(5.36601207912e-10) A[2]:(0.999977231026) A[3]:(6.4061503572e-29)\n",
      " state (10)  A[0]:(1.00914603536e-05) A[1]:(4.04460076542e-10) A[2]:(0.999989926815) A[3]:(2.2996873446e-29)\n",
      " state (11)  A[0]:(7.82494316809e-06) A[1]:(3.70480229828e-10) A[2]:(0.999992191792) A[3]:(1.67639201632e-29)\n",
      " state (12)  A[0]:(7.23496077626e-06) A[1]:(3.60604268668e-10) A[2]:(0.999992787838) A[3]:(1.52118824341e-29)\n",
      " state (13)  A[0]:(7.06197488398e-06) A[1]:(3.57607748969e-10) A[2]:(0.999992966652) A[3]:(1.47624365889e-29)\n",
      " state (14)  A[0]:(7.00899636286e-06) A[1]:(3.56679963343e-10) A[2]:(0.999992966652) A[3]:(1.46251102607e-29)\n",
      " state (15)  A[0]:(6.9922257353e-06) A[1]:(3.56386864464e-10) A[2]:(0.999993026257) A[3]:(1.45818806566e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 601000 finished after 5 . Running score: 0.11. Policy_loss: -92050.6111844, Value_loss: 1.21091506798. Times trained:               15920. Times reached goal: 111.               Steps done: 7292922.\n",
      " state (0)  A[0]:(0.994652450085) A[1]:(0.00139147765003) A[2]:(0.00150352320634) A[3]:(0.00245253113098)\n",
      " state (1)  A[0]:(0.0127700427547) A[1]:(0.00178590719588) A[2]:(0.00432688370347) A[3]:(0.981117188931)\n",
      " state (2)  A[0]:(0.999999582767) A[1]:(1.18061773691e-08) A[2]:(3.82543703381e-07) A[3]:(1.09495648225e-12)\n",
      " state (3)  A[0]:(0.999999761581) A[1]:(5.79714276583e-09) A[2]:(2.26081496635e-07) A[3]:(4.46204081713e-13)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(5.90394266808e-09) A[2]:(2.66929987447e-07) A[3]:(3.28941219966e-13)\n",
      " state (5)  A[0]:(0.999998807907) A[1]:(7.09952319156e-09) A[2]:(1.19407229704e-06) A[3]:(2.77296140558e-14)\n",
      " state (6)  A[0]:(0.999862372875) A[1]:(1.31078659038e-08) A[2]:(0.0001376063301) A[3]:(4.06333053716e-17)\n",
      " state (7)  A[0]:(0.632097423077) A[1]:(4.12141467621e-08) A[2]:(0.367902576923) A[3]:(2.92784636976e-21)\n",
      " state (8)  A[0]:(0.00927028991282) A[1]:(5.25689936026e-09) A[2]:(0.990729689598) A[3]:(4.22946098005e-25)\n",
      " state (9)  A[0]:(0.000370449444745) A[1]:(1.47986556343e-09) A[2]:(0.999629557133) A[3]:(2.94902726427e-27)\n",
      " state (10)  A[0]:(8.25487804832e-05) A[1]:(8.63856708389e-10) A[2]:(0.999917447567) A[3]:(3.77617276789e-28)\n",
      " state (11)  A[0]:(4.78188048874e-05) A[1]:(7.13099912453e-10) A[2]:(0.999952197075) A[3]:(1.83671352676e-28)\n",
      " state (12)  A[0]:(3.97364638047e-05) A[1]:(6.68402388992e-10) A[2]:(0.999960243702) A[3]:(1.44193953639e-28)\n",
      " state (13)  A[0]:(3.73546099581e-05) A[1]:(6.54117038312e-10) A[2]:(0.999962627888) A[3]:(1.33028675713e-28)\n",
      " state (14)  A[0]:(3.65934465663e-05) A[1]:(6.49424680699e-10) A[2]:(0.999963402748) A[3]:(1.29505752622e-28)\n",
      " state (15)  A[0]:(3.63428407582e-05) A[1]:(6.47862263836e-10) A[2]:(0.999963641167) A[3]:(1.28350014099e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 602000 finished after 37 . Running score: 0.15. Policy_loss: -92050.6112503, Value_loss: 1.20437333687. Times trained:               15471. Times reached goal: 94.               Steps done: 7308393.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996505081654) A[1]:(0.00126463058405) A[2]:(0.00133969518356) A[3]:(0.000890584895387)\n",
      " state (1)  A[0]:(0.0137600153685) A[1]:(0.00185969285667) A[2]:(0.00485628843307) A[3]:(0.97952401638)\n",
      " state (2)  A[0]:(0.999999582767) A[1]:(9.95933469028e-09) A[2]:(4.00133473022e-07) A[3]:(8.91707443561e-13)\n",
      " state (3)  A[0]:(0.999999701977) A[1]:(6.51408704755e-09) A[2]:(3.05571887793e-07) A[3]:(4.90117224149e-13)\n",
      " state (4)  A[0]:(0.999998033047) A[1]:(9.17926179511e-09) A[2]:(1.97381950784e-06) A[3]:(3.18652303979e-14)\n",
      " state (5)  A[0]:(0.92274415493) A[1]:(5.90578927984e-08) A[2]:(0.0772557780147) A[3]:(8.89578658944e-20)\n",
      " state (6)  A[0]:(0.00153334182687) A[1]:(2.94980662119e-09) A[2]:(0.998466670513) A[3]:(5.27388151064e-26)\n",
      " state (7)  A[0]:(1.33110088427e-05) A[1]:(4.65446625686e-10) A[2]:(0.999986708164) A[3]:(5.69167548682e-29)\n",
      " state (8)  A[0]:(2.60793603957e-06) A[1]:(2.59481186538e-10) A[2]:(0.999997377396) A[3]:(7.47163167756e-30)\n",
      " state (9)  A[0]:(1.73286662175e-06) A[1]:(2.24428850504e-10) A[2]:(0.999998271465) A[3]:(4.54774464514e-30)\n",
      " state (10)  A[0]:(1.57037152348e-06) A[1]:(2.16726331326e-10) A[2]:(0.999998450279) A[3]:(4.03692520547e-30)\n",
      " state (11)  A[0]:(1.53331234287e-06) A[1]:(2.14899931184e-10) A[2]:(0.999998450279) A[3]:(3.92192762756e-30)\n",
      " state (12)  A[0]:(1.52400548359e-06) A[1]:(2.14439299651e-10) A[2]:(0.999998450279) A[3]:(3.89306952337e-30)\n",
      " state (13)  A[0]:(1.52110430918e-06) A[1]:(2.1430028585e-10) A[2]:(0.999998450279) A[3]:(3.88405075455e-30)\n",
      " state (14)  A[0]:(1.51955009642e-06) A[1]:(2.14228371154e-10) A[2]:(0.999998509884) A[3]:(3.87916483579e-30)\n",
      " state (15)  A[0]:(1.51807853399e-06) A[1]:(2.14162992895e-10) A[2]:(0.999998509884) A[3]:(3.87452116291e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 603000 finished after 8 . Running score: 0.13. Policy_loss: -92050.6111829, Value_loss: 1.62880190146. Times trained:               15314. Times reached goal: 114.               Steps done: 7323707.\n",
      " state (0)  A[0]:(0.996695756912) A[1]:(0.00127659435384) A[2]:(0.00128015188966) A[3]:(0.000747497193515)\n",
      " state (1)  A[0]:(0.0106808077544) A[1]:(0.00166951538995) A[2]:(0.00417672097683) A[3]:(0.983472943306)\n",
      " state (2)  A[0]:(0.999995708466) A[1]:(1.85930502994e-07) A[2]:(4.09357744502e-06) A[3]:(1.4531667536e-10)\n",
      " state (3)  A[0]:(0.999999523163) A[1]:(1.12363265359e-08) A[2]:(4.54448240816e-07) A[3]:(1.20453256744e-12)\n",
      " state (4)  A[0]:(0.999999344349) A[1]:(9.88258275214e-09) A[2]:(6.52792834899e-07) A[3]:(4.54834358111e-13)\n",
      " state (5)  A[0]:(0.0143880071118) A[1]:(1.42747156318e-08) A[2]:(0.985611975193) A[3]:(2.0687550651e-23)\n",
      " state (6)  A[0]:(4.13481166106e-06) A[1]:(4.84463358319e-10) A[2]:(0.99999588728) A[3]:(9.39696909756e-29)\n",
      " state (7)  A[0]:(5.16914610671e-07) A[1]:(2.3578000663e-10) A[2]:(0.999999463558) A[3]:(7.34605201738e-30)\n",
      " state (8)  A[0]:(3.456034392e-07) A[1]:(2.05371941675e-10) A[2]:(0.999999642372) A[3]:(4.54140600344e-30)\n",
      " state (9)  A[0]:(3.19250148095e-07) A[1]:(1.99845348603e-10) A[2]:(0.999999701977) A[3]:(4.12850392664e-30)\n",
      " state (10)  A[0]:(3.14047753136e-07) A[1]:(1.98713434596e-10) A[2]:(0.999999701977) A[3]:(4.04642808988e-30)\n",
      " state (11)  A[0]:(3.12852023399e-07) A[1]:(1.98451324818e-10) A[2]:(0.999999701977) A[3]:(4.02720979168e-30)\n",
      " state (12)  A[0]:(3.12459633278e-07) A[1]:(1.98369196069e-10) A[2]:(0.999999701977) A[3]:(4.02088544399e-30)\n",
      " state (13)  A[0]:(3.12204065267e-07) A[1]:(1.9831812581e-10) A[2]:(0.999999701977) A[3]:(4.01689929563e-30)\n",
      " state (14)  A[0]:(3.11933234798e-07) A[1]:(1.98266694729e-10) A[2]:(0.999999701977) A[3]:(4.01276418862e-30)\n",
      " state (15)  A[0]:(3.11596068059e-07) A[1]:(1.98202787516e-10) A[2]:(0.999999701977) A[3]:(4.0076548319e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 604000 finished after 15 . Running score: 0.06. Policy_loss: -92050.611181, Value_loss: 1.00176241623. Times trained:               15342. Times reached goal: 98.               Steps done: 7339049.\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0012,  0.0012,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0012,  0.0012,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0012,  0.0012,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0012,  0.0012,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0012,  0.0012,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.0696e-09,  2.7540e-07,  5.2416e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0012,  0.0012,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0012,  0.0012,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.0696e-09,  2.7539e-07,  5.2422e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0878e-08,  1.8328e-10,  1.0000e+00,  3.0351e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.3324e-08,  1.7118e-10,  1.0000e+00,  2.3926e-30]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.1208e-08,  1.6750e-10,  1.0000e+00,  2.2176e-30]])\n",
      "On state=13, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.3324e-08,  1.7118e-10,  1.0000e+00,  2.3927e-30]])\n",
      "On state=9, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.1772e-08,  1.6848e-10,  1.0000e+00,  2.2639e-30]])\n",
      "On state=10, selected action=2\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996973574162) A[1]:(0.00120311696082) A[2]:(0.00121250981465) A[3]:(0.000610821938608)\n",
      " state (1)  A[0]:(0.0113229854032) A[1]:(0.00173090642784) A[2]:(0.00433008978143) A[3]:(0.982616007328)\n",
      " state (2)  A[0]:(0.999997437) A[1]:(1.06339413719e-07) A[2]:(2.42990881816e-06) A[3]:(6.40951597353e-11)\n",
      " state (3)  A[0]:(0.999999642372) A[1]:(8.28540169806e-09) A[2]:(3.2623299262e-07) A[3]:(8.66647899278e-13)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(6.06937700098e-09) A[2]:(2.7536412972e-07) A[3]:(5.24272790979e-13)\n",
      " state (5)  A[0]:(0.99949491024) A[1]:(6.27758041105e-08) A[2]:(0.000505010364577) A[3]:(5.90640256346e-16)\n",
      " state (6)  A[0]:(8.0530771811e-06) A[1]:(1.16787557403e-09) A[2]:(0.999991953373) A[3]:(2.07858358142e-27)\n",
      " state (7)  A[0]:(1.03175928245e-07) A[1]:(2.50260978607e-10) A[2]:(0.999999880791) A[3]:(9.01996048685e-30)\n",
      " state (8)  A[0]:(4.0887254471e-08) A[1]:(1.83279683097e-10) A[2]:(0.999999940395) A[3]:(3.03558720761e-30)\n",
      " state (9)  A[0]:(3.33274421394e-08) A[1]:(1.71176947616e-10) A[2]:(0.999999940395) A[3]:(2.39275185302e-30)\n",
      " state (10)  A[0]:(3.17744692779e-08) A[1]:(1.68476815832e-10) A[2]:(0.999999940395) A[3]:(2.26390864437e-30)\n",
      " state (11)  A[0]:(3.13952988051e-08) A[1]:(1.67807823193e-10) A[2]:(0.999999940395) A[3]:(2.23263936617e-30)\n",
      " state (12)  A[0]:(3.12774943723e-08) A[1]:(1.67603111945e-10) A[2]:(0.999999940395) A[3]:(2.22295122385e-30)\n",
      " state (13)  A[0]:(3.12133110469e-08) A[1]:(1.67496364001e-10) A[2]:(0.999999940395) A[3]:(2.2176998674e-30)\n",
      " state (14)  A[0]:(3.11502681427e-08) A[1]:(1.67393848782e-10) A[2]:(0.999999940395) A[3]:(2.21257903785e-30)\n",
      " state (15)  A[0]:(3.10712877649e-08) A[1]:(1.6726714458e-10) A[2]:(0.999999940395) A[3]:(2.20619055519e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 605000 finished after 14 . Running score: 0.12. Policy_loss: -92050.6111894, Value_loss: 0.998429236726. Times trained:               15673. Times reached goal: 103.               Steps done: 7354722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997108817101) A[1]:(0.00114721478894) A[2]:(0.00116078206338) A[3]:(0.000583182147238)\n",
      " state (1)  A[0]:(0.0112736113369) A[1]:(0.0016904999502) A[2]:(0.00425424147397) A[3]:(0.982781648636)\n",
      " state (2)  A[0]:(0.999999284744) A[1]:(2.4274417143e-08) A[2]:(7.02420663856e-07) A[3]:(4.44575582945e-12)\n",
      " state (3)  A[0]:(0.999999761581) A[1]:(6.43955955226e-09) A[2]:(2.58932942643e-07) A[3]:(7.09351956514e-13)\n",
      " state (4)  A[0]:(0.999999761581) A[1]:(5.28937382782e-09) A[2]:(2.54668748312e-07) A[3]:(4.26338109781e-13)\n",
      " state (5)  A[0]:(0.283036053181) A[1]:(2.1365532632e-07) A[2]:(0.716963708401) A[3]:(5.77501379156e-20)\n",
      " state (6)  A[0]:(7.0407787689e-07) A[1]:(5.2612458834e-10) A[2]:(0.999999284744) A[3]:(1.3473219686e-28)\n",
      " state (7)  A[0]:(4.56795454795e-08) A[1]:(2.10915007681e-10) A[2]:(0.999999940395) A[3]:(5.42963323289e-30)\n",
      " state (8)  A[0]:(2.64255994864e-08) A[1]:(1.76359593596e-10) A[2]:(1.0) A[3]:(2.91033386467e-30)\n",
      " state (9)  A[0]:(2.34863843929e-08) A[1]:(1.69709177267e-10) A[2]:(1.0) A[3]:(2.54578899236e-30)\n",
      " state (10)  A[0]:(2.28564989158e-08) A[1]:(1.68212888063e-10) A[2]:(1.0) A[3]:(2.46819752548e-30)\n",
      " state (11)  A[0]:(2.26958505323e-08) A[1]:(1.67828972941e-10) A[2]:(1.0) A[3]:(2.44833599677e-30)\n",
      " state (12)  A[0]:(2.2637442143e-08) A[1]:(1.67692970621e-10) A[2]:(1.0) A[3]:(2.44115513688e-30)\n",
      " state (13)  A[0]:(2.25972360823e-08) A[1]:(1.6760408339e-10) A[2]:(1.0) A[3]:(2.43626169496e-30)\n",
      " state (14)  A[0]:(2.25525411679e-08) A[1]:(1.6750596743e-10) A[2]:(1.0) A[3]:(2.4308773666e-30)\n",
      " state (15)  A[0]:(2.24947598326e-08) A[1]:(1.67380456717e-10) A[2]:(1.0) A[3]:(2.42398802931e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 606000 finished after 27 . Running score: 0.16. Policy_loss: -92050.611189, Value_loss: 1.82931778844. Times trained:               15342. Times reached goal: 111.               Steps done: 7370064.\n",
      " state (0)  A[0]:(0.996050775051) A[1]:(0.00138651486486) A[2]:(0.00141209491994) A[3]:(0.00115059525706)\n",
      " state (1)  A[0]:(0.00972275901586) A[1]:(0.00154047878459) A[2]:(0.00390678877011) A[3]:(0.984829962254)\n",
      " state (2)  A[0]:(0.999955952168) A[1]:(3.33079947268e-06) A[2]:(4.05959835916e-05) A[3]:(9.96992923774e-08)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(5.39585887083e-09) A[2]:(1.99099957854e-07) A[3]:(7.7097887536e-13)\n",
      " state (4)  A[0]:(0.999999821186) A[1]:(3.46970341347e-09) A[2]:(1.59150445711e-07) A[3]:(3.73344801255e-13)\n",
      " state (5)  A[0]:(0.289048135281) A[1]:(3.05178019744e-07) A[2]:(0.710951566696) A[3]:(1.94065205403e-19)\n",
      " state (6)  A[0]:(2.42635650238e-07) A[1]:(6.8551853083e-10) A[2]:(0.999999761581) A[3]:(3.29815743718e-28)\n",
      " state (7)  A[0]:(1.17194467464e-08) A[1]:(2.70612143805e-10) A[2]:(1.0) A[3]:(1.2102730345e-29)\n",
      " state (8)  A[0]:(6.25218365968e-09) A[1]:(2.24102861268e-10) A[2]:(1.0) A[3]:(6.20850488116e-30)\n",
      " state (9)  A[0]:(5.42276756832e-09) A[1]:(2.14731857295e-10) A[2]:(1.0) A[3]:(5.3326239159e-30)\n",
      " state (10)  A[0]:(5.23621457305e-09) A[1]:(2.12478035166e-10) A[2]:(1.0) A[3]:(5.13365804936e-30)\n",
      " state (11)  A[0]:(5.18715870257e-09) A[1]:(2.11873435463e-10) A[2]:(1.0) A[3]:(5.08031806538e-30)\n",
      " state (12)  A[0]:(5.17198417427e-09) A[1]:(2.11686806972e-10) A[2]:(1.0) A[3]:(5.06352410673e-30)\n",
      " state (13)  A[0]:(5.16548714913e-09) A[1]:(2.1160809216e-10) A[2]:(1.0) A[3]:(5.05642073543e-30)\n",
      " state (14)  A[0]:(5.16101605896e-09) A[1]:(2.1155684149e-10) A[2]:(1.0) A[3]:(5.05171650108e-30)\n",
      " state (15)  A[0]:(5.1564996717e-09) A[1]:(2.11506007153e-10) A[2]:(1.0) A[3]:(5.04709351689e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 607000 finished after 14 . Running score: 0.1. Policy_loss: -92050.6111917, Value_loss: 1.2094519122. Times trained:               15798. Times reached goal: 112.               Steps done: 7385862.\n",
      " state (0)  A[0]:(0.993620753288) A[1]:(0.00157884333748) A[2]:(0.00189282232895) A[3]:(0.00290755368769)\n",
      " state (1)  A[0]:(0.00896102469414) A[1]:(0.001499353908) A[2]:(0.00407071923837) A[3]:(0.985468924046)\n",
      " state (2)  A[0]:(0.999994277954) A[1]:(2.93949511843e-07) A[2]:(5.44842487216e-06) A[3]:(1.18505993907e-09)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(4.03963884565e-09) A[2]:(1.63402148701e-07) A[3]:(6.14168221488e-13)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(2.63053445693e-09) A[2]:(1.25162841869e-07) A[3]:(3.35779852274e-13)\n",
      " state (5)  A[0]:(0.999979794025) A[1]:(2.15981366125e-08) A[2]:(2.01642578759e-05) A[3]:(1.11103143287e-14)\n",
      " state (6)  A[0]:(3.21847164741e-06) A[1]:(1.73503755807e-09) A[2]:(0.999996781349) A[3]:(1.0668916957e-26)\n",
      " state (7)  A[0]:(6.47357456529e-09) A[1]:(2.44292669427e-10) A[2]:(1.0) A[3]:(1.17536608088e-29)\n",
      " state (8)  A[0]:(1.59578295023e-09) A[1]:(1.61910443386e-10) A[2]:(1.0) A[3]:(2.85537151041e-30)\n",
      " state (9)  A[0]:(1.15209242146e-09) A[1]:(1.47232587233e-10) A[2]:(1.0) A[3]:(2.06098220823e-30)\n",
      " state (10)  A[0]:(1.06308961634e-09) A[1]:(1.43824965826e-10) A[2]:(1.0) A[3]:(1.90191337229e-30)\n",
      " state (11)  A[0]:(1.04130137846e-09) A[1]:(1.42959893923e-10) A[2]:(1.0) A[3]:(1.86290840086e-30)\n",
      " state (12)  A[0]:(1.03534225637e-09) A[1]:(1.42722320073e-10) A[2]:(1.0) A[3]:(1.85220858108e-30)\n",
      " state (13)  A[0]:(1.03324326872e-09) A[1]:(1.42640677048e-10) A[2]:(1.0) A[3]:(1.84842537006e-30)\n",
      " state (14)  A[0]:(1.03196307055e-09) A[1]:(1.42591702335e-10) A[2]:(1.0) A[3]:(1.84611406605e-30)\n",
      " state (15)  A[0]:(1.03066088997e-09) A[1]:(1.42542755377e-10) A[2]:(1.0) A[3]:(1.84376326542e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 608000 finished after 25 . Running score: 0.15. Policy_loss: -92050.6114256, Value_loss: 0.978753212408. Times trained:               15855. Times reached goal: 116.               Steps done: 7401717.\n",
      " state (0)  A[0]:(0.992794632912) A[1]:(0.001583155361) A[2]:(0.00198266468942) A[3]:(0.00363957160152)\n",
      " state (1)  A[0]:(0.00866853166372) A[1]:(0.00148415705189) A[2]:(0.00406506052241) A[3]:(0.985782265663)\n",
      " state (2)  A[0]:(0.999993264675) A[1]:(2.98007563515e-07) A[2]:(6.40935877527e-06) A[3]:(1.79102244147e-09)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(3.48994677601e-09) A[2]:(1.73453670982e-07) A[3]:(7.36602727598e-13)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(2.10041939397e-09) A[2]:(1.2478062672e-07) A[3]:(3.72699484122e-13)\n",
      " state (5)  A[0]:(0.999997496605) A[1]:(8.02002997347e-09) A[2]:(2.47791263064e-06) A[3]:(6.09721759301e-14)\n",
      " state (6)  A[0]:(1.23800746223e-05) A[1]:(1.91775217928e-09) A[2]:(0.999987602234) A[3]:(8.83643652319e-26)\n",
      " state (7)  A[0]:(5.63319613178e-09) A[1]:(1.37386713117e-10) A[2]:(1.0) A[3]:(1.49250512801e-29)\n",
      " state (8)  A[0]:(9.81548287093e-10) A[1]:(8.00628660591e-11) A[2]:(1.0) A[3]:(2.49327505177e-30)\n",
      " state (9)  A[0]:(6.52195519812e-10) A[1]:(7.06402714101e-11) A[2]:(1.0) A[3]:(1.64691084369e-30)\n",
      " state (10)  A[0]:(5.89310711341e-10) A[1]:(6.84850509636e-11) A[2]:(1.0) A[3]:(1.48665729752e-30)\n",
      " state (11)  A[0]:(5.74444047885e-10) A[1]:(6.79540451687e-11) A[2]:(1.0) A[3]:(1.44906461202e-30)\n",
      " state (12)  A[0]:(5.70669900224e-10) A[1]:(6.78189934766e-11) A[2]:(1.0) A[3]:(1.43968689425e-30)\n",
      " state (13)  A[0]:(5.69367386571e-10) A[1]:(6.77742514887e-11) A[2]:(1.0) A[3]:(1.43653797998e-30)\n",
      " state (14)  A[0]:(5.68275926316e-10) A[1]:(6.77383218961e-11) A[2]:(1.0) A[3]:(1.43386622238e-30)\n",
      " state (15)  A[0]:(5.66707514249e-10) A[1]:(6.7686661831e-11) A[2]:(1.0) A[3]:(1.42990066868e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 609000 finished after 10 . Running score: 0.09. Policy_loss: -92050.6112014, Value_loss: 0.995631682557. Times trained:               15452. Times reached goal: 119.               Steps done: 7417169.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9958,  0.0013,  0.0014,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9958,  0.0013,  0.0014,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9958,  0.0013,  0.0014,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.4567e-09,  1.1284e-07,  3.0916e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.3185e-10,  3.5347e-11,  1.0000e+00,  1.2166e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.9893e-10,  3.3244e-11,  1.0000e+00,  1.0002e-30]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.6580e-10,  3.2677e-11,  1.0000e+00,  9.4721e-31]])\n",
      "On state=13, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.6408e-10,  3.2651e-11,  1.0000e+00,  9.4455e-31]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.6406e-10,  3.2651e-11,  1.0000e+00,  9.4456e-31]])\n",
      "On state=14, selected action=2\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.995766699314) A[1]:(0.00125326530542) A[2]:(0.00141348456964) A[3]:(0.00156655407045)\n",
      " state (1)  A[0]:(0.00939645897597) A[1]:(0.00141177664045) A[2]:(0.00387256848626) A[3]:(0.985319197178)\n",
      " state (2)  A[0]:(0.999999463558) A[1]:(1.14559979281e-08) A[2]:(4.96673578709e-07) A[3]:(7.91713829912e-12)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.6642073275e-09) A[2]:(1.1235071895e-07) A[3]:(4.43874917291e-13)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(1.45667833351e-09) A[2]:(1.12843920874e-07) A[3]:(3.09154394793e-13)\n",
      " state (5)  A[0]:(0.994292855263) A[1]:(5.92449183046e-08) A[2]:(0.00570711353794) A[3]:(7.53077935162e-17)\n",
      " state (6)  A[0]:(2.00712435117e-07) A[1]:(2.08360356746e-10) A[2]:(0.999999821186) A[3]:(3.74075395325e-28)\n",
      " state (7)  A[0]:(1.86745352515e-09) A[1]:(4.71532396462e-11) A[2]:(1.0) A[3]:(3.06434826299e-30)\n",
      " state (8)  A[0]:(7.31679272725e-10) A[1]:(3.53465381964e-11) A[2]:(1.0) A[3]:(1.21660007172e-30)\n",
      " state (9)  A[0]:(5.98807947672e-10) A[1]:(3.3243591907e-11) A[2]:(1.0) A[3]:(1.00016024656e-30)\n",
      " state (10)  A[0]:(5.7317228741e-10) A[1]:(3.28026911189e-11) A[2]:(1.0) A[3]:(9.58716359452e-31)\n",
      " state (11)  A[0]:(5.67918378991e-10) A[1]:(3.27118471199e-11) A[2]:(1.0) A[3]:(9.50450283177e-31)\n",
      " state (12)  A[0]:(5.66669711155e-10) A[1]:(3.26914502413e-11) A[2]:(1.0) A[3]:(9.4863917552e-31)\n",
      " state (13)  A[0]:(5.65702096278e-10) A[1]:(3.2677048567e-11) A[2]:(1.0) A[3]:(9.47221717412e-31)\n",
      " state (14)  A[0]:(5.64010615989e-10) A[1]:(3.26508819981e-11) A[2]:(1.0) A[3]:(9.44565946535e-31)\n",
      " state (15)  A[0]:(5.61148849609e-10) A[1]:(3.26056993905e-11) A[2]:(1.0) A[3]:(9.39950767635e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 610000 finished after 9 . Running score: 0.13. Policy_loss: -92050.6111924, Value_loss: 1.85587825939. Times trained:               16264. Times reached goal: 109.               Steps done: 7433433.\n",
      " state (0)  A[0]:(0.996919155121) A[1]:(0.00138691603206) A[2]:(0.00106609647628) A[3]:(0.000627844478004)\n",
      " state (1)  A[0]:(0.00959557108581) A[1]:(0.0015893895179) A[2]:(0.00363832921721) A[3]:(0.985176682472)\n",
      " state (2)  A[0]:(0.999998867512) A[1]:(4.17245082929e-08) A[2]:(1.08884341898e-06) A[3]:(8.79725181591e-11)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(1.7326388102e-09) A[2]:(8.2704872284e-08) A[3]:(4.44456103865e-13)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(1.15254727984e-09) A[2]:(6.40707256139e-08) A[3]:(2.38634605782e-13)\n",
      " state (5)  A[0]:(0.999720573425) A[1]:(2.65736659344e-08) A[2]:(0.000279385887552) A[3]:(6.72800758163e-16)\n",
      " state (6)  A[0]:(1.0729125961e-06) A[1]:(5.82828896256e-10) A[2]:(0.999998927116) A[3]:(3.53181077655e-27)\n",
      " state (7)  A[0]:(5.64071145348e-09) A[1]:(1.0662528499e-10) A[2]:(1.0) A[3]:(1.35608278318e-29)\n",
      " state (8)  A[0]:(1.81066794891e-09) A[1]:(7.49341699802e-11) A[2]:(1.0) A[3]:(4.31450550108e-30)\n",
      " state (9)  A[0]:(1.39863676196e-09) A[1]:(6.9183908602e-11) A[2]:(1.0) A[3]:(3.33137243125e-30)\n",
      " state (10)  A[0]:(1.31664468217e-09) A[1]:(6.79031275652e-11) A[2]:(1.0) A[3]:(3.13649070229e-30)\n",
      " state (11)  A[0]:(1.29905608492e-09) A[1]:(6.76207215222e-11) A[2]:(1.0) A[3]:(3.09515204543e-30)\n",
      " state (12)  A[0]:(1.29605148835e-09) A[1]:(6.75723713095e-11) A[2]:(1.0) A[3]:(3.08852357385e-30)\n",
      " state (13)  A[0]:(1.29607125032e-09) A[1]:(6.75735370437e-11) A[2]:(1.0) A[3]:(3.08899490006e-30)\n",
      " state (14)  A[0]:(1.29622956813e-09) A[1]:(6.75771452685e-11) A[2]:(1.0) A[3]:(3.08972558735e-30)\n",
      " state (15)  A[0]:(1.29582411468e-09) A[1]:(6.75723713095e-11) A[2]:(1.0) A[3]:(3.08899490006e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 611000 finished after 54 . Running score: 0.11. Policy_loss: -92050.6111752, Value_loss: 1.02441528047. Times trained:               16326. Times reached goal: 112.               Steps done: 7449759.\n",
      " state (0)  A[0]:(0.997288525105) A[1]:(0.00127190898638) A[2]:(0.000938321929425) A[3]:(0.000501271337271)\n",
      " state (1)  A[0]:(0.00931443460286) A[1]:(0.00150441657752) A[2]:(0.00338967167772) A[3]:(0.985791504383)\n",
      " state (2)  A[0]:(0.999999582767) A[1]:(1.40537830262e-08) A[2]:(4.19163569632e-07) A[3]:(1.33303229566e-11)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(1.65030555888e-09) A[2]:(7.49947588474e-08) A[3]:(4.6624726649e-13)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(1.30781696583e-09) A[2]:(7.23083886101e-08) A[3]:(2.62022852417e-13)\n",
      " state (5)  A[0]:(0.10038369894) A[1]:(1.46299399262e-07) A[2]:(0.89961618185) A[3]:(6.14544788808e-20)\n",
      " state (6)  A[0]:(1.13863208639e-07) A[1]:(4.01187194576e-10) A[2]:(0.999999880791) A[3]:(6.37742860283e-28)\n",
      " state (7)  A[0]:(5.38754019175e-09) A[1]:(1.52786117091e-10) A[2]:(1.0) A[3]:(2.60436314235e-29)\n",
      " state (8)  A[0]:(2.82442425004e-09) A[1]:(1.24970880866e-10) A[2]:(1.0) A[3]:(1.34224873811e-29)\n",
      " state (9)  A[0]:(2.44366415991e-09) A[1]:(1.19465687343e-10) A[2]:(1.0) A[3]:(1.15696713045e-29)\n",
      " state (10)  A[0]:(2.36425146127e-09) A[1]:(1.18239737446e-10) A[2]:(1.0) A[3]:(1.11832943988e-29)\n",
      " state (11)  A[0]:(2.34765207274e-09) A[1]:(1.1797886279e-10) A[2]:(1.0) A[3]:(1.11027860132e-29)\n",
      " state (12)  A[0]:(2.34522623543e-09) A[1]:(1.17939269462e-10) A[2]:(1.0) A[3]:(1.10915253416e-29)\n",
      " state (13)  A[0]:(2.34571384539e-09) A[1]:(1.17946694078e-10) A[2]:(1.0) A[3]:(1.10944872112e-29)\n",
      " state (14)  A[0]:(2.34646124753e-09) A[1]:(1.17958615098e-10) A[2]:(1.0) A[3]:(1.10986354837e-29)\n",
      " state (15)  A[0]:(2.34687735912e-09) A[1]:(1.17965595625e-10) A[2]:(1.0) A[3]:(1.11011760562e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 612000 finished after 4 . Running score: 0.12. Policy_loss: -92050.6111769, Value_loss: 1.83548705558. Times trained:               15530. Times reached goal: 129.               Steps done: 7465289.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.985889315605) A[1]:(0.00132556934841) A[2]:(0.00156070536468) A[3]:(0.0112244188786)\n",
      " state (1)  A[0]:(0.0092060258612) A[1]:(0.00139071943704) A[2]:(0.00332657271065) A[3]:(0.986076653004)\n",
      " state (2)  A[0]:(0.99999922514) A[1]:(2.59679975301e-08) A[2]:(7.42553140753e-07) A[3]:(5.04489609809e-11)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(1.67487834712e-09) A[2]:(8.16036234141e-08) A[3]:(5.98961201817e-13)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(1.37872413486e-09) A[2]:(8.6006679112e-08) A[3]:(3.26111994397e-13)\n",
      " state (5)  A[0]:(0.0087119108066) A[1]:(4.83844750931e-08) A[2]:(0.99128806591) A[3]:(3.14892755408e-21)\n",
      " state (6)  A[0]:(1.24596857276e-08) A[1]:(1.98596777912e-10) A[2]:(1.0) A[3]:(1.08073931978e-28)\n",
      " state (7)  A[0]:(8.03056177112e-10) A[1]:(8.31228905152e-11) A[2]:(1.0) A[3]:(6.17317497526e-30)\n",
      " state (8)  A[0]:(4.70501304584e-10) A[1]:(7.02428462618e-11) A[2]:(1.0) A[3]:(3.54833859575e-30)\n",
      " state (9)  A[0]:(4.2122885735e-10) A[1]:(6.78349043604e-11) A[2]:(1.0) A[3]:(3.16422729083e-30)\n",
      " state (10)  A[0]:(4.12924777704e-10) A[1]:(6.74090158692e-11) A[2]:(1.0) A[3]:(3.1007063973e-30)\n",
      " state (11)  A[0]:(4.13130391008e-10) A[1]:(6.7419819727e-11) A[2]:(1.0) A[3]:(3.10411476668e-30)\n",
      " state (12)  A[0]:(4.15158907252e-10) A[1]:(6.75250896864e-11) A[2]:(1.0) A[3]:(3.12197513381e-30)\n",
      " state (13)  A[0]:(4.17407497455e-10) A[1]:(6.76416214707e-11) A[2]:(1.0) A[3]:(3.14154382336e-30)\n",
      " state (14)  A[0]:(4.19461798629e-10) A[1]:(6.7748529009e-11) A[2]:(1.0) A[3]:(3.15949898235e-30)\n",
      " state (15)  A[0]:(4.21198326217e-10) A[1]:(6.7838915041e-11) A[2]:(1.0) A[3]:(3.17481877705e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 613000 finished after 24 . Running score: 0.1. Policy_loss: -92050.6113707, Value_loss: 0.990888362015. Times trained:               15819. Times reached goal: 99.               Steps done: 7481108.\n",
      " state (0)  A[0]:(0.99381262064) A[1]:(0.00115523312707) A[2]:(0.00122147856746) A[3]:(0.00381069211289)\n",
      " state (1)  A[0]:(0.00977047998458) A[1]:(0.00137032277416) A[2]:(0.0033419849351) A[3]:(0.985517203808)\n",
      " state (2)  A[0]:(0.999969005585) A[1]:(1.97576991923e-06) A[2]:(2.88439605356e-05) A[3]:(1.5456751612e-07)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.79699854996e-09) A[2]:(9.16565383591e-08) A[3]:(7.36734837633e-13)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(1.15549236845e-09) A[2]:(7.75456925339e-08) A[3]:(3.24655422988e-13)\n",
      " state (5)  A[0]:(0.0158748328686) A[1]:(4.79061199599e-08) A[2]:(0.984125137329) A[3]:(6.56723366399e-21)\n",
      " state (6)  A[0]:(7.4069141931e-09) A[1]:(1.1192385907e-10) A[2]:(1.0) A[3]:(5.04368943572e-29)\n",
      " state (7)  A[0]:(4.0360026432e-10) A[1]:(4.38914182777e-11) A[2]:(1.0) A[3]:(2.46125872339e-30)\n",
      " state (8)  A[0]:(2.2592042126e-10) A[1]:(3.6471409226e-11) A[2]:(1.0) A[3]:(1.35159421912e-30)\n",
      " state (9)  A[0]:(1.97565366844e-10) A[1]:(3.49400022803e-11) A[2]:(1.0) A[3]:(1.17517264153e-30)\n",
      " state (10)  A[0]:(1.90672366784e-10) A[1]:(3.45442043026e-11) A[2]:(1.0) A[3]:(1.13177470665e-30)\n",
      " state (11)  A[0]:(1.88401003132e-10) A[1]:(3.44124173601e-11) A[2]:(1.0) A[3]:(1.11729271029e-30)\n",
      " state (12)  A[0]:(1.87165685728e-10) A[1]:(3.43421922844e-11) A[2]:(1.0) A[3]:(1.10943551797e-30)\n",
      " state (13)  A[0]:(1.86018991877e-10) A[1]:(3.427766751e-11) A[2]:(1.0) A[3]:(1.10222202635e-30)\n",
      " state (14)  A[0]:(1.84644965984e-10) A[1]:(3.42004757847e-11) A[2]:(1.0) A[3]:(1.09361947658e-30)\n",
      " state (15)  A[0]:(1.82876380705e-10) A[1]:(3.41007500015e-11) A[2]:(1.0) A[3]:(1.08257848132e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 614000 finished after 25 . Running score: 0.1. Policy_loss: -92050.6122401, Value_loss: 0.980836444036. Times trained:               15814. Times reached goal: 108.               Steps done: 7496922.\n",
      "action_dist \n",
      "tensor([[ 0.9939,  0.0014,  0.0012,  0.0035]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9939,  0.0014,  0.0012,  0.0035]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9939,  0.0014,  0.0012,  0.0035]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.9090e-10,  3.1181e-08,  1.5444e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.5601e-10,  4.2046e-11,  1.0000e+00,  8.6518e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993895292282) A[1]:(0.00141587178223) A[2]:(0.00117378216237) A[3]:(0.00351503537968)\n",
      " state (1)  A[0]:(0.01035760995) A[1]:(0.00160130229779) A[2]:(0.00338712451048) A[3]:(0.984653949738)\n",
      " state (2)  A[0]:(0.999999701977) A[1]:(1.33539348468e-08) A[2]:(3.08064272758e-07) A[3]:(2.15254931935e-11)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(9.08493835716e-10) A[2]:(3.36439747173e-08) A[3]:(2.89439885904e-13)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(6.90442092388e-10) A[2]:(3.11459302793e-08) A[3]:(1.54364111012e-13)\n",
      " state (5)  A[0]:(0.522917747498) A[1]:(2.50109309263e-07) A[2]:(0.477081984282) A[3]:(6.40774324141e-19)\n",
      " state (6)  A[0]:(1.5245442242e-08) A[1]:(1.52229936989e-10) A[2]:(1.0) A[3]:(5.31282941858e-29)\n",
      " state (7)  A[0]:(4.94065843792e-10) A[1]:(5.15954431957e-11) A[2]:(1.0) A[3]:(1.67172317845e-30)\n",
      " state (8)  A[0]:(2.56154736311e-10) A[1]:(4.20551093949e-11) A[2]:(1.0) A[3]:(8.65447653561e-31)\n",
      " state (9)  A[0]:(2.22545995521e-10) A[1]:(4.02523778198e-11) A[2]:(1.0) A[3]:(7.5146664317e-31)\n",
      " state (10)  A[0]:(2.16116138874e-10) A[1]:(3.98851576766e-11) A[2]:(1.0) A[3]:(7.29798238584e-31)\n",
      " state (11)  A[0]:(2.15295350992e-10) A[1]:(3.98374874755e-11) A[2]:(1.0) A[3]:(7.27280329684e-31)\n",
      " state (12)  A[0]:(2.15696516204e-10) A[1]:(3.98609721619e-11) A[2]:(1.0) A[3]:(7.28957948202e-31)\n",
      " state (13)  A[0]:(2.1626747615e-10) A[1]:(3.98946674307e-11) A[2]:(1.0) A[3]:(7.31213862921e-31)\n",
      " state (14)  A[0]:(2.16725831725e-10) A[1]:(3.99226797454e-11) A[2]:(1.0) A[3]:(7.33068322808e-31)\n",
      " state (15)  A[0]:(2.16969026079e-10) A[1]:(3.99389792072e-11) A[2]:(1.0) A[3]:(7.34159745803e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 615000 finished after 5 . Running score: 0.08. Policy_loss: -92050.6111818, Value_loss: 1.21965126467. Times trained:               15485. Times reached goal: 111.               Steps done: 7512407.\n",
      " state (0)  A[0]:(0.996285736561) A[1]:(0.00133363204077) A[2]:(0.00087316299323) A[3]:(0.00150745140854)\n",
      " state (1)  A[0]:(0.010777072981) A[1]:(0.00162848550826) A[2]:(0.0031583996024) A[3]:(0.984436035156)\n",
      " state (2)  A[0]:(0.999999761581) A[1]:(1.03042525623e-08) A[2]:(2.11506062442e-07) A[3]:(2.06253538554e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.44291017707e-10) A[2]:(2.36718999957e-08) A[3]:(3.10986615149e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.19214449213e-10) A[2]:(1.89964328712e-08) A[3]:(1.69412986244e-13)\n",
      " state (5)  A[0]:(0.999976456165) A[1]:(9.76066782954e-09) A[2]:(2.35292027355e-05) A[3]:(2.20917881699e-15)\n",
      " state (6)  A[0]:(4.06811210496e-07) A[1]:(4.06053968227e-10) A[2]:(0.999999582767) A[3]:(2.44686356465e-27)\n",
      " state (7)  A[0]:(1.06350739326e-09) A[1]:(5.4763228674e-11) A[2]:(1.0) A[3]:(4.45662784623e-30)\n",
      " state (8)  A[0]:(3.28107790937e-10) A[1]:(3.74512747203e-11) A[2]:(1.0) A[3]:(1.32635231171e-30)\n",
      " state (9)  A[0]:(2.52648513221e-10) A[1]:(3.44323770884e-11) A[2]:(1.0) A[3]:(1.01445435191e-30)\n",
      " state (10)  A[0]:(2.37954600468e-10) A[1]:(3.37762491587e-11) A[2]:(1.0) A[3]:(9.54541567755e-31)\n",
      " state (11)  A[0]:(2.35113706282e-10) A[1]:(3.36468769513e-11) A[2]:(1.0) A[3]:(9.4335603371e-31)\n",
      " state (12)  A[0]:(2.34979202762e-10) A[1]:(3.36412946111e-11) A[2]:(1.0) A[3]:(9.43226541252e-31)\n",
      " state (13)  A[0]:(2.35400893223e-10) A[1]:(3.3661382709e-11) A[2]:(1.0) A[3]:(9.45330205943e-31)\n",
      " state (14)  A[0]:(2.35829605844e-10) A[1]:(3.36820640823e-11) A[2]:(1.0) A[3]:(9.47416849475e-31)\n",
      " state (15)  A[0]:(2.36108660401e-10) A[1]:(3.36962020786e-11) A[2]:(1.0) A[3]:(9.48841830746e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 616000 finished after 26 . Running score: 0.09. Policy_loss: -92050.6111791, Value_loss: 0.98266912629. Times trained:               15712. Times reached goal: 117.               Steps done: 7528119.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.992549240589) A[1]:(0.00129234918859) A[2]:(0.00103965145536) A[3]:(0.00511878402904)\n",
      " state (1)  A[0]:(0.00979576259851) A[1]:(0.00139772577677) A[2]:(0.00284379185177) A[3]:(0.985962748528)\n",
      " state (2)  A[0]:(0.999994695187) A[1]:(4.26151473221e-07) A[2]:(4.88968498757e-06) A[3]:(1.81522370468e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.80994388552e-10) A[2]:(2.60973145316e-08) A[3]:(4.86832254197e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.00582028554e-10) A[2]:(2.43746960393e-08) A[3]:(1.87087229931e-13)\n",
      " state (5)  A[0]:(0.00203255238011) A[1]:(1.62054973885e-08) A[2]:(0.997967422009) A[3]:(1.50347990184e-22)\n",
      " state (6)  A[0]:(8.67552585326e-10) A[1]:(4.55130239052e-11) A[2]:(1.0) A[3]:(3.80498230254e-30)\n",
      " state (7)  A[0]:(8.06192682057e-11) A[1]:(2.09763092313e-11) A[2]:(1.0) A[3]:(3.29475785109e-31)\n",
      " state (8)  A[0]:(4.35094182905e-11) A[1]:(1.72476009169e-11) A[2]:(1.0) A[3]:(1.7383626585e-31)\n",
      " state (9)  A[0]:(3.15187355382e-11) A[1]:(1.56103463489e-11) A[2]:(1.0) A[3]:(1.23915218945e-31)\n",
      " state (10)  A[0]:(2.43040015196e-11) A[1]:(1.44233939028e-11) A[2]:(1.0) A[3]:(9.4197117206e-32)\n",
      " state (11)  A[0]:(1.92065495452e-11) A[1]:(1.34371593713e-11) A[2]:(1.0) A[3]:(7.3473375145e-32)\n",
      " state (12)  A[0]:(1.55185257006e-11) A[1]:(1.26107700671e-11) A[2]:(1.0) A[3]:(5.8666636713e-32)\n",
      " state (13)  A[0]:(1.28517222905e-11) A[1]:(1.19304861129e-11) A[2]:(1.0) A[3]:(4.80781538815e-32)\n",
      " state (14)  A[0]:(1.09196462636e-11) A[1]:(1.13798510545e-11) A[2]:(1.0) A[3]:(4.04799964354e-32)\n",
      " state (15)  A[0]:(9.50357934709e-12) A[1]:(1.09373014118e-11) A[2]:(1.0) A[3]:(3.49561604018e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 617000 finished after 11 . Running score: 0.12. Policy_loss: -92050.6111834, Value_loss: 1.20394005423. Times trained:               16519. Times reached goal: 117.               Steps done: 7544638.\n",
      " state (0)  A[0]:(0.995743572712) A[1]:(0.00121505884454) A[2]:(0.000873395940289) A[3]:(0.00216799229383)\n",
      " state (1)  A[0]:(0.00989819690585) A[1]:(0.00144113064744) A[2]:(0.0028027871158) A[3]:(0.985857903957)\n",
      " state (2)  A[0]:(0.999999403954) A[1]:(3.52801592385e-08) A[2]:(5.34681930731e-07) A[3]:(2.16297049715e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.00119390556e-09) A[2]:(2.60955221876e-08) A[3]:(5.31733348758e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.53632584221e-10) A[2]:(1.74476753045e-08) A[3]:(2.13732067526e-13)\n",
      " state (5)  A[0]:(0.997527420521) A[1]:(7.45156825133e-08) A[2]:(0.00247250078246) A[3]:(1.60698693706e-16)\n",
      " state (6)  A[0]:(2.53000784767e-08) A[1]:(2.21583265625e-10) A[2]:(1.0) A[3]:(2.29009946349e-28)\n",
      " state (7)  A[0]:(2.80791639939e-10) A[1]:(4.90766143002e-11) A[2]:(1.0) A[3]:(1.92800633761e-30)\n",
      " state (8)  A[0]:(1.05688020835e-10) A[1]:(3.57308037324e-11) A[2]:(1.0) A[3]:(6.89684540882e-31)\n",
      " state (9)  A[0]:(7.40651151521e-11) A[1]:(3.18958644219e-11) A[2]:(1.0) A[3]:(4.71919886573e-31)\n",
      " state (10)  A[0]:(5.87227905191e-11) A[1]:(2.96533075872e-11) A[2]:(1.0) A[3]:(3.67242866948e-31)\n",
      " state (11)  A[0]:(4.76572253882e-11) A[1]:(2.77891494538e-11) A[2]:(1.0) A[3]:(2.92876439358e-31)\n",
      " state (12)  A[0]:(3.88516857575e-11) A[1]:(2.60885150288e-11) A[2]:(1.0) A[3]:(2.34742859945e-31)\n",
      " state (13)  A[0]:(3.17997642107e-11) A[1]:(2.45301071122e-11) A[2]:(1.0) A[3]:(1.89020243929e-31)\n",
      " state (14)  A[0]:(2.62217643432e-11) A[1]:(2.31252707955e-11) A[2]:(1.0) A[3]:(1.53470837977e-31)\n",
      " state (15)  A[0]:(2.1863226507e-11) A[1]:(2.18831688881e-11) A[2]:(1.0) A[3]:(1.2613961866e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 618000 finished after 19 . Running score: 0.09. Policy_loss: -92050.6111795, Value_loss: 1.22575379961. Times trained:               15951. Times reached goal: 111.               Steps done: 7560589.\n",
      " state (0)  A[0]:(0.994625508785) A[1]:(0.00149849033915) A[2]:(0.00095154234441) A[3]:(0.00292448350228)\n",
      " state (1)  A[0]:(0.00916360598058) A[1]:(0.00152607145719) A[2]:(0.00274537852965) A[3]:(0.986564934254)\n",
      " state (2)  A[0]:(0.999999821186) A[1]:(8.01506327974e-09) A[2]:(1.4695544337e-07) A[3]:(1.57146622104e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.76749950418e-10) A[2]:(2.28407213143e-08) A[3]:(4.51281373381e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.76762423879e-10) A[2]:(2.37475585863e-08) A[3]:(1.85669906641e-13)\n",
      " state (5)  A[0]:(0.00270719849505) A[1]:(2.46687310579e-08) A[2]:(0.997292757034) A[3]:(3.17257187373e-22)\n",
      " state (6)  A[0]:(6.31749874636e-10) A[1]:(5.33063142238e-11) A[2]:(1.0) A[3]:(3.95386985278e-30)\n",
      " state (7)  A[0]:(5.3065787875e-11) A[1]:(2.39535041108e-11) A[2]:(1.0) A[3]:(3.13930201458e-31)\n",
      " state (8)  A[0]:(2.82939800061e-11) A[1]:(1.96582698037e-11) A[2]:(1.0) A[3]:(1.64216114122e-31)\n",
      " state (9)  A[0]:(2.04664115516e-11) A[1]:(1.78017513924e-11) A[2]:(1.0) A[3]:(1.17064861046e-31)\n",
      " state (10)  A[0]:(1.57732837208e-11) A[1]:(1.64579634643e-11) A[2]:(1.0) A[3]:(8.90215625162e-32)\n",
      " state (11)  A[0]:(1.24606176072e-11) A[1]:(1.53412994114e-11) A[2]:(1.0) A[3]:(6.94571282487e-32)\n",
      " state (12)  A[0]:(1.00669628883e-11) A[1]:(1.44060726889e-11) A[2]:(1.0) A[3]:(5.5485578553e-32)\n",
      " state (13)  A[0]:(8.33867697114e-12) A[1]:(1.36370801804e-11) A[2]:(1.0) A[3]:(4.55031629764e-32)\n",
      " state (14)  A[0]:(7.08780386813e-12) A[1]:(1.30152989081e-11) A[2]:(1.0) A[3]:(3.8344702131e-32)\n",
      " state (15)  A[0]:(6.171183356e-12) A[1]:(1.25156404338e-11) A[2]:(1.0) A[3]:(3.31400451397e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 619000 finished after 6 . Running score: 0.12. Policy_loss: -92050.611181, Value_loss: 1.20599356625. Times trained:               15743. Times reached goal: 114.               Steps done: 7576332.\n",
      "action_dist \n",
      "tensor([[ 0.9958,  0.0017,  0.0008,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3727e-10,  2.0648e-08,  1.5112e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3464e-11,  1.3114e-11,  1.0000e+00,  7.5517e-32]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995781958103) A[1]:(0.0016699108528) A[2]:(0.000834472244605) A[3]:(0.00171368627343)\n",
      " state (1)  A[0]:(0.00942245870829) A[1]:(0.00166936812457) A[2]:(0.00269086356275) A[3]:(0.986217319965)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(6.87761092522e-09) A[2]:(1.17196741201e-07) A[3]:(1.18173370395e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.97189814161e-10) A[2]:(1.88703292991e-08) A[3]:(3.77505156041e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.37260022529e-10) A[2]:(2.06469596975e-08) A[3]:(1.51114784206e-13)\n",
      " state (5)  A[0]:(0.0111007876694) A[1]:(4.05776177104e-08) A[2]:(0.988899171352) A[3]:(1.29676424064e-21)\n",
      " state (6)  A[0]:(4.95078977814e-10) A[1]:(4.02385590126e-11) A[2]:(1.0) A[3]:(2.57646619757e-30)\n",
      " state (7)  A[0]:(2.76616507477e-11) A[1]:(1.6303961653e-11) A[2]:(1.0) A[3]:(1.54300901557e-31)\n",
      " state (8)  A[0]:(1.34643797847e-11) A[1]:(1.31139266113e-11) A[2]:(1.0) A[3]:(7.55147386346e-32)\n",
      " state (9)  A[0]:(9.41912433466e-12) A[1]:(1.18049936146e-11) A[2]:(1.0) A[3]:(5.2541112141e-32)\n",
      " state (10)  A[0]:(7.25101662741e-12) A[1]:(1.09473610732e-11) A[2]:(1.0) A[3]:(4.0189502394e-32)\n",
      " state (11)  A[0]:(5.87582760225e-12) A[1]:(1.0314346599e-11) A[2]:(1.0) A[3]:(3.23842140271e-32)\n",
      " state (12)  A[0]:(4.95951760562e-12) A[1]:(9.83937150506e-12) A[2]:(1.0) A[3]:(2.72068578921e-32)\n",
      " state (13)  A[0]:(4.32586692162e-12) A[1]:(9.47948403801e-12) A[2]:(1.0) A[3]:(2.3641695198e-32)\n",
      " state (14)  A[0]:(3.86406748612e-12) A[1]:(9.19821076945e-12) A[2]:(1.0) A[3]:(2.10544894372e-32)\n",
      " state (15)  A[0]:(3.50241211901e-12) A[1]:(8.96543082829e-12) A[2]:(1.0) A[3]:(1.90382771186e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 620000 finished after 3 . Running score: 0.08. Policy_loss: -92050.6114242, Value_loss: 1.20275787614. Times trained:               15052. Times reached goal: 110.               Steps done: 7591384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.99730014801) A[1]:(0.00132576748729) A[2]:(0.000658734526951) A[3]:(0.000715370522812)\n",
      " state (1)  A[0]:(0.0102228270844) A[1]:(0.00158692430705) A[2]:(0.00267488998361) A[3]:(0.985515356064)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(1.285879625e-09) A[2]:(2.97815638817e-08) A[3]:(9.21040695968e-13)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.49761977661e-10) A[2]:(1.25038486232e-08) A[3]:(2.09382343278e-13)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(8.21382240002e-10) A[2]:(6.70544153536e-08) A[3]:(4.41554277408e-14)\n",
      " state (5)  A[0]:(8.24786411613e-07) A[1]:(2.8224547699e-10) A[2]:(0.999999165535) A[3]:(1.46625302971e-27)\n",
      " state (6)  A[0]:(5.39766599472e-11) A[1]:(1.17811281564e-11) A[2]:(1.0) A[3]:(9.6902200071e-32)\n",
      " state (7)  A[0]:(1.26643782267e-11) A[1]:(7.73777049379e-12) A[2]:(1.0) A[3]:(2.54914883733e-32)\n",
      " state (8)  A[0]:(8.13074954586e-12) A[1]:(6.85405551759e-12) A[2]:(1.0) A[3]:(1.70034185208e-32)\n",
      " state (9)  A[0]:(6.32531743996e-12) A[1]:(6.41855752576e-12) A[2]:(1.0) A[3]:(1.35260373367e-32)\n",
      " state (10)  A[0]:(5.29207146854e-12) A[1]:(6.1350828931e-12) A[2]:(1.0) A[3]:(1.1496402342e-32)\n",
      " state (11)  A[0]:(4.6275600539e-12) A[1]:(5.93569724622e-12) A[2]:(1.0) A[3]:(1.01723563619e-32)\n",
      " state (12)  A[0]:(4.15722664626e-12) A[1]:(5.78544850915e-12) A[2]:(1.0) A[3]:(9.22648968975e-33)\n",
      " state (13)  A[0]:(3.77752386663e-12) A[1]:(5.65808381051e-12) A[2]:(1.0) A[3]:(8.45856788303e-33)\n",
      " state (14)  A[0]:(3.42290410425e-12) A[1]:(5.5330371361e-12) A[2]:(1.0) A[3]:(7.73683198404e-33)\n",
      " state (15)  A[0]:(3.05407739029e-12) A[1]:(5.39465781074e-12) A[2]:(1.0) A[3]:(6.97749716356e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 621000 finished after 27 . Running score: 0.09. Policy_loss: -92050.6111781, Value_loss: 1.21111211449. Times trained:               16415. Times reached goal: 99.               Steps done: 7607799.\n",
      " state (0)  A[0]:(0.99673640728) A[1]:(0.00151909794658) A[2]:(0.000788662640844) A[3]:(0.000955830269959)\n",
      " state (1)  A[0]:(0.00983697362244) A[1]:(0.00163733004592) A[2]:(0.00276778242551) A[3]:(0.985757887363)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(4.01350419565e-09) A[2]:(7.78349473762e-08) A[3]:(5.89784601365e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.15569087423e-10) A[2]:(1.3995021142e-08) A[3]:(2.35296374398e-13)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(7.14627634846e-10) A[2]:(4.18229504362e-08) A[3]:(6.57128838106e-14)\n",
      " state (5)  A[0]:(7.3891901593e-06) A[1]:(6.84941214857e-10) A[2]:(0.999992609024) A[3]:(1.80905789016e-26)\n",
      " state (6)  A[0]:(8.69958133309e-11) A[1]:(1.39236972646e-11) A[2]:(1.0) A[3]:(1.51063836969e-31)\n",
      " state (7)  A[0]:(1.54630076105e-11) A[1]:(8.40622623594e-12) A[2]:(1.0) A[3]:(3.09132591712e-32)\n",
      " state (8)  A[0]:(9.36772187599e-12) A[1]:(7.31844619628e-12) A[2]:(1.0) A[3]:(1.95773867458e-32)\n",
      " state (9)  A[0]:(7.0821282866e-12) A[1]:(6.79597090408e-12) A[2]:(1.0) A[3]:(1.5177596614e-32)\n",
      " state (10)  A[0]:(5.78739096577e-12) A[1]:(6.45209103159e-12) A[2]:(1.0) A[3]:(1.26276760647e-32)\n",
      " state (11)  A[0]:(4.96362456345e-12) A[1]:(6.20839577664e-12) A[2]:(1.0) A[3]:(1.09774788914e-32)\n",
      " state (12)  A[0]:(4.4000697183e-12) A[1]:(6.02827380028e-12) A[2]:(1.0) A[3]:(9.83504458454e-33)\n",
      " state (13)  A[0]:(3.97391147766e-12) A[1]:(5.88410440164e-12) A[2]:(1.0) A[3]:(8.96461673169e-33)\n",
      " state (14)  A[0]:(3.60678739479e-12) A[1]:(5.75357990418e-12) A[2]:(1.0) A[3]:(8.21046878003e-33)\n",
      " state (15)  A[0]:(3.24914552728e-12) A[1]:(5.61915141153e-12) A[2]:(1.0) A[3]:(7.46967370951e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 622000 finished after 7 . Running score: 0.07. Policy_loss: -92050.6111902, Value_loss: 1.19770557105. Times trained:               15223. Times reached goal: 110.               Steps done: 7623022.\n",
      " state (0)  A[0]:(0.985262930393) A[1]:(0.00171690457501) A[2]:(0.00189645157661) A[3]:(0.0111237326637)\n",
      " state (1)  A[0]:(0.00782838929445) A[1]:(0.00131290452555) A[2]:(0.00326203694567) A[3]:(0.987596690655)\n",
      " state (2)  A[0]:(0.999999523163) A[1]:(3.37598571321e-08) A[2]:(4.61188506051e-07) A[3]:(2.99487074029e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.54307680154e-10) A[2]:(1.92988967029e-08) A[3]:(6.3512942735e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.1344056784e-10) A[2]:(1.41949811905e-08) A[3]:(2.75105079776e-13)\n",
      " state (5)  A[0]:(0.999996960163) A[1]:(4.32810498552e-09) A[2]:(3.04974651044e-06) A[3]:(6.50683425597e-15)\n",
      " state (6)  A[0]:(2.88247914426e-08) A[1]:(1.02243290223e-10) A[2]:(1.0) A[3]:(7.84909799755e-29)\n",
      " state (7)  A[0]:(2.65721316478e-11) A[1]:(9.78868115037e-12) A[2]:(1.0) A[3]:(7.58779957764e-32)\n",
      " state (8)  A[0]:(8.18489807181e-12) A[1]:(6.81662538915e-12) A[2]:(1.0) A[3]:(2.46043016444e-32)\n",
      " state (9)  A[0]:(6.04158043038e-12) A[1]:(6.23648832229e-12) A[2]:(1.0) A[3]:(1.83756847485e-32)\n",
      " state (10)  A[0]:(5.40259677273e-12) A[1]:(6.04568391877e-12) A[2]:(1.0) A[3]:(1.64990241804e-32)\n",
      " state (11)  A[0]:(5.08394108062e-12) A[1]:(5.95063755215e-12) A[2]:(1.0) A[3]:(1.55635040626e-32)\n",
      " state (12)  A[0]:(4.83945826121e-12) A[1]:(5.87891063955e-12) A[2]:(1.0) A[3]:(1.48495543443e-32)\n",
      " state (13)  A[0]:(4.60364081925e-12) A[1]:(5.80968780028e-12) A[2]:(1.0) A[3]:(1.41632742937e-32)\n",
      " state (14)  A[0]:(4.36938549578e-12) A[1]:(5.73994411029e-12) A[2]:(1.0) A[3]:(1.34817358934e-32)\n",
      " state (15)  A[0]:(4.15079299057e-12) A[1]:(5.67343958272e-12) A[2]:(1.0) A[3]:(1.2844748729e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 623000 finished after 10 . Running score: 0.1. Policy_loss: -92050.611206, Value_loss: 0.991801454249. Times trained:               15838. Times reached goal: 100.               Steps done: 7638860.\n",
      " state (0)  A[0]:(0.993159353733) A[1]:(0.0016880115727) A[2]:(0.0016557360068) A[3]:(0.0034968808759)\n",
      " state (1)  A[0]:(0.00833987630904) A[1]:(0.00141717039514) A[2]:(0.00354570616037) A[3]:(0.986697256565)\n",
      " state (2)  A[0]:(0.999998867512) A[1]:(1.01992661428e-07) A[2]:(1.02498779597e-06) A[3]:(1.91425453266e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.25375989777e-10) A[2]:(1.4818357208e-08) A[3]:(5.5651777674e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.2168392656e-10) A[2]:(1.59925743759e-08) A[3]:(1.97056997456e-13)\n",
      " state (5)  A[0]:(0.00416242796928) A[1]:(2.00196641487e-08) A[2]:(0.995837569237) A[3]:(8.63797326963e-23)\n",
      " state (6)  A[0]:(2.21013859991e-10) A[1]:(2.14414978827e-11) A[2]:(1.0) A[3]:(2.19538475284e-31)\n",
      " state (7)  A[0]:(2.19858766609e-11) A[1]:(1.0380699772e-11) A[2]:(1.0) A[3]:(2.53899756199e-32)\n",
      " state (8)  A[0]:(1.40653261607e-11) A[1]:(9.09009499617e-12) A[2]:(1.0) A[3]:(1.6888869535e-32)\n",
      " state (9)  A[0]:(1.21461321903e-11) A[1]:(8.73139407997e-12) A[2]:(1.0) A[3]:(1.48277048431e-32)\n",
      " state (10)  A[0]:(1.10912251605e-11) A[1]:(8.5387998755e-12) A[2]:(1.0) A[3]:(1.37193488521e-32)\n",
      " state (11)  A[0]:(1.02595458171e-11) A[1]:(8.38809577353e-12) A[2]:(1.0) A[3]:(1.28505306918e-32)\n",
      " state (12)  A[0]:(9.622870209e-12) A[1]:(8.27075560816e-12) A[2]:(1.0) A[3]:(1.21811278013e-32)\n",
      " state (13)  A[0]:(9.19180963982e-12) A[1]:(8.18972320515e-12) A[2]:(1.0) A[3]:(1.17240427005e-32)\n",
      " state (14)  A[0]:(8.92332648744e-12) A[1]:(8.1381819686e-12) A[2]:(1.0) A[3]:(1.1437000939e-32)\n",
      " state (15)  A[0]:(8.75554229812e-12) A[1]:(8.10512421068e-12) A[2]:(1.0) A[3]:(1.1255907218e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 624000 finished after 4 . Running score: 0.15. Policy_loss: -92050.6111989, Value_loss: 1.20454501248. Times trained:               16144. Times reached goal: 119.               Steps done: 7655004.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9965,  0.0014,  0.0011,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0014,  0.0011,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0014,  0.0011,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0014,  0.0011,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0014,  0.0011,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.7697e-10,  6.2968e-09,  2.1844e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0014,  0.0011,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0014,  0.0011,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.7699e-10,  6.2970e-09,  2.1844e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0014,  0.0011,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.7700e-10,  6.2972e-09,  2.1845e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.6481e-11,  1.4975e-11,  1.0000e+00,  3.5649e-32]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.99649053812) A[1]:(0.00135877868161) A[2]:(0.00112956087105) A[3]:(0.00102113233879)\n",
      " state (1)  A[0]:(0.00926279462874) A[1]:(0.00145335390698) A[2]:(0.00348640466109) A[3]:(0.985797464848)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.85349358084e-09) A[2]:(1.6970254535e-08) A[3]:(2.42727192512e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.34603647409e-10) A[2]:(6.81876866082e-09) A[3]:(4.53135792777e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.77009931554e-10) A[2]:(6.2974350179e-09) A[3]:(2.18454472719e-13)\n",
      " state (5)  A[0]:(0.999996781349) A[1]:(5.85285464538e-09) A[2]:(3.22278378917e-06) A[3]:(2.07694977414e-15)\n",
      " state (6)  A[0]:(1.00849703699e-07) A[1]:(2.15896106548e-10) A[2]:(0.999999880791) A[3]:(6.98864546147e-29)\n",
      " state (7)  A[0]:(1.15048005844e-10) A[1]:(2.14303765705e-11) A[2]:(1.0) A[3]:(9.9926353245e-32)\n",
      " state (8)  A[0]:(3.6480662241e-11) A[1]:(1.49749466299e-11) A[2]:(1.0) A[3]:(3.56494140727e-32)\n",
      " state (9)  A[0]:(2.73448069743e-11) A[1]:(1.37483635587e-11) A[2]:(1.0) A[3]:(2.7640365515e-32)\n",
      " state (10)  A[0]:(2.42831501435e-11) A[1]:(1.33147512113e-11) A[2]:(1.0) A[3]:(2.4961463848e-32)\n",
      " state (11)  A[0]:(2.23618137896e-11) A[1]:(1.30516179461e-11) A[2]:(1.0) A[3]:(2.3297550125e-32)\n",
      " state (12)  A[0]:(2.0882618551e-11) A[1]:(1.28503258381e-11) A[2]:(1.0) A[3]:(2.20143026152e-32)\n",
      " state (13)  A[0]:(1.98083320579e-11) A[1]:(1.2702455407e-11) A[2]:(1.0) A[3]:(2.1075542541e-32)\n",
      " state (14)  A[0]:(1.91013697914e-11) A[1]:(1.26036047918e-11) A[2]:(1.0) A[3]:(2.04530036716e-32)\n",
      " state (15)  A[0]:(1.86616989223e-11) A[1]:(1.25410177035e-11) A[2]:(1.0) A[3]:(2.00624574284e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 625000 finished after 12 . Running score: 0.1. Policy_loss: -92050.6111921, Value_loss: 0.992451705253. Times trained:               15787. Times reached goal: 119.               Steps done: 7670791.\n",
      " state (0)  A[0]:(0.996181190014) A[1]:(0.00178507855162) A[2]:(0.00160254864022) A[3]:(0.000431208231021)\n",
      " state (1)  A[0]:(0.00958544947207) A[1]:(0.00178890966345) A[2]:(0.00481335772201) A[3]:(0.983812272549)\n",
      " state (2)  A[0]:(0.999999761581) A[1]:(2.32628831753e-08) A[2]:(2.23147935685e-07) A[3]:(1.58297971953e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.77253872375e-10) A[2]:(1.07627204926e-08) A[3]:(5.26184402039e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.17515760748e-10) A[2]:(1.22414611781e-08) A[3]:(1.8167909397e-13)\n",
      " state (5)  A[0]:(0.696532189846) A[1]:(2.10813922763e-07) A[2]:(0.303467601538) A[3]:(2.11087445036e-19)\n",
      " state (6)  A[0]:(3.93139520938e-10) A[1]:(1.63964588434e-11) A[2]:(1.0) A[3]:(1.70086826784e-31)\n",
      " state (7)  A[0]:(1.81106674796e-11) A[1]:(6.33248791945e-12) A[2]:(1.0) A[3]:(1.20191145577e-32)\n",
      " state (8)  A[0]:(1.05162736158e-11) A[1]:(5.45133990032e-12) A[2]:(1.0) A[3]:(7.69210075058e-33)\n",
      " state (9)  A[0]:(8.83738394963e-12) A[1]:(5.22328924904e-12) A[2]:(1.0) A[3]:(6.69500603459e-33)\n",
      " state (10)  A[0]:(8.11502427756e-12) A[1]:(5.1271781959e-12) A[2]:(1.0) A[3]:(6.26533419669e-33)\n",
      " state (11)  A[0]:(7.7374755908e-12) A[1]:(5.07806860797e-12) A[2]:(1.0) A[3]:(6.04004078612e-33)\n",
      " state (12)  A[0]:(7.51975738622e-12) A[1]:(5.04963388812e-12) A[2]:(1.0) A[3]:(5.90881778034e-33)\n",
      " state (13)  A[0]:(7.35731267576e-12) A[1]:(5.02776986711e-12) A[2]:(1.0) A[3]:(5.80913806345e-33)\n",
      " state (14)  A[0]:(7.18167365854e-12) A[1]:(5.00317495766e-12) A[2]:(1.0) A[3]:(5.69925725964e-33)\n",
      " state (15)  A[0]:(6.94069801632e-12) A[1]:(4.96832262831e-12) A[2]:(1.0) A[3]:(5.54603708114e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 626000 finished after 21 . Running score: 0.12. Policy_loss: -92050.6111902, Value_loss: 1.24135117923. Times trained:               15374. Times reached goal: 107.               Steps done: 7686165.\n",
      " state (0)  A[0]:(0.993494212627) A[1]:(0.00185091223102) A[2]:(0.00252403481863) A[3]:(0.00213084812276)\n",
      " state (1)  A[0]:(0.00943638104945) A[1]:(0.00171583378688) A[2]:(0.00530955614522) A[3]:(0.983538210392)\n",
      " state (2)  A[0]:(0.999999761581) A[1]:(1.84740596154e-08) A[2]:(2.12085353724e-07) A[3]:(1.1230339575e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.64872784501e-10) A[2]:(1.41770151174e-08) A[3]:(6.12133390851e-13)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(1.29199373422e-09) A[2]:(7.42211980764e-08) A[3]:(6.25408741722e-14)\n",
      " state (5)  A[0]:(4.93060952067e-07) A[1]:(1.4970291673e-10) A[2]:(0.999999523163) A[3]:(1.54233381068e-28)\n",
      " state (6)  A[0]:(3.15265522022e-11) A[1]:(4.87465276477e-12) A[2]:(1.0) A[3]:(1.20901144165e-32)\n",
      " state (7)  A[0]:(1.04484962349e-11) A[1]:(3.57037403115e-12) A[2]:(1.0) A[3]:(4.80178275115e-33)\n",
      " state (8)  A[0]:(8.23598134136e-12) A[1]:(3.36894357897e-12) A[2]:(1.0) A[3]:(3.95997708372e-33)\n",
      " state (9)  A[0]:(7.59300174819e-12) A[1]:(3.31103699146e-12) A[2]:(1.0) A[3]:(3.71483581838e-33)\n",
      " state (10)  A[0]:(7.33201260122e-12) A[1]:(3.28877485141e-12) A[2]:(1.0) A[3]:(3.61664787728e-33)\n",
      " state (11)  A[0]:(7.17775708661e-12) A[1]:(3.27611570684e-12) A[2]:(1.0) A[3]:(3.55957211642e-33)\n",
      " state (12)  A[0]:(7.0173311606e-12) A[1]:(3.2630695021e-12) A[2]:(1.0) A[3]:(3.50069821654e-33)\n",
      " state (13)  A[0]:(6.78064115273e-12) A[1]:(3.2435356484e-12) A[2]:(1.0) A[3]:(3.41329690548e-33)\n",
      " state (14)  A[0]:(6.40561822335e-12) A[1]:(3.21168374014e-12) A[2]:(1.0) A[3]:(3.27188420039e-33)\n",
      " state (15)  A[0]:(5.84183396102e-12) A[1]:(3.16162590899e-12) A[2]:(1.0) A[3]:(3.05145384669e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 627000 finished after 5 . Running score: 0.14. Policy_loss: -92050.6114315, Value_loss: 0.989008500438. Times trained:               15756. Times reached goal: 112.               Steps done: 7701921.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994509518147) A[1]:(0.00170209968928) A[2]:(0.00210402044468) A[3]:(0.00168438872788)\n",
      " state (1)  A[0]:(0.00794714689255) A[1]:(0.00135610543657) A[2]:(0.00446594133973) A[3]:(0.986230790615)\n",
      " state (2)  A[0]:(0.999965369701) A[1]:(3.86962074117e-06) A[2]:(2.92732511298e-05) A[3]:(1.49666993821e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.00335229014e-09) A[2]:(1.48907188802e-08) A[3]:(1.18790274926e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.0204053158e-09) A[2]:(1.53250905299e-08) A[3]:(9.22800898195e-13)\n",
      " state (5)  A[0]:(0.999995470047) A[1]:(1.0519311644e-08) A[2]:(4.5126976147e-06) A[3]:(1.71150376179e-14)\n",
      " state (6)  A[0]:(1.44212704001e-08) A[1]:(6.67058214221e-11) A[2]:(1.0) A[3]:(2.83755443909e-29)\n",
      " state (7)  A[0]:(3.10925868385e-11) A[1]:(6.03872334082e-12) A[2]:(1.0) A[3]:(2.56984782348e-32)\n",
      " state (8)  A[0]:(1.17440874733e-11) A[1]:(4.27292576322e-12) A[2]:(1.0) A[3]:(8.57239026634e-33)\n",
      " state (9)  A[0]:(9.22210224852e-12) A[1]:(3.93758983752e-12) A[2]:(1.0) A[3]:(6.51188532057e-33)\n",
      " state (10)  A[0]:(8.49098742706e-12) A[1]:(3.83273968119e-12) A[2]:(1.0) A[3]:(5.92868657361e-33)\n",
      " state (11)  A[0]:(8.13644290831e-12) A[1]:(3.78424505273e-12) A[2]:(1.0) A[3]:(5.66172592907e-33)\n",
      " state (12)  A[0]:(7.82829356893e-12) A[1]:(3.74787700874e-12) A[2]:(1.0) A[3]:(5.45401681196e-33)\n",
      " state (13)  A[0]:(7.42724631797e-12) A[1]:(3.70571065153e-12) A[2]:(1.0) A[3]:(5.20473560245e-33)\n",
      " state (14)  A[0]:(6.83983382085e-12) A[1]:(3.64585228327e-12) A[2]:(1.0) A[3]:(4.84749919582e-33)\n",
      " state (15)  A[0]:(6.02255051385e-12) A[1]:(3.56083240151e-12) A[2]:(1.0) A[3]:(4.34016795629e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 628000 finished after 19 . Running score: 0.13. Policy_loss: -92050.6111963, Value_loss: 1.00105598943. Times trained:               15557. Times reached goal: 94.               Steps done: 7717478.\n",
      " state (0)  A[0]:(0.994826972485) A[1]:(0.00176581961568) A[2]:(0.00206533051096) A[3]:(0.00134189031087)\n",
      " state (1)  A[0]:(0.00760642625391) A[1]:(0.001323447912) A[2]:(0.00440908316523) A[3]:(0.986661016941)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(4.86544493583e-09) A[2]:(6.45107007813e-08) A[3]:(2.01959716312e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.98329299837e-10) A[2]:(1.35580764393e-08) A[3]:(1.09077959339e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.64976987206e-10) A[2]:(2.0304693038e-08) A[3]:(4.5252343539e-13)\n",
      " state (5)  A[0]:(0.0094752702862) A[1]:(3.27207914097e-08) A[2]:(0.990524709225) A[3]:(7.59218920282e-22)\n",
      " state (6)  A[0]:(1.0548441981e-10) A[1]:(6.78121014203e-12) A[2]:(1.0) A[3]:(6.75789997821e-32)\n",
      " state (7)  A[0]:(1.2044785308e-11) A[1]:(3.21469305169e-12) A[2]:(1.0) A[3]:(6.99828945457e-33)\n",
      " state (8)  A[0]:(8.25066664295e-12) A[1]:(2.86013985933e-12) A[2]:(1.0) A[3]:(4.73033253108e-33)\n",
      " state (9)  A[0]:(7.51557063111e-12) A[1]:(2.78193093561e-12) A[2]:(1.0) A[3]:(4.29130559486e-33)\n",
      " state (10)  A[0]:(7.28189557264e-12) A[1]:(2.75748239346e-12) A[2]:(1.0) A[3]:(4.15527959263e-33)\n",
      " state (11)  A[0]:(7.13609510025e-12) A[1]:(2.74463199564e-12) A[2]:(1.0) A[3]:(4.07929527046e-33)\n",
      " state (12)  A[0]:(6.95625718486e-12) A[1]:(2.73130996986e-12) A[2]:(1.0) A[3]:(3.99487420492e-33)\n",
      " state (13)  A[0]:(6.67085831019e-12) A[1]:(2.71126979375e-12) A[2]:(1.0) A[3]:(3.86488142745e-33)\n",
      " state (14)  A[0]:(6.21574189688e-12) A[1]:(2.67899764872e-12) A[2]:(1.0) A[3]:(3.65556886258e-33)\n",
      " state (15)  A[0]:(5.55284941292e-12) A[1]:(2.63028487879e-12) A[2]:(1.0) A[3]:(3.33985458983e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 629000 finished after 17 . Running score: 0.07. Policy_loss: -92050.611194, Value_loss: 1.00316487245. Times trained:               15890. Times reached goal: 95.               Steps done: 7733368.\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0018,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.6368e-10,  1.3640e-08,  7.7326e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0018,  0.0020,  0.0014]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.6377e-10,  1.3641e-08,  7.7334e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3986e-11,  4.7981e-12,  1.0000e+00,  8.4781e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.9948310256) A[1]:(0.00182804127689) A[2]:(0.00195093266666) A[3]:(0.00139002432115)\n",
      " state (1)  A[0]:(0.0077524962835) A[1]:(0.00138514721766) A[2]:(0.00424558762461) A[3]:(0.986616790295)\n",
      " state (2)  A[0]:(0.999999523163) A[1]:(4.45801049409e-08) A[2]:(4.34929347648e-07) A[3]:(7.6219824896e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(9.79136105528e-10) A[2]:(1.28445618586e-08) A[3]:(1.15033850501e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.63853108438e-10) A[2]:(1.3641810348e-08) A[3]:(7.73418113408e-13)\n",
      " state (5)  A[0]:(0.999940931797) A[1]:(2.4448777225e-08) A[2]:(5.90489144088e-05) A[3]:(1.82043135178e-15)\n",
      " state (6)  A[0]:(5.77269299029e-09) A[1]:(4.66574313907e-11) A[2]:(1.0) A[3]:(7.51034286944e-30)\n",
      " state (7)  A[0]:(3.24793282547e-11) A[1]:(6.42875119458e-12) A[2]:(1.0) A[3]:(2.1705435597e-32)\n",
      " state (8)  A[0]:(1.39839259963e-11) A[1]:(4.79807122852e-12) A[2]:(1.0) A[3]:(8.47807961989e-33)\n",
      " state (9)  A[0]:(1.13940116023e-11) A[1]:(4.48226004923e-12) A[2]:(1.0) A[3]:(6.72870011079e-33)\n",
      " state (10)  A[0]:(1.06854065526e-11) A[1]:(4.3888803182e-12) A[2]:(1.0) A[3]:(6.25287028315e-33)\n",
      " state (11)  A[0]:(1.03997444337e-11) A[1]:(4.35127151324e-12) A[2]:(1.0) A[3]:(6.06418984819e-33)\n",
      " state (12)  A[0]:(1.02115971928e-11) A[1]:(4.32877214976e-12) A[2]:(1.0) A[3]:(5.94880369006e-33)\n",
      " state (13)  A[0]:(1.00091037888e-11) A[1]:(4.30764625359e-12) A[2]:(1.0) A[3]:(5.83623651431e-33)\n",
      " state (14)  A[0]:(9.72527700732e-12) A[1]:(4.28032392516e-12) A[2]:(1.0) A[3]:(5.68713864756e-33)\n",
      " state (15)  A[0]:(9.29705618047e-12) A[1]:(4.23996471613e-12) A[2]:(1.0) A[3]:(5.46601383384e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 630000 finished after 5 . Running score: 0.09. Policy_loss: -92050.6111807, Value_loss: 1.21564710966. Times trained:               15717. Times reached goal: 115.               Steps done: 7749085.\n",
      " state (0)  A[0]:(0.996345341206) A[1]:(0.00149536400568) A[2]:(0.00154391978867) A[3]:(0.000615350611042)\n",
      " state (1)  A[0]:(0.00795818865299) A[1]:(0.00129426817875) A[2]:(0.00419239094481) A[3]:(0.986555159092)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(3.91322485527e-09) A[2]:(4.92675447106e-08) A[3]:(2.49926954132e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.82705311885e-10) A[2]:(5.85312998069e-09) A[3]:(4.67015423729e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.92026982993e-10) A[2]:(2.89710193613e-08) A[3]:(5.81371702082e-14)\n",
      " state (5)  A[0]:(1.24029668314e-06) A[1]:(1.19891485628e-10) A[2]:(0.999998748302) A[3]:(3.79838283275e-28)\n",
      " state (6)  A[0]:(1.18803078486e-10) A[1]:(3.06880996309e-12) A[2]:(1.0) A[3]:(1.28432778917e-32)\n",
      " state (7)  A[0]:(4.30079340197e-11) A[1]:(2.21708176991e-12) A[2]:(1.0) A[3]:(4.55425596696e-33)\n",
      " state (8)  A[0]:(3.54890769549e-11) A[1]:(2.09595014955e-12) A[2]:(1.0) A[3]:(3.74682910119e-33)\n",
      " state (9)  A[0]:(3.3710728653e-11) A[1]:(2.06660253146e-12) A[2]:(1.0) A[3]:(3.55930065069e-33)\n",
      " state (10)  A[0]:(3.28096994018e-11) A[1]:(2.0542742854e-12) A[2]:(1.0) A[3]:(3.47499309382e-33)\n",
      " state (11)  A[0]:(3.16987582016e-11) A[1]:(2.04196880758e-12) A[2]:(1.0) A[3]:(3.38306723154e-33)\n",
      " state (12)  A[0]:(2.98445816671e-11) A[1]:(2.02242194346e-12) A[2]:(1.0) A[3]:(3.23317370205e-33)\n",
      " state (13)  A[0]:(2.6866901065e-11) A[1]:(1.99019772017e-12) A[2]:(1.0) A[3]:(2.98611601358e-33)\n",
      " state (14)  A[0]:(2.28167831851e-11) A[1]:(1.94475490399e-12) A[2]:(1.0) A[3]:(2.62875213927e-33)\n",
      " state (15)  A[0]:(1.85429796518e-11) A[1]:(1.90126343488e-12) A[2]:(1.0) A[3]:(2.21035714929e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 631000 finished after 12 . Running score: 0.09. Policy_loss: -92050.6111781, Value_loss: 1.19225391693. Times trained:               15451. Times reached goal: 84.               Steps done: 7764536.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996834814548) A[1]:(0.00141356152017) A[2]:(0.00134325865656) A[3]:(0.000408354273532)\n",
      " state (1)  A[0]:(0.00790111720562) A[1]:(0.00126140646171) A[2]:(0.00414573261514) A[3]:(0.986691772938)\n",
      " state (2)  A[0]:(0.999999761581) A[1]:(1.80965340491e-08) A[2]:(2.080528958e-07) A[3]:(3.26448507115e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.7723796309e-10) A[2]:(5.89559467912e-09) A[3]:(4.58718620814e-13)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(6.37802255454e-10) A[2]:(3.63971039974e-08) A[3]:(3.70806121539e-14)\n",
      " state (5)  A[0]:(1.02212432296e-07) A[1]:(2.9745469976e-11) A[2]:(0.999999880791) A[3]:(8.48440074672e-30)\n",
      " state (6)  A[0]:(9.45507491745e-11) A[1]:(1.95120503955e-12) A[2]:(1.0) A[3]:(4.86179614586e-33)\n",
      " state (7)  A[0]:(4.7094817629e-11) A[1]:(1.59930597411e-12) A[2]:(1.0) A[3]:(2.56112650028e-33)\n",
      " state (8)  A[0]:(4.11293568647e-11) A[1]:(1.54528874505e-12) A[2]:(1.0) A[3]:(2.26662732918e-33)\n",
      " state (9)  A[0]:(3.94684805671e-11) A[1]:(1.53101890974e-12) A[2]:(1.0) A[3]:(2.18720744109e-33)\n",
      " state (10)  A[0]:(3.82936564403e-11) A[1]:(1.52346549005e-12) A[2]:(1.0) A[3]:(2.13851405697e-33)\n",
      " state (11)  A[0]:(3.65580482553e-11) A[1]:(1.51430224696e-12) A[2]:(1.0) A[3]:(2.07241839735e-33)\n",
      " state (12)  A[0]:(3.36358406405e-11) A[1]:(1.49930296043e-12) A[2]:(1.0) A[3]:(1.96042060168e-33)\n",
      " state (13)  A[0]:(2.92774415822e-11) A[1]:(1.47669485147e-12) A[2]:(1.0) A[3]:(1.78363543487e-33)\n",
      " state (14)  A[0]:(2.41863577777e-11) A[1]:(1.45347915561e-12) A[2]:(1.0) A[3]:(1.55404191902e-33)\n",
      " state (15)  A[0]:(1.99317697763e-11) A[1]:(1.45582645331e-12) A[2]:(1.0) A[3]:(1.32556631668e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 632000 finished after 23 . Running score: 0.13. Policy_loss: -92050.6111776, Value_loss: 1.00200065684. Times trained:               15391. Times reached goal: 114.               Steps done: 7779927.\n",
      " state (0)  A[0]:(0.994268476963) A[1]:(0.00135284790304) A[2]:(0.00168439559639) A[3]:(0.00269429152831)\n",
      " state (1)  A[0]:(0.00756440032274) A[1]:(0.001137017156) A[2]:(0.00373853021301) A[3]:(0.987560033798)\n",
      " state (2)  A[0]:(0.999999284744) A[1]:(6.59195436015e-08) A[2]:(6.59159979932e-07) A[3]:(3.13297032761e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.73884867511e-10) A[2]:(5.46931344658e-09) A[3]:(5.42208422258e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.49358209029e-10) A[2]:(6.82828238396e-09) A[3]:(2.05089065305e-13)\n",
      " state (5)  A[0]:(0.750035226345) A[1]:(1.27287449914e-07) A[2]:(0.249964654446) A[3]:(1.39462961469e-19)\n",
      " state (6)  A[0]:(1.05631448033e-09) A[1]:(4.9617124645e-12) A[2]:(1.0) A[3]:(3.22208173736e-32)\n",
      " state (7)  A[0]:(1.35118014e-10) A[1]:(2.46645542532e-12) A[2]:(1.0) A[3]:(4.24001620494e-33)\n",
      " state (8)  A[0]:(9.643233434e-11) A[1]:(2.24070588473e-12) A[2]:(1.0) A[3]:(3.09527719444e-33)\n",
      " state (9)  A[0]:(8.87983228615e-11) A[1]:(2.19146510834e-12) A[2]:(1.0) A[3]:(2.86469534499e-33)\n",
      " state (10)  A[0]:(8.61484078518e-11) A[1]:(2.17518256011e-12) A[2]:(1.0) A[3]:(2.78634074865e-33)\n",
      " state (11)  A[0]:(8.40988667594e-11) A[1]:(2.16504158351e-12) A[2]:(1.0) A[3]:(2.73213521439e-33)\n",
      " state (12)  A[0]:(8.11868211548e-11) A[1]:(2.15273784042e-12) A[2]:(1.0) A[3]:(2.66085525734e-33)\n",
      " state (13)  A[0]:(7.64312085844e-11) A[1]:(2.13325299266e-12) A[2]:(1.0) A[3]:(2.54534806002e-33)\n",
      " state (14)  A[0]:(6.91358983951e-11) A[1]:(2.1028335323e-12) A[2]:(1.0) A[3]:(2.36237428279e-33)\n",
      " state (15)  A[0]:(5.95484148103e-11) A[1]:(2.06251508927e-12) A[2]:(1.0) A[3]:(2.1052953213e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 633000 finished after 11 . Running score: 0.14. Policy_loss: -92050.6125101, Value_loss: 1.41792425055. Times trained:               15769. Times reached goal: 112.               Steps done: 7795696.\n",
      " state (0)  A[0]:(0.996239423752) A[1]:(0.00114201917313) A[2]:(0.00137527647894) A[3]:(0.00124325277284)\n",
      " state (1)  A[0]:(0.00640769209713) A[1]:(0.000902199943084) A[2]:(0.00373503309675) A[3]:(0.988955080509)\n",
      " state (2)  A[0]:(0.999999642372) A[1]:(2.43592097604e-08) A[2]:(3.28002442984e-07) A[3]:(8.44965042379e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.43886807919e-10) A[2]:(6.53130749484e-09) A[3]:(7.33945239653e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.12082221177e-10) A[2]:(6.03526650877e-09) A[3]:(4.91626650414e-13)\n",
      " state (5)  A[0]:(0.999998629093) A[1]:(3.06000802475e-09) A[2]:(1.38051245813e-06) A[3]:(5.62105378655e-15)\n",
      " state (6)  A[0]:(1.2428356122e-08) A[1]:(1.31812330148e-11) A[2]:(1.0) A[3]:(7.33812551792e-31)\n",
      " state (7)  A[0]:(1.4754997224e-10) A[1]:(2.30007701253e-12) A[2]:(1.0) A[3]:(6.18294600169e-33)\n",
      " state (8)  A[0]:(7.8756536015e-11) A[1]:(1.89242111565e-12) A[2]:(1.0) A[3]:(3.31439448422e-33)\n",
      " state (9)  A[0]:(6.62565557974e-11) A[1]:(1.80423232778e-12) A[2]:(1.0) A[3]:(2.78802060355e-33)\n",
      " state (10)  A[0]:(6.13366996083e-11) A[1]:(1.77063170983e-12) A[2]:(1.0) A[3]:(2.58385394887e-33)\n",
      " state (11)  A[0]:(5.76362187144e-11) A[1]:(1.74977957471e-12) A[2]:(1.0) A[3]:(2.4428001369e-33)\n",
      " state (12)  A[0]:(5.31008188809e-11) A[1]:(1.72860586522e-12) A[2]:(1.0) A[3]:(2.28078248521e-33)\n",
      " state (13)  A[0]:(4.69138963788e-11) A[1]:(1.70249133801e-12) A[2]:(1.0) A[3]:(2.05651046903e-33)\n",
      " state (14)  A[0]:(3.97265519092e-11) A[1]:(1.67858565589e-12) A[2]:(1.0) A[3]:(1.7729850887e-33)\n",
      " state (15)  A[0]:(3.35836601584e-11) A[1]:(1.68824394568e-12) A[2]:(1.0) A[3]:(1.48829486694e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 634000 finished after 6 . Running score: 0.12. Policy_loss: -92050.6114411, Value_loss: 0.979848821397. Times trained:               15778. Times reached goal: 125.               Steps done: 7811474.\n",
      "action_dist \n",
      "tensor([[ 0.9964,  0.0015,  0.0012,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9185e-10,  6.1943e-09,  5.6861e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9185e-10,  6.1943e-09,  5.6862e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9185e-10,  6.1943e-09,  5.6863e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9185e-10,  6.1943e-09,  5.6863e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9964,  0.0015,  0.0012,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9186e-10,  6.1943e-09,  5.6865e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.0266e-11,  2.2825e-12,  1.0000e+00,  3.1962e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996386408806) A[1]:(0.0015280356165) A[2]:(0.00123291346245) A[3]:(0.00085264747031)\n",
      " state (1)  A[0]:(0.00611834786832) A[1]:(0.000992969144136) A[2]:(0.00362051674165) A[3]:(0.989268183708)\n",
      " state (2)  A[0]:(0.999999642372) A[1]:(2.89422334987e-08) A[2]:(3.20008496146e-07) A[3]:(1.01619423987e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.8494987753e-10) A[2]:(5.93970428397e-09) A[3]:(8.07421404042e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.91856380677e-10) A[2]:(6.19423534687e-09) A[3]:(5.68666478924e-13)\n",
      " state (5)  A[0]:(0.999920845032) A[1]:(2.18169589061e-08) A[2]:(7.91510974523e-05) A[3]:(6.59275600759e-16)\n",
      " state (6)  A[0]:(1.69977809605e-09) A[1]:(8.2756943659e-12) A[2]:(1.0) A[3]:(1.40035619333e-31)\n",
      " state (7)  A[0]:(8.06992667135e-11) A[1]:(2.61953067744e-12) A[2]:(1.0) A[3]:(5.21841358125e-33)\n",
      " state (8)  A[0]:(5.02662182711e-11) A[1]:(2.28250404688e-12) A[2]:(1.0) A[3]:(3.19623746549e-33)\n",
      " state (9)  A[0]:(4.35751955352e-11) A[1]:(2.20109152259e-12) A[2]:(1.0) A[3]:(2.74684928125e-33)\n",
      " state (10)  A[0]:(4.09140325786e-11) A[1]:(2.16917911584e-12) A[2]:(1.0) A[3]:(2.56856242041e-33)\n",
      " state (11)  A[0]:(3.94166470297e-11) A[1]:(2.15249953278e-12) A[2]:(1.0) A[3]:(2.47214194527e-33)\n",
      " state (12)  A[0]:(3.82767879892e-11) A[1]:(2.14143503277e-12) A[2]:(1.0) A[3]:(2.40353403381e-33)\n",
      " state (13)  A[0]:(3.71336121574e-11) A[1]:(2.13227265705e-12) A[2]:(1.0) A[3]:(2.33863131728e-33)\n",
      " state (14)  A[0]:(3.57357129999e-11) A[1]:(2.12306864797e-12) A[2]:(1.0) A[3]:(2.26099689491e-33)\n",
      " state (15)  A[0]:(3.38748300244e-11) A[1]:(2.11313041718e-12) A[2]:(1.0) A[3]:(2.15619128796e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 635000 finished after 8 . Running score: 0.07. Policy_loss: -92050.6111761, Value_loss: 1.41930731686. Times trained:               15649. Times reached goal: 102.               Steps done: 7827123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995913803577) A[1]:(0.00154734274838) A[2]:(0.00126598414499) A[3]:(0.00127287977375)\n",
      " state (1)  A[0]:(0.00549048278481) A[1]:(0.0008991897339) A[2]:(0.00334204221144) A[3]:(0.990268290043)\n",
      " state (2)  A[0]:(0.999997913837) A[1]:(2.17696822347e-07) A[2]:(1.83630299944e-06) A[3]:(3.26445750432e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.22767820751e-10) A[2]:(5.47960921082e-09) A[3]:(1.09214113447e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.80257905561e-10) A[2]:(8.21382695193e-09) A[3]:(1.49305329385e-12)\n",
      " state (5)  A[0]:(0.999999523163) A[1]:(1.30614683513e-08) A[2]:(4.84188774408e-07) A[3]:(1.2360240869e-12)\n",
      " state (6)  A[0]:(5.15993576755e-07) A[1]:(2.33453478771e-10) A[2]:(0.999999463558) A[3]:(1.83240620851e-27)\n",
      " state (7)  A[0]:(2.19557996783e-10) A[1]:(6.71095644333e-12) A[2]:(1.0) A[3]:(7.22730425985e-32)\n",
      " state (8)  A[0]:(5.96755353466e-11) A[1]:(4.05202694315e-12) A[2]:(1.0) A[3]:(1.39867385521e-32)\n",
      " state (9)  A[0]:(3.93047122627e-11) A[1]:(3.51343194989e-12) A[2]:(1.0) A[3]:(8.19732454914e-33)\n",
      " state (10)  A[0]:(3.22373898409e-11) A[1]:(3.30441251618e-12) A[2]:(1.0) A[3]:(6.31972652435e-33)\n",
      " state (11)  A[0]:(2.88252963165e-11) A[1]:(3.20004136037e-12) A[2]:(1.0) A[3]:(5.43822771878e-33)\n",
      " state (12)  A[0]:(2.69253317559e-11) A[1]:(3.14157272245e-12) A[2]:(1.0) A[3]:(4.95451032404e-33)\n",
      " state (13)  A[0]:(2.57864441605e-11) A[1]:(3.10670196166e-12) A[2]:(1.0) A[3]:(4.66731060552e-33)\n",
      " state (14)  A[0]:(2.5072336568e-11) A[1]:(3.08507819985e-12) A[2]:(1.0) A[3]:(4.48867991257e-33)\n",
      " state (15)  A[0]:(2.46093510153e-11) A[1]:(3.07131620483e-12) A[2]:(1.0) A[3]:(4.37384109399e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 636000 finished after 12 . Running score: 0.1. Policy_loss: -92050.6111823, Value_loss: 0.996248689178. Times trained:               15709. Times reached goal: 119.               Steps done: 7842832.\n",
      " state (0)  A[0]:(0.99566411972) A[1]:(0.00136629387271) A[2]:(0.0013487690594) A[3]:(0.00162084458862)\n",
      " state (1)  A[0]:(0.00591299124062) A[1]:(0.000903021835256) A[2]:(0.00355692836456) A[3]:(0.989627063274)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(5.65811353326e-09) A[2]:(6.54759944041e-08) A[3]:(8.22847900928e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.46829129253e-10) A[2]:(6.14755535366e-09) A[3]:(1.10772870911e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.64727720353e-09) A[2]:(1.9505689508e-08) A[3]:(2.52410310535e-12)\n",
      " state (5)  A[0]:(0.974692344666) A[1]:(4.96820007356e-07) A[2]:(0.0253071393818) A[3]:(3.04292907002e-16)\n",
      " state (6)  A[0]:(8.5894136248e-10) A[1]:(1.18645414673e-11) A[2]:(1.0) A[3]:(3.9717513788e-31)\n",
      " state (7)  A[0]:(4.92635272853e-11) A[1]:(3.5607578084e-12) A[2]:(1.0) A[3]:(1.01245071297e-32)\n",
      " state (8)  A[0]:(2.85156759317e-11) A[1]:(2.91313739624e-12) A[2]:(1.0) A[3]:(4.86327984013e-33)\n",
      " state (9)  A[0]:(2.33085564738e-11) A[1]:(2.72811829551e-12) A[2]:(1.0) A[3]:(3.65718663668e-33)\n",
      " state (10)  A[0]:(2.10920135524e-11) A[1]:(2.64838238145e-12) A[2]:(1.0) A[3]:(3.15642236981e-33)\n",
      " state (11)  A[0]:(1.99435572223e-11) A[1]:(2.6076213671e-12) A[2]:(1.0) A[3]:(2.89974638248e-33)\n",
      " state (12)  A[0]:(1.92901077056e-11) A[1]:(2.58496110801e-12) A[2]:(1.0) A[3]:(2.75497157988e-33)\n",
      " state (13)  A[0]:(1.88947902463e-11) A[1]:(2.57172234897e-12) A[2]:(1.0) A[3]:(2.66847888921e-33)\n",
      " state (14)  A[0]:(1.86397858953e-11) A[1]:(2.56366108897e-12) A[2]:(1.0) A[3]:(2.613992522e-33)\n",
      " state (15)  A[0]:(1.84589947494e-11) A[1]:(2.55852218752e-12) A[2]:(1.0) A[3]:(2.57704214278e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 637000 finished after 15 . Running score: 0.12. Policy_loss: -92050.6112015, Value_loss: 1.21453119698. Times trained:               15028. Times reached goal: 128.               Steps done: 7857860.\n",
      " state (0)  A[0]:(0.994498550892) A[1]:(0.00153316115029) A[2]:(0.0014651970705) A[3]:(0.00250310217962)\n",
      " state (1)  A[0]:(0.00566782010719) A[1]:(0.000904903921764) A[2]:(0.0034848083742) A[3]:(0.989942491055)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(3.64864716218e-09) A[2]:(3.50690427808e-08) A[3]:(4.38304288697e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.11340303433e-10) A[2]:(5.38541344852e-09) A[3]:(1.3574739206e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.8774750643e-09) A[2]:(1.65193156931e-08) A[3]:(3.50469089513e-12)\n",
      " state (5)  A[0]:(0.999979257584) A[1]:(6.40453308165e-08) A[2]:(2.07026369026e-05) A[3]:(9.99286238609e-14)\n",
      " state (6)  A[0]:(8.78549943906e-09) A[1]:(3.35249085437e-11) A[2]:(1.0) A[3]:(5.49548085292e-30)\n",
      " state (7)  A[0]:(7.76018069248e-11) A[1]:(4.49660317661e-12) A[2]:(1.0) A[3]:(1.72244687641e-32)\n",
      " state (8)  A[0]:(3.40201269933e-11) A[1]:(3.33934247441e-12) A[2]:(1.0) A[3]:(6.17945625284e-33)\n",
      " state (9)  A[0]:(2.55912877695e-11) A[1]:(3.05389697905e-12) A[2]:(1.0) A[3]:(4.21344452248e-33)\n",
      " state (10)  A[0]:(2.23018391948e-11) A[1]:(2.93922395103e-12) A[2]:(1.0) A[3]:(3.45387423578e-33)\n",
      " state (11)  A[0]:(2.06569327615e-11) A[1]:(2.88336390354e-12) A[2]:(1.0) A[3]:(3.07419029515e-33)\n",
      " state (12)  A[0]:(1.97359247001e-11) A[1]:(2.85347028124e-12) A[2]:(1.0) A[3]:(2.86185542411e-33)\n",
      " state (13)  A[0]:(1.91866297156e-11) A[1]:(2.83656171468e-12) A[2]:(1.0) A[3]:(2.7357645532e-33)\n",
      " state (14)  A[0]:(1.88433608994e-11) A[1]:(2.82659702935e-12) A[2]:(1.0) A[3]:(2.65775103389e-33)\n",
      " state (15)  A[0]:(1.86190698276e-11) A[1]:(2.82058686303e-12) A[2]:(1.0) A[3]:(2.60773779003e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 638000 finished after 15 . Running score: 0.18. Policy_loss: -92050.6112018, Value_loss: 1.21176788118. Times trained:               15370. Times reached goal: 120.               Steps done: 7873230.\n",
      " state (0)  A[0]:(0.996193230152) A[1]:(0.00137171021197) A[2]:(0.0012249336578) A[3]:(0.00121014204342)\n",
      " state (1)  A[0]:(0.00572827458382) A[1]:(0.000887735572178) A[2]:(0.00351683306508) A[3]:(0.989867150784)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(9.12685216292e-09) A[2]:(8.98777372527e-08) A[3]:(3.91165572156e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.64906824307e-10) A[2]:(2.11669548555e-09) A[3]:(5.16454067012e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.28151428042e-10) A[2]:(1.29801147608e-08) A[3]:(5.88384321733e-13)\n",
      " state (5)  A[0]:(0.0110927298665) A[1]:(2.11882422718e-08) A[2]:(0.988907277584) A[3]:(1.50542681056e-22)\n",
      " state (6)  A[0]:(4.50684239928e-10) A[1]:(1.66755400721e-12) A[2]:(1.0) A[3]:(7.75740093113e-33)\n",
      " state (7)  A[0]:(9.75080224896e-11) A[1]:(1.03695730388e-12) A[2]:(1.0) A[3]:(1.66410474079e-33)\n",
      " state (8)  A[0]:(7.43619471555e-11) A[1]:(1.00335267438e-12) A[2]:(1.0) A[3]:(1.28130785234e-33)\n",
      " state (9)  A[0]:(6.92591678453e-11) A[1]:(9.99707803519e-13) A[2]:(1.0) A[3]:(1.18832055539e-33)\n",
      " state (10)  A[0]:(6.76338776651e-11) A[1]:(9.99092085105e-13) A[2]:(1.0) A[3]:(1.15562390452e-33)\n",
      " state (11)  A[0]:(6.69959018818e-11) A[1]:(9.98949187259e-13) A[2]:(1.0) A[3]:(1.1418206621e-33)\n",
      " state (12)  A[0]:(6.67081112571e-11) A[1]:(9.9891492647e-13) A[2]:(1.0) A[3]:(1.13534910653e-33)\n",
      " state (13)  A[0]:(6.65634700137e-11) A[1]:(9.9891492647e-13) A[2]:(1.0) A[3]:(1.13209684426e-33)\n",
      " state (14)  A[0]:(6.64815077989e-11) A[1]:(9.98934008428e-13) A[2]:(1.0) A[3]:(1.13033617413e-33)\n",
      " state (15)  A[0]:(6.64259897087e-11) A[1]:(9.98968269217e-13) A[2]:(1.0) A[3]:(1.12923286248e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 639000 finished after 17 . Running score: 0.17. Policy_loss: -92050.6111989, Value_loss: 1.20974487807. Times trained:               15999. Times reached goal: 117.               Steps done: 7889229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8091e-10,  1.8302e-08,  2.1046e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8089e-10,  1.8299e-08,  2.1049e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8081e-10,  1.8293e-08,  2.1057e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8080e-10,  1.8291e-08,  2.1058e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8078e-10,  1.8289e-08,  2.1063e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8077e-10,  1.8287e-08,  2.1065e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8077e-10,  1.8287e-08,  2.1066e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0013,  0.0014,  0.0010]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8077e-10,  1.8286e-08,  2.1067e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.6030e-11,  4.8876e-13,  1.0000e+00,  1.0015e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8061e-10,  1.8274e-08,  2.1079e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.6027e-11,  4.8877e-13,  1.0000e+00,  1.0015e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996308505535) A[1]:(0.00132641929667) A[2]:(0.00137915718369) A[3]:(0.000985939987004)\n",
      " state (1)  A[0]:(0.00474190618843) A[1]:(0.000790695368778) A[2]:(0.0034680867102) A[3]:(0.990999341011)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.06351646776e-10) A[2]:(5.79513192989e-09) A[3]:(4.12663739985e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.41725936542e-10) A[2]:(2.33146257855e-09) A[3]:(6.25310079324e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.8031945088e-10) A[2]:(1.82523915981e-08) A[3]:(2.10973044048e-13)\n",
      " state (5)  A[0]:(0.00259026000276) A[1]:(4.59891902338e-09) A[2]:(0.997409760952) A[3]:(6.43347385986e-24)\n",
      " state (6)  A[0]:(4.74440264853e-10) A[1]:(8.46716089169e-13) A[2]:(1.0) A[3]:(6.12645762066e-33)\n",
      " state (7)  A[0]:(9.3165697379e-11) A[1]:(4.99955708343e-13) A[2]:(1.0) A[3]:(1.29672804234e-33)\n",
      " state (8)  A[0]:(6.6026954737e-11) A[1]:(4.88767500865e-13) A[2]:(1.0) A[3]:(1.00152936026e-33)\n",
      " state (9)  A[0]:(6.01479352436e-11) A[1]:(4.9116548502e-13) A[2]:(1.0) A[3]:(9.39522676102e-34)\n",
      " state (10)  A[0]:(5.8529341096e-11) A[1]:(4.92433730511e-13) A[2]:(1.0) A[3]:(9.21484256111e-34)\n",
      " state (11)  A[0]:(5.80202760836e-11) A[1]:(4.92913002081e-13) A[2]:(1.0) A[3]:(9.15339267557e-34)\n",
      " state (12)  A[0]:(5.78396219808e-11) A[1]:(4.93093521743e-13) A[2]:(1.0) A[3]:(9.12974870873e-34)\n",
      " state (13)  A[0]:(5.77650913214e-11) A[1]:(4.93176300579e-13) A[2]:(1.0) A[3]:(9.11951547939e-34)\n",
      " state (14)  A[0]:(5.7725220437e-11) A[1]:(4.93223300743e-13) A[2]:(1.0) A[3]:(9.11422942823e-34)\n",
      " state (15)  A[0]:(5.76956121767e-11) A[1]:(4.93262819912e-13) A[2]:(1.0) A[3]:(9.11068365973e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 640000 finished after 26 . Running score: 0.15. Policy_loss: -92050.6111955, Value_loss: 0.992609589137. Times trained:               16052. Times reached goal: 126.               Steps done: 7905281.\n",
      " state (0)  A[0]:(0.997316598892) A[1]:(0.00117243546993) A[2]:(0.00109158223495) A[3]:(0.000419383664848)\n",
      " state (1)  A[0]:(0.00339277321473) A[1]:(0.000644156418275) A[2]:(0.0028439082671) A[3]:(0.993119180202)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.13153261603e-10) A[2]:(6.35485752909e-09) A[3]:(5.0474286209e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.07870956559e-10) A[2]:(3.73575703705e-09) A[3]:(1.20778376449e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.46178483049e-10) A[2]:(1.56587987021e-08) A[3]:(4.13312613301e-13)\n",
      " state (5)  A[0]:(0.998478353024) A[1]:(4.00454958083e-08) A[2]:(0.00152160704602) A[3]:(6.52147937622e-17)\n",
      " state (6)  A[0]:(3.40176975477e-09) A[1]:(1.95821418976e-12) A[2]:(1.0) A[3]:(9.68401222143e-32)\n",
      " state (7)  A[0]:(9.59728685412e-11) A[1]:(3.91238148649e-13) A[2]:(1.0) A[3]:(1.66684933643e-33)\n",
      " state (8)  A[0]:(5.55912572919e-11) A[1]:(3.5361673984e-13) A[2]:(1.0) A[3]:(1.0221245723e-33)\n",
      " state (9)  A[0]:(4.77862957537e-11) A[1]:(3.54206030825e-13) A[2]:(1.0) A[3]:(9.25203777377e-34)\n",
      " state (10)  A[0]:(4.57309988489e-11) A[1]:(3.55411311275e-13) A[2]:(1.0) A[3]:(9.01196325642e-34)\n",
      " state (11)  A[0]:(4.51379107702e-11) A[1]:(3.55850873941e-13) A[2]:(1.0) A[3]:(8.94224082938e-34)\n",
      " state (12)  A[0]:(4.49596193608e-11) A[1]:(3.55981195042e-13) A[2]:(1.0) A[3]:(8.92057224407e-34)\n",
      " state (13)  A[0]:(4.49039139205e-11) A[1]:(3.56020578686e-13) A[2]:(1.0) A[3]:(8.91336040255e-34)\n",
      " state (14)  A[0]:(4.48841346035e-11) A[1]:(3.56031447813e-13) A[2]:(1.0) A[3]:(8.91070911178e-34)\n",
      " state (15)  A[0]:(4.48745485215e-11) A[1]:(3.56040961687e-13) A[2]:(1.0) A[3]:(8.90948586297e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 641000 finished after 8 . Running score: 0.13. Policy_loss: -92050.6111768, Value_loss: 1.21021744489. Times trained:               15594. Times reached goal: 106.               Steps done: 7920875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997522115707) A[1]:(0.00115208886564) A[2]:(0.00097885762807) A[3]:(0.00034692249028)\n",
      " state (1)  A[0]:(0.0024590685498) A[1]:(0.000524121394847) A[2]:(0.00228716013953) A[3]:(0.9947296381)\n",
      " state (2)  A[0]:(0.999999821186) A[1]:(1.44940672797e-08) A[2]:(1.71747387867e-07) A[3]:(1.80784143211e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.91288332396e-10) A[2]:(3.15735815271e-09) A[3]:(1.76155010717e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.22152435336e-10) A[2]:(4.26843582702e-09) A[3]:(1.28244376923e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.78908071585e-10) A[2]:(1.3089881179e-08) A[3]:(4.39170835115e-13)\n",
      " state (6)  A[0]:(0.99999588728) A[1]:(3.96121757618e-09) A[2]:(4.08158757637e-06) A[3]:(3.43292733819e-15)\n",
      " state (7)  A[0]:(0.000136395610753) A[1]:(6.86223189383e-10) A[2]:(0.999863624573) A[3]:(1.24307246982e-25)\n",
      " state (8)  A[0]:(1.81865289495e-09) A[1]:(1.32010277275e-12) A[2]:(1.0) A[3]:(3.62489573339e-32)\n",
      " state (9)  A[0]:(1.91500801328e-10) A[1]:(5.09626845932e-13) A[2]:(1.0) A[3]:(3.14205452276e-33)\n",
      " state (10)  A[0]:(1.09106369772e-10) A[1]:(4.40892222904e-13) A[2]:(1.0) A[3]:(1.80333910791e-33)\n",
      " state (11)  A[0]:(8.93409443647e-11) A[1]:(4.32491607631e-13) A[2]:(1.0) A[3]:(1.53410975927e-33)\n",
      " state (12)  A[0]:(8.27520899027e-11) A[1]:(4.31660078775e-13) A[2]:(1.0) A[3]:(1.4506940169e-33)\n",
      " state (13)  A[0]:(8.03198271782e-11) A[1]:(4.31548107796e-13) A[2]:(1.0) A[3]:(1.41972919982e-33)\n",
      " state (14)  A[0]:(7.93802870658e-11) A[1]:(4.31436191026e-13) A[2]:(1.0) A[3]:(1.40711256385e-33)\n",
      " state (15)  A[0]:(7.90071896795e-11) A[1]:(4.31314408017e-13) A[2]:(1.0) A[3]:(1.40164807632e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 642000 finished after 12 . Running score: 0.12. Policy_loss: -92050.6111777, Value_loss: 1.2055190448. Times trained:               15635. Times reached goal: 100.               Steps done: 7936510.\n",
      " state (0)  A[0]:(0.997165203094) A[1]:(0.00109544373117) A[2]:(0.000950151879806) A[3]:(0.000789219862781)\n",
      " state (1)  A[0]:(0.00126625725534) A[1]:(0.000327897403622) A[2]:(0.00146798498463) A[3]:(0.996937870979)\n",
      " state (2)  A[0]:(0.999998509884) A[1]:(1.15677210033e-07) A[2]:(1.30790192543e-06) A[3]:(7.7282614086e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.84993256824e-10) A[2]:(3.38928307642e-09) A[3]:(2.82993095103e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.83267817588e-10) A[2]:(3.98673227764e-09) A[3]:(1.67650908347e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.57013132998e-10) A[2]:(1.04521200583e-08) A[3]:(4.87314398903e-13)\n",
      " state (6)  A[0]:(0.999997258186) A[1]:(2.0007340229e-09) A[2]:(2.74552030533e-06) A[3]:(3.02424050565e-15)\n",
      " state (7)  A[0]:(7.86819655332e-05) A[1]:(2.90255097735e-10) A[2]:(0.999921321869) A[3]:(5.47367779122e-26)\n",
      " state (8)  A[0]:(1.218173451e-09) A[1]:(6.46078188778e-13) A[2]:(1.0) A[3]:(2.49285411899e-32)\n",
      " state (9)  A[0]:(1.71121464221e-10) A[1]:(2.80217147229e-13) A[2]:(1.0) A[3]:(2.84606926966e-33)\n",
      " state (10)  A[0]:(1.08018247125e-10) A[1]:(2.4891151423e-13) A[2]:(1.0) A[3]:(1.79447312544e-33)\n",
      " state (11)  A[0]:(9.1790949841e-11) A[1]:(2.45363056045e-13) A[2]:(1.0) A[3]:(1.58192629806e-33)\n",
      " state (12)  A[0]:(8.61495597082e-11) A[1]:(2.4511886661e-13) A[2]:(1.0) A[3]:(1.51745199413e-33)\n",
      " state (13)  A[0]:(8.40154959492e-11) A[1]:(2.45113255864e-13) A[2]:(1.0) A[3]:(1.49386909789e-33)\n",
      " state (14)  A[0]:(8.31728505513e-11) A[1]:(2.45070267248e-13) A[2]:(1.0) A[3]:(1.48428064557e-33)\n",
      " state (15)  A[0]:(8.2829312853e-11) A[1]:(2.45014186891e-13) A[2]:(1.0) A[3]:(1.48010791613e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 643000 finished after 12 . Running score: 0.06. Policy_loss: -92050.6111776, Value_loss: 1.420648243. Times trained:               16261. Times reached goal: 114.               Steps done: 7952771.\n",
      " state (0)  A[0]:(0.990151941776) A[1]:(0.000994676724076) A[2]:(0.00126099970657) A[3]:(0.00759238051251)\n",
      " state (1)  A[0]:(0.000907793350052) A[1]:(0.000245235365583) A[2]:(0.00118605897296) A[3]:(0.997660934925)\n",
      " state (2)  A[0]:(0.999999821186) A[1]:(1.10898019656e-08) A[2]:(1.64829060623e-07) A[3]:(2.9445812455e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.49234305469e-10) A[2]:(3.12805847891e-09) A[3]:(3.29818507575e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.3216484851e-10) A[2]:(3.18208526195e-09) A[3]:(1.95937992394e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.4173837104e-10) A[2]:(5.14234654858e-09) A[3]:(7.85737035332e-13)\n",
      " state (6)  A[0]:(0.999999940395) A[1]:(2.45092585116e-10) A[2]:(4.46643788621e-08) A[3]:(3.98272418463e-14)\n",
      " state (7)  A[0]:(0.989202320576) A[1]:(1.16492921975e-08) A[2]:(0.0107976477593) A[3]:(1.01694825278e-18)\n",
      " state (8)  A[0]:(9.43849300938e-08) A[1]:(1.98107762833e-12) A[2]:(0.999999880791) A[3]:(1.04527590782e-30)\n",
      " state (9)  A[0]:(7.77980679878e-10) A[1]:(2.01569311372e-13) A[2]:(1.0) A[3]:(4.39917520131e-33)\n",
      " state (10)  A[0]:(3.35300398557e-10) A[1]:(1.50315347734e-13) A[2]:(1.0) A[3]:(1.81403353511e-33)\n",
      " state (11)  A[0]:(2.61092869547e-10) A[1]:(1.44730663001e-13) A[2]:(1.0) A[3]:(1.47765948997e-33)\n",
      " state (12)  A[0]:(2.36595548708e-10) A[1]:(1.44346909642e-13) A[2]:(1.0) A[3]:(1.39099506693e-33)\n",
      " state (13)  A[0]:(2.27160512623e-10) A[1]:(1.44452388961e-13) A[2]:(1.0) A[3]:(1.36140842513e-33)\n",
      " state (14)  A[0]:(2.2331178573e-10) A[1]:(1.44507236038e-13) A[2]:(1.0) A[3]:(1.34963943079e-33)\n",
      " state (15)  A[0]:(2.21684712254e-10) A[1]:(1.4451164061e-13) A[2]:(1.0) A[3]:(1.34452134676e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 644000 finished after 9 . Running score: 0.11. Policy_loss: -92050.6121441, Value_loss: 1.00139972616. Times trained:               15641. Times reached goal: 123.               Steps done: 7968412.\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0009,  0.0007,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0009,  0.0007,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0009,  0.0007,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0009,  0.0007,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0009,  0.0007,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.2097e-10,  3.2555e-09,  1.6524e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.5773e-09,  2.6246e-13,  1.0000e+00,  1.3155e-32]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.997118115425) A[1]:(0.000868296658155) A[2]:(0.00067347555887) A[3]:(0.00134009181056)\n",
      " state (1)  A[0]:(0.000904982211068) A[1]:(0.000236562933424) A[2]:(0.00112342159264) A[3]:(0.997735023499)\n",
      " state (2)  A[0]:(0.999999761581) A[1]:(1.33633877297e-08) A[2]:(1.99884354402e-07) A[3]:(4.51909265564e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.34521171979e-10) A[2]:(2.9627953424e-09) A[3]:(3.15654642181e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.20971468953e-10) A[2]:(3.25557092395e-09) A[3]:(1.65247654969e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.36880590196e-10) A[2]:(7.04526348372e-09) A[3]:(4.12090148246e-13)\n",
      " state (6)  A[0]:(0.999999582767) A[1]:(4.2528938704e-10) A[2]:(4.14139918803e-07) A[3]:(3.27401061652e-15)\n",
      " state (7)  A[0]:(0.00214556558058) A[1]:(6.88217316469e-10) A[2]:(0.997854411602) A[3]:(1.81066770259e-24)\n",
      " state (8)  A[0]:(2.5773341239e-09) A[1]:(2.62463065604e-13) A[2]:(1.0) A[3]:(1.31553731112e-32)\n",
      " state (9)  A[0]:(3.8953065773e-10) A[1]:(1.19028119067e-13) A[2]:(1.0) A[3]:(1.64046885547e-33)\n",
      " state (10)  A[0]:(2.67209060434e-10) A[1]:(1.09343076585e-13) A[2]:(1.0) A[3]:(1.14628965285e-33)\n",
      " state (11)  A[0]:(2.34428143564e-10) A[1]:(1.08837248838e-13) A[2]:(1.0) A[3]:(1.05184630411e-33)\n",
      " state (12)  A[0]:(2.22665302863e-10) A[1]:(1.0907083342e-13) A[2]:(1.0) A[3]:(1.02457612087e-33)\n",
      " state (13)  A[0]:(2.18136286811e-10) A[1]:(1.09201561097e-13) A[2]:(1.0) A[3]:(1.01485147651e-33)\n",
      " state (14)  A[0]:(2.16328538416e-10) A[1]:(1.09242808213e-13) A[2]:(1.0) A[3]:(1.01094121292e-33)\n",
      " state (15)  A[0]:(2.15583814689e-10) A[1]:(1.09248222448e-13) A[2]:(1.0) A[3]:(1.00924583782e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 645000 finished after 7 . Running score: 0.12. Policy_loss: -92050.6111753, Value_loss: 1.01988726682. Times trained:               16072. Times reached goal: 110.               Steps done: 7984484.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996877014637) A[1]:(0.000953921466134) A[2]:(0.000759026501328) A[3]:(0.00141004926991)\n",
      " state (1)  A[0]:(0.00088824739214) A[1]:(0.000239492524997) A[2]:(0.00116214132868) A[3]:(0.997710108757)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.04187947159e-09) A[2]:(1.80725656662e-08) A[3]:(9.51198078636e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.04076275376e-10) A[2]:(2.23407758959e-09) A[3]:(2.44261663847e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.83514719718e-11) A[2]:(2.28537833102e-09) A[3]:(1.28297470304e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(7.95371407625e-11) A[2]:(3.624841316e-09) A[3]:(3.2100838372e-13)\n",
      " state (6)  A[0]:(0.999999940395) A[1]:(1.06228220664e-10) A[2]:(5.13622246956e-08) A[3]:(3.47293079847e-15)\n",
      " state (7)  A[0]:(0.862004458904) A[1]:(3.24618887326e-09) A[2]:(0.137995570898) A[3]:(1.86065110337e-21)\n",
      " state (8)  A[0]:(3.33882752557e-07) A[1]:(5.38418810404e-13) A[2]:(0.999999642372) A[3]:(1.14056771002e-31)\n",
      " state (9)  A[0]:(5.48274536882e-09) A[1]:(1.35600339009e-13) A[2]:(1.0) A[3]:(2.66038836567e-33)\n",
      " state (10)  A[0]:(2.10884953944e-09) A[1]:(1.23273963194e-13) A[2]:(1.0) A[3]:(1.42748415645e-33)\n",
      " state (11)  A[0]:(1.57178592364e-09) A[1]:(1.24740400843e-13) A[2]:(1.0) A[3]:(1.24354537183e-33)\n",
      " state (12)  A[0]:(1.40667888449e-09) A[1]:(1.2622260008e-13) A[2]:(1.0) A[3]:(1.19617046993e-33)\n",
      " state (13)  A[0]:(1.34458388867e-09) A[1]:(1.26947402785e-13) A[2]:(1.0) A[3]:(1.18018898139e-33)\n",
      " state (14)  A[0]:(1.3191054915e-09) A[1]:(1.27248973619e-13) A[2]:(1.0) A[3]:(1.17384020987e-33)\n",
      " state (15)  A[0]:(1.30804156395e-09) A[1]:(1.27365051015e-13) A[2]:(1.0) A[3]:(1.17104932914e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 646000 finished after 4 . Running score: 0.09. Policy_loss: -92050.6111771, Value_loss: 1.20992103899. Times trained:               15420. Times reached goal: 115.               Steps done: 7999904.\n",
      " state (0)  A[0]:(0.995308279991) A[1]:(0.000972992507741) A[2]:(0.0010276848916) A[3]:(0.00269102910534)\n",
      " state (1)  A[0]:(0.000831893470604) A[1]:(0.000220500995056) A[2]:(0.00113700772636) A[3]:(0.997810602188)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(2.55454701836e-09) A[2]:(4.59665692176e-08) A[3]:(4.82258177836e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.5165534347e-11) A[2]:(2.08367656462e-09) A[3]:(2.36043129487e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.45066708382e-11) A[2]:(1.99573158e-09) A[3]:(1.05146436263e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(4.76540439054e-11) A[2]:(3.04483993574e-09) A[3]:(1.64274003471e-13)\n",
      " state (6)  A[0]:(0.999999940395) A[1]:(6.89863791092e-11) A[2]:(6.81936924707e-08) A[3]:(9.62622769006e-16)\n",
      " state (7)  A[0]:(0.955084979534) A[1]:(1.43064804448e-09) A[2]:(0.0449150279164) A[3]:(1.74715489552e-21)\n",
      " state (8)  A[0]:(2.61958166448e-06) A[1]:(8.08350023203e-13) A[2]:(0.999997377396) A[3]:(6.02396985347e-31)\n",
      " state (9)  A[0]:(1.32058648461e-08) A[1]:(1.01553951338e-13) A[2]:(1.0) A[3]:(3.88928469017e-33)\n",
      " state (10)  A[0]:(3.29691962619e-09) A[1]:(8.23589446967e-14) A[2]:(1.0) A[3]:(1.5107404724e-33)\n",
      " state (11)  A[0]:(2.17032880556e-09) A[1]:(8.26083044201e-14) A[2]:(1.0) A[3]:(1.21042756337e-33)\n",
      " state (12)  A[0]:(1.86297111071e-09) A[1]:(8.37873268488e-14) A[2]:(1.0) A[3]:(1.1362936346e-33)\n",
      " state (13)  A[0]:(1.75215009168e-09) A[1]:(8.44657460294e-14) A[2]:(1.0) A[3]:(1.11245543569e-33)\n",
      " state (14)  A[0]:(1.70829361767e-09) A[1]:(8.47803205135e-14) A[2]:(1.0) A[3]:(1.103629769e-33)\n",
      " state (15)  A[0]:(1.69030534014e-09) A[1]:(8.49162659134e-14) A[2]:(1.0) A[3]:(1.10011586741e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 647000 finished after 9 . Running score: 0.07. Policy_loss: -92050.6114051, Value_loss: 1.20188777387. Times trained:               16247. Times reached goal: 120.               Steps done: 8016151.\n",
      " state (0)  A[0]:(0.993846356869) A[1]:(0.000918525271118) A[2]:(0.00104449375067) A[3]:(0.00419061258435)\n",
      " state (1)  A[0]:(0.00081640528515) A[1]:(0.000208180848858) A[2]:(0.00105809606612) A[3]:(0.997917294502)\n",
      " state (2)  A[0]:(0.99999243021) A[1]:(4.5880975108e-07) A[2]:(5.49443757336e-06) A[3]:(1.63803088071e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.2662983162e-10) A[2]:(2.68806643611e-09) A[3]:(5.43032546013e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.4894875024e-11) A[2]:(1.72989811364e-09) A[3]:(2.26179752011e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.95308108364e-11) A[2]:(1.60236790503e-09) A[3]:(1.26804209494e-12)\n",
      " state (6)  A[0]:(1.0) A[1]:(4.28357661841e-11) A[2]:(1.77370018672e-09) A[3]:(3.63641923648e-13)\n",
      " state (7)  A[0]:(1.0) A[1]:(3.84397201569e-11) A[2]:(5.63623769878e-09) A[3]:(2.18443935629e-14)\n",
      " state (8)  A[0]:(0.999999523163) A[1]:(9.24304036087e-11) A[2]:(5.03797025431e-07) A[3]:(6.40169038864e-17)\n",
      " state (9)  A[0]:(0.859764516354) A[1]:(1.55271517865e-09) A[2]:(0.140235498548) A[3]:(4.28166385834e-22)\n",
      " state (10)  A[0]:(1.34229831019e-05) A[1]:(2.01551319037e-12) A[2]:(0.999986588955) A[3]:(4.3113544239e-30)\n",
      " state (11)  A[0]:(5.54841399492e-08) A[1]:(1.87369339336e-13) A[2]:(0.999999940395) A[3]:(1.44389820856e-32)\n",
      " state (12)  A[0]:(9.03221142323e-09) A[1]:(1.22276158383e-13) A[2]:(1.0) A[3]:(3.43917541361e-33)\n",
      " state (13)  A[0]:(4.81020157039e-09) A[1]:(1.15665005243e-13) A[2]:(1.0) A[3]:(2.26771704917e-33)\n",
      " state (14)  A[0]:(3.73822306443e-09) A[1]:(1.15712005407e-13) A[2]:(1.0) A[3]:(1.96698261522e-33)\n",
      " state (15)  A[0]:(3.33818861442e-09) A[1]:(1.16388439142e-13) A[2]:(1.0) A[3]:(1.86055482607e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 648000 finished after 25 . Running score: 0.0. Policy_loss: -92050.611195, Value_loss: 0.978536576499. Times trained:               16759. Times reached goal: 38.               Steps done: 8032910.\n",
      " state (0)  A[0]:(0.99681198597) A[1]:(0.000895507400855) A[2]:(0.00087066157721) A[3]:(0.00142184807919)\n",
      " state (1)  A[0]:(0.000796919804998) A[1]:(0.000206941957003) A[2]:(0.00110526103526) A[3]:(0.997890889645)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(5.45182610168e-09) A[2]:(1.04455068595e-07) A[3]:(1.74921477303e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.37594923975e-11) A[2]:(2.46523623737e-09) A[3]:(2.32942766638e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.94426322001e-11) A[2]:(3.04643155147e-09) A[3]:(7.87992772162e-13)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.93633406321e-11) A[2]:(9.45075750991e-09) A[3]:(4.31386595553e-14)\n",
      " state (6)  A[0]:(0.999996721745) A[1]:(1.54864301938e-10) A[2]:(3.24858069689e-06) A[3]:(1.42509174016e-17)\n",
      " state (7)  A[0]:(0.00301061104983) A[1]:(4.43794306859e-11) A[2]:(0.996989369392) A[3]:(1.04991215805e-26)\n",
      " state (8)  A[0]:(4.81091930737e-08) A[1]:(9.28214482273e-14) A[2]:(0.999999940395) A[3]:(9.35521146395e-33)\n",
      " state (9)  A[0]:(2.94268387435e-09) A[1]:(4.47083132505e-14) A[2]:(1.0) A[3]:(1.16893380665e-33)\n",
      " state (10)  A[0]:(1.44346368192e-09) A[1]:(4.32541711324e-14) A[2]:(1.0) A[3]:(8.01094349133e-34)\n",
      " state (11)  A[0]:(1.1489609264e-09) A[1]:(4.39474777641e-14) A[2]:(1.0) A[3]:(7.33058802406e-34)\n",
      " state (12)  A[0]:(1.05498554337e-09) A[1]:(4.44142335757e-14) A[2]:(1.0) A[3]:(7.14576725133e-34)\n",
      " state (13)  A[0]:(1.02030994764e-09) A[1]:(4.46299288216e-14) A[2]:(1.0) A[3]:(7.08442665003e-34)\n",
      " state (14)  A[0]:(1.00687780336e-09) A[1]:(4.47199108257e-14) A[2]:(1.0) A[3]:(7.06170838499e-34)\n",
      " state (15)  A[0]:(1.001564498e-09) A[1]:(4.47561773883e-14) A[2]:(1.0) A[3]:(7.05287794286e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 649000 finished after 17 . Running score: 0.11. Policy_loss: -92050.61119, Value_loss: 1.21755103071. Times trained:               16026. Times reached goal: 85.               Steps done: 8048936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8264e-11,  1.6293e-09,  1.6637e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8264e-11,  1.6293e-09,  1.6638e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.0841e-11,  5.2579e-07,  1.7631e-17]])\n",
      "On state=8, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8264e-11,  1.6293e-09,  1.6639e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8265e-11,  1.6293e-09,  1.6647e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8265e-11,  1.6293e-09,  1.6649e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8265e-11,  1.6293e-09,  1.6655e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8266e-11,  1.6293e-09,  1.6664e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8266e-11,  1.6293e-09,  1.6665e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8266e-11,  1.6293e-09,  1.6666e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.0843e-11,  5.2580e-07,  1.7673e-17]])\n",
      "On state=8, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8266e-11,  1.6293e-09,  1.6668e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.0843e-11,  5.2581e-07,  1.7675e-17]])\n",
      "On state=8, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8266e-11,  1.6293e-09,  1.6669e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8266e-11,  1.6293e-09,  1.6670e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8266e-11,  1.6293e-09,  1.6670e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0023,  0.0010,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8266e-11,  1.6293e-09,  1.6674e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.8266e-11,  1.6293e-09,  1.6675e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.0844e-11,  5.2581e-07,  1.7686e-17]])\n",
      "On state=8, selected action=0\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994631052017) A[1]:(0.00234296685085) A[2]:(0.000998797710054) A[3]:(0.00202716211788)\n",
      " state (1)  A[0]:(0.000750972074457) A[1]:(0.000258603744442) A[2]:(0.00100039271638) A[3]:(0.997990012169)\n",
      " state (2)  A[0]:(0.999999821186) A[1]:(1.43188145785e-08) A[2]:(1.82021608452e-07) A[3]:(5.97248916989e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.0680338558e-10) A[2]:(2.00450966936e-09) A[3]:(3.19224442887e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.82662545884e-11) A[2]:(1.62934399306e-09) A[3]:(1.66764627281e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.96208082904e-11) A[2]:(1.57098267728e-09) A[3]:(6.97722207701e-13)\n",
      " state (6)  A[0]:(1.0) A[1]:(4.24204560368e-11) A[2]:(2.05880401616e-09) A[3]:(1.1753240796e-13)\n",
      " state (7)  A[0]:(1.0) A[1]:(4.2795423455e-11) A[2]:(9.74731673153e-09) A[3]:(4.25757595751e-15)\n",
      " state (8)  A[0]:(0.999999463558) A[1]:(8.08438871402e-11) A[2]:(5.25809980445e-07) A[3]:(1.76867526966e-17)\n",
      " state (9)  A[0]:(0.996907114983) A[1]:(3.96398275315e-10) A[2]:(0.00309290434234) A[3]:(1.6365222149e-21)\n",
      " state (10)  A[0]:(0.0105136279017) A[1]:(4.28612388637e-11) A[2]:(0.989486396313) A[3]:(2.85032990278e-27)\n",
      " state (11)  A[0]:(2.00839986064e-05) A[1]:(9.42455532019e-13) A[2]:(0.999979913235) A[3]:(5.53747505946e-31)\n",
      " state (12)  A[0]:(4.08418998177e-07) A[1]:(1.74083322627e-13) A[2]:(0.999999582767) A[3]:(1.62026262186e-32)\n",
      " state (13)  A[0]:(5.42468363562e-08) A[1]:(9.17941260113e-14) A[2]:(0.999999940395) A[3]:(3.70266834982e-33)\n",
      " state (14)  A[0]:(2.06335748487e-08) A[1]:(7.67246712669e-14) A[2]:(1.0) A[3]:(2.04293663169e-33)\n",
      " state (15)  A[0]:(1.27365371583e-08) A[1]:(7.40586452298e-14) A[2]:(1.0) A[3]:(1.58861467725e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 650000 finished after 49 . Running score: 0.0. Policy_loss: -92050.6111852, Value_loss: 0.978211138067. Times trained:               16915. Times reached goal: 93.               Steps done: 8065851.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996163725853) A[1]:(0.00206194166094) A[2]:(0.000776793574914) A[3]:(0.000997521681711)\n",
      " state (1)  A[0]:(0.000746849342249) A[1]:(0.000247649179073) A[2]:(0.000935978430789) A[3]:(0.998069524765)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.71839723007e-10) A[2]:(2.89120505315e-09) A[3]:(7.03418876966e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.87961215773e-11) A[2]:(1.47997358813e-09) A[3]:(1.19201762176e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.3118900811e-11) A[2]:(1.96474925218e-09) A[3]:(1.21226745547e-13)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.00344488064e-11) A[2]:(2.82699641474e-08) A[3]:(6.60516027571e-16)\n",
      " state (6)  A[0]:(0.999967217445) A[1]:(1.65927119267e-10) A[2]:(3.27563902829e-05) A[3]:(9.54654056332e-20)\n",
      " state (7)  A[0]:(0.271998107433) A[1]:(3.80472264805e-10) A[2]:(0.728001892567) A[3]:(3.59805745107e-25)\n",
      " state (8)  A[0]:(0.000426403945312) A[1]:(3.98522404313e-12) A[2]:(0.999573588371) A[3]:(5.62572073693e-30)\n",
      " state (9)  A[0]:(8.61860098667e-06) A[1]:(5.83832732593e-13) A[2]:(0.999991357327) A[3]:(9.53597516487e-32)\n",
      " state (10)  A[0]:(4.18172959371e-07) A[1]:(1.90502222829e-13) A[2]:(0.999999582767) A[3]:(1.0398577311e-32)\n",
      " state (11)  A[0]:(6.27936245223e-08) A[1]:(1.11101205276e-13) A[2]:(0.999999940395) A[3]:(3.18061992102e-33)\n",
      " state (12)  A[0]:(2.33115251547e-08) A[1]:(9.41839650975e-14) A[2]:(1.0) A[3]:(1.86671147773e-33)\n",
      " state (13)  A[0]:(1.42638638678e-08) A[1]:(9.17512525916e-14) A[2]:(1.0) A[3]:(1.49158000631e-33)\n",
      " state (14)  A[0]:(1.12410267761e-08) A[1]:(9.22033377925e-14) A[2]:(1.0) A[3]:(1.35538704716e-33)\n",
      " state (15)  A[0]:(1.00281134507e-08) A[1]:(9.28499627444e-14) A[2]:(1.0) A[3]:(1.29919382541e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 651000 finished after 7 . Running score: 0.08. Policy_loss: -92050.6111958, Value_loss: 1.22934066488. Times trained:               16915. Times reached goal: 42.               Steps done: 8082766.\n",
      " state (0)  A[0]:(0.996802389622) A[1]:(0.00177720317151) A[2]:(0.00063092133496) A[3]:(0.000789494486526)\n",
      " state (1)  A[0]:(0.000715725240298) A[1]:(0.000226834090427) A[2]:(0.000863036781084) A[3]:(0.998194396496)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.38244166115e-10) A[2]:(2.37601405217e-09) A[3]:(6.38726918578e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.32526688077e-11) A[2]:(1.09838482754e-09) A[3]:(5.94855816081e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.92957637316e-11) A[2]:(2.40959940889e-09) A[3]:(2.62226729859e-14)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(4.60696272797e-11) A[2]:(1.12951369147e-07) A[3]:(4.65754797246e-17)\n",
      " state (6)  A[0]:(0.999775350094) A[1]:(1.53344961729e-10) A[2]:(0.000224631716264) A[3]:(4.35944458434e-21)\n",
      " state (7)  A[0]:(0.361631333828) A[1]:(2.62451893551e-10) A[2]:(0.638368666172) A[3]:(1.82384863154e-25)\n",
      " state (8)  A[0]:(0.00532736442983) A[1]:(1.0955580193e-11) A[2]:(0.994672656059) A[3]:(5.14897250261e-29)\n",
      " state (9)  A[0]:(0.00058165518567) A[1]:(2.71857081117e-12) A[2]:(0.999418318272) A[3]:(2.04319274693e-30)\n",
      " state (10)  A[0]:(0.000105372680991) A[1]:(1.18506886162e-12) A[2]:(0.999894618988) A[3]:(3.5448919993e-31)\n",
      " state (11)  A[0]:(1.79086964636e-05) A[1]:(5.72873725454e-13) A[2]:(0.999982118607) A[3]:(8.69419014206e-32)\n",
      " state (12)  A[0]:(2.67502105089e-06) A[1]:(2.85185070004e-13) A[2]:(0.999997317791) A[3]:(2.3734344724e-32)\n",
      " state (13)  A[0]:(5.19944364896e-07) A[1]:(1.67320408288e-13) A[2]:(0.999999463558) A[3]:(8.50243072005e-33)\n",
      " state (14)  A[0]:(1.77100744736e-07) A[1]:(1.26070269675e-13) A[2]:(0.999999821186) A[3]:(4.53632673937e-33)\n",
      " state (15)  A[0]:(9.54803880404e-08) A[1]:(1.1286391472e-13) A[2]:(0.999999880791) A[3]:(3.26430409854e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 652000 finished after 25 . Running score: 0.14. Policy_loss: -92050.6113633, Value_loss: 1.19019238658. Times trained:               16865. Times reached goal: 77.               Steps done: 8099631.\n",
      " state (0)  A[0]:(0.991752386093) A[1]:(0.00179312145337) A[2]:(0.0007768063806) A[3]:(0.00567765999585)\n",
      " state (1)  A[0]:(0.000658908102196) A[1]:(0.000200463502551) A[2]:(0.000755837187171) A[3]:(0.998384773731)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.87565588061e-10) A[2]:(3.03092195786e-09) A[3]:(1.35128765816e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.48247680693e-11) A[2]:(7.80694953129e-10) A[3]:(7.57445430162e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.34186246784e-11) A[2]:(1.1027468938e-09) A[3]:(8.87661865128e-14)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.91178886558e-11) A[2]:(1.31842705642e-08) A[3]:(9.88911601194e-16)\n",
      " state (6)  A[0]:(0.999997019768) A[1]:(6.40891367754e-11) A[2]:(2.96369535135e-06) A[3]:(6.22431438961e-19)\n",
      " state (7)  A[0]:(0.992036998272) A[1]:(2.23960308632e-10) A[2]:(0.00796299707144) A[3]:(9.16840712883e-23)\n",
      " state (8)  A[0]:(0.188744753599) A[1]:(1.29522378933e-10) A[2]:(0.811255216599) A[3]:(3.55039946718e-26)\n",
      " state (9)  A[0]:(0.00952313374728) A[1]:(1.36182410834e-11) A[2]:(0.990476846695) A[3]:(1.17467896023e-28)\n",
      " state (10)  A[0]:(0.00160145829432) A[1]:(4.28276338005e-12) A[2]:(0.998398542404) A[3]:(7.50129024638e-30)\n",
      " state (11)  A[0]:(0.000386900326703) A[1]:(2.03170921927e-12) A[2]:(0.999613106251) A[3]:(1.43727553216e-30)\n",
      " state (12)  A[0]:(8.91653980943e-05) A[1]:(1.05873990131e-12) A[2]:(0.999910831451) A[3]:(3.81436302958e-31)\n",
      " state (13)  A[0]:(1.67373291333e-05) A[1]:(5.45812256069e-13) A[2]:(0.999983251095) A[3]:(1.07661717331e-31)\n",
      " state (14)  A[0]:(3.05393177769e-06) A[1]:(2.94789339423e-13) A[2]:(0.999996960163) A[3]:(3.39869741258e-32)\n",
      " state (15)  A[0]:(7.88110298799e-07) A[1]:(1.89925305301e-13) A[2]:(0.99999922514) A[3]:(1.43770849611e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 653000 finished after 10 . Running score: 0.09. Policy_loss: -92050.6121143, Value_loss: 1.62182020042. Times trained:               15747. Times reached goal: 101.               Steps done: 8115378.\n",
      " state (0)  A[0]:(0.994214653969) A[1]:(0.00169883202761) A[2]:(0.000892659765668) A[3]:(0.00319383642636)\n",
      " state (1)  A[0]:(0.000608977745287) A[1]:(0.000183541793376) A[2]:(0.000722486933228) A[3]:(0.998484969139)\n",
      " state (2)  A[0]:(0.999999642372) A[1]:(2.60496140214e-08) A[2]:(3.18911730801e-07) A[3]:(2.70414215464e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.56908218258e-11) A[2]:(8.99212149186e-10) A[3]:(8.1320952576e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.45556665279e-11) A[2]:(1.37341193973e-09) A[3]:(8.9054645277e-14)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.97099775337e-11) A[2]:(1.68559601832e-08) A[3]:(9.93270750378e-16)\n",
      " state (6)  A[0]:(0.999718427658) A[1]:(1.23960355869e-10) A[2]:(0.000281560671283) A[3]:(4.31923787124e-21)\n",
      " state (7)  A[0]:(0.0996692627668) A[1]:(7.2313717514e-11) A[2]:(0.900330722332) A[3]:(1.00594263816e-26)\n",
      " state (8)  A[0]:(0.00265704700723) A[1]:(4.96231875036e-12) A[2]:(0.997342944145) A[3]:(1.44876440017e-29)\n",
      " state (9)  A[0]:(0.000358834950021) A[1]:(1.62242864542e-12) A[2]:(0.999641180038) A[3]:(1.20252686527e-30)\n",
      " state (10)  A[0]:(5.149325807e-05) A[1]:(6.93138092495e-13) A[2]:(0.999948501587) A[3]:(2.25170762427e-31)\n",
      " state (11)  A[0]:(6.01617193752e-06) A[1]:(3.04395587512e-13) A[2]:(0.999993979931) A[3]:(4.98114673216e-32)\n",
      " state (12)  A[0]:(9.00774864476e-07) A[1]:(1.58587146236e-13) A[2]:(0.99999910593) A[3]:(1.49135761748e-32)\n",
      " state (13)  A[0]:(2.67464884018e-07) A[1]:(1.12242409377e-13) A[2]:(0.999999761581) A[3]:(7.25317850682e-33)\n",
      " state (14)  A[0]:(1.3833299306e-07) A[1]:(9.82752494292e-14) A[2]:(0.999999880791) A[3]:(5.05504300903e-33)\n",
      " state (15)  A[0]:(9.78371588189e-08) A[1]:(9.4100630608e-14) A[2]:(0.999999880791) A[3]:(4.24765691822e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 654000 finished after 29 . Running score: 0.13. Policy_loss: -92050.6113075, Value_loss: 1.40709072001. Times trained:               16171. Times reached goal: 135.               Steps done: 8131549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4962e-11,  1.9623e-08,  1.8132e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4962e-11,  1.9622e-08,  1.8133e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4962e-11,  1.9621e-08,  1.8136e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4963e-11,  1.9621e-08,  1.8137e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4963e-11,  1.9621e-08,  1.8137e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4963e-11,  1.9621e-08,  1.8138e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.3784e-06,  4.7468e-13,  9.9999e-01,  6.2171e-32]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4963e-11,  1.9621e-08,  1.8138e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4963e-11,  1.9620e-08,  1.8139e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4964e-11,  1.9620e-08,  1.8141e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4964e-11,  1.9620e-08,  1.8142e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4964e-11,  1.9620e-08,  1.8142e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4964e-11,  1.9619e-08,  1.8142e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4965e-11,  1.9620e-08,  1.8143e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4965e-11,  1.9620e-08,  1.8144e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4966e-11,  1.9620e-08,  1.8144e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4967e-11,  1.9620e-08,  1.8145e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4968e-11,  1.9620e-08,  1.8146e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4968e-11,  1.9620e-08,  1.8146e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4968e-11,  1.9620e-08,  1.8146e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4970e-11,  1.9621e-08,  1.8147e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4970e-11,  1.9621e-08,  1.8148e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0019,  0.0008,  0.0025]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4970e-11,  1.9621e-08,  1.8148e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4971e-11,  1.9621e-08,  1.8148e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4971e-11,  1.9621e-08,  1.8148e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.4971e-11,  1.9621e-08,  1.8148e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.3814e-06,  4.7476e-13,  9.9999e-01,  6.2202e-32]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.1286e-07,  1.8794e-13,  1.0000e+00,  1.1476e-32]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994718432426) A[1]:(0.00192902458366) A[2]:(0.000834190170281) A[3]:(0.00251834676601)\n",
      " state (1)  A[0]:(0.000586057140026) A[1]:(0.000191520506633) A[2]:(0.000733136199415) A[3]:(0.998489260674)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.55740129126e-10) A[2]:(2.93648838579e-09) A[3]:(7.97211862497e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.5875234167e-11) A[2]:(1.86285031845e-09) A[3]:(2.86855744236e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.49716271167e-11) A[2]:(1.96219254178e-08) A[3]:(1.81483924026e-15)\n",
      " state (5)  A[0]:(0.996561527252) A[1]:(2.92455393236e-10) A[2]:(0.00343845505267) A[3]:(5.68118741766e-22)\n",
      " state (6)  A[0]:(0.0034316254314) A[1]:(9.13279001036e-12) A[2]:(0.996568381786) A[3]:(3.52443404356e-29)\n",
      " state (7)  A[0]:(0.000139767682413) A[1]:(1.40243784467e-12) A[2]:(0.999860227108) A[3]:(4.95318007586e-31)\n",
      " state (8)  A[0]:(9.38076846069e-06) A[1]:(4.74751910961e-13) A[2]:(0.999990642071) A[3]:(6.22010367199e-32)\n",
      " state (9)  A[0]:(6.12842882219e-07) A[1]:(1.87940131123e-13) A[2]:(0.999999403954) A[3]:(1.14760648203e-32)\n",
      " state (10)  A[0]:(9.15972933058e-08) A[1]:(1.11928587058e-13) A[2]:(0.999999880791) A[3]:(3.99341108179e-33)\n",
      " state (11)  A[0]:(3.56759670694e-08) A[1]:(9.55435071879e-14) A[2]:(0.999999940395) A[3]:(2.51628873764e-33)\n",
      " state (12)  A[0]:(2.34268817678e-08) A[1]:(9.26166627657e-14) A[2]:(1.0) A[3]:(2.09847984179e-33)\n",
      " state (13)  A[0]:(1.95199802988e-08) A[1]:(9.22742378383e-14) A[2]:(1.0) A[3]:(1.95179449353e-33)\n",
      " state (14)  A[0]:(1.80483485934e-08) A[1]:(9.23119138638e-14) A[2]:(1.0) A[3]:(1.89445938922e-33)\n",
      " state (15)  A[0]:(1.74524679153e-08) A[1]:(9.2363515111e-14) A[2]:(1.0) A[3]:(1.87090321712e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 655000 finished after 64 . Running score: 0.1. Policy_loss: -92050.6112809, Value_loss: 1.19452052965. Times trained:               16076. Times reached goal: 119.               Steps done: 8147625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995632767677) A[1]:(0.00188465043902) A[2]:(0.000688433996402) A[3]:(0.00179415254388)\n",
      " state (1)  A[0]:(0.000565447553527) A[1]:(0.000186053424841) A[2]:(0.000718058610801) A[3]:(0.998530447483)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.76504630356e-10) A[2]:(3.67987418315e-09) A[3]:(8.49268832342e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.6200866877e-11) A[2]:(4.35960245682e-09) A[3]:(9.42001332624e-14)\n",
      " state (4)  A[0]:(0.99999910593) A[1]:(9.3992903738e-11) A[2]:(8.98592077192e-07) A[3]:(1.31407665733e-17)\n",
      " state (5)  A[0]:(0.0230416879058) A[1]:(4.85497614333e-11) A[2]:(0.976958334446) A[3]:(2.49036146282e-27)\n",
      " state (6)  A[0]:(8.54152167449e-05) A[1]:(1.10681049829e-12) A[2]:(0.999914586544) A[3]:(4.01752042684e-31)\n",
      " state (7)  A[0]:(4.55746112493e-06) A[1]:(3.38548769097e-13) A[2]:(0.999995470047) A[3]:(4.31556772479e-32)\n",
      " state (8)  A[0]:(2.77681635907e-07) A[1]:(1.33119399835e-13) A[2]:(0.999999701977) A[3]:(8.24354425233e-33)\n",
      " state (9)  A[0]:(3.56324427742e-08) A[1]:(7.75158813548e-14) A[2]:(0.999999940395) A[3]:(2.81098057961e-33)\n",
      " state (10)  A[0]:(1.30297745926e-08) A[1]:(6.66065510038e-14) A[2]:(1.0) A[3]:(1.79154577716e-33)\n",
      " state (11)  A[0]:(8.56017567941e-09) A[1]:(6.51684381372e-14) A[2]:(1.0) A[3]:(1.52674216475e-33)\n",
      " state (12)  A[0]:(7.23315318751e-09) A[1]:(6.51847214986e-14) A[2]:(1.0) A[3]:(1.4411627781e-33)\n",
      " state (13)  A[0]:(6.76465905514e-09) A[1]:(6.52963401122e-14) A[2]:(1.0) A[3]:(1.41020769557e-33)\n",
      " state (14)  A[0]:(6.58658949604e-09) A[1]:(6.53591425231e-14) A[2]:(1.0) A[3]:(1.39835834518e-33)\n",
      " state (15)  A[0]:(6.51613918379e-09) A[1]:(6.53880703923e-14) A[2]:(1.0) A[3]:(1.39369328564e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 656000 finished after 6 . Running score: 0.12. Policy_loss: -92050.6112506, Value_loss: 1.40716172663. Times trained:               16174. Times reached goal: 119.               Steps done: 8163799.\n",
      " state (0)  A[0]:(0.995260000229) A[1]:(0.00198395876214) A[2]:(0.000624696607701) A[3]:(0.00213132658973)\n",
      " state (1)  A[0]:(0.000604439876042) A[1]:(0.000194526321138) A[2]:(0.000742089585401) A[3]:(0.998458921909)\n",
      " state (2)  A[0]:(0.999996006489) A[1]:(2.59947341874e-07) A[2]:(3.13956888931e-06) A[3]:(6.14111513642e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.99927972683e-11) A[2]:(2.7167921246e-09) A[3]:(8.58407380008e-14)\n",
      " state (4)  A[0]:(0.999999761581) A[1]:(6.52892045983e-11) A[2]:(2.65495600615e-07) A[3]:(2.67115826927e-17)\n",
      " state (5)  A[0]:(0.0243675261736) A[1]:(5.88698326198e-11) A[2]:(0.975632488728) A[3]:(5.88946218469e-27)\n",
      " state (6)  A[0]:(7.6722490121e-06) A[1]:(3.48668657965e-13) A[2]:(0.999992311001) A[3]:(6.16686024312e-32)\n",
      " state (7)  A[0]:(1.78904898007e-07) A[1]:(8.5399630347e-14) A[2]:(0.999999821186) A[3]:(4.52091711046e-33)\n",
      " state (8)  A[0]:(2.17886597653e-08) A[1]:(4.96638456271e-14) A[2]:(1.0) A[3]:(1.44543524089e-33)\n",
      " state (9)  A[0]:(8.85427020592e-09) A[1]:(4.52882394401e-14) A[2]:(1.0) A[3]:(9.93227523243e-34)\n",
      " state (10)  A[0]:(6.25149176869e-09) A[1]:(4.53483144048e-14) A[2]:(1.0) A[3]:(8.85520465626e-34)\n",
      " state (11)  A[0]:(5.45262723861e-09) A[1]:(4.56866430809e-14) A[2]:(1.0) A[3]:(8.51804624415e-34)\n",
      " state (12)  A[0]:(5.08941111477e-09) A[1]:(4.59674819367e-14) A[2]:(1.0) A[3]:(8.37511623299e-34)\n",
      " state (13)  A[0]:(4.79419082211e-09) A[1]:(4.62947991845e-14) A[2]:(1.0) A[3]:(8.27091508727e-34)\n",
      " state (14)  A[0]:(4.42700631709e-09) A[1]:(4.67934474805e-14) A[2]:(1.0) A[3]:(8.14535851528e-34)\n",
      " state (15)  A[0]:(3.93220700445e-09) A[1]:(4.75962009334e-14) A[2]:(1.0) A[3]:(7.9681527419e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 657000 finished after 3 . Running score: 0.12. Policy_loss: -92050.6112552, Value_loss: 0.996395005608. Times trained:               15518. Times reached goal: 107.               Steps done: 8179317.\n",
      " state (0)  A[0]:(0.996347725391) A[1]:(0.00170815270394) A[2]:(0.00120156386402) A[3]:(0.000742572767194)\n",
      " state (1)  A[0]:(0.00057610013755) A[1]:(0.000178270260221) A[2]:(0.000922699982766) A[3]:(0.99832290411)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(4.54327508947e-09) A[2]:(9.89432393794e-08) A[3]:(1.68267511036e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.37555894295e-11) A[2]:(2.07975370259e-09) A[3]:(7.69906328361e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.1446468613e-11) A[2]:(1.74020424737e-08) A[3]:(2.32333310847e-14)\n",
      " state (5)  A[0]:(0.0867284089327) A[1]:(3.57809198936e-10) A[2]:(0.913271605968) A[3]:(9.99888297116e-24)\n",
      " state (6)  A[0]:(6.51477520819e-08) A[1]:(5.45394451986e-14) A[2]:(0.999999940395) A[3]:(8.98982079994e-33)\n",
      " state (7)  A[0]:(2.71411448871e-09) A[1]:(2.10234610889e-14) A[2]:(1.0) A[3]:(7.08783282858e-34)\n",
      " state (8)  A[0]:(1.14607623392e-09) A[1]:(2.00461934378e-14) A[2]:(1.0) A[3]:(4.60753331986e-34)\n",
      " state (9)  A[0]:(8.86690665336e-10) A[1]:(2.02607604359e-14) A[2]:(1.0) A[3]:(4.18213660798e-34)\n",
      " state (10)  A[0]:(8.05787270242e-10) A[1]:(2.04317458948e-14) A[2]:(1.0) A[3]:(4.05996509238e-34)\n",
      " state (11)  A[0]:(7.53188011959e-10) A[1]:(2.06163584197e-14) A[2]:(1.0) A[3]:(3.99372919995e-34)\n",
      " state (12)  A[0]:(6.89110712937e-10) A[1]:(2.09047833026e-14) A[2]:(1.0) A[3]:(3.91875516006e-34)\n",
      " state (13)  A[0]:(6.03568917068e-10) A[1]:(2.1363333059e-14) A[2]:(1.0) A[3]:(3.81229715703e-34)\n",
      " state (14)  A[0]:(5.086963073e-10) A[1]:(2.20030851856e-14) A[2]:(1.0) A[3]:(3.68189144125e-34)\n",
      " state (15)  A[0]:(4.24655671738e-10) A[1]:(2.28237110336e-14) A[2]:(1.0) A[3]:(3.57171685569e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 658000 finished after 11 . Running score: 0.13. Policy_loss: -92050.6112507, Value_loss: 1.20637708461. Times trained:               16005. Times reached goal: 128.               Steps done: 8195322.\n",
      " state (0)  A[0]:(0.997153639793) A[1]:(0.00175810942892) A[2]:(0.00079363695113) A[3]:(0.000294636760373)\n",
      " state (1)  A[0]:(0.000551003671717) A[1]:(0.00017243270122) A[2]:(0.000839446263853) A[3]:(0.998437106609)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(1.60608848443e-09) A[2]:(3.32655218926e-08) A[3]:(3.3613856143e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.41707936788e-11) A[2]:(1.96681160247e-09) A[3]:(2.22463020227e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.94019408714e-11) A[2]:(3.52880347165e-09) A[3]:(3.18571015921e-13)\n",
      " state (5)  A[0]:(0.987541735172) A[1]:(1.59865654048e-09) A[2]:(0.0124582806602) A[3]:(1.90692673723e-19)\n",
      " state (6)  A[0]:(1.81993424775e-08) A[1]:(7.7672324952e-14) A[2]:(1.0) A[3]:(1.2539640354e-32)\n",
      " state (7)  A[0]:(7.03351210607e-10) A[1]:(3.12319174158e-14) A[2]:(1.0) A[3]:(8.09042252148e-34)\n",
      " state (8)  A[0]:(3.92002902361e-10) A[1]:(3.01505816028e-14) A[2]:(1.0) A[3]:(5.88072869998e-34)\n",
      " state (9)  A[0]:(3.34413913228e-10) A[1]:(3.02396183181e-14) A[2]:(1.0) A[3]:(5.49295469499e-34)\n",
      " state (10)  A[0]:(3.06405206763e-10) A[1]:(3.04658642064e-14) A[2]:(1.0) A[3]:(5.33696797217e-34)\n",
      " state (11)  A[0]:(2.74928219079e-10) A[1]:(3.08804224596e-14) A[2]:(1.0) A[3]:(5.17940121963e-34)\n",
      " state (12)  A[0]:(2.34394614829e-10) A[1]:(3.15783200099e-14) A[2]:(1.0) A[3]:(4.96466732992e-34)\n",
      " state (13)  A[0]:(1.92692306555e-10) A[1]:(3.26418647429e-14) A[2]:(1.0) A[3]:(4.73385028976e-34)\n",
      " state (14)  A[0]:(1.57976187687e-10) A[1]:(3.43408298759e-14) A[2]:(1.0) A[3]:(4.59044594827e-34)\n",
      " state (15)  A[0]:(1.33933600321e-10) A[1]:(3.69045038398e-14) A[2]:(1.0) A[3]:(4.54999929987e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 659000 finished after 3 . Running score: 0.09. Policy_loss: -92050.6111782, Value_loss: 1.20985418011. Times trained:               15809. Times reached goal: 101.               Steps done: 8211131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9954,  0.0015,  0.0011,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.2120e-11,  2.2294e-09,  1.7656e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4246e-10,  2.4338e-14,  1.0000e+00,  6.8410e-34]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8625e-10,  2.4037e-14,  1.0000e+00,  5.9200e-34]])\n",
      "On state=9, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5991e-10,  2.4227e-14,  1.0000e+00,  5.5743e-34]])\n",
      "On state=10, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.3699e-11,  2.8824e-14,  1.0000e+00,  4.9272e-34]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.3721e-11,  2.8831e-14,  1.0000e+00,  4.9281e-34]])\n",
      "On state=14, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6004e-10,  2.4242e-14,  1.0000e+00,  5.5775e-34]])\n",
      "On state=10, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.3759e-11,  2.8842e-14,  1.0000e+00,  4.9299e-34]])\n",
      "On state=14, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6010e-10,  2.4249e-14,  1.0000e+00,  5.5791e-34]])\n",
      "On state=10, selected action=2\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.0810e-06,  8.6208e-13,  1.0000e+00,  7.7594e-30]])\n",
      "On state=6, selected action=2\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995444953442) A[1]:(0.00150238827337) A[2]:(0.00106767588295) A[3]:(0.00198495388031)\n",
      " state (1)  A[0]:(0.000422274693847) A[1]:(0.000127050821902) A[2]:(0.000675350544043) A[3]:(0.998775303364)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(2.17829954074e-09) A[2]:(4.86065196981e-08) A[3]:(7.43448191898e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.56227858112e-11) A[2]:(2.38128139429e-09) A[3]:(4.52055233524e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.20759442513e-11) A[2]:(2.22264451288e-09) A[3]:(1.76419631941e-12)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(1.00972341599e-10) A[2]:(9.69986047039e-08) A[3]:(1.58367446219e-14)\n",
      " state (6)  A[0]:(2.09311588151e-06) A[1]:(8.64561243777e-13) A[2]:(0.999997913837) A[3]:(7.80914713873e-30)\n",
      " state (7)  A[0]:(6.98647473207e-10) A[1]:(2.92362163125e-14) A[2]:(1.0) A[3]:(1.46599252487e-33)\n",
      " state (8)  A[0]:(2.43243258868e-10) A[1]:(2.4385502569e-14) A[2]:(1.0) A[3]:(6.85494627552e-34)\n",
      " state (9)  A[0]:(1.86764506638e-10) A[1]:(2.40768217402e-14) A[2]:(1.0) A[3]:(5.92997272973e-34)\n",
      " state (10)  A[0]:(1.60298760377e-10) A[1]:(2.4261122557e-14) A[2]:(1.0) A[3]:(5.58230879604e-34)\n",
      " state (11)  A[0]:(1.34492181281e-10) A[1]:(2.47272244591e-14) A[2]:(1.0) A[3]:(5.29769819565e-34)\n",
      " state (12)  A[0]:(1.08606616755e-10) A[1]:(2.55084039828e-14) A[2]:(1.0) A[3]:(5.04823854978e-34)\n",
      " state (13)  A[0]:(8.71402949798e-11) A[1]:(2.68440140044e-14) A[2]:(1.0) A[3]:(4.93656612727e-34)\n",
      " state (14)  A[0]:(7.38647823462e-11) A[1]:(2.88603505243e-14) A[2]:(1.0) A[3]:(4.93310255153e-34)\n",
      " state (15)  A[0]:(6.87107523656e-11) A[1]:(3.09817174357e-14) A[2]:(1.0) A[3]:(4.92081082955e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 660000 finished after 11 . Running score: 0.12. Policy_loss: -92050.6111799, Value_loss: 1.21786571194. Times trained:               15462. Times reached goal: 114.               Steps done: 8226593.\n",
      " state (0)  A[0]:(0.996805071831) A[1]:(0.00153100909665) A[2]:(0.000685944221914) A[3]:(0.000977968331426)\n",
      " state (1)  A[0]:(0.000421579316026) A[1]:(0.000122142315377) A[2]:(0.000637291057501) A[3]:(0.998818993568)\n",
      " state (2)  A[0]:(0.999996006489) A[1]:(1.91417797168e-07) A[2]:(3.06483229906e-06) A[3]:(7.65942104408e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.41905070764e-11) A[2]:(2.26615792798e-09) A[3]:(3.43811697862e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.78245013036e-11) A[2]:(4.60367433064e-09) A[3]:(3.53784709861e-13)\n",
      " state (5)  A[0]:(0.310891717672) A[1]:(1.72052594394e-09) A[2]:(0.68910831213) A[3]:(1.18445781174e-21)\n",
      " state (6)  A[0]:(1.86610060737e-09) A[1]:(2.3707613631e-14) A[2]:(1.0) A[3]:(1.92720186601e-33)\n",
      " state (7)  A[0]:(2.69589017776e-10) A[1]:(1.65986660045e-14) A[2]:(1.0) A[3]:(5.07600684875e-34)\n",
      " state (8)  A[0]:(1.919396031e-10) A[1]:(1.65593281004e-14) A[2]:(1.0) A[3]:(4.35813336706e-34)\n",
      " state (9)  A[0]:(1.72994674141e-10) A[1]:(1.66609940769e-14) A[2]:(1.0) A[3]:(4.20305581565e-34)\n",
      " state (10)  A[0]:(1.60120916526e-10) A[1]:(1.68235329232e-14) A[2]:(1.0) A[3]:(4.12157293497e-34)\n",
      " state (11)  A[0]:(1.43215883597e-10) A[1]:(1.711935918e-14) A[2]:(1.0) A[3]:(4.02664947026e-34)\n",
      " state (12)  A[0]:(1.21596760438e-10) A[1]:(1.76013429499e-14) A[2]:(1.0) A[3]:(3.90502621257e-34)\n",
      " state (13)  A[0]:(1.00165375994e-10) A[1]:(1.82971601873e-14) A[2]:(1.0) A[3]:(3.80305896529e-34)\n",
      " state (14)  A[0]:(8.25666826576e-11) A[1]:(1.93430595299e-14) A[2]:(1.0) A[3]:(3.78332213964e-34)\n",
      " state (15)  A[0]:(7.11134207076e-11) A[1]:(2.08002204734e-14) A[2]:(1.0) A[3]:(3.81613817666e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 661000 finished after 4 . Running score: 0.09. Policy_loss: -92050.6114679, Value_loss: 1.19330596168. Times trained:               15401. Times reached goal: 107.               Steps done: 8241994.\n",
      " state (0)  A[0]:(0.997368216515) A[1]:(0.00142161617987) A[2]:(0.000732315878849) A[3]:(0.000477833091281)\n",
      " state (1)  A[0]:(0.000429513369454) A[1]:(0.000121003504319) A[2]:(0.000625181710348) A[3]:(0.998824298382)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.51523399067e-10) A[2]:(1.68445186688e-08) A[3]:(1.6785714485e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.1028737619e-11) A[2]:(1.96589255985e-09) A[3]:(3.23041571791e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.49273881986e-11) A[2]:(2.8762607851e-09) A[3]:(6.14379207231e-13)\n",
      " state (5)  A[0]:(0.999996960163) A[1]:(2.0143678392e-10) A[2]:(3.02929356621e-06) A[3]:(2.36159482549e-16)\n",
      " state (6)  A[0]:(1.77367127208e-07) A[1]:(1.98831429836e-13) A[2]:(0.999999821186) A[3]:(2.73315907731e-31)\n",
      " state (7)  A[0]:(3.69854757931e-10) A[1]:(1.94868874184e-14) A[2]:(1.0) A[3]:(9.35188959039e-34)\n",
      " state (8)  A[0]:(1.47883844059e-10) A[1]:(1.77291093996e-14) A[2]:(1.0) A[3]:(5.66144252472e-34)\n",
      " state (9)  A[0]:(1.1880035844e-10) A[1]:(1.77165682298e-14) A[2]:(1.0) A[3]:(5.18343325709e-34)\n",
      " state (10)  A[0]:(1.09365981549e-10) A[1]:(1.77941547537e-14) A[2]:(1.0) A[3]:(5.04873951241e-34)\n",
      " state (11)  A[0]:(1.02242901645e-10) A[1]:(1.79245165124e-14) A[2]:(1.0) A[3]:(4.96656143703e-34)\n",
      " state (12)  A[0]:(9.29906499025e-11) A[1]:(1.81490242892e-14) A[2]:(1.0) A[3]:(4.86834475142e-34)\n",
      " state (13)  A[0]:(8.09122699397e-11) A[1]:(1.8503028157e-14) A[2]:(1.0) A[3]:(4.73977689351e-34)\n",
      " state (14)  A[0]:(6.79088243971e-11) A[1]:(1.90097503711e-14) A[2]:(1.0) A[3]:(4.61361879902e-34)\n",
      " state (15)  A[0]:(5.63171870249e-11) A[1]:(1.97558186851e-14) A[2]:(1.0) A[3]:(4.54746555854e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 662000 finished after 11 . Running score: 0.13. Policy_loss: -92050.6114646, Value_loss: 1.00545250437. Times trained:               15069. Times reached goal: 134.               Steps done: 8257063.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997502446175) A[1]:(0.00115332927089) A[2]:(0.000861934095155) A[3]:(0.000482274364913)\n",
      " state (1)  A[0]:(0.000401414785301) A[1]:(0.000104166698293) A[2]:(0.000613958458416) A[3]:(0.998880445957)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(4.76171102548e-09) A[2]:(1.09533345949e-07) A[3]:(3.58384721899e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.29788157788e-11) A[2]:(2.14134021626e-09) A[3]:(3.31497372955e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.88424986619e-11) A[2]:(3.61521634851e-09) A[3]:(5.0710580483e-13)\n",
      " state (5)  A[0]:(0.999990344048) A[1]:(2.09401412876e-10) A[2]:(9.6642288554e-06) A[3]:(9.23608960851e-17)\n",
      " state (6)  A[0]:(7.7134721721e-08) A[1]:(7.89883769828e-14) A[2]:(0.999999940395) A[3]:(1.0806756851e-31)\n",
      " state (7)  A[0]:(3.11384779073e-10) A[1]:(1.04334841819e-14) A[2]:(1.0) A[3]:(7.82632475669e-34)\n",
      " state (8)  A[0]:(1.29800031834e-10) A[1]:(9.91098682616e-15) A[2]:(1.0) A[3]:(5.11117617036e-34)\n",
      " state (9)  A[0]:(1.04964606451e-10) A[1]:(9.98536733035e-15) A[2]:(1.0) A[3]:(4.74581966915e-34)\n",
      " state (10)  A[0]:(9.80101624859e-11) A[1]:(1.00344732184e-14) A[2]:(1.0) A[3]:(4.65235868471e-34)\n",
      " state (11)  A[0]:(9.43032874012e-11) A[1]:(1.00802045273e-14) A[2]:(1.0) A[3]:(4.61136653347e-34)\n",
      " state (12)  A[0]:(9.01819036114e-11) A[1]:(1.01489226162e-14) A[2]:(1.0) A[3]:(4.5731074066e-34)\n",
      " state (13)  A[0]:(8.41434699694e-11) A[1]:(1.02624851705e-14) A[2]:(1.0) A[3]:(4.51882941412e-34)\n",
      " state (14)  A[0]:(7.56919041334e-11) A[1]:(1.04383410688e-14) A[2]:(1.0) A[3]:(4.44076832403e-34)\n",
      " state (15)  A[0]:(6.56375787056e-11) A[1]:(1.0676589414e-14) A[2]:(1.0) A[3]:(4.34674346965e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 663000 finished after 13 . Running score: 0.1. Policy_loss: -92050.6111763, Value_loss: 1.4177141143. Times trained:               15938. Times reached goal: 111.               Steps done: 8273001.\n",
      " state (0)  A[0]:(0.997328460217) A[1]:(0.000880080042407) A[2]:(0.000909309659619) A[3]:(0.000882168184035)\n",
      " state (1)  A[0]:(0.000367323955288) A[1]:(9.20450984268e-05) A[2]:(0.00056140543893) A[3]:(0.998979210854)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.20307963381e-10) A[2]:(1.49137999728e-08) A[3]:(1.40302325313e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.4551391182e-11) A[2]:(2.34656405418e-09) A[3]:(3.74488287602e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.20494923603e-11) A[2]:(4.56123139259e-09) A[3]:(5.09759064387e-13)\n",
      " state (5)  A[0]:(0.999973535538) A[1]:(2.71599881474e-10) A[2]:(2.64611026068e-05) A[3]:(4.69268296529e-17)\n",
      " state (6)  A[0]:(3.42731389935e-08) A[1]:(5.34588547746e-14) A[2]:(0.999999940395) A[3]:(5.84062647143e-32)\n",
      " state (7)  A[0]:(2.16610118731e-10) A[1]:(9.42327880765e-15) A[2]:(1.0) A[3]:(7.79611087846e-34)\n",
      " state (8)  A[0]:(9.87771739402e-11) A[1]:(9.14039606129e-15) A[2]:(1.0) A[3]:(5.42217110879e-34)\n",
      " state (9)  A[0]:(8.16777825929e-11) A[1]:(9.22622252171e-15) A[2]:(1.0) A[3]:(5.09334768631e-34)\n",
      " state (10)  A[0]:(7.66830904331e-11) A[1]:(9.27605262296e-15) A[2]:(1.0) A[3]:(5.0077303716e-34)\n",
      " state (11)  A[0]:(7.38325303673e-11) A[1]:(9.32245224078e-15) A[2]:(1.0) A[3]:(4.96819105791e-34)\n",
      " state (12)  A[0]:(7.0518306472e-11) A[1]:(9.39234347035e-15) A[2]:(1.0) A[3]:(4.92941535636e-34)\n",
      " state (13)  A[0]:(6.5663252613e-11) A[1]:(9.50668360087e-15) A[2]:(1.0) A[3]:(4.87473742031e-34)\n",
      " state (14)  A[0]:(5.90257565047e-11) A[1]:(9.6789252096e-15) A[2]:(1.0) A[3]:(4.80033367885e-34)\n",
      " state (15)  A[0]:(5.13618557407e-11) A[1]:(9.90493985795e-15) A[2]:(1.0) A[3]:(4.72216284534e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 664000 finished after 17 . Running score: 0.09. Policy_loss: -92050.6111767, Value_loss: 0.988709545723. Times trained:               15640. Times reached goal: 106.               Steps done: 8288641.\n",
      "action_dist \n",
      "tensor([[ 0.9976,  0.0010,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9976,  0.0010,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9976,  0.0010,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9976,  0.0010,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9521e-11,  1.5757e-09,  2.7548e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.5135e-09,  2.0296e-14,  1.0000e+00,  3.5421e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0520e-10,  1.3724e-14,  1.0000e+00,  9.7799e-34]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.9418e-10,  1.3518e-14,  1.0000e+00,  6.9169e-34]])\n",
      "On state=13, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8602e-10,  1.3597e-14,  1.0000e+00,  6.8415e-34]])\n",
      "On state=14, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.5271e-10,  1.3403e-14,  1.0000e+00,  7.6836e-34]])\n",
      "On state=10, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8602e-10,  1.3597e-14,  1.0000e+00,  6.8415e-34]])\n",
      "On state=14, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.5269e-10,  1.3403e-14,  1.0000e+00,  7.6834e-34]])\n",
      "On state=10, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8602e-10,  1.3597e-14,  1.0000e+00,  6.8415e-34]])\n",
      "On state=14, selected action=2\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.997638463974) A[1]:(0.000996353337541) A[2]:(0.000787556695286) A[3]:(0.000577652361244)\n",
      " state (1)  A[0]:(0.000358959689038) A[1]:(9.0944202384e-05) A[2]:(0.000527970842086) A[3]:(0.999022126198)\n",
      " state (2)  A[0]:(0.999999463558) A[1]:(2.31364118974e-08) A[2]:(4.38499824895e-07) A[3]:(4.8295621724e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.9174332101e-11) A[2]:(1.90649540599e-09) A[3]:(5.72719302538e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.95270170761e-11) A[2]:(1.57598267769e-09) A[3]:(2.75489592076e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.11514113116e-11) A[2]:(2.26005858472e-09) A[3]:(7.14140172779e-13)\n",
      " state (6)  A[0]:(0.999999701977) A[1]:(7.87366838395e-11) A[2]:(3.09155893774e-07) A[3]:(1.98497004288e-15)\n",
      " state (7)  A[0]:(3.86547981179e-05) A[1]:(2.36521175909e-12) A[2]:(0.999961316586) A[3]:(2.17652472651e-28)\n",
      " state (8)  A[0]:(2.50993492656e-09) A[1]:(2.02897645381e-14) A[2]:(1.0) A[3]:(3.53872215271e-33)\n",
      " state (9)  A[0]:(4.04958955258e-10) A[1]:(1.3723976789e-14) A[2]:(1.0) A[3]:(9.77821475914e-34)\n",
      " state (10)  A[0]:(2.52639825726e-10) A[1]:(1.34033002796e-14) A[2]:(1.0) A[3]:(7.68326984926e-34)\n",
      " state (11)  A[0]:(2.1594019628e-10) A[1]:(1.34257644403e-14) A[2]:(1.0) A[3]:(7.18227828954e-34)\n",
      " state (12)  A[0]:(2.0225049413e-10) A[1]:(1.34669277005e-14) A[2]:(1.0) A[3]:(7.00621908232e-34)\n",
      " state (13)  A[0]:(1.94117694141e-10) A[1]:(1.35189135006e-14) A[2]:(1.0) A[3]:(6.91656835262e-34)\n",
      " state (14)  A[0]:(1.85963300314e-10) A[1]:(1.35975808385e-14) A[2]:(1.0) A[3]:(6.84120538941e-34)\n",
      " state (15)  A[0]:(1.75272407699e-10) A[1]:(1.37213069411e-14) A[2]:(1.0) A[3]:(6.74897913321e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 665000 finished after 13 . Running score: 0.11. Policy_loss: -92050.6111768, Value_loss: 1.21009254783. Times trained:               15296. Times reached goal: 104.               Steps done: 8303937.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995216846466) A[1]:(0.000845623668283) A[2]:(0.00132536084857) A[3]:(0.0026121896226)\n",
      " state (1)  A[0]:(0.0003225892724) A[1]:(7.70046463003e-05) A[2]:(0.00046732701594) A[3]:(0.999133050442)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(1.58097435143e-09) A[2]:(3.70677852857e-08) A[3]:(1.05621966728e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.91345922282e-11) A[2]:(1.61183355551e-09) A[3]:(5.75670804429e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.3793197407e-11) A[2]:(1.32164656996e-09) A[3]:(3.08400158709e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.55883595562e-11) A[2]:(1.43136758002e-09) A[3]:(1.23616611739e-12)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.6383404142e-11) A[2]:(6.24662899185e-09) A[3]:(8.82588679875e-14)\n",
      " state (7)  A[0]:(0.998273253441) A[1]:(3.12950637626e-10) A[2]:(0.00172677461524) A[3]:(1.86814204776e-19)\n",
      " state (8)  A[0]:(3.42908447237e-06) A[1]:(2.41362024768e-13) A[2]:(0.999996542931) A[3]:(1.25237384428e-30)\n",
      " state (9)  A[0]:(1.01729078494e-08) A[1]:(1.76091678402e-14) A[2]:(1.0) A[3]:(4.3226534578e-33)\n",
      " state (10)  A[0]:(2.3314359332e-09) A[1]:(1.24252838514e-14) A[2]:(1.0) A[3]:(1.58725789963e-33)\n",
      " state (11)  A[0]:(1.3707843749e-09) A[1]:(1.2174921241e-14) A[2]:(1.0) A[3]:(1.24404826301e-33)\n",
      " state (12)  A[0]:(1.06429465241e-09) A[1]:(1.23600928076e-14) A[2]:(1.0) A[3]:(1.14260493724e-33)\n",
      " state (13)  A[0]:(8.932360962e-10) A[1]:(1.26303937266e-14) A[2]:(1.0) A[3]:(1.09322150121e-33)\n",
      " state (14)  A[0]:(7.37201300005e-10) A[1]:(1.30362978442e-14) A[2]:(1.0) A[3]:(1.05275354698e-33)\n",
      " state (15)  A[0]:(5.70030245228e-10) A[1]:(1.36620264933e-14) A[2]:(1.0) A[3]:(1.00786848905e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 666000 finished after 11 . Running score: 0.15. Policy_loss: -92050.6111807, Value_loss: 0.987220251484. Times trained:               15732. Times reached goal: 114.               Steps done: 8319669.\n",
      " state (0)  A[0]:(0.994249224663) A[1]:(0.00108478276525) A[2]:(0.00140968407504) A[3]:(0.00325633049943)\n",
      " state (1)  A[0]:(0.000312356452923) A[1]:(7.68176978454e-05) A[2]:(0.000447000697022) A[3]:(0.999163806438)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.44034423277e-10) A[2]:(3.87708176675e-09) A[3]:(3.01207392361e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.00428683267e-11) A[2]:(1.30056587722e-09) A[3]:(4.61074104244e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.73439708376e-11) A[2]:(1.07528042026e-09) A[3]:(2.31541240174e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.14352008365e-11) A[2]:(1.26152099877e-09) A[3]:(8.39558240637e-13)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.41377837856e-11) A[2]:(8.44923864207e-09) A[3]:(3.40852224375e-14)\n",
      " state (7)  A[0]:(0.994565188885) A[1]:(2.67478733607e-10) A[2]:(0.00543481437489) A[3]:(1.56259086758e-20)\n",
      " state (8)  A[0]:(0.000123371515656) A[1]:(9.87117831051e-13) A[2]:(0.999876618385) A[3]:(1.12853604104e-29)\n",
      " state (9)  A[0]:(3.05727240857e-07) A[1]:(5.67059792948e-14) A[2]:(0.999999701977) A[3]:(2.74035034033e-32)\n",
      " state (10)  A[0]:(2.15029629658e-08) A[1]:(2.24099422152e-14) A[2]:(1.0) A[3]:(4.37464226686e-33)\n",
      " state (11)  A[0]:(9.56881951453e-09) A[1]:(1.85656713256e-14) A[2]:(1.0) A[3]:(2.70663708888e-33)\n",
      " state (12)  A[0]:(7.04285874065e-09) A[1]:(1.79202101969e-14) A[2]:(1.0) A[3]:(2.32427687822e-33)\n",
      " state (13)  A[0]:(6.09362160731e-09) A[1]:(1.78306093577e-14) A[2]:(1.0) A[3]:(2.18670693763e-33)\n",
      " state (14)  A[0]:(5.53967716144e-09) A[1]:(1.78973233667e-14) A[2]:(1.0) A[3]:(2.11603603359e-33)\n",
      " state (15)  A[0]:(5.01865615732e-09) A[1]:(1.8076957032e-14) A[2]:(1.0) A[3]:(2.05911657674e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 667000 finished after 8 . Running score: 0.09. Policy_loss: -92050.6111819, Value_loss: 0.994293390937. Times trained:               15736. Times reached goal: 112.               Steps done: 8335405.\n",
      " state (0)  A[0]:(0.99352145195) A[1]:(0.00106360041536) A[2]:(0.00124804058578) A[3]:(0.00416693324223)\n",
      " state (1)  A[0]:(0.000327211979311) A[1]:(7.60381881264e-05) A[2]:(0.000419452262577) A[3]:(0.999177277088)\n",
      " state (2)  A[0]:(1.0) A[1]:(6.14922626707e-11) A[2]:(1.56000345974e-09) A[3]:(9.10744916982e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.3889242107e-11) A[2]:(9.59666901501e-10) A[3]:(4.08037362579e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.57182192209e-11) A[2]:(8.02237165587e-10) A[3]:(2.66693524903e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.04223096767e-11) A[2]:(7.38067995609e-10) A[3]:(1.61168344295e-12)\n",
      " state (6)  A[0]:(1.0) A[1]:(1.64523134699e-11) A[2]:(8.77062755755e-10) A[3]:(5.92068982817e-13)\n",
      " state (7)  A[0]:(1.0) A[1]:(1.82591875647e-11) A[2]:(4.53604531714e-09) A[3]:(3.1958966452e-14)\n",
      " state (8)  A[0]:(0.999994277954) A[1]:(6.03288946577e-11) A[2]:(5.74301702727e-06) A[3]:(4.99105520132e-18)\n",
      " state (9)  A[0]:(0.470269024372) A[1]:(2.77006667604e-10) A[2]:(0.529730975628) A[3]:(9.12775333985e-24)\n",
      " state (10)  A[0]:(0.000931055576075) A[1]:(2.97339214882e-12) A[2]:(0.999068915844) A[3]:(5.87448415529e-29)\n",
      " state (11)  A[0]:(1.39740959639e-05) A[1]:(3.1202500978e-13) A[2]:(0.999986052513) A[3]:(3.79456347293e-31)\n",
      " state (12)  A[0]:(4.44715084313e-07) A[1]:(7.93226161838e-14) A[2]:(0.999999582767) A[3]:(2.69438851121e-32)\n",
      " state (13)  A[0]:(7.96570418515e-08) A[1]:(4.40378426271e-14) A[2]:(0.999999940395) A[3]:(8.56036422445e-33)\n",
      " state (14)  A[0]:(3.95738446457e-08) A[1]:(3.6144481505e-14) A[2]:(0.999999940395) A[3]:(5.52496267148e-33)\n",
      " state (15)  A[0]:(2.90692678817e-08) A[1]:(3.37661485144e-14) A[2]:(1.0) A[3]:(4.60997954197e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 668000 finished after 21 . Running score: 0.0. Policy_loss: -92050.6112023, Value_loss: 0.979761975488. Times trained:               17339. Times reached goal: 38.               Steps done: 8352744.\n",
      " state (0)  A[0]:(0.995775163174) A[1]:(0.00114038481843) A[2]:(0.00105594052002) A[3]:(0.00202852068469)\n",
      " state (1)  A[0]:(0.000329947157297) A[1]:(7.67741876189e-05) A[2]:(0.00040390479262) A[3]:(0.999189376831)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.54998269564e-11) A[2]:(1.31585886631e-09) A[3]:(8.23560403901e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.63730912109e-11) A[2]:(7.18673731637e-10) A[3]:(2.92733502369e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.93698564133e-11) A[2]:(6.05075434201e-10) A[3]:(1.6591371289e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.51823328215e-11) A[2]:(6.4280958334e-10) A[3]:(6.88140299951e-13)\n",
      " state (6)  A[0]:(1.0) A[1]:(1.52786915064e-11) A[2]:(2.20664775341e-09) A[3]:(5.72388883795e-14)\n",
      " state (7)  A[0]:(0.999999523163) A[1]:(3.44428027765e-11) A[2]:(4.66989916958e-07) A[3]:(4.44077106543e-17)\n",
      " state (8)  A[0]:(0.997821986675) A[1]:(1.45675790875e-10) A[2]:(0.00217800657265) A[3]:(3.46829197194e-21)\n",
      " state (9)  A[0]:(0.378688842058) A[1]:(1.85901655181e-10) A[2]:(0.621311187744) A[3]:(1.57340910787e-24)\n",
      " state (10)  A[0]:(0.00899338163435) A[1]:(1.05756835581e-11) A[2]:(0.991006612778) A[3]:(6.13643169258e-28)\n",
      " state (11)  A[0]:(0.000664059247356) A[1]:(1.75295878074e-12) A[2]:(0.999335944653) A[3]:(6.59285090879e-30)\n",
      " state (12)  A[0]:(5.50685253984e-05) A[1]:(5.19887681082e-13) A[2]:(0.999944925308) A[3]:(4.49930572029e-31)\n",
      " state (13)  A[0]:(3.96809036829e-06) A[1]:(1.89304084561e-13) A[2]:(0.999996006489) A[3]:(6.7483925799e-32)\n",
      " state (14)  A[0]:(6.88548936978e-07) A[1]:(1.0293240598e-13) A[2]:(0.999999284744) A[3]:(2.24193823766e-32)\n",
      " state (15)  A[0]:(2.90263926672e-07) A[1]:(7.78292022301e-14) A[2]:(0.999999701977) A[3]:(1.33122678116e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 669000 finished after 5 . Running score: 0.04. Policy_loss: -92050.6114747, Value_loss: 0.97868833921. Times trained:               17127. Times reached goal: 56.               Steps done: 8369871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.0130e-11,  9.8421e-10,  7.8258e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.0131e-11,  9.8422e-10,  7.8259e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.0131e-11,  9.8423e-10,  7.8259e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2622e-04,  1.7622e-12,  9.9937e-01,  7.6928e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.0131e-11,  9.8420e-10,  7.8262e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.0130e-11,  9.8411e-10,  7.8270e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.0130e-11,  9.8408e-10,  7.8273e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.0130e-11,  9.8405e-10,  7.8278e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.0130e-11,  9.8403e-10,  7.8281e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0014,  0.0010,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.0130e-11,  9.8401e-10,  7.8285e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2743e-04,  1.7645e-12,  9.9937e-01,  7.7155e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995491743088) A[1]:(0.00141474197153) A[2]:(0.00100731023122) A[3]:(0.00208618841134)\n",
      " state (1)  A[0]:(0.000304673507344) A[1]:(7.32884946046e-05) A[2]:(0.000394439644879) A[3]:(0.999227583408)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.71366634561e-11) A[2]:(1.47964329678e-09) A[3]:(8.79682091059e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.71921044709e-11) A[2]:(8.81033024314e-10) A[3]:(2.55825872639e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.01296340707e-11) A[2]:(9.83921277786e-10) A[3]:(7.82923042803e-13)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.04779700141e-11) A[2]:(5.19501996976e-09) A[3]:(2.87769652129e-14)\n",
      " state (6)  A[0]:(0.999865949154) A[1]:(1.05650682647e-10) A[2]:(0.000134061992867) A[3]:(1.10668093488e-19)\n",
      " state (7)  A[0]:(0.0410369932652) A[1]:(4.00348400265e-11) A[2]:(0.958963036537) A[3]:(1.95257187257e-26)\n",
      " state (8)  A[0]:(0.000627902511042) A[1]:(1.76525222391e-12) A[2]:(0.999372124672) A[3]:(7.72328978366e-30)\n",
      " state (9)  A[0]:(3.2160543924e-05) A[1]:(4.01732683578e-13) A[2]:(0.999967813492) A[3]:(3.0082259309e-31)\n",
      " state (10)  A[0]:(1.11505266887e-06) A[1]:(1.1667302866e-13) A[2]:(0.999998867512) A[3]:(3.30473427164e-32)\n",
      " state (11)  A[0]:(1.798807574e-07) A[1]:(6.42379148464e-14) A[2]:(0.999999821186) A[3]:(1.15111452452e-32)\n",
      " state (12)  A[0]:(9.07376005443e-08) A[1]:(5.27333439502e-14) A[2]:(0.999999880791) A[3]:(7.89075047966e-33)\n",
      " state (13)  A[0]:(6.96540496392e-08) A[1]:(4.93168467218e-14) A[2]:(0.999999940395) A[3]:(6.85502754993e-33)\n",
      " state (14)  A[0]:(6.22999678512e-08) A[1]:(4.80822589318e-14) A[2]:(0.999999940395) A[3]:(6.4712235016e-33)\n",
      " state (15)  A[0]:(5.90639110953e-08) A[1]:(4.75743237665e-14) A[2]:(0.999999940395) A[3]:(6.30167093112e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 670000 finished after 25 . Running score: 0.12. Policy_loss: -92050.6111966, Value_loss: 0.97854907381. Times trained:               16230. Times reached goal: 117.               Steps done: 8386101.\n",
      " state (0)  A[0]:(0.994954824448) A[1]:(0.00131049577612) A[2]:(0.0011551262578) A[3]:(0.00257956073619)\n",
      " state (1)  A[0]:(0.000309806346195) A[1]:(7.37401423976e-05) A[2]:(0.000392473913962) A[3]:(0.99922400713)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.50809528596e-11) A[2]:(1.13425902004e-09) A[3]:(5.73399140669e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.47260354552e-11) A[2]:(7.99711796784e-10) A[3]:(1.87575410933e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.94262054359e-11) A[2]:(1.06464792538e-09) A[3]:(4.61979684108e-13)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.44033144386e-11) A[2]:(1.51774344204e-08) A[3]:(4.94106039517e-15)\n",
      " state (6)  A[0]:(0.996793687344) A[1]:(1.93330032539e-10) A[2]:(0.00320631405339) A[3]:(2.55544424577e-21)\n",
      " state (7)  A[0]:(0.0265721566975) A[1]:(3.01182169482e-11) A[2]:(0.973427832127) A[3]:(5.04181034198e-27)\n",
      " state (8)  A[0]:(0.000916473509278) A[1]:(2.45625980493e-12) A[2]:(0.999083518982) A[3]:(8.39879692227e-30)\n",
      " state (9)  A[0]:(7.10635722498e-05) A[1]:(6.61413090096e-13) A[2]:(0.999928951263) A[3]:(4.20226676139e-31)\n",
      " state (10)  A[0]:(2.73868704426e-06) A[1]:(1.96503973033e-13) A[2]:(0.999997258186) A[3]:(4.56226167123e-32)\n",
      " state (11)  A[0]:(3.39118656711e-07) A[1]:(9.77159908436e-14) A[2]:(0.999999642372) A[3]:(1.33928494187e-32)\n",
      " state (12)  A[0]:(1.47389016547e-07) A[1]:(7.58513328644e-14) A[2]:(0.999999880791) A[3]:(8.38359044523e-33)\n",
      " state (13)  A[0]:(1.0676340878e-07) A[1]:(6.94164371167e-14) A[2]:(0.999999880791) A[3]:(7.02819917365e-33)\n",
      " state (14)  A[0]:(9.35236883493e-08) A[1]:(6.71123042122e-14) A[2]:(0.999999880791) A[3]:(6.54650509857e-33)\n",
      " state (15)  A[0]:(8.82907826849e-08) A[1]:(6.61886249476e-14) A[2]:(0.999999940395) A[3]:(6.35046570163e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 671000 finished after 8 . Running score: 0.15. Policy_loss: -92050.6111757, Value_loss: 1.19960588802. Times trained:               16453. Times reached goal: 75.               Steps done: 8402554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996493697166) A[1]:(0.00104575988371) A[2]:(0.00103941839188) A[3]:(0.00142110802699)\n",
      " state (1)  A[0]:(0.000325310073094) A[1]:(7.23245975678e-05) A[2]:(0.000415128452005) A[3]:(0.999187231064)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.34653736614e-11) A[2]:(1.35696598402e-09) A[3]:(4.61574788807e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.78936561349e-11) A[2]:(1.52826418187e-09) A[3]:(8.90876511016e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.8056397483e-11) A[2]:(1.08559916612e-08) A[3]:(1.66752682761e-14)\n",
      " state (5)  A[0]:(0.985191881657) A[1]:(2.95250546234e-10) A[2]:(0.0148080959916) A[3]:(9.23622884562e-22)\n",
      " state (6)  A[0]:(0.000925039290451) A[1]:(2.66861836448e-12) A[2]:(0.999074935913) A[3]:(1.71416672476e-29)\n",
      " state (7)  A[0]:(2.92681870633e-05) A[1]:(3.87584387328e-13) A[2]:(0.999970734119) A[3]:(2.06475300603e-31)\n",
      " state (8)  A[0]:(4.40292524218e-07) A[1]:(8.47324394351e-14) A[2]:(0.999999582767) A[3]:(1.41622956946e-32)\n",
      " state (9)  A[0]:(5.33284598703e-08) A[1]:(4.42609951511e-14) A[2]:(0.999999940395) A[3]:(4.47667554385e-33)\n",
      " state (10)  A[0]:(2.81111312006e-08) A[1]:(3.80639259261e-14) A[2]:(1.0) A[3]:(3.25699068697e-33)\n",
      " state (11)  A[0]:(2.24506528923e-08) A[1]:(3.66766553651e-14) A[2]:(1.0) A[3]:(2.94455696182e-33)\n",
      " state (12)  A[0]:(2.00526013572e-08) A[1]:(3.63132307969e-14) A[2]:(1.0) A[3]:(2.81752955252e-33)\n",
      " state (13)  A[0]:(1.80408452621e-08) A[1]:(3.62860884731e-14) A[2]:(1.0) A[3]:(2.72235583607e-33)\n",
      " state (14)  A[0]:(1.54161501342e-08) A[1]:(3.65283703892e-14) A[2]:(1.0) A[3]:(2.60431801982e-33)\n",
      " state (15)  A[0]:(1.17976739489e-08) A[1]:(3.72979540319e-14) A[2]:(1.0) A[3]:(2.43725253825e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 672000 finished after 8 . Running score: 0.07. Policy_loss: -92050.6114475, Value_loss: 1.19338640245. Times trained:               16376. Times reached goal: 120.               Steps done: 8418930.\n",
      " state (0)  A[0]:(0.996865928173) A[1]:(0.00107375835069) A[2]:(0.000885091954842) A[3]:(0.00117521861102)\n",
      " state (1)  A[0]:(0.000292971439194) A[1]:(6.86231869622e-05) A[2]:(0.000395700277295) A[3]:(0.999242722988)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.2503001946e-11) A[2]:(1.62535240822e-09) A[3]:(5.83399517931e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.3165501595e-11) A[2]:(1.92452920267e-09) A[3]:(9.45652627385e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.46851609934e-11) A[2]:(1.96753671133e-08) A[3]:(1.00239522222e-14)\n",
      " state (5)  A[0]:(0.881315171719) A[1]:(4.47855141861e-10) A[2]:(0.118684828281) A[3]:(1.25808483798e-22)\n",
      " state (6)  A[0]:(0.000348447763827) A[1]:(1.43988488667e-12) A[2]:(0.999651551247) A[3]:(4.32324816978e-30)\n",
      " state (7)  A[0]:(1.31630104079e-05) A[1]:(2.68923582404e-13) A[2]:(0.999986827374) A[3]:(1.10769924228e-31)\n",
      " state (8)  A[0]:(2.53963946761e-07) A[1]:(6.93144204685e-14) A[2]:(0.999999761581) A[3]:(1.05299887632e-32)\n",
      " state (9)  A[0]:(3.79295279629e-08) A[1]:(4.06461634641e-14) A[2]:(0.999999940395) A[3]:(3.95167699157e-33)\n",
      " state (10)  A[0]:(1.97569338667e-08) A[1]:(3.5965754165e-14) A[2]:(1.0) A[3]:(2.95103393569e-33)\n",
      " state (11)  A[0]:(1.36486288937e-08) A[1]:(3.51415877192e-14) A[2]:(1.0) A[3]:(2.59153378407e-33)\n",
      " state (12)  A[0]:(8.72351701986e-09) A[1]:(3.58499410442e-14) A[2]:(1.0) A[3]:(2.3051688502e-33)\n",
      " state (13)  A[0]:(4.377633811e-09) A[1]:(3.85692115724e-14) A[2]:(1.0) A[3]:(1.9970284345e-33)\n",
      " state (14)  A[0]:(2.09634531956e-09) A[1]:(4.28251624972e-14) A[2]:(1.0) A[3]:(1.74387360376e-33)\n",
      " state (15)  A[0]:(1.37603406447e-09) A[1]:(4.62706113116e-14) A[2]:(1.0) A[3]:(1.62098577129e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 673000 finished after 33 . Running score: 0.1. Policy_loss: -92050.6111882, Value_loss: 1.41452436418. Times trained:               15310. Times reached goal: 108.               Steps done: 8434240.\n",
      " state (0)  A[0]:(0.996467769146) A[1]:(0.00112316850573) A[2]:(0.00100344396196) A[3]:(0.00140560581349)\n",
      " state (1)  A[0]:(0.000284585839836) A[1]:(6.58154749544e-05) A[2]:(0.000392413407099) A[3]:(0.999257206917)\n",
      " state (2)  A[0]:(1.0) A[1]:(6.21375381704e-11) A[2]:(2.13559725459e-09) A[3]:(7.37909559845e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.5896591255e-11) A[2]:(2.89766965977e-09) A[3]:(6.55338088483e-13)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(5.07790580417e-11) A[2]:(1.10269738229e-07) A[3]:(1.42006693558e-15)\n",
      " state (5)  A[0]:(0.019362334162) A[1]:(4.42921428701e-11) A[2]:(0.980637669563) A[3]:(3.30890480034e-26)\n",
      " state (6)  A[0]:(7.11410484655e-06) A[1]:(2.07160284478e-13) A[2]:(0.999992907047) A[3]:(1.12424879215e-31)\n",
      " state (7)  A[0]:(5.0771195248e-08) A[1]:(3.60441081008e-14) A[2]:(0.999999940395) A[3]:(5.25621270413e-33)\n",
      " state (8)  A[0]:(7.53122275654e-09) A[1]:(2.1935329326e-14) A[2]:(1.0) A[3]:(2.03868593391e-33)\n",
      " state (9)  A[0]:(4.00642985454e-09) A[1]:(2.06539073902e-14) A[2]:(1.0) A[3]:(1.61301628693e-33)\n",
      " state (10)  A[0]:(2.29112639971e-09) A[1]:(2.14763374185e-14) A[2]:(1.0) A[3]:(1.4149712946e-33)\n",
      " state (11)  A[0]:(1.00344987874e-09) A[1]:(2.42079661574e-14) A[2]:(1.0) A[3]:(1.23017393992e-33)\n",
      " state (12)  A[0]:(5.06264863365e-10) A[1]:(2.7439957587e-14) A[2]:(1.0) A[3]:(1.11172571084e-33)\n",
      " state (13)  A[0]:(3.77433001519e-10) A[1]:(2.96499817428e-14) A[2]:(1.0) A[3]:(1.07798204347e-33)\n",
      " state (14)  A[0]:(3.27886967577e-10) A[1]:(3.12729273629e-14) A[2]:(1.0) A[3]:(1.07099758664e-33)\n",
      " state (15)  A[0]:(3.0689184527e-10) A[1]:(3.22446605007e-14) A[2]:(1.0) A[3]:(1.06707446608e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 674000 finished after 10 . Running score: 0.09. Policy_loss: -92050.6111882, Value_loss: 1.20093918036. Times trained:               15488. Times reached goal: 98.               Steps done: 8449728.\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.9880e-11,  6.8323e-09,  5.4185e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.9881e-11,  6.8324e-09,  5.4188e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.9888e-11,  6.8328e-09,  5.4205e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.9889e-11,  6.8329e-09,  5.4211e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0011,  0.0013,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.9892e-11,  6.8329e-09,  5.4220e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.6692e-10,  2.8555e-14,  1.0000e+00,  1.5105e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.9893e-11,  6.8329e-09,  5.4225e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.6687e-10,  2.8555e-14,  1.0000e+00,  1.5106e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.9894e-11,  6.8329e-09,  5.4230e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.9894e-11,  6.8329e-09,  5.4232e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.6681e-10,  2.8555e-14,  1.0000e+00,  1.5107e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994759917259) A[1]:(0.00114225957077) A[2]:(0.00133730750531) A[3]:(0.00276053370908)\n",
      " state (1)  A[0]:(0.000253749691183) A[1]:(6.26672699582e-05) A[2]:(0.000364022736903) A[3]:(0.999319553375)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(2.8174722555e-09) A[2]:(6.05287198141e-08) A[3]:(2.83329426587e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.35864624843e-11) A[2]:(2.11607398271e-09) A[3]:(6.98526826659e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.98948182717e-11) A[2]:(6.83277701086e-09) A[3]:(5.42355277442e-13)\n",
      " state (5)  A[0]:(0.131770849228) A[1]:(4.16634587941e-10) A[2]:(0.868229150772) A[3]:(2.29034246462e-23)\n",
      " state (6)  A[0]:(1.20461180586e-07) A[1]:(7.04480148262e-14) A[2]:(0.999999880791) A[3]:(1.93934894709e-32)\n",
      " state (7)  A[0]:(3.9095029436e-09) A[1]:(2.72017109358e-14) A[2]:(1.0) A[3]:(2.27131865366e-33)\n",
      " state (8)  A[0]:(9.66810076442e-10) A[1]:(2.85553644532e-14) A[2]:(1.0) A[3]:(1.51072899296e-33)\n",
      " state (9)  A[0]:(3.86255361029e-10) A[1]:(3.29686737708e-14) A[2]:(1.0) A[3]:(1.29198795321e-33)\n",
      " state (10)  A[0]:(2.74442024661e-10) A[1]:(3.62174991332e-14) A[2]:(1.0) A[3]:(1.26357175481e-33)\n",
      " state (11)  A[0]:(2.40236136539e-10) A[1]:(3.82960807163e-14) A[2]:(1.0) A[3]:(1.25712921924e-33)\n",
      " state (12)  A[0]:(2.30523100608e-10) A[1]:(3.91444723044e-14) A[2]:(1.0) A[3]:(1.25222814249e-33)\n",
      " state (13)  A[0]:(2.27798321872e-10) A[1]:(3.94182401292e-14) A[2]:(1.0) A[3]:(1.25028067895e-33)\n",
      " state (14)  A[0]:(2.27025370725e-10) A[1]:(3.94995993379e-14) A[2]:(1.0) A[3]:(1.24966078935e-33)\n",
      " state (15)  A[0]:(2.26802909786e-10) A[1]:(3.95231129725e-14) A[2]:(1.0) A[3]:(1.24947968976e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 675000 finished after 23 . Running score: 0.12. Policy_loss: -92050.6111996, Value_loss: 1.20135175951. Times trained:               15403. Times reached goal: 107.               Steps done: 8465131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995445132256) A[1]:(0.000973306363449) A[2]:(0.00145767943468) A[3]:(0.00212389859371)\n",
      " state (1)  A[0]:(0.000231889236602) A[1]:(5.7720913901e-05) A[2]:(0.000361970538506) A[3]:(0.999348402023)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.46259704137e-10) A[2]:(1.46016763125e-08) A[3]:(2.41374281673e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.16799258837e-11) A[2]:(2.59090326971e-09) A[3]:(8.57509809687e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.65408769751e-11) A[2]:(5.35172484106e-09) A[3]:(1.50673646736e-12)\n",
      " state (5)  A[0]:(0.999915540218) A[1]:(3.83597403841e-10) A[2]:(8.44751048135e-05) A[3]:(1.63999342874e-17)\n",
      " state (6)  A[0]:(1.66873360286e-06) A[1]:(2.36470240091e-13) A[2]:(0.99999833107) A[3]:(3.42762162657e-31)\n",
      " state (7)  A[0]:(2.83741008467e-09) A[1]:(2.47845465668e-14) A[2]:(1.0) A[3]:(2.37749022178e-33)\n",
      " state (8)  A[0]:(3.12213810361e-10) A[1]:(2.47647005848e-14) A[2]:(1.0) A[3]:(1.14128066939e-33)\n",
      " state (9)  A[0]:(1.56605159396e-10) A[1]:(2.755155926e-14) A[2]:(1.0) A[3]:(1.02157888578e-33)\n",
      " state (10)  A[0]:(1.23944438046e-10) A[1]:(2.95910553547e-14) A[2]:(1.0) A[3]:(1.00733804723e-33)\n",
      " state (11)  A[0]:(1.15529801004e-10) A[1]:(3.05318311323e-14) A[2]:(1.0) A[3]:(9.98401351425e-34)\n",
      " state (12)  A[0]:(1.13389075906e-10) A[1]:(3.08339847253e-14) A[2]:(1.0) A[3]:(9.94812512071e-34)\n",
      " state (13)  A[0]:(1.12824284637e-10) A[1]:(3.09194367971e-14) A[2]:(1.0) A[3]:(9.93735373536e-34)\n",
      " state (14)  A[0]:(1.12672461638e-10) A[1]:(3.09429199386e-14) A[2]:(1.0) A[3]:(9.93432132728e-34)\n",
      " state (15)  A[0]:(1.12630772764e-10) A[1]:(3.09492930145e-14) A[2]:(1.0) A[3]:(9.93348746097e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 676000 finished after 20 . Running score: 0.11. Policy_loss: -92050.6114318, Value_loss: 1.4549144077. Times trained:               15812. Times reached goal: 114.               Steps done: 8480943.\n",
      " state (0)  A[0]:(0.993844926357) A[1]:(0.00108442234341) A[2]:(0.00265622325242) A[3]:(0.00241445354186)\n",
      " state (1)  A[0]:(0.000222953676712) A[1]:(5.71809869143e-05) A[2]:(0.000402581412345) A[3]:(0.999317288399)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.10775064069e-10) A[2]:(6.95031321385e-09) A[3]:(5.3995606597e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.68508937043e-11) A[2]:(3.41966610584e-09) A[3]:(7.59952170637e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.12120518004e-11) A[2]:(2.35138166715e-08) A[3]:(2.77730122971e-13)\n",
      " state (5)  A[0]:(0.00199892208911) A[1]:(2.67276183824e-11) A[2]:(0.998001098633) A[3]:(3.38844570704e-26)\n",
      " state (6)  A[0]:(3.71196295923e-09) A[1]:(2.31198981265e-14) A[2]:(1.0) A[3]:(2.54274727877e-33)\n",
      " state (7)  A[0]:(1.64602317887e-10) A[1]:(1.82953458427e-14) A[2]:(1.0) A[3]:(6.69405737391e-34)\n",
      " state (8)  A[0]:(8.57821105815e-11) A[1]:(1.9935160971e-14) A[2]:(1.0) A[3]:(5.95636211873e-34)\n",
      " state (9)  A[0]:(6.94817189895e-11) A[1]:(2.11208055033e-14) A[2]:(1.0) A[3]:(5.81053148343e-34)\n",
      " state (10)  A[0]:(6.60786078632e-11) A[1]:(2.15381166136e-14) A[2]:(1.0) A[3]:(5.74292448705e-34)\n",
      " state (11)  A[0]:(6.53540069284e-11) A[1]:(2.16422186568e-14) A[2]:(1.0) A[3]:(5.7239397941e-34)\n",
      " state (12)  A[0]:(6.51930245898e-11) A[1]:(2.16660914334e-14) A[2]:(1.0) A[3]:(5.71931266263e-34)\n",
      " state (13)  A[0]:(6.51561096743e-11) A[1]:(2.16714650104e-14) A[2]:(1.0) A[3]:(5.71817849425e-34)\n",
      " state (14)  A[0]:(6.51476581015e-11) A[1]:(2.16727050667e-14) A[2]:(1.0) A[3]:(5.71791676309e-34)\n",
      " state (15)  A[0]:(6.51455417389e-11) A[1]:(2.16729947519e-14) A[2]:(1.0) A[3]:(5.71782951937e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 677000 finished after 35 . Running score: 0.1. Policy_loss: -92050.6123846, Value_loss: 1.41380863584. Times trained:               15210. Times reached goal: 114.               Steps done: 8496153.\n",
      " state (0)  A[0]:(0.991043806076) A[1]:(0.00178397144191) A[2]:(0.00279899779707) A[3]:(0.00437323004007)\n",
      " state (1)  A[0]:(0.000198706664378) A[1]:(5.77172977501e-05) A[2]:(0.000354872172466) A[3]:(0.999388694763)\n",
      " state (2)  A[0]:(0.999999642372) A[1]:(1.99815808344e-08) A[2]:(3.03202824625e-07) A[3]:(5.38351443424e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.63339538748e-11) A[2]:(2.07716088774e-09) A[3]:(1.03248763705e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.71993172114e-11) A[2]:(6.68294175554e-09) A[3]:(8.84679048768e-13)\n",
      " state (5)  A[0]:(0.755412936211) A[1]:(2.26149499127e-09) A[2]:(0.244587063789) A[3]:(3.53615349781e-21)\n",
      " state (6)  A[0]:(8.44043128723e-08) A[1]:(1.12800007778e-13) A[2]:(0.999999940395) A[3]:(3.1130525792e-32)\n",
      " state (7)  A[0]:(7.04642955096e-10) A[1]:(3.72940407397e-14) A[2]:(1.0) A[3]:(2.23581505085e-33)\n",
      " state (8)  A[0]:(1.75294473381e-10) A[1]:(4.19633810122e-14) A[2]:(1.0) A[3]:(1.65399126494e-33)\n",
      " state (9)  A[0]:(1.17061860205e-10) A[1]:(4.54652320566e-14) A[2]:(1.0) A[3]:(1.61360715652e-33)\n",
      " state (10)  A[0]:(1.01163591393e-10) A[1]:(4.75454806006e-14) A[2]:(1.0) A[3]:(1.60025611209e-33)\n",
      " state (11)  A[0]:(9.7315912273e-11) A[1]:(4.82584858305e-14) A[2]:(1.0) A[3]:(1.59283819172e-33)\n",
      " state (12)  A[0]:(9.63676083376e-11) A[1]:(4.84538319569e-14) A[2]:(1.0) A[3]:(1.59044587704e-33)\n",
      " state (13)  A[0]:(9.61278556755e-11) A[1]:(4.85048741623e-14) A[2]:(1.0) A[3]:(1.58977860033e-33)\n",
      " state (14)  A[0]:(9.60653570581e-11) A[1]:(4.85181962965e-14) A[2]:(1.0) A[3]:(1.58959676605e-33)\n",
      " state (15)  A[0]:(9.6048682896e-11) A[1]:(4.85215302182e-14) A[2]:(1.0) A[3]:(1.58953615462e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 678000 finished after 35 . Running score: 0.08. Policy_loss: -92050.6111929, Value_loss: 0.978630795059. Times trained:               15817. Times reached goal: 116.               Steps done: 8511970.\n",
      " state (0)  A[0]:(0.993422687054) A[1]:(0.00135230703745) A[2]:(0.00193626282271) A[3]:(0.00328874099068)\n",
      " state (1)  A[0]:(0.000208821642445) A[1]:(5.52382189198e-05) A[2]:(0.000343765539583) A[3]:(0.999392151833)\n",
      " state (2)  A[0]:(0.999995350838) A[1]:(1.88945250557e-07) A[2]:(2.59614353126e-06) A[3]:(1.85252110896e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.22465282044e-11) A[2]:(1.84911508327e-09) A[3]:(8.49667385061e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.9952182935e-11) A[2]:(7.58254969924e-09) A[3]:(3.68390485218e-13)\n",
      " state (5)  A[0]:(0.195981502533) A[1]:(9.22653620172e-10) A[2]:(0.804018497467) A[3]:(5.862411613e-23)\n",
      " state (6)  A[0]:(2.38803696817e-08) A[1]:(5.9818801659e-14) A[2]:(1.0) A[3]:(1.23881602745e-32)\n",
      " state (7)  A[0]:(9.64555546545e-10) A[1]:(2.81056831318e-14) A[2]:(1.0) A[3]:(1.9887116283e-33)\n",
      " state (8)  A[0]:(2.25173601986e-10) A[1]:(3.31618447166e-14) A[2]:(1.0) A[3]:(1.48973760259e-33)\n",
      " state (9)  A[0]:(1.25112073479e-10) A[1]:(3.69747499762e-14) A[2]:(1.0) A[3]:(1.40979893578e-33)\n",
      " state (10)  A[0]:(9.91650997428e-11) A[1]:(3.93248225595e-14) A[2]:(1.0) A[3]:(1.40106005364e-33)\n",
      " state (11)  A[0]:(9.19483031381e-11) A[1]:(4.0396143055e-14) A[2]:(1.0) A[3]:(1.39319360871e-33)\n",
      " state (12)  A[0]:(9.00718805097e-11) A[1]:(4.07380631507e-14) A[2]:(1.0) A[3]:(1.3898175522e-33)\n",
      " state (13)  A[0]:(8.95789761812e-11) A[1]:(4.08338253076e-14) A[2]:(1.0) A[3]:(1.38878945382e-33)\n",
      " state (14)  A[0]:(8.94480184366e-11) A[1]:(4.08596903057e-14) A[2]:(1.0) A[3]:(1.38850338625e-33)\n",
      " state (15)  A[0]:(8.9413393356e-11) A[1]:(4.08667071266e-14) A[2]:(1.0) A[3]:(1.388429275e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 679000 finished after 6 . Running score: 0.12. Policy_loss: -92050.6111798, Value_loss: 1.41344882597. Times trained:               15912. Times reached goal: 103.               Steps done: 8527882.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9956,  0.0013,  0.0016,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0013,  0.0016,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0013,  0.0016,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.0388e-11,  1.4556e-09,  4.9636e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0013,  0.0016,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0013,  0.0016,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0013,  0.0016,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.0390e-11,  1.4557e-09,  4.9638e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9956,  0.0013,  0.0016,  0.0015]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.0391e-11,  1.4557e-09,  4.9640e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3126e-08,  5.9160e-14,  1.0000e+00,  9.3444e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.99559122324) A[1]:(0.00127370050177) A[2]:(0.00160539348144) A[3]:(0.00152967416216)\n",
      " state (1)  A[0]:(0.000216746062506) A[1]:(5.38766253158e-05) A[2]:(0.000343668099958) A[3]:(0.999385714531)\n",
      " state (2)  A[0]:(0.999996721745) A[1]:(1.34613202363e-07) A[2]:(1.87889759218e-06) A[3]:(1.24773396237e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.22806320405e-11) A[2]:(1.42269851455e-09) A[3]:(1.0036153332e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.03914895922e-11) A[2]:(1.45575640431e-09) A[3]:(4.96415495516e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(4.54847305653e-11) A[2]:(3.53923801377e-09) A[3]:(5.58267733257e-13)\n",
      " state (6)  A[0]:(0.999985575676) A[1]:(2.2455189097e-10) A[2]:(1.44318428283e-05) A[3]:(2.72459314423e-17)\n",
      " state (7)  A[0]:(4.85271702928e-05) A[1]:(2.64599713667e-12) A[2]:(0.999951481819) A[3]:(4.29071311926e-29)\n",
      " state (8)  A[0]:(1.31261082004e-08) A[1]:(5.91594576604e-14) A[2]:(1.0) A[3]:(9.34451226131e-33)\n",
      " state (9)  A[0]:(7.12193359842e-10) A[1]:(3.90008629504e-14) A[2]:(1.0) A[3]:(2.15568141728e-33)\n",
      " state (10)  A[0]:(2.22187601651e-10) A[1]:(4.34159273203e-14) A[2]:(1.0) A[3]:(1.61459236772e-33)\n",
      " state (11)  A[0]:(1.56717208655e-10) A[1]:(4.63509913502e-14) A[2]:(1.0) A[3]:(1.54003186307e-33)\n",
      " state (12)  A[0]:(1.34764116533e-10) A[1]:(4.83661132249e-14) A[2]:(1.0) A[3]:(1.5230076743e-33)\n",
      " state (13)  A[0]:(1.27501842417e-10) A[1]:(4.92970092102e-14) A[2]:(1.0) A[3]:(1.51498804776e-33)\n",
      " state (14)  A[0]:(1.25127103123e-10) A[1]:(4.96438624258e-14) A[2]:(1.0) A[3]:(1.51172054081e-33)\n",
      " state (15)  A[0]:(1.24332280582e-10) A[1]:(4.9763978474e-14) A[2]:(1.0) A[3]:(1.51053310785e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 680000 finished after 11 . Running score: 0.1. Policy_loss: -92050.6111824, Value_loss: 1.1970347707. Times trained:               15642. Times reached goal: 117.               Steps done: 8543524.\n",
      " state (0)  A[0]:(0.993882060051) A[1]:(0.00104931660462) A[2]:(0.00242947647348) A[3]:(0.00263912184164)\n",
      " state (1)  A[0]:(0.000245723436819) A[1]:(4.80378002976e-05) A[2]:(0.000324875931256) A[3]:(0.999381363392)\n",
      " state (2)  A[0]:(0.00300755142234) A[1]:(0.000111064189696) A[2]:(0.000824016868137) A[3]:(0.996057391167)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.99683168506e-11) A[2]:(8.93399632051e-10) A[3]:(1.01648038944e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.49542574104e-11) A[2]:(5.90600235384e-10) A[3]:(4.99542247845e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.17334223535e-11) A[2]:(5.50167189584e-10) A[3]:(3.84128015854e-12)\n",
      " state (6)  A[0]:(1.0) A[1]:(1.82515981495e-11) A[2]:(5.31395538683e-10) A[3]:(2.43575428924e-12)\n",
      " state (7)  A[0]:(1.0) A[1]:(1.45785831324e-11) A[2]:(6.19739926044e-10) A[3]:(8.62790470579e-13)\n",
      " state (8)  A[0]:(1.0) A[1]:(1.26414026816e-11) A[2]:(2.59135291003e-09) A[3]:(2.79469203584e-14)\n",
      " state (9)  A[0]:(0.999832212925) A[1]:(5.93166002427e-11) A[2]:(0.000167801961652) A[3]:(1.67260572749e-20)\n",
      " state (10)  A[0]:(0.000119305608678) A[1]:(7.912058053e-13) A[2]:(0.999880671501) A[3]:(1.76498257443e-30)\n",
      " state (11)  A[0]:(7.31758120764e-08) A[1]:(6.88192382313e-14) A[2]:(0.999999940395) A[3]:(4.75145542988e-33)\n",
      " state (12)  A[0]:(1.87677429153e-08) A[1]:(5.31933912608e-14) A[2]:(1.0) A[3]:(2.14649088817e-33)\n",
      " state (13)  A[0]:(1.40560141304e-08) A[1]:(5.33100954602e-14) A[2]:(1.0) A[3]:(1.91284632492e-33)\n",
      " state (14)  A[0]:(1.24131940282e-08) A[1]:(5.4387158839e-14) A[2]:(1.0) A[3]:(1.86025691172e-33)\n",
      " state (15)  A[0]:(1.16873470901e-08) A[1]:(5.51010415951e-14) A[2]:(1.0) A[3]:(1.84094225384e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 681000 finished after 31 . Running score: 0.0. Policy_loss: -92050.6111858, Value_loss: 0.978241279389. Times trained:               15948. Times reached goal: 65.               Steps done: 8559472.\n",
      " state (0)  A[0]:(0.994580805302) A[1]:(0.000951179827098) A[2]:(0.00201299111359) A[3]:(0.00245504523627)\n",
      " state (1)  A[0]:(0.000252517202171) A[1]:(4.62345051346e-05) A[2]:(0.000318407459417) A[3]:(0.999382853508)\n",
      " state (2)  A[0]:(0.00173174520023) A[1]:(8.78932914929e-05) A[2]:(0.000652242801152) A[3]:(0.997528135777)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.88888270145e-11) A[2]:(1.13957621117e-09) A[3]:(1.51273542304e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.40181936839e-11) A[2]:(5.97771776523e-10) A[3]:(5.24219253076e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.12070222494e-11) A[2]:(5.55319401574e-10) A[3]:(4.2202456807e-12)\n",
      " state (6)  A[0]:(1.0) A[1]:(1.80628203367e-11) A[2]:(5.27309418352e-10) A[3]:(2.92344035265e-12)\n",
      " state (7)  A[0]:(1.0) A[1]:(1.4281264539e-11) A[2]:(5.65503588401e-10) A[3]:(1.22782892794e-12)\n",
      " state (8)  A[0]:(1.0) A[1]:(1.13313993605e-11) A[2]:(1.59570379132e-09) A[3]:(6.81983088708e-14)\n",
      " state (9)  A[0]:(0.99998909235) A[1]:(3.16835412073e-11) A[2]:(1.0881367416e-05) A[3]:(3.04555778176e-19)\n",
      " state (10)  A[0]:(0.000546665047295) A[1]:(1.36804836123e-12) A[2]:(0.999453306198) A[3]:(1.06576125728e-29)\n",
      " state (11)  A[0]:(1.23755157233e-07) A[1]:(5.6519184813e-14) A[2]:(0.999999880791) A[3]:(5.9886026227e-33)\n",
      " state (12)  A[0]:(1.90022308999e-08) A[1]:(3.91073078868e-14) A[2]:(1.0) A[3]:(2.03073775539e-33)\n",
      " state (13)  A[0]:(1.36392603878e-08) A[1]:(3.85709801771e-14) A[2]:(1.0) A[3]:(1.74708307068e-33)\n",
      " state (14)  A[0]:(1.19148584332e-08) A[1]:(3.93075735806e-14) A[2]:(1.0) A[3]:(1.6913295573e-33)\n",
      " state (15)  A[0]:(1.11402389535e-08) A[1]:(3.98576435528e-14) A[2]:(1.0) A[3]:(1.67313070069e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 682000 finished after 8 . Running score: 0.0. Policy_loss: -92050.6111812, Value_loss: 0.979440942705. Times trained:               17998. Times reached goal: 0.               Steps done: 8577470.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994578719139) A[1]:(0.000915702315979) A[2]:(0.00212660222314) A[3]:(0.00237895967439)\n",
      " state (1)  A[0]:(0.000254595244769) A[1]:(4.84879128635e-05) A[2]:(0.00032944654231) A[3]:(0.99936747551)\n",
      " state (2)  A[0]:(0.8884152174) A[1]:(0.00017753506836) A[2]:(0.00178361102007) A[3]:(0.109623625875)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.81248704881e-11) A[2]:(7.25893900544e-10) A[3]:(5.40004195873e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.29084442388e-11) A[2]:(7.35576266564e-10) A[3]:(2.70750761221e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.74836933126e-11) A[2]:(1.374921621e-09) A[3]:(2.81934821626e-13)\n",
      " state (6)  A[0]:(0.999999642372) A[1]:(2.06629938854e-11) A[2]:(3.32509387135e-07) A[3]:(1.25619461125e-17)\n",
      " state (7)  A[0]:(0.00156027113553) A[1]:(1.68145456326e-12) A[2]:(0.998439729214) A[3]:(1.57068915688e-29)\n",
      " state (8)  A[0]:(2.29127678608e-07) A[1]:(3.45363047514e-14) A[2]:(0.999999761581) A[3]:(4.42414857879e-33)\n",
      " state (9)  A[0]:(1.2487903156e-08) A[1]:(2.77281656295e-14) A[2]:(1.0) A[3]:(1.45732775397e-33)\n",
      " state (10)  A[0]:(7.93582621839e-09) A[1]:(2.90300620456e-14) A[2]:(1.0) A[3]:(1.32303027945e-33)\n",
      " state (11)  A[0]:(6.79611122933e-09) A[1]:(3.01339662045e-14) A[2]:(1.0) A[3]:(1.30675886624e-33)\n",
      " state (12)  A[0]:(6.44575415265e-09) A[1]:(3.06075728185e-14) A[2]:(1.0) A[3]:(1.30112806463e-33)\n",
      " state (13)  A[0]:(6.33585628407e-09) A[1]:(3.07706503778e-14) A[2]:(1.0) A[3]:(1.29902530728e-33)\n",
      " state (14)  A[0]:(6.29946539377e-09) A[1]:(3.08248706508e-14) A[2]:(1.0) A[3]:(1.29829209267e-33)\n",
      " state (15)  A[0]:(6.28671825709e-09) A[1]:(3.08431021879e-14) A[2]:(1.0) A[3]:(1.29801484131e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 683000 finished after 15 . Running score: 0.14. Policy_loss: -92050.6111809, Value_loss: 1.00660758838. Times trained:               15644. Times reached goal: 109.               Steps done: 8593114.\n",
      " state (0)  A[0]:(0.995183706284) A[1]:(0.00123619369697) A[2]:(0.00186459335964) A[3]:(0.00171549792867)\n",
      " state (1)  A[0]:(0.000270033313427) A[1]:(5.28232594661e-05) A[2]:(0.000334503740305) A[3]:(0.999342620373)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.80419107274e-11) A[2]:(1.02178232542e-09) A[3]:(1.12623842544e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.11290186736e-11) A[2]:(5.56743706692e-10) A[3]:(2.53572271687e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.59133765193e-11) A[2]:(6.29166940769e-10) A[3]:(7.5918107521e-13)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.18117954653e-11) A[2]:(1.18598020293e-09) A[3]:(6.64128379569e-14)\n",
      " state (6)  A[0]:(0.999999940395) A[1]:(1.07913157577e-11) A[2]:(3.31877600956e-08) A[3]:(5.48155188059e-17)\n",
      " state (7)  A[0]:(0.90491771698) A[1]:(7.9468105707e-11) A[2]:(0.09508228302) A[3]:(9.52877807799e-25)\n",
      " state (8)  A[0]:(0.00110341340769) A[1]:(7.63384960713e-13) A[2]:(0.998896598816) A[3]:(8.01210460792e-31)\n",
      " state (9)  A[0]:(7.11524444341e-06) A[1]:(9.30652581908e-14) A[2]:(0.999992907047) A[3]:(1.44806930332e-32)\n",
      " state (10)  A[0]:(1.52266153464e-07) A[1]:(4.61113149075e-14) A[2]:(0.999999821186) A[3]:(2.62710754921e-33)\n",
      " state (11)  A[0]:(3.49321886972e-08) A[1]:(4.83871467471e-14) A[2]:(0.999999940395) A[3]:(1.79671960534e-33)\n",
      " state (12)  A[0]:(2.57913033153e-08) A[1]:(5.04069849012e-14) A[2]:(1.0) A[3]:(1.69016875662e-33)\n",
      " state (13)  A[0]:(2.3308679431e-08) A[1]:(5.17168468152e-14) A[2]:(1.0) A[3]:(1.67223732499e-33)\n",
      " state (14)  A[0]:(2.23991563075e-08) A[1]:(5.23355535612e-14) A[2]:(1.0) A[3]:(1.66803236129e-33)\n",
      " state (15)  A[0]:(2.20508802329e-08) A[1]:(5.25855028196e-14) A[2]:(1.0) A[3]:(1.666760256e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 684000 finished after 46 . Running score: 0.07. Policy_loss: -92050.6112034, Value_loss: 0.978903059526. Times trained:               16249. Times reached goal: 111.               Steps done: 8609363.\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.5235e-11,  7.6990e-09,  4.0344e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4192e-07,  1.8455e-14,  1.0000e+00,  1.9886e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.5235e-11,  7.6982e-09,  4.0353e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.5235e-11,  7.6967e-09,  4.0370e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.5235e-11,  7.6963e-09,  4.0376e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.5235e-11,  7.6962e-09,  4.0378e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.5236e-11,  7.6960e-09,  4.0385e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.5236e-11,  7.6959e-09,  4.0386e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.5236e-11,  7.6959e-09,  4.0387e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4195e-07,  1.8455e-14,  1.0000e+00,  1.9893e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.5236e-11,  7.6959e-09,  4.0389e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0012,  0.0016,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.5237e-11,  7.6960e-09,  4.0395e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.5237e-11,  7.6960e-09,  4.0396e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4194e-07,  1.8455e-14,  1.0000e+00,  1.9896e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 2.0520e-08,  2.1506e-14,  1.0000e+00,  1.3581e-33]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0512e-08,  2.4445e-14,  1.0000e+00,  1.2701e-33]])\n",
      "On state=13, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0477e-08,  2.4470e-14,  1.0000e+00,  1.2701e-33]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0477e-08,  2.4471e-14,  1.0000e+00,  1.2701e-33]])\n",
      "On state=14, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2437e-08,  2.3368e-14,  1.0000e+00,  1.2761e-33]])\n",
      "On state=10, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0477e-08,  2.4470e-14,  1.0000e+00,  1.2701e-33]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0477e-08,  2.4470e-14,  1.0000e+00,  1.2702e-33]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0477e-08,  2.4470e-14,  1.0000e+00,  1.2702e-33]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0477e-08,  2.4470e-14,  1.0000e+00,  1.2702e-33]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0477e-08,  2.4470e-14,  1.0000e+00,  1.2702e-33]])\n",
      "On state=14, selected action=2\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.995027303696) A[1]:(0.00123086257372) A[2]:(0.00164518586826) A[3]:(0.0020966748707)\n",
      " state (1)  A[0]:(0.00024638758623) A[1]:(4.7893361625e-05) A[2]:(0.000316377263516) A[3]:(0.999389350414)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.71731283308e-11) A[2]:(7.96012311621e-10) A[3]:(4.17393550278e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.00312023912e-11) A[2]:(1.31476751708e-09) A[3]:(4.78337584386e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.5237365189e-11) A[2]:(7.69615748908e-09) A[3]:(4.0400282505e-15)\n",
      " state (5)  A[0]:(0.999783396721) A[1]:(3.7065781e-11) A[2]:(0.000216601867578) A[3]:(1.7320515792e-21)\n",
      " state (6)  A[0]:(0.000708718784153) A[1]:(4.28497217092e-13) A[2]:(0.999291300774) A[3]:(5.96490173254e-31)\n",
      " state (7)  A[0]:(3.59886212209e-06) A[1]:(4.10924481833e-14) A[2]:(0.999996423721) A[3]:(9.23166774237e-33)\n",
      " state (8)  A[0]:(1.41936880027e-07) A[1]:(1.84553740833e-14) A[2]:(0.999999880791) A[3]:(1.98972824724e-33)\n",
      " state (9)  A[0]:(2.05190531233e-08) A[1]:(2.15055634433e-14) A[2]:(1.0) A[3]:(1.35819234606e-33)\n",
      " state (10)  A[0]:(1.24371295485e-08) A[1]:(2.33675451493e-14) A[2]:(1.0) A[3]:(1.27619564578e-33)\n",
      " state (11)  A[0]:(1.10340812043e-08) A[1]:(2.40867896239e-14) A[2]:(1.0) A[3]:(1.27008601389e-33)\n",
      " state (12)  A[0]:(1.06298063685e-08) A[1]:(2.43594698584e-14) A[2]:(1.0) A[3]:(1.27001814746e-33)\n",
      " state (13)  A[0]:(1.05116573224e-08) A[1]:(2.44452963189e-14) A[2]:(1.0) A[3]:(1.2701344112e-33)\n",
      " state (14)  A[0]:(1.04771089582e-08) A[1]:(2.44703939051e-14) A[2]:(1.0) A[3]:(1.27017325761e-33)\n",
      " state (15)  A[0]:(1.04668824719e-08) A[1]:(2.44775835207e-14) A[2]:(1.0) A[3]:(1.27018290034e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 685000 finished after 52 . Running score: 0.14. Policy_loss: -92050.6111858, Value_loss: 1.41391969841. Times trained:               15184. Times reached goal: 124.               Steps done: 8624547.\n",
      " state (0)  A[0]:(0.995183169842) A[1]:(0.00119664170779) A[2]:(0.00168034876697) A[3]:(0.00193984957878)\n",
      " state (1)  A[0]:(0.000231018027989) A[1]:(4.62943753519e-05) A[2]:(0.000305535591906) A[3]:(0.999417126179)\n",
      " state (2)  A[0]:(0.999999761581) A[1]:(8.84473116969e-09) A[2]:(1.7665338703e-07) A[3]:(2.72549147695e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.57614710814e-11) A[2]:(1.72667424803e-09) A[3]:(6.02675353199e-13)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(2.36010811588e-11) A[2]:(9.07228852043e-08) A[3]:(1.11245641228e-16)\n",
      " state (5)  A[0]:(0.00256227445789) A[1]:(1.98052208314e-12) A[2]:(0.99743771553) A[3]:(2.4793951154e-29)\n",
      " state (6)  A[0]:(8.56086217027e-07) A[1]:(2.64798355768e-14) A[2]:(0.999999165535) A[3]:(5.39549115228e-33)\n",
      " state (7)  A[0]:(9.52119663111e-08) A[1]:(1.32755253595e-14) A[2]:(0.999999880791) A[3]:(1.60433893457e-33)\n",
      " state (8)  A[0]:(2.6720067936e-08) A[1]:(1.42077774445e-14) A[2]:(1.0) A[3]:(1.22434073287e-33)\n",
      " state (9)  A[0]:(6.4741669803e-09) A[1]:(1.83785160078e-14) A[2]:(1.0) A[3]:(1.09636209151e-33)\n",
      " state (10)  A[0]:(4.21167989373e-09) A[1]:(1.99598147119e-14) A[2]:(1.0) A[3]:(1.09958717046e-33)\n",
      " state (11)  A[0]:(3.72232822343e-09) A[1]:(2.05288954916e-14) A[2]:(1.0) A[3]:(1.10846509154e-33)\n",
      " state (12)  A[0]:(3.59487684065e-09) A[1]:(2.07136757291e-14) A[2]:(1.0) A[3]:(1.11131869592e-33)\n",
      " state (13)  A[0]:(3.56075635644e-09) A[1]:(2.07668829507e-14) A[2]:(1.0) A[3]:(1.11210747099e-33)\n",
      " state (14)  A[0]:(3.55153173537e-09) A[1]:(2.07815027394e-14) A[2]:(1.0) A[3]:(1.11231961099e-33)\n",
      " state (15)  A[0]:(3.54901263933e-09) A[1]:(2.078542789e-14) A[2]:(1.0) A[3]:(1.11237902856e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 686000 finished after 18 . Running score: 0.11. Policy_loss: -92050.6111835, Value_loss: 1.20163039119. Times trained:               15515. Times reached goal: 101.               Steps done: 8640062.\n",
      " state (0)  A[0]:(0.996299922466) A[1]:(0.00122397067025) A[2]:(0.00107013899833) A[3]:(0.0014059683308)\n",
      " state (1)  A[0]:(0.000231920814258) A[1]:(4.49463550467e-05) A[2]:(0.000272910634521) A[3]:(0.999450206757)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.73503891809e-10) A[2]:(3.51488504968e-09) A[3]:(7.52472528731e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.4564706172e-11) A[2]:(1.0798225647e-09) A[3]:(8.2599519672e-13)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(2.42311917059e-11) A[2]:(4.41350174185e-08) A[3]:(1.94222272525e-16)\n",
      " state (5)  A[0]:(0.000131748689455) A[1]:(8.98571148045e-13) A[2]:(0.999868273735) A[3]:(3.28678151066e-30)\n",
      " state (6)  A[0]:(2.58657991026e-07) A[1]:(3.34864412946e-14) A[2]:(0.999999761581) A[3]:(3.9542399366e-33)\n",
      " state (7)  A[0]:(1.16539609962e-07) A[1]:(2.73176528056e-14) A[2]:(0.999999880791) A[3]:(2.44414203717e-33)\n",
      " state (8)  A[0]:(2.95220878854e-08) A[1]:(3.27396970483e-14) A[2]:(1.0) A[3]:(2.01833482062e-33)\n",
      " state (9)  A[0]:(7.74803154968e-09) A[1]:(4.20596954346e-14) A[2]:(1.0) A[3]:(1.89240815158e-33)\n",
      " state (10)  A[0]:(5.57929302758e-09) A[1]:(4.49265495953e-14) A[2]:(1.0) A[3]:(1.91205837636e-33)\n",
      " state (11)  A[0]:(5.14828402132e-09) A[1]:(4.57839061802e-14) A[2]:(1.0) A[3]:(1.92209195533e-33)\n",
      " state (12)  A[0]:(5.04551112002e-09) A[1]:(4.60180125342e-14) A[2]:(1.0) A[3]:(1.92482148995e-33)\n",
      " state (13)  A[0]:(5.01999641855e-09) A[1]:(4.60787922304e-14) A[2]:(1.0) A[3]:(1.92552641922e-33)\n",
      " state (14)  A[0]:(5.01356645088e-09) A[1]:(4.60940862573e-14) A[2]:(1.0) A[3]:(1.92570274337e-33)\n",
      " state (15)  A[0]:(5.01196018021e-09) A[1]:(4.60979555038e-14) A[2]:(1.0) A[3]:(1.92576151809e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 687000 finished after 22 . Running score: 0.12. Policy_loss: -92050.611181, Value_loss: 1.22807397626. Times trained:               15376. Times reached goal: 109.               Steps done: 8655438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996840000153) A[1]:(0.0011869991431) A[2]:(0.000901391671505) A[3]:(0.00107160373591)\n",
      " state (1)  A[0]:(0.000229696597671) A[1]:(4.45088480774e-05) A[2]:(0.000266807415755) A[3]:(0.999458968639)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.10225908798e-10) A[2]:(7.7763386841e-09) A[3]:(2.84578249854e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.46842858653e-11) A[2]:(9.23851994905e-10) A[3]:(1.22695798833e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.90315471715e-11) A[2]:(8.17849610257e-09) A[3]:(3.14401350549e-15)\n",
      " state (5)  A[0]:(0.180322512984) A[1]:(1.02414909886e-10) A[2]:(0.819677472115) A[3]:(1.60661273176e-25)\n",
      " state (6)  A[0]:(7.70994006416e-07) A[1]:(4.1445677863e-14) A[2]:(0.99999922514) A[3]:(5.96579766495e-33)\n",
      " state (7)  A[0]:(2.33536326277e-07) A[1]:(2.69621310548e-14) A[2]:(0.999999761581) A[3]:(2.49623039591e-33)\n",
      " state (8)  A[0]:(8.26430621714e-08) A[1]:(2.91493480015e-14) A[2]:(0.999999940395) A[3]:(2.01722618251e-33)\n",
      " state (9)  A[0]:(1.59394648591e-08) A[1]:(3.92892844452e-14) A[2]:(1.0) A[3]:(1.8183290486e-33)\n",
      " state (10)  A[0]:(9.50460954385e-09) A[1]:(4.32851183282e-14) A[2]:(1.0) A[3]:(1.82649046915e-33)\n",
      " state (11)  A[0]:(8.35846680758e-09) A[1]:(4.44803597438e-14) A[2]:(1.0) A[3]:(1.83953827277e-33)\n",
      " state (12)  A[0]:(8.08404632124e-09) A[1]:(4.482563409e-14) A[2]:(1.0) A[3]:(1.84352834141e-33)\n",
      " state (13)  A[0]:(8.01379584914e-09) A[1]:(4.4919265112e-14) A[2]:(1.0) A[3]:(1.84459767392e-33)\n",
      " state (14)  A[0]:(7.99550559094e-09) A[1]:(4.49440323554e-14) A[2]:(1.0) A[3]:(1.84487924156e-33)\n",
      " state (15)  A[0]:(7.99068811119e-09) A[1]:(4.49505477328e-14) A[2]:(1.0) A[3]:(1.84496354654e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 688000 finished after 54 . Running score: 0.11. Policy_loss: -92050.6111815, Value_loss: 1.41787308976. Times trained:               15054. Times reached goal: 107.               Steps done: 8670492.\n",
      " state (0)  A[0]:(0.996877193451) A[1]:(0.00170278875157) A[2]:(0.000778467219789) A[3]:(0.000641568505671)\n",
      " state (1)  A[0]:(0.000214424697333) A[1]:(4.45285586466e-05) A[2]:(0.000262740941253) A[3]:(0.999478280544)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.12314263129e-10) A[2]:(1.24580230576e-08) A[3]:(6.5064614807e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.31487441663e-11) A[2]:(9.47224632064e-10) A[3]:(3.01715531899e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.29413787914e-11) A[2]:(5.73634117984e-09) A[3]:(4.21958949444e-14)\n",
      " state (5)  A[0]:(0.510185718536) A[1]:(7.3555439517e-10) A[2]:(0.489814251661) A[3]:(5.52158205009e-23)\n",
      " state (6)  A[0]:(2.56173649404e-07) A[1]:(4.47089841006e-14) A[2]:(0.999999761581) A[3]:(6.6047486396e-33)\n",
      " state (7)  A[0]:(1.13944246038e-07) A[1]:(2.81344280419e-14) A[2]:(0.999999880791) A[3]:(2.39920986764e-33)\n",
      " state (8)  A[0]:(7.58632836551e-08) A[1]:(2.7143921266e-14) A[2]:(0.999999940395) A[3]:(1.97326526518e-33)\n",
      " state (9)  A[0]:(2.00804652906e-08) A[1]:(3.3723539369e-14) A[2]:(1.0) A[3]:(1.74979093212e-33)\n",
      " state (10)  A[0]:(5.86906034883e-09) A[1]:(4.24608637909e-14) A[2]:(1.0) A[3]:(1.68658772329e-33)\n",
      " state (11)  A[0]:(4.1512584481e-09) A[1]:(4.54178219285e-14) A[2]:(1.0) A[3]:(1.70581770884e-33)\n",
      " state (12)  A[0]:(3.77010156427e-09) A[1]:(4.63738070297e-14) A[2]:(1.0) A[3]:(1.71602540791e-33)\n",
      " state (13)  A[0]:(3.66814356667e-09) A[1]:(4.66664365842e-14) A[2]:(1.0) A[3]:(1.71924920117e-33)\n",
      " state (14)  A[0]:(3.63954222315e-09) A[1]:(4.67522308573e-14) A[2]:(1.0) A[3]:(1.72020686172e-33)\n",
      " state (15)  A[0]:(3.63140162385e-09) A[1]:(4.67767609315e-14) A[2]:(1.0) A[3]:(1.72048255188e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 689000 finished after 8 . Running score: 0.1. Policy_loss: -92050.6111801, Value_loss: 1.4137280297. Times trained:               15322. Times reached goal: 118.               Steps done: 8685814.\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9351e-11,  2.0704e-09,  2.2860e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9357e-11,  2.0709e-09,  2.2881e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9358e-11,  2.0711e-09,  2.2883e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9360e-11,  2.0714e-09,  2.2888e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9362e-11,  2.0718e-09,  2.2893e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9916,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9916,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9374e-11,  2.0742e-09,  2.2916e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9375e-11,  2.0744e-09,  2.2918e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9376e-11,  2.0746e-09,  2.2920e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.8387e-09,  4.2036e-14,  1.0000e+00,  3.2809e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9376e-11,  2.0747e-09,  2.2928e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9376e-11,  2.0747e-09,  2.2931e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9916,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9916,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9375e-11,  2.0748e-09,  2.2938e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9916,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9916,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9916,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9916,  0.0018,  0.0029,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9377e-11,  2.0754e-09,  2.2950e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.8329e-09,  4.1994e-14,  1.0000e+00,  3.2789e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.0038e-09,  5.6713e-14,  1.0000e+00,  2.9027e-33]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0474e-09,  6.4079e-14,  1.0000e+00,  2.9323e-33]])\n",
      "On state=13, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0470e-09,  6.4075e-14,  1.0000e+00,  2.9323e-33]])\n",
      "On state=13, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 1.0466e-09,  6.4072e-14,  1.0000e+00,  2.9322e-33]])\n",
      "On state=13, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0463e-09,  6.4069e-14,  1.0000e+00,  2.9321e-33]])\n",
      "On state=13, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.0006e-09,  5.6698e-14,  1.0000e+00,  2.9023e-33]])\n",
      "On state=9, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2355e-09,  6.1953e-14,  1.0000e+00,  2.9119e-33]])\n",
      "On state=10, selected action=2\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.1125e-07,  8.0876e-14,  1.0000e+00,  2.6126e-32]])\n",
      "On state=6, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2305e-09,  6.1949e-14,  1.0000e+00,  2.9116e-33]])\n",
      "On state=10, selected action=2\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.991601407528) A[1]:(0.00179380632471) A[2]:(0.00291633699089) A[3]:(0.0036884592846)\n",
      " state (1)  A[0]:(0.00014744985674) A[1]:(3.49698420905e-05) A[2]:(0.000218023400521) A[3]:(0.999599575996)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.09858313954e-10) A[2]:(7.30373139746e-09) A[3]:(3.17206066969e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.91421833781e-11) A[2]:(1.050299403e-09) A[3]:(1.14136746929e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.95557692104e-11) A[2]:(2.11976214359e-09) A[3]:(2.2144614701e-12)\n",
      " state (5)  A[0]:(0.927815258503) A[1]:(1.85611503944e-09) A[2]:(0.0721847340465) A[3]:(1.56075652995e-20)\n",
      " state (6)  A[0]:(1.06762591656e-07) A[1]:(7.93441308207e-14) A[2]:(0.999999880791) A[3]:(2.50285875141e-32)\n",
      " state (7)  A[0]:(2.99356699429e-08) A[1]:(3.91755414729e-14) A[2]:(0.999999940395) A[3]:(4.67254780019e-33)\n",
      " state (8)  A[0]:(9.73527658488e-09) A[1]:(4.19552156146e-14) A[2]:(1.0) A[3]:(3.2748809763e-33)\n",
      " state (9)  A[0]:(1.98513561145e-09) A[1]:(5.66871785515e-14) A[2]:(1.0) A[3]:(2.90156105188e-33)\n",
      " state (10)  A[0]:(1.22619026044e-09) A[1]:(6.1946921117e-14) A[2]:(1.0) A[3]:(2.9115176727e-33)\n",
      " state (11)  A[0]:(1.08218545236e-09) A[1]:(6.35038286604e-14) A[2]:(1.0) A[3]:(2.92701913711e-33)\n",
      " state (12)  A[0]:(1.04732678086e-09) A[1]:(6.39421038361e-14) A[2]:(1.0) A[3]:(2.9310861639e-33)\n",
      " state (13)  A[0]:(1.0382849025e-09) A[1]:(6.40588046475e-14) A[2]:(1.0) A[3]:(2.93186896967e-33)\n",
      " state (14)  A[0]:(1.03583996935e-09) A[1]:(6.40892368472e-14) A[2]:(1.0) A[3]:(2.93189137753e-33)\n",
      " state (15)  A[0]:(1.0351527413e-09) A[1]:(6.40973006009e-14) A[2]:(1.0) A[3]:(2.9318465618e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 690000 finished after 50 . Running score: 0.14. Policy_loss: -92050.6111883, Value_loss: 0.991319478007. Times trained:               15909. Times reached goal: 124.               Steps done: 8701723.\n",
      " state (0)  A[0]:(0.992532074451) A[1]:(0.00180096726399) A[2]:(0.00277700810693) A[3]:(0.00288997311145)\n",
      " state (1)  A[0]:(0.000139955169288) A[1]:(3.30260663759e-05) A[2]:(0.000216939093661) A[3]:(0.999610066414)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.01769004335e-09) A[2]:(1.86909012712e-08) A[3]:(1.37014666279e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.7708975659e-11) A[2]:(1.09174702612e-09) A[3]:(1.29354842809e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.62293953118e-11) A[2]:(1.35393596334e-09) A[3]:(6.93778628297e-12)\n",
      " state (5)  A[0]:(0.999999761581) A[1]:(9.33132807135e-11) A[2]:(2.11084923762e-07) A[3]:(2.384268574e-15)\n",
      " state (6)  A[0]:(2.0826423679e-06) A[1]:(3.31010040341e-13) A[2]:(0.999997913837) A[3]:(9.58765542135e-31)\n",
      " state (7)  A[0]:(2.31451178223e-08) A[1]:(3.26811738479e-14) A[2]:(1.0) A[3]:(4.60990901231e-33)\n",
      " state (8)  A[0]:(3.08349457079e-09) A[1]:(3.81384851543e-14) A[2]:(1.0) A[3]:(2.59594666333e-33)\n",
      " state (9)  A[0]:(1.17932130728e-09) A[1]:(4.45787138215e-14) A[2]:(1.0) A[3]:(2.43970840309e-33)\n",
      " state (10)  A[0]:(9.34720634227e-10) A[1]:(4.62286188063e-14) A[2]:(1.0) A[3]:(2.44112340441e-33)\n",
      " state (11)  A[0]:(8.81429707e-10) A[1]:(4.6669106432e-14) A[2]:(1.0) A[3]:(2.43844290995e-33)\n",
      " state (12)  A[0]:(8.67539318161e-10) A[1]:(4.67733694116e-14) A[2]:(1.0) A[3]:(2.43502222139e-33)\n",
      " state (13)  A[0]:(8.63241200744e-10) A[1]:(4.67955006884e-14) A[2]:(1.0) A[3]:(2.43260841721e-33)\n",
      " state (14)  A[0]:(8.61615945258e-10) A[1]:(4.67994275331e-14) A[2]:(1.0) A[3]:(2.43112398825e-33)\n",
      " state (15)  A[0]:(8.60866877783e-10) A[1]:(4.67994275331e-14) A[2]:(1.0) A[3]:(2.43023391862e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 691000 finished after 37 . Running score: 0.11. Policy_loss: -92050.6111827, Value_loss: 1.42293636033. Times trained:               15411. Times reached goal: 106.               Steps done: 8717134.\n",
      " state (0)  A[0]:(0.994383752346) A[1]:(0.00141540367622) A[2]:(0.00216441648081) A[3]:(0.0020364287775)\n",
      " state (1)  A[0]:(0.000136612638016) A[1]:(2.98457889585e-05) A[2]:(0.000205209376873) A[3]:(0.999628305435)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.16336962286e-09) A[2]:(2.29951577779e-08) A[3]:(1.87985071953e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.71580968719e-11) A[2]:(1.19822196609e-09) A[3]:(1.36340374754e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.77771815166e-11) A[2]:(1.87668747209e-09) A[3]:(4.95935671002e-12)\n",
      " state (5)  A[0]:(0.999967217445) A[1]:(3.09847258961e-10) A[2]:(3.27570123773e-05) A[3]:(2.27030985315e-17)\n",
      " state (6)  A[0]:(4.0594919426e-08) A[1]:(4.71019267155e-14) A[2]:(0.999999940395) A[3]:(2.28577433218e-32)\n",
      " state (7)  A[0]:(1.68433866854e-09) A[1]:(2.28678533087e-14) A[2]:(1.0) A[3]:(2.10205316094e-33)\n",
      " state (8)  A[0]:(3.34659550072e-10) A[1]:(2.85531858845e-14) A[2]:(1.0) A[3]:(1.75263673048e-33)\n",
      " state (9)  A[0]:(2.4289834033e-10) A[1]:(2.97848835981e-14) A[2]:(1.0) A[3]:(1.74129449569e-33)\n",
      " state (10)  A[0]:(2.28460583784e-10) A[1]:(3.0070350642e-14) A[2]:(1.0) A[3]:(1.73367233318e-33)\n",
      " state (11)  A[0]:(2.25535076726e-10) A[1]:(3.01224160632e-14) A[2]:(1.0) A[3]:(1.72883792899e-33)\n",
      " state (12)  A[0]:(2.24836274598e-10) A[1]:(3.01271865528e-14) A[2]:(1.0) A[3]:(1.72613612869e-33)\n",
      " state (13)  A[0]:(2.24629662093e-10) A[1]:(3.01248284131e-14) A[2]:(1.0) A[3]:(1.72466180164e-33)\n",
      " state (14)  A[0]:(2.24551696681e-10) A[1]:(3.01225312597e-14) A[2]:(1.0) A[3]:(1.72385934307e-33)\n",
      " state (15)  A[0]:(2.2451487891e-10) A[1]:(3.01211522901e-14) A[2]:(1.0) A[3]:(1.72341228788e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 692000 finished after 19 . Running score: 0.19. Policy_loss: -92050.611181, Value_loss: 1.62761601902. Times trained:               15410. Times reached goal: 126.               Steps done: 8732544.\n",
      " state (0)  A[0]:(0.994427323341) A[1]:(0.00234906608239) A[2]:(0.00208942242898) A[3]:(0.00113417196553)\n",
      " state (1)  A[0]:(0.000130230939249) A[1]:(3.1357682019e-05) A[2]:(0.000199693444301) A[3]:(0.999638736248)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.29183474818e-10) A[2]:(9.0317806567e-09) A[3]:(4.93124374668e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.09755648751e-11) A[2]:(1.21268850517e-09) A[3]:(1.83241859186e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.0722787909e-11) A[2]:(1.45538725516e-09) A[3]:(1.16153042046e-11)\n",
      " state (5)  A[0]:(0.999999761581) A[1]:(1.47499845671e-10) A[2]:(2.22272447559e-07) A[3]:(7.70053661272e-15)\n",
      " state (6)  A[0]:(1.30541026877e-08) A[1]:(9.5871912026e-14) A[2]:(1.0) A[3]:(5.04897569494e-32)\n",
      " state (7)  A[0]:(1.58300428321e-10) A[1]:(3.29788618831e-14) A[2]:(1.0) A[3]:(1.79043897576e-33)\n",
      " state (8)  A[0]:(9.88080728348e-11) A[1]:(3.26246869147e-14) A[2]:(1.0) A[3]:(1.59981677107e-33)\n",
      " state (9)  A[0]:(9.12828840294e-11) A[1]:(3.24421919721e-14) A[2]:(1.0) A[3]:(1.55042984529e-33)\n",
      " state (10)  A[0]:(8.94742960278e-11) A[1]:(3.23342698103e-14) A[2]:(1.0) A[3]:(1.52617149898e-33)\n",
      " state (11)  A[0]:(8.86324971749e-11) A[1]:(3.22949471527e-14) A[2]:(1.0) A[3]:(1.51296665666e-33)\n",
      " state (12)  A[0]:(8.81276301934e-11) A[1]:(3.22841661174e-14) A[2]:(1.0) A[3]:(1.50514925188e-33)\n",
      " state (13)  A[0]:(8.78083300515e-11) A[1]:(3.22826279055e-14) A[2]:(1.0) A[3]:(1.50027673596e-33)\n",
      " state (14)  A[0]:(8.76047429044e-11) A[1]:(3.2284105131e-14) A[2]:(1.0) A[3]:(1.49717802265e-33)\n",
      " state (15)  A[0]:(8.74731745371e-11) A[1]:(3.22858296901e-14) A[2]:(1.0) A[3]:(1.49519180454e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 693000 finished after 13 . Running score: 0.12. Policy_loss: -92050.6111804, Value_loss: 1.21222044301. Times trained:               15817. Times reached goal: 117.               Steps done: 8748361.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.991356134415) A[1]:(0.00143287831452) A[2]:(0.00646467879415) A[3]:(0.000746311037801)\n",
      " state (1)  A[0]:(0.000129213571199) A[1]:(2.83104873233e-05) A[2]:(0.000263530324446) A[3]:(0.999578952789)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.09955533462e-10) A[2]:(7.31028215739e-09) A[3]:(2.42874248491e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.03561714496e-11) A[2]:(1.61758972883e-09) A[3]:(1.94841677514e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.9843657807e-11) A[2]:(2.10080819407e-09) A[3]:(1.05779725257e-11)\n",
      " state (5)  A[0]:(0.99999833107) A[1]:(1.77442380234e-10) A[2]:(1.68357553321e-06) A[3]:(8.96300406873e-16)\n",
      " state (6)  A[0]:(1.00444363937e-09) A[1]:(2.33148783347e-14) A[2]:(1.0) A[3]:(4.36660592626e-33)\n",
      " state (7)  A[0]:(4.6162608458e-11) A[1]:(1.44530028001e-14) A[2]:(1.0) A[3]:(7.99171405679e-34)\n",
      " state (8)  A[0]:(3.198395368e-11) A[1]:(1.4429013133e-14) A[2]:(1.0) A[3]:(7.55106300982e-34)\n",
      " state (9)  A[0]:(2.99177072005e-11) A[1]:(1.43684011493e-14) A[2]:(1.0) A[3]:(7.38137810439e-34)\n",
      " state (10)  A[0]:(2.93304859567e-11) A[1]:(1.43220515065e-14) A[2]:(1.0) A[3]:(7.28320320393e-34)\n",
      " state (11)  A[0]:(2.90749906789e-11) A[1]:(1.42947329998e-14) A[2]:(1.0) A[3]:(7.22150949515e-34)\n",
      " state (12)  A[0]:(2.89394411213e-11) A[1]:(1.427890026e-14) A[2]:(1.0) A[3]:(7.18167538951e-34)\n",
      " state (13)  A[0]:(2.88607245741e-11) A[1]:(1.4269670989e-14) A[2]:(1.0) A[3]:(7.15580533024e-34)\n",
      " state (14)  A[0]:(2.88126536518e-11) A[1]:(1.42642008502e-14) A[2]:(1.0) A[3]:(7.13917346271e-34)\n",
      " state (15)  A[0]:(2.87822786438e-11) A[1]:(1.42609905953e-14) A[2]:(1.0) A[3]:(7.12839702641e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 694000 finished after 9 . Running score: 0.09. Policy_loss: -92050.6111843, Value_loss: 1.64652943882. Times trained:               15569. Times reached goal: 108.               Steps done: 8763930.\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3105e-11,  1.8886e-09,  1.3688e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.9797e-11,  2.1164e-14,  1.0000e+00,  1.2110e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3105e-11,  1.8886e-09,  1.3688e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3106e-11,  1.8886e-09,  1.3688e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3108e-11,  1.8887e-09,  1.3689e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3108e-11,  1.8887e-09,  1.3689e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3108e-11,  1.8887e-09,  1.3689e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3109e-11,  1.8887e-09,  1.3689e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3110e-11,  1.8888e-09,  1.3689e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3111e-11,  1.8888e-09,  1.3690e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3111e-11,  1.8888e-09,  1.3690e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0015,  0.0034,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3113e-11,  1.8889e-09,  1.3690e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3113e-11,  1.8889e-09,  1.3690e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3113e-11,  1.8889e-09,  1.3691e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3113e-11,  1.8889e-09,  1.3691e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3114e-11,  1.8889e-09,  1.3691e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3114e-11,  1.8889e-09,  1.3691e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.3114e-11,  1.8889e-09,  1.3691e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.9748e-11,  2.1163e-14,  1.0000e+00,  1.2113e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.7271e-11,  2.0995e-14,  1.0000e+00,  1.1839e-33]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.6032e-11,  2.0811e-14,  1.0000e+00,  1.1451e-33]])\n",
      "On state=13, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.6031e-11,  2.0811e-14,  1.0000e+00,  1.1451e-33]])\n",
      "On state=13, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.7267e-11,  2.0995e-14,  1.0000e+00,  1.1839e-33]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.6029e-11,  2.0811e-14,  1.0000e+00,  1.1451e-33]])\n",
      "On state=13, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.6028e-11,  2.0811e-14,  1.0000e+00,  1.1451e-33]])\n",
      "On state=13, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.7265e-11,  2.0995e-14,  1.0000e+00,  1.1839e-33]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 4.6027e-11,  2.0811e-14,  1.0000e+00,  1.1451e-33]])\n",
      "On state=13, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.7263e-11,  2.0995e-14,  1.0000e+00,  1.1839e-33]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994419455528) A[1]:(0.00148356170394) A[2]:(0.00342670199461) A[3]:(0.000670256617013)\n",
      " state (1)  A[0]:(0.000103410369775) A[1]:(2.33896844293e-05) A[2]:(0.000197240326088) A[3]:(0.999675989151)\n",
      " state (2)  A[0]:(1.0) A[1]:(8.15903233864e-10) A[2]:(1.5943083298e-08) A[3]:(1.21258669772e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.47302489387e-11) A[2]:(1.4902730161e-09) A[3]:(2.54041492703e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.31159430275e-11) A[2]:(1.88899806908e-09) A[3]:(1.36913423307e-11)\n",
      " state (5)  A[0]:(0.999995052814) A[1]:(2.5883997723e-10) A[2]:(4.93516245115e-06) A[3]:(4.75241652282e-16)\n",
      " state (6)  A[0]:(6.5185351561e-10) A[1]:(3.45091827564e-14) A[2]:(1.0) A[3]:(5.67331409931e-33)\n",
      " state (7)  A[0]:(6.51951409525e-11) A[1]:(2.14286274407e-14) A[2]:(1.0) A[3]:(1.2915346532e-33)\n",
      " state (8)  A[0]:(4.9739573571e-11) A[1]:(2.116298605e-14) A[2]:(1.0) A[3]:(1.21129595982e-33)\n",
      " state (9)  A[0]:(4.72635403037e-11) A[1]:(2.09952515036e-14) A[2]:(1.0) A[3]:(1.18388646213e-33)\n",
      " state (10)  A[0]:(4.65283922502e-11) A[1]:(2.09054219655e-14) A[2]:(1.0) A[3]:(1.167908647e-33)\n",
      " state (11)  A[0]:(4.62249891142e-11) A[1]:(2.08580253899e-14) A[2]:(1.0) A[3]:(1.15737089116e-33)\n",
      " state (12)  A[0]:(4.60879147346e-11) A[1]:(2.08294803796e-14) A[2]:(1.0) A[3]:(1.15012653988e-33)\n",
      " state (13)  A[0]:(4.60270363489e-11) A[1]:(2.08108168556e-14) A[2]:(1.0) A[3]:(1.14510956673e-33)\n",
      " state (14)  A[0]:(4.60008836578e-11) A[1]:(2.07980385166e-14) A[2]:(1.0) A[3]:(1.14165517454e-33)\n",
      " state (15)  A[0]:(4.59903538863e-11) A[1]:(2.07891158715e-14) A[2]:(1.0) A[3]:(1.13930583888e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 695000 finished after 46 . Running score: 0.07. Policy_loss: -92050.6114513, Value_loss: 1.22096431529. Times trained:               15687. Times reached goal: 111.               Steps done: 8779617.\n",
      " state (0)  A[0]:(0.995561361313) A[1]:(0.00180826697033) A[2]:(0.00227749487385) A[3]:(0.000352885748725)\n",
      " state (1)  A[0]:(0.000101004508906) A[1]:(2.36787927861e-05) A[2]:(0.000188435646123) A[3]:(0.999686896801)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.06144420715e-10) A[2]:(1.31682114102e-08) A[3]:(9.21947351795e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.24336216007e-11) A[2]:(1.55766644028e-09) A[3]:(2.89397707187e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.88509943503e-11) A[2]:(1.68315361648e-09) A[3]:(2.02892546514e-11)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(9.93862561693e-11) A[2]:(4.10379676907e-08) A[3]:(1.01331439171e-13)\n",
      " state (6)  A[0]:(4.97269114774e-09) A[1]:(8.9410866677e-14) A[2]:(1.0) A[3]:(5.03897047477e-32)\n",
      " state (7)  A[0]:(7.55394566343e-11) A[1]:(2.24648062332e-14) A[2]:(1.0) A[3]:(1.38434636068e-33)\n",
      " state (8)  A[0]:(5.18938989003e-11) A[1]:(2.19585617457e-14) A[2]:(1.0) A[3]:(1.25712921924e-33)\n",
      " state (9)  A[0]:(4.81612701109e-11) A[1]:(2.17981946919e-14) A[2]:(1.0) A[3]:(1.22577070338e-33)\n",
      " state (10)  A[0]:(4.70387097329e-11) A[1]:(2.17224496176e-14) A[2]:(1.0) A[3]:(1.20827778624e-33)\n",
      " state (11)  A[0]:(4.65726103516e-11) A[1]:(2.16863507675e-14) A[2]:(1.0) A[3]:(1.19660861709e-33)\n",
      " state (12)  A[0]:(4.63577821963e-11) A[1]:(2.16662574519e-14) A[2]:(1.0) A[3]:(1.18838401372e-33)\n",
      " state (13)  A[0]:(4.62572688487e-11) A[1]:(2.1653201286e-14) A[2]:(1.0) A[3]:(1.18255943921e-33)\n",
      " state (14)  A[0]:(4.62105319288e-11) A[1]:(2.16440770471e-14) A[2]:(1.0) A[3]:(1.17845246399e-33)\n",
      " state (15)  A[0]:(4.61892052384e-11) A[1]:(2.16375142359e-14) A[2]:(1.0) A[3]:(1.17559683924e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 696000 finished after 33 . Running score: 0.1. Policy_loss: -92050.611179, Value_loss: 1.42828170067. Times trained:               14857. Times reached goal: 112.               Steps done: 8794474.\n",
      " state (0)  A[0]:(0.995665192604) A[1]:(0.00225964048877) A[2]:(0.00158211158123) A[3]:(0.000493027502671)\n",
      " state (1)  A[0]:(8.36879989947e-05) A[1]:(1.93949126697e-05) A[2]:(0.000156913505634) A[3]:(0.999740004539)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(1.35754318897e-09) A[2]:(2.75227538538e-08) A[3]:(3.18535198218e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.03580249617e-11) A[2]:(1.72490655093e-09) A[3]:(3.64019127341e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.64291607833e-11) A[2]:(1.71646208358e-09) A[3]:(3.02581293354e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(6.49526127328e-11) A[2]:(2.47581510848e-09) A[3]:(1.31972575229e-11)\n",
      " state (6)  A[0]:(0.999998867512) A[1]:(1.11800624314e-10) A[2]:(1.12774262107e-06) A[3]:(8.26363331638e-16)\n",
      " state (7)  A[0]:(9.19052656201e-09) A[1]:(2.86946437748e-14) A[2]:(1.0) A[3]:(2.0430857358e-32)\n",
      " state (8)  A[0]:(9.98954044484e-11) A[1]:(8.70421051587e-15) A[2]:(1.0) A[3]:(9.12619743009e-34)\n",
      " state (9)  A[0]:(5.74567581013e-11) A[1]:(8.5464777345e-15) A[2]:(1.0) A[3]:(8.33788336778e-34)\n",
      " state (10)  A[0]:(4.89401401349e-11) A[1]:(8.54556548002e-15) A[2]:(1.0) A[3]:(8.22177850506e-34)\n",
      " state (11)  A[0]:(4.6150281513e-11) A[1]:(8.53201464693e-15) A[2]:(1.0) A[3]:(8.15132598582e-34)\n",
      " state (12)  A[0]:(4.48395556796e-11) A[1]:(8.51861627977e-15) A[2]:(1.0) A[3]:(8.09894485552e-34)\n",
      " state (13)  A[0]:(4.40698449955e-11) A[1]:(8.50916508614e-15) A[2]:(1.0) A[3]:(8.05949599979e-34)\n",
      " state (14)  A[0]:(4.35604018134e-11) A[1]:(8.50332394694e-15) A[2]:(1.0) A[3]:(8.02985150163e-34)\n",
      " state (15)  A[0]:(4.32014667096e-11) A[1]:(8.49998324899e-15) A[2]:(1.0) A[3]:(8.00758231217e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 697000 finished after 44 . Running score: 0.15. Policy_loss: -92050.6111801, Value_loss: 1.20923236382. Times trained:               16110. Times reached goal: 106.               Steps done: 8810584.\n",
      " state (0)  A[0]:(0.996270477772) A[1]:(0.00254834536463) A[2]:(0.000865489244461) A[3]:(0.000315710116411)\n",
      " state (1)  A[0]:(7.7294644143e-05) A[1]:(1.75669792952e-05) A[2]:(0.000141517579323) A[3]:(0.999763607979)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.16475892767e-10) A[2]:(5.19141440947e-09) A[3]:(2.25648250085e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.62829791054e-11) A[2]:(1.79073733619e-09) A[3]:(3.52227726153e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.44061054489e-11) A[2]:(2.27095653393e-09) A[3]:(1.96804794683e-11)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(7.59362780989e-11) A[2]:(9.80684546903e-08) A[3]:(2.34448657049e-14)\n",
      " state (6)  A[0]:(8.73797532108e-08) A[1]:(4.2814052813e-14) A[2]:(0.999999940395) A[3]:(1.52297424251e-31)\n",
      " state (7)  A[0]:(7.96589044727e-11) A[1]:(5.68573745756e-15) A[2]:(1.0) A[3]:(7.65816340215e-34)\n",
      " state (8)  A[0]:(3.7408656034e-11) A[1]:(5.8416800348e-15) A[2]:(1.0) A[3]:(7.15979190913e-34)\n",
      " state (9)  A[0]:(3.15441145426e-11) A[1]:(5.89079947541e-15) A[2]:(1.0) A[3]:(7.12339520611e-34)\n",
      " state (10)  A[0]:(3.01447616868e-11) A[1]:(5.89516042454e-15) A[2]:(1.0) A[3]:(7.08658891679e-34)\n",
      " state (11)  A[0]:(2.96249864917e-11) A[1]:(5.89244017823e-15) A[2]:(1.0) A[3]:(7.05944601754e-34)\n",
      " state (12)  A[0]:(2.93630050829e-11) A[1]:(5.89010279081e-15) A[2]:(1.0) A[3]:(7.04104999013e-34)\n",
      " state (13)  A[0]:(2.92071644648e-11) A[1]:(5.88875473788e-15) A[2]:(1.0) A[3]:(7.02891989862e-34)\n",
      " state (14)  A[0]:(2.91072270453e-11) A[1]:(5.88812581592e-15) A[2]:(1.0) A[3]:(7.02093434305e-34)\n",
      " state (15)  A[0]:(2.90409606085e-11) A[1]:(5.88790135218e-15) A[2]:(1.0) A[3]:(7.01574012739e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 698000 finished after 9 . Running score: 0.11. Policy_loss: -92050.6111778, Value_loss: 0.984585544415. Times trained:               15753. Times reached goal: 92.               Steps done: 8826337.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996413290501) A[1]:(0.00228113564663) A[2]:(0.000534623046406) A[3]:(0.00077094900189)\n",
      " state (1)  A[0]:(6.48899222142e-05) A[1]:(1.4740552615e-05) A[2]:(0.000114617563668) A[3]:(0.999805748463)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(1.37184097415e-09) A[2]:(2.63529660316e-08) A[3]:(4.0666274792e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.32887084354e-11) A[2]:(1.71829317441e-09) A[3]:(4.64601065953e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.0304505273e-11) A[2]:(2.0525383615e-09) A[3]:(2.75557528184e-11)\n",
      " state (5)  A[0]:(0.999999761581) A[1]:(1.12072726099e-10) A[2]:(2.11544801232e-07) A[3]:(1.58104849064e-14)\n",
      " state (6)  A[0]:(1.46882606078e-09) A[1]:(2.61819916875e-14) A[2]:(1.0) A[3]:(1.37812445071e-32)\n",
      " state (7)  A[0]:(5.1281586616e-11) A[1]:(1.14884848935e-14) A[2]:(1.0) A[3]:(1.16665298027e-33)\n",
      " state (8)  A[0]:(3.59882471046e-11) A[1]:(1.14006899285e-14) A[2]:(1.0) A[3]:(1.10582114761e-33)\n",
      " state (9)  A[0]:(3.35677770302e-11) A[1]:(1.13299338783e-14) A[2]:(1.0) A[3]:(1.09015649152e-33)\n",
      " state (10)  A[0]:(3.28540354644e-11) A[1]:(1.12853325114e-14) A[2]:(1.0) A[3]:(1.08099635996e-33)\n",
      " state (11)  A[0]:(3.25501500753e-11) A[1]:(1.12591879925e-14) A[2]:(1.0) A[3]:(1.07474649527e-33)\n",
      " state (12)  A[0]:(3.24039614585e-11) A[1]:(1.12420211757e-14) A[2]:(1.0) A[3]:(1.07022975006e-33)\n",
      " state (13)  A[0]:(3.23319739037e-11) A[1]:(1.12298484652e-14) A[2]:(1.0) A[3]:(1.06691981511e-33)\n",
      " state (14)  A[0]:(3.22970886146e-11) A[1]:(1.12211985648e-14) A[2]:(1.0) A[3]:(1.06454559203e-33)\n",
      " state (15)  A[0]:(3.22799668939e-11) A[1]:(1.12149508498e-14) A[2]:(1.0) A[3]:(1.0628333192e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 699000 finished after 33 . Running score: 0.11. Policy_loss: -92050.6111795, Value_loss: 1.19362198263. Times trained:               15507. Times reached goal: 127.               Steps done: 8841844.\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0017,  0.0007,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0017,  0.0007,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0017,  0.0007,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.6901e-11,  2.3627e-09,  3.7091e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9965,  0.0017,  0.0007,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.6902e-11,  2.3627e-09,  3.7091e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.6903e-11,  2.3627e-09,  3.7091e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.6904e-11,  2.3627e-09,  3.7091e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4969e-11,  9.3473e-15,  1.0000e+00,  1.0476e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.6905e-11,  2.3627e-09,  3.7091e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.6906e-11,  2.3627e-09,  3.7091e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4969e-11,  9.3474e-15,  1.0000e+00,  1.0476e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996539413929) A[1]:(0.00170285475906) A[2]:(0.000656531541608) A[3]:(0.00110121478792)\n",
      " state (1)  A[0]:(5.57364401175e-05) A[1]:(1.29228446895e-05) A[2]:(0.000107112136902) A[3]:(0.999824225903)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.57724183608e-10) A[2]:(8.13926526178e-09) A[3]:(5.74318037572e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.0047898865e-11) A[2]:(2.05969552525e-09) A[3]:(5.6787585051e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.69066130224e-11) A[2]:(2.36268249409e-09) A[3]:(3.70912814351e-11)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(9.40971953134e-11) A[2]:(8.29642985423e-08) A[3]:(5.73113130846e-14)\n",
      " state (6)  A[0]:(1.1215235407e-09) A[1]:(3.41570711602e-14) A[2]:(1.0) A[3]:(2.32361320146e-32)\n",
      " state (7)  A[0]:(3.52826830252e-11) A[1]:(9.57525345905e-15) A[2]:(1.0) A[3]:(1.10899792109e-33)\n",
      " state (8)  A[0]:(2.49686365333e-11) A[1]:(9.34744987712e-15) A[2]:(1.0) A[3]:(1.04763362623e-33)\n",
      " state (9)  A[0]:(2.32357154351e-11) A[1]:(9.26250941317e-15) A[2]:(1.0) A[3]:(1.03382597571e-33)\n",
      " state (10)  A[0]:(2.27738695957e-11) A[1]:(9.22319607299e-15) A[2]:(1.0) A[3]:(1.026485289e-33)\n",
      " state (11)  A[0]:(2.26138448245e-11) A[1]:(9.2039353908e-15) A[2]:(1.0) A[3]:(1.02195302359e-33)\n",
      " state (12)  A[0]:(2.25516896823e-11) A[1]:(9.19298748995e-15) A[2]:(1.0) A[3]:(1.01897893105e-33)\n",
      " state (13)  A[0]:(2.25267131337e-11) A[1]:(9.18604690198e-15) A[2]:(1.0) A[3]:(1.01697517236e-33)\n",
      " state (14)  A[0]:(2.25167471474e-11) A[1]:(9.18149240583e-15) A[2]:(1.0) A[3]:(1.0156338231e-33)\n",
      " state (15)  A[0]:(2.25127104458e-11) A[1]:(9.17841089996e-15) A[2]:(1.0) A[3]:(1.01473530461e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 700000 finished after 12 . Running score: 0.1. Policy_loss: -92050.6111762, Value_loss: 1.44104450849. Times trained:               15602. Times reached goal: 115.               Steps done: 8857446.\n",
      " state (0)  A[0]:(0.996072173119) A[1]:(0.00121511321049) A[2]:(0.00114131683949) A[3]:(0.00157141801901)\n",
      " state (1)  A[0]:(5.41197950952e-05) A[1]:(1.18086745715e-05) A[2]:(0.000104041137092) A[3]:(0.999830007553)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.56837706553e-10) A[2]:(1.08810134236e-08) A[3]:(9.39315625281e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.51829848378e-11) A[2]:(2.06337613662e-09) A[3]:(5.83681367128e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.18359538521e-11) A[2]:(2.36471597859e-09) A[3]:(3.78871760354e-11)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(8.39145974929e-11) A[2]:(8.45264338523e-08) A[3]:(5.73391974092e-14)\n",
      " state (6)  A[0]:(1.10322662117e-09) A[1]:(2.76648261997e-14) A[2]:(1.0) A[3]:(2.37546969393e-32)\n",
      " state (7)  A[0]:(3.52573872875e-11) A[1]:(7.79776244253e-15) A[2]:(1.0) A[3]:(1.15894366587e-33)\n",
      " state (8)  A[0]:(2.49399355334e-11) A[1]:(7.5991332164e-15) A[2]:(1.0) A[3]:(1.09363861803e-33)\n",
      " state (9)  A[0]:(2.31911295723e-11) A[1]:(7.52040573912e-15) A[2]:(1.0) A[3]:(1.07822054025e-33)\n",
      " state (10)  A[0]:(2.27159541177e-11) A[1]:(7.48189030398e-15) A[2]:(1.0) A[3]:(1.06994404983e-33)\n",
      " state (11)  A[0]:(2.25461420367e-11) A[1]:(7.4618816917e-15) A[2]:(1.0) A[3]:(1.06483808808e-33)\n",
      " state (12)  A[0]:(2.24771416757e-11) A[1]:(7.45002068933e-15) A[2]:(1.0) A[3]:(1.06148811285e-33)\n",
      " state (13)  A[0]:(2.24473217791e-11) A[1]:(7.44235165303e-15) A[2]:(1.0) A[3]:(1.05923906155e-33)\n",
      " state (14)  A[0]:(2.24339678778e-11) A[1]:(7.43729994853e-15) A[2]:(1.0) A[3]:(1.05772891865e-33)\n",
      " state (15)  A[0]:(2.24276361371e-11) A[1]:(7.43389657015e-15) A[2]:(1.0) A[3]:(1.05672065674e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 701000 finished after 36 . Running score: 0.17. Policy_loss: -92050.6111789, Value_loss: 1.20829770405. Times trained:               15997. Times reached goal: 117.               Steps done: 8873443.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997359335423) A[1]:(0.00138236943167) A[2]:(0.000755868095439) A[3]:(0.00050245563034)\n",
      " state (1)  A[0]:(5.18088927492e-05) A[1]:(1.10888877316e-05) A[2]:(0.000101319135865) A[3]:(0.999835789204)\n",
      " state (2)  A[0]:(0.999993979931) A[1]:(1.08409714983e-07) A[2]:(1.89168656561e-06) A[3]:(4.01132865591e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(9.56047255252e-11) A[2]:(2.44573450381e-09) A[3]:(9.12351930116e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.62475985128e-11) A[2]:(2.03328576198e-09) A[3]:(6.13569473007e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(7.36731439743e-11) A[2]:(2.44323539178e-09) A[3]:(3.69583773308e-11)\n",
      " state (6)  A[0]:(0.999999344349) A[1]:(1.86071033581e-10) A[2]:(6.45903583063e-07) A[3]:(9.20666876612e-15)\n",
      " state (7)  A[0]:(4.99421082267e-09) A[1]:(8.3781573801e-14) A[2]:(1.0) A[3]:(1.56776222297e-31)\n",
      " state (8)  A[0]:(7.19925924431e-11) A[1]:(1.02657352359e-14) A[2]:(1.0) A[3]:(1.40842287271e-33)\n",
      " state (9)  A[0]:(4.29004436142e-11) A[1]:(9.35868915729e-15) A[2]:(1.0) A[3]:(1.14727826197e-33)\n",
      " state (10)  A[0]:(3.59311157216e-11) A[1]:(9.21373640903e-15) A[2]:(1.0) A[3]:(1.1120311557e-33)\n",
      " state (11)  A[0]:(3.32044912399e-11) A[1]:(9.16448652535e-15) A[2]:(1.0) A[3]:(1.09954529347e-33)\n",
      " state (12)  A[0]:(3.18348437889e-11) A[1]:(9.13997762702e-15) A[2]:(1.0) A[3]:(1.09198778315e-33)\n",
      " state (13)  A[0]:(3.1040559012e-11) A[1]:(9.12325719664e-15) A[2]:(1.0) A[3]:(1.08617147384e-33)\n",
      " state (14)  A[0]:(3.05420723434e-11) A[1]:(9.10934637454e-15) A[2]:(1.0) A[3]:(1.08127673373e-33)\n",
      " state (15)  A[0]:(3.02163155608e-11) A[1]:(9.09712284208e-15) A[2]:(1.0) A[3]:(1.07706130078e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 702000 finished after 9 . Running score: 0.12. Policy_loss: -92050.6115369, Value_loss: 1.42610645966. Times trained:               15780. Times reached goal: 102.               Steps done: 8889223.\n",
      " state (0)  A[0]:(0.995159566402) A[1]:(0.00259838905185) A[2]:(0.000996242510155) A[3]:(0.0012458008714)\n",
      " state (1)  A[0]:(4.70996783406e-05) A[1]:(1.13743426482e-05) A[2]:(9.20942766243e-05) A[3]:(0.999849438667)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.97900010215e-10) A[2]:(6.45275344269e-09) A[3]:(3.59573898434e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.4861347116e-11) A[2]:(2.07059080992e-09) A[3]:(4.84403211987e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.27325222219e-11) A[2]:(2.51363818649e-09) A[3]:(2.84467935319e-11)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(9.89253609585e-11) A[2]:(6.41473931751e-08) A[3]:(4.75581725422e-14)\n",
      " state (6)  A[0]:(6.54840892622e-09) A[1]:(9.50211995676e-14) A[2]:(1.0) A[3]:(9.10826037812e-32)\n",
      " state (7)  A[0]:(4.38023506355e-11) A[1]:(1.38033632579e-14) A[2]:(1.0) A[3]:(8.6255580595e-34)\n",
      " state (8)  A[0]:(2.91367797944e-11) A[1]:(1.35372077182e-14) A[2]:(1.0) A[3]:(8.139642674e-34)\n",
      " state (9)  A[0]:(2.68268567083e-11) A[1]:(1.34291712069e-14) A[2]:(1.0) A[3]:(8.03475368042e-34)\n",
      " state (10)  A[0]:(2.62088632047e-11) A[1]:(1.33769939773e-14) A[2]:(1.0) A[3]:(7.97648589482e-34)\n",
      " state (11)  A[0]:(2.59813056486e-11) A[1]:(1.33526239924e-14) A[2]:(1.0) A[3]:(7.94029811756e-34)\n",
      " state (12)  A[0]:(2.58824316146e-11) A[1]:(1.33393111756e-14) A[2]:(1.0) A[3]:(7.91628405367e-34)\n",
      " state (13)  A[0]:(2.58357727573e-11) A[1]:(1.3331044981e-14) A[2]:(1.0) A[3]:(7.90005396643e-34)\n",
      " state (14)  A[0]:(2.5812180518e-11) A[1]:(1.33255536664e-14) A[2]:(1.0) A[3]:(7.88903187018e-34)\n",
      " state (15)  A[0]:(2.57997269382e-11) A[1]:(1.33218436621e-14) A[2]:(1.0) A[3]:(7.88157207283e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 703000 finished after 16 . Running score: 0.14. Policy_loss: -92050.6111778, Value_loss: 1.43173042544. Times trained:               16341. Times reached goal: 115.               Steps done: 8905564.\n",
      " state (0)  A[0]:(0.994861066341) A[1]:(0.00226545962505) A[2]:(0.00135854410473) A[3]:(0.00151495274622)\n",
      " state (1)  A[0]:(4.76015993627e-05) A[1]:(1.14930971904e-05) A[2]:(9.33463525143e-05) A[3]:(0.999847531319)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.57903742701e-10) A[2]:(9.29714083497e-09) A[3]:(6.68046562513e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.79628869743e-11) A[2]:(2.0275232604e-09) A[3]:(5.04764285925e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.48652259577e-11) A[2]:(2.28882024444e-09) A[3]:(3.33999390645e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(9.60283519369e-11) A[2]:(2.50496228205e-08) A[3]:(2.52700638192e-13)\n",
      " state (6)  A[0]:(5.19452711956e-08) A[1]:(4.01119269093e-13) A[2]:(0.999999940395) A[3]:(1.22306811183e-30)\n",
      " state (7)  A[0]:(5.5985070746e-11) A[1]:(1.89126092445e-14) A[2]:(1.0) A[3]:(9.60102000601e-34)\n",
      " state (8)  A[0]:(3.48852648158e-11) A[1]:(1.82273917775e-14) A[2]:(1.0) A[3]:(8.65746813935e-34)\n",
      " state (9)  A[0]:(3.14443956984e-11) A[1]:(1.809593057e-14) A[2]:(1.0) A[3]:(8.5233120915e-34)\n",
      " state (10)  A[0]:(3.04066979928e-11) A[1]:(1.80429249422e-14) A[2]:(1.0) A[3]:(8.45666064345e-34)\n",
      " state (11)  A[0]:(2.99694505324e-11) A[1]:(1.80198822579e-14) A[2]:(1.0) A[3]:(8.41103952402e-34)\n",
      " state (12)  A[0]:(2.9752960512e-11) A[1]:(1.80070666494e-14) A[2]:(1.0) A[3]:(8.37626693176e-34)\n",
      " state (13)  A[0]:(2.96380975318e-11) A[1]:(1.79980355841e-14) A[2]:(1.0) A[3]:(8.34915066481e-34)\n",
      " state (14)  A[0]:(2.95753005419e-11) A[1]:(1.79911712291e-14) A[2]:(1.0) A[3]:(8.32815615203e-34)\n",
      " state (15)  A[0]:(2.95401793304e-11) A[1]:(1.79856807616e-14) A[2]:(1.0) A[3]:(8.31203351232e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 704000 finished after 10 . Running score: 0.06. Policy_loss: -92050.6115114, Value_loss: 0.984693872534. Times trained:               15217. Times reached goal: 116.               Steps done: 8920781.\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0015,  0.0015,  0.0035]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.9239e-11,  3.0983e-09,  1.8107e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.9239e-11,  3.0983e-09,  1.8106e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.3234e-11,  1.2956e-14,  1.0000e+00,  9.2759e-34]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.99357187748) A[1]:(0.00152389379218) A[2]:(0.00145046971738) A[3]:(0.00345376203768)\n",
      " state (1)  A[0]:(4.34205285273e-05) A[1]:(1.01612031358e-05) A[2]:(8.44204114401e-05) A[3]:(0.999862015247)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.26376728526e-10) A[2]:(2.96606916805e-09) A[3]:(1.07416062156e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.04590491454e-11) A[2]:(2.08685846381e-09) A[3]:(4.60801813373e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.9238088313e-11) A[2]:(3.09841019508e-09) A[3]:(1.81036227676e-11)\n",
      " state (5)  A[0]:(0.999999046326) A[1]:(1.2595187393e-10) A[2]:(9.67566961663e-07) A[3]:(7.42408019945e-16)\n",
      " state (6)  A[0]:(6.76503020269e-10) A[1]:(2.32502649674e-14) A[2]:(1.0) A[3]:(4.55436029208e-33)\n",
      " state (7)  A[0]:(4.64260921373e-11) A[1]:(1.29201281225e-14) A[2]:(1.0) A[3]:(9.45801285304e-34)\n",
      " state (8)  A[0]:(3.32343354226e-11) A[1]:(1.2956082977e-14) A[2]:(1.0) A[3]:(9.27536123473e-34)\n",
      " state (9)  A[0]:(3.08675134064e-11) A[1]:(1.29589247726e-14) A[2]:(1.0) A[3]:(9.22426855644e-34)\n",
      " state (10)  A[0]:(3.01706472305e-11) A[1]:(1.29503265411e-14) A[2]:(1.0) A[3]:(9.18760507131e-34)\n",
      " state (11)  A[0]:(2.99089190914e-11) A[1]:(1.29407703154e-14) A[2]:(1.0) A[3]:(9.16016829848e-34)\n",
      " state (12)  A[0]:(2.97973173913e-11) A[1]:(1.29323813011e-14) A[2]:(1.0) A[3]:(9.1399240817e-34)\n",
      " state (13)  A[0]:(2.97456989595e-11) A[1]:(1.29256736472e-14) A[2]:(1.0) A[3]:(9.1252919321e-34)\n",
      " state (14)  A[0]:(2.97205177135e-11) A[1]:(1.29207193515e-14) A[2]:(1.0) A[3]:(9.11492462294e-34)\n",
      " state (15)  A[0]:(2.97074274902e-11) A[1]:(1.29171465665e-14) A[2]:(1.0) A[3]:(9.10769533268e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 705000 finished after 4 . Running score: 0.11. Policy_loss: -92050.611181, Value_loss: 1.20536238714. Times trained:               15529. Times reached goal: 110.               Steps done: 8936310.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995014548302) A[1]:(0.00140196527354) A[2]:(0.00110097788274) A[3]:(0.00248250714503)\n",
      " state (1)  A[0]:(4.29590763815e-05) A[1]:(9.90380340227e-06) A[2]:(8.54697718751e-05) A[3]:(0.999861657619)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.01157221488e-10) A[2]:(2.68576028084e-09) A[3]:(7.97454047241e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.9092829508e-11) A[2]:(2.34360308937e-09) A[3]:(4.47940781689e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.70990146726e-11) A[2]:(3.52420936878e-09) A[3]:(1.68404144796e-11)\n",
      " state (5)  A[0]:(0.999999761581) A[1]:(8.59624177396e-11) A[2]:(2.48221681431e-07) A[3]:(3.98967087123e-15)\n",
      " state (6)  A[0]:(1.12681597386e-08) A[1]:(3.94767768822e-14) A[2]:(1.0) A[3]:(3.60481799601e-32)\n",
      " state (7)  A[0]:(4.64147574541e-11) A[1]:(8.13252087472e-15) A[2]:(1.0) A[3]:(6.69492567853e-34)\n",
      " state (8)  A[0]:(2.75780995262e-11) A[1]:(8.15812413961e-15) A[2]:(1.0) A[3]:(6.50148201189e-34)\n",
      " state (9)  A[0]:(2.45829866879e-11) A[1]:(8.18046886876e-15) A[2]:(1.0) A[3]:(6.48008893306e-34)\n",
      " state (10)  A[0]:(2.37823337412e-11) A[1]:(8.17940753648e-15) A[2]:(1.0) A[3]:(6.4595549753e-34)\n",
      " state (11)  A[0]:(2.35062472959e-11) A[1]:(8.17320047904e-15) A[2]:(1.0) A[3]:(6.44277112002e-34)\n",
      " state (12)  A[0]:(2.33973725811e-11) A[1]:(8.16681130952e-15) A[2]:(1.0) A[3]:(6.43000369017e-34)\n",
      " state (13)  A[0]:(2.33506512737e-11) A[1]:(8.16161052722e-15) A[2]:(1.0) A[3]:(6.42078753895e-34)\n",
      " state (14)  A[0]:(2.33290158025e-11) A[1]:(8.1577201049e-15) A[2]:(1.0) A[3]:(6.41432461591e-34)\n",
      " state (15)  A[0]:(2.33185155213e-11) A[1]:(8.15498164738e-15) A[2]:(1.0) A[3]:(6.40987289023e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 706000 finished after 4 . Running score: 0.1. Policy_loss: -92050.611181, Value_loss: 1.19759250086. Times trained:               15213. Times reached goal: 103.               Steps done: 8951523.\n",
      " state (0)  A[0]:(0.992158353329) A[1]:(0.00164306955412) A[2]:(0.00227588275447) A[3]:(0.00392270181328)\n",
      " state (1)  A[0]:(3.78648692276e-05) A[1]:(9.23467541725e-06) A[2]:(7.93725266703e-05) A[3]:(0.999873518944)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.35138825078e-10) A[2]:(5.89032289611e-09) A[3]:(3.04819697261e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.31135368862e-11) A[2]:(2.42781261761e-09) A[3]:(5.17628162555e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.10321185152e-11) A[2]:(3.58451757165e-09) A[3]:(2.03686251893e-11)\n",
      " state (5)  A[0]:(0.999999761581) A[1]:(9.13987219242e-11) A[2]:(2.37149521354e-07) A[3]:(5.98153364472e-15)\n",
      " state (6)  A[0]:(1.09448103913e-08) A[1]:(4.0261224259e-14) A[2]:(1.0) A[3]:(4.5603949862e-32)\n",
      " state (7)  A[0]:(4.45328045273e-11) A[1]:(8.1364307788e-15) A[2]:(1.0) A[3]:(8.20467230719e-34)\n",
      " state (8)  A[0]:(2.63863739886e-11) A[1]:(8.14966397454e-15) A[2]:(1.0) A[3]:(7.96080039207e-34)\n",
      " state (9)  A[0]:(2.34904717206e-11) A[1]:(8.16774643389e-15) A[2]:(1.0) A[3]:(7.93218445147e-34)\n",
      " state (10)  A[0]:(2.27104515749e-11) A[1]:(8.16469372715e-15) A[2]:(1.0) A[3]:(7.90584235775e-34)\n",
      " state (11)  A[0]:(2.24375622249e-11) A[1]:(8.15725338974e-15) A[2]:(1.0) A[3]:(7.88433815797e-34)\n",
      " state (12)  A[0]:(2.23278028011e-11) A[1]:(8.15012984266e-15) A[2]:(1.0) A[3]:(7.86817419229e-34)\n",
      " state (13)  A[0]:(2.22792635035e-11) A[1]:(8.14441152323e-15) A[2]:(1.0) A[3]:(7.85647710515e-34)\n",
      " state (14)  A[0]:(2.22560737201e-11) A[1]:(8.14018736992e-15) A[2]:(1.0) A[3]:(7.84826976686e-34)\n",
      " state (15)  A[0]:(2.22441492309e-11) A[1]:(8.13720666098e-15) A[2]:(1.0) A[3]:(7.84264300601e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 707000 finished after 12 . Running score: 0.1. Policy_loss: -92050.6111817, Value_loss: 1.02271154984. Times trained:               15876. Times reached goal: 109.               Steps done: 8967399.\n",
      " state (0)  A[0]:(0.995475828648) A[1]:(0.00193993153516) A[2]:(0.000811907695606) A[3]:(0.00177230429836)\n",
      " state (1)  A[0]:(3.61923121091e-05) A[1]:(8.91705713002e-06) A[2]:(7.14506677468e-05) A[3]:(0.999883413315)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.25492965914e-10) A[2]:(7.52925544134e-09) A[3]:(5.17018650115e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.29249446888e-11) A[2]:(2.33418573359e-09) A[3]:(5.10937438192e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.89421167324e-11) A[2]:(3.61965279971e-09) A[3]:(1.67265472306e-11)\n",
      " state (5)  A[0]:(0.999999761581) A[1]:(7.45599068597e-11) A[2]:(2.20275552465e-07) A[3]:(3.1905647422e-15)\n",
      " state (6)  A[0]:(4.22191241967e-08) A[1]:(5.88716223123e-14) A[2]:(0.999999940395) A[3]:(1.24639605583e-31)\n",
      " state (7)  A[0]:(5.82204839894e-11) A[1]:(7.82413227225e-15) A[2]:(1.0) A[3]:(9.13525332837e-34)\n",
      " state (8)  A[0]:(3.30985794328e-11) A[1]:(7.8464592137e-15) A[2]:(1.0) A[3]:(8.83307230168e-34)\n",
      " state (9)  A[0]:(2.93541579932e-11) A[1]:(7.87753007628e-15) A[2]:(1.0) A[3]:(8.80844202161e-34)\n",
      " state (10)  A[0]:(2.83662798112e-11) A[1]:(7.88218960452e-15) A[2]:(1.0) A[3]:(8.7832763406e-34)\n",
      " state (11)  A[0]:(2.80247890866e-11) A[1]:(7.87966375227e-15) A[2]:(1.0) A[3]:(8.76159214325e-34)\n",
      " state (12)  A[0]:(2.7889772089e-11) A[1]:(7.87572759016e-15) A[2]:(1.0) A[3]:(8.74469624866e-34)\n",
      " state (13)  A[0]:(2.78317958957e-11) A[1]:(7.87203283245e-15) A[2]:(1.0) A[3]:(8.73209550024e-34)\n",
      " state (14)  A[0]:(2.78056380004e-11) A[1]:(7.86918087251e-15) A[2]:(1.0) A[3]:(8.72303960196e-34)\n",
      " state (15)  A[0]:(2.77928600273e-11) A[1]:(7.86698959828e-15) A[2]:(1.0) A[3]:(8.71665336156e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 708000 finished after 8 . Running score: 0.12. Policy_loss: -92050.6112121, Value_loss: 1.18981357581. Times trained:               16206. Times reached goal: 107.               Steps done: 8983605.\n",
      " state (0)  A[0]:(0.994962453842) A[1]:(0.00146292336285) A[2]:(0.000557992607355) A[3]:(0.00301664089784)\n",
      " state (1)  A[0]:(3.22758751281e-05) A[1]:(7.48108232074e-06) A[2]:(6.04985179962e-05) A[3]:(0.999899744987)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.51045153951e-10) A[2]:(1.25684307406e-08) A[3]:(1.45529555073e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.64126956154e-11) A[2]:(2.20904383674e-09) A[3]:(5.71871439092e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.14470566043e-11) A[2]:(3.46095285764e-09) A[3]:(1.79288528468e-11)\n",
      " state (5)  A[0]:(0.999999761581) A[1]:(6.22791956895e-11) A[2]:(2.58565250988e-07) A[3]:(2.40946738066e-15)\n",
      " state (6)  A[0]:(2.15503455081e-08) A[1]:(3.51619029574e-14) A[2]:(1.0) A[3]:(9.09867539719e-32)\n",
      " state (7)  A[0]:(6.43323172511e-11) A[1]:(6.87840431653e-15) A[2]:(1.0) A[3]:(1.52525185832e-33)\n",
      " state (8)  A[0]:(3.81257421467e-11) A[1]:(6.91608118906e-15) A[2]:(1.0) A[3]:(1.48844244659e-33)\n",
      " state (9)  A[0]:(3.42647230034e-11) A[1]:(6.94092551243e-15) A[2]:(1.0) A[3]:(1.48603699944e-33)\n",
      " state (10)  A[0]:(3.32602070885e-11) A[1]:(6.94410358005e-15) A[2]:(1.0) A[3]:(1.48293369435e-33)\n",
      " state (11)  A[0]:(3.29222864248e-11) A[1]:(6.94172002934e-15) A[2]:(1.0) A[3]:(1.48017569073e-33)\n",
      " state (12)  A[0]:(3.27940591349e-11) A[1]:(6.93833147403e-15) A[2]:(1.0) A[3]:(1.47799781194e-33)\n",
      " state (13)  A[0]:(3.27423713142e-11) A[1]:(6.93523514509e-15) A[2]:(1.0) A[3]:(1.47637489505e-33)\n",
      " state (14)  A[0]:(3.27209578876e-11) A[1]:(6.93288124053e-15) A[2]:(1.0) A[3]:(1.4752151964e-33)\n",
      " state (15)  A[0]:(3.27117222199e-11) A[1]:(6.93113592914e-15) A[2]:(1.0) A[3]:(1.47440502366e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 709000 finished after 4 . Running score: 0.12. Policy_loss: -92050.611213, Value_loss: 0.979419254981. Times trained:               15538. Times reached goal: 103.               Steps done: 8999143.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9961,  0.0015,  0.0008,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0015,  0.0008,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0015,  0.0008,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0015,  0.0008,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0015,  0.0008,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0015,  0.0008,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0015,  0.0008,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.9515e-11,  7.4322e-09,  4.9557e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0015,  0.0008,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0015,  0.0008,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0015,  0.0008,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0015,  0.0008,  0.0016]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.9520e-11,  7.4314e-09,  4.9585e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.5137e-11,  4.5903e-15,  1.0000e+00,  1.0203e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.2798e-11,  4.6164e-15,  1.0000e+00,  1.0206e-33]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996144711971) A[1]:(0.00146230857354) A[2]:(0.000793988583609) A[3]:(0.0015990199754)\n",
      " state (1)  A[0]:(2.97838250845e-05) A[1]:(7.120508144e-06) A[2]:(6.32831361145e-05) A[3]:(0.999899804592)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.78689885006e-10) A[2]:(1.01778372397e-08) A[3]:(8.1282619524e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.1659401463e-11) A[2]:(2.95179058973e-09) A[3]:(5.32621030613e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.95248025817e-11) A[2]:(7.42963823797e-09) A[3]:(4.96231831668e-12)\n",
      " state (5)  A[0]:(0.999970495701) A[1]:(9.23991647084e-11) A[2]:(2.95316804113e-05) A[3]:(4.67390795061e-18)\n",
      " state (6)  A[0]:(8.341548563e-10) A[1]:(6.09334749948e-15) A[2]:(1.0) A[3]:(3.33211726561e-33)\n",
      " state (7)  A[0]:(3.97356453297e-11) A[1]:(4.49940132285e-15) A[2]:(1.0) A[3]:(1.01661839146e-33)\n",
      " state (8)  A[0]:(2.51356851999e-11) A[1]:(4.59024518293e-15) A[2]:(1.0) A[3]:(1.0203637185e-33)\n",
      " state (9)  A[0]:(2.279755551e-11) A[1]:(4.61632193924e-15) A[2]:(1.0) A[3]:(1.02056612393e-33)\n",
      " state (10)  A[0]:(2.21808942741e-11) A[1]:(4.62217197229e-15) A[2]:(1.0) A[3]:(1.01932111011e-33)\n",
      " state (11)  A[0]:(2.19743060553e-11) A[1]:(4.62289491491e-15) A[2]:(1.0) A[3]:(1.01828731793e-33)\n",
      " state (12)  A[0]:(2.18960700266e-11) A[1]:(4.62245403426e-15) A[2]:(1.0) A[3]:(1.01759607215e-33)\n",
      " state (13)  A[0]:(2.18641441757e-11) A[1]:(4.62190727449e-15) A[2]:(1.0) A[3]:(1.01716141474e-33)\n",
      " state (14)  A[0]:(2.18502195504e-11) A[1]:(4.62146681736e-15) A[2]:(1.0) A[3]:(1.01689757136e-33)\n",
      " state (15)  A[0]:(2.18439693417e-11) A[1]:(4.62118475539e-15) A[2]:(1.0) A[3]:(1.01673465519e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 710000 finished after 15 . Running score: 0.06. Policy_loss: -92050.6112071, Value_loss: 1.42211965933. Times trained:               16122. Times reached goal: 120.               Steps done: 9015265.\n",
      " state (0)  A[0]:(0.993758916855) A[1]:(0.00113462517038) A[2]:(0.00335982511751) A[3]:(0.00174661190249)\n",
      " state (1)  A[0]:(2.87360689981e-05) A[1]:(6.61717240291e-06) A[2]:(7.72667699493e-05) A[3]:(0.999887406826)\n",
      " state (2)  A[0]:(0.998609781265) A[1]:(3.73804778064e-06) A[2]:(7.52515916247e-05) A[3]:(0.00131121766753)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.00321584373e-10) A[2]:(3.72015640515e-09) A[3]:(1.23935930962e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.1090405224e-11) A[2]:(3.22868354274e-09) A[3]:(8.04513747288e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(7.53968068534e-11) A[2]:(3.95720167745e-09) A[3]:(4.22735353667e-11)\n",
      " state (6)  A[0]:(0.999999940395) A[1]:(5.39004223199e-11) A[2]:(5.86557113991e-08) A[3]:(7.06908354553e-14)\n",
      " state (7)  A[0]:(0.000186707402463) A[1]:(7.85683670901e-12) A[2]:(0.999813318253) A[3]:(7.99741716072e-27)\n",
      " state (8)  A[0]:(2.50384185607e-10) A[1]:(6.51761105616e-15) A[2]:(1.0) A[3]:(2.37964964164e-33)\n",
      " state (9)  A[0]:(7.05282637847e-11) A[1]:(4.48673521567e-15) A[2]:(1.0) A[3]:(1.22009793295e-33)\n",
      " state (10)  A[0]:(4.79040175583e-11) A[1]:(4.30771353172e-15) A[2]:(1.0) A[3]:(1.13736039573e-33)\n",
      " state (11)  A[0]:(3.90884269397e-11) A[1]:(4.27417441514e-15) A[2]:(1.0) A[3]:(1.12301256882e-33)\n",
      " state (12)  A[0]:(3.47193142292e-11) A[1]:(4.26215840575e-15) A[2]:(1.0) A[3]:(1.11826742873e-33)\n",
      " state (13)  A[0]:(3.2273812095e-11) A[1]:(4.25314428113e-15) A[2]:(1.0) A[3]:(1.11478337368e-33)\n",
      " state (14)  A[0]:(3.07814988776e-11) A[1]:(4.24414921474e-15) A[2]:(1.0) A[3]:(1.11126781905e-33)\n",
      " state (15)  A[0]:(2.9812211727e-11) A[1]:(4.23518930023e-15) A[2]:(1.0) A[3]:(1.10769569376e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 711000 finished after 7 . Running score: 0.14. Policy_loss: -92050.6112102, Value_loss: 1.4332444461. Times trained:               14893. Times reached goal: 110.               Steps done: 9030158.\n",
      " state (0)  A[0]:(0.99715077877) A[1]:(0.00103228911757) A[2]:(0.00124282937031) A[3]:(0.000574073463213)\n",
      " state (1)  A[0]:(3.25363107549e-05) A[1]:(6.5581234594e-06) A[2]:(7.83555806265e-05) A[3]:(0.99988257885)\n",
      " state (2)  A[0]:(0.999999284744) A[1]:(1.46130609835e-08) A[2]:(4.35450090208e-07) A[3]:(2.94427081826e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.37591576758e-11) A[2]:(2.816961997e-09) A[3]:(6.86104367764e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.74694736244e-11) A[2]:(2.8617921366e-09) A[3]:(4.66509331165e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.0016126657e-11) A[2]:(4.14872580734e-09) A[3]:(1.35149122796e-11)\n",
      " state (6)  A[0]:(0.999999880791) A[1]:(2.50986488537e-11) A[2]:(9.20767178059e-08) A[3]:(2.3301749324e-15)\n",
      " state (7)  A[0]:(0.000914624135476) A[1]:(5.46378177077e-12) A[2]:(0.999085366726) A[3]:(3.09429650071e-27)\n",
      " state (8)  A[0]:(5.6777321733e-10) A[1]:(6.82771447683e-15) A[2]:(1.0) A[3]:(1.64999605351e-33)\n",
      " state (9)  A[0]:(1.72672931509e-10) A[1]:(5.22894275351e-15) A[2]:(1.0) A[3]:(1.0031275734e-33)\n",
      " state (10)  A[0]:(1.19079857086e-10) A[1]:(5.12766895323e-15) A[2]:(1.0) A[3]:(9.67137609797e-34)\n",
      " state (11)  A[0]:(9.928111111e-11) A[1]:(5.1119471747e-15) A[2]:(1.0) A[3]:(9.61729509264e-34)\n",
      " state (12)  A[0]:(9.01966973332e-11) A[1]:(5.10105771913e-15) A[2]:(1.0) A[3]:(9.58411400952e-34)\n",
      " state (13)  A[0]:(8.54825585317e-11) A[1]:(5.08867748557e-15) A[2]:(1.0) A[3]:(9.54806031208e-34)\n",
      " state (14)  A[0]:(8.28168297828e-11) A[1]:(5.07576573884e-15) A[2]:(1.0) A[3]:(9.51098081216e-34)\n",
      " state (15)  A[0]:(8.12108227888e-11) A[1]:(5.06358201693e-15) A[2]:(1.0) A[3]:(9.47628628006e-34)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 712000 finished after 9 . Running score: 0.1. Policy_loss: -92050.6112071, Value_loss: 1.43317527995. Times trained:               15622. Times reached goal: 101.               Steps done: 9045780.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996106564999) A[1]:(0.00139861635398) A[2]:(0.0014931561891) A[3]:(0.00100167212076)\n",
      " state (1)  A[0]:(3.10089635605e-05) A[1]:(6.70933786751e-06) A[2]:(7.58763490012e-05) A[3]:(0.999886393547)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(2.48358578148e-09) A[2]:(8.20791044021e-08) A[3]:(1.77877250707e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.5704719443e-11) A[2]:(3.19431414653e-09) A[3]:(5.1767850423e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.72884552297e-11) A[2]:(5.55122703361e-09) A[3]:(9.4451746771e-12)\n",
      " state (5)  A[0]:(0.999999344349) A[1]:(3.8002732905e-11) A[2]:(6.73110776006e-07) A[3]:(1.08092660343e-16)\n",
      " state (6)  A[0]:(1.75747274511e-08) A[1]:(2.04565453254e-14) A[2]:(1.0) A[3]:(2.3821303388e-32)\n",
      " state (7)  A[0]:(8.5783581627e-11) A[1]:(6.0309787695e-15) A[2]:(1.0) A[3]:(1.0359893445e-33)\n",
      " state (8)  A[0]:(4.74672627593e-11) A[1]:(6.09276601136e-15) A[2]:(1.0) A[3]:(1.02384162057e-33)\n",
      " state (9)  A[0]:(4.12815129303e-11) A[1]:(6.11549275236e-15) A[2]:(1.0) A[3]:(1.02224157072e-33)\n",
      " state (10)  A[0]:(3.9607241098e-11) A[1]:(6.11598276092e-15) A[2]:(1.0) A[3]:(1.01921219321e-33)\n",
      " state (11)  A[0]:(3.90052365407e-11) A[1]:(6.11213426673e-15) A[2]:(1.0) A[3]:(1.01672694101e-33)\n",
      " state (12)  A[0]:(3.87514048628e-11) A[1]:(6.10835819385e-15) A[2]:(1.0) A[3]:(1.01497536259e-33)\n",
      " state (13)  A[0]:(3.8632812227e-11) A[1]:(6.1055392682e-15) A[2]:(1.0) A[3]:(1.01381447009e-33)\n",
      " state (14)  A[0]:(3.85725097696e-11) A[1]:(6.10356017572e-15) A[2]:(1.0) A[3]:(1.01306444959e-33)\n",
      " state (15)  A[0]:(3.85403722825e-11) A[1]:(6.10230275531e-15) A[2]:(1.0) A[3]:(1.01259314982e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 713000 finished after 21 . Running score: 0.08. Policy_loss: -92050.6114742, Value_loss: 1.61332454805. Times trained:               15271. Times reached goal: 107.               Steps done: 9061051.\n",
      " state (0)  A[0]:(0.996417343616) A[1]:(0.00120376516134) A[2]:(0.000954606221057) A[3]:(0.00142425938975)\n",
      " state (1)  A[0]:(2.62411631411e-05) A[1]:(5.38061794941e-06) A[2]:(6.15849567112e-05) A[3]:(0.999906778336)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.58156337921e-10) A[2]:(1.28121664389e-08) A[3]:(1.19640874985e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.30437507754e-11) A[2]:(2.89368196071e-09) A[3]:(6.43367373265e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.54060131108e-11) A[2]:(5.0281516728e-09) A[3]:(1.25834759279e-11)\n",
      " state (5)  A[0]:(0.999998629093) A[1]:(4.05100328282e-11) A[2]:(1.39404539823e-06) A[3]:(5.68587541806e-17)\n",
      " state (6)  A[0]:(1.86473436692e-09) A[1]:(1.30098187472e-14) A[2]:(1.0) A[3]:(1.05832276371e-32)\n",
      " state (7)  A[0]:(6.4803870603e-11) A[1]:(6.04911205084e-15) A[2]:(1.0) A[3]:(1.8076783351e-33)\n",
      " state (8)  A[0]:(4.02179956005e-11) A[1]:(6.08561197108e-15) A[2]:(1.0) A[3]:(1.80318776301e-33)\n",
      " state (9)  A[0]:(3.58078983131e-11) A[1]:(6.10274533003e-15) A[2]:(1.0) A[3]:(1.80251387414e-33)\n",
      " state (10)  A[0]:(3.44964022625e-11) A[1]:(6.10444490163e-15) A[2]:(1.0) A[3]:(1.79876349615e-33)\n",
      " state (11)  A[0]:(3.39834202445e-11) A[1]:(6.10258227618e-15) A[2]:(1.0) A[3]:(1.79528109413e-33)\n",
      " state (12)  A[0]:(3.37475290768e-11) A[1]:(6.10034780327e-15) A[2]:(1.0) A[3]:(1.7926257626e-33)\n",
      " state (13)  A[0]:(3.36266049727e-11) A[1]:(6.09848644837e-15) A[2]:(1.0) A[3]:(1.79076682848e-33)\n",
      " state (14)  A[0]:(3.35598389356e-11) A[1]:(6.09718371169e-15) A[2]:(1.0) A[3]:(1.78951033523e-33)\n",
      " state (15)  A[0]:(3.35210054159e-11) A[1]:(6.09632312622e-15) A[2]:(1.0) A[3]:(1.78869134627e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 714000 finished after 3 . Running score: 0.18. Policy_loss: -92050.6112062, Value_loss: 1.42792459583. Times trained:               16044. Times reached goal: 119.               Steps done: 9077095.\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0018,  0.0010,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0018,  0.0010,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0018,  0.0010,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0018,  0.0010,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0018,  0.0010,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0018,  0.0010,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0018,  0.0010,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.9801e-11,  3.8846e-09,  2.2586e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9936,  0.0018,  0.0010,  0.0037]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.9805e-11,  3.8841e-09,  2.2597e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.2194e-11,  1.0978e-14,  1.0000e+00,  3.6365e-33]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.5867e-11,  1.0997e-14,  1.0000e+00,  3.6389e-33]])\n",
      "On state=9, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.3698e-11,  1.0999e-14,  1.0000e+00,  3.6344e-33]])\n",
      "On state=10, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.1762e-11,  1.0991e-14,  1.0000e+00,  3.6182e-33]])\n",
      "On state=14, selected action=2\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.993595898151) A[1]:(0.00175461219624) A[2]:(0.000961893878412) A[3]:(0.00368760433048)\n",
      " state (1)  A[0]:(1.94881959033e-05) A[1]:(4.16615966969e-06) A[2]:(4.64018266939e-05) A[3]:(0.999929964542)\n",
      " state (2)  A[0]:(1.0) A[1]:(6.14949924316e-10) A[2]:(1.79209145301e-08) A[3]:(3.46871797952e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.74568664816e-11) A[2]:(2.47317211155e-09) A[3]:(9.63288337985e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.98244856431e-11) A[2]:(3.88029253173e-09) A[3]:(2.26652741714e-11)\n",
      " state (5)  A[0]:(0.999998033047) A[1]:(5.28630368646e-11) A[2]:(1.97533358914e-06) A[3]:(3.74532326677e-17)\n",
      " state (6)  A[0]:(7.82039211167e-10) A[1]:(1.75392113891e-14) A[2]:(1.0) A[3]:(9.72706514284e-33)\n",
      " state (7)  A[0]:(9.1932871038e-11) A[1]:(1.0945298758e-14) A[2]:(1.0) A[3]:(3.63526587109e-33)\n",
      " state (8)  A[0]:(6.21780821275e-11) A[1]:(1.09795061836e-14) A[2]:(1.0) A[3]:(3.63687482898e-33)\n",
      " state (9)  A[0]:(5.58496200675e-11) A[1]:(1.09982018948e-14) A[2]:(1.0) A[3]:(3.63920634855e-33)\n",
      " state (10)  A[0]:(5.3681583484e-11) A[1]:(1.09998798671e-14) A[2]:(1.0) A[3]:(3.63460024741e-33)\n",
      " state (11)  A[0]:(5.27147284157e-11) A[1]:(1.09974463414e-14) A[2]:(1.0) A[3]:(3.62911362753e-33)\n",
      " state (12)  A[0]:(5.2211800855e-11) A[1]:(1.0994762094e-14) A[2]:(1.0) A[3]:(3.62446528205e-33)\n",
      " state (13)  A[0]:(5.19236008356e-11) A[1]:(1.09925809842e-14) A[2]:(1.0) A[3]:(3.62089986075e-33)\n",
      " state (14)  A[0]:(5.17481058004e-11) A[1]:(1.09911139231e-14) A[2]:(1.0) A[3]:(3.61833140559e-33)\n",
      " state (15)  A[0]:(5.16366949199e-11) A[1]:(1.09901491526e-14) A[2]:(1.0) A[3]:(3.61651012403e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 715000 finished after 14 . Running score: 0.07. Policy_loss: -92050.6112133, Value_loss: 1.41379776714. Times trained:               14970. Times reached goal: 105.               Steps done: 9092065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.99552243948) A[1]:(0.00173621112481) A[2]:(0.000801193935331) A[3]:(0.00194018147886)\n",
      " state (1)  A[0]:(1.90375249076e-05) A[1]:(3.94024709749e-06) A[2]:(4.55941008113e-05) A[3]:(0.999931454659)\n",
      " state (2)  A[0]:(1.0) A[1]:(8.0709006145e-10) A[2]:(2.06588950391e-08) A[3]:(5.57959101144e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.465569829e-11) A[2]:(2.26633090072e-09) A[3]:(1.34960889686e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.45974107538e-11) A[2]:(2.47630627115e-09) A[3]:(6.53891663038e-11)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(5.211409429e-11) A[2]:(5.3401336686e-08) A[3]:(3.30698772674e-14)\n",
      " state (6)  A[0]:(3.19821076289e-08) A[1]:(1.36069527499e-13) A[2]:(0.999999940395) A[3]:(4.53934352807e-31)\n",
      " state (7)  A[0]:(1.09837590412e-10) A[1]:(1.04565141607e-14) A[2]:(1.0) A[3]:(2.87590735704e-33)\n",
      " state (8)  A[0]:(6.36550256949e-11) A[1]:(9.8373864393e-15) A[2]:(1.0) A[3]:(2.59816577626e-33)\n",
      " state (9)  A[0]:(5.20612442045e-11) A[1]:(9.77189554589e-15) A[2]:(1.0) A[3]:(2.57647202802e-33)\n",
      " state (10)  A[0]:(4.69888503107e-11) A[1]:(9.75520984386e-15) A[2]:(1.0) A[3]:(2.57189604892e-33)\n",
      " state (11)  A[0]:(4.40227992948e-11) A[1]:(9.74832770116e-15) A[2]:(1.0) A[3]:(2.56938563379e-33)\n",
      " state (12)  A[0]:(4.20224757769e-11) A[1]:(9.74475915136e-15) A[2]:(1.0) A[3]:(2.56705393055e-33)\n",
      " state (13)  A[0]:(4.05852608465e-11) A[1]:(9.74289991404e-15) A[2]:(1.0) A[3]:(2.56454829087e-33)\n",
      " state (14)  A[0]:(3.95227947592e-11) A[1]:(9.74219433559e-15) A[2]:(1.0) A[3]:(2.56190820403e-33)\n",
      " state (15)  A[0]:(3.87240690902e-11) A[1]:(9.74230529691e-15) A[2]:(1.0) A[3]:(2.55917334295e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 716000 finished after 36 . Running score: 0.09. Policy_loss: -92050.6112018, Value_loss: 0.978416446562. Times trained:               16213. Times reached goal: 116.               Steps done: 9108278.\n",
      " state (0)  A[0]:(0.997073173523) A[1]:(0.00130037206691) A[2]:(0.000494579493534) A[3]:(0.00113186414819)\n",
      " state (1)  A[0]:(1.89283182408e-05) A[1]:(3.62583500646e-06) A[2]:(4.44705874543e-05) A[3]:(0.999933004379)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(1.3519952935e-09) A[2]:(3.94969283946e-08) A[3]:(1.48108965092e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.76541392353e-11) A[2]:(2.47399767339e-09) A[3]:(1.40373532242e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.77367841037e-11) A[2]:(2.78576894885e-09) A[3]:(6.26954044236e-11)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(4.41090254599e-11) A[2]:(1.03986337763e-07) A[3]:(1.22609103317e-14)\n",
      " state (6)  A[0]:(5.66369617871e-09) A[1]:(3.34808271602e-14) A[2]:(1.0) A[3]:(9.70131608602e-32)\n",
      " state (7)  A[0]:(7.7185972891e-11) A[1]:(4.65540403943e-15) A[2]:(1.0) A[3]:(2.02674456435e-33)\n",
      " state (8)  A[0]:(4.82854416173e-11) A[1]:(4.43991377545e-15) A[2]:(1.0) A[3]:(1.88635398833e-33)\n",
      " state (9)  A[0]:(3.96152485815e-11) A[1]:(4.40993516186e-15) A[2]:(1.0) A[3]:(1.87831177026e-33)\n",
      " state (10)  A[0]:(3.56266856294e-11) A[1]:(4.39824229554e-15) A[2]:(1.0) A[3]:(1.87748084269e-33)\n",
      " state (11)  A[0]:(3.32661086178e-11) A[1]:(4.39061476385e-15) A[2]:(1.0) A[3]:(1.87673605682e-33)\n",
      " state (12)  A[0]:(3.1671561207e-11) A[1]:(4.38470670904e-15) A[2]:(1.0) A[3]:(1.8755337465e-33)\n",
      " state (13)  A[0]:(3.05247771504e-11) A[1]:(4.37995866586e-15) A[2]:(1.0) A[3]:(1.87398888979e-33)\n",
      " state (14)  A[0]:(2.96743532524e-11) A[1]:(4.37621774485e-15) A[2]:(1.0) A[3]:(1.8722741374e-33)\n",
      " state (15)  A[0]:(2.90313242196e-11) A[1]:(4.37333063305e-15) A[2]:(1.0) A[3]:(1.87047508003e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 717000 finished after 10 . Running score: 0.08. Policy_loss: -92050.6111988, Value_loss: 1.193784142. Times trained:               15741. Times reached goal: 96.               Steps done: 9124019.\n",
      " state (0)  A[0]:(0.991109967232) A[1]:(0.00208937074058) A[2]:(0.00584711367264) A[3]:(0.000953528855462)\n",
      " state (1)  A[0]:(1.70548082679e-05) A[1]:(3.64392008123e-06) A[2]:(6.03957923886e-05) A[3]:(0.999918878078)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(1.16642462356e-09) A[2]:(4.67148453254e-08) A[3]:(1.08420854517e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.10204395293e-11) A[2]:(3.5978195978e-09) A[3]:(1.30831789846e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.20145185204e-11) A[2]:(4.92147389508e-09) A[3]:(4.16110791657e-11)\n",
      " state (5)  A[0]:(0.999996602535) A[1]:(6.81105657985e-11) A[2]:(3.4107290503e-06) A[3]:(9.32070422947e-17)\n",
      " state (6)  A[0]:(2.52980247861e-10) A[1]:(6.99966174161e-15) A[2]:(1.0) A[3]:(4.86654624506e-33)\n",
      " state (7)  A[0]:(4.89472351539e-11) A[1]:(4.16110737514e-15) A[2]:(1.0) A[3]:(1.8069337329e-33)\n",
      " state (8)  A[0]:(3.38015102019e-11) A[1]:(4.08495403099e-15) A[2]:(1.0) A[3]:(1.77253876819e-33)\n",
      " state (9)  A[0]:(2.90383567886e-11) A[1]:(4.0627778614e-15) A[2]:(1.0) A[3]:(1.76810862386e-33)\n",
      " state (10)  A[0]:(2.68022565947e-11) A[1]:(4.04845156964e-15) A[2]:(1.0) A[3]:(1.76375690704e-33)\n",
      " state (11)  A[0]:(2.5477129087e-11) A[1]:(4.03720890133e-15) A[2]:(1.0) A[3]:(1.7589728287e-33)\n",
      " state (12)  A[0]:(2.45953674094e-11) A[1]:(4.02808678001e-15) A[2]:(1.0) A[3]:(1.75420197468e-33)\n",
      " state (13)  A[0]:(2.39737067004e-11) A[1]:(4.02084041314e-15) A[2]:(1.0) A[3]:(1.74977752414e-33)\n",
      " state (14)  A[0]:(2.35207339716e-11) A[1]:(4.01520002075e-15) A[2]:(1.0) A[3]:(1.74581721021e-33)\n",
      " state (15)  A[0]:(2.31829919844e-11) A[1]:(4.01086744722e-15) A[2]:(1.0) A[3]:(1.74238403202e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 718000 finished after 14 . Running score: 0.15. Policy_loss: -92050.6112003, Value_loss: 0.987279085164. Times trained:               15767. Times reached goal: 129.               Steps done: 9139786.\n",
      " state (0)  A[0]:(0.993601262569) A[1]:(0.00164452102035) A[2]:(0.00381855340675) A[3]:(0.000935642921831)\n",
      " state (1)  A[0]:(1.56190326379e-05) A[1]:(3.31806131726e-06) A[2]:(5.35248618689e-05) A[3]:(0.999927520752)\n",
      " state (2)  A[0]:(0.999999582767) A[1]:(6.3226228697e-09) A[2]:(2.25177060997e-07) A[3]:(1.61997888881e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.81539277739e-11) A[2]:(3.60468144223e-09) A[3]:(1.96060431779e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.84388309913e-11) A[2]:(3.34769545418e-09) A[3]:(1.44823944503e-10)\n",
      " state (5)  A[0]:(1.0) A[1]:(6.22501147851e-11) A[2]:(4.33630686913e-09) A[3]:(6.09915035144e-11)\n",
      " state (6)  A[0]:(0.999998927116) A[1]:(7.8354898958e-11) A[2]:(1.0873170595e-06) A[3]:(1.06330417561e-15)\n",
      " state (7)  A[0]:(3.45223192255e-08) A[1]:(5.77686363372e-14) A[2]:(0.999999940395) A[3]:(3.72076734818e-31)\n",
      " state (8)  A[0]:(1.83349974092e-10) A[1]:(4.65158730897e-15) A[2]:(1.0) A[3]:(3.29842649549e-33)\n",
      " state (9)  A[0]:(1.03500999438e-10) A[1]:(3.8812870764e-15) A[2]:(1.0) A[3]:(2.44112340441e-33)\n",
      " state (10)  A[0]:(8.41373706817e-11) A[1]:(3.74283488249e-15) A[2]:(1.0) A[3]:(2.31274380927e-33)\n",
      " state (11)  A[0]:(7.54257212243e-11) A[1]:(3.69296920586e-15) A[2]:(1.0) A[3]:(2.27352050152e-33)\n",
      " state (12)  A[0]:(7.07269590117e-11) A[1]:(3.66666332713e-15) A[2]:(1.0) A[3]:(2.25575896555e-33)\n",
      " state (13)  A[0]:(6.79112807656e-11) A[1]:(3.64961085983e-15) A[2]:(1.0) A[3]:(2.24545649225e-33)\n",
      " state (14)  A[0]:(6.60910909334e-11) A[1]:(3.63737970408e-15) A[2]:(1.0) A[3]:(2.23856313623e-33)\n",
      " state (15)  A[0]:(6.48534559389e-11) A[1]:(3.62834398011e-15) A[2]:(1.0) A[3]:(2.23370099773e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 719000 finished after 18 . Running score: 0.13. Policy_loss: -92050.6124828, Value_loss: 1.43148272148. Times trained:               15300. Times reached goal: 119.               Steps done: 9155086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9953,  0.0010,  0.0018,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0010,  0.0018,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0010,  0.0018,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0010,  0.0018,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0010,  0.0018,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.4714e-11,  2.9974e-09,  1.7002e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.4714e-11,  2.9974e-09,  1.7002e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0010,  0.0018,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0010,  0.0018,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.4716e-11,  2.9974e-09,  1.7003e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0010,  0.0018,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0010,  0.0018,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0010,  0.0018,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.4717e-11,  2.9975e-09,  1.7004e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.4718e-11,  2.9975e-09,  1.7004e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.4718e-11,  2.9975e-09,  1.7004e-10]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.9810e-10,  7.4644e-15,  1.0000e+00,  1.6532e-32]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5324e-10,  4.2693e-15,  1.0000e+00,  6.3624e-33]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.7858e-11,  3.6528e-15,  1.0000e+00,  5.0187e-33]])\n",
      "On state=13, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.4526e-11,  3.6331e-15,  1.0000e+00,  4.9918e-33]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.4526e-11,  3.6331e-15,  1.0000e+00,  4.9919e-33]])\n",
      "On state=14, selected action=2\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.995305240154) A[1]:(0.0010228769388) A[2]:(0.00178368587513) A[3]:(0.00188818911556)\n",
      " state (1)  A[0]:(1.34442670969e-05) A[1]:(2.66317965725e-06) A[2]:(4.18478884967e-05) A[3]:(0.999942064285)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(7.89802556689e-10) A[2]:(3.04510834326e-08) A[3]:(8.30866664359e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.23284210302e-11) A[2]:(3.22747673032e-09) A[3]:(2.13496886836e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.47200001924e-11) A[2]:(2.99758262656e-09) A[3]:(1.70049391235e-10)\n",
      " state (5)  A[0]:(1.0) A[1]:(6.09323216882e-11) A[2]:(3.40529471288e-09) A[3]:(1.06951586476e-10)\n",
      " state (6)  A[0]:(0.999999940395) A[1]:(6.77658415493e-11) A[2]:(7.28819031792e-08) A[3]:(1.79524093074e-13)\n",
      " state (7)  A[0]:(8.46486364026e-05) A[1]:(6.71061860594e-12) A[2]:(0.999915361404) A[3]:(1.48639837249e-26)\n",
      " state (8)  A[0]:(5.98069427316e-10) A[1]:(7.46430166483e-15) A[2]:(1.0) A[3]:(1.65345593746e-32)\n",
      " state (9)  A[0]:(1.53232343481e-10) A[1]:(4.26926924734e-15) A[2]:(1.0) A[3]:(6.36322201939e-33)\n",
      " state (10)  A[0]:(1.08369688223e-10) A[1]:(3.8579640242e-15) A[2]:(1.0) A[3]:(5.41120200162e-33)\n",
      " state (11)  A[0]:(9.17452641636e-11) A[1]:(3.73607344199e-15) A[2]:(1.0) A[3]:(5.16155381747e-33)\n",
      " state (12)  A[0]:(8.304049115e-11) A[1]:(3.68278405818e-15) A[2]:(1.0) A[3]:(5.06581531273e-33)\n",
      " state (13)  A[0]:(7.78568112758e-11) A[1]:(3.65284229053e-15) A[2]:(1.0) A[3]:(5.01915039105e-33)\n",
      " state (14)  A[0]:(7.45253719847e-11) A[1]:(3.6330945644e-15) A[2]:(1.0) A[3]:(4.99207361337e-33)\n",
      " state (15)  A[0]:(7.22581022794e-11) A[1]:(3.61879241308e-15) A[2]:(1.0) A[3]:(4.974281037e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 720000 finished after 21 . Running score: 0.11. Policy_loss: -92050.6112026, Value_loss: 1.41819902063. Times trained:               15191. Times reached goal: 105.               Steps done: 9170277.\n",
      " state (0)  A[0]:(0.997131943703) A[1]:(0.00103719183244) A[2]:(0.000721996941138) A[3]:(0.00110888830386)\n",
      " state (1)  A[0]:(1.50054065671e-05) A[1]:(2.5987430945e-06) A[2]:(3.80998535547e-05) A[3]:(0.999944269657)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.77224990028e-10) A[2]:(2.00368504011e-08) A[3]:(6.34141761324e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.92320594014e-11) A[2]:(1.96942417929e-09) A[3]:(1.50741932825e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.04232584905e-11) A[2]:(1.66320801576e-09) A[3]:(1.08503914187e-10)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.71896090312e-11) A[2]:(1.6596967134e-09) A[3]:(8.17471368375e-11)\n",
      " state (6)  A[0]:(1.0) A[1]:(3.63440527673e-11) A[2]:(4.2016345958e-09) A[3]:(8.06101539685e-12)\n",
      " state (7)  A[0]:(0.999929189682) A[1]:(1.74989772672e-10) A[2]:(7.08065272192e-05) A[3]:(2.64642457634e-18)\n",
      " state (8)  A[0]:(6.64870640321e-06) A[1]:(2.74004614571e-13) A[2]:(0.99999332428) A[3]:(1.0440222478e-29)\n",
      " state (9)  A[0]:(1.61945514776e-08) A[1]:(1.15047555494e-14) A[2]:(1.0) A[3]:(2.68818895401e-32)\n",
      " state (10)  A[0]:(4.0351424424e-09) A[1]:(6.37537982476e-15) A[2]:(1.0) A[3]:(1.00176289793e-32)\n",
      " state (11)  A[0]:(2.5437871809e-09) A[1]:(5.38608515291e-15) A[2]:(1.0) A[3]:(7.64202909933e-33)\n",
      " state (12)  A[0]:(2.04406069848e-09) A[1]:(5.04982831945e-15) A[2]:(1.0) A[3]:(6.91011121521e-33)\n",
      " state (13)  A[0]:(1.79732440042e-09) A[1]:(4.90189825037e-15) A[2]:(1.0) A[3]:(6.60666543007e-33)\n",
      " state (14)  A[0]:(1.65036861954e-09) A[1]:(4.82448409768e-15) A[2]:(1.0) A[3]:(6.45677447198e-33)\n",
      " state (15)  A[0]:(1.55260915236e-09) A[1]:(4.7778540869e-15) A[2]:(1.0) A[3]:(6.3721124301e-33)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 721000 finished after 29 . Running score: 0.12. Policy_loss: -92050.6124949, Value_loss: 1.19365890953. Times trained:               16155. Times reached goal: 93.               Steps done: 9186432.\n",
      " state (0)  A[0]:(0.994074225426) A[1]:(0.00310615892522) A[2]:(0.000761985720601) A[3]:(0.00205764826387)\n",
      " state (1)  A[0]:(1.42184499055e-05) A[1]:(2.78669403997e-06) A[2]:(3.49222718796e-05) A[3]:(0.999948084354)\n",
      " state (2)  A[0]:(0.999941647053) A[1]:(2.33844332342e-07) A[2]:(5.05860862177e-06) A[3]:(5.30650322617e-05)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.44010525727e-10) A[2]:(4.36601821363e-09) A[3]:(6.69487965066e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.33645089124e-11) A[2]:(2.31272778706e-09) A[3]:(2.40269582008e-10)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.20455588349e-11) A[2]:(1.69041503018e-09) A[3]:(1.39741482275e-10)\n",
      " state (6)  A[0]:(1.0) A[1]:(4.43603730138e-11) A[2]:(1.5636466566e-09) A[3]:(8.98011942585e-11)\n",
      " state (7)  A[0]:(1.0) A[1]:(1.3155947165e-10) A[2]:(9.54790824181e-09) A[3]:(1.56594025941e-11)\n",
      " state (8)  A[0]:(0.999997854233) A[1]:(1.73180830787e-10) A[2]:(2.12359304896e-06) A[3]:(3.9718184872e-16)\n",
      " state (9)  A[0]:(0.777721524239) A[1]:(6.69209576643e-10) A[2]:(0.222278445959) A[3]:(7.98482887807e-22)\n",
      " state (10)  A[0]:(0.000280034728348) A[1]:(2.56316322333e-12) A[2]:(0.999719977379) A[3]:(5.51237935346e-28)\n",
      " state (11)  A[0]:(4.94286814501e-06) A[1]:(2.1237536469e-13) A[2]:(0.999995052814) A[3]:(3.4959754946e-30)\n",
      " state (12)  A[0]:(5.47718741473e-07) A[1]:(6.84623188761e-14) A[2]:(0.999999463558) A[3]:(4.21244983386e-31)\n",
      " state (13)  A[0]:(1.27732775468e-07) A[1]:(3.48257054163e-14) A[2]:(0.999999880791) A[3]:(1.27668372573e-31)\n",
      " state (14)  A[0]:(4.6184339908e-08) A[1]:(2.2275531639e-14) A[2]:(0.999999940395) A[3]:(5.96205092364e-32)\n",
      " state (15)  A[0]:(2.30467946949e-08) A[1]:(1.65718404711e-14) A[2]:(1.0) A[3]:(3.64291664954e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 722000 finished after 15 . Running score: 0.0. Policy_loss: -92050.6123303, Value_loss: 0.978825161433. Times trained:               16745. Times reached goal: 48.               Steps done: 9203177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996021807194) A[1]:(0.00226417346857) A[2]:(0.000512220547535) A[3]:(0.00120178959332)\n",
      " state (1)  A[0]:(1.48346880451e-05) A[1]:(2.68038434115e-06) A[2]:(3.46037668351e-05) A[3]:(0.99994790554)\n",
      " state (2)  A[0]:(0.999996840954) A[1]:(2.89794375163e-08) A[2]:(7.19352783562e-07) A[3]:(2.41394241129e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.16300352366e-11) A[2]:(2.39263142632e-09) A[3]:(2.61951321745e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.62784463529e-11) A[2]:(1.59669932831e-09) A[3]:(1.33716135386e-10)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.85399316627e-11) A[2]:(1.38537759042e-09) A[3]:(9.50501691244e-11)\n",
      " state (6)  A[0]:(1.0) A[1]:(3.93054616632e-11) A[2]:(1.84290405159e-09) A[3]:(4.56514340219e-11)\n",
      " state (7)  A[0]:(0.999999880791) A[1]:(1.37146877188e-10) A[2]:(9.0059984359e-08) A[3]:(8.94487256617e-14)\n",
      " state (8)  A[0]:(0.999954104424) A[1]:(9.97271015768e-11) A[2]:(4.58772956335e-05) A[3]:(6.26143515358e-19)\n",
      " state (9)  A[0]:(0.514028668404) A[1]:(2.43549097556e-10) A[2]:(0.485971301794) A[3]:(2.0098735405e-23)\n",
      " state (10)  A[0]:(0.00376444077119) A[1]:(5.83915598165e-12) A[2]:(0.99623554945) A[3]:(2.13129936548e-27)\n",
      " state (11)  A[0]:(0.000431498832768) A[1]:(1.18334096862e-12) A[2]:(0.999568521976) A[3]:(7.17424191353e-29)\n",
      " state (12)  A[0]:(0.000152237422299) A[1]:(5.82953390421e-13) A[2]:(0.999847769737) A[3]:(1.68422573637e-29)\n",
      " state (13)  A[0]:(7.50028411858e-05) A[1]:(3.73777126871e-13) A[2]:(0.999925017357) A[3]:(6.99225191575e-30)\n",
      " state (14)  A[0]:(3.98892443627e-05) A[1]:(2.59076505908e-13) A[2]:(0.999960124493) A[3]:(3.47193146295e-30)\n",
      " state (15)  A[0]:(2.08983838093e-05) A[1]:(1.83240982067e-13) A[2]:(0.99997907877) A[3]:(1.83632661796e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 723000 finished after 32 . Running score: 0.01. Policy_loss: -92050.6111833, Value_loss: 0.978830689151. Times trained:               16801. Times reached goal: 83.               Steps done: 9219978.\n",
      " state (0)  A[0]:(0.995729923248) A[1]:(0.00259184255265) A[2]:(0.000603892607614) A[3]:(0.00107434974052)\n",
      " state (1)  A[0]:(1.43639763337e-05) A[1]:(2.67461314252e-06) A[2]:(3.6126912164e-05) A[3]:(0.999946832657)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.29931801429e-10) A[2]:(4.82096629284e-09) A[3]:(5.64358226818e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.15203045556e-11) A[2]:(1.74935510522e-09) A[3]:(9.12186645663e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.73364464346e-11) A[2]:(1.89770843484e-09) A[3]:(5.40815656147e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.17064950683e-11) A[2]:(1.26815109525e-08) A[3]:(3.79328350415e-13)\n",
      " state (6)  A[0]:(0.99968034029) A[1]:(1.16294612829e-10) A[2]:(0.000319642102113) A[3]:(5.48186162871e-20)\n",
      " state (7)  A[0]:(0.00207649450749) A[1]:(4.03670066124e-12) A[2]:(0.997923493385) A[3]:(1.06201410479e-27)\n",
      " state (8)  A[0]:(3.74857190764e-05) A[1]:(2.46727931055e-13) A[2]:(0.999962508678) A[3]:(3.57164460503e-30)\n",
      " state (9)  A[0]:(8.78288847161e-06) A[1]:(1.14064899019e-13) A[2]:(0.999991238117) A[3]:(8.98166175243e-31)\n",
      " state (10)  A[0]:(3.11723169943e-06) A[1]:(7.15333147534e-14) A[2]:(0.999996900558) A[3]:(4.18738171153e-31)\n",
      " state (11)  A[0]:(1.1031596614e-06) A[1]:(4.66400430375e-14) A[2]:(0.999998867512) A[3]:(2.15303498277e-31)\n",
      " state (12)  A[0]:(3.72132262783e-07) A[1]:(3.05409655356e-14) A[2]:(0.999999642372) A[3]:(1.13586103064e-31)\n",
      " state (13)  A[0]:(1.34715222089e-07) A[1]:(2.08319825149e-14) A[2]:(0.999999880791) A[3]:(6.43668497965e-32)\n",
      " state (14)  A[0]:(5.9417349263e-08) A[1]:(1.54027876201e-14) A[2]:(0.999999940395) A[3]:(4.13184060262e-32)\n",
      " state (15)  A[0]:(3.31027933953e-08) A[1]:(1.24466307757e-14) A[2]:(0.999999940395) A[3]:(3.02989223135e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 724000 finished after 15 . Running score: 0.13. Policy_loss: -92050.6111789, Value_loss: 1.19937098403. Times trained:               16772. Times reached goal: 69.               Steps done: 9236750.\n",
      "action_dist \n",
      "tensor([[ 0.9928,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7514e-11,  1.8587e-09,  6.3198e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7515e-11,  1.8585e-09,  6.3212e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.0180e-05,  3.2094e-13,  9.9995e-01,  6.5006e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7516e-11,  1.8581e-09,  6.3241e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9928,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9928,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9928,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9928,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7519e-11,  1.8573e-09,  6.3304e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7519e-11,  1.8572e-09,  6.3314e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9928,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7520e-11,  1.8570e-09,  6.3332e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.1051e-05,  3.2418e-13,  9.9995e-01,  6.6246e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7521e-11,  1.8568e-09,  6.3352e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7522e-11,  1.8566e-09,  6.3363e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7522e-11,  1.8565e-09,  6.3373e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7522e-11,  1.8564e-09,  6.3382e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.1448e-05,  3.2564e-13,  9.9995e-01,  6.6805e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2391e-05,  1.4868e-13,  9.9999e-01,  1.5991e-30]])\n",
      "On state=9, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.7452e-06,  9.5601e-14,  1.0000e+00,  7.6967e-31]])\n",
      "On state=10, selected action=2\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9998e-01,  8.4069e-11,  1.8787e-05,  1.6004e-18]])\n",
      "On state=6, selected action=0\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.7529e-06,  9.5671e-14,  1.0000e+00,  7.7065e-31]])\n",
      "On state=10, selected action=2\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9998e-01,  8.3854e-11,  1.8429e-05,  1.6404e-18]])\n",
      "On state=6, selected action=0\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.9382e-11,  3.8151e-09,  4.0063e-10]])\n",
      "On state=2, selected action=0\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.9404e-11,  3.8157e-09,  4.0081e-10]])\n",
      "On state=2, selected action=0\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3385e-05,  2.5034e-06,  3.4410e-05,  9.9995e-01]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.9446e-11,  3.8167e-09,  4.0111e-10]])\n",
      "On state=2, selected action=0\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3385e-05,  2.5036e-06,  3.4409e-05,  9.9995e-01]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9928,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7531e-11,  1.8530e-09,  6.3765e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9927,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9927,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7517e-11,  1.8524e-09,  6.3846e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9927,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9927,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9927,  0.0029,  0.0011,  0.0032]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7505e-11,  1.8519e-09,  6.3925e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.2305e-05,  3.2855e-13,  9.9995e-01,  6.8270e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2546e-05,  1.4941e-13,  9.9999e-01,  1.6201e-30]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.3240e-07,  2.8957e-14,  1.0000e+00,  1.2528e-31]])\n",
      "On state=13, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 9.7716e-08,  2.1021e-14,  1.0000e+00,  7.8254e-32]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.7148e-08,  2.0975e-14,  1.0000e+00,  7.8023e-32]])\n",
      "On state=14, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.6640e-08,  2.0933e-14,  1.0000e+00,  7.7816e-32]])\n",
      "On state=14, selected action=2\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.992741048336) A[1]:(0.00291037582792) A[2]:(0.00114504259545) A[3]:(0.00320351473056)\n",
      " state (1)  A[0]:(1.33643980007e-05) A[1]:(2.4956730158e-06) A[2]:(3.43610117852e-05) A[3]:(0.999949753284)\n",
      " state (2)  A[0]:(1.0) A[1]:(9.93280804829e-11) A[2]:(3.81845843833e-09) A[3]:(4.02124944454e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.0612596619e-11) A[2]:(1.74077441351e-09) A[3]:(9.46958553238e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.74925854252e-11) A[2]:(1.85271242792e-09) A[3]:(6.39443983874e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.84349124108e-11) A[2]:(4.7088937194e-09) A[3]:(3.38702265019e-12)\n",
      " state (6)  A[0]:(0.999982535839) A[1]:(8.31215998809e-11) A[2]:(1.74801061803e-05) A[3]:(1.76311396896e-18)\n",
      " state (7)  A[0]:(0.0062219821848) A[1]:(1.03220158115e-11) A[2]:(0.993777990341) A[3]:(8.91529149068e-27)\n",
      " state (8)  A[0]:(5.15235187777e-05) A[1]:(3.25732198376e-13) A[2]:(0.999948501587) A[3]:(6.73337082004e-30)\n",
      " state (9)  A[0]:(1.23193740365e-05) A[1]:(1.48127387092e-13) A[2]:(0.999987661839) A[3]:(1.59959804243e-30)\n",
      " state (10)  A[0]:(4.70085569759e-06) A[1]:(9.51041613626e-14) A[2]:(0.999995291233) A[3]:(7.6806956048e-31)\n",
      " state (11)  A[0]:(1.78411960405e-06) A[1]:(6.35008064469e-14) A[2]:(0.999998211861) A[3]:(4.08003885879e-31)\n",
      " state (12)  A[0]:(6.27498025096e-07) A[1]:(4.21408513553e-14) A[2]:(0.999999344349) A[3]:(2.19158461471e-31)\n",
      " state (13)  A[0]:(2.2642974784e-07) A[1]:(2.86697443943e-14) A[2]:(0.999999761581) A[3]:(1.23592275382e-31)\n",
      " state (14)  A[0]:(9.60971959785e-08) A[1]:(2.08884406489e-14) A[2]:(0.999999880791) A[3]:(7.7592677631e-32)\n",
      " state (15)  A[0]:(5.09823543382e-08) A[1]:(1.65783524604e-14) A[2]:(0.999999940395) A[3]:(5.53933668987e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 725000 finished after 44 . Running score: 0.12. Policy_loss: -92050.6111771, Value_loss: 1.40983519801. Times trained:               15718. Times reached goal: 108.               Steps done: 9252468.\n",
      " state (0)  A[0]:(0.991941869259) A[1]:(0.0021215446759) A[2]:(0.00267890328541) A[3]:(0.00325770583004)\n",
      " state (1)  A[0]:(1.29242671392e-05) A[1]:(2.35316974795e-06) A[2]:(3.5199445847e-05) A[3]:(0.999949514866)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.50667478493e-10) A[2]:(5.97111160516e-09) A[3]:(8.26447188462e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.02371018449e-11) A[2]:(1.7950906317e-09) A[3]:(1.06253963961e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.73154424027e-11) A[2]:(1.8197597873e-09) A[3]:(7.99391108863e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.36750627383e-11) A[2]:(3.32978178363e-09) A[3]:(1.40466934959e-11)\n",
      " state (6)  A[0]:(0.999997615814) A[1]:(8.57783080677e-11) A[2]:(2.36423124989e-06) A[3]:(5.30895965316e-17)\n",
      " state (7)  A[0]:(0.551320850849) A[1]:(2.96585728199e-10) A[2]:(0.448679149151) A[3]:(4.29329911493e-23)\n",
      " state (8)  A[0]:(0.000251424498856) A[1]:(1.01689902159e-12) A[2]:(0.999748587608) A[3]:(7.52603649015e-29)\n",
      " state (9)  A[0]:(1.71482297446e-05) A[1]:(1.95179579421e-13) A[2]:(0.999982833862) A[3]:(3.08745754153e-30)\n",
      " state (10)  A[0]:(3.85381872547e-06) A[1]:(9.43641323935e-14) A[2]:(0.999996125698) A[3]:(8.79220685771e-31)\n",
      " state (11)  A[0]:(1.11994222607e-06) A[1]:(5.55412954398e-14) A[2]:(0.999998867512) A[3]:(3.73946052954e-31)\n",
      " state (12)  A[0]:(3.647633946e-07) A[1]:(3.54321267964e-14) A[2]:(0.999999642372) A[3]:(1.86170622278e-31)\n",
      " state (13)  A[0]:(1.43413444675e-07) A[1]:(2.47159030167e-14) A[2]:(0.999999880791) A[3]:(1.07804880787e-31)\n",
      " state (14)  A[0]:(7.23180448858e-08) A[1]:(1.90823665056e-14) A[2]:(0.999999940395) A[3]:(7.32137260753e-32)\n",
      " state (15)  A[0]:(4.58415385651e-08) A[1]:(1.60951896207e-14) A[2]:(0.999999940395) A[3]:(5.68876847048e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 726000 finished after 21 . Running score: 0.08. Policy_loss: -92050.611196, Value_loss: 0.984321473161. Times trained:               17803. Times reached goal: 27.               Steps done: 9270271.\n",
      " state (0)  A[0]:(0.992144465446) A[1]:(0.00166025338694) A[2]:(0.00206227344461) A[3]:(0.00413303030655)\n",
      " state (1)  A[0]:(1.25161941469e-05) A[1]:(2.20716719923e-06) A[2]:(3.27513807861e-05) A[3]:(0.999952554703)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.91819204754e-10) A[2]:(2.11337649603e-08) A[3]:(6.80959244548e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.8824870402e-11) A[2]:(1.72650627128e-09) A[3]:(1.02118952183e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.42620445903e-11) A[2]:(1.81809800548e-09) A[3]:(5.96188792779e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.92906193691e-11) A[2]:(1.22725074547e-08) A[3]:(8.02739493801e-13)\n",
      " state (6)  A[0]:(0.99997729063) A[1]:(8.23553100715e-11) A[2]:(2.26840747928e-05) A[3]:(1.18597950903e-18)\n",
      " state (7)  A[0]:(0.921099901199) A[1]:(2.54068016625e-10) A[2]:(0.0789000988007) A[3]:(2.13483557655e-22)\n",
      " state (8)  A[0]:(0.0397363938391) A[1]:(3.50903681112e-11) A[2]:(0.960263609886) A[3]:(1.0743603904e-25)\n",
      " state (9)  A[0]:(0.00234045856632) A[1]:(4.15287986291e-12) A[2]:(0.997659564018) A[3]:(1.17598052776e-27)\n",
      " state (10)  A[0]:(0.000767266086768) A[1]:(1.85705986179e-12) A[2]:(0.999232709408) A[3]:(2.27496454302e-28)\n",
      " state (11)  A[0]:(0.000467877631309) A[1]:(1.31621742585e-12) A[2]:(0.999532103539) A[3]:(1.13932743277e-28)\n",
      " state (12)  A[0]:(0.000340735859936) A[1]:(1.0617430329e-12) A[2]:(0.999659240246) A[3]:(7.43417324292e-29)\n",
      " state (13)  A[0]:(0.000254697108176) A[1]:(8.75778291033e-13) A[2]:(0.999745309353) A[3]:(5.08882029377e-29)\n",
      " state (14)  A[0]:(0.000185396085726) A[1]:(7.13615635767e-13) A[2]:(0.999814629555) A[3]:(3.41454619854e-29)\n",
      " state (15)  A[0]:(0.000128988554934) A[1]:(5.68943546789e-13) A[2]:(0.999871015549) A[3]:(2.20798674818e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 727000 finished after 36 . Running score: 0.15. Policy_loss: -92050.6128115, Value_loss: 1.20091613259. Times trained:               15402. Times reached goal: 101.               Steps done: 9285673.\n",
      " state (0)  A[0]:(0.994826555252) A[1]:(0.00121254555415) A[2]:(0.00119761237875) A[3]:(0.00276328972541)\n",
      " state (1)  A[0]:(1.23206436911e-05) A[1]:(2.06241929845e-06) A[2]:(3.220104918e-05) A[3]:(0.999953389168)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.36900171754e-10) A[2]:(5.95390492464e-09) A[3]:(7.35312533084e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.52980283891e-11) A[2]:(1.89778814885e-09) A[3]:(7.68312496957e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.98995654624e-11) A[2]:(2.73808975493e-09) A[3]:(2.05740060405e-11)\n",
      " state (5)  A[0]:(0.999999523163) A[1]:(5.67671951113e-11) A[2]:(4.91814546422e-07) A[3]:(7.30313077672e-16)\n",
      " state (6)  A[0]:(0.995032310486) A[1]:(1.46900755449e-10) A[2]:(0.00496768206358) A[3]:(4.63378339658e-21)\n",
      " state (7)  A[0]:(0.217926651239) A[1]:(1.11794032365e-10) A[2]:(0.782073378563) A[3]:(2.50524059976e-24)\n",
      " state (8)  A[0]:(0.00477952556685) A[1]:(6.42684083035e-12) A[2]:(0.995220482349) A[3]:(4.68840065644e-27)\n",
      " state (9)  A[0]:(0.000757949601393) A[1]:(1.70067215519e-12) A[2]:(0.999242067337) A[3]:(3.1637867494e-28)\n",
      " state (10)  A[0]:(0.000346148648532) A[1]:(9.93083761926e-13) A[2]:(0.999653875828) A[3]:(1.0876762787e-28)\n",
      " state (11)  A[0]:(0.000213561899727) A[1]:(7.21975485039e-13) A[2]:(0.999786436558) A[3]:(5.83095212001e-29)\n",
      " state (12)  A[0]:(0.000136770162499) A[1]:(5.43497755481e-13) A[2]:(0.99986320734) A[3]:(3.37132652503e-29)\n",
      " state (13)  A[0]:(8.34930979181e-05) A[1]:(4.01811694811e-13) A[2]:(0.999916493893) A[3]:(1.89935572227e-29)\n",
      " state (14)  A[0]:(4.73635082017e-05) A[1]:(2.88972242402e-13) A[2]:(0.999952614307) A[3]:(1.02819433862e-29)\n",
      " state (15)  A[0]:(2.48888281931e-05) A[1]:(2.03399891873e-13) A[2]:(0.999975085258) A[3]:(5.4397812286e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 728000 finished after 28 . Running score: 0.13. Policy_loss: -92050.6111405, Value_loss: 1.44372874908. Times trained:               16168. Times reached goal: 101.               Steps done: 9301841.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.992879331112) A[1]:(0.00445996597409) A[2]:(0.00097839301452) A[3]:(0.0016822933685)\n",
      " state (1)  A[0]:(1.39705307447e-05) A[1]:(2.69626821137e-06) A[2]:(3.68414403056e-05) A[3]:(0.999946475029)\n",
      " state (2)  A[0]:(1.0) A[1]:(6.85520945565e-11) A[2]:(2.97639468627e-09) A[3]:(1.61819682654e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.12792265647e-11) A[2]:(2.13808348803e-09) A[3]:(5.49627808544e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.52914329704e-11) A[2]:(3.13304426847e-09) A[3]:(1.23078621947e-11)\n",
      " state (5)  A[0]:(0.999999701977) A[1]:(5.87635565208e-11) A[2]:(2.87537432087e-07) A[3]:(4.07677778077e-16)\n",
      " state (6)  A[0]:(0.985622644424) A[1]:(4.93677210223e-10) A[2]:(0.0143773546442) A[3]:(1.48885876255e-21)\n",
      " state (7)  A[0]:(0.015087755397) A[1]:(4.38233546673e-11) A[2]:(0.984912216663) A[3]:(2.76494853648e-26)\n",
      " state (8)  A[0]:(0.000333946372848) A[1]:(2.72225970065e-12) A[2]:(0.999666035175) A[3]:(9.42344341206e-29)\n",
      " state (9)  A[0]:(7.32856569812e-05) A[1]:(1.01669031267e-12) A[2]:(0.999926686287) A[3]:(1.39643567987e-29)\n",
      " state (10)  A[0]:(2.753978697e-05) A[1]:(5.77001283125e-13) A[2]:(0.999972462654) A[3]:(4.89824884869e-30)\n",
      " state (11)  A[0]:(1.06572779259e-05) A[1]:(3.52020767137e-13) A[2]:(0.999989330769) A[3]:(2.04901341879e-30)\n",
      " state (12)  A[0]:(3.81195877708e-06) A[1]:(2.16948421033e-13) A[2]:(0.999996185303) A[3]:(9.09583422734e-31)\n",
      " state (13)  A[0]:(1.3186228216e-06) A[1]:(1.36957245133e-13) A[2]:(0.999998688698) A[3]:(4.35213711895e-31)\n",
      " state (14)  A[0]:(4.95774941101e-07) A[1]:(9.17715136197e-14) A[2]:(0.999999523163) A[3]:(2.3426325825e-31)\n",
      " state (15)  A[0]:(2.24183949626e-07) A[1]:(6.70478145118e-14) A[2]:(0.999999761581) A[3]:(1.45835214587e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 729000 finished after 10 . Running score: 0.13. Policy_loss: -92050.6115014, Value_loss: 0.991692341595. Times trained:               15302. Times reached goal: 109.               Steps done: 9317143.\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0037,  0.0018,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0037,  0.0018,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0037,  0.0018,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0037,  0.0018,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9937,  0.0037,  0.0018,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7671e-11,  2.6613e-09,  2.8008e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.7672e-11,  2.6615e-09,  2.8006e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.0432e-04,  2.8156e-12,  9.9970e-01,  7.0858e-29]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993668138981) A[1]:(0.00370737304911) A[2]:(0.00183486274909) A[3]:(0.000789601239376)\n",
      " state (1)  A[0]:(1.34456067826e-05) A[1]:(2.61568607129e-06) A[2]:(3.94700946345e-05) A[3]:(0.999944448471)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.2784202863e-10) A[2]:(5.64282087723e-09) A[3]:(4.31930852196e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.27126563285e-11) A[2]:(2.27316654389e-09) A[3]:(6.53844270393e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.76731562612e-11) A[2]:(2.66131405802e-09) A[3]:(2.80129253127e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.05019620672e-11) A[2]:(1.7540836339e-08) A[3]:(9.01224421154e-14)\n",
      " state (6)  A[0]:(0.999880194664) A[1]:(2.29934321583e-10) A[2]:(0.000119797405205) A[3]:(1.25760145936e-19)\n",
      " state (7)  A[0]:(0.0679074972868) A[1]:(1.50625928397e-10) A[2]:(0.932092487812) A[3]:(2.95517131205e-25)\n",
      " state (8)  A[0]:(0.000304913730361) A[1]:(2.81924809019e-12) A[2]:(0.999695062637) A[3]:(7.10341583983e-29)\n",
      " state (9)  A[0]:(3.83621918445e-05) A[1]:(7.60652012297e-13) A[2]:(0.999961614609) A[3]:(5.67472850701e-30)\n",
      " state (10)  A[0]:(1.04558948806e-05) A[1]:(3.80027579501e-13) A[2]:(0.999989569187) A[3]:(1.63168565278e-30)\n",
      " state (11)  A[0]:(3.13429427479e-06) A[1]:(2.16051257288e-13) A[2]:(0.999996840954) A[3]:(6.29661213241e-31)\n",
      " state (12)  A[0]:(9.58580812949e-07) A[1]:(1.30201893104e-13) A[2]:(0.999999046326) A[3]:(2.79728911202e-31)\n",
      " state (13)  A[0]:(3.36092199404e-07) A[1]:(8.52921723592e-14) A[2]:(0.999999642372) A[3]:(1.4551623244e-31)\n",
      " state (14)  A[0]:(1.50086876261e-07) A[1]:(6.22351630408e-14) A[2]:(0.999999821186) A[3]:(9.04613844042e-32)\n",
      " state (15)  A[0]:(8.60719922002e-08) A[1]:(5.02561656028e-14) A[2]:(0.999999940395) A[3]:(6.58626075832e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 730000 finished after 8 . Running score: 0.14. Policy_loss: -92050.6114384, Value_loss: 1.0053554674. Times trained:               15566. Times reached goal: 120.               Steps done: 9332709.\n",
      " state (0)  A[0]:(0.993940412998) A[1]:(0.00228464161046) A[2]:(0.00259949616157) A[3]:(0.00117542943917)\n",
      " state (1)  A[0]:(1.27444291138e-05) A[1]:(2.3659240469e-06) A[2]:(3.83763217542e-05) A[3]:(0.999946534634)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.1701249131e-10) A[2]:(1.03763211357e-08) A[3]:(1.04040998039e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.96077684539e-11) A[2]:(2.48685183557e-09) A[3]:(5.91941981543e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.31833449607e-11) A[2]:(3.25112181621e-09) A[3]:(1.64942608183e-11)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(3.63105830126e-11) A[2]:(9.63122488429e-08) A[3]:(2.65578879689e-15)\n",
      " state (6)  A[0]:(0.995398044586) A[1]:(3.86687099008e-10) A[2]:(0.00460192980245) A[3]:(5.05389542422e-21)\n",
      " state (7)  A[0]:(0.0121165839955) A[1]:(3.5423181749e-11) A[2]:(0.987883388996) A[3]:(2.21615957514e-26)\n",
      " state (8)  A[0]:(0.000162023410667) A[1]:(1.58602211225e-12) A[2]:(0.999837994576) A[3]:(4.0698459686e-29)\n",
      " state (9)  A[0]:(2.79495707218e-05) A[1]:(5.40851543239e-13) A[2]:(0.999972045422) A[3]:(5.33210857917e-30)\n",
      " state (10)  A[0]:(8.16021201899e-06) A[1]:(2.83535781659e-13) A[2]:(0.999991834164) A[3]:(1.70620353107e-30)\n",
      " state (11)  A[0]:(2.45889918915e-06) A[1]:(1.62172521505e-13) A[2]:(0.99999755621) A[3]:(6.74750590451e-31)\n",
      " state (12)  A[0]:(7.50443973629e-07) A[1]:(9.76010247557e-14) A[2]:(0.99999922514) A[3]:(3.02433725647e-31)\n",
      " state (13)  A[0]:(2.65828248303e-07) A[1]:(6.40409559692e-14) A[2]:(0.999999761581) A[3]:(1.5901559781e-31)\n",
      " state (14)  A[0]:(1.21124259067e-07) A[1]:(4.69639518247e-14) A[2]:(0.999999880791) A[3]:(1.00166013033e-31)\n",
      " state (15)  A[0]:(7.0933083407e-08) A[1]:(3.81426288395e-14) A[2]:(0.999999940395) A[3]:(7.38217328958e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 731000 finished after 24 . Running score: 0.15. Policy_loss: -92050.6111454, Value_loss: 1.44660739949. Times trained:               14831. Times reached goal: 115.               Steps done: 9347540.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996783554554) A[1]:(0.00145292305388) A[2]:(0.00125236413442) A[3]:(0.000511132180691)\n",
      " state (1)  A[0]:(1.24817343021e-05) A[1]:(2.16377520701e-06) A[2]:(3.18317906931e-05) A[3]:(0.999953508377)\n",
      " state (2)  A[0]:(0.878139972687) A[1]:(3.20073449984e-05) A[2]:(0.000631905335467) A[3]:(0.121196083724)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.04810533652e-11) A[2]:(1.98958205466e-09) A[3]:(1.47817577623e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.42156129818e-11) A[2]:(1.45623413328e-09) A[3]:(7.429134391e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.12512724032e-11) A[2]:(1.50450463199e-09) A[3]:(4.98776089874e-11)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.58263636171e-11) A[2]:(2.36588881819e-09) A[3]:(8.35888389755e-12)\n",
      " state (7)  A[0]:(0.999999880791) A[1]:(4.63386794214e-11) A[2]:(9.6130740701e-08) A[3]:(1.62187878213e-15)\n",
      " state (8)  A[0]:(0.999967694283) A[1]:(1.13767981147e-10) A[2]:(3.23106833093e-05) A[3]:(1.74553345996e-19)\n",
      " state (9)  A[0]:(0.984608232975) A[1]:(2.89443996548e-10) A[2]:(0.0153917679563) A[3]:(2.48807638274e-22)\n",
      " state (10)  A[0]:(0.653676331043) A[1]:(3.66231572846e-10) A[2]:(0.346323639154) A[3]:(5.9535151079e-24)\n",
      " state (11)  A[0]:(0.181988418102) A[1]:(1.58987809029e-10) A[2]:(0.818011581898) A[3]:(3.48754148951e-25)\n",
      " state (12)  A[0]:(0.0621878281236) A[1]:(7.10312433871e-11) A[2]:(0.937812149525) A[3]:(5.49249274011e-26)\n",
      " state (13)  A[0]:(0.0358630158007) A[1]:(4.67710314922e-11) A[2]:(0.964136958122) A[3]:(2.23219255674e-26)\n",
      " state (14)  A[0]:(0.027936886996) A[1]:(3.86973786348e-11) A[2]:(0.97206312418) A[3]:(1.49355112223e-26)\n",
      " state (15)  A[0]:(0.024996811524) A[1]:(3.55701856858e-11) A[2]:(0.975003182888) A[3]:(1.25041841345e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 732000 finished after 4 . Running score: 0.0. Policy_loss: -92050.6111526, Value_loss: 0.991627355878. Times trained:               15892. Times reached goal: 79.               Steps done: 9363432.\n",
      " state (0)  A[0]:(0.994120359421) A[1]:(0.00148157181684) A[2]:(0.00151166366413) A[3]:(0.00288637774065)\n",
      " state (1)  A[0]:(1.04649989225e-05) A[1]:(1.92488573703e-06) A[2]:(2.99151615764e-05) A[3]:(0.999957680702)\n",
      " state (2)  A[0]:(0.999998986721) A[1]:(1.32551098986e-08) A[2]:(4.583505131e-07) A[3]:(5.54893517801e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.87162385485e-11) A[2]:(2.47756304361e-09) A[3]:(4.78585365782e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.09056877312e-11) A[2]:(3.78609188445e-09) A[3]:(8.60430997285e-12)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(2.79885680604e-11) A[2]:(1.23091425053e-07) A[3]:(1.1618758349e-15)\n",
      " state (6)  A[0]:(0.992413759232) A[1]:(3.69658997856e-10) A[2]:(0.00758626870811) A[3]:(4.32577655626e-21)\n",
      " state (7)  A[0]:(0.12338501215) A[1]:(1.73393022163e-10) A[2]:(0.87661498785) A[3]:(1.10067464548e-24)\n",
      " state (8)  A[0]:(0.00395780894905) A[1]:(1.28937529056e-11) A[2]:(0.996042191982) A[3]:(4.53941264187e-27)\n",
      " state (9)  A[0]:(0.000904681161046) A[1]:(4.37920923482e-12) A[2]:(0.999095320702) A[3]:(5.15872709704e-28)\n",
      " state (10)  A[0]:(0.000420560158091) A[1]:(2.55745121261e-12) A[2]:(0.999579429626) A[3]:(1.77665063292e-28)\n",
      " state (11)  A[0]:(0.000219780631596) A[1]:(1.65121236996e-12) A[2]:(0.999780237675) A[3]:(7.55965299547e-29)\n",
      " state (12)  A[0]:(0.000109630411316) A[1]:(1.05714026942e-12) A[2]:(0.999890387058) A[3]:(3.21408578028e-29)\n",
      " state (13)  A[0]:(5.06423966726e-05) A[1]:(6.65742526211e-13) A[2]:(0.999949336052) A[3]:(1.3545871782e-29)\n",
      " state (14)  A[0]:(2.1902298613e-05) A[1]:(4.20350603283e-13) A[2]:(0.999978125095) A[3]:(5.91494425707e-30)\n",
      " state (15)  A[0]:(9.12272844289e-06) A[1]:(2.71546159039e-13) A[2]:(0.999990880489) A[3]:(2.78401392518e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 733000 finished after 3 . Running score: 0.13. Policy_loss: -92050.6111435, Value_loss: 1.2089796777. Times trained:               16242. Times reached goal: 85.               Steps done: 9379674.\n",
      " state (0)  A[0]:(0.992365002632) A[1]:(0.00146794470493) A[2]:(0.00339039717801) A[3]:(0.00277663045563)\n",
      " state (1)  A[0]:(1.00839397419e-05) A[1]:(1.85022508958e-06) A[2]:(3.13428972731e-05) A[3]:(0.999956727028)\n",
      " state (2)  A[0]:(0.999999582767) A[1]:(6.05145888954e-09) A[2]:(2.48230378475e-07) A[3]:(1.65292135534e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.66455928102e-11) A[2]:(2.7954081272e-09) A[3]:(4.11752715257e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.94579742544e-11) A[2]:(4.33424007795e-09) A[3]:(7.70944159206e-12)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(2.04984449553e-11) A[2]:(8.19320788992e-08) A[3]:(2.25026160899e-15)\n",
      " state (6)  A[0]:(0.993056237698) A[1]:(3.31450172864e-10) A[2]:(0.00694373250008) A[3]:(5.48633963235e-21)\n",
      " state (7)  A[0]:(0.027255570516) A[1]:(5.20655012159e-11) A[2]:(0.97274440527) A[3]:(1.16772602284e-25)\n",
      " state (8)  A[0]:(0.000218016604776) A[1]:(1.57896395611e-12) A[2]:(0.999781966209) A[3]:(1.04045091165e-28)\n",
      " state (9)  A[0]:(2.38050761254e-05) A[1]:(4.03935077661e-13) A[2]:(0.999976217747) A[3]:(8.0528051124e-30)\n",
      " state (10)  A[0]:(4.79457776237e-06) A[1]:(1.76244151109e-13) A[2]:(0.999995231628) A[3]:(1.90345750167e-30)\n",
      " state (11)  A[0]:(1.15484510843e-06) A[1]:(9.19950761077e-14) A[2]:(0.999998867512) A[3]:(6.61397726942e-31)\n",
      " state (12)  A[0]:(3.58713947435e-07) A[1]:(5.60327168507e-14) A[2]:(0.999999642372) A[3]:(3.06852150303e-31)\n",
      " state (13)  A[0]:(1.56188335154e-07) A[1]:(3.990677824e-14) A[2]:(0.999999821186) A[3]:(1.84337521123e-31)\n",
      " state (14)  A[0]:(9.18148899132e-08) A[1]:(3.22543132882e-14) A[2]:(0.999999880791) A[3]:(1.34734645274e-31)\n",
      " state (15)  A[0]:(6.66064963184e-08) A[1]:(2.8394165613e-14) A[2]:(0.999999940395) A[3]:(1.11935250553e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 734000 finished after 11 . Running score: 0.14. Policy_loss: -92050.6113081, Value_loss: 1.20593749607. Times trained:               16743. Times reached goal: 105.               Steps done: 9396417.\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0017,  0.0016,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0150e-11,  3.4853e-09,  1.0375e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0151e-11,  3.4852e-09,  1.0376e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0152e-11,  3.4852e-09,  1.0376e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0152e-11,  3.4852e-09,  1.0377e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0153e-11,  3.4851e-09,  1.0377e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0017,  0.0016,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0017,  0.0016,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9959,  0.0017,  0.0016,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0156e-11,  3.4851e-09,  1.0379e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0156e-11,  3.4851e-09,  1.0379e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8887e-03,  1.9499e-11,  9.9211e-01,  9.2694e-27]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8278e-04,  3.5995e-12,  9.9922e-01,  3.1251e-28]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6143e-05,  3.2211e-13,  9.9998e-01,  3.2767e-30]])\n",
      "On state=13, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8017e-04,  3.5915e-12,  9.9922e-01,  3.1119e-28]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995922029018) A[1]:(0.00169930874836) A[2]:(0.00163563119713) A[3]:(0.000743020442314)\n",
      " state (1)  A[0]:(1.05759063445e-05) A[1]:(1.95305028683e-06) A[2]:(3.1380172004e-05) A[3]:(0.999956071377)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.59117863119e-10) A[2]:(2.5134525572e-08) A[3]:(3.92018062456e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.53920052987e-11) A[2]:(2.54616572271e-09) A[3]:(3.49028340951e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.01582786522e-11) A[2]:(3.48620199375e-09) A[3]:(1.03730002018e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.94383623781e-11) A[2]:(1.702952801e-08) A[3]:(6.16317842031e-14)\n",
      " state (6)  A[0]:(0.999961256981) A[1]:(1.10274567255e-10) A[2]:(3.87185536965e-05) A[3]:(4.61711952002e-19)\n",
      " state (7)  A[0]:(0.447837561369) A[1]:(3.85330073405e-10) A[2]:(0.552162408829) A[3]:(9.11278943734e-24)\n",
      " state (8)  A[0]:(0.00785203184932) A[1]:(1.9434655274e-11) A[2]:(0.992147982121) A[3]:(9.20976309769e-27)\n",
      " state (9)  A[0]:(0.000779327470809) A[1]:(3.58898804773e-12) A[2]:(0.99922066927) A[3]:(3.10772726206e-28)\n",
      " state (10)  A[0]:(0.000253857288044) A[1]:(1.65993445118e-12) A[2]:(0.999746143818) A[3]:(6.86471006399e-29)\n",
      " state (11)  A[0]:(0.000105005557998) A[1]:(9.39505743168e-13) A[2]:(0.999894976616) A[3]:(2.31044230917e-29)\n",
      " state (12)  A[0]:(4.24530226155e-05) A[1]:(5.46495140807e-13) A[2]:(0.999957561493) A[3]:(8.43878855664e-30)\n",
      " state (13)  A[0]:(1.60568924912e-05) A[1]:(3.212378009e-13) A[2]:(0.999983966351) A[3]:(3.26170530093e-30)\n",
      " state (14)  A[0]:(5.85260249863e-06) A[1]:(1.94591508163e-13) A[2]:(0.999994158745) A[3]:(1.38468372682e-30)\n",
      " state (15)  A[0]:(2.20339529733e-06) A[1]:(1.24439995526e-13) A[2]:(0.999997794628) A[3]:(6.66833683018e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 735000 finished after 15 . Running score: 0.06. Policy_loss: -92050.6112815, Value_loss: 0.994369834368. Times trained:               15455. Times reached goal: 99.               Steps done: 9411872.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994478344917) A[1]:(0.00172741094138) A[2]:(0.00214958447032) A[3]:(0.00164465152193)\n",
      " state (1)  A[0]:(9.29809993977e-06) A[1]:(1.75619697984e-06) A[2]:(2.75939801213e-05) A[3]:(0.99996137619)\n",
      " state (2)  A[0]:(0.00267560244538) A[1]:(1.51310932779e-05) A[2]:(0.000294653844321) A[3]:(0.997014641762)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.48306600176e-11) A[2]:(2.63550159474e-09) A[3]:(8.04817670841e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.40740283211e-11) A[2]:(2.88658741354e-09) A[3]:(2.84300326336e-11)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.36997383524e-11) A[2]:(6.6588174974e-09) A[3]:(1.72970080099e-12)\n",
      " state (6)  A[0]:(0.99999922514) A[1]:(2.88419375188e-11) A[2]:(7.52020127948e-07) A[3]:(8.03782121479e-17)\n",
      " state (7)  A[0]:(0.883051753044) A[1]:(3.98154426096e-10) A[2]:(0.116948254406) A[3]:(3.73896516844e-22)\n",
      " state (8)  A[0]:(0.0367448851466) A[1]:(5.01055238966e-11) A[2]:(0.963255107403) A[3]:(2.63834825574e-25)\n",
      " state (9)  A[0]:(0.0022366088815) A[1]:(6.16694802863e-12) A[2]:(0.997763395309) A[3]:(3.65520356734e-27)\n",
      " state (10)  A[0]:(0.000716935726814) A[1]:(2.72044366201e-12) A[2]:(0.999283075333) A[3]:(7.16577046993e-28)\n",
      " state (11)  A[0]:(0.000357748765964) A[1]:(1.68240768539e-12) A[2]:(0.999642252922) A[3]:(2.79117973122e-28)\n",
      " state (12)  A[0]:(0.000184761491255) A[1]:(1.08465341743e-12) A[2]:(0.999815225601) A[3]:(1.19423376955e-28)\n",
      " state (13)  A[0]:(8.90447481652e-05) A[1]:(6.83357504697e-13) A[2]:(0.999910950661) A[3]:(4.96493876214e-29)\n",
      " state (14)  A[0]:(3.95732458855e-05) A[1]:(4.22155799901e-13) A[2]:(0.999960422516) A[3]:(2.03366947474e-29)\n",
      " state (15)  A[0]:(1.654031621e-05) A[1]:(2.61448442106e-13) A[2]:(0.999983429909) A[3]:(8.61523909842e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 736000 finished after 17 . Running score: 0.05. Policy_loss: -92050.6131208, Value_loss: 1.19247232492. Times trained:               15983. Times reached goal: 98.               Steps done: 9427855.\n",
      " state (0)  A[0]:(0.99035859108) A[1]:(0.00147365033627) A[2]:(0.00272240582854) A[3]:(0.00544536439702)\n",
      " state (1)  A[0]:(9.91912838799e-06) A[1]:(1.91721210285e-06) A[2]:(3.00299689115e-05) A[3]:(0.999958157539)\n",
      " state (2)  A[0]:(0.999998211861) A[1]:(2.16327951108e-08) A[2]:(8.05309241514e-07) A[3]:(9.86811528492e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.86480673853e-11) A[2]:(3.03037284155e-09) A[3]:(3.15233637804e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.12140382985e-11) A[2]:(4.48189085844e-09) A[3]:(7.38455911004e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.90989290355e-11) A[2]:(2.35409203242e-08) A[3]:(4.55523815825e-14)\n",
      " state (6)  A[0]:(0.999958634377) A[1]:(9.58114282357e-11) A[2]:(4.13511697843e-05) A[3]:(1.03844719389e-18)\n",
      " state (7)  A[0]:(0.298935681581) A[1]:(3.21821264082e-10) A[2]:(0.701064348221) A[3]:(1.2952964324e-23)\n",
      " state (8)  A[0]:(0.00168501154985) A[1]:(7.26604930737e-12) A[2]:(0.998314976692) A[3]:(3.61966669319e-27)\n",
      " state (9)  A[0]:(0.00010609903984) A[1]:(1.10343613587e-12) A[2]:(0.999893903732) A[3]:(8.97470113316e-29)\n",
      " state (10)  A[0]:(2.28475983022e-05) A[1]:(4.38272980191e-13) A[2]:(0.999977171421) A[3]:(1.59117759083e-29)\n",
      " state (11)  A[0]:(5.92662991039e-06) A[1]:(2.13374809358e-13) A[2]:(0.99999409914) A[3]:(4.41147419299e-30)\n",
      " state (12)  A[0]:(1.59961632562e-06) A[1]:(1.13850958825e-13) A[2]:(0.999998390675) A[3]:(1.52442000652e-30)\n",
      " state (13)  A[0]:(4.96547215789e-07) A[1]:(6.78101848219e-14) A[2]:(0.999999523163) A[3]:(6.60419950741e-31)\n",
      " state (14)  A[0]:(1.98127153794e-07) A[1]:(4.60336318218e-14) A[2]:(0.999999821186) A[3]:(3.61223889184e-31)\n",
      " state (15)  A[0]:(1.03759681735e-07) A[1]:(3.53063898376e-14) A[2]:(0.999999880791) A[3]:(2.41391926191e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 737000 finished after 19 . Running score: 0.09. Policy_loss: -92050.611449, Value_loss: 1.01805059192. Times trained:               16303. Times reached goal: 109.               Steps done: 9444158.\n",
      " state (0)  A[0]:(0.995488703251) A[1]:(0.00165769737214) A[2]:(0.00115736527368) A[3]:(0.0016962189693)\n",
      " state (1)  A[0]:(1.06205407064e-05) A[1]:(2.1043988454e-06) A[2]:(3.11510302708e-05) A[3]:(0.999956130981)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(1.90993110216e-09) A[2]:(7.85001930126e-08) A[3]:(1.69182854393e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.93643555252e-11) A[2]:(3.2247755577e-09) A[3]:(1.96981667089e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.2041411191e-11) A[2]:(5.04998842743e-09) A[3]:(4.10275546175e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.18441358751e-11) A[2]:(2.74061484618e-08) A[3]:(2.87441071108e-14)\n",
      " state (6)  A[0]:(0.999970912933) A[1]:(1.10492490157e-10) A[2]:(2.90913194476e-05) A[3]:(1.54753684605e-18)\n",
      " state (7)  A[0]:(0.474089950323) A[1]:(5.48570633363e-10) A[2]:(0.525910019875) A[3]:(3.44026469061e-23)\n",
      " state (8)  A[0]:(0.0124371321872) A[1]:(4.01696592656e-11) A[2]:(0.987562894821) A[3]:(6.9844789287e-26)\n",
      " state (9)  A[0]:(0.000926058739424) A[1]:(6.17412978382e-12) A[2]:(0.999073922634) A[3]:(1.59721848075e-27)\n",
      " state (10)  A[0]:(0.000276725622825) A[1]:(2.71210202733e-12) A[2]:(0.999723255634) A[3]:(3.14519743284e-28)\n",
      " state (11)  A[0]:(0.000108098196506) A[1]:(1.48147184624e-12) A[2]:(0.999891877174) A[3]:(9.73739286341e-29)\n",
      " state (12)  A[0]:(4.05413411499e-05) A[1]:(8.20483438135e-13) A[2]:(0.999959468842) A[3]:(3.17603602496e-29)\n",
      " state (13)  A[0]:(1.40451184052e-05) A[1]:(4.55388222791e-13) A[2]:(0.999985933304) A[3]:(1.07560306022e-29)\n",
      " state (14)  A[0]:(4.7036360229e-06) A[1]:(2.61467496959e-13) A[2]:(0.999995291233) A[3]:(4.02863618354e-30)\n",
      " state (15)  A[0]:(1.65016069786e-06) A[1]:(1.60370414864e-13) A[2]:(0.99999833107) A[3]:(1.75523913685e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 738000 finished after 7 . Running score: 0.13. Policy_loss: -92050.6126765, Value_loss: 1.00187823429. Times trained:               15685. Times reached goal: 109.               Steps done: 9459843.\n",
      " state (0)  A[0]:(0.996481001377) A[1]:(0.00103751942515) A[2]:(0.00105282943696) A[3]:(0.0014286438236)\n",
      " state (1)  A[0]:(1.03531137938e-05) A[1]:(1.96277210307e-06) A[2]:(3.10291579808e-05) A[3]:(0.999956667423)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(7.33052341051e-10) A[2]:(3.77948445873e-08) A[3]:(3.90962906494e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.65733207608e-11) A[2]:(3.90347487667e-09) A[3]:(1.47153903646e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.83865760758e-11) A[2]:(7.37488603519e-09) A[3]:(1.81858770664e-12)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(2.09938403467e-11) A[2]:(9.65174820067e-08) A[3]:(2.32742101653e-15)\n",
      " state (6)  A[0]:(0.996986925602) A[1]:(3.27272098311e-10) A[2]:(0.00301306229085) A[3]:(3.47826474898e-20)\n",
      " state (7)  A[0]:(0.0660049021244) A[1]:(1.45664855178e-10) A[2]:(0.933995068073) A[3]:(1.58005634568e-24)\n",
      " state (8)  A[0]:(0.000107046580524) A[1]:(1.63058780887e-12) A[2]:(0.999892950058) A[3]:(1.89116402e-28)\n",
      " state (9)  A[0]:(4.42924010713e-06) A[1]:(2.59803219519e-13) A[2]:(0.999995589256) A[3]:(6.21359505381e-30)\n",
      " state (10)  A[0]:(6.1763063286e-07) A[1]:(1.00549912363e-13) A[2]:(0.999999403954) A[3]:(1.21590342674e-30)\n",
      " state (11)  A[0]:(1.51849675945e-07) A[1]:(5.4459810549e-14) A[2]:(0.999999821186) A[3]:(4.49255509133e-31)\n",
      " state (12)  A[0]:(6.1018582187e-08) A[1]:(3.72542234149e-14) A[2]:(0.999999940395) A[3]:(2.48108160185e-31)\n",
      " state (13)  A[0]:(3.55740432667e-08) A[1]:(2.99101258897e-14) A[2]:(0.999999940395) A[3]:(1.77471905307e-31)\n",
      " state (14)  A[0]:(2.6218053506e-08) A[1]:(2.64541620082e-14) A[2]:(1.0) A[3]:(1.47570067924e-31)\n",
      " state (15)  A[0]:(2.2117516707e-08) A[1]:(2.47142563847e-14) A[2]:(1.0) A[3]:(1.33356084274e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 739000 finished after 12 . Running score: 0.11. Policy_loss: -92050.6111907, Value_loss: 1.41251136027. Times trained:               15901. Times reached goal: 106.               Steps done: 9475744.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9971,  0.0013,  0.0008,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0013,  0.0008,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.2012e-11,  7.7179e-09,  8.1128e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0982e-03,  1.4407e-11,  9.9590e-01,  1.7365e-26]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.2010e-11,  7.7201e-09,  8.1062e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.2010e-11,  7.7208e-09,  8.1041e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9971,  0.0013,  0.0008,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.2009e-11,  7.7219e-09,  8.1005e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.0973e-03,  1.4405e-11,  9.9590e-01,  1.7360e-26]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.997059583664) A[1]:(0.00131039321423) A[2]:(0.000813446065877) A[3]:(0.000816604937427)\n",
      " state (1)  A[0]:(1.0907624528e-05) A[1]:(2.02034357244e-06) A[2]:(3.11849689751e-05) A[3]:(0.999955892563)\n",
      " state (2)  A[0]:(0.99999910593) A[1]:(1.23873080682e-08) A[2]:(5.71370037505e-07) A[3]:(3.07682086031e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.07100733732e-11) A[2]:(3.81690012929e-09) A[3]:(9.39241219522e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.20085755176e-11) A[2]:(7.72118191605e-09) A[3]:(8.10151913003e-13)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(1.36049426933e-11) A[2]:(8.92154048415e-08) A[3]:(8.9046392635e-16)\n",
      " state (6)  A[0]:(0.999175906181) A[1]:(1.6644408074e-10) A[2]:(0.000824108661618) A[3]:(5.50734266898e-20)\n",
      " state (7)  A[0]:(0.240690916777) A[1]:(2.71567268673e-10) A[2]:(0.759309053421) A[3]:(8.86043914465e-24)\n",
      " state (8)  A[0]:(0.00410634186119) A[1]:(1.44264383425e-11) A[2]:(0.995893657207) A[3]:(1.74109028168e-26)\n",
      " state (9)  A[0]:(0.000376927928301) A[1]:(2.75706345775e-12) A[2]:(0.999623060226) A[3]:(6.65525314547e-28)\n",
      " state (10)  A[0]:(0.000100703247881) A[1]:(1.17495076168e-12) A[2]:(0.99989926815) A[3]:(1.28749728822e-28)\n",
      " state (11)  A[0]:(3.02897569782e-05) A[1]:(5.69493725181e-13) A[2]:(0.99996972084) A[3]:(3.30140503654e-29)\n",
      " state (12)  A[0]:(8.50864398672e-06) A[1]:(2.80894882007e-13) A[2]:(0.999991476536) A[3]:(9.11264059921e-30)\n",
      " state (13)  A[0]:(2.35215907196e-06) A[1]:(1.45707231003e-13) A[2]:(0.999997675419) A[3]:(2.88488206099e-30)\n",
      " state (14)  A[0]:(7.22788740859e-07) A[1]:(8.34370550082e-14) A[2]:(0.999999284744) A[3]:(1.12912608278e-30)\n",
      " state (15)  A[0]:(2.74426639635e-07) A[1]:(5.41548583592e-14) A[2]:(0.999999701977) A[3]:(5.59781835667e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 740000 finished after 9 . Running score: 0.1. Policy_loss: -92050.6111122, Value_loss: 1.42195099536. Times trained:               15847. Times reached goal: 101.               Steps done: 9491591.\n",
      " state (0)  A[0]:(0.995234727859) A[1]:(0.000864563102368) A[2]:(0.00109613139648) A[3]:(0.00280459527858)\n",
      " state (1)  A[0]:(1.11674053187e-05) A[1]:(1.94875678972e-06) A[2]:(3.62022037734e-05) A[3]:(0.999950706959)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.00447565333e-11) A[2]:(5.69202640577e-09) A[3]:(3.67521291178e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.6274881576e-11) A[2]:(9.56126644525e-09) A[3]:(1.9342600438e-12)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(1.56026060127e-11) A[2]:(9.58683514796e-08) A[3]:(4.04012015357e-15)\n",
      " state (5)  A[0]:(0.997398376465) A[1]:(2.33949859485e-10) A[2]:(0.00260164332576) A[3]:(9.90597638887e-20)\n",
      " state (6)  A[0]:(0.00132846948691) A[1]:(8.15233557744e-12) A[2]:(0.998671531677) A[3]:(2.0226157876e-26)\n",
      " state (7)  A[0]:(3.24224401993e-07) A[1]:(5.3527301813e-14) A[2]:(0.999999701977) A[3]:(1.69544653524e-30)\n",
      " state (8)  A[0]:(2.11498960567e-08) A[1]:(1.60070507603e-14) A[2]:(1.0) A[3]:(2.46250644011e-31)\n",
      " state (9)  A[0]:(7.31983940128e-09) A[1]:(1.03668328533e-14) A[2]:(1.0) A[3]:(1.28130917845e-31)\n",
      " state (10)  A[0]:(4.68508121187e-09) A[1]:(8.67193009025e-15) A[2]:(1.0) A[3]:(9.86710428278e-32)\n",
      " state (11)  A[0]:(3.86139165087e-09) A[1]:(8.03262519705e-15) A[2]:(1.0) A[3]:(8.83469874505e-32)\n",
      " state (12)  A[0]:(3.54461549001e-09) A[1]:(7.76617658393e-15) A[2]:(1.0) A[3]:(8.41772445019e-32)\n",
      " state (13)  A[0]:(3.4110867464e-09) A[1]:(7.6498281383e-15) A[2]:(1.0) A[3]:(8.23817532882e-32)\n",
      " state (14)  A[0]:(3.35241745475e-09) A[1]:(7.59788723094e-15) A[2]:(1.0) A[3]:(8.15848974227e-32)\n",
      " state (15)  A[0]:(3.32616378884e-09) A[1]:(7.57444644115e-15) A[2]:(1.0) A[3]:(8.12259132029e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 741000 finished after 39 . Running score: 0.09. Policy_loss: -92050.6123421, Value_loss: 1.42935771598. Times trained:               16072. Times reached goal: 93.               Steps done: 9507663.\n",
      " state (0)  A[0]:(0.997034847736) A[1]:(0.00084353168495) A[2]:(0.00106889789458) A[3]:(0.00105269555934)\n",
      " state (1)  A[0]:(1.26230424939e-05) A[1]:(2.07875700653e-06) A[2]:(4.27276863775e-05) A[3]:(0.999942541122)\n",
      " state (2)  A[0]:(0.999998986721) A[1]:(9.80554126784e-09) A[2]:(8.60784439283e-07) A[3]:(1.32849208967e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.02496647922e-11) A[2]:(1.13692806281e-08) A[3]:(6.91602265908e-13)\n",
      " state (4)  A[0]:(0.999999642372) A[1]:(1.03259562359e-11) A[2]:(3.58539523404e-07) A[3]:(8.01794968949e-17)\n",
      " state (5)  A[0]:(0.271595686674) A[1]:(2.99294561357e-10) A[2]:(0.728404283524) A[3]:(5.28484647015e-23)\n",
      " state (6)  A[0]:(1.66173492744e-07) A[1]:(3.79295526194e-14) A[2]:(0.999999821186) A[3]:(9.59133142729e-31)\n",
      " state (7)  A[0]:(4.01530408922e-09) A[1]:(7.21879763539e-15) A[2]:(1.0) A[3]:(6.95785862025e-32)\n",
      " state (8)  A[0]:(2.3064372634e-09) A[1]:(5.77029760372e-15) A[2]:(1.0) A[3]:(5.02353917268e-32)\n",
      " state (9)  A[0]:(2.00457073163e-09) A[1]:(5.46043728151e-15) A[2]:(1.0) A[3]:(4.64367552835e-32)\n",
      " state (10)  A[0]:(1.91714333297e-09) A[1]:(5.36631498041e-15) A[2]:(1.0) A[3]:(4.53074372895e-32)\n",
      " state (11)  A[0]:(1.88793913836e-09) A[1]:(5.33439369676e-15) A[2]:(1.0) A[3]:(4.49270796437e-32)\n",
      " state (12)  A[0]:(1.87766846516e-09) A[1]:(5.32309173614e-15) A[2]:(1.0) A[3]:(4.47929175347e-32)\n",
      " state (13)  A[0]:(1.87399762375e-09) A[1]:(5.31903190723e-15) A[2]:(1.0) A[3]:(4.47450984245e-32)\n",
      " state (14)  A[0]:(1.87266846474e-09) A[1]:(5.31757119891e-15) A[2]:(1.0) A[3]:(4.47276922919e-32)\n",
      " state (15)  A[0]:(1.87218973657e-09) A[1]:(5.3170439209e-15) A[2]:(1.0) A[3]:(4.47215503339e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 742000 finished after 18 . Running score: 0.1. Policy_loss: -92050.6111778, Value_loss: 1.43708313602. Times trained:               15262. Times reached goal: 103.               Steps done: 9522925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.99673718214) A[1]:(0.00160031882115) A[2]:(0.00104405207094) A[3]:(0.000618429447059)\n",
      " state (1)  A[0]:(1.54997724167e-05) A[1]:(2.73534260486e-06) A[2]:(4.67982936243e-05) A[3]:(0.999934971333)\n",
      " state (2)  A[0]:(0.999995350838) A[1]:(5.46406688784e-08) A[2]:(2.91618471238e-06) A[3]:(1.70066300598e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.43612734152e-11) A[2]:(4.00600175254e-09) A[3]:(2.97625964672e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.5526283384e-11) A[2]:(7.80765319064e-09) A[3]:(1.79396089455e-13)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(6.94954640709e-12) A[2]:(4.54182185194e-08) A[3]:(3.18680502765e-16)\n",
      " state (6)  A[0]:(0.999937534332) A[1]:(2.81429150817e-11) A[2]:(6.24365493422e-05) A[3]:(4.27445096861e-20)\n",
      " state (7)  A[0]:(0.0888459160924) A[1]:(1.14037917998e-10) A[2]:(0.911154091358) A[3]:(1.51416466781e-24)\n",
      " state (8)  A[0]:(4.17877017753e-05) A[1]:(6.24241272822e-13) A[2]:(0.999958217144) A[3]:(4.82980889686e-29)\n",
      " state (9)  A[0]:(1.13675025659e-06) A[1]:(9.77151709157e-14) A[2]:(0.999998867512) A[3]:(1.87559884979e-30)\n",
      " state (10)  A[0]:(1.90825062418e-07) A[1]:(4.49112047465e-14) A[2]:(0.999999821186) A[3]:(5.41145407169e-31)\n",
      " state (11)  A[0]:(7.47951958147e-08) A[1]:(3.05299066735e-14) A[2]:(0.999999940395) A[3]:(2.99529126126e-31)\n",
      " state (12)  A[0]:(4.60574867134e-08) A[1]:(2.51086688062e-14) A[2]:(0.999999940395) A[3]:(2.23491333648e-31)\n",
      " state (13)  A[0]:(3.59115013282e-08) A[1]:(2.27343388933e-14) A[2]:(0.999999940395) A[3]:(1.92955775505e-31)\n",
      " state (14)  A[0]:(3.16017292334e-08) A[1]:(2.16086422708e-14) A[2]:(0.999999940395) A[3]:(1.7908913868e-31)\n",
      " state (15)  A[0]:(2.95858981758e-08) A[1]:(2.1051589359e-14) A[2]:(1.0) A[3]:(1.72375584814e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 743000 finished after 17 . Running score: 0.08. Policy_loss: -92050.6111783, Value_loss: 1.22311279869. Times trained:               15579. Times reached goal: 123.               Steps done: 9538504.\n",
      " state (0)  A[0]:(0.995121836662) A[1]:(0.00189057609532) A[2]:(0.00142648303881) A[3]:(0.00156110106036)\n",
      " state (1)  A[0]:(1.36912012749e-05) A[1]:(2.48043443207e-06) A[2]:(4.10396751249e-05) A[3]:(0.999942779541)\n",
      " state (2)  A[0]:(0.999999701977) A[1]:(4.33320535009e-09) A[2]:(2.66603592536e-07) A[3]:(3.36258736411e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.48320912444e-11) A[2]:(4.20454471239e-09) A[3]:(2.87068784542e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.66721133427e-11) A[2]:(8.08606692715e-09) A[3]:(2.02115207166e-13)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(8.31918388344e-12) A[2]:(4.29155626591e-08) A[3]:(5.64428557095e-16)\n",
      " state (6)  A[0]:(0.99997729063) A[1]:(2.80661986707e-11) A[2]:(2.2730891942e-05) A[3]:(1.59387092863e-19)\n",
      " state (7)  A[0]:(0.296081513166) A[1]:(3.20347887106e-10) A[2]:(0.703918516636) A[3]:(1.58255705447e-23)\n",
      " state (8)  A[0]:(0.00162461830769) A[1]:(7.48889145141e-12) A[2]:(0.998375356197) A[3]:(5.28654827485e-27)\n",
      " state (9)  A[0]:(7.27361912141e-05) A[1]:(9.61086678991e-13) A[2]:(0.999927282333) A[3]:(9.8520729638e-29)\n",
      " state (10)  A[0]:(1.11827075671e-05) A[1]:(3.37195793206e-13) A[2]:(0.999988794327) A[3]:(1.45962875154e-29)\n",
      " state (11)  A[0]:(2.40500116888e-06) A[1]:(1.59708834699e-13) A[2]:(0.999997615814) A[3]:(4.06055245384e-30)\n",
      " state (12)  A[0]:(6.96976314885e-07) A[1]:(9.22984629806e-14) A[2]:(0.999999284744) A[3]:(1.66448062862e-30)\n",
      " state (13)  A[0]:(2.88627575173e-07) A[1]:(6.37401915103e-14) A[2]:(0.999999701977) A[3]:(9.30737807151e-31)\n",
      " state (14)  A[0]:(1.6381015655e-07) A[1]:(5.05709738679e-14) A[2]:(0.999999821186) A[3]:(6.52362642263e-31)\n",
      " state (15)  A[0]:(1.1616987905e-07) A[1]:(4.40327163837e-14) A[2]:(0.999999880791) A[3]:(5.28956800285e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 744000 finished after 7 . Running score: 0.14. Policy_loss: -92050.6111776, Value_loss: 1.41919509493. Times trained:               15529. Times reached goal: 126.               Steps done: 9554033.\n",
      "action_dist \n",
      "tensor([[ 0.9939,  0.0014,  0.0035,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1154e-11,  8.6325e-09,  5.8693e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1155e-11,  8.6306e-09,  5.8725e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1156e-11,  8.6289e-09,  5.8754e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1156e-11,  8.6274e-09,  5.8780e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1156e-11,  8.6260e-09,  5.8804e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.8845e-02,  8.8151e-11,  9.2116e-01,  8.8552e-25]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1159e-11,  8.6176e-09,  5.8939e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.0283e-02,  8.9298e-11,  9.1972e-01,  9.0978e-25]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.5967e-03,  9.0991e-12,  9.9640e-01,  8.6002e-27]])\n",
      "On state=9, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.3002e-04,  2.7013e-12,  9.9937e-01,  7.8828e-28]])\n",
      "On state=10, selected action=2\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.7694e-12,  9.9146e-07,  7.2210e-19]])\n",
      "On state=6, selected action=0\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.3188e-04,  2.7066e-12,  9.9937e-01,  7.9124e-28]])\n",
      "On state=10, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.2988e-06,  1.5898e-13,  1.0000e+00,  4.4688e-30]])\n",
      "On state=14, selected action=2\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.99389398098) A[1]:(0.00138980406336) A[2]:(0.00350519409403) A[3]:(0.00121102540288)\n",
      " state (1)  A[0]:(1.98512607312e-05) A[1]:(3.16209320772e-06) A[2]:(6.3037914515e-05) A[3]:(0.999913930893)\n",
      " state (2)  A[0]:(0.999992489815) A[1]:(7.36634291343e-08) A[2]:(5.70029305891e-06) A[3]:(1.72269005816e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.67787277128e-11) A[2]:(4.79101869288e-09) A[3]:(7.595054685e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.1161106761e-11) A[2]:(8.61041371536e-09) A[3]:(5.90706072923e-14)\n",
      " state (5)  A[0]:(1.0) A[1]:(6.04563794859e-12) A[2]:(2.95663795669e-08) A[3]:(5.6363896357e-16)\n",
      " state (6)  A[0]:(0.999998986721) A[1]:(6.76828124796e-12) A[2]:(9.90878675111e-07) A[3]:(7.22533217777e-19)\n",
      " state (7)  A[0]:(0.842198610306) A[1]:(3.02815328368e-10) A[2]:(0.157801359892) A[3]:(1.04844934382e-22)\n",
      " state (8)  A[0]:(0.0805137827992) A[1]:(8.94891730163e-11) A[2]:(0.919486224651) A[3]:(9.13963693774e-25)\n",
      " state (9)  A[0]:(0.00360150635242) A[1]:(9.10862010817e-12) A[2]:(0.996398508549) A[3]:(8.61878687203e-27)\n",
      " state (10)  A[0]:(0.000630017369986) A[1]:(2.70165053523e-12) A[2]:(0.999369978905) A[3]:(7.8853489343e-28)\n",
      " state (11)  A[0]:(0.000164087468875) A[1]:(1.13103612847e-12) A[2]:(0.999835908413) A[3]:(1.48398992968e-28)\n",
      " state (12)  A[0]:(4.49285907962e-05) A[1]:(5.26391647285e-13) A[2]:(0.999955058098) A[3]:(3.59234805101e-29)\n",
      " state (13)  A[0]:(1.29557274704e-05) A[1]:(2.7151661453e-13) A[2]:(0.999987065792) A[3]:(1.10861364993e-29)\n",
      " state (14)  A[0]:(4.28954581366e-06) A[1]:(1.58825142166e-13) A[2]:(0.999995708466) A[3]:(4.46198396273e-30)\n",
      " state (15)  A[0]:(1.75998059149e-06) A[1]:(1.06008070703e-13) A[2]:(0.999998211861) A[3]:(2.30484293117e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 745000 finished after 14 . Running score: 0.1. Policy_loss: -92050.6142331, Value_loss: 1.63983293088. Times trained:               15497. Times reached goal: 104.               Steps done: 9569530.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995667397976) A[1]:(0.00100784830283) A[2]:(0.00275279465131) A[3]:(0.000571959128138)\n",
      " state (1)  A[0]:(2.89249546768e-05) A[1]:(4.24267864219e-06) A[2]:(0.000104364611616) A[3]:(0.999862492085)\n",
      " state (2)  A[0]:(0.99999922514) A[1]:(5.49810375006e-09) A[2]:(7.36240053811e-07) A[3]:(1.44073473152e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.51720563196e-11) A[2]:(8.22963386327e-09) A[3]:(1.88226875492e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.46174562311e-12) A[2]:(1.91219218237e-08) A[3]:(6.74328011507e-15)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(5.61121418427e-12) A[2]:(1.28976921587e-07) A[3]:(1.98052102218e-17)\n",
      " state (6)  A[0]:(0.99697381258) A[1]:(1.01269957697e-10) A[2]:(0.00302616553381) A[3]:(3.08806051076e-21)\n",
      " state (7)  A[0]:(0.0379073098302) A[1]:(6.74126379718e-11) A[2]:(0.96209269762) A[3]:(4.67823162308e-25)\n",
      " state (8)  A[0]:(0.000151408210513) A[1]:(1.41050148991e-12) A[2]:(0.999848604202) A[3]:(2.18536402273e-28)\n",
      " state (9)  A[0]:(9.01159819477e-06) A[1]:(2.69319966719e-13) A[2]:(0.999990999699) A[3]:(1.02795931498e-29)\n",
      " state (10)  A[0]:(1.34522542794e-06) A[1]:(1.0452696922e-13) A[2]:(0.999998629093) A[3]:(2.02624230647e-30)\n",
      " state (11)  A[0]:(3.36809648616e-07) A[1]:(5.59948985237e-14) A[2]:(0.999999642372) A[3]:(7.35493743812e-31)\n",
      " state (12)  A[0]:(1.36924910521e-07) A[1]:(3.80873040355e-14) A[2]:(0.999999880791) A[3]:(4.02249464874e-31)\n",
      " state (13)  A[0]:(8.05213389299e-08) A[1]:(3.05147142905e-14) A[2]:(0.999999940395) A[3]:(2.86445756452e-31)\n",
      " state (14)  A[0]:(5.97241225364e-08) A[1]:(2.69771574193e-14) A[2]:(0.999999940395) A[3]:(2.37765549629e-31)\n",
      " state (15)  A[0]:(5.06259532074e-08) A[1]:(2.52105939748e-14) A[2]:(0.999999940395) A[3]:(2.14793451278e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 746000 finished after 18 . Running score: 0.08. Policy_loss: -92050.6123724, Value_loss: 1.42921422524. Times trained:               17241. Times reached goal: 115.               Steps done: 9586771.\n",
      " state (0)  A[0]:(0.995407223701) A[1]:(0.00119987351354) A[2]:(0.00226598978043) A[3]:(0.00112692744005)\n",
      " state (1)  A[0]:(2.35874340433e-05) A[1]:(3.60373906005e-06) A[2]:(8.13768419903e-05) A[3]:(0.999891459942)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(3.31313115831e-10) A[2]:(5.61879787142e-08) A[3]:(1.93102159263e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.37402632674e-11) A[2]:(8.23318213605e-09) A[3]:(1.95874742749e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.02732402719e-12) A[2]:(1.65523381668e-08) A[3]:(1.32133319653e-14)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(5.24903558119e-12) A[2]:(7.28277100848e-08) A[3]:(1.15414459462e-16)\n",
      " state (6)  A[0]:(0.99996727705) A[1]:(2.09903119192e-11) A[2]:(3.27449561155e-05) A[3]:(1.24690510931e-19)\n",
      " state (7)  A[0]:(0.328112185001) A[1]:(2.44095438307e-10) A[2]:(0.671887814999) A[3]:(2.42686304842e-23)\n",
      " state (8)  A[0]:(0.00596506521106) A[1]:(1.37415799226e-11) A[2]:(0.994034945965) A[3]:(4.87183177102e-26)\n",
      " state (9)  A[0]:(0.000353985698894) A[1]:(1.91880235659e-12) A[2]:(0.999646008015) A[3]:(1.03343561553e-27)\n",
      " state (10)  A[0]:(5.95214332861e-05) A[1]:(6.19924359242e-13) A[2]:(0.99994045496) A[3]:(1.21953988717e-28)\n",
      " state (11)  A[0]:(1.22418441606e-05) A[1]:(2.51208315217e-13) A[2]:(0.999987781048) A[3]:(2.36541632434e-29)\n",
      " state (12)  A[0]:(2.85459236693e-06) A[1]:(1.18642237961e-13) A[2]:(0.999997138977) A[3]:(6.45280119528e-30)\n",
      " state (13)  A[0]:(8.5066938027e-07) A[1]:(6.68058680207e-14) A[2]:(0.999999165535) A[3]:(2.49365346691e-30)\n",
      " state (14)  A[0]:(3.48720192278e-07) A[1]:(4.47425875917e-14) A[2]:(0.999999642372) A[3]:(1.31382304656e-30)\n",
      " state (15)  A[0]:(1.91571203345e-07) A[1]:(3.44644458642e-14) A[2]:(0.999999821186) A[3]:(8.74313984272e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 747000 finished after 9 . Running score: 0.11. Policy_loss: -92050.6113339, Value_loss: 0.989842030453. Times trained:               15074. Times reached goal: 103.               Steps done: 9601845.\n",
      " state (0)  A[0]:(0.995687842369) A[1]:(0.00170251633972) A[2]:(0.00187832012307) A[3]:(0.000731327512767)\n",
      " state (1)  A[0]:(2.56703824562e-05) A[1]:(4.1318703552e-06) A[2]:(9.068979125e-05) A[3]:(0.999879479408)\n",
      " state (2)  A[0]:(1.0) A[1]:(9.20090045819e-11) A[2]:(1.84343225129e-08) A[3]:(1.39611664243e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.417737195e-11) A[2]:(9.73698544016e-09) A[3]:(8.97291477773e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.35757894782e-12) A[2]:(2.12488799889e-08) A[3]:(4.74525009469e-15)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(6.27107914208e-12) A[2]:(1.31791225044e-07) A[3]:(2.69792748826e-17)\n",
      " state (6)  A[0]:(0.997735619545) A[1]:(1.05521535954e-10) A[2]:(0.00226440420374) A[3]:(6.10139365422e-21)\n",
      " state (7)  A[0]:(0.201398164034) A[1]:(2.39862713025e-10) A[2]:(0.798601865768) A[3]:(9.69435268503e-24)\n",
      " state (8)  A[0]:(0.00268123554997) A[1]:(1.07111246955e-11) A[2]:(0.997318744659) A[3]:(1.50411107315e-26)\n",
      " state (9)  A[0]:(0.000160544601385) A[1]:(1.62347175633e-12) A[2]:(0.999839484692) A[3]:(3.81006841269e-28)\n",
      " state (10)  A[0]:(2.4100749215e-05) A[1]:(5.26505000622e-13) A[2]:(0.999975919724) A[3]:(4.652951051e-29)\n",
      " state (11)  A[0]:(4.67635300083e-06) A[1]:(2.21971909169e-13) A[2]:(0.999995350838) A[3]:(1.00225288982e-29)\n",
      " state (12)  A[0]:(1.19166304557e-06) A[1]:(1.15600020875e-13) A[2]:(0.999998807907) A[3]:(3.32286486141e-30)\n",
      " state (13)  A[0]:(4.35947185906e-07) A[1]:(7.37319005763e-14) A[2]:(0.999999582767) A[3]:(1.59884835915e-30)\n",
      " state (14)  A[0]:(2.24411550676e-07) A[1]:(5.53811892721e-14) A[2]:(0.999999761581) A[3]:(1.01637529775e-30)\n",
      " state (15)  A[0]:(1.48751979623e-07) A[1]:(4.6550152513e-14) A[2]:(0.999999880791) A[3]:(7.75752591557e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 748000 finished after 14 . Running score: 0.1. Policy_loss: -92050.611073, Value_loss: 1.43517980597. Times trained:               16227. Times reached goal: 109.               Steps done: 9618072.\n",
      " state (0)  A[0]:(0.989465892315) A[1]:(0.00259481696412) A[2]:(0.00296976789832) A[3]:(0.00496953772381)\n",
      " state (1)  A[0]:(1.44276536957e-05) A[1]:(2.60097931459e-06) A[2]:(4.74291482533e-05) A[3]:(0.999935567379)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.63382751772e-10) A[2]:(2.56456136327e-08) A[3]:(6.32470534279e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.61084843381e-11) A[2]:(9.06763197861e-09) A[3]:(2.39163642233e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.09039123217e-11) A[2]:(1.85801098951e-08) A[3]:(1.87968469457e-14)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(7.14800050747e-12) A[2]:(8.49591259566e-08) A[3]:(2.43300949676e-16)\n",
      " state (6)  A[0]:(0.999952375889) A[1]:(3.14976447702e-11) A[2]:(4.76416498714e-05) A[3]:(3.28631180097e-19)\n",
      " state (7)  A[0]:(0.504891216755) A[1]:(4.44623116103e-10) A[2]:(0.49510872364) A[3]:(2.05574211027e-22)\n",
      " state (8)  A[0]:(0.0370918288827) A[1]:(7.96863131036e-11) A[2]:(0.962908148766) A[3]:(2.87634502705e-24)\n",
      " state (9)  A[0]:(0.00214807409793) A[1]:(1.05982601167e-11) A[2]:(0.997851908207) A[3]:(5.0717556252e-26)\n",
      " state (10)  A[0]:(0.000392816931708) A[1]:(3.40518563867e-12) A[2]:(0.999607205391) A[3]:(5.54649874316e-27)\n",
      " state (11)  A[0]:(9.35746793402e-05) A[1]:(1.38545988917e-12) A[2]:(0.999906420708) A[3]:(9.96925761571e-28)\n",
      " state (12)  A[0]:(2.18670684262e-05) A[1]:(5.95355850123e-13) A[2]:(0.999978125095) A[3]:(2.0774678902e-28)\n",
      " state (13)  A[0]:(5.3245298659e-06) A[1]:(2.80286753009e-13) A[2]:(0.999994695187) A[3]:(5.38310178742e-29)\n",
      " state (14)  A[0]:(1.54561234922e-06) A[1]:(1.52388382945e-13) A[2]:(0.999998450279) A[3]:(1.88047784724e-29)\n",
      " state (15)  A[0]:(5.85102270634e-07) A[1]:(9.70898844177e-14) A[2]:(0.999999403954) A[3]:(8.86319279865e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 749000 finished after 12 . Running score: 0.12. Policy_loss: -92050.6110158, Value_loss: 1.43040138078. Times trained:               16135. Times reached goal: 104.               Steps done: 9634207.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9951,  0.0019,  0.0014,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9951,  0.0019,  0.0014,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.5920e-12,  2.7116e-08,  8.9320e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.5925e-12,  2.7116e-08,  8.9327e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.4088e-04,  4.2310e-12,  9.9946e-01,  1.0719e-26]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4229e-05,  4.4960e-13,  9.9999e-01,  1.5413e-28]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.4169e-08,  3.0015e-14,  1.0000e+00,  1.5344e-30]])\n",
      "On state=13, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4222e-05,  4.4952e-13,  9.9999e-01,  1.5407e-28]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995072007179) A[1]:(0.00187122425996) A[2]:(0.0013657619711) A[3]:(0.00169102614745)\n",
      " state (1)  A[0]:(1.74066735781e-05) A[1]:(2.83473514173e-06) A[2]:(6.04105698585e-05) A[3]:(0.999919354916)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(2.22235063685e-10) A[2]:(4.71605048347e-08) A[3]:(7.49779058284e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.42361192279e-11) A[2]:(1.24941212931e-08) A[3]:(1.23090001191e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.59380491716e-12) A[2]:(2.71152860165e-08) A[3]:(8.9345873839e-15)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(6.79897804723e-12) A[2]:(1.45906298599e-07) A[3]:(1.0861310915e-16)\n",
      " state (6)  A[0]:(0.999008834362) A[1]:(8.49146308823e-11) A[2]:(0.00099118007347) A[3]:(5.42653552419e-20)\n",
      " state (7)  A[0]:(0.127636060119) A[1]:(1.92843505054e-10) A[2]:(0.87236392498) A[3]:(2.40905940442e-23)\n",
      " state (8)  A[0]:(0.000540546898264) A[1]:(4.22978319037e-12) A[2]:(0.999459445477) A[3]:(1.07123536106e-26)\n",
      " state (9)  A[0]:(1.42202188727e-05) A[1]:(4.4949159954e-13) A[2]:(0.99998575449) A[3]:(1.54048985313e-28)\n",
      " state (10)  A[0]:(1.50276378008e-06) A[1]:(1.37202735609e-13) A[2]:(0.999998509884) A[3]:(1.8669546593e-29)\n",
      " state (11)  A[0]:(3.01310194573e-07) A[1]:(6.3950377654e-14) A[2]:(0.999999701977) A[3]:(5.1574466696e-30)\n",
      " state (12)  A[0]:(1.0342931489e-07) A[1]:(3.96795362409e-14) A[2]:(0.999999880791) A[3]:(2.38171443126e-30)\n",
      " state (13)  A[0]:(5.41597877657e-08) A[1]:(3.00144769607e-14) A[2]:(0.999999940395) A[3]:(1.53429350079e-30)\n",
      " state (14)  A[0]:(3.73497464068e-08) A[1]:(2.56418658821e-14) A[2]:(0.999999940395) A[3]:(1.20238035165e-30)\n",
      " state (15)  A[0]:(3.02965155186e-08) A[1]:(2.3486490598e-14) A[2]:(0.999999940395) A[3]:(1.05105764732e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 750000 finished after 8 . Running score: 0.11. Policy_loss: -92050.6123393, Value_loss: 1.63437647729. Times trained:               15986. Times reached goal: 121.               Steps done: 9650193.\n",
      " state (0)  A[0]:(0.995672166348) A[1]:(0.00128159648739) A[2]:(0.0018515419215) A[3]:(0.00119467941113)\n",
      " state (1)  A[0]:(3.16090845445e-05) A[1]:(4.22564016844e-06) A[2]:(0.000129885025672) A[3]:(0.999834299088)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(3.03053859785e-10) A[2]:(1.06279046008e-07) A[3]:(7.8168832518e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.11863834168e-11) A[2]:(1.41690401634e-08) A[3]:(7.44684262172e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.20140049623e-12) A[2]:(2.44924152071e-08) A[3]:(1.10336265068e-14)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(5.90360833136e-12) A[2]:(6.75428353247e-08) A[3]:(5.6996100573e-16)\n",
      " state (6)  A[0]:(0.999994516373) A[1]:(1.35745416643e-11) A[2]:(5.47269382878e-06) A[3]:(1.87412160717e-18)\n",
      " state (7)  A[0]:(0.275852292776) A[1]:(2.6460772462e-10) A[2]:(0.724147737026) A[3]:(7.97846030931e-23)\n",
      " state (8)  A[0]:(0.000617742654867) A[1]:(3.92085539383e-12) A[2]:(0.999382257462) A[3]:(1.23407959417e-26)\n",
      " state (9)  A[0]:(6.35870674159e-06) A[1]:(2.39442851397e-13) A[2]:(0.999993622303) A[3]:(6.31198983666e-29)\n",
      " state (10)  A[0]:(5.55169606287e-07) A[1]:(6.89200961384e-14) A[2]:(0.999999463558) A[3]:(7.14555970091e-30)\n",
      " state (11)  A[0]:(1.26150283108e-07) A[1]:(3.480107031e-14) A[2]:(0.999999880791) A[3]:(2.31197959247e-30)\n",
      " state (12)  A[0]:(5.26155581326e-08) A[1]:(2.37201412483e-14) A[2]:(0.999999940395) A[3]:(1.25645506662e-30)\n",
      " state (13)  A[0]:(3.2215801582e-08) A[1]:(1.92347104634e-14) A[2]:(0.999999940395) A[3]:(9.07147328241e-31)\n",
      " state (14)  A[0]:(2.461971782e-08) A[1]:(1.71733863295e-14) A[2]:(1.0) A[3]:(7.62497294079e-31)\n",
      " state (15)  A[0]:(2.12631512397e-08) A[1]:(1.61516850242e-14) A[2]:(1.0) A[3]:(6.94643152212e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 751000 finished after 3 . Running score: 0.12. Policy_loss: -92050.6328165, Value_loss: 0.996498554708. Times trained:               15827. Times reached goal: 121.               Steps done: 9666020.\n",
      " state (0)  A[0]:(0.997209787369) A[1]:(0.00102436332963) A[2]:(0.00110787549056) A[3]:(0.00065796385752)\n",
      " state (1)  A[0]:(0.000106895793579) A[1]:(1.10545906864e-05) A[2]:(0.000547860749066) A[3]:(0.999334216118)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.28543278052e-11) A[2]:(1.78790564576e-08) A[3]:(3.37350075175e-13)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.68803553405e-12) A[2]:(1.70652931786e-08) A[3]:(1.66142192235e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.07745580891e-12) A[2]:(2.53911558445e-08) A[3]:(4.17431897154e-15)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(5.62076947486e-12) A[2]:(5.72315279612e-08) A[3]:(3.90916054842e-16)\n",
      " state (6)  A[0]:(0.999998033047) A[1]:(9.80310450871e-12) A[2]:(1.94519111574e-06) A[3]:(2.26446234796e-18)\n",
      " state (7)  A[0]:(0.542221367359) A[1]:(3.7812569742e-10) A[2]:(0.457778602839) A[3]:(1.5821322371e-22)\n",
      " state (8)  A[0]:(0.00509020779282) A[1]:(1.69782087694e-11) A[2]:(0.994909763336) A[3]:(1.1736130206e-25)\n",
      " state (9)  A[0]:(2.70815162366e-05) A[1]:(5.78425165838e-13) A[2]:(0.999972939491) A[3]:(1.7289010518e-28)\n",
      " state (10)  A[0]:(1.66618929143e-06) A[1]:(1.28834212721e-13) A[2]:(0.99999833107) A[3]:(1.16078468471e-29)\n",
      " state (11)  A[0]:(3.25255086864e-07) A[1]:(5.900020869e-14) A[2]:(0.999999701977) A[3]:(3.09763149216e-30)\n",
      " state (12)  A[0]:(1.24509014654e-07) A[1]:(3.82904530294e-14) A[2]:(0.999999880791) A[3]:(1.53508954556e-30)\n",
      " state (13)  A[0]:(7.24489268578e-08) A[1]:(3.02202415924e-14) A[2]:(0.999999940395) A[3]:(1.05544515647e-30)\n",
      " state (14)  A[0]:(5.37668150002e-08) A[1]:(2.65787791895e-14) A[2]:(0.999999940395) A[3]:(8.6402257825e-31)\n",
      " state (15)  A[0]:(4.56812898619e-08) A[1]:(2.47915413648e-14) A[2]:(0.999999940395) A[3]:(7.75936156755e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 752000 finished after 5 . Running score: 0.17. Policy_loss: -92050.6115361, Value_loss: 1.21402463831. Times trained:               16324. Times reached goal: 85.               Steps done: 9682344.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996155679226) A[1]:(0.00126992270816) A[2]:(0.00101768260356) A[3]:(0.00155673734844)\n",
      " state (1)  A[0]:(3.13027667289e-05) A[1]:(3.98182646677e-06) A[2]:(0.000129522348288) A[3]:(0.999835193157)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.14634152065e-11) A[2]:(1.57832378278e-08) A[3]:(1.44294949253e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.14708763321e-11) A[2]:(1.80852204323e-08) A[3]:(4.40956746486e-14)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(8.8965258771e-12) A[2]:(3.20478754645e-08) A[3]:(6.79837156147e-15)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(6.95177335835e-12) A[2]:(1.15154207947e-07) A[3]:(2.37169489929e-16)\n",
      " state (6)  A[0]:(0.999838888645) A[1]:(5.12655994045e-11) A[2]:(0.000161126139574) A[3]:(2.6185400077e-19)\n",
      " state (7)  A[0]:(0.0747307315469) A[1]:(1.38425507168e-10) A[2]:(0.925269246101) A[3]:(1.40888222644e-23)\n",
      " state (8)  A[0]:(4.50736442872e-05) A[1]:(9.14646396816e-13) A[2]:(0.999954938889) A[3]:(7.23433020557e-28)\n",
      " state (9)  A[0]:(7.26008067886e-07) A[1]:(9.30933661322e-14) A[2]:(0.999999284744) A[3]:(1.18312050678e-29)\n",
      " state (10)  A[0]:(1.07703016283e-07) A[1]:(3.77227817126e-14) A[2]:(0.999999880791) A[3]:(2.64358861771e-30)\n",
      " state (11)  A[0]:(4.14659133696e-08) A[1]:(2.46812339582e-14) A[2]:(0.999999940395) A[3]:(1.3493891798e-30)\n",
      " state (12)  A[0]:(2.56303565038e-08) A[1]:(2.00443570704e-14) A[2]:(1.0) A[3]:(9.78132610901e-31)\n",
      " state (13)  A[0]:(2.01052223758e-08) A[1]:(1.80702705539e-14) A[2]:(1.0) A[3]:(8.35066014649e-31)\n",
      " state (14)  A[0]:(1.7776670802e-08) A[1]:(1.7150799349e-14) A[2]:(1.0) A[3]:(7.71585464083e-31)\n",
      " state (15)  A[0]:(1.66967062398e-08) A[1]:(1.67022293348e-14) A[2]:(1.0) A[3]:(7.41353065934e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 753000 finished after 29 . Running score: 0.13. Policy_loss: -92050.6115321, Value_loss: 1.44016809361. Times trained:               15247. Times reached goal: 113.               Steps done: 9697591.\n",
      " state (0)  A[0]:(0.995896756649) A[1]:(0.00158411322627) A[2]:(0.000958887278102) A[3]:(0.00156022631563)\n",
      " state (1)  A[0]:(3.79804077966e-05) A[1]:(4.87996021548e-06) A[2]:(0.000172713640495) A[3]:(0.99978441)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.77267271642e-11) A[2]:(1.86021313908e-08) A[3]:(6.6795765967e-13)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.18844725727e-11) A[2]:(2.49891609627e-08) A[3]:(2.54608159778e-14)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(9.5037241965e-12) A[2]:(4.62663365397e-08) A[3]:(3.91593834724e-15)\n",
      " state (5)  A[0]:(0.999999821186) A[1]:(7.84241508539e-12) A[2]:(2.06891073162e-07) A[3]:(1.06955320088e-16)\n",
      " state (6)  A[0]:(0.999486446381) A[1]:(7.27480090057e-11) A[2]:(0.000513577950187) A[3]:(1.40644291706e-19)\n",
      " state (7)  A[0]:(0.0999348685145) A[1]:(1.87084195358e-10) A[2]:(0.900065124035) A[3]:(2.94416016474e-23)\n",
      " state (8)  A[0]:(0.000160870069521) A[1]:(2.23440515222e-12) A[2]:(0.999839127064) A[3]:(4.47656685149e-27)\n",
      " state (9)  A[0]:(1.82081464573e-06) A[1]:(1.60273148377e-13) A[2]:(0.999998152256) A[3]:(3.4961616929e-29)\n",
      " state (10)  A[0]:(1.9571359644e-07) A[1]:(5.24213613869e-14) A[2]:(0.999999821186) A[3]:(5.17609245503e-30)\n",
      " state (11)  A[0]:(6.00370668735e-08) A[1]:(3.03099491577e-14) A[2]:(0.999999940395) A[3]:(2.11937550137e-30)\n",
      " state (12)  A[0]:(3.22627791149e-08) A[1]:(2.29586670991e-14) A[2]:(0.999999940395) A[3]:(1.36432698585e-30)\n",
      " state (13)  A[0]:(2.33750157008e-08) A[1]:(1.99265686688e-14) A[2]:(1.0) A[3]:(1.09367787513e-30)\n",
      " state (14)  A[0]:(1.97834797433e-08) A[1]:(1.85297283295e-14) A[2]:(1.0) A[3]:(9.77319544969e-31)\n",
      " state (15)  A[0]:(1.81441652813e-08) A[1]:(1.78475534048e-14) A[2]:(1.0) A[3]:(9.2244587e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 754000 finished after 26 . Running score: 0.1. Policy_loss: -92050.6111803, Value_loss: 0.995477131742. Times trained:               16222. Times reached goal: 87.               Steps done: 9713813.\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0015,  0.0011,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0015,  0.0011,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9955,  0.0015,  0.0011,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0504e-11,  1.1825e-08,  2.5571e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9986e-01,  6.6131e-11,  1.3599e-04,  1.4728e-19]])\n",
      "On state=8, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0506e-11,  1.1825e-08,  2.5578e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9986e-01,  6.6151e-11,  1.3608e-04,  1.4722e-19]])\n",
      "On state=8, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9986e-01,  6.6160e-11,  1.3612e-04,  1.4719e-19]])\n",
      "On state=8, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9986e-01,  6.6168e-11,  1.3615e-04,  1.4717e-19]])\n",
      "On state=8, selected action=0\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995531797409) A[1]:(0.00149255362339) A[2]:(0.00107742310502) A[3]:(0.00189822888933)\n",
      " state (1)  A[0]:(2.45406790782e-05) A[1]:(3.38695440405e-06) A[2]:(8.20374261821e-05) A[3]:(0.99989002943)\n",
      " state (2)  A[0]:(0.0882517918944) A[1]:(0.000273711106274) A[2]:(0.0261313263327) A[3]:(0.885343194008)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.39008232009e-11) A[2]:(8.45239789271e-09) A[3]:(1.25132163745e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.05069911105e-11) A[2]:(1.18241212377e-08) A[3]:(2.558607521e-14)\n",
      " state (5)  A[0]:(1.0) A[1]:(8.93231755922e-12) A[2]:(1.63374505036e-08) A[3]:(7.82622190253e-15)\n",
      " state (6)  A[0]:(1.0) A[1]:(7.49535676581e-12) A[2]:(2.76313727454e-08) A[3]:(1.45597414452e-15)\n",
      " state (7)  A[0]:(0.999999880791) A[1]:(6.91305779982e-12) A[2]:(1.20644216395e-07) A[3]:(4.71146095781e-17)\n",
      " state (8)  A[0]:(0.999863803387) A[1]:(6.61799712254e-11) A[2]:(0.000136219372507) A[3]:(1.47125962664e-19)\n",
      " state (9)  A[0]:(0.632997393608) A[1]:(5.62691004902e-10) A[2]:(0.367002606392) A[3]:(2.25772092048e-22)\n",
      " state (10)  A[0]:(0.190678104758) A[1]:(2.8660807061e-10) A[2]:(0.809321880341) A[3]:(2.30057020403e-23)\n",
      " state (11)  A[0]:(0.0296549722552) A[1]:(7.71062311222e-11) A[2]:(0.970345020294) A[3]:(1.39171018393e-24)\n",
      " state (12)  A[0]:(0.00665630446747) A[1]:(2.68781524815e-11) A[2]:(0.993343710899) A[3]:(1.68752939956e-25)\n",
      " state (13)  A[0]:(0.00184545479715) A[1]:(1.11743045372e-11) A[2]:(0.998154520988) A[3]:(3.00593013559e-26)\n",
      " state (14)  A[0]:(0.000507850258145) A[1]:(4.78660600739e-12) A[2]:(0.999492168427) A[3]:(5.81835608077e-27)\n",
      " state (15)  A[0]:(0.000136076545459) A[1]:(2.10365470703e-12) A[2]:(0.999863922596) A[3]:(1.21729519174e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 755000 finished after 9 . Running score: 0.04. Policy_loss: -92050.6018982, Value_loss: 1.20322276662. Times trained:               16314. Times reached goal: 102.               Steps done: 9730127.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.992848515511) A[1]:(0.00278177973814) A[2]:(0.0013089068234) A[3]:(0.00306078675203)\n",
      " state (1)  A[0]:(0.000105461811472) A[1]:(1.53586206579e-05) A[2]:(0.000691274472047) A[3]:(0.999187886715)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(2.41133935736e-11) A[2]:(3.24609175095e-08) A[3]:(2.96036422643e-14)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(1.69085804386e-11) A[2]:(8.74291004038e-08) A[3]:(8.83598353736e-16)\n",
      " state (4)  A[0]:(0.999999582767) A[1]:(1.90787125681e-11) A[2]:(4.23190215315e-07) A[3]:(3.37676301868e-17)\n",
      " state (5)  A[0]:(0.999642074108) A[1]:(1.93892818467e-10) A[2]:(0.000357935699867) A[3]:(2.69266236992e-19)\n",
      " state (6)  A[0]:(0.329205542803) A[1]:(8.96184570998e-10) A[2]:(0.670794487) A[3]:(1.58955636485e-22)\n",
      " state (7)  A[0]:(0.069049693644) A[1]:(3.32099236999e-10) A[2]:(0.930950284004) A[3]:(1.45026855166e-23)\n",
      " state (8)  A[0]:(0.00020326799131) A[1]:(5.96160490765e-12) A[2]:(0.999796748161) A[3]:(5.2779832792e-27)\n",
      " state (9)  A[0]:(2.47258481068e-06) A[1]:(4.42539912051e-13) A[2]:(0.99999755621) A[3]:(4.36920379229e-29)\n",
      " state (10)  A[0]:(2.83359383957e-07) A[1]:(1.46787882413e-13) A[2]:(0.999999701977) A[3]:(6.56209019646e-30)\n",
      " state (11)  A[0]:(9.51074241584e-08) A[1]:(8.73501507129e-14) A[2]:(0.999999880791) A[3]:(2.79781554541e-30)\n",
      " state (12)  A[0]:(5.46714673533e-08) A[1]:(6.76782916276e-14) A[2]:(0.999999940395) A[3]:(1.85998283052e-30)\n",
      " state (13)  A[0]:(4.13070893046e-08) A[1]:(5.95896758587e-14) A[2]:(0.999999940395) A[3]:(1.52173809265e-30)\n",
      " state (14)  A[0]:(3.58374663278e-08) A[1]:(5.58937560135e-14) A[2]:(0.999999940395) A[3]:(1.37668603942e-30)\n",
      " state (15)  A[0]:(3.33448646472e-08) A[1]:(5.41131572331e-14) A[2]:(0.999999940395) A[3]:(1.30895095162e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 756000 finished after 43 . Running score: 0.14. Policy_loss: -92050.6019969, Value_loss: 0.997470315073. Times trained:               16013. Times reached goal: 93.               Steps done: 9746140.\n",
      " state (0)  A[0]:(0.994772255421) A[1]:(0.00193748471793) A[2]:(0.00113487278577) A[3]:(0.0021553770639)\n",
      " state (1)  A[0]:(0.00015457146219) A[1]:(2.07853172469e-05) A[2]:(0.00162295124028) A[3]:(0.998201668262)\n",
      " state (2)  A[0]:(0.102729476988) A[1]:(0.00112007488497) A[2]:(0.676203250885) A[3]:(0.219947233796)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(2.03769761481e-11) A[2]:(1.66151934877e-07) A[3]:(1.74410214241e-15)\n",
      " state (4)  A[0]:(0.999984025955) A[1]:(5.6426172762e-11) A[2]:(1.59767951118e-05) A[3]:(4.03193597234e-18)\n",
      " state (5)  A[0]:(0.0668282657862) A[1]:(3.29616417494e-10) A[2]:(0.933171749115) A[3]:(3.31505069239e-23)\n",
      " state (6)  A[0]:(2.69882093562e-06) A[1]:(4.26722703397e-13) A[2]:(0.999997317791) A[3]:(8.01928881561e-29)\n",
      " state (7)  A[0]:(7.25556548176e-09) A[1]:(2.10116365089e-14) A[2]:(1.0) A[3]:(4.80904707113e-31)\n",
      " state (8)  A[0]:(2.65423749646e-09) A[1]:(1.35058725813e-14) A[2]:(1.0) A[3]:(2.44282137662e-31)\n",
      " state (9)  A[0]:(2.15588036312e-09) A[1]:(1.23572637176e-14) A[2]:(1.0) A[3]:(2.13801051927e-31)\n",
      " state (10)  A[0]:(2.04022021499e-09) A[1]:(1.20705015603e-14) A[2]:(1.0) A[3]:(2.06430937446e-31)\n",
      " state (11)  A[0]:(2.0065906714e-09) A[1]:(1.19853434089e-14) A[2]:(1.0) A[3]:(2.04258012629e-31)\n",
      " state (12)  A[0]:(1.99592564698e-09) A[1]:(1.1958170592e-14) A[2]:(1.0) A[3]:(2.03567268639e-31)\n",
      " state (13)  A[0]:(1.99241112497e-09) A[1]:(1.19491878076e-14) A[2]:(1.0) A[3]:(2.03339105185e-31)\n",
      " state (14)  A[0]:(1.99125627098e-09) A[1]:(1.19462248863e-14) A[2]:(1.0) A[3]:(2.0326309772e-31)\n",
      " state (15)  A[0]:(1.99085370411e-09) A[1]:(1.19452228463e-14) A[2]:(1.0) A[3]:(2.03236743137e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 757000 finished after 22 . Running score: 0.13. Policy_loss: -92050.6111823, Value_loss: 1.63784610712. Times trained:               16010. Times reached goal: 96.               Steps done: 9762150.\n",
      " state (0)  A[0]:(0.99636477232) A[1]:(0.00144278723747) A[2]:(0.000854572106618) A[3]:(0.00133784115314)\n",
      " state (1)  A[0]:(0.000229694545851) A[1]:(2.71403514489e-05) A[2]:(0.00227656052448) A[3]:(0.997466623783)\n",
      " state (2)  A[0]:(0.999996960163) A[1]:(1.76755166059e-09) A[2]:(3.05627440866e-06) A[3]:(3.05429674607e-11)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.38746505604e-11) A[2]:(1.07350260237e-07) A[3]:(1.15714303831e-15)\n",
      " state (4)  A[0]:(0.999999463558) A[1]:(1.15833731495e-11) A[2]:(5.66140897718e-07) A[3]:(1.23489917725e-17)\n",
      " state (5)  A[0]:(0.981331884861) A[1]:(1.80347042855e-10) A[2]:(0.0186681225896) A[3]:(1.96165195642e-21)\n",
      " state (6)  A[0]:(3.35243703375e-06) A[1]:(3.96378242729e-13) A[2]:(0.99999666214) A[3]:(6.7343920143e-29)\n",
      " state (7)  A[0]:(8.03249644576e-09) A[1]:(1.97717632333e-14) A[2]:(1.0) A[3]:(4.24915252907e-31)\n",
      " state (8)  A[0]:(3.36833827497e-09) A[1]:(1.36486391376e-14) A[2]:(1.0) A[3]:(2.42495785916e-31)\n",
      " state (9)  A[0]:(2.84925438798e-09) A[1]:(1.27381015892e-14) A[2]:(1.0) A[3]:(2.18922962932e-31)\n",
      " state (10)  A[0]:(2.73452505084e-09) A[1]:(1.25243612952e-14) A[2]:(1.0) A[3]:(2.13522295197e-31)\n",
      " state (11)  A[0]:(2.70322364493e-09) A[1]:(1.24650672949e-14) A[2]:(1.0) A[3]:(2.12032073989e-31)\n",
      " state (12)  A[0]:(2.69386646323e-09) A[1]:(1.24472016759e-14) A[2]:(1.0) A[3]:(2.11582823558e-31)\n",
      " state (13)  A[0]:(2.69094946326e-09) A[1]:(1.24415994e-14) A[2]:(1.0) A[3]:(2.1144082384e-31)\n",
      " state (14)  A[0]:(2.6900053296e-09) A[1]:(1.2439748633e-14) A[2]:(1.0) A[3]:(2.11395661347e-31)\n",
      " state (15)  A[0]:(2.68971800388e-09) A[1]:(1.24392268607e-14) A[2]:(1.0) A[3]:(2.11382754419e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 758000 finished after 32 . Running score: 0.14. Policy_loss: -92050.6111801, Value_loss: 1.61527513442. Times trained:               16227. Times reached goal: 123.               Steps done: 9778377.\n",
      " state (0)  A[0]:(0.996811032295) A[1]:(0.00122622295748) A[2]:(0.000732574379072) A[3]:(0.00123017723672)\n",
      " state (1)  A[0]:(6.98453441146e-05) A[1]:(9.02380634216e-06) A[2]:(0.000437802111264) A[3]:(0.999483346939)\n",
      " state (2)  A[0]:(0.999614834785) A[1]:(5.1143086921e-07) A[2]:(0.000383875827538) A[3]:(7.95212315552e-07)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(1.34809533328e-11) A[2]:(6.59229826283e-08) A[3]:(3.79632628366e-15)\n",
      " state (4)  A[0]:(0.999999642372) A[1]:(1.16897325153e-11) A[2]:(3.31220007865e-07) A[3]:(5.76555938082e-17)\n",
      " state (5)  A[0]:(0.919217228889) A[1]:(3.19387821746e-10) A[2]:(0.08078276366) A[3]:(1.27831251901e-21)\n",
      " state (6)  A[0]:(1.57601277806e-06) A[1]:(2.90855637101e-13) A[2]:(0.999998450279) A[3]:(4.3867095927e-29)\n",
      " state (7)  A[0]:(5.48345679974e-09) A[1]:(1.86854621131e-14) A[2]:(1.0) A[3]:(4.40236928394e-31)\n",
      " state (8)  A[0]:(2.35882824384e-09) A[1]:(1.31180094185e-14) A[2]:(1.0) A[3]:(2.57724150662e-31)\n",
      " state (9)  A[0]:(1.99222882635e-09) A[1]:(1.22481497804e-14) A[2]:(1.0) A[3]:(2.32789964161e-31)\n",
      " state (10)  A[0]:(1.9079724467e-09) A[1]:(1.2035374257e-14) A[2]:(1.0) A[3]:(2.26837707456e-31)\n",
      " state (11)  A[0]:(1.88414750468e-09) A[1]:(1.19740101081e-14) A[2]:(1.0) A[3]:(2.25129102907e-31)\n",
      " state (12)  A[0]:(1.87680204711e-09) A[1]:(1.1954932385e-14) A[2]:(1.0) A[3]:(2.24599001975e-31)\n",
      " state (13)  A[0]:(1.87444793021e-09) A[1]:(1.19488227364e-14) A[2]:(1.0) A[3]:(2.2442942516e-31)\n",
      " state (14)  A[0]:(1.87368298654e-09) A[1]:(1.19468178094e-14) A[2]:(1.0) A[3]:(2.24374623613e-31)\n",
      " state (15)  A[0]:(1.87342563684e-09) A[1]:(1.19461342538e-14) A[2]:(1.0) A[3]:(2.24354075972e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 759000 finished after 10 . Running score: 0.15. Policy_loss: -92050.6111886, Value_loss: 1.63917313957. Times trained:               16047. Times reached goal: 123.               Steps done: 9794424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9976,  0.0012,  0.0005,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.3708e-12,  9.5726e-08,  1.4494e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7007e-09,  6.1889e-15,  1.0000e+00,  1.1904e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.99761736393) A[1]:(0.00119650212582) A[2]:(0.000528011820279) A[3]:(0.000658097211272)\n",
      " state (1)  A[0]:(0.000138127594255) A[1]:(1.45109124787e-05) A[2]:(0.000924300809857) A[3]:(0.998923063278)\n",
      " state (2)  A[0]:(0.99999833107) A[1]:(1.11555231719e-09) A[2]:(1.66594929851e-06) A[3]:(4.51025501669e-11)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(6.76975966604e-12) A[2]:(3.86394987117e-08) A[3]:(2.17430795286e-15)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(5.36863652492e-12) A[2]:(9.56422425702e-08) A[3]:(1.4503301787e-16)\n",
      " state (5)  A[0]:(0.844044268131) A[1]:(2.23003546185e-10) A[2]:(0.155955702066) A[3]:(7.17217542307e-22)\n",
      " state (6)  A[0]:(1.9932256734e-08) A[1]:(1.86707847262e-14) A[2]:(1.0) A[3]:(6.93991599203e-31)\n",
      " state (7)  A[0]:(2.20511209292e-09) A[1]:(6.88268267995e-15) A[2]:(1.0) A[3]:(1.3968775529e-31)\n",
      " state (8)  A[0]:(1.7013155329e-09) A[1]:(6.18894956713e-15) A[2]:(1.0) A[3]:(1.19037246527e-31)\n",
      " state (9)  A[0]:(1.62326418973e-09) A[1]:(6.07299626237e-15) A[2]:(1.0) A[3]:(1.1573543571e-31)\n",
      " state (10)  A[0]:(1.6069435782e-09) A[1]:(6.04832769833e-15) A[2]:(1.0) A[3]:(1.15039977987e-31)\n",
      " state (11)  A[0]:(1.60308621933e-09) A[1]:(6.04247004198e-15) A[2]:(1.0) A[3]:(1.1487596126e-31)\n",
      " state (12)  A[0]:(1.60212032529e-09) A[1]:(6.04101780399e-15) A[2]:(1.0) A[3]:(1.14835653559e-31)\n",
      " state (13)  A[0]:(1.6018575355e-09) A[1]:(6.04060318137e-15) A[2]:(1.0) A[3]:(1.14824263019e-31)\n",
      " state (14)  A[0]:(1.60179036701e-09) A[1]:(6.04051085478e-15) A[2]:(1.0) A[3]:(1.14821641666e-31)\n",
      " state (15)  A[0]:(1.60177204833e-09) A[1]:(6.04048798489e-15) A[2]:(1.0) A[3]:(1.14820760046e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 760000 finished after 3 . Running score: 0.16. Policy_loss: -92050.6111892, Value_loss: 0.979956305521. Times trained:               15790. Times reached goal: 120.               Steps done: 9810214.\n",
      " state (0)  A[0]:(0.995863139629) A[1]:(0.00196034787223) A[2]:(0.00121433695313) A[3]:(0.000962170946877)\n",
      " state (1)  A[0]:(8.43211164465e-05) A[1]:(1.14845634016e-05) A[2]:(0.000597033707891) A[3]:(0.999307155609)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.57899596176e-11) A[2]:(2.61916621724e-08) A[3]:(5.16372053894e-14)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(7.64109001766e-12) A[2]:(3.47877708862e-08) A[3]:(2.25541792206e-15)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(6.39788872922e-12) A[2]:(8.20863093054e-08) A[3]:(1.90576867713e-16)\n",
      " state (5)  A[0]:(0.690413177013) A[1]:(4.71825578607e-10) A[2]:(0.309586852789) A[3]:(1.06571336909e-21)\n",
      " state (6)  A[0]:(4.32759028612e-09) A[1]:(1.32505328053e-14) A[2]:(1.0) A[3]:(3.27031532685e-31)\n",
      " state (7)  A[0]:(8.27294066585e-10) A[1]:(6.36942433611e-15) A[2]:(1.0) A[3]:(9.95823565782e-32)\n",
      " state (8)  A[0]:(6.79219402944e-10) A[1]:(5.90679865724e-15) A[2]:(1.0) A[3]:(8.87577521965e-32)\n",
      " state (9)  A[0]:(6.55530352223e-10) A[1]:(5.82919222806e-15) A[2]:(1.0) A[3]:(8.70031799411e-32)\n",
      " state (10)  A[0]:(6.50793807733e-10) A[1]:(5.81349204887e-15) A[2]:(1.0) A[3]:(8.66501025804e-32)\n",
      " state (11)  A[0]:(6.49729658964e-10) A[1]:(5.80998914411e-15) A[2]:(1.0) A[3]:(8.65727903169e-32)\n",
      " state (12)  A[0]:(6.49439779732e-10) A[1]:(5.80910272413e-15) A[2]:(1.0) A[3]:(8.65529773597e-32)\n",
      " state (13)  A[0]:(6.49308440348e-10) A[1]:(5.80872579447e-15) A[2]:(1.0) A[3]:(8.65450545277e-32)\n",
      " state (14)  A[0]:(6.49206910452e-10) A[1]:(5.8084822725e-15) A[2]:(1.0) A[3]:(8.65404289575e-32)\n",
      " state (15)  A[0]:(6.49097942063e-10) A[1]:(5.80821630416e-15) A[2]:(1.0) A[3]:(8.65358092647e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 761000 finished after 30 . Running score: 0.1. Policy_loss: -92050.6111796, Value_loss: 0.979104144345. Times trained:               15409. Times reached goal: 115.               Steps done: 9825623.\n",
      " state (0)  A[0]:(0.994270980358) A[1]:(0.00156985432841) A[2]:(0.00239099515602) A[3]:(0.00176818866748)\n",
      " state (1)  A[0]:(0.000346826855093) A[1]:(3.81200916308e-05) A[2]:(0.00369476503693) A[3]:(0.995920300484)\n",
      " state (2)  A[0]:(0.99998831749) A[1]:(7.41420613792e-09) A[2]:(1.17037243399e-05) A[3]:(3.72236796942e-10)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(6.04771354523e-12) A[2]:(3.41140058424e-08) A[3]:(9.15005329568e-16)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(4.81557328735e-12) A[2]:(9.97927074309e-08) A[3]:(3.77257456661e-17)\n",
      " state (5)  A[0]:(0.0551561824977) A[1]:(1.05359991565e-10) A[2]:(0.944843828678) A[3]:(8.24402796647e-24)\n",
      " state (6)  A[0]:(4.12897449564e-09) A[1]:(1.3862675046e-14) A[2]:(1.0) A[3]:(2.02189706809e-31)\n",
      " state (7)  A[0]:(1.1552765411e-09) A[1]:(8.07801091643e-15) A[2]:(1.0) A[3]:(8.49592303639e-32)\n",
      " state (8)  A[0]:(9.94760163131e-10) A[1]:(7.63038873216e-15) A[2]:(1.0) A[3]:(7.78334071191e-32)\n",
      " state (9)  A[0]:(9.7147567768e-10) A[1]:(7.56197472804e-15) A[2]:(1.0) A[3]:(7.67636367302e-32)\n",
      " state (10)  A[0]:(9.67322888457e-10) A[1]:(7.54943694635e-15) A[2]:(1.0) A[3]:(7.65676935768e-32)\n",
      " state (11)  A[0]:(9.66474567043e-10) A[1]:(7.54684502554e-15) A[2]:(1.0) A[3]:(7.65268098833e-32)\n",
      " state (12)  A[0]:(9.66275504055e-10) A[1]:(7.54621144489e-15) A[2]:(1.0) A[3]:(7.65168828335e-32)\n",
      " state (13)  A[0]:(9.66223878685e-10) A[1]:(7.54606744929e-15) A[2]:(1.0) A[3]:(7.65145494772e-32)\n",
      " state (14)  A[0]:(9.66209112718e-10) A[1]:(7.54600985105e-15) A[2]:(1.0) A[3]:(7.65139676075e-32)\n",
      " state (15)  A[0]:(9.66209112718e-10) A[1]:(7.54600985105e-15) A[2]:(1.0) A[3]:(7.65139676075e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 762000 finished after 10 . Running score: 0.09. Policy_loss: -92050.6112068, Value_loss: 0.978978953247. Times trained:               15904. Times reached goal: 116.               Steps done: 9841527.\n",
      " state (0)  A[0]:(0.994563996792) A[1]:(0.0010497943731) A[2]:(0.00260940054432) A[3]:(0.00177679082844)\n",
      " state (1)  A[0]:(0.000189580969163) A[1]:(2.00430276891e-05) A[2]:(0.00149640650488) A[3]:(0.998293995857)\n",
      " state (2)  A[0]:(0.999942719936) A[1]:(4.58451268059e-08) A[2]:(5.724559378e-05) A[3]:(1.14319274047e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.54628522328e-12) A[2]:(2.24876384181e-08) A[3]:(1.16947530863e-15)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(3.72489322373e-12) A[2]:(3.74893502908e-08) A[3]:(1.90783451108e-16)\n",
      " state (5)  A[0]:(0.999998450279) A[1]:(5.36675521731e-12) A[2]:(1.55548434577e-06) A[3]:(5.34818898323e-19)\n",
      " state (6)  A[0]:(5.93779247993e-07) A[1]:(1.52800935425e-13) A[2]:(0.999999403954) A[3]:(1.66003650765e-29)\n",
      " state (7)  A[0]:(1.18232579283e-09) A[1]:(8.54595596221e-15) A[2]:(1.0) A[3]:(1.09419892476e-31)\n",
      " state (8)  A[0]:(6.34673869016e-10) A[1]:(6.85498639664e-15) A[2]:(1.0) A[3]:(7.83696852743e-32)\n",
      " state (9)  A[0]:(5.71589275911e-10) A[1]:(6.62793032128e-15) A[2]:(1.0) A[3]:(7.45945440244e-32)\n",
      " state (10)  A[0]:(5.58382506899e-10) A[1]:(6.58006533646e-15) A[2]:(1.0) A[3]:(7.38189234643e-32)\n",
      " state (11)  A[0]:(5.53276813253e-10) A[1]:(6.56336904652e-15) A[2]:(1.0) A[3]:(7.3560873067e-32)\n",
      " state (12)  A[0]:(5.48849743431e-10) A[1]:(6.55063771781e-15) A[2]:(1.0) A[3]:(7.33747805563e-32)\n",
      " state (13)  A[0]:(5.4324855725e-10) A[1]:(6.53526237575e-15) A[2]:(1.0) A[3]:(7.3155104172e-32)\n",
      " state (14)  A[0]:(5.35722688433e-10) A[1]:(6.51482685886e-15) A[2]:(1.0) A[3]:(7.28648981267e-32)\n",
      " state (15)  A[0]:(5.25836041376e-10) A[1]:(6.48799243158e-15) A[2]:(1.0) A[3]:(7.24861949891e-32)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 763000 finished after 20 . Running score: 0.1. Policy_loss: -92050.6111803, Value_loss: 1.2035671545. Times trained:               15867. Times reached goal: 125.               Steps done: 9857394.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.985941529274) A[1]:(0.00986335519701) A[2]:(0.00206791330129) A[3]:(0.00212720874697)\n",
      " state (1)  A[0]:(5.30901124876e-05) A[1]:(1.21639204735e-05) A[2]:(0.000248118274612) A[3]:(0.999686598778)\n",
      " state (2)  A[0]:(0.999773323536) A[1]:(1.42805561154e-06) A[2]:(0.000224154064199) A[3]:(1.09319205421e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.82268974219e-11) A[2]:(1.08544595534e-08) A[3]:(2.03625619377e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.74400962422e-11) A[2]:(1.59401931654e-08) A[3]:(4.2478562015e-16)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(1.84508623657e-11) A[2]:(5.81770542851e-08) A[3]:(1.62104834573e-17)\n",
      " state (6)  A[0]:(0.00162737397477) A[1]:(1.07910978764e-10) A[2]:(0.998372614384) A[3]:(1.01782168162e-25)\n",
      " state (7)  A[0]:(4.11176293014e-09) A[1]:(1.19331370414e-13) A[2]:(1.0) A[3]:(2.9835685263e-31)\n",
      " state (8)  A[0]:(1.13902853816e-09) A[1]:(7.33964484242e-14) A[2]:(1.0) A[3]:(1.3926530613e-31)\n",
      " state (9)  A[0]:(9.18330245181e-10) A[1]:(6.84667912101e-14) A[2]:(1.0) A[3]:(1.25599925693e-31)\n",
      " state (10)  A[0]:(8.71287042514e-10) A[1]:(6.73761380346e-14) A[2]:(1.0) A[3]:(1.2270725744e-31)\n",
      " state (11)  A[0]:(8.5305718045e-10) A[1]:(6.69857032798e-14) A[2]:(1.0) A[3]:(1.2171983043e-31)\n",
      " state (12)  A[0]:(8.38560887395e-10) A[1]:(6.67142461609e-14) A[2]:(1.0) A[3]:(1.21072438671e-31)\n",
      " state (13)  A[0]:(8.21504475557e-10) A[1]:(6.64160092483e-14) A[2]:(1.0) A[3]:(1.20383469677e-31)\n",
      " state (14)  A[0]:(8.00093213904e-10) A[1]:(6.60482682002e-14) A[2]:(1.0) A[3]:(1.19546012237e-31)\n",
      " state (15)  A[0]:(7.74581121465e-10) A[1]:(6.56115718939e-14) A[2]:(1.0) A[3]:(1.18557797646e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 764000 finished after 17 . Running score: 0.08. Policy_loss: -92050.6123483, Value_loss: 0.987545295758. Times trained:               16132. Times reached goal: 109.               Steps done: 9873526.\n",
      "action_dist \n",
      "tensor([[ 0.9939,  0.0036,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9938,  0.0036,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0827e-11,  7.4547e-09,  4.5908e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0827e-11,  7.4548e-09,  4.5910e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0827e-11,  7.4549e-09,  4.5912e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5097e-08,  2.0268e-13,  1.0000e+00,  4.4647e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0828e-11,  7.4551e-09,  4.5916e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0828e-11,  7.4552e-09,  4.5917e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9938,  0.0036,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0828e-11,  7.4554e-09,  4.5921e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0828e-11,  7.4555e-09,  4.5922e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5098e-08,  2.0268e-13,  1.0000e+00,  4.4652e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0829e-11,  7.4556e-09,  4.5925e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0829e-11,  7.4556e-09,  4.5926e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9938,  0.0036,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0829e-11,  7.4558e-09,  4.5929e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0829e-11,  7.4559e-09,  4.5930e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9938,  0.0036,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9938,  0.0036,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9938,  0.0036,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9938,  0.0036,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9938,  0.0036,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9938,  0.0036,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9938,  0.0036,  0.0014,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0831e-11,  7.4569e-09,  4.5947e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5097e-08,  2.0268e-13,  1.0000e+00,  4.4665e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993843793869) A[1]:(0.00359388953075) A[2]:(0.00143432465848) A[3]:(0.00112796365283)\n",
      " state (1)  A[0]:(0.000100458812085) A[1]:(1.77494421223e-05) A[2]:(0.000408955354942) A[3]:(0.999472856522)\n",
      " state (2)  A[0]:(0.999998688698) A[1]:(5.93715876462e-09) A[2]:(1.30969453949e-06) A[3]:(1.66134384028e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.13584697203e-11) A[2]:(6.03479444194e-09) A[3]:(1.29515795146e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.08315491992e-11) A[2]:(7.45715222905e-09) A[3]:(4.59525326523e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.03723279965e-11) A[2]:(1.0048370136e-08) A[3]:(1.34495875067e-16)\n",
      " state (6)  A[0]:(0.999999940395) A[1]:(1.58416318258e-11) A[2]:(7.80470870154e-08) A[3]:(3.12772768076e-18)\n",
      " state (7)  A[0]:(0.000722943106666) A[1]:(6.29999732937e-11) A[2]:(0.999277055264) A[3]:(1.92046739077e-26)\n",
      " state (8)  A[0]:(1.50970542734e-08) A[1]:(2.02681540171e-13) A[2]:(1.0) A[3]:(4.46658183835e-31)\n",
      " state (9)  A[0]:(3.4222564782e-09) A[1]:(1.12397111475e-13) A[2]:(1.0) A[3]:(1.72217845227e-31)\n",
      " state (10)  A[0]:(2.48795251068e-09) A[1]:(1.00562055428e-13) A[2]:(1.0) A[3]:(1.4515922305e-31)\n",
      " state (11)  A[0]:(2.26599805586e-09) A[1]:(9.74784963577e-14) A[2]:(1.0) A[3]:(1.38514012426e-31)\n",
      " state (12)  A[0]:(2.18871143431e-09) A[1]:(9.64031304329e-14) A[2]:(1.0) A[3]:(1.36265679646e-31)\n",
      " state (13)  A[0]:(2.1490325075e-09) A[1]:(9.58892863658e-14) A[2]:(1.0) A[3]:(1.35228975913e-31)\n",
      " state (14)  A[0]:(2.1169983544e-09) A[1]:(9.55150875385e-14) A[2]:(1.0) A[3]:(1.34503566594e-31)\n",
      " state (15)  A[0]:(2.0821033786e-09) A[1]:(9.51332721909e-14) A[2]:(1.0) A[3]:(1.33784093522e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 765000 finished after 26 . Running score: 0.1. Policy_loss: -92050.6111847, Value_loss: 1.20322628902. Times trained:               15837. Times reached goal: 96.               Steps done: 9889363.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995746314526) A[1]:(0.00247405283153) A[2]:(0.000748383172322) A[3]:(0.00103122054134)\n",
      " state (1)  A[0]:(4.1097002395e-05) A[1]:(6.67934955345e-06) A[2]:(0.00011660157179) A[3]:(0.99983561039)\n",
      " state (2)  A[0]:(0.130322292447) A[1]:(0.00144969800021) A[2]:(0.0990635454655) A[3]:(0.769164502621)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.9455678148e-12) A[2]:(3.18075010775e-09) A[3]:(3.38424708416e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.18965035076e-12) A[2]:(3.74471698095e-09) A[3]:(1.11902867327e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(6.86970879488e-12) A[2]:(4.21045909249e-09) A[3]:(5.78150967297e-16)\n",
      " state (6)  A[0]:(1.0) A[1]:(6.55902415714e-12) A[2]:(4.86201301442e-09) A[3]:(2.76204579787e-16)\n",
      " state (7)  A[0]:(1.0) A[1]:(6.20284466152e-12) A[2]:(6.17147533077e-09) A[3]:(9.78482931548e-17)\n",
      " state (8)  A[0]:(1.0) A[1]:(6.7948303234e-12) A[2]:(1.62019038186e-08) A[3]:(7.9221486088e-18)\n",
      " state (9)  A[0]:(0.999952852726) A[1]:(1.89559590247e-10) A[2]:(4.7165012802e-05) A[3]:(2.88606643829e-20)\n",
      " state (10)  A[0]:(0.00388997117989) A[1]:(1.01166783284e-10) A[2]:(0.996110022068) A[3]:(5.69806619423e-26)\n",
      " state (11)  A[0]:(1.33477215059e-06) A[1]:(1.08111577416e-12) A[2]:(0.999998688698) A[3]:(1.11435058899e-29)\n",
      " state (12)  A[0]:(7.12946999215e-08) A[1]:(2.76130707926e-13) A[2]:(0.999999940395) A[3]:(1.08050726027e-30)\n",
      " state (13)  A[0]:(2.30594583428e-08) A[1]:(1.71730671675e-13) A[2]:(1.0) A[3]:(5.02352259821e-31)\n",
      " state (14)  A[0]:(1.39688856038e-08) A[1]:(1.40617714955e-13) A[2]:(1.0) A[3]:(3.67674602513e-31)\n",
      " state (15)  A[0]:(1.09157687334e-08) A[1]:(1.27838671181e-13) A[2]:(1.0) A[3]:(3.17708263811e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 766000 finished after 22 . Running score: 0.0. Policy_loss: -92050.6114662, Value_loss: 0.978602959919. Times trained:               16805. Times reached goal: 58.               Steps done: 9906168.\n",
      " state (0)  A[0]:(0.99729937315) A[1]:(0.00133401271887) A[2]:(0.000613752112258) A[3]:(0.000752877502237)\n",
      " state (1)  A[0]:(8.95987177501e-05) A[1]:(1.10671771836e-05) A[2]:(0.000236018633586) A[3]:(0.999663293362)\n",
      " state (2)  A[0]:(0.999998092651) A[1]:(8.47208880828e-09) A[2]:(1.88037711268e-06) A[3]:(7.95634336193e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.97515960721e-12) A[2]:(2.7460813623e-09) A[3]:(1.30604814818e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.66721800096e-12) A[2]:(3.07788305953e-09) A[3]:(6.34502481091e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(4.50337510338e-12) A[2]:(3.32432437133e-09) A[3]:(4.04849588005e-16)\n",
      " state (6)  A[0]:(1.0) A[1]:(4.33811233463e-12) A[2]:(3.64882835058e-09) A[3]:(2.44093031361e-16)\n",
      " state (7)  A[0]:(1.0) A[1]:(4.1471132084e-12) A[2]:(4.1730263689e-09) A[3]:(1.25968437045e-16)\n",
      " state (8)  A[0]:(1.0) A[1]:(3.93123381071e-12) A[2]:(5.52382939389e-09) A[3]:(4.19778841486e-17)\n",
      " state (9)  A[0]:(1.0) A[1]:(5.75406259098e-12) A[2]:(2.33743033817e-08) A[3]:(2.32590883938e-18)\n",
      " state (10)  A[0]:(0.999930679798) A[1]:(1.68672201206e-10) A[2]:(6.92946123309e-05) A[3]:(1.48288038333e-20)\n",
      " state (11)  A[0]:(0.0506818667054) A[1]:(3.62984919899e-10) A[2]:(0.949318110943) A[3]:(9.57825642004e-25)\n",
      " state (12)  A[0]:(3.04025033984e-05) A[1]:(4.02821612872e-12) A[2]:(0.999969601631) A[3]:(1.51906027138e-28)\n",
      " state (13)  A[0]:(7.63743230436e-07) A[1]:(6.18909220748e-13) A[2]:(0.99999922514) A[3]:(5.36612644529e-30)\n",
      " state (14)  A[0]:(1.31896584321e-07) A[1]:(2.79827213918e-13) A[2]:(0.999999880791) A[3]:(1.41582087946e-30)\n",
      " state (15)  A[0]:(5.41847917646e-08) A[1]:(1.91817061909e-13) A[2]:(0.999999940395) A[3]:(7.69568880014e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 767000 finished after 16 . Running score: 0.0. Policy_loss: -92050.6111768, Value_loss: 0.979245253117. Times trained:               17423. Times reached goal: 0.               Steps done: 9923591.\n",
      " state (0)  A[0]:(0.997176229954) A[1]:(0.00092560169287) A[2]:(0.000602406216785) A[3]:(0.00129575247411)\n",
      " state (1)  A[0]:(2.28124208661e-05) A[1]:(2.8927825042e-06) A[2]:(4.45025179943e-05) A[3]:(0.999929785728)\n",
      " state (2)  A[0]:(0.00287039624527) A[1]:(0.00012187606626) A[2]:(0.00719163473696) A[3]:(0.989816069603)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.36806363249e-12) A[2]:(2.61214538888e-09) A[3]:(1.23883210276e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.07957218354e-12) A[2]:(2.97347813039e-09) A[3]:(2.43538828291e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.72258517414e-12) A[2]:(3.42830630551e-09) A[3]:(1.03421409959e-15)\n",
      " state (6)  A[0]:(1.0) A[1]:(3.45358854362e-12) A[2]:(3.99523525374e-09) A[3]:(4.60768082675e-16)\n",
      " state (7)  A[0]:(1.0) A[1]:(3.18745008686e-12) A[2]:(4.95947771739e-09) A[3]:(1.71102667049e-16)\n",
      " state (8)  A[0]:(1.0) A[1]:(2.90670460791e-12) A[2]:(8.45557401874e-09) A[3]:(2.56542043331e-17)\n",
      " state (9)  A[0]:(0.999999880791) A[1]:(8.40523137202e-12) A[2]:(9.06244821408e-08) A[3]:(1.18324071402e-18)\n",
      " state (10)  A[0]:(0.999820530415) A[1]:(1.62725583008e-10) A[2]:(0.000179441034561) A[3]:(1.29549037038e-20)\n",
      " state (11)  A[0]:(0.33045938611) A[1]:(7.48380524218e-10) A[2]:(0.669540643692) A[3]:(1.97773250732e-23)\n",
      " state (12)  A[0]:(0.000893323041964) A[1]:(1.83874304671e-11) A[2]:(0.999106705189) A[3]:(8.60796160499e-27)\n",
      " state (13)  A[0]:(1.31736596813e-05) A[1]:(1.69726841089e-12) A[2]:(0.999986827374) A[3]:(1.01500721041e-28)\n",
      " state (14)  A[0]:(9.79502829068e-07) A[1]:(4.6708090954e-13) A[2]:(0.999999046326) A[3]:(1.04717934351e-29)\n",
      " state (15)  A[0]:(2.00166880404e-07) A[1]:(2.2663214892e-13) A[2]:(0.999999821186) A[3]:(3.10290654657e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 768000 finished after 7 . Running score: 0.0. Policy_loss: -92050.6111769, Value_loss: 0.979572307712. Times trained:               17827. Times reached goal: 0.               Steps done: 9941418.\n",
      " state (0)  A[0]:(0.99312710762) A[1]:(0.00121459551156) A[2]:(0.000885642366484) A[3]:(0.00477267568931)\n",
      " state (1)  A[0]:(2.93195498671e-05) A[1]:(3.88051603295e-06) A[2]:(7.28298255126e-05) A[3]:(0.999893963337)\n",
      " state (2)  A[0]:(0.999964058399) A[1]:(1.57613342822e-07) A[2]:(3.55853771907e-05) A[3]:(2.20840220777e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.99766244014e-12) A[2]:(4.57240689755e-09) A[3]:(2.2455455413e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.4691988825e-12) A[2]:(6.2132032852e-09) A[3]:(5.08178846073e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(4.99102278603e-12) A[2]:(1.9354670755e-08) A[3]:(2.62968624577e-17)\n",
      " state (6)  A[0]:(0.999978601933) A[1]:(1.42646047752e-10) A[2]:(2.13978801185e-05) A[3]:(2.89897491487e-19)\n",
      " state (7)  A[0]:(0.0564029999077) A[1]:(3.44434869515e-10) A[2]:(0.943597018719) A[3]:(3.9533231784e-24)\n",
      " state (8)  A[0]:(5.63518233321e-06) A[1]:(1.2667537375e-12) A[2]:(0.999994337559) A[3]:(7.64410081056e-29)\n",
      " state (9)  A[0]:(6.69600979109e-08) A[1]:(1.42478924167e-13) A[2]:(0.999999940395) A[3]:(1.63865116211e-30)\n",
      " state (10)  A[0]:(1.06200381822e-08) A[1]:(6.52525180157e-14) A[2]:(1.0) A[3]:(4.56723941961e-31)\n",
      " state (11)  A[0]:(4.62560789671e-09) A[1]:(4.71817748174e-14) A[2]:(1.0) A[3]:(2.74505037817e-31)\n",
      " state (12)  A[0]:(3.09585823643e-09) A[1]:(4.07083898925e-14) A[2]:(1.0) A[3]:(2.18964740002e-31)\n",
      " state (13)  A[0]:(2.52254750421e-09) A[1]:(3.78890678326e-14) A[2]:(1.0) A[3]:(1.9651396164e-31)\n",
      " state (14)  A[0]:(2.2578268144e-09) A[1]:(3.65071268029e-14) A[2]:(1.0) A[3]:(1.85935064965e-31)\n",
      " state (15)  A[0]:(2.11408512918e-09) A[1]:(3.5753657115e-14) A[2]:(1.0) A[3]:(1.80324465694e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 769000 finished after 3 . Running score: 0.15. Policy_loss: -92050.6111854, Value_loss: 1.41203996122. Times trained:               15437. Times reached goal: 93.               Steps done: 9956855.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5113e-12,  3.3206e-09,  5.3503e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5114e-12,  3.3206e-09,  5.3504e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5115e-12,  3.3207e-09,  5.3504e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5117e-12,  3.3209e-09,  5.3507e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5119e-12,  3.3210e-09,  5.3511e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9989e-01,  1.5586e-10,  1.1090e-04,  2.3170e-20]])\n",
      "On state=8, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9989e-01,  1.5588e-10,  1.1095e-04,  2.3164e-20]])\n",
      "On state=8, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5121e-12,  3.3211e-09,  5.3515e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5122e-12,  3.3212e-09,  5.3514e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5128e-12,  3.3216e-09,  5.3526e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9962,  0.0014,  0.0006,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5130e-12,  3.3216e-09,  5.3532e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5131e-12,  3.3217e-09,  5.3535e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5132e-12,  3.3217e-09,  5.3537e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9989e-01,  1.5624e-10,  1.1181e-04,  2.3063e-20]])\n",
      "On state=8, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5133e-12,  3.3218e-09,  5.3539e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5133e-12,  3.3218e-09,  5.3539e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9989e-01,  1.5634e-10,  1.1205e-04,  2.3033e-20]])\n",
      "On state=8, selected action=0\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996158719063) A[1]:(0.00136604404543) A[2]:(0.000590976327658) A[3]:(0.00188423297368)\n",
      " state (1)  A[0]:(6.49853172945e-05) A[1]:(7.27672295397e-06) A[2]:(0.000137866940349) A[3]:(0.999789893627)\n",
      " state (2)  A[0]:(0.991582036018) A[1]:(5.0371789257e-05) A[2]:(0.00728697795421) A[3]:(0.00108060857747)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.83851934613e-12) A[2]:(2.82402012886e-09) A[3]:(1.45911557796e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.51346534332e-12) A[2]:(3.32206795406e-09) A[3]:(5.35351292444e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.28224513541e-12) A[2]:(3.9821119735e-09) A[3]:(2.11897838431e-16)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.99576054068e-12) A[2]:(7.02194746793e-09) A[3]:(2.63721044606e-17)\n",
      " state (7)  A[0]:(0.999999880791) A[1]:(1.4665903908e-11) A[2]:(1.41820237332e-07) A[3]:(1.14889865653e-18)\n",
      " state (8)  A[0]:(0.999887704849) A[1]:(1.56424581621e-10) A[2]:(0.000112283880298) A[3]:(2.30023483338e-20)\n",
      " state (9)  A[0]:(0.0460920669138) A[1]:(2.27896021499e-10) A[2]:(0.953907907009) A[3]:(1.0779824699e-24)\n",
      " state (10)  A[0]:(8.80586958374e-06) A[1]:(1.17909989498e-12) A[2]:(0.999991178513) A[3]:(4.13816773153e-29)\n",
      " state (11)  A[0]:(1.74576001655e-07) A[1]:(1.54247545726e-13) A[2]:(0.999999821186) A[3]:(1.1173947432e-30)\n",
      " state (12)  A[0]:(3.891841871e-08) A[1]:(7.76844680164e-14) A[2]:(0.999999940395) A[3]:(3.57598923215e-31)\n",
      " state (13)  A[0]:(2.0522655575e-08) A[1]:(5.90407578513e-14) A[2]:(1.0) A[3]:(2.30751163249e-31)\n",
      " state (14)  A[0]:(1.51261652093e-08) A[1]:(5.20371404658e-14) A[2]:(1.0) A[3]:(1.89480826125e-31)\n",
      " state (15)  A[0]:(1.29201920274e-08) A[1]:(4.8823243354e-14) A[2]:(1.0) A[3]:(1.71769076749e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 770000 finished after 35 . Running score: 0.01. Policy_loss: -92050.6111816, Value_loss: 1.19816901234. Times trained:               15146. Times reached goal: 88.               Steps done: 9972001.\n",
      " state (0)  A[0]:(0.994356632233) A[1]:(0.00204876973294) A[2]:(0.00081467081327) A[3]:(0.00277992221527)\n",
      " state (1)  A[0]:(7.3109556979e-05) A[1]:(8.50983269629e-06) A[2]:(0.000214458603295) A[3]:(0.999703943729)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.58830889949e-12) A[2]:(3.80924936039e-09) A[3]:(3.15381537075e-15)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.68318309879e-12) A[2]:(5.31706012552e-09) A[3]:(3.88519586876e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.3976339537e-12) A[2]:(6.77937128657e-09) A[3]:(1.33711138746e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.0176616414e-12) A[2]:(1.73674905568e-08) A[3]:(6.99615382745e-18)\n",
      " state (6)  A[0]:(0.999992847443) A[1]:(6.01214703022e-11) A[2]:(7.16760359865e-06) A[3]:(1.92878304621e-19)\n",
      " state (7)  A[0]:(0.390257060528) A[1]:(6.33147256845e-10) A[2]:(0.609742939472) A[3]:(2.36612643852e-23)\n",
      " state (8)  A[0]:(0.000107778723759) A[1]:(3.69590989757e-12) A[2]:(0.999892234802) A[3]:(7.10755875588e-28)\n",
      " state (9)  A[0]:(4.14982025632e-07) A[1]:(1.72000719331e-13) A[2]:(0.999999582767) A[3]:(2.8048677592e-30)\n",
      " state (10)  A[0]:(4.86644076148e-08) A[1]:(6.26304021666e-14) A[2]:(0.999999940395) A[3]:(5.14524174685e-31)\n",
      " state (11)  A[0]:(2.06148484949e-08) A[1]:(4.29717872885e-14) A[2]:(1.0) A[3]:(2.80417210164e-31)\n",
      " state (12)  A[0]:(1.41270248832e-08) A[1]:(3.6603485271e-14) A[2]:(1.0) A[3]:(2.17677174019e-31)\n",
      " state (13)  A[0]:(1.18183489661e-08) A[1]:(3.39785742132e-14) A[2]:(1.0) A[3]:(1.93787860936e-31)\n",
      " state (14)  A[0]:(1.08115854047e-08) A[1]:(3.27542490743e-14) A[2]:(1.0) A[3]:(1.83056596684e-31)\n",
      " state (15)  A[0]:(1.03113722005e-08) A[1]:(3.21282612331e-14) A[2]:(1.0) A[3]:(1.77684622765e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 771000 finished after 17 . Running score: 0.05. Policy_loss: -92050.6111845, Value_loss: 1.40545079934. Times trained:               15745. Times reached goal: 94.               Steps done: 9987746.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.99041634798) A[1]:(0.00318796443753) A[2]:(0.0035605172161) A[3]:(0.00283516384661)\n",
      " state (1)  A[0]:(5.9140893427e-05) A[1]:(8.06506341178e-06) A[2]:(0.000185343145858) A[3]:(0.99974745512)\n",
      " state (2)  A[0]:(1.0) A[1]:(6.12234134917e-12) A[2]:(4.28439017597e-09) A[3]:(2.22232392129e-15)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.18807089303e-12) A[2]:(6.48222320265e-09) A[3]:(2.31707318559e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.81513266759e-12) A[2]:(8.58141024906e-09) A[3]:(7.36181414575e-17)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(4.58883365334e-12) A[2]:(3.01073406206e-08) A[3]:(2.35602938062e-18)\n",
      " state (6)  A[0]:(0.999737083912) A[1]:(2.20903503823e-10) A[2]:(0.000262936315266) A[3]:(1.56520181795e-20)\n",
      " state (7)  A[0]:(0.34710213542) A[1]:(7.68616115199e-10) A[2]:(0.652897834778) A[3]:(1.25428513166e-23)\n",
      " state (8)  A[0]:(0.003298250027) A[1]:(3.84564359523e-11) A[2]:(0.996701776981) A[3]:(2.60137791964e-26)\n",
      " state (9)  A[0]:(4.09159183619e-05) A[1]:(2.62287804323e-12) A[2]:(0.999959111214) A[3]:(1.70474411251e-28)\n",
      " state (10)  A[0]:(2.212356776e-06) A[1]:(5.43693291343e-13) A[2]:(0.999997794628) A[3]:(1.02423993801e-29)\n",
      " state (11)  A[0]:(3.29868299787e-07) A[1]:(2.15094638606e-13) A[2]:(0.999999642372) A[3]:(2.09605820735e-30)\n",
      " state (12)  A[0]:(1.03348853031e-07) A[1]:(1.26473213412e-13) A[2]:(0.999999880791) A[3]:(8.68450430371e-31)\n",
      " state (13)  A[0]:(5.32548654064e-08) A[1]:(9.43792502375e-14) A[2]:(0.999999940395) A[3]:(5.39340129965e-31)\n",
      " state (14)  A[0]:(3.68539652129e-08) A[1]:(8.04834782262e-14) A[2]:(0.999999940395) A[3]:(4.1745076974e-31)\n",
      " state (15)  A[0]:(3.00698062006e-08) A[1]:(7.37768068751e-14) A[2]:(0.999999940395) A[3]:(3.63280157942e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 772000 finished after 18 . Running score: 0.1. Policy_loss: -92050.6195593, Value_loss: 1.42648308011. Times trained:               16426. Times reached goal: 98.               Steps done: 10004172.\n",
      " state (0)  A[0]:(0.994101345539) A[1]:(0.00248580798507) A[2]:(0.00222569820471) A[3]:(0.00118716584984)\n",
      " state (1)  A[0]:(6.62502789055e-05) A[1]:(8.08777986094e-06) A[2]:(0.00018647647812) A[3]:(0.999739170074)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.14377558844e-12) A[2]:(4.07917388756e-09) A[3]:(2.30277299304e-15)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.07940260633e-12) A[2]:(6.49187459345e-09) A[3]:(1.80609473681e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.67057295217e-12) A[2]:(8.77868178151e-09) A[3]:(5.39901877692e-17)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(6.75771027678e-12) A[2]:(4.64849065906e-08) A[3]:(1.1806346815e-18)\n",
      " state (6)  A[0]:(0.998570621014) A[1]:(4.51096243692e-10) A[2]:(0.00142938061617) A[3]:(3.92512467955e-21)\n",
      " state (7)  A[0]:(0.424413859844) A[1]:(1.14093323678e-09) A[2]:(0.575586140156) A[3]:(1.44727703333e-23)\n",
      " state (8)  A[0]:(0.0110513791442) A[1]:(1.13313546046e-10) A[2]:(0.988948643208) A[3]:(1.00779119195e-25)\n",
      " state (9)  A[0]:(0.000199775604415) A[1]:(8.96268215894e-12) A[2]:(0.999800205231) A[3]:(8.23305873387e-28)\n",
      " state (10)  A[0]:(1.02396288639e-05) A[1]:(1.64422869851e-12) A[2]:(0.999989748001) A[3]:(3.71925720469e-29)\n",
      " state (11)  A[0]:(1.19379228636e-06) A[1]:(5.42222245836e-13) A[2]:(0.999998807907) A[3]:(5.30255495848e-30)\n",
      " state (12)  A[0]:(2.82495960846e-07) A[1]:(2.71660433948e-13) A[2]:(0.999999701977) A[3]:(1.63790091459e-30)\n",
      " state (13)  A[0]:(1.16933712491e-07) A[1]:(1.8139133316e-13) A[2]:(0.999999880791) A[3]:(8.37126327108e-31)\n",
      " state (14)  A[0]:(7.01107012446e-08) A[1]:(1.44412151508e-13) A[2]:(0.999999940395) A[3]:(5.76217785759e-31)\n",
      " state (15)  A[0]:(5.24218286557e-08) A[1]:(1.27113610978e-13) A[2]:(0.999999940395) A[3]:(4.68379532687e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 773000 finished after 5 . Running score: 0.07. Policy_loss: -92050.6187114, Value_loss: 1.00139232208. Times trained:               16159. Times reached goal: 61.               Steps done: 10020331.\n",
      " state (0)  A[0]:(0.996156334877) A[1]:(0.00137425179128) A[2]:(0.00182622205466) A[3]:(0.0006431977381)\n",
      " state (1)  A[0]:(6.38561177766e-05) A[1]:(5.85733369007e-06) A[2]:(0.000165633813594) A[3]:(0.999764680862)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.74278991617e-12) A[2]:(4.23176249598e-09) A[3]:(1.30650478365e-14)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.8639070202e-12) A[2]:(6.53403597894e-09) A[3]:(2.85788731115e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.45659221332e-12) A[2]:(8.87753870416e-09) A[3]:(7.98535281766e-17)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.87346882e-12) A[2]:(2.14113704544e-08) A[3]:(5.25815843856e-18)\n",
      " state (6)  A[0]:(0.999934017658) A[1]:(1.07478449751e-10) A[2]:(6.59737124806e-05) A[3]:(2.88298583675e-20)\n",
      " state (7)  A[0]:(0.330131977797) A[1]:(7.03806235514e-10) A[2]:(0.669867992401) A[3]:(1.18194556113e-23)\n",
      " state (8)  A[0]:(0.00324783707038) A[1]:(3.60588399417e-11) A[2]:(0.996752142906) A[3]:(2.71202305717e-26)\n",
      " state (9)  A[0]:(4.08912637795e-05) A[1]:(2.43976496991e-12) A[2]:(0.999959111214) A[3]:(1.74300198863e-28)\n",
      " state (10)  A[0]:(2.20136212192e-06) A[1]:(4.94695755714e-13) A[2]:(0.999997794628) A[3]:(9.94372300457e-30)\n",
      " state (11)  A[0]:(3.27501112452e-07) A[1]:(1.93140872761e-13) A[2]:(0.999999701977) A[3]:(1.97088310531e-30)\n",
      " state (12)  A[0]:(1.02735270957e-07) A[1]:(1.12896644073e-13) A[2]:(0.999999880791) A[3]:(8.03841311188e-31)\n",
      " state (13)  A[0]:(5.30545314348e-08) A[1]:(8.40446961158e-14) A[2]:(0.999999940395) A[3]:(4.95500632389e-31)\n",
      " state (14)  A[0]:(3.67941268564e-08) A[1]:(7.16071082638e-14) A[2]:(0.999999940395) A[3]:(3.8218692663e-31)\n",
      " state (15)  A[0]:(3.00792137864e-08) A[1]:(6.56294612297e-14) A[2]:(0.999999940395) A[3]:(3.32113077215e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 774000 finished after 19 . Running score: 0.14. Policy_loss: -92050.6112125, Value_loss: 1.41719305409. Times trained:               15798. Times reached goal: 103.               Steps done: 10036129.\n",
      "action_dist \n",
      "tensor([[ 0.9893,  0.0014,  0.0029,  0.0064]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9893,  0.0014,  0.0029,  0.0064]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.6383e-12,  3.2071e-08,  2.7261e-17]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.2764e-06,  5.0439e-13,  1.0000e+00,  6.2917e-29]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.989275097847) A[1]:(0.00139583402779) A[2]:(0.00293026282452) A[3]:(0.00639879330993)\n",
      " state (1)  A[0]:(1.20771392176e-05) A[1]:(1.34980177791e-06) A[2]:(3.64869229088e-05) A[3]:(0.999950110912)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.063646109e-12) A[2]:(5.26600096862e-09) A[3]:(2.08155009478e-14)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.43368215006e-12) A[2]:(1.27442403297e-08) A[3]:(4.4126927835e-16)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(4.63826936928e-12) A[2]:(3.20710498158e-08) A[3]:(2.72618149644e-17)\n",
      " state (5)  A[0]:(0.999873042107) A[1]:(1.27877736178e-10) A[2]:(0.000126950748381) A[3]:(1.3187099441e-19)\n",
      " state (6)  A[0]:(0.0891750380397) A[1]:(3.42124412134e-10) A[2]:(0.91082495451) A[3]:(1.33093156718e-23)\n",
      " state (7)  A[0]:(0.000205225558602) A[1]:(6.85505471831e-12) A[2]:(0.999794781208) A[3]:(7.57443518187e-27)\n",
      " state (8)  A[0]:(2.27635814554e-06) A[1]:(5.04391179431e-13) A[2]:(0.999997735023) A[3]:(6.29177419263e-29)\n",
      " state (9)  A[0]:(1.61588886272e-07) A[1]:(1.3207138291e-13) A[2]:(0.999999821186) A[3]:(6.06135330098e-30)\n",
      " state (10)  A[0]:(3.6712808793e-08) A[1]:(6.63688938876e-14) A[2]:(0.999999940395) A[3]:(1.90525346895e-30)\n",
      " state (11)  A[0]:(1.71611933553e-08) A[1]:(4.73781882053e-14) A[2]:(1.0) A[3]:(1.09424540381e-30)\n",
      " state (12)  A[0]:(1.17854837001e-08) A[1]:(4.02699791915e-14) A[2]:(1.0) A[3]:(8.40146407194e-31)\n",
      " state (13)  A[0]:(9.79125402978e-09) A[1]:(3.7205017577e-14) A[2]:(1.0) A[3]:(7.39302768687e-31)\n",
      " state (14)  A[0]:(8.92878837533e-09) A[1]:(3.57779398556e-14) A[2]:(1.0) A[3]:(6.94192843836e-31)\n",
      " state (15)  A[0]:(8.5248181847e-09) A[1]:(3.50849957539e-14) A[2]:(1.0) A[3]:(6.72716703106e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 775000 finished after 4 . Running score: 0.06. Policy_loss: -92050.6111903, Value_loss: 1.42896759216. Times trained:               15531. Times reached goal: 116.               Steps done: 10051660.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993836402893) A[1]:(0.00137249566615) A[2]:(0.00198838743381) A[3]:(0.00280271749943)\n",
      " state (1)  A[0]:(3.05403482344e-05) A[1]:(3.18500656249e-06) A[2]:(0.000110590990516) A[3]:(0.999855697155)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.31340855931e-12) A[2]:(9.27802723538e-09) A[3]:(2.68038896302e-15)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.00937789114e-12) A[2]:(2.27544081355e-08) A[3]:(9.45437926344e-17)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(6.16278382493e-12) A[2]:(1.28000067434e-07) A[3]:(2.24010063815e-18)\n",
      " state (5)  A[0]:(0.60642015934) A[1]:(1.22370769073e-09) A[2]:(0.39357984066) A[3]:(2.44924498868e-22)\n",
      " state (6)  A[0]:(0.00332594127394) A[1]:(5.01074459702e-11) A[2]:(0.996674060822) A[3]:(1.87779402173e-25)\n",
      " state (7)  A[0]:(7.1469357863e-06) A[1]:(1.17740062491e-12) A[2]:(0.999992847443) A[3]:(1.66359553591e-28)\n",
      " state (8)  A[0]:(2.14646959762e-07) A[1]:(1.85323490495e-13) A[2]:(0.999999761581) A[3]:(6.09715904698e-30)\n",
      " state (9)  A[0]:(3.18543236233e-08) A[1]:(7.54876304694e-14) A[2]:(0.999999940395) A[3]:(1.31250094456e-30)\n",
      " state (10)  A[0]:(1.23655068407e-08) A[1]:(4.96425376663e-14) A[2]:(1.0) A[3]:(6.53134612913e-31)\n",
      " state (11)  A[0]:(7.93455523507e-09) A[1]:(4.10338538322e-14) A[2]:(1.0) A[3]:(4.77770604075e-31)\n",
      " state (12)  A[0]:(6.45010755917e-09) A[1]:(3.75973666263e-14) A[2]:(1.0) A[3]:(4.14303313086e-31)\n",
      " state (13)  A[0]:(5.84867709819e-09) A[1]:(3.60881368734e-14) A[2]:(1.0) A[3]:(3.87640233003e-31)\n",
      " state (14)  A[0]:(5.5798889953e-09) A[1]:(3.53883758506e-14) A[2]:(1.0) A[3]:(3.75533252446e-31)\n",
      " state (15)  A[0]:(5.45156675358e-09) A[1]:(3.50492780686e-14) A[2]:(1.0) A[3]:(3.69730894781e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 776000 finished after 12 . Running score: 0.07. Policy_loss: -92050.6110995, Value_loss: 1.21533979491. Times trained:               15745. Times reached goal: 109.               Steps done: 10067405.\n",
      " state (0)  A[0]:(0.99400383234) A[1]:(0.00119930668734) A[2]:(0.00177062232979) A[3]:(0.00302623375319)\n",
      " state (1)  A[0]:(1.18474454212e-05) A[1]:(1.08126369014e-06) A[2]:(2.444801612e-05) A[3]:(0.999962627888)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(5.41210076754e-10) A[2]:(9.63732560422e-08) A[3]:(4.69006722348e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.49382110585e-12) A[2]:(6.80118139584e-09) A[3]:(2.83779118978e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.67258479772e-12) A[2]:(1.16036522613e-08) A[3]:(3.08343656784e-16)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(4.90770531852e-12) A[2]:(3.19249409131e-08) A[3]:(1.58534028218e-17)\n",
      " state (6)  A[0]:(0.999584317207) A[1]:(1.74498540617e-10) A[2]:(0.000415655405959) A[3]:(3.14164392182e-20)\n",
      " state (7)  A[0]:(0.0569696612656) A[1]:(3.07192216109e-10) A[2]:(0.943030357361) A[3]:(5.73313576998e-24)\n",
      " state (8)  A[0]:(0.000291857140837) A[1]:(1.06442346257e-11) A[2]:(0.999708116055) A[3]:(9.21812625588e-27)\n",
      " state (9)  A[0]:(6.25802385912e-06) A[1]:(1.11624370771e-12) A[2]:(0.999993741512) A[3]:(1.40436705059e-28)\n",
      " state (10)  A[0]:(4.98473525568e-07) A[1]:(2.96493054717e-13) A[2]:(0.999999523163) A[3]:(1.31841746153e-29)\n",
      " state (11)  A[0]:(9.78640315452e-08) A[1]:(1.35687251365e-13) A[2]:(0.999999880791) A[3]:(3.4237997653e-30)\n",
      " state (12)  A[0]:(3.7555334842e-08) A[1]:(8.77877618147e-14) A[2]:(0.999999940395) A[3]:(1.6435844768e-30)\n",
      " state (13)  A[0]:(2.20946638763e-08) A[1]:(6.94991075323e-14) A[2]:(1.0) A[3]:(1.11481486224e-30)\n",
      " state (14)  A[0]:(1.6567845762e-08) A[1]:(6.13652805328e-14) A[2]:(1.0) A[3]:(9.08061486688e-31)\n",
      " state (15)  A[0]:(1.4174852403e-08) A[1]:(5.74028875106e-14) A[2]:(1.0) A[3]:(8.13894138877e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 777000 finished after 5 . Running score: 0.04. Policy_loss: -92050.6111115, Value_loss: 0.999796452585. Times trained:               15289. Times reached goal: 94.               Steps done: 10082694.\n",
      " state (0)  A[0]:(0.9940315485) A[1]:(0.00105782866012) A[2]:(0.00319232279435) A[3]:(0.00171830970794)\n",
      " state (1)  A[0]:(2.16304524656e-05) A[1]:(1.84362045275e-06) A[2]:(5.478877938e-05) A[3]:(0.999921739101)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(4.79171924361e-10) A[2]:(1.07120570192e-07) A[3]:(1.25068636003e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.03259239038e-12) A[2]:(8.00883004359e-09) A[3]:(1.29409121934e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.29533578045e-12) A[2]:(1.31723565389e-08) A[3]:(1.80138841e-16)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(5.61631123552e-12) A[2]:(3.21958957272e-08) A[3]:(1.34850788489e-17)\n",
      " state (6)  A[0]:(0.99994379282) A[1]:(9.67007932062e-11) A[2]:(5.61854467378e-05) A[3]:(7.79441951451e-20)\n",
      " state (7)  A[0]:(0.266173183918) A[1]:(9.51059231369e-10) A[2]:(0.733826816082) A[3]:(3.63944828265e-23)\n",
      " state (8)  A[0]:(0.00594535144046) A[1]:(8.18706699657e-11) A[2]:(0.994054675102) A[3]:(2.45269674423e-25)\n",
      " state (9)  A[0]:(0.000184754433576) A[1]:(9.13995875512e-12) A[2]:(0.999815225601) A[3]:(3.78871634962e-27)\n",
      " state (10)  A[0]:(1.27260982481e-05) A[1]:(1.92349998776e-12) A[2]:(0.999987244606) A[3]:(2.10165492215e-28)\n",
      " state (11)  A[0]:(1.46700631376e-06) A[1]:(6.08546958485e-13) A[2]:(0.999998509884) A[3]:(2.64807784004e-29)\n",
      " state (12)  A[0]:(2.92456803663e-07) A[1]:(2.74755776941e-13) A[2]:(0.999999701977) A[3]:(6.59204668258e-30)\n",
      " state (13)  A[0]:(1.00305271644e-07) A[1]:(1.66794109449e-13) A[2]:(0.999999880791) A[3]:(2.80850878242e-30)\n",
      " state (14)  A[0]:(5.21294829525e-08) A[1]:(1.24227532558e-13) A[2]:(0.999999940395) A[3]:(1.71041386971e-30)\n",
      " state (15)  A[0]:(3.54699736249e-08) A[1]:(1.04840384963e-13) A[2]:(0.999999940395) A[3]:(1.28908039511e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 778000 finished after 10 . Running score: 0.1. Policy_loss: -92050.6111081, Value_loss: 0.997039797796. Times trained:               15336. Times reached goal: 107.               Steps done: 10098030.\n",
      " state (0)  A[0]:(0.99502658844) A[1]:(0.0010159475496) A[2]:(0.00229250593111) A[3]:(0.00166496192105)\n",
      " state (1)  A[0]:(1.64308275998e-05) A[1]:(1.32479885906e-06) A[2]:(4.47677593911e-05) A[3]:(0.999937474728)\n",
      " state (2)  A[0]:(0.999999761581) A[1]:(1.46471323959e-09) A[2]:(2.62817707153e-07) A[3]:(1.91818894102e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.54903084488e-12) A[2]:(9.12283226739e-09) A[3]:(2.02001708986e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.59001022155e-12) A[2]:(1.76450605238e-08) A[3]:(1.63390670292e-16)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(6.07857904636e-12) A[2]:(5.35786526257e-08) A[3]:(8.86058087784e-18)\n",
      " state (6)  A[0]:(0.997828960419) A[1]:(2.64769595137e-10) A[2]:(0.00217106635682) A[3]:(7.82960942567e-21)\n",
      " state (7)  A[0]:(0.0839959830046) A[1]:(4.96491570079e-10) A[2]:(0.916004002094) A[3]:(1.07633941068e-23)\n",
      " state (8)  A[0]:(0.000264411472017) A[1]:(1.29477860725e-11) A[2]:(0.999735593796) A[3]:(9.68398713902e-27)\n",
      " state (9)  A[0]:(3.32789636559e-06) A[1]:(1.02766818492e-12) A[2]:(0.99999666214) A[3]:(8.63750390182e-29)\n",
      " state (10)  A[0]:(2.57109576296e-07) A[1]:(2.75464167535e-13) A[2]:(0.999999761581) A[3]:(8.18424306026e-30)\n",
      " state (11)  A[0]:(5.93469025034e-08) A[1]:(1.37684500845e-13) A[2]:(0.999999940395) A[3]:(2.46140881051e-30)\n",
      " state (12)  A[0]:(2.67528061926e-08) A[1]:(9.6116711324e-14) A[2]:(1.0) A[3]:(1.33719366101e-30)\n",
      " state (13)  A[0]:(1.75661476476e-08) A[1]:(7.98920459411e-14) A[2]:(1.0) A[3]:(9.80501090959e-31)\n",
      " state (14)  A[0]:(1.40836560192e-08) A[1]:(7.2597133925e-14) A[2]:(1.0) A[3]:(8.35830932333e-31)\n",
      " state (15)  A[0]:(1.25357750846e-08) A[1]:(6.90519757801e-14) A[2]:(1.0) A[3]:(7.69116972966e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 779000 finished after 5 . Running score: 0.19. Policy_loss: -92050.6130929, Value_loss: 1.66576729561. Times trained:               15708. Times reached goal: 130.               Steps done: 10113738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9941,  0.0017,  0.0016,  0.0026]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9941,  0.0017,  0.0016,  0.0026]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9941,  0.0017,  0.0016,  0.0026]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.4682e-12,  1.5978e-08,  3.3396e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9941,  0.0017,  0.0016,  0.0026]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.4713e-12,  1.5977e-08,  3.3419e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.5746e-04,  2.4538e-11,  9.9944e-01,  3.3463e-26]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.1356e-05,  2.4208e-12,  9.9999e-01,  4.3255e-28]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994046211243) A[1]:(0.0017319972394) A[2]:(0.00160779815633) A[3]:(0.00261400826275)\n",
      " state (1)  A[0]:(9.04712578631e-06) A[1]:(9.3222530495e-07) A[2]:(2.20597758016e-05) A[3]:(0.999967932701)\n",
      " state (2)  A[0]:(0.999999344349) A[1]:(5.83627013384e-09) A[2]:(5.70330882965e-07) A[3]:(5.50926628762e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(9.9840873416e-12) A[2]:(6.98743773953e-09) A[3]:(9.1401521158e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.47548593808e-12) A[2]:(1.5976231893e-08) A[3]:(3.34467741064e-16)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(7.62731457854e-12) A[2]:(5.01391284047e-08) A[3]:(1.46046148685e-17)\n",
      " state (6)  A[0]:(0.999250173569) A[1]:(2.14352827155e-10) A[2]:(0.000749854370952) A[3]:(2.16722548335e-20)\n",
      " state (7)  A[0]:(0.0809682160616) A[1]:(5.84765846856e-10) A[2]:(0.91903179884) A[3]:(1.51063297699e-23)\n",
      " state (8)  A[0]:(0.000557664199732) A[1]:(2.4549135294e-11) A[2]:(0.999442338943) A[3]:(3.34947104795e-26)\n",
      " state (9)  A[0]:(1.13584619612e-05) A[1]:(2.42135803595e-12) A[2]:(0.999988615513) A[3]:(4.32756506652e-28)\n",
      " state (10)  A[0]:(7.96260394509e-07) A[1]:(5.8100128441e-13) A[2]:(0.99999922514) A[3]:(3.22563835068e-29)\n",
      " state (11)  A[0]:(1.32080870685e-07) A[1]:(2.40169754744e-13) A[2]:(0.999999880791) A[3]:(6.79601169148e-30)\n",
      " state (12)  A[0]:(4.37975700152e-08) A[1]:(1.44021825173e-13) A[2]:(0.999999940395) A[3]:(2.81657718756e-30)\n",
      " state (13)  A[0]:(2.33448975706e-08) A[1]:(1.08721889727e-13) A[2]:(1.0) A[3]:(1.74791891035e-30)\n",
      " state (14)  A[0]:(1.64920521684e-08) A[1]:(9.33834918572e-14) A[2]:(1.0) A[3]:(1.35346193859e-30)\n",
      " state (15)  A[0]:(1.36314577404e-08) A[1]:(8.5998875664e-14) A[2]:(1.0) A[3]:(1.17920524543e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 780000 finished after 8 . Running score: 0.1. Policy_loss: -92050.6112087, Value_loss: 1.20183359187. Times trained:               15521. Times reached goal: 117.               Steps done: 10129259.\n",
      " state (0)  A[0]:(0.993316650391) A[1]:(0.00383853842504) A[2]:(0.00129699078389) A[3]:(0.00154780677985)\n",
      " state (1)  A[0]:(2.28669778153e-05) A[1]:(3.18909587804e-06) A[2]:(7.49444734538e-05) A[3]:(0.999898970127)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.34345971847e-11) A[2]:(6.43056408123e-09) A[3]:(3.25082198844e-14)\n",
      " state (3)  A[0]:(1.0) A[1]:(9.68680777952e-12) A[2]:(2.22379892278e-08) A[3]:(2.22921707542e-16)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(8.34967338326e-12) A[2]:(5.48032232928e-08) A[3]:(1.85301923712e-17)\n",
      " state (5)  A[0]:(0.999997258186) A[1]:(2.16585048507e-11) A[2]:(2.75189290733e-06) A[3]:(4.76783596353e-19)\n",
      " state (6)  A[0]:(0.284115701914) A[1]:(1.06196218486e-09) A[2]:(0.715884327888) A[3]:(1.01282615239e-22)\n",
      " state (7)  A[0]:(0.00367293483578) A[1]:(6.58857760016e-11) A[2]:(0.99632704258) A[3]:(3.65432024342e-25)\n",
      " state (8)  A[0]:(4.4002863433e-05) A[1]:(4.27021222202e-12) A[2]:(0.999956011772) A[3]:(2.06721732067e-27)\n",
      " state (9)  A[0]:(2.06914864975e-06) A[1]:(7.60376516525e-13) A[2]:(0.999997913837) A[3]:(8.5538354865e-29)\n",
      " state (10)  A[0]:(2.23277581313e-07) A[1]:(2.4291273203e-13) A[2]:(0.999999761581) A[3]:(1.11296677823e-29)\n",
      " state (11)  A[0]:(5.19393239529e-08) A[1]:(1.21347728957e-13) A[2]:(0.999999940395) A[3]:(3.32972034447e-30)\n",
      " state (12)  A[0]:(2.23137028854e-08) A[1]:(8.2694220666e-14) A[2]:(1.0) A[3]:(1.73080051526e-30)\n",
      " state (13)  A[0]:(1.40854288233e-08) A[1]:(6.74891932162e-14) A[2]:(1.0) A[3]:(1.22861418822e-30)\n",
      " state (14)  A[0]:(1.1016626722e-08) A[1]:(6.06486161205e-14) A[2]:(1.0) A[3]:(1.0271644551e-30)\n",
      " state (15)  A[0]:(9.66478719278e-09) A[1]:(5.731962756e-14) A[2]:(1.0) A[3]:(9.34773044158e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 781000 finished after 10 . Running score: 0.14. Policy_loss: -92050.6121961, Value_loss: 0.979752495494. Times trained:               15162. Times reached goal: 133.               Steps done: 10144421.\n",
      " state (0)  A[0]:(0.992409825325) A[1]:(0.0036481115967) A[2]:(0.00294621638022) A[3]:(0.000995851703919)\n",
      " state (1)  A[0]:(6.18419799139e-05) A[1]:(8.41716428113e-06) A[2]:(0.000329880364006) A[3]:(0.99959987402)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.47659419414e-11) A[2]:(2.01285139667e-08) A[3]:(2.53481936301e-15)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(1.2200975473e-11) A[2]:(7.55582973966e-08) A[3]:(4.18086429952e-17)\n",
      " state (4)  A[0]:(0.999999165535) A[1]:(1.7041120251e-11) A[2]:(8.62950400915e-07) A[3]:(1.56725703865e-18)\n",
      " state (5)  A[0]:(0.183737188578) A[1]:(1.02239761102e-09) A[2]:(0.816262841225) A[3]:(9.86131036245e-23)\n",
      " state (6)  A[0]:(0.000116453797091) A[1]:(1.0154358257e-11) A[2]:(0.999883532524) A[3]:(1.30641189917e-26)\n",
      " state (7)  A[0]:(1.50801952259e-07) A[1]:(2.35307108e-13) A[2]:(0.999999821186) A[3]:(1.23362553693e-29)\n",
      " state (8)  A[0]:(1.07517843517e-08) A[1]:(6.55064110594e-14) A[2]:(1.0) A[3]:(1.30317165715e-30)\n",
      " state (9)  A[0]:(3.55261575713e-09) A[1]:(3.9989225039e-14) A[2]:(1.0) A[3]:(5.6264218958e-31)\n",
      " state (10)  A[0]:(2.23066276561e-09) A[1]:(3.27602494557e-14) A[2]:(1.0) A[3]:(4.02802323377e-31)\n",
      " state (11)  A[0]:(1.8300418958e-09) A[1]:(3.01451775326e-14) A[2]:(1.0) A[3]:(3.50722492852e-31)\n",
      " state (12)  A[0]:(1.68009051116e-09) A[1]:(2.90911432855e-14) A[2]:(1.0) A[3]:(3.30608914643e-31)\n",
      " state (13)  A[0]:(1.61860203018e-09) A[1]:(2.86449364933e-14) A[2]:(1.0) A[3]:(3.22251361398e-31)\n",
      " state (14)  A[0]:(1.59236546171e-09) A[1]:(2.8451958671e-14) A[2]:(1.0) A[3]:(3.18662295026e-31)\n",
      " state (15)  A[0]:(1.58096835623e-09) A[1]:(2.83674806871e-14) A[2]:(1.0) A[3]:(3.17100439192e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 782000 finished after 6 . Running score: 0.14. Policy_loss: -92050.6119303, Value_loss: 1.20302438759. Times trained:               16055. Times reached goal: 122.               Steps done: 10160476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995092272758) A[1]:(0.00177229742985) A[2]:(0.00263590458781) A[3]:(0.000499548681546)\n",
      " state (1)  A[0]:(4.98784647789e-05) A[1]:(4.76552941109e-06) A[2]:(0.000142942983075) A[3]:(0.999802410603)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(1.94125812647e-10) A[2]:(9.39631803476e-08) A[3]:(6.65913678366e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.86354965118e-12) A[2]:(2.76624838591e-08) A[3]:(3.23093306192e-16)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(7.67850540095e-12) A[2]:(1.35488278374e-07) A[3]:(5.89022782413e-18)\n",
      " state (5)  A[0]:(0.309705883265) A[1]:(9.03648378348e-10) A[2]:(0.690294086933) A[3]:(1.58749434555e-22)\n",
      " state (6)  A[0]:(7.55658811613e-06) A[1]:(1.43907975814e-12) A[2]:(0.99999243021) A[3]:(5.31763798416e-28)\n",
      " state (7)  A[0]:(1.02585238082e-08) A[1]:(4.687894021e-14) A[2]:(1.0) A[3]:(1.1113840174e-30)\n",
      " state (8)  A[0]:(2.52919596377e-09) A[1]:(2.55649519024e-14) A[2]:(1.0) A[3]:(3.97335757428e-31)\n",
      " state (9)  A[0]:(1.68673053302e-09) A[1]:(2.16676618325e-14) A[2]:(1.0) A[3]:(3.01873790668e-31)\n",
      " state (10)  A[0]:(1.46679357549e-09) A[1]:(2.04920427822e-14) A[2]:(1.0) A[3]:(2.75352451694e-31)\n",
      " state (11)  A[0]:(1.39313616199e-09) A[1]:(2.00784128771e-14) A[2]:(1.0) A[3]:(2.6627669491e-31)\n",
      " state (12)  A[0]:(1.36620093016e-09) A[1]:(1.99244798855e-14) A[2]:(1.0) A[3]:(2.62933635996e-31)\n",
      " state (13)  A[0]:(1.35598277051e-09) A[1]:(1.98657754201e-14) A[2]:(1.0) A[3]:(2.61662903093e-31)\n",
      " state (14)  A[0]:(1.35202116169e-09) A[1]:(1.98430156448e-14) A[2]:(1.0) A[3]:(2.61170276921e-31)\n",
      " state (15)  A[0]:(1.35043354277e-09) A[1]:(1.98338964881e-14) A[2]:(1.0) A[3]:(2.60975074329e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 783000 finished after 14 . Running score: 0.12. Policy_loss: -92050.611214, Value_loss: 0.984109171179. Times trained:               15580. Times reached goal: 110.               Steps done: 10176056.\n",
      " state (0)  A[0]:(0.99664825201) A[1]:(0.00135964830406) A[2]:(0.00141117034946) A[3]:(0.000580930965953)\n",
      " state (1)  A[0]:(4.17821865994e-05) A[1]:(3.40117389896e-06) A[2]:(8.85800182004e-05) A[3]:(0.999866247177)\n",
      " state (2)  A[0]:(0.999993383884) A[1]:(2.4016973299e-08) A[2]:(6.54975610814e-06) A[3]:(2.97085787082e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.52882738791e-12) A[2]:(2.21638014608e-08) A[3]:(3.52965246566e-16)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(6.35358649373e-12) A[2]:(1.21288849186e-07) A[3]:(4.57965609649e-18)\n",
      " state (5)  A[0]:(0.345775455236) A[1]:(7.72635067037e-10) A[2]:(0.654224514961) A[3]:(1.34192623716e-22)\n",
      " state (6)  A[0]:(1.15546827146e-05) A[1]:(1.60853231776e-12) A[2]:(0.999988436699) A[3]:(7.00779028845e-28)\n",
      " state (7)  A[0]:(1.22528138746e-08) A[1]:(4.47899096284e-14) A[2]:(1.0) A[3]:(1.10890231373e-30)\n",
      " state (8)  A[0]:(2.93544122343e-09) A[1]:(2.38689124211e-14) A[2]:(1.0) A[3]:(3.80220912329e-31)\n",
      " state (9)  A[0]:(1.95178118112e-09) A[1]:(2.01332514842e-14) A[2]:(1.0) A[3]:(2.86316875252e-31)\n",
      " state (10)  A[0]:(1.69575631315e-09) A[1]:(1.9008771201e-14) A[2]:(1.0) A[3]:(2.60358574562e-31)\n",
      " state (11)  A[0]:(1.60989477305e-09) A[1]:(1.86126850423e-14) A[2]:(1.0) A[3]:(2.51473694551e-31)\n",
      " state (12)  A[0]:(1.57842539039e-09) A[1]:(1.84647761491e-14) A[2]:(1.0) A[3]:(2.48191479224e-31)\n",
      " state (13)  A[0]:(1.56645862948e-09) A[1]:(1.84081960422e-14) A[2]:(1.0) A[3]:(2.46941129393e-31)\n",
      " state (14)  A[0]:(1.56181656497e-09) A[1]:(1.83862646652e-14) A[2]:(1.0) A[3]:(2.46455532677e-31)\n",
      " state (15)  A[0]:(1.55995882878e-09) A[1]:(1.83775351436e-14) A[2]:(1.0) A[3]:(2.46263809548e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 784000 finished after 6 . Running score: 0.11. Policy_loss: -92050.6111935, Value_loss: 1.00312474964. Times trained:               15927. Times reached goal: 119.               Steps done: 10191983.\n",
      "action_dist \n",
      "tensor([[ 0.9939,  0.0015,  0.0035,  0.0011]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.1383e-12,  5.6343e-08,  2.8668e-17]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.1389e-12,  5.6349e-08,  2.8672e-17]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.1394e-12,  5.6354e-08,  2.8676e-17]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3338e-08,  3.7741e-14,  1.0000e+00,  1.4025e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.7642e-09,  2.1454e-14,  1.0000e+00,  5.3548e-31]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993942975998) A[1]:(0.00147059862502) A[2]:(0.00352213508449) A[3]:(0.0010643156711)\n",
      " state (1)  A[0]:(2.85388032353e-05) A[1]:(2.50907214649e-06) A[2]:(7.67635865486e-05) A[3]:(0.999892175198)\n",
      " state (2)  A[0]:(0.999998509884) A[1]:(4.65354688117e-09) A[2]:(1.47870252931e-06) A[3]:(4.65304106356e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.73565456882e-12) A[2]:(1.95354008525e-08) A[3]:(8.84724960495e-16)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(5.1405438066e-12) A[2]:(5.63648754337e-08) A[3]:(2.86880827899e-17)\n",
      " state (5)  A[0]:(0.999999165535) A[1]:(7.15765424361e-12) A[2]:(8.34594686694e-07) A[3]:(5.51266092333e-19)\n",
      " state (6)  A[0]:(0.0204308349639) A[1]:(1.17804696553e-10) A[2]:(0.979569137096) A[3]:(3.91652912806e-24)\n",
      " state (7)  A[0]:(8.81662288066e-07) A[1]:(3.07672371738e-13) A[2]:(0.99999910593) A[3]:(5.83624301068e-29)\n",
      " state (8)  A[0]:(1.33364261856e-08) A[1]:(3.77413520629e-14) A[2]:(1.0) A[3]:(1.4025755972e-30)\n",
      " state (9)  A[0]:(3.76392250701e-09) A[1]:(2.14537792371e-14) A[2]:(1.0) A[3]:(5.35494241588e-31)\n",
      " state (10)  A[0]:(2.37901609523e-09) A[1]:(1.76459493989e-14) A[2]:(1.0) A[3]:(3.8604059678e-31)\n",
      " state (11)  A[0]:(1.97965310811e-09) A[1]:(1.63452557695e-14) A[2]:(1.0) A[3]:(3.39911142169e-31)\n",
      " state (12)  A[0]:(1.83280146615e-09) A[1]:(1.58339663538e-14) A[2]:(1.0) A[3]:(3.22482557627e-31)\n",
      " state (13)  A[0]:(1.77310144345e-09) A[1]:(1.56204801698e-14) A[2]:(1.0) A[3]:(3.15320035448e-31)\n",
      " state (14)  A[0]:(1.74754422044e-09) A[1]:(1.55283619486e-14) A[2]:(1.0) A[3]:(3.12250960758e-31)\n",
      " state (15)  A[0]:(1.73608916132e-09) A[1]:(1.54871588779e-14) A[2]:(1.0) A[3]:(3.10881768448e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 785000 finished after 6 . Running score: 0.07. Policy_loss: -92050.6114975, Value_loss: 1.62583682713. Times trained:               15956. Times reached goal: 112.               Steps done: 10207939.\n",
      " state (0)  A[0]:(0.994564116001) A[1]:(0.00175841024611) A[2]:(0.00219661509618) A[3]:(0.00148088810965)\n",
      " state (1)  A[0]:(2.45715291385e-05) A[1]:(2.2514830107e-06) A[2]:(5.52167075512e-05) A[3]:(0.999917984009)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(4.03822753015e-10) A[2]:(1.31820499405e-07) A[3]:(7.46857367617e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.14958257528e-12) A[2]:(1.40341276378e-08) A[3]:(8.63248440117e-16)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(5.93150919007e-12) A[2]:(3.14069801277e-08) A[3]:(5.30194880116e-17)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(5.14924821532e-12) A[2]:(7.99454795697e-08) A[3]:(3.91569352163e-18)\n",
      " state (6)  A[0]:(0.991605520248) A[1]:(1.98965816045e-10) A[2]:(0.00839450955391) A[3]:(1.2403699503e-21)\n",
      " state (7)  A[0]:(0.0214377623051) A[1]:(1.46868295303e-10) A[2]:(0.978562235832) A[3]:(2.3757701842e-24)\n",
      " state (8)  A[0]:(0.000297253369354) A[1]:(1.09251279898e-11) A[2]:(0.999702751637) A[3]:(1.85340728048e-26)\n",
      " state (9)  A[0]:(8.11386962596e-06) A[1]:(1.34366855749e-12) A[2]:(0.999991893768) A[3]:(3.64702399842e-28)\n",
      " state (10)  A[0]:(1.00715487861e-06) A[1]:(4.30306560783e-13) A[2]:(0.999998986721) A[3]:(4.45106995847e-29)\n",
      " state (11)  A[0]:(3.43853770346e-07) A[1]:(2.45881575734e-13) A[2]:(0.999999642372) A[3]:(1.60533362731e-29)\n",
      " state (12)  A[0]:(2.02324841325e-07) A[1]:(1.87888604414e-13) A[2]:(0.999999821186) A[3]:(9.87253431436e-30)\n",
      " state (13)  A[0]:(1.56299790888e-07) A[1]:(1.65123408111e-13) A[2]:(0.999999821186) A[3]:(7.82526476495e-30)\n",
      " state (14)  A[0]:(1.37907790076e-07) A[1]:(1.55161406184e-13) A[2]:(0.999999880791) A[3]:(6.99800563146e-30)\n",
      " state (15)  A[0]:(1.29776452695e-07) A[1]:(1.50560662028e-13) A[2]:(0.999999880791) A[3]:(6.63023050067e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 786000 finished after 37 . Running score: 0.07. Policy_loss: -92050.6112265, Value_loss: 1.00054061846. Times trained:               15506. Times reached goal: 103.               Steps done: 10223445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996371269226) A[1]:(0.00137310195714) A[2]:(0.00134625099599) A[3]:(0.000909387832507)\n",
      " state (1)  A[0]:(2.9559021641e-05) A[1]:(2.46971671913e-06) A[2]:(5.62447603443e-05) A[3]:(0.999911725521)\n",
      " state (2)  A[0]:(0.999986350536) A[1]:(7.21207698007e-08) A[2]:(1.33611392812e-05) A[3]:(2.39901709165e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.67786181854e-12) A[2]:(1.13285425485e-08) A[3]:(1.02652600504e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.49590234034e-12) A[2]:(2.29256134077e-08) A[3]:(7.93988170677e-17)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(5.56620981945e-12) A[2]:(4.69378598211e-08) A[3]:(8.78559199222e-18)\n",
      " state (6)  A[0]:(0.999998509884) A[1]:(1.18698887525e-11) A[2]:(1.48994615756e-06) A[3]:(1.6870862996e-19)\n",
      " state (7)  A[0]:(0.128662794828) A[1]:(5.06214459239e-10) A[2]:(0.871337234974) A[3]:(1.86121680499e-23)\n",
      " state (8)  A[0]:(0.00100550020579) A[1]:(2.56963287454e-11) A[2]:(0.998994529247) A[3]:(6.07927336591e-26)\n",
      " state (9)  A[0]:(1.09809016067e-05) A[1]:(1.79949046115e-12) A[2]:(0.999989032745) A[3]:(4.14481212965e-28)\n",
      " state (10)  A[0]:(1.04352272956e-06) A[1]:(4.95336464987e-13) A[2]:(0.999998927116) A[3]:(3.82169410824e-29)\n",
      " state (11)  A[0]:(3.57187360578e-07) A[1]:(2.83438501619e-13) A[2]:(0.999999642372) A[3]:(1.3838419036e-29)\n",
      " state (12)  A[0]:(2.1871690592e-07) A[1]:(2.21020210055e-13) A[2]:(0.999999761581) A[3]:(8.8347732949e-30)\n",
      " state (13)  A[0]:(1.7419151277e-07) A[1]:(1.97211116795e-13) A[2]:(0.999999821186) A[3]:(7.19902381709e-30)\n",
      " state (14)  A[0]:(1.56553397801e-07) A[1]:(1.87005860558e-13) A[2]:(0.999999821186) A[3]:(6.54494114447e-30)\n",
      " state (15)  A[0]:(1.48850745063e-07) A[1]:(1.82381440137e-13) A[2]:(0.999999880791) A[3]:(6.25796178412e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 787000 finished after 20 . Running score: 0.08. Policy_loss: -92050.6116374, Value_loss: 1.21339823057. Times trained:               16327. Times reached goal: 112.               Steps done: 10239772.\n",
      " state (0)  A[0]:(0.996857643127) A[1]:(0.00095150800189) A[2]:(0.00115092948545) A[3]:(0.0010398972081)\n",
      " state (1)  A[0]:(2.97787082673e-05) A[1]:(2.12862755689e-06) A[2]:(5.2265553677e-05) A[3]:(0.999915838242)\n",
      " state (2)  A[0]:(1.0) A[1]:(6.10215211694e-11) A[2]:(2.43930191601e-08) A[3]:(2.24435856185e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.06565709518e-12) A[2]:(1.14926841377e-08) A[3]:(3.9522544084e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.22498147579e-12) A[2]:(2.06038031081e-08) A[3]:(4.78680685457e-17)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(3.6659204318e-12) A[2]:(3.428712958e-08) A[3]:(9.34308608249e-18)\n",
      " state (6)  A[0]:(0.999999880791) A[1]:(3.81853966849e-12) A[2]:(1.12406446817e-07) A[3]:(9.1962300909e-19)\n",
      " state (7)  A[0]:(0.81557071209) A[1]:(5.84921999724e-10) A[2]:(0.184429317713) A[3]:(2.0737622177e-22)\n",
      " state (8)  A[0]:(0.0415214411914) A[1]:(1.72461600556e-10) A[2]:(0.958478569984) A[3]:(2.98066655695e-24)\n",
      " state (9)  A[0]:(0.000887922476977) A[1]:(1.57994277383e-11) A[2]:(0.999112069607) A[3]:(3.23570374687e-26)\n",
      " state (10)  A[0]:(3.17859958159e-05) A[1]:(2.20263412544e-12) A[2]:(0.999968230724) A[3]:(8.04445915517e-28)\n",
      " state (11)  A[0]:(4.3403242671e-06) A[1]:(7.26255264694e-13) A[2]:(0.999995648861) A[3]:(1.03514617836e-28)\n",
      " state (12)  A[0]:(1.49642539782e-06) A[1]:(4.11655871066e-13) A[2]:(0.999998509884) A[3]:(3.6762394529e-29)\n",
      " state (13)  A[0]:(8.61790738327e-07) A[1]:(3.09042749074e-13) A[2]:(0.999999165535) A[3]:(2.1890631323e-29)\n",
      " state (14)  A[0]:(6.49167873235e-07) A[1]:(2.67279850596e-13) A[2]:(0.999999344349) A[3]:(1.68574105203e-29)\n",
      " state (15)  A[0]:(5.61458818993e-07) A[1]:(2.48252346414e-13) A[2]:(0.999999463558) A[3]:(1.47644467783e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 788000 finished after 10 . Running score: 0.08. Policy_loss: -92050.6115244, Value_loss: 1.21823832085. Times trained:               15582. Times reached goal: 112.               Steps done: 10255354.\n",
      " state (0)  A[0]:(0.989238977432) A[1]:(0.00635688425973) A[2]:(0.00241444003768) A[3]:(0.00198971922509)\n",
      " state (1)  A[0]:(4.16213260905e-05) A[1]:(7.78740104579e-06) A[2]:(8.12449288787e-05) A[3]:(0.999869346619)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.7475786363e-11) A[2]:(6.23396445576e-09) A[3]:(2.166518172e-14)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.08316774889e-11) A[2]:(1.4986998309e-08) A[3]:(1.08089847929e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.19431703311e-11) A[2]:(2.55948897632e-08) A[3]:(1.68580384911e-17)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(3.33394700736e-11) A[2]:(4.32504201342e-08) A[3]:(3.3729902479e-18)\n",
      " state (6)  A[0]:(0.999999463558) A[1]:(6.21865892114e-11) A[2]:(5.13949544256e-07) A[3]:(1.69537800976e-19)\n",
      " state (7)  A[0]:(0.676432073116) A[1]:(8.07561129079e-09) A[2]:(0.323567926884) A[3]:(1.10202404334e-22)\n",
      " state (8)  A[0]:(0.0947840213776) A[1]:(3.38836869673e-09) A[2]:(0.905215978622) A[3]:(6.26508203331e-24)\n",
      " state (9)  A[0]:(0.00605579139665) A[1]:(5.91645787917e-10) A[2]:(0.993944227695) A[3]:(2.11778633996e-25)\n",
      " state (10)  A[0]:(0.000332235387759) A[1]:(1.00933039704e-10) A[2]:(0.99966776371) A[3]:(7.53250075363e-27)\n",
      " state (11)  A[0]:(3.53280738636e-05) A[1]:(2.73601176437e-11) A[2]:(0.999964654446) A[3]:(6.56499684604e-28)\n",
      " state (12)  A[0]:(8.42634290166e-06) A[1]:(1.23024784804e-11) A[2]:(0.999991595745) A[3]:(1.50034227862e-28)\n",
      " state (13)  A[0]:(3.66148105968e-06) A[1]:(7.84010009691e-12) A[2]:(0.999996364117) A[3]:(6.58270577188e-29)\n",
      " state (14)  A[0]:(2.30468504014e-06) A[1]:(6.13336031269e-12) A[2]:(0.999997675419) A[3]:(4.21405388523e-29)\n",
      " state (15)  A[0]:(1.791724344e-06) A[1]:(5.37449815555e-12) A[2]:(0.999998211861) A[3]:(3.31808930747e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 789000 finished after 22 . Running score: 0.09. Policy_loss: -92050.6195359, Value_loss: 1.42063001587. Times trained:               15967. Times reached goal: 100.               Steps done: 10271321.\n",
      "action_dist \n",
      "tensor([[ 0.9895,  0.0041,  0.0023,  0.0041]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9262e-11,  2.4369e-07,  1.5365e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9895,  0.0041,  0.0023,  0.0041]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9895,  0.0041,  0.0023,  0.0041]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9895,  0.0041,  0.0023,  0.0041]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9895,  0.0041,  0.0023,  0.0041]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9261e-11,  2.4364e-07,  1.5376e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9261e-11,  2.4363e-07,  1.5378e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.1615e-07,  2.4384e-12,  1.0000e+00,  2.6317e-29]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9261e-11,  2.4362e-07,  1.5381e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9261e-11,  2.4362e-07,  1.5383e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.9261e-11,  2.4361e-07,  1.5384e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.1634e-07,  2.4388e-12,  1.0000e+00,  2.6332e-29]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.989459574223) A[1]:(0.00407946994528) A[2]:(0.00234884349629) A[3]:(0.00411212444305)\n",
      " state (1)  A[0]:(3.33466305165e-05) A[1]:(5.35326898898e-06) A[2]:(8.15925377537e-05) A[3]:(0.999879717827)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.63309521356e-11) A[2]:(2.11614370471e-08) A[3]:(1.17216844988e-15)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(4.08049219414e-11) A[2]:(7.40152259482e-08) A[3]:(1.96522099326e-17)\n",
      " state (4)  A[0]:(0.999999761581) A[1]:(4.92612617364e-11) A[2]:(2.43604802108e-07) A[3]:(1.53868673728e-18)\n",
      " state (5)  A[0]:(0.388431340456) A[1]:(6.17892936816e-09) A[2]:(0.611568689346) A[3]:(1.19015760315e-22)\n",
      " state (6)  A[0]:(0.00375003367662) A[1]:(4.41758407632e-10) A[2]:(0.996249973774) A[3]:(3.80702621182e-25)\n",
      " state (7)  A[0]:(1.30804710352e-05) A[1]:(1.47527146749e-11) A[2]:(0.999986946583) A[3]:(7.08513948688e-28)\n",
      " state (8)  A[0]:(5.16378520388e-07) A[1]:(2.4389513846e-12) A[2]:(0.999999463558) A[3]:(2.63351359669e-29)\n",
      " state (9)  A[0]:(1.35673943191e-07) A[1]:(1.21066438125e-12) A[2]:(0.999999880791) A[3]:(7.49448704932e-30)\n",
      " state (10)  A[0]:(7.93122936216e-08) A[1]:(9.21040695968e-13) A[2]:(0.999999940395) A[3]:(4.61148126539e-30)\n",
      " state (11)  A[0]:(6.39057162743e-08) A[1]:(8.26187371555e-13) A[2]:(0.999999940395) A[3]:(3.8052723205e-30)\n",
      " state (12)  A[0]:(5.85618096238e-08) A[1]:(7.90842977043e-13) A[2]:(0.999999940395) A[3]:(3.52268611167e-30)\n",
      " state (13)  A[0]:(5.65187114887e-08) A[1]:(7.76935482096e-13) A[2]:(0.999999940395) A[3]:(3.41417425332e-30)\n",
      " state (14)  A[0]:(5.56998109857e-08) A[1]:(7.712981729e-13) A[2]:(0.999999940395) A[3]:(3.37064146573e-30)\n",
      " state (15)  A[0]:(5.53591910091e-08) A[1]:(7.6894794785e-13) A[2]:(0.999999940395) A[3]:(3.35253471499e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 790000 finished after 13 . Running score: 0.1. Policy_loss: -92050.6112981, Value_loss: 1.62620357908. Times trained:               16032. Times reached goal: 101.               Steps done: 10287353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994254887104) A[1]:(0.00211200886406) A[2]:(0.00142574054189) A[3]:(0.00220737955533)\n",
      " state (1)  A[0]:(1.633638567e-05) A[1]:(2.24441032515e-06) A[2]:(2.72040397249e-05) A[3]:(0.999954223633)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.32654112589e-11) A[2]:(1.01794679352e-08) A[3]:(5.81362825177e-14)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(3.59740397193e-11) A[2]:(6.87463739268e-08) A[3]:(3.50893556178e-17)\n",
      " state (4)  A[0]:(0.999999403954) A[1]:(5.6471397003e-11) A[2]:(5.97316500262e-07) A[3]:(8.45068910287e-19)\n",
      " state (5)  A[0]:(0.0341906286776) A[1]:(1.66815661284e-09) A[2]:(0.965809345245) A[3]:(6.32424896778e-24)\n",
      " state (6)  A[0]:(2.57111059909e-06) A[1]:(5.65335538799e-12) A[2]:(0.999997437) A[3]:(1.69811120978e-28)\n",
      " state (7)  A[0]:(2.21338911643e-08) A[1]:(4.50613938524e-13) A[2]:(1.0) A[3]:(1.7792957698e-30)\n",
      " state (8)  A[0]:(7.83586440178e-09) A[1]:(2.7164601406e-13) A[2]:(1.0) A[3]:(7.33633406453e-31)\n",
      " state (9)  A[0]:(5.78848169397e-09) A[1]:(2.35423876574e-13) A[2]:(1.0) A[3]:(5.72410171478e-31)\n",
      " state (10)  A[0]:(5.10089837036e-09) A[1]:(2.22165656098e-13) A[2]:(1.0) A[3]:(5.1784015021e-31)\n",
      " state (11)  A[0]:(4.66270533295e-09) A[1]:(2.13550328137e-13) A[2]:(1.0) A[3]:(4.83668012214e-31)\n",
      " state (12)  A[0]:(4.2479197937e-09) A[1]:(2.05199409981e-13) A[2]:(1.0) A[3]:(4.5150973114e-31)\n",
      " state (13)  A[0]:(3.8396965607e-09) A[1]:(1.96656750671e-13) A[2]:(1.0) A[3]:(4.19609353526e-31)\n",
      " state (14)  A[0]:(3.48267947814e-09) A[1]:(1.88848990699e-13) A[2]:(1.0) A[3]:(3.91339819353e-31)\n",
      " state (15)  A[0]:(3.21156612415e-09) A[1]:(1.82676695494e-13) A[2]:(1.0) A[3]:(3.69592703665e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 791000 finished after 6 . Running score: 0.09. Policy_loss: -92050.6112677, Value_loss: 1.22170510214. Times trained:               16048. Times reached goal: 103.               Steps done: 10303401.\n",
      " state (0)  A[0]:(0.995643854141) A[1]:(0.00199661380611) A[2]:(0.00116230233107) A[3]:(0.00119725510012)\n",
      " state (1)  A[0]:(2.56962830463e-05) A[1]:(3.30347143063e-06) A[2]:(4.24351965194e-05) A[3]:(0.999928593636)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.27244810311e-11) A[2]:(1.58926827254e-08) A[3]:(8.83919605804e-14)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(3.60254673315e-11) A[2]:(6.51634053384e-08) A[3]:(2.92202213129e-17)\n",
      " state (4)  A[0]:(0.999999821186) A[1]:(4.22048854198e-11) A[2]:(2.01902707886e-07) A[3]:(1.52514682463e-18)\n",
      " state (5)  A[0]:(0.0421774797142) A[1]:(1.85571491507e-09) A[2]:(0.957822501659) A[3]:(6.60529683691e-24)\n",
      " state (6)  A[0]:(1.24398786738e-06) A[1]:(3.9438764754e-12) A[2]:(0.999998748302) A[3]:(6.50886020114e-29)\n",
      " state (7)  A[0]:(1.86444371053e-08) A[1]:(4.31584320148e-13) A[2]:(1.0) A[3]:(1.21928555909e-30)\n",
      " state (8)  A[0]:(7.44402139929e-09) A[1]:(2.76395253256e-13) A[2]:(1.0) A[3]:(5.59393499353e-31)\n",
      " state (9)  A[0]:(5.66966207316e-09) A[1]:(2.43132039024e-13) A[2]:(1.0) A[3]:(4.47934535601e-31)\n",
      " state (10)  A[0]:(5.04041341998e-09) A[1]:(2.30433944495e-13) A[2]:(1.0) A[3]:(4.08300439594e-31)\n",
      " state (11)  A[0]:(4.61670568441e-09) A[1]:(2.21742735935e-13) A[2]:(1.0) A[3]:(3.82108215529e-31)\n",
      " state (12)  A[0]:(4.2059644656e-09) A[1]:(2.13107621285e-13) A[2]:(1.0) A[3]:(3.56841293596e-31)\n",
      " state (13)  A[0]:(3.80255604782e-09) A[1]:(2.04285603733e-13) A[2]:(1.0) A[3]:(3.31801594722e-31)\n",
      " state (14)  A[0]:(3.45363981857e-09) A[1]:(1.96313465158e-13) A[2]:(1.0) A[3]:(3.09854104267e-31)\n",
      " state (15)  A[0]:(3.19198756316e-09) A[1]:(1.90094332419e-13) A[2]:(1.0) A[3]:(2.93182726166e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 792000 finished after 21 . Running score: 0.09. Policy_loss: -92050.6112647, Value_loss: 1.22293545843. Times trained:               15190. Times reached goal: 90.               Steps done: 10318591.\n",
      " state (0)  A[0]:(0.996320545673) A[1]:(0.0022652761545) A[2]:(0.000591446761973) A[3]:(0.000822725880425)\n",
      " state (1)  A[0]:(1.69575578184e-05) A[1]:(2.13159296436e-06) A[2]:(1.86038887477e-05) A[3]:(0.999962329865)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.58289389876e-10) A[2]:(2.09150581298e-08) A[3]:(1.66335539092e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.01315396245e-11) A[2]:(1.86620123799e-08) A[3]:(1.04547887016e-16)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(3.581663785e-11) A[2]:(7.00265019304e-08) A[3]:(1.45991505133e-18)\n",
      " state (5)  A[0]:(0.076693251729) A[1]:(3.18445358971e-09) A[2]:(0.923306763172) A[3]:(9.30434537551e-24)\n",
      " state (6)  A[0]:(8.85288855557e-08) A[1]:(1.50967346263e-12) A[2]:(0.999999940395) A[3]:(5.48969742071e-30)\n",
      " state (7)  A[0]:(4.67674210469e-09) A[1]:(3.52486106709e-13) A[2]:(1.0) A[3]:(4.16286748214e-31)\n",
      " state (8)  A[0]:(2.59979371364e-09) A[1]:(2.72720648148e-13) A[2]:(1.0) A[3]:(2.67459641895e-31)\n",
      " state (9)  A[0]:(2.06698813621e-09) A[1]:(2.49319201352e-13) A[2]:(1.0) A[3]:(2.29550113651e-31)\n",
      " state (10)  A[0]:(1.8481559616e-09) A[1]:(2.39445263747e-13) A[2]:(1.0) A[3]:(2.14362914717e-31)\n",
      " state (11)  A[0]:(1.75654024659e-09) A[1]:(2.3526945908e-13) A[2]:(1.0) A[3]:(2.08083353365e-31)\n",
      " state (12)  A[0]:(1.71989333886e-09) A[1]:(2.33594475143e-13) A[2]:(1.0) A[3]:(2.05588507665e-31)\n",
      " state (13)  A[0]:(1.705636965e-09) A[1]:(2.32943980945e-13) A[2]:(1.0) A[3]:(2.04623003625e-31)\n",
      " state (14)  A[0]:(1.70014125001e-09) A[1]:(2.32693530243e-13) A[2]:(1.0) A[3]:(2.04251782509e-31)\n",
      " state (15)  A[0]:(1.69804781347e-09) A[1]:(2.32599475705e-13) A[2]:(1.0) A[3]:(2.04111593053e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 793000 finished after 16 . Running score: 0.1. Policy_loss: -92050.6111961, Value_loss: 1.41386611498. Times trained:               15507. Times reached goal: 108.               Steps done: 10334098.\n",
      " state (0)  A[0]:(0.996879220009) A[1]:(0.00114442233462) A[2]:(0.000592252647039) A[3]:(0.00138413324021)\n",
      " state (1)  A[0]:(1.33630483106e-05) A[1]:(1.08963547518e-06) A[2]:(8.9576415121e-06) A[3]:(0.999976575375)\n",
      " state (2)  A[0]:(0.000188894147868) A[1]:(5.10695599587e-06) A[2]:(5.02007314935e-05) A[3]:(0.99975579977)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.12221143836e-11) A[2]:(4.66478411454e-10) A[3]:(3.47754133432e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.09610463067e-11) A[2]:(1.94489513383e-09) A[3]:(9.09972153917e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.05695825356e-11) A[2]:(7.87066056773e-09) A[3]:(3.6642525357e-18)\n",
      " state (6)  A[0]:(0.999992907047) A[1]:(1.60985808018e-10) A[2]:(7.07623985363e-06) A[3]:(3.14427118962e-20)\n",
      " state (7)  A[0]:(0.000329170870828) A[1]:(1.18375892422e-10) A[2]:(0.999670803547) A[3]:(1.71310590329e-26)\n",
      " state (8)  A[0]:(1.03949453489e-07) A[1]:(1.59423787106e-12) A[2]:(0.999999880791) A[3]:(6.93523460929e-30)\n",
      " state (9)  A[0]:(1.38638283076e-08) A[1]:(6.39533564994e-13) A[2]:(1.0) A[3]:(1.43369121478e-30)\n",
      " state (10)  A[0]:(7.52166240403e-09) A[1]:(4.99534387379e-13) A[2]:(1.0) A[3]:(9.48443761339e-31)\n",
      " state (11)  A[0]:(5.95807403414e-09) A[1]:(4.57509301711e-13) A[2]:(1.0) A[3]:(8.21141672806e-31)\n",
      " state (12)  A[0]:(5.3801669786e-09) A[1]:(4.40955296365e-13) A[2]:(1.0) A[3]:(7.73666277163e-31)\n",
      " state (13)  A[0]:(5.12281639331e-09) A[1]:(4.33428900409e-13) A[2]:(1.0) A[3]:(7.52671571899e-31)\n",
      " state (14)  A[0]:(4.99590679937e-09) A[1]:(4.29696805481e-13) A[2]:(1.0) A[3]:(7.42422859832e-31)\n",
      " state (15)  A[0]:(4.92934670859e-09) A[1]:(4.27734291129e-13) A[2]:(1.0) A[3]:(7.37089406864e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 794000 finished after 3 . Running score: 0.15. Policy_loss: -92050.6111943, Value_loss: 0.989206936141. Times trained:               15479. Times reached goal: 109.               Steps done: 10349577.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9932,  0.0013,  0.0026,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.0551e-12,  2.8479e-10,  1.1396e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.0552e-12,  2.8479e-10,  1.1397e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9932,  0.0013,  0.0026,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9932,  0.0013,  0.0026,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9932,  0.0013,  0.0026,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9932,  0.0013,  0.0026,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9932,  0.0013,  0.0026,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.0561e-12,  2.8477e-10,  1.1408e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.3565e-11,  6.5172e-08,  8.8938e-19]])\n",
      "On state=8, selected action=0\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.993219673634) A[1]:(0.0013442983618) A[2]:(0.0026241785381) A[3]:(0.00281182304025)\n",
      " state (1)  A[0]:(1.1102816643e-05) A[1]:(9.28708459469e-07) A[2]:(8.85629469849e-06) A[3]:(0.999979138374)\n",
      " state (2)  A[0]:(9.45450574363e-06) A[1]:(8.98793643955e-07) A[2]:(8.46556213219e-06) A[3]:(0.999981164932)\n",
      " state (3)  A[0]:(0.999792635441) A[1]:(4.65935187322e-07) A[2]:(6.57566988593e-06) A[3]:(0.000200319715077)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.0563832475e-12) A[2]:(2.84759577029e-10) A[3]:(1.14113829063e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(8.10967092091e-12) A[2]:(6.27951246557e-10) A[3]:(2.47099687039e-14)\n",
      " state (6)  A[0]:(1.0) A[1]:(6.92767024302e-12) A[2]:(2.01383709708e-09) A[3]:(7.96235719663e-17)\n",
      " state (7)  A[0]:(1.0) A[1]:(6.90849764548e-12) A[2]:(6.01233329789e-09) A[3]:(1.41857070316e-18)\n",
      " state (8)  A[0]:(0.999999940395) A[1]:(4.35653839392e-11) A[2]:(6.51726068668e-08) A[3]:(8.89406721698e-19)\n",
      " state (9)  A[0]:(0.99364054203) A[1]:(2.18111906314e-09) A[2]:(0.00635945005342) A[3]:(1.04697518367e-21)\n",
      " state (10)  A[0]:(0.0119018247351) A[1]:(5.97065952235e-10) A[2]:(0.988098144531) A[3]:(4.03962678011e-25)\n",
      " state (11)  A[0]:(1.21318989841e-05) A[1]:(1.05787722332e-11) A[2]:(0.999987840652) A[3]:(2.31810055895e-28)\n",
      " state (12)  A[0]:(4.08418998177e-07) A[1]:(1.77011768958e-12) A[2]:(0.999999582767) A[3]:(9.5410847802e-30)\n",
      " state (13)  A[0]:(1.05189222666e-07) A[1]:(9.26380825349e-13) A[2]:(0.999999880791) A[3]:(3.12752948567e-30)\n",
      " state (14)  A[0]:(5.7522303365e-08) A[1]:(7.08479932707e-13) A[2]:(0.999999940395) A[3]:(1.99362901504e-30)\n",
      " state (15)  A[0]:(4.2377088505e-08) A[1]:(6.22900711046e-13) A[2]:(0.999999940395) A[3]:(1.61288940407e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 795000 finished after 10 . Running score: 0.0. Policy_loss: -92050.6111886, Value_loss: 0.978819714489. Times trained:               16431. Times reached goal: 89.               Steps done: 10366008.\n",
      " state (0)  A[0]:(0.995696485043) A[1]:(0.000961228564847) A[2]:(0.00168953323737) A[3]:(0.00165276415646)\n",
      " state (1)  A[0]:(1.59665250976e-05) A[1]:(1.09366521883e-06) A[2]:(1.0463376384e-05) A[3]:(0.999972462654)\n",
      " state (2)  A[0]:(1.12308371172e-05) A[1]:(8.67152323281e-07) A[2]:(8.42968165671e-06) A[3]:(0.999979496002)\n",
      " state (3)  A[0]:(1.72841628228e-05) A[1]:(1.29235093027e-06) A[2]:(1.27001103465e-05) A[3]:(0.999968707561)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(3.44644690564e-09) A[2]:(6.63771970721e-08) A[3]:(5.20724405817e-08)\n",
      " state (5)  A[0]:(1.0) A[1]:(7.49178410281e-12) A[2]:(4.08961864373e-10) A[3]:(9.4805050312e-14)\n",
      " state (6)  A[0]:(1.0) A[1]:(5.64502958267e-12) A[2]:(1.89934890038e-09) A[3]:(3.24331694831e-17)\n",
      " state (7)  A[0]:(1.0) A[1]:(1.51361857076e-11) A[2]:(1.39800278021e-08) A[3]:(1.12811770819e-18)\n",
      " state (8)  A[0]:(0.999995410442) A[1]:(2.8327612478e-10) A[2]:(4.59506327388e-06) A[3]:(1.01435866048e-19)\n",
      " state (9)  A[0]:(0.329975098372) A[1]:(4.07108213807e-09) A[2]:(0.670024931431) A[3]:(1.74928564669e-23)\n",
      " state (10)  A[0]:(0.000292362907203) A[1]:(6.21684440039e-11) A[2]:(0.999707639217) A[3]:(4.66841759253e-27)\n",
      " state (11)  A[0]:(2.5471929348e-06) A[1]:(4.43688705568e-12) A[2]:(0.999997437) A[3]:(3.81021797318e-29)\n",
      " state (12)  A[0]:(3.54060432528e-07) A[1]:(1.64886160281e-12) A[2]:(0.999999642372) A[3]:(6.69830911494e-30)\n",
      " state (13)  A[0]:(1.49549649109e-07) A[1]:(1.10621668076e-12) A[2]:(0.999999821186) A[3]:(3.39096604517e-30)\n",
      " state (14)  A[0]:(9.66217328369e-08) A[1]:(9.14157204795e-13) A[2]:(0.999999880791) A[3]:(2.46687946718e-30)\n",
      " state (15)  A[0]:(7.50349613554e-08) A[1]:(8.2260701072e-13) A[2]:(0.999999940395) A[3]:(2.07477085101e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 796000 finished after 7 . Running score: 0.0. Policy_loss: -92050.6111811, Value_loss: 0.979020758828. Times trained:               18829. Times reached goal: 0.               Steps done: 10384837.\n",
      " state (0)  A[0]:(0.997202396393) A[1]:(0.000731842243113) A[2]:(0.00113811821211) A[3]:(0.000927623128518)\n",
      " state (1)  A[0]:(1.44316391015e-05) A[1]:(9.5386258181e-07) A[2]:(9.3738999567e-06) A[3]:(0.999975264072)\n",
      " state (2)  A[0]:(1.23286272355e-05) A[1]:(9.21088712857e-07) A[2]:(9.27562905417e-06) A[3]:(0.999977469444)\n",
      " state (3)  A[0]:(0.000441360025434) A[1]:(5.16086265634e-06) A[2]:(6.14455639152e-05) A[3]:(0.999492049217)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.08069915863e-11) A[2]:(3.64726804314e-10) A[3]:(2.32291009344e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(6.36014721792e-12) A[2]:(7.39020233897e-10) A[3]:(6.54888859477e-15)\n",
      " state (6)  A[0]:(1.0) A[1]:(5.2977375091e-12) A[2]:(3.90922672011e-09) A[3]:(3.68641890817e-18)\n",
      " state (7)  A[0]:(0.999999940395) A[1]:(5.28036815661e-11) A[2]:(6.56382965758e-08) A[3]:(1.19725232642e-18)\n",
      " state (8)  A[0]:(0.999458789825) A[1]:(9.6628682833e-10) A[2]:(0.000541214016266) A[3]:(2.65922855351e-21)\n",
      " state (9)  A[0]:(0.230232179165) A[1]:(2.67467958892e-09) A[2]:(0.769767820835) A[3]:(6.3291347778e-24)\n",
      " state (10)  A[0]:(0.00131863716524) A[1]:(1.22718446516e-10) A[2]:(0.998681366444) A[3]:(1.56536442762e-26)\n",
      " state (11)  A[0]:(1.78775699169e-05) A[1]:(1.06569648939e-11) A[2]:(0.999982118607) A[3]:(1.73488274947e-28)\n",
      " state (12)  A[0]:(2.22321091314e-06) A[1]:(3.60589206064e-12) A[2]:(0.999997794628) A[3]:(2.50046193019e-29)\n",
      " state (13)  A[0]:(8.43740338041e-07) A[1]:(2.255376658e-12) A[2]:(0.999999165535) A[3]:(1.10527426797e-29)\n",
      " state (14)  A[0]:(5.12016185894e-07) A[1]:(1.7931363859e-12) A[2]:(0.999999463558) A[3]:(7.48288557835e-30)\n",
      " state (15)  A[0]:(3.82576558877e-07) A[1]:(1.57719464659e-12) A[2]:(0.999999642372) A[3]:(6.04122319532e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 797000 finished after 3 . Running score: 0.0. Policy_loss: -92050.6114764, Value_loss: 0.978690394224. Times trained:               16201. Times reached goal: 57.               Steps done: 10401038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.992216467857) A[1]:(0.00107597140595) A[2]:(0.00454366393387) A[3]:(0.00216390891001)\n",
      " state (1)  A[0]:(1.11962690426e-05) A[1]:(9.40633924529e-07) A[2]:(1.16199826152e-05) A[3]:(0.999976217747)\n",
      " state (2)  A[0]:(0.999999761581) A[1]:(4.22295665103e-09) A[2]:(1.27199470512e-07) A[3]:(7.95340611148e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.56895587872e-12) A[2]:(9.19017140166e-10) A[3]:(4.26282713826e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.1256117366e-12) A[2]:(3.29584448622e-09) A[3]:(7.02780126138e-17)\n",
      " state (5)  A[0]:(1.0) A[1]:(6.06943878836e-12) A[2]:(1.17210108286e-08) A[3]:(7.01461618853e-19)\n",
      " state (6)  A[0]:(0.999864518642) A[1]:(5.65678337505e-10) A[2]:(0.000135495967697) A[3]:(1.02335045301e-20)\n",
      " state (7)  A[0]:(0.0550342462957) A[1]:(9.42265265813e-10) A[2]:(0.944965779781) A[3]:(1.6448373074e-24)\n",
      " state (8)  A[0]:(4.91015998705e-05) A[1]:(1.48667224364e-11) A[2]:(0.999950885773) A[3]:(7.82303980429e-28)\n",
      " state (9)  A[0]:(8.55204973504e-07) A[1]:(1.58357214061e-12) A[2]:(0.999999165535) A[3]:(1.33031955813e-29)\n",
      " state (10)  A[0]:(2.18737767455e-07) A[1]:(7.97436281504e-13) A[2]:(0.999999761581) A[3]:(3.9520594034e-30)\n",
      " state (11)  A[0]:(1.32604895953e-07) A[1]:(6.30154294631e-13) A[2]:(0.999999880791) A[3]:(2.62860529652e-30)\n",
      " state (12)  A[0]:(1.07428917318e-07) A[1]:(5.73462230393e-13) A[2]:(0.999999880791) A[3]:(2.23937767594e-30)\n",
      " state (13)  A[0]:(9.71239373371e-08) A[1]:(5.49072939893e-13) A[2]:(0.999999880791) A[3]:(2.08266866841e-30)\n",
      " state (14)  A[0]:(9.20070277743e-08) A[1]:(5.36756891104e-13) A[2]:(0.999999880791) A[3]:(2.00643118296e-30)\n",
      " state (15)  A[0]:(8.9144982951e-08) A[1]:(5.29819894554e-13) A[2]:(0.999999940395) A[3]:(1.96445832339e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 798000 finished after 11 . Running score: 0.13. Policy_loss: -92050.6114325, Value_loss: 1.62628887793. Times trained:               15796. Times reached goal: 130.               Steps done: 10416834.\n",
      " state (0)  A[0]:(0.995394706726) A[1]:(0.000855947786476) A[2]:(0.00269678398035) A[3]:(0.00105257262476)\n",
      " state (1)  A[0]:(2.07870543818e-05) A[1]:(1.55360282861e-06) A[2]:(2.40914323513e-05) A[3]:(0.999953567982)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.65968104915e-12) A[2]:(1.1710056258e-09) A[3]:(5.83921000202e-14)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.52326716949e-12) A[2]:(5.43729816727e-09) A[3]:(3.67657595519e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.17255638563e-12) A[2]:(2.01014653811e-08) A[3]:(5.14880795332e-19)\n",
      " state (5)  A[0]:(0.982719957829) A[1]:(2.1411119544e-09) A[2]:(0.0172800496221) A[3]:(5.76846244437e-22)\n",
      " state (6)  A[0]:(0.00798674859107) A[1]:(2.78436412815e-10) A[2]:(0.992013275623) A[3]:(2.17230673579e-25)\n",
      " state (7)  A[0]:(2.17089063881e-06) A[1]:(2.27913196128e-12) A[2]:(0.999997854233) A[3]:(3.27323229258e-29)\n",
      " state (8)  A[0]:(9.77539684754e-08) A[1]:(4.39548706677e-13) A[2]:(0.999999880791) A[3]:(1.71661051169e-30)\n",
      " state (9)  A[0]:(4.40542748947e-08) A[1]:(2.98990758367e-13) A[2]:(0.999999940395) A[3]:(8.76899413567e-31)\n",
      " state (10)  A[0]:(3.48854456433e-08) A[1]:(2.68638979334e-13) A[2]:(0.999999940395) A[3]:(7.2960334162e-31)\n",
      " state (11)  A[0]:(3.23564499638e-08) A[1]:(2.59832574293e-13) A[2]:(0.999999940395) A[3]:(6.89468767139e-31)\n",
      " state (12)  A[0]:(3.15106056803e-08) A[1]:(2.56887257776e-13) A[2]:(0.999999940395) A[3]:(6.76422190438e-31)\n",
      " state (13)  A[0]:(3.11897423444e-08) A[1]:(2.55786684151e-13) A[2]:(0.999999940395) A[3]:(6.71634590026e-31)\n",
      " state (14)  A[0]:(3.10553502914e-08) A[1]:(2.55333352118e-13) A[2]:(0.999999940395) A[3]:(6.69695353485e-31)\n",
      " state (15)  A[0]:(3.09942791432e-08) A[1]:(2.55130362366e-13) A[2]:(0.999999940395) A[3]:(6.68832399573e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 799000 finished after 15 . Running score: 0.08. Policy_loss: -92050.61118, Value_loss: 0.98679678692. Times trained:               15847. Times reached goal: 122.               Steps done: 10432681.\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0010,  0.0022,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0010,  0.0022,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6385e-11,  4.6226e-08,  9.5882e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6385e-11,  4.6226e-08,  9.5883e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0010,  0.0022,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0010,  0.0022,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6385e-11,  4.6226e-08,  9.5887e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.8805e-08,  3.9219e-13,  1.0000e+00,  2.0240e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994788587093) A[1]:(0.000952849513851) A[2]:(0.00224231765606) A[3]:(0.0020162591245)\n",
      " state (1)  A[0]:(9.73111582425e-06) A[1]:(8.02611168638e-07) A[2]:(1.02735402834e-05) A[3]:(0.999979197979)\n",
      " state (2)  A[0]:(1.0) A[1]:(9.51079666411e-12) A[2]:(9.1939583724e-10) A[3]:(3.96696483171e-13)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.9126592431e-12) A[2]:(5.32732302716e-09) A[3]:(9.94225502093e-17)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(1.63851709978e-11) A[2]:(4.62250540068e-08) A[3]:(9.58897647971e-19)\n",
      " state (5)  A[0]:(0.378100216389) A[1]:(3.37743899514e-09) A[2]:(0.621899783611) A[3]:(4.55570044219e-23)\n",
      " state (6)  A[0]:(0.000322390405927) A[1]:(5.14277231911e-11) A[2]:(0.999677598476) A[3]:(1.46749991528e-26)\n",
      " state (7)  A[0]:(2.98882127936e-07) A[1]:(9.76120985256e-13) A[2]:(0.999999701977) A[3]:(1.02166633889e-29)\n",
      " state (8)  A[0]:(4.88045799329e-08) A[1]:(3.92195228117e-13) A[2]:(0.999999940395) A[3]:(2.02401996387e-30)\n",
      " state (9)  A[0]:(3.07570680036e-08) A[1]:(3.16547379671e-13) A[2]:(0.999999940395) A[3]:(1.39429616731e-30)\n",
      " state (10)  A[0]:(2.69219331273e-08) A[1]:(2.98412499138e-13) A[2]:(1.0) A[3]:(1.2599012399e-30)\n",
      " state (11)  A[0]:(2.58187675684e-08) A[1]:(2.93111590772e-13) A[2]:(1.0) A[3]:(1.22212611768e-30)\n",
      " state (12)  A[0]:(2.54508911723e-08) A[1]:(2.91364724231e-13) A[2]:(1.0) A[3]:(1.20993548895e-30)\n",
      " state (13)  A[0]:(2.53134970762e-08) A[1]:(2.90724096273e-13) A[2]:(1.0) A[3]:(1.20553105267e-30)\n",
      " state (14)  A[0]:(2.5257358871e-08) A[1]:(2.90467465618e-13) A[2]:(1.0) A[3]:(1.20378473826e-30)\n",
      " state (15)  A[0]:(2.52327065908e-08) A[1]:(2.90355033853e-13) A[2]:(1.0) A[3]:(1.20303195168e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 800000 finished after 8 . Running score: 0.09. Policy_loss: -92050.6111868, Value_loss: 0.984310104789. Times trained:               15998. Times reached goal: 109.               Steps done: 10448679.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996455609798) A[1]:(0.000972302455921) A[2]:(0.00136533612385) A[3]:(0.00120674492791)\n",
      " state (1)  A[0]:(8.74150464369e-06) A[1]:(7.63030641338e-07) A[2]:(8.2548976934e-06) A[3]:(0.999982237816)\n",
      " state (2)  A[0]:(1.0) A[1]:(9.2469547644e-12) A[2]:(6.09052752676e-10) A[3]:(1.55694229211e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.59453697846e-12) A[2]:(3.00427949185e-09) A[3]:(6.17866118732e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.85799160516e-12) A[2]:(1.53365533606e-08) A[3]:(1.20245198375e-18)\n",
      " state (5)  A[0]:(0.700786828995) A[1]:(3.97474808622e-09) A[2]:(0.299213171005) A[3]:(1.44200832601e-22)\n",
      " state (6)  A[0]:(4.27938066423e-05) A[1]:(1.64757270327e-11) A[2]:(0.999957203865) A[3]:(1.99618729098e-27)\n",
      " state (7)  A[0]:(5.89012358887e-08) A[1]:(4.46363351012e-13) A[2]:(0.999999940395) A[3]:(2.80142553558e-30)\n",
      " state (8)  A[0]:(1.67880358504e-08) A[1]:(2.44966969886e-13) A[2]:(1.0) A[3]:(9.77588027878e-31)\n",
      " state (9)  A[0]:(1.26305765846e-08) A[1]:(2.1585751036e-13) A[2]:(1.0) A[3]:(7.86389498878e-31)\n",
      " state (10)  A[0]:(1.17160041668e-08) A[1]:(2.0903398912e-13) A[2]:(1.0) A[3]:(7.4463518722e-31)\n",
      " state (11)  A[0]:(1.14608118551e-08) A[1]:(2.07138044781e-13) A[2]:(1.0) A[3]:(7.33348043444e-31)\n",
      " state (12)  A[0]:(1.13792602008e-08) A[1]:(2.06545867107e-13) A[2]:(1.0) A[3]:(7.29892889389e-31)\n",
      " state (13)  A[0]:(1.13500844279e-08) A[1]:(2.0633915041e-13) A[2]:(1.0) A[3]:(7.28702160631e-31)\n",
      " state (14)  A[0]:(1.13387024214e-08) A[1]:(2.06259664838e-13) A[2]:(1.0) A[3]:(7.28251993315e-31)\n",
      " state (15)  A[0]:(1.13339027052e-08) A[1]:(2.06227409824e-13) A[2]:(1.0) A[3]:(7.28068663216e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 801000 finished after 12 . Running score: 0.06. Policy_loss: -92050.6114396, Value_loss: 0.979248030829. Times trained:               16244. Times reached goal: 97.               Steps done: 10464923.\n",
      " state (0)  A[0]:(0.995314717293) A[1]:(0.000873975688592) A[2]:(0.00119181117043) A[3]:(0.00261951144785)\n",
      " state (1)  A[0]:(5.9382591644e-06) A[1]:(4.8484804438e-07) A[2]:(5.66186463402e-06) A[3]:(0.999987900257)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.9708869713e-12) A[2]:(7.22068849157e-10) A[3]:(1.21788820348e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.92485002032e-12) A[2]:(3.72156860884e-09) A[3]:(4.39337725593e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.0612430913e-12) A[2]:(1.47237786408e-08) A[3]:(2.16742310197e-18)\n",
      " state (5)  A[0]:(0.980909705162) A[1]:(1.59387703036e-09) A[2]:(0.0190902911127) A[3]:(1.73173431828e-21)\n",
      " state (6)  A[0]:(0.0189776811749) A[1]:(3.37012695528e-10) A[2]:(0.981022298336) A[3]:(1.62970024992e-24)\n",
      " state (7)  A[0]:(4.12413719459e-05) A[1]:(8.65750526824e-12) A[2]:(0.999958753586) A[3]:(2.06887978339e-27)\n",
      " state (8)  A[0]:(1.66909001109e-06) A[1]:(1.37194140597e-12) A[2]:(0.99999833107) A[3]:(7.16108179348e-29)\n",
      " state (9)  A[0]:(5.90576007653e-07) A[1]:(7.84923178971e-13) A[2]:(0.999999403954) A[3]:(2.62558899682e-29)\n",
      " state (10)  A[0]:(3.8984251205e-07) A[1]:(6.35144931441e-13) A[2]:(0.999999582767) A[3]:(1.80152690704e-29)\n",
      " state (11)  A[0]:(3.21742987808e-07) A[1]:(5.77915753447e-13) A[2]:(0.999999701977) A[3]:(1.52451002117e-29)\n",
      " state (12)  A[0]:(2.94403349699e-07) A[1]:(5.53723462481e-13) A[2]:(0.999999701977) A[3]:(1.41392688535e-29)\n",
      " state (13)  A[0]:(2.82874452751e-07) A[1]:(5.43308724833e-13) A[2]:(0.999999701977) A[3]:(1.36759442757e-29)\n",
      " state (14)  A[0]:(2.77887210132e-07) A[1]:(5.38775350289e-13) A[2]:(0.999999701977) A[3]:(1.34771762683e-29)\n",
      " state (15)  A[0]:(2.75682594975e-07) A[1]:(5.36773154137e-13) A[2]:(0.999999701977) A[3]:(1.33901618507e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 802000 finished after 4 . Running score: 0.08. Policy_loss: -92050.6128609, Value_loss: 0.987842031474. Times trained:               15597. Times reached goal: 112.               Steps done: 10480520.\n",
      " state (0)  A[0]:(0.996709108353) A[1]:(0.00110075355042) A[2]:(0.000769103760831) A[3]:(0.00142101268284)\n",
      " state (1)  A[0]:(5.35886056241e-06) A[1]:(4.84917507038e-07) A[2]:(4.08678715758e-06) A[3]:(0.999990046024)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.76399190394e-11) A[2]:(5.99506222443e-10) A[3]:(5.11380070234e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.59323333377e-12) A[2]:(6.04243932667e-10) A[3]:(6.10237880193e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.6229786452e-12) A[2]:(2.08364481225e-09) A[3]:(1.15994068226e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(4.38595123761e-12) A[2]:(9.0378637907e-09) A[3]:(2.78615999042e-18)\n",
      " state (6)  A[0]:(0.999903082848) A[1]:(3.36478678253e-10) A[2]:(9.69089232967e-05) A[3]:(3.30342978676e-20)\n",
      " state (7)  A[0]:(0.102829299867) A[1]:(8.59718241042e-10) A[2]:(0.897170722485) A[3]:(7.5807710723e-24)\n",
      " state (8)  A[0]:(0.00152601231821) A[1]:(7.01265573388e-11) A[2]:(0.998474001884) A[3]:(7.91473637191e-26)\n",
      " state (9)  A[0]:(2.87397942884e-05) A[1]:(6.52312969265e-12) A[2]:(0.999971270561) A[3]:(1.00571878733e-27)\n",
      " state (10)  A[0]:(5.09242272528e-06) A[1]:(2.44297225678e-12) A[2]:(0.999994933605) A[3]:(1.68792857743e-28)\n",
      " state (11)  A[0]:(2.42080523094e-06) A[1]:(1.63504084404e-12) A[2]:(0.99999755621) A[3]:(8.21735867333e-29)\n",
      " state (12)  A[0]:(1.64987750395e-06) A[1]:(1.3401961832e-12) A[2]:(0.99999833107) A[3]:(5.7748052436e-29)\n",
      " state (13)  A[0]:(1.32598734126e-06) A[1]:(1.20036641217e-12) A[2]:(0.999998688698) A[3]:(4.75639184553e-29)\n",
      " state (14)  A[0]:(1.16988974241e-06) A[1]:(1.12824896994e-12) A[2]:(0.999998807907) A[3]:(4.26754057091e-29)\n",
      " state (15)  A[0]:(1.09024097128e-06) A[1]:(1.09009210079e-12) A[2]:(0.999998927116) A[3]:(4.01955000718e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 803000 finished after 16 . Running score: 0.08. Policy_loss: -92050.6111936, Value_loss: 1.41062136658. Times trained:               16005. Times reached goal: 104.               Steps done: 10496525.\n",
      " state (0)  A[0]:(0.996229946613) A[1]:(0.000899344740901) A[2]:(0.000903498614207) A[3]:(0.00196718401276)\n",
      " state (1)  A[0]:(5.01535487274e-06) A[1]:(4.44608986072e-07) A[2]:(3.73675788978e-06) A[3]:(0.999990820885)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.00713320964e-10) A[2]:(5.75905634292e-09) A[3]:(2.82594925238e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.5553686571e-12) A[2]:(5.74953751276e-10) A[3]:(7.72202018041e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.35148184447e-12) A[2]:(2.11766870706e-09) A[3]:(1.04557514075e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(4.07076542613e-12) A[2]:(8.44820746693e-09) A[3]:(3.16867953174e-18)\n",
      " state (6)  A[0]:(0.999871909618) A[1]:(3.13314346689e-10) A[2]:(0.000128101601149) A[3]:(2.36471097719e-20)\n",
      " state (7)  A[0]:(0.100700229406) A[1]:(8.21978873855e-10) A[2]:(0.899299800396) A[3]:(8.26551180437e-24)\n",
      " state (8)  A[0]:(0.000845579721499) A[1]:(4.81350515003e-11) A[2]:(0.999154448509) A[3]:(4.69488503145e-26)\n",
      " state (9)  A[0]:(1.49464322021e-05) A[1]:(4.40491610201e-12) A[2]:(0.999985039234) A[3]:(5.86958735803e-28)\n",
      " state (10)  A[0]:(2.94871392725e-06) A[1]:(1.77731917725e-12) A[2]:(0.999997079372) A[3]:(1.14067522262e-28)\n",
      " state (11)  A[0]:(1.52665006681e-06) A[1]:(1.25208665051e-12) A[2]:(0.999998450279) A[3]:(6.1187528594e-29)\n",
      " state (12)  A[0]:(1.13632961529e-06) A[1]:(1.07617213752e-12) A[2]:(0.999998867512) A[3]:(4.68922985354e-29)\n",
      " state (13)  A[0]:(9.88741589936e-07) A[1]:(1.00400254516e-12) A[2]:(0.999998986721) A[3]:(4.15560100864e-29)\n",
      " state (14)  A[0]:(9.24227890664e-07) A[1]:(9.71443520244e-13) A[2]:(0.999999046326) A[3]:(3.92598833048e-29)\n",
      " state (15)  A[0]:(8.93516130418e-07) A[1]:(9.55802928124e-13) A[2]:(0.99999910593) A[3]:(3.81863488889e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 804000 finished after 25 . Running score: 0.1. Policy_loss: -92050.6112019, Value_loss: 1.20831643911. Times trained:               15284. Times reached goal: 94.               Steps done: 10511809.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9960,  0.0019,  0.0009,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0019,  0.0009,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0019,  0.0009,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1885e-11,  1.7822e-09,  1.4143e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1885e-11,  1.7809e-09,  1.4178e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1885e-11,  1.7798e-09,  1.4209e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0019,  0.0009,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0019,  0.0009,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0019,  0.0009,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0019,  0.0009,  0.0012]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1886e-11,  1.7756e-09,  1.4325e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0012e-01,  2.7222e-09,  8.9988e-01,  5.7917e-24]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995985209942) A[1]:(0.00189745682292) A[2]:(0.000910017406568) A[3]:(0.00120733981021)\n",
      " state (1)  A[0]:(5.23879270986e-06) A[1]:(6.28341922493e-07) A[2]:(4.06888602811e-06) A[3]:(0.999990046024)\n",
      " state (2)  A[0]:(0.999999761581) A[1]:(3.68344066359e-09) A[2]:(6.92891219956e-08) A[3]:(1.48544700096e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.23877722316e-11) A[2]:(5.54424062216e-10) A[3]:(6.33124087011e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.18849713057e-11) A[2]:(1.77124914735e-09) A[3]:(1.44407619035e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.15356448352e-11) A[2]:(5.86244786049e-09) A[3]:(5.95368457123e-18)\n",
      " state (6)  A[0]:(0.999999880791) A[1]:(6.6093602813e-11) A[2]:(9.94260247467e-08) A[3]:(9.5786305182e-19)\n",
      " state (7)  A[0]:(0.517508983612) A[1]:(6.74907285614e-09) A[2]:(0.48249104619) A[3]:(5.40259342659e-23)\n",
      " state (8)  A[0]:(0.100785240531) A[1]:(2.73261724359e-09) A[2]:(0.899214744568) A[3]:(5.82773241986e-24)\n",
      " state (9)  A[0]:(0.00513119157404) A[1]:(4.53479642726e-10) A[2]:(0.994868814945) A[3]:(2.08293988858e-25)\n",
      " state (10)  A[0]:(0.000309040246066) A[1]:(8.19575865507e-11) A[2]:(0.999690949917) A[3]:(8.92393814576e-27)\n",
      " state (11)  A[0]:(7.13591070962e-05) A[1]:(3.47694546765e-11) A[2]:(0.99992865324) A[3]:(1.87193187616e-27)\n",
      " state (12)  A[0]:(3.42491475749e-05) A[1]:(2.30118840649e-11) A[2]:(0.999965727329) A[3]:(8.91580378805e-28)\n",
      " state (13)  A[0]:(2.28396693274e-05) A[1]:(1.84645597423e-11) A[2]:(0.999977171421) A[3]:(6.03353984827e-28)\n",
      " state (14)  A[0]:(1.79635680979e-05) A[1]:(1.62659625352e-11) A[2]:(0.999982059002) A[3]:(4.83241437915e-28)\n",
      " state (15)  A[0]:(1.54782774189e-05) A[1]:(1.50655009301e-11) A[2]:(0.999984502792) A[3]:(4.23273757237e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 805000 finished after 12 . Running score: 0.13. Policy_loss: -92050.615965, Value_loss: 0.994028378635. Times trained:               15777. Times reached goal: 102.               Steps done: 10527586.\n",
      " state (0)  A[0]:(0.993960976601) A[1]:(0.00321736512706) A[2]:(0.000859603227582) A[3]:(0.00196205987595)\n",
      " state (1)  A[0]:(3.83025690098e-06) A[1]:(6.08809955338e-07) A[2]:(2.96118196275e-06) A[3]:(0.999992609024)\n",
      " state (2)  A[0]:(9.8109985629e-06) A[1]:(8.52031348586e-07) A[2]:(6.77305934005e-06) A[3]:(0.999982535839)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.49109829986e-11) A[2]:(4.10164097131e-10) A[3]:(1.3474271178e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.6339961462e-11) A[2]:(1.01756969517e-09) A[3]:(5.17799202766e-14)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.25116183975e-11) A[2]:(5.39111733033e-09) A[3]:(2.11573031074e-17)\n",
      " state (6)  A[0]:(0.999999880791) A[1]:(1.38001512995e-10) A[2]:(9.80952066243e-08) A[3]:(8.5954700085e-19)\n",
      " state (7)  A[0]:(0.338827043772) A[1]:(2.17073203856e-08) A[2]:(0.661172926426) A[3]:(4.08251611214e-23)\n",
      " state (8)  A[0]:(0.0457532815635) A[1]:(7.17878112511e-09) A[2]:(0.95424669981) A[3]:(4.08928532757e-24)\n",
      " state (9)  A[0]:(0.00110038614366) A[1]:(7.51478657079e-10) A[2]:(0.998899638653) A[3]:(6.36864541858e-26)\n",
      " state (10)  A[0]:(7.91939819464e-05) A[1]:(1.56035212528e-10) A[2]:(0.999920785427) A[3]:(3.52546445219e-27)\n",
      " state (11)  A[0]:(2.39205910475e-05) A[1]:(7.87031481653e-11) A[2]:(0.999976098537) A[3]:(1.01627923899e-27)\n",
      " state (12)  A[0]:(1.40255233418e-05) A[1]:(5.8671588421e-11) A[2]:(0.999985992908) A[3]:(6.00305863652e-28)\n",
      " state (13)  A[0]:(1.08988979264e-05) A[1]:(5.13069552122e-11) A[2]:(0.99998909235) A[3]:(4.73695614294e-28)\n",
      " state (14)  A[0]:(9.58236705628e-06) A[1]:(4.80168960149e-11) A[2]:(0.999990403652) A[3]:(4.22186136449e-28)\n",
      " state (15)  A[0]:(8.92021034815e-06) A[1]:(4.63291696673e-11) A[2]:(0.999991059303) A[3]:(3.97161589791e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 806000 finished after 5 . Running score: 0.09. Policy_loss: -92050.6172075, Value_loss: 1.22724987646. Times trained:               15561. Times reached goal: 105.               Steps done: 10543147.\n",
      " state (0)  A[0]:(0.995697200298) A[1]:(0.00251726456918) A[2]:(0.000553814054001) A[3]:(0.00123174046166)\n",
      " state (1)  A[0]:(4.23392566518e-06) A[1]:(7.30540648419e-07) A[2]:(3.26307758769e-06) A[3]:(0.999991774559)\n",
      " state (2)  A[0]:(0.999999344349) A[1]:(1.83720754166e-08) A[2]:(1.36446971055e-07) A[3]:(4.81502240746e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.96275624762e-11) A[2]:(6.59065801933e-10) A[3]:(4.00318287633e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.23090329332e-11) A[2]:(2.69102651274e-09) A[3]:(3.57936032947e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(7.39476743727e-11) A[2]:(7.87692311377e-09) A[3]:(3.48808829424e-18)\n",
      " state (6)  A[0]:(0.999998867512) A[1]:(9.57680046376e-10) A[2]:(1.13287148906e-06) A[3]:(1.94705430272e-19)\n",
      " state (7)  A[0]:(0.243198409677) A[1]:(7.04548099861e-08) A[2]:(0.756801486015) A[3]:(1.90342575436e-23)\n",
      " state (8)  A[0]:(0.0385204069316) A[1]:(2.58736321257e-08) A[2]:(0.96147954464) A[3]:(2.55138166575e-24)\n",
      " state (9)  A[0]:(0.00082757108612) A[1]:(2.51018361652e-09) A[2]:(0.999172449112) A[3]:(3.36978698959e-26)\n",
      " state (10)  A[0]:(6.6412409069e-05) A[1]:(5.62951130156e-10) A[2]:(0.999933600426) A[3]:(2.16191857011e-27)\n",
      " state (11)  A[0]:(2.27967575483e-05) A[1]:(3.06863756627e-10) A[2]:(0.999977231026) A[3]:(7.20573688665e-28)\n",
      " state (12)  A[0]:(1.47070250023e-05) A[1]:(2.41387299038e-10) A[2]:(0.999985277653) A[3]:(4.69566613086e-28)\n",
      " state (13)  A[0]:(1.2110028365e-05) A[1]:(2.17829712601e-10) A[2]:(0.999987900257) A[3]:(3.92174057162e-28)\n",
      " state (14)  A[0]:(1.09904140118e-05) A[1]:(2.07326600332e-10) A[2]:(0.999989032745) A[3]:(3.60231160883e-28)\n",
      " state (15)  A[0]:(1.04069104054e-05) A[1]:(2.01822947243e-10) A[2]:(0.999989569187) A[3]:(3.44252177543e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 807000 finished after 28 . Running score: 0.12. Policy_loss: -92050.6118221, Value_loss: 1.42616768085. Times trained:               15101. Times reached goal: 100.               Steps done: 10558248.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996727526188) A[1]:(0.00146665109787) A[2]:(0.000601171224844) A[3]:(0.00120464991778)\n",
      " state (1)  A[0]:(4.06859953728e-06) A[1]:(7.28330803668e-07) A[2]:(3.11566509481e-06) A[3]:(0.999992072582)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.94341920334e-09) A[2]:(7.65567609307e-09) A[3]:(4.76458206222e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.03649165639e-10) A[2]:(7.03564539961e-10) A[3]:(4.07810748062e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.27575797149e-10) A[2]:(3.17326653843e-09) A[3]:(2.9776334177e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.51727005959e-10) A[2]:(9.90410065072e-09) A[3]:(3.07322867931e-18)\n",
      " state (6)  A[0]:(0.999669790268) A[1]:(2.68329802822e-08) A[2]:(0.000330160255544) A[3]:(1.21338704003e-20)\n",
      " state (7)  A[0]:(0.118020065129) A[1]:(2.02893986057e-07) A[2]:(0.881979703903) A[3]:(1.16324734886e-23)\n",
      " state (8)  A[0]:(0.00154038961045) A[1]:(1.71232574786e-08) A[2]:(0.99845957756) A[3]:(1.12427679677e-25)\n",
      " state (9)  A[0]:(1.87403784366e-05) A[1]:(1.30320088054e-09) A[2]:(0.999981284142) A[3]:(9.44805571341e-28)\n",
      " state (10)  A[0]:(3.58331408279e-06) A[1]:(5.20742893251e-10) A[2]:(0.999996423721) A[3]:(1.76910532088e-28)\n",
      " state (11)  A[0]:(2.06404001801e-06) A[1]:(3.87740423102e-10) A[2]:(0.999997913837) A[3]:(1.03951707636e-28)\n",
      " state (12)  A[0]:(1.7014255036e-06) A[1]:(3.50636658597e-10) A[2]:(0.999998271465) A[3]:(8.68774659924e-29)\n",
      " state (13)  A[0]:(1.58237412506e-06) A[1]:(3.37984418231e-10) A[2]:(0.999998390675) A[3]:(8.14291124948e-29)\n",
      " state (14)  A[0]:(1.53627775035e-06) A[1]:(3.33114064111e-10) A[2]:(0.999998450279) A[3]:(7.94021494692e-29)\n",
      " state (15)  A[0]:(1.5161687088e-06) A[1]:(3.31035976409e-10) A[2]:(0.999998509884) A[3]:(7.85579602078e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 808000 finished after 22 . Running score: 0.07. Policy_loss: -92050.6120587, Value_loss: 1.41908682055. Times trained:               15717. Times reached goal: 107.               Steps done: 10573965.\n",
      " state (0)  A[0]:(0.996982693672) A[1]:(0.00123271928169) A[2]:(0.00048251519911) A[3]:(0.00130206067115)\n",
      " state (1)  A[0]:(3.74922819901e-06) A[1]:(8.30682836295e-07) A[2]:(4.19219804826e-06) A[3]:(0.999991238117)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.98640140395e-10) A[2]:(9.92429138869e-10) A[3]:(1.12576538803e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.94332646933e-10) A[2]:(9.15251341382e-09) A[3]:(8.99191846483e-17)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(1.06836028912e-09) A[2]:(2.90771406952e-08) A[3]:(2.40203964783e-18)\n",
      " state (5)  A[0]:(0.278788030148) A[1]:(2.68349140242e-06) A[2]:(0.721209287643) A[3]:(8.49367861938e-23)\n",
      " state (6)  A[0]:(0.00107585731894) A[1]:(1.4974935425e-07) A[2]:(0.998924016953) A[3]:(1.90583851995e-25)\n",
      " state (7)  A[0]:(4.7845651352e-07) A[1]:(1.73257863612e-09) A[2]:(0.999999523163) A[3]:(5.78345387276e-29)\n",
      " state (8)  A[0]:(7.61398695204e-08) A[1]:(6.47945086474e-10) A[2]:(0.999999940395) A[3]:(1.00744763445e-29)\n",
      " state (9)  A[0]:(5.11951725457e-08) A[1]:(5.28766752605e-10) A[2]:(0.999999940395) A[3]:(7.05429093409e-30)\n",
      " state (10)  A[0]:(4.65268605865e-08) A[1]:(5.04004504798e-10) A[2]:(0.999999940395) A[3]:(6.48870925631e-30)\n",
      " state (11)  A[0]:(4.53995099292e-08) A[1]:(4.97948349221e-10) A[2]:(0.999999940395) A[3]:(6.35423119795e-30)\n",
      " state (12)  A[0]:(4.50977886146e-08) A[1]:(4.96349628065e-10) A[2]:(0.999999940395) A[3]:(6.31918152991e-30)\n",
      " state (13)  A[0]:(4.5010480676e-08) A[1]:(4.95899210584e-10) A[2]:(0.999999940395) A[3]:(6.30940216923e-30)\n",
      " state (14)  A[0]:(4.49830181992e-08) A[1]:(4.95761154351e-10) A[2]:(0.999999940395) A[3]:(6.30646663069e-30)\n",
      " state (15)  A[0]:(4.49732375785e-08) A[1]:(4.9571385885e-10) A[2]:(0.999999940395) A[3]:(6.30540812154e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 809000 finished after 10 . Running score: 0.16. Policy_loss: -92050.6111776, Value_loss: 1.41723876519. Times trained:               15575. Times reached goal: 110.               Steps done: 10589540.\n",
      "action_dist \n",
      "tensor([[ 0.9943,  0.0022,  0.0017,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9943,  0.0022,  0.0017,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9943,  0.0022,  0.0017,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9943,  0.0022,  0.0017,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9943,  0.0022,  0.0017,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0017,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0017,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0017,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0017,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6318e-09,  6.9311e-08,  1.0957e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0017,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6316e-09,  6.9338e-08,  1.0960e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5227e-08,  1.8217e-10,  1.0000e+00,  2.4727e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6313e-09,  6.9352e-08,  1.0964e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0017,  0.0019]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6311e-09,  6.9365e-08,  1.0967e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6310e-09,  6.9373e-08,  1.0968e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6309e-09,  6.9379e-08,  1.0969e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6308e-09,  6.9385e-08,  1.0971e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5212e-08,  1.8190e-10,  1.0000e+00,  2.4724e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3089e-08,  1.6858e-10,  1.0000e+00,  2.1790e-30]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994209527969) A[1]:(0.00222122436389) A[2]:(0.00170739751775) A[3]:(0.00186184467748)\n",
      " state (1)  A[0]:(4.70532586405e-06) A[1]:(1.04302307591e-06) A[2]:(7.00213513483e-06) A[3]:(0.999987244606)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.97877861266e-10) A[2]:(2.14080109195e-09) A[3]:(1.30951917062e-13)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.73231875698e-10) A[2]:(1.87781274974e-08) A[3]:(1.54129043302e-17)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(1.63035729361e-09) A[2]:(6.9384348933e-08) A[3]:(1.09742062207e-18)\n",
      " state (5)  A[0]:(0.0452284999192) A[1]:(9.52944731125e-07) A[2]:(0.954770565033) A[3]:(7.33881954406e-24)\n",
      " state (6)  A[0]:(3.29451245307e-06) A[1]:(3.69180619408e-09) A[2]:(0.999996721745) A[3]:(4.40792401197e-28)\n",
      " state (7)  A[0]:(3.23120445955e-08) A[1]:(2.69176403389e-10) A[2]:(0.999999940395) A[3]:(4.77845525383e-30)\n",
      " state (8)  A[0]:(1.52100660955e-08) A[1]:(1.81854448167e-10) A[2]:(1.0) A[3]:(2.47240034097e-30)\n",
      " state (9)  A[0]:(1.30879591609e-08) A[1]:(1.68557182101e-10) A[2]:(1.0) A[3]:(2.17894127959e-30)\n",
      " state (10)  A[0]:(1.26629968733e-08) A[1]:(1.65811697705e-10) A[2]:(1.0) A[3]:(2.12050773753e-30)\n",
      " state (11)  A[0]:(1.25663692785e-08) A[1]:(1.65190472412e-10) A[2]:(1.0) A[3]:(2.10745993831e-30)\n",
      " state (12)  A[0]:(1.25427108699e-08) A[1]:(1.65040869859e-10) A[2]:(1.0) A[3]:(2.10435907825e-30)\n",
      " state (13)  A[0]:(1.25366357295e-08) A[1]:(1.65003108399e-10) A[2]:(1.0) A[3]:(2.1035885182e-30)\n",
      " state (14)  A[0]:(1.25348185165e-08) A[1]:(1.64992408624e-10) A[2]:(1.0) A[3]:(2.10336376368e-30)\n",
      " state (15)  A[0]:(1.25342927149e-08) A[1]:(1.64989258367e-10) A[2]:(1.0) A[3]:(2.10329962871e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 810000 finished after 21 . Running score: 0.09. Policy_loss: -92050.6111822, Value_loss: 1.20510328715. Times trained:               16097. Times reached goal: 109.               Steps done: 10605637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993818342686) A[1]:(0.00187724793795) A[2]:(0.00216135871597) A[3]:(0.00214306684211)\n",
      " state (1)  A[0]:(3.82760663342e-06) A[1]:(7.46676334984e-07) A[2]:(4.47631282441e-06) A[3]:(0.999990940094)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.17962728474e-09) A[2]:(7.35307326138e-09) A[3]:(6.30220209352e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.47373491083e-10) A[2]:(6.96487090224e-09) A[3]:(1.24087878318e-16)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(6.50148601622e-10) A[2]:(3.10998053976e-08) A[3]:(8.60764403917e-19)\n",
      " state (5)  A[0]:(0.0147171542048) A[1]:(2.77868451803e-07) A[2]:(0.985282540321) A[3]:(2.66107277462e-24)\n",
      " state (6)  A[0]:(4.85294471275e-08) A[1]:(2.30728075534e-10) A[2]:(0.999999940395) A[3]:(1.32517446637e-29)\n",
      " state (7)  A[0]:(5.11982722884e-09) A[1]:(7.24413584674e-11) A[2]:(1.0) A[3]:(1.93025820863e-30)\n",
      " state (8)  A[0]:(3.67889163577e-09) A[1]:(6.1803624718e-11) A[2]:(1.0) A[3]:(1.48916599054e-30)\n",
      " state (9)  A[0]:(3.44849238054e-09) A[1]:(5.99596275408e-11) A[2]:(1.0) A[3]:(1.4171611311e-30)\n",
      " state (10)  A[0]:(3.39616845757e-09) A[1]:(5.95433077843e-11) A[2]:(1.0) A[3]:(1.40087517409e-30)\n",
      " state (11)  A[0]:(3.3809413047e-09) A[1]:(5.94257629216e-11) A[2]:(1.0) A[3]:(1.39616971722e-30)\n",
      " state (12)  A[0]:(3.37532268802e-09) A[1]:(5.93837271023e-11) A[2]:(1.0) A[3]:(1.39444521999e-30)\n",
      " state (13)  A[0]:(3.37283867502e-09) A[1]:(5.9365831695e-11) A[2]:(1.0) A[3]:(1.39369008242e-30)\n",
      " state (14)  A[0]:(3.37160366293e-09) A[1]:(5.93571164442e-11) A[2]:(1.0) A[3]:(1.39330734146e-30)\n",
      " state (15)  A[0]:(3.37093486458e-09) A[1]:(5.93524743242e-11) A[2]:(1.0) A[3]:(1.39310534451e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 811000 finished after 5 . Running score: 0.06. Policy_loss: -92050.6115066, Value_loss: 1.42320457628. Times trained:               15641. Times reached goal: 119.               Steps done: 10621278.\n",
      " state (0)  A[0]:(0.992306470871) A[1]:(0.00191949366126) A[2]:(0.0029772145208) A[3]:(0.0027968341019)\n",
      " state (1)  A[0]:(3.97257326767e-06) A[1]:(7.77940044827e-07) A[2]:(4.20440892412e-06) A[3]:(0.999991059303)\n",
      " state (2)  A[0]:(0.999999821186) A[1]:(1.35191333683e-08) A[2]:(5.12616082915e-08) A[3]:(1.07167977603e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.61792829134e-10) A[2]:(7.93429377755e-10) A[3]:(1.86447442229e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.68353791913e-10) A[2]:(3.79951847762e-09) A[3]:(9.65575605269e-17)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.52980383706e-10) A[2]:(9.08217145934e-09) A[3]:(2.33296448321e-18)\n",
      " state (6)  A[0]:(0.999932289124) A[1]:(4.64469351869e-08) A[2]:(6.76656345604e-05) A[3]:(1.69203300122e-20)\n",
      " state (7)  A[0]:(0.0024608517997) A[1]:(2.05696167654e-07) A[2]:(0.997538924217) A[3]:(9.02931474015e-25)\n",
      " state (8)  A[0]:(5.29594956333e-07) A[1]:(1.73358671862e-09) A[2]:(0.999999463558) A[3]:(2.59393248633e-28)\n",
      " state (9)  A[0]:(3.73605750781e-08) A[1]:(4.04093980499e-10) A[2]:(0.999999940395) A[3]:(2.22958499926e-29)\n",
      " state (10)  A[0]:(1.82598434151e-08) A[1]:(2.77674411242e-10) A[2]:(1.0) A[3]:(1.19832865773e-29)\n",
      " state (11)  A[0]:(1.45550735908e-08) A[1]:(2.47444481571e-10) A[2]:(1.0) A[3]:(9.92108655687e-30)\n",
      " state (12)  A[0]:(1.34247812866e-08) A[1]:(2.37728225994e-10) A[2]:(1.0) A[3]:(9.29344253896e-30)\n",
      " state (13)  A[0]:(1.3002267707e-08) A[1]:(2.34076508177e-10) A[2]:(1.0) A[3]:(9.06127619806e-30)\n",
      " state (14)  A[0]:(1.28207222616e-08) A[1]:(2.32520863674e-10) A[2]:(1.0) A[3]:(8.96206597948e-30)\n",
      " state (15)  A[0]:(1.27331851729e-08) A[1]:(2.31776972615e-10) A[2]:(1.0) A[3]:(8.91405916635e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 812000 finished after 3 . Running score: 0.13. Policy_loss: -92050.6111802, Value_loss: 1.85089391948. Times trained:               15335. Times reached goal: 127.               Steps done: 10636613.\n",
      " state (0)  A[0]:(0.996952652931) A[1]:(0.00108610081952) A[2]:(0.00127823196817) A[3]:(0.000683025165927)\n",
      " state (1)  A[0]:(7.90894409874e-06) A[1]:(1.0921530702e-06) A[2]:(5.93289860262e-06) A[3]:(0.999985039234)\n",
      " state (2)  A[0]:(0.00225661904551) A[1]:(1.17704485092e-05) A[2]:(5.79568441026e-05) A[3]:(0.997673630714)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.63029994691e-11) A[2]:(2.25237162255e-10) A[3]:(7.51971401813e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.09131141623e-10) A[2]:(3.54846957373e-10) A[3]:(3.93132927471e-13)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.54667334495e-10) A[2]:(6.64207022716e-10) A[3]:(1.0323659584e-14)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.31435939857e-10) A[2]:(1.48952850054e-09) A[3]:(1.24565935773e-16)\n",
      " state (7)  A[0]:(1.0) A[1]:(3.3184566206e-10) A[2]:(3.2823126439e-09) A[3]:(2.3148620491e-18)\n",
      " state (8)  A[0]:(1.0) A[1]:(5.54265466857e-10) A[2]:(6.93390722617e-09) A[3]:(6.48896927563e-19)\n",
      " state (9)  A[0]:(0.999997317791) A[1]:(2.19773408361e-08) A[2]:(2.64966183749e-06) A[3]:(1.08751128268e-19)\n",
      " state (10)  A[0]:(0.780400753021) A[1]:(2.12619556805e-06) A[2]:(0.219597116113) A[3]:(1.27098971886e-22)\n",
      " state (11)  A[0]:(0.210962712765) A[1]:(1.7891495645e-06) A[2]:(0.789035499096) A[3]:(1.70354102275e-23)\n",
      " state (12)  A[0]:(0.0384265333414) A[1]:(7.51697655232e-07) A[2]:(0.961572706699) A[3]:(3.19900708387e-24)\n",
      " state (13)  A[0]:(0.00498766126111) A[1]:(2.34496184248e-07) A[2]:(0.995012104511) A[3]:(4.32339952272e-25)\n",
      " state (14)  A[0]:(0.000989900319837) A[1]:(9.22651395285e-08) A[2]:(0.999010026455) A[3]:(8.93710626805e-26)\n",
      " state (15)  A[0]:(0.000347143999534) A[1]:(5.07424395835e-08) A[2]:(0.999652802944) A[3]:(3.28272232201e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 813000 finished after 26 . Running score: 0.0. Policy_loss: -92050.6111907, Value_loss: 0.978635455828. Times trained:               17177. Times reached goal: 66.               Steps done: 10653790.\n",
      " state (0)  A[0]:(0.992033481598) A[1]:(0.00127791170962) A[2]:(0.000928960507736) A[3]:(0.00575965084136)\n",
      " state (1)  A[0]:(7.50334629629e-06) A[1]:(1.05028391317e-06) A[2]:(4.92642084282e-06) A[3]:(0.99998652935)\n",
      " state (2)  A[0]:(0.99999755621) A[1]:(7.22900139749e-08) A[2]:(1.93890699052e-07) A[3]:(2.1546698008e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.5991630483e-11) A[2]:(2.39622904852e-10) A[3]:(1.93135134621e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.26930896349e-10) A[2]:(4.69593086638e-10) A[3]:(4.35116759246e-14)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.8454110462e-10) A[2]:(9.58706447562e-10) A[3]:(8.10010037102e-16)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.67663557985e-10) A[2]:(2.12918727094e-09) A[3]:(1.2807612137e-17)\n",
      " state (7)  A[0]:(1.0) A[1]:(3.44244466266e-10) A[2]:(3.77750097869e-09) A[3]:(9.74559382292e-19)\n",
      " state (8)  A[0]:(0.999999940395) A[1]:(1.71873237864e-09) A[2]:(3.00499714001e-08) A[3]:(4.80986449347e-19)\n",
      " state (9)  A[0]:(0.972449839115) A[1]:(1.00392060176e-06) A[2]:(0.0275491569191) A[3]:(4.59552901955e-22)\n",
      " state (10)  A[0]:(0.267200708389) A[1]:(1.88384206012e-06) A[2]:(0.732797384262) A[3]:(2.0118886068e-23)\n",
      " state (11)  A[0]:(0.0407422445714) A[1]:(7.41398309856e-07) A[2]:(0.959257006645) A[3]:(3.17115851898e-24)\n",
      " state (12)  A[0]:(0.0027373242192) A[1]:(1.56855563205e-07) A[2]:(0.997262537479) A[3]:(2.22063802478e-25)\n",
      " state (13)  A[0]:(0.000319984799717) A[1]:(4.50068959879e-08) A[2]:(0.999679982662) A[3]:(2.6852871436e-26)\n",
      " state (14)  A[0]:(8.79889703356e-05) A[1]:(2.14465138981e-08) A[2]:(0.99991196394) A[3]:(7.72679549977e-27)\n",
      " state (15)  A[0]:(4.26565784437e-05) A[1]:(1.42566456418e-08) A[2]:(0.999957323074) A[3]:(3.91165731725e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 814000 finished after 17 . Running score: 0.0. Policy_loss: -92050.6112523, Value_loss: 0.979007769406. Times trained:               18520. Times reached goal: 0.               Steps done: 10672310.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9963,  0.0011,  0.0005,  0.0022]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0011,  0.0005,  0.0022]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.1129e-11,  2.5545e-10,  4.9081e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.1129e-11,  2.5545e-10,  4.9086e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5222e-10,  3.7311e-09,  5.5892e-19]])\n",
      "On state=8, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.1129e-11,  2.5544e-10,  4.9095e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5223e-10,  3.7311e-09,  5.5894e-19]])\n",
      "On state=8, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5224e-10,  3.7311e-09,  5.5895e-19]])\n",
      "On state=8, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.5224e-10,  3.7311e-09,  5.5896e-19]])\n",
      "On state=8, selected action=0\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996308386326) A[1]:(0.00107088929508) A[2]:(0.000466528756078) A[3]:(0.00215422199108)\n",
      " state (1)  A[0]:(8.86892485141e-06) A[1]:(1.13271983082e-06) A[2]:(4.78906395074e-06) A[3]:(0.999985218048)\n",
      " state (2)  A[0]:(0.000470005645184) A[1]:(6.45472482574e-06) A[2]:(2.72656143352e-05) A[3]:(0.999496281147)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.73023658973e-11) A[2]:(1.68738606421e-10) A[3]:(7.24718635797e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.11294512074e-11) A[2]:(2.55434617902e-10) A[3]:(4.91073707306e-13)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.27490892843e-10) A[2]:(4.64289495739e-10) A[3]:(1.51806888051e-14)\n",
      " state (6)  A[0]:(1.0) A[1]:(1.85595136482e-10) A[2]:(9.83437220548e-10) A[3]:(2.21703065359e-16)\n",
      " state (7)  A[0]:(1.0) A[1]:(2.67984884283e-10) A[2]:(2.22060614341e-09) A[3]:(3.09028659111e-18)\n",
      " state (8)  A[0]:(1.0) A[1]:(3.52239321044e-10) A[2]:(3.73106434637e-09) A[3]:(5.58959647512e-19)\n",
      " state (9)  A[0]:(0.999999940395) A[1]:(3.04740188639e-09) A[2]:(5.62273250182e-08) A[3]:(4.22535282324e-19)\n",
      " state (10)  A[0]:(0.992539644241) A[1]:(5.86228622979e-07) A[2]:(0.00745976343751) A[3]:(6.71483956105e-22)\n",
      " state (11)  A[0]:(0.467476963997) A[1]:(2.19518119593e-06) A[2]:(0.532520830631) A[3]:(3.04696010029e-23)\n",
      " state (12)  A[0]:(0.155054748058) A[1]:(1.45796559536e-06) A[2]:(0.844943761826) A[3]:(8.8290953343e-24)\n",
      " state (13)  A[0]:(0.0271379780024) A[1]:(5.66336666452e-07) A[2]:(0.972861468792) A[3]:(1.57539654432e-24)\n",
      " state (14)  A[0]:(0.004080417566) A[1]:(1.88236143117e-07) A[2]:(0.995919406414) A[3]:(2.37862566346e-25)\n",
      " state (15)  A[0]:(0.000966490420979) A[1]:(8.11994880223e-08) A[2]:(0.99903345108) A[3]:(5.70618406598e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 815000 finished after 9 . Running score: 0.0. Policy_loss: -92050.6112473, Value_loss: 0.979091878574. Times trained:               17252. Times reached goal: 0.               Steps done: 10689562.\n",
      " state (0)  A[0]:(0.994569838047) A[1]:(0.000729050370865) A[2]:(0.00375127163716) A[3]:(0.000949824170675)\n",
      " state (1)  A[0]:(1.15037892101e-05) A[1]:(1.2940220131e-06) A[2]:(1.46771899381e-05) A[3]:(0.999972522259)\n",
      " state (2)  A[0]:(0.999988734722) A[1]:(2.34581733594e-07) A[2]:(9.79470314633e-07) A[3]:(1.00668221421e-05)\n",
      " state (3)  A[0]:(1.0) A[1]:(9.50409195788e-11) A[2]:(2.84136214557e-10) A[3]:(1.14066612059e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.42655359747e-10) A[2]:(5.19994436399e-10) A[3]:(2.28525050717e-14)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.04760278177e-10) A[2]:(9.97275484416e-10) A[3]:(4.02710115129e-16)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.83603446283e-10) A[2]:(2.00931737915e-09) A[3]:(7.86580768292e-18)\n",
      " state (7)  A[0]:(1.0) A[1]:(3.51635193185e-10) A[2]:(3.38320549353e-09) A[3]:(6.05431688301e-19)\n",
      " state (8)  A[0]:(1.0) A[1]:(7.38828553892e-10) A[2]:(1.06951487666e-08) A[3]:(1.88616592557e-19)\n",
      " state (9)  A[0]:(0.994416117668) A[1]:(5.42328223219e-07) A[2]:(0.00558335753158) A[3]:(4.27445430075e-22)\n",
      " state (10)  A[0]:(0.40775975585) A[1]:(2.56868202086e-06) A[2]:(0.592237710953) A[3]:(1.73379084079e-23)\n",
      " state (11)  A[0]:(0.107051998377) A[1]:(1.48690401147e-06) A[2]:(0.892946541309) A[3]:(4.46848642598e-24)\n",
      " state (12)  A[0]:(0.0114598590881) A[1]:(4.2688535018e-07) A[2]:(0.98853969574) A[3]:(4.99297775654e-25)\n",
      " state (13)  A[0]:(0.00144183763769) A[1]:(1.27802593397e-07) A[2]:(0.998558044434) A[3]:(6.41405052665e-26)\n",
      " state (14)  A[0]:(0.000371944886865) A[1]:(5.83576422741e-08) A[2]:(0.999628007412) A[3]:(1.70630860318e-26)\n",
      " state (15)  A[0]:(0.000168568498339) A[1]:(3.71554591538e-08) A[2]:(0.99983137846) A[3]:(8.00036845577e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 816000 finished after 14 . Running score: 0.0. Policy_loss: -92050.6111818, Value_loss: 0.97859087809. Times trained:               17946. Times reached goal: 0.               Steps done: 10707508.\n",
      " state (0)  A[0]:(0.995221674442) A[1]:(0.00103998219129) A[2]:(0.00296507566236) A[3]:(0.000773261999711)\n",
      " state (1)  A[0]:(1.45368676385e-05) A[1]:(1.89155718999e-06) A[2]:(1.60192794283e-05) A[3]:(0.999967575073)\n",
      " state (2)  A[0]:(2.19102348638e-05) A[1]:(2.15569821194e-06) A[2]:(1.51407493831e-05) A[3]:(0.999960780144)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.87402959606e-10) A[2]:(6.59199028696e-10) A[3]:(1.06701980584e-10)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.69342817546e-10) A[2]:(1.79693274149e-10) A[3]:(2.66165518445e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.41757142083e-10) A[2]:(2.57454696451e-10) A[3]:(2.42405515148e-13)\n",
      " state (6)  A[0]:(1.0) A[1]:(4.86621354323e-10) A[2]:(5.60823554263e-10) A[3]:(1.182213943e-15)\n",
      " state (7)  A[0]:(1.0) A[1]:(1.11064568653e-09) A[2]:(1.68221958585e-09) A[3]:(1.56216098569e-18)\n",
      " state (8)  A[0]:(1.0) A[1]:(1.57973856219e-09) A[2]:(2.84203571788e-09) A[3]:(2.30550203518e-19)\n",
      " state (9)  A[0]:(1.0) A[1]:(4.02929778431e-09) A[2]:(9.96800508801e-09) A[3]:(1.20458163778e-19)\n",
      " state (10)  A[0]:(0.999987781048) A[1]:(1.59187081294e-07) A[2]:(1.20720442283e-05) A[3]:(3.05758730624e-21)\n",
      " state (11)  A[0]:(0.96461635828) A[1]:(7.53368658479e-06) A[2]:(0.0353760793805) A[3]:(5.85929466579e-23)\n",
      " state (12)  A[0]:(0.682144463062) A[1]:(1.95845859707e-05) A[2]:(0.317835897207) A[3]:(1.78136199328e-23)\n",
      " state (13)  A[0]:(0.478332012892) A[1]:(2.14027368202e-05) A[2]:(0.521646559238) A[3]:(1.12085207055e-23)\n",
      " state (14)  A[0]:(0.293361783028) A[1]:(1.87944988284e-05) A[2]:(0.706619381905) A[3]:(6.79048863973e-24)\n",
      " state (15)  A[0]:(0.13896356523) A[1]:(1.32230188683e-05) A[2]:(0.861023247242) A[3]:(3.16560592428e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 817000 finished after 27 . Running score: 0.0. Policy_loss: -92050.6111836, Value_loss: 0.978555285679. Times trained:               17051. Times reached goal: 4.               Steps done: 10724559.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994993984699) A[1]:(0.000819198729005) A[2]:(0.00338000245392) A[3]:(0.000806824478786)\n",
      " state (1)  A[0]:(1.00655688584e-05) A[1]:(1.31201852582e-06) A[2]:(1.18826910693e-05) A[3]:(0.999976754189)\n",
      " state (2)  A[0]:(0.000220331901801) A[1]:(4.77812409372e-06) A[2]:(3.00726205751e-05) A[3]:(0.999744832516)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.0601109707e-10) A[2]:(2.14709333646e-10) A[3]:(2.92358706688e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.50766371609e-10) A[2]:(1.56862856038e-10) A[3]:(6.21951318572e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.95119212454e-10) A[2]:(1.97966712467e-10) A[3]:(1.17613199995e-12)\n",
      " state (6)  A[0]:(1.0) A[1]:(3.4979827368e-10) A[2]:(3.59377139159e-10) A[3]:(1.32158027604e-14)\n",
      " state (7)  A[0]:(1.0) A[1]:(9.28046473003e-10) A[2]:(1.25375387849e-09) A[3]:(5.24918022019e-18)\n",
      " state (8)  A[0]:(1.0) A[1]:(1.44838374627e-09) A[2]:(2.39751041242e-09) A[3]:(3.64854530645e-19)\n",
      " state (9)  A[0]:(1.0) A[1]:(3.76231445998e-09) A[2]:(8.54474357936e-09) A[3]:(1.71486625574e-19)\n",
      " state (10)  A[0]:(0.999989569187) A[1]:(1.50058141912e-07) A[2]:(1.03068168755e-05) A[3]:(4.22358057262e-21)\n",
      " state (11)  A[0]:(0.969869732857) A[1]:(7.17055672794e-06) A[2]:(0.030123071745) A[3]:(8.05072501694e-23)\n",
      " state (12)  A[0]:(0.71728259325) A[1]:(1.9572531528e-05) A[2]:(0.282697826624) A[3]:(2.55219016929e-23)\n",
      " state (13)  A[0]:(0.519646942616) A[1]:(2.2170435841e-05) A[2]:(0.480330914259) A[3]:(1.65475739105e-23)\n",
      " state (14)  A[0]:(0.329010248184) A[1]:(2.01080783881e-05) A[2]:(0.670969665051) A[3]:(1.03222070795e-23)\n",
      " state (15)  A[0]:(0.160780832171) A[1]:(1.4569063751e-05) A[2]:(0.839204609394) A[3]:(4.95370056928e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 818000 finished after 6 . Running score: 0.0. Policy_loss: -92050.6111805, Value_loss: 0.97945805896. Times trained:               18477. Times reached goal: 0.               Steps done: 10743036.\n",
      " state (0)  A[0]:(0.991543531418) A[1]:(0.00449316436425) A[2]:(0.00195205339696) A[3]:(0.00201123137958)\n",
      " state (1)  A[0]:(7.16564318282e-06) A[1]:(1.44482623909e-06) A[2]:(6.9779362093e-06) A[3]:(0.999984383583)\n",
      " state (2)  A[0]:(0.999990522861) A[1]:(2.54517317444e-07) A[2]:(3.46266375573e-07) A[3]:(8.8542337835e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.49359885571e-10) A[2]:(1.07714989706e-10) A[3]:(1.33314210365e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.56802723583e-10) A[2]:(1.28635740948e-10) A[3]:(6.28688854448e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.8087271958e-10) A[2]:(1.5457884972e-10) A[3]:(2.0096287915e-12)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.29547311714e-10) A[2]:(1.96184249401e-10) A[3]:(2.64678524323e-13)\n",
      " state (7)  A[0]:(1.0) A[1]:(3.9919351158e-10) A[2]:(3.72993802511e-10) A[3]:(2.12385930579e-15)\n",
      " state (8)  A[0]:(1.0) A[1]:(7.5703204816e-10) A[2]:(9.11858311081e-10) A[3]:(7.21094285403e-18)\n",
      " state (9)  A[0]:(1.0) A[1]:(9.86061121644e-10) A[2]:(1.36836286746e-09) A[3]:(9.87240267877e-19)\n",
      " state (10)  A[0]:(1.0) A[1]:(1.20276710813e-09) A[2]:(1.80465498101e-09) A[3]:(4.85982827042e-19)\n",
      " state (11)  A[0]:(1.0) A[1]:(1.59386659426e-09) A[2]:(2.62625343694e-09) A[3]:(3.18760304601e-19)\n",
      " state (12)  A[0]:(1.0) A[1]:(2.88092216749e-09) A[2]:(6.50794174106e-09) A[3]:(2.00920761997e-19)\n",
      " state (13)  A[0]:(0.999999821186) A[1]:(1.46082479446e-08) A[2]:(1.45557720543e-07) A[3]:(3.84503921753e-20)\n",
      " state (14)  A[0]:(0.999924302101) A[1]:(2.88349411903e-07) A[2]:(7.53811254981e-05) A[3]:(1.08979369532e-21)\n",
      " state (15)  A[0]:(0.991807997227) A[1]:(2.86725116894e-06) A[2]:(0.0081891566515) A[3]:(1.21578699976e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 819000 finished after 10 . Running score: 0.0. Policy_loss: -92050.6111848, Value_loss: 0.979020771189. Times trained:               17923. Times reached goal: 0.               Steps done: 10760959.\n",
      "action_dist \n",
      "tensor([[ 0.9909,  0.0048,  0.0023,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9909,  0.0048,  0.0023,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3804e-10,  1.2403e-10,  6.8167e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3804e-10,  1.2403e-10,  6.8168e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3804e-10,  1.2404e-10,  6.8169e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9909,  0.0048,  0.0023,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9909,  0.0048,  0.0023,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.3805e-10,  1.2404e-10,  6.8173e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.9479e-10,  1.0614e-09,  2.4328e-18]])\n",
      "On state=8, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.9484e-10,  1.0615e-09,  2.4323e-18]])\n",
      "On state=8, selected action=0\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.990899205208) A[1]:(0.00485007697716) A[2]:(0.00229227310047) A[3]:(0.00195845006965)\n",
      " state (1)  A[0]:(7.73717420088e-06) A[1]:(1.50636185481e-06) A[2]:(7.93158596935e-06) A[3]:(0.999982833862)\n",
      " state (2)  A[0]:(0.999997496605) A[1]:(1.12608965708e-07) A[2]:(1.58814856377e-07) A[3]:(2.23917322728e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.35072661389e-10) A[2]:(1.06750025486e-10) A[3]:(1.28863569121e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.38058134369e-10) A[2]:(1.24044718941e-10) A[3]:(6.81762675828e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.55962479043e-10) A[2]:(1.48546494549e-10) A[3]:(2.33089003827e-12)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.06716047058e-10) A[2]:(2.02181452136e-10) A[3]:(1.72719111691e-13)\n",
      " state (7)  A[0]:(1.0) A[1]:(4.19100337767e-10) A[2]:(4.97606622574e-10) A[3]:(2.44730767761e-16)\n",
      " state (8)  A[0]:(1.0) A[1]:(6.94912960508e-10) A[2]:(1.06161035518e-09) A[3]:(2.43092934431e-18)\n",
      " state (9)  A[0]:(1.0) A[1]:(8.83877582236e-10) A[2]:(1.51032097939e-09) A[3]:(6.30811450651e-19)\n",
      " state (10)  A[0]:(1.0) A[1]:(1.18780030256e-09) A[2]:(2.20631113379e-09) A[3]:(3.40177845967e-19)\n",
      " state (11)  A[0]:(1.0) A[1]:(2.27914909168e-09) A[2]:(5.6248858904e-09) A[3]:(2.05918626032e-19)\n",
      " state (12)  A[0]:(0.999999701977) A[1]:(1.70358571694e-08) A[2]:(2.61436042592e-07) A[3]:(2.64372255209e-20)\n",
      " state (13)  A[0]:(0.999634563923) A[1]:(5.41259851161e-07) A[2]:(0.00036487847683) A[3]:(4.65194683688e-22)\n",
      " state (14)  A[0]:(0.970906436443) A[1]:(4.65841276309e-06) A[2]:(0.0290888957679) A[3]:(6.92498011551e-23)\n",
      " state (15)  A[0]:(0.844072639942) A[1]:(1.04129439933e-05) A[2]:(0.155916914344) A[3]:(3.33367538282e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 820000 finished after 10 . Running score: 0.0. Policy_loss: -92050.6111904, Value_loss: 0.979024382458. Times trained:               17330. Times reached goal: 0.               Steps done: 10778289.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996652364731) A[1]:(0.00152906170115) A[2]:(0.000912041927222) A[3]:(0.000906537112314)\n",
      " state (1)  A[0]:(1.04604459921e-05) A[1]:(1.3150521454e-06) A[2]:(7.8874090832e-06) A[3]:(0.999980330467)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.7764224253e-09) A[2]:(3.09572834034e-09) A[3]:(3.57028406839e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.05091609026e-10) A[2]:(9.55186971185e-11) A[3]:(6.61631990514e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.19392787323e-10) A[2]:(1.26586852112e-10) A[3]:(2.28255630542e-12)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.81372264429e-10) A[2]:(2.2050183901e-10) A[3]:(2.69134300485e-14)\n",
      " state (6)  A[0]:(1.0) A[1]:(3.79072162549e-10) A[2]:(6.96779023368e-10) A[3]:(7.46682455908e-18)\n",
      " state (7)  A[0]:(1.0) A[1]:(5.00904262513e-10) A[2]:(1.10456777058e-09) A[3]:(7.95202844047e-19)\n",
      " state (8)  A[0]:(1.0) A[1]:(6.57185361685e-10) A[2]:(1.5932465347e-09) A[3]:(3.45462366858e-19)\n",
      " state (9)  A[0]:(1.0) A[1]:(1.29779798019e-09) A[2]:(3.90433108066e-09) A[3]:(1.93352641004e-19)\n",
      " state (10)  A[0]:(0.99999922514) A[1]:(2.13977191521e-08) A[2]:(7.74137106418e-07) A[3]:(1.20344560538e-20)\n",
      " state (11)  A[0]:(0.994831442833) A[1]:(1.48624781104e-06) A[2]:(0.00516710057855) A[3]:(1.17987868245e-22)\n",
      " state (12)  A[0]:(0.864968061447) A[1]:(7.26031566955e-06) A[2]:(0.135024696589) A[3]:(2.94941366282e-23)\n",
      " state (13)  A[0]:(0.700759768486) A[1]:(1.01360919871e-05) A[2]:(0.299230128527) A[3]:(1.92694832437e-23)\n",
      " state (14)  A[0]:(0.557978451252) A[1]:(1.09342081487e-05) A[2]:(0.442010611296) A[3]:(1.47336687228e-23)\n",
      " state (15)  A[0]:(0.393876284361) A[1]:(1.02902322396e-05) A[2]:(0.606113433838) A[3]:(1.0116508443e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 821000 finished after 5 . Running score: 0.0. Policy_loss: -92050.6112055, Value_loss: 0.979198380147. Times trained:               18704. Times reached goal: 0.               Steps done: 10796993.\n",
      " state (0)  A[0]:(0.995365560055) A[1]:(0.00225839600898) A[2]:(0.00129093509167) A[3]:(0.0010851083789)\n",
      " state (1)  A[0]:(2.01456332434e-05) A[1]:(3.16431396641e-06) A[2]:(1.11459503387e-05) A[3]:(0.999965548515)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.37626499086e-09) A[2]:(1.65842062305e-10) A[3]:(5.17337735151e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.13486853038e-09) A[2]:(3.23686438763e-10) A[3]:(9.06238720676e-14)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(8.22911943033e-08) A[2]:(1.37378908249e-09) A[3]:(8.30199160044e-18)\n",
      " state (5)  A[0]:(0.999999701977) A[1]:(3.14334187124e-07) A[2]:(4.06781319739e-09) A[3]:(3.71486115164e-19)\n",
      " state (6)  A[0]:(0.999938189983) A[1]:(4.97723303852e-05) A[2]:(1.20590184451e-05) A[3]:(7.47165105455e-21)\n",
      " state (7)  A[0]:(0.732682526112) A[1]:(0.0109553067014) A[2]:(0.256362169981) A[3]:(4.35887270436e-23)\n",
      " state (8)  A[0]:(0.359159708023) A[1]:(0.0134006030858) A[2]:(0.627439677715) A[3]:(1.62018824401e-23)\n",
      " state (9)  A[0]:(0.0756959319115) A[1]:(0.00682661682367) A[2]:(0.917477428913) A[3]:(3.30942216984e-24)\n",
      " state (10)  A[0]:(0.0115132937208) A[1]:(0.00247987639159) A[2]:(0.986006855965) A[3]:(4.922445703e-25)\n",
      " state (11)  A[0]:(0.00334372464567) A[1]:(0.00127821066417) A[2]:(0.99537807703) A[3]:(1.47750541408e-25)\n",
      " state (12)  A[0]:(0.00159264018293) A[1]:(0.000870799296536) A[2]:(0.997536540031) A[3]:(7.42873580824e-26)\n",
      " state (13)  A[0]:(0.00100058328826) A[1]:(0.000690666260198) A[2]:(0.998308777809) A[3]:(4.92481703121e-26)\n",
      " state (14)  A[0]:(0.000743427837733) A[1]:(0.00059839192545) A[2]:(0.998658180237) A[3]:(3.82765695827e-26)\n",
      " state (15)  A[0]:(0.000614694319665) A[1]:(0.000547298463061) A[2]:(0.99883800745) A[3]:(3.27689492022e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 822000 finished after 23 . Running score: 0.0. Policy_loss: -92050.6305535, Value_loss: 0.979110470471. Times trained:               18336. Times reached goal: 0.               Steps done: 10815329.\n",
      " state (0)  A[0]:(0.996480286121) A[1]:(0.00140608067159) A[2]:(0.00114175840281) A[3]:(0.000971885223407)\n",
      " state (1)  A[0]:(0.000477960478747) A[1]:(1.60914642038e-05) A[2]:(4.720243669e-05) A[3]:(0.999458730221)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.40823333392e-09) A[2]:(4.27395702163e-10) A[3]:(4.55593570682e-13)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(7.66217951309e-08) A[2]:(1.72249325914e-09) A[3]:(8.53858246099e-17)\n",
      " state (4)  A[0]:(0.999999463558) A[1]:(5.27302915998e-07) A[2]:(7.03458535867e-09) A[3]:(4.78207225886e-19)\n",
      " state (5)  A[0]:(0.999713122845) A[1]:(0.000227012016694) A[2]:(5.98596598138e-05) A[3]:(1.3924365844e-20)\n",
      " state (6)  A[0]:(0.398199051619) A[1]:(0.0203080046922) A[2]:(0.581492960453) A[3]:(2.99358987352e-23)\n",
      " state (7)  A[0]:(0.13506616652) A[1]:(0.0147926304489) A[2]:(0.850141227245) A[3]:(9.49638882972e-24)\n",
      " state (8)  A[0]:(0.00517744384706) A[1]:(0.00250705052167) A[2]:(0.992315530777) A[3]:(3.65146900429e-25)\n",
      " state (9)  A[0]:(0.000283281784505) A[1]:(0.000495705462527) A[2]:(0.999221026897) A[3]:(2.13094611141e-26)\n",
      " state (10)  A[0]:(6.75776173011e-05) A[1]:(0.00022976618493) A[2]:(0.999702632427) A[3]:(5.63721890282e-27)\n",
      " state (11)  A[0]:(3.56055170414e-05) A[1]:(0.000165033125086) A[2]:(0.999799370766) A[3]:(3.19919208254e-27)\n",
      " state (12)  A[0]:(2.6780873668e-05) A[1]:(0.000143117184052) A[2]:(0.999830126762) A[3]:(2.51227276779e-27)\n",
      " state (13)  A[0]:(2.35402021644e-05) A[1]:(0.000134408779559) A[2]:(0.999842047691) A[3]:(2.26078464306e-27)\n",
      " state (14)  A[0]:(2.21541940846e-05) A[1]:(0.000130593572976) A[2]:(0.999847233295) A[3]:(2.1550543632e-27)\n",
      " state (15)  A[0]:(2.15005038626e-05) A[1]:(0.000128791099996) A[2]:(0.999849736691) A[3]:(2.10633469117e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 823000 finished after 3 . Running score: 0.07. Policy_loss: -92050.6141804, Value_loss: 1.00163824583. Times trained:               15421. Times reached goal: 89.               Steps done: 10830750.\n",
      " state (0)  A[0]:(0.997352778912) A[1]:(0.00123190740123) A[2]:(0.000624988926575) A[3]:(0.000790304155089)\n",
      " state (1)  A[0]:(7.47055455577e-05) A[1]:(6.2331105255e-06) A[2]:(2.13443036046e-05) A[3]:(0.99989771843)\n",
      " state (2)  A[0]:(1.0) A[1]:(6.8126455588e-09) A[2]:(7.27909788001e-10) A[3]:(2.29084221211e-13)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(1.74722245561e-07) A[2]:(4.29968149973e-09) A[3]:(1.83641888028e-17)\n",
      " state (4)  A[0]:(0.999998629093) A[1]:(1.33006028591e-06) A[2]:(4.01752977552e-08) A[3]:(3.05164608956e-19)\n",
      " state (5)  A[0]:(0.0790188536048) A[1]:(0.00955861527473) A[2]:(0.911422550678) A[3]:(1.12910292453e-23)\n",
      " state (6)  A[0]:(4.53414140793e-05) A[1]:(0.000161029136507) A[2]:(0.99979364872) A[3]:(9.36327049947e-27)\n",
      " state (7)  A[0]:(4.6736170134e-07) A[1]:(1.29131894937e-05) A[2]:(0.99998664856) A[3]:(1.26712504201e-28)\n",
      " state (8)  A[0]:(1.43767934446e-07) A[1]:(7.01921680957e-06) A[2]:(0.999992847443) A[3]:(4.54364730441e-29)\n",
      " state (9)  A[0]:(1.05163309172e-07) A[1]:(6.01227156949e-06) A[2]:(0.999993860722) A[3]:(3.50857160306e-29)\n",
      " state (10)  A[0]:(9.64602477893e-08) A[1]:(5.76734191782e-06) A[2]:(0.999994158745) A[3]:(3.27444382288e-29)\n",
      " state (11)  A[0]:(9.41014661748e-08) A[1]:(5.70047677684e-06) A[2]:(0.999994218349) A[3]:(3.21201721115e-29)\n",
      " state (12)  A[0]:(9.34069248615e-08) A[1]:(5.68100404053e-06) A[2]:(0.999994218349) A[3]:(3.19403173381e-29)\n",
      " state (13)  A[0]:(9.31891150913e-08) A[1]:(5.67502593185e-06) A[2]:(0.999994218349) A[3]:(3.18855336589e-29)\n",
      " state (14)  A[0]:(9.31173289587e-08) A[1]:(5.67307824895e-06) A[2]:(0.999994218349) A[3]:(3.18677820015e-29)\n",
      " state (15)  A[0]:(9.30924670683e-08) A[1]:(5.67240704186e-06) A[2]:(0.999994218349) A[3]:(3.18617032851e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 824000 finished after 12 . Running score: 0.1. Policy_loss: -92050.6123794, Value_loss: 1.20304434344. Times trained:               14761. Times reached goal: 104.               Steps done: 10845511.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.8037e-08,  1.8562e-08,  1.8870e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.8041e-08,  1.8564e-08,  1.8874e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.8042e-08,  1.8565e-08,  1.8874e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7822e-09,  2.8062e-08,  1.0000e+00,  1.2208e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.8027e-08,  1.8562e-08,  1.8877e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.8017e-08,  1.8559e-08,  1.8879e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.8010e-08,  1.8557e-08,  1.8881e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.8007e-08,  1.8556e-08,  1.8882e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.7999e-08,  1.8555e-08,  1.8889e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9930,  0.0011,  0.0032,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.8000e-08,  1.8555e-08,  1.8890e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7827e-09,  2.8067e-08,  1.0000e+00,  1.2214e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5521e-09,  2.6310e-08,  1.0000e+00,  1.1086e-30]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.992961764336) A[1]:(0.00108827196527) A[2]:(0.00321338279173) A[3]:(0.00273655191995)\n",
      " state (1)  A[0]:(0.000294206896797) A[1]:(7.77971945354e-06) A[2]:(4.68103608e-05) A[3]:(0.999651193619)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.98119409767e-09) A[2]:(9.77095182542e-10) A[3]:(1.21742839334e-12)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(2.61082657715e-08) A[2]:(6.01744964968e-09) A[3]:(1.38618662354e-16)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(9.79760912401e-08) A[2]:(1.85518853613e-08) A[3]:(1.88938557101e-18)\n",
      " state (5)  A[0]:(0.0173787996173) A[1]:(0.000336827390129) A[2]:(0.982284367085) A[3]:(3.80669720752e-24)\n",
      " state (6)  A[0]:(7.18442620951e-08) A[1]:(2.05625468652e-07) A[2]:(0.999999701977) A[3]:(2.60573356187e-29)\n",
      " state (7)  A[0]:(3.18746096184e-09) A[1]:(3.73769708517e-08) A[2]:(0.999999940395) A[3]:(1.88263996691e-30)\n",
      " state (8)  A[0]:(1.78236492232e-09) A[1]:(2.80627006077e-08) A[2]:(0.999999940395) A[3]:(1.22129640675e-30)\n",
      " state (9)  A[0]:(1.55192814155e-09) A[1]:(2.63080188745e-08) A[2]:(1.0) A[3]:(1.10847941788e-30)\n",
      " state (10)  A[0]:(1.49577483732e-09) A[1]:(2.58778314333e-08) A[2]:(1.0) A[3]:(1.08144756171e-30)\n",
      " state (11)  A[0]:(1.47988321597e-09) A[1]:(2.57590535568e-08) A[2]:(1.0) A[3]:(1.07403912611e-30)\n",
      " state (12)  A[0]:(1.47492928981e-09) A[1]:(2.57234091805e-08) A[2]:(1.0) A[3]:(1.07181255174e-30)\n",
      " state (13)  A[0]:(1.47324241695e-09) A[1]:(2.57114400881e-08) A[2]:(1.0) A[3]:(1.07107688035e-30)\n",
      " state (14)  A[0]:(1.47262990691e-09) A[1]:(2.57073207166e-08) A[2]:(1.0) A[3]:(1.07082353781e-30)\n",
      " state (15)  A[0]:(1.47239398451e-09) A[1]:(2.57057521935e-08) A[2]:(1.0) A[3]:(1.0707255486e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 825000 finished after 33 . Running score: 0.14. Policy_loss: -92050.6123924, Value_loss: 1.19983052211. Times trained:               14766. Times reached goal: 126.               Steps done: 10860277.\n",
      " state (0)  A[0]:(0.995299816132) A[1]:(0.00099576159846) A[2]:(0.00210445211269) A[3]:(0.00159999344032)\n",
      " state (1)  A[0]:(0.000392208719859) A[1]:(8.21975663712e-06) A[2]:(4.84519841848e-05) A[3]:(0.99955111742)\n",
      " state (2)  A[0]:(1.0) A[1]:(8.8816154431e-10) A[2]:(6.02484340195e-10) A[3]:(1.05432210271e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.1573657284e-09) A[2]:(2.47929787811e-09) A[3]:(7.17431678543e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.71040070995e-08) A[2]:(6.4121188359e-09) A[3]:(6.76847101968e-19)\n",
      " state (5)  A[0]:(0.0101919481531) A[1]:(6.85210325173e-05) A[2]:(0.989739537239) A[3]:(1.69082375665e-24)\n",
      " state (6)  A[0]:(2.2239005304e-08) A[1]:(3.14358068465e-08) A[2]:(0.999999940395) A[3]:(6.83089208833e-30)\n",
      " state (7)  A[0]:(2.20272444729e-09) A[1]:(8.77655548237e-09) A[2]:(1.0) A[3]:(9.47062696537e-31)\n",
      " state (8)  A[0]:(1.4586192254e-09) A[1]:(7.15371317739e-09) A[2]:(1.0) A[3]:(6.97840920024e-31)\n",
      " state (9)  A[0]:(1.32281430254e-09) A[1]:(6.83396317314e-09) A[2]:(1.0) A[3]:(6.52691310483e-31)\n",
      " state (10)  A[0]:(1.28775246022e-09) A[1]:(6.75241285109e-09) A[2]:(1.0) A[3]:(6.41519083035e-31)\n",
      " state (11)  A[0]:(1.27712684872e-09) A[1]:(6.72844846505e-09) A[2]:(1.0) A[3]:(6.38287178867e-31)\n",
      " state (12)  A[0]:(1.27352672852e-09) A[1]:(6.72054767392e-09) A[2]:(1.0) A[3]:(6.37236192877e-31)\n",
      " state (13)  A[0]:(1.27221089219e-09) A[1]:(6.71772859562e-09) A[2]:(1.0) A[3]:(6.36861915476e-31)\n",
      " state (14)  A[0]:(1.27169164088e-09) A[1]:(6.7166263662e-09) A[2]:(1.0) A[3]:(6.36716201196e-31)\n",
      " state (15)  A[0]:(1.27148791496e-09) A[1]:(6.7161654016e-09) A[2]:(1.0) A[3]:(6.36657896677e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 826000 finished after 24 . Running score: 0.07. Policy_loss: -92050.6122225, Value_loss: 1.23398692861. Times trained:               15867. Times reached goal: 102.               Steps done: 10876144.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.992617964745) A[1]:(0.00156782451086) A[2]:(0.0028920979239) A[3]:(0.00292212888598)\n",
      " state (1)  A[0]:(1.09247930595e-05) A[1]:(1.4486248574e-06) A[2]:(9.6806570582e-06) A[3]:(0.999977946281)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.81019232726e-10) A[2]:(4.63560356767e-10) A[3]:(7.96848524665e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.99193617156e-09) A[2]:(1.33466493413e-09) A[3]:(5.01288158932e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.98226854112e-09) A[2]:(5.65755264859e-09) A[3]:(2.09997727636e-18)\n",
      " state (5)  A[0]:(0.716946125031) A[1]:(7.44116550777e-05) A[2]:(0.282979428768) A[3]:(4.11080113011e-22)\n",
      " state (6)  A[0]:(1.0461418043e-07) A[1]:(2.08888231157e-08) A[2]:(0.999999880791) A[3]:(4.70963538223e-29)\n",
      " state (7)  A[0]:(1.76404080232e-09) A[1]:(1.91098070701e-09) A[2]:(1.0) A[3]:(1.42975885704e-30)\n",
      " state (8)  A[0]:(9.58766843695e-10) A[1]:(1.37627031993e-09) A[2]:(1.0) A[3]:(9.10288061068e-31)\n",
      " state (9)  A[0]:(8.34737390321e-10) A[1]:(1.28053190274e-09) A[2]:(1.0) A[3]:(8.26641669815e-31)\n",
      " state (10)  A[0]:(8.02713173709e-10) A[1]:(1.25529475703e-09) A[2]:(1.0) A[3]:(8.05425219296e-31)\n",
      " state (11)  A[0]:(7.92529430971e-10) A[1]:(1.2473138078e-09) A[2]:(1.0) A[3]:(7.98876963446e-31)\n",
      " state (12)  A[0]:(7.88873688595e-10) A[1]:(1.24447641081e-09) A[2]:(1.0) A[3]:(7.96576379942e-31)\n",
      " state (13)  A[0]:(7.87451548412e-10) A[1]:(1.24337551366e-09) A[2]:(1.0) A[3]:(7.95695605535e-31)\n",
      " state (14)  A[0]:(7.86878062708e-10) A[1]:(1.24293453307e-09) A[2]:(1.0) A[3]:(7.95349728077e-31)\n",
      " state (15)  A[0]:(7.86634923866e-10) A[1]:(1.24274956992e-09) A[2]:(1.0) A[3]:(7.95198042286e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 827000 finished after 4 . Running score: 0.15. Policy_loss: -92050.6135998, Value_loss: 1.46419542084. Times trained:               15948. Times reached goal: 130.               Steps done: 10892092.\n",
      " state (0)  A[0]:(0.995589077473) A[1]:(0.00164861278608) A[2]:(0.0014403720852) A[3]:(0.00132191099692)\n",
      " state (1)  A[0]:(4.7705148063e-06) A[1]:(9.81103539743e-07) A[2]:(6.49280809739e-06) A[3]:(0.999987781048)\n",
      " state (2)  A[0]:(0.999998748302) A[1]:(8.32714448507e-08) A[2]:(1.55395227353e-07) A[3]:(9.86134750747e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.17402770733e-10) A[2]:(4.15257883635e-10) A[3]:(4.43490990459e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.70323122273e-09) A[2]:(4.68484895322e-09) A[3]:(2.81418879868e-18)\n",
      " state (5)  A[0]:(0.0125125031918) A[1]:(4.27761233368e-06) A[2]:(0.987483203411) A[3]:(4.39267373754e-24)\n",
      " state (6)  A[0]:(6.14453110614e-09) A[1]:(5.56910462191e-10) A[2]:(1.0) A[3]:(4.64874913829e-30)\n",
      " state (7)  A[0]:(7.06192937461e-10) A[1]:(1.62166655104e-10) A[2]:(1.0) A[3]:(8.6845052441e-31)\n",
      " state (8)  A[0]:(4.56962329087e-10) A[1]:(1.29108182478e-10) A[2]:(1.0) A[3]:(6.47715772035e-31)\n",
      " state (9)  A[0]:(3.94079463506e-10) A[1]:(1.19901241713e-10) A[2]:(1.0) A[3]:(5.90075970779e-31)\n",
      " state (10)  A[0]:(3.68702429698e-10) A[1]:(1.16091303237e-10) A[2]:(1.0) A[3]:(5.66549109625e-31)\n",
      " state (11)  A[0]:(3.56181417693e-10) A[1]:(1.14199080747e-10) A[2]:(1.0) A[3]:(5.54798962133e-31)\n",
      " state (12)  A[0]:(3.4936020743e-10) A[1]:(1.1316998727e-10) A[2]:(1.0) A[3]:(5.48326972356e-31)\n",
      " state (13)  A[0]:(3.45425743564e-10) A[1]:(1.12580084144e-10) A[2]:(1.0) A[3]:(5.44558196408e-31)\n",
      " state (14)  A[0]:(3.43072487086e-10) A[1]:(1.12229767146e-10) A[2]:(1.0) A[3]:(5.42286200927e-31)\n",
      " state (15)  A[0]:(3.41626782419e-10) A[1]:(1.12016333709e-10) A[2]:(1.0) A[3]:(5.40881344118e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 828000 finished after 4 . Running score: 0.14. Policy_loss: -92050.6134841, Value_loss: 1.20811788597. Times trained:               15489. Times reached goal: 130.               Steps done: 10907581.\n",
      " state (0)  A[0]:(0.995978236198) A[1]:(0.000705044600181) A[2]:(0.00181709195022) A[3]:(0.00149961642455)\n",
      " state (1)  A[0]:(5.66770768273e-06) A[1]:(8.11214249552e-07) A[2]:(7.94375046098e-06) A[3]:(0.999985575676)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.29419730277e-10) A[2]:(8.94548934927e-10) A[3]:(2.48206455389e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.23177656911e-10) A[2]:(3.91236515407e-10) A[3]:(2.67036962204e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.33078930892e-10) A[2]:(3.09882985938e-09) A[3]:(9.07447220665e-19)\n",
      " state (5)  A[0]:(0.000712660315912) A[1]:(4.1052163624e-07) A[2]:(0.999286949635) A[3]:(3.12473548452e-25)\n",
      " state (6)  A[0]:(1.4953013272e-09) A[1]:(1.48757728358e-10) A[2]:(1.0) A[3]:(2.88425067946e-30)\n",
      " state (7)  A[0]:(3.44867495672e-10) A[1]:(6.887213716e-11) A[2]:(1.0) A[3]:(1.06799238317e-30)\n",
      " state (8)  A[0]:(2.59878119024e-10) A[1]:(6.0075243391e-11) A[2]:(1.0) A[3]:(9.02721921149e-31)\n",
      " state (9)  A[0]:(2.37306008177e-10) A[1]:(5.76175322731e-11) A[2]:(1.0) A[3]:(8.58241120874e-31)\n",
      " state (10)  A[0]:(2.28314214756e-10) A[1]:(5.66390649659e-11) A[2]:(1.0) A[3]:(8.40620930753e-31)\n",
      " state (11)  A[0]:(2.24047738695e-10) A[1]:(5.61798593757e-11) A[2]:(1.0) A[3]:(8.32280187077e-31)\n",
      " state (12)  A[0]:(2.21843335368e-10) A[1]:(5.5945893751e-11) A[2]:(1.0) A[3]:(8.27973551934e-31)\n",
      " state (13)  A[0]:(2.20647417004e-10) A[1]:(5.58209728441e-11) A[2]:(1.0) A[3]:(8.2563958439e-31)\n",
      " state (14)  A[0]:(2.1997927091e-10) A[1]:(5.57520245248e-11) A[2]:(1.0) A[3]:(8.24336666452e-31)\n",
      " state (15)  A[0]:(2.19596119067e-10) A[1]:(5.57133297829e-11) A[2]:(1.0) A[3]:(8.23594882497e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 829000 finished after 18 . Running score: 0.16. Policy_loss: -92050.6122195, Value_loss: 1.63863461346. Times trained:               15917. Times reached goal: 126.               Steps done: 10923498.\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0006,  0.0011,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0006,  0.0011,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0006,  0.0011,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0006,  0.0011,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0006,  0.0011,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0006,  0.0011,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0006,  0.0011,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0006,  0.0011,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.7319e-10,  1.8486e-09,  8.9146e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.7320e-10,  1.8487e-09,  8.9152e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.7320e-10,  1.8487e-09,  8.9157e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4198e-10,  4.1869e-11,  1.0000e+00,  7.7326e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.7320e-10,  1.8486e-09,  8.9172e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0006,  0.0011,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9970,  0.0006,  0.0011,  0.0013]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.7321e-10,  1.8486e-09,  8.9190e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.7321e-10,  1.8485e-09,  8.9196e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.7322e-10,  1.8485e-09,  8.9201e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4198e-10,  4.1870e-11,  1.0000e+00,  7.7330e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3293e-10,  4.0638e-11,  1.0000e+00,  7.4565e-31]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2818e-10,  4.0024e-11,  1.0000e+00,  7.3201e-31]])\n",
      "On state=13, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.2804e-10,  4.0008e-11,  1.0000e+00,  7.3166e-31]])\n",
      "On state=14, selected action=2\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.996977627277) A[1]:(0.00063411553856) A[2]:(0.0011321681086) A[3]:(0.00125606742222)\n",
      " state (1)  A[0]:(5.37551295565e-06) A[1]:(7.69791938637e-07) A[2]:(6.24672793492e-06) A[3]:(0.999987602234)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.51020176867e-10) A[2]:(3.5118141728e-10) A[3]:(7.28837476482e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.02198036092e-10) A[2]:(3.33214372761e-10) A[3]:(2.11886850296e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.73271644097e-10) A[2]:(1.84861781438e-09) A[3]:(8.92448678401e-19)\n",
      " state (5)  A[0]:(0.0139625119045) A[1]:(1.71817168848e-06) A[2]:(0.986035764217) A[3]:(5.79112256854e-24)\n",
      " state (6)  A[0]:(1.15332587924e-09) A[1]:(1.2426755458e-10) A[2]:(1.0) A[3]:(3.19069942361e-30)\n",
      " state (7)  A[0]:(1.86631321508e-10) A[1]:(4.77210805594e-11) A[2]:(1.0) A[3]:(9.09822941463e-31)\n",
      " state (8)  A[0]:(1.41855485691e-10) A[1]:(4.18684670267e-11) A[2]:(1.0) A[3]:(7.73312171245e-31)\n",
      " state (9)  A[0]:(1.32806696196e-10) A[1]:(4.06357655858e-11) A[2]:(1.0) A[3]:(7.45647146797e-31)\n",
      " state (10)  A[0]:(1.29974433993e-10) A[1]:(4.02605171423e-11) A[2]:(1.0) A[3]:(7.37319991836e-31)\n",
      " state (11)  A[0]:(1.28846433523e-10) A[1]:(4.01164067243e-11) A[2]:(1.0) A[3]:(7.34120531312e-31)\n",
      " state (12)  A[0]:(1.28330401861e-10) A[1]:(4.00526452282e-11) A[2]:(1.0) A[3]:(7.32693716269e-31)\n",
      " state (13)  A[0]:(1.28076091399e-10) A[1]:(4.00220967478e-11) A[2]:(1.0) A[3]:(7.32000879898e-31)\n",
      " state (14)  A[0]:(1.27945209982e-10) A[1]:(4.00065293393e-11) A[2]:(1.0) A[3]:(7.31649124969e-31)\n",
      " state (15)  A[0]:(1.27875446343e-10) A[1]:(3.99987473698e-11) A[2]:(1.0) A[3]:(7.31464948514e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 830000 finished after 22 . Running score: 0.08. Policy_loss: -92050.611195, Value_loss: 1.19194044744. Times trained:               15720. Times reached goal: 115.               Steps done: 10939218.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997177302837) A[1]:(0.000593730655964) A[2]:(0.000820745772216) A[3]:(0.00140819186345)\n",
      " state (1)  A[0]:(4.60762248622e-06) A[1]:(6.58669250697e-07) A[2]:(4.92368508276e-06) A[3]:(0.999989807606)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.95900329081e-10) A[2]:(8.67186933373e-10) A[3]:(4.72189620737e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.73192099551e-10) A[2]:(2.58378568541e-10) A[3]:(2.03140260689e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.43188977237e-10) A[2]:(1.15593112859e-09) A[3]:(4.2174485955e-18)\n",
      " state (5)  A[0]:(0.999916136265) A[1]:(6.73137847684e-08) A[2]:(8.37891566334e-05) A[3]:(6.2340220488e-21)\n",
      " state (6)  A[0]:(2.58143373344e-08) A[1]:(7.26546323104e-10) A[2]:(1.0) A[3]:(6.30481033094e-29)\n",
      " state (7)  A[0]:(1.56029730802e-10) A[1]:(4.04839807511e-11) A[2]:(1.0) A[3]:(1.17836885769e-30)\n",
      " state (8)  A[0]:(8.61135815433e-11) A[1]:(3.04440708754e-11) A[2]:(1.0) A[3]:(8.24846266763e-31)\n",
      " state (9)  A[0]:(7.64978566603e-11) A[1]:(2.8850267661e-11) A[2]:(1.0) A[3]:(7.72575042247e-31)\n",
      " state (10)  A[0]:(7.41504843638e-11) A[1]:(2.84614467411e-11) A[2]:(1.0) A[3]:(7.60063691634e-31)\n",
      " state (11)  A[0]:(7.33988009261e-11) A[1]:(2.83418358227e-11) A[2]:(1.0) A[3]:(7.56251845573e-31)\n",
      " state (12)  A[0]:(7.31065069592e-11) A[1]:(2.82975431282e-11) A[2]:(1.0) A[3]:(7.54845343073e-31)\n",
      " state (13)  A[0]:(7.2976111265e-11) A[1]:(2.82787664813e-11) A[2]:(1.0) A[3]:(7.54240903877e-31)\n",
      " state (14)  A[0]:(7.29129465138e-11) A[1]:(2.82699211263e-11) A[2]:(1.0) A[3]:(7.53958973312e-31)\n",
      " state (15)  A[0]:(7.2880687596e-11) A[1]:(2.82656086037e-11) A[2]:(1.0) A[3]:(7.53820970276e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 831000 finished after 8 . Running score: 0.1. Policy_loss: -92050.6111932, Value_loss: 1.41176599626. Times trained:               15359. Times reached goal: 104.               Steps done: 10954577.\n",
      " state (0)  A[0]:(0.991892278194) A[1]:(0.00379435135983) A[2]:(0.00196684710681) A[3]:(0.00234653940424)\n",
      " state (1)  A[0]:(4.55969620816e-06) A[1]:(1.27863677335e-06) A[2]:(6.14260352449e-06) A[3]:(0.999988019466)\n",
      " state (2)  A[0]:(0.999998927116) A[1]:(7.76110198331e-08) A[2]:(1.12056135038e-07) A[3]:(9.03782904516e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.70733990781e-10) A[2]:(3.30238086876e-10) A[3]:(7.56094351625e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.75018857582e-10) A[2]:(1.52134815856e-09) A[3]:(9.07665286154e-19)\n",
      " state (5)  A[0]:(0.051431376487) A[1]:(2.16937519326e-06) A[2]:(0.948566436768) A[3]:(1.72982886578e-23)\n",
      " state (6)  A[0]:(2.4848498259e-10) A[1]:(5.10977371526e-11) A[2]:(1.0) A[3]:(1.20865297759e-30)\n",
      " state (7)  A[0]:(4.24126081477e-11) A[1]:(2.14027632423e-11) A[2]:(1.0) A[3]:(4.00253569506e-31)\n",
      " state (8)  A[0]:(3.37226913061e-11) A[1]:(1.93656809339e-11) A[2]:(1.0) A[3]:(3.55615558617e-31)\n",
      " state (9)  A[0]:(3.21402626735e-11) A[1]:(1.89797448591e-11) A[2]:(1.0) A[3]:(3.47406893486e-31)\n",
      " state (10)  A[0]:(3.17118206694e-11) A[1]:(1.88780761856e-11) A[2]:(1.0) A[3]:(3.45274499714e-31)\n",
      " state (11)  A[0]:(3.1563973657e-11) A[1]:(1.88446185739e-11) A[2]:(1.0) A[3]:(3.44577125935e-31)\n",
      " state (12)  A[0]:(3.15039487553e-11) A[1]:(1.88316844757e-11) A[2]:(1.0) A[3]:(3.44306456606e-31)\n",
      " state (13)  A[0]:(3.14771611554e-11) A[1]:(1.88261541773e-11) A[2]:(1.0) A[3]:(3.44193515109e-31)\n",
      " state (14)  A[0]:(3.14643137933e-11) A[1]:(1.88236405629e-11) A[2]:(1.0) A[3]:(3.44138384424e-31)\n",
      " state (15)  A[0]:(3.14578363358e-11) A[1]:(1.88222753356e-11) A[2]:(1.0) A[3]:(3.4411212388e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 832000 finished after 22 . Running score: 0.12. Policy_loss: -92050.6112012, Value_loss: 0.991960538706. Times trained:               15945. Times reached goal: 95.               Steps done: 10970522.\n",
      " state (0)  A[0]:(0.995180785656) A[1]:(0.00247647077776) A[2]:(0.00114981783554) A[3]:(0.00119294703472)\n",
      " state (1)  A[0]:(6.14718237557e-06) A[1]:(1.49550703554e-06) A[2]:(6.94802201906e-06) A[3]:(0.999985396862)\n",
      " state (2)  A[0]:(1.0) A[1]:(6.08138339686e-09) A[2]:(6.8734546943e-09) A[3]:(8.13260836452e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.62267790818e-10) A[2]:(3.07745301464e-10) A[3]:(1.23003170149e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.8966885274e-10) A[2]:(1.03979702626e-09) A[3]:(5.37146532868e-19)\n",
      " state (5)  A[0]:(0.993008971214) A[1]:(5.39537950317e-07) A[2]:(0.00699049606919) A[3]:(3.21863276779e-22)\n",
      " state (6)  A[0]:(3.05427576563e-08) A[1]:(8.76213268608e-10) A[2]:(0.999999940395) A[3]:(3.3024618906e-29)\n",
      " state (7)  A[0]:(9.49036127462e-11) A[1]:(4.01318665799e-11) A[2]:(1.0) A[3]:(5.35020517364e-31)\n",
      " state (8)  A[0]:(4.66220256845e-11) A[1]:(2.95307146791e-11) A[2]:(1.0) A[3]:(3.73048868646e-31)\n",
      " state (9)  A[0]:(4.1411159224e-11) A[1]:(2.81359779225e-11) A[2]:(1.0) A[3]:(3.52990844301e-31)\n",
      " state (10)  A[0]:(4.0395433526e-11) A[1]:(2.78579798119e-11) A[2]:(1.0) A[3]:(3.49014146912e-31)\n",
      " state (11)  A[0]:(4.01274291573e-11) A[1]:(2.77862333836e-11) A[2]:(1.0) A[3]:(3.4798250956e-31)\n",
      " state (12)  A[0]:(4.00358426966e-11) A[1]:(2.7762712268e-11) A[2]:(1.0) A[3]:(3.47642862222e-31)\n",
      " state (13)  A[0]:(3.99981367472e-11) A[1]:(2.77533950682e-11) A[2]:(1.0) A[3]:(3.47507609842e-31)\n",
      " state (14)  A[0]:(3.99807444096e-11) A[1]:(2.77492664263e-11) A[2]:(1.0) A[3]:(3.47446648705e-31)\n",
      " state (15)  A[0]:(3.99722061006e-11) A[1]:(2.77472541471e-11) A[2]:(1.0) A[3]:(3.47417496445e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 833000 finished after 22 . Running score: 0.1. Policy_loss: -92050.6111965, Value_loss: 1.20862988009. Times trained:               15394. Times reached goal: 119.               Steps done: 10985916.\n",
      " state (0)  A[0]:(0.996046185493) A[1]:(0.00179583230056) A[2]:(0.000894096156117) A[3]:(0.00126386806369)\n",
      " state (1)  A[0]:(6.20181981503e-06) A[1]:(1.33950686632e-06) A[2]:(6.24786298431e-06) A[3]:(0.999986231327)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.42016931337e-10) A[2]:(4.71822303449e-10) A[3]:(1.04261647926e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.82195655732e-10) A[2]:(3.24903104421e-10) A[3]:(1.17734504939e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.14609707677e-10) A[2]:(9.95735938147e-10) A[3]:(3.56018147902e-19)\n",
      " state (5)  A[0]:(0.83255982399) A[1]:(3.06753850055e-06) A[2]:(0.167437136173) A[3]:(1.10846304159e-22)\n",
      " state (6)  A[0]:(1.98333527379e-08) A[1]:(8.26491319827e-10) A[2]:(1.0) A[3]:(2.09850018872e-29)\n",
      " state (7)  A[0]:(1.28472441019e-10) A[1]:(5.84539222581e-11) A[2]:(1.0) A[3]:(6.37878247892e-31)\n",
      " state (8)  A[0]:(6.58939777742e-11) A[1]:(4.37602627745e-11) A[2]:(1.0) A[3]:(4.53114374968e-31)\n",
      " state (9)  A[0]:(5.89825410735e-11) A[1]:(4.18046292028e-11) A[2]:(1.0) A[3]:(4.29835496101e-31)\n",
      " state (10)  A[0]:(5.76674472064e-11) A[1]:(4.14256962067e-11) A[2]:(1.0) A[3]:(4.25346659334e-31)\n",
      " state (11)  A[0]:(5.73298838957e-11) A[1]:(4.13306715241e-11) A[2]:(1.0) A[3]:(4.24218843034e-31)\n",
      " state (12)  A[0]:(5.72167105362e-11) A[1]:(4.13000987576e-11) A[2]:(1.0) A[3]:(4.23853264291e-31)\n",
      " state (13)  A[0]:(5.71706744446e-11) A[1]:(4.12882818213e-11) A[2]:(1.0) A[3]:(4.2370778511e-31)\n",
      " state (14)  A[0]:(5.71497432711e-11) A[1]:(4.12829284646e-11) A[2]:(1.0) A[3]:(4.23643132921e-31)\n",
      " state (15)  A[0]:(5.71399351446e-11) A[1]:(4.12805657712e-11) A[2]:(1.0) A[3]:(4.2361402768e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 834000 finished after 64 . Running score: 0.09. Policy_loss: -92050.6125142, Value_loss: 1.00029896134. Times trained:               15320. Times reached goal: 113.               Steps done: 11001236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.9252e-10,  7.0937e-10,  5.3364e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.9253e-10,  7.0935e-10,  5.3377e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.9255e-10,  7.0933e-10,  5.3413e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.9257e-10,  7.0934e-10,  5.3449e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.9267e-10,  7.0949e-10,  5.3546e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9787e-11,  4.8103e-11,  1.0000e+00,  6.9493e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.9269e-10,  7.0952e-10,  5.3572e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9783e-11,  4.8104e-11,  1.0000e+00,  6.9506e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.9271e-10,  7.0953e-10,  5.3597e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0012,  0.0011,  0.0033]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.9273e-10,  7.0955e-10,  5.3620e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.9273e-10,  7.0956e-10,  5.3630e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9775e-11,  4.8106e-11,  1.0000e+00,  6.9531e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.4938e-11,  4.2659e-11,  1.0000e+00,  6.0467e-31]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994408130646) A[1]:(0.00117822329048) A[2]:(0.00106406654231) A[3]:(0.00334957707673)\n",
      " state (1)  A[0]:(4.12735244026e-06) A[1]:(7.84101814588e-07) A[2]:(4.25666303272e-06) A[3]:(0.999990820885)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.38475117326e-10) A[2]:(4.18818257852e-10) A[3]:(1.53610707487e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.2663081134e-10) A[2]:(2.58176091616e-10) A[3]:(8.0798947177e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.92749990916e-10) A[2]:(7.09567016255e-10) A[3]:(5.36605815241e-19)\n",
      " state (5)  A[0]:(0.999731063843) A[1]:(1.100598368e-07) A[2]:(0.000268841686193) A[3]:(9.51886066857e-22)\n",
      " state (6)  A[0]:(8.08371623862e-06) A[1]:(2.27326211188e-08) A[2]:(0.999991893768) A[3]:(3.62385250933e-27)\n",
      " state (7)  A[0]:(5.39123801158e-10) A[1]:(1.05150159413e-10) A[2]:(1.0) A[3]:(1.81152613219e-30)\n",
      " state (8)  A[0]:(9.97731550156e-11) A[1]:(4.81071051051e-11) A[2]:(1.0) A[3]:(6.95396126873e-31)\n",
      " state (9)  A[0]:(7.49375977938e-11) A[1]:(4.26592441927e-11) A[2]:(1.0) A[3]:(6.04696346337e-31)\n",
      " state (10)  A[0]:(7.09516542741e-11) A[1]:(4.171462481e-11) A[2]:(1.0) A[3]:(5.89279690906e-31)\n",
      " state (11)  A[0]:(7.01084607035e-11) A[1]:(4.15144446597e-11) A[2]:(1.0) A[3]:(5.86006832515e-31)\n",
      " state (12)  A[0]:(6.98860344595e-11) A[1]:(4.14631662338e-11) A[2]:(1.0) A[3]:(5.85157984534e-31)\n",
      " state (13)  A[0]:(6.98111637942e-11) A[1]:(4.14465614607e-11) A[2]:(1.0) A[3]:(5.84881273164e-31)\n",
      " state (14)  A[0]:(6.97805424554e-11) A[1]:(4.14400805338e-11) A[2]:(1.0) A[3]:(5.8476969524e-31)\n",
      " state (15)  A[0]:(6.97666993621e-11) A[1]:(4.14372321178e-11) A[2]:(1.0) A[3]:(5.84720653616e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 835000 finished after 28 . Running score: 0.16. Policy_loss: -92050.6111957, Value_loss: 1.41350486551. Times trained:               15232. Times reached goal: 112.               Steps done: 11016468.\n",
      " state (0)  A[0]:(0.997113585472) A[1]:(0.00132241088431) A[2]:(0.000526649178937) A[3]:(0.00103735469747)\n",
      " state (1)  A[0]:(5.5082673498e-06) A[1]:(1.07076937184e-06) A[2]:(4.75166689284e-06) A[3]:(0.999988675117)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.13761275242e-10) A[2]:(3.78833631132e-10) A[3]:(8.69283048321e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.1142089468e-10) A[2]:(2.67839250778e-10) A[3]:(1.20641030735e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.43251613297e-10) A[2]:(6.6218058814e-10) A[3]:(3.36304443501e-19)\n",
      " state (5)  A[0]:(0.999879837036) A[1]:(6.10730808148e-08) A[2]:(0.000120128912386) A[3]:(8.94360966497e-22)\n",
      " state (6)  A[0]:(4.03944477512e-06) A[1]:(1.12369082927e-08) A[2]:(0.999995946884) A[3]:(1.56575687437e-27)\n",
      " state (7)  A[0]:(3.34817257253e-10) A[1]:(5.8830974814e-11) A[2]:(1.0) A[3]:(1.04664860311e-30)\n",
      " state (8)  A[0]:(7.58762011555e-11) A[1]:(2.97749325195e-11) A[2]:(1.0) A[3]:(4.6289806147e-31)\n",
      " state (9)  A[0]:(5.93096960433e-11) A[1]:(2.6848049825e-11) A[2]:(1.0) A[3]:(4.11339562689e-31)\n",
      " state (10)  A[0]:(5.65468817604e-11) A[1]:(2.63279571755e-11) A[2]:(1.0) A[3]:(4.02320135594e-31)\n",
      " state (11)  A[0]:(5.59318112658e-11) A[1]:(2.62133127704e-11) A[2]:(1.0) A[3]:(4.00326826314e-31)\n",
      " state (12)  A[0]:(5.57554245828e-11) A[1]:(2.61817321295e-11) A[2]:(1.0) A[3]:(3.99771381723e-31)\n",
      " state (13)  A[0]:(5.56903793913e-11) A[1]:(2.61705483673e-11) A[2]:(1.0) A[3]:(3.99573146356e-31)\n",
      " state (14)  A[0]:(5.5661919518e-11) A[1]:(2.6165857675e-11) A[2]:(1.0) A[3]:(3.99490861751e-31)\n",
      " state (15)  A[0]:(5.56481215275e-11) A[1]:(2.61637621291e-11) A[2]:(1.0) A[3]:(3.99451224082e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 836000 finished after 7 . Running score: 0.14. Policy_loss: -92050.6125387, Value_loss: 1.41671660286. Times trained:               15047. Times reached goal: 119.               Steps done: 11031515.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.98924934864) A[1]:(0.00102264143061) A[2]:(0.00827491190284) A[3]:(0.00145309511572)\n",
      " state (1)  A[0]:(4.93306515637e-06) A[1]:(8.853781992e-07) A[2]:(1.21957918964e-05) A[3]:(0.999981999397)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.31747113219e-09) A[2]:(3.01734970343e-09) A[3]:(1.1339561512e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.72911546192e-10) A[2]:(3.9603784141e-10) A[3]:(1.72885225016e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.7744064551e-10) A[2]:(9.35264532487e-10) A[3]:(6.23279143993e-19)\n",
      " state (5)  A[0]:(0.999999344349) A[1]:(3.44708284139e-09) A[2]:(6.27919462204e-07) A[3]:(1.27650027453e-20)\n",
      " state (6)  A[0]:(7.48452339394e-06) A[1]:(1.21469865277e-08) A[2]:(0.999992489815) A[3]:(3.57665181826e-27)\n",
      " state (7)  A[0]:(1.97505081734e-10) A[1]:(3.12914728851e-11) A[2]:(1.0) A[3]:(8.02438241131e-31)\n",
      " state (8)  A[0]:(4.54354817658e-11) A[1]:(1.58097285263e-11) A[2]:(1.0) A[3]:(3.50810795987e-31)\n",
      " state (9)  A[0]:(3.57528937012e-11) A[1]:(1.42658012139e-11) A[2]:(1.0) A[3]:(3.11470550058e-31)\n",
      " state (10)  A[0]:(3.40168830604e-11) A[1]:(1.3972435188e-11) A[2]:(1.0) A[3]:(3.04109134236e-31)\n",
      " state (11)  A[0]:(3.35734114121e-11) A[1]:(1.38988083195e-11) A[2]:(1.0) A[3]:(3.02258647519e-31)\n",
      " state (12)  A[0]:(3.34221157694e-11) A[1]:(1.38748118897e-11) A[2]:(1.0) A[3]:(3.01650470252e-31)\n",
      " state (13)  A[0]:(3.33574105837e-11) A[1]:(1.38651286632e-11) A[2]:(1.0) A[3]:(3.01402017766e-31)\n",
      " state (14)  A[0]:(3.33253598328e-11) A[1]:(1.38605281766e-11) A[2]:(1.0) A[3]:(3.01280166021e-31)\n",
      " state (15)  A[0]:(3.33082034176e-11) A[1]:(1.38580431852e-11) A[2]:(1.0) A[3]:(3.01218099919e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 837000 finished after 23 . Running score: 0.1. Policy_loss: -92050.6112031, Value_loss: 1.40873163694. Times trained:               15607. Times reached goal: 109.               Steps done: 11047122.\n",
      " state (0)  A[0]:(0.995323896408) A[1]:(0.00114663201384) A[2]:(0.00274295732379) A[3]:(0.000786521239206)\n",
      " state (1)  A[0]:(7.30410420147e-06) A[1]:(1.25796668726e-06) A[2]:(1.25400256366e-05) A[3]:(0.999978899956)\n",
      " state (2)  A[0]:(0.999999463558) A[1]:(4.14209964106e-08) A[2]:(9.71287192897e-08) A[3]:(3.73205523374e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.53902807165e-10) A[2]:(2.16506854112e-10) A[3]:(2.01072145045e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.63058053171e-10) A[2]:(3.72526676173e-10) A[3]:(1.11468252074e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.61984578528e-10) A[2]:(6.743168246e-10) A[3]:(1.86606970184e-19)\n",
      " state (6)  A[0]:(0.999993085861) A[1]:(1.17858744986e-08) A[2]:(6.92374715072e-06) A[3]:(2.07417255159e-21)\n",
      " state (7)  A[0]:(7.19217568985e-05) A[1]:(5.07448909559e-08) A[2]:(0.999928057194) A[3]:(1.71704080928e-26)\n",
      " state (8)  A[0]:(1.56222768055e-09) A[1]:(1.00010742743e-10) A[2]:(1.0) A[3]:(2.16213302693e-30)\n",
      " state (9)  A[0]:(1.90110580056e-10) A[1]:(3.47279253965e-11) A[2]:(1.0) A[3]:(5.63922867166e-31)\n",
      " state (10)  A[0]:(1.24144125535e-10) A[1]:(2.85393982752e-11) A[2]:(1.0) A[3]:(4.44815008693e-31)\n",
      " state (11)  A[0]:(1.11698442162e-10) A[1]:(2.72253174866e-11) A[2]:(1.0) A[3]:(4.20413673781e-31)\n",
      " state (12)  A[0]:(1.08214770478e-10) A[1]:(2.68553217858e-11) A[2]:(1.0) A[3]:(4.13561152972e-31)\n",
      " state (13)  A[0]:(1.06940158118e-10) A[1]:(2.67246138413e-11) A[2]:(1.0) A[3]:(4.11119933324e-31)\n",
      " state (14)  A[0]:(1.06353925666e-10) A[1]:(2.66672794957e-11) A[2]:(1.0) A[3]:(4.10036127533e-31)\n",
      " state (15)  A[0]:(1.06037942316e-10) A[1]:(2.66378967495e-11) A[2]:(1.0) A[3]:(4.09470291572e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 838000 finished after 14 . Running score: 0.1. Policy_loss: -92050.6111917, Value_loss: 1.21059944552. Times trained:               16644. Times reached goal: 99.               Steps done: 11063766.\n",
      " state (0)  A[0]:(0.997024595737) A[1]:(0.000797733315267) A[2]:(0.00142721657176) A[3]:(0.000750468170736)\n",
      " state (1)  A[0]:(6.60391515339e-06) A[1]:(9.77990566753e-07) A[2]:(8.71592055773e-06) A[3]:(0.999983727932)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.36654423888e-10) A[2]:(2.14351300598e-10) A[3]:(1.49399416449e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.50127896226e-10) A[2]:(3.18615855921e-10) A[3]:(1.04062961183e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.48331028016e-10) A[2]:(7.24957205378e-10) A[3]:(4.20119656442e-19)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(8.85950146579e-10) A[2]:(3.59524854332e-08) A[3]:(3.64971247122e-20)\n",
      " state (6)  A[0]:(7.6276010077e-06) A[1]:(1.08961737411e-08) A[2]:(0.999992370605) A[3]:(3.46465282901e-27)\n",
      " state (7)  A[0]:(1.1096033814e-10) A[1]:(2.18176952199e-11) A[2]:(1.0) A[3]:(4.93580203758e-31)\n",
      " state (8)  A[0]:(2.83760202835e-11) A[1]:(1.17537229949e-11) A[2]:(1.0) A[3]:(2.35029586527e-31)\n",
      " state (9)  A[0]:(2.30780793781e-11) A[1]:(1.07736510685e-11) A[2]:(1.0) A[3]:(2.12780746341e-31)\n",
      " state (10)  A[0]:(2.21866067185e-11) A[1]:(1.05993000835e-11) A[2]:(1.0) A[3]:(2.08875448479e-31)\n",
      " state (11)  A[0]:(2.1993636079e-11) A[1]:(1.05618847002e-11) A[2]:(1.0) A[3]:(2.08037297497e-31)\n",
      " state (12)  A[0]:(2.19426855158e-11) A[1]:(1.05524209162e-11) A[2]:(1.0) A[3]:(2.07826319771e-31)\n",
      " state (13)  A[0]:(2.19258673717e-11) A[1]:(1.05495230607e-11) A[2]:(1.0) A[3]:(2.07759739771e-31)\n",
      " state (14)  A[0]:(2.19190932765e-11) A[1]:(1.05483963578e-11) A[2]:(1.0) A[3]:(2.07735947765e-31)\n",
      " state (15)  A[0]:(2.19160835313e-11) A[1]:(1.05479939019e-11) A[2]:(1.0) A[3]:(2.07724874608e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 839000 finished after 52 . Running score: 0.16. Policy_loss: -92050.6124636, Value_loss: 1.42254042849. Times trained:               15727. Times reached goal: 114.               Steps done: 11079493.\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1922e-10,  5.8202e-10,  2.0373e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9980,  0.0006,  0.0008,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1922e-10,  5.8204e-10,  2.0374e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1922e-10,  5.8205e-10,  2.0375e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6943e-11,  8.9155e-12,  1.0000e+00,  1.5293e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1923e-10,  5.8206e-10,  2.0377e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.1923e-10,  5.8207e-10,  2.0377e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6943e-11,  8.9157e-12,  1.0000e+00,  1.5293e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.997975289822) A[1]:(0.000626809662208) A[2]:(0.000810350931715) A[3]:(0.000587520597037)\n",
      " state (1)  A[0]:(8.48473791848e-06) A[1]:(1.14652925731e-06) A[2]:(8.66946356837e-06) A[3]:(0.999981701374)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.26962620972e-10) A[2]:(1.89204416023e-10) A[3]:(8.97841957032e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.28638683039e-10) A[2]:(2.80910017469e-10) A[3]:(1.87281991605e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.19234400131e-10) A[2]:(5.82065229349e-10) A[3]:(2.0378606394e-19)\n",
      " state (5)  A[0]:(0.999998509884) A[1]:(4.61071492097e-09) A[2]:(1.4596823803e-06) A[3]:(5.94878569745e-21)\n",
      " state (6)  A[0]:(1.9721392519e-08) A[1]:(3.41489336808e-10) A[2]:(1.0) A[3]:(2.01686107162e-29)\n",
      " state (7)  A[0]:(3.1054950278e-11) A[1]:(1.1628315498e-11) A[2]:(1.0) A[3]:(2.08722516664e-31)\n",
      " state (8)  A[0]:(1.69429973523e-11) A[1]:(8.91580732854e-12) A[2]:(1.0) A[3]:(1.52935506094e-31)\n",
      " state (9)  A[0]:(1.54638350736e-11) A[1]:(8.57738879922e-12) A[2]:(1.0) A[3]:(1.46332331148e-31)\n",
      " state (10)  A[0]:(1.51918928826e-11) A[1]:(8.51417807785e-12) A[2]:(1.0) A[3]:(1.45103869021e-31)\n",
      " state (11)  A[0]:(1.51290282385e-11) A[1]:(8.4999316613e-12) A[2]:(1.0) A[3]:(1.44828474205e-31)\n",
      " state (12)  A[0]:(1.5110801499e-11) A[1]:(8.49600945152e-12) A[2]:(1.0) A[3]:(1.44753348361e-31)\n",
      " state (13)  A[0]:(1.5104289347e-11) A[1]:(8.49477779785e-12) A[2]:(1.0) A[3]:(1.44729062648e-31)\n",
      " state (14)  A[0]:(1.51014097061e-11) A[1]:(8.49422702315e-12) A[2]:(1.0) A[3]:(1.44718013001e-31)\n",
      " state (15)  A[0]:(1.51001416232e-11) A[1]:(8.49396768199e-12) A[2]:(1.0) A[3]:(1.44713604897e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 840000 finished after 15 . Running score: 0.12. Policy_loss: -92050.6111878, Value_loss: 1.6287634565. Times trained:               15617. Times reached goal: 123.               Steps done: 11095110.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997789204121) A[1]:(0.00109377969056) A[2]:(0.000566355476622) A[3]:(0.000550641969312)\n",
      " state (1)  A[0]:(7.47528792999e-06) A[1]:(1.3219404309e-06) A[2]:(6.91287868904e-06) A[3]:(0.999984264374)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.41204908877e-10) A[2]:(1.8968246418e-10) A[3]:(9.0509266748e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.16943135731e-10) A[2]:(2.9610480734e-10) A[3]:(2.41122301526e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.02519416567e-10) A[2]:(5.78113057426e-10) A[3]:(1.29717261664e-19)\n",
      " state (5)  A[0]:(0.870999991894) A[1]:(8.2357479414e-07) A[2]:(0.128999188542) A[3]:(1.60024334752e-22)\n",
      " state (6)  A[0]:(2.68450928154e-10) A[1]:(2.55858702281e-11) A[2]:(1.0) A[3]:(7.5680597361e-31)\n",
      " state (7)  A[0]:(1.81112208564e-11) A[1]:(7.06523641675e-12) A[2]:(1.0) A[3]:(1.51152598548e-31)\n",
      " state (8)  A[0]:(1.39659872209e-11) A[1]:(6.32420765062e-12) A[2]:(1.0) A[3]:(1.332320226e-31)\n",
      " state (9)  A[0]:(1.34268091437e-11) A[1]:(6.22062861291e-12) A[2]:(1.0) A[3]:(1.3077674404e-31)\n",
      " state (10)  A[0]:(1.33296412103e-11) A[1]:(6.20198120291e-12) A[2]:(1.0) A[3]:(1.30335498725e-31)\n",
      " state (11)  A[0]:(1.33082008955e-11) A[1]:(6.19798396634e-12) A[2]:(1.0) A[3]:(1.30241059509e-31)\n",
      " state (12)  A[0]:(1.33024147253e-11) A[1]:(6.19699127083e-12) A[2]:(1.0) A[3]:(1.30217220484e-31)\n",
      " state (13)  A[0]:(1.33004354058e-11) A[1]:(6.19668379109e-12) A[2]:(1.0) A[3]:(1.30210261557e-31)\n",
      " state (14)  A[0]:(1.32996235552e-11) A[1]:(6.19654197745e-12) A[2]:(1.0) A[3]:(1.30207275801e-31)\n",
      " state (15)  A[0]:(1.32992688043e-11) A[1]:(6.19647085379e-12) A[2]:(1.0) A[3]:(1.30205289216e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 841000 finished after 25 . Running score: 0.07. Policy_loss: -92050.6111888, Value_loss: 1.6277885024. Times trained:               15411. Times reached goal: 85.               Steps done: 11110521.\n",
      " state (0)  A[0]:(0.99640494585) A[1]:(0.00180654693395) A[2]:(0.000694713322446) A[3]:(0.00109378423076)\n",
      " state (1)  A[0]:(4.05541914006e-06) A[1]:(9.39872734307e-07) A[2]:(4.36550089944e-06) A[3]:(0.999990642071)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.80204684508e-10) A[2]:(2.30183039296e-10) A[3]:(2.1561002983e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.57622234709e-10) A[2]:(3.60305063563e-10) A[3]:(1.02667084768e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.40121539371e-10) A[2]:(7.27668258982e-10) A[3]:(2.87177308042e-19)\n",
      " state (5)  A[0]:(0.998045504093) A[1]:(1.60239480351e-07) A[2]:(0.00195431360044) A[3]:(1.02672309212e-21)\n",
      " state (6)  A[0]:(3.13157305643e-10) A[1]:(3.61689532491e-11) A[2]:(1.0) A[3]:(1.60677307187e-30)\n",
      " state (7)  A[0]:(1.19099331092e-11) A[1]:(7.21434890993e-12) A[2]:(1.0) A[3]:(2.06304994982e-31)\n",
      " state (8)  A[0]:(8.95237616677e-12) A[1]:(6.3706002279e-12) A[2]:(1.0) A[3]:(1.78777903041e-31)\n",
      " state (9)  A[0]:(8.57326622888e-12) A[1]:(6.25308008498e-12) A[2]:(1.0) A[3]:(1.75034012308e-31)\n",
      " state (10)  A[0]:(8.50544547987e-12) A[1]:(6.23174385359e-12) A[2]:(1.0) A[3]:(1.743542827e-31)\n",
      " state (11)  A[0]:(8.49160238653e-12) A[1]:(6.22739533551e-12) A[2]:(1.0) A[3]:(1.74215997545e-31)\n",
      " state (12)  A[0]:(8.48842870993e-12) A[1]:(6.22639743583e-12) A[2]:(1.0) A[3]:(1.74184094628e-31)\n",
      " state (13)  A[0]:(8.48758650168e-12) A[1]:(6.22613635995e-12) A[2]:(1.0) A[3]:(1.74174796468e-31)\n",
      " state (14)  A[0]:(8.48732802788e-12) A[1]:(6.22606480261e-12) A[2]:(1.0) A[3]:(1.74173468159e-31)\n",
      " state (15)  A[0]:(8.48726297575e-12) A[1]:(6.22601753139e-12) A[2]:(1.0) A[3]:(1.7417213985e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 842000 finished after 17 . Running score: 0.11. Policy_loss: -92050.611189, Value_loss: 2.05267749539. Times trained:               15735. Times reached goal: 109.               Steps done: 11126256.\n",
      " state (0)  A[0]:(0.993044614792) A[1]:(0.00146405503619) A[2]:(0.00242544873618) A[3]:(0.00306589063257)\n",
      " state (1)  A[0]:(2.76158107226e-06) A[1]:(5.93711718011e-07) A[2]:(4.12442386732e-06) A[3]:(0.999992549419)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.72339295612e-10) A[2]:(2.69768013483e-10) A[3]:(3.53205242831e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.49939879956e-10) A[2]:(4.13879652772e-10) A[3]:(7.13703343912e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.37523048505e-10) A[2]:(8.43583647381e-10) A[3]:(3.50815621289e-19)\n",
      " state (5)  A[0]:(0.339159816504) A[1]:(1.65436711086e-06) A[2]:(0.660838484764) A[3]:(1.52728633557e-22)\n",
      " state (6)  A[0]:(7.40648306574e-11) A[1]:(1.65610598152e-11) A[2]:(1.0) A[3]:(8.07936263309e-31)\n",
      " state (7)  A[0]:(1.05729296845e-11) A[1]:(6.64122749849e-12) A[2]:(1.0) A[3]:(2.63628635276e-31)\n",
      " state (8)  A[0]:(8.79064876447e-12) A[1]:(6.13388940335e-12) A[2]:(1.0) A[3]:(2.40812525026e-31)\n",
      " state (9)  A[0]:(8.54926286015e-12) A[1]:(6.06124829147e-12) A[2]:(1.0) A[3]:(2.37587885413e-31)\n",
      " state (10)  A[0]:(8.50732765484e-12) A[1]:(6.0484985076e-12) A[2]:(1.0) A[3]:(2.37021203096e-31)\n",
      " state (11)  A[0]:(8.49912154544e-12) A[1]:(6.04600701101e-12) A[2]:(1.0) A[3]:(2.36910918216e-31)\n",
      " state (12)  A[0]:(8.49737034209e-12) A[1]:(6.0454996044e-12) A[2]:(1.0) A[3]:(2.36887408329e-31)\n",
      " state (13)  A[0]:(8.49688461951e-12) A[1]:(6.0453612602e-12) A[2]:(1.0) A[3]:(2.36882001055e-31)\n",
      " state (14)  A[0]:(8.49672242287e-12) A[1]:(6.04529187126e-12) A[2]:(1.0) A[3]:(2.36878380533e-31)\n",
      " state (15)  A[0]:(8.49669033048e-12) A[1]:(6.04531485635e-12) A[2]:(1.0) A[3]:(2.36880190794e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 843000 finished after 6 . Running score: 0.12. Policy_loss: -92050.6114443, Value_loss: 1.40770391781. Times trained:               15763. Times reached goal: 126.               Steps done: 11142019.\n",
      " state (0)  A[0]:(0.994069814682) A[1]:(0.00108986883424) A[2]:(0.00198981049471) A[3]:(0.00285047851503)\n",
      " state (1)  A[0]:(2.79601022157e-06) A[1]:(5.73153613459e-07) A[2]:(3.99523469241e-06) A[3]:(0.999992609024)\n",
      " state (2)  A[0]:(0.000856766593643) A[1]:(6.02158388574e-06) A[2]:(2.90250027319e-05) A[3]:(0.999108195305)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.88721815952e-10) A[2]:(3.31822874733e-10) A[3]:(3.44923881124e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.68105765419e-10) A[2]:(4.87356988099e-10) A[3]:(3.83693219612e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.08739081575e-10) A[2]:(1.8235226662e-09) A[3]:(4.99483345217e-19)\n",
      " state (6)  A[0]:(9.21865250803e-07) A[1]:(4.17453493995e-09) A[2]:(0.999999046326) A[3]:(3.11204856738e-27)\n",
      " state (7)  A[0]:(1.01994315771e-11) A[1]:(6.59829873031e-12) A[2]:(1.0) A[3]:(3.58117692382e-31)\n",
      " state (8)  A[0]:(4.50846304734e-12) A[1]:(4.49285660759e-12) A[2]:(1.0) A[3]:(2.26621463515e-31)\n",
      " state (9)  A[0]:(3.85285206517e-12) A[1]:(4.19430983001e-12) A[2]:(1.0) A[3]:(2.09881836212e-31)\n",
      " state (10)  A[0]:(3.66075702737e-12) A[1]:(4.10453225227e-12) A[2]:(1.0) A[3]:(2.05041808752e-31)\n",
      " state (11)  A[0]:(3.58417722585e-12) A[1]:(4.06869286526e-12) A[2]:(1.0) A[3]:(2.03156127735e-31)\n",
      " state (12)  A[0]:(3.54914275053e-12) A[1]:(4.05233615761e-12) A[2]:(1.0) A[3]:(2.02308525778e-31)\n",
      " state (13)  A[0]:(3.53197636069e-12) A[1]:(4.04436727164e-12) A[2]:(1.0) A[3]:(2.01899900432e-31)\n",
      " state (14)  A[0]:(3.52324289535e-12) A[1]:(4.04031192183e-12) A[2]:(1.0) A[3]:(2.0169207303e-31)\n",
      " state (15)  A[0]:(3.51871656812e-12) A[1]:(4.03821637587e-12) A[2]:(1.0) A[3]:(2.01585902381e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 844000 finished after 25 . Running score: 0.19. Policy_loss: -92050.6112139, Value_loss: 0.986744200266. Times trained:               16168. Times reached goal: 129.               Steps done: 11158187.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0011,  0.0018]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0011,  0.0018]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0011,  0.0018]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0011,  0.0018]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0011,  0.0018]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.4932e-10,  4.8921e-10,  3.9385e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7854e-12,  3.1043e-12,  1.0000e+00,  1.8705e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996290802956) A[1]:(0.000774535757955) A[2]:(0.0011202179594) A[3]:(0.00181442697067)\n",
      " state (1)  A[0]:(3.35447543875e-06) A[1]:(5.9274071873e-07) A[2]:(3.80399774258e-06) A[3]:(0.999992251396)\n",
      " state (2)  A[0]:(0.00015561276814) A[1]:(2.76663354271e-06) A[2]:(1.43686984302e-05) A[3]:(0.999827265739)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.7563919763e-10) A[2]:(3.28124694082e-10) A[3]:(3.95057632441e-11)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.49326842558e-10) A[2]:(4.89212170773e-10) A[3]:(3.93908648714e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.05941522116e-10) A[2]:(4.04044131486e-09) A[3]:(4.85067861887e-19)\n",
      " state (6)  A[0]:(1.91213391787e-08) A[1]:(4.48243553386e-10) A[2]:(1.0) A[3]:(1.89263675455e-28)\n",
      " state (7)  A[0]:(5.11302762282e-12) A[1]:(4.14701042603e-12) A[2]:(1.0) A[3]:(2.65522027537e-31)\n",
      " state (8)  A[0]:(2.78542466869e-12) A[1]:(3.10426163941e-12) A[2]:(1.0) A[3]:(1.87054840884e-31)\n",
      " state (9)  A[0]:(2.46776817731e-12) A[1]:(2.94334175088e-12) A[2]:(1.0) A[3]:(1.76149944361e-31)\n",
      " state (10)  A[0]:(2.37496849444e-12) A[1]:(2.89600765244e-12) A[2]:(1.0) A[3]:(1.73072688229e-31)\n",
      " state (11)  A[0]:(2.33951746864e-12) A[1]:(2.87806735909e-12) A[2]:(1.0) A[3]:(1.71935585524e-31)\n",
      " state (12)  A[0]:(2.3240977285e-12) A[1]:(2.87033743128e-12) A[2]:(1.0) A[3]:(1.71454843599e-31)\n",
      " state (13)  A[0]:(2.31687455679e-12) A[1]:(2.86672638752e-12) A[2]:(1.0) A[3]:(1.71233909436e-31)\n",
      " state (14)  A[0]:(2.31335957335e-12) A[1]:(2.86499925346e-12) A[2]:(1.0) A[3]:(1.71128126699e-31)\n",
      " state (15)  A[0]:(2.31159969638e-12) A[1]:(2.86412495283e-12) A[2]:(1.0) A[3]:(1.71074606442e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 845000 finished after 7 . Running score: 0.07. Policy_loss: -92050.6112081, Value_loss: 1.43118668992. Times trained:               15684. Times reached goal: 106.               Steps done: 11173871.\n",
      " state (0)  A[0]:(0.99679261446) A[1]:(0.000759285467211) A[2]:(0.00137678836472) A[3]:(0.00107130338438)\n",
      " state (1)  A[0]:(4.48764103567e-06) A[1]:(7.41799794923e-07) A[2]:(5.413861345e-06) A[3]:(0.999989330769)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(8.12397171757e-09) A[2]:(1.85523454377e-08) A[3]:(5.67180649114e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.41916103868e-10) A[2]:(3.3390337717e-10) A[3]:(1.01855850143e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.1406280781e-10) A[2]:(8.48578263213e-10) A[3]:(1.66989784836e-18)\n",
      " state (5)  A[0]:(0.375500619411) A[1]:(2.54224619312e-06) A[2]:(0.624496817589) A[3]:(7.90173670741e-22)\n",
      " state (6)  A[0]:(1.03716748731e-11) A[1]:(5.14943513177e-12) A[2]:(1.0) A[3]:(2.89036216863e-31)\n",
      " state (7)  A[0]:(3.18783324391e-12) A[1]:(2.9131484551e-12) A[2]:(1.0) A[3]:(1.43185944194e-31)\n",
      " state (8)  A[0]:(2.78713575656e-12) A[1]:(2.74717206449e-12) A[2]:(1.0) A[3]:(1.3398430372e-31)\n",
      " state (9)  A[0]:(2.70066347757e-12) A[1]:(2.71097489075e-12) A[2]:(1.0) A[3]:(1.32056093316e-31)\n",
      " state (10)  A[0]:(2.67441819558e-12) A[1]:(2.7001278817e-12) A[2]:(1.0) A[3]:(1.31493090297e-31)\n",
      " state (11)  A[0]:(2.66506890341e-12) A[1]:(2.69629891331e-12) A[2]:(1.0) A[3]:(1.31299615682e-31)\n",
      " state (12)  A[0]:(2.6614420303e-12) A[1]:(2.69484890132e-12) A[2]:(1.0) A[3]:(1.31226511688e-31)\n",
      " state (13)  A[0]:(2.65994995127e-12) A[1]:(2.69425280697e-12) A[2]:(1.0) A[3]:(1.31196477807e-31)\n",
      " state (14)  A[0]:(2.65932089717e-12) A[1]:(2.69400604255e-12) A[2]:(1.0) A[3]:(1.31184464255e-31)\n",
      " state (15)  A[0]:(2.65902686154e-12) A[1]:(2.69388287719e-12) A[2]:(1.0) A[3]:(1.31178457479e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 846000 finished after 34 . Running score: 0.16. Policy_loss: -92050.6112062, Value_loss: 1.66711910781. Times trained:               15973. Times reached goal: 121.               Steps done: 11189844.\n",
      " state (0)  A[0]:(0.996229350567) A[1]:(0.000820757355541) A[2]:(0.00143361615483) A[3]:(0.0015162929194)\n",
      " state (1)  A[0]:(3.82771258955e-06) A[1]:(6.4794642185e-07) A[2]:(4.5162855713e-06) A[3]:(0.999990999699)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.95204036163e-10) A[2]:(6.69459876423e-10) A[3]:(2.90515583812e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.26031782233e-10) A[2]:(2.81218270892e-10) A[3]:(6.56004317165e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.88278625602e-11) A[2]:(6.09287453823e-10) A[3]:(5.6712340317e-19)\n",
      " state (5)  A[0]:(0.331160753965) A[1]:(1.20628965306e-06) A[2]:(0.668838083744) A[3]:(1.61847524833e-22)\n",
      " state (6)  A[0]:(2.15726846109e-11) A[1]:(7.60556895241e-12) A[2]:(1.0) A[3]:(4.31534273517e-31)\n",
      " state (7)  A[0]:(5.27330045949e-12) A[1]:(4.03179746533e-12) A[2]:(1.0) A[3]:(2.04061752092e-31)\n",
      " state (8)  A[0]:(4.62759518205e-12) A[1]:(3.81854010217e-12) A[2]:(1.0) A[3]:(1.92278326356e-31)\n",
      " state (9)  A[0]:(4.51763886716e-12) A[1]:(3.78111387686e-12) A[2]:(1.0) A[3]:(1.90255817796e-31)\n",
      " state (10)  A[0]:(4.49042365791e-12) A[1]:(3.77187951012e-12) A[2]:(1.0) A[3]:(1.89765836485e-31)\n",
      " state (11)  A[0]:(4.48214035331e-12) A[1]:(3.76907446226e-12) A[2]:(1.0) A[3]:(1.89619663763e-31)\n",
      " state (12)  A[0]:(4.47923469149e-12) A[1]:(3.76811169073e-12) A[2]:(1.0) A[3]:(1.89569035221e-31)\n",
      " state (13)  A[0]:(4.47815829557e-12) A[1]:(3.76776648076e-12) A[2]:(1.0) A[3]:(1.89551684925e-31)\n",
      " state (14)  A[0]:(4.47773111992e-12) A[1]:(3.76760862092e-12) A[2]:(1.0) A[3]:(1.89544443879e-31)\n",
      " state (15)  A[0]:(4.47757759689e-12) A[1]:(3.76756525283e-12) A[2]:(1.0) A[3]:(1.89541552163e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 847000 finished after 23 . Running score: 0.08. Policy_loss: -92050.6115556, Value_loss: 1.66307900408. Times trained:               16029. Times reached goal: 121.               Steps done: 11205873.\n",
      " state (0)  A[0]:(0.996001601219) A[1]:(0.000709594169166) A[2]:(0.00142736290582) A[3]:(0.00186142628081)\n",
      " state (1)  A[0]:(3.79138396056e-06) A[1]:(6.01346982876e-07) A[2]:(4.12619783674e-06) A[3]:(0.999991476536)\n",
      " state (2)  A[0]:(0.999999821186) A[1]:(1.15981624305e-08) A[2]:(2.03361611995e-08) A[3]:(1.28878923533e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.19172102742e-10) A[2]:(1.84221776722e-10) A[3]:(2.45355211842e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.2576924465e-11) A[2]:(2.11780912251e-10) A[3]:(2.62908571265e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(6.41914854604e-11) A[2]:(3.17794096594e-10) A[3]:(4.67934159666e-19)\n",
      " state (6)  A[0]:(0.999999940395) A[1]:(5.55692325488e-10) A[2]:(3.81781930514e-08) A[3]:(3.63910517233e-20)\n",
      " state (7)  A[0]:(2.43543991019e-06) A[1]:(2.74629985419e-09) A[2]:(0.99999755621) A[3]:(1.95937506602e-27)\n",
      " state (8)  A[0]:(5.19616433214e-11) A[1]:(1.00383165322e-11) A[2]:(1.0) A[3]:(6.49363438956e-31)\n",
      " state (9)  A[0]:(1.52274009374e-11) A[1]:(5.82717771605e-12) A[2]:(1.0) A[3]:(3.44267054035e-31)\n",
      " state (10)  A[0]:(1.24655147316e-11) A[1]:(5.36073746157e-12) A[2]:(1.0) A[3]:(3.14162620201e-31)\n",
      " state (11)  A[0]:(1.18861500503e-11) A[1]:(5.25659030193e-12) A[2]:(1.0) A[3]:(3.07585917387e-31)\n",
      " state (12)  A[0]:(1.17214406584e-11) A[1]:(5.22647810447e-12) A[2]:(1.0) A[3]:(3.05707265825e-31)\n",
      " state (13)  A[0]:(1.16656814417e-11) A[1]:(5.2162402002e-12) A[2]:(1.0) A[3]:(3.05078188269e-31)\n",
      " state (14)  A[0]:(1.16442072998e-11) A[1]:(5.21230194422e-12) A[2]:(1.0) A[3]:(3.0483852848e-31)\n",
      " state (15)  A[0]:(1.1634971632e-11) A[1]:(5.21059237424e-12) A[2]:(1.0) A[3]:(3.04736213452e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 848000 finished after 11 . Running score: 0.13. Policy_loss: -92050.6112076, Value_loss: 1.41577702913. Times trained:               15693. Times reached goal: 119.               Steps done: 11221566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995839059353) A[1]:(0.000723982288036) A[2]:(0.00130571227055) A[3]:(0.00213125743903)\n",
      " state (1)  A[0]:(3.35301751875e-06) A[1]:(5.41818337751e-07) A[2]:(3.63940944226e-06) A[3]:(0.999992489815)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.79199044492e-10) A[2]:(2.77891099021e-10) A[3]:(7.2316007349e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.11504736e-10) A[2]:(2.40614445035e-10) A[3]:(3.63857503697e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.9849203638e-11) A[2]:(3.84559467603e-10) A[3]:(1.98355201874e-18)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.54821988563e-10) A[2]:(2.16478057702e-09) A[3]:(1.59765696014e-19)\n",
      " state (6)  A[0]:(8.26522409625e-06) A[1]:(4.74590411415e-09) A[2]:(0.999991714954) A[3]:(6.88337145848e-27)\n",
      " state (7)  A[0]:(2.01537987687e-11) A[1]:(6.27814076767e-12) A[2]:(1.0) A[3]:(3.88039689493e-31)\n",
      " state (8)  A[0]:(6.9356161439e-12) A[1]:(3.98841463328e-12) A[2]:(1.0) A[3]:(2.30404604004e-31)\n",
      " state (9)  A[0]:(6.07193982594e-12) A[1]:(3.78237588819e-12) A[2]:(1.0) A[3]:(2.17662245241e-31)\n",
      " state (10)  A[0]:(5.92020703294e-12) A[1]:(3.74471851097e-12) A[2]:(1.0) A[3]:(2.15366152126e-31)\n",
      " state (11)  A[0]:(5.88582177788e-12) A[1]:(3.73611471621e-12) A[2]:(1.0) A[3]:(2.14844279654e-31)\n",
      " state (12)  A[0]:(5.87680295053e-12) A[1]:(3.73385610625e-12) A[2]:(1.0) A[3]:(2.14708274957e-31)\n",
      " state (13)  A[0]:(5.87413581318e-12) A[1]:(3.73318693667e-12) A[2]:(1.0) A[3]:(2.14667320734e-31)\n",
      " state (14)  A[0]:(5.87323939483e-12) A[1]:(3.73295925421e-12) A[2]:(1.0) A[3]:(2.14654225727e-31)\n",
      " state (15)  A[0]:(5.87292584356e-12) A[1]:(3.73288769687e-12) A[2]:(1.0) A[3]:(2.14649312161e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 849000 finished after 35 . Running score: 0.14. Policy_loss: -92050.6112083, Value_loss: 1.85452927085. Times trained:               15333. Times reached goal: 121.               Steps done: 11236899.\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.9325e-11,  4.1279e-10,  8.7302e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.9327e-11,  4.1280e-10,  8.7312e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9963,  0.0008,  0.0012,  0.0017]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.9340e-11,  4.1284e-10,  8.7383e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.9341e-11,  4.1284e-10,  8.7387e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.3129e-12,  5.2365e-12,  1.0000e+00,  2.5295e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996339321136) A[1]:(0.000758818641771) A[2]:(0.00115883594844) A[3]:(0.00174302642699)\n",
      " state (1)  A[0]:(3.4961090023e-06) A[1]:(5.89409410168e-07) A[2]:(3.55232987204e-06) A[3]:(0.999992370605)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.53552115467e-10) A[2]:(2.22053736509e-10) A[3]:(3.73246260288e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.16062520705e-10) A[2]:(2.50890919151e-10) A[3]:(6.44494748775e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.93419643822e-11) A[2]:(4.12842093844e-10) A[3]:(8.73946095664e-19)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.35559208092e-10) A[2]:(2.26272547366e-08) A[3]:(6.19049174985e-20)\n",
      " state (6)  A[0]:(3.99756260094e-07) A[1]:(1.29543065164e-09) A[2]:(0.999999582767) A[3]:(5.27512596954e-28)\n",
      " state (7)  A[0]:(1.80258221544e-11) A[1]:(7.6600314633e-12) A[2]:(1.0) A[3]:(3.89537621944e-31)\n",
      " state (8)  A[0]:(7.31285084571e-12) A[1]:(5.23649613254e-12) A[2]:(1.0) A[3]:(2.52953430349e-31)\n",
      " state (9)  A[0]:(6.47961762371e-12) A[1]:(4.98849095712e-12) A[2]:(1.0) A[3]:(2.40065921544e-31)\n",
      " state (10)  A[0]:(6.33072500672e-12) A[1]:(4.94250040201e-12) A[2]:(1.0) A[3]:(2.37702096444e-31)\n",
      " state (11)  A[0]:(6.29707744282e-12) A[1]:(4.93199101351e-12) A[2]:(1.0) A[3]:(2.37167716712e-31)\n",
      " state (12)  A[0]:(6.28819565862e-12) A[1]:(4.92920764969e-12) A[2]:(1.0) A[3]:(2.37028420631e-31)\n",
      " state (13)  A[0]:(6.28548558687e-12) A[1]:(4.92838018659e-12) A[2]:(1.0) A[3]:(2.36986831641e-31)\n",
      " state (14)  A[0]:(6.28459870949e-12) A[1]:(4.92809829403e-12) A[2]:(1.0) A[3]:(2.36972373061e-31)\n",
      " state (15)  A[0]:(6.28423875437e-12) A[1]:(4.927985537e-12) A[2]:(1.0) A[3]:(2.36966942277e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 850000 finished after 26 . Running score: 0.09. Policy_loss: -92050.6112092, Value_loss: 1.62603273081. Times trained:               16125. Times reached goal: 103.               Steps done: 11253024.\n",
      " state (0)  A[0]:(0.993203938007) A[1]:(0.00388048728928) A[2]:(0.00130942533724) A[3]:(0.00160615402274)\n",
      " state (1)  A[0]:(3.81002905669e-06) A[1]:(1.14826502795e-06) A[2]:(3.57611361324e-06) A[3]:(0.999991476536)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(1.63127253927e-08) A[2]:(2.04376142676e-08) A[3]:(1.11996413921e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.5645081064e-10) A[2]:(1.97771216071e-10) A[3]:(8.47878028638e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.16110350501e-10) A[2]:(2.39692377058e-10) A[3]:(1.26483168379e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(9.61916865605e-11) A[2]:(3.58784696397e-10) A[3]:(4.0799623889e-19)\n",
      " state (6)  A[0]:(0.999998629093) A[1]:(4.63109284254e-09) A[2]:(1.37705819725e-06) A[3]:(9.14363348853e-21)\n",
      " state (7)  A[0]:(5.94153100053e-08) A[1]:(5.78105285864e-10) A[2]:(0.999999940395) A[3]:(9.57679137091e-29)\n",
      " state (8)  A[0]:(2.95828049557e-11) A[1]:(1.0857425202e-11) A[2]:(1.0) A[3]:(4.84388120053e-31)\n",
      " state (9)  A[0]:(1.31960250019e-11) A[1]:(7.62449391817e-12) A[2]:(1.0) A[3]:(3.2395719178e-31)\n",
      " state (10)  A[0]:(1.12858663387e-11) A[1]:(7.1414597326e-12) A[2]:(1.0) A[3]:(3.01740231e-31)\n",
      " state (11)  A[0]:(1.07702406021e-11) A[1]:(7.00466724554e-12) A[2]:(1.0) A[3]:(2.95621994493e-31)\n",
      " state (12)  A[0]:(1.05766012268e-11) A[1]:(6.9526225055e-12) A[2]:(1.0) A[3]:(2.93341582472e-31)\n",
      " state (13)  A[0]:(1.04898902065e-11) A[1]:(6.929216749e-12) A[2]:(1.0) A[3]:(2.92333972225e-31)\n",
      " state (14)  A[0]:(1.04472801937e-11) A[1]:(6.91772767542e-12) A[2]:(1.0) A[3]:(2.91843697041e-31)\n",
      " state (15)  A[0]:(1.04252648181e-11) A[1]:(6.91176629819e-12) A[2]:(1.0) A[3]:(2.91592211779e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 851000 finished after 20 . Running score: 0.1. Policy_loss: -92050.611209, Value_loss: 1.63500756454. Times trained:               15685. Times reached goal: 126.               Steps done: 11268709.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.99470949173) A[1]:(0.00217450759374) A[2]:(0.00161601847503) A[3]:(0.00149999314453)\n",
      " state (1)  A[0]:(3.70239831682e-06) A[1]:(8.75433784131e-07) A[2]:(3.96808036385e-06) A[3]:(0.999991476536)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.05111846899e-09) A[2]:(4.24637258689e-09) A[3]:(8.23106294234e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.31401986514e-10) A[2]:(2.04172179163e-10) A[3]:(1.19259073103e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.80777187295e-11) A[2]:(2.41673708823e-10) A[3]:(2.07392324601e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(6.69656205488e-11) A[2]:(3.60594498705e-10) A[3]:(4.61298516792e-19)\n",
      " state (6)  A[0]:(0.999999582767) A[1]:(1.8302156457e-09) A[2]:(4.26567112299e-07) A[3]:(1.52431906241e-20)\n",
      " state (7)  A[0]:(1.09819310978e-07) A[1]:(5.7110755014e-10) A[2]:(0.999999880791) A[3]:(1.62420834262e-28)\n",
      " state (8)  A[0]:(3.16529615019e-11) A[1]:(7.77840378913e-12) A[2]:(1.0) A[3]:(5.13559469982e-31)\n",
      " state (9)  A[0]:(1.33899315247e-11) A[1]:(5.3254050475e-12) A[2]:(1.0) A[3]:(3.33280719263e-31)\n",
      " state (10)  A[0]:(1.13541806165e-11) A[1]:(4.96930838492e-12) A[2]:(1.0) A[3]:(3.0912920041e-31)\n",
      " state (11)  A[0]:(1.08090949386e-11) A[1]:(4.86909557812e-12) A[2]:(1.0) A[3]:(3.02519348656e-31)\n",
      " state (12)  A[0]:(1.0604638695e-11) A[1]:(4.83101969886e-12) A[2]:(1.0) A[3]:(3.00062095265e-31)\n",
      " state (13)  A[0]:(1.05129646308e-11) A[1]:(4.81390231497e-12) A[2]:(1.0) A[3]:(2.98974363322e-31)\n",
      " state (14)  A[0]:(1.04677455939e-11) A[1]:(4.80545334428e-12) A[2]:(1.0) A[3]:(2.98443369014e-31)\n",
      " state (15)  A[0]:(1.04443719298e-11) A[1]:(4.80109224946e-12) A[2]:(1.0) A[3]:(2.98170254657e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 852000 finished after 17 . Running score: 0.13. Policy_loss: -92050.6112106, Value_loss: 1.63071510692. Times trained:               15272. Times reached goal: 96.               Steps done: 11283981.\n",
      " state (0)  A[0]:(0.995685994625) A[1]:(0.00173449690919) A[2]:(0.00114752189256) A[3]:(0.0014320036862)\n",
      " state (1)  A[0]:(3.50666778104e-06) A[1]:(7.89681905644e-07) A[2]:(3.23850781569e-06) A[3]:(0.999992489815)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.015955124e-09) A[2]:(4.99071628468e-09) A[3]:(1.31513440138e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.2145628947e-10) A[2]:(1.64066968344e-10) A[3]:(1.45828314702e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.15846765145e-11) A[2]:(1.64766922328e-10) A[3]:(1.0858459458e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.64356825161e-11) A[2]:(2.10377437693e-10) A[3]:(1.38331171596e-18)\n",
      " state (6)  A[0]:(1.0) A[1]:(6.94041005223e-11) A[2]:(4.58192928043e-10) A[3]:(1.37484125724e-19)\n",
      " state (7)  A[0]:(0.0695278570056) A[1]:(9.49287198182e-07) A[2]:(0.93047118187) A[3]:(2.04760333765e-23)\n",
      " state (8)  A[0]:(7.99864341428e-10) A[1]:(3.82071319027e-11) A[2]:(1.0) A[3]:(2.90492113636e-30)\n",
      " state (9)  A[0]:(4.1774497056e-11) A[1]:(9.17882783669e-12) A[2]:(1.0) A[3]:(5.03891499144e-31)\n",
      " state (10)  A[0]:(2.59685727783e-11) A[1]:(7.46616397179e-12) A[2]:(1.0) A[3]:(3.9974086589e-31)\n",
      " state (11)  A[0]:(2.27793860164e-11) A[1]:(7.06457592078e-12) A[2]:(1.0) A[3]:(3.76528755102e-31)\n",
      " state (12)  A[0]:(2.16439036832e-11) A[1]:(6.9156039402e-12) A[2]:(1.0) A[3]:(3.6815178269e-31)\n",
      " state (13)  A[0]:(2.11183414506e-11) A[1]:(6.84562866463e-12) A[2]:(1.0) A[3]:(3.64293222484e-31)\n",
      " state (14)  A[0]:(2.0840957432e-11) A[1]:(6.80847826035e-12) A[2]:(1.0) A[3]:(3.62269961607e-31)\n",
      " state (15)  A[0]:(2.06853197765e-11) A[1]:(6.78756400044e-12) A[2]:(1.0) A[3]:(3.61138524784e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 853000 finished after 8 . Running score: 0.13. Policy_loss: -92050.6112087, Value_loss: 1.1940337723. Times trained:               15520. Times reached goal: 110.               Steps done: 11299501.\n",
      " state (0)  A[0]:(0.996076703072) A[1]:(0.00130370922852) A[2]:(0.001422203728) A[3]:(0.00119737361092)\n",
      " state (1)  A[0]:(3.16256546284e-06) A[1]:(6.42403904294e-07) A[2]:(3.48320440935e-06) A[3]:(0.999992728233)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.58600418857e-10) A[2]:(3.48383377702e-10) A[3]:(7.81484113532e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.94987070565e-11) A[2]:(1.59507004827e-10) A[3]:(1.0549532439e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.41683885247e-11) A[2]:(1.67561894915e-10) A[3]:(4.6926816418e-17)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.9082910519e-11) A[2]:(2.18987952771e-10) A[3]:(2.59251147725e-19)\n",
      " state (6)  A[0]:(0.999999940395) A[1]:(4.27946261761e-10) A[2]:(3.18758388573e-08) A[3]:(2.37336454655e-20)\n",
      " state (7)  A[0]:(5.70713950765e-07) A[1]:(1.03654684835e-09) A[2]:(0.999999403954) A[3]:(3.44142640277e-28)\n",
      " state (8)  A[0]:(7.40653927078e-11) A[1]:(8.53964728792e-12) A[2]:(1.0) A[3]:(5.0622273954e-31)\n",
      " state (9)  A[0]:(2.88321155145e-11) A[1]:(5.61734036023e-12) A[2]:(1.0) A[3]:(3.12484507975e-31)\n",
      " state (10)  A[0]:(2.41374142895e-11) A[1]:(5.21238130782e-12) A[2]:(1.0) A[3]:(2.87909364468e-31)\n",
      " state (11)  A[0]:(2.290241434e-11) A[1]:(5.10003454399e-12) A[2]:(1.0) A[3]:(2.81285007114e-31)\n",
      " state (12)  A[0]:(2.24431705859e-11) A[1]:(5.05772116896e-12) A[2]:(1.0) A[3]:(2.78838497741e-31)\n",
      " state (13)  A[0]:(2.22379562681e-11) A[1]:(5.03873331947e-12) A[2]:(1.0) A[3]:(2.77759887635e-31)\n",
      " state (14)  A[0]:(2.21368982173e-11) A[1]:(5.02940007349e-12) A[2]:(1.0) A[3]:(2.77234841328e-31)\n",
      " state (15)  A[0]:(2.20844349752e-11) A[1]:(5.02452940365e-12) A[2]:(1.0) A[3]:(2.76964242528e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 854000 finished after 4 . Running score: 0.05. Policy_loss: -92050.6112088, Value_loss: 1.19552278031. Times trained:               15868. Times reached goal: 118.               Steps done: 11315369.\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0014,  0.0012,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0014,  0.0012,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0014,  0.0012,  0.0027]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.2505e-06,  4.8158e-07,  2.2142e-06,  1.0000e+00]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0014,  0.0012,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9947,  0.0014,  0.0012,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0014,  0.0012,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0014,  0.0012,  0.0027]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.2011e-11,  1.2602e-10,  1.3155e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.2371e-11,  9.1542e-12,  1.0000e+00,  6.5091e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994818806648) A[1]:(0.00136253191158) A[2]:(0.00118600763381) A[3]:(0.00263263843954)\n",
      " state (1)  A[0]:(2.26596444008e-06) A[1]:(4.83336805246e-07) A[2]:(2.19861590267e-06) A[3]:(0.999995052814)\n",
      " state (2)  A[0]:(0.999989032745) A[1]:(1.76226947701e-07) A[2]:(3.16277237289e-07) A[3]:(1.0483967344e-05)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.05315547949e-10) A[2]:(1.42218500865e-10) A[3]:(2.39310242629e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.19799905843e-11) A[2]:(1.25875865287e-10) A[3]:(1.31507201369e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.70524617932e-11) A[2]:(1.62594993025e-10) A[3]:(4.77890157218e-19)\n",
      " state (6)  A[0]:(0.999999582767) A[1]:(1.87737758672e-09) A[2]:(4.17750442239e-07) A[3]:(1.51743246069e-20)\n",
      " state (7)  A[0]:(3.02790716944e-08) A[1]:(2.11449524556e-10) A[2]:(0.999999940395) A[3]:(3.99582676324e-29)\n",
      " state (8)  A[0]:(7.23883245013e-11) A[1]:(9.15534575235e-12) A[2]:(1.0) A[3]:(6.51015337658e-31)\n",
      " state (9)  A[0]:(3.71775492336e-11) A[1]:(6.81583218684e-12) A[2]:(1.0) A[3]:(4.64656601018e-31)\n",
      " state (10)  A[0]:(3.19321444286e-11) A[1]:(6.39032750327e-12) A[2]:(1.0) A[3]:(4.33084468447e-31)\n",
      " state (11)  A[0]:(3.02520127005e-11) A[1]:(6.24809665811e-12) A[2]:(1.0) A[3]:(4.22845536494e-31)\n",
      " state (12)  A[0]:(2.95248582527e-11) A[1]:(6.18581965164e-12) A[2]:(1.0) A[3]:(4.18452055828e-31)\n",
      " state (13)  A[0]:(2.91630782023e-11) A[1]:(6.15467963053e-12) A[2]:(1.0) A[3]:(4.16280400544e-31)\n",
      " state (14)  A[0]:(2.89709194135e-11) A[1]:(6.13817286929e-12) A[2]:(1.0) A[3]:(4.15135469047e-31)\n",
      " state (15)  A[0]:(2.88654603692e-11) A[1]:(6.12907120889e-12) A[2]:(1.0) A[3]:(4.14505639173e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 855000 finished after 10 . Running score: 0.04. Policy_loss: -92050.6125605, Value_loss: 1.40751482654. Times trained:               15695. Times reached goal: 115.               Steps done: 11331064.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995268464088) A[1]:(0.00149909080938) A[2]:(0.00116735510528) A[3]:(0.00206510070711)\n",
      " state (1)  A[0]:(2.45891101258e-06) A[1]:(5.12532096764e-07) A[2]:(2.33342029787e-06) A[3]:(0.999994695187)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(6.77356748469e-09) A[2]:(1.00044603712e-08) A[3]:(2.7434230887e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(9.41361572027e-11) A[2]:(1.34484992587e-10) A[3]:(8.97949358099e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.77629992449e-11) A[2]:(1.24049923111e-10) A[3]:(5.43688478925e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.66715338651e-11) A[2]:(1.60272475846e-10) A[3]:(3.68026251306e-19)\n",
      " state (6)  A[0]:(0.999999880791) A[1]:(1.01014197007e-09) A[2]:(1.1854429971e-07) A[3]:(1.88491326393e-20)\n",
      " state (7)  A[0]:(5.4862208998e-08) A[1]:(3.03863045836e-10) A[2]:(0.999999940395) A[3]:(5.34633397922e-29)\n",
      " state (8)  A[0]:(7.83735160104e-11) A[1]:(9.81708291048e-12) A[2]:(1.0) A[3]:(5.69826293835e-31)\n",
      " state (9)  A[0]:(3.83443797547e-11) A[1]:(7.14787820946e-12) A[2]:(1.0) A[3]:(3.95967905182e-31)\n",
      " state (10)  A[0]:(3.26345686585e-11) A[1]:(6.67582265509e-12) A[2]:(1.0) A[3]:(3.67467080741e-31)\n",
      " state (11)  A[0]:(3.0824211239e-11) A[1]:(6.51921225336e-12) A[2]:(1.0) A[3]:(3.58300787382e-31)\n",
      " state (12)  A[0]:(3.00422499377e-11) A[1]:(6.45068850766e-12) A[2]:(1.0) A[3]:(3.54372402811e-31)\n",
      " state (13)  A[0]:(2.96526275756e-11) A[1]:(6.41642771901e-12) A[2]:(1.0) A[3]:(3.5243112091e-31)\n",
      " state (14)  A[0]:(2.94452240368e-11) A[1]:(6.39816975442e-12) A[2]:(1.0) A[3]:(3.51405478579e-31)\n",
      " state (15)  A[0]:(2.93308745347e-11) A[1]:(6.38809751624e-12) A[2]:(1.0) A[3]:(3.50842910493e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 856000 finished after 32 . Running score: 0.1. Policy_loss: -92050.6112094, Value_loss: 1.2105423426. Times trained:               14852. Times reached goal: 122.               Steps done: 11345916.\n",
      " state (0)  A[0]:(0.996585905552) A[1]:(0.00111777044367) A[2]:(0.00102415995207) A[3]:(0.00127218314447)\n",
      " state (1)  A[0]:(4.24227209805e-06) A[1]:(6.19100717358e-07) A[2]:(3.37955816576e-06) A[3]:(0.999991774559)\n",
      " state (2)  A[0]:(1.0) A[1]:(8.84460629735e-11) A[2]:(1.5726728253e-10) A[3]:(2.86304356958e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.57881683957e-11) A[2]:(1.37665809308e-10) A[3]:(1.52082701037e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.48979187961e-11) A[2]:(1.66660171774e-10) A[3]:(5.64249053939e-19)\n",
      " state (5)  A[0]:(1.0) A[1]:(4.81607219383e-11) A[2]:(9.24192111729e-10) A[3]:(6.03130859578e-20)\n",
      " state (6)  A[0]:(5.28269310962e-07) A[1]:(5.85165027545e-10) A[2]:(0.999999463558) A[3]:(3.16239454279e-28)\n",
      " state (7)  A[0]:(4.10301584375e-11) A[1]:(3.77493826129e-12) A[2]:(1.0) A[3]:(3.09053733673e-31)\n",
      " state (8)  A[0]:(2.17812295977e-11) A[1]:(2.87084678946e-12) A[2]:(1.0) A[3]:(2.26662958466e-31)\n",
      " state (9)  A[0]:(1.98096539172e-11) A[1]:(2.76071353278e-12) A[2]:(1.0) A[3]:(2.17282278447e-31)\n",
      " state (10)  A[0]:(1.93522402964e-11) A[1]:(2.73460442658e-12) A[2]:(1.0) A[3]:(2.15106697013e-31)\n",
      " state (11)  A[0]:(1.92075019084e-11) A[1]:(2.72634454075e-12) A[2]:(1.0) A[3]:(2.14431610607e-31)\n",
      " state (12)  A[0]:(1.91527731175e-11) A[1]:(2.72324198781e-12) A[2]:(1.0) A[3]:(2.14181465409e-31)\n",
      " state (13)  A[0]:(1.91299909941e-11) A[1]:(2.72194875146e-12) A[2]:(1.0) A[3]:(2.14080161306e-31)\n",
      " state (14)  A[0]:(1.9119922659e-11) A[1]:(2.72139862728e-12) A[2]:(1.0) A[3]:(2.14036080268e-31)\n",
      " state (15)  A[0]:(1.91153291113e-11) A[1]:(2.72113386511e-12) A[2]:(1.0) A[3]:(2.1401485084e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 857000 finished after 10 . Running score: 0.09. Policy_loss: -92050.6112059, Value_loss: 1.23747098525. Times trained:               15276. Times reached goal: 121.               Steps done: 11361192.\n",
      " state (0)  A[0]:(0.997628808022) A[1]:(0.000699304626323) A[2]:(0.000937466567848) A[3]:(0.000734446221031)\n",
      " state (1)  A[0]:(4.30017644248e-06) A[1]:(6.17731643615e-07) A[2]:(3.72672752746e-06) A[3]:(0.999991357327)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.72030495089e-11) A[2]:(1.50889786776e-10) A[3]:(1.13373600872e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.80851149229e-11) A[2]:(1.34373720484e-10) A[3]:(3.22090366243e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.25427870815e-11) A[2]:(1.66749211661e-10) A[3]:(3.08663272755e-19)\n",
      " state (5)  A[0]:(1.0) A[1]:(4.60128150859e-11) A[2]:(9.46904887833e-10) A[3]:(4.79537119054e-20)\n",
      " state (6)  A[0]:(4.46247895525e-07) A[1]:(5.1657289557e-10) A[2]:(0.999999582767) A[3]:(2.28063062892e-28)\n",
      " state (7)  A[0]:(4.04585080716e-11) A[1]:(3.65007590125e-12) A[2]:(1.0) A[3]:(2.53678169636e-31)\n",
      " state (8)  A[0]:(2.17671922154e-11) A[1]:(2.79220136595e-12) A[2]:(1.0) A[3]:(1.87231882088e-31)\n",
      " state (9)  A[0]:(1.98333970775e-11) A[1]:(2.68713458602e-12) A[2]:(1.0) A[3]:(1.7961871064e-31)\n",
      " state (10)  A[0]:(1.93842320667e-11) A[1]:(2.66223414841e-12) A[2]:(1.0) A[3]:(1.77854164315e-31)\n",
      " state (11)  A[0]:(1.92420437223e-11) A[1]:(2.65435481754e-12) A[2]:(1.0) A[3]:(1.77306807126e-31)\n",
      " state (12)  A[0]:(1.91884598488e-11) A[1]:(2.6514200991e-12) A[2]:(1.0) A[3]:(1.77105362659e-31)\n",
      " state (13)  A[0]:(1.91661478355e-11) A[1]:(2.65019646853e-12) A[2]:(1.0) A[3]:(1.77022960505e-31)\n",
      " state (14)  A[0]:(1.91563535867e-11) A[1]:(2.64966065582e-12) A[2]:(1.0) A[3]:(1.7698649667e-31)\n",
      " state (15)  A[0]:(1.9151822489e-11) A[1]:(2.64940304938e-12) A[2]:(1.0) A[3]:(1.76970298358e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 858000 finished after 17 . Running score: 0.08. Policy_loss: -92050.6112059, Value_loss: 1.63689649881. Times trained:               15662. Times reached goal: 110.               Steps done: 11376854.\n",
      " state (0)  A[0]:(0.99699562788) A[1]:(0.00141796609387) A[2]:(0.000917637022212) A[3]:(0.000668740191031)\n",
      " state (1)  A[0]:(3.77061064682e-06) A[1]:(6.86405599026e-07) A[2]:(3.10243035528e-06) A[3]:(0.99999243021)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.54862822566e-09) A[2]:(2.52368326237e-09) A[3]:(8.93628004928e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.3610060835e-11) A[2]:(1.12915565786e-10) A[3]:(2.40690612243e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.97100920255e-11) A[2]:(1.14165107923e-10) A[3]:(1.29484048947e-17)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.08154136289e-11) A[2]:(1.41659184383e-10) A[3]:(1.06019260897e-19)\n",
      " state (6)  A[0]:(0.999999940395) A[1]:(4.84747242346e-10) A[2]:(7.40766594731e-08) A[3]:(1.323291313e-20)\n",
      " state (7)  A[0]:(2.04106971324e-08) A[1]:(1.05803595052e-10) A[2]:(1.0) A[3]:(1.82597869417e-29)\n",
      " state (8)  A[0]:(6.16224710148e-11) A[1]:(5.13682976364e-12) A[2]:(1.0) A[3]:(3.25361625411e-31)\n",
      " state (9)  A[0]:(3.52050402719e-11) A[1]:(4.00656894814e-12) A[2]:(1.0) A[3]:(2.44086511892e-31)\n",
      " state (10)  A[0]:(3.13849571332e-11) A[1]:(3.81555507675e-12) A[2]:(1.0) A[3]:(2.31244612267e-31)\n",
      " state (11)  A[0]:(3.02767186322e-11) A[1]:(3.75859326301e-12) A[2]:(1.0) A[3]:(2.27518859412e-31)\n",
      " state (12)  A[0]:(2.98519264863e-11) A[1]:(3.73668457287e-12) A[2]:(1.0) A[3]:(2.26115436707e-31)\n",
      " state (13)  A[0]:(2.96630359164e-11) A[1]:(3.72696882037e-12) A[2]:(1.0) A[3]:(2.25502134284e-31)\n",
      " state (14)  A[0]:(2.95721988564e-11) A[1]:(3.72232279722e-12) A[2]:(1.0) A[3]:(2.25213291812e-31)\n",
      " state (15)  A[0]:(2.95262078676e-11) A[1]:(3.71996617538e-12) A[2]:(1.0) A[3]:(2.25067271904e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 859000 finished after 13 . Running score: 0.12. Policy_loss: -92050.6114904, Value_loss: 1.19379009457. Times trained:               15637. Times reached goal: 111.               Steps done: 11392491.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9967,  0.0016,  0.0009,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0016,  0.0009,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0016,  0.0009,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0016,  0.0009,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0016,  0.0009,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.4513e-11,  1.0323e-10,  2.0552e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.4515e-11,  1.0323e-10,  2.0561e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9967,  0.0016,  0.0009,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.4518e-11,  1.0324e-10,  2.0579e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3068e-10,  7.1522e-12,  1.0000e+00,  6.3527e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.4525e-11,  1.0324e-10,  2.0618e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.4528e-11,  1.0324e-10,  2.0636e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3080e-10,  7.1556e-12,  1.0000e+00,  6.3571e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.996660351753) A[1]:(0.00157676357776) A[2]:(0.00092667696299) A[3]:(0.00083623174578)\n",
      " state (1)  A[0]:(2.90482284981e-06) A[1]:(5.58114891192e-07) A[2]:(2.56341400018e-06) A[3]:(0.999993979931)\n",
      " state (2)  A[0]:(1.0) A[1]:(9.37059096984e-10) A[2]:(1.51967183282e-09) A[3]:(4.33249380816e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.09180275668e-11) A[2]:(1.13987562445e-10) A[3]:(1.40579090252e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.45358359966e-11) A[2]:(1.03247049799e-10) A[3]:(2.06914227442e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.02359205781e-11) A[2]:(1.18891937961e-10) A[3]:(3.4449249759e-19)\n",
      " state (6)  A[0]:(1.0) A[1]:(3.25653011501e-11) A[2]:(3.60997759463e-10) A[3]:(7.00904900451e-20)\n",
      " state (7)  A[0]:(3.56542113877e-05) A[1]:(8.02114108467e-09) A[2]:(0.999964356422) A[3]:(1.64087305332e-26)\n",
      " state (8)  A[0]:(1.30871466442e-10) A[1]:(7.15746906188e-12) A[2]:(1.0) A[3]:(6.35934544473e-31)\n",
      " state (9)  A[0]:(4.26581062141e-11) A[1]:(4.22817380066e-12) A[2]:(1.0) A[3]:(3.36743984229e-31)\n",
      " state (10)  A[0]:(3.48615858403e-11) A[1]:(3.87069152771e-12) A[2]:(1.0) A[3]:(3.04692061884e-31)\n",
      " state (11)  A[0]:(3.28135782435e-11) A[1]:(3.77193675599e-12) A[2]:(1.0) A[3]:(2.96154752043e-31)\n",
      " state (12)  A[0]:(3.20547997867e-11) A[1]:(3.7349815081e-12) A[2]:(1.0) A[3]:(2.93028430777e-31)\n",
      " state (13)  A[0]:(3.17227077939e-11) A[1]:(3.71880998218e-12) A[2]:(1.0) A[3]:(2.91683430141e-31)\n",
      " state (14)  A[0]:(3.15631340508e-11) A[1]:(3.71105099775e-12) A[2]:(1.0) A[3]:(2.91045465847e-31)\n",
      " state (15)  A[0]:(3.14820843006e-11) A[1]:(3.70712488484e-12) A[2]:(1.0) A[3]:(2.90725872443e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 860000 finished after 13 . Running score: 0.13. Policy_loss: -92050.6112068, Value_loss: 1.19993301965. Times trained:               14802. Times reached goal: 107.               Steps done: 11407293.\n",
      " state (0)  A[0]:(0.994600474834) A[1]:(0.00142831599806) A[2]:(0.00131869560573) A[3]:(0.00265253195539)\n",
      " state (1)  A[0]:(2.24560517381e-06) A[1]:(4.28991000945e-07) A[2]:(2.03343051908e-06) A[3]:(0.999995291233)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.64025112884e-09) A[2]:(4.65392657745e-09) A[3]:(3.3810378941e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.2795665329e-11) A[2]:(1.29253552306e-10) A[3]:(5.18146235553e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.73158560943e-11) A[2]:(1.17349130413e-10) A[3]:(3.39032031039e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.81341529934e-11) A[2]:(1.22081747489e-10) A[3]:(5.11065228301e-18)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.18921443473e-11) A[2]:(1.47962281316e-10) A[3]:(1.34620400594e-19)\n",
      " state (7)  A[0]:(0.999997615814) A[1]:(6.54449783255e-09) A[2]:(2.35980269281e-06) A[3]:(1.96609829455e-20)\n",
      " state (8)  A[0]:(8.55812203326e-08) A[1]:(2.98262720078e-10) A[2]:(0.999999940395) A[3]:(8.76017661148e-29)\n",
      " state (9)  A[0]:(1.01672718855e-10) A[1]:(7.65375176431e-12) A[2]:(1.0) A[3]:(6.43617340451e-31)\n",
      " state (10)  A[0]:(4.12349251966e-11) A[1]:(4.99788144898e-12) A[2]:(1.0) A[3]:(3.87604733073e-31)\n",
      " state (11)  A[0]:(3.28747203071e-11) A[1]:(4.51944861743e-12) A[2]:(1.0) A[3]:(3.45875624015e-31)\n",
      " state (12)  A[0]:(3.02715214007e-11) A[1]:(4.36146691679e-12) A[2]:(1.0) A[3]:(3.32549209129e-31)\n",
      " state (13)  A[0]:(2.91929085072e-11) A[1]:(4.29484789746e-12) A[2]:(1.0) A[3]:(3.27031532685e-31)\n",
      " state (14)  A[0]:(2.86804052729e-11) A[1]:(4.26304304357e-12) A[2]:(1.0) A[3]:(3.24427154422e-31)\n",
      " state (15)  A[0]:(2.84175079301e-11) A[1]:(4.24672276511e-12) A[2]:(1.0) A[3]:(3.23105616653e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 861000 finished after 35 . Running score: 0.08. Policy_loss: -92050.6112112, Value_loss: 1.19956035659. Times trained:               15794. Times reached goal: 112.               Steps done: 11423087.\n",
      " state (0)  A[0]:(0.991824448109) A[1]:(0.00209807185456) A[2]:(0.00192108051851) A[3]:(0.00415642140433)\n",
      " state (1)  A[0]:(1.76299045052e-06) A[1]:(3.65406208402e-07) A[2]:(1.83573934009e-06) A[3]:(0.999996006489)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.01668307106e-09) A[2]:(1.74423087085e-09) A[3]:(9.48605860529e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.02296423741e-11) A[2]:(1.40204681198e-10) A[3]:(1.0778393152e-12)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.33407901182e-11) A[2]:(1.25850427302e-10) A[3]:(7.12191570095e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.102463772e-11) A[2]:(1.27652458048e-10) A[3]:(6.60140049167e-18)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.48362597849e-11) A[2]:(1.57132348799e-10) A[3]:(1.59832141882e-19)\n",
      " state (7)  A[0]:(0.999739468098) A[1]:(7.87458986906e-08) A[2]:(0.0002604641777) A[3]:(3.7519963428e-21)\n",
      " state (8)  A[0]:(1.71134104221e-07) A[1]:(4.22078039186e-10) A[2]:(0.999999821186) A[3]:(1.22859645236e-28)\n",
      " state (9)  A[0]:(2.09180686661e-10) A[1]:(1.13391613807e-11) A[2]:(1.0) A[3]:(1.07236061422e-30)\n",
      " state (10)  A[0]:(7.13664335961e-11) A[1]:(6.85345443591e-12) A[2]:(1.0) A[3]:(5.95503087138e-31)\n",
      " state (11)  A[0]:(5.30951498046e-11) A[1]:(6.0182444965e-12) A[2]:(1.0) A[3]:(5.15219503104e-31)\n",
      " state (12)  A[0]:(4.73392366973e-11) A[1]:(5.73171718421e-12) A[2]:(1.0) A[3]:(4.8860457127e-31)\n",
      " state (13)  A[0]:(4.48753187388e-11) A[1]:(5.60552429127e-12) A[2]:(1.0) A[3]:(4.77107672281e-31)\n",
      " state (14)  A[0]:(4.36548887317e-11) A[1]:(5.54240594391e-12) A[2]:(1.0) A[3]:(4.71434078267e-31)\n",
      " state (15)  A[0]:(4.30003879726e-11) A[1]:(5.50842834887e-12) A[2]:(1.0) A[3]:(4.68408167729e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 862000 finished after 9 . Running score: 0.1. Policy_loss: -92050.6112136, Value_loss: 1.41569917397. Times trained:               15656. Times reached goal: 131.               Steps done: 11438743.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993206739426) A[1]:(0.00210541649722) A[2]:(0.00231556082144) A[3]:(0.00237226369791)\n",
      " state (1)  A[0]:(2.03109334507e-06) A[1]:(4.16184349206e-07) A[2]:(2.13965972762e-06) A[3]:(0.999995410442)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.92734672666e-09) A[2]:(3.23348436915e-09) A[3]:(1.52057355596e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.38091956945e-11) A[2]:(1.3650046371e-10) A[3]:(4.32923255621e-13)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.65456952095e-11) A[2]:(1.24643795285e-10) A[3]:(2.5189914936e-15)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.50930152992e-11) A[2]:(1.26846907977e-10) A[3]:(3.72456971879e-18)\n",
      " state (6)  A[0]:(1.0) A[1]:(2.88711866914e-11) A[2]:(1.48206030781e-10) A[3]:(1.29146002978e-19)\n",
      " state (7)  A[0]:(0.999987781048) A[1]:(2.29262511198e-08) A[2]:(1.21730863611e-05) A[3]:(7.74168513487e-21)\n",
      " state (8)  A[0]:(7.53014410293e-06) A[1]:(5.47647527327e-09) A[2]:(0.999992489815) A[3]:(1.71750445765e-27)\n",
      " state (9)  A[0]:(7.88629994641e-10) A[1]:(3.56286562753e-11) A[2]:(1.0) A[3]:(1.79816931902e-30)\n",
      " state (10)  A[0]:(1.40742431598e-10) A[1]:(1.57953216479e-11) A[2]:(1.0) A[3]:(6.77114086413e-31)\n",
      " state (11)  A[0]:(8.70213692772e-11) A[1]:(1.27934416533e-11) A[2]:(1.0) A[3]:(5.32666425358e-31)\n",
      " state (12)  A[0]:(7.19286366579e-11) A[1]:(1.18073805941e-11) A[2]:(1.0) A[3]:(4.87401993529e-31)\n",
      " state (13)  A[0]:(6.55835177832e-11) A[1]:(1.13683966754e-11) A[2]:(1.0) A[3]:(4.67743919381e-31)\n",
      " state (14)  A[0]:(6.23645579623e-11) A[1]:(1.11402215577e-11) A[2]:(1.0) A[3]:(4.57690151298e-31)\n",
      " state (15)  A[0]:(6.05609729032e-11) A[1]:(1.10109378212e-11) A[2]:(1.0) A[3]:(4.52057746606e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 863000 finished after 5 . Running score: 0.06. Policy_loss: -92050.6112101, Value_loss: 1.42336133077. Times trained:               15394. Times reached goal: 87.               Steps done: 11454137.\n",
      " state (0)  A[0]:(0.994433999062) A[1]:(0.00179794593714) A[2]:(0.00205534836277) A[3]:(0.00171270861756)\n",
      " state (1)  A[0]:(2.57841998064e-06) A[1]:(5.2395870398e-07) A[2]:(2.47518164542e-06) A[3]:(0.999994397163)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.58568860767e-10) A[2]:(2.41838798987e-10) A[3]:(1.06246209747e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.1089221208e-10) A[2]:(1.1963213753e-10) A[3]:(5.38682766199e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.91166715475e-11) A[2]:(1.14433691689e-10) A[3]:(1.96584435657e-16)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.72309005742e-11) A[2]:(1.24285332026e-10) A[3]:(8.11295229274e-19)\n",
      " state (6)  A[0]:(1.0) A[1]:(5.9505268768e-11) A[2]:(1.64240732126e-10) A[3]:(1.05640580198e-19)\n",
      " state (7)  A[0]:(0.97849458456) A[1]:(2.38143479692e-06) A[2]:(0.0215030293912) A[3]:(3.71913838021e-22)\n",
      " state (8)  A[0]:(5.94075390836e-06) A[1]:(1.25360113401e-08) A[2]:(0.999994039536) A[3]:(9.81443499638e-28)\n",
      " state (9)  A[0]:(2.08811656854e-09) A[1]:(1.73429451356e-10) A[2]:(1.0) A[3]:(2.80228073123e-30)\n",
      " state (10)  A[0]:(3.15684478558e-10) A[1]:(7.13786807438e-11) A[2]:(1.0) A[3]:(9.41687772127e-31)\n",
      " state (11)  A[0]:(1.71107114588e-10) A[1]:(5.4563461388e-11) A[2]:(1.0) A[3]:(6.87278774143e-31)\n",
      " state (12)  A[0]:(1.31236924106e-10) A[1]:(4.8805997438e-11) A[2]:(1.0) A[3]:(6.05120934897e-31)\n",
      " state (13)  A[0]:(1.14313711275e-10) A[1]:(4.61347939018e-11) A[2]:(1.0) A[3]:(5.68150650148e-31)\n",
      " state (14)  A[0]:(1.05457864663e-10) A[1]:(4.46756485972e-11) A[2]:(1.0) A[3]:(5.48360450435e-31)\n",
      " state (15)  A[0]:(1.00271464742e-10) A[1]:(4.38006818315e-11) A[2]:(1.0) A[3]:(5.36663952506e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 864000 finished after 11 . Running score: 0.1. Policy_loss: -92050.6112046, Value_loss: 0.980061743226. Times trained:               16659. Times reached goal: 58.               Steps done: 11470796.\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0017,  0.0019,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0017,  0.0019,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0017,  0.0019,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.4221e-11,  1.2384e-10,  6.2303e-17]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0017,  0.0019,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0017,  0.0019,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0017,  0.0019,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0017,  0.0019,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0017,  0.0019,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.4231e-11,  1.2385e-10,  6.2361e-17]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0017,  0.0019,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0017,  0.0019,  0.0021]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  7.4236e-11,  1.2385e-10,  6.2392e-17]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0584e-06,  4.4104e-09,  1.0000e+00,  2.3084e-28]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5154e-09,  1.4254e-10,  1.0000e+00,  2.3738e-30]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.1529e-10,  4.4781e-11,  1.0000e+00,  5.9961e-31]])\n",
      "On state=13, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.1529e-10,  4.4781e-11,  1.0000e+00,  5.9962e-31]])\n",
      "On state=13, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5153e-09,  1.4254e-10,  1.0000e+00,  2.3738e-30]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994162023067) A[1]:(0.00173976807855) A[2]:(0.00194803171325) A[3]:(0.0021501716692)\n",
      " state (1)  A[0]:(2.52916697718e-06) A[1]:(5.02667660385e-07) A[2]:(2.37028780248e-06) A[3]:(0.999994575977)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.00667385486e-10) A[2]:(1.94822075139e-10) A[3]:(5.03472697561e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.04710656812e-10) A[2]:(1.25480348334e-10) A[3]:(2.12064642512e-14)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.42434655421e-11) A[2]:(1.23858881484e-10) A[3]:(6.2435777924e-17)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.58824410357e-11) A[2]:(1.35157704473e-10) A[3]:(4.50493987029e-19)\n",
      " state (6)  A[0]:(1.0) A[1]:(7.41793432235e-11) A[2]:(2.30213334507e-10) A[3]:(1.11921427212e-19)\n",
      " state (7)  A[0]:(0.622946679592) A[1]:(7.57063162382e-06) A[2]:(0.37704578042) A[3]:(8.8784818135e-23)\n",
      " state (8)  A[0]:(1.05794003957e-06) A[1]:(4.40946434921e-09) A[2]:(0.999998927116) A[3]:(2.30789120431e-28)\n",
      " state (9)  A[0]:(1.51523193992e-09) A[1]:(1.42534609116e-10) A[2]:(1.0) A[3]:(2.37375097425e-30)\n",
      " state (10)  A[0]:(2.98582047975e-10) A[1]:(6.71651692596e-11) A[2]:(1.0) A[3]:(9.54723722359e-31)\n",
      " state (11)  A[0]:(1.70472538863e-10) A[1]:(5.26315414862e-11) A[2]:(1.0) A[3]:(7.19235105288e-31)\n",
      " state (12)  A[0]:(1.3226622575e-10) A[1]:(4.7341221221e-11) A[2]:(1.0) A[3]:(6.37873357835e-31)\n",
      " state (13)  A[0]:(1.15285905822e-10) A[1]:(4.47809220261e-11) A[2]:(1.0) A[3]:(5.99624464371e-31)\n",
      " state (14)  A[0]:(1.06041585235e-10) A[1]:(4.33254092325e-11) A[2]:(1.0) A[3]:(5.78318629263e-31)\n",
      " state (15)  A[0]:(1.00424203675e-10) A[1]:(4.24181106906e-11) A[2]:(1.0) A[3]:(5.65219343395e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 865000 finished after 18 . Running score: 0.13. Policy_loss: -92050.611206, Value_loss: 0.99483784144. Times trained:               15544. Times reached goal: 124.               Steps done: 11486340.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.988110542297) A[1]:(0.00282672955655) A[2]:(0.00241783913225) A[3]:(0.00664487108588)\n",
      " state (1)  A[0]:(1.71333442722e-06) A[1]:(3.55065139956e-07) A[2]:(1.83065901638e-06) A[3]:(0.999996125698)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.46143111501e-10) A[2]:(1.78378561921e-10) A[3]:(2.79681724828e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.68611779725e-11) A[2]:(1.73124722891e-10) A[3]:(7.37626624836e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.03825392465e-11) A[2]:(2.00010147333e-10) A[3]:(1.01747185811e-18)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.57944851642e-10) A[2]:(1.14260367834e-09) A[3]:(1.68584707964e-19)\n",
      " state (6)  A[0]:(0.000168009428307) A[1]:(7.66312879819e-08) A[2]:(0.999831914902) A[3]:(4.57937484081e-26)\n",
      " state (7)  A[0]:(1.96052576951e-10) A[1]:(4.26345174442e-11) A[2]:(1.0) A[3]:(1.3080225932e-30)\n",
      " state (8)  A[0]:(4.86192475169e-11) A[1]:(2.32550905616e-11) A[2]:(1.0) A[3]:(6.40501857243e-31)\n",
      " state (9)  A[0]:(3.75684656995e-11) A[1]:(2.09790310124e-11) A[2]:(1.0) A[3]:(5.70417849612e-31)\n",
      " state (10)  A[0]:(3.46424486319e-11) A[1]:(2.03355908501e-11) A[2]:(1.0) A[3]:(5.51120229053e-31)\n",
      " state (11)  A[0]:(3.35384671424e-11) A[1]:(2.00905490161e-11) A[2]:(1.0) A[3]:(5.43877256041e-31)\n",
      " state (12)  A[0]:(3.3056182791e-11) A[1]:(1.99839207682e-11) A[2]:(1.0) A[3]:(5.40757541053e-31)\n",
      " state (13)  A[0]:(3.28297319885e-11) A[1]:(1.99342018586e-11) A[2]:(1.0) A[3]:(5.39315491604e-31)\n",
      " state (14)  A[0]:(3.27190843863e-11) A[1]:(1.99101117537e-11) A[2]:(1.0) A[3]:(5.38616448623e-31)\n",
      " state (15)  A[0]:(3.26635905823e-11) A[1]:(1.98981144062e-11) A[2]:(1.0) A[3]:(5.38275508242e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 866000 finished after 14 . Running score: 0.14. Policy_loss: -92050.611205, Value_loss: 1.42828692335. Times trained:               15958. Times reached goal: 123.               Steps done: 11502298.\n",
      " state (0)  A[0]:(0.994695484638) A[1]:(0.0015025032917) A[2]:(0.00153601157945) A[3]:(0.00226600281894)\n",
      " state (1)  A[0]:(2.74783019449e-06) A[1]:(4.34745004441e-07) A[2]:(2.62637240667e-06) A[3]:(0.999994218349)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.09893538713e-10) A[2]:(1.68868363737e-10) A[3]:(4.2893770133e-13)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.26062604536e-11) A[2]:(1.77577771932e-10) A[3]:(4.57934433212e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.78616417332e-11) A[2]:(2.0696025449e-10) A[3]:(3.63838546058e-19)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.37055383604e-10) A[2]:(6.10432371317e-09) A[3]:(7.93466217126e-20)\n",
      " state (6)  A[0]:(1.09231837087e-06) A[1]:(3.40895645046e-09) A[2]:(0.999998927116) A[3]:(4.7669669463e-28)\n",
      " state (7)  A[0]:(7.35843885824e-11) A[1]:(2.28843367867e-11) A[2]:(1.0) A[3]:(5.72274801548e-31)\n",
      " state (8)  A[0]:(3.33502843397e-11) A[1]:(1.64943544934e-11) A[2]:(1.0) A[3]:(3.92224566932e-31)\n",
      " state (9)  A[0]:(2.86639982583e-11) A[1]:(1.55614150038e-11) A[2]:(1.0) A[3]:(3.6761288906e-31)\n",
      " state (10)  A[0]:(2.73584748606e-11) A[1]:(1.52946405541e-11) A[2]:(1.0) A[3]:(3.60697949502e-31)\n",
      " state (11)  A[0]:(2.68789972918e-11) A[1]:(1.51970502155e-11) A[2]:(1.0) A[3]:(3.58199671358e-31)\n",
      " state (12)  A[0]:(2.66799984883e-11) A[1]:(1.51570440227e-11) A[2]:(1.0) A[3]:(3.5718176378e-31)\n",
      " state (13)  A[0]:(2.65922093373e-11) A[1]:(1.51395927045e-11) A[2]:(1.0) A[3]:(3.56743280877e-31)\n",
      " state (14)  A[0]:(2.65516636455e-11) A[1]:(1.51315678737e-11) A[2]:(1.0) A[3]:(3.56541942205e-31)\n",
      " state (15)  A[0]:(2.65326285248e-11) A[1]:(1.51279318933e-11) A[2]:(1.0) A[3]:(3.56452181456e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 867000 finished after 15 . Running score: 0.12. Policy_loss: -92050.6111855, Value_loss: 0.978363256347. Times trained:               16228. Times reached goal: 118.               Steps done: 11518526.\n",
      " state (0)  A[0]:(0.996710419655) A[1]:(0.00120037700981) A[2]:(0.000898440775927) A[3]:(0.00119076296687)\n",
      " state (1)  A[0]:(3.81166228181e-06) A[1]:(5.3543459444e-07) A[2]:(2.84644079329e-06) A[3]:(0.999992787838)\n",
      " state (2)  A[0]:(1.0) A[1]:(9.68208360708e-11) A[2]:(1.2769654778e-10) A[3]:(2.09058315906e-13)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.58486625002e-11) A[2]:(1.28150365319e-10) A[3]:(1.24804167098e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.54869891753e-11) A[2]:(1.45162437626e-10) A[3]:(2.04971904897e-19)\n",
      " state (5)  A[0]:(0.999998748302) A[1]:(8.01760879909e-09) A[2]:(1.21518951346e-06) A[3]:(1.11435146794e-20)\n",
      " state (6)  A[0]:(3.25542934831e-08) A[1]:(5.72663028109e-10) A[2]:(0.999999940395) A[3]:(2.57498338197e-29)\n",
      " state (7)  A[0]:(7.78921580014e-11) A[1]:(3.0152928765e-11) A[2]:(1.0) A[3]:(5.8816574544e-31)\n",
      " state (8)  A[0]:(4.41667848128e-11) A[1]:(2.39941105179e-11) A[2]:(1.0) A[3]:(4.53311434841e-31)\n",
      " state (9)  A[0]:(3.90228717395e-11) A[1]:(2.28906234245e-11) A[2]:(1.0) A[3]:(4.30271886624e-31)\n",
      " state (10)  A[0]:(3.74974495898e-11) A[1]:(2.25556899547e-11) A[2]:(1.0) A[3]:(4.23394351296e-31)\n",
      " state (11)  A[0]:(3.69317389171e-11) A[1]:(2.24318723319e-11) A[2]:(1.0) A[3]:(4.2087578412e-31)\n",
      " state (12)  A[0]:(3.66997335299e-11) A[1]:(2.23815271871e-11) A[2]:(1.0) A[3]:(4.1986236693e-31)\n",
      " state (13)  A[0]:(3.65990709961e-11) A[1]:(2.23597668159e-11) A[2]:(1.0) A[3]:(4.19430114147e-31)\n",
      " state (14)  A[0]:(3.65540028802e-11) A[1]:(2.23503004299e-11) A[2]:(1.0) A[3]:(4.1923817943e-31)\n",
      " state (15)  A[0]:(3.65332347707e-11) A[1]:(2.23457814752e-11) A[2]:(1.0) A[3]:(4.19151804105e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 868000 finished after 22 . Running score: 0.13. Policy_loss: -92050.6113462, Value_loss: 1.22209899093. Times trained:               15478. Times reached goal: 137.               Steps done: 11534004.\n",
      " state (0)  A[0]:(0.996268093586) A[1]:(0.00151054386515) A[2]:(0.00135199888609) A[3]:(0.000869360286742)\n",
      " state (1)  A[0]:(4.22175935455e-06) A[1]:(6.42156749109e-07) A[2]:(3.82249709219e-06) A[3]:(0.999991297722)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(9.21852727487e-09) A[2]:(1.83371824392e-08) A[3]:(2.66708655161e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.81443539149e-11) A[2]:(1.61245072849e-10) A[3]:(1.75865182586e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.9144712605e-11) A[2]:(2.08491585107e-10) A[3]:(1.22966666534e-19)\n",
      " state (5)  A[0]:(0.00286152469926) A[1]:(4.22120137955e-07) A[2]:(0.997138082981) A[3]:(5.12196736138e-25)\n",
      " state (6)  A[0]:(7.7558306999e-11) A[1]:(2.9798906398e-11) A[2]:(1.0) A[3]:(5.19597279165e-31)\n",
      " state (7)  A[0]:(2.61932212031e-11) A[1]:(1.90498866681e-11) A[2]:(1.0) A[3]:(3.07879391307e-31)\n",
      " state (8)  A[0]:(2.25012907612e-11) A[1]:(1.79910617654e-11) A[2]:(1.0) A[3]:(2.88815764652e-31)\n",
      " state (9)  A[0]:(2.16255000018e-11) A[1]:(1.77318090766e-11) A[2]:(1.0) A[3]:(2.84225318153e-31)\n",
      " state (10)  A[0]:(2.13373034519e-11) A[1]:(1.7646044348e-11) A[2]:(1.0) A[3]:(2.82711445998e-31)\n",
      " state (11)  A[0]:(2.12267304428e-11) A[1]:(1.76130238866e-11) A[2]:(1.0) A[3]:(2.82129670334e-31)\n",
      " state (12)  A[0]:(2.11801409744e-11) A[1]:(1.75989205847e-11) A[2]:(1.0) A[3]:(2.81880089374e-31)\n",
      " state (13)  A[0]:(2.11588212229e-11) A[1]:(1.75923425133e-11) A[2]:(1.0) A[3]:(2.81763974042e-31)\n",
      " state (14)  A[0]:(2.11486540086e-11) A[1]:(1.75892547055e-11) A[2]:(1.0) A[3]:(2.8171025395e-31)\n",
      " state (15)  A[0]:(2.11435712688e-11) A[1]:(1.75876448821e-11) A[2]:(1.0) A[3]:(2.81680161295e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 869000 finished after 5 . Running score: 0.09. Policy_loss: -92050.6110871, Value_loss: 0.991533511368. Times trained:               15536. Times reached goal: 100.               Steps done: 11549540.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9203e-11,  1.4760e-10,  9.8356e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9209e-11,  1.4762e-10,  9.8374e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9228e-11,  1.4768e-10,  9.8431e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9241e-11,  1.4773e-10,  9.8472e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0025,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9250e-11,  1.4775e-10,  9.8496e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9252e-11,  1.4776e-10,  9.8504e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9260e-11,  1.4779e-10,  9.8527e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9267e-11,  1.4781e-10,  9.8549e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.6903e-11,  2.5594e-11,  1.0000e+00,  3.7269e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9272e-11,  1.4783e-10,  9.8563e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9276e-11,  1.4784e-10,  9.8576e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9280e-11,  1.4785e-10,  9.8589e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9287e-11,  1.4788e-10,  9.8610e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9289e-11,  1.4788e-10,  9.8616e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9291e-11,  1.4789e-10,  9.8622e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9299e-11,  1.4792e-10,  9.8646e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9303e-11,  1.4793e-10,  9.8660e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9306e-11,  1.4794e-10,  9.8666e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.6871e-11,  2.5596e-11,  1.0000e+00,  3.7276e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9309e-11,  1.4795e-10,  9.8677e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9311e-11,  1.4796e-10,  9.8682e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9312e-11,  1.4796e-10,  9.8686e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9325e-11,  1.4800e-10,  9.8724e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.6856e-11,  2.5597e-11,  1.0000e+00,  3.7280e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9329e-11,  1.4801e-10,  9.8737e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9341e-11,  1.4806e-10,  9.8776e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.6842e-11,  2.5598e-11,  1.0000e+00,  3.7283e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9346e-11,  1.4807e-10,  9.8789e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9352e-11,  1.4809e-10,  9.8807e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9371e-11,  1.4815e-10,  9.8867e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9374e-11,  1.4816e-10,  9.8875e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.6815e-11,  2.5599e-11,  1.0000e+00,  3.7290e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9378e-11,  1.4818e-10,  9.8888e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9960,  0.0010,  0.0026,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9384e-11,  1.4820e-10,  9.8908e-20]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.6807e-11,  2.5600e-11,  1.0000e+00,  3.7292e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.99603998661) A[1]:(0.000986889470369) A[2]:(0.00256163300946) A[3]:(0.000411516055465)\n",
      " state (1)  A[0]:(6.08613163422e-06) A[1]:(8.07516869372e-07) A[2]:(5.74377691009e-06) A[3]:(0.999987363815)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(9.77192460283e-09) A[2]:(2.05768611039e-08) A[3]:(2.91669999264e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.90666014763e-11) A[2]:(1.24764629184e-10) A[3]:(1.52299683746e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.93883120564e-11) A[2]:(1.48211123929e-10) A[3]:(9.89196278607e-20)\n",
      " state (5)  A[0]:(0.0140893412754) A[1]:(1.07775417746e-06) A[2]:(0.985909581184) A[3]:(1.84686636016e-24)\n",
      " state (6)  A[0]:(1.69815703166e-10) A[1]:(4.7971494832e-11) A[2]:(1.0) A[3]:(7.80132859666e-31)\n",
      " state (7)  A[0]:(4.43775537151e-11) A[1]:(2.74765557529e-11) A[2]:(1.0) A[3]:(4.03842353759e-31)\n",
      " state (8)  A[0]:(3.6805582887e-11) A[1]:(2.56001244509e-11) A[2]:(1.0) A[3]:(3.72920810291e-31)\n",
      " state (9)  A[0]:(3.50613670042e-11) A[1]:(2.51473373375e-11) A[2]:(1.0) A[3]:(3.65587911962e-31)\n",
      " state (10)  A[0]:(3.44931791463e-11) A[1]:(2.49982274464e-11) A[2]:(1.0) A[3]:(3.63185953825e-31)\n",
      " state (11)  A[0]:(3.4276487898e-11) A[1]:(2.49409815717e-11) A[2]:(1.0) A[3]:(3.6226718744e-31)\n",
      " state (12)  A[0]:(3.41852136876e-11) A[1]:(2.49166364624e-11) A[2]:(1.0) A[3]:(3.61874924975e-31)\n",
      " state (13)  A[0]:(3.41437711437e-11) A[1]:(2.49053295348e-11) A[2]:(1.0) A[3]:(3.61692770371e-31)\n",
      " state (14)  A[0]:(3.41241097879e-11) A[1]:(2.49001045477e-11) A[2]:(1.0) A[3]:(3.61607217892e-31)\n",
      " state (15)  A[0]:(3.41142149252e-11) A[1]:(2.4897254397e-11) A[2]:(1.0) A[3]:(3.61563089834e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 870000 finished after 85 . Running score: 0.16. Policy_loss: -92050.6111824, Value_loss: 0.978241239418. Times trained:               16171. Times reached goal: 113.               Steps done: 11565711.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996705830097) A[1]:(0.000749741855543) A[2]:(0.00178017583676) A[3]:(0.000764260941651)\n",
      " state (1)  A[0]:(4.67741256216e-06) A[1]:(5.94115306285e-07) A[2]:(3.63708659279e-06) A[3]:(0.999991118908)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.51354104197e-10) A[2]:(3.60450086445e-10) A[3]:(4.22799642519e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.12367327784e-11) A[2]:(1.00214871124e-10) A[3]:(6.38617760886e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.93223829834e-11) A[2]:(2.05495828687e-10) A[3]:(1.30856392765e-19)\n",
      " state (5)  A[0]:(5.28974851477e-06) A[1]:(1.16415419527e-08) A[2]:(0.999994695187) A[3]:(2.34320712615e-27)\n",
      " state (6)  A[0]:(7.09224276529e-11) A[1]:(3.73742078952e-11) A[2]:(1.0) A[3]:(9.24178736752e-31)\n",
      " state (7)  A[0]:(3.85007720149e-11) A[1]:(2.95006519213e-11) A[2]:(1.0) A[3]:(7.04882742442e-31)\n",
      " state (8)  A[0]:(3.51510938412e-11) A[1]:(2.85301469949e-11) A[2]:(1.0) A[3]:(6.79054122289e-31)\n",
      " state (9)  A[0]:(3.43634218303e-11) A[1]:(2.82966792359e-11) A[2]:(1.0) A[3]:(6.72880990196e-31)\n",
      " state (10)  A[0]:(3.41200713516e-11) A[1]:(2.82240186084e-11) A[2]:(1.0) A[3]:(6.7096371189e-31)\n",
      " state (11)  A[0]:(3.40332380333e-11) A[1]:(2.81979752048e-11) A[2]:(1.0) A[3]:(6.70278116565e-31)\n",
      " state (12)  A[0]:(3.39992409226e-11) A[1]:(2.81876501307e-11) A[2]:(1.0) A[3]:(6.70002016452e-31)\n",
      " state (13)  A[0]:(3.39851063957e-11) A[1]:(2.81832421983e-11) A[2]:(1.0) A[3]:(6.69889592172e-31)\n",
      " state (14)  A[0]:(3.39788856774e-11) A[1]:(2.81814155345e-11) A[2]:(1.0) A[3]:(6.69838481678e-31)\n",
      " state (15)  A[0]:(3.3975901953e-11) A[1]:(2.81804475588e-11) A[2]:(1.0) A[3]:(6.69812902921e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 871000 finished after 29 . Running score: 0.11. Policy_loss: -92050.6111788, Value_loss: 1.4285933653. Times trained:               16042. Times reached goal: 118.               Steps done: 11581753.\n",
      " state (0)  A[0]:(0.997729063034) A[1]:(0.000612465606537) A[2]:(0.00119638047181) A[3]:(0.000462094612885)\n",
      " state (1)  A[0]:(5.27574320586e-06) A[1]:(6.33475337963e-07) A[2]:(3.28187456944e-06) A[3]:(0.999990820885)\n",
      " state (2)  A[0]:(1.0) A[1]:(8.58653093072e-10) A[2]:(1.20230159162e-09) A[3]:(6.19127596413e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.76244001232e-11) A[2]:(8.28161983435e-11) A[3]:(9.47115928391e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.0884934327e-11) A[2]:(1.52685031285e-10) A[3]:(1.36300818008e-19)\n",
      " state (5)  A[0]:(1.49181223605e-05) A[1]:(1.98511465044e-08) A[2]:(0.999985039234) A[3]:(6.14567211186e-27)\n",
      " state (6)  A[0]:(8.56513124314e-11) A[1]:(3.82233134033e-11) A[2]:(1.0) A[3]:(1.17554033616e-30)\n",
      " state (7)  A[0]:(4.35310017199e-11) A[1]:(2.92702945215e-11) A[2]:(1.0) A[3]:(8.66028912008e-31)\n",
      " state (8)  A[0]:(3.92451557363e-11) A[1]:(2.81541231301e-11) A[2]:(1.0) A[3]:(8.29326404872e-31)\n",
      " state (9)  A[0]:(3.8166924482e-11) A[1]:(2.78630816336e-11) A[2]:(1.0) A[3]:(8.19820887354e-31)\n",
      " state (10)  A[0]:(3.779053806e-11) A[1]:(2.77594301712e-11) A[2]:(1.0) A[3]:(8.16431513962e-31)\n",
      " state (11)  A[0]:(3.76320051509e-11) A[1]:(2.77147783889e-11) A[2]:(1.0) A[3]:(8.14975311561e-31)\n",
      " state (12)  A[0]:(3.75558542598e-11) A[1]:(2.76931134274e-11) A[2]:(1.0) A[3]:(8.14254310346e-31)\n",
      " state (13)  A[0]:(3.75161915422e-11) A[1]:(2.76814959843e-11) A[2]:(1.0) A[3]:(8.13869312436e-31)\n",
      " state (14)  A[0]:(3.74940183068e-11) A[1]:(2.7675054956e-11) A[2]:(1.0) A[3]:(8.1365198704e-31)\n",
      " state (15)  A[0]:(3.74812889059e-11) A[1]:(2.76711483588e-11) A[2]:(1.0) A[3]:(8.13527854837e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 872000 finished after 5 . Running score: 0.14. Policy_loss: -92050.6111767, Value_loss: 0.990935311281. Times trained:               15552. Times reached goal: 112.               Steps done: 11597305.\n",
      " state (0)  A[0]:(0.992805421352) A[1]:(0.00197049113922) A[2]:(0.00409527914599) A[3]:(0.00112880719826)\n",
      " state (1)  A[0]:(3.88040052712e-06) A[1]:(7.28924931082e-07) A[2]:(3.3392370824e-06) A[3]:(0.999992072582)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(1.35161437598e-08) A[2]:(2.1978948439e-08) A[3]:(6.30552365877e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.56019127909e-11) A[2]:(1.37401645617e-10) A[3]:(2.97933615244e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.80232632774e-11) A[2]:(1.98060762235e-10) A[3]:(1.68100510038e-19)\n",
      " state (5)  A[0]:(0.000223658440518) A[1]:(1.36769529036e-07) A[2]:(0.999776184559) A[3]:(9.30237598426e-26)\n",
      " state (6)  A[0]:(6.579124745e-11) A[1]:(3.8431802879e-11) A[2]:(1.0) A[3]:(9.1193553991e-31)\n",
      " state (7)  A[0]:(2.57267054882e-11) A[1]:(2.58099652761e-11) A[2]:(1.0) A[3]:(5.70274251222e-31)\n",
      " state (8)  A[0]:(2.19241100968e-11) A[1]:(2.42181170951e-11) A[2]:(1.0) A[3]:(5.30650452546e-31)\n",
      " state (9)  A[0]:(2.08468416141e-11) A[1]:(2.37411808962e-11) A[2]:(1.0) A[3]:(5.1895545925e-31)\n",
      " state (10)  A[0]:(2.03859984449e-11) A[1]:(2.35310989444e-11) A[2]:(1.0) A[3]:(5.13814176098e-31)\n",
      " state (11)  A[0]:(2.01349718149e-11) A[1]:(2.34140692945e-11) A[2]:(1.0) A[3]:(5.10948791068e-31)\n",
      " state (12)  A[0]:(1.99773652482e-11) A[1]:(2.33394293475e-11) A[2]:(1.0) A[3]:(5.09119909937e-31)\n",
      " state (13)  A[0]:(1.9870882717e-11) A[1]:(2.32886470525e-11) A[2]:(1.0) A[3]:(5.07870688581e-31)\n",
      " state (14)  A[0]:(1.97962080756e-11) A[1]:(2.32527833793e-11) A[2]:(1.0) A[3]:(5.06988033383e-31)\n",
      " state (15)  A[0]:(1.97425131798e-11) A[1]:(2.32268093647e-11) A[2]:(1.0) A[3]:(5.06346354526e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 873000 finished after 5 . Running score: 0.1. Policy_loss: -92050.6112033, Value_loss: 0.981919171801. Times trained:               15953. Times reached goal: 113.               Steps done: 11613258.\n",
      " state (0)  A[0]:(0.996204733849) A[1]:(0.000994430040009) A[2]:(0.00177103292663) A[3]:(0.00102978444193)\n",
      " state (1)  A[0]:(5.25014684172e-06) A[1]:(7.51484947159e-07) A[2]:(2.84283350993e-06) A[3]:(0.999991178513)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.88257409661e-09) A[2]:(2.661816767e-09) A[3]:(7.10621700373e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.74656564853e-11) A[2]:(6.09492872838e-11) A[3]:(4.47816186251e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.06271683761e-11) A[2]:(6.65129618049e-11) A[3]:(8.06275108911e-20)\n",
      " state (5)  A[0]:(0.163086920977) A[1]:(3.12679503622e-06) A[2]:(0.83690994978) A[3]:(1.91021058923e-23)\n",
      " state (6)  A[0]:(4.36365971135e-10) A[1]:(5.49740010458e-11) A[2]:(1.0) A[3]:(1.75871389815e-30)\n",
      " state (7)  A[0]:(7.26419827068e-11) A[1]:(2.49108390166e-11) A[2]:(1.0) A[3]:(6.85958693987e-31)\n",
      " state (8)  A[0]:(5.63036527124e-11) A[1]:(2.24685253042e-11) A[2]:(1.0) A[3]:(6.10849495026e-31)\n",
      " state (9)  A[0]:(5.23999593716e-11) A[1]:(2.18343051972e-11) A[2]:(1.0) A[3]:(5.9175307208e-31)\n",
      " state (10)  A[0]:(5.0917364075e-11) A[1]:(2.15851139046e-11) A[2]:(1.0) A[3]:(5.84279137938e-31)\n",
      " state (11)  A[0]:(5.01828752164e-11) A[1]:(2.14590099162e-11) A[2]:(1.0) A[3]:(5.80493528911e-31)\n",
      " state (12)  A[0]:(4.97503878372e-11) A[1]:(2.13835043422e-11) A[2]:(1.0) A[3]:(5.78225953288e-31)\n",
      " state (13)  A[0]:(4.94674717855e-11) A[1]:(2.13335581839e-11) A[2]:(1.0) A[3]:(5.76723577468e-31)\n",
      " state (14)  A[0]:(4.92721731471e-11) A[1]:(2.12988376935e-11) A[2]:(1.0) A[3]:(5.75677340476e-31)\n",
      " state (15)  A[0]:(4.91329026076e-11) A[1]:(2.12739079825e-11) A[2]:(1.0) A[3]:(5.74922390984e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 874000 finished after 16 . Running score: 0.13. Policy_loss: -92050.6115125, Value_loss: 0.979763383134. Times trained:               16312. Times reached goal: 114.               Steps done: 11629570.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9946,  0.0032,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0032,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.9381e-11,  7.0952e-11,  1.2078e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0032,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0032,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.9388e-11,  7.0959e-11,  1.2079e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0032,  0.0013,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9946,  0.0032,  0.0013,  0.0009]])\n",
      "On state=0, selected action=1\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.3609e-06,  1.0838e-06,  2.2576e-06,  9.9999e-01]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0006,  0.0000,  0.0001,  0.9994]])\n",
      "On state=2, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.3967e-06,  1.0851e-06,  2.2667e-06,  9.9999e-01]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.4128e-06,  1.0858e-06,  2.2709e-06,  9.9999e-01]])\n",
      "On state=1, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.4274e-06,  1.0864e-06,  2.2747e-06,  9.9999e-01]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0030,  0.0012,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0030,  0.0012,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0030,  0.0012,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0030,  0.0012,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.7574e-11,  6.9442e-11,  1.1774e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0030,  0.0012,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0030,  0.0012,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.7312e-11,  6.9222e-11,  1.1728e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0029,  0.0012,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0029,  0.0012,  0.0008]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.7123e-11,  6.9064e-11,  1.1695e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  6.7073e-11,  6.9022e-11,  1.1686e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.9612e-11,  4.4950e-11,  1.0000e+00,  5.6617e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7540e-11,  4.3585e-11,  1.0000e+00,  5.4694e-31]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.99505084753) A[1]:(0.00291295512579) A[2]:(0.00120925239753) A[3]:(0.000826969742775)\n",
      " state (1)  A[0]:(4.52787389804e-06) A[1]:(1.08982396796e-06) A[2]:(2.30065370488e-06) A[3]:(0.999992072582)\n",
      " state (2)  A[0]:(0.000654526869766) A[1]:(2.23560891754e-05) A[2]:(6.44418396405e-05) A[3]:(0.999258697033)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.06532588307e-10) A[2]:(7.75293163002e-11) A[3]:(3.99201413494e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.69492725147e-11) A[2]:(6.89193146997e-11) A[3]:(1.16646101925e-19)\n",
      " state (5)  A[0]:(0.0897872745991) A[1]:(7.41802841731e-06) A[2]:(0.910205304623) A[3]:(2.43063711676e-23)\n",
      " state (6)  A[0]:(1.82362264178e-10) A[1]:(1.03950896502e-10) A[2]:(1.0) A[3]:(1.55563981992e-30)\n",
      " state (7)  A[0]:(3.75886786974e-11) A[1]:(4.97786742382e-11) A[2]:(1.0) A[3]:(6.3623058097e-31)\n",
      " state (8)  A[0]:(2.96404151223e-11) A[1]:(4.49438437855e-11) A[2]:(1.0) A[3]:(5.66134301178e-31)\n",
      " state (9)  A[0]:(2.75523371479e-11) A[1]:(4.35823425959e-11) A[2]:(1.0) A[3]:(5.46918965223e-31)\n",
      " state (10)  A[0]:(2.66395221854e-11) A[1]:(4.29690658055e-11) A[2]:(1.0) A[3]:(5.38337104146e-31)\n",
      " state (11)  A[0]:(2.60975737548e-11) A[1]:(4.2597939065e-11) A[2]:(1.0) A[3]:(5.33150211813e-31)\n",
      " state (12)  A[0]:(2.57135580189e-11) A[1]:(4.23313051279e-11) A[2]:(1.0) A[3]:(5.29433251656e-31)\n",
      " state (13)  A[0]:(2.54164102959e-11) A[1]:(4.2123023819e-11) A[2]:(1.0) A[3]:(5.26532977954e-31)\n",
      " state (14)  A[0]:(2.51766142656e-11) A[1]:(4.19539993024e-11) A[2]:(1.0) A[3]:(5.241761588e-31)\n",
      " state (15)  A[0]:(2.49789720158e-11) A[1]:(4.18138787484e-11) A[2]:(1.0) A[3]:(5.22228223601e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 875000 finished after 27 . Running score: 0.09. Policy_loss: -92050.6126704, Value_loss: 1.19909693977. Times trained:               15548. Times reached goal: 113.               Steps done: 11645118.\n",
      " state (0)  A[0]:(0.994934380054) A[1]:(0.00272868480533) A[2]:(0.00120428681839) A[3]:(0.00113262690138)\n",
      " state (1)  A[0]:(4.40962776338e-06) A[1]:(1.03500838122e-06) A[2]:(2.0658367248e-06) A[3]:(0.999992489815)\n",
      " state (2)  A[0]:(0.000119341806567) A[1]:(9.90178250504e-06) A[2]:(2.81964712485e-05) A[3]:(0.999842584133)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.18994036846e-10) A[2]:(8.08120168005e-11) A[3]:(1.61608602968e-15)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.96369620354e-11) A[2]:(5.67117047767e-11) A[3]:(1.55835683376e-19)\n",
      " state (5)  A[0]:(0.996461212635) A[1]:(2.05206947612e-06) A[2]:(0.00353674753569) A[3]:(1.54648584685e-21)\n",
      " state (6)  A[0]:(1.99954564017e-09) A[1]:(3.6791003577e-10) A[2]:(1.0) A[3]:(8.28400472674e-30)\n",
      " state (7)  A[0]:(5.96837301803e-11) A[1]:(6.37823890925e-11) A[2]:(1.0) A[3]:(8.82409378767e-31)\n",
      " state (8)  A[0]:(3.76792659573e-11) A[1]:(5.18788553783e-11) A[2]:(1.0) A[3]:(6.92114522804e-31)\n",
      " state (9)  A[0]:(3.32086094734e-11) A[1]:(4.91089183208e-11) A[2]:(1.0) A[3]:(6.49933459677e-31)\n",
      " state (10)  A[0]:(3.15291959208e-11) A[1]:(4.80238626643e-11) A[2]:(1.0) A[3]:(6.33658317212e-31)\n",
      " state (11)  A[0]:(3.06280001361e-11) A[1]:(4.7428169031e-11) A[2]:(1.0) A[3]:(6.24767912377e-31)\n",
      " state (12)  A[0]:(3.00130406639e-11) A[1]:(4.70154748466e-11) A[2]:(1.0) A[3]:(6.18620829239e-31)\n",
      " state (13)  A[0]:(2.95324042998e-11) A[1]:(4.66892982609e-11) A[2]:(1.0) A[3]:(6.13769234915e-31)\n",
      " state (14)  A[0]:(2.91314992962e-11) A[1]:(4.64151148072e-11) A[2]:(1.0) A[3]:(6.09704093331e-31)\n",
      " state (15)  A[0]:(2.87877152672e-11) A[1]:(4.61786338335e-11) A[2]:(1.0) A[3]:(6.0620220162e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 876000 finished after 22 . Running score: 0.17. Policy_loss: -92050.6112036, Value_loss: 1.40669676864. Times trained:               15618. Times reached goal: 112.               Steps done: 11660736.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995543420315) A[1]:(0.00172959745396) A[2]:(0.00162195775192) A[3]:(0.00110502471216)\n",
      " state (1)  A[0]:(6.69189330438e-06) A[1]:(1.26824920699e-06) A[2]:(3.3989927033e-06) A[3]:(0.999988615513)\n",
      " state (2)  A[0]:(0.99999922514) A[1]:(2.59520163581e-07) A[2]:(3.88555889685e-07) A[3]:(1.15138092838e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.74064607014e-11) A[2]:(6.42249170513e-11) A[3]:(1.25696132496e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.22107026951e-11) A[2]:(1.36440858611e-10) A[3]:(6.70514607864e-20)\n",
      " state (5)  A[0]:(2.08464030038e-07) A[1]:(3.00931990438e-09) A[2]:(0.999999761581) A[3]:(1.99610385006e-28)\n",
      " state (6)  A[0]:(4.26156540612e-11) A[1]:(3.51292432643e-11) A[2]:(1.0) A[3]:(5.09547366703e-31)\n",
      " state (7)  A[0]:(2.57216036664e-11) A[1]:(2.82649615518e-11) A[2]:(1.0) A[3]:(3.96412242046e-31)\n",
      " state (8)  A[0]:(2.32176847192e-11) A[1]:(2.70999403473e-11) A[2]:(1.0) A[3]:(3.78259200336e-31)\n",
      " state (9)  A[0]:(2.23607903027e-11) A[1]:(2.66861064496e-11) A[2]:(1.0) A[3]:(3.71895120941e-31)\n",
      " state (10)  A[0]:(2.19167513998e-11) A[1]:(2.64665164784e-11) A[2]:(1.0) A[3]:(3.68533982923e-31)\n",
      " state (11)  A[0]:(2.16269016584e-11) A[1]:(2.63207268481e-11) A[2]:(1.0) A[3]:(3.66302659547e-31)\n",
      " state (12)  A[0]:(2.14157493822e-11) A[1]:(2.62136128776e-11) A[2]:(1.0) A[3]:(3.64663080026e-31)\n",
      " state (13)  A[0]:(2.12545224476e-11) A[1]:(2.61313436167e-11) A[2]:(1.0) A[3]:(3.63407699079e-31)\n",
      " state (14)  A[0]:(2.11282519258e-11) A[1]:(2.6066431999e-11) A[2]:(1.0) A[3]:(3.62416475223e-31)\n",
      " state (15)  A[0]:(2.102838563e-11) A[1]:(2.60150737758e-11) A[2]:(1.0) A[3]:(3.61629293676e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 877000 finished after 5 . Running score: 0.13. Policy_loss: -92050.6112021, Value_loss: 1.4386526557. Times trained:               15580. Times reached goal: 113.               Steps done: 11676316.\n",
      " state (0)  A[0]:(0.995126485825) A[1]:(0.00175750616472) A[2]:(0.00140260718763) A[3]:(0.00171343062539)\n",
      " state (1)  A[0]:(5.71937198401e-06) A[1]:(1.04373668819e-06) A[2]:(2.89952595267e-06) A[3]:(0.999990344048)\n",
      " state (2)  A[0]:(1.0) A[1]:(4.85314632925e-09) A[2]:(5.95811977533e-09) A[3]:(7.23371154643e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.50041037822e-11) A[2]:(5.84753356847e-11) A[3]:(4.70942667253e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.16421596994e-10) A[2]:(1.89251792015e-09) A[3]:(2.35319907875e-19)\n",
      " state (5)  A[0]:(9.78975567278e-10) A[1]:(1.33115074519e-10) A[2]:(1.0) A[3]:(4.59550131307e-30)\n",
      " state (6)  A[0]:(2.6065039016e-11) A[1]:(2.15273059795e-11) A[2]:(1.0) A[3]:(4.21508858357e-31)\n",
      " state (7)  A[0]:(2.06008248654e-11) A[1]:(1.95077426435e-11) A[2]:(1.0) A[3]:(3.76422466903e-31)\n",
      " state (8)  A[0]:(1.95832725036e-11) A[1]:(1.91145259343e-11) A[2]:(1.0) A[3]:(3.68033833587e-31)\n",
      " state (9)  A[0]:(1.92195946658e-11) A[1]:(1.89709133819e-11) A[2]:(1.0) A[3]:(3.65011002844e-31)\n",
      " state (10)  A[0]:(1.903209014e-11) A[1]:(1.88953679092e-11) A[2]:(1.0) A[3]:(3.63429892413e-31)\n",
      " state (11)  A[0]:(1.89142625173e-11) A[1]:(1.88473507634e-11) A[2]:(1.0) A[3]:(3.62422000046e-31)\n",
      " state (12)  A[0]:(1.88330497031e-11) A[1]:(1.88138775392e-11) A[2]:(1.0) A[3]:(3.61720347468e-31)\n",
      " state (13)  A[0]:(1.8774946875e-11) A[1]:(1.87898498843e-11) A[2]:(1.0) A[3]:(3.6121843489e-31)\n",
      " state (14)  A[0]:(1.87323802303e-11) A[1]:(1.87721539702e-11) A[2]:(1.0) A[3]:(3.60846579008e-31)\n",
      " state (15)  A[0]:(1.87010372465e-11) A[1]:(1.87590533385e-11) A[2]:(1.0) A[3]:(3.60574146437e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 878000 finished after 5 . Running score: 0.1. Policy_loss: -92050.6112037, Value_loss: 1.19875390355. Times trained:               14847. Times reached goal: 115.               Steps done: 11691163.\n",
      " state (0)  A[0]:(0.996813058853) A[1]:(0.00100379076321) A[2]:(0.000871177413501) A[3]:(0.00131197960582)\n",
      " state (1)  A[0]:(5.97876805841e-06) A[1]:(8.81318385382e-07) A[2]:(2.67194832304e-06) A[3]:(0.999990463257)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.3784610099e-10) A[2]:(5.73906311363e-10) A[3]:(2.52188938135e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.49300424054e-11) A[2]:(4.07158300131e-11) A[3]:(6.27049536622e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.84822182401e-10) A[2]:(5.30263555287e-10) A[3]:(4.87466323772e-19)\n",
      " state (5)  A[0]:(0.999996840954) A[1]:(3.81406657368e-08) A[2]:(3.12381439471e-06) A[3]:(3.80371249864e-20)\n",
      " state (6)  A[0]:(7.27176075088e-06) A[1]:(2.1375143433e-08) A[2]:(0.999992728233) A[3]:(1.22991134251e-26)\n",
      " state (7)  A[0]:(1.39156325352e-10) A[1]:(4.20241584587e-11) A[2]:(1.0) A[3]:(1.31570929182e-30)\n",
      " state (8)  A[0]:(3.66907754179e-11) A[1]:(2.19112349792e-11) A[2]:(1.0) A[3]:(5.66026343777e-31)\n",
      " state (9)  A[0]:(2.93204731328e-11) A[1]:(1.9915505009e-11) A[2]:(1.0) A[3]:(5.06180280684e-31)\n",
      " state (10)  A[0]:(2.76272719979e-11) A[1]:(1.94422915267e-11) A[2]:(1.0) A[3]:(4.92733612726e-31)\n",
      " state (11)  A[0]:(2.70743150121e-11) A[1]:(1.92861351889e-11) A[2]:(1.0) A[3]:(4.88380945224e-31)\n",
      " state (12)  A[0]:(2.68509173229e-11) A[1]:(1.92223077733e-11) A[2]:(1.0) A[3]:(4.86617985817e-31)\n",
      " state (13)  A[0]:(2.67415430077e-11) A[1]:(1.9190510292e-11) A[2]:(1.0) A[3]:(4.85742618684e-31)\n",
      " state (14)  A[0]:(2.6678676629e-11) A[1]:(1.91722176329e-11) A[2]:(1.0) A[3]:(4.85235134262e-31)\n",
      " state (15)  A[0]:(2.66380997122e-11) A[1]:(1.91602272243e-11) A[2]:(1.0) A[3]:(4.84905807765e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 879000 finished after 30 . Running score: 0.17. Policy_loss: -92050.6111791, Value_loss: 1.20353494639. Times trained:               15287. Times reached goal: 107.               Steps done: 11706450.\n",
      "action_dist \n",
      "tensor([[ 0.9978,  0.0012,  0.0006,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9978,  0.0012,  0.0006,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.4714e-10,  8.1373e-10,  3.5223e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7045e-11,  1.5168e-11,  1.0000e+00,  3.7958e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.4714e-10,  8.1373e-10,  3.5224e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7045e-11,  1.5168e-11,  1.0000e+00,  3.7959e-31]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.4789e-11,  1.4637e-11,  1.0000e+00,  3.6465e-31]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.997760593891) A[1]:(0.00118796841707) A[2]:(0.000635388714727) A[3]:(0.000416056427639)\n",
      " state (1)  A[0]:(1.39500798468e-05) A[1]:(1.82177984698e-06) A[2]:(5.45907550986e-06) A[3]:(0.999978780746)\n",
      " state (2)  A[0]:(1.0) A[1]:(8.05623970312e-11) A[2]:(9.561749309e-11) A[3]:(3.89875289578e-15)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.9769380577e-11) A[2]:(4.04896949302e-11) A[3]:(1.17374563702e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.4714704989e-10) A[2]:(8.13735190341e-10) A[3]:(3.52242404448e-19)\n",
      " state (5)  A[0]:(0.937097072601) A[1]:(3.42424550581e-06) A[2]:(0.0628995373845) A[3]:(1.9438110277e-21)\n",
      " state (6)  A[0]:(3.36544858648e-09) A[1]:(2.01006780665e-10) A[2]:(1.0) A[3]:(1.34533473992e-29)\n",
      " state (7)  A[0]:(4.22351702223e-11) A[1]:(1.85241023909e-11) A[2]:(1.0) A[3]:(4.83786125886e-31)\n",
      " state (8)  A[0]:(2.70452063522e-11) A[1]:(1.51676293053e-11) A[2]:(1.0) A[3]:(3.79591928811e-31)\n",
      " state (9)  A[0]:(2.4788933059e-11) A[1]:(1.46370658649e-11) A[2]:(1.0) A[3]:(3.6465193634e-31)\n",
      " state (10)  A[0]:(2.41945699586e-11) A[1]:(1.44964812723e-11) A[2]:(1.0) A[3]:(3.60810796959e-31)\n",
      " state (11)  A[0]:(2.39971306715e-11) A[1]:(1.44494425106e-11) A[2]:(1.0) A[3]:(3.59541263061e-31)\n",
      " state (12)  A[0]:(2.39186240258e-11) A[1]:(1.44305487698e-11) A[2]:(1.0) A[3]:(3.59034154798e-31)\n",
      " state (13)  A[0]:(2.38814263503e-11) A[1]:(1.44214683598e-11) A[2]:(1.0) A[3]:(3.58790451309e-31)\n",
      " state (14)  A[0]:(2.38609375314e-11) A[1]:(1.44164081714e-11) A[2]:(1.0) A[3]:(3.58653600257e-31)\n",
      " state (15)  A[0]:(2.38486539544e-11) A[1]:(1.44134383248e-11) A[2]:(1.0) A[3]:(3.58571527241e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 880000 finished after 7 . Running score: 0.08. Policy_loss: -92050.6126869, Value_loss: 0.979762928968. Times trained:               15676. Times reached goal: 117.               Steps done: 11722126.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996538281441) A[1]:(0.00110323820263) A[2]:(0.00194088730495) A[3]:(0.000417615752667)\n",
      " state (1)  A[0]:(2.34211365751e-05) A[1]:(2.77422259387e-06) A[2]:(1.07322675831e-05) A[3]:(0.99996304512)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.59396817862e-11) A[2]:(5.05344203983e-11) A[3]:(1.32383427836e-16)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.72857370778e-11) A[2]:(3.83158678396e-11) A[3]:(6.27231025219e-20)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.80076930934e-10) A[2]:(9.89008874797e-10) A[3]:(2.75092716281e-19)\n",
      " state (5)  A[0]:(0.537125349045) A[1]:(6.09062772128e-06) A[2]:(0.462868511677) A[3]:(6.13236688296e-22)\n",
      " state (6)  A[0]:(1.70099756502e-09) A[1]:(1.27490171198e-10) A[2]:(1.0) A[3]:(6.68447627358e-30)\n",
      " state (7)  A[0]:(4.62101780452e-11) A[1]:(1.83662789838e-11) A[2]:(1.0) A[3]:(4.56730900888e-31)\n",
      " state (8)  A[0]:(3.1431926506e-11) A[1]:(1.54827227428e-11) A[2]:(1.0) A[3]:(3.7158314474e-31)\n",
      " state (9)  A[0]:(2.9112837141e-11) A[1]:(1.50090027212e-11) A[2]:(1.0) A[3]:(3.58801383406e-31)\n",
      " state (10)  A[0]:(2.85129558852e-11) A[1]:(1.48860680727e-11) A[2]:(1.0) A[3]:(3.55574863002e-31)\n",
      " state (11)  A[0]:(2.83272433288e-11) A[1]:(1.48478434409e-11) A[2]:(1.0) A[3]:(3.54583357027e-31)\n",
      " state (12)  A[0]:(2.82621582387e-11) A[1]:(1.48343681089e-11) A[2]:(1.0) A[3]:(3.54234540834e-31)\n",
      " state (13)  A[0]:(2.82364027993e-11) A[1]:(1.48289366897e-11) A[2]:(1.0) A[3]:(3.54096749386e-31)\n",
      " state (14)  A[0]:(2.82245563726e-11) A[1]:(1.48264490962e-11) A[2]:(1.0) A[3]:(3.54031909117e-31)\n",
      " state (15)  A[0]:(2.82184206557e-11) A[1]:(1.48250908077e-11) A[2]:(1.0) A[3]:(3.53996808856e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 881000 finished after 4 . Running score: 0.1. Policy_loss: -92050.611179, Value_loss: 1.20770063067. Times trained:               15705. Times reached goal: 119.               Steps done: 11737831.\n",
      " state (0)  A[0]:(0.994152843952) A[1]:(0.000889863993507) A[2]:(0.0021919910796) A[3]:(0.00276531861164)\n",
      " state (1)  A[0]:(8.78773789736e-06) A[1]:(8.93150229331e-07) A[2]:(4.1068065002e-06) A[3]:(0.999986231327)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.665517633e-11) A[2]:(4.68697129719e-11) A[3]:(4.26339210924e-16)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.08351851796e-11) A[2]:(3.0962801767e-11) A[3]:(1.31638944398e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.69392674834e-10) A[2]:(5.22724086238e-10) A[3]:(4.66181002057e-19)\n",
      " state (5)  A[0]:(0.00037545178202) A[1]:(1.10483661331e-07) A[2]:(0.999624431133) A[3]:(6.62940166516e-25)\n",
      " state (6)  A[0]:(1.10410590393e-10) A[1]:(1.95348494325e-11) A[2]:(1.0) A[3]:(1.49903760402e-30)\n",
      " state (7)  A[0]:(3.39230657453e-11) A[1]:(1.0947528474e-11) A[2]:(1.0) A[3]:(7.06476383644e-31)\n",
      " state (8)  A[0]:(2.93768967485e-11) A[1]:(1.03081310848e-11) A[2]:(1.0) A[3]:(6.58995909865e-31)\n",
      " state (9)  A[0]:(2.84794757222e-11) A[1]:(1.01819767895e-11) A[2]:(1.0) A[3]:(6.50092151414e-31)\n",
      " state (10)  A[0]:(2.82441587479e-11) A[1]:(1.01488990822e-11) A[2]:(1.0) A[3]:(6.47794859295e-31)\n",
      " state (11)  A[0]:(2.81742129626e-11) A[1]:(1.01389929438e-11) A[2]:(1.0) A[3]:(6.47113166611e-31)\n",
      " state (12)  A[0]:(2.81511151196e-11) A[1]:(1.01357065102e-11) A[2]:(1.0) A[3]:(6.46886108122e-31)\n",
      " state (13)  A[0]:(2.81425247689e-11) A[1]:(1.0134468785e-11) A[2]:(1.0) A[3]:(6.46802224845e-31)\n",
      " state (14)  A[0]:(2.81387690926e-11) A[1]:(1.01338893874e-11) A[2]:(1.0) A[3]:(6.46762728235e-31)\n",
      " state (15)  A[0]:(2.81369441635e-11) A[1]:(1.01336187705e-11) A[2]:(1.0) A[3]:(6.4674297993e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 882000 finished after 22 . Running score: 0.11. Policy_loss: -92050.611412, Value_loss: 1.20225605975. Times trained:               16243. Times reached goal: 112.               Steps done: 11754074.\n",
      " state (0)  A[0]:(0.996079266071) A[1]:(0.00141511263791) A[2]:(0.00141285790596) A[3]:(0.00109273870476)\n",
      " state (1)  A[0]:(1.37666493174e-05) A[1]:(1.84433883987e-06) A[2]:(4.91037326356e-06) A[3]:(0.999979496002)\n",
      " state (2)  A[0]:(1.0) A[1]:(9.17955017554e-11) A[2]:(9.19884723949e-11) A[3]:(1.06170895846e-14)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.38629342381e-11) A[2]:(2.60326950968e-11) A[3]:(1.26814400551e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(7.98023536142e-10) A[2]:(2.11521045124e-09) A[3]:(2.81739913834e-19)\n",
      " state (5)  A[0]:(9.16315414656e-08) A[1]:(1.33448152528e-09) A[2]:(0.999999880791) A[3]:(3.60650291387e-28)\n",
      " state (6)  A[0]:(4.83988543687e-11) A[1]:(1.94402150627e-11) A[2]:(1.0) A[3]:(8.01569315707e-31)\n",
      " state (7)  A[0]:(3.15764983605e-11) A[1]:(1.60499374208e-11) A[2]:(1.0) A[3]:(6.32687311858e-31)\n",
      " state (8)  A[0]:(2.9696550774e-11) A[1]:(1.56613437496e-11) A[2]:(1.0) A[3]:(6.15353989379e-31)\n",
      " state (9)  A[0]:(2.92906220112e-11) A[1]:(1.55782834549e-11) A[2]:(1.0) A[3]:(6.11754296538e-31)\n",
      " state (10)  A[0]:(2.91841099898e-11) A[1]:(1.55564294085e-11) A[2]:(1.0) A[3]:(6.10821518261e-31)\n",
      " state (11)  A[0]:(2.91527323115e-11) A[1]:(1.55499033788e-11) A[2]:(1.0) A[3]:(6.10541985704e-31)\n",
      " state (12)  A[0]:(2.91422823373e-11) A[1]:(1.55478269148e-11) A[2]:(1.0) A[3]:(6.10453494489e-31)\n",
      " state (13)  A[0]:(2.91381675732e-11) A[1]:(1.55468780211e-11) A[2]:(1.0) A[3]:(6.10416254828e-31)\n",
      " state (14)  A[0]:(2.91362801941e-11) A[1]:(1.55465224028e-11) A[2]:(1.0) A[3]:(6.10397587978e-31)\n",
      " state (15)  A[0]:(2.91352775239e-11) A[1]:(1.55462847456e-11) A[2]:(1.0) A[3]:(6.10388278063e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 883000 finished after 11 . Running score: 0.1. Policy_loss: -92050.6111806, Value_loss: 0.988244760542. Times trained:               15885. Times reached goal: 102.               Steps done: 11769959.\n",
      " state (0)  A[0]:(0.995868980885) A[1]:(0.000903505482711) A[2]:(0.0016253128415) A[3]:(0.00160222814884)\n",
      " state (1)  A[0]:(9.41243342822e-06) A[1]:(1.10104372197e-06) A[2]:(3.2655996165e-06) A[3]:(0.999986231327)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.31936156444e-10) A[2]:(1.25522231498e-10) A[3]:(7.34785293049e-14)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.2124861011e-11) A[2]:(2.18964863602e-11) A[3]:(1.8630824166e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.41306111046e-10) A[2]:(1.05325848043e-09) A[3]:(3.89340937933e-19)\n",
      " state (5)  A[0]:(1.10478106308e-06) A[1]:(5.61541435573e-09) A[2]:(0.999998867512) A[3]:(3.64613320767e-27)\n",
      " state (6)  A[0]:(7.72600594612e-11) A[1]:(2.58679223875e-11) A[2]:(1.0) A[3]:(1.429442508e-30)\n",
      " state (7)  A[0]:(4.24989314574e-11) A[1]:(1.97151947545e-11) A[2]:(1.0) A[3]:(1.01797425219e-30)\n",
      " state (8)  A[0]:(3.90301679865e-11) A[1]:(1.90521400739e-11) A[2]:(1.0) A[3]:(9.78998715139e-31)\n",
      " state (9)  A[0]:(3.82772979979e-11) A[1]:(1.8908562216e-11) A[2]:(1.0) A[3]:(9.70831756548e-31)\n",
      " state (10)  A[0]:(3.80687738277e-11) A[1]:(1.88687173525e-11) A[2]:(1.0) A[3]:(9.6859013584e-31)\n",
      " state (11)  A[0]:(3.80002904143e-11) A[1]:(1.88555490666e-11) A[2]:(1.0) A[3]:(9.6785145519e-31)\n",
      " state (12)  A[0]:(3.79731940336e-11) A[1]:(1.88501558113e-11) A[2]:(1.0) A[3]:(9.6755607697e-31)\n",
      " state (13)  A[0]:(3.79605930023e-11) A[1]:(1.88475658691e-11) A[2]:(1.0) A[3]:(9.67415864004e-31)\n",
      " state (14)  A[0]:(3.79539316642e-11) A[1]:(1.88463446238e-11) A[2]:(1.0) A[3]:(9.67342042958e-31)\n",
      " state (15)  A[0]:(3.79501673142e-11) A[1]:(1.88455535899e-11) A[2]:(1.0) A[3]:(9.67297750331e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 884000 finished after 5 . Running score: 0.1. Policy_loss: -92050.611179, Value_loss: 0.979510257606. Times trained:               15846. Times reached goal: 106.               Steps done: 11785805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9972,  0.0004,  0.0018,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.4497e-10,  2.0028e-10,  3.2259e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.2946e-11,  1.4639e-11,  1.0000e+00,  1.0264e-30]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.997237324715) A[1]:(0.000433645502198) A[2]:(0.00182355800644) A[3]:(0.000505480566062)\n",
      " state (1)  A[0]:(3.79405246349e-05) A[1]:(2.65063499683e-06) A[2]:(1.0632806152e-05) A[3]:(0.99994879961)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.43363143473e-10) A[2]:(4.75530115285e-10) A[3]:(2.66753416231e-13)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.99943705826e-12) A[2]:(1.30242388055e-11) A[3]:(1.44433218351e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.44964609761e-10) A[2]:(2.00263222672e-10) A[3]:(3.22595269017e-19)\n",
      " state (5)  A[0]:(0.00165691028815) A[1]:(2.40383826622e-07) A[2]:(0.998342871666) A[3]:(2.91987555509e-24)\n",
      " state (6)  A[0]:(2.14158413225e-10) A[1]:(2.89954640159e-11) A[2]:(1.0) A[3]:(2.53421987099e-30)\n",
      " state (7)  A[0]:(6.09919684202e-11) A[1]:(1.55283858688e-11) A[2]:(1.0) A[3]:(1.10047430135e-30)\n",
      " state (8)  A[0]:(5.29460850163e-11) A[1]:(1.46394684569e-11) A[2]:(1.0) A[3]:(1.02643593073e-30)\n",
      " state (9)  A[0]:(5.14040962574e-11) A[1]:(1.44723356563e-11) A[2]:(1.0) A[3]:(1.01330180321e-30)\n",
      " state (10)  A[0]:(5.10030941092e-11) A[1]:(1.44292277779e-11) A[2]:(1.0) A[3]:(1.00998295948e-30)\n",
      " state (11)  A[0]:(5.08880437788e-11) A[1]:(1.44169025676e-11) A[2]:(1.0) A[3]:(1.00904331631e-30)\n",
      " state (12)  A[0]:(5.08531133869e-11) A[1]:(1.44131633711e-11) A[2]:(1.0) A[3]:(1.00875856456e-30)\n",
      " state (13)  A[0]:(5.08424448376e-11) A[1]:(1.44120089127e-11) A[2]:(1.0) A[3]:(1.00867383493e-30)\n",
      " state (14)  A[0]:(5.08387602849e-11) A[1]:(1.44116793152e-11) A[2]:(1.0) A[3]:(1.00864308399e-30)\n",
      " state (15)  A[0]:(5.08375980202e-11) A[1]:(1.44115145165e-11) A[2]:(1.0) A[3]:(1.00863537275e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 885000 finished after 3 . Running score: 0.12. Policy_loss: -92050.6112093, Value_loss: 1.00322821071. Times trained:               15677. Times reached goal: 112.               Steps done: 11801482.\n",
      " state (0)  A[0]:(0.997108578682) A[1]:(0.000327035930241) A[2]:(0.00222510215826) A[3]:(0.000339257676387)\n",
      " state (1)  A[0]:(0.000118657837447) A[1]:(6.20136188445e-06) A[2]:(3.14870485454e-05) A[3]:(0.999843657017)\n",
      " state (2)  A[0]:(1.0) A[1]:(3.01075275821e-11) A[2]:(5.33485235155e-11) A[3]:(7.43672851893e-16)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.58121691624e-12) A[2]:(1.32027713415e-11) A[3]:(7.37101936316e-20)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.60813237726e-10) A[2]:(3.87107956801e-10) A[3]:(2.53878170207e-19)\n",
      " state (5)  A[0]:(6.00480660751e-08) A[1]:(5.48765921593e-10) A[2]:(0.999999940395) A[3]:(2.48676122568e-28)\n",
      " state (6)  A[0]:(6.28728735741e-11) A[1]:(1.25699945244e-11) A[2]:(1.0) A[3]:(1.07526069984e-30)\n",
      " state (7)  A[0]:(4.41885243674e-11) A[1]:(1.07603847707e-11) A[2]:(1.0) A[3]:(8.88658776934e-31)\n",
      " state (8)  A[0]:(4.20086118669e-11) A[1]:(1.05492810668e-11) A[2]:(1.0) A[3]:(8.69073630456e-31)\n",
      " state (9)  A[0]:(4.15367809592e-11) A[1]:(1.05043057586e-11) A[2]:(1.0) A[3]:(8.65025227911e-31)\n",
      " state (10)  A[0]:(4.14163738027e-11) A[1]:(1.04928921454e-11) A[2]:(1.0) A[3]:(8.64009412713e-31)\n",
      " state (11)  A[0]:(4.13843126434e-11) A[1]:(1.04898902065e-11) A[2]:(1.0) A[3]:(8.6374581986e-31)\n",
      " state (12)  A[0]:(4.13750006478e-11) A[1]:(1.04890098343e-11) A[2]:(1.0) A[3]:(8.636667326e-31)\n",
      " state (13)  A[0]:(4.13723187653e-11) A[1]:(1.04887704425e-11) A[2]:(1.0) A[3]:(8.63646984295e-31)\n",
      " state (14)  A[0]:(4.1371687326e-11) A[1]:(1.04886897778e-11) A[2]:(1.0) A[3]:(8.63640401527e-31)\n",
      " state (15)  A[0]:(4.13713716063e-11) A[1]:(1.04886897778e-11) A[2]:(1.0) A[3]:(8.63640401527e-31)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 886000 finished after 40 . Running score: 0.11. Policy_loss: -92050.6112091, Value_loss: 0.984688087423. Times trained:               15378. Times reached goal: 95.               Steps done: 11816860.\n",
      " state (0)  A[0]:(0.997043967247) A[1]:(0.000524089322425) A[2]:(0.00198336248286) A[3]:(0.000448564707767)\n",
      " state (1)  A[0]:(7.26445650798e-05) A[1]:(5.48446951143e-06) A[2]:(1.72285508597e-05) A[3]:(0.999904632568)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.0367052489e-09) A[2]:(2.51780307714e-09) A[3]:(4.90051272131e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.64304896214e-12) A[2]:(9.88481779068e-12) A[3]:(1.85807151155e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.07863218357e-10) A[2]:(1.13178404149e-10) A[3]:(3.02956579558e-19)\n",
      " state (5)  A[0]:(0.063342243433) A[1]:(1.66366123722e-06) A[2]:(0.936656117439) A[3]:(7.24228269079e-23)\n",
      " state (6)  A[0]:(3.91055965387e-10) A[1]:(4.12560090257e-11) A[2]:(1.0) A[3]:(4.28681273496e-30)\n",
      " state (7)  A[0]:(6.47989301106e-11) A[1]:(1.70622734691e-11) A[2]:(1.0) A[3]:(1.36348420342e-30)\n",
      " state (8)  A[0]:(5.37833527714e-11) A[1]:(1.580098552e-11) A[2]:(1.0) A[3]:(1.24935291783e-30)\n",
      " state (9)  A[0]:(5.17752576923e-11) A[1]:(1.55703228089e-11) A[2]:(1.0) A[3]:(1.22966442189e-30)\n",
      " state (10)  A[0]:(5.12425761545e-11) A[1]:(1.55094409537e-11) A[2]:(1.0) A[3]:(1.22456211813e-30)\n",
      " state (11)  A[0]:(5.108175688e-11) A[1]:(1.54911118655e-11) A[2]:(1.0) A[3]:(1.22304018209e-30)\n",
      " state (12)  A[0]:(5.10309225432e-11) A[1]:(1.54853803391e-11) A[2]:(1.0) A[3]:(1.22255503206e-30)\n",
      " state (13)  A[0]:(5.10139916421e-11) A[1]:(1.54834912253e-11) A[2]:(1.0) A[3]:(1.22239648138e-30)\n",
      " state (14)  A[0]:(5.10081525629e-11) A[1]:(1.5482840704e-11) A[2]:(1.0) A[3]:(1.22234052785e-30)\n",
      " state (15)  A[0]:(5.10062062031e-11) A[1]:(1.54826047816e-11) A[2]:(1.0) A[3]:(1.22232190802e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 887000 finished after 39 . Running score: 0.13. Policy_loss: -92050.6112074, Value_loss: 1.42492383907. Times trained:               15392. Times reached goal: 111.               Steps done: 11832252.\n",
      " state (0)  A[0]:(0.998161256313) A[1]:(0.00033640008769) A[2]:(0.00103261019103) A[3]:(0.000469757651445)\n",
      " state (1)  A[0]:(0.000168192127603) A[1]:(9.10517883312e-06) A[2]:(2.16789921978e-05) A[3]:(0.999801039696)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.32672024009e-11) A[2]:(1.61969673784e-11) A[3]:(9.25657086517e-16)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.17317916335e-12) A[2]:(5.51716615102e-12) A[3]:(1.7673255332e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.05407585438e-10) A[2]:(8.31726909567e-11) A[3]:(5.35031949029e-19)\n",
      " state (5)  A[0]:(0.968017995358) A[1]:(1.65701771948e-06) A[2]:(0.0319803357124) A[3]:(3.01603804022e-21)\n",
      " state (6)  A[0]:(1.6743650022e-08) A[1]:(3.7723438262e-10) A[2]:(1.0) A[3]:(1.57090762956e-28)\n",
      " state (7)  A[0]:(1.43792325269e-10) A[1]:(3.09518209674e-11) A[2]:(1.0) A[3]:(5.09145422867e-30)\n",
      " state (8)  A[0]:(8.73253136469e-11) A[1]:(2.49359387305e-11) A[2]:(1.0) A[3]:(3.93950700452e-30)\n",
      " state (9)  A[0]:(7.96189572605e-11) A[1]:(2.40460637513e-11) A[2]:(1.0) A[3]:(3.78517757372e-30)\n",
      " state (10)  A[0]:(7.76761432952e-11) A[1]:(2.3820740519e-11) A[2]:(1.0) A[3]:(3.74730634308e-30)\n",
      " state (11)  A[0]:(7.70822503049e-11) A[1]:(2.37519604679e-11) A[2]:(1.0) A[3]:(3.73585947313e-30)\n",
      " state (12)  A[0]:(7.68881347479e-11) A[1]:(2.3729501003e-11) A[2]:(1.0) A[3]:(3.7321276077e-30)\n",
      " state (13)  A[0]:(7.68221666836e-11) A[1]:(2.37218075044e-11) A[2]:(1.0) A[3]:(3.73087500092e-30)\n",
      " state (14)  A[0]:(7.67993099671e-11) A[1]:(2.37190943969e-11) A[2]:(1.0) A[3]:(3.73044806137e-30)\n",
      " state (15)  A[0]:(7.67916980005e-11) A[1]:(2.37182790769e-11) A[2]:(1.0) A[3]:(3.73030549742e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 888000 finished after 7 . Running score: 0.14. Policy_loss: -92050.6111753, Value_loss: 0.994414907368. Times trained:               15909. Times reached goal: 96.               Steps done: 11848161.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.998331904411) A[1]:(0.000262539688265) A[2]:(0.000907303008717) A[3]:(0.000498277484439)\n",
      " state (1)  A[0]:(0.000100785138784) A[1]:(5.6581266108e-06) A[2]:(1.30258031277e-05) A[3]:(0.999880552292)\n",
      " state (2)  A[0]:(1.0) A[1]:(5.02736241526e-10) A[2]:(3.42581407686e-10) A[3]:(9.18465498968e-13)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.70366193911e-12) A[2]:(5.00000778628e-12) A[3]:(5.47619621699e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.87201015379e-11) A[2]:(6.34673563704e-11) A[3]:(8.06201709556e-19)\n",
      " state (5)  A[0]:(0.979098796844) A[1]:(1.59000853728e-06) A[2]:(0.0208996348083) A[3]:(3.36416758311e-21)\n",
      " state (6)  A[0]:(1.44043914574e-08) A[1]:(4.60404048219e-10) A[2]:(1.0) A[3]:(2.18486978096e-28)\n",
      " state (7)  A[0]:(1.16962106667e-10) A[1]:(4.14933851167e-11) A[2]:(1.0) A[3]:(8.91473926036e-30)\n",
      " state (8)  A[0]:(6.82291168008e-11) A[1]:(3.31872064552e-11) A[2]:(1.0) A[3]:(6.88315550757e-30)\n",
      " state (9)  A[0]:(6.14375147978e-11) A[1]:(3.18889289974e-11) A[2]:(1.0) A[3]:(6.5903396767e-30)\n",
      " state (10)  A[0]:(5.96910160189e-11) A[1]:(3.15512130311e-11) A[2]:(1.0) A[3]:(6.51594536638e-30)\n",
      " state (11)  A[0]:(5.91409282658e-11) A[1]:(3.14446385596e-11) A[2]:(1.0) A[3]:(6.49267170671e-30)\n",
      " state (12)  A[0]:(5.89501503168e-11) A[1]:(3.14078346664e-11) A[2]:(1.0) A[3]:(6.48465201405e-30)\n",
      " state (13)  A[0]:(5.88793597212e-11) A[1]:(3.13941789232e-11) A[2]:(1.0) A[3]:(6.48163522535e-30)\n",
      " state (14)  A[0]:(5.88506118837e-11) A[1]:(3.13884300496e-11) A[2]:(1.0) A[3]:(6.48039841721e-30)\n",
      " state (15)  A[0]:(5.8838496575e-11) A[1]:(3.13861540924e-11) A[2]:(1.0) A[3]:(6.47990414535e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 889000 finished after 14 . Running score: 0.13. Policy_loss: -92050.6111753, Value_loss: 1.42939647744. Times trained:               16238. Times reached goal: 104.               Steps done: 11864399.\n",
      "action_dist \n",
      "tensor([[ 0.9985,  0.0004,  0.0006,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9985,  0.0004,  0.0006,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9985,  0.0004,  0.0006,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0948e-10,  5.4866e-11,  9.8322e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.1489e-11,  4.3856e-11,  1.0000e+00,  1.0332e-29]])\n",
      "On state=8, selected action=2\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0948e-10,  5.4873e-11,  9.8322e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9985,  0.0004,  0.0006,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9985,  0.0004,  0.0006,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0948e-10,  5.4884e-11,  9.8322e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.1472e-11,  4.3844e-11,  1.0000e+00,  1.0329e-29]])\n",
      "On state=8, selected action=2\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.998455345631) A[1]:(0.000367295229807) A[2]:(0.000647709239274) A[3]:(0.000529670331161)\n",
      " state (1)  A[0]:(6.85016275384e-05) A[1]:(5.00735541209e-06) A[2]:(7.93243816588e-06) A[3]:(0.999918580055)\n",
      " state (2)  A[0]:(0.999995946884) A[1]:(1.12762097615e-06) A[2]:(9.08791434995e-07) A[3]:(2.01449279302e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(6.75045826529e-12) A[2]:(4.25217760308e-12) A[3]:(2.67550183603e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.09485989719e-10) A[2]:(5.48903908437e-11) A[3]:(9.83215103619e-19)\n",
      " state (5)  A[0]:(0.0168240424246) A[1]:(1.19630249173e-06) A[2]:(0.983174741268) A[3]:(2.62289387896e-23)\n",
      " state (6)  A[0]:(4.28263924324e-10) A[1]:(1.00696943839e-10) A[2]:(1.0) A[3]:(2.76386143811e-29)\n",
      " state (7)  A[0]:(7.66909938332e-11) A[1]:(4.76886863332e-11) A[2]:(1.0) A[3]:(1.13282484628e-29)\n",
      " state (8)  A[0]:(6.14679904198e-11) A[1]:(4.3841135583e-11) A[2]:(1.0) A[3]:(1.03281107858e-29)\n",
      " state (9)  A[0]:(5.82829201567e-11) A[1]:(4.30128536955e-11) A[2]:(1.0) A[3]:(1.0118769724e-29)\n",
      " state (10)  A[0]:(5.72383217212e-11) A[1]:(4.2742972356e-11) A[2]:(1.0) A[3]:(1.00509845131e-29)\n",
      " state (11)  A[0]:(5.67976568555e-11) A[1]:(4.26296394018e-11) A[2]:(1.0) A[3]:(1.00224988055e-29)\n",
      " state (12)  A[0]:(5.65747136638e-11) A[1]:(4.25717863739e-11) A[2]:(1.0) A[3]:(1.000790462e-29)\n",
      " state (13)  A[0]:(5.64470935271e-11) A[1]:(4.25381813107e-11) A[2]:(1.0) A[3]:(9.9993568012e-30)\n",
      " state (14)  A[0]:(5.63674801279e-11) A[1]:(4.25166048201e-11) A[2]:(1.0) A[3]:(9.99394162787e-30)\n",
      " state (15)  A[0]:(5.63148243315e-11) A[1]:(4.25021719208e-11) A[2]:(1.0) A[3]:(9.99028236097e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 890000 finished after 10 . Running score: 0.09. Policy_loss: -92050.6111749, Value_loss: 0.983857126427. Times trained:               15508. Times reached goal: 102.               Steps done: 11879907.\n",
      " state (0)  A[0]:(0.997144162655) A[1]:(0.00132204289548) A[2]:(0.00116343656555) A[3]:(0.000370378664229)\n",
      " state (1)  A[0]:(0.000175859298906) A[1]:(2.47696789302e-05) A[2]:(2.24679952225e-05) A[3]:(0.999776899815)\n",
      " state (2)  A[0]:(0.999999701977) A[1]:(2.12292775359e-07) A[2]:(7.32579081841e-08) A[3]:(9.19411835554e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.99619019231e-11) A[2]:(5.77559181036e-12) A[3]:(1.33720694336e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.71570108015e-10) A[2]:(8.18110926226e-11) A[3]:(7.98129460753e-19)\n",
      " state (5)  A[0]:(0.00507388217375) A[1]:(1.60107265401e-06) A[2]:(0.994924545288) A[3]:(6.78588169205e-24)\n",
      " state (6)  A[0]:(2.42147107921e-10) A[1]:(1.89765689274e-10) A[2]:(1.0) A[3]:(1.51311980065e-29)\n",
      " state (7)  A[0]:(4.76954205297e-11) A[1]:(9.3859878203e-11) A[2]:(1.0) A[3]:(6.63260932508e-30)\n",
      " state (8)  A[0]:(3.8521925233e-11) A[1]:(8.65014171403e-11) A[2]:(1.0) A[3]:(6.07274111409e-30)\n",
      " state (9)  A[0]:(3.65789724899e-11) A[1]:(8.48950146293e-11) A[2]:(1.0) A[3]:(5.95401167076e-30)\n",
      " state (10)  A[0]:(3.5942148563e-11) A[1]:(8.43710379339e-11) A[2]:(1.0) A[3]:(5.91561532328e-30)\n",
      " state (11)  A[0]:(3.56745570584e-11) A[1]:(8.41511790806e-11) A[2]:(1.0) A[3]:(5.89948001762e-30)\n",
      " state (12)  A[0]:(3.5540084764e-11) A[1]:(8.40395392165e-11) A[2]:(1.0) A[3]:(5.89129368689e-30)\n",
      " state (13)  A[0]:(3.54638367284e-11) A[1]:(8.39757638427e-11) A[2]:(1.0) A[3]:(5.886576287e-30)\n",
      " state (14)  A[0]:(3.5416249794e-11) A[1]:(8.39344496684e-11) A[2]:(1.0) A[3]:(5.88356814993e-30)\n",
      " state (15)  A[0]:(3.53854584523e-11) A[1]:(8.39078806436e-11) A[2]:(1.0) A[3]:(5.8815485566e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 891000 finished after 17 . Running score: 0.18. Policy_loss: -92050.6111772, Value_loss: 1.41955543961. Times trained:               16655. Times reached goal: 120.               Steps done: 11896562.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.991468071938) A[1]:(0.00123729847837) A[2]:(0.00376617140137) A[3]:(0.00352847529575)\n",
      " state (1)  A[0]:(2.92234126391e-05) A[1]:(4.17674345954e-06) A[2]:(6.62443426336e-06) A[3]:(0.999959945679)\n",
      " state (2)  A[0]:(1.0) A[1]:(9.69735514111e-10) A[2]:(3.72353370359e-10) A[3]:(6.00603204323e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.97856002415e-11) A[2]:(7.23856608334e-12) A[3]:(1.15997042767e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.98872737722e-10) A[2]:(4.73488089014e-11) A[3]:(1.62231583459e-18)\n",
      " state (5)  A[0]:(0.999776482582) A[1]:(4.1395199446e-07) A[2]:(0.000223123352043) A[3]:(7.58464117973e-21)\n",
      " state (6)  A[0]:(0.000104153514258) A[1]:(9.53758103606e-08) A[2]:(0.999895751476) A[3]:(1.91348664968e-25)\n",
      " state (7)  A[0]:(1.28040000824e-09) A[1]:(2.54196219629e-10) A[2]:(1.0) A[3]:(7.59432635485e-29)\n",
      " state (8)  A[0]:(8.2101846155e-11) A[1]:(7.42322800451e-11) A[2]:(1.0) A[3]:(1.86717102549e-29)\n",
      " state (9)  A[0]:(4.84165832426e-11) A[1]:(5.9970146904e-11) A[2]:(1.0) A[3]:(1.48843419359e-29)\n",
      " state (10)  A[0]:(4.21809115414e-11) A[1]:(5.69019692476e-11) A[2]:(1.0) A[3]:(1.41021059287e-29)\n",
      " state (11)  A[0]:(4.02150812651e-11) A[1]:(5.59111194842e-11) A[2]:(1.0) A[3]:(1.38530004321e-29)\n",
      " state (12)  A[0]:(3.94088268962e-11) A[1]:(5.5505017249e-11) A[2]:(1.0) A[3]:(1.37513850581e-29)\n",
      " state (13)  A[0]:(3.90130497352e-11) A[1]:(5.53061312025e-11) A[2]:(1.0) A[3]:(1.37015350698e-29)\n",
      " state (14)  A[0]:(3.87897110266e-11) A[1]:(5.51937939797e-11) A[2]:(1.0) A[3]:(1.3673341261e-29)\n",
      " state (15)  A[0]:(3.86492469973e-11) A[1]:(5.51224656198e-11) A[2]:(1.0) A[3]:(1.3655305228e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 892000 finished after 21 . Running score: 0.11. Policy_loss: -92050.6111594, Value_loss: 1.03032981076. Times trained:               16175. Times reached goal: 122.               Steps done: 11912737.\n",
      " state (0)  A[0]:(0.996919214725) A[1]:(0.000771255115978) A[2]:(0.00124675221741) A[3]:(0.00106279738247)\n",
      " state (1)  A[0]:(9.01767998585e-05) A[1]:(9.33934370551e-06) A[2]:(9.73298483586e-06) A[3]:(0.999890744686)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.96060909122e-09) A[2]:(9.45567513178e-10) A[3]:(3.42235753625e-11)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.38258076862e-11) A[2]:(3.61179228886e-12) A[3]:(6.33945389837e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.20808232809e-10) A[2]:(3.56504097077e-11) A[3]:(2.35131589869e-18)\n",
      " state (5)  A[0]:(0.634121656418) A[1]:(9.12542600418e-06) A[2]:(0.365869194269) A[3]:(1.45598974328e-21)\n",
      " state (6)  A[0]:(2.41714523952e-08) A[1]:(9.12352360327e-10) A[2]:(1.0) A[3]:(8.63038963698e-28)\n",
      " state (7)  A[0]:(1.52641635442e-10) A[1]:(7.9494529015e-11) A[2]:(1.0) A[3]:(4.81029952745e-29)\n",
      " state (8)  A[0]:(7.30741647748e-11) A[1]:(5.8267141112e-11) A[2]:(1.0) A[3]:(3.45614839182e-29)\n",
      " state (9)  A[0]:(6.29643212569e-11) A[1]:(5.49677109385e-11) A[2]:(1.0) A[3]:(3.25769304719e-29)\n",
      " state (10)  A[0]:(6.02974892239e-11) A[1]:(5.40800182414e-11) A[2]:(1.0) A[3]:(3.20527946561e-29)\n",
      " state (11)  A[0]:(5.93411569882e-11) A[1]:(5.37636601905e-11) A[2]:(1.0) A[3]:(3.18665060729e-29)\n",
      " state (12)  A[0]:(5.89240739535e-11) A[1]:(5.36266274442e-11) A[2]:(1.0) A[3]:(3.17856501172e-29)\n",
      " state (13)  A[0]:(5.8710696027e-11) A[1]:(5.35567111493e-11) A[2]:(1.0) A[3]:(3.17442065122e-29)\n",
      " state (14)  A[0]:(5.85869755487e-11) A[1]:(5.35156606529e-11) A[2]:(1.0) A[3]:(3.1719996971e-29)\n",
      " state (15)  A[0]:(5.85081358362e-11) A[1]:(5.34891297921e-11) A[2]:(1.0) A[3]:(3.1704029808e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 893000 finished after 4 . Running score: 0.1. Policy_loss: -92050.6111766, Value_loss: 1.42731328284. Times trained:               16033. Times reached goal: 119.               Steps done: 11928770.\n",
      " state (0)  A[0]:(0.997158408165) A[1]:(0.00145702774171) A[2]:(0.000585708767176) A[3]:(0.000798836234026)\n",
      " state (1)  A[0]:(9.63533166214e-05) A[1]:(1.2692047676e-05) A[2]:(8.04048886494e-06) A[3]:(0.999882936478)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.55185070034e-09) A[2]:(1.43720557677e-09) A[3]:(1.29358371237e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.96970027094e-11) A[2]:(2.45324810813e-12) A[3]:(6.76873251215e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(6.1151117503e-10) A[2]:(9.03923394469e-11) A[3]:(2.50024294374e-18)\n",
      " state (5)  A[0]:(6.05476657256e-07) A[1]:(6.44275077732e-09) A[2]:(0.999999403954) A[3]:(1.45936817683e-26)\n",
      " state (6)  A[0]:(1.08519852826e-10) A[1]:(8.41203634527e-11) A[2]:(1.0) A[3]:(7.90227412702e-29)\n",
      " state (7)  A[0]:(5.83274054056e-11) A[1]:(6.46505349255e-11) A[2]:(1.0) A[3]:(6.01884704909e-29)\n",
      " state (8)  A[0]:(5.28048924031e-11) A[1]:(6.223076221e-11) A[2]:(1.0) A[3]:(5.79356981979e-29)\n",
      " state (9)  A[0]:(5.12574323264e-11) A[1]:(6.15785269997e-11) A[2]:(1.0) A[3]:(5.73280492449e-29)\n",
      " state (10)  A[0]:(5.05897788938e-11) A[1]:(6.13073480871e-11) A[2]:(1.0) A[3]:(5.70744945491e-29)\n",
      " state (11)  A[0]:(5.02209836217e-11) A[1]:(6.11599521028e-11) A[2]:(1.0) A[3]:(5.6936188705e-29)\n",
      " state (12)  A[0]:(4.99883745508e-11) A[1]:(6.10664643852e-11) A[2]:(1.0) A[3]:(5.68480774101e-29)\n",
      " state (13)  A[0]:(4.9831302279e-11) A[1]:(6.10026751335e-11) A[2]:(1.0) A[3]:(5.67882532112e-29)\n",
      " state (14)  A[0]:(4.97206026351e-11) A[1]:(6.09566147558e-11) A[2]:(1.0) A[3]:(5.67449438615e-29)\n",
      " state (15)  A[0]:(4.96406249439e-11) A[1]:(6.09231345927e-11) A[2]:(1.0) A[3]:(5.67129192577e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 894000 finished after 17 . Running score: 0.11. Policy_loss: -92050.6111757, Value_loss: 1.43076198405. Times trained:               15472. Times reached goal: 111.               Steps done: 11944242.\n",
      "action_dist \n",
      "tensor([[ 0.9974,  0.0011,  0.0011,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9974,  0.0011,  0.0011,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.9209e-10,  4.0790e-11,  2.2987e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.3629e-11,  4.5252e-11,  1.0000e+00,  4.8438e-29]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.3130e-11,  4.5094e-11,  1.0000e+00,  4.8260e-29]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.997448921204) A[1]:(0.0010791439563) A[2]:(0.00111571245361) A[3]:(0.000356219738023)\n",
      " state (1)  A[0]:(0.000252117431955) A[1]:(2.49171971518e-05) A[2]:(2.20522251766e-05) A[3]:(0.999700903893)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.17550580381e-09) A[2]:(2.64739008493e-10) A[3]:(2.35937354723e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.45480285804e-11) A[2]:(2.48358690384e-12) A[3]:(2.9971516376e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.93514851313e-10) A[2]:(4.10794939731e-11) A[3]:(2.29783805171e-18)\n",
      " state (5)  A[0]:(8.32153013164e-09) A[1]:(5.52652423824e-10) A[2]:(1.0) A[3]:(8.79191834438e-28)\n",
      " state (6)  A[0]:(5.59433332992e-11) A[1]:(4.98594429632e-11) A[2]:(1.0) A[3]:(5.34690573968e-29)\n",
      " state (7)  A[0]:(4.52550635854e-11) A[1]:(4.58288823557e-11) A[2]:(1.0) A[3]:(4.90644646417e-29)\n",
      " state (8)  A[0]:(4.36405703241e-11) A[1]:(4.525748179e-11) A[2]:(1.0) A[3]:(4.84410802392e-29)\n",
      " state (9)  A[0]:(4.31351031283e-11) A[1]:(4.50966937404e-11) A[2]:(1.0) A[3]:(4.82617972262e-29)\n",
      " state (10)  A[0]:(4.29089541676e-11) A[1]:(4.50287897247e-11) A[2]:(1.0) A[3]:(4.81849044732e-29)\n",
      " state (11)  A[0]:(4.2785712473e-11) A[1]:(4.49923917567e-11) A[2]:(1.0) A[3]:(4.8143379618e-29)\n",
      " state (12)  A[0]:(4.27108626244e-11) A[1]:(4.49697397376e-11) A[2]:(1.0) A[3]:(4.81176744718e-29)\n",
      " state (13)  A[0]:(4.26629877259e-11) A[1]:(4.49548176462e-11) A[2]:(1.0) A[3]:(4.81007924921e-29)\n",
      " state (14)  A[0]:(4.2631263103e-11) A[1]:(4.49447007389e-11) A[2]:(1.0) A[3]:(4.8089050338e-29)\n",
      " state (15)  A[0]:(4.26096415096e-11) A[1]:(4.49375016365e-11) A[2]:(1.0) A[3]:(4.80809794878e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 895000 finished after 5 . Running score: 0.07. Policy_loss: -92050.6125179, Value_loss: 1.21817580468. Times trained:               15669. Times reached goal: 110.               Steps done: 11959911.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.998644411564) A[1]:(0.000843167654239) A[2]:(0.000464544951683) A[3]:(4.78686160932e-05)\n",
      " state (1)  A[0]:(0.00454406719655) A[1]:(0.000245157774771) A[2]:(0.000150370178744) A[3]:(0.995060384274)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.98659196732e-11) A[2]:(2.76337936908e-12) A[3]:(1.06482695556e-16)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.06085227408e-11) A[2]:(1.66385264941e-12) A[3]:(3.80559872893e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.67359148762e-09) A[2]:(1.38956679496e-09) A[3]:(4.8484586729e-19)\n",
      " state (5)  A[0]:(2.75747480405e-10) A[1]:(9.93386900516e-11) A[2]:(1.0) A[3]:(7.8222559508e-29)\n",
      " state (6)  A[0]:(6.45876824246e-11) A[1]:(5.24289500703e-11) A[2]:(1.0) A[3]:(3.87368639265e-29)\n",
      " state (7)  A[0]:(5.93038118613e-11) A[1]:(5.07759632951e-11) A[2]:(1.0) A[3]:(3.74775133822e-29)\n",
      " state (8)  A[0]:(5.83049372671e-11) A[1]:(5.04949172753e-11) A[2]:(1.0) A[3]:(3.72596997333e-29)\n",
      " state (9)  A[0]:(5.79873059292e-11) A[1]:(5.04140826307e-11) A[2]:(1.0) A[3]:(3.7195791961e-29)\n",
      " state (10)  A[0]:(5.78547383612e-11) A[1]:(5.03823614773e-11) A[2]:(1.0) A[3]:(3.71702613522e-29)\n",
      " state (11)  A[0]:(5.77898874587e-11) A[1]:(5.03667940688e-11) A[2]:(1.0) A[3]:(3.71575020663e-29)\n",
      " state (12)  A[0]:(5.77550680891e-11) A[1]:(5.0358342496e-11) A[2]:(1.0) A[3]:(3.71506981169e-29)\n",
      " state (13)  A[0]:(5.77354622444e-11) A[1]:(5.03533464924e-11) A[2]:(1.0) A[3]:(3.7146731905e-29)\n",
      " state (14)  A[0]:(5.77244502198e-11) A[1]:(5.03504668514e-11) A[2]:(1.0) A[3]:(3.71444629188e-29)\n",
      " state (15)  A[0]:(5.7717403773e-11) A[1]:(5.03487355974e-11) A[2]:(1.0) A[3]:(3.71430485639e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 896000 finished after 7 . Running score: 0.09. Policy_loss: -92050.6111755, Value_loss: 1.21283079478. Times trained:               14767. Times reached goal: 106.               Steps done: 11974678.\n",
      " state (0)  A[0]:(0.995930671692) A[1]:(0.0037246986758) A[2]:(0.000329832517309) A[3]:(1.47989794641e-05)\n",
      " state (1)  A[0]:(0.00255282479338) A[1]:(0.000322292529745) A[2]:(0.000100144432508) A[3]:(0.997024714947)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(7.99669805929e-08) A[2]:(9.00228425138e-09) A[3]:(5.44956746396e-10)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.38965080362e-11) A[2]:(1.65870366487e-12) A[3]:(4.76354289615e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.12743561288e-11) A[2]:(4.82888251954e-12) A[3]:(1.00008617599e-18)\n",
      " state (5)  A[0]:(0.492442905903) A[1]:(1.31964816319e-05) A[2]:(0.507543921471) A[3]:(2.53970196474e-21)\n",
      " state (6)  A[0]:(2.11715880938e-10) A[1]:(1.55141829938e-10) A[2]:(1.0) A[3]:(9.54411496013e-29)\n",
      " state (7)  A[0]:(6.93218121794e-11) A[1]:(9.53321310782e-11) A[2]:(1.0) A[3]:(5.65526156825e-29)\n",
      " state (8)  A[0]:(6.06384109592e-11) A[1]:(9.05459179856e-11) A[2]:(1.0) A[3]:(5.36443471144e-29)\n",
      " state (9)  A[0]:(5.84951878602e-11) A[1]:(8.94152668574e-11) A[2]:(1.0) A[3]:(5.29615568008e-29)\n",
      " state (10)  A[0]:(5.76344597047e-11) A[1]:(8.89814333327e-11) A[2]:(1.0) A[3]:(5.26999673661e-29)\n",
      " state (11)  A[0]:(5.71835460927e-11) A[1]:(8.87580217657e-11) A[2]:(1.0) A[3]:(5.25650439365e-29)\n",
      " state (12)  A[0]:(5.69084813995e-11) A[1]:(8.86206663608e-11) A[2]:(1.0) A[3]:(5.24824997827e-29)\n",
      " state (13)  A[0]:(5.67262035955e-11) A[1]:(8.85287607111e-11) A[2]:(1.0) A[3]:(5.24268705e-29)\n",
      " state (14)  A[0]:(5.65988887702e-11) A[1]:(8.84625914188e-11) A[2]:(1.0) A[3]:(5.23872866211e-29)\n",
      " state (15)  A[0]:(5.65078504822e-11) A[1]:(8.84153653069e-11) A[2]:(1.0) A[3]:(5.23585180426e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 897000 finished after 10 . Running score: 0.09. Policy_loss: -92050.6111766, Value_loss: 0.995528282879. Times trained:               15082. Times reached goal: 101.               Steps done: 11989760.\n",
      " state (0)  A[0]:(0.996559858322) A[1]:(0.0012067790376) A[2]:(0.00207555643283) A[3]:(0.000157824659254)\n",
      " state (1)  A[0]:(0.00444071600214) A[1]:(0.000302368658595) A[2]:(0.000587915361393) A[3]:(0.994669020176)\n",
      " state (2)  A[0]:(0.999997675419) A[1]:(1.17007368772e-06) A[2]:(1.03509069049e-06) A[3]:(1.17956943768e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.25062139464e-11) A[2]:(7.30521112352e-12) A[3]:(5.21592590521e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.05816248661e-11) A[2]:(2.10511399978e-11) A[3]:(9.53335685532e-19)\n",
      " state (5)  A[0]:(0.233623832464) A[1]:(2.35558445638e-06) A[2]:(0.766373813152) A[3]:(9.13493860052e-22)\n",
      " state (6)  A[0]:(5.6737455012e-11) A[1]:(1.57114286858e-11) A[2]:(1.0) A[3]:(2.4274777617e-29)\n",
      " state (7)  A[0]:(1.40845477642e-11) A[1]:(8.52776529947e-12) A[2]:(1.0) A[3]:(1.27685077758e-29)\n",
      " state (8)  A[0]:(1.18096782353e-11) A[1]:(7.96964057248e-12) A[2]:(1.0) A[3]:(1.19238249949e-29)\n",
      " state (9)  A[0]:(1.12408806222e-11) A[1]:(7.835614102e-12) A[2]:(1.0) A[3]:(1.17204385172e-29)\n",
      " state (10)  A[0]:(1.10012398497e-11) A[1]:(7.78166853871e-12) A[2]:(1.0) A[3]:(1.16381057644e-29)\n",
      " state (11)  A[0]:(1.08674319543e-11) A[1]:(7.75186251994e-12) A[2]:(1.0) A[3]:(1.15926440376e-29)\n",
      " state (12)  A[0]:(1.07800227078e-11) A[1]:(7.73225233841e-12) A[2]:(1.0) A[3]:(1.15628763829e-29)\n",
      " state (13)  A[0]:(1.07173705674e-11) A[1]:(7.71798944199e-12) A[2]:(1.0) A[3]:(1.1541107356e-29)\n",
      " state (14)  A[0]:(1.06698877164e-11) A[1]:(7.70701558128e-12) A[2]:(1.0) A[3]:(1.15244774024e-29)\n",
      " state (15)  A[0]:(1.06326284582e-11) A[1]:(7.69834716807e-12) A[2]:(1.0) A[3]:(1.1511296067e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 898000 finished after 18 . Running score: 0.18. Policy_loss: -92050.6111751, Value_loss: 1.21994374711. Times trained:               15706. Times reached goal: 108.               Steps done: 12005466.\n",
      " state (0)  A[0]:(0.997749149799) A[1]:(0.000467907928396) A[2]:(0.00177304795943) A[3]:(9.88355805021e-06)\n",
      " state (1)  A[0]:(0.0316221229732) A[1]:(0.00131673295982) A[2]:(0.00336806965061) A[3]:(0.963693082333)\n",
      " state (2)  A[0]:(0.999986350536) A[1]:(5.33668617209e-06) A[2]:(7.50993103793e-06) A[3]:(7.88062607171e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(7.14632433091e-12) A[2]:(7.11644068327e-12) A[3]:(2.39749946024e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.72589884692e-11) A[2]:(2.53458278671e-11) A[3]:(8.25518599907e-19)\n",
      " state (5)  A[0]:(0.0693026483059) A[1]:(6.71931957186e-07) A[2]:(0.930696666241) A[3]:(1.6522557869e-22)\n",
      " state (6)  A[0]:(5.29176147346e-11) A[1]:(8.26022410194e-12) A[2]:(1.0) A[3]:(1.77522201419e-29)\n",
      " state (7)  A[0]:(1.24654670267e-11) A[1]:(4.36831864084e-12) A[2]:(1.0) A[3]:(9.17725554885e-30)\n",
      " state (8)  A[0]:(1.01853568982e-11) A[1]:(4.03027524548e-12) A[2]:(1.0) A[3]:(8.47094782513e-30)\n",
      " state (9)  A[0]:(9.58528135336e-12) A[1]:(3.94074573321e-12) A[2]:(1.0) A[3]:(8.28507903454e-30)\n",
      " state (10)  A[0]:(9.32299983741e-12) A[1]:(3.90216635046e-12) A[2]:(1.0) A[3]:(8.20531468987e-30)\n",
      " state (11)  A[0]:(9.17488914703e-12) A[1]:(3.88036044269e-12) A[2]:(1.0) A[3]:(8.16055186499e-30)\n",
      " state (12)  A[0]:(9.07945593709e-12) A[1]:(3.86622027795e-12) A[2]:(1.0) A[3]:(8.13159069345e-30)\n",
      " state (13)  A[0]:(9.01323286839e-12) A[1]:(3.85630720065e-12) A[2]:(1.0) A[3]:(8.11139099853e-30)\n",
      " state (14)  A[0]:(8.96522526356e-12) A[1]:(3.84907643952e-12) A[2]:(1.0) A[3]:(8.09667569005e-30)\n",
      " state (15)  A[0]:(8.92945613284e-12) A[1]:(3.84370616932e-12) A[2]:(1.0) A[3]:(8.08574904688e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 899000 finished after 8 . Running score: 0.19. Policy_loss: -92050.6112111, Value_loss: 0.97877285806. Times trained:               15863. Times reached goal: 111.               Steps done: 12021329.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.8273,  0.0070,  0.0099,  0.1558]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8273,  0.0070,  0.0099,  0.1558]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.2881e-11,  4.0343e-11,  1.9011e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8273,  0.0070,  0.0099,  0.1558]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8273,  0.0070,  0.0099,  0.1558]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8273,  0.0070,  0.0099,  0.1558]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  8.2884e-11,  4.0345e-11,  1.9012e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8272,  0.0070,  0.0099,  0.1558]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8272,  0.0070,  0.0099,  0.1558]])\n",
      "On state=0, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8272,  0.0070,  0.0099,  0.1558]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.8272,  0.0070,  0.0099,  0.1558]])\n",
      "On state=0, selected action=3\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0007,  0.0001,  0.0002,  0.9989]])\n",
      "On state=1, selected action=3\n",
      "new state=2, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9999e-01,  2.6610e-06,  1.8083e-06,  5.5541e-07]])\n",
      "On state=2, selected action=0\n",
      "new state=6, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.6051e-10,  7.4400e-11,  1.0000e+00,  1.3165e-28]])\n",
      "On state=6, selected action=2\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.827639520168) A[1]:(0.00704269809648) A[2]:(0.00990500953048) A[3]:(0.155412793159)\n",
      " state (1)  A[0]:(0.000738286064006) A[1]:(0.000122976343846) A[2]:(0.000207629214856) A[3]:(0.998931109905)\n",
      " state (2)  A[0]:(0.99999499321) A[1]:(2.65850940195e-06) A[2]:(1.80641188763e-06) A[3]:(5.54149380605e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(3.07921847742e-11) A[2]:(1.38500105482e-11) A[3]:(2.56732956617e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(8.28776700046e-11) A[2]:(4.03417646511e-11) A[3]:(1.90029318816e-18)\n",
      " state (5)  A[0]:(0.998700916767) A[1]:(5.26451458427e-07) A[2]:(0.00129856565036) A[3]:(2.0893737321e-20)\n",
      " state (6)  A[0]:(7.6061423826e-10) A[1]:(7.44012421117e-11) A[2]:(1.0) A[3]:(1.31640345039e-28)\n",
      " state (7)  A[0]:(1.1682871684e-11) A[1]:(1.04700614498e-11) A[2]:(1.0) A[3]:(1.57368382748e-29)\n",
      " state (8)  A[0]:(6.77950317413e-12) A[1]:(8.30249237416e-12) A[2]:(1.0) A[3]:(1.24466788636e-29)\n",
      " state (9)  A[0]:(5.88285887018e-12) A[1]:(7.8494355113e-12) A[2]:(1.0) A[3]:(1.17624516257e-29)\n",
      " state (10)  A[0]:(5.54706974798e-12) A[1]:(7.67855050376e-12) A[2]:(1.0) A[3]:(1.15033072193e-29)\n",
      " state (11)  A[0]:(5.3697840445e-12) A[1]:(7.58822431973e-12) A[2]:(1.0) A[3]:(1.13666007998e-29)\n",
      " state (12)  A[0]:(5.25697107373e-12) A[1]:(7.53040685364e-12) A[2]:(1.0) A[3]:(1.12796081993e-29)\n",
      " state (13)  A[0]:(5.17760227053e-12) A[1]:(7.48935462258e-12) A[2]:(1.0) A[3]:(1.12183305252e-29)\n",
      " state (14)  A[0]:(5.1186476932e-12) A[1]:(7.45870552821e-12) A[2]:(1.0) A[3]:(1.11728898632e-29)\n",
      " state (15)  A[0]:(5.07362468011e-12) A[1]:(7.43524079111e-12) A[2]:(1.0) A[3]:(1.11383352194e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 900000 finished after 14 . Running score: 0.08. Policy_loss: -92050.612905, Value_loss: 0.979466053586. Times trained:               16981. Times reached goal: 121.               Steps done: 12038310.\n",
      " state (0)  A[0]:(0.99564576149) A[1]:(0.00166221451946) A[2]:(0.00157180917449) A[3]:(0.00112021469977)\n",
      " state (1)  A[0]:(0.0448347069323) A[1]:(0.00296741863713) A[2]:(0.00293076690286) A[3]:(0.949267089367)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.61674673649e-10) A[2]:(4.81963739751e-11) A[3]:(2.93331660096e-15)\n",
      " state (3)  A[0]:(1.0) A[1]:(9.57000971524e-12) A[2]:(3.9066267582e-12) A[3]:(6.51819460065e-19)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.04118963451e-10) A[2]:(2.89482725985e-11) A[3]:(1.14391768507e-18)\n",
      " state (5)  A[0]:(0.00249287765473) A[1]:(1.86663385193e-07) A[2]:(0.997506916523) A[3]:(3.29245337733e-24)\n",
      " state (6)  A[0]:(1.02237052158e-10) A[1]:(2.50214102909e-11) A[2]:(1.0) A[3]:(2.77431893678e-29)\n",
      " state (7)  A[0]:(2.57780845281e-11) A[1]:(1.3684267261e-11) A[2]:(1.0) A[3]:(1.50196324959e-29)\n",
      " state (8)  A[0]:(2.06444861206e-11) A[1]:(1.25006576271e-11) A[2]:(1.0) A[3]:(1.37447767109e-29)\n",
      " state (9)  A[0]:(1.92058903503e-11) A[1]:(1.21546141207e-11) A[2]:(1.0) A[3]:(1.33747483926e-29)\n",
      " state (10)  A[0]:(1.85453839785e-11) A[1]:(1.19945555224e-11) A[2]:(1.0) A[3]:(1.32045142411e-29)\n",
      " state (11)  A[0]:(1.81581919639e-11) A[1]:(1.19001683507e-11) A[2]:(1.0) A[3]:(1.31045554677e-29)\n",
      " state (12)  A[0]:(1.79014476853e-11) A[1]:(1.18370105384e-11) A[2]:(1.0) A[3]:(1.30379378519e-29)\n",
      " state (13)  A[0]:(1.77206513352e-11) A[1]:(1.17922564075e-11) A[2]:(1.0) A[3]:(1.29907751377e-29)\n",
      " state (14)  A[0]:(1.7589188786e-11) A[1]:(1.17594640622e-11) A[2]:(1.0) A[3]:(1.29562317786e-29)\n",
      " state (15)  A[0]:(1.74919644741e-11) A[1]:(1.17350408904e-11) A[2]:(1.0) A[3]:(1.2930556725e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 901000 finished after 30 . Running score: 0.11. Policy_loss: -92050.6128309, Value_loss: 1.22069946791. Times trained:               15922. Times reached goal: 105.               Steps done: 12054232.\n",
      " state (0)  A[0]:(0.993940532207) A[1]:(0.00237648119219) A[2]:(0.00257710809819) A[3]:(0.00110585801303)\n",
      " state (1)  A[0]:(0.0201890934259) A[1]:(0.00173422461376) A[2]:(0.00132922979537) A[3]:(0.976747453213)\n",
      " state (2)  A[0]:(0.999186217785) A[1]:(0.000261310953647) A[2]:(0.000135025984491) A[3]:(0.000417457486037)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.73089303307e-11) A[2]:(3.75756110255e-12) A[3]:(1.5690837016e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.77232097327e-12) A[2]:(3.30926280302e-12) A[3]:(3.25983555902e-19)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.12160856991e-10) A[2]:(2.11763436647e-11) A[3]:(1.02385334974e-18)\n",
      " state (6)  A[0]:(1.0) A[1]:(3.29789440201e-09) A[2]:(2.01772909492e-08) A[3]:(7.24824895815e-20)\n",
      " state (7)  A[0]:(0.143110647798) A[1]:(2.17977526518e-06) A[2]:(0.856887161732) A[3]:(8.06508039215e-23)\n",
      " state (8)  A[0]:(9.86793452284e-07) A[1]:(2.61159982529e-09) A[2]:(0.999998986721) A[3]:(3.72486137194e-27)\n",
      " state (9)  A[0]:(3.57281138008e-09) A[1]:(1.59125351784e-10) A[2]:(1.0) A[3]:(1.42449049072e-28)\n",
      " state (10)  A[0]:(5.38256605953e-10) A[1]:(6.59138368886e-11) A[2]:(1.0) A[3]:(5.56902925869e-29)\n",
      " state (11)  A[0]:(2.66630217904e-10) A[1]:(4.80667484981e-11) A[2]:(1.0) A[3]:(4.02434105884e-29)\n",
      " state (12)  A[0]:(1.93556476402e-10) A[1]:(4.17722662016e-11) A[2]:(1.0) A[3]:(3.49307539017e-29)\n",
      " state (13)  A[0]:(1.62070801224e-10) A[1]:(3.87030962834e-11) A[2]:(1.0) A[3]:(3.2380184691e-29)\n",
      " state (14)  A[0]:(1.44548525927e-10) A[1]:(3.68675541484e-11) A[2]:(1.0) A[3]:(3.08714713579e-29)\n",
      " state (15)  A[0]:(1.33289865256e-10) A[1]:(3.56268209378e-11) A[2]:(1.0) A[3]:(2.98589437823e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 902000 finished after 6 . Running score: 0.16. Policy_loss: -92050.6119459, Value_loss: 0.979187838107. Times trained:               15679. Times reached goal: 112.               Steps done: 12069911.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994498670101) A[1]:(0.00201781024225) A[2]:(0.00268135499209) A[3]:(0.00080215203343)\n",
      " state (1)  A[0]:(0.0199811197817) A[1]:(0.0015408549225) A[2]:(0.0014377970947) A[3]:(0.977040231228)\n",
      " state (2)  A[0]:(0.999932587147) A[1]:(3.34346259478e-05) A[2]:(1.88250123756e-05) A[3]:(1.51798212755e-05)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.38922883614e-11) A[2]:(3.78995142561e-12) A[3]:(1.14795306501e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(9.15586963884e-12) A[2]:(3.82262103915e-12) A[3]:(3.05324539158e-19)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.35268338197e-10) A[2]:(3.06527823957e-11) A[3]:(1.19077095273e-18)\n",
      " state (6)  A[0]:(0.999997913837) A[1]:(2.25612719618e-08) A[2]:(2.06532399716e-06) A[3]:(1.30585282703e-20)\n",
      " state (7)  A[0]:(0.0327851176262) A[1]:(7.85075258136e-07) A[2]:(0.967214107513) A[3]:(1.8317771471e-23)\n",
      " state (8)  A[0]:(4.15422334754e-07) A[1]:(1.31297017703e-09) A[2]:(0.999999582767) A[3]:(2.16087394571e-27)\n",
      " state (9)  A[0]:(2.28414509529e-09) A[1]:(9.90511075938e-11) A[2]:(1.0) A[3]:(1.11770309135e-28)\n",
      " state (10)  A[0]:(4.06492728366e-10) A[1]:(4.43914349724e-11) A[2]:(1.0) A[3]:(4.79352016374e-29)\n",
      " state (11)  A[0]:(2.13882467293e-10) A[1]:(3.3291307211e-11) A[2]:(1.0) A[3]:(3.5737838919e-29)\n",
      " state (12)  A[0]:(1.59325178051e-10) A[1]:(2.92783366995e-11) A[2]:(1.0) A[3]:(3.1422287322e-29)\n",
      " state (13)  A[0]:(1.35232991472e-10) A[1]:(2.72966562548e-11) A[2]:(1.0) A[3]:(2.93194215918e-29)\n",
      " state (14)  A[0]:(1.21656601459e-10) A[1]:(2.61040460081e-11) A[2]:(1.0) A[3]:(2.8065505781e-29)\n",
      " state (15)  A[0]:(1.12874223857e-10) A[1]:(2.52954081292e-11) A[2]:(1.0) A[3]:(2.72200827207e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 903000 finished after 8 . Running score: 0.13. Policy_loss: -92050.6111797, Value_loss: 1.62190746271. Times trained:               15938. Times reached goal: 140.               Steps done: 12085849.\n",
      " state (0)  A[0]:(0.996533632278) A[1]:(0.00128789257724) A[2]:(0.00183374935295) A[3]:(0.000344709027559)\n",
      " state (1)  A[0]:(0.0784412398934) A[1]:(0.00493348576128) A[2]:(0.00433749053627) A[3]:(0.912287771702)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.89417739774e-10) A[2]:(7.65238358791e-11) A[3]:(9.90365490896e-15)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.3100177193e-11) A[2]:(3.39479030824e-12) A[3]:(3.29172699055e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.20346814111e-11) A[2]:(4.10218430405e-12) A[3]:(2.6073784978e-19)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.69876598899e-10) A[2]:(3.11654382856e-11) A[3]:(1.06631770046e-18)\n",
      " state (6)  A[0]:(0.999997138977) A[1]:(3.06430365526e-08) A[2]:(2.83283134195e-06) A[3]:(9.98965184706e-21)\n",
      " state (7)  A[0]:(0.0274903587997) A[1]:(8.22852655347e-07) A[2]:(0.972508847713) A[3]:(1.31524956459e-23)\n",
      " state (8)  A[0]:(4.10291306707e-07) A[1]:(1.52799528585e-09) A[2]:(0.999999582767) A[3]:(1.84785640371e-27)\n",
      " state (9)  A[0]:(2.43494047147e-09) A[1]:(1.20318643937e-10) A[2]:(1.0) A[3]:(1.01439344061e-28)\n",
      " state (10)  A[0]:(4.39396047325e-10) A[1]:(5.43080476423e-11) A[2]:(1.0) A[3]:(4.39264416527e-29)\n",
      " state (11)  A[0]:(2.3154808626e-10) A[1]:(4.07790953783e-11) A[2]:(1.0) A[3]:(3.2806395997e-29)\n",
      " state (12)  A[0]:(1.72199324244e-10) A[1]:(3.58610745266e-11) A[2]:(1.0) A[3]:(2.88470331061e-29)\n",
      " state (13)  A[0]:(1.45650005945e-10) A[1]:(3.34049073125e-11) A[2]:(1.0) A[3]:(2.68965987125e-29)\n",
      " state (14)  A[0]:(1.30404020915e-10) A[1]:(3.18985393655e-11) A[2]:(1.0) A[3]:(2.57127355942e-29)\n",
      " state (15)  A[0]:(1.20341139831e-10) A[1]:(3.08550338057e-11) A[2]:(1.0) A[3]:(2.48982613253e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 904000 finished after 5 . Running score: 0.06. Policy_loss: -92050.611178, Value_loss: 1.43372258087. Times trained:               15831. Times reached goal: 119.               Steps done: 12101680.\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0014,  0.0019,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.8872e-11,  6.8148e-12,  3.8388e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0014,  0.0019,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9961,  0.0014,  0.0019,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.8873e-11,  6.8146e-12,  3.8388e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.8874e-11,  6.8146e-12,  3.8388e-19]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8001e-05,  1.0318e-07,  9.9998e-01,  1.8215e-26]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6608e-08,  3.9077e-09,  1.0000e+00,  2.6611e-28]])\n",
      "On state=9, selected action=2\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3340e-10,  5.3188e-10,  1.0000e+00,  2.7823e-29]])\n",
      "On state=13, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6607e-08,  3.9063e-09,  1.0000e+00,  2.6610e-28]])\n",
      "On state=9, selected action=2\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.99605858326) A[1]:(0.00143135956023) A[2]:(0.00192636949942) A[3]:(0.00058365840232)\n",
      " state (1)  A[0]:(0.0895286872983) A[1]:(0.0061279325746) A[2]:(0.00569602055475) A[3]:(0.898647367954)\n",
      " state (2)  A[0]:(1.0) A[1]:(9.18645090553e-11) A[2]:(1.91054515958e-11) A[3]:(6.52731159511e-16)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.82043841807e-11) A[2]:(4.22217816265e-12) A[3]:(2.38032140046e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.88754818667e-11) A[2]:(6.81446739315e-12) A[3]:(3.83885707642e-19)\n",
      " state (5)  A[0]:(1.0) A[1]:(3.24764937165e-10) A[2]:(4.96907376357e-11) A[3]:(1.122442939e-18)\n",
      " state (6)  A[0]:(0.999972105026) A[1]:(3.02138033703e-07) A[2]:(2.75806760328e-05) A[3]:(4.72168426136e-21)\n",
      " state (7)  A[0]:(0.244983881712) A[1]:(1.82172188943e-05) A[2]:(0.754997909069) A[3]:(9.94419629331e-23)\n",
      " state (8)  A[0]:(1.79968992597e-05) A[1]:(1.03130787465e-07) A[2]:(0.999981880188) A[3]:(1.82124594522e-26)\n",
      " state (9)  A[0]:(1.66060640794e-08) A[1]:(3.9058356549e-09) A[2]:(1.0) A[3]:(2.66097362621e-28)\n",
      " state (10)  A[0]:(8.49763870381e-10) A[1]:(1.1059696492e-09) A[2]:(1.0) A[3]:(6.23611301168e-29)\n",
      " state (11)  A[0]:(2.79639339462e-10) A[1]:(7.06448899379e-10) A[2]:(1.0) A[3]:(3.79667627826e-29)\n",
      " state (12)  A[0]:(1.72454384106e-10) A[1]:(5.85588522117e-10) A[2]:(1.0) A[3]:(3.09174318705e-29)\n",
      " state (13)  A[0]:(1.33393643353e-10) A[1]:(5.31693578054e-10) A[2]:(1.0) A[3]:(2.78228897655e-29)\n",
      " state (14)  A[0]:(1.13339344854e-10) A[1]:(5.00821162319e-10) A[2]:(1.0) A[3]:(2.60643050777e-29)\n",
      " state (15)  A[0]:(1.00834944561e-10) A[1]:(4.80057549268e-10) A[2]:(1.0) A[3]:(2.48891447554e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 905000 finished after 10 . Running score: 0.11. Policy_loss: -92050.6111665, Value_loss: 1.62489263699. Times trained:               16345. Times reached goal: 83.               Steps done: 12118025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995167911053) A[1]:(0.00262198434211) A[2]:(0.00178384117316) A[3]:(0.000426280370448)\n",
      " state (1)  A[0]:(0.0605125762522) A[1]:(0.00814766902477) A[2]:(0.00509556988254) A[3]:(0.926244199276)\n",
      " state (2)  A[0]:(1.0) A[1]:(7.58947682478e-10) A[2]:(3.10643108459e-11) A[3]:(1.59774554291e-15)\n",
      " state (3)  A[0]:(1.0) A[1]:(1.63176902546e-10) A[2]:(5.72831799356e-12) A[3]:(4.42227671314e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.82605019783e-10) A[2]:(1.37741389475e-11) A[3]:(8.68578417272e-19)\n",
      " state (5)  A[0]:(1.0) A[1]:(8.54399395678e-09) A[2]:(2.82499384996e-10) A[3]:(6.59812402927e-19)\n",
      " state (6)  A[0]:(0.993290901184) A[1]:(0.000192293678992) A[2]:(0.0065167886205) A[3]:(9.44611178967e-22)\n",
      " state (7)  A[0]:(0.108749859035) A[1]:(0.000752770458348) A[2]:(0.890497386456) A[3]:(3.36132937517e-23)\n",
      " state (8)  A[0]:(0.00011706638179) A[1]:(2.09531663131e-05) A[2]:(0.999861955643) A[3]:(4.65178888227e-26)\n",
      " state (9)  A[0]:(4.33751694118e-07) A[1]:(1.99692931346e-06) A[2]:(0.99999755621) A[3]:(1.30298154831e-27)\n",
      " state (10)  A[0]:(1.68071210283e-08) A[1]:(6.09299604548e-07) A[2]:(0.999999344349) A[3]:(2.58656098946e-28)\n",
      " state (11)  A[0]:(3.8839487182e-09) A[1]:(3.68222146108e-07) A[2]:(0.999999642372) A[3]:(1.32861167956e-28)\n",
      " state (12)  A[0]:(2.00846628218e-09) A[1]:(2.96454430782e-07) A[2]:(0.999999701977) A[3]:(9.96641783201e-29)\n",
      " state (13)  A[0]:(1.43958645005e-09) A[1]:(2.66990838327e-07) A[2]:(0.999999701977) A[3]:(8.65349273148e-29)\n",
      " state (14)  A[0]:(1.17937948296e-09) A[1]:(2.51439303156e-07) A[2]:(0.999999761581) A[3]:(7.96570944456e-29)\n",
      " state (15)  A[0]:(1.02141184399e-09) A[1]:(2.41115998278e-07) A[2]:(0.999999761581) A[3]:(7.51008678094e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 906000 finished after 6 . Running score: 0.11. Policy_loss: -92050.6112838, Value_loss: 1.41192556601. Times trained:               15958. Times reached goal: 77.               Steps done: 12133983.\n",
      " state (0)  A[0]:(0.99609541893) A[1]:(0.00178003590554) A[2]:(0.00160240905825) A[3]:(0.000522142625414)\n",
      " state (1)  A[0]:(0.0119949169457) A[1]:(0.00210821838118) A[2]:(0.00165682507213) A[3]:(0.984240055084)\n",
      " state (2)  A[0]:(0.999937474728) A[1]:(5.05557109136e-05) A[2]:(8.77076581673e-06) A[3]:(3.21578363582e-06)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.12134021355e-10) A[2]:(9.76762507682e-12) A[3]:(9.83786416589e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(4.90574525447e-10) A[2]:(9.11933896452e-12) A[3]:(2.62111202242e-18)\n",
      " state (5)  A[0]:(1.0) A[1]:(5.74599967607e-09) A[2]:(5.02872188335e-11) A[3]:(2.81943250338e-18)\n",
      " state (6)  A[0]:(0.999915122986) A[1]:(3.27328853018e-05) A[2]:(5.21705005667e-05) A[3]:(8.00008618004e-21)\n",
      " state (7)  A[0]:(0.0159506034106) A[1]:(0.00124402611982) A[2]:(0.982805371284) A[3]:(6.76593218861e-24)\n",
      " state (8)  A[0]:(5.86583610129e-06) A[1]:(3.01879572362e-05) A[2]:(0.99996393919) A[3]:(7.98225315871e-27)\n",
      " state (9)  A[0]:(5.1733408668e-08) A[1]:(5.21694528288e-06) A[2]:(0.999994754791) A[3]:(6.26104032295e-28)\n",
      " state (10)  A[0]:(5.86667292524e-09) A[1]:(2.48664082392e-06) A[2]:(0.999997496605) A[3]:(2.29095866898e-28)\n",
      " state (11)  A[0]:(2.35328734277e-09) A[1]:(1.85173325917e-06) A[2]:(0.999998152256) A[3]:(1.53493619339e-28)\n",
      " state (12)  A[0]:(1.54296497801e-09) A[1]:(1.6277534769e-06) A[2]:(0.999998390675) A[3]:(1.28169530389e-28)\n",
      " state (13)  A[0]:(1.2226875068e-09) A[1]:(1.52199243075e-06) A[2]:(0.999998450279) A[3]:(1.1622964344e-28)\n",
      " state (14)  A[0]:(1.0441539855e-09) A[1]:(1.45698993492e-06) A[2]:(0.999998569489) A[3]:(1.08837515053e-28)\n",
      " state (15)  A[0]:(9.18612685918e-10) A[1]:(1.4073929151e-06) A[2]:(0.999998569489) A[3]:(1.03207371824e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 907000 finished after 29 . Running score: 0.1. Policy_loss: -92050.6112822, Value_loss: 1.20518894807. Times trained:               15490. Times reached goal: 113.               Steps done: 12149473.\n",
      " state (0)  A[0]:(0.997967541218) A[1]:(0.000727455830202) A[2]:(0.00112894282211) A[3]:(0.000176078232471)\n",
      " state (1)  A[0]:(0.0199659503996) A[1]:(0.00272134831175) A[2]:(0.00201932806522) A[3]:(0.975293397903)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(4.67210803379e-08) A[2]:(1.75249093015e-09) A[3]:(3.06497817577e-12)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.17796305832e-10) A[2]:(6.91665908575e-12) A[3]:(3.65658862461e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.20921006541e-09) A[2]:(9.45281613401e-12) A[3]:(1.5862712326e-18)\n",
      " state (5)  A[0]:(1.0) A[1]:(1.18035705654e-08) A[2]:(5.02966175653e-11) A[3]:(2.22745532134e-18)\n",
      " state (6)  A[0]:(0.999497532845) A[1]:(0.000248814147199) A[2]:(0.000253677048022) A[3]:(3.773693678e-21)\n",
      " state (7)  A[0]:(0.0939385294914) A[1]:(0.0100932866335) A[2]:(0.895968198776) A[3]:(3.32230253292e-23)\n",
      " state (8)  A[0]:(0.000168355676578) A[1]:(0.000424220808782) A[2]:(0.999407410622) A[3]:(5.9302372031e-26)\n",
      " state (9)  A[0]:(1.53368296196e-06) A[1]:(6.162840873e-05) A[2]:(0.999936819077) A[3]:(2.97707631539e-27)\n",
      " state (10)  A[0]:(7.06667222516e-08) A[1]:(2.00639351533e-05) A[2]:(0.99997985363) A[3]:(6.58342944006e-28)\n",
      " state (11)  A[0]:(1.52348551552e-08) A[1]:(1.1784642993e-05) A[2]:(0.99998819828) A[3]:(3.29463611501e-28)\n",
      " state (12)  A[0]:(7.43016403959e-09) A[1]:(9.2871259767e-06) A[2]:(0.999990701675) A[3]:(2.40995369505e-28)\n",
      " state (13)  A[0]:(5.19848608604e-09) A[1]:(8.29833243188e-06) A[2]:(0.999991714954) A[3]:(2.0693414288e-28)\n",
      " state (14)  A[0]:(4.26295088118e-09) A[1]:(7.82058577897e-06) A[2]:(0.999992191792) A[3]:(1.90365704606e-28)\n",
      " state (15)  A[0]:(3.76059272611e-09) A[1]:(7.54477878218e-06) A[2]:(0.99999243021) A[3]:(1.80675219572e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 908000 finished after 9 . Running score: 0.12. Policy_loss: -92050.6111962, Value_loss: 1.18969574017. Times trained:               15742. Times reached goal: 110.               Steps done: 12165215.\n",
      " state (0)  A[0]:(0.994504511356) A[1]:(0.00367527571507) A[2]:(0.00139457301702) A[3]:(0.000425625883508)\n",
      " state (1)  A[0]:(0.0198827274144) A[1]:(0.00359299592674) A[2]:(0.00240569887683) A[3]:(0.974118590355)\n",
      " state (2)  A[0]:(1.0) A[1]:(2.14034390211e-09) A[2]:(3.61591590003e-11) A[3]:(2.58330607001e-15)\n",
      " state (3)  A[0]:(1.0) A[1]:(8.99853858094e-10) A[2]:(9.35588325562e-12) A[3]:(8.13244687183e-18)\n",
      " state (4)  A[0]:(1.0) A[1]:(1.16353442436e-08) A[2]:(4.74048335308e-11) A[3]:(2.89299553641e-18)\n",
      " state (5)  A[0]:(0.997249484062) A[1]:(0.00111341057345) A[2]:(0.00163711805362) A[3]:(3.90272819901e-21)\n",
      " state (6)  A[0]:(0.000741343828849) A[1]:(0.00123986077961) A[2]:(0.998018801212) A[3]:(2.38453102689e-25)\n",
      " state (7)  A[0]:(5.31077375854e-07) A[1]:(5.44381800864e-05) A[2]:(0.999945044518) A[3]:(1.76530757962e-27)\n",
      " state (8)  A[0]:(4.69349759058e-09) A[1]:(9.61088517215e-06) A[2]:(0.999990403652) A[3]:(2.00612338023e-28)\n",
      " state (9)  A[0]:(7.11923631158e-10) A[1]:(5.01959630128e-06) A[2]:(0.99999499321) A[3]:(9.03717529129e-29)\n",
      " state (10)  A[0]:(3.47602752138e-10) A[1]:(3.99166810894e-06) A[2]:(0.999996006489) A[3]:(6.74307615279e-29)\n",
      " state (11)  A[0]:(2.49758436155e-10) A[1]:(3.62635501006e-06) A[2]:(0.999996364117) A[3]:(5.90470741279e-29)\n",
      " state (12)  A[0]:(2.05424580124e-10) A[1]:(3.44367458638e-06) A[2]:(0.999996542931) A[3]:(5.46337996418e-29)\n",
      " state (13)  A[0]:(1.78175987853e-10) A[1]:(3.32358285959e-06) A[2]:(0.99999666214) A[3]:(5.16517077715e-29)\n",
      " state (14)  A[0]:(1.58499019465e-10) A[1]:(3.23010090142e-06) A[2]:(0.999996781349) A[3]:(4.93375795634e-29)\n",
      " state (15)  A[0]:(1.43244388573e-10) A[1]:(3.15170314025e-06) A[2]:(0.999996840954) A[3]:(4.7433373507e-29)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 909000 finished after 46 . Running score: 0.08. Policy_loss: -92050.6111645, Value_loss: 1.21855936605. Times trained:               16101. Times reached goal: 94.               Steps done: 12181316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9959,  0.0019,  0.0019,  0.0003]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.3681e-09,  7.2640e-11,  3.1843e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  5.3683e-09,  7.2642e-11,  3.1844e-18]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.5439e-11,  1.6575e-07,  1.0000e+00,  1.5735e-29]])\n",
      "On state=8, selected action=2\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7499e-11,  1.4130e-07,  1.0000e+00,  1.2723e-29]])\n",
      "On state=9, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.1842e-11,  1.3308e-07,  1.0000e+00,  1.1550e-29]])\n",
      "On state=10, selected action=2\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.4604e-11,  1.2123e-07,  1.0000e+00,  9.7514e-30]])\n",
      "On state=14, selected action=2\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.1830e-11,  1.3302e-07,  1.0000e+00,  1.1546e-29]])\n",
      "On state=10, selected action=2\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995940625668) A[1]:(0.00191296089906) A[2]:(0.00187787972391) A[3]:(0.000268523144769)\n",
      " state (1)  A[0]:(0.0186693090945) A[1]:(0.00241698231548) A[2]:(0.00310415332206) A[3]:(0.975809574127)\n",
      " state (2)  A[0]:(0.999980390072) A[1]:(1.52650100063e-05) A[2]:(4.07387506129e-06) A[3]:(2.97021472306e-07)\n",
      " state (3)  A[0]:(1.0) A[1]:(4.09698081016e-10) A[2]:(1.62956297761e-11) A[3]:(2.71955478712e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.36925970351e-09) A[2]:(7.26529322814e-11) A[3]:(3.18474834232e-18)\n",
      " state (5)  A[0]:(0.86681252718) A[1]:(0.0034318510443) A[2]:(0.129755616188) A[3]:(1.38789809722e-21)\n",
      " state (6)  A[0]:(2.03816497901e-07) A[1]:(5.33312731932e-06) A[2]:(0.999994456768) A[3]:(1.03602233204e-27)\n",
      " state (7)  A[0]:(2.3214323519e-10) A[1]:(3.05755236241e-07) A[2]:(0.999999701977) A[3]:(3.22982634553e-29)\n",
      " state (8)  A[0]:(4.5408097421e-11) A[1]:(1.65694316934e-07) A[2]:(0.999999821186) A[3]:(1.57293933519e-29)\n",
      " state (9)  A[0]:(2.74822126861e-11) A[1]:(1.41251987884e-07) A[2]:(0.999999880791) A[3]:(1.27195996876e-29)\n",
      " state (10)  A[0]:(2.18305529903e-11) A[1]:(1.33039122829e-07) A[2]:(0.999999880791) A[3]:(1.15469190001e-29)\n",
      " state (11)  A[0]:(1.88805950041e-11) A[1]:(1.28658996346e-07) A[2]:(0.999999880791) A[3]:(1.0862481866e-29)\n",
      " state (12)  A[0]:(1.69559904312e-11) A[1]:(1.25574132426e-07) A[2]:(0.999999880791) A[3]:(1.0381988676e-29)\n",
      " state (13)  A[0]:(1.55936808605e-11) A[1]:(1.23167396282e-07) A[2]:(0.999999880791) A[3]:(1.00232624066e-29)\n",
      " state (14)  A[0]:(1.46032682474e-11) A[1]:(1.21248604046e-07) A[2]:(0.999999880791) A[3]:(9.75171079838e-30)\n",
      " state (15)  A[0]:(1.3875603791e-11) A[1]:(1.19718592373e-07) A[2]:(0.999999880791) A[3]:(9.54589283421e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 910000 finished after 8 . Running score: 0.14. Policy_loss: -92050.6111852, Value_loss: 0.989025252754. Times trained:               15876. Times reached goal: 103.               Steps done: 12197192.\n",
      " state (0)  A[0]:(0.99792444706) A[1]:(0.00095070397947) A[2]:(0.000943118415307) A[3]:(0.000181753974175)\n",
      " state (1)  A[0]:(0.0155935678631) A[1]:(0.00149792572483) A[2]:(0.00195140647702) A[3]:(0.980957090855)\n",
      " state (2)  A[0]:(0.999993860722) A[1]:(4.67229847345e-06) A[2]:(1.3708147435e-06) A[3]:(8.28483592841e-08)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.69206712478e-10) A[2]:(1.45696892051e-11) A[3]:(3.03557285357e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(3.61685770223e-09) A[2]:(6.48644679635e-11) A[3]:(3.28751064417e-18)\n",
      " state (5)  A[0]:(0.978537917137) A[1]:(0.000766677432694) A[2]:(0.0206954069436) A[3]:(2.33214748565e-21)\n",
      " state (6)  A[0]:(2.31493254432e-06) A[1]:(7.67486562836e-06) A[2]:(0.99998998642) A[3]:(5.22297756443e-27)\n",
      " state (7)  A[0]:(4.77487716033e-10) A[1]:(1.54760783744e-07) A[2]:(0.999999821186) A[3]:(4.88619892193e-29)\n",
      " state (8)  A[0]:(4.71190725326e-11) A[1]:(5.90130113665e-08) A[2]:(0.999999940395) A[3]:(1.69928741131e-29)\n",
      " state (9)  A[0]:(2.4311696753e-11) A[1]:(4.63983624854e-08) A[2]:(0.999999940395) A[3]:(1.27567249966e-29)\n",
      " state (10)  A[0]:(1.86878412051e-11) A[1]:(4.2939635847e-08) A[2]:(0.999999940395) A[3]:(1.13836099209e-29)\n",
      " state (11)  A[0]:(1.61816185451e-11) A[1]:(4.14926546455e-08) A[2]:(0.999999940395) A[3]:(1.06839632065e-29)\n",
      " state (12)  A[0]:(1.4685834146e-11) A[1]:(4.06466753589e-08) A[2]:(0.999999940395) A[3]:(1.02307843674e-29)\n",
      " state (13)  A[0]:(1.36734130962e-11) A[1]:(4.00472011108e-08) A[2]:(0.999999940395) A[3]:(9.90739515099e-30)\n",
      " state (14)  A[0]:(1.29548663463e-11) A[1]:(3.95874337755e-08) A[2]:(0.999999940395) A[3]:(9.66977225936e-30)\n",
      " state (15)  A[0]:(1.24349749248e-11) A[1]:(3.92254548842e-08) A[2]:(0.999999940395) A[3]:(9.4937452717e-30)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 911000 finished after 8 . Running score: 0.09. Policy_loss: -92050.6111864, Value_loss: 1.41092080279. Times trained:               15555. Times reached goal: 127.               Steps done: 12212747.\n",
      " state (0)  A[0]:(0.995811522007) A[1]:(0.00190293160267) A[2]:(0.00083363475278) A[3]:(0.00145188823808)\n",
      " state (1)  A[0]:(0.00892061274499) A[1]:(0.00169577472843) A[2]:(0.000984712969512) A[3]:(0.988398909569)\n",
      " state (2)  A[0]:(0.999997854233) A[1]:(1.99407509172e-06) A[2]:(1.24762649989e-07) A[3]:(4.07692457571e-09)\n",
      " state (3)  A[0]:(1.0) A[1]:(9.15395814705e-10) A[2]:(1.07906175315e-11) A[3]:(9.77568863883e-17)\n",
      " state (4)  A[0]:(1.0) A[1]:(2.9901192633e-09) A[2]:(1.47064114359e-11) A[3]:(2.62659705706e-18)\n",
      " state (5)  A[0]:(1.0) A[1]:(2.53487666413e-08) A[2]:(7.27191987182e-11) A[3]:(1.90326049181e-18)\n",
      " state (6)  A[0]:(0.999782443047) A[1]:(0.000169785751496) A[2]:(4.77498069813e-05) A[3]:(7.93756295795e-21)\n",
      " state (7)  A[0]:(0.789698302746) A[1]:(0.0298976115882) A[2]:(0.180404052138) A[3]:(6.82393208227e-22)\n",
      " state (8)  A[0]:(0.146982640028) A[1]:(0.026387039572) A[2]:(0.826630353928) A[3]:(7.74098667112e-23)\n",
      " state (9)  A[0]:(0.00430453708395) A[1]:(0.00374986114912) A[2]:(0.991945624352) A[3]:(1.54056803953e-24)\n",
      " state (10)  A[0]:(0.000144067729707) A[1]:(0.000573571713176) A[2]:(0.999282360077) A[3]:(5.98873432071e-26)\n",
      " state (11)  A[0]:(1.43859169839e-05) A[1]:(0.000175690773176) A[2]:(0.999809920788) A[3]:(1.01307651379e-26)\n",
      " state (12)  A[0]:(3.94231346945e-06) A[1]:(9.34412746574e-05) A[2]:(0.99990260601) A[3]:(4.30130722655e-27)\n",
      " state (13)  A[0]:(2.12965460378e-06) A[1]:(6.9854031608e-05) A[2]:(0.999927997589) A[3]:(2.95112325383e-27)\n",
      " state (14)  A[0]:(1.66034328686e-06) A[1]:(6.24216772849e-05) A[2]:(0.999935925007) A[3]:(2.55502764219e-27)\n",
      " state (15)  A[0]:(1.53746850629e-06) A[1]:(6.06218782195e-05) A[2]:(0.999937832355) A[3]:(2.45673838525e-27)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 912000 finished after 7 . Running score: 0.06. Policy_loss: -92050.6263645, Value_loss: 1.20154134373. Times trained:               15700. Times reached goal: 101.               Steps done: 12228447.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995481848717) A[1]:(0.00300227687694) A[2]:(0.00122563459445) A[3]:(0.000290251045953)\n",
      " state (1)  A[0]:(0.0567182712257) A[1]:(0.016398511827) A[2]:(0.00514043308794) A[3]:(0.921742796898)\n",
      " state (2)  A[0]:(1.0) A[1]:(1.14275460206e-08) A[2]:(1.05551234419e-10) A[3]:(7.37182599578e-15)\n",
      " state (3)  A[0]:(1.0) A[1]:(2.73738476331e-09) A[2]:(1.15658056049e-11) A[3]:(1.06527330222e-17)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(3.4156844464e-08) A[2]:(4.51696874348e-11) A[3]:(2.18932746498e-18)\n",
      " state (5)  A[0]:(0.999996006489) A[1]:(3.95473171011e-06) A[2]:(2.99181444063e-08) A[3]:(1.12332884944e-19)\n",
      " state (6)  A[0]:(0.911033630371) A[1]:(0.0485402643681) A[2]:(0.0404260791838) A[3]:(1.03398455045e-21)\n",
      " state (7)  A[0]:(0.678034186363) A[1]:(0.122243769467) A[2]:(0.199722066522) A[3]:(5.69698432573e-22)\n",
      " state (8)  A[0]:(0.551086127758) A[1]:(0.141566261649) A[2]:(0.307347565889) A[3]:(4.2987153708e-22)\n",
      " state (9)  A[0]:(0.386858254671) A[1]:(0.144623443484) A[2]:(0.468518316746) A[3]:(2.76470074591e-22)\n",
      " state (10)  A[0]:(0.152335450053) A[1]:(0.104435652494) A[2]:(0.743228852749) A[3]:(9.27164377778e-23)\n",
      " state (11)  A[0]:(0.0190686713904) A[1]:(0.0346804484725) A[2]:(0.946250855923) A[3]:(9.27081642046e-24)\n",
      " state (12)  A[0]:(0.00135497387964) A[1]:(0.00787914358079) A[2]:(0.990765869617) A[3]:(6.10085401183e-25)\n",
      " state (13)  A[0]:(0.000153504501213) A[1]:(0.00242237467319) A[2]:(0.997424125671) A[3]:(8.23533745566e-26)\n",
      " state (14)  A[0]:(3.62406790373e-05) A[1]:(0.00114740757272) A[2]:(0.998816370964) A[3]:(2.52617081753e-26)\n",
      " state (15)  A[0]:(1.53803830472e-05) A[1]:(0.000748440681491) A[2]:(0.999236166477) A[3]:(1.32756323219e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 913000 finished after 9 . Running score: 0.02. Policy_loss: -92050.6417117, Value_loss: 0.979065539671. Times trained:               16441. Times reached goal: 54.               Steps done: 12244888.\n",
      " state (0)  A[0]:(0.994421362877) A[1]:(0.0041572060436) A[2]:(0.00119440827984) A[3]:(0.000227007447393)\n",
      " state (1)  A[0]:(0.0525899752975) A[1]:(0.0269736908376) A[2]:(0.00539007876068) A[3]:(0.915046274662)\n",
      " state (2)  A[0]:(0.981804668903) A[1]:(0.0129859624431) A[2]:(0.00138465955388) A[3]:(0.00382471363991)\n",
      " state (3)  A[0]:(1.0) A[1]:(5.36749844571e-09) A[2]:(2.29658427692e-11) A[3]:(1.46318932998e-16)\n",
      " state (4)  A[0]:(1.0) A[1]:(5.07942266026e-09) A[2]:(1.42686851917e-11) A[3]:(4.11928004118e-18)\n",
      " state (5)  A[0]:(0.999999940395) A[1]:(4.18614973796e-08) A[2]:(4.89357478151e-11) A[3]:(1.83529747152e-18)\n",
      " state (6)  A[0]:(0.999829649925) A[1]:(0.000167435151525) A[2]:(2.9388220355e-06) A[3]:(6.13679803457e-20)\n",
      " state (7)  A[0]:(0.345387279987) A[1]:(0.29973217845) A[2]:(0.354880541563) A[3]:(5.24287812336e-22)\n",
      " state (8)  A[0]:(0.026370620355) A[1]:(0.128232240677) A[2]:(0.845397114754) A[3]:(3.55310871875e-23)\n",
      " state (9)  A[0]:(0.00020778887847) A[1]:(0.0105743966997) A[2]:(0.989217817783) A[3]:(3.87579565427e-25)\n",
      " state (10)  A[0]:(1.38778887049e-06) A[1]:(0.000858565501403) A[2]:(0.999140024185) A[3]:(7.73226668156e-27)\n",
      " state (11)  A[0]:(5.2611831336e-08) A[1]:(0.000181530223927) A[2]:(0.999818444252) A[3]:(8.98999542433e-28)\n",
      " state (12)  A[0]:(1.00473256381e-08) A[1]:(8.48456620588e-05) A[2]:(0.999915122986) A[3]:(3.34031845107e-28)\n",
      " state (13)  A[0]:(4.77770578655e-09) A[1]:(6.07777947152e-05) A[2]:(0.999939203262) A[3]:(2.18549546745e-28)\n",
      " state (14)  A[0]:(3.53535223319e-09) A[1]:(5.33037346031e-05) A[2]:(0.999946713448) A[3]:(1.8511864088e-28)\n",
      " state (15)  A[0]:(3.22164672717e-09) A[1]:(5.13539889653e-05) A[2]:(0.999948620796) A[3]:(1.76543425766e-28)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 914000 finished after 18 . Running score: 0.14. Policy_loss: -92050.6199837, Value_loss: 0.985178349778. Times trained:               15808. Times reached goal: 113.               Steps done: 12260696.\n",
      "action_dist \n",
      "tensor([[ 0.9858,  0.0126,  0.0011,  0.0005]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9416e-08,  1.8717e-11,  1.5883e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.9395e-08,  1.8717e-11,  1.5918e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.1418e-01,  8.6183e-01,  2.3989e-02,  1.2630e-21]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.985803723335) A[1]:(0.0125559074804) A[2]:(0.00110621727072) A[3]:(0.000534148653969)\n",
      " state (1)  A[0]:(0.00942758843303) A[1]:(0.020509744063) A[2]:(0.000784181291237) A[3]:(0.969278514385)\n",
      " state (2)  A[0]:(0.998425006866) A[1]:(0.00154843681958) A[2]:(1.19635551528e-05) A[3]:(1.46038037201e-05)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(3.90073005008e-08) A[2]:(2.18757390674e-11) A[3]:(1.43688308068e-15)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(3.93358909889e-08) A[2]:(1.87137371571e-11) A[3]:(1.60164401205e-16)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(9.88323378692e-08) A[2]:(2.64783282106e-11) A[3]:(1.16424819221e-17)\n",
      " state (6)  A[0]:(0.999997735023) A[1]:(2.26403494707e-06) A[2]:(2.15195514186e-10) A[3]:(4.94000492264e-18)\n",
      " state (7)  A[0]:(0.991833209991) A[1]:(0.00815628562123) A[2]:(1.05144545159e-05) A[3]:(2.27044534534e-19)\n",
      " state (8)  A[0]:(0.115029610693) A[1]:(0.861078977585) A[2]:(0.0238914117217) A[3]:(1.27535841793e-21)\n",
      " state (9)  A[0]:(0.0403158701956) A[1]:(0.917707502842) A[2]:(0.0419766008854) A[3]:(3.61643323419e-22)\n",
      " state (10)  A[0]:(0.0250261500478) A[1]:(0.922799348831) A[2]:(0.0521744973958) A[3]:(2.47442441903e-22)\n",
      " state (11)  A[0]:(0.0158399883658) A[1]:(0.918842852116) A[2]:(0.0653171911836) A[3]:(1.91714581255e-22)\n",
      " state (12)  A[0]:(0.00835478212684) A[1]:(0.896471560001) A[2]:(0.0951736420393) A[3]:(1.36196469255e-22)\n",
      " state (13)  A[0]:(0.00294555840082) A[1]:(0.809263110161) A[2]:(0.187791362405) A[3]:(7.02794476522e-23)\n",
      " state (14)  A[0]:(0.00050891097635) A[1]:(0.539245128632) A[2]:(0.460245996714) A[3]:(1.91447517128e-23)\n",
      " state (15)  A[0]:(3.76026000595e-05) A[1]:(0.191599801183) A[2]:(0.808362603188) A[3]:(2.42239896344e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 915000 finished after 4 . Running score: 0.16. Policy_loss: -92050.623121, Value_loss: 1.20021454225. Times trained:               14170. Times reached goal: 129.               Steps done: 12274866.\n",
      " state (0)  A[0]:(0.981471478939) A[1]:(0.0139817921445) A[2]:(0.00177880644333) A[3]:(0.00276790000498)\n",
      " state (1)  A[0]:(0.00374108576216) A[1]:(0.0111964615062) A[2]:(0.000344906497048) A[3]:(0.984717547894)\n",
      " state (2)  A[0]:(0.999897480011) A[1]:(0.000102212849015) A[2]:(2.34378092046e-07) A[3]:(4.48545307563e-08)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(6.90893315891e-08) A[2]:(2.25436024015e-11) A[3]:(2.0444285794e-15)\n",
      " state (4)  A[0]:(0.999999940395) A[1]:(8.428398246e-08) A[2]:(2.23186035625e-11) A[3]:(2.38617863329e-16)\n",
      " state (5)  A[0]:(0.999999701977) A[1]:(2.79017399407e-07) A[2]:(4.01699194741e-11) A[3]:(2.50008127129e-17)\n",
      " state (6)  A[0]:(0.999995350838) A[1]:(4.63205651613e-06) A[2]:(2.43499304053e-10) A[3]:(7.60404472371e-18)\n",
      " state (7)  A[0]:(0.995662510395) A[1]:(0.00433610519394) A[2]:(1.41123609865e-06) A[3]:(8.13227161294e-19)\n",
      " state (8)  A[0]:(0.0350667312741) A[1]:(0.950906813145) A[2]:(0.0140264844522) A[3]:(6.77042030229e-22)\n",
      " state (9)  A[0]:(0.00957632809877) A[1]:(0.965372860432) A[2]:(0.0250508282334) A[3]:(1.87131211043e-22)\n",
      " state (10)  A[0]:(0.00415408564731) A[1]:(0.957045257092) A[2]:(0.0388006456196) A[3]:(1.25000475661e-22)\n",
      " state (11)  A[0]:(0.00113581016194) A[1]:(0.900642514229) A[2]:(0.0982216894627) A[3]:(6.2462139004e-23)\n",
      " state (12)  A[0]:(0.000103469501482) A[1]:(0.576115369797) A[2]:(0.42378115654) A[3]:(1.24503439648e-23)\n",
      " state (13)  A[0]:(2.280183935e-06) A[1]:(0.118938565254) A[2]:(0.881059169769) A[3]:(6.91453149505e-25)\n",
      " state (14)  A[0]:(7.24926607631e-08) A[1]:(0.021862924099) A[2]:(0.978137016296) A[3]:(5.03132496307e-26)\n",
      " state (15)  A[0]:(8.26910717677e-09) A[1]:(0.00741344876587) A[2]:(0.992586553097) A[3]:(1.01752348603e-26)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 916000 finished after 23 . Running score: 0.16. Policy_loss: -92050.6325687, Value_loss: 1.19159564719. Times trained:               13294. Times reached goal: 129.               Steps done: 12288160.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.976358473301) A[1]:(0.0123198302463) A[2]:(0.00166803295724) A[3]:(0.00965365488082)\n",
      " state (1)  A[0]:(0.000957745767664) A[1]:(0.00515230558813) A[2]:(5.87448994338e-05) A[3]:(0.993831217289)\n",
      " state (2)  A[0]:(0.999999046326) A[1]:(9.69000211626e-07) A[2]:(7.3921369026e-11) A[3]:(4.83707963007e-13)\n",
      " state (3)  A[0]:(0.999999582767) A[1]:(4.04727757086e-07) A[2]:(1.78572451964e-11) A[3]:(3.78534916018e-15)\n",
      " state (4)  A[0]:(0.99999910593) A[1]:(9.07915932657e-07) A[2]:(2.60206092784e-11) A[3]:(2.87177468102e-16)\n",
      " state (5)  A[0]:(0.999981343746) A[1]:(1.86369361472e-05) A[2]:(1.57255958255e-10) A[3]:(3.06663844865e-17)\n",
      " state (6)  A[0]:(0.998028934002) A[1]:(0.00197102199309) A[2]:(2.10437285375e-08) A[3]:(9.35877438998e-18)\n",
      " state (7)  A[0]:(0.00753772584721) A[1]:(0.991939604282) A[2]:(0.000522658810951) A[3]:(1.16053662084e-21)\n",
      " state (8)  A[0]:(0.000837014464196) A[1]:(0.998076677322) A[2]:(0.00108630547766) A[3]:(8.95308849138e-23)\n",
      " state (9)  A[0]:(0.000459354545455) A[1]:(0.998258054256) A[2]:(0.00128260359634) A[3]:(5.73345557419e-23)\n",
      " state (10)  A[0]:(0.000336531316862) A[1]:(0.998268783092) A[2]:(0.00139468139969) A[3]:(4.91946408488e-23)\n",
      " state (11)  A[0]:(0.000271086319117) A[1]:(0.998251020908) A[2]:(0.00147790822666) A[3]:(4.54914153708e-23)\n",
      " state (12)  A[0]:(0.000225001305807) A[1]:(0.998218417168) A[2]:(0.00155656074639) A[3]:(4.29926700545e-23)\n",
      " state (13)  A[0]:(0.000185437718756) A[1]:(0.99816018343) A[2]:(0.00165437161922) A[3]:(4.06414480384e-23)\n",
      " state (14)  A[0]:(0.000147725295392) A[1]:(0.998041093349) A[2]:(0.00181120529305) A[3]:(3.77833828907e-23)\n",
      " state (15)  A[0]:(0.000110674634925) A[1]:(0.997772216797) A[2]:(0.00211712880991) A[3]:(3.36903843948e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 917000 finished after 9 . Running score: 0.09. Policy_loss: -92050.6114524, Value_loss: 0.979417495368. Times trained:               13243. Times reached goal: 135.               Steps done: 12301403.\n",
      " state (0)  A[0]:(0.987278401852) A[1]:(0.00684885215014) A[2]:(0.00044896491454) A[3]:(0.00542380241677)\n",
      " state (1)  A[0]:(0.000272015982773) A[1]:(0.00168786593713) A[2]:(8.09583616501e-06) A[3]:(0.998032033443)\n",
      " state (2)  A[0]:(0.994512617588) A[1]:(0.00535176275298) A[2]:(1.30381192776e-06) A[3]:(0.000134304209496)\n",
      " state (3)  A[0]:(0.999999403954) A[1]:(5.75332762764e-07) A[2]:(3.27507660748e-12) A[3]:(2.50408367137e-14)\n",
      " state (4)  A[0]:(0.99999910593) A[1]:(9.09958146167e-07) A[2]:(2.67533412958e-12) A[3]:(1.16311207949e-15)\n",
      " state (5)  A[0]:(0.999989628792) A[1]:(1.03791981019e-05) A[2]:(7.45945319403e-12) A[3]:(4.50142755801e-17)\n",
      " state (6)  A[0]:(0.998760819435) A[1]:(0.00123916484881) A[2]:(3.32283867088e-10) A[3]:(1.57909920446e-17)\n",
      " state (7)  A[0]:(0.00253851292655) A[1]:(0.997451603413) A[2]:(9.90393073153e-06) A[3]:(7.24575632934e-22)\n",
      " state (8)  A[0]:(0.000152808992425) A[1]:(0.99982625246) A[2]:(2.09122536035e-05) A[3]:(3.31568178112e-23)\n",
      " state (9)  A[0]:(8.26117902761e-05) A[1]:(0.999894618988) A[2]:(2.27910477406e-05) A[3]:(2.30202044591e-23)\n",
      " state (10)  A[0]:(6.17227924522e-05) A[1]:(0.999914705753) A[2]:(2.35632960539e-05) A[3]:(2.05924992228e-23)\n",
      " state (11)  A[0]:(5.00787173223e-05) A[1]:(0.999925732613) A[2]:(2.41735669988e-05) A[3]:(1.93673745712e-23)\n",
      " state (12)  A[0]:(4.11173459725e-05) A[1]:(0.999933838844) A[2]:(2.50141056313e-05) A[3]:(1.83737805952e-23)\n",
      " state (13)  A[0]:(3.29490685544e-05) A[1]:(0.999940335751) A[2]:(2.6707406505e-05) A[3]:(1.72092025958e-23)\n",
      " state (14)  A[0]:(2.48248215939e-05) A[1]:(0.999944448471) A[2]:(3.07427435473e-05) A[3]:(1.54519723309e-23)\n",
      " state (15)  A[0]:(1.65732380992e-05) A[1]:(0.999942362309) A[2]:(4.1071765736e-05) A[3]:(1.26741548476e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 918000 finished after 29 . Running score: 0.1. Policy_loss: -92050.6114096, Value_loss: 0.978541010835. Times trained:               12832. Times reached goal: 131.               Steps done: 12314235.\n",
      " state (0)  A[0]:(0.991110265255) A[1]:(0.00403634319082) A[2]:(0.00363464420661) A[3]:(0.00121873582248)\n",
      " state (1)  A[0]:(0.00133629445918) A[1]:(0.00767899723724) A[2]:(2.99270486721e-05) A[3]:(0.990954756737)\n",
      " state (2)  A[0]:(0.999997496605) A[1]:(2.51284473052e-06) A[2]:(1.43415002096e-11) A[3]:(1.15995277619e-12)\n",
      " state (3)  A[0]:(0.999999165535) A[1]:(8.13872304661e-07) A[2]:(1.35621765554e-12) A[3]:(6.2541151999e-15)\n",
      " state (4)  A[0]:(0.999998748302) A[1]:(1.24063262774e-06) A[2]:(1.11928944845e-12) A[3]:(8.61707581307e-16)\n",
      " state (5)  A[0]:(0.999995648861) A[1]:(4.36276741311e-06) A[2]:(1.25942669921e-12) A[3]:(6.31565367876e-17)\n",
      " state (6)  A[0]:(0.999843299389) A[1]:(0.000156718539074) A[2]:(6.65832276467e-12) A[3]:(2.16354929108e-17)\n",
      " state (7)  A[0]:(0.927680790424) A[1]:(0.0723192319274) A[2]:(3.1740350348e-09) A[3]:(1.04515734506e-17)\n",
      " state (8)  A[0]:(0.000664869439788) A[1]:(0.999334573746) A[2]:(5.71754810608e-07) A[3]:(3.24530762598e-22)\n",
      " state (9)  A[0]:(5.20890316693e-05) A[1]:(0.999946892262) A[2]:(1.04237437881e-06) A[3]:(2.19613164661e-23)\n",
      " state (10)  A[0]:(2.21102036448e-05) A[1]:(0.999976694584) A[2]:(1.20880554277e-06) A[3]:(1.18816296835e-23)\n",
      " state (11)  A[0]:(1.37017586894e-05) A[1]:(0.99998497963) A[2]:(1.30314811031e-06) A[3]:(9.64942705648e-24)\n",
      " state (12)  A[0]:(9.15356486075e-06) A[1]:(0.999989390373) A[2]:(1.4355980511e-06) A[3]:(8.81773889271e-24)\n",
      " state (13)  A[0]:(5.60635589864e-06) A[1]:(0.999992668629) A[2]:(1.72897443917e-06) A[3]:(8.28498010265e-24)\n",
      " state (14)  A[0]:(2.81904772237e-06) A[1]:(0.999994754791) A[2]:(2.4024452614e-06) A[3]:(7.67433470883e-24)\n",
      " state (15)  A[0]:(1.18366654078e-06) A[1]:(0.99999499321) A[2]:(3.81310314879e-06) A[3]:(6.88886827208e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 919000 finished after 20 . Running score: 0.16. Policy_loss: -92050.6125188, Value_loss: 0.99325204138. Times trained:               12653. Times reached goal: 116.               Steps done: 12326888.\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0028,  0.0018,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0028,  0.0018,  0.0007]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.4278e-06,  2.9235e-13,  3.6611e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0028,  0.0018,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0028,  0.0018,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0028,  0.0018,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.4280e-06,  2.9153e-13,  3.6586e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5990e-03,  9.9840e-01,  6.9488e-08,  9.4409e-22]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5989e-03,  9.9840e-01,  6.9460e-08,  9.4419e-22]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994801342487) A[1]:(0.0027993558906) A[2]:(0.00175422639586) A[3]:(0.000645048450679)\n",
      " state (1)  A[0]:(0.00151212152559) A[1]:(0.00775501970202) A[2]:(2.19773937715e-05) A[3]:(0.99071085453)\n",
      " state (2)  A[0]:(0.999996364117) A[1]:(3.63151866623e-06) A[2]:(8.9287769886e-12) A[3]:(2.52850778509e-12)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(7.85229417488e-07) A[2]:(3.79105980549e-13) A[3]:(3.97087139849e-15)\n",
      " state (4)  A[0]:(0.999998569489) A[1]:(1.42785563639e-06) A[2]:(2.91109882511e-13) A[3]:(3.65809204192e-16)\n",
      " state (5)  A[0]:(0.999989390373) A[1]:(1.05813642222e-05) A[2]:(4.47795500767e-13) A[3]:(3.04186935239e-17)\n",
      " state (6)  A[0]:(0.999612390995) A[1]:(0.000387607666198) A[2]:(3.31847049839e-12) A[3]:(2.40067963883e-17)\n",
      " state (7)  A[0]:(0.631484210491) A[1]:(0.368515759706) A[2]:(4.77220440942e-09) A[3]:(4.64252223663e-18)\n",
      " state (8)  A[0]:(0.00160066550598) A[1]:(0.99839925766) A[2]:(6.94224908671e-08) A[3]:(9.45569121163e-22)\n",
      " state (9)  A[0]:(0.000201133370865) A[1]:(0.999798774719) A[2]:(1.12494511484e-07) A[3]:(9.02550844684e-23)\n",
      " state (10)  A[0]:(6.70103909215e-05) A[1]:(0.999932825565) A[2]:(1.43238722217e-07) A[3]:(3.41005289567e-23)\n",
      " state (11)  A[0]:(2.91874603136e-05) A[1]:(0.99997061491) A[2]:(1.71470858845e-07) A[3]:(2.00377911669e-23)\n",
      " state (12)  A[0]:(1.33607582029e-05) A[1]:(0.999986410141) A[2]:(2.07720617595e-07) A[3]:(1.48165448718e-23)\n",
      " state (13)  A[0]:(5.26996927874e-06) A[1]:(0.999994456768) A[2]:(2.75746344869e-07) A[3]:(1.20993765926e-23)\n",
      " state (14)  A[0]:(1.50313007907e-06) A[1]:(0.999998092651) A[2]:(4.31607872997e-07) A[3]:(1.0033297817e-23)\n",
      " state (15)  A[0]:(3.07814701728e-07) A[1]:(0.999998867512) A[2]:(8.01422288532e-07) A[3]:(8.12602384138e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 920000 finished after 9 . Running score: 0.11. Policy_loss: -92050.6115601, Value_loss: 1.00166579341. Times trained:               12628. Times reached goal: 120.               Steps done: 12339516.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.99097263813) A[1]:(0.00224450416863) A[2]:(0.00193751277402) A[3]:(0.00484533235431)\n",
      " state (1)  A[0]:(0.000473286607303) A[1]:(0.00242126290686) A[2]:(6.11238374404e-06) A[3]:(0.997099339962)\n",
      " state (2)  A[0]:(0.999998629093) A[1]:(1.35625202802e-06) A[2]:(7.3255128092e-13) A[3]:(3.76301935575e-13)\n",
      " state (3)  A[0]:(0.999998927116) A[1]:(1.0831771533e-06) A[2]:(1.46437373403e-13) A[3]:(4.41094016645e-15)\n",
      " state (4)  A[0]:(0.999996960163) A[1]:(3.05898038278e-06) A[2]:(1.14680509013e-13) A[3]:(2.25182517887e-16)\n",
      " state (5)  A[0]:(0.999918162823) A[1]:(8.18358530523e-05) A[2]:(3.84525229583e-13) A[3]:(4.29947670541e-17)\n",
      " state (6)  A[0]:(0.989990949631) A[1]:(0.0100090336055) A[2]:(1.73431321387e-11) A[3]:(4.33861359804e-17)\n",
      " state (7)  A[0]:(0.010784455575) A[1]:(0.989215552807) A[2]:(6.80440104261e-09) A[3]:(2.67774100102e-20)\n",
      " state (8)  A[0]:(0.00028837431455) A[1]:(0.999711632729) A[2]:(1.46304683923e-08) A[3]:(2.86906843206e-22)\n",
      " state (9)  A[0]:(6.61202066112e-05) A[1]:(0.999933838844) A[2]:(1.94192448788e-08) A[3]:(7.53167662941e-23)\n",
      " state (10)  A[0]:(2.15167274291e-05) A[1]:(0.999978482723) A[2]:(2.43182611825e-08) A[3]:(3.74074086278e-23)\n",
      " state (11)  A[0]:(6.79610093357e-06) A[1]:(0.999993145466) A[2]:(3.17662554039e-08) A[3]:(2.48448460438e-23)\n",
      " state (12)  A[0]:(1.60903493907e-06) A[1]:(0.99999833107) A[2]:(4.728454428e-08) A[3]:(1.85635710627e-23)\n",
      " state (13)  A[0]:(2.63171585857e-07) A[1]:(0.999999642372) A[2]:(8.27244193147e-08) A[3]:(1.41530734074e-23)\n",
      " state (14)  A[0]:(3.90119119231e-08) A[1]:(0.999999821186) A[2]:(1.5540265963e-07) A[3]:(1.08082440047e-23)\n",
      " state (15)  A[0]:(7.78874564844e-09) A[1]:(0.999999701977) A[2]:(2.73534084272e-07) A[3]:(8.39367566906e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 921000 finished after 9 . Running score: 0.13. Policy_loss: -92050.6116136, Value_loss: 1.21186784402. Times trained:               12066. Times reached goal: 110.               Steps done: 12351582.\n",
      " state (0)  A[0]:(0.989340305328) A[1]:(0.00229258136824) A[2]:(0.00210149679333) A[3]:(0.00626563606784)\n",
      " state (1)  A[0]:(0.000228864868404) A[1]:(0.00118724082131) A[2]:(3.13133682539e-06) A[3]:(0.998580753803)\n",
      " state (2)  A[0]:(0.999997377396) A[1]:(2.60876186076e-06) A[2]:(1.16182584387e-12) A[3]:(3.48242593616e-12)\n",
      " state (3)  A[0]:(0.999998867512) A[1]:(1.11948941139e-06) A[2]:(7.63571267304e-14) A[3]:(7.70044174503e-15)\n",
      " state (4)  A[0]:(0.999995529652) A[1]:(4.44752504336e-06) A[2]:(5.73238559485e-14) A[3]:(2.01881290018e-16)\n",
      " state (5)  A[0]:(0.999797284603) A[1]:(0.000202726805583) A[2]:(2.94598953522e-13) A[3]:(5.39650381698e-17)\n",
      " state (6)  A[0]:(0.97314697504) A[1]:(0.0268530268222) A[2]:(1.51442955398e-11) A[3]:(5.21609572539e-17)\n",
      " state (7)  A[0]:(0.0219898093492) A[1]:(0.978010177612) A[2]:(1.31597521769e-09) A[3]:(1.12816966548e-19)\n",
      " state (8)  A[0]:(0.000453107088106) A[1]:(0.99954688549) A[2]:(2.83298851045e-09) A[3]:(7.97778794739e-22)\n",
      " state (9)  A[0]:(6.8298097176e-05) A[1]:(0.999931693077) A[2]:(4.04480715588e-09) A[3]:(1.41875232787e-22)\n",
      " state (10)  A[0]:(1.16509490908e-05) A[1]:(0.99998831749) A[2]:(5.72650105113e-09) A[3]:(5.05528037367e-23)\n",
      " state (11)  A[0]:(1.46090235376e-06) A[1]:(0.999998509884) A[2]:(8.92370177752e-09) A[3]:(2.53748517093e-23)\n",
      " state (12)  A[0]:(1.38264923066e-07) A[1]:(0.999999821186) A[2]:(1.54291477372e-08) A[3]:(1.55712339003e-23)\n",
      " state (13)  A[0]:(1.53365515843e-08) A[1]:(0.999999940395) A[2]:(2.65683191003e-08) A[3]:(1.08126568926e-23)\n",
      " state (14)  A[0]:(2.82115575345e-09) A[1]:(0.999999940395) A[2]:(4.1595189515e-08) A[3]:(8.16671092029e-24)\n",
      " state (15)  A[0]:(8.70446492662e-10) A[1]:(0.999999940395) A[2]:(5.87867532431e-08) A[3]:(6.44088678578e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 922000 finished after 12 . Running score: 0.14. Policy_loss: -92050.6117836, Value_loss: 1.21280264431. Times trained:               12790. Times reached goal: 134.               Steps done: 12364372.\n",
      " state (0)  A[0]:(0.994526386261) A[1]:(0.00174750422593) A[2]:(0.00102441664785) A[3]:(0.00270167738199)\n",
      " state (1)  A[0]:(0.000533406680916) A[1]:(0.00287716370076) A[2]:(3.61172692465e-06) A[3]:(0.996585845947)\n",
      " state (2)  A[0]:(0.999998748302) A[1]:(1.24218115616e-06) A[2]:(9.392946219e-14) A[3]:(2.08630232231e-13)\n",
      " state (3)  A[0]:(0.999998748302) A[1]:(1.26256600197e-06) A[2]:(2.26504741612e-14) A[3]:(3.02173117054e-15)\n",
      " state (4)  A[0]:(0.999995529652) A[1]:(4.48172386314e-06) A[2]:(1.72531548103e-14) A[3]:(9.85088795919e-17)\n",
      " state (5)  A[0]:(0.999782204628) A[1]:(0.00021780966199) A[2]:(1.06397550004e-13) A[3]:(3.9862131504e-17)\n",
      " state (6)  A[0]:(0.969199061394) A[1]:(0.0308009330183) A[2]:(4.94774056795e-12) A[3]:(4.76375895573e-17)\n",
      " state (7)  A[0]:(0.011104369536) A[1]:(0.988895654678) A[2]:(3.02945002417e-10) A[3]:(5.41974424211e-20)\n",
      " state (8)  A[0]:(6.38396086288e-05) A[1]:(0.999936163425) A[2]:(6.41903585841e-10) A[3]:(1.97394114715e-22)\n",
      " state (9)  A[0]:(2.92522008749e-06) A[1]:(0.999997079372) A[2]:(1.04050623673e-09) A[3]:(3.58211008553e-23)\n",
      " state (10)  A[0]:(1.45827002029e-07) A[1]:(0.999999880791) A[2]:(1.78969572495e-09) A[3]:(1.49855109891e-23)\n",
      " state (11)  A[0]:(9.70228075658e-09) A[1]:(1.0) A[2]:(3.03699820847e-09) A[3]:(8.72088886165e-24)\n",
      " state (12)  A[0]:(1.3133414356e-09) A[1]:(1.0) A[2]:(4.63841853815e-09) A[3]:(6.01684210036e-24)\n",
      " state (13)  A[0]:(3.51256440601e-10) A[1]:(1.0) A[2]:(6.43968345315e-09) A[3]:(4.40498903956e-24)\n",
      " state (14)  A[0]:(1.49227477597e-10) A[1]:(1.0) A[2]:(8.57772874951e-09) A[3]:(3.14929267519e-24)\n",
      " state (15)  A[0]:(8.20107592947e-11) A[1]:(1.0) A[2]:(1.1331178662e-08) A[3]:(2.15953492982e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 923000 finished after 7 . Running score: 0.09. Policy_loss: -92050.6115361, Value_loss: 1.2010035976. Times trained:               12973. Times reached goal: 125.               Steps done: 12377345.\n",
      " state (0)  A[0]:(0.996653854847) A[1]:(0.00213819905184) A[2]:(0.000630894093774) A[3]:(0.00057707563974)\n",
      " state (1)  A[0]:(0.00157187122386) A[1]:(0.0110799390823) A[2]:(6.83116968503e-06) A[3]:(0.987341344357)\n",
      " state (2)  A[0]:(0.999996602535) A[1]:(3.42644329976e-06) A[2]:(1.43079748353e-13) A[3]:(3.01286502257e-13)\n",
      " state (3)  A[0]:(0.999997973442) A[1]:(2.02899605029e-06) A[2]:(1.11517045937e-14) A[3]:(7.18123373589e-16)\n",
      " state (4)  A[0]:(0.999989628792) A[1]:(1.03773372757e-05) A[2]:(1.05574084902e-14) A[3]:(2.53318586659e-17)\n",
      " state (5)  A[0]:(0.999154210091) A[1]:(0.000845760165248) A[2]:(1.28420887748e-13) A[3]:(3.23140223876e-17)\n",
      " state (6)  A[0]:(0.942164719105) A[1]:(0.057835303247) A[2]:(3.39266353726e-12) A[3]:(4.42916241272e-17)\n",
      " state (7)  A[0]:(0.0914442017674) A[1]:(0.908555805683) A[2]:(6.72561034643e-11) A[3]:(1.08492771328e-18)\n",
      " state (8)  A[0]:(0.00243287254125) A[1]:(0.997567117214) A[2]:(1.25697355302e-10) A[3]:(8.22084404498e-21)\n",
      " state (9)  A[0]:(0.000444231904112) A[1]:(0.999555766582) A[2]:(1.64781702172e-10) A[3]:(1.09195918793e-21)\n",
      " state (10)  A[0]:(0.000115343333164) A[1]:(0.999884665012) A[2]:(2.04660913217e-10) A[3]:(2.81387997557e-22)\n",
      " state (11)  A[0]:(2.6319110475e-05) A[1]:(0.999973654747) A[2]:(2.5866858655e-10) A[3]:(8.75469502243e-23)\n",
      " state (12)  A[0]:(4.62903562948e-06) A[1]:(0.999995350838) A[2]:(3.41910721957e-10) A[3]:(3.30795409969e-23)\n",
      " state (13)  A[0]:(6.11481425494e-07) A[1]:(0.999999403954) A[2]:(4.80643636003e-10) A[3]:(1.56238256791e-23)\n",
      " state (14)  A[0]:(7.08297918095e-08) A[1]:(0.999999940395) A[2]:(7.04505787041e-10) A[3]:(8.97605223234e-24)\n",
      " state (15)  A[0]:(9.77863745533e-09) A[1]:(1.0) A[2]:(1.01815988973e-09) A[3]:(5.98641534082e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 924000 finished after 7 . Running score: 0.13. Policy_loss: -92050.6114912, Value_loss: 1.22796003103. Times trained:               12767. Times reached goal: 120.               Steps done: 12390112.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9922,  0.0024,  0.0027,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9922,  0.0024,  0.0027,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9922,  0.0024,  0.0027,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9922,  0.0024,  0.0027,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9922,  0.0024,  0.0027,  0.0028]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9960e-01,  3.9566e-04,  1.4233e-13,  4.0644e-17]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7548e-07,  1.0000e+00,  6.4405e-10,  1.6370e-23]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.3424e-09,  1.0000e+00,  1.1891e-09,  6.2923e-24]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.7553e-07,  1.0000e+00,  6.4407e-10,  1.6372e-23]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.992170274258) A[1]:(0.00238436949439) A[2]:(0.00266930786893) A[3]:(0.00277606444433)\n",
      " state (1)  A[0]:(0.0015796875814) A[1]:(0.0156990233809) A[2]:(1.19051874208e-05) A[3]:(0.982709407806)\n",
      " state (2)  A[0]:(0.999997079372) A[1]:(2.91471633318e-06) A[2]:(4.47346220939e-14) A[3]:(8.01426745198e-15)\n",
      " state (3)  A[0]:(0.999992847443) A[1]:(7.15847727406e-06) A[2]:(2.34835480055e-14) A[3]:(1.66539634387e-16)\n",
      " state (4)  A[0]:(0.999604523182) A[1]:(0.000395468116039) A[2]:(1.42305289189e-13) A[3]:(4.06471623012e-17)\n",
      " state (5)  A[0]:(0.945149362087) A[1]:(0.0548506304622) A[2]:(5.29405772692e-12) A[3]:(8.27050844459e-17)\n",
      " state (6)  A[0]:(0.0517783761024) A[1]:(0.948221623898) A[2]:(1.09043787888e-10) A[3]:(1.24406526902e-18)\n",
      " state (7)  A[0]:(2.63821348199e-05) A[1]:(0.999973595142) A[2]:(3.35856065181e-10) A[3]:(1.84543911357e-22)\n",
      " state (8)  A[0]:(2.75550121387e-07) A[1]:(0.999999701977) A[2]:(6.44085784707e-10) A[3]:(1.63728838185e-23)\n",
      " state (9)  A[0]:(6.34345020956e-09) A[1]:(1.0) A[2]:(1.18917853342e-09) A[3]:(6.29288464705e-24)\n",
      " state (10)  A[0]:(4.22110318921e-10) A[1]:(1.0) A[2]:(1.93139104709e-09) A[3]:(3.65727314701e-24)\n",
      " state (11)  A[0]:(8.25818025074e-11) A[1]:(1.0) A[2]:(2.76589351422e-09) A[3]:(2.41763266585e-24)\n",
      " state (12)  A[0]:(3.06904571201e-11) A[1]:(1.0) A[2]:(3.89123178124e-09) A[3]:(1.50103920564e-24)\n",
      " state (13)  A[0]:(1.51669354159e-11) A[1]:(1.0) A[2]:(5.61572077729e-09) A[3]:(8.53398996384e-25)\n",
      " state (14)  A[0]:(9.14072376818e-12) A[1]:(1.0) A[2]:(7.73273622912e-09) A[3]:(5.14223615626e-25)\n",
      " state (15)  A[0]:(6.59648637796e-12) A[1]:(1.0) A[2]:(9.66290691906e-09) A[3]:(3.60241147023e-25)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 925000 finished after 9 . Running score: 0.13. Policy_loss: -92050.6115321, Value_loss: 1.2207952854. Times trained:               12897. Times reached goal: 124.               Steps done: 12403009.\n",
      " state (0)  A[0]:(0.995000481606) A[1]:(0.00237504369579) A[2]:(0.00204144162126) A[3]:(0.000583042798098)\n",
      " state (1)  A[0]:(0.00124017405324) A[1]:(0.0102742342278) A[2]:(1.39789472087e-05) A[3]:(0.988471627235)\n",
      " state (2)  A[0]:(0.999978840351) A[1]:(2.11741025851e-05) A[2]:(1.05847264547e-12) A[3]:(1.89328370689e-12)\n",
      " state (3)  A[0]:(0.999995470047) A[1]:(4.55560120827e-06) A[2]:(1.34504630741e-14) A[3]:(3.35369513515e-16)\n",
      " state (4)  A[0]:(0.999956429005) A[1]:(4.35962065239e-05) A[2]:(2.31373300646e-14) A[3]:(3.28634092807e-17)\n",
      " state (5)  A[0]:(0.948990821838) A[1]:(0.0510091930628) A[2]:(2.46950225026e-12) A[3]:(7.6297743421e-17)\n",
      " state (6)  A[0]:(0.0159818641841) A[1]:(0.984018146992) A[2]:(5.2192805633e-11) A[3]:(2.39829980918e-19)\n",
      " state (7)  A[0]:(1.3615651369e-05) A[1]:(0.999986410141) A[2]:(1.18816886885e-10) A[3]:(9.587438051e-23)\n",
      " state (8)  A[0]:(2.24189079745e-07) A[1]:(0.999999761581) A[2]:(1.83692977496e-10) A[3]:(1.39967795517e-23)\n",
      " state (9)  A[0]:(6.57130572179e-09) A[1]:(1.0) A[2]:(2.81647705158e-10) A[3]:(6.14950247221e-24)\n",
      " state (10)  A[0]:(5.08839914648e-10) A[1]:(1.0) A[2]:(3.94168170326e-10) A[3]:(3.83670981509e-24)\n",
      " state (11)  A[0]:(1.10806794296e-10) A[1]:(1.0) A[2]:(4.96796770388e-10) A[3]:(2.80116717443e-24)\n",
      " state (12)  A[0]:(4.75045801307e-11) A[1]:(1.0) A[2]:(5.96482530035e-10) A[3]:(2.12429513268e-24)\n",
      " state (13)  A[0]:(2.84451646265e-11) A[1]:(1.0) A[2]:(7.10107195268e-10) A[3]:(1.59840268654e-24)\n",
      " state (14)  A[0]:(2.03763585865e-11) A[1]:(1.0) A[2]:(8.26464507941e-10) A[3]:(1.24011182554e-24)\n",
      " state (15)  A[0]:(1.6448988005e-11) A[1]:(1.0) A[2]:(9.24304965899e-10) A[3]:(1.02657605319e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 926000 finished after 3 . Running score: 0.13. Policy_loss: -92050.6115298, Value_loss: 1.65117021272. Times trained:               12560. Times reached goal: 135.               Steps done: 12415569.\n",
      " state (0)  A[0]:(0.995694875717) A[1]:(0.000811741047073) A[2]:(0.0010817181319) A[3]:(0.00241168029606)\n",
      " state (1)  A[0]:(7.3176677688e-05) A[1]:(0.000186353514437) A[2]:(1.0000110251e-06) A[3]:(0.999739468098)\n",
      " state (2)  A[0]:(0.00542563106865) A[1]:(0.0159747637808) A[2]:(7.55120163376e-06) A[3]:(0.978592038155)\n",
      " state (3)  A[0]:(0.999997973442) A[1]:(2.03517424779e-06) A[2]:(1.77240356723e-14) A[3]:(2.46772308805e-14)\n",
      " state (4)  A[0]:(0.999994754791) A[1]:(5.25250970895e-06) A[2]:(5.5693161671e-15) A[3]:(1.4054918279e-16)\n",
      " state (5)  A[0]:(0.99923813343) A[1]:(0.000761880306527) A[2]:(8.16021986853e-14) A[3]:(1.07057877249e-16)\n",
      " state (6)  A[0]:(0.943341851234) A[1]:(0.0566581338644) A[2]:(1.91910896896e-12) A[3]:(1.79015410614e-16)\n",
      " state (7)  A[0]:(0.296565115452) A[1]:(0.703434884548) A[2]:(2.01246051074e-11) A[3]:(2.40901513786e-17)\n",
      " state (8)  A[0]:(0.0138538200408) A[1]:(0.98614615202) A[2]:(3.871917717e-11) A[3]:(2.73363417662e-19)\n",
      " state (9)  A[0]:(0.0022157644853) A[1]:(0.997784256935) A[2]:(4.96244399739e-11) A[3]:(2.41050544073e-20)\n",
      " state (10)  A[0]:(0.000563612731639) A[1]:(0.999436378479) A[2]:(5.92527554799e-11) A[3]:(4.78714785202e-21)\n",
      " state (11)  A[0]:(0.000127399456687) A[1]:(0.999872624874) A[2]:(7.02330971158e-11) A[3]:(1.1458008506e-21)\n",
      " state (12)  A[0]:(2.02524843189e-05) A[1]:(0.999979734421) A[2]:(8.42850442218e-11) A[3]:(3.2631235131e-22)\n",
      " state (13)  A[0]:(1.95535744751e-06) A[1]:(0.999998033047) A[2]:(1.05218965485e-10) A[3]:(1.20558418836e-22)\n",
      " state (14)  A[0]:(1.35626336828e-07) A[1]:(0.999999880791) A[2]:(1.37029734781e-10) A[3]:(5.94189345127e-23)\n",
      " state (15)  A[0]:(1.2412436412e-08) A[1]:(1.0) A[2]:(1.76225090076e-10) A[3]:(3.69423656578e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 927000 finished after 11 . Running score: 0.12. Policy_loss: -92050.6123824, Value_loss: 1.42112970364. Times trained:               12773. Times reached goal: 126.               Steps done: 12428342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995861172676) A[1]:(0.000686375889927) A[2]:(0.00101081316825) A[3]:(0.00244161486626)\n",
      " state (1)  A[0]:(0.000211273712921) A[1]:(0.000701464654412) A[2]:(1.92341030925e-06) A[3]:(0.999085366726)\n",
      " state (2)  A[0]:(0.999321877956) A[1]:(0.000677917327266) A[2]:(6.28508856071e-10) A[3]:(1.86662347801e-07)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(1.86356749055e-06) A[2]:(5.42568944891e-15) A[3]:(2.57847713518e-15)\n",
      " state (4)  A[0]:(0.999994754791) A[1]:(5.21963283973e-06) A[2]:(4.29079870728e-15) A[3]:(1.23523317624e-16)\n",
      " state (5)  A[0]:(0.999035596848) A[1]:(0.000964378472418) A[2]:(8.10283033704e-14) A[3]:(7.778890522e-17)\n",
      " state (6)  A[0]:(0.719099402428) A[1]:(0.280900627375) A[2]:(5.47440912047e-12) A[3]:(1.0236221114e-16)\n",
      " state (7)  A[0]:(0.0190108753741) A[1]:(0.980989098549) A[2]:(2.14540520765e-11) A[3]:(4.64917276876e-19)\n",
      " state (8)  A[0]:(0.000575231213588) A[1]:(0.999424755573) A[2]:(3.15476221535e-11) A[3]:(5.68466062707e-21)\n",
      " state (9)  A[0]:(5.35619728907e-05) A[1]:(0.999946415424) A[2]:(3.90304559506e-11) A[3]:(6.63645682689e-22)\n",
      " state (10)  A[0]:(3.56391547029e-06) A[1]:(0.999996423721) A[2]:(4.82420041414e-11) A[3]:(1.49735808878e-22)\n",
      " state (11)  A[0]:(1.29310933517e-07) A[1]:(0.999999880791) A[2]:(6.27932913999e-11) A[3]:(5.36408186828e-23)\n",
      " state (12)  A[0]:(6.35597308118e-09) A[1]:(1.0) A[2]:(8.13660250287e-11) A[3]:(2.84189602352e-23)\n",
      " state (13)  A[0]:(8.76089645274e-10) A[1]:(1.0) A[2]:(9.92871687644e-11) A[3]:(1.91609864703e-23)\n",
      " state (14)  A[0]:(2.80481193826e-10) A[1]:(1.0) A[2]:(1.16684467644e-10) A[3]:(1.42264232721e-23)\n",
      " state (15)  A[0]:(1.42715783635e-10) A[1]:(1.0) A[2]:(1.36068184498e-10) A[3]:(1.07900781158e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 928000 finished after 8 . Running score: 0.12. Policy_loss: -92050.6115482, Value_loss: 1.2107924492. Times trained:               12685. Times reached goal: 127.               Steps done: 12441027.\n",
      " state (0)  A[0]:(0.998651623726) A[1]:(0.000777082226705) A[2]:(0.000405093509471) A[3]:(0.000166190438904)\n",
      " state (1)  A[0]:(0.00164544361178) A[1]:(0.00591828627512) A[2]:(7.3791388786e-06) A[3]:(0.992428898811)\n",
      " state (2)  A[0]:(0.99834883213) A[1]:(0.00165100838058) A[2]:(1.06184860904e-09) A[3]:(1.6340064235e-07)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(1.86597185348e-06) A[2]:(2.64720687082e-15) A[3]:(7.66707595256e-16)\n",
      " state (4)  A[0]:(0.999996423721) A[1]:(3.58544707524e-06) A[2]:(2.09096435777e-15) A[3]:(7.50970676008e-17)\n",
      " state (5)  A[0]:(0.999692320824) A[1]:(0.000307676178636) A[2]:(2.32624486893e-14) A[3]:(4.44535463864e-17)\n",
      " state (6)  A[0]:(0.7795317173) A[1]:(0.220468312502) A[2]:(2.72080600237e-12) A[3]:(7.14520002787e-17)\n",
      " state (7)  A[0]:(0.0111059611663) A[1]:(0.988894045353) A[2]:(1.31441446269e-11) A[3]:(1.64476890268e-19)\n",
      " state (8)  A[0]:(3.7483718188e-05) A[1]:(0.999962508678) A[2]:(2.19006375535e-11) A[3]:(5.86329690426e-22)\n",
      " state (9)  A[0]:(1.93832335071e-07) A[1]:(0.999999821186) A[2]:(3.1191119132e-11) A[3]:(4.89950306408e-23)\n",
      " state (10)  A[0]:(2.89033930123e-09) A[1]:(1.0) A[2]:(4.13421831491e-11) A[3]:(1.60493498732e-23)\n",
      " state (11)  A[0]:(3.03068931062e-10) A[1]:(1.0) A[2]:(5.1349414304e-11) A[3]:(9.04379645143e-24)\n",
      " state (12)  A[0]:(9.25017007436e-11) A[1]:(1.0) A[2]:(6.75890038382e-11) A[3]:(5.1952580615e-24)\n",
      " state (13)  A[0]:(4.19220040626e-11) A[1]:(1.0) A[2]:(9.46545064551e-11) A[3]:(2.79463520891e-24)\n",
      " state (14)  A[0]:(2.44665104149e-11) A[1]:(1.0) A[2]:(1.2660907045e-10) A[3]:(1.6585796588e-24)\n",
      " state (15)  A[0]:(1.75521229495e-11) A[1]:(1.0) A[2]:(1.54080789794e-10) A[3]:(1.16995181853e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 929000 finished after 5 . Running score: 0.14. Policy_loss: -92050.6115314, Value_loss: 1.64538005254. Times trained:               12846. Times reached goal: 126.               Steps done: 12453873.\n",
      "action_dist \n",
      "tensor([[ 0.9948,  0.0009,  0.0040,  0.0003]])\n",
      "On state=0, selected action=2\n",
      "new state=1, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.0012,  0.0042,  0.0000,  0.9946]])\n",
      "On state=1, selected action=3\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9949,  0.0009,  0.0039,  0.0003]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0009,  0.0038,  0.0003]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.7390e-06,  1.0142e-14,  9.6970e-17]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9951,  0.0009,  0.0037,  0.0003]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0009,  0.0037,  0.0003]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0009,  0.0036,  0.0003]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9952,  0.0009,  0.0036,  0.0003]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0009,  0.0036,  0.0002]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0009,  0.0036,  0.0002]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9953,  0.0009,  0.0035,  0.0002]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.7107e-06,  9.7600e-15,  9.6337e-17]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.0179e-02,  9.7982e-01,  6.2399e-11,  3.1613e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.0277e-02,  9.7972e-01,  6.2238e-11,  3.1813e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995374977589) A[1]:(0.000910980685148) A[2]:(0.00348328473046) A[3]:(0.000230778765399)\n",
      " state (1)  A[0]:(0.0013892732095) A[1]:(0.00479364395142) A[2]:(2.60486485786e-05) A[3]:(0.993791043758)\n",
      " state (2)  A[0]:(0.99924248457) A[1]:(0.000757458561566) A[2]:(1.58733981515e-09) A[3]:(5.50986563042e-08)\n",
      " state (3)  A[0]:(0.999998509884) A[1]:(1.51440565332e-06) A[2]:(1.20514365428e-14) A[3]:(1.10179578795e-15)\n",
      " state (4)  A[0]:(0.999997317791) A[1]:(2.69484985438e-06) A[2]:(9.67280878468e-15) A[3]:(9.64969116444e-17)\n",
      " state (5)  A[0]:(0.999927103519) A[1]:(7.29228704586e-05) A[2]:(5.00390236245e-14) A[3]:(3.8706801725e-17)\n",
      " state (6)  A[0]:(0.97802734375) A[1]:(0.0219726711512) A[2]:(2.20454340547e-12) A[3]:(8.47655714994e-17)\n",
      " state (7)  A[0]:(0.448180049658) A[1]:(0.551819980145) A[2]:(2.96548480216e-11) A[3]:(2.7714866749e-17)\n",
      " state (8)  A[0]:(0.0208742339164) A[1]:(0.979125738144) A[2]:(6.19723092288e-11) A[3]:(3.29799753658e-19)\n",
      " state (9)  A[0]:(0.000920527148992) A[1]:(0.999079465866) A[2]:(8.67421690032e-11) A[3]:(9.06303184642e-21)\n",
      " state (10)  A[0]:(3.90320383303e-05) A[1]:(0.999960958958) A[2]:(1.13436128546e-10) A[3]:(6.60034895932e-22)\n",
      " state (11)  A[0]:(7.84361247952e-07) A[1]:(0.99999922514) A[2]:(1.46262904566e-10) A[3]:(9.43787507208e-23)\n",
      " state (12)  A[0]:(1.68913949494e-08) A[1]:(1.0) A[2]:(1.82540441096e-10) A[3]:(2.9569987182e-23)\n",
      " state (13)  A[0]:(1.32190625113e-09) A[1]:(1.0) A[2]:(2.1562822361e-10) A[3]:(1.60917101261e-23)\n",
      " state (14)  A[0]:(3.21335374975e-10) A[1]:(1.0) A[2]:(2.5475052623e-10) A[3]:(1.05226503246e-23)\n",
      " state (15)  A[0]:(1.34821959152e-10) A[1]:(1.0) A[2]:(3.16868031813e-10) A[3]:(6.75081603595e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 930000 finished after 15 . Running score: 0.23. Policy_loss: -92050.6125741, Value_loss: 1.62264278398. Times trained:               12553. Times reached goal: 124.               Steps done: 12466426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994441926479) A[1]:(0.00104441319127) A[2]:(0.00405108602718) A[3]:(0.000462597119622)\n",
      " state (1)  A[0]:(0.000455747562228) A[1]:(0.00160853320267) A[2]:(1.10620994747e-05) A[3]:(0.997924685478)\n",
      " state (2)  A[0]:(0.973782837391) A[1]:(0.0260351113975) A[2]:(4.22717846504e-07) A[3]:(0.000181610725122)\n",
      " state (3)  A[0]:(0.999997615814) A[1]:(2.3734212391e-06) A[2]:(1.11681336447e-14) A[3]:(1.7228716851e-15)\n",
      " state (4)  A[0]:(0.999993085861) A[1]:(6.92189814799e-06) A[2]:(1.07373309937e-14) A[3]:(1.20620747472e-16)\n",
      " state (5)  A[0]:(0.998030364513) A[1]:(0.00196963432245) A[2]:(2.58568638506e-13) A[3]:(9.92767017237e-17)\n",
      " state (6)  A[0]:(0.720698595047) A[1]:(0.279301434755) A[2]:(9.51325910409e-12) A[3]:(1.13502917858e-16)\n",
      " state (7)  A[0]:(0.043635148555) A[1]:(0.956364870071) A[2]:(2.81080055065e-11) A[3]:(1.60516703632e-18)\n",
      " state (8)  A[0]:(0.000672048248816) A[1]:(0.99932795763) A[2]:(4.16686234128e-11) A[3]:(1.0066638613e-20)\n",
      " state (9)  A[0]:(1.29240861497e-05) A[1]:(0.999987065792) A[2]:(5.3630436897e-11) A[3]:(4.28616983183e-22)\n",
      " state (10)  A[0]:(1.3728684678e-07) A[1]:(0.999999880791) A[2]:(6.52160270231e-11) A[3]:(6.4280772624e-23)\n",
      " state (11)  A[0]:(2.97394331383e-09) A[1]:(1.0) A[2]:(7.63989982389e-11) A[3]:(2.4272907688e-23)\n",
      " state (12)  A[0]:(3.45498213372e-10) A[1]:(1.0) A[2]:(8.88301723845e-11) A[3]:(1.41172196793e-23)\n",
      " state (13)  A[0]:(1.06684688861e-10) A[1]:(1.0) A[2]:(1.10658107677e-10) A[3]:(8.50620509946e-24)\n",
      " state (14)  A[0]:(4.89669381432e-11) A[1]:(1.0) A[2]:(1.45960368791e-10) A[3]:(4.9289516361e-24)\n",
      " state (15)  A[0]:(2.84346417939e-11) A[1]:(1.0) A[2]:(1.88343521335e-10) A[3]:(3.05147944497e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 931000 finished after 5 . Running score: 0.09. Policy_loss: -92050.6113672, Value_loss: 1.21896691976. Times trained:               12719. Times reached goal: 129.               Steps done: 12479145.\n",
      " state (0)  A[0]:(0.995459914207) A[1]:(0.00126574817114) A[2]:(0.00184699962847) A[3]:(0.00142731599044)\n",
      " state (1)  A[0]:(0.000638759869616) A[1]:(0.00287386169657) A[2]:(8.00371071819e-06) A[3]:(0.996479392052)\n",
      " state (2)  A[0]:(0.999909758568) A[1]:(9.02297833818e-05) A[2]:(7.85076344212e-12) A[3]:(1.70597674876e-10)\n",
      " state (3)  A[0]:(0.999996781349) A[1]:(3.22676351061e-06) A[2]:(5.33761284547e-15) A[3]:(1.45489798916e-15)\n",
      " state (4)  A[0]:(0.999976038933) A[1]:(2.39721994149e-05) A[2]:(1.03813848793e-14) A[3]:(2.58435654968e-16)\n",
      " state (5)  A[0]:(0.982322752476) A[1]:(0.0176772214472) A[2]:(6.24803594279e-13) A[3]:(3.64851712854e-16)\n",
      " state (6)  A[0]:(0.429184526205) A[1]:(0.570815503597) A[2]:(8.23850015985e-12) A[3]:(1.57109497472e-16)\n",
      " state (7)  A[0]:(0.0214669089764) A[1]:(0.978533089161) A[2]:(1.3940453826e-11) A[3]:(1.77050668889e-18)\n",
      " state (8)  A[0]:(0.000740159186535) A[1]:(0.999259829521) A[2]:(1.93908066687e-11) A[3]:(3.19897755491e-20)\n",
      " state (9)  A[0]:(1.97194531211e-05) A[1]:(0.999980270863) A[2]:(2.46402412363e-11) A[3]:(1.54784415173e-21)\n",
      " state (10)  A[0]:(2.13056196685e-07) A[1]:(0.999999761581) A[2]:(2.93469866464e-11) A[3]:(2.06393263217e-22)\n",
      " state (11)  A[0]:(4.32706181996e-09) A[1]:(1.0) A[2]:(3.32662994373e-11) A[3]:(7.50630244508e-23)\n",
      " state (12)  A[0]:(4.89036811047e-10) A[1]:(1.0) A[2]:(3.76355960285e-11) A[3]:(4.40218590122e-23)\n",
      " state (13)  A[0]:(1.54595364288e-10) A[1]:(1.0) A[2]:(4.50263853979e-11) A[3]:(2.80990171847e-23)\n",
      " state (14)  A[0]:(7.62592350378e-11) A[1]:(1.0) A[2]:(5.56449365752e-11) A[3]:(1.81962884693e-23)\n",
      " state (15)  A[0]:(4.82077745778e-11) A[1]:(1.0) A[2]:(6.69538730014e-11) A[3]:(1.27363423305e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 932000 finished after 7 . Running score: 0.18. Policy_loss: -92050.6015022, Value_loss: 1.21819863933. Times trained:               12468. Times reached goal: 124.               Steps done: 12491613.\n",
      " state (0)  A[0]:(0.996470272541) A[1]:(0.00142104621045) A[2]:(0.00151307671331) A[3]:(0.000595629564486)\n",
      " state (1)  A[0]:(0.00199658353813) A[1]:(0.0119317062199) A[2]:(2.07370212593e-05) A[3]:(0.986050963402)\n",
      " state (2)  A[0]:(0.999978244305) A[1]:(2.17632987187e-05) A[2]:(3.37367178464e-13) A[3]:(1.22970524822e-12)\n",
      " state (3)  A[0]:(0.999995589256) A[1]:(4.39116820417e-06) A[2]:(5.44080559889e-15) A[3]:(8.41329397964e-16)\n",
      " state (4)  A[0]:(0.999946177006) A[1]:(5.38263921044e-05) A[2]:(1.65262548519e-14) A[3]:(2.40173915793e-16)\n",
      " state (5)  A[0]:(0.778701901436) A[1]:(0.221298083663) A[2]:(4.06676472012e-12) A[3]:(3.44618822132e-16)\n",
      " state (6)  A[0]:(0.0708008855581) A[1]:(0.929199099541) A[2]:(1.06353944748e-11) A[3]:(9.00214787659e-18)\n",
      " state (7)  A[0]:(0.00868218019605) A[1]:(0.991317808628) A[2]:(1.36656476066e-11) A[3]:(5.3819684545e-19)\n",
      " state (8)  A[0]:(0.00107205961831) A[1]:(0.998927950859) A[2]:(1.75957512449e-11) A[3]:(4.7723012525e-20)\n",
      " state (9)  A[0]:(4.32577762695e-05) A[1]:(0.999956727028) A[2]:(2.34135922456e-11) A[3]:(2.54332209161e-21)\n",
      " state (10)  A[0]:(4.30963581266e-07) A[1]:(0.999999582767) A[2]:(2.99692076711e-11) A[3]:(2.21604205409e-22)\n",
      " state (11)  A[0]:(6.95356172642e-09) A[1]:(1.0) A[2]:(3.4840578339e-11) A[3]:(6.62188839613e-23)\n",
      " state (12)  A[0]:(6.6526950615e-10) A[1]:(1.0) A[2]:(3.94989804753e-11) A[3]:(3.65952195279e-23)\n",
      " state (13)  A[0]:(1.92455759662e-10) A[1]:(1.0) A[2]:(4.71009238556e-11) A[3]:(2.2973325611e-23)\n",
      " state (14)  A[0]:(9.14452957801e-11) A[1]:(1.0) A[2]:(5.76736054747e-11) A[3]:(1.49793405191e-23)\n",
      " state (15)  A[0]:(5.69814716245e-11) A[1]:(1.0) A[2]:(6.86690912466e-11) A[3]:(1.06342268111e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 933000 finished after 3 . Running score: 0.14. Policy_loss: -92050.6113879, Value_loss: 1.20462070545. Times trained:               12977. Times reached goal: 128.               Steps done: 12504590.\n",
      " state (0)  A[0]:(0.996926605701) A[1]:(0.00166053511202) A[2]:(0.00105234829243) A[3]:(0.000360532460036)\n",
      " state (1)  A[0]:(0.00293659418821) A[1]:(0.0239474158734) A[2]:(2.18669774767e-05) A[3]:(0.973094105721)\n",
      " state (2)  A[0]:(0.99996650219) A[1]:(3.34774995281e-05) A[2]:(3.07667601248e-13) A[3]:(1.42781099444e-12)\n",
      " state (3)  A[0]:(0.999994397163) A[1]:(5.59728323424e-06) A[2]:(4.16964885538e-15) A[3]:(1.01455700598e-15)\n",
      " state (4)  A[0]:(0.999987542629) A[1]:(1.24583139041e-05) A[2]:(5.08310400878e-15) A[3]:(2.89027096891e-16)\n",
      " state (5)  A[0]:(0.707030296326) A[1]:(0.292969703674) A[2]:(3.74984982301e-12) A[3]:(2.21696686142e-16)\n",
      " state (6)  A[0]:(0.0098075568676) A[1]:(0.990192472935) A[2]:(6.56504017815e-12) A[3]:(8.43148093507e-19)\n",
      " state (7)  A[0]:(0.000278392748442) A[1]:(0.999721586704) A[2]:(9.24963577953e-12) A[3]:(1.67171893171e-20)\n",
      " state (8)  A[0]:(2.12252803067e-06) A[1]:(0.999997854233) A[2]:(1.2337379382e-11) A[3]:(4.57340506837e-22)\n",
      " state (9)  A[0]:(8.75665762123e-09) A[1]:(1.0) A[2]:(1.46657391092e-11) A[3]:(5.8877428833e-23)\n",
      " state (10)  A[0]:(3.51496387552e-10) A[1]:(1.0) A[2]:(1.64749863057e-11) A[3]:(2.61109567526e-23)\n",
      " state (11)  A[0]:(7.9587073043e-11) A[1]:(1.0) A[2]:(1.98899195725e-11) A[3]:(1.51477623223e-23)\n",
      " state (12)  A[0]:(3.47718000226e-11) A[1]:(1.0) A[2]:(2.52994604433e-11) A[3]:(9.06870394565e-24)\n",
      " state (13)  A[0]:(2.10233341152e-11) A[1]:(1.0) A[2]:(3.09590235392e-11) A[3]:(6.07992217935e-24)\n",
      " state (14)  A[0]:(1.55610004049e-11) A[1]:(1.0) A[2]:(3.5447741964e-11) A[3]:(4.6705914066e-24)\n",
      " state (15)  A[0]:(1.29742536159e-11) A[1]:(1.0) A[2]:(3.85658865953e-11) A[3]:(3.95952717499e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 934000 finished after 23 . Running score: 0.09. Policy_loss: -92050.6113811, Value_loss: 1.2121981292. Times trained:               12858. Times reached goal: 125.               Steps done: 12517448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9982,  0.0008,  0.0007,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9982,  0.0008,  0.0007,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3859e-06,  2.2110e-15,  4.7607e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9982,  0.0008,  0.0007,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9982,  0.0008,  0.0007,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9982,  0.0008,  0.0007,  0.0004]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3645e-06,  2.2075e-15,  4.8115e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3604e-06,  2.2068e-15,  4.8213e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.0002e-02,  9.2000e-01,  8.3474e-12,  6.5065e-18]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.998197078705) A[1]:(0.00077244406566) A[2]:(0.000667465210427) A[3]:(0.000363005266991)\n",
      " state (1)  A[0]:(0.00122678559273) A[1]:(0.00385236926377) A[2]:(8.29406417324e-06) A[3]:(0.994912564754)\n",
      " state (2)  A[0]:(0.999184131622) A[1]:(0.000815835781395) A[2]:(3.41634665002e-10) A[3]:(3.94424333194e-08)\n",
      " state (3)  A[0]:(0.999998152256) A[1]:(1.83179429314e-06) A[2]:(2.98666188894e-15) A[3]:(2.68433380721e-15)\n",
      " state (4)  A[0]:(0.999997675419) A[1]:(2.34903859564e-06) A[2]:(2.20503301423e-15) A[3]:(4.84952779143e-16)\n",
      " state (5)  A[0]:(0.999993681908) A[1]:(6.3085731199e-06) A[2]:(3.39984392534e-15) A[3]:(2.05376346482e-16)\n",
      " state (6)  A[0]:(0.911292016506) A[1]:(0.0887079760432) A[2]:(1.80883541652e-12) A[3]:(2.77812804186e-16)\n",
      " state (7)  A[0]:(0.157331973314) A[1]:(0.842668056488) A[2]:(7.67651827521e-12) A[3]:(1.79407434667e-17)\n",
      " state (8)  A[0]:(0.0806058943272) A[1]:(0.919394075871) A[2]:(8.34985899867e-12) A[3]:(6.56168920483e-18)\n",
      " state (9)  A[0]:(0.0601953268051) A[1]:(0.939804673195) A[2]:(8.74807257883e-12) A[3]:(4.38323254746e-18)\n",
      " state (10)  A[0]:(0.0454593934119) A[1]:(0.954540610313) A[2]:(9.17740015927e-12) A[3]:(3.00586164266e-18)\n",
      " state (11)  A[0]:(0.0300062634051) A[1]:(0.969993710518) A[2]:(9.83877476018e-12) A[3]:(1.74544828621e-18)\n",
      " state (12)  A[0]:(0.0153698436916) A[1]:(0.984630167484) A[2]:(1.09189844666e-11) A[3]:(7.46527142408e-19)\n",
      " state (13)  A[0]:(0.00536279473454) A[1]:(0.994637191296) A[2]:(1.26120398847e-11) A[3]:(2.08012711301e-19)\n",
      " state (14)  A[0]:(0.00111128704157) A[1]:(0.998888731003) A[2]:(1.50740427085e-11) A[3]:(3.62205006519e-20)\n",
      " state (15)  A[0]:(0.000126179817016) A[1]:(0.999873816967) A[2]:(1.81765522772e-11) A[3]:(4.96564559262e-21)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 935000 finished after 9 . Running score: 0.14. Policy_loss: -92050.6131519, Value_loss: 1.42407543284. Times trained:               12464. Times reached goal: 121.               Steps done: 12529912.\n",
      " state (0)  A[0]:(0.995425403118) A[1]:(0.000896738434676) A[2]:(0.000951983907726) A[3]:(0.00272586895153)\n",
      " state (1)  A[0]:(0.00055591709679) A[1]:(0.00276777218096) A[2]:(4.07745619668e-06) A[3]:(0.996672213078)\n",
      " state (2)  A[0]:(0.999996602535) A[1]:(3.41665418091e-06) A[2]:(1.29388509388e-14) A[3]:(1.03796637084e-13)\n",
      " state (3)  A[0]:(0.999997019768) A[1]:(2.98423492495e-06) A[2]:(3.41607964111e-15) A[3]:(3.14533572392e-15)\n",
      " state (4)  A[0]:(0.999994397163) A[1]:(5.61139358979e-06) A[2]:(3.52191450205e-15) A[3]:(6.52177411722e-16)\n",
      " state (5)  A[0]:(0.989590644836) A[1]:(0.0104093756527) A[2]:(4.8419851028e-13) A[3]:(3.51292806481e-16)\n",
      " state (6)  A[0]:(0.090928286314) A[1]:(0.909071743488) A[2]:(7.4577514303e-12) A[3]:(1.94269223297e-17)\n",
      " state (7)  A[0]:(0.0207616370171) A[1]:(0.979238390923) A[2]:(8.73030120419e-12) A[3]:(2.83730167257e-18)\n",
      " state (8)  A[0]:(0.00416891602799) A[1]:(0.99583107233) A[2]:(1.08038942376e-11) A[3]:(4.25623328497e-19)\n",
      " state (9)  A[0]:(0.000336973025696) A[1]:(0.999663054943) A[2]:(1.39550515241e-11) A[3]:(3.17300246809e-20)\n",
      " state (10)  A[0]:(5.48998605154e-06) A[1]:(0.999994516373) A[2]:(1.82482258471e-11) A[3]:(1.6005623755e-21)\n",
      " state (11)  A[0]:(5.05466424272e-08) A[1]:(0.999999940395) A[2]:(2.16196418407e-11) A[3]:(2.35950543427e-22)\n",
      " state (12)  A[0]:(2.05183381397e-09) A[1]:(1.0) A[2]:(2.38149257259e-11) A[3]:(9.80946579267e-23)\n",
      " state (13)  A[0]:(3.70213276701e-10) A[1]:(1.0) A[2]:(2.71547870995e-11) A[3]:(5.77003221446e-23)\n",
      " state (14)  A[0]:(1.37586012028e-10) A[1]:(1.0) A[2]:(3.30065384746e-11) A[3]:(3.55771692861e-23)\n",
      " state (15)  A[0]:(7.24286464138e-11) A[1]:(1.0) A[2]:(4.05031355677e-11) A[3]:(2.3062536313e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 936000 finished after 12 . Running score: 0.1. Policy_loss: -92050.6114915, Value_loss: 1.21073437372. Times trained:               13027. Times reached goal: 106.               Steps done: 12542939.\n",
      " state (0)  A[0]:(0.996376395226) A[1]:(0.000949569512159) A[2]:(0.00107906141784) A[3]:(0.00159495626576)\n",
      " state (1)  A[0]:(0.00110519560985) A[1]:(0.00678358692676) A[2]:(8.1399039118e-06) A[3]:(0.992103099823)\n",
      " state (2)  A[0]:(0.999996304512) A[1]:(3.69448025594e-06) A[2]:(1.24973976954e-14) A[3]:(5.93648462094e-14)\n",
      " state (3)  A[0]:(0.999996602535) A[1]:(3.4198881167e-06) A[2]:(3.9450728925e-15) A[3]:(2.41600372216e-15)\n",
      " state (4)  A[0]:(0.999993681908) A[1]:(6.29621627013e-06) A[2]:(4.14081288924e-15) A[3]:(5.57515285938e-16)\n",
      " state (5)  A[0]:(0.977500379086) A[1]:(0.022499633953) A[2]:(8.90212382976e-13) A[3]:(3.16841064841e-16)\n",
      " state (6)  A[0]:(0.0403855219483) A[1]:(0.9596144557) A[2]:(7.77195148516e-12) A[3]:(6.20081387822e-18)\n",
      " state (7)  A[0]:(0.000861027801875) A[1]:(0.999138951302) A[2]:(1.16174613332e-11) A[3]:(7.93512164424e-20)\n",
      " state (8)  A[0]:(3.39635630553e-06) A[1]:(0.999996602535) A[2]:(1.61343507998e-11) A[3]:(1.13565536729e-21)\n",
      " state (9)  A[0]:(1.11874669528e-08) A[1]:(1.0) A[2]:(1.86491030951e-11) A[3]:(1.23004947871e-22)\n",
      " state (10)  A[0]:(4.6668846565e-10) A[1]:(1.0) A[2]:(2.08060062235e-11) A[3]:(5.08815220759e-23)\n",
      " state (11)  A[0]:(1.01551780873e-10) A[1]:(1.0) A[2]:(2.64640948044e-11) A[3]:(2.59836740456e-23)\n",
      " state (12)  A[0]:(4.13379261377e-11) A[1]:(1.0) A[2]:(3.57971083331e-11) A[3]:(1.37226582758e-23)\n",
      " state (13)  A[0]:(2.41018073555e-11) A[1]:(1.0) A[2]:(4.53350655627e-11) A[3]:(8.60017184391e-24)\n",
      " state (14)  A[0]:(1.7788718415e-11) A[1]:(1.0) A[2]:(5.24715687567e-11) A[3]:(6.4761184971e-24)\n",
      " state (15)  A[0]:(1.50818923322e-11) A[1]:(1.0) A[2]:(5.69690822294e-11) A[3]:(5.52555767239e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 937000 finished after 4 . Running score: 0.16. Policy_loss: -92050.6113787, Value_loss: 1.20694427287. Times trained:               13109. Times reached goal: 147.               Steps done: 12556048.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.992025375366) A[1]:(0.00101144693326) A[2]:(0.00247728801332) A[3]:(0.00448589213192)\n",
      " state (1)  A[0]:(0.000104686929262) A[1]:(0.000385824736441) A[2]:(2.22119660975e-06) A[3]:(0.999507248402)\n",
      " state (2)  A[0]:(0.996779441833) A[1]:(0.00321714766324) A[2]:(6.1448210964e-09) A[3]:(3.39400276061e-06)\n",
      " state (3)  A[0]:(0.999997198582) A[1]:(2.79345931631e-06) A[2]:(7.22323947617e-15) A[3]:(1.04718530803e-14)\n",
      " state (4)  A[0]:(0.999996006489) A[1]:(4.02161003876e-06) A[2]:(5.40664899529e-15) A[3]:(1.20451960275e-15)\n",
      " state (5)  A[0]:(0.99997407198) A[1]:(2.5899224056e-05) A[2]:(1.54860780639e-14) A[3]:(4.28423097263e-16)\n",
      " state (6)  A[0]:(0.854401767254) A[1]:(0.145598262548) A[2]:(5.41461840642e-12) A[3]:(2.73783097879e-16)\n",
      " state (7)  A[0]:(0.0250950828195) A[1]:(0.974904894829) A[2]:(1.30707510787e-11) A[3]:(3.69640959561e-18)\n",
      " state (8)  A[0]:(1.60395084094e-05) A[1]:(0.999983966351) A[2]:(2.36463487679e-11) A[3]:(2.94678630163e-21)\n",
      " state (9)  A[0]:(1.31617579058e-08) A[1]:(1.0) A[2]:(2.73784155069e-11) A[3]:(1.53619150233e-22)\n",
      " state (10)  A[0]:(5.04814634539e-10) A[1]:(1.0) A[2]:(3.01950582593e-11) A[3]:(6.23318318043e-23)\n",
      " state (11)  A[0]:(1.13756899733e-10) A[1]:(1.0) A[2]:(3.89526744193e-11) A[3]:(3.10535757983e-23)\n",
      " state (12)  A[0]:(4.77669778731e-11) A[1]:(1.0) A[2]:(5.24905847954e-11) A[3]:(1.65248610273e-23)\n",
      " state (13)  A[0]:(2.8672418606e-11) A[1]:(1.0) A[2]:(6.55617574119e-11) A[3]:(1.06200383588e-23)\n",
      " state (14)  A[0]:(2.15911264562e-11) A[1]:(1.0) A[2]:(7.50282683204e-11) A[3]:(8.16390888636e-24)\n",
      " state (15)  A[0]:(1.85335028574e-11) A[1]:(1.0) A[2]:(8.08854233592e-11) A[3]:(7.05738158321e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 938000 finished after 18 . Running score: 0.1. Policy_loss: -92050.611329, Value_loss: 1.44182403232. Times trained:               12497. Times reached goal: 141.               Steps done: 12568545.\n",
      " state (0)  A[0]:(0.99496281147) A[1]:(0.000818123808131) A[2]:(0.00164635060355) A[3]:(0.00257270270959)\n",
      " state (1)  A[0]:(0.000173357824679) A[1]:(0.000537316431291) A[2]:(2.71616863756e-06) A[3]:(0.999286592007)\n",
      " state (2)  A[0]:(0.999739348888) A[1]:(0.000260635075392) A[2]:(6.89217224958e-11) A[3]:(9.64150181915e-09)\n",
      " state (3)  A[0]:(0.99999755621) A[1]:(2.43374029196e-06) A[2]:(5.1488058134e-15) A[3]:(7.58900439642e-15)\n",
      " state (4)  A[0]:(0.999996542931) A[1]:(3.4445286019e-06) A[2]:(4.02357294143e-15) A[3]:(1.05886529945e-15)\n",
      " state (5)  A[0]:(0.999981760979) A[1]:(1.82262738235e-05) A[2]:(1.0081021067e-14) A[3]:(3.85359677526e-16)\n",
      " state (6)  A[0]:(0.950888633728) A[1]:(0.049111366272) A[2]:(2.08474557062e-12) A[3]:(2.47068181648e-16)\n",
      " state (7)  A[0]:(0.0647181048989) A[1]:(0.935281872749) A[2]:(9.7371060867e-12) A[3]:(9.95312764834e-18)\n",
      " state (8)  A[0]:(0.000618258607574) A[1]:(0.99938172102) A[2]:(1.54484394027e-11) A[3]:(3.97265056782e-20)\n",
      " state (9)  A[0]:(1.03145589492e-06) A[1]:(0.999998986721) A[2]:(1.99487110858e-11) A[3]:(5.66306961769e-22)\n",
      " state (10)  A[0]:(7.48499662251e-09) A[1]:(1.0) A[2]:(2.07631915133e-11) A[3]:(1.26368966315e-22)\n",
      " state (11)  A[0]:(6.6606942184e-10) A[1]:(1.0) A[2]:(2.24856747805e-11) A[3]:(6.55458530805e-23)\n",
      " state (12)  A[0]:(1.9714903321e-10) A[1]:(1.0) A[2]:(2.69129579733e-11) A[3]:(3.86360373996e-23)\n",
      " state (13)  A[0]:(9.56979495648e-11) A[1]:(1.0) A[2]:(3.30036414864e-11) A[3]:(2.43594520402e-23)\n",
      " state (14)  A[0]:(6.11683481644e-11) A[1]:(1.0) A[2]:(3.88425749898e-11) A[3]:(1.73017280914e-23)\n",
      " state (15)  A[0]:(4.6506919843e-11) A[1]:(1.0) A[2]:(4.33722086024e-11) A[3]:(1.37952461008e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 939000 finished after 28 . Running score: 0.1. Policy_loss: -92050.6114155, Value_loss: 1.20984587869. Times trained:               12736. Times reached goal: 117.               Steps done: 12581281.\n",
      "action_dist \n",
      "tensor([[ 0.9954,  0.0015,  0.0025,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9954,  0.0015,  0.0025,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9954,  0.0015,  0.0025,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9954,  0.0015,  0.0025,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9954,  0.0015,  0.0025,  0.0006]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.6571e-06,  7.1754e-15,  5.9793e-16]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.9902e-04,  9.9950e-01,  2.2833e-11,  2.9224e-20]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.9960e-04,  9.9950e-01,  2.2834e-11,  2.9256e-20]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995436370373) A[1]:(0.00146348343696) A[2]:(0.00250419415534) A[3]:(0.000595979217906)\n",
      " state (1)  A[0]:(0.000824489165097) A[1]:(0.00386041519232) A[2]:(1.55826037371e-05) A[3]:(0.995299518108)\n",
      " state (2)  A[0]:(0.999994337559) A[1]:(5.67710503674e-06) A[2]:(6.06819417848e-14) A[3]:(2.26296574795e-13)\n",
      " state (3)  A[0]:(0.999997019768) A[1]:(2.99197472486e-06) A[2]:(7.1901941798e-15) A[3]:(2.58852654582e-15)\n",
      " state (4)  A[0]:(0.999995350838) A[1]:(4.65623816126e-06) A[2]:(7.17700079461e-15) A[3]:(5.98022624937e-16)\n",
      " state (5)  A[0]:(0.999732077122) A[1]:(0.000267918483587) A[2]:(9.66763019465e-14) A[3]:(2.38959244077e-16)\n",
      " state (6)  A[0]:(0.730115532875) A[1]:(0.269884496927) A[2]:(9.65655246737e-12) A[3]:(1.74594520669e-16)\n",
      " state (7)  A[0]:(0.0441142618656) A[1]:(0.955885767937) A[2]:(1.46304166698e-11) A[3]:(5.4699538985e-18)\n",
      " state (8)  A[0]:(0.000501183152664) A[1]:(0.999498844147) A[2]:(2.28329421342e-11) A[3]:(2.93412143136e-20)\n",
      " state (9)  A[0]:(9.97854385787e-07) A[1]:(0.999998986721) A[2]:(2.80417650905e-11) A[3]:(5.08704363713e-22)\n",
      " state (10)  A[0]:(7.66343344338e-09) A[1]:(1.0) A[2]:(2.84764330172e-11) A[3]:(1.12973464434e-22)\n",
      " state (11)  A[0]:(7.01786018187e-10) A[1]:(1.0) A[2]:(3.03898850529e-11) A[3]:(5.86921979815e-23)\n",
      " state (12)  A[0]:(2.12114423248e-10) A[1]:(1.0) A[2]:(3.57332566314e-11) A[3]:(3.5324282568e-23)\n",
      " state (13)  A[0]:(1.05131306438e-10) A[1]:(1.0) A[2]:(4.30026847464e-11) A[3]:(2.29104896844e-23)\n",
      " state (14)  A[0]:(6.83072418073e-11) A[1]:(1.0) A[2]:(4.99013053101e-11) A[3]:(1.66455756785e-23)\n",
      " state (15)  A[0]:(5.25402672757e-11) A[1]:(1.0) A[2]:(5.52148535227e-11) A[3]:(1.34757306129e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 940000 finished after 8 . Running score: 0.13. Policy_loss: -92050.6113994, Value_loss: 1.00390497934. Times trained:               12988. Times reached goal: 125.               Steps done: 12594269.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997164607048) A[1]:(0.00178413675167) A[2]:(0.000982838915661) A[3]:(6.84403785272e-05)\n",
      " state (1)  A[0]:(0.0032627817709) A[1]:(0.0153826950118) A[2]:(3.5119795939e-05) A[3]:(0.98131942749)\n",
      " state (2)  A[0]:(0.999990701675) A[1]:(9.27745895751e-06) A[2]:(9.53787491153e-14) A[3]:(3.23492832998e-13)\n",
      " state (3)  A[0]:(0.999997496605) A[1]:(2.49450749834e-06) A[2]:(3.90609709494e-15) A[3]:(1.22976923139e-15)\n",
      " state (4)  A[0]:(0.999995410442) A[1]:(4.61390982309e-06) A[2]:(4.71863970762e-15) A[3]:(3.99016892661e-16)\n",
      " state (5)  A[0]:(0.998874783516) A[1]:(0.00112523755524) A[2]:(1.76670310326e-13) A[3]:(1.72537342338e-16)\n",
      " state (6)  A[0]:(0.611027538776) A[1]:(0.388972461224) A[2]:(8.65810634992e-12) A[3]:(1.05655812729e-16)\n",
      " state (7)  A[0]:(0.0243912991136) A[1]:(0.975608706474) A[2]:(1.21792922969e-11) A[3]:(2.03151029629e-18)\n",
      " state (8)  A[0]:(4.08738742408e-06) A[1]:(0.99999588728) A[2]:(2.0093761724e-11) A[3]:(1.0222747749e-21)\n",
      " state (9)  A[0]:(5.02559149851e-09) A[1]:(1.0) A[2]:(1.95754298188e-11) A[3]:(1.05204882147e-22)\n",
      " state (10)  A[0]:(4.37935188113e-10) A[1]:(1.0) A[2]:(2.20094151238e-11) A[3]:(4.87073109806e-23)\n",
      " state (11)  A[0]:(1.39801198396e-10) A[1]:(1.0) A[2]:(2.86890494e-11) A[3]:(2.52626599114e-23)\n",
      " state (12)  A[0]:(7.3542297252e-11) A[1]:(1.0) A[2]:(3.64198289582e-11) A[3]:(1.5312410215e-23)\n",
      " state (13)  A[0]:(5.16547221663e-11) A[1]:(1.0) A[2]:(4.24848281555e-11) A[3]:(1.12418839998e-23)\n",
      " state (14)  A[0]:(4.28337226799e-11) A[1]:(1.0) A[2]:(4.63468187439e-11) A[3]:(9.46831327011e-24)\n",
      " state (15)  A[0]:(3.88505755344e-11) A[1]:(1.0) A[2]:(4.85588201593e-11) A[3]:(8.64134486114e-24)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 941000 finished after 11 . Running score: 0.12. Policy_loss: -92050.6136716, Value_loss: 0.9887311416. Times trained:               12848. Times reached goal: 122.               Steps done: 12607117.\n",
      " state (0)  A[0]:(0.995298683643) A[1]:(0.0015771582257) A[2]:(0.000925425032619) A[3]:(0.00219874060713)\n",
      " state (1)  A[0]:(0.000179680122528) A[1]:(0.000576964812353) A[2]:(2.15120940084e-06) A[3]:(0.999241232872)\n",
      " state (2)  A[0]:(0.999983072281) A[1]:(1.69006107171e-05) A[2]:(4.01692839148e-13) A[3]:(3.5121548031e-11)\n",
      " state (3)  A[0]:(0.999997854233) A[1]:(2.15342151932e-06) A[2]:(4.34891575537e-15) A[3]:(2.94973938396e-14)\n",
      " state (4)  A[0]:(0.999996483326) A[1]:(3.50610685018e-06) A[2]:(4.82651020049e-15) A[3]:(9.71922026096e-15)\n",
      " state (5)  A[0]:(0.999546229839) A[1]:(0.000453760614619) A[2]:(1.11485012845e-13) A[3]:(3.83857205191e-15)\n",
      " state (6)  A[0]:(0.753096580505) A[1]:(0.246903419495) A[2]:(7.67444961747e-12) A[3]:(2.53293429118e-15)\n",
      " state (7)  A[0]:(0.0475985072553) A[1]:(0.952401518822) A[2]:(1.32680342407e-11) A[3]:(9.14734901065e-17)\n",
      " state (8)  A[0]:(1.95561497094e-05) A[1]:(0.999980449677) A[2]:(2.20669923306e-11) A[3]:(5.48425980625e-20)\n",
      " state (9)  A[0]:(1.27683517093e-08) A[1]:(1.0) A[2]:(2.15044891616e-11) A[3]:(3.00927923145e-21)\n",
      " state (10)  A[0]:(7.23759330246e-10) A[1]:(1.0) A[2]:(2.27227993366e-11) A[3]:(1.31310802496e-21)\n",
      " state (11)  A[0]:(1.99925465072e-10) A[1]:(1.0) A[2]:(2.87022940137e-11) A[3]:(6.83665584069e-22)\n",
      " state (12)  A[0]:(9.63756990879e-11) A[1]:(1.0) A[2]:(3.6759699451e-11) A[3]:(3.99515538358e-22)\n",
      " state (13)  A[0]:(6.36067171156e-11) A[1]:(1.0) A[2]:(4.37492472805e-11) A[3]:(2.80252744673e-22)\n",
      " state (14)  A[0]:(5.06703151659e-11) A[1]:(1.0) A[2]:(4.8519011725e-11) A[3]:(2.2805428558e-22)\n",
      " state (15)  A[0]:(4.48659026597e-11) A[1]:(1.0) A[2]:(5.13868426977e-11) A[3]:(2.03626456654e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 942000 finished after 18 . Running score: 0.08. Policy_loss: -92050.6113811, Value_loss: 1.42564856264. Times trained:               12550. Times reached goal: 117.               Steps done: 12619667.\n",
      " state (0)  A[0]:(0.9937530756) A[1]:(0.00115986925084) A[2]:(0.00103552825749) A[3]:(0.00405152374879)\n",
      " state (1)  A[0]:(0.000174619810423) A[1]:(0.0005579526769) A[2]:(2.08551932701e-06) A[3]:(0.999265313148)\n",
      " state (2)  A[0]:(0.999997615814) A[1]:(2.35467268794e-06) A[2]:(9.06440026526e-15) A[3]:(2.5684158576e-13)\n",
      " state (3)  A[0]:(0.999997317791) A[1]:(2.65815560851e-06) A[2]:(4.44837817569e-15) A[3]:(2.04566469694e-14)\n",
      " state (4)  A[0]:(0.99994546175) A[1]:(5.45492075616e-05) A[2]:(2.50916925719e-14) A[3]:(6.30848116258e-15)\n",
      " state (5)  A[0]:(0.939191460609) A[1]:(0.0608085207641) A[2]:(2.91185044826e-12) A[3]:(3.46039661667e-15)\n",
      " state (6)  A[0]:(0.155933603644) A[1]:(0.844066381454) A[2]:(1.14692031913e-11) A[3]:(4.42088570968e-16)\n",
      " state (7)  A[0]:(0.00277634803206) A[1]:(0.997223675251) A[2]:(1.59221941187e-11) A[3]:(3.37453728245e-18)\n",
      " state (8)  A[0]:(3.39596340382e-07) A[1]:(0.999999642372) A[2]:(2.00641187692e-11) A[3]:(7.98405390111e-21)\n",
      " state (9)  A[0]:(2.59350385612e-09) A[1]:(1.0) A[2]:(1.88282358454e-11) A[3]:(2.04872422695e-21)\n",
      " state (10)  A[0]:(4.30810248586e-10) A[1]:(1.0) A[2]:(2.13060697557e-11) A[3]:(1.10041638268e-21)\n",
      " state (11)  A[0]:(1.82286449824e-10) A[1]:(1.0) A[2]:(2.59823464827e-11) A[3]:(6.70746693014e-22)\n",
      " state (12)  A[0]:(1.1529294186e-10) A[1]:(1.0) A[2]:(3.03406536006e-11) A[3]:(4.79578710325e-22)\n",
      " state (13)  A[0]:(9.06904135123e-11) A[1]:(1.0) A[2]:(3.33195138147e-11) A[3]:(3.95056921765e-22)\n",
      " state (14)  A[0]:(8.01849697751e-11) A[1]:(1.0) A[2]:(3.50627062107e-11) A[3]:(3.56126270061e-22)\n",
      " state (15)  A[0]:(7.53173287626e-11) A[1]:(1.0) A[2]:(3.6008983989e-11) A[3]:(3.37477244852e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 943000 finished after 5 . Running score: 0.09. Policy_loss: -92050.6127342, Value_loss: 1.41218347278. Times trained:               12785. Times reached goal: 122.               Steps done: 12632452.\n",
      " state (0)  A[0]:(0.986172199249) A[1]:(0.00085718248738) A[2]:(0.00131098192651) A[3]:(0.0116596315056)\n",
      " state (1)  A[0]:(4.013758371e-05) A[1]:(6.4050189394e-05) A[2]:(5.31689408945e-07) A[3]:(0.999895274639)\n",
      " state (2)  A[0]:(0.999996006489) A[1]:(3.9830742935e-06) A[2]:(2.65461480922e-14) A[3]:(2.4783241865e-12)\n",
      " state (3)  A[0]:(0.999997258186) A[1]:(2.71304497801e-06) A[2]:(4.05729967581e-15) A[3]:(3.22446333956e-14)\n",
      " state (4)  A[0]:(0.999978363514) A[1]:(2.16309563257e-05) A[2]:(1.13294298937e-14) A[3]:(8.60517711775e-15)\n",
      " state (5)  A[0]:(0.847605466843) A[1]:(0.152394548059) A[2]:(3.89458387415e-12) A[3]:(4.19224176519e-15)\n",
      " state (6)  A[0]:(0.0577055215836) A[1]:(0.942294478416) A[2]:(8.22478283397e-12) A[3]:(1.38759322763e-16)\n",
      " state (7)  A[0]:(3.07933878503e-05) A[1]:(0.999969184399) A[2]:(1.32522682064e-11) A[3]:(6.91537532379e-20)\n",
      " state (8)  A[0]:(8.5129565619e-09) A[1]:(1.0) A[2]:(1.18236974031e-11) A[3]:(2.64712295911e-21)\n",
      " state (9)  A[0]:(4.92173413136e-10) A[1]:(1.0) A[2]:(1.20125047062e-11) A[3]:(1.23794424719e-21)\n",
      " state (10)  A[0]:(1.64376429135e-10) A[1]:(1.0) A[2]:(1.42165819048e-11) A[3]:(7.44985244223e-22)\n",
      " state (11)  A[0]:(9.6120889026e-11) A[1]:(1.0) A[2]:(1.66628308373e-11) A[3]:(5.19906996361e-22)\n",
      " state (12)  A[0]:(7.35007887886e-11) A[1]:(1.0) A[2]:(1.83918696245e-11) A[3]:(4.22127068692e-22)\n",
      " state (13)  A[0]:(6.44680489548e-11) A[1]:(1.0) A[2]:(1.93825320377e-11) A[3]:(3.78897819252e-22)\n",
      " state (14)  A[0]:(6.05113251173e-11) A[1]:(1.0) A[2]:(1.9898190734e-11) A[3]:(3.59152239519e-22)\n",
      " state (15)  A[0]:(5.86905454791e-11) A[1]:(1.0) A[2]:(2.01558752327e-11) A[3]:(3.49898232147e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 944000 finished after 12 . Running score: 0.12. Policy_loss: -92050.6113857, Value_loss: 1.43132026272. Times trained:               12604. Times reached goal: 122.               Steps done: 12645056.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9950,  0.0013,  0.0013,  0.0024]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9950,  0.0013,  0.0013,  0.0024]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 9.9997e-01,  3.1111e-05,  1.5802e-14,  6.8559e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.5308e-04,  9.9975e-01,  1.3677e-11,  2.6034e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.5351e-04,  9.9975e-01,  1.3677e-11,  2.6071e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3151e-06,  1.0000e+00,  1.4455e-11,  1.0277e-20]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.5558e-10,  1.0000e+00,  1.8297e-11,  4.5208e-22]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.995002686977) A[1]:(0.00131241732743) A[2]:(0.00133056507912) A[3]:(0.00235431990586)\n",
      " state (1)  A[0]:(7.97654865892e-05) A[1]:(5.20725661772e-05) A[2]:(9.14643919714e-07) A[3]:(0.999867260456)\n",
      " state (2)  A[0]:(0.998305022717) A[1]:(0.00169077026658) A[2]:(4.23404378225e-09) A[3]:(4.19829711973e-06)\n",
      " state (3)  A[0]:(0.999997019768) A[1]:(2.98791428577e-06) A[2]:(5.3051151559e-15) A[3]:(2.55964937153e-14)\n",
      " state (4)  A[0]:(0.999969005585) A[1]:(3.10236537189e-05) A[2]:(1.57769813652e-14) A[3]:(6.86037225563e-15)\n",
      " state (5)  A[0]:(0.891839742661) A[1]:(0.108160249889) A[2]:(3.42167288427e-12) A[3]:(3.19370342279e-15)\n",
      " state (6)  A[0]:(0.0809446498752) A[1]:(0.919055342674) A[2]:(8.26283052396e-12) A[3]:(2.19280445251e-16)\n",
      " state (7)  A[0]:(0.0118037108332) A[1]:(0.988196313381) A[2]:(1.01508090128e-11) A[3]:(1.82603395856e-17)\n",
      " state (8)  A[0]:(0.000254200247582) A[1]:(0.99974578619) A[2]:(1.36762493691e-11) A[3]:(2.61304778692e-19)\n",
      " state (9)  A[0]:(1.31839658479e-06) A[1]:(0.999998688698) A[2]:(1.44563701288e-11) A[3]:(1.02884676122e-20)\n",
      " state (10)  A[0]:(1.6239216194e-08) A[1]:(1.0) A[2]:(1.38605281766e-11) A[3]:(2.16382612646e-21)\n",
      " state (11)  A[0]:(1.61823154876e-09) A[1]:(1.0) A[2]:(1.44023360946e-11) A[3]:(1.04614648844e-21)\n",
      " state (12)  A[0]:(4.95005314516e-10) A[1]:(1.0) A[2]:(1.61567009771e-11) A[3]:(6.44258940005e-22)\n",
      " state (13)  A[0]:(2.55662713222e-10) A[1]:(1.0) A[2]:(1.82964216694e-11) A[3]:(4.5216502394e-22)\n",
      " state (14)  A[0]:(1.74533817954e-10) A[1]:(1.0) A[2]:(2.01560278884e-11) A[3]:(3.55615820257e-22)\n",
      " state (15)  A[0]:(1.39929248744e-10) A[1]:(1.0) A[2]:(2.15022739197e-11) A[3]:(3.05632886483e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 945000 finished after 7 . Running score: 0.13. Policy_loss: -92050.6114347, Value_loss: 1.4441184814. Times trained:               13022. Times reached goal: 126.               Steps done: 12658078.\n",
      " state (0)  A[0]:(0.996976315975) A[1]:(0.00131165399216) A[2]:(0.00125531852245) A[3]:(0.000456697365735)\n",
      " state (1)  A[0]:(0.000473452004371) A[1]:(0.00035625329474) A[2]:(4.98552117278e-06) A[3]:(0.999165296555)\n",
      " state (2)  A[0]:(0.99996817112) A[1]:(3.18347738357e-05) A[2]:(3.62447159959e-12) A[3]:(1.91885188294e-10)\n",
      " state (3)  A[0]:(0.99999755621) A[1]:(2.43930776378e-06) A[2]:(6.01784171201e-15) A[3]:(1.90650768691e-14)\n",
      " state (4)  A[0]:(0.99999588728) A[1]:(4.1401776798e-06) A[2]:(6.28533048159e-15) A[3]:(7.86542682249e-15)\n",
      " state (5)  A[0]:(0.998708486557) A[1]:(0.00129150121938) A[2]:(2.1871564347e-13) A[3]:(3.59913616651e-15)\n",
      " state (6)  A[0]:(0.113642603159) A[1]:(0.886357426643) A[2]:(1.04386863736e-11) A[3]:(2.27651036594e-16)\n",
      " state (7)  A[0]:(0.0151896188036) A[1]:(0.984810352325) A[2]:(1.23037352875e-11) A[3]:(1.68984446097e-17)\n",
      " state (8)  A[0]:(0.000159158778843) A[1]:(0.999840855598) A[2]:(1.74196681385e-11) A[3]:(1.3148400313e-19)\n",
      " state (9)  A[0]:(4.59762162563e-07) A[1]:(0.999999523163) A[2]:(1.83657099945e-11) A[3]:(4.63759214325e-21)\n",
      " state (10)  A[0]:(8.08173439282e-09) A[1]:(1.0) A[2]:(1.8065954982e-11) A[3]:(1.14881371865e-21)\n",
      " state (11)  A[0]:(1.09341002918e-09) A[1]:(1.0) A[2]:(1.98231864951e-11) A[3]:(5.62290713128e-22)\n",
      " state (12)  A[0]:(3.83685416772e-10) A[1]:(1.0) A[2]:(2.31558574398e-11) A[3]:(3.36818766877e-22)\n",
      " state (13)  A[0]:(2.13269790716e-10) A[1]:(1.0) A[2]:(2.66177843655e-11) A[3]:(2.34829023034e-22)\n",
      " state (14)  A[0]:(1.52843238066e-10) A[1]:(1.0) A[2]:(2.93330013057e-11) A[3]:(1.86497812534e-22)\n",
      " state (15)  A[0]:(1.26677668355e-10) A[1]:(1.0) A[2]:(3.11578297885e-11) A[3]:(1.62474031853e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 946000 finished after 8 . Running score: 0.16. Policy_loss: -92050.6114041, Value_loss: 1.63356952167. Times trained:               12597. Times reached goal: 145.               Steps done: 12670675.\n",
      " state (0)  A[0]:(0.9948092103) A[1]:(0.00202649505809) A[2]:(0.00184463791084) A[3]:(0.00131963170134)\n",
      " state (1)  A[0]:(0.000316564255627) A[1]:(0.00032062586979) A[2]:(4.41360361947e-06) A[3]:(0.999358415604)\n",
      " state (2)  A[0]:(0.999997019768) A[1]:(2.96617781714e-06) A[2]:(4.29090865216e-14) A[3]:(7.00811696001e-13)\n",
      " state (3)  A[0]:(0.999997675419) A[1]:(2.34250637732e-06) A[2]:(6.82973083876e-15) A[3]:(1.9885441831e-14)\n",
      " state (4)  A[0]:(0.99999165535) A[1]:(8.3150316641e-06) A[2]:(1.16295311258e-14) A[3]:(7.65758950119e-15)\n",
      " state (5)  A[0]:(0.892591059208) A[1]:(0.107408948243) A[2]:(5.20476023391e-12) A[3]:(3.35220827469e-15)\n",
      " state (6)  A[0]:(0.0870992541313) A[1]:(0.912900745869) A[2]:(1.36107739662e-11) A[3]:(1.74020920545e-16)\n",
      " state (7)  A[0]:(0.0094710290432) A[1]:(0.990528941154) A[2]:(1.825385329e-11) A[3]:(1.16311809806e-17)\n",
      " state (8)  A[0]:(6.46251064609e-05) A[1]:(0.999935388565) A[2]:(2.42416000468e-11) A[3]:(9.49281841371e-20)\n",
      " state (9)  A[0]:(1.85241376016e-07) A[1]:(0.999999821186) A[2]:(2.44009812356e-11) A[3]:(4.6858917362e-21)\n",
      " state (10)  A[0]:(5.08209740957e-09) A[1]:(1.0) A[2]:(2.46244708652e-11) A[3]:(1.34859782947e-21)\n",
      " state (11)  A[0]:(8.42559355618e-10) A[1]:(1.0) A[2]:(2.86892679752e-11) A[3]:(6.36080635985e-22)\n",
      " state (12)  A[0]:(3.1476216078e-10) A[1]:(1.0) A[2]:(3.50827734918e-11) A[3]:(3.5963343205e-22)\n",
      " state (13)  A[0]:(1.79367964925e-10) A[1]:(1.0) A[2]:(4.12588400944e-11) A[3]:(2.42955224369e-22)\n",
      " state (14)  A[0]:(1.30419938738e-10) A[1]:(1.0) A[2]:(4.58950134841e-11) A[3]:(1.90545047631e-22)\n",
      " state (15)  A[0]:(1.09108867774e-10) A[1]:(1.0) A[2]:(4.89213808352e-11) A[3]:(1.65310485367e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 947000 finished after 9 . Running score: 0.09. Policy_loss: -92050.611432, Value_loss: 1.43961834437. Times trained:               12523. Times reached goal: 126.               Steps done: 12683198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.996507883072) A[1]:(0.00168629793916) A[2]:(0.00107871810906) A[3]:(0.00072710152017)\n",
      " state (1)  A[0]:(0.00021218029724) A[1]:(0.000134349305881) A[2]:(2.1035066311e-06) A[3]:(0.999651372433)\n",
      " state (2)  A[0]:(0.999477624893) A[1]:(0.00052195659373) A[2]:(9.14383790906e-10) A[3]:(4.42623445451e-07)\n",
      " state (3)  A[0]:(0.99999833107) A[1]:(1.65172252764e-06) A[2]:(4.87739909292e-15) A[3]:(2.88990836469e-14)\n",
      " state (4)  A[0]:(0.999997198582) A[1]:(2.81417669612e-06) A[2]:(4.85041601136e-15) A[3]:(1.00259503729e-14)\n",
      " state (5)  A[0]:(0.998978674412) A[1]:(0.00102135329507) A[2]:(1.79347937326e-13) A[3]:(4.20199238496e-15)\n",
      " state (6)  A[0]:(0.152944281697) A[1]:(0.847055733204) A[2]:(1.04829721292e-11) A[3]:(3.39706375145e-16)\n",
      " state (7)  A[0]:(0.0111535703763) A[1]:(0.988846421242) A[2]:(1.37385979329e-11) A[3]:(1.35291717115e-17)\n",
      " state (8)  A[0]:(1.58075981744e-05) A[1]:(0.999984204769) A[2]:(1.80368185665e-11) A[3]:(4.04650907236e-20)\n",
      " state (9)  A[0]:(4.3284011042e-08) A[1]:(0.999999940395) A[2]:(1.69773500813e-11) A[3]:(3.06754336195e-21)\n",
      " state (10)  A[0]:(2.30155938752e-09) A[1]:(1.0) A[2]:(1.81456603215e-11) A[3]:(1.06959966394e-21)\n",
      " state (11)  A[0]:(5.35894384424e-10) A[1]:(1.0) A[2]:(2.2372137129e-11) A[3]:(5.13340492942e-22)\n",
      " state (12)  A[0]:(2.40174741206e-10) A[1]:(1.0) A[2]:(2.75343688821e-11) A[3]:(3.01697871583e-22)\n",
      " state (13)  A[0]:(1.53647636281e-10) A[1]:(1.0) A[2]:(3.18258579213e-11) A[3]:(2.15375359702e-22)\n",
      " state (14)  A[0]:(1.20260371106e-10) A[1]:(1.0) A[2]:(3.47360057384e-11) A[3]:(1.76998791275e-22)\n",
      " state (15)  A[0]:(1.05273775808e-10) A[1]:(1.0) A[2]:(3.65064853347e-11) A[3]:(1.58609850351e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 948000 finished after 3 . Running score: 0.08. Policy_loss: -92050.6113781, Value_loss: 1.64577530415. Times trained:               12765. Times reached goal: 119.               Steps done: 12695963.\n",
      " state (0)  A[0]:(0.981093645096) A[1]:(0.000622410443611) A[2]:(0.00121016148478) A[3]:(0.0170737691224)\n",
      " state (1)  A[0]:(2.05934175028e-05) A[1]:(6.22596326139e-06) A[2]:(2.26502294254e-07) A[3]:(0.999972939491)\n",
      " state (2)  A[0]:(0.999892711639) A[1]:(0.000106887011498) A[2]:(2.03133385113e-10) A[3]:(4.15918634644e-07)\n",
      " state (3)  A[0]:(0.999999046326) A[1]:(9.56776148087e-07) A[2]:(4.08710125951e-15) A[3]:(8.09552959066e-14)\n",
      " state (4)  A[0]:(0.999998152256) A[1]:(1.81955090284e-06) A[2]:(3.80292890549e-15) A[3]:(1.62788297517e-14)\n",
      " state (5)  A[0]:(0.999796628952) A[1]:(0.000203395946301) A[2]:(6.13092814906e-14) A[3]:(5.54517149293e-15)\n",
      " state (6)  A[0]:(0.409423708916) A[1]:(0.590576291084) A[2]:(9.69905319254e-12) A[3]:(1.44939174561e-15)\n",
      " state (7)  A[0]:(0.0924160927534) A[1]:(0.907583892345) A[2]:(1.10798323641e-11) A[3]:(2.00558171615e-16)\n",
      " state (8)  A[0]:(0.00306675047614) A[1]:(0.996933221817) A[2]:(1.66666350859e-11) A[3]:(3.22878371581e-18)\n",
      " state (9)  A[0]:(1.52685133799e-05) A[1]:(0.999984741211) A[2]:(1.88485008851e-11) A[3]:(4.9935542949e-20)\n",
      " state (10)  A[0]:(1.26132960077e-07) A[1]:(0.999999880791) A[2]:(1.75464972413e-11) A[3]:(6.65192112596e-21)\n",
      " state (11)  A[0]:(8.42723668626e-09) A[1]:(1.0) A[2]:(1.7571416544e-11) A[3]:(2.68006377103e-21)\n",
      " state (12)  A[0]:(1.98693128617e-09) A[1]:(1.0) A[2]:(1.95987427676e-11) A[3]:(1.47764598199e-21)\n",
      " state (13)  A[0]:(8.61215099235e-10) A[1]:(1.0) A[2]:(2.25977188351e-11) A[3]:(9.41860944794e-22)\n",
      " state (14)  A[0]:(5.21761855943e-10) A[1]:(1.0) A[2]:(2.55074208283e-11) A[3]:(6.84174292067e-22)\n",
      " state (15)  A[0]:(3.86438103739e-10) A[1]:(1.0) A[2]:(2.77894131318e-11) A[3]:(5.54623742436e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 949000 finished after 11 . Running score: 0.1. Policy_loss: -92050.6113929, Value_loss: 1.64519808358. Times trained:               12760. Times reached goal: 138.               Steps done: 12708723.\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0006,  0.0010,  0.0067]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0006,  0.0010,  0.0067]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9917,  0.0006,  0.0010,  0.0067]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.6073e-06,  3.2458e-15,  1.2522e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7742e-03,  9.9823e-01,  1.5168e-11,  1.5685e-18]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.3540e-06,  1.0000e+00,  1.5899e-11,  1.9706e-20]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.1218e-10,  1.0000e+00,  2.2602e-11,  4.4610e-22]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.1225e-10,  1.0000e+00,  2.2602e-11,  4.4617e-22]])\n",
      "On state=13, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.1231e-10,  1.0000e+00,  2.2601e-11,  4.4623e-22]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.991677880287) A[1]:(0.000610388058703) A[2]:(0.000979050528258) A[3]:(0.00673268688843)\n",
      " state (1)  A[0]:(4.70933009638e-05) A[1]:(1.44420891957e-05) A[2]:(4.27327478292e-07) A[3]:(0.999938011169)\n",
      " state (2)  A[0]:(0.999994575977) A[1]:(5.41643021279e-06) A[2]:(8.89547279153e-13) A[3]:(3.14111542332e-10)\n",
      " state (3)  A[0]:(0.99999910593) A[1]:(8.86772397735e-07) A[2]:(3.36469078748e-15) A[3]:(4.98249106361e-14)\n",
      " state (4)  A[0]:(0.999998390675) A[1]:(1.60719469022e-06) A[2]:(3.24644668211e-15) A[3]:(1.25261087242e-14)\n",
      " state (5)  A[0]:(0.999920487404) A[1]:(7.94981533545e-05) A[2]:(3.11660046999e-14) A[3]:(4.34019385712e-15)\n",
      " state (6)  A[0]:(0.656173050404) A[1]:(0.343826979399) A[2]:(7.12876762829e-12) A[3]:(1.61076634514e-15)\n",
      " state (7)  A[0]:(0.108601868153) A[1]:(0.891398131847) A[2]:(9.83717014097e-12) A[3]:(1.89807722093e-16)\n",
      " state (8)  A[0]:(0.00177795311902) A[1]:(0.998222053051) A[2]:(1.5169105555e-11) A[3]:(1.57196762545e-18)\n",
      " state (9)  A[0]:(3.36161883752e-06) A[1]:(0.99999666214) A[2]:(1.59019134666e-11) A[3]:(1.97291817252e-20)\n",
      " state (10)  A[0]:(3.51849536173e-08) A[1]:(0.999999940395) A[2]:(1.45208324531e-11) A[3]:(3.46475545171e-21)\n",
      " state (11)  A[0]:(3.34027761006e-09) A[1]:(1.0) A[2]:(1.54933565977e-11) A[3]:(1.45277411904e-21)\n",
      " state (12)  A[0]:(9.06703934156e-10) A[1]:(1.0) A[2]:(1.86188217621e-11) A[3]:(7.47190975044e-22)\n",
      " state (13)  A[0]:(4.12363615476e-10) A[1]:(1.0) A[2]:(2.26017711491e-11) A[3]:(4.46280449753e-22)\n",
      " state (14)  A[0]:(2.54803012023e-10) A[1]:(1.0) A[2]:(2.62565455489e-11) A[3]:(3.11722766454e-22)\n",
      " state (15)  A[0]:(1.90682900025e-10) A[1]:(1.0) A[2]:(2.90388563889e-11) A[3]:(2.47491111465e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 950000 finished after 9 . Running score: 0.16. Policy_loss: -92050.6110365, Value_loss: 1.42846606623. Times trained:               12795. Times reached goal: 124.               Steps done: 12721518.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.997277677059) A[1]:(0.000773512758315) A[2]:(0.000851412769407) A[3]:(0.00109739729669)\n",
      " state (1)  A[0]:(0.000202142357011) A[1]:(7.08709994797e-05) A[2]:(1.61204809501e-06) A[3]:(0.999725401402)\n",
      " state (2)  A[0]:(0.999997973442) A[1]:(2.05004130294e-06) A[2]:(1.26939881133e-13) A[3]:(1.01184555526e-11)\n",
      " state (3)  A[0]:(0.99999910593) A[1]:(8.90144690402e-07) A[2]:(3.25169172187e-15) A[3]:(2.29547521128e-14)\n",
      " state (4)  A[0]:(0.999998450279) A[1]:(1.54785448103e-06) A[2]:(3.30814202089e-15) A[3]:(7.83412556696e-15)\n",
      " state (5)  A[0]:(0.999881923199) A[1]:(0.000118090014439) A[2]:(4.28284354325e-14) A[3]:(2.77221284021e-15)\n",
      " state (6)  A[0]:(0.450748443604) A[1]:(0.549251556396) A[2]:(9.79006806179e-12) A[3]:(6.11405851383e-16)\n",
      " state (7)  A[0]:(0.0183924883604) A[1]:(0.981607496738) A[2]:(1.30597928305e-11) A[3]:(1.76818806027e-17)\n",
      " state (8)  A[0]:(1.55866803198e-06) A[1]:(0.999998450279) A[2]:(1.5409047302e-11) A[3]:(1.10501297041e-20)\n",
      " state (9)  A[0]:(6.33941033001e-09) A[1]:(1.0) A[2]:(1.38969001237e-11) A[3]:(1.41956461479e-21)\n",
      " state (10)  A[0]:(6.46737552401e-10) A[1]:(1.0) A[2]:(1.80687114576e-11) A[3]:(4.659811717e-22)\n",
      " state (11)  A[0]:(1.9581714028e-10) A[1]:(1.0) A[2]:(2.54073949379e-11) A[3]:(1.97503116359e-22)\n",
      " state (12)  A[0]:(1.02583178063e-10) A[1]:(1.0) A[2]:(3.2290066454e-11) A[3]:(1.14915942906e-22)\n",
      " state (13)  A[0]:(7.29822383083e-11) A[1]:(1.0) A[2]:(3.70842072328e-11) A[3]:(8.49424407487e-23)\n",
      " state (14)  A[0]:(6.11408215723e-11) A[1]:(1.0) A[2]:(3.99619920488e-11) A[3]:(7.23106004001e-23)\n",
      " state (15)  A[0]:(5.57711654636e-11) A[1]:(1.0) A[2]:(4.15705421475e-11) A[3]:(6.64499129215e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 951000 finished after 19 . Running score: 0.1. Policy_loss: -92050.6121726, Value_loss: 1.62340188198. Times trained:               12825. Times reached goal: 124.               Steps done: 12734343.\n",
      " state (0)  A[0]:(0.961485385895) A[1]:(0.000573867699131) A[2]:(0.00220335344784) A[3]:(0.0357373803854)\n",
      " state (1)  A[0]:(2.38296524913e-05) A[1]:(9.13347594178e-06) A[2]:(4.01275485729e-07) A[3]:(0.999966621399)\n",
      " state (2)  A[0]:(0.99999910593) A[1]:(9.12255529784e-07) A[2]:(1.72994163618e-14) A[3]:(1.58553660652e-12)\n",
      " state (3)  A[0]:(0.999998509884) A[1]:(1.48371020714e-06) A[2]:(6.09008515208e-15) A[3]:(5.05390881597e-14)\n",
      " state (4)  A[0]:(0.999992728233) A[1]:(7.2786160672e-06) A[2]:(1.15430490619e-14) A[3]:(1.11812084453e-14)\n",
      " state (5)  A[0]:(0.799611091614) A[1]:(0.200388938189) A[2]:(8.11093293224e-12) A[3]:(2.15506632891e-15)\n",
      " state (6)  A[0]:(0.021937282756) A[1]:(0.978062689304) A[2]:(1.46891561414e-11) A[3]:(4.88248915213e-17)\n",
      " state (7)  A[0]:(1.54541325514e-07) A[1]:(0.999999821186) A[2]:(1.59278267658e-11) A[3]:(7.14622172259e-21)\n",
      " state (8)  A[0]:(6.90950630045e-10) A[1]:(1.0) A[2]:(1.66164946386e-11) A[3]:(9.00885616111e-22)\n",
      " state (9)  A[0]:(9.50177228565e-11) A[1]:(1.0) A[2]:(2.75819922457e-11) A[3]:(2.31757867623e-22)\n",
      " state (10)  A[0]:(3.92155266593e-11) A[1]:(1.0) A[2]:(3.89690224534e-11) A[3]:(1.07153071971e-22)\n",
      " state (11)  A[0]:(2.68270614057e-11) A[1]:(1.0) A[2]:(4.58986910978e-11) A[3]:(7.53203130128e-23)\n",
      " state (12)  A[0]:(2.28199160957e-11) A[1]:(1.0) A[2]:(4.93210645935e-11) A[3]:(6.4621787727e-23)\n",
      " state (13)  A[0]:(2.12839745828e-11) A[1]:(1.0) A[2]:(5.0888820935e-11) A[3]:(6.04673432776e-23)\n",
      " state (14)  A[0]:(2.06469268765e-11) A[1]:(1.0) A[2]:(5.15919008903e-11) A[3]:(5.87360965132e-23)\n",
      " state (15)  A[0]:(2.03715395247e-11) A[1]:(1.0) A[2]:(5.19051815417e-11) A[3]:(5.79865082593e-23)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 952000 finished after 9 . Running score: 0.11. Policy_loss: -92050.6115342, Value_loss: 1.42058032465. Times trained:               13081. Times reached goal: 120.               Steps done: 12747424.\n",
      " state (0)  A[0]:(0.996729791164) A[1]:(0.000393021269701) A[2]:(0.00159568025265) A[3]:(0.0012815000955)\n",
      " state (1)  A[0]:(0.000140069721965) A[1]:(2.13759230974e-05) A[2]:(1.71414012584e-06) A[3]:(0.999836862087)\n",
      " state (2)  A[0]:(0.999622881413) A[1]:(0.000307576206978) A[2]:(3.55679574682e-08) A[3]:(6.9478257501e-05)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(1.49293157392e-07) A[2]:(5.33532924465e-15) A[3]:(4.40665773728e-14)\n",
      " state (4)  A[0]:(0.999999463558) A[1]:(5.10868972015e-07) A[2]:(4.302193418e-15) A[3]:(8.90280586069e-15)\n",
      " state (5)  A[0]:(0.999977111816) A[1]:(2.29178676818e-05) A[2]:(2.81582686313e-14) A[3]:(2.55751815193e-15)\n",
      " state (6)  A[0]:(0.758629381657) A[1]:(0.241370633245) A[2]:(1.25136741247e-11) A[3]:(5.80484331308e-16)\n",
      " state (7)  A[0]:(0.0266458652914) A[1]:(0.973354160786) A[2]:(2.7430288943e-11) A[3]:(1.81385587471e-17)\n",
      " state (8)  A[0]:(6.35996821075e-06) A[1]:(0.999993622303) A[2]:(3.10064543485e-11) A[3]:(1.9848696399e-20)\n",
      " state (9)  A[0]:(3.72003015059e-08) A[1]:(0.999999940395) A[2]:(2.59515377937e-11) A[3]:(2.81026878491e-21)\n",
      " state (10)  A[0]:(4.24238155716e-09) A[1]:(1.0) A[2]:(2.79492019806e-11) A[3]:(1.17284476948e-21)\n",
      " state (11)  A[0]:(1.45646739114e-09) A[1]:(1.0) A[2]:(3.30113228419e-11) A[3]:(6.44497390569e-22)\n",
      " state (12)  A[0]:(8.34562252638e-10) A[1]:(1.0) A[2]:(3.76788357459e-11) A[3]:(4.43385519558e-22)\n",
      " state (13)  A[0]:(6.25401674892e-10) A[1]:(1.0) A[2]:(4.08342006042e-11) A[3]:(3.59311198147e-22)\n",
      " state (14)  A[0]:(5.39204014771e-10) A[1]:(1.0) A[2]:(4.26836621603e-11) A[3]:(3.21188794394e-22)\n",
      " state (15)  A[0]:(4.99695229639e-10) A[1]:(1.0) A[2]:(4.36972090456e-11) A[3]:(3.02916832076e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 953000 finished after 10 . Running score: 0.14. Policy_loss: -92050.6113819, Value_loss: 2.06373430224. Times trained:               12905. Times reached goal: 128.               Steps done: 12760329.\n",
      " state (0)  A[0]:(0.997058928013) A[1]:(0.00041256606346) A[2]:(0.00162212317809) A[3]:(0.00090638815891)\n",
      " state (1)  A[0]:(0.000168256927282) A[1]:(2.86386002699e-05) A[2]:(2.10229700315e-06) A[3]:(0.999800980091)\n",
      " state (2)  A[0]:(0.99999243021) A[1]:(7.58465921535e-06) A[2]:(4.15060520675e-11) A[3]:(1.23694237075e-08)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.40401041904e-07) A[2]:(4.88854604651e-15) A[3]:(4.19539315127e-14)\n",
      " state (4)  A[0]:(0.999999642372) A[1]:(3.77134085738e-07) A[2]:(4.05059752761e-15) A[3]:(1.07434652063e-14)\n",
      " state (5)  A[0]:(0.999997138977) A[1]:(2.84248721982e-06) A[2]:(8.33771799432e-15) A[3]:(4.25462489472e-15)\n",
      " state (6)  A[0]:(0.968501091003) A[1]:(0.0314989127219) A[2]:(3.05645981614e-12) A[3]:(7.62936392817e-16)\n",
      " state (7)  A[0]:(0.12269885838) A[1]:(0.877301156521) A[2]:(1.89584025562e-11) A[3]:(1.04607351376e-16)\n",
      " state (8)  A[0]:(0.00211166427471) A[1]:(0.997888326645) A[2]:(2.96250246556e-11) A[3]:(5.0741264687e-19)\n",
      " state (9)  A[0]:(2.29833513004e-05) A[1]:(0.999976992607) A[2]:(2.84543881512e-11) A[3]:(2.46768268499e-20)\n",
      " state (10)  A[0]:(4.98235863233e-07) A[1]:(0.999999523163) A[2]:(2.46719415731e-11) A[3]:(5.48729202096e-21)\n",
      " state (11)  A[0]:(5.61505899555e-08) A[1]:(0.999999940395) A[2]:(2.29788982981e-11) A[3]:(2.6757731755e-21)\n",
      " state (12)  A[0]:(1.66471636476e-08) A[1]:(1.0) A[2]:(2.28982215134e-11) A[3]:(1.75445351235e-21)\n",
      " state (13)  A[0]:(8.1938917873e-09) A[1]:(1.0) A[2]:(2.35253552749e-11) A[3]:(1.32455334908e-21)\n",
      " state (14)  A[0]:(5.37631006381e-09) A[1]:(1.0) A[2]:(2.42787040472e-11) A[3]:(1.09822736308e-21)\n",
      " state (15)  A[0]:(4.18437595684e-09) A[1]:(1.0) A[2]:(2.49031437832e-11) A[3]:(9.73662363886e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 954000 finished after 8 . Running score: 0.18. Policy_loss: -92050.6113889, Value_loss: 1.8478373345. Times trained:               12865. Times reached goal: 119.               Steps done: 12773194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9966,  0.0003,  0.0011,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0003,  0.0011,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0003,  0.0011,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0003,  0.0011,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0003,  0.0011,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3139e-07,  2.9910e-15,  1.5874e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0003,  0.0011,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3140e-07,  2.9912e-15,  1.5876e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3140e-07,  2.9912e-15,  1.5877e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9966,  0.0003,  0.0011,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3140e-07,  2.9914e-15,  1.5878e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3141e-07,  2.9914e-15,  1.5879e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3141e-07,  2.9915e-15,  1.5880e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.1033e-04,  9.9949e-01,  2.5448e-11,  1.9783e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.0510e-06,  1.0000e+00,  2.1379e-11,  1.1601e-20]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.0989e-04,  9.9949e-01,  2.5448e-11,  1.9773e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 2.0481e-06,  1.0000e+00,  2.1378e-11,  1.1595e-20]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.7928e-08,  1.0000e+00,  1.8878e-11,  3.4668e-21]])\n",
      "On state=10, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7911e-09,  1.0000e+00,  2.4289e-11,  6.6063e-22]])\n",
      "On state=14, selected action=1\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.996586620808) A[1]:(0.000283232016955) A[2]:(0.00113156868611) A[3]:(0.00199856539257)\n",
      " state (1)  A[0]:(7.03412370058e-05) A[1]:(8.44630812935e-06) A[2]:(7.09426444701e-07) A[3]:(0.999920487404)\n",
      " state (2)  A[0]:(0.9999371171) A[1]:(5.71121090616e-05) A[2]:(2.27291963029e-09) A[3]:(5.78830349696e-06)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(8.46193444204e-08) A[2]:(3.70705494025e-15) A[3]:(7.57499870664e-14)\n",
      " state (4)  A[0]:(0.999999761581) A[1]:(2.31453896049e-07) A[2]:(2.9916610774e-15) A[3]:(1.58822370674e-14)\n",
      " state (5)  A[0]:(0.999998807907) A[1]:(1.18925618153e-06) A[2]:(4.41240383939e-15) A[3]:(5.88854552074e-15)\n",
      " state (6)  A[0]:(0.985154569149) A[1]:(0.0148454299197) A[2]:(1.52110897662e-12) A[3]:(9.18044377904e-16)\n",
      " state (7)  A[0]:(0.110473409295) A[1]:(0.889526605606) A[2]:(1.74349996246e-11) A[3]:(9.32229638672e-17)\n",
      " state (8)  A[0]:(0.000508625351358) A[1]:(0.999491393566) A[2]:(2.54439247449e-11) A[3]:(1.97416527654e-19)\n",
      " state (9)  A[0]:(2.04352181754e-06) A[1]:(0.999997973442) A[2]:(2.13728583442e-11) A[3]:(1.15863519596e-20)\n",
      " state (10)  A[0]:(6.78593323755e-08) A[1]:(0.999999940395) A[2]:(1.88755538977e-11) A[3]:(3.46566785454e-21)\n",
      " state (11)  A[0]:(1.1732820937e-08) A[1]:(1.0) A[2]:(1.92548928191e-11) A[3]:(1.77445751029e-21)\n",
      " state (12)  A[0]:(4.40520731004e-09) A[1]:(1.0) A[2]:(2.095495652e-11) A[3]:(1.11346840821e-21)\n",
      " state (13)  A[0]:(2.49321607804e-09) A[1]:(1.0) A[2]:(2.28106041e-11) A[3]:(8.09606003275e-22)\n",
      " state (14)  A[0]:(1.79072034978e-09) A[1]:(1.0) A[2]:(2.42868555128e-11) A[3]:(6.60551782841e-22)\n",
      " state (15)  A[0]:(1.47986634058e-09) A[1]:(1.0) A[2]:(2.52929951289e-11) A[3]:(5.8388969947e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 955000 finished after 19 . Running score: 0.12. Policy_loss: -92050.611711, Value_loss: 1.8432081796. Times trained:               12576. Times reached goal: 129.               Steps done: 12785770.\n",
      " state (0)  A[0]:(0.996478617191) A[1]:(0.000309190363623) A[2]:(0.00165219581686) A[3]:(0.0015600246843)\n",
      " state (1)  A[0]:(0.000105633065687) A[1]:(1.37152128445e-05) A[2]:(1.39801136356e-06) A[3]:(0.99987924099)\n",
      " state (2)  A[0]:(0.999941885471) A[1]:(5.62642671866e-05) A[2]:(1.94170945989e-09) A[3]:(1.8208511392e-06)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(1.52781069573e-07) A[2]:(5.18172278428e-15) A[3]:(3.37937177428e-14)\n",
      " state (4)  A[0]:(0.999999463558) A[1]:(5.38836502528e-07) A[2]:(4.56650750809e-15) A[3]:(7.95853522515e-15)\n",
      " state (5)  A[0]:(0.999940752983) A[1]:(5.92606847931e-05) A[2]:(5.38192842343e-14) A[3]:(1.95496051259e-15)\n",
      " state (6)  A[0]:(0.46777048707) A[1]:(0.532229483128) A[2]:(1.68277562024e-11) A[3]:(4.45827268518e-16)\n",
      " state (7)  A[0]:(0.0158876515925) A[1]:(0.98411232233) A[2]:(2.83904965509e-11) A[3]:(6.21351110062e-18)\n",
      " state (8)  A[0]:(5.58337851544e-05) A[1]:(0.999944150448) A[2]:(3.02844971323e-11) A[3]:(4.46766969682e-20)\n",
      " state (9)  A[0]:(3.59200043931e-07) A[1]:(0.999999642372) A[2]:(2.43413848106e-11) A[3]:(5.44481823521e-21)\n",
      " state (10)  A[0]:(2.48615119602e-08) A[1]:(1.0) A[2]:(2.27703515765e-11) A[3]:(2.12918248571e-21)\n",
      " state (11)  A[0]:(6.31575503007e-09) A[1]:(1.0) A[2]:(2.41871869755e-11) A[3]:(1.19295155915e-21)\n",
      " state (12)  A[0]:(2.96614821593e-09) A[1]:(1.0) A[2]:(2.6481261628e-11) A[3]:(8.04572187176e-22)\n",
      " state (13)  A[0]:(1.94006744003e-09) A[1]:(1.0) A[2]:(2.84979505272e-11) A[3]:(6.2572240005e-22)\n",
      " state (14)  A[0]:(1.53059787067e-09) A[1]:(1.0) A[2]:(2.99100605394e-11) A[3]:(5.38358061659e-22)\n",
      " state (15)  A[0]:(1.34225208726e-09) A[1]:(1.0) A[2]:(3.07917753795e-11) A[3]:(4.93806580368e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 956000 finished after 6 . Running score: 0.11. Policy_loss: -92050.6134438, Value_loss: 1.20894321321. Times trained:               12836. Times reached goal: 120.               Steps done: 12798606.\n",
      " state (0)  A[0]:(0.995958507061) A[1]:(0.00036856069346) A[2]:(0.0014821216464) A[3]:(0.00219078478403)\n",
      " state (1)  A[0]:(0.000122293931781) A[1]:(1.76996218215e-05) A[2]:(1.45751403124e-06) A[3]:(0.999858558178)\n",
      " state (2)  A[0]:(0.999999523163) A[1]:(5.04208912844e-07) A[2]:(2.58691668347e-13) A[3]:(2.64986296794e-11)\n",
      " state (3)  A[0]:(0.999999761581) A[1]:(2.09572306176e-07) A[2]:(4.37771953426e-15) A[3]:(2.69234860236e-14)\n",
      " state (4)  A[0]:(0.99999922514) A[1]:(7.64877427173e-07) A[2]:(4.45405795512e-15) A[3]:(7.20685023567e-15)\n",
      " state (5)  A[0]:(0.999524712563) A[1]:(0.000475294684293) A[2]:(1.72533320096e-13) A[3]:(1.27227673282e-15)\n",
      " state (6)  A[0]:(0.313098073006) A[1]:(0.686901926994) A[2]:(1.49633951063e-11) A[3]:(3.40951645926e-16)\n",
      " state (7)  A[0]:(0.0100065395236) A[1]:(0.989993453026) A[2]:(2.48948466008e-11) A[3]:(3.34464380229e-18)\n",
      " state (8)  A[0]:(6.124767242e-05) A[1]:(0.999938726425) A[2]:(2.5188229974e-11) A[3]:(4.6654711995e-20)\n",
      " state (9)  A[0]:(5.14063287937e-07) A[1]:(0.999999463558) A[2]:(2.01474080475e-11) A[3]:(6.39567244676e-21)\n",
      " state (10)  A[0]:(3.71849786518e-08) A[1]:(0.999999940395) A[2]:(1.84533603675e-11) A[3]:(2.58363301008e-21)\n",
      " state (11)  A[0]:(9.57754764386e-09) A[1]:(1.0) A[2]:(1.90275174089e-11) A[3]:(1.50994308341e-21)\n",
      " state (12)  A[0]:(4.54500881375e-09) A[1]:(1.0) A[2]:(2.03248112784e-11) A[3]:(1.05640475992e-21)\n",
      " state (13)  A[0]:(2.99278957172e-09) A[1]:(1.0) A[2]:(2.15265669873e-11) A[3]:(8.41577664861e-22)\n",
      " state (14)  A[0]:(2.36985875368e-09) A[1]:(1.0) A[2]:(2.23855396025e-11) A[3]:(7.34249920153e-22)\n",
      " state (15)  A[0]:(2.08213513098e-09) A[1]:(1.0) A[2]:(2.29277655889e-11) A[3]:(6.78697249735e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 957000 finished after 10 . Running score: 0.09. Policy_loss: -92050.6113792, Value_loss: 1.21849062927. Times trained:               12958. Times reached goal: 147.               Steps done: 12811564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995438814163) A[1]:(0.00071753654629) A[2]:(0.00248814607039) A[3]:(0.00135552964639)\n",
      " state (1)  A[0]:(0.000201643852051) A[1]:(4.36884620285e-05) A[2]:(3.61589377462e-06) A[3]:(0.999751031399)\n",
      " state (2)  A[0]:(0.999999403954) A[1]:(5.92921367115e-07) A[2]:(1.72307372363e-13) A[3]:(5.02665443991e-12)\n",
      " state (3)  A[0]:(0.999999582767) A[1]:(4.17975201117e-07) A[2]:(7.34832252556e-15) A[3]:(1.8455648522e-14)\n",
      " state (4)  A[0]:(0.999997615814) A[1]:(2.36590926761e-06) A[2]:(1.0886475708e-14) A[3]:(5.1582819945e-15)\n",
      " state (5)  A[0]:(0.907509028912) A[1]:(0.0924909934402) A[2]:(7.55464701213e-12) A[3]:(8.90822327166e-16)\n",
      " state (6)  A[0]:(0.161828741431) A[1]:(0.838171243668) A[2]:(2.23266093113e-11) A[3]:(1.67196455498e-16)\n",
      " state (7)  A[0]:(0.0287635046989) A[1]:(0.971236467361) A[2]:(3.10748857202e-11) A[3]:(1.35270822532e-17)\n",
      " state (8)  A[0]:(0.00269801681861) A[1]:(0.997301995754) A[2]:(3.69703295755e-11) A[3]:(6.16637865951e-19)\n",
      " state (9)  A[0]:(0.000124534082715) A[1]:(0.999875485897) A[2]:(3.59272542272e-11) A[3]:(5.82349562275e-20)\n",
      " state (10)  A[0]:(4.00874387196e-06) A[1]:(0.999996006489) A[2]:(3.12727309326e-11) A[3]:(1.178033066e-20)\n",
      " state (11)  A[0]:(3.23659662627e-07) A[1]:(0.999999701977) A[2]:(2.81274916553e-11) A[3]:(4.61232839944e-21)\n",
      " state (12)  A[0]:(6.60569057231e-08) A[1]:(0.999999940395) A[2]:(2.69552869608e-11) A[3]:(2.63654106534e-21)\n",
      " state (13)  A[0]:(2.38746320491e-08) A[1]:(1.0) A[2]:(2.70586209689e-11) A[3]:(1.80061426989e-21)\n",
      " state (14)  A[0]:(1.22821086634e-08) A[1]:(1.0) A[2]:(2.77976842933e-11) A[3]:(1.36529128743e-21)\n",
      " state (15)  A[0]:(7.95844190549e-09) A[1]:(1.0) A[2]:(2.87071130756e-11) A[3]:(1.1190508676e-21)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 958000 finished after 12 . Running score: 0.14. Policy_loss: -92050.6091465, Value_loss: 1.64965765165. Times trained:               12540. Times reached goal: 116.               Steps done: 12824104.\n",
      " state (0)  A[0]:(0.992954432964) A[1]:(0.00062666577287) A[2]:(0.00339854415506) A[3]:(0.00302037619986)\n",
      " state (1)  A[0]:(0.000127567283926) A[1]:(3.06306319544e-05) A[2]:(3.08679113914e-06) A[3]:(0.999838709831)\n",
      " state (2)  A[0]:(0.99999922514) A[1]:(8.0168246086e-07) A[2]:(1.67320367631e-13) A[3]:(4.26997283018e-12)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(7.58930866596e-07) A[2]:(1.02323384208e-14) A[3]:(1.91570392362e-14)\n",
      " state (4)  A[0]:(0.999977350235) A[1]:(2.2629968953e-05) A[2]:(4.67022423609e-14) A[3]:(4.32182044194e-15)\n",
      " state (5)  A[0]:(0.170254558325) A[1]:(0.829745411873) A[2]:(2.13663305798e-11) A[3]:(2.59081888167e-16)\n",
      " state (6)  A[0]:(0.0251962635666) A[1]:(0.974803745747) A[2]:(2.83287907021e-11) A[3]:(1.74913991056e-17)\n",
      " state (7)  A[0]:(0.00182302168105) A[1]:(0.99817699194) A[2]:(3.34185040751e-11) A[3]:(5.99554208157e-19)\n",
      " state (8)  A[0]:(2.88168594125e-05) A[1]:(0.999971210957) A[2]:(3.04372707594e-11) A[3]:(3.13964834859e-20)\n",
      " state (9)  A[0]:(3.4087088352e-07) A[1]:(0.999999642372) A[2]:(2.48028299288e-11) A[3]:(4.75228105829e-21)\n",
      " state (10)  A[0]:(2.5469054421e-08) A[1]:(1.0) A[2]:(2.34745261424e-11) A[3]:(1.78346279298e-21)\n",
      " state (11)  A[0]:(6.28918872536e-09) A[1]:(1.0) A[2]:(2.49515443029e-11) A[3]:(9.67630670296e-22)\n",
      " state (12)  A[0]:(2.84508372417e-09) A[1]:(1.0) A[2]:(2.72662673689e-11) A[3]:(6.40175038653e-22)\n",
      " state (13)  A[0]:(1.80576364972e-09) A[1]:(1.0) A[2]:(2.9324612183e-11) A[3]:(4.91084568483e-22)\n",
      " state (14)  A[0]:(1.3951677591e-09) A[1]:(1.0) A[2]:(3.07909531205e-11) A[3]:(4.18345458839e-22)\n",
      " state (15)  A[0]:(1.20708554263e-09) A[1]:(1.0) A[2]:(3.1722343502e-11) A[3]:(3.81136063765e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 959000 finished after 9 . Running score: 0.13. Policy_loss: -92050.6115936, Value_loss: 2.28036085925. Times trained:               12726. Times reached goal: 122.               Steps done: 12836830.\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0016,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0016,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0016,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0016,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0016,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.4000e-06,  8.1432e-15,  5.5137e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0016,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9942,  0.0022,  0.0016,  0.0020]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.3997e-06,  8.1439e-15,  5.5156e-15]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7134e-05,  9.9998e-01,  1.5842e-11,  2.3128e-20]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994224667549) A[1]:(0.00224355282262) A[2]:(0.00157197448425) A[3]:(0.00195983471349)\n",
      " state (1)  A[0]:(0.000163733449881) A[1]:(5.61855595151e-05) A[2]:(2.07980292544e-06) A[3]:(0.999777972698)\n",
      " state (2)  A[0]:(0.999998807907) A[1]:(1.20932122627e-06) A[2]:(1.45241254462e-13) A[3]:(8.28558749388e-12)\n",
      " state (3)  A[0]:(0.99999922514) A[1]:(7.5995063753e-07) A[2]:(5.01611090243e-15) A[3]:(1.78743162578e-14)\n",
      " state (4)  A[0]:(0.999996602535) A[1]:(3.3990263546e-06) A[2]:(8.14376269599e-15) A[3]:(5.51703390546e-15)\n",
      " state (5)  A[0]:(0.218808874488) A[1]:(0.781191110611) A[2]:(1.23110480144e-11) A[3]:(2.90952588069e-16)\n",
      " state (6)  A[0]:(0.0265185311437) A[1]:(0.973481476307) A[2]:(1.57796917893e-11) A[3]:(1.55169983928e-17)\n",
      " state (7)  A[0]:(0.00190393871162) A[1]:(0.998096048832) A[2]:(1.79779871545e-11) A[3]:(6.16604158341e-19)\n",
      " state (8)  A[0]:(1.71345636772e-05) A[1]:(0.999982893467) A[2]:(1.58423604096e-11) A[3]:(2.3130724503e-20)\n",
      " state (9)  A[0]:(1.60680997396e-07) A[1]:(0.999999821186) A[2]:(1.29122372516e-11) A[3]:(3.17639536263e-21)\n",
      " state (10)  A[0]:(1.25571695264e-08) A[1]:(1.0) A[2]:(1.31300951015e-11) A[3]:(1.10845493841e-21)\n",
      " state (11)  A[0]:(3.26187965527e-09) A[1]:(1.0) A[2]:(1.50520620273e-11) A[3]:(5.51709627143e-22)\n",
      " state (12)  A[0]:(1.55781210154e-09) A[1]:(1.0) A[2]:(1.71502378271e-11) A[3]:(3.50645163517e-22)\n",
      " state (13)  A[0]:(1.04135500223e-09) A[1]:(1.0) A[2]:(1.87348105779e-11) A[3]:(2.67778342028e-22)\n",
      " state (14)  A[0]:(8.38148328519e-10) A[1]:(1.0) A[2]:(1.97441698407e-11) A[3]:(2.3008068623e-22)\n",
      " state (15)  A[0]:(7.46098960391e-10) A[1]:(1.0) A[2]:(2.03342724603e-11) A[3]:(2.11736401144e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 960000 finished after 10 . Running score: 0.11. Policy_loss: -92050.61138, Value_loss: 1.644790782. Times trained:               12320. Times reached goal: 120.               Steps done: 12849150.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.99508357048) A[1]:(0.00154311349615) A[2]:(0.00135489087552) A[3]:(0.00201841187663)\n",
      " state (1)  A[0]:(7.42942138459e-05) A[1]:(1.69932009157e-05) A[2]:(8.91976981165e-07) A[3]:(0.999907791615)\n",
      " state (2)  A[0]:(0.999990582466) A[1]:(9.40696372709e-06) A[2]:(1.63599724046e-11) A[3]:(5.97921534506e-09)\n",
      " state (3)  A[0]:(0.999999582767) A[1]:(4.18731644913e-07) A[2]:(4.52570296641e-15) A[3]:(2.44622640829e-14)\n",
      " state (4)  A[0]:(0.999998807907) A[1]:(1.19024548439e-06) A[2]:(4.83693505848e-15) A[3]:(5.92394091002e-15)\n",
      " state (5)  A[0]:(0.544347286224) A[1]:(0.455652743578) A[2]:(1.18267843435e-11) A[3]:(7.40540312296e-16)\n",
      " state (6)  A[0]:(0.0684109628201) A[1]:(0.931589007378) A[2]:(1.65321211582e-11) A[3]:(4.39849882741e-17)\n",
      " state (7)  A[0]:(0.00948427896947) A[1]:(0.990515708923) A[2]:(1.96883551129e-11) A[3]:(3.24572747669e-18)\n",
      " state (8)  A[0]:(0.000250970362686) A[1]:(0.999749004841) A[2]:(1.9699646428e-11) A[3]:(1.07037701177e-19)\n",
      " state (9)  A[0]:(1.73792432179e-06) A[1]:(0.999998271465) A[2]:(1.5809881182e-11) A[3]:(8.1271795731e-21)\n",
      " state (10)  A[0]:(5.68300322357e-08) A[1]:(0.999999940395) A[2]:(1.44925547257e-11) A[3]:(2.00744335035e-21)\n",
      " state (11)  A[0]:(8.8883815863e-09) A[1]:(1.0) A[2]:(1.60469987992e-11) A[3]:(8.38184780419e-22)\n",
      " state (12)  A[0]:(3.10428993622e-09) A[1]:(1.0) A[2]:(1.86817141618e-11) A[3]:(4.56080096437e-22)\n",
      " state (13)  A[0]:(1.70018665813e-09) A[1]:(1.0) A[2]:(2.11143134227e-11) A[3]:(3.07609607829e-22)\n",
      " state (14)  A[0]:(1.20980525598e-09) A[1]:(1.0) A[2]:(2.28770925814e-11) A[3]:(2.42694004124e-22)\n",
      " state (15)  A[0]:(1.00008312742e-09) A[1]:(1.0) A[2]:(2.40050045813e-11) A[3]:(2.11637875572e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 961000 finished after 9 . Running score: 0.18. Policy_loss: -92050.611378, Value_loss: 2.07536748423. Times trained:               13257. Times reached goal: 154.               Steps done: 12862407.\n",
      " state (0)  A[0]:(0.995675325394) A[1]:(0.00217306800187) A[2]:(0.00178746250458) A[3]:(0.000364146602806)\n",
      " state (1)  A[0]:(0.000519256282132) A[1]:(0.00017850389122) A[2]:(6.46327816867e-06) A[3]:(0.999295771122)\n",
      " state (2)  A[0]:(0.999998569489) A[1]:(1.45730678014e-06) A[2]:(1.76408177345e-13) A[3]:(3.98097613902e-12)\n",
      " state (3)  A[0]:(0.999999046326) A[1]:(9.4166244935e-07) A[2]:(7.02477245685e-15) A[3]:(1.24847574228e-14)\n",
      " state (4)  A[0]:(0.999993443489) A[1]:(6.55092480883e-06) A[2]:(1.55898091067e-14) A[3]:(4.08873137442e-15)\n",
      " state (5)  A[0]:(0.150666207075) A[1]:(0.849333763123) A[2]:(1.43748424622e-11) A[3]:(1.5616030441e-16)\n",
      " state (6)  A[0]:(0.0190752968192) A[1]:(0.980924725533) A[2]:(1.79172440773e-11) A[3]:(8.41959021377e-18)\n",
      " state (7)  A[0]:(0.00253589288332) A[1]:(0.997464120388) A[2]:(1.99924903022e-11) A[3]:(5.70718226715e-19)\n",
      " state (8)  A[0]:(8.53731980897e-05) A[1]:(0.999914646149) A[2]:(1.86010477854e-11) A[3]:(3.62321231858e-20)\n",
      " state (9)  A[0]:(1.12764575988e-06) A[1]:(0.999998867512) A[2]:(1.47508359694e-11) A[3]:(4.82608350195e-21)\n",
      " state (10)  A[0]:(5.74418308474e-08) A[1]:(0.999999940395) A[2]:(1.32289361757e-11) A[3]:(1.50176306283e-21)\n",
      " state (11)  A[0]:(1.03747774816e-08) A[1]:(1.0) A[2]:(1.37132692354e-11) A[3]:(7.20691307081e-22)\n",
      " state (12)  A[0]:(3.68960106911e-09) A[1]:(1.0) A[2]:(1.51063068304e-11) A[3]:(4.24575339532e-22)\n",
      " state (13)  A[0]:(1.9507018223e-09) A[1]:(1.0) A[2]:(1.66414867997e-11) A[3]:(2.92465800855e-22)\n",
      " state (14)  A[0]:(1.32355626459e-09) A[1]:(1.0) A[2]:(1.79137434053e-11) A[3]:(2.28709431406e-22)\n",
      " state (15)  A[0]:(1.04989084093e-09) A[1]:(1.0) A[2]:(1.88190452804e-11) A[3]:(1.96111969619e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 962000 finished after 8 . Running score: 0.12. Policy_loss: -92050.6114028, Value_loss: 1.42132492281. Times trained:               13050. Times reached goal: 139.               Steps done: 12875457.\n",
      " state (0)  A[0]:(0.998356938362) A[1]:(0.000830521690659) A[2]:(0.000696361123119) A[3]:(0.000116205934319)\n",
      " state (1)  A[0]:(0.000272835604846) A[1]:(3.63831823051e-05) A[2]:(2.42848136622e-06) A[3]:(0.999688327312)\n",
      " state (2)  A[0]:(0.285170078278) A[1]:(0.0077773691155) A[2]:(4.2580999434e-05) A[3]:(0.707009971142)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(9.25752488001e-08) A[2]:(6.5632682496e-15) A[3]:(7.38576205945e-14)\n",
      " state (4)  A[0]:(0.999999821186) A[1]:(1.99998197559e-07) A[2]:(3.1388000177e-15) A[3]:(6.55906273102e-15)\n",
      " state (5)  A[0]:(0.999998927116) A[1]:(1.08092342543e-06) A[2]:(5.34432769916e-15) A[3]:(2.73953600317e-15)\n",
      " state (6)  A[0]:(0.603139519691) A[1]:(0.396860450506) A[2]:(1.30312071897e-11) A[3]:(4.13443240181e-16)\n",
      " state (7)  A[0]:(0.181178301573) A[1]:(0.818821668625) A[2]:(2.04923734232e-11) A[3]:(6.92883737289e-17)\n",
      " state (8)  A[0]:(0.0553202219307) A[1]:(0.944679796696) A[2]:(2.54500361757e-11) A[3]:(1.29930635488e-17)\n",
      " state (9)  A[0]:(0.00949998944998) A[1]:(0.990500032902) A[2]:(2.80747300407e-11) A[3]:(1.4978163636e-18)\n",
      " state (10)  A[0]:(0.0004135898198) A[1]:(0.99958640337) A[2]:(2.67355807632e-11) A[3]:(9.81376836879e-20)\n",
      " state (11)  A[0]:(8.9563882284e-06) A[1]:(0.999991059303) A[2]:(2.18132976959e-11) A[3]:(1.27328961822e-20)\n",
      " state (12)  A[0]:(4.69558642635e-07) A[1]:(0.999999523163) A[2]:(1.8610717134e-11) A[3]:(3.80559344596e-21)\n",
      " state (13)  A[0]:(7.57698686016e-08) A[1]:(0.999999940395) A[2]:(1.78285268493e-11) A[3]:(1.84539006535e-21)\n",
      " state (14)  A[0]:(2.38432313893e-08) A[1]:(1.0) A[2]:(1.85149898885e-11) A[3]:(1.09715532005e-21)\n",
      " state (15)  A[0]:(1.10331548342e-08) A[1]:(1.0) A[2]:(1.98953822167e-11) A[3]:(7.33709556744e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 963000 finished after 16 . Running score: 0.13. Policy_loss: -92050.611138, Value_loss: 2.07342336194. Times trained:               12368. Times reached goal: 128.               Steps done: 12887825.\n",
      " state (0)  A[0]:(0.992963910103) A[1]:(0.000638812372927) A[2]:(0.00156966969371) A[3]:(0.00482758833095)\n",
      " state (1)  A[0]:(4.30230466009e-05) A[1]:(4.80192738905e-06) A[2]:(6.15148678662e-07) A[3]:(0.999951541424)\n",
      " state (2)  A[0]:(0.999728381634) A[1]:(0.000149628525833) A[2]:(2.06696384453e-08) A[3]:(0.000121942633996)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(8.63034586018e-08) A[2]:(6.60773366768e-15) A[3]:(1.7845179002e-13)\n",
      " state (4)  A[0]:(0.999999821186) A[1]:(1.97624430598e-07) A[2]:(5.61555273107e-15) A[3]:(2.9381329997e-14)\n",
      " state (5)  A[0]:(0.999999523163) A[1]:(4.86033741254e-07) A[2]:(6.43781885846e-15) A[3]:(9.21890076891e-15)\n",
      " state (6)  A[0]:(0.914332568645) A[1]:(0.0856674388051) A[2]:(1.06709519693e-11) A[3]:(1.76236163165e-15)\n",
      " state (7)  A[0]:(0.216790348291) A[1]:(0.783209621906) A[2]:(3.93359581019e-11) A[3]:(2.15299483043e-16)\n",
      " state (8)  A[0]:(0.0331475399435) A[1]:(0.966852486134) A[2]:(5.35222560716e-11) A[3]:(1.73753506288e-17)\n",
      " state (9)  A[0]:(0.00141307956073) A[1]:(0.998586893082) A[2]:(5.50065791527e-11) A[3]:(7.20160088298e-19)\n",
      " state (10)  A[0]:(1.5674115275e-05) A[1]:(0.999984323978) A[2]:(4.48395348629e-11) A[3]:(4.73807342348e-20)\n",
      " state (11)  A[0]:(4.16155131688e-07) A[1]:(0.999999582767) A[2]:(3.78414417856e-11) A[3]:(9.89704962372e-21)\n",
      " state (12)  A[0]:(4.84510174203e-08) A[1]:(0.999999940395) A[2]:(3.83046094843e-11) A[3]:(3.87017169488e-21)\n",
      " state (13)  A[0]:(1.29012303063e-08) A[1]:(1.0) A[2]:(4.33265680277e-11) A[3]:(1.91529672258e-21)\n",
      " state (14)  A[0]:(5.49260548155e-09) A[1]:(1.0) A[2]:(5.00372139556e-11) A[3]:(1.12207736717e-21)\n",
      " state (15)  A[0]:(3.16983062021e-09) A[1]:(1.0) A[2]:(5.63917836038e-11) A[3]:(7.68012258129e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 964000 finished after 9 . Running score: 0.12. Policy_loss: -92050.611501, Value_loss: 2.48081345741. Times trained:               12431. Times reached goal: 127.               Steps done: 12900256.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9893,  0.0007,  0.0015,  0.0085]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8742e-07,  5.0160e-15,  2.1613e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9893,  0.0007,  0.0015,  0.0085]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9893,  0.0007,  0.0015,  0.0085]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8742e-07,  5.0166e-15,  2.1636e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9892,  0.0007,  0.0015,  0.0085]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9892,  0.0007,  0.0015,  0.0085]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.8742e-07,  5.0173e-15,  2.1665e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3499e-05,  9.9999e-01,  2.1808e-11,  5.3672e-20]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.0931e-08,  1.0000e+00,  1.7114e-11,  5.1088e-21]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.3499e-05,  9.9999e-01,  2.1808e-11,  5.3695e-20]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.989145517349) A[1]:(0.000743198150303) A[2]:(0.00149690965191) A[3]:(0.00861438270658)\n",
      " state (1)  A[0]:(2.89641848212e-05) A[1]:(4.01972783948e-06) A[2]:(3.97732236479e-07) A[3]:(0.999966621399)\n",
      " state (2)  A[0]:(0.999931514263) A[1]:(6.16626493866e-05) A[2]:(1.56700274978e-09) A[3]:(6.84668248141e-06)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(2.03695563528e-07) A[2]:(5.84375230091e-15) A[3]:(1.61399268516e-13)\n",
      " state (4)  A[0]:(0.999999523163) A[1]:(4.87422596507e-07) A[2]:(5.01815987513e-15) A[3]:(2.16998287557e-14)\n",
      " state (5)  A[0]:(0.999983787537) A[1]:(1.61904226843e-05) A[2]:(3.23741040757e-14) A[3]:(5.36491822308e-15)\n",
      " state (6)  A[0]:(0.216922521591) A[1]:(0.783077478409) A[2]:(2.01965579677e-11) A[3]:(3.49869394083e-16)\n",
      " state (7)  A[0]:(0.00666885171086) A[1]:(0.993331134319) A[2]:(2.74582145215e-11) A[3]:(5.4053622596e-18)\n",
      " state (8)  A[0]:(1.34994943437e-05) A[1]:(0.99998652935) A[2]:(2.18082062825e-11) A[3]:(5.37063200883e-20)\n",
      " state (9)  A[0]:(8.09339937291e-08) A[1]:(0.999999940395) A[2]:(1.71146170153e-11) A[3]:(5.11085860318e-21)\n",
      " state (10)  A[0]:(6.00198735157e-09) A[1]:(1.0) A[2]:(1.97191811491e-11) A[3]:(1.37042047366e-21)\n",
      " state (11)  A[0]:(1.47332102074e-09) A[1]:(1.0) A[2]:(2.5622400035e-11) A[3]:(5.39892263591e-22)\n",
      " state (12)  A[0]:(6.99527491488e-10) A[1]:(1.0) A[2]:(3.10213243981e-11) A[3]:(3.07977103415e-22)\n",
      " state (13)  A[0]:(4.77628103734e-10) A[1]:(1.0) A[2]:(3.46061269918e-11) A[3]:(2.27593691785e-22)\n",
      " state (14)  A[0]:(3.93642257679e-10) A[1]:(1.0) A[2]:(3.66669888896e-11) A[3]:(1.94619217595e-22)\n",
      " state (15)  A[0]:(3.56857349226e-10) A[1]:(1.0) A[2]:(3.77805911556e-11) A[3]:(1.79634609063e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 965000 finished after 11 . Running score: 0.13. Policy_loss: -92050.6104406, Value_loss: 2.06075339609. Times trained:               12599. Times reached goal: 136.               Steps done: 12912855.\n",
      " state (0)  A[0]:(0.997834026814) A[1]:(0.000553463411052) A[2]:(0.00107221410144) A[3]:(0.000540324894246)\n",
      " state (1)  A[0]:(0.000492236111313) A[1]:(5.1332059229e-05) A[2]:(3.44003069586e-06) A[3]:(0.999453008175)\n",
      " state (2)  A[0]:(0.999999642372) A[1]:(3.53822798616e-07) A[2]:(1.3780057824e-13) A[3]:(1.15643458351e-11)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(1.72634301521e-07) A[2]:(5.40077439828e-15) A[3]:(4.70836003107e-14)\n",
      " state (4)  A[0]:(0.999999642372) A[1]:(3.37082155966e-07) A[2]:(4.86224694405e-15) A[3]:(1.36249645667e-14)\n",
      " state (5)  A[0]:(0.999999344349) A[1]:(6.67698088819e-07) A[2]:(5.53982544449e-15) A[3]:(5.91017535408e-15)\n",
      " state (6)  A[0]:(0.955879807472) A[1]:(0.0441202148795) A[2]:(4.55601181781e-12) A[3]:(1.19062402725e-15)\n",
      " state (7)  A[0]:(0.233250319958) A[1]:(0.766749680042) A[2]:(2.11423534929e-11) A[3]:(1.87595708786e-16)\n",
      " state (8)  A[0]:(0.0485573001206) A[1]:(0.951442718506) A[2]:(2.85588479948e-11) A[3]:(2.2726160327e-17)\n",
      " state (9)  A[0]:(0.00488752499223) A[1]:(0.995112478733) A[2]:(2.99968591633e-11) A[3]:(1.98167369836e-18)\n",
      " state (10)  A[0]:(7.50566468923e-05) A[1]:(0.999924957752) A[2]:(2.5591466446e-11) A[3]:(8.18695096561e-20)\n",
      " state (11)  A[0]:(1.12342524972e-06) A[1]:(0.999998867512) A[2]:(1.98870885038e-11) A[3]:(9.5486224089e-21)\n",
      " state (12)  A[0]:(7.50653086357e-08) A[1]:(0.999999940395) A[2]:(1.79151780216e-11) A[3]:(2.92037831823e-21)\n",
      " state (13)  A[0]:(1.46825538394e-08) A[1]:(1.0) A[2]:(1.87777397798e-11) A[3]:(1.3250283317e-21)\n",
      " state (14)  A[0]:(5.09405362337e-09) A[1]:(1.0) A[2]:(2.11118171556e-11) A[3]:(7.19845042345e-22)\n",
      " state (15)  A[0]:(2.50614617947e-09) A[1]:(1.0) A[2]:(2.39088644716e-11) A[3]:(4.51970143742e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 966000 finished after 5 . Running score: 0.09. Policy_loss: -92050.6121775, Value_loss: 2.05686247293. Times trained:               12773. Times reached goal: 160.               Steps done: 12925628.\n",
      " state (0)  A[0]:(0.997563898563) A[1]:(0.000666253443342) A[2]:(0.000846248818561) A[3]:(0.00092357682297)\n",
      " state (1)  A[0]:(0.000504955882207) A[1]:(5.50603326701e-05) A[2]:(2.26869610742e-06) A[3]:(0.999437689781)\n",
      " state (2)  A[0]:(0.999999642372) A[1]:(3.76714240247e-07) A[2]:(5.44323870103e-14) A[3]:(5.28395079427e-12)\n",
      " state (3)  A[0]:(0.999999701977) A[1]:(3.21638680134e-07) A[2]:(5.43703122008e-15) A[3]:(6.22724392668e-14)\n",
      " state (4)  A[0]:(0.999999463558) A[1]:(5.64441677398e-07) A[2]:(4.9544964553e-15) A[3]:(1.55141436536e-14)\n",
      " state (5)  A[0]:(0.999997615814) A[1]:(2.40962958742e-06) A[2]:(9.53993726735e-15) A[3]:(5.92235229973e-15)\n",
      " state (6)  A[0]:(0.411685436964) A[1]:(0.588314592838) A[2]:(1.5068224446e-11) A[3]:(5.45517328938e-16)\n",
      " state (7)  A[0]:(0.0241580102593) A[1]:(0.975841999054) A[2]:(2.21053696181e-11) A[3]:(1.68811648067e-17)\n",
      " state (8)  A[0]:(0.000119347030704) A[1]:(0.999880671501) A[2]:(1.95786199753e-11) A[3]:(1.6514857385e-19)\n",
      " state (9)  A[0]:(2.89326465008e-07) A[1]:(0.999999701977) A[2]:(1.34900554269e-11) A[3]:(6.78606776856e-21)\n",
      " state (10)  A[0]:(1.14416351948e-08) A[1]:(1.0) A[2]:(1.27195242186e-11) A[3]:(1.58698902019e-21)\n",
      " state (11)  A[0]:(2.1415933471e-09) A[1]:(1.0) A[2]:(1.52522248303e-11) A[3]:(6.03866939251e-22)\n",
      " state (12)  A[0]:(8.41181624356e-10) A[1]:(1.0) A[2]:(1.85626548854e-11) A[3]:(3.1346853457e-22)\n",
      " state (13)  A[0]:(5.02654751156e-10) A[1]:(1.0) A[2]:(2.12471446687e-11) A[3]:(2.11091213897e-22)\n",
      " state (14)  A[0]:(3.80264125743e-10) A[1]:(1.0) A[2]:(2.30174143634e-11) A[3]:(1.68884403921e-22)\n",
      " state (15)  A[0]:(3.27208482531e-10) A[1]:(1.0) A[2]:(2.40735972823e-11) A[3]:(1.49427067652e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 967000 finished after 6 . Running score: 0.1. Policy_loss: -92050.6113798, Value_loss: 1.64404708799. Times trained:               13126. Times reached goal: 131.               Steps done: 12938754.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.998526930809) A[1]:(0.000432127562817) A[2]:(0.000450763094705) A[3]:(0.000590152863879)\n",
      " state (1)  A[0]:(0.00104156008456) A[1]:(7.76194647187e-05) A[2]:(2.41062662099e-06) A[3]:(0.998878419399)\n",
      " state (2)  A[0]:(0.999999701977) A[1]:(3.02497568327e-07) A[2]:(3.4465492797e-14) A[3]:(3.4827130329e-12)\n",
      " state (3)  A[0]:(0.999999701977) A[1]:(3.22518388884e-07) A[2]:(5.46164345642e-15) A[3]:(7.17041308057e-14)\n",
      " state (4)  A[0]:(0.999999463558) A[1]:(5.58133933737e-07) A[2]:(5.03818458104e-15) A[3]:(1.6148596742e-14)\n",
      " state (5)  A[0]:(0.999990046024) A[1]:(9.94217407424e-06) A[2]:(2.26052188849e-14) A[3]:(4.57267051982e-15)\n",
      " state (6)  A[0]:(0.330908566713) A[1]:(0.669091463089) A[2]:(1.6277488038e-11) A[3]:(4.79688998772e-16)\n",
      " state (7)  A[0]:(0.044867515564) A[1]:(0.955132484436) A[2]:(2.39777520755e-11) A[3]:(3.35373662653e-17)\n",
      " state (8)  A[0]:(0.00217571575195) A[1]:(0.997824311256) A[2]:(2.49418419945e-11) A[3]:(1.48057688566e-18)\n",
      " state (9)  A[0]:(1.03206475615e-05) A[1]:(0.999989688396) A[2]:(1.93228297946e-11) A[3]:(3.93127473509e-20)\n",
      " state (10)  A[0]:(1.39428578905e-07) A[1]:(0.999999880791) A[2]:(1.52189198743e-11) A[3]:(5.3994396238e-21)\n",
      " state (11)  A[0]:(1.37012117207e-08) A[1]:(1.0) A[2]:(1.54048388629e-11) A[3]:(1.8429981886e-21)\n",
      " state (12)  A[0]:(3.72305253293e-09) A[1]:(1.0) A[2]:(1.76575577077e-11) A[3]:(8.78048282231e-22)\n",
      " state (13)  A[0]:(1.72879977001e-09) A[1]:(1.0) A[2]:(2.02924274606e-11) A[3]:(5.28172895496e-22)\n",
      " state (14)  A[0]:(1.10212872162e-09) A[1]:(1.0) A[2]:(2.24620131523e-11) A[3]:(3.82181399127e-22)\n",
      " state (15)  A[0]:(8.48834003087e-10) A[1]:(1.0) A[2]:(2.39746226344e-11) A[3]:(3.14263529657e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 968000 finished after 6 . Running score: 0.09. Policy_loss: -92050.6113729, Value_loss: 1.65692479446. Times trained:               13122. Times reached goal: 135.               Steps done: 12951876.\n",
      " state (0)  A[0]:(0.998143553734) A[1]:(0.000205608492251) A[2]:(0.000360353413271) A[3]:(0.00129049504176)\n",
      " state (1)  A[0]:(0.000123329649796) A[1]:(6.94500113241e-06) A[2]:(6.7899941314e-07) A[3]:(0.999869048595)\n",
      " state (2)  A[0]:(0.999998927116) A[1]:(1.09160600914e-06) A[2]:(2.61861864657e-12) A[3]:(2.65818633771e-09)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.38253852811e-07) A[2]:(8.32150408964e-15) A[3]:(2.90589926254e-13)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(3.03688466374e-07) A[2]:(7.45732550147e-15) A[3]:(3.52783801521e-14)\n",
      " state (5)  A[0]:(0.99999332428) A[1]:(6.69707105772e-06) A[2]:(3.66654304845e-14) A[3]:(7.53080560965e-15)\n",
      " state (6)  A[0]:(0.460467070341) A[1]:(0.539532959461) A[2]:(3.16103254683e-11) A[3]:(1.03200980222e-15)\n",
      " state (7)  A[0]:(0.0846853479743) A[1]:(0.915314674377) A[2]:(5.41420623612e-11) A[3]:(9.84231439758e-17)\n",
      " state (8)  A[0]:(0.00423766765743) A[1]:(0.995762348175) A[2]:(5.93048943287e-11) A[3]:(3.94315095489e-18)\n",
      " state (9)  A[0]:(1.99397218239e-05) A[1]:(0.999980032444) A[2]:(4.53639140141e-11) A[3]:(1.02697342331e-19)\n",
      " state (10)  A[0]:(2.15602184994e-07) A[1]:(0.999999761581) A[2]:(3.46253997696e-11) A[3]:(1.30376468066e-20)\n",
      " state (11)  A[0]:(1.90105691189e-08) A[1]:(1.0) A[2]:(3.52204862497e-11) A[3]:(4.21080289398e-21)\n",
      " state (12)  A[0]:(4.95256768929e-09) A[1]:(1.0) A[2]:(4.12800939265e-11) A[3]:(1.90987582191e-21)\n",
      " state (13)  A[0]:(2.28680430148e-09) A[1]:(1.0) A[2]:(4.81208684011e-11) A[3]:(1.12260818852e-21)\n",
      " state (14)  A[0]:(1.47437240194e-09) A[1]:(1.0) A[2]:(5.350463822e-11) A[3]:(8.10257892684e-22)\n",
      " state (15)  A[0]:(1.15241549636e-09) A[1]:(1.0) A[2]:(5.70928720967e-11) A[3]:(6.70063855014e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 969000 finished after 19 . Running score: 0.13. Policy_loss: -92050.6153809, Value_loss: 1.64495644532. Times trained:               12470. Times reached goal: 115.               Steps done: 12964346.\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0013,  0.0058,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0013,  0.0058,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0013,  0.0058,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0013,  0.0058,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0013,  0.0058,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0013,  0.0058,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0013,  0.0058,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0013,  0.0058,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0013,  0.0058,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.5153e-07,  1.4381e-14,  2.6406e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.5153e-07,  1.4383e-14,  2.6414e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0013,  0.0058,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0013,  0.0058,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.5152e-07,  1.4388e-14,  2.6437e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  4.5152e-07,  1.4390e-14,  2.6444e-14]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 3.2498e-04,  9.9968e-01,  5.3557e-11,  5.5774e-19]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.991954863071) A[1]:(0.0012809681939) A[2]:(0.00586060946807) A[3]:(0.000903531850781)\n",
      " state (1)  A[0]:(0.000348336470779) A[1]:(2.89095059998e-05) A[2]:(4.5263109314e-06) A[3]:(0.99961823225)\n",
      " state (2)  A[0]:(0.999999403954) A[1]:(5.91507159697e-07) A[2]:(8.49360187218e-13) A[3]:(1.04342957885e-10)\n",
      " state (3)  A[0]:(0.999999761581) A[1]:(2.36056735048e-07) A[2]:(1.85230096642e-14) A[3]:(1.87854546914e-13)\n",
      " state (4)  A[0]:(0.999999523163) A[1]:(4.5151156769e-07) A[2]:(1.43934648542e-14) A[3]:(2.64577839211e-14)\n",
      " state (5)  A[0]:(0.999965429306) A[1]:(3.45578118868e-05) A[2]:(1.42551199794e-13) A[3]:(4.90032658074e-15)\n",
      " state (6)  A[0]:(0.347084015608) A[1]:(0.652916014194) A[2]:(3.87910814581e-11) A[3]:(6.61754919136e-16)\n",
      " state (7)  A[0]:(0.0340856015682) A[1]:(0.965914368629) A[2]:(6.05708677659e-11) A[3]:(3.40810522296e-17)\n",
      " state (8)  A[0]:(0.000325407920172) A[1]:(0.999674618244) A[2]:(5.35668662205e-11) A[3]:(5.58289838011e-19)\n",
      " state (9)  A[0]:(6.66141147576e-07) A[1]:(0.999999344349) A[2]:(3.57785190364e-11) A[3]:(1.82625759589e-20)\n",
      " state (10)  A[0]:(1.70729776983e-08) A[1]:(1.0) A[2]:(3.40917120922e-11) A[3]:(3.15085959434e-21)\n",
      " state (11)  A[0]:(2.54489362916e-09) A[1]:(1.0) A[2]:(4.30956312303e-11) A[3]:(9.76221150984e-22)\n",
      " state (12)  A[0]:(9.16643261295e-10) A[1]:(1.0) A[2]:(5.40486717882e-11) A[3]:(4.60673564068e-22)\n",
      " state (13)  A[0]:(5.37155098179e-10) A[1]:(1.0) A[2]:(6.23369689201e-11) A[3]:(3.01644935861e-22)\n",
      " state (14)  A[0]:(4.07556322024e-10) A[1]:(1.0) A[2]:(6.74658731659e-11) A[3]:(2.40733716329e-22)\n",
      " state (15)  A[0]:(3.53346241155e-10) A[1]:(1.0) A[2]:(7.03659422396e-11) A[3]:(2.13905629794e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 970000 finished after 16 . Running score: 0.09. Policy_loss: -92050.6112453, Value_loss: 1.44544672893. Times trained:               12827. Times reached goal: 129.               Steps done: 12977173.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.992251574993) A[1]:(0.000844798341859) A[2]:(0.00285926810466) A[3]:(0.00404435303062)\n",
      " state (1)  A[0]:(3.36923585564e-05) A[1]:(2.66708389063e-06) A[2]:(5.19679588251e-07) A[3]:(0.999963104725)\n",
      " state (2)  A[0]:(0.999992311001) A[1]:(7.25435802451e-06) A[2]:(1.0912426518e-10) A[3]:(4.55275255717e-07)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(1.73790624558e-07) A[2]:(1.62140317313e-14) A[3]:(1.8673435194e-12)\n",
      " state (4)  A[0]:(0.999999642372) A[1]:(3.62758243e-07) A[2]:(1.15630932496e-14) A[3]:(1.61368463622e-13)\n",
      " state (5)  A[0]:(0.999998390675) A[1]:(1.59307012382e-06) A[2]:(1.96571579814e-14) A[3]:(2.45111819296e-14)\n",
      " state (6)  A[0]:(0.552635908127) A[1]:(0.447364091873) A[2]:(2.8154976614e-11) A[3]:(2.38266852876e-15)\n",
      " state (7)  A[0]:(0.0509534552693) A[1]:(0.949046552181) A[2]:(4.89504166368e-11) A[3]:(1.01903264866e-16)\n",
      " state (8)  A[0]:(0.00109478586819) A[1]:(0.998905241489) A[2]:(4.71312745776e-11) A[3]:(2.30942767927e-18)\n",
      " state (9)  A[0]:(3.04063337353e-06) A[1]:(0.999996960163) A[2]:(3.16384973775e-11) A[3]:(7.42048153261e-20)\n",
      " state (10)  A[0]:(5.22674739045e-08) A[1]:(0.999999940395) A[2]:(2.59015916354e-11) A[3]:(1.20599702136e-20)\n",
      " state (11)  A[0]:(6.30915719668e-09) A[1]:(1.0) A[2]:(2.89772199291e-11) A[3]:(3.94105840766e-21)\n",
      " state (12)  A[0]:(1.96826022147e-09) A[1]:(1.0) A[2]:(3.49844146708e-11) A[3]:(1.83278323505e-21)\n",
      " state (13)  A[0]:(1.02985120431e-09) A[1]:(1.0) A[2]:(4.04688504929e-11) A[3]:(1.13759306211e-21)\n",
      " state (14)  A[0]:(7.21317283681e-10) A[1]:(1.0) A[2]:(4.43310214926e-11) A[3]:(8.6297576304e-22)\n",
      " state (15)  A[0]:(5.93724291953e-10) A[1]:(1.0) A[2]:(4.6734383724e-11) A[3]:(7.39196191625e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 971000 finished after 3 . Running score: 0.11. Policy_loss: -92050.6265222, Value_loss: 1.4332844368. Times trained:               12634. Times reached goal: 136.               Steps done: 12989807.\n",
      " state (0)  A[0]:(0.998016834259) A[1]:(0.000459618633613) A[2]:(0.00106428423896) A[3]:(0.000459264585515)\n",
      " state (1)  A[0]:(0.00115251541138) A[1]:(4.26551305281e-05) A[2]:(2.27000646191e-06) A[3]:(0.998802542686)\n",
      " state (2)  A[0]:(0.999999821186) A[1]:(2.0244064558e-07) A[2]:(3.51275777943e-14) A[3]:(8.54453313659e-12)\n",
      " state (3)  A[0]:(0.999999642372) A[1]:(3.61926140613e-07) A[2]:(1.19266872244e-14) A[3]:(3.56926104341e-13)\n",
      " state (4)  A[0]:(0.99999922514) A[1]:(7.72648718339e-07) A[2]:(1.18172666612e-14) A[3]:(3.38011750208e-14)\n",
      " state (5)  A[0]:(0.855941474438) A[1]:(0.144058510661) A[2]:(1.38542718964e-11) A[3]:(2.53268949866e-15)\n",
      " state (6)  A[0]:(0.0299982707947) A[1]:(0.970001757145) A[2]:(3.55160657828e-11) A[3]:(5.11281494672e-17)\n",
      " state (7)  A[0]:(7.29866005713e-05) A[1]:(0.99992698431) A[2]:(2.80219944471e-11) A[3]:(3.02515252852e-19)\n",
      " state (8)  A[0]:(8.10164735299e-08) A[1]:(0.999999940395) A[2]:(1.69649190529e-11) A[3]:(1.10989818279e-20)\n",
      " state (9)  A[0]:(3.15239367943e-09) A[1]:(1.0) A[2]:(1.85025623295e-11) A[3]:(2.13495699802e-21)\n",
      " state (10)  A[0]:(6.82028655774e-10) A[1]:(1.0) A[2]:(2.45525284132e-11) A[3]:(7.40810869991e-22)\n",
      " state (11)  A[0]:(3.31108446217e-10) A[1]:(1.0) A[2]:(2.96920196763e-11) A[3]:(4.18669535521e-22)\n",
      " state (12)  A[0]:(2.37420999527e-10) A[1]:(1.0) A[2]:(3.27242018205e-11) A[3]:(3.1798495889e-22)\n",
      " state (13)  A[0]:(2.03789332631e-10) A[1]:(1.0) A[2]:(3.4276487898e-11) A[3]:(2.79645081962e-22)\n",
      " state (14)  A[0]:(1.89885898672e-10) A[1]:(1.0) A[2]:(3.50292850282e-11) A[3]:(2.63407699203e-22)\n",
      " state (15)  A[0]:(1.83723855574e-10) A[1]:(1.0) A[2]:(3.53884282989e-11) A[3]:(2.56138516356e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 972000 finished after 5 . Running score: 0.14. Policy_loss: -92050.621534, Value_loss: 1.20526893342. Times trained:               12779. Times reached goal: 140.               Steps done: 13002586.\n",
      " state (0)  A[0]:(0.993945717812) A[1]:(0.000795218278654) A[2]:(0.00216654757969) A[3]:(0.00309251644649)\n",
      " state (1)  A[0]:(7.55354194553e-05) A[1]:(3.40099904861e-06) A[2]:(4.10077404922e-07) A[3]:(0.999920666218)\n",
      " state (2)  A[0]:(0.999999582767) A[1]:(4.04278722499e-07) A[2]:(4.40220613868e-13) A[3]:(8.27271695591e-10)\n",
      " state (3)  A[0]:(0.999999701977) A[1]:(2.93678738217e-07) A[2]:(2.28265790872e-14) A[3]:(1.70739138973e-12)\n",
      " state (4)  A[0]:(0.99999910593) A[1]:(8.70364090133e-07) A[2]:(2.68200395838e-14) A[3]:(6.36281663208e-14)\n",
      " state (5)  A[0]:(0.540522217751) A[1]:(0.459477812052) A[2]:(5.15377879262e-11) A[3]:(3.19635061251e-15)\n",
      " state (6)  A[0]:(0.00894435215741) A[1]:(0.9910556674) A[2]:(8.044377664e-11) A[3]:(2.58382321671e-17)\n",
      " state (7)  A[0]:(2.90897355626e-06) A[1]:(0.999997079372) A[2]:(4.56128294857e-11) A[3]:(1.10674581686e-19)\n",
      " state (8)  A[0]:(1.22465522168e-08) A[1]:(1.0) A[2]:(3.36400768353e-11) A[3]:(9.38235991357e-21)\n",
      " state (9)  A[0]:(1.18235732316e-09) A[1]:(1.0) A[2]:(4.40379156752e-11) A[3]:(2.2397213613e-21)\n",
      " state (10)  A[0]:(4.08624245551e-10) A[1]:(1.0) A[2]:(5.70680482037e-11) A[3]:(9.87325384278e-22)\n",
      " state (11)  A[0]:(2.56710375179e-10) A[1]:(1.0) A[2]:(6.53325657463e-11) A[3]:(6.71169017588e-22)\n",
      " state (12)  A[0]:(2.10025830061e-10) A[1]:(1.0) A[2]:(6.94859586536e-11) A[3]:(5.65821679784e-22)\n",
      " state (13)  A[0]:(1.92486596107e-10) A[1]:(1.0) A[2]:(7.14059159024e-11) A[3]:(5.25049107286e-22)\n",
      " state (14)  A[0]:(1.85253840046e-10) A[1]:(1.0) A[2]:(7.22707796386e-11) A[3]:(5.08037732072e-22)\n",
      " state (15)  A[0]:(1.82130061033e-10) A[1]:(1.0) A[2]:(7.26588927913e-11) A[3]:(5.0065954758e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 973000 finished after 7 . Running score: 0.06. Policy_loss: -92050.6215268, Value_loss: 1.19598199191. Times trained:               12710. Times reached goal: 118.               Steps done: 13015296.\n",
      " state (0)  A[0]:(0.992035090923) A[1]:(0.00528483744711) A[2]:(0.00179799296893) A[3]:(0.000882107473444)\n",
      " state (1)  A[0]:(4.31292355643e-05) A[1]:(1.15758848551e-05) A[2]:(4.29449926287e-07) A[3]:(0.999944865704)\n",
      " state (2)  A[0]:(0.999867022038) A[1]:(0.000102564765257) A[2]:(2.26115504098e-09) A[3]:(3.04365166812e-05)\n",
      " state (3)  A[0]:(0.999999642372) A[1]:(3.33246532591e-07) A[2]:(2.87500194008e-14) A[3]:(2.30490257956e-12)\n",
      " state (4)  A[0]:(0.999999403954) A[1]:(5.90875743001e-07) A[2]:(1.93109027711e-14) A[3]:(7.48397315799e-14)\n",
      " state (5)  A[0]:(0.966137528419) A[1]:(0.0338625013828) A[2]:(1.07581426406e-11) A[3]:(7.72974484984e-15)\n",
      " state (6)  A[0]:(0.000552715442609) A[1]:(0.999447286129) A[2]:(2.44223183343e-11) A[3]:(1.16160849344e-18)\n",
      " state (7)  A[0]:(7.34073068998e-08) A[1]:(0.999999940395) A[2]:(9.79071511364e-12) A[3]:(6.87544447218e-21)\n",
      " state (8)  A[0]:(1.16888121404e-09) A[1]:(1.0) A[2]:(9.02906308747e-12) A[3]:(9.19004526792e-22)\n",
      " state (9)  A[0]:(2.10080314256e-10) A[1]:(1.0) A[2]:(1.19257537873e-11) A[3]:(2.80503009217e-22)\n",
      " state (10)  A[0]:(1.02020440707e-10) A[1]:(1.0) A[2]:(1.42324806454e-11) A[3]:(1.57791530224e-22)\n",
      " state (11)  A[0]:(7.58518942101e-11) A[1]:(1.0) A[2]:(1.54040166039e-11) A[3]:(1.23446343947e-22)\n",
      " state (12)  A[0]:(6.71139463448e-11) A[1]:(1.0) A[2]:(1.59186587523e-11) A[3]:(1.11450470638e-22)\n",
      " state (13)  A[0]:(6.37495542466e-11) A[1]:(1.0) A[2]:(1.61352667338e-11) A[3]:(1.06773194424e-22)\n",
      " state (14)  A[0]:(6.23602766647e-11) A[1]:(1.0) A[2]:(1.62256926645e-11) A[3]:(1.04841109984e-22)\n",
      " state (15)  A[0]:(6.17639134282e-11) A[1]:(1.0) A[2]:(1.62638028045e-11) A[3]:(1.0401491387e-22)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 974000 finished after 17 . Running score: 0.11. Policy_loss: -92050.6215305, Value_loss: 0.99537259215. Times trained:               12925. Times reached goal: 130.               Steps done: 13028221.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9923,  0.0027,  0.0016,  0.0034]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9923,  0.0027,  0.0016,  0.0034]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9923,  0.0027,  0.0016,  0.0034]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9923,  0.0027,  0.0016,  0.0034]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3522e-07,  2.0979e-14,  3.2236e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3523e-07,  2.0979e-14,  3.2240e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9923,  0.0027,  0.0016,  0.0034]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3523e-07,  2.0980e-14,  3.2246e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  2.3523e-07,  2.0980e-14,  3.2248e-13]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.8833e-06,  9.9999e-01,  2.7771e-11,  7.2888e-20]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.8834e-06,  9.9999e-01,  2.7771e-11,  7.2890e-20]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.8834e-06,  9.9999e-01,  2.7771e-11,  7.2892e-20]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 8.6012e-08,  1.0000e+00,  1.7192e-11,  1.2273e-20]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8567e-09,  1.0000e+00,  1.6362e-11,  1.8030e-21]])\n",
      "On state=13, selected action=1\n",
      "new state=14, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.5958e-09,  1.0000e+00,  1.6564e-11,  1.6338e-21]])\n",
      "On state=14, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8567e-09,  1.0000e+00,  1.6362e-11,  1.8030e-21]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.992264330387) A[1]:(0.00270164688118) A[2]:(0.00160464679357) A[3]:(0.00342937954701)\n",
      " state (1)  A[0]:(3.4323282307e-05) A[1]:(4.27523445978e-06) A[2]:(1.8799626389e-07) A[3]:(0.999961197376)\n",
      " state (2)  A[0]:(0.999990224838) A[1]:(8.15067778603e-06) A[2]:(7.60258314636e-11) A[3]:(1.6099061213e-06)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.44763944832e-07) A[2]:(2.84735613993e-14) A[3]:(1.1722377409e-11)\n",
      " state (4)  A[0]:(0.999999761581) A[1]:(2.35244101532e-07) A[2]:(2.09829949368e-14) A[3]:(3.22630621221e-13)\n",
      " state (5)  A[0]:(0.999998807907) A[1]:(1.21324876545e-06) A[2]:(3.55945267293e-14) A[3]:(4.4993126385e-14)\n",
      " state (6)  A[0]:(0.196365877986) A[1]:(0.803634107113) A[2]:(9.38965363795e-11) A[3]:(7.42783731996e-16)\n",
      " state (7)  A[0]:(0.00213188841008) A[1]:(0.99786812067) A[2]:(5.4632097457e-11) A[3]:(3.25882443412e-18)\n",
      " state (8)  A[0]:(5.88329112361e-06) A[1]:(0.99999409914) A[2]:(2.77714466007e-11) A[3]:(7.28988263859e-20)\n",
      " state (9)  A[0]:(8.60119300228e-08) A[1]:(0.999999940395) A[2]:(1.71914357749e-11) A[3]:(1.22737430876e-20)\n",
      " state (10)  A[0]:(1.14842260146e-08) A[1]:(1.0) A[2]:(1.52346624899e-11) A[3]:(5.07106669207e-21)\n",
      " state (11)  A[0]:(4.17719903112e-09) A[1]:(1.0) A[2]:(1.54758133392e-11) A[3]:(2.96199180383e-21)\n",
      " state (12)  A[0]:(2.45987719083e-09) A[1]:(1.0) A[2]:(1.59968201879e-11) A[3]:(2.15582614287e-21)\n",
      " state (13)  A[0]:(1.85670645525e-09) A[1]:(1.0) A[2]:(1.63623732619e-11) A[3]:(1.80304754606e-21)\n",
      " state (14)  A[0]:(1.59584989667e-09) A[1]:(1.0) A[2]:(1.65643453814e-11) A[3]:(1.63387144031e-21)\n",
      " state (15)  A[0]:(1.46922396471e-09) A[1]:(1.0) A[2]:(1.66646106636e-11) A[3]:(1.54759121137e-21)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 975000 finished after 16 . Running score: 0.16. Policy_loss: -92050.6217226, Value_loss: 1.2173471757. Times trained:               12614. Times reached goal: 147.               Steps done: 13040835.\n",
      " state (0)  A[0]:(0.997134745121) A[1]:(0.0018105203053) A[2]:(0.000672358612064) A[3]:(0.000382350874133)\n",
      " state (1)  A[0]:(0.000227029711823) A[1]:(1.97109075089e-05) A[2]:(4.69006039339e-07) A[3]:(0.999752819538)\n",
      " state (2)  A[0]:(0.999999582767) A[1]:(3.86926785723e-07) A[2]:(2.7363758503e-13) A[3]:(5.96976301726e-10)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.4061329523e-07) A[2]:(1.96067120872e-14) A[3]:(2.53656405776e-12)\n",
      " state (4)  A[0]:(0.999999761581) A[1]:(2.23618528139e-07) A[2]:(1.49640990923e-14) A[3]:(9.6428121293e-14)\n",
      " state (5)  A[0]:(0.999993443489) A[1]:(6.53495044389e-06) A[2]:(7.36222606317e-14) A[3]:(2.29421906142e-14)\n",
      " state (6)  A[0]:(0.266752541065) A[1]:(0.733247458935) A[2]:(7.24872592506e-11) A[3]:(7.03918207547e-16)\n",
      " state (7)  A[0]:(0.0251502078027) A[1]:(0.974849820137) A[2]:(5.62235327739e-11) A[3]:(2.84432009327e-17)\n",
      " state (8)  A[0]:(0.000824413378723) A[1]:(0.999175608158) A[2]:(4.15741573112e-11) A[3]:(7.76725128428e-19)\n",
      " state (9)  A[0]:(7.55774135541e-06) A[1]:(0.99999243021) A[2]:(2.44040603697e-11) A[3]:(5.31737547143e-20)\n",
      " state (10)  A[0]:(2.47613826332e-07) A[1]:(0.999999761581) A[2]:(1.65908380784e-11) A[3]:(1.26341102991e-20)\n",
      " state (11)  A[0]:(4.15325622782e-08) A[1]:(0.999999940395) A[2]:(1.42615563456e-11) A[3]:(6.06487411485e-21)\n",
      " state (12)  A[0]:(1.60685846851e-08) A[1]:(1.0) A[2]:(1.37118571705e-11) A[3]:(3.92975374058e-21)\n",
      " state (13)  A[0]:(9.49873779632e-09) A[1]:(1.0) A[2]:(1.3659910876e-11) A[3]:(3.01683068766e-21)\n",
      " state (14)  A[0]:(7.06268732387e-09) A[1]:(1.0) A[2]:(1.37057388008e-11) A[3]:(2.57585880479e-21)\n",
      " state (15)  A[0]:(5.96639848638e-09) A[1]:(1.0) A[2]:(1.37471050168e-11) A[3]:(2.34745860686e-21)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 976000 finished after 21 . Running score: 0.14. Policy_loss: -92050.6214732, Value_loss: 1.4227634664. Times trained:               12657. Times reached goal: 113.               Steps done: 13053492.\n",
      " state (0)  A[0]:(0.998252213001) A[1]:(0.000666774169076) A[2]:(0.00025472548441) A[3]:(0.000826307572424)\n",
      " state (1)  A[0]:(0.000104488055513) A[1]:(4.7497869673e-06) A[2]:(1.21819482501e-07) A[3]:(0.999890625477)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(1.45763607406e-07) A[2]:(1.1018320452e-13) A[3]:(1.40568090501e-09)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.00616070142e-07) A[2]:(2.26797222089e-14) A[3]:(2.63900949704e-11)\n",
      " state (4)  A[0]:(0.999999821186) A[1]:(1.66047385619e-07) A[2]:(1.73786681524e-14) A[3]:(7.68903820822e-13)\n",
      " state (5)  A[0]:(0.999995648861) A[1]:(4.32469005318e-06) A[2]:(7.97811456114e-14) A[3]:(1.30304797443e-13)\n",
      " state (6)  A[0]:(0.453583270311) A[1]:(0.546416699886) A[2]:(9.13919842582e-11) A[3]:(7.13766712564e-15)\n",
      " state (7)  A[0]:(0.0706447064877) A[1]:(0.929355323315) A[2]:(8.77866460081e-11) A[3]:(4.95822594137e-16)\n",
      " state (8)  A[0]:(0.00773325795308) A[1]:(0.992266714573) A[2]:(7.29273794131e-11) A[3]:(2.90510529515e-17)\n",
      " state (9)  A[0]:(0.000174109009095) A[1]:(0.999825894833) A[2]:(5.03388164486e-11) A[3]:(1.34966707512e-18)\n",
      " state (10)  A[0]:(3.03670390167e-06) A[1]:(0.999996960163) A[2]:(3.18544253475e-11) A[3]:(1.81804501719e-19)\n",
      " state (11)  A[0]:(2.48719544516e-07) A[1]:(0.999999761581) A[2]:(2.46868237658e-11) A[3]:(6.38677847803e-20)\n",
      " state (12)  A[0]:(6.51164455689e-08) A[1]:(0.999999940395) A[2]:(2.24596972964e-11) A[3]:(3.58747262312e-20)\n",
      " state (13)  A[0]:(3.10822514393e-08) A[1]:(0.999999940395) A[2]:(2.18526342854e-11) A[3]:(2.5363603247e-20)\n",
      " state (14)  A[0]:(2.04299368534e-08) A[1]:(1.0) A[2]:(2.17170136041e-11) A[3]:(2.05508887286e-20)\n",
      " state (15)  A[0]:(1.60413318184e-08) A[1]:(1.0) A[2]:(2.16922573654e-11) A[3]:(1.81120355498e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 977000 finished after 15 . Running score: 0.09. Policy_loss: -92050.6215744, Value_loss: 1.42749580586. Times trained:               12959. Times reached goal: 135.               Steps done: 13066451.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.885864913464) A[1]:(0.000644352287054) A[2]:(0.000965296814684) A[3]:(0.112525455654)\n",
      " state (1)  A[0]:(4.58184695162e-06) A[1]:(1.54998176072e-07) A[2]:(1.00261523528e-08) A[3]:(0.999995231628)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(9.24192917751e-08) A[2]:(1.70928714433e-13) A[3]:(2.78264735698e-08)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.15045757809e-07) A[2]:(6.22586766755e-14) A[3]:(8.1970752408e-10)\n",
      " state (4)  A[0]:(0.999999761581) A[1]:(2.34896162965e-07) A[2]:(5.11754809536e-14) A[3]:(9.14302314414e-12)\n",
      " state (5)  A[0]:(0.998347938061) A[1]:(0.00165209057741) A[2]:(7.64801850323e-12) A[3]:(4.25929501087e-13)\n",
      " state (6)  A[0]:(0.0562100335956) A[1]:(0.943789958954) A[2]:(1.99420105429e-10) A[3]:(3.19676099997e-15)\n",
      " state (7)  A[0]:(0.00227380893193) A[1]:(0.997726202011) A[2]:(1.45198589263e-10) A[3]:(6.74055319837e-17)\n",
      " state (8)  A[0]:(1.15901757454e-05) A[1]:(0.999988436699) A[2]:(8.23502654956e-11) A[3]:(2.18497235905e-18)\n",
      " state (9)  A[0]:(2.07628332305e-07) A[1]:(0.999999821186) A[2]:(5.38551980789e-11) A[3]:(3.79895491765e-19)\n",
      " state (10)  A[0]:(3.17071453537e-08) A[1]:(0.999999940395) A[2]:(4.83789154571e-11) A[3]:(1.66404027225e-19)\n",
      " state (11)  A[0]:(1.32035973266e-08) A[1]:(1.0) A[2]:(4.84045789562e-11) A[3]:(1.07148336584e-19)\n",
      " state (12)  A[0]:(8.68155503042e-09) A[1]:(1.0) A[2]:(4.90812009091e-11) A[3]:(8.51505280182e-20)\n",
      " state (13)  A[0]:(7.08284142448e-09) A[1]:(1.0) A[2]:(4.95229794673e-11) A[3]:(7.58061271321e-20)\n",
      " state (14)  A[0]:(6.40694741705e-09) A[1]:(1.0) A[2]:(4.97365378049e-11) A[3]:(7.15110822743e-20)\n",
      " state (15)  A[0]:(6.09304029453e-09) A[1]:(1.0) A[2]:(4.98267399562e-11) A[3]:(6.9438477852e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 978000 finished after 10 . Running score: 0.14. Policy_loss: -92050.6219788, Value_loss: 2.05615062082. Times trained:               12864. Times reached goal: 127.               Steps done: 13079315.\n",
      " state (0)  A[0]:(0.992503225803) A[1]:(0.000284678826574) A[2]:(0.000548213953152) A[3]:(0.0066638533026)\n",
      " state (1)  A[0]:(2.37665008171e-05) A[1]:(4.80506400891e-07) A[2]:(4.55293474033e-08) A[3]:(0.999975681305)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(7.71267778532e-08) A[2]:(2.88078968232e-13) A[3]:(7.0741195124e-09)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.16061357858e-07) A[2]:(1.54528489614e-13) A[3]:(3.33830130206e-10)\n",
      " state (4)  A[0]:(0.999999761581) A[1]:(2.24963528694e-07) A[2]:(1.27589684152e-13) A[3]:(5.55655651699e-12)\n",
      " state (5)  A[0]:(0.999821424484) A[1]:(0.000178596441401) A[2]:(4.4166454348e-12) A[3]:(3.20391418474e-13)\n",
      " state (6)  A[0]:(0.0392342284322) A[1]:(0.960765779018) A[2]:(3.37920913474e-10) A[3]:(1.35319760192e-15)\n",
      " state (7)  A[0]:(0.000477004883578) A[1]:(0.999522984028) A[2]:(2.08669748147e-10) A[3]:(1.23992190065e-17)\n",
      " state (8)  A[0]:(1.49456570853e-06) A[1]:(0.999998509884) A[2]:(1.01614015813e-10) A[3]:(5.62717787529e-19)\n",
      " state (9)  A[0]:(5.60739614741e-08) A[1]:(0.999999940395) A[2]:(7.33702404387e-11) A[3]:(1.40948616623e-19)\n",
      " state (10)  A[0]:(1.32218431759e-08) A[1]:(1.0) A[2]:(7.08548289485e-11) A[3]:(7.06380642236e-20)\n",
      " state (11)  A[0]:(6.868828617e-09) A[1]:(1.0) A[2]:(7.25719137562e-11) A[3]:(4.93031569083e-20)\n",
      " state (12)  A[0]:(5.09121722558e-09) A[1]:(1.0) A[2]:(7.39880240408e-11) A[3]:(4.13416792218e-20)\n",
      " state (13)  A[0]:(4.43528902494e-09) A[1]:(1.0) A[2]:(7.47326298067e-11) A[3]:(3.80294121734e-20)\n",
      " state (14)  A[0]:(4.15924761299e-09) A[1]:(1.0) A[2]:(7.50752238154e-11) A[3]:(3.65622037931e-20)\n",
      " state (15)  A[0]:(4.03489641698e-09) A[1]:(1.0) A[2]:(7.52242920732e-11) A[3]:(3.58862259804e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 979000 finished after 22 . Running score: 0.15. Policy_loss: -92050.6213916, Value_loss: 1.63359377948. Times trained:               13065. Times reached goal: 135.               Steps done: 13092380.\n",
      "action_dist \n",
      "tensor([[ 0.9997,  0.0001,  0.0000,  0.0002]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.9910e-07,  7.3506e-14,  1.6767e-12]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.6596e-04,  9.9983e-01,  1.2435e-10,  2.8262e-18]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.7072e-04,  9.9983e-01,  1.2488e-10,  2.8691e-18]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.999662220478) A[1]:(0.00010185449355) A[2]:(4.72786414321e-05) A[3]:(0.000188674472156)\n",
      " state (1)  A[0]:(0.000165914083482) A[1]:(1.78150446573e-06) A[2]:(7.30918472414e-08) A[3]:(0.999832212925)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(6.56216485595e-08) A[2]:(1.54364707323e-13) A[3]:(2.46287212846e-09)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.03270430429e-07) A[2]:(8.87744671068e-14) A[3]:(9.99155896908e-11)\n",
      " state (4)  A[0]:(0.999999821186) A[1]:(1.98299545673e-07) A[2]:(7.34740298659e-14) A[3]:(1.68795706579e-12)\n",
      " state (5)  A[0]:(0.998121261597) A[1]:(0.00187872059178) A[2]:(1.04450502067e-11) A[3]:(1.49572628588e-13)\n",
      " state (6)  A[0]:(0.0763264223933) A[1]:(0.923673570156) A[2]:(2.43464831629e-10) A[3]:(1.55521654803e-15)\n",
      " state (7)  A[0]:(0.00708204926923) A[1]:(0.992917954922) A[2]:(1.91722915321e-10) A[3]:(5.76608679118e-17)\n",
      " state (8)  A[0]:(0.000179482784006) A[1]:(0.999820530415) A[2]:(1.2582998532e-10) A[3]:(2.94717462576e-18)\n",
      " state (9)  A[0]:(2.6966392852e-06) A[1]:(0.999997317791) A[2]:(7.25124960077e-11) A[3]:(4.08009137738e-19)\n",
      " state (10)  A[0]:(2.41890774078e-07) A[1]:(0.999999761581) A[2]:(5.54669019048e-11) A[3]:(1.55155288548e-19)\n",
      " state (11)  A[0]:(7.50509911995e-08) A[1]:(0.999999940395) A[2]:(5.11689302041e-11) A[3]:(9.45883680017e-20)\n",
      " state (12)  A[0]:(4.21500878645e-08) A[1]:(0.999999940395) A[2]:(5.0213510433e-11) A[3]:(7.24968618446e-20)\n",
      " state (13)  A[0]:(3.15965458242e-08) A[1]:(0.999999940395) A[2]:(5.00372104861e-11) A[3]:(6.29947731919e-20)\n",
      " state (14)  A[0]:(2.73269122886e-08) A[1]:(1.0) A[2]:(5.0011453312e-11) A[3]:(5.85639156173e-20)\n",
      " state (15)  A[0]:(2.53843772668e-08) A[1]:(1.0) A[2]:(5.00061103637e-11) A[3]:(5.64005604345e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 980000 finished after 4 . Running score: 0.2. Policy_loss: -92050.6201662, Value_loss: 1.62478551104. Times trained:               13262. Times reached goal: 143.               Steps done: 13105642.\n",
      " state (0)  A[0]:(0.998431921005) A[1]:(0.000142779710586) A[2]:(0.00113183178473) A[3]:(0.000293441436952)\n",
      " state (1)  A[0]:(0.830062389374) A[1]:(9.77274758043e-05) A[2]:(1.67426151165e-06) A[3]:(0.169838190079)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(7.15770127613e-08) A[2]:(3.45279122134e-12) A[3]:(1.37858735538e-09)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.16252778071e-07) A[2]:(3.16370063827e-12) A[3]:(1.20786311508e-10)\n",
      " state (4)  A[0]:(0.999999761581) A[1]:(2.12957061763e-07) A[2]:(3.48736621178e-12) A[3]:(2.38179961864e-12)\n",
      " state (5)  A[0]:(0.999525368214) A[1]:(0.000474610133097) A[2]:(2.1087323554e-10) A[3]:(1.52775714172e-13)\n",
      " state (6)  A[0]:(0.0547701977193) A[1]:(0.945229768753) A[2]:(8.1125657303e-09) A[3]:(1.15510624171e-15)\n",
      " state (7)  A[0]:(0.00292578991503) A[1]:(0.997074186802) A[2]:(5.92531979038e-09) A[3]:(2.60837476447e-17)\n",
      " state (8)  A[0]:(3.4723245335e-05) A[1]:(0.999965250492) A[2]:(3.32970429007e-09) A[3]:(1.45346304255e-18)\n",
      " state (9)  A[0]:(6.44735507649e-07) A[1]:(0.999999344349) A[2]:(2.01972905067e-09) A[3]:(2.50186204639e-19)\n",
      " state (10)  A[0]:(8.93390250667e-08) A[1]:(0.999999880791) A[2]:(1.74107628315e-09) A[3]:(1.05902835226e-19)\n",
      " state (11)  A[0]:(3.57260852013e-08) A[1]:(0.999999940395) A[2]:(1.71750014211e-09) A[3]:(6.76198889635e-20)\n",
      " state (12)  A[0]:(2.32861072647e-08) A[1]:(1.0) A[2]:(1.73395131586e-09) A[3]:(5.38594809798e-20)\n",
      " state (13)  A[0]:(1.90552214008e-08) A[1]:(1.0) A[2]:(1.74767755823e-09) A[3]:(4.81978077342e-20)\n",
      " state (14)  A[0]:(1.73364806955e-08) A[1]:(1.0) A[2]:(1.75516046141e-09) A[3]:(4.56930369848e-20)\n",
      " state (15)  A[0]:(1.657454618e-08) A[1]:(1.0) A[2]:(1.75879977249e-09) A[3]:(4.45382928497e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 981000 finished after 51 . Running score: 0.1. Policy_loss: -92050.6202117, Value_loss: 1.19302742674. Times trained:               13006. Times reached goal: 109.               Steps done: 13118648.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.993083894253) A[1]:(0.000248247641139) A[2]:(0.0063883382827) A[3]:(0.000279538537143)\n",
      " state (1)  A[0]:(0.00940630212426) A[1]:(1.70184939634e-05) A[2]:(8.73400949786e-06) A[3]:(0.990567922592)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(7.6165434848e-08) A[2]:(2.1667775213e-11) A[3]:(2.16013607002e-09)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.3210606653e-07) A[2]:(2.18194819851e-11) A[3]:(1.0437843645e-10)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(3.21366428579e-07) A[2]:(3.45884328123e-11) A[3]:(1.13701225084e-12)\n",
      " state (5)  A[0]:(0.79658561945) A[1]:(0.20341438055) A[2]:(2.81960854664e-08) A[3]:(5.34095337401e-14)\n",
      " state (6)  A[0]:(0.0556436628103) A[1]:(0.944356322289) A[2]:(3.18142312494e-08) A[3]:(1.0433590908e-15)\n",
      " state (7)  A[0]:(0.019132912159) A[1]:(0.980867087841) A[2]:(2.7585697282e-08) A[3]:(2.15871852322e-16)\n",
      " state (8)  A[0]:(0.00209523364902) A[1]:(0.997904717922) A[2]:(2.2134496902e-08) A[3]:(1.44883744857e-17)\n",
      " state (9)  A[0]:(7.21985488781e-05) A[1]:(0.999927759171) A[2]:(1.24960077841e-08) A[3]:(1.82661641779e-18)\n",
      " state (10)  A[0]:(2.70098371402e-06) A[1]:(0.999997317791) A[2]:(7.0388108675e-09) A[3]:(4.12280388228e-19)\n",
      " state (11)  A[0]:(4.26944808396e-07) A[1]:(0.999999582767) A[2]:(5.33558841553e-09) A[3]:(1.91227549387e-19)\n",
      " state (12)  A[0]:(1.70653649434e-07) A[1]:(0.999999821186) A[2]:(4.8013277798e-09) A[3]:(1.28997291413e-19)\n",
      " state (13)  A[0]:(1.08245885144e-07) A[1]:(0.999999880791) A[2]:(4.61362414939e-09) A[3]:(1.05093523007e-19)\n",
      " state (14)  A[0]:(8.62047428996e-08) A[1]:(0.999999880791) A[2]:(4.53939463796e-09) A[3]:(9.45425499506e-20)\n",
      " state (15)  A[0]:(7.68624346392e-08) A[1]:(0.999999940395) A[2]:(4.50695614163e-09) A[3]:(8.95517363377e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 982000 finished after 7 . Running score: 0.09. Policy_loss: -92050.6204846, Value_loss: 1.41411482097. Times trained:               12689. Times reached goal: 133.               Steps done: 13131337.\n",
      " state (0)  A[0]:(0.997437179089) A[1]:(0.000149924788275) A[2]:(0.00199320260435) A[3]:(0.000419682852225)\n",
      " state (1)  A[0]:(0.0107626561075) A[1]:(1.27018711282e-05) A[2]:(3.20690855915e-06) A[3]:(0.989221453667)\n",
      " state (2)  A[0]:(0.999999940395) A[1]:(7.49805977307e-08) A[2]:(1.03110410835e-11) A[3]:(5.47701750619e-09)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.33559311166e-07) A[2]:(9.66447668421e-12) A[3]:(3.18848059067e-10)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(3.27537350131e-07) A[2]:(1.51026136042e-11) A[3]:(2.48809046283e-12)\n",
      " state (5)  A[0]:(0.875381410122) A[1]:(0.124618597329) A[2]:(8.2582447547e-09) A[3]:(7.95076827184e-14)\n",
      " state (6)  A[0]:(0.0346599146724) A[1]:(0.965340077877) A[2]:(1.06656683485e-08) A[3]:(7.5926942837e-16)\n",
      " state (7)  A[0]:(0.00405405694619) A[1]:(0.995945930481) A[2]:(8.32559710062e-09) A[3]:(4.7153652503e-17)\n",
      " state (8)  A[0]:(6.99314987287e-05) A[1]:(0.999930083752) A[2]:(4.01897670699e-09) A[3]:(2.51163466834e-18)\n",
      " state (9)  A[0]:(9.96066887637e-07) A[1]:(0.999998986721) A[2]:(1.73859260322e-09) A[3]:(3.55462747819e-19)\n",
      " state (10)  A[0]:(1.06793955013e-07) A[1]:(0.999999880791) A[2]:(1.22836008032e-09) A[3]:(1.36379865456e-19)\n",
      " state (11)  A[0]:(3.85055258789e-08) A[1]:(0.999999940395) A[2]:(1.11680598103e-09) A[3]:(8.4273496849e-20)\n",
      " state (12)  A[0]:(2.41666580081e-08) A[1]:(1.0) A[2]:(1.09113085234e-09) A[3]:(6.63580637132e-20)\n",
      " state (13)  A[0]:(1.95306313344e-08) A[1]:(1.0) A[2]:(1.08473963145e-09) A[3]:(5.92002501529e-20)\n",
      " state (14)  A[0]:(1.7710945599e-08) A[1]:(1.0) A[2]:(1.08290809653e-09) A[3]:(5.61189894465e-20)\n",
      " state (15)  A[0]:(1.69273892681e-08) A[1]:(1.0) A[2]:(1.08228448426e-09) A[3]:(5.4736660783e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 983000 finished after 14 . Running score: 0.12. Policy_loss: -92050.6202122, Value_loss: 1.21000084047. Times trained:               12750. Times reached goal: 138.               Steps done: 13144087.\n",
      " state (0)  A[0]:(0.994928598404) A[1]:(0.000281190354144) A[2]:(0.001027563354) A[3]:(0.00376265868545)\n",
      " state (1)  A[0]:(0.00443445704877) A[1]:(3.05325420413e-06) A[2]:(4.46197077508e-07) A[3]:(0.995562016964)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(6.65782948772e-08) A[2]:(7.21195412418e-12) A[3]:(7.5421802137e-08)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.19606752946e-07) A[2]:(6.41380828656e-12) A[3]:(6.67735999826e-09)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(3.13998640422e-07) A[2]:(9.74324700781e-12) A[3]:(4.4909211766e-11)\n",
      " state (5)  A[0]:(0.988547086716) A[1]:(0.0114529086277) A[2]:(1.80924131232e-09) A[3]:(5.88769050874e-13)\n",
      " state (6)  A[0]:(0.0350549258292) A[1]:(0.964945077896) A[2]:(8.31193691653e-09) A[3]:(4.29674445505e-15)\n",
      " state (7)  A[0]:(0.00356451701373) A[1]:(0.996435463428) A[2]:(6.5274567973e-09) A[3]:(2.39204698343e-16)\n",
      " state (8)  A[0]:(4.89691701659e-05) A[1]:(0.999951004982) A[2]:(3.10663739178e-09) A[3]:(1.13447581129e-17)\n",
      " state (9)  A[0]:(9.60755414781e-07) A[1]:(0.999999046326) A[2]:(1.53156598515e-09) A[3]:(1.74596196537e-18)\n",
      " state (10)  A[0]:(1.37847308679e-07) A[1]:(0.999999880791) A[2]:(1.2002988603e-09) A[3]:(7.01068294472e-19)\n",
      " state (11)  A[0]:(5.93009623628e-08) A[1]:(0.999999940395) A[2]:(1.13399500901e-09) A[3]:(4.5447081292e-19)\n",
      " state (12)  A[0]:(4.13933740617e-08) A[1]:(0.999999940395) A[2]:(1.12003561981e-09) A[3]:(3.7365071758e-19)\n",
      " state (13)  A[0]:(3.55473197544e-08) A[1]:(0.999999940395) A[2]:(1.11669518077e-09) A[3]:(3.43153343877e-19)\n",
      " state (14)  A[0]:(3.33241345629e-08) A[1]:(0.999999940395) A[2]:(1.11571163419e-09) A[3]:(3.3085691633e-19)\n",
      " state (15)  A[0]:(3.24188427214e-08) A[1]:(0.999999940395) A[2]:(1.1153626911e-09) A[3]:(3.25728706725e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 984000 finished after 7 . Running score: 0.18. Policy_loss: -92050.6202241, Value_loss: 1.20921647774. Times trained:               12845. Times reached goal: 145.               Steps done: 13156932.\n",
      "action_dist \n",
      "tensor([[ 0.9983,  0.0003,  0.0005,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  1.0839e-07,  9.9478e-12,  4.2156e-09]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8298e-02,  9.8170e-01,  1.5271e-08,  1.2943e-15]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 6.5774e-04,  9.9934e-01,  8.5546e-09,  5.7292e-17]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8534e-02,  9.8147e-01,  1.5307e-08,  1.3143e-15]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.8640e-02,  9.8136e-01,  1.5323e-08,  1.3234e-15]])\n",
      "On state=8, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.998289585114) A[1]:(0.000325987581164) A[2]:(0.000467322737677) A[3]:(0.000917112105526)\n",
      " state (1)  A[0]:(7.83401992521e-06) A[1]:(1.06686016466e-07) A[2]:(1.47752061253e-07) A[3]:(0.999991893768)\n",
      " state (2)  A[0]:(0.99999910593) A[1]:(1.0338465728e-07) A[2]:(4.23934810867e-11) A[3]:(7.69471967033e-07)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(6.41901038989e-08) A[2]:(9.18651526377e-12) A[3]:(4.02768556285e-08)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(1.07765899315e-07) A[2]:(9.93359821483e-12) A[3]:(4.2497512176e-09)\n",
      " state (5)  A[0]:(0.999999761581) A[1]:(2.40604208557e-07) A[2]:(1.67782749499e-11) A[3]:(3.5816086208e-11)\n",
      " state (6)  A[0]:(0.998300433159) A[1]:(0.00169956986792) A[2]:(1.25367782822e-09) A[3]:(8.08764188433e-13)\n",
      " state (7)  A[0]:(0.0920137315989) A[1]:(0.907986223698) A[2]:(1.94186231539e-08) A[3]:(1.10575712811e-14)\n",
      " state (8)  A[0]:(0.0190078876913) A[1]:(0.980992078781) A[2]:(1.53788057844e-08) A[3]:(1.35449610343e-15)\n",
      " state (9)  A[0]:(0.000695545226336) A[1]:(0.9993044734) A[2]:(8.6714146974e-09) A[3]:(5.94314976438e-17)\n",
      " state (10)  A[0]:(1.5679617718e-05) A[1]:(0.999984323978) A[2]:(3.8177723205e-09) A[3]:(7.37018008503e-18)\n",
      " state (11)  A[0]:(1.29829334128e-06) A[1]:(0.999998688698) A[2]:(2.3769266555e-09) A[3]:(2.25925235088e-18)\n",
      " state (12)  A[0]:(3.64643739204e-07) A[1]:(0.999999642372) A[2]:(1.99263849865e-09) A[3]:(1.21173460458e-18)\n",
      " state (13)  A[0]:(1.95497960931e-07) A[1]:(0.999999821186) A[2]:(1.87608573121e-09) A[3]:(8.75186866583e-19)\n",
      " state (14)  A[0]:(1.4420011496e-07) A[1]:(0.999999880791) A[2]:(1.83534087927e-09) A[3]:(7.41807301531e-19)\n",
      " state (15)  A[0]:(1.2427933882e-07) A[1]:(0.999999880791) A[2]:(1.81913495378e-09) A[3]:(6.83103430832e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 985000 finished after 6 . Running score: 0.16. Policy_loss: -92050.6204764, Value_loss: 0.995061320774. Times trained:               12477. Times reached goal: 112.               Steps done: 13169409.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.994687438011) A[1]:(0.000517374603078) A[2]:(0.00120130868163) A[3]:(0.00359386811033)\n",
      " state (1)  A[0]:(9.56822896114e-06) A[1]:(9.98976332767e-08) A[2]:(1.23052544154e-07) A[3]:(0.999990224838)\n",
      " state (2)  A[0]:(0.99999922514) A[1]:(7.92290819618e-08) A[2]:(2.6234299455e-11) A[3]:(6.90545391535e-07)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(7.62253051789e-08) A[2]:(1.10175332399e-11) A[3]:(7.29620097673e-08)\n",
      " state (4)  A[0]:(0.999999821186) A[1]:(1.48477568018e-07) A[2]:(1.25746912882e-11) A[3]:(3.91702537073e-09)\n",
      " state (5)  A[0]:(0.999999523163) A[1]:(4.63872169121e-07) A[2]:(2.74926886812e-11) A[3]:(2.32322043547e-11)\n",
      " state (6)  A[0]:(0.95363765955) A[1]:(0.0463623516262) A[2]:(4.93581264749e-09) A[3]:(1.07685355858e-12)\n",
      " state (7)  A[0]:(0.0974272936583) A[1]:(0.902572691441) A[2]:(1.40328513254e-08) A[3]:(2.50715975623e-14)\n",
      " state (8)  A[0]:(0.0581295080483) A[1]:(0.941870510578) A[2]:(1.28160211332e-08) A[3]:(1.25352515978e-14)\n",
      " state (9)  A[0]:(0.0310179237276) A[1]:(0.968982040882) A[2]:(1.18273710825e-08) A[3]:(5.4455028201e-15)\n",
      " state (10)  A[0]:(0.0045968121849) A[1]:(0.995403170586) A[2]:(8.79707595658e-09) A[3]:(5.4590013489e-16)\n",
      " state (11)  A[0]:(0.000258798740106) A[1]:(0.999741196632) A[2]:(4.63536098394e-09) A[3]:(5.87304852358e-17)\n",
      " state (12)  A[0]:(1.51734629981e-05) A[1]:(0.999984800816) A[2]:(2.35277131111e-09) A[3]:(1.25442808433e-17)\n",
      " state (13)  A[0]:(2.27236728279e-06) A[1]:(0.999997735023) A[2]:(1.57494539632e-09) A[3]:(4.90633205426e-18)\n",
      " state (14)  A[0]:(7.71288171109e-07) A[1]:(0.99999922514) A[2]:(1.31133293113e-09) A[3]:(2.8475657432e-18)\n",
      " state (15)  A[0]:(4.26134477038e-07) A[1]:(0.999999582767) A[2]:(1.21082510685e-09) A[3]:(2.08512917757e-18)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 986000 finished after 11 . Running score: 0.13. Policy_loss: -92050.6364082, Value_loss: 1.41106230799. Times trained:               12814. Times reached goal: 110.               Steps done: 13182223.\n",
      " state (0)  A[0]:(0.99942779541) A[1]:(0.000151209445903) A[2]:(0.000283859699266) A[3]:(0.000137110677315)\n",
      " state (1)  A[0]:(0.000900300801732) A[1]:(1.16812236683e-06) A[2]:(7.88788781847e-07) A[3]:(0.999097764492)\n",
      " state (2)  A[0]:(0.999999761581) A[1]:(5.52227170658e-08) A[2]:(3.05427072522e-11) A[3]:(1.68823163449e-07)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(7.99560027076e-08) A[2]:(2.62564432002e-11) A[3]:(2.43238833519e-08)\n",
      " state (4)  A[0]:(0.999999821186) A[1]:(1.71466055576e-07) A[2]:(4.08021151588e-11) A[3]:(5.30876731464e-10)\n",
      " state (5)  A[0]:(0.999998033047) A[1]:(1.9670283109e-06) A[2]:(1.67317243394e-10) A[3]:(3.84619246174e-12)\n",
      " state (6)  A[0]:(0.111942358315) A[1]:(0.888057589531) A[2]:(4.05792270897e-08) A[3]:(1.68768909167e-14)\n",
      " state (7)  A[0]:(0.00572694372386) A[1]:(0.994273006916) A[2]:(2.13674216099e-08) A[3]:(6.83173364817e-16)\n",
      " state (8)  A[0]:(1.29463405756e-05) A[1]:(0.999987065792) A[2]:(4.53047910298e-09) A[3]:(9.05278373779e-18)\n",
      " state (9)  A[0]:(3.16947023293e-07) A[1]:(0.999999701977) A[2]:(1.99377914178e-09) A[3]:(1.37476290773e-18)\n",
      " state (10)  A[0]:(6.29427958643e-08) A[1]:(0.999999940395) A[2]:(1.67595526346e-09) A[3]:(5.4898746824e-19)\n",
      " state (11)  A[0]:(3.15576400567e-08) A[1]:(0.999999940395) A[2]:(1.63926905383e-09) A[3]:(3.54225880159e-19)\n",
      " state (12)  A[0]:(2.36535306897e-08) A[1]:(1.0) A[2]:(1.63925661933e-09) A[3]:(2.92401548147e-19)\n",
      " state (13)  A[0]:(2.09950741237e-08) A[1]:(1.0) A[2]:(1.64169089434e-09) A[3]:(2.69721522371e-19)\n",
      " state (14)  A[0]:(1.99841796444e-08) A[1]:(1.0) A[2]:(1.64305680173e-09) A[3]:(2.60800508712e-19)\n",
      " state (15)  A[0]:(1.95777740686e-08) A[1]:(1.0) A[2]:(1.6436774164e-09) A[3]:(2.57166911076e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 987000 finished after 13 . Running score: 0.15. Policy_loss: -92050.620209, Value_loss: 1.20724724558. Times trained:               13009. Times reached goal: 127.               Steps done: 13195232.\n",
      " state (0)  A[0]:(0.992952764034) A[1]:(0.00050708220806) A[2]:(0.0029958835803) A[3]:(0.00354426214471)\n",
      " state (1)  A[0]:(0.0870570912957) A[1]:(8.56797669258e-06) A[2]:(1.94039898815e-06) A[3]:(0.912932395935)\n",
      " state (2)  A[0]:(0.999999821186) A[1]:(3.55319969003e-08) A[2]:(2.42899086261e-11) A[3]:(1.31268137693e-07)\n",
      " state (3)  A[0]:(0.999999940395) A[1]:(5.78279042429e-08) A[2]:(2.71153464265e-11) A[3]:(1.45680152386e-08)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(1.29984542241e-07) A[2]:(5.60046072018e-11) A[3]:(9.48493089625e-11)\n",
      " state (5)  A[0]:(0.999125659466) A[1]:(0.000874325167388) A[2]:(3.23396198709e-09) A[3]:(1.33952733888e-12)\n",
      " state (6)  A[0]:(0.165540397167) A[1]:(0.834459543228) A[2]:(6.41576818339e-08) A[3]:(2.90869860478e-14)\n",
      " state (7)  A[0]:(0.0303537193686) A[1]:(0.969646215439) A[2]:(4.73930938938e-08) A[3]:(4.36174618694e-15)\n",
      " state (8)  A[0]:(0.000185988857993) A[1]:(0.999813973904) A[2]:(1.4826710526e-08) A[3]:(6.64466311304e-17)\n",
      " state (9)  A[0]:(1.34491244808e-06) A[1]:(0.999998629093) A[2]:(5.52940848664e-09) A[3]:(3.75210201189e-18)\n",
      " state (10)  A[0]:(9.88591111195e-08) A[1]:(0.999999880791) A[2]:(4.56920679071e-09) A[3]:(7.23764217625e-19)\n",
      " state (11)  A[0]:(2.97254949544e-08) A[1]:(0.999999940395) A[2]:(4.6562704803e-09) A[3]:(3.11729165996e-19)\n",
      " state (12)  A[0]:(1.7483653636e-08) A[1]:(1.0) A[2]:(4.78301132034e-09) A[3]:(2.11538930554e-19)\n",
      " state (13)  A[0]:(1.38538363004e-08) A[1]:(1.0) A[2]:(4.85392392946e-09) A[3]:(1.78034445131e-19)\n",
      " state (14)  A[0]:(1.25027748155e-08) A[1]:(1.0) A[2]:(4.88779683394e-09) A[3]:(1.6493972367e-19)\n",
      " state (15)  A[0]:(1.19446212921e-08) A[1]:(1.0) A[2]:(4.90327822789e-09) A[3]:(1.59419895744e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 988000 finished after 11 . Running score: 0.16. Policy_loss: -92050.6202223, Value_loss: 1.84687272491. Times trained:               12447. Times reached goal: 118.               Steps done: 13207679.\n",
      " state (0)  A[0]:(0.998663604259) A[1]:(0.00024391716579) A[2]:(0.000666469044518) A[3]:(0.000426022248575)\n",
      " state (1)  A[0]:(0.98395639658) A[1]:(6.11027780906e-06) A[2]:(1.7144201081e-07) A[3]:(0.0160373505205)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(4.85605546885e-08) A[2]:(2.22443157644e-11) A[3]:(7.90624810065e-08)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(9.22537424231e-08) A[2]:(2.69031012745e-11) A[3]:(4.59781190898e-09)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(2.80568201561e-07) A[2]:(7.38498290298e-11) A[3]:(1.83866741277e-11)\n",
      " state (5)  A[0]:(0.778370380402) A[1]:(0.221629559994) A[2]:(3.70421453511e-08) A[3]:(3.17385413741e-13)\n",
      " state (6)  A[0]:(0.110542394221) A[1]:(0.889457583427) A[2]:(4.71822012571e-08) A[3]:(1.5508858168e-14)\n",
      " state (7)  A[0]:(0.041690889746) A[1]:(0.958309054375) A[2]:(3.95315424839e-08) A[3]:(5.50865336148e-15)\n",
      " state (8)  A[0]:(0.00081890146248) A[1]:(0.999181091785) A[2]:(1.69730540733e-08) A[3]:(1.53882513701e-16)\n",
      " state (9)  A[0]:(3.64350739801e-06) A[1]:(0.999996364117) A[2]:(5.17162979108e-09) A[3]:(5.22096302155e-18)\n",
      " state (10)  A[0]:(1.21582544921e-07) A[1]:(0.999999880791) A[2]:(3.49975270986e-09) A[3]:(6.51682613373e-19)\n",
      " state (11)  A[0]:(2.33683294937e-08) A[1]:(1.0) A[2]:(3.44957129528e-09) A[3]:(2.10070542795e-19)\n",
      " state (12)  A[0]:(1.10342917026e-08) A[1]:(1.0) A[2]:(3.55261575713e-09) A[3]:(1.21923191116e-19)\n",
      " state (13)  A[0]:(7.88182230593e-09) A[1]:(1.0) A[2]:(3.62162100309e-09) A[3]:(9.50763851761e-20)\n",
      " state (14)  A[0]:(6.77405864735e-09) A[1]:(1.0) A[2]:(3.65686858572e-09) A[3]:(8.49425696423e-20)\n",
      " state (15)  A[0]:(6.32373486908e-09) A[1]:(1.0) A[2]:(3.67359076492e-09) A[3]:(8.06961281078e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 989000 finished after 5 . Running score: 0.14. Policy_loss: -92050.620166, Value_loss: 1.04319359684. Times trained:               12486. Times reached goal: 125.               Steps done: 13220165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_dist \n",
      "tensor([[ 0.9945,  0.0006,  0.0041,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9945,  0.0006,  0.0041,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9944,  0.0006,  0.0041,  0.0009]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0982e-07,  1.9200e-10,  1.5484e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0984e-07,  1.9214e-10,  1.5486e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  3.0985e-07,  1.9226e-10,  1.5488e-11]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.1191e-05,  9.9993e-01,  2.0935e-08,  2.5257e-17]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 5.5460e-07,  1.0000e+00,  8.4504e-09,  1.4750e-18]])\n",
      "On state=9, selected action=1\n",
      "new state=13, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.4056e-09,  1.0000e+00,  7.7107e-09,  7.9282e-20]])\n",
      "On state=13, selected action=1\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.994320213795) A[1]:(0.000586663198192) A[2]:(0.00420215725899) A[3]:(0.000890971743502)\n",
      " state (1)  A[0]:(0.978908956051) A[1]:(9.2434202088e-06) A[2]:(6.47427214062e-07) A[3]:(0.0210811682045)\n",
      " state (2)  A[0]:(0.999999880791) A[1]:(6.13422912465e-08) A[2]:(6.29210919478e-11) A[3]:(4.899802164e-08)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(1.13685480585e-07) A[2]:(7.72061373167e-11) A[3]:(2.67985500457e-09)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(3.09902702611e-07) A[2]:(1.9264227713e-10) A[3]:(1.54940608954e-11)\n",
      " state (5)  A[0]:(0.789695203304) A[1]:(0.210304692388) A[2]:(7.92671315253e-08) A[3]:(2.79142946822e-13)\n",
      " state (6)  A[0]:(0.0890137404203) A[1]:(0.910986185074) A[2]:(9.9045202262e-08) A[3]:(1.08749196375e-14)\n",
      " state (7)  A[0]:(0.0138309076428) A[1]:(0.986169040203) A[2]:(7.06525042915e-08) A[3]:(1.50267880008e-15)\n",
      " state (8)  A[0]:(7.12035835022e-05) A[1]:(0.999928772449) A[2]:(2.09580530708e-08) A[3]:(2.52663343808e-17)\n",
      " state (9)  A[0]:(5.5464460047e-07) A[1]:(0.999999463558) A[2]:(8.45602166066e-09) A[3]:(1.47531001991e-18)\n",
      " state (10)  A[0]:(4.5100712498e-08) A[1]:(0.999999940395) A[2]:(7.28098958902e-09) A[3]:(2.90942866835e-19)\n",
      " state (11)  A[0]:(1.47032359621e-08) A[1]:(1.0) A[2]:(7.44101180672e-09) A[3]:(1.31370059e-19)\n",
      " state (12)  A[0]:(9.09194675103e-09) A[1]:(1.0) A[2]:(7.61903962143e-09) A[3]:(9.23257188337e-20)\n",
      " state (13)  A[0]:(7.40558636636e-09) A[1]:(1.0) A[2]:(7.71326291726e-09) A[3]:(7.92885251992e-20)\n",
      " state (14)  A[0]:(6.78088429851e-09) A[1]:(1.0) A[2]:(7.75666642028e-09) A[3]:(7.42528951992e-20)\n",
      " state (15)  A[0]:(6.52688658676e-09) A[1]:(1.0) A[2]:(7.77592390477e-09) A[3]:(7.21704421577e-20)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 990000 finished after 9 . Running score: 0.13. Policy_loss: -92050.6202128, Value_loss: 1.41269193319. Times trained:               12722. Times reached goal: 119.               Steps done: 13232887.\n",
      " state (0)  A[0]:(0.992895841599) A[1]:(0.00085225654766) A[2]:(0.00401723198593) A[3]:(0.0022346645128)\n",
      " state (1)  A[0]:(0.00321674346924) A[1]:(2.08546452996e-06) A[2]:(1.48330173033e-06) A[3]:(0.996779680252)\n",
      " state (2)  A[0]:(0.999999701977) A[1]:(5.18721918752e-08) A[2]:(4.59892887661e-11) A[3]:(2.59506919065e-07)\n",
      " state (3)  A[0]:(0.999999880791) A[1]:(7.71868187144e-08) A[2]:(4.2392870464e-11) A[3]:(4.17965573263e-08)\n",
      " state (4)  A[0]:(0.999999821186) A[1]:(1.76484334702e-07) A[2]:(7.5754715001e-11) A[3]:(5.05809172324e-10)\n",
      " state (5)  A[0]:(0.999842464924) A[1]:(0.000157527552801) A[2]:(1.60896596046e-09) A[3]:(4.02609109246e-12)\n",
      " state (6)  A[0]:(0.156883388758) A[1]:(0.84311658144) A[2]:(5.67291280618e-08) A[3]:(3.64345191387e-14)\n",
      " state (7)  A[0]:(0.0589104890823) A[1]:(0.941089451313) A[2]:(4.61030857934e-08) A[3]:(1.17802513214e-14)\n",
      " state (8)  A[0]:(0.00178003951441) A[1]:(0.998219966888) A[2]:(1.97008613867e-08) A[3]:(4.05320829492e-16)\n",
      " state (9)  A[0]:(8.01790247351e-06) A[1]:(0.999991953373) A[2]:(4.86543427769e-09) A[3]:(1.16700915566e-17)\n",
      " state (10)  A[0]:(2.0544985091e-07) A[1]:(0.999999821186) A[2]:(2.65764121821e-09) A[3]:(1.21359906968e-18)\n",
      " state (11)  A[0]:(3.3680954914e-08) A[1]:(0.999999940395) A[2]:(2.38249420192e-09) A[3]:(3.50949443602e-19)\n",
      " state (12)  A[0]:(1.47655700999e-08) A[1]:(1.0) A[2]:(2.35953923067e-09) A[3]:(1.93263408895e-19)\n",
      " state (13)  A[0]:(1.01988248957e-08) A[1]:(1.0) A[2]:(2.3651083314e-09) A[3]:(1.47142635523e-19)\n",
      " state (14)  A[0]:(8.63244942195e-09) A[1]:(1.0) A[2]:(2.37023844996e-09) A[3]:(1.30033348055e-19)\n",
      " state (15)  A[0]:(8.00221844344e-09) A[1]:(1.0) A[2]:(2.3730248877e-09) A[3]:(1.22914140565e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 991000 finished after 15 . Running score: 0.16. Policy_loss: -92050.6201757, Value_loss: 1.83721186408. Times trained:               12356. Times reached goal: 134.               Steps done: 13245243.\n",
      " state (0)  A[0]:(0.996230065823) A[1]:(0.000538038322702) A[2]:(0.00149468926247) A[3]:(0.00173718004953)\n",
      " state (1)  A[0]:(0.00181884446647) A[1]:(1.18910463698e-06) A[2]:(5.43872090475e-07) A[3]:(0.99817943573)\n",
      " state (2)  A[0]:(0.999999165535) A[1]:(5.20938598925e-08) A[2]:(2.60875973601e-11) A[3]:(7.56641384214e-07)\n",
      " state (3)  A[0]:(0.999999821186) A[1]:(8.39159994825e-08) A[2]:(2.1149134527e-11) A[3]:(1.17861169713e-07)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(3.05272749301e-07) A[2]:(5.68104244203e-11) A[3]:(4.24198981497e-10)\n",
      " state (5)  A[0]:(0.657936871052) A[1]:(0.342063069344) A[2]:(2.12779500686e-08) A[3]:(8.87529904171e-13)\n",
      " state (6)  A[0]:(0.0587752871215) A[1]:(0.941224694252) A[2]:(1.70516969433e-08) A[3]:(2.84243013512e-14)\n",
      " state (7)  A[0]:(0.00102961459197) A[1]:(0.998970389366) A[2]:(5.48642686837e-09) A[3]:(6.31746659397e-16)\n",
      " state (8)  A[0]:(1.80235667813e-06) A[1]:(0.999998211861) A[2]:(8.70033323164e-10) A[3]:(1.12701091985e-17)\n",
      " state (9)  A[0]:(4.24902388829e-08) A[1]:(0.999999940395) A[2]:(4.71635119847e-10) A[3]:(1.04954961207e-18)\n",
      " state (10)  A[0]:(8.41117842043e-09) A[1]:(1.0) A[2]:(4.32435337538e-10) A[3]:(3.30429367351e-19)\n",
      " state (11)  A[0]:(4.40411529468e-09) A[1]:(1.0) A[2]:(4.28192065138e-10) A[3]:(2.04330969169e-19)\n",
      " state (12)  A[0]:(3.41074857246e-09) A[1]:(1.0) A[2]:(4.27796964519e-10) A[3]:(1.68650959961e-19)\n",
      " state (13)  A[0]:(3.08100189805e-09) A[1]:(1.0) A[2]:(4.27806734482e-10) A[3]:(1.56226151399e-19)\n",
      " state (14)  A[0]:(2.95767432767e-09) A[1]:(1.0) A[2]:(4.27823054761e-10) A[3]:(1.51494678295e-19)\n",
      " state (15)  A[0]:(2.90911073009e-09) A[1]:(1.0) A[2]:(4.278312149e-10) A[3]:(1.49620093156e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 992000 finished after 8 . Running score: 0.14. Policy_loss: -92050.6202111, Value_loss: 1.21678117526. Times trained:               12808. Times reached goal: 125.               Steps done: 13258051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.999682128429) A[1]:(6.73698086757e-05) A[2]:(0.000103676240542) A[3]:(0.000146832942846)\n",
      " state (1)  A[0]:(3.23148706229e-05) A[1]:(8.62526050582e-08) A[2]:(1.28781991293e-07) A[3]:(0.999967455864)\n",
      " state (2)  A[0]:(0.999997735023) A[1]:(3.85180740636e-08) A[2]:(2.80932291319e-11) A[3]:(2.22650396609e-06)\n",
      " state (3)  A[0]:(0.999999403954) A[1]:(4.64713174608e-08) A[2]:(1.73543748816e-11) A[3]:(5.72473595639e-07)\n",
      " state (4)  A[0]:(0.999999880791) A[1]:(1.17638386143e-07) A[2]:(2.45130322291e-11) A[3]:(1.38532003646e-08)\n",
      " state (5)  A[0]:(0.99996984005) A[1]:(3.01875570585e-05) A[2]:(3.41203620913e-10) A[3]:(2.87223491519e-11)\n",
      " state (6)  A[0]:(0.163145214319) A[1]:(0.836854755878) A[2]:(1.92621154582e-08) A[3]:(1.6240035304e-13)\n",
      " state (7)  A[0]:(0.0379007682204) A[1]:(0.962099194527) A[2]:(1.33826674187e-08) A[3]:(3.23950562827e-14)\n",
      " state (8)  A[0]:(0.000293912307825) A[1]:(0.999706089497) A[2]:(2.90158475025e-09) A[3]:(5.06433164109e-16)\n",
      " state (9)  A[0]:(1.58376303716e-06) A[1]:(0.999998390675) A[2]:(6.1246735461e-10) A[3]:(2.1366770054e-17)\n",
      " state (10)  A[0]:(8.89615350275e-08) A[1]:(0.999999940395) A[2]:(3.71673580801e-10) A[3]:(3.37466322069e-18)\n",
      " state (11)  A[0]:(2.38692585697e-08) A[1]:(1.0) A[2]:(3.33550104203e-10) A[3]:(1.33067821358e-18)\n",
      " state (12)  A[0]:(1.34902347071e-08) A[1]:(1.0) A[2]:(3.24824417364e-10) A[3]:(8.74980174828e-19)\n",
      " state (13)  A[0]:(1.05575104214e-08) A[1]:(1.0) A[2]:(3.22152776677e-10) A[3]:(7.29120935874e-19)\n",
      " state (14)  A[0]:(9.49823064644e-09) A[1]:(1.0) A[2]:(3.21166260253e-10) A[3]:(6.73755824621e-19)\n",
      " state (15)  A[0]:(9.07178421272e-09) A[1]:(1.0) A[2]:(3.20760973338e-10) A[3]:(6.51009340053e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 993000 finished after 8 . Running score: 0.08. Policy_loss: -92050.6202114, Value_loss: 1.00294644039. Times trained:               12876. Times reached goal: 106.               Steps done: 13270927.\n",
      " state (0)  A[0]:(0.995689272881) A[1]:(0.000418746087234) A[2]:(0.000576503924094) A[3]:(0.003315476235)\n",
      " state (1)  A[0]:(1.37587761628e-07) A[1]:(5.34222888149e-09) A[2]:(3.85893379473e-08) A[3]:(0.999999821186)\n",
      " state (2)  A[0]:(0.999786496162) A[1]:(2.5560461836e-07) A[2]:(9.96952187471e-10) A[3]:(0.000213220031583)\n",
      " state (3)  A[0]:(0.999998033047) A[1]:(3.38724888138e-08) A[2]:(1.9087747008e-11) A[3]:(1.93809910343e-06)\n",
      " state (4)  A[0]:(0.999999403954) A[1]:(4.73138257462e-08) A[2]:(1.69199931843e-11) A[3]:(5.45348257219e-07)\n",
      " state (5)  A[0]:(0.999999880791) A[1]:(1.25023987607e-07) A[2]:(3.30855724762e-11) A[3]:(7.29597537941e-09)\n",
      " state (6)  A[0]:(0.998253285885) A[1]:(0.00174672121648) A[2]:(2.4646176211e-09) A[3]:(1.13733284943e-11)\n",
      " state (7)  A[0]:(0.063535451889) A[1]:(0.936464548111) A[2]:(1.53462060837e-08) A[3]:(9.83442453449e-14)\n",
      " state (8)  A[0]:(2.7381509426e-05) A[1]:(0.999972641468) A[2]:(1.12118925255e-09) A[3]:(1.93542462411e-16)\n",
      " state (9)  A[0]:(1.39958274303e-07) A[1]:(0.999999880791) A[2]:(3.16207504625e-10) A[3]:(6.47569261321e-18)\n",
      " state (10)  A[0]:(1.56230601789e-08) A[1]:(1.0) A[2]:(2.52610904417e-10) A[3]:(1.33467411633e-18)\n",
      " state (11)  A[0]:(6.61744081754e-09) A[1]:(1.0) A[2]:(2.40470837687e-10) A[3]:(7.01132556066e-19)\n",
      " state (12)  A[0]:(4.71248773337e-09) A[1]:(1.0) A[2]:(2.36732772274e-10) A[3]:(5.4273651605e-19)\n",
      " state (13)  A[0]:(4.10947365026e-09) A[1]:(1.0) A[2]:(2.35329061793e-10) A[3]:(4.89475028492e-19)\n",
      " state (14)  A[0]:(3.88518017758e-09) A[1]:(1.0) A[2]:(2.34760599849e-10) A[3]:(4.69197884847e-19)\n",
      " state (15)  A[0]:(3.79565756603e-09) A[1]:(1.0) A[2]:(2.34522512521e-10) A[3]:(4.61036403936e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 994000 finished after 19 . Running score: 0.08. Policy_loss: -92050.6202156, Value_loss: 1.20510210297. Times trained:               12775. Times reached goal: 126.               Steps done: 13283702.\n",
      "action_dist \n",
      "tensor([[ 0.9922,  0.0007,  0.0021,  0.0050]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9921,  0.0007,  0.0022,  0.0050]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.0667e-08,  6.8943e-11,  6.1059e-08]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.0665e-08,  6.8943e-11,  6.1098e-08]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9921,  0.0007,  0.0022,  0.0051]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.0658e-08,  6.8944e-11,  6.1173e-08]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0007,  0.0022,  0.0051]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0007,  0.0022,  0.0051]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0007,  0.0022,  0.0051]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9920,  0.0007,  0.0022,  0.0051]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.0643e-08,  6.8949e-11,  6.1402e-08]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.0640e-08,  6.8950e-11,  6.1452e-08]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9919,  0.0007,  0.0022,  0.0052]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.0635e-08,  6.8952e-11,  6.1549e-08]])\n",
      "On state=4, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.0632e-08,  6.8953e-11,  6.1596e-08]])\n",
      "On state=4, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9918,  0.0007,  0.0022,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=0, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 0.9918,  0.0007,  0.0022,  0.0053]])\n",
      "On state=0, selected action=0\n",
      "new state=4, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 1.0000e+00,  9.0624e-08,  6.8955e-11,  6.1737e-08]])\n",
      "On state=4, selected action=0\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.2551e-05,  9.9993e-01,  4.2528e-09,  2.8488e-16]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.7338e-07,  1.0000e+00,  9.1432e-10,  1.2627e-17]])\n",
      "On state=9, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.2489e-05,  9.9993e-01,  4.2523e-09,  2.8490e-16]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.2463e-05,  9.9993e-01,  4.2521e-09,  2.8492e-16]])\n",
      "On state=8, selected action=1\n",
      "new state=8, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 7.2441e-05,  9.9993e-01,  4.2519e-09,  2.8493e-16]])\n",
      "On state=8, selected action=1\n",
      "new state=9, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.7262e-07,  1.0000e+00,  9.1427e-10,  1.2626e-17]])\n",
      "On state=9, selected action=1\n",
      "new state=10, done=False. Reward: 0.0\n",
      "action_dist \n",
      "tensor([[ 4.2581e-08,  1.0000e+00,  6.1128e-10,  2.4226e-18]])\n",
      "On state=10, selected action=1\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.991582095623) A[1]:(0.000719835050404) A[2]:(0.00228106044233) A[3]:(0.0054170251824)\n",
      " state (1)  A[0]:(9.48157719449e-07) A[1]:(1.62243374291e-08) A[2]:(1.28777202235e-07) A[3]:(0.999998927116)\n",
      " state (2)  A[0]:(0.999993681908) A[1]:(6.10568591242e-08) A[2]:(1.68371677711e-10) A[3]:(6.23626556262e-06)\n",
      " state (3)  A[0]:(0.999998986721) A[1]:(4.47585719598e-08) A[2]:(5.35298992632e-11) A[3]:(9.82736423794e-07)\n",
      " state (4)  A[0]:(0.999999821186) A[1]:(9.0622755522e-08) A[2]:(6.8967560829e-11) A[3]:(6.19712068328e-08)\n",
      " state (5)  A[0]:(0.999999344349) A[1]:(6.25366453733e-07) A[2]:(3.26285887198e-10) A[3]:(1.89661356065e-10)\n",
      " state (6)  A[0]:(0.318498373032) A[1]:(0.681501567364) A[2]:(7.42165724432e-08) A[3]:(5.14032826721e-13)\n",
      " state (7)  A[0]:(0.0349605903029) A[1]:(0.965039372444) A[2]:(4.01629023372e-08) A[3]:(4.14105056669e-14)\n",
      " state (8)  A[0]:(7.2269634984e-05) A[1]:(0.999927699566) A[2]:(4.24868229487e-09) A[3]:(2.846724211e-16)\n",
      " state (9)  A[0]:(4.72022634312e-07) A[1]:(0.999999523163) A[2]:(9.14100739546e-10) A[3]:(1.26198214584e-17)\n",
      " state (10)  A[0]:(4.25616164534e-08) A[1]:(0.999999940395) A[2]:(6.11282524599e-10) A[3]:(2.42211697566e-18)\n",
      " state (11)  A[0]:(1.53433408201e-08) A[1]:(1.0) A[2]:(5.52532297693e-10) A[3]:(1.14560327237e-18)\n",
      " state (12)  A[0]:(1.00803125846e-08) A[1]:(1.0) A[2]:(5.35028299442e-10) A[3]:(8.36550552641e-19)\n",
      " state (13)  A[0]:(8.4772775466e-09) A[1]:(1.0) A[2]:(5.28645793807e-10) A[3]:(7.34346287503e-19)\n",
      " state (14)  A[0]:(7.88868081969e-09) A[1]:(1.0) A[2]:(5.26106935794e-10) A[3]:(6.9560202649e-19)\n",
      " state (15)  A[0]:(7.65443441963e-09) A[1]:(1.0) A[2]:(5.25056331746e-10) A[3]:(6.7999147397e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 995000 finished after 25 . Running score: 0.14. Policy_loss: -92050.6202155, Value_loss: 1.61742528989. Times trained:               12434. Times reached goal: 122.               Steps done: 13296136.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.995831131935) A[1]:(0.00118376396131) A[2]:(0.00115063949488) A[3]:(0.00183448498137)\n",
      " state (1)  A[0]:(0.000243876522291) A[1]:(4.4161785695e-07) A[2]:(5.80757046009e-07) A[3]:(0.999755084515)\n",
      " state (2)  A[0]:(0.999998450279) A[1]:(6.21583993166e-08) A[2]:(8.39609007319e-11) A[3]:(1.51455003561e-06)\n",
      " state (3)  A[0]:(0.999999642372) A[1]:(8.71267715752e-08) A[2]:(5.71972677554e-11) A[3]:(2.95026154618e-07)\n",
      " state (4)  A[0]:(0.999999701977) A[1]:(2.87658110665e-07) A[2]:(1.45453413203e-10) A[3]:(2.33716090925e-09)\n",
      " state (5)  A[0]:(0.998595297337) A[1]:(0.00140467507299) A[2]:(4.35113767239e-09) A[3]:(8.28039980333e-12)\n",
      " state (6)  A[0]:(0.0996227115393) A[1]:(0.900377213955) A[2]:(5.03160215715e-08) A[3]:(8.61973320954e-14)\n",
      " state (7)  A[0]:(0.0154807306826) A[1]:(0.98451924324) A[2]:(2.81887775344e-08) A[3]:(1.25718485034e-14)\n",
      " state (8)  A[0]:(7.17554285075e-05) A[1]:(0.999928236008) A[2]:(3.40713590674e-09) A[3]:(1.77886170111e-16)\n",
      " state (9)  A[0]:(6.61860440232e-07) A[1]:(0.999999344349) A[2]:(6.54680254453e-10) A[3]:(1.05268915541e-17)\n",
      " state (10)  A[0]:(6.15081887645e-08) A[1]:(0.999999940395) A[2]:(3.86196408186e-10) A[3]:(2.18928569235e-18)\n",
      " state (11)  A[0]:(2.1844835274e-08) A[1]:(1.0) A[2]:(3.32386618229e-10) A[3]:(1.04348958351e-18)\n",
      " state (12)  A[0]:(1.42172540407e-08) A[1]:(1.0) A[2]:(3.1621838481e-10) A[3]:(7.6057008427e-19)\n",
      " state (13)  A[0]:(1.19222693939e-08) A[1]:(1.0) A[2]:(3.10381470525e-10) A[3]:(6.67297250097e-19)\n",
      " state (14)  A[0]:(1.10930828967e-08) A[1]:(1.0) A[2]:(3.08112951819e-10) A[3]:(6.32391986006e-19)\n",
      " state (15)  A[0]:(1.07693525209e-08) A[1]:(1.0) A[2]:(3.07194020222e-10) A[3]:(6.18577397692e-19)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 996000 finished after 14 . Running score: 0.17. Policy_loss: -92050.620445, Value_loss: 1.41474044313. Times trained:               12948. Times reached goal: 138.               Steps done: 13309084.\n",
      " state (0)  A[0]:(0.998451948166) A[1]:(0.000646808301099) A[2]:(0.00029265138437) A[3]:(0.00060859444784)\n",
      " state (1)  A[0]:(0.00666615460068) A[1]:(2.17579395212e-06) A[2]:(7.62581862546e-07) A[3]:(0.993330895901)\n",
      " state (2)  A[0]:(0.99999755621) A[1]:(6.92091859378e-08) A[2]:(6.43850667226e-11) A[3]:(2.36614391724e-06)\n",
      " state (3)  A[0]:(0.999999403954) A[1]:(1.02577793371e-07) A[2]:(4.19373355487e-11) A[3]:(4.77429978218e-07)\n",
      " state (4)  A[0]:(0.999999582767) A[1]:(4.10982124777e-07) A[2]:(1.29127819548e-10) A[3]:(2.75820077889e-09)\n",
      " state (5)  A[0]:(0.98971170187) A[1]:(0.010288300924) A[2]:(7.48745332402e-09) A[3]:(1.19369965301e-11)\n",
      " state (6)  A[0]:(0.0837849006057) A[1]:(0.916215062141) A[2]:(3.95661956532e-08) A[3]:(1.3580574111e-13)\n",
      " state (7)  A[0]:(0.0206454675645) A[1]:(0.979354500771) A[2]:(2.60876849012e-08) A[3]:(3.03631021692e-14)\n",
      " state (8)  A[0]:(0.000204110881896) A[1]:(0.999795913696) A[2]:(4.08442923927e-09) A[3]:(5.80009886978e-16)\n",
      " state (9)  A[0]:(1.50448454406e-06) A[1]:(0.999998509884) A[2]:(5.90511473053e-10) A[3]:(2.9288814438e-17)\n",
      " state (10)  A[0]:(9.63423829603e-08) A[1]:(0.999999880791) A[2]:(2.78591594238e-10) A[3]:(5.06166168405e-18)\n",
      " state (11)  A[0]:(2.75789826532e-08) A[1]:(1.0) A[2]:(2.21950832713e-10) A[3]:(2.10663690748e-18)\n",
      " state (12)  A[0]:(1.61514268626e-08) A[1]:(1.0) A[2]:(2.05496605843e-10) A[3]:(1.42738607035e-18)\n",
      " state (13)  A[0]:(1.2897933388e-08) A[1]:(1.0) A[2]:(1.99576480342e-10) A[3]:(1.20934756813e-18)\n",
      " state (14)  A[0]:(1.17364473695e-08) A[1]:(1.0) A[2]:(1.97240807021e-10) A[3]:(1.12776491566e-18)\n",
      " state (15)  A[0]:(1.12798579366e-08) A[1]:(1.0) A[2]:(1.9628230985e-10) A[3]:(1.09508724896e-18)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 997000 finished after 7 . Running score: 0.12. Policy_loss: -92050.6202391, Value_loss: 0.988578988578. Times trained:               12595. Times reached goal: 129.               Steps done: 13321679.\n",
      " state (0)  A[0]:(0.993360638618) A[1]:(0.00128043675795) A[2]:(0.000493818079121) A[3]:(0.0048650931567)\n",
      " state (1)  A[0]:(0.0554552339017) A[1]:(5.04812896907e-06) A[2]:(4.84624308683e-07) A[3]:(0.944539248943)\n",
      " state (2)  A[0]:(0.99999320507) A[1]:(9.98126026275e-08) A[2]:(4.79289143096e-11) A[3]:(6.71774432703e-06)\n",
      " state (3)  A[0]:(0.999998390675) A[1]:(1.57197092676e-07) A[2]:(2.49690251114e-11) A[3]:(1.47504738379e-06)\n",
      " state (4)  A[0]:(0.999999165535) A[1]:(8.52774689974e-07) A[2]:(8.5959579732e-11) A[3]:(1.12615321513e-08)\n",
      " state (5)  A[0]:(0.44187054038) A[1]:(0.558129429817) A[2]:(3.50995570386e-08) A[3]:(4.887881766e-12)\n",
      " state (6)  A[0]:(0.00788857974112) A[1]:(0.992111384869) A[2]:(1.09199689291e-08) A[3]:(4.40663910255e-14)\n",
      " state (7)  A[0]:(3.80556571145e-06) A[1]:(0.999996185303) A[2]:(4.44955544632e-10) A[3]:(1.61783689972e-16)\n",
      " state (8)  A[0]:(3.33545315812e-08) A[1]:(0.999999940395) A[2]:(9.80264203143e-11) A[3]:(8.93917544374e-18)\n",
      " state (9)  A[0]:(5.02204589026e-09) A[1]:(1.0) A[2]:(7.14721407058e-11) A[3]:(2.34074659842e-18)\n",
      " state (10)  A[0]:(2.54113952103e-09) A[1]:(1.0) A[2]:(6.62059018719e-11) A[3]:(1.40824562796e-18)\n",
      " state (11)  A[0]:(2.00267535888e-09) A[1]:(1.0) A[2]:(6.47122216924e-11) A[3]:(1.17676337284e-18)\n",
      " state (12)  A[0]:(1.84317117125e-09) A[1]:(1.0) A[2]:(6.42365605152e-11) A[3]:(1.10540363877e-18)\n",
      " state (13)  A[0]:(1.79058368133e-09) A[1]:(1.0) A[2]:(6.40823644149e-11) A[3]:(1.08169757299e-18)\n",
      " state (14)  A[0]:(1.77264158907e-09) A[1]:(1.0) A[2]:(6.40342293079e-11) A[3]:(1.07367691958e-18)\n",
      " state (15)  A[0]:(1.76647851902e-09) A[1]:(1.0) A[2]:(6.40207956093e-11) A[3]:(1.07097307295e-18)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 998000 finished after 25 . Running score: 0.13. Policy_loss: -92050.6202156, Value_loss: 1.00247983491. Times trained:               12596. Times reached goal: 127.               Steps done: 13334275.\n",
      " state (0)  A[0]:(0.991778373718) A[1]:(0.000925754022319) A[2]:(0.000580543244723) A[3]:(0.00671531073749)\n",
      " state (1)  A[0]:(0.000492657127324) A[1]:(3.61538127436e-07) A[2]:(1.63875611747e-07) A[3]:(0.999506831169)\n",
      " state (2)  A[0]:(0.999990582466) A[1]:(8.17100200834e-08) A[2]:(4.91335687414e-11) A[3]:(9.34836771194e-06)\n",
      " state (3)  A[0]:(0.999996960163) A[1]:(1.11110530554e-07) A[2]:(2.07453949852e-11) A[3]:(2.91087121695e-06)\n",
      " state (4)  A[0]:(0.999999523163) A[1]:(3.06534133188e-07) A[2]:(3.10176433149e-11) A[3]:(1.81574591807e-07)\n",
      " state (5)  A[0]:(0.999638617039) A[1]:(0.000361383630661) A[2]:(1.20952503568e-09) A[3]:(1.17603329852e-10)\n",
      " state (6)  A[0]:(0.0205448232591) A[1]:(0.979455173016) A[2]:(1.24194547979e-08) A[3]:(1.51703451775e-13)\n",
      " state (7)  A[0]:(5.27880274603e-06) A[1]:(0.999994695187) A[2]:(3.55868529089e-10) A[3]:(2.83988627284e-16)\n",
      " state (8)  A[0]:(3.67760222275e-08) A[1]:(0.999999940395) A[2]:(6.18131656971e-11) A[3]:(1.47781788677e-17)\n",
      " state (9)  A[0]:(5.41223821315e-09) A[1]:(1.0) A[2]:(4.3789988996e-11) A[3]:(3.89246091336e-18)\n",
      " state (10)  A[0]:(2.69272604214e-09) A[1]:(1.0) A[2]:(4.05230710099e-11) A[3]:(2.30743003809e-18)\n",
      " state (11)  A[0]:(2.10405248779e-09) A[1]:(1.0) A[2]:(3.96364850663e-11) A[3]:(1.91148225351e-18)\n",
      " state (12)  A[0]:(1.9290200548e-09) A[1]:(1.0) A[2]:(3.93569968282e-11) A[3]:(1.78859568132e-18)\n",
      " state (13)  A[0]:(1.87084769898e-09) A[1]:(1.0) A[2]:(3.92649211756e-11) A[3]:(1.7473429434e-18)\n",
      " state (14)  A[0]:(1.85072401848e-09) A[1]:(1.0) A[2]:(3.92346780065e-11) A[3]:(1.73311678103e-18)\n",
      " state (15)  A[0]:(1.84367754397e-09) A[1]:(1.0) A[2]:(3.92255498916e-11) A[3]:(1.7282116e-18)\n",
      " state (0)  A[0]:(0.012915879488)\n",
      " state (1)  A[0]:(0.0123670790344)\n",
      " state (2)  A[0]:(0.0124307777733)\n",
      " state (3)  A[0]:(0.012569444254)\n",
      " state (4)  A[0]:(0.012864658609)\n",
      " state (5)  A[0]:(0.0134701943025)\n",
      " state (6)  A[0]:(0.0146606592461)\n",
      " state (7)  A[0]:(0.0169144216925)\n",
      " state (8)  A[0]:(0.021057613194)\n",
      " state (9)  A[0]:(0.0284605715424)\n",
      " state (10)  A[0]:(0.0411117263138)\n",
      " state (11)  A[0]:(0.0611295998096)\n",
      " state (12)  A[0]:(0.0894578769803)\n",
      " state (13)  A[0]:(0.124634884298)\n",
      " state (14)  A[0]:(0.163109332323)\n",
      " state (15)  A[0]:(0.200943216681)\n",
      "Episode 999000 finished after 19 . Running score: 0.09. Policy_loss: -92050.6202165, Value_loss: 1.41345497787. Times trained:               12853. Times reached goal: 130.               Steps done: 13347128.\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "steps_done = 0\n",
    "\n",
    "policy_loss_avg = [1.0]\n",
    "v_loss_avg = [1.0]\n",
    "\n",
    "\n",
    "TARGET_UPDATE = 1000\n",
    "\n",
    "\n",
    "\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    reward = 0\n",
    "    episode_step = 0\n",
    "    #print(\"b\")\n",
    "    I = 1.0\n",
    "    #entropy_impact = (ENTROPY_REDUCTION_STEPS - k) / ENTROPY_REDUCTION_STEPS\n",
    "    if k == 0:\n",
    "        entropy_impact = 1.0\n",
    "    else:\n",
    "        entropy_impact = min(1, (1 / (k * 0.005)))\n",
    "        \n",
    "    if k > ENTROPY_REDUCTION_STEPS:\n",
    "        entropy_impact = 0.0\n",
    "        \n",
    "    # test entropy always 0\n",
    "    # entropy_impact = 0.0\n",
    "        \n",
    "    #entropy_impact = 0.0\n",
    "    #if entropy_impact < 0.0:\n",
    "    #    entropy_impact = 0\n",
    "    while not done:\n",
    "        #print(\"c\")\n",
    "        steps_done += 1\n",
    "        \n",
    "        # Get action from pi\n",
    "        np_observation = np.array(get_state_repr(observation))\n",
    "        np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        #print(np_observation)\n",
    "        observation_tensor = FloatTensor(np_observation)\n",
    "\n",
    "        # action distribution\n",
    "        pi_net.eval()\n",
    "        action_distr = pi_net(observation_tensor)\n",
    "        action_probs = F.softmax(action_distr, dim=1)\n",
    "        log_action_probs = 0\n",
    "        #log_action_probs = F.log_softmax(action_distr, dim=1)\n",
    "        # Decide on an action based on the distribution\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        log_prob = m.log_prob(action).unsqueeze(1)\n",
    "            \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        old_state = observation                    \n",
    "            \n",
    "        \n",
    "        observation, reward, done, info = env.step(action.item()) \n",
    "        new_state = observation\n",
    "        \n",
    "        if k%5000 == 0:\n",
    "            #print(\"old_state != new_state\")\n",
    "            #print(old_state != new_state)\n",
    "            #print(\"oldstate \" + str(old_state) + \" newstate \" + str(new_state))\n",
    "            print(\"action_dist \")\n",
    "            print(action_probs)\n",
    "            print(\"On state=\"+ str(old_state) + \", selected action=\" + str(action.item()) )\n",
    "            print(\"new state=\"+ str(new_state) + \", done=\"+str(done) + \\\n",
    "             \". Reward: \" + str(reward))\n",
    "\n",
    "        # Perform one step of the optimization        \n",
    "#         policy_loss, value_loss = optimize_model(I, \\\n",
    "#                                                  old_state, \\\n",
    "#                                                  log_prob, \\\n",
    "#                                                  log_actions_probs, \\\n",
    "#                                                  action_probs, \\\n",
    "#                                                  reward, \\\n",
    "#                                                  new_state, \\\n",
    "#                                                  entropy_impact, \\\n",
    "#                                                  done)\n",
    "        \n",
    "#         I = I * GAMMA\n",
    "        #if (not done) or (done and new_state in [5,7,11,12,15]):\n",
    "        memory.push(get_state_repr(old_state), action.item(), log_prob, action_probs, log_action_probs, get_state_repr(new_state), reward, entropy_impact, done)\n",
    "        \n",
    "\n",
    "            \n",
    "        if len(memory) >= MEMORY_SIZE:\n",
    "            policy_loss, value_loss = optimize(k)\n",
    "            if len(policy_loss_avg) < PRINT_OUT_TIMES :\n",
    "                policy_loss_avg.append(policy_loss)\n",
    "                v_loss_avg.append(value_loss)\n",
    "            else:\n",
    "                policy_loss_avg[episode_step % PRINT_OUT_TIMES] = policy_loss\n",
    "                v_loss_avg[episode_step % PRINT_OUT_TIMES] = value_loss\n",
    "        \n",
    "        times_trained = times_trained + 1\n",
    "\n",
    "        episode_step += 1\n",
    "        #env.render()\n",
    "        \n",
    "    \n",
    "    if k % PRINT_OUT_TIMES ==0:\n",
    "        print_pi_table()\n",
    "        print_v_table()\n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "    \n",
    "    if k % TARGET_UPDATE == 0:\n",
    "        target_v_net.load_state_dict(v_net.state_dict())\n",
    "    \n",
    "\n",
    "    if k%PRINT_OUT_TIMES == 0:\n",
    "        print(\"Episode {} finished after {} . Running score: {}. Policy_loss: {}, Value_loss: {}. Times trained: \\\n",
    "              {}. Times reached goal: {}. \\\n",
    "              Steps done: {}.\".format(k, episode_step, np.mean(score), np.mean(policy_loss_avg),\\\n",
    "                                      np.mean(v_loss_avg) , times_trained, \\\n",
    "                                                                       times_reach_goal, steps_done))\n",
    "        #print(\"policy_loss_avg\")\n",
    "        #print(policy_loss_avg)\n",
    "        #print(\"value_loss_avg\")\n",
    "        #print(v_loss_avg)\n",
    "#         print(\"times_reach_goal\")\n",
    "#         print(times_reach_goal)\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "        #print(\"Game finished. \" + \"-\" * 5)\n",
    "        #print(len(episode_series))\n",
    "#         for param in net.parameters():\n",
    "#             print(param.data)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if reward > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "use_cuda = torch.cuda.is_available\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "np_observation = np.array(0.0)\n",
    "np_observation = np.expand_dims(np_observation, axis=0)\n",
    "observation_tensor = FloatTensor(np_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc168462a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(value_loss_cum)\n",
    "#plt.plot([1,2,3,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(7.11019567876e-13) A[1]:(1.0) A[2]:(2.80345261472e-18) A[3]:(6.42097865763e-13)\n",
      " state (1)  A[0]:(7.02210696275e-13) A[1]:(1.0) A[2]:(2.77235620716e-18) A[3]:(6.33949381705e-13)\n",
      " state (2)  A[0]:(4.84666126677e-13) A[1]:(1.0) A[2]:(1.87235268143e-18) A[3]:(4.35673361537e-13)\n",
      " state (3)  A[0]:(1.45018240851e-14) A[1]:(1.0) A[2]:(5.01193224478e-20) A[3]:(1.3062182324e-14)\n",
      " state (4)  A[0]:(6.27346812927e-17) A[1]:(1.0) A[2]:(2.76044670303e-22) A[3]:(6.42145603481e-17)\n",
      " state (5)  A[0]:(4.24651153669e-16) A[1]:(1.0) A[2]:(6.56580306159e-21) A[3]:(4.37863676506e-16)\n",
      " state (6)  A[0]:(1.1792987209e-07) A[1]:(0.997145831585) A[2]:(0.00285392673686) A[3]:(1.12159206367e-07)\n",
      " state (7)  A[0]:(6.88912468738e-13) A[1]:(3.1177907478e-12) A[2]:(1.0) A[3]:(6.67472370777e-13)\n",
      " state (8)  A[0]:(9.80245138532e-12) A[1]:(5.16503853576e-11) A[2]:(1.0) A[3]:(9.88536422858e-12)\n",
      " state (9)  A[0]:(4.75406302866e-12) A[1]:(1.0) A[2]:(4.81434510313e-11) A[3]:(4.73379451957e-12)\n",
      " state (10)  A[0]:(1.00128846187e-11) A[1]:(1.0) A[2]:(2.03038211244e-10) A[3]:(9.97141258452e-12)\n",
      " state (11)  A[0]:(5.60712804312e-12) A[1]:(1.2024883693e-11) A[2]:(1.0) A[3]:(5.60712804312e-12)\n",
      " state (12)  A[0]:(4.26047435179e-12) A[1]:(6.80655662041e-12) A[2]:(1.0) A[3]:(4.26047435179e-12)\n",
      " state (13)  A[0]:(4.25112332489e-12) A[1]:(6.77174852651e-12) A[2]:(1.0) A[3]:(4.25008509289e-12)\n",
      " state (14)  A[0]:(4.25112332489e-12) A[1]:(6.7700962024e-12) A[2]:(1.0) A[3]:(4.24904772825e-12)\n",
      " state (15)  A[0]:(4.25112332489e-12) A[1]:(6.7700962024e-12) A[2]:(1.0) A[3]:(4.24904772825e-12)\n"
     ]
    }
   ],
   "source": [
    "print_pi_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
