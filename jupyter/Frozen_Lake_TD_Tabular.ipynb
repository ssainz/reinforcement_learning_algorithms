{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "   id='FrozenLakeNotSlippery-v0',\n",
    "   entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "   kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "   max_episode_steps=100,\n",
    "   reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_values \n",
      "[0. 0. 0. 0.]\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "[0. 0. 0. 0.]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0. 0. 0. 0.]\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0. 0. 0. 0.]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0. 0. 0. 0.]\n",
      "On state=8, selected action=0 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0. 0. 0. 0.]\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 0 finished after 0 timesteps with r=0.0. Running score: 0.0. Times trained: 0. Times reached goal: 0.\n",
      "Episode 1000 finished after 0 timesteps with r=0.0. Running score: 0.13. Times trained: 0. Times reached goal: 96.\n",
      "Episode 2000 finished after 0 timesteps with r=1.0. Running score: 0.09. Times trained: 0. Times reached goal: 100.\n",
      "Episode 3000 finished after 0 timesteps with r=0.0. Running score: 0.06. Times trained: 0. Times reached goal: 133.\n",
      "Episode 4000 finished after 0 timesteps with r=0.0. Running score: 0.1. Times trained: 0. Times reached goal: 112.\n",
      "Episode 5000 finished after 0 timesteps with r=0.0. Running score: 0.17. Times trained: 0. Times reached goal: 128.\n",
      "Episode 6000 finished after 0 timesteps with r=0.0. Running score: 0.1. Times trained: 0. Times reached goal: 125.\n",
      "Episode 7000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained: 0. Times reached goal: 137.\n",
      "Episode 8000 finished after 0 timesteps with r=0.0. Running score: 0.08. Times trained: 0. Times reached goal: 141.\n",
      "Episode 9000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained: 0. Times reached goal: 146.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "q_values \n",
      "[0.531441 0.       0.6561   0.      ]\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 10000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained: 0. Times reached goal: 168.\n",
      "Episode 11000 finished after 0 timesteps with r=0.0. Running score: 0.19. Times trained: 0. Times reached goal: 187.\n",
      "Episode 12000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained: 0. Times reached goal: 161.\n",
      "Episode 13000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained: 0. Times reached goal: 173.\n",
      "Episode 14000 finished after 0 timesteps with r=0.0. Running score: 0.21. Times trained: 0. Times reached goal: 184.\n",
      "Episode 15000 finished after 0 timesteps with r=1.0. Running score: 0.21. Times trained: 0. Times reached goal: 193.\n",
      "Episode 16000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained: 0. Times reached goal: 195.\n",
      "Episode 17000 finished after 0 timesteps with r=0.0. Running score: 0.23. Times trained: 0. Times reached goal: 213.\n",
      "Episode 18000 finished after 0 timesteps with r=1.0. Running score: 0.28. Times trained: 0. Times reached goal: 209.\n",
      "Episode 19000 finished after 0 timesteps with r=0.0. Running score: 0.18. Times trained: 0. Times reached goal: 213.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 20000 finished after 0 timesteps with r=0.0. Running score: 0.25. Times trained: 0. Times reached goal: 243.\n",
      "Episode 21000 finished after 0 timesteps with r=1.0. Running score: 0.22. Times trained: 0. Times reached goal: 256.\n",
      "Episode 22000 finished after 0 timesteps with r=0.0. Running score: 0.25. Times trained: 0. Times reached goal: 233.\n",
      "Episode 23000 finished after 0 timesteps with r=0.0. Running score: 0.3. Times trained: 0. Times reached goal: 246.\n",
      "Episode 24000 finished after 0 timesteps with r=0.0. Running score: 0.34. Times trained: 0. Times reached goal: 273.\n",
      "Episode 25000 finished after 0 timesteps with r=1.0. Running score: 0.28. Times trained: 0. Times reached goal: 228.\n",
      "Episode 26000 finished after 0 timesteps with r=0.0. Running score: 0.35. Times trained: 0. Times reached goal: 280.\n",
      "Episode 27000 finished after 0 timesteps with r=0.0. Running score: 0.28. Times trained: 0. Times reached goal: 281.\n",
      "Episode 28000 finished after 0 timesteps with r=1.0. Running score: 0.26. Times trained: 0. Times reached goal: 281.\n",
      "Episode 29000 finished after 0 timesteps with r=0.0. Running score: 0.29. Times trained: 0. Times reached goal: 298.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "q_values \n",
      "[0.531441 0.       0.6561   0.      ]\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "q_values \n",
      "[0.59049 0.729   0.59049 0.     ]\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "q_values \n",
      "[0.   0.81 0.   0.  ]\n",
      "On state=6, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 30000 finished after 0 timesteps with r=0.0. Running score: 0.35. Times trained: 0. Times reached goal: 306.\n",
      "Episode 31000 finished after 0 timesteps with r=0.0. Running score: 0.34. Times trained: 0. Times reached goal: 321.\n",
      "Episode 32000 finished after 0 timesteps with r=0.0. Running score: 0.32. Times trained: 0. Times reached goal: 334.\n",
      "Episode 33000 finished after 0 timesteps with r=0.0. Running score: 0.29. Times trained: 0. Times reached goal: 327.\n",
      "Episode 34000 finished after 0 timesteps with r=1.0. Running score: 0.3. Times trained: 0. Times reached goal: 323.\n",
      "Episode 35000 finished after 0 timesteps with r=0.0. Running score: 0.36. Times trained: 0. Times reached goal: 369.\n",
      "Episode 36000 finished after 0 timesteps with r=0.0. Running score: 0.33. Times trained: 0. Times reached goal: 337.\n",
      "Episode 37000 finished after 0 timesteps with r=0.0. Running score: 0.36. Times trained: 0. Times reached goal: 361.\n",
      "Episode 38000 finished after 0 timesteps with r=1.0. Running score: 0.31. Times trained: 0. Times reached goal: 336.\n",
      "Episode 39000 finished after 0 timesteps with r=0.0. Running score: 0.4. Times trained: 0. Times reached goal: 380.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=2 , \n",
      "new state=10, done=False\n",
      "q_values \n",
      "[0.729 0.9   0.    0.   ]\n",
      "On state=10, selected action=2 , \n",
      "new state=11, done=True\n",
      "Episode 40000 finished after 0 timesteps with r=0.0. Running score: 0.38. Times trained: 0. Times reached goal: 357.\n",
      "Episode 41000 finished after 0 timesteps with r=0.0. Running score: 0.43. Times trained: 0. Times reached goal: 377.\n",
      "Episode 42000 finished after 0 timesteps with r=0.0. Running score: 0.33. Times trained: 0. Times reached goal: 381.\n",
      "Episode 43000 finished after 0 timesteps with r=1.0. Running score: 0.4. Times trained: 0. Times reached goal: 378.\n",
      "Episode 44000 finished after 0 timesteps with r=0.0. Running score: 0.37. Times trained: 0. Times reached goal: 390.\n",
      "Episode 45000 finished after 0 timesteps with r=0.0. Running score: 0.47. Times trained: 0. Times reached goal: 408.\n",
      "Episode 46000 finished after 0 timesteps with r=0.0. Running score: 0.41. Times trained: 0. Times reached goal: 417.\n",
      "Episode 47000 finished after 0 timesteps with r=1.0. Running score: 0.49. Times trained: 0. Times reached goal: 412.\n",
      "Episode 48000 finished after 0 timesteps with r=0.0. Running score: 0.4. Times trained: 0. Times reached goal: 450.\n",
      "Episode 49000 finished after 0 timesteps with r=0.0. Running score: 0.36. Times trained: 0. Times reached goal: 421.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 50000 finished after 0 timesteps with r=0.0. Running score: 0.4. Times trained: 0. Times reached goal: 489.\n",
      "Episode 51000 finished after 0 timesteps with r=1.0. Running score: 0.37. Times trained: 0. Times reached goal: 435.\n",
      "Episode 52000 finished after 0 timesteps with r=0.0. Running score: 0.47. Times trained: 0. Times reached goal: 456.\n",
      "Episode 53000 finished after 0 timesteps with r=1.0. Running score: 0.41. Times trained: 0. Times reached goal: 440.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 54000 finished after 0 timesteps with r=0.0. Running score: 0.45. Times trained: 0. Times reached goal: 438.\n",
      "Episode 55000 finished after 0 timesteps with r=0.0. Running score: 0.58. Times trained: 0. Times reached goal: 506.\n",
      "Episode 56000 finished after 0 timesteps with r=0.0. Running score: 0.49. Times trained: 0. Times reached goal: 506.\n",
      "Episode 57000 finished after 0 timesteps with r=1.0. Running score: 0.46. Times trained: 0. Times reached goal: 491.\n",
      "Episode 58000 finished after 0 timesteps with r=0.0. Running score: 0.46. Times trained: 0. Times reached goal: 507.\n",
      "Episode 59000 finished after 0 timesteps with r=1.0. Running score: 0.55. Times trained: 0. Times reached goal: 504.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 60000 finished after 0 timesteps with r=1.0. Running score: 0.46. Times trained: 0. Times reached goal: 490.\n",
      "Episode 61000 finished after 0 timesteps with r=0.0. Running score: 0.48. Times trained: 0. Times reached goal: 520.\n",
      "Episode 62000 finished after 0 timesteps with r=0.0. Running score: 0.61. Times trained: 0. Times reached goal: 549.\n",
      "Episode 63000 finished after 0 timesteps with r=0.0. Running score: 0.56. Times trained: 0. Times reached goal: 535.\n",
      "Episode 64000 finished after 0 timesteps with r=0.0. Running score: 0.58. Times trained: 0. Times reached goal: 516.\n",
      "Episode 65000 finished after 0 timesteps with r=1.0. Running score: 0.57. Times trained: 0. Times reached goal: 534.\n",
      "Episode 66000 finished after 0 timesteps with r=1.0. Running score: 0.55. Times trained: 0. Times reached goal: 542.\n",
      "Episode 67000 finished after 0 timesteps with r=1.0. Running score: 0.51. Times trained: 0. Times reached goal: 557.\n",
      "Episode 68000 finished after 0 timesteps with r=1.0. Running score: 0.57. Times trained: 0. Times reached goal: 539.\n",
      "Episode 69000 finished after 0 timesteps with r=1.0. Running score: 0.66. Times trained: 0. Times reached goal: 577.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=0 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=0 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 70000 finished after 0 timesteps with r=0.0. Running score: 0.61. Times trained: 0. Times reached goal: 585.\n",
      "Episode 71000 finished after 0 timesteps with r=1.0. Running score: 0.55. Times trained: 0. Times reached goal: 554.\n",
      "Episode 72000 finished after 0 timesteps with r=1.0. Running score: 0.59. Times trained: 0. Times reached goal: 599.\n",
      "Episode 73000 finished after 0 timesteps with r=1.0. Running score: 0.58. Times trained: 0. Times reached goal: 555.\n",
      "Episode 74000 finished after 0 timesteps with r=1.0. Running score: 0.58. Times trained: 0. Times reached goal: 577.\n",
      "Episode 75000 finished after 0 timesteps with r=1.0. Running score: 0.58. Times trained: 0. Times reached goal: 591.\n",
      "Episode 76000 finished after 0 timesteps with r=1.0. Running score: 0.64. Times trained: 0. Times reached goal: 582.\n",
      "Episode 77000 finished after 0 timesteps with r=1.0. Running score: 0.68. Times trained: 0. Times reached goal: 564.\n",
      "Episode 78000 finished after 0 timesteps with r=1.0. Running score: 0.59. Times trained: 0. Times reached goal: 611.\n",
      "Episode 79000 finished after 0 timesteps with r=0.0. Running score: 0.61. Times trained: 0. Times reached goal: 586.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=0 , \n",
      "new state=12, done=True\n",
      "Episode 80000 finished after 0 timesteps with r=0.0. Running score: 0.63. Times trained: 0. Times reached goal: 626.\n",
      "Episode 81000 finished after 0 timesteps with r=1.0. Running score: 0.67. Times trained: 0. Times reached goal: 622.\n",
      "Episode 82000 finished after 0 timesteps with r=1.0. Running score: 0.64. Times trained: 0. Times reached goal: 627.\n",
      "Episode 83000 finished after 0 timesteps with r=1.0. Running score: 0.59. Times trained: 0. Times reached goal: 619.\n",
      "Episode 84000 finished after 0 timesteps with r=0.0. Running score: 0.67. Times trained: 0. Times reached goal: 614.\n",
      "Episode 85000 finished after 0 timesteps with r=1.0. Running score: 0.66. Times trained: 0. Times reached goal: 637.\n",
      "Episode 86000 finished after 0 timesteps with r=1.0. Running score: 0.72. Times trained: 0. Times reached goal: 650.\n",
      "Episode 87000 finished after 0 timesteps with r=0.0. Running score: 0.67. Times trained: 0. Times reached goal: 653.\n",
      "Episode 88000 finished after 0 timesteps with r=1.0. Running score: 0.67. Times trained: 0. Times reached goal: 636.\n",
      "Episode 89000 finished after 0 timesteps with r=0.0. Running score: 0.66. Times trained: 0. Times reached goal: 644.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=0 , \n",
      "new state=12, done=True\n",
      "Episode 90000 finished after 0 timesteps with r=0.0. Running score: 0.64. Times trained: 0. Times reached goal: 672.\n",
      "Episode 91000 finished after 0 timesteps with r=1.0. Running score: 0.72. Times trained: 0. Times reached goal: 664.\n",
      "Episode 92000 finished after 0 timesteps with r=1.0. Running score: 0.69. Times trained: 0. Times reached goal: 669.\n",
      "Episode 93000 finished after 0 timesteps with r=0.0. Running score: 0.71. Times trained: 0. Times reached goal: 681.\n",
      "Episode 94000 finished after 0 timesteps with r=1.0. Running score: 0.59. Times trained: 0. Times reached goal: 666.\n",
      "Episode 95000 finished after 0 timesteps with r=0.0. Running score: 0.64. Times trained: 0. Times reached goal: 687.\n",
      "Episode 96000 finished after 0 timesteps with r=0.0. Running score: 0.64. Times trained: 0. Times reached goal: 677.\n",
      "Episode 97000 finished after 0 timesteps with r=1.0. Running score: 0.68. Times trained: 0. Times reached goal: 690.\n",
      "Episode 98000 finished after 0 timesteps with r=1.0. Running score: 0.69. Times trained: 0. Times reached goal: 696.\n",
      "Episode 99000 finished after 0 timesteps with r=0.0. Running score: 0.61. Times trained: 0. Times reached goal: 686.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=0 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 100000 finished after 0 timesteps with r=1.0. Running score: 0.68. Times trained: 0. Times reached goal: 706.\n",
      "Episode 101000 finished after 0 timesteps with r=0.0. Running score: 0.71. Times trained: 0. Times reached goal: 695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 102000 finished after 0 timesteps with r=1.0. Running score: 0.66. Times trained: 0. Times reached goal: 721.\n",
      "Episode 103000 finished after 0 timesteps with r=1.0. Running score: 0.76. Times trained: 0. Times reached goal: 722.\n",
      "Episode 104000 finished after 0 timesteps with r=1.0. Running score: 0.71. Times trained: 0. Times reached goal: 703.\n",
      "Episode 105000 finished after 0 timesteps with r=1.0. Running score: 0.72. Times trained: 0. Times reached goal: 729.\n",
      "Episode 106000 finished after 0 timesteps with r=0.0. Running score: 0.72. Times trained: 0. Times reached goal: 740.\n",
      "Episode 107000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained: 0. Times reached goal: 715.\n",
      "Episode 108000 finished after 0 timesteps with r=1.0. Running score: 0.73. Times trained: 0. Times reached goal: 736.\n",
      "Episode 109000 finished after 0 timesteps with r=1.0. Running score: 0.68. Times trained: 0. Times reached goal: 723.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 110000 finished after 0 timesteps with r=1.0. Running score: 0.77. Times trained: 0. Times reached goal: 738.\n",
      "Episode 111000 finished after 0 timesteps with r=1.0. Running score: 0.72. Times trained: 0. Times reached goal: 732.\n",
      "Episode 112000 finished after 0 timesteps with r=0.0. Running score: 0.74. Times trained: 0. Times reached goal: 748.\n",
      "Episode 113000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained: 0. Times reached goal: 731.\n",
      "Episode 114000 finished after 0 timesteps with r=0.0. Running score: 0.66. Times trained: 0. Times reached goal: 723.\n",
      "Episode 115000 finished after 0 timesteps with r=0.0. Running score: 0.83. Times trained: 0. Times reached goal: 770.\n",
      "Episode 116000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained: 0. Times reached goal: 772.\n",
      "Episode 117000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained: 0. Times reached goal: 768.\n",
      "Episode 118000 finished after 0 timesteps with r=0.0. Running score: 0.79. Times trained: 0. Times reached goal: 755.\n",
      "Episode 119000 finished after 0 timesteps with r=0.0. Running score: 0.8. Times trained: 0. Times reached goal: 776.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 120000 finished after 0 timesteps with r=0.0. Running score: 0.73. Times trained: 0. Times reached goal: 778.\n",
      "Episode 121000 finished after 0 timesteps with r=1.0. Running score: 0.76. Times trained: 0. Times reached goal: 773.\n",
      "Episode 122000 finished after 0 timesteps with r=1.0. Running score: 0.78. Times trained: 0. Times reached goal: 779.\n",
      "Episode 123000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained: 0. Times reached goal: 778.\n",
      "Episode 124000 finished after 0 timesteps with r=1.0. Running score: 0.72. Times trained: 0. Times reached goal: 775.\n",
      "Episode 125000 finished after 0 timesteps with r=1.0. Running score: 0.73. Times trained: 0. Times reached goal: 735.\n",
      "Episode 126000 finished after 0 timesteps with r=1.0. Running score: 0.79. Times trained: 0. Times reached goal: 785.\n",
      "Episode 127000 finished after 0 timesteps with r=1.0. Running score: 0.69. Times trained: 0. Times reached goal: 789.\n",
      "Episode 128000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained: 0. Times reached goal: 799.\n",
      "Episode 129000 finished after 0 timesteps with r=0.0. Running score: 0.8. Times trained: 0. Times reached goal: 790.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=2 , \n",
      "new state=10, done=False\n",
      "q_values \n",
      "[0.729 0.9   0.    0.   ]\n",
      "On state=10, selected action=1 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 130000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained: 0. Times reached goal: 788.\n",
      "Episode 131000 finished after 0 timesteps with r=0.0. Running score: 0.8. Times trained: 0. Times reached goal: 796.\n",
      "Episode 132000 finished after 0 timesteps with r=1.0. Running score: 0.77. Times trained: 0. Times reached goal: 800.\n",
      "Episode 133000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 809.\n",
      "Episode 134000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained: 0. Times reached goal: 796.\n",
      "Episode 135000 finished after 0 timesteps with r=1.0. Running score: 0.76. Times trained: 0. Times reached goal: 796.\n",
      "Episode 136000 finished after 0 timesteps with r=0.0. Running score: 0.85. Times trained: 0. Times reached goal: 825.\n",
      "Episode 137000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained: 0. Times reached goal: 818.\n",
      "Episode 138000 finished after 0 timesteps with r=0.0. Running score: 0.8. Times trained: 0. Times reached goal: 805.\n",
      "Episode 139000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 822.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 140000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained: 0. Times reached goal: 825.\n",
      "Episode 141000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 846.\n",
      "Episode 142000 finished after 0 timesteps with r=1.0. Running score: 0.82. Times trained: 0. Times reached goal: 825.\n",
      "Episode 143000 finished after 0 timesteps with r=0.0. Running score: 0.8. Times trained: 0. Times reached goal: 825.\n",
      "Episode 144000 finished after 0 timesteps with r=0.0. Running score: 0.86. Times trained: 0. Times reached goal: 838.\n",
      "Episode 145000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 831.\n",
      "Episode 146000 finished after 0 timesteps with r=0.0. Running score: 0.81. Times trained: 0. Times reached goal: 847.\n",
      "Episode 147000 finished after 0 timesteps with r=0.0. Running score: 0.85. Times trained: 0. Times reached goal: 820.\n",
      "Episode 148000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 850.\n",
      "Episode 149000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained: 0. Times reached goal: 856.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 150000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 855.\n",
      "Episode 151000 finished after 0 timesteps with r=1.0. Running score: 0.78. Times trained: 0. Times reached goal: 837.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 152000 finished after 0 timesteps with r=1.0. Running score: 0.82. Times trained: 0. Times reached goal: 838.\n",
      "Episode 153000 finished after 0 timesteps with r=1.0. Running score: 0.79. Times trained: 0. Times reached goal: 839.\n",
      "Episode 154000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 841.\n",
      "Episode 155000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained: 0. Times reached goal: 842.\n",
      "Episode 156000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 861.\n",
      "Episode 157000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained: 0. Times reached goal: 836.\n",
      "Episode 158000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 878.\n",
      "Episode 159000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 864.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 160000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 871.\n",
      "Episode 161000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 867.\n",
      "Episode 162000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 874.\n",
      "Episode 163000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained: 0. Times reached goal: 876.\n",
      "Episode 164000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained: 0. Times reached goal: 859.\n",
      "Episode 165000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 891.\n",
      "Episode 166000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 870.\n",
      "Episode 167000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained: 0. Times reached goal: 867.\n",
      "Episode 168000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 863.\n",
      "Episode 169000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 866.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 170000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 871.\n",
      "Episode 171000 finished after 0 timesteps with r=0.0. Running score: 0.88. Times trained: 0. Times reached goal: 884.\n",
      "Episode 172000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained: 0. Times reached goal: 883.\n",
      "Episode 173000 finished after 0 timesteps with r=0.0. Running score: 0.83. Times trained: 0. Times reached goal: 881.\n",
      "Episode 174000 finished after 0 timesteps with r=0.0. Running score: 0.9. Times trained: 0. Times reached goal: 887.\n",
      "Episode 175000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 884.\n",
      "Episode 176000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 894.\n",
      "Episode 177000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 899.\n",
      "Episode 178000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 886.\n",
      "Episode 179000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 878.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 180000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 890.\n",
      "Episode 181000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained: 0. Times reached goal: 891.\n",
      "Episode 182000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 884.\n",
      "Episode 183000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 892.\n",
      "Episode 184000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained: 0. Times reached goal: 891.\n",
      "Episode 185000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 891.\n",
      "Episode 186000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained: 0. Times reached goal: 893.\n",
      "Episode 187000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained: 0. Times reached goal: 895.\n",
      "Episode 188000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 901.\n",
      "Episode 189000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 901.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 190000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 909.\n",
      "Episode 191000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 887.\n",
      "Episode 192000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 903.\n",
      "Episode 193000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 900.\n",
      "Episode 194000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 899.\n",
      "Episode 195000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 905.\n",
      "Episode 196000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 928.\n",
      "Episode 197000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 915.\n",
      "Episode 198000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 919.\n",
      "Episode 199000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 903.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 200000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 913.\n",
      "Episode 201000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 915.\n",
      "Episode 202000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 203000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 920.\n",
      "Episode 204000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 909.\n",
      "Episode 205000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 920.\n",
      "Episode 206000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained: 0. Times reached goal: 925.\n",
      "Episode 207000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 923.\n",
      "Episode 208000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained: 0. Times reached goal: 920.\n",
      "Episode 209000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 919.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 210000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 908.\n",
      "Episode 211000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 926.\n",
      "Episode 212000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 912.\n",
      "Episode 213000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 918.\n",
      "Episode 214000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained: 0. Times reached goal: 926.\n",
      "Episode 215000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 935.\n",
      "Episode 216000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained: 0. Times reached goal: 921.\n",
      "Episode 217000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 931.\n",
      "Episode 218000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 917.\n",
      "Episode 219000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 937.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 220000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 934.\n",
      "Episode 221000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 930.\n",
      "Episode 222000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained: 0. Times reached goal: 924.\n",
      "Episode 223000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 920.\n",
      "Episode 224000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 936.\n",
      "Episode 225000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 937.\n",
      "Episode 226000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 944.\n",
      "Episode 227000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained: 0. Times reached goal: 931.\n",
      "Episode 228000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 956.\n",
      "Episode 229000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 940.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 230000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 944.\n",
      "Episode 231000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 940.\n",
      "Episode 232000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 932.\n",
      "Episode 233000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 946.\n",
      "Episode 234000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 952.\n",
      "Episode 235000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 939.\n",
      "Episode 236000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 948.\n",
      "Episode 237000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 950.\n",
      "Episode 238000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 938.\n",
      "Episode 239000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 934.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 240000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 939.\n",
      "Episode 241000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 946.\n",
      "Episode 242000 finished after 0 timesteps with r=0.0. Running score: 0.93. Times trained: 0. Times reached goal: 942.\n",
      "Episode 243000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 948.\n",
      "Episode 244000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 948.\n",
      "Episode 245000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 958.\n",
      "Episode 246000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 944.\n",
      "Episode 247000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 942.\n",
      "Episode 248000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 950.\n",
      "Episode 249000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 959.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 250000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 951.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 251000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 950.\n",
      "Episode 252000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 950.\n",
      "Episode 253000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 961.\n",
      "Episode 254000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 954.\n",
      "Episode 255000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 952.\n",
      "Episode 256000 finished after 0 timesteps with r=0.0. Running score: 0.9. Times trained: 0. Times reached goal: 964.\n",
      "Episode 257000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 953.\n",
      "Episode 258000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 952.\n",
      "Episode 259000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 972.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 260000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 968.\n",
      "Episode 261000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained: 0. Times reached goal: 953.\n",
      "Episode 262000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 957.\n",
      "Episode 263000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 963.\n",
      "Episode 264000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 954.\n",
      "Episode 265000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 958.\n",
      "Episode 266000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 958.\n",
      "Episode 267000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 973.\n",
      "Episode 268000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 972.\n",
      "Episode 269000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 970.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 270000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 950.\n",
      "Episode 271000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained: 0. Times reached goal: 962.\n",
      "Episode 272000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 953.\n",
      "Episode 273000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 964.\n",
      "Episode 274000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 969.\n",
      "Episode 275000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 971.\n",
      "Episode 276000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 961.\n",
      "Episode 277000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 974.\n",
      "Episode 278000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 961.\n",
      "Episode 279000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained: 0. Times reached goal: 967.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 280000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 968.\n",
      "Episode 281000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 975.\n",
      "Episode 282000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 977.\n",
      "Episode 283000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 970.\n",
      "Episode 284000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 975.\n",
      "Episode 285000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 963.\n",
      "Episode 286000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 964.\n",
      "Episode 287000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 957.\n",
      "Episode 288000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 963.\n",
      "Episode 289000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 963.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 290000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 970.\n",
      "Episode 291000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 970.\n",
      "Episode 292000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 969.\n",
      "Episode 293000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 975.\n",
      "Episode 294000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 969.\n",
      "Episode 295000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 974.\n",
      "Episode 296000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 972.\n",
      "Episode 297000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 976.\n",
      "Episode 298000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 972.\n",
      "Episode 299000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 980.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 300000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 973.\n",
      "Episode 301000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 970.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 302000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 974.\n",
      "Episode 303000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained: 0. Times reached goal: 974.\n",
      "Episode 304000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 972.\n",
      "Episode 305000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained: 0. Times reached goal: 964.\n",
      "Episode 306000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 974.\n",
      "Episode 307000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 975.\n",
      "Episode 308000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 970.\n",
      "Episode 309000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 970.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 310000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 976.\n",
      "Episode 311000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 977.\n",
      "Episode 312000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 971.\n",
      "Episode 313000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 981.\n",
      "Episode 314000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 981.\n",
      "Episode 315000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 981.\n",
      "Episode 316000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 982.\n",
      "Episode 317000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 982.\n",
      "Episode 318000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 978.\n",
      "Episode 319000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 975.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 320000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 978.\n",
      "Episode 321000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 988.\n",
      "Episode 322000 finished after 0 timesteps with r=0.0. Running score: 0.97. Times trained: 0. Times reached goal: 979.\n",
      "Episode 323000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 983.\n",
      "Episode 324000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 990.\n",
      "Episode 325000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 981.\n",
      "Episode 326000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 979.\n",
      "Episode 327000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 986.\n",
      "Episode 328000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 984.\n",
      "Episode 329000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 981.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 330000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 982.\n",
      "Episode 331000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 979.\n",
      "Episode 332000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 987.\n",
      "Episode 333000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 982.\n",
      "Episode 334000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 988.\n",
      "Episode 335000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 986.\n",
      "Episode 336000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 978.\n",
      "Episode 337000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 980.\n",
      "Episode 338000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 983.\n",
      "Episode 339000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 982.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 340000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 989.\n",
      "Episode 341000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 976.\n",
      "Episode 342000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 978.\n",
      "Episode 343000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 983.\n",
      "Episode 344000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 986.\n",
      "Episode 345000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 988.\n",
      "Episode 346000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 986.\n",
      "Episode 347000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 985.\n",
      "Episode 348000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 988.\n",
      "Episode 349000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 983.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 350000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 987.\n",
      "Episode 351000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 991.\n",
      "Episode 352000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 980.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 353000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 991.\n",
      "Episode 354000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 988.\n",
      "Episode 355000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 991.\n",
      "Episode 356000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 982.\n",
      "Episode 357000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 985.\n",
      "Episode 358000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 988.\n",
      "Episode 359000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 987.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 360000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 986.\n",
      "Episode 361000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 362000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 363000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 992.\n",
      "Episode 364000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 986.\n",
      "Episode 365000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 989.\n",
      "Episode 366000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 986.\n",
      "Episode 367000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 989.\n",
      "Episode 368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 990.\n",
      "Episode 369000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 370000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 994.\n",
      "Episode 371000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 989.\n",
      "Episode 372000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 988.\n",
      "Episode 373000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 984.\n",
      "Episode 374000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 980.\n",
      "Episode 375000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n",
      "Episode 376000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 991.\n",
      "Episode 377000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 994.\n",
      "Episode 378000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 379000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 990.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 380000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 990.\n",
      "Episode 381000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 990.\n",
      "Episode 382000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 987.\n",
      "Episode 383000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n",
      "Episode 384000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 988.\n",
      "Episode 385000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 993.\n",
      "Episode 386000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained: 0. Times reached goal: 988.\n",
      "Episode 387000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 388000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 992.\n",
      "Episode 389000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 990.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 390000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 989.\n",
      "Episode 391000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 990.\n",
      "Episode 392000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 990.\n",
      "Episode 393000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 394000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 990.\n",
      "Episode 395000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 396000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 992.\n",
      "Episode 397000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 398000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 399000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 990.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 400000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 401000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 992.\n",
      "Episode 402000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 403000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 991.\n",
      "Episode 404000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 405000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 406000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 407000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 991.\n",
      "Episode 408000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 409000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 410000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 411000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 412000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 413000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 990.\n",
      "Episode 414000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 415000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 996.\n",
      "Episode 416000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 993.\n",
      "Episode 417000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 418000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 994.\n",
      "Episode 419000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 420000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "Episode 421000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 422000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 423000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 424000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 992.\n",
      "Episode 425000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 994.\n",
      "Episode 426000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained: 0. Times reached goal: 990.\n",
      "Episode 427000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 992.\n",
      "Episode 428000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 430000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "Episode 431000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "Episode 432000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 433000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n",
      "Episode 434000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 435000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 994.\n",
      "Episode 436000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 437000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 438000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 992.\n",
      "Episode 439000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 440000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 994.\n",
      "Episode 441000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 442000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 443000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 994.\n",
      "Episode 444000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 445000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 446000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 447000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 449000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 450000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 451000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 452000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 453000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 995.\n",
      "Episode 454000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 455000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "Episode 456000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 458000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 994.\n",
      "Episode 459000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 460000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 461000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 462000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "Episode 463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 464000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 465000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 466000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 467000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 469000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 470000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 993.\n",
      "Episode 471000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 472000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 473000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 474000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 475000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 477000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 478000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 479000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 480000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 481000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 482000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 483000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 484000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 485000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 994.\n",
      "Episode 486000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 488000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 995.\n",
      "Episode 489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 490000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 491000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 493000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 993.\n",
      "Episode 494000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 495000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 497000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 498000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 499000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 501000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 502000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 503000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 505000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 506000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 508000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 509000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 510000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 511000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 512000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "Episode 513000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 514000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 515000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 998.\n",
      "Episode 516000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 995.\n",
      "Episode 517000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 998.\n",
      "Episode 518000 finished after 0 timesteps with r=0.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "Episode 519000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 520000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 521000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 998.\n",
      "Episode 522000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 523000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained: 0. Times reached goal: 995.\n",
      "Episode 524000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 525000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 526000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 527000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 998.\n",
      "Episode 528000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 533000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 534000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 996.\n",
      "Episode 535000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 536000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 997.\n",
      "Episode 537000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 996.\n",
      "Episode 538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 540000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 543000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 544000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 997.\n",
      "Episode 545000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 998.\n",
      "Episode 546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 547000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 548000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 995.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 552000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 553000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 554000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 555000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 556000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 558000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 559000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 560000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 561000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 998.\n",
      "Episode 562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 563000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 569000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 570000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 571000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 572000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 574000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 576000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 578000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 998.\n",
      "Episode 579000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 581000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 582000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 590000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 591000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 592000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 598000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 605000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained: 0. Times reached goal: 999.\n",
      "Episode 606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 999.\n",
      "Episode 607000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 608000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 998.\n",
      "Episode 609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "q_values \n",
      "[0.531441 0.59049  0.59049  0.      ]\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "[0.59049 0.6561  0.      0.     ]\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "[0.6561 0.     0.729  0.    ]\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "[0.6561 0.81   0.81   0.    ]\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "q_values \n",
      "[0.   0.81 0.9  0.  ]\n",
      "On state=13, selected action=2 , \n",
      "new state=14, done=False\n",
      "q_values \n",
      "[0.81 0.9  1.   0.  ]\n",
      "On state=14, selected action=2 , \n",
      "new state=15, done=True\n",
      "Episode 610000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n",
      "Episode 611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained: 0. Times reached goal: 1000.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5d4acef52350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mq_value_new_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mq_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m         \u001b[0mALPHA\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_value_new_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "\n",
    "NUM_EPISODES = 10000000\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.00\n",
    "EPS_DECAY = 500000\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "\n",
    "q_function = []\n",
    "for s in range(16):\n",
    "    action_list = []\n",
    "    for a in range(4):\n",
    "        action_list.append(0.0)\n",
    "    q_function.append(action_list)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    episode_series = []\n",
    "    reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Get action from pi\n",
    "\n",
    "\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        steps_done += 1\n",
    "        q_values = np.array(q_function[observation])\n",
    "        if sample >= eps_threshold: \n",
    "            \n",
    "            #action = q_values.max(1)[1] # First 1 is the dimension, second 1 is the index (this is argmax)\n",
    "            action = q_values.argmax()\n",
    "            \n",
    "        else:\n",
    "            action = np.random.randint(0,3)\n",
    "        \n",
    "            \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        old_state = observation\n",
    "        if k%10000 == 0:\n",
    "            print(\"q_values \")\n",
    "            print(q_values)\n",
    "            print(\"On state=\"+ str(observation) + \", selected action=\" + str(action) + \" , \")\n",
    "        \n",
    "        observation, reward, done, info = env.step(action) \n",
    "        new_state = observation\n",
    "        \n",
    "        # Store the transition in memory\n",
    "        #memory.push(old_state, action, new_state, reward, done)\n",
    "\n",
    "        \n",
    "        if k%10000 == 0:\n",
    "            print(\"new state=\"+ str(observation) + \", done=\"+str(done))\n",
    "        #if done and reward != 1.0:\n",
    "        #    reward = -1.0\n",
    "\n",
    "        \n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        q_value_new_state = np.array(q_function[new_state]).argmax()\n",
    "        q_function[old_state][action] = q_function[old_state][action] + \\\n",
    "        ALPHA *( reward + (q_function[new_state][q_value_new_state] * GAMMA) - q_function[old_state][action])\n",
    "        \n",
    "    \n",
    "        \n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "\n",
    "    if k%1000 == 0:\n",
    "        print(\"Episode {} finished after {} timesteps with r={}. Running score: {}. Times trained: {}. Times reached goal: {}.\".format(k, len(episode_series), reward, np.mean(score), times_trained, times_reach_goal))\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "  \n",
    "    \n",
    "    if reward > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
