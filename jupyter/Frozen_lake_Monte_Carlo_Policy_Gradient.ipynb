{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(10):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "\n",
    "#env = gym.make('FrozenLake8x8-v0')\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.render()\n",
    "class pi_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pi_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 20)\n",
    "        self.linear2 = nn.Linear(20, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        #x = F.softmax(self.linear2(x), dim=0)\n",
    "        x = self.linear2(x)\n",
    "        return x.view(-1, 4)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "action_probs_orig \n",
      "tensor([[ 1.8978,  2.1219,  2.2214,  2.1729]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.8978,  2.1219,  2.2214,  2.1729]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.8978,  2.1219,  2.2214,  2.1729]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.8978,  2.1219,  2.2214,  2.1729]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 19.0885,  19.3126,  19.4121,  19.3636]])\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 19.0885,  19.3126,  19.4121,  19.3636]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 39.0885,  39.3126,  39.4121,  39.3636]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 0 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 0. Times reached goal: 0.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fc27b7004658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#print(action_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#FOR EXPLORATION:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m#print(\"action_probs after dropout\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#print(action_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sergio/.local/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sergio/.local/lib/python2.7/site-packages/torch/nn/_functions/dropout.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, input, p, train, inplace)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# custom weights initialization \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        #m.weight.data.normal_(0.0, 0.02)\n",
    "        #m.weight.data.uniform_(0.0, 0.02)\n",
    "        m.weight.data.fill_(0.5)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "NUM_EPISODES = 1000000\n",
    "GAMMA = 0.9\n",
    "net = pi_net()\n",
    "print \"a\"\n",
    "net.apply(weights_init)\n",
    "print \"b\"\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "\n",
    "    episode_series = []\n",
    "    while not done:\n",
    "        # Get action from pi\n",
    "        # action = env.action_space.sample()\n",
    "        np_observation = np.array(observation)\n",
    "        #np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        observation_tensor = torch.FloatTensor(np_observation) \n",
    "        #print(observation_tensor)\n",
    "        #net.eval()\n",
    "        #print(\"before eval\")\n",
    "        action_probs = net(observation_tensor)\n",
    "        action_probs_orig = action_probs\n",
    "        #print(\"action_probs after net\")\n",
    "        #print(action_probs)\n",
    "        #FOR EXPLORATION: \n",
    "        action_probs = F.dropout(action_probs, p=0.3, training=True)\n",
    "        #print(\"action_probs after dropout\")\n",
    "        #print(action_probs)\n",
    "        action_probs = F.softmax(action_probs, dim=1)\n",
    "        #print(\"action_probs after softmax\")\n",
    "        #print(action_probs)\n",
    "        #action = action_probs.multinomial(num_samples=1)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"after eval\")\n",
    "        #print(\"action_probs\")\n",
    "        #print(action_probs)\n",
    "        log_prob = m.log_prob(action)\n",
    "        #print(\"log_prob\")\n",
    "        #print(log_prob)\n",
    "        #break\n",
    "        #print(\"softmax\")\n",
    "        #print(action_probs)\n",
    "        #print(\"action\")\n",
    "        #print(str(action.item()))\n",
    "        #print(type(prob.multinomial))\n",
    "        \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        \n",
    "        if k%10000 == 0:\n",
    "            print(\"action_probs_orig \")\n",
    "            print(action_probs_orig)\n",
    "            print(\"On state=\"+ str(observation) + \", selected action=\" + str(action.item()) + \" , \")\n",
    "        \n",
    "        observation, reward, done, info = env.step(action.item()) \n",
    "        \n",
    "        if k%10000 == 0:\n",
    "            print(\"new state=\"+ str(observation) + \", done=\"+str(done))\n",
    "#         if done and reward != 1.0:\n",
    "#             reward = -1.0\n",
    "        step_data = [observation, action,log_prob, reward, done, info]\n",
    "        episode_series.append(step_data)\n",
    "        #env.render()\n",
    "        \n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "\n",
    "    if k%1000 == 0:\n",
    "        print(\"Episode {} finished after {} timesteps with r={}. Running score: {}. Times trained: {}. Times reached goal: {}.\".format(k, len(episode_series), reward, np.mean(score), times_trained, times_reach_goal))\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "        #print(\"Game finished. \" + \"-\" * 5)\n",
    "        #print(len(episode_series))\n",
    "#         for param in net.parameters():\n",
    "#             print(param.data)\n",
    "        \n",
    "    #break\n",
    "    #Training:\n",
    "    #episode_series.reverse()\n",
    "    policy_loss = []\n",
    "    rewards_list = []\n",
    "    for i in range(len(episode_series)):\n",
    "        j = i\n",
    "        G = 0\n",
    "        alpha = 1 / len(episode_series)\n",
    "        \n",
    "        # get the log_prob of the last state:\n",
    "        gamma_cum = GAMMA\n",
    "        \n",
    "        while j < len(episode_series): \n",
    "            [observation, action, log_prob, reward, done, info] = episode_series[j]\n",
    "            G = G + reward * gamma_cum\n",
    "            \n",
    "            gamma_cum = gamma_cum * GAMMA\n",
    "            j = j + 1\n",
    "        policy_loss.append(G * -log_prob)\n",
    "        rewards_list.append(G)\n",
    "    \n",
    "    if G > 0.0 or True: # Optimize only if rewards are non zero.\n",
    "        #print \"Reward list\"\n",
    "        #print rewards_list\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        times_trained = times_trained + 1\n",
    "    \n",
    "    if G > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1\n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
