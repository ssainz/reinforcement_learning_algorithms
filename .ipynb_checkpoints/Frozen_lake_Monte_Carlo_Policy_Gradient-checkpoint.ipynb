{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(10):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "\n",
    "#env = gym.make('FrozenLake8x8-v0')\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.render()\n",
    "class pi_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pi_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 20)\n",
    "        self.linear2 = nn.Linear(20, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        #x = F.softmax(self.linear2(x), dim=0)\n",
    "        x = self.linear2(x)\n",
    "        return x.view(-1, 4)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "action_probs_orig \n",
      "tensor([[ 3.2432,  3.1103,  3.4332,  3.3915]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.5391,  6.4061,  6.7291,  6.6874]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.5391,  6.4061,  6.7291,  6.6874]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.5391,  6.4061,  6.7291,  6.6874]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.5391,  6.4061,  6.7291,  6.6874]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.1905,  11.0576,  11.3805,  11.3388]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.5391,  6.4061,  6.7291,  6.6874]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.5391,  6.4061,  6.7291,  6.6874]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.5391,  6.4061,  6.7291,  6.6874]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 0 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 0. Times reached goal: 0.\n",
      "Episode 1000 finished after 17 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 9.\n",
      "Episode 2000 finished after 10 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 12.\n",
      "Episode 3000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 4000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 5000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 6000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 7000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 8000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 9000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1857,  3.0435,  3.9648,  3.3406]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7257,  6.5748,  8.0604,  6.8870]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5921,  11.4291,  13.6906,  11.7621]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6602,  16.4846,  19.5541,  16.8393]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5921,  11.4291,  13.6906,  11.7621]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8645,  31.6510,  37.1446,  32.0708]])\n",
      "On state=6, selected action=0 , \n",
      "new state=5, done=True\n",
      "Episode 10000 finished after 20 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 11000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 12000 finished after 15 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 10.\n",
      "Episode 13000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 14000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 15000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 16000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 17000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 18000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 19000 finished after 19 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 20000 finished after 31 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21000 finished after 25 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 22000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 23000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 24000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 25000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 26000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 27000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 28000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 29000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 30000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 31000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 32000 finished after 39 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 33000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 34000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 35000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 36000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 37000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 38000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 39000 finished after 23 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 40000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 41000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 42000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 43000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 44000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 45000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 46000 finished after 6 timesteps with r=1.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 47000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 48000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 49000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 50000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 51000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 52000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 53000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 54000 finished after 19 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 55000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 56000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 57000 finished after 27 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 58000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 59000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 60000 finished after 16 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 61000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 62000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 63000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 64000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 65000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 66000 finished after 20 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 67000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 68000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 69000 finished after 33 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 41.9870,  41.7495,  49.0488,  42.2108]])\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 47.0533,  46.8034,  54.9332,  47.2861]])\n",
      "On state=9, selected action=3 , \n",
      "new state=5, done=True\n",
      "Episode 70000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 71000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 72000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 73000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 74000 finished after 74 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 75000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 76000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 77000 finished after 42 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 78000 finished after 45 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 79000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 80000 finished after 18 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 81000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 82000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 83000 finished after 17 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 84000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 85000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 86000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 87000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 88000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 89000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 8.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 90000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 91000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 92000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 93000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 94000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 95000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 96000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 97000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 98000 finished after 26 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 99000 finished after 17 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 100000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 101000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 102000 finished after 15 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 103000 finished after 58 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 104000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 105000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 106000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 107000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 108000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 109000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 110000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 111000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 112000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 113000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 114000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 115000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 116000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 117000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 118000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 119000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 120000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 121000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 122000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 123000 finished after 71 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 124000 finished after 24 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 125000 finished after 12 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 126000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 127000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 128000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 129000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 130000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 131000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 132000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 133000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 134000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 135000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 136000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 137000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 138000 finished after 38 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 139000 finished after 10 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 9.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 140000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 141000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 142000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 143000 finished after 23 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 144000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 145000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 146000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 147000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 148000 finished after 18 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 149000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 150000 finished after 12 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 151000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 152000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 153000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 154000 finished after 11 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 155000 finished after 13 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 156000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 157000 finished after 18 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 158000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 159000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 160000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 161000 finished after 15 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 162000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 163000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 164000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 165000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 166000 finished after 12 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 167000 finished after 14 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 168000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 169000 finished after 25 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 170000 finished after 28 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 171000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 172000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 173000 finished after 21 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 174000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 175000 finished after 25 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 176000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 177000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 178000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 179000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 180000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 181000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 182000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 183000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 184000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 185000 finished after 40 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 186000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 187000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 188000 finished after 16 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 189000 finished after 23 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 190000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 191000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 192000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 193000 finished after 16 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 194000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 195000 finished after 19 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 196000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 197000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 198000 finished after 19 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 199000 finished after 18 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=0 , \n",
      "new state=5, done=True\n",
      "Episode 200000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 201000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 202000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 203000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 204000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 205000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 206000 finished after 21 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 207000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 208000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 9.\n",
      "Episode 209000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 210000 finished after 30 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 211000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 212000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 213000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 214000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 215000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 216000 finished after 21 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 217000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 218000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 219000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 220000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 221000 finished after 15 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 222000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 223000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 224000 finished after 24 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 225000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 226000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 227000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 228000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 229000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 230000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 231000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 232000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 233000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 234000 finished after 19 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 235000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 236000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 237000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 238000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 239000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=1 , \n",
      "new state=10, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 52.1196,  51.8572,  60.8176,  52.3614]])\n",
      "On state=10, selected action=2 , \n",
      "new state=11, done=True\n",
      "Episode 240000 finished after 9 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 241000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 242000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 243000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 244000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 245000 finished after 19 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 246000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 247000 finished after 12 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 248000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 249000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 250000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 251000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 252000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 253000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 254000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 255000 finished after 24 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 256000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 257000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 258000 finished after 21 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 259000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 260000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 261000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 262000 finished after 39 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 263000 finished after 20 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 264000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 265000 finished after 27 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 266000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 267000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 268000 finished after 22 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 269000 finished after 24 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=1 , \n",
      "new state=10, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 52.1196,  51.8572,  60.8176,  52.3614]])\n",
      "On state=10, selected action=2 , \n",
      "new state=11, done=True\n",
      "Episode 270000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 271000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 272000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 273000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 274000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 275000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 276000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 277000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 278000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 279000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 280000 finished after 37 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 281000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 282000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 283000 finished after 7 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 284000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 285000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 286000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 287000 finished after 31 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 288000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 289000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 290000 finished after 19 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 291000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 292000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 293000 finished after 26 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 294000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 295000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 296000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 297000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 298000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 299000 finished after 23 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 300000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 301000 finished after 12 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 302000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 303000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 304000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 305000 finished after 28 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 306000 finished after 25 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 307000 finished after 21 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 308000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 309000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 310000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 311000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 312000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 313000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 314000 finished after 15 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 315000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 316000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 317000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 318000 finished after 24 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 319000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 8.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 320000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 321000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 322000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 323000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 324000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 325000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 326000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 9.\n",
      "Episode 327000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 328000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 329000 finished after 36 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 330000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 331000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 332000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 333000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 334000 finished after 30 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 335000 finished after 40 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 336000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 337000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 338000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 339000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 340000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 341000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 342000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 343000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 344000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 345000 finished after 38 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 346000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 347000 finished after 22 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 348000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 349000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 350000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 351000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 352000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 353000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 354000 finished after 19 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 355000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 356000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 357000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 358000 finished after 27 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 359000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 360000 finished after 28 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 361000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 362000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 363000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 364000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 365000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 366000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 367000 finished after 22 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 368000 finished after 50 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 369000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 370000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 371000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 372000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 373000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 374000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 375000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 376000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 377000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 378000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 379000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 380000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 381000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 382000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 383000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 384000 finished after 28 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 385000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 386000 finished after 22 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 387000 finished after 26 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 388000 finished after 24 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 389000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 390000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 391000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 392000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 393000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 394000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 395000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 9.\n",
      "Episode 396000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 397000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 398000 finished after 29 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 399000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 400000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 401000 finished after 15 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 402000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 403000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 404000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 405000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 406000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 407000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 408000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 409000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 410000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 411000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 412000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 413000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 414000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 415000 finished after 64 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 416000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 417000 finished after 26 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 418000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 419000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 420000 finished after 9 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 421000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 422000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 423000 finished after 28 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 424000 finished after 25 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 425000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 426000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 427000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 428000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 429000 finished after 52 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 430000 finished after 16 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 431000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 432000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 433000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 434000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 435000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 436000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 437000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 438000 finished after 15 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 439000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 440000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 441000 finished after 23 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 442000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 443000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 444000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 445000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 446000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 447000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 448000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 449000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 450000 finished after 24 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 451000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 452000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 453000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 454000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 455000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 456000 finished after 26 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 457000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 458000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 459000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 460000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 461000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 462000 finished after 13 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 463000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 464000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 465000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 466000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 467000 finished after 58 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 468000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 469000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 470000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 471000 finished after 21 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 472000 finished after 15 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 473000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 474000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 475000 finished after 24 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 476000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 477000 finished after 13 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 478000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 479000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 480000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 481000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 482000 finished after 16 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 483000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 484000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 485000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 486000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 487000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 488000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 489000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 490000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 491000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 492000 finished after 9 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 493000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 494000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 495000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 496000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 497000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 498000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 9.\n",
      "Episode 499000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 500000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 501000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 502000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 503000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 504000 finished after 41 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 505000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 506000 finished after 20 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 507000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 508000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 509000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 510000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 511000 finished after 34 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 512000 finished after 11 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 513000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 514000 finished after 20 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 515000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 516000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 517000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 518000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 519000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 520000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 521000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 522000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 523000 finished after 21 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 524000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 525000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 526000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 527000 finished after 46 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 528000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 529000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 530000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 531000 finished after 26 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 532000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 533000 finished after 41 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 10.\n",
      "Episode 534000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 535000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 536000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 537000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 538000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 539000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 540000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 541000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 542000 finished after 7 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 543000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 544000 finished after 7 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 545000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 546000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 547000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 548000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 549000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 550000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 551000 finished after 45 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 552000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 553000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 554000 finished after 25 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 555000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 556000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 557000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 558000 finished after 27 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 559000 finished after 20 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 560000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 561000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 562000 finished after 35 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 563000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 564000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 565000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 566000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 567000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 568000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 569000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 570000 finished after 18 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 571000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 572000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 573000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 574000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 575000 finished after 29 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 576000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 577000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 578000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 579000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 580000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 581000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 582000 finished after 52 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 583000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 584000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 585000 finished after 22 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 586000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 587000 finished after 24 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 588000 finished after 29 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 589000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 590000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 591000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 592000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 593000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 594000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 595000 finished after 48 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 596000 finished after 16 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 597000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 598000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 599000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 600000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 601000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 602000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 603000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 604000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 605000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 606000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 607000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 608000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 609000 finished after 19 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 610000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 611000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 612000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 613000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 614000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 615000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 616000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 617000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 618000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 619000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 8.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 620000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 621000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 622000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 623000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 624000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 625000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 626000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 627000 finished after 9 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 628000 finished after 8 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 629000 finished after 39 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 8.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 630000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 631000 finished after 13 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 632000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 633000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 634000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 635000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 636000 finished after 15 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 637000 finished after 16 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 638000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 639000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 640000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 641000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 642000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 643000 finished after 20 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 644000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 645000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 646000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 647000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 648000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 649000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 650000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 651000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 652000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 653000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 654000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 655000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 656000 finished after 49 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 657000 finished after 15 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 658000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 659000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 660000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 661000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 662000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 663000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 664000 finished after 15 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 665000 finished after 21 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 666000 finished after 13 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 667000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 668000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 669000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 670000 finished after 23 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 671000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 672000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 673000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 674000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 675000 finished after 45 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 676000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 677000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 678000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 679000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 680000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 681000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 682000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 683000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 684000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 685000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 686000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 687000 finished after 19 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 688000 finished after 21 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 689000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 690000 finished after 25 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 691000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 692000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 693000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 694000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 695000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 696000 finished after 9 timesteps with r=1.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 697000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 698000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 699000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 700000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 701000 finished after 31 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 702000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 703000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 704000 finished after 17 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 705000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 706000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 707000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 708000 finished after 12 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 709000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 710000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 711000 finished after 11 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 712000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 713000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 714000 finished after 18 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 715000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 716000 finished after 11 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 717000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 718000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 719000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 720000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 721000 finished after 20 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 722000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 723000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 724000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 725000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 726000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 727000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 728000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 729000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 730000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 731000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 732000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 733000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 734000 finished after 15 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 735000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 736000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 737000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 738000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 739000 finished after 7 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 740000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 741000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 742000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 743000 finished after 16 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 744000 finished after 33 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 745000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 746000 finished after 13 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 747000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 748000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 749000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 750000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 751000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 752000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 753000 finished after 21 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 754000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 755000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 756000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 757000 finished after 29 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 758000 finished after 15 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 759000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 760000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 761000 finished after 35 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 762000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 763000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 764000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 765000 finished after 13 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 766000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 767000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 768000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 769000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 770000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 771000 finished after 9 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 772000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 773000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 774000 finished after 33 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 775000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 776000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 777000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 778000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 779000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 780000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 781000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 782000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 783000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 784000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 785000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 786000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 787000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 788000 finished after 16 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 789000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 8.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 790000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 791000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 792000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 793000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 794000 finished after 23 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 795000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 796000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 797000 finished after 7 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 798000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 799000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 800000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 801000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 802000 finished after 48 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 803000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 804000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 805000 finished after 47 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 806000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 807000 finished after 23 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 808000 finished after 27 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 809000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 810000 finished after 19 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 811000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 812000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 813000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 814000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 815000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 816000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 817000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 818000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 819000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 820000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 821000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 822000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 823000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 824000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 825000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 826000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 827000 finished after 57 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 828000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 829000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 830000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 831000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 832000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 833000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 834000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 835000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 836000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 837000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 838000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 839000 finished after 19 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 840000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 841000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 842000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 843000 finished after 12 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 844000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 845000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 846000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 847000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 848000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 849000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 850000 finished after 15 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 851000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 852000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 853000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 854000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 855000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 856000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 857000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 858000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 859000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 860000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 861000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 862000 finished after 25 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 863000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 864000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 865000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 866000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 867000 finished after 27 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 868000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 869000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=0 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 870000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 871000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 872000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 873000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 874000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 875000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 876000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 877000 finished after 23 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 878000 finished after 46 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 879000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 21.7219,  21.5342,  25.5111,  21.9096]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 880000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 881000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 882000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 883000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 884000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 885000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 886000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 887000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 888000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 889000 finished after 31 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=0 , \n",
      "new state=5, done=True\n",
      "Episode 890000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 891000 finished after 28 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 892000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 893000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 894000 finished after 43 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 895000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 896000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 897000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 898000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 899000 finished after 12 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 900000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 901000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 902000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 903000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 904000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 905000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 906000 finished after 18 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 907000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 908000 finished after 11 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 909000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 910000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 911000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 912000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 913000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 914000 finished after 20 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 915000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 916000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 917000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 918000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 919000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 31.8544,  31.6419,  37.2800,  32.0602]])\n",
      "On state=6, selected action=0 , \n",
      "new state=5, done=True\n",
      "Episode 920000 finished after 22 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 921000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 922000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 923000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 924000 finished after 8 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 925000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 926000 finished after 17 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 927000 finished after 7 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 928000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 929000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 930000 finished after 10 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 931000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 932000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 933000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 934000 finished after 11 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 935000 finished after 37 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 936000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 937000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 938000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 939000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 940000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 941000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 942000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 943000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 944000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 945000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 946000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 947000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 948000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 949000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 950000 finished after 11 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 951000 finished after 14 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 952000 finished after 15 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 953000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 954000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 955000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 956000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 957000 finished after 13 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 958000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 959000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 960000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 961000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 962000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 963000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 8.\n",
      "Episode 964000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 965000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 966000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 967000 finished after 61 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 968000 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 969000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.6556,  16.4804,  19.6267,  16.8344]])\n",
      "On state=3, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=3 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.5893,  11.4266,  13.7423,  11.7591]])\n",
      "On state=2, selected action=0 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 970000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 971000 finished after 29 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 972000 finished after 15 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 973000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 974000 finished after 12 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 0.\n",
      "Episode 975000 finished after 29 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 976000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 977000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 978000 finished after 17 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 979000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 2.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 980000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 981000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 982000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 983000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 984000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 985000 finished after 14 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 3.\n",
      "Episode 986000 finished after 18 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 987000 finished after 20 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 1.\n",
      "Episode 988000 finished after 7 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 989000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 3.1804,  3.0384,  3.9755,  3.3353]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7222,  6.5715,  8.0893,  6.8834]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 990000 finished after 5 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 991000 finished after 38 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 992000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 6.\n",
      "Episode 993000 finished after 29 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 2.\n",
      "Episode 994000 finished after 12 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 7.\n",
      "Episode 995000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 996000 finished after 38 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 5.\n",
      "Episode 997000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 998000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 4.\n",
      "Episode 999000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 6.\n"
     ]
    }
   ],
   "source": [
    "# custom weights initialization \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        #m.weight.data.normal_(0.0, 0.02)\n",
    "        #m.weight.data.uniform_(0.0, 0.02)\n",
    "        m.weight.data.fill_(0.5)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "NUM_EPISODES = 1000000\n",
    "GAMMA = 0.9\n",
    "net = pi_net()\n",
    "print \"a\"\n",
    "net.apply(weights_init)\n",
    "print \"b\"\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "\n",
    "    episode_series = []\n",
    "    while not done:\n",
    "        # Get action from pi\n",
    "        # action = env.action_space.sample()\n",
    "        np_observation = np.array(observation)\n",
    "        #np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        observation_tensor = torch.FloatTensor(np_observation) \n",
    "        #print(observation_tensor)\n",
    "        #net.eval()\n",
    "        #print(\"before eval\")\n",
    "        action_probs = net(observation_tensor)\n",
    "        action_probs_orig = action_probs\n",
    "        #print(\"action_probs after net\")\n",
    "        #print(action_probs)\n",
    "        #FOR EXPLORATION: \n",
    "        action_probs = F.dropout(action_probs, p=0.3, training=True)\n",
    "        #print(\"action_probs after dropout\")\n",
    "        #print(action_probs)\n",
    "        action_probs = F.softmax(action_probs, dim=1)\n",
    "        #print(\"action_probs after softmax\")\n",
    "        #print(action_probs)\n",
    "        #action = action_probs.multinomial(num_samples=1)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"after eval\")\n",
    "        #print(\"action_probs\")\n",
    "        #print(action_probs)\n",
    "        log_prob = m.log_prob(action)\n",
    "        #print(\"log_prob\")\n",
    "        #print(log_prob)\n",
    "        #break\n",
    "        #print(\"softmax\")\n",
    "        #print(action_probs)\n",
    "        #print(\"action\")\n",
    "        #print(str(action.item()))\n",
    "        #print(type(prob.multinomial))\n",
    "        \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        \n",
    "        if k%10000 == 0:\n",
    "            print(\"action_probs_orig \")\n",
    "            print(action_probs_orig)\n",
    "            print(\"On state=\"+ str(observation) + \", selected action=\" + str(action.item()) + \" , \")\n",
    "        \n",
    "        observation, reward, done, info = env.step(action.item()) \n",
    "        \n",
    "        if k%10000 == 0:\n",
    "            print(\"new state=\"+ str(observation) + \", done=\"+str(done))\n",
    "#         if done and reward != 1.0:\n",
    "#             reward = -1.0\n",
    "        step_data = [observation, action,log_prob, reward, done, info]\n",
    "        episode_series.append(step_data)\n",
    "        #env.render()\n",
    "        \n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "\n",
    "    if k%1000 == 0:\n",
    "        print(\"Episode {} finished after {} timesteps with r={}. Running score: {}. Times trained: {}. Times reached goal: {}.\".format(k, len(episode_series), reward, np.mean(score), times_trained, times_reach_goal))\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "        #print(\"Game finished. \" + \"-\" * 5)\n",
    "        #print(len(episode_series))\n",
    "#         for param in net.parameters():\n",
    "#             print(param.data)\n",
    "        \n",
    "    #break\n",
    "    #Training:\n",
    "    #episode_series.reverse()\n",
    "    policy_loss = []\n",
    "    rewards_list = []\n",
    "    for i in range(len(episode_series)):\n",
    "        j = i\n",
    "        G = 0\n",
    "        alpha = 1 / len(episode_series)\n",
    "        \n",
    "        # get the log_prob of the last state:\n",
    "        gamma_cum = GAMMA\n",
    "        \n",
    "        while j < len(episode_series): \n",
    "            [observation, action, log_prob, reward, done, info] = episode_series[j]\n",
    "            G = G + reward * gamma_cum\n",
    "            \n",
    "            gamma_cum = gamma_cum * GAMMA\n",
    "            j = j + 1\n",
    "        policy_loss.append(G * -log_prob)\n",
    "        rewards_list.append(G)\n",
    "    \n",
    "    if G > 0.0 or True: # Optimize only if rewards are non zero.\n",
    "        #print \"Reward list\"\n",
    "        #print rewards_list\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        times_trained = times_trained + 1\n",
    "    \n",
    "    if G > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1\n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
