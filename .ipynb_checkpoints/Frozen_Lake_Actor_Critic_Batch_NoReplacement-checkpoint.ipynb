{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import random\n",
    "import heapq\n",
    "\n",
    "from gym.envs.registration import register\n",
    "# register(\n",
    "#    id='FrozenLakeNotSlippery-v0',\n",
    "#    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "#    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "#    max_episode_steps=100,\n",
    "#    reward_threshold=0.78, # optimum = .8196\n",
    "# )\n",
    "\n",
    "#env = gym.make('FrozenLake8x8-v0')\n",
    "env = gym.make('FrozenLake-v0')\n",
    "#env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.render()\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "class value_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(value_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 40)\n",
    "        #self.batch1 = nn.BatchNorm1d(40)\n",
    "        self.linear2 = nn.Linear(40, 40, bias=True)\n",
    "        #self.batch2 = nn.BatchNorm1d(40)\n",
    "        self.linear3 = nn.Linear(40, 40, bias=True)\n",
    "        #self.batch2 = nn.BatchNorm1d(40)\n",
    "        self.linear4 = nn.Linear(40, 40, bias=True)\n",
    "        #self.batch2 = nn.BatchNorm1d(40)\n",
    "        self.linear5 = nn.Linear(40, 1, bias=False)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "#         print(x.shape)\n",
    "#         print(x)\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "        x = x.view(-1,1)\n",
    "        x = F.sigmoid(self.linear1(x))\n",
    "        #x = F.softmax(self.linear2(x), dim=0)\n",
    "        #x = self.batch1(x)        \n",
    "        #x = self.dropout(x)\n",
    "        x = F.sigmoid(self.linear2(x))\n",
    "        x = F.sigmoid(self.linear3(x))\n",
    "        x = F.sigmoid(self.linear4(x))\n",
    "        #x = self.batch2(x)        \n",
    "        x = self.linear5(x)\n",
    "        x = x.view(-1,1)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class policy_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(policy_net, self).__init__()\n",
    "        #self.batch1 = nn.BatchNorm1d(1)\n",
    "        self.linear1 = nn.Linear(1, 64)\n",
    "        #self.batch2 = nn.BatchNorm1d(64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64, 64)\n",
    "        self.linear4 = nn.Linear(64, 64)\n",
    "        self.linear5 = nn.Linear(64, 4, bias=False)\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        #print(x\n",
    "        x = x.view(-1,1)\n",
    "        #x = self.batch1(x)\n",
    "        x = F.tanh(self.linear1(x))\n",
    "        #x = self.batch2(x)\n",
    "        #x = self.dropout()\n",
    "        x = F.sigmoid(self.linear2(x))        \n",
    "#         x = F.tanh(self.linear3(x))        \n",
    "#         x = F.tanh(self.linear4(x))        \n",
    "        x = self.linear5(x)\n",
    "        x = x.view(-1,4)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'log_prob','action_prob','log_action_prob', 'next_state', 'reward','entropy_impact' ,'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    \n",
    "    \n",
    "class ReplayMemoryNoReplacement(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.h = []\n",
    "        \n",
    "    def push(self, *args):\n",
    "        random_index = random.random()\n",
    "        heapq.heappush(self.h, (random_index, Transition(*args)))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "    \n",
    "        result = []\n",
    "        for i in range(batch_size):            \n",
    "            result.append(heapq.heappop(self.h)[1])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.h)\n",
    "\n",
    "class ReplayMemoryNew(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.h = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, *args):\n",
    "        tran = Transition(*args)\n",
    "        self.push_transition(tran)\n",
    "\n",
    "    def push_transition(self, tran):\n",
    "        if self.capacity <= len(self.h):\n",
    "            heapq.heappop(self.h)\n",
    "        random_index = random.random()\n",
    "        heapq.heappush(self.h, (random_index, tran))\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        result = []\n",
    "        for i in range(batch_size):\n",
    "            el = heapq.heappop(self.h)[1]\n",
    "            result.append(el)            \n",
    "            heapq.heappush(self.h, (random.random(), el))\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_v_table():\n",
    "    for i in range(16):\n",
    "        st = np.array(get_state_repr(i))\n",
    "        st = np.expand_dims(st, axis=0)\n",
    "        v_net.eval()\n",
    "        action_probs = v_net(FloatTensor(st))\n",
    "        #action_probs = F.softmax(action_probs, dim=1)\n",
    "        outp = \" state (\" +str(i) + \") \"\n",
    "        n = 0\n",
    "        for tensr in action_probs:\n",
    "            for cell in tensr:\n",
    "                outp = outp + \" A[\" + str(n) + \"]:(\" + str(cell.item()) + \")\"\n",
    "                n += 1\n",
    "        print(outp)\n",
    "\n",
    "def print_pi_table():\n",
    "    for i in range(16):\n",
    "        st = np.array(get_state_repr(i))\n",
    "        st = np.expand_dims(st, axis=0)\n",
    "        pi_net.eval()\n",
    "        action_probs = pi_net(FloatTensor(st))\n",
    "        action_probs = F.softmax(action_probs, dim=1)\n",
    "        outp = \" state (\" +str(i) + \") \"\n",
    "        n = 0\n",
    "        for tensr in action_probs:\n",
    "            for cell in tensr:\n",
    "                outp = outp + \" A[\" + str(n) + \"]:(\" + str(cell.item()) + \")\"\n",
    "                n += 1\n",
    "        print(outp)\n",
    "        \n",
    "def get_state_repr(state_idx):\n",
    "    return state_idx * 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'value_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5c0cb7bab863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mNUM_STEPS_VALUE_FUNCTION_LEARNS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mENTROPY_REDUCTION_STEPS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mv_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mv_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mv_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'value_net' is not defined"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "random.seed(1999)\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "# custom weights initialization \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print classname\n",
    "    #print q_net\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.01, 0.02)\n",
    "        #if not m.bias is None:\n",
    "        #    m.bias.data.normal_(0.1, 0.02)\n",
    "        #m.weight.data.uniform_(0.0, 0.02)        \n",
    "        #m.weight.data.fill_(0.01)\n",
    "        if not m.bias is None:\n",
    "            m.bias.data.fill_(0.0)\n",
    "        print m\n",
    "        \n",
    "\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 300\n",
    "GAMMA = 0.99\n",
    "TARGET_UPDATE = 1000\n",
    "PRINT_OUT_TIMES = 1000\n",
    "ENTROPY_REDUCTION_STEPS = 100000.0\n",
    "NUM_EPISODES =           1000000\n",
    "#NUM_STEPS_VALUE_FUNCTION_LEARNS = NUM_EPISODES\n",
    "NUM_STEPS_VALUE_FUNCTION_LEARNS = (ENTROPY_REDUCTION_STEPS * 1)\n",
    "\n",
    "v_net = value_net()\n",
    "v_net.apply(weights_init)\n",
    "v_net.to(device)\n",
    "target_v_net = value_net()\n",
    "target_v_net.load_state_dict(v_net.state_dict())\n",
    "target_v_net.to(device)\n",
    "pi_net = policy_net()\n",
    "pi_net.apply(weights_init).to(device)\n",
    "\n",
    "# prepare for optimizer, merge both networks parameters\n",
    "\n",
    "# parameters = set()\n",
    "# for net_ in [v_net, pi_net]:\n",
    "#     parameters |= set(net_.parameters())\n",
    "\n",
    "#optimizer = optim.RMSprop(online_net.parameters(), lr=0.001)\n",
    "\n",
    "#optimizer = optim.Adam(parameters, lr=0.0001)\n",
    "\n",
    "\n",
    "v_optimizer = optim.Adam(v_net.parameters(), lr=0.0001)\n",
    "pi_optimizer =  optim.Adam(pi_net.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "#scheduler = StepLR(v_optimizer, step_size=10000, gamma=0.5)\n",
    "\n",
    "\n",
    "MEMORY_SIZE = 2000\n",
    "#memory = ReplayMemoryNoReplacement(MEMORY_SIZE)\n",
    "memory = ReplayMemoryNew(MEMORY_SIZE)\n",
    "#memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "value_loss_cum = []\n",
    "\n",
    "def get_expected_value_fixed(s):\n",
    "    r = 0\n",
    "    if s == 0:\n",
    "        r = 0.050\n",
    "    elif s == 1:\n",
    "        r = 0.092\n",
    "    elif s == 2:\n",
    "        r = 0.083\n",
    "    elif s == 3:\n",
    "        r = 0.1258\n",
    "    elif s == 4:\n",
    "        r = 0.1235\n",
    "    elif s == 5:\n",
    "        r = 0.0\n",
    "    elif s == 6:\n",
    "        r = 0.1421\n",
    "    elif s == 7:\n",
    "        r = 0.0\n",
    "    elif s == 8:\n",
    "        r = 0.203364819288\n",
    "    elif s == 9:\n",
    "        r = 0.349448651075\n",
    "    elif s == 10:\n",
    "        r = 0.393933832645\n",
    "    elif s == 11:\n",
    "        r = 0.0\n",
    "    elif s == 12:\n",
    "        r = 0.0\n",
    "    elif s == 13:\n",
    "        r = 0.565665841103\n",
    "    elif s == 14:\n",
    "        r = 0.99\n",
    "    elif s == 15:\n",
    "        r = 0.0\n",
    "    return r\n",
    "\n",
    "def optimize(k):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "        \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    final_mask = torch.tensor(tuple(map(lambda d: d is True,\n",
    "                                         batch.done)), device=device, dtype=torch.uint8).unsqueeze(1)\n",
    "    final_mask_list = [d for d in batch.done if d is True ]\n",
    "    # Compute states that are final.\n",
    "#     next_state_final_mask = torch.tensor(tuple(map(lambda d: (d) in [5,7,11,12,15],\n",
    "#                                           batch.next_state)), device=device, dtype=torch.uint8).unsqueeze(1) \n",
    "#     next_state_finak_list = [d for d in batch.next_state if d in [5,7,11,12,15] ]\n",
    "    \n",
    "    \n",
    "    # Unpack the parameters from the memory\n",
    "        \n",
    "    state_batch = FloatTensor(batch.state)\n",
    "    state_batch = state_batch.view(BATCH_SIZE, 1)\n",
    "    next_state_batch = FloatTensor(batch.next_state)\n",
    "    next_state_batch = next_state_batch.view(BATCH_SIZE, 1)    \n",
    "    action_batch = LongTensor(batch.action).view(BATCH_SIZE,1)    \n",
    "    reward_batch = Tensor(batch.reward).view(BATCH_SIZE,1)            \n",
    "    entropy_impact_batch = FloatTensor(batch.entropy_impact).view(BATCH_SIZE,1)\n",
    "    #log_prob_batch = torch.cat(batch.log_prob).view(BATCH_SIZE, 1)\n",
    "    #action_probs_batch = torch.cat(batch.action_prob).view(BATCH_SIZE,4)\n",
    "    #log_action_probs_batch = torch.cat(batch.log_action_prob).view(BATCH_SIZE,4)\n",
    "    \n",
    "    \n",
    "    #FIRST , calculate V(next_state)and backpropagate MSE on V\n",
    "    \n",
    "        \n",
    "    target_v_net.eval()\n",
    "    v_next = target_v_net(next_state_batch).detach()\n",
    "    #v_next[next_state_final_mask] = torch.zeros(len(next_state_finak_list), device=device).view(len(next_state_finak_list))\n",
    "    v_next[final_mask] = torch.zeros(len(final_mask_list), device=device).view(len(final_mask_list))\n",
    "    \n",
    "    \n",
    "    ##HACK FIXING expected value\n",
    "#     v_current_fixed = [get_expected_value_fixed(_st) for _st in batch.state]\n",
    "#     v_current_fixed = FloatTensor(v_current_fixed).view(BATCH_SIZE,1)    \n",
    "    ##HACK FIXING expected value\n",
    "    \n",
    "    ##HACK FIXING current value\n",
    "#     v_next_fixed = [get_expected_value_fixed(_st) for _st in batch.next_state]\n",
    "#     v_next_fixed = FloatTensor(v_next_fixed).view(BATCH_SIZE,1)    \n",
    "    #v_next = v_next_fixed\n",
    "    ##HACK FIXING current value\n",
    "    \n",
    "    \n",
    "    expected_value = reward_batch + v_next * GAMMA\n",
    "    \n",
    "    \n",
    "    ##HACK FIXING expected value\n",
    "    #expected_value = expected_value_fixed\n",
    "    ##HACK FIXING expected value\n",
    "    \n",
    "    \n",
    "    # calculate V(current_state)\n",
    "    if k <= NUM_STEPS_VALUE_FUNCTION_LEARNS:\n",
    "        v_net.train()\n",
    "    else:\n",
    "        v_net.eval()\n",
    "        \n",
    "    v_current = v_net(state_batch)\n",
    "    \n",
    "    # backpropagate:\n",
    "    value_loss = torch.sum((expected_value - v_current)** 2)\n",
    "    \n",
    "    if k <= NUM_STEPS_VALUE_FUNCTION_LEARNS:\n",
    "        v_optimizer.zero_grad()\n",
    "        #value_loss.backward(retain_graph=True) # keep graph for policy net optimizer\n",
    "        value_loss.backward() # keep graph for policy net optimizer\n",
    "        v_optimizer.step()\n",
    "        #scheduler.step()\n",
    "    \n",
    "    value_loss_cum.append(value_loss.item())\n",
    "    \n",
    "    v_current = v_current.detach()\n",
    "    \n",
    "    \n",
    "    ##HACK FIXING expected value\n",
    "    #v_current = v_current_fixed\n",
    "    ##HACK FIXING expected value\n",
    "    \n",
    "    \n",
    "    # SECOND, calculate gradient loss:\n",
    "    # H(X) = P(X) log ( P(X) )\n",
    "\n",
    "    # calculate the action probability\n",
    "    actions_distr = pi_net(state_batch)\n",
    "    actions_prob_batch = F.softmax(actions_distr, dim=1)\n",
    "    log_actions_prob_batch = F.log_softmax(actions_distr, dim=1)\n",
    "    \n",
    "    action_batch = action_batch\n",
    "    action_mask = FloatTensor(BATCH_SIZE, 4).zero_()    \n",
    "    action_mask.scatter_(1,action_batch,1) # This will have shape (BATCH_SIZE, 4), and its contents will be \n",
    "                                            # like : [[0,0,1,0],[1,0,0,0],...]\n",
    "    #log_prob_batch = log_actions_prob_batch.gather(1,action_batch)\n",
    "    log_prob_batch = torch.sum(log_actions_prob_batch * action_mask, dim=1).view(BATCH_SIZE,1) # sum up across rows (ending tensor is shape (BATCH_SIZE, 1))\n",
    "    \n",
    "    entropy = entropy_impact_batch * torch.sum(actions_prob_batch * log_actions_prob_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    policy_loss = torch.sum( -log_prob_batch * (expected_value - v_current) + entropy) \n",
    "    \n",
    "    pi_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    pi_optimizer.step()\n",
    "    \n",
    "    return policy_loss.item(), value_loss.item()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_EPISODES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c75f080fb350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_EPISODES' is not defined"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "steps_done = 0\n",
    "\n",
    "policy_loss_avg = [1.0]\n",
    "v_loss_avg = [1.0]\n",
    "\n",
    "\n",
    "TARGET_UPDATE = 1000\n",
    "\n",
    "\n",
    "\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    reward = 0\n",
    "    episode_step = 0\n",
    "    #print(\"b\")\n",
    "    I = 1.0\n",
    "    #entropy_impact = (ENTROPY_REDUCTION_STEPS - k) / ENTROPY_REDUCTION_STEPS\n",
    "    if k == 0:\n",
    "        entropy_impact = 1.0\n",
    "    else:\n",
    "        entropy_impact = min(1, (1 / (k * 0.005)))\n",
    "        \n",
    "    if k > ENTROPY_REDUCTION_STEPS:\n",
    "        entropy_impact = 0.0\n",
    "        \n",
    "    # test entropy always 0\n",
    "    # entropy_impact = 0.0\n",
    "        \n",
    "    #entropy_impact = 0.0\n",
    "    #if entropy_impact < 0.0:\n",
    "    #    entropy_impact = 0\n",
    "    while not done:\n",
    "        #print(\"c\")\n",
    "        steps_done += 1\n",
    "        \n",
    "        # Get action from pi\n",
    "        np_observation = np.array(get_state_repr(observation))\n",
    "        np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        #print(np_observation)\n",
    "        observation_tensor = FloatTensor(np_observation)\n",
    "\n",
    "        # action distribution\n",
    "        pi_net.eval()\n",
    "        action_distr = pi_net(observation_tensor)\n",
    "        action_probs = F.softmax(action_distr, dim=1)\n",
    "        log_action_probs = 0\n",
    "        #log_action_probs = F.log_softmax(action_distr, dim=1)\n",
    "        # Decide on an action based on the distribution\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        log_prob = m.log_prob(action).unsqueeze(1)\n",
    "            \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        old_state = observation                    \n",
    "            \n",
    "        \n",
    "        observation, reward, done, info = env.step(action.item()) \n",
    "        new_state = observation\n",
    "        \n",
    "        if k%5000 == 0:\n",
    "            #print(\"old_state != new_state\")\n",
    "            #print(old_state != new_state)\n",
    "            #print(\"oldstate \" + str(old_state) + \" newstate \" + str(new_state))\n",
    "            print(\"action_dist \")\n",
    "            print(action_probs)\n",
    "            print(\"On state=\"+ str(old_state) + \", selected action=\" + str(action.item()) )\n",
    "            print(\"new state=\"+ str(new_state) + \", done=\"+str(done) + \\\n",
    "             \". Reward: \" + str(reward))\n",
    "\n",
    "        # Perform one step of the optimization        \n",
    "#         policy_loss, value_loss = optimize_model(I, \\\n",
    "#                                                  old_state, \\\n",
    "#                                                  log_prob, \\\n",
    "#                                                  log_actions_probs, \\\n",
    "#                                                  action_probs, \\\n",
    "#                                                  reward, \\\n",
    "#                                                  new_state, \\\n",
    "#                                                  entropy_impact, \\\n",
    "#                                                  done)\n",
    "        \n",
    "#         I = I * GAMMA\n",
    "        #if (not done) or (done and new_state in [5,7,11,12,15]):\n",
    "        memory.push(get_state_repr(old_state), action.item(), log_prob, action_probs, log_action_probs, get_state_repr(new_state), reward, entropy_impact, done)\n",
    "        \n",
    "\n",
    "            \n",
    "        if len(memory) >= MEMORY_SIZE:\n",
    "            policy_loss, value_loss = optimize(k)\n",
    "            if len(policy_loss_avg) < PRINT_OUT_TIMES :\n",
    "                policy_loss_avg.append(policy_loss)\n",
    "                v_loss_avg.append(value_loss)\n",
    "            else:\n",
    "                policy_loss_avg[episode_step % PRINT_OUT_TIMES] = policy_loss\n",
    "                v_loss_avg[episode_step % PRINT_OUT_TIMES] = value_loss\n",
    "        \n",
    "        times_trained = times_trained + 1\n",
    "\n",
    "        episode_step += 1\n",
    "        #env.render()\n",
    "        \n",
    "    \n",
    "    if k % PRINT_OUT_TIMES ==0:\n",
    "        print_pi_table()\n",
    "        print_v_table()\n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "    \n",
    "    if k % TARGET_UPDATE == 0:\n",
    "        target_v_net.load_state_dict(v_net.state_dict())\n",
    "    \n",
    "\n",
    "    if k%PRINT_OUT_TIMES == 0:\n",
    "        print(\"Episode {} finished after {} . Running score: {}. Policy_loss: {}, Value_loss: {}. Times trained: \\\n",
    "              {}. Times reached goal: {}. \\\n",
    "              Steps done: {}.\".format(k, episode_step, np.mean(score), np.mean(policy_loss_avg),\\\n",
    "                                      np.mean(v_loss_avg) , times_trained, \\\n",
    "                                                                       times_reach_goal, steps_done))\n",
    "        #print(\"policy_loss_avg\")\n",
    "        #print(policy_loss_avg)\n",
    "        #print(\"value_loss_avg\")\n",
    "        #print(v_loss_avg)\n",
    "#         print(\"times_reach_goal\")\n",
    "#         print(times_reach_goal)\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "        #print(\"Game finished. \" + \"-\" * 5)\n",
    "        #print(len(episode_series))\n",
    "#         for param in net.parameters():\n",
    "#             print(param.data)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if reward > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "use_cuda = torch.cuda.is_available\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "np_observation = np.array(0.0)\n",
    "np_observation = np.expand_dims(np_observation, axis=0)\n",
    "observation_tensor = FloatTensor(np_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc168462a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(value_loss_cum)\n",
    "#plt.plot([1,2,3,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_pi_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0cfce2eb8193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_pi_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'print_pi_table' is not defined"
     ]
    }
   ],
   "source": [
    "print_pi_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
