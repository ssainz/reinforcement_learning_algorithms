{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "   id='FrozenLakeNotSlippery-v0',\n",
    "   entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "   kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "   max_episode_steps=100,\n",
    "   reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "\n",
    "#env = gym.make('FrozenLake8x8-v0')\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.render()\n",
    "class pi_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pi_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 20)\n",
    "        self.linear2 = nn.Linear(20, 4 + 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        #x = F.softmax(self.linear2(x), dim=0)\n",
    "        x = self.linear2(x)\n",
    "        x = x.view(-1,5)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        actions = x[:,:4]\n",
    "        value = x[:,4]\n",
    "        return actions, value\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_probs_orig \n",
      "tensor([[ 2.9848,  2.8283,  3.1146,  3.1600]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 2.9848,  2.8283,  3.1146,  3.1600]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 2.9848,  2.8283,  3.1146,  3.1600]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.1218,  5.9652,  6.2515,  6.2969]])\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 2.9848,  2.8283,  3.1146,  3.1600]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 20.6047,  20.4481,  20.7344,  20.7798]])\n",
      "On state=4, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 2.9848,  2.8283,  3.1146,  3.1600]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 2.9848,  2.8283,  3.1146,  3.1600]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.1218,  5.9652,  6.2515,  6.2969]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 0 finished after 9 timesteps with r=0.0. Running score: 0.0. Times trained: 0. Times reached goal: 0.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-24ed097b3355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#print(\"action_probs after dropout\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#print(action_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m#print(\"action_probs after softmax\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#print(action_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sergio/.local/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# custom weights initialization \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        #m.weight.data.normal_(0.0, 0.02)\n",
    "        #m.weight.data.uniform_(0.0, 0.02)\n",
    "        m.weight.data.fill_(0.5)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "NUM_EPISODES = 1000000\n",
    "GAMMA = 0.9\n",
    "net = pi_net()\n",
    "net.apply(weights_init)\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    episode_series = []\n",
    "    while not done:\n",
    "        # Get action from pi\n",
    "        # action = env.action_space.sample()\n",
    "        np_observation = np.array(observation)\n",
    "        #np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        observation_tensor = torch.FloatTensor(np_observation) \n",
    "        #print(observation_tensor)\n",
    "        #net.eval()\n",
    "        #print(\"before eval\")\n",
    "        action_probs, value = net(observation_tensor)\n",
    "        action_probs_orig = action_probs\n",
    "        #print(\"action_probs after net\")\n",
    "        #print(action_probs)\n",
    "        #FOR EXPLORATION: \n",
    "        #action_probs = F.dropout(action_probs, p=0.3, training=True)\n",
    "        #print(\"action_probs after dropout\")\n",
    "        #print(action_probs)\n",
    "        action_probs = F.softmax(action_probs, dim=1)\n",
    "        #print(\"action_probs after softmax\")\n",
    "        #print(action_probs)\n",
    "        #action = action_probs.multinomial(num_samples=1)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"after eval\")\n",
    "        #print(\"action_probs\")\n",
    "        #print(action_probs)\n",
    "        log_prob = m.log_prob(action)\n",
    "        #print(\"log_prob\")\n",
    "        #print(log_prob)\n",
    "        #break\n",
    "        #print(\"softmax\")\n",
    "        #print(action_probs)\n",
    "        #print(\"action\")\n",
    "        #print(str(action.item()))\n",
    "        #print(type(prob.multinomial))\n",
    "        \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        \n",
    "        if k%10000 == 0:\n",
    "            print(\"action_probs_orig \")\n",
    "            print(action_probs_orig)\n",
    "            print(\"On state=\"+ str(observation) + \", selected action=\" + str(action.item()) + \" , \")\n",
    "        \n",
    "        observation, reward, done, info = env.step(action.item()) \n",
    "        \n",
    "        if k%10000 == 0:\n",
    "            print(\"new state=\"+ str(observation) + \", done=\"+str(done))\n",
    "        #if done and reward != 1.0:\n",
    "        #    reward = -1.0\n",
    "        step_data = [observation, action,log_prob, value, reward, done, info]\n",
    "        episode_series.append(step_data)\n",
    "        #env.render()\n",
    "        \n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "\n",
    "    if k%1000 == 0:\n",
    "        print(\"Episode {} finished after {} timesteps with r={}. Running score: {}. Times trained: {}. Times reached goal: {}.\".format(k, len(episode_series), reward, np.mean(score), times_trained, times_reach_goal))\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "        #print(\"Game finished. \" + \"-\" * 5)\n",
    "        #print(len(episode_series))\n",
    "#         for param in net.parameters():\n",
    "#             print(param.data)\n",
    "        \n",
    "\n",
    "    \n",
    "    discounted_rewards = []\n",
    "    actions = []\n",
    "    values = []\n",
    "    log_probs = []\n",
    "    \n",
    "    R = 0\n",
    "    for i in reversed(range(len(episode_series))): # Build the different containers: action, value, reward\n",
    "        [observation, action, log_prob, value, reward, done, info] = episode_series[i]\n",
    "        R = reward + R * GAMMA\n",
    "        discounted_rewards.append(R)\n",
    "        actions.append(action)\n",
    "        values.append(value)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "    value_loss = 0\n",
    "    policy_log_loss = 0\n",
    "    for action, value, log_prob, reward in zip(actions, values, log_probs, discounted_rewards):\n",
    "        reward_diff = reward - value.item() # Treat critic value as baseline\n",
    "        #action.reinforce(reward_diff) # Try to perform better than baseline\n",
    "        policy_log_loss += -log_prob * reward_diff\n",
    "        value_loss += mse(value, torch.Tensor([reward])) # Compare with actual reward\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    if R > 0.0 or True: # Optimize only if rewards are non zero.\n",
    "        #print \"Reward list\"\n",
    "        #print rewards_list\n",
    "        optimizer.zero_grad()\n",
    "        nodes = [value_loss] + [policy_log_loss]\n",
    "        gradients = [torch.ones(1)] + [torch.ones(1)] # No gradients for reinforced values\n",
    "        autograd.backward(nodes, gradients)\n",
    "        optimizer.step()\n",
    "        times_trained = times_trained + 1\n",
    "    \n",
    "    if R > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
