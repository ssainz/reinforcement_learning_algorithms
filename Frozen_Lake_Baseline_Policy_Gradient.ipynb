{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "#register(\n",
    "#    id='FrozenLakeNotSlippery-v0',\n",
    "#    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "#    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "#    max_episode_steps=100,\n",
    "#    reward_threshold=0.78, # optimum = .8196\n",
    "#)\n",
    "\n",
    "#env = gym.make('FrozenLake8x8-v0')\n",
    "env = gym.make('FrozenLake-v0')\n",
    "#env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.render()\n",
    "class pi_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pi_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 20)\n",
    "        self.linear2 = nn.Linear(20, 4 + 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        #x = F.softmax(self.linear2(x), dim=0)\n",
    "        x = self.linear2(x)\n",
    "        x = x.view(-1,5)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        actions = x[:,:4]\n",
    "        value = x[:,4]\n",
    "        return actions, value\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_probs_orig \n",
      "tensor([[ 3.3964,  3.3324,  3.4507,  3.6962]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7988,  6.7349,  6.8532,  7.0986]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.1356,  11.0716,  11.1900,  11.4354]])\n",
      "On state=2, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 6.7988,  6.7349,  6.8532,  7.0986]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 11.1356,  11.0716,  11.1900,  11.4354]])\n",
      "On state=2, selected action=3 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 16.1356,  16.0716,  16.1900,  16.4354]])\n",
      "On state=3, selected action=0 , \n",
      "new state=7, done=True\n",
      "Episode 0 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 0. Times reached goal: 0.\n",
      "Episode 1000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 13.\n",
      "Episode 2000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 18.\n",
      "Episode 3000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 11.\n",
      "Episode 4000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 20.\n",
      "Episode 5000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 16.\n",
      "Episode 6000 finished after 10 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 20.\n",
      "Episode 7000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 20.\n",
      "Episode 8000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 16.\n",
      "Episode 9000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 10.\n",
      "action_probs_orig \n",
      "tensor([[ 1.2362,  1.1908,  1.2881,  1.5002]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.6458,  1.6045,  1.6950,  1.9011]])\n",
      "On state=1, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.2362,  1.1908,  1.2881,  1.5002]])\n",
      "On state=0, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.6458,  1.6045,  1.6950,  1.9011]])\n",
      "On state=1, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.6458,  1.6045,  1.6950,  1.9011]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 10000 finished after 5 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 17.\n",
      "Episode 11000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 14.\n",
      "Episode 12000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 14.\n",
      "Episode 13000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 14.\n",
      "Episode 14000 finished after 15 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 13.\n",
      "Episode 15000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 12.\n",
      "Episode 16000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 17.\n",
      "Episode 17000 finished after 16 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 13.\n",
      "Episode 18000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 10.\n",
      "Episode 19000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 17.\n",
      "action_probs_orig \n",
      "tensor([[ 1.7054,  1.8124,  2.0025,  1.9412]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.7054,  1.8124,  2.0025,  1.9412]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.7054,  1.8124,  2.0025,  1.9412]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.7054,  1.8124,  2.0025,  1.9412]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.7836,  1.8967,  2.0888,  2.0175]])\n",
      "On state=4, selected action=3 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.7836,  1.8967,  2.0888,  2.0175]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 20000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 17.\n",
      "Episode 21000 finished after 3 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 15.\n",
      "Episode 22000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 15.\n",
      "Episode 23000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 10.\n",
      "Episode 24000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 10.\n",
      "Episode 25000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 14.\n",
      "Episode 26000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 18.\n",
      "Episode 27000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 12.\n",
      "Episode 28000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 24.\n",
      "Episode 29000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 22.\n",
      "action_probs_orig \n",
      "tensor([[ 1.5463,  2.0033,  2.1522,  1.7321]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.5718,  2.0352,  2.1844,  1.7565]])\n",
      "On state=1, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.5718,  2.0352,  2.1844,  1.7565]])\n",
      "On state=1, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.5463,  2.0033,  2.1522,  1.7321]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.5463,  2.0033,  2.1522,  1.7321]])\n",
      "On state=0, selected action=3 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.5718,  2.0352,  2.1844,  1.7565]])\n",
      "On state=1, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 30000 finished after 6 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 23.\n",
      "Episode 31000 finished after 10 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 19.\n",
      "Episode 32000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 25.\n",
      "Episode 33000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 15.\n",
      "Episode 34000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 16.\n",
      "Episode 35000 finished after 20 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 28.\n",
      "Episode 36000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 24.\n",
      "Episode 37000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 26.\n",
      "Episode 38000 finished after 9 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 25.\n",
      "Episode 39000 finished after 6 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 26.\n",
      "action_probs_orig \n",
      "tensor([[ 1.2980,  2.1764,  2.3525,  1.4658]])\n",
      "On state=0, selected action=2 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.2980,  2.1764,  2.3525,  1.4658]])\n",
      "On state=0, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.2980,  2.1764,  2.3525,  1.4658]])\n",
      "On state=0, selected action=0 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.3408,  2.2431,  2.4198,  1.5079]])\n",
      "On state=4, selected action=3 , \n",
      "new state=5, done=True\n",
      "Episode 40000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 21.\n",
      "Episode 41000 finished after 9 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 21.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 42000 finished after 10 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 33.\n",
      "Episode 43000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 31.\n",
      "Episode 44000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 25.\n",
      "Episode 45000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 16.\n",
      "Episode 46000 finished after 7 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 35.\n",
      "Episode 47000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 36.\n",
      "Episode 48000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 20.\n",
      "Episode 49000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 29.\n",
      "action_probs_orig \n",
      "tensor([[ 1.1751,  2.2906,  2.4858,  1.2577]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.1751,  2.2906,  2.4858,  1.2577]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.1751,  2.2906,  2.4858,  1.2577]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.1875,  2.3120,  2.5076,  1.2692]])\n",
      "On state=1, selected action=3 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.1751,  2.2906,  2.4858,  1.2577]])\n",
      "On state=0, selected action=2 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.1751,  2.2906,  2.4858,  1.2577]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.2361,  2.3994,  2.5969,  1.3190]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 50000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 24.\n",
      "Episode 51000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 18.\n",
      "Episode 52000 finished after 12 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 25.\n",
      "Episode 53000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 54000 finished after 17 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 55000 finished after 8 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 33.\n",
      "Episode 56000 finished after 7 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 32.\n",
      "Episode 57000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 58000 finished after 24 timesteps with r=1.0. Running score: 0.03. Times trained: 1000. Times reached goal: 33.\n",
      "Episode 59000 finished after 8 timesteps with r=1.0. Running score: 0.04. Times trained: 1000. Times reached goal: 28.\n",
      "action_probs_orig \n",
      "tensor([[ 1.1668,  2.4105,  2.5164,  0.9989]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.1843,  2.4423,  2.5480,  1.0128]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.2017,  2.4741,  2.5796,  1.0267]])\n",
      "On state=2, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.2192,  2.5058,  2.6113,  1.0407]])\n",
      "On state=3, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.2192,  2.5058,  2.6113,  1.0407]])\n",
      "On state=3, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.2192,  2.5058,  2.6113,  1.0407]])\n",
      "On state=3, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.2192,  2.5058,  2.6113,  1.0407]])\n",
      "On state=3, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 60000 finished after 7 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 28.\n",
      "Episode 61000 finished after 13 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 62000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 63000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 26.\n",
      "Episode 64000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 33.\n",
      "Episode 65000 finished after 10 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 31.\n",
      "Episode 66000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 34.\n",
      "Episode 67000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 32.\n",
      "Episode 68000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 23.\n",
      "Episode 69000 finished after 8 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 44.\n",
      "action_probs_orig \n",
      "tensor([[ 1.0142,  2.5758,  2.5004,  0.8341]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.1098,  2.8041,  2.7179,  0.9314]])\n",
      "On state=4, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.0142,  2.5758,  2.5004,  0.8341]])\n",
      "On state=0, selected action=2 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.0142,  2.5758,  2.5004,  0.8341]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.1098,  2.8041,  2.7179,  0.9314]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.1098,  2.8041,  2.7179,  0.9314]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 70000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 34.\n",
      "Episode 71000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 26.\n",
      "Episode 72000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 32.\n",
      "Episode 73000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 32.\n",
      "Episode 74000 finished after 14 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 75000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 34.\n",
      "Episode 76000 finished after 11 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 77000 finished after 9 timesteps with r=1.0. Running score: 0.07. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 78000 finished after 15 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 33.\n",
      "Episode 79000 finished after 9 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 31.\n",
      "action_probs_orig \n",
      "tensor([[ 0.8938,  2.6815,  2.5057,  0.6415]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.8938,  2.6815,  2.5057,  0.6415]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.8938,  2.6815,  2.5057,  0.6415]])\n",
      "On state=0, selected action=2 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.9963,  2.9856,  2.7897,  0.7545]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.2305,  3.7508,  3.5162,  1.0836]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 1.2305,  3.7508,  3.5162,  1.0836]])\n",
      "On state=8, selected action=2 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.9963,  2.9856,  2.7897,  0.7545]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 80000 finished after 7 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 32.\n",
      "Episode 81000 finished after 4 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 82000 finished after 9 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 39.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 83000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 84000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 85000 finished after 7 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 86000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 87000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 33.\n",
      "Episode 88000 finished after 8 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 89000 finished after 12 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 36.\n",
      "action_probs_orig \n",
      "tensor([[ 0.8080,  2.7033,  2.5863,  0.3624]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.9213,  3.0826,  2.9547,  0.4756]])\n",
      "On state=4, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 90000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 91000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 92000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 93000 finished after 12 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 94000 finished after 17 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 95000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 96000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 36.\n",
      "Episode 97000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 98000 finished after 7 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 99000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 42.\n",
      "action_probs_orig \n",
      "tensor([[ 0.6596,  2.8446,  2.5784, -0.0126]])\n",
      "On state=0, selected action=2 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.7662,  3.3020,  3.0111,  0.0860]])\n",
      "On state=4, selected action=2 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.6596,  2.8446,  2.5784, -0.0126]])\n",
      "On state=0, selected action=2 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.6596,  2.8446,  2.5784, -0.0126]])\n",
      "On state=0, selected action=2 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.7662,  3.3020,  3.0111,  0.0860]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 100000 finished after 5 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 101000 finished after 8 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 102000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 103000 finished after 7 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 104000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 105000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 106000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 107000 finished after 6 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 32.\n",
      "Episode 108000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 109000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 43.\n",
      "action_probs_orig \n",
      "tensor([[ 0.5016,  2.8970,  2.5740, -0.2770]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.5016,  2.8970,  2.5740, -0.2770]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.5016,  2.8970,  2.5740, -0.2770]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.5016,  2.8970,  2.5740, -0.2770]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.5125,  2.9402,  2.6143, -0.2742]])\n",
      "On state=1, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.5125,  2.9402,  2.6143, -0.2742]])\n",
      "On state=1, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 110000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 111000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 112000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 113000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 114000 finished after 7 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 115000 finished after 6 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 116000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 117000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 118000 finished after 19 timesteps with r=1.0. Running score: 0.08. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 119000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 40.\n",
      "action_probs_orig \n",
      "tensor([[ 0.4184,  2.9518,  2.5509, -0.4990]])\n",
      "On state=0, selected action=0 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.4184,  2.9518,  2.5509, -0.4990]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.4297,  3.0034,  2.5982, -0.4987]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.4426,  3.0833,  2.6728, -0.4928]])\n",
      "On state=2, selected action=0 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.5208,  3.9244,  3.4830, -0.3488]])\n",
      "On state=6, selected action=0 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.4426,  3.0833,  2.6728, -0.4928]])\n",
      "On state=2, selected action=2 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.4590,  3.2296,  2.8113, -0.4736]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 120000 finished after 7 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 121000 finished after 10 timesteps with r=1.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 122000 finished after 12 timesteps with r=1.0. Running score: 0.07. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 123000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 124000 finished after 10 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 125000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 126000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 127000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 128000 finished after 19 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 129000 finished after 6 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 41.\n",
      "action_probs_orig \n",
      "tensor([[ 0.1815,  3.0251,  2.5011, -0.7222]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.1815,  3.0251,  2.5011, -0.7222]])\n",
      "On state=0, selected action=0 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.2179,  3.5333,  2.9714, -0.6872]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 130000 finished after 3 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 54.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 131000 finished after 8 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 35.\n",
      "Episode 132000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 133000 finished after 8 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 134000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 135000 finished after 7 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 34.\n",
      "Episode 136000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 33.\n",
      "Episode 137000 finished after 6 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 138000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 139000 finished after 12 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 35.\n",
      "action_probs_orig \n",
      "tensor([[ 0.0570,  3.0994,  2.4140, -0.9868]])\n",
      "On state=0, selected action=2 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.0636,  3.1524,  2.4600, -0.9919]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.0691,  3.2312,  2.5299, -0.9942]])\n",
      "On state=2, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.0691,  3.2312,  2.5299, -0.9942]])\n",
      "On state=2, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.0691,  3.2312,  2.5299, -0.9942]])\n",
      "On state=2, selected action=2 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[ 0.0651,  4.0757,  3.3136, -0.9215]])\n",
      "On state=6, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 140000 finished after 6 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 141000 finished after 9 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 142000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 143000 finished after 8 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 144000 finished after 14 timesteps with r=1.0. Running score: 0.07. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 145000 finished after 11 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 146000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 35.\n",
      "Episode 147000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 148000 finished after 9 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 149000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 53.\n",
      "action_probs_orig \n",
      "tensor([[-0.1190,  3.0619,  2.4398, -1.2727]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-0.1258,  3.4804,  2.8388, -1.2842]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 150000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 151000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 33.\n",
      "Episode 152000 finished after 14 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 153000 finished after 2 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 154000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 155000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 156000 finished after 9 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 157000 finished after 18 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 158000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 159000 finished after 6 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 37.\n",
      "action_probs_orig \n",
      "tensor([[-0.3771,  3.2294,  2.3101, -1.5430]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-0.3771,  3.2294,  2.3101, -1.5430]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-0.4143,  3.6951,  2.7312, -1.5714]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-0.4143,  3.6951,  2.7312, -1.5714]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-0.4143,  3.6951,  2.7312, -1.5714]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-0.4143,  3.6951,  2.7312, -1.5714]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 160000 finished after 6 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 161000 finished after 9 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 162000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 163000 finished after 5 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 164000 finished after 8 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 165000 finished after 11 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 166000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 167000 finished after 2 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 36.\n",
      "Episode 168000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 169000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 41.\n",
      "action_probs_orig \n",
      "tensor([[-0.7173,  3.2957,  2.2939, -1.8918]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-0.7970,  3.8838,  2.8019, -1.9744]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-0.9704,  5.0207,  3.8306, -2.0128]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-0.9990,  5.4451,  4.2636, -1.9642]])\n",
      "On state=9, selected action=1 , \n",
      "new state=10, done=False\n",
      "action_probs_orig \n",
      "tensor([[-1.0275,  5.8708,  4.6983, -1.9150]])\n",
      "On state=10, selected action=1 , \n",
      "new state=11, done=True\n",
      "Episode 170000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 171000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 172000 finished after 11 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 173000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 174000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 175000 finished after 13 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 176000 finished after 6 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 177000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 178000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 179000 finished after 7 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_probs_orig \n",
      "tensor([[-1.1258,  3.3118,  2.2872, -2.1427]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-1.1258,  3.3118,  2.2872, -2.1427]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-1.1355,  3.3774,  2.3402, -2.1657]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-1.1614,  3.5098,  2.4518, -2.1926]])\n",
      "On state=2, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-1.1959,  3.6776,  2.5946, -2.2215]])\n",
      "On state=3, selected action=2 , \n",
      "new state=7, done=True\n",
      "Episode 180000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 181000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 182000 finished after 7 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 183000 finished after 2 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 184000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 185000 finished after 10 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 186000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 187000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 188000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 189000 finished after 10 timesteps with r=1.0. Running score: 0.07. Times trained: 1000. Times reached goal: 45.\n",
      "action_probs_orig \n",
      "tensor([[-1.4884,  3.2779,  2.3010, -2.4685]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-1.4884,  3.2779,  2.3010, -2.4685]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-1.4884,  3.2779,  2.3010, -2.4685]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-1.6311,  3.8169,  2.7668, -2.5801]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 190000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 191000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 192000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 193000 finished after 12 timesteps with r=1.0. Running score: 0.04. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 194000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 195000 finished after 8 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 196000 finished after 10 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 197000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 198000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 199000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 41.\n",
      "action_probs_orig \n",
      "tensor([[-1.8188,  3.3925,  2.2171, -2.6502]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-1.8396,  3.4628,  2.2711, -2.6824]])\n",
      "On state=1, selected action=2 , \n",
      "new state=5, done=True\n",
      "Episode 200000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 201000 finished after 5 timesteps with r=0.0. Running score: 0.13. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 202000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 203000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 36.\n",
      "Episode 204000 finished after 4 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 205000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 206000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 207000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 208000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 209000 finished after 14 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 43.\n",
      "action_probs_orig \n",
      "tensor([[-2.1469,  3.4794,  2.1460, -2.8739]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-2.4143,  4.2312,  2.7313, -3.0844]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-2.8381,  5.5349,  3.7994, -3.2858]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-2.8381,  5.5349,  3.7994, -3.2858]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 210000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 211000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 212000 finished after 4 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 213000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 214000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 215000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 216000 finished after 8 timesteps with r=0.0. Running score: 0.12. Times trained: 1000. Times reached goal: 61.\n",
      "Episode 217000 finished after 9 timesteps with r=1.0. Running score: 0.05. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 218000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 219000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 37.\n",
      "action_probs_orig \n",
      "tensor([[-2.4678,  3.4769,  2.1265, -3.0108]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-2.7058,  4.0659,  2.6016, -3.1662]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.1125,  5.1993,  3.5619, -3.3208]])\n",
      "On state=8, selected action=2 , \n",
      "new state=12, done=True\n",
      "Episode 220000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 221000 finished after 7 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 222000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 223000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 224000 finished after 6 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 225000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 226000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 227000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 36.\n",
      "Episode 228000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 229000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_probs_orig \n",
      "tensor([[-2.6708,  3.5005,  2.1172, -3.1975]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-2.9382,  4.1187,  2.6139, -3.3930]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-2.9382,  4.1187,  2.6139, -3.3930]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.3842,  5.2894,  3.6101, -3.5973]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.3842,  5.2894,  3.6101, -3.5973]])\n",
      "On state=8, selected action=2 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-2.9382,  4.1187,  2.6139, -3.3930]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-2.9382,  4.1187,  2.6139, -3.3930]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.3842,  5.2894,  3.6101, -3.5973]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 230000 finished after 8 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 231000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 232000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 233000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 234000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 235000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 236000 finished after 13 timesteps with r=1.0. Running score: 0.07. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 237000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 238000 finished after 5 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 239000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 59.\n",
      "action_probs_orig \n",
      "tensor([[-2.8658,  3.4665,  2.1333, -3.3399]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-2.8658,  3.4665,  2.1333, -3.3399]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.1078,  3.9781,  2.5610, -3.5191]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.5555,  5.0961,  3.5849, -3.7066]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.5555,  5.0961,  3.5849, -3.7066]])\n",
      "On state=8, selected action=2 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.1078,  3.9781,  2.5610, -3.5191]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.5555,  5.0961,  3.5849, -3.7066]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.5555,  5.0961,  3.5849, -3.7066]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.5555,  5.0961,  3.5849, -3.7066]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.5555,  5.0961,  3.5849, -3.7066]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 240000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 241000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 242000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 33.\n",
      "Episode 243000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 244000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 245000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 246000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 247000 finished after 6 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 248000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 249000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 45.\n",
      "action_probs_orig \n",
      "tensor([[-3.0408,  3.4689,  2.1188, -3.4798]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.3295,  4.0411,  2.5857, -3.7130]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 250000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 251000 finished after 7 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 252000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 253000 finished after 7 timesteps with r=1.0. Running score: 0.06. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 254000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 255000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 256000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 257000 finished after 5 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 258000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 36.\n",
      "Episode 259000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 52.\n",
      "action_probs_orig \n",
      "tensor([[-3.2211,  3.5109,  2.0858, -3.6340]])\n",
      "On state=0, selected action=2 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.4974,  4.0372,  2.5144, -3.8533]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.4974,  4.0372,  2.5144, -3.8533]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.9903,  5.1562,  3.5164, -4.0993]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1348,  5.5517,  3.9503, -4.1300]])\n",
      "On state=9, selected action=1 , \n",
      "new state=10, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2794,  5.9472,  4.3843, -4.1608]])\n",
      "On state=10, selected action=1 , \n",
      "new state=11, done=True\n",
      "Episode 260000 finished after 6 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 261000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 262000 finished after 15 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 263000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 264000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 265000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 266000 finished after 4 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 267000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 268000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 269000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "action_probs_orig \n",
      "tensor([[-3.3066,  3.5512,  2.0275, -3.7313]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.3405,  3.6041,  2.0677, -3.7691]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.3066,  3.5512,  2.0275, -3.7313]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.5699,  4.0445,  2.4261, -3.9391]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 270000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 36.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 271000 finished after 7 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 272000 finished after 18 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 34.\n",
      "Episode 273000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 274000 finished after 3 timesteps with r=0.0. Running score: 0.11. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 275000 finished after 7 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 276000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 277000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 278000 finished after 7 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 279000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 54.\n",
      "action_probs_orig \n",
      "tensor([[-3.4314,  3.6642,  1.9310, -3.8410]])\n",
      "On state=0, selected action=2 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.4314,  3.6642,  1.9310, -3.8410]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.7893,  4.3082,  2.4124, -4.1413]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3684,  5.5272,  3.4101, -4.4809]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5435,  5.9461,  3.8389, -4.5418]])\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.2491,  7.6414,  5.5853, -4.7815]])\n",
      "On state=13, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 280000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 281000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 282000 finished after 7 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 283000 finished after 7 timesteps with r=1.0. Running score: 0.07. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 284000 finished after 7 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 285000 finished after 2 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 286000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 287000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 288000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 289000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 51.\n",
      "action_probs_orig \n",
      "tensor([[-3.5312,  3.7439,  1.8559, -3.9269]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.5312,  3.7439,  1.8559, -3.9269]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.5312,  3.7439,  1.8559, -3.9269]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.5312,  3.7439,  1.8559, -3.9269]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.5312,  3.7439,  1.8559, -3.9269]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.8944,  4.3867,  2.3219, -4.2321]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.8944,  4.3867,  2.3219, -4.2321]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 290000 finished after 7 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 291000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 292000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 293000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 294000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 295000 finished after 8 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 296000 finished after 2 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 297000 finished after 6 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 298000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 299000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 46.\n",
      "action_probs_orig \n",
      "tensor([[-3.6566,  3.8222,  1.8218, -4.0572]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.6566,  3.8222,  1.8218, -4.0572]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0142,  4.4347,  2.2681, -4.3605]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 300000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 301000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 302000 finished after 5 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 303000 finished after 2 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 59.\n",
      "Episode 304000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 305000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 59.\n",
      "Episode 306000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 307000 finished after 5 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 308000 finished after 4 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 59.\n",
      "Episode 309000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 36.\n",
      "action_probs_orig \n",
      "tensor([[-3.6807,  3.7676,  1.8298, -4.1114]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.6807,  3.7676,  1.8298, -4.1114]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.7149,  3.8146,  1.8647, -4.1488]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 310000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 311000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 312000 finished after 6 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 313000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 314000 finished after 9 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 315000 finished after 6 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 316000 finished after 4 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 317000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 318000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 319000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 56.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_probs_orig \n",
      "tensor([[-3.7436,  3.9259,  1.6919, -4.1540]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.7878,  3.9879,  1.7335, -4.2029]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 320000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 321000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 322000 finished after 4 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 323000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 324000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 325000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 326000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 327000 finished after 12 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 328000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 329000 finished after 3 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 44.\n",
      "action_probs_orig \n",
      "tensor([[-3.7949,  4.0213,  1.6197, -4.1954]])\n",
      "On state=0, selected action=2 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0993,  4.5314,  1.9909, -4.4441]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0993,  4.5314,  1.9909, -4.4441]])\n",
      "On state=4, selected action=2 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.7949,  4.0213,  1.6197, -4.1954]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.8378,  4.0810,  1.6592, -4.2424]])\n",
      "On state=1, selected action=2 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.8982,  4.1755,  1.7255, -4.2978]])\n",
      "On state=2, selected action=2 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3740,  5.0750,  2.4317, -4.5993]])\n",
      "On state=6, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 330000 finished after 7 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 331000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 332000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 333000 finished after 30 timesteps with r=1.0. Running score: 0.08. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 334000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 335000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 336000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 337000 finished after 2 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 338000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 69.\n",
      "Episode 339000 finished after 6 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 53.\n",
      "action_probs_orig \n",
      "tensor([[-3.8386,  4.1651,  1.5028, -4.2307]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.8386,  4.1651,  1.5028, -4.2307]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2160,  4.7965,  1.9265, -4.5596]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 340000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 341000 finished after 10 timesteps with r=1.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 342000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 343000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 344000 finished after 14 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 345000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 346000 finished after 8 timesteps with r=1.0. Running score: 0.03. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 347000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 348000 finished after 22 timesteps with r=1.0. Running score: 0.02. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 349000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "action_probs_orig \n",
      "tensor([[-3.8966,  4.1622,  1.5008, -4.2866]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2168,  4.6845,  1.8705, -4.5576]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2168,  4.6845,  1.8705, -4.5576]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.8112,  5.8224,  2.8146, -4.9014]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 350000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 351000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 352000 finished after 3 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 353000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 354000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 355000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 356000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 357000 finished after 6 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 358000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 359000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "action_probs_orig \n",
      "tensor([[-3.8724,  4.2172,  1.4415, -4.2733]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.9248,  4.2904,  1.4850, -4.3316]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 360000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 62.\n",
      "Episode 361000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 362000 finished after 11 timesteps with r=1.0. Running score: 0.09. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 363000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 364000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 30.\n",
      "Episode 365000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 366000 finished after 13 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 367000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 61.\n",
      "Episode 368000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 32.\n",
      "Episode 369000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 51.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_probs_orig \n",
      "tensor([[-3.8789,  4.2688,  1.3845, -4.2853]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.8789,  4.2688,  1.3845, -4.2853]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.9205,  4.3261,  1.4198, -4.3313]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.8789,  4.2688,  1.3845, -4.2853]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1614,  4.7195,  1.7059, -4.5256]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 370000 finished after 5 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 371000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 372000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 373000 finished after 9 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 374000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 375000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 376000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 377000 finished after 15 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 378000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 379000 finished after 6 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 63.\n",
      "action_probs_orig \n",
      "tensor([[-3.8850,  4.2322,  1.3864, -4.3093]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.9203,  4.2795,  1.4166, -4.3482]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.8850,  4.2322,  1.3864, -4.3093]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.8850,  4.2322,  1.3864, -4.3093]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1388,  4.6273,  1.6781, -4.5203]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1388,  4.6273,  1.6781, -4.5203]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1388,  4.6273,  1.6781, -4.5203]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 380000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 381000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 382000 finished after 7 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 383000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 384000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 385000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 386000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 387000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 388000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 389000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 50.\n",
      "action_probs_orig \n",
      "tensor([[-3.9369,  4.3490,  1.2759, -4.3238]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2309,  4.8069,  1.5908, -4.5766]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2309,  4.8069,  1.5908, -4.5766]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.8300,  5.8615,  2.4590, -4.9416]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.0675,  6.2219,  2.8759, -5.0529]])\n",
      "On state=9, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.8300,  5.8615,  2.4590, -4.9416]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 390000 finished after 6 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 391000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 392000 finished after 8 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 393000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 394000 finished after 8 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 395000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 396000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 397000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 398000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 35.\n",
      "Episode 399000 finished after 8 timesteps with r=1.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "action_probs_orig \n",
      "tensor([[-3.9853,  4.4115,  1.2149, -4.3273]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0300,  4.4724,  1.2486, -4.3763]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-3.9853,  4.4115,  1.2149, -4.3273]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0300,  4.4724,  1.2486, -4.3763]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0747,  4.5334,  1.2824, -4.4253]])\n",
      "On state=2, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0300,  4.4724,  1.2486, -4.3763]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 400000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 401000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 402000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 69.\n",
      "Episode 403000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 404000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 405000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 406000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 407000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 408000 finished after 4 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 409000 finished after 6 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 40.\n",
      "action_probs_orig \n",
      "tensor([[-4.0438,  4.4815,  1.1382, -4.3281]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0438,  4.4815,  1.1382, -4.3281]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0438,  4.4815,  1.1382, -4.3281]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0438,  4.4815,  1.1382, -4.3281]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0438,  4.4815,  1.1382, -4.3281]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0438,  4.4815,  1.1382, -4.3281]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0438,  4.4815,  1.1382, -4.3281]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.0438,  4.4815,  1.1382, -4.3281]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3086,  4.8862,  1.4126, -4.5507]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 410000 finished after 9 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 48.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 411000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 412000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 413000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 414000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 415000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 416000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 68.\n",
      "Episode 417000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 418000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 419000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 49.\n",
      "action_probs_orig \n",
      "tensor([[-4.0714,  4.5296,  1.0680, -4.3177]])\n",
      "On state=0, selected action=2 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2617,  4.8141,  1.2833, -4.4619]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7746,  5.7068,  2.0477, -4.7285]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7746,  5.7068,  2.0477, -4.7285]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7746,  5.7068,  2.0477, -4.7285]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.0043,  6.0255,  2.4416, -4.8255]])\n",
      "On state=9, selected action=2 , \n",
      "new state=10, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.2339,  6.3442,  2.8355, -4.9226]])\n",
      "On state=10, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.0043,  6.0255,  2.4416, -4.8255]])\n",
      "On state=9, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7746,  5.7068,  2.0477, -4.7285]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.0043,  6.0255,  2.4416, -4.8255]])\n",
      "On state=9, selected action=1 , \n",
      "new state=10, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.2339,  6.3442,  2.8355, -4.9226]])\n",
      "On state=10, selected action=1 , \n",
      "new state=14, done=False\n",
      "action_probs_orig \n",
      "tensor([[-6.1551,  7.8153,  4.6550, -5.2648]])\n",
      "On state=14, selected action=1 , \n",
      "new state=14, done=False\n",
      "action_probs_orig \n",
      "tensor([[-6.1551,  7.8153,  4.6550, -5.2648]])\n",
      "On state=14, selected action=1 , \n",
      "new state=15, done=True\n",
      "Episode 420000 finished after 13 timesteps with r=1.0. Running score: 0.02. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 421000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 422000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 423000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 424000 finished after 5 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 62.\n",
      "Episode 425000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 426000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 35.\n",
      "Episode 427000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 428000 finished after 18 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 429000 finished after 7 timesteps with r=1.0. Running score: 0.04. Times trained: 1000. Times reached goal: 41.\n",
      "action_probs_orig \n",
      "tensor([[-4.1125,  4.5942,  1.0194, -4.3433]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3109,  4.8869,  1.2400, -4.4952]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.8286,  5.7673,  1.9903, -4.7746]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.8286,  5.7673,  1.9903, -4.7746]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.0652,  6.0797,  2.3875, -4.8785]])\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[-6.0123,  7.3296,  3.9771, -5.2943]])\n",
      "On state=13, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 430000 finished after 6 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 431000 finished after 8 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 432000 finished after 10 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 433000 finished after 8 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 434000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 435000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 436000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 437000 finished after 10 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 438000 finished after 8 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 439000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "action_probs_orig \n",
      "tensor([[-4.1262,  4.6306,  0.9745, -4.3694]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1586,  4.6723,  0.9981, -4.4034]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 440000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 441000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 442000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 443000 finished after 2 timesteps with r=0.0. Running score: 0.11. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 444000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 445000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 446000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 447000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 448000 finished after 12 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 449000 finished after 2 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 64.\n",
      "action_probs_orig \n",
      "tensor([[-4.1605,  4.7011,  0.9056, -4.3610]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1605,  4.7011,  0.9056, -4.3610]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4322,  5.1075,  1.1624, -4.5894]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 450000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 451000 finished after 6 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 452000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 64.\n",
      "Episode 453000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 454000 finished after 15 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 51.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 455000 finished after 2 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 456000 finished after 6 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 64.\n",
      "Episode 457000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 458000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 459000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 49.\n",
      "action_probs_orig \n",
      "tensor([[-4.1931,  4.8602,  0.7354, -4.3957]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3958,  5.1626,  0.9303, -4.5612]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 460000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 461000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 462000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 463000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 63.\n",
      "Episode 464000 finished after 13 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 465000 finished after 5 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 466000 finished after 8 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 467000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 468000 finished after 5 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 469000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 38.\n",
      "action_probs_orig \n",
      "tensor([[-4.2315,  5.0008,  0.6061, -4.4156]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2741,  5.0587,  0.6313, -4.4601]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 470000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 471000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 472000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 473000 finished after 7 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 474000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 475000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 476000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 477000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 478000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 59.\n",
      "Episode 479000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 41.\n",
      "action_probs_orig \n",
      "tensor([[-4.2804,  5.3245,  0.3266, -4.4903]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.6415,  5.9060,  0.5859, -4.8124]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.3381,  7.1509,  1.3313, -5.2753]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.6467,  7.5589,  1.7180, -5.4473]])\n",
      "On state=9, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.3381,  7.1509,  1.3313, -5.2753]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 480000 finished after 5 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 481000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 482000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 483000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 484000 finished after 8 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 485000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 486000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 487000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 488000 finished after 6 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 489000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 57.\n",
      "action_probs_orig \n",
      "tensor([[-4.2964,  5.5291,  0.1213, -4.5117]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5788,  5.9837,  0.3292, -4.7580]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5788,  5.9837,  0.3292, -4.7580]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.1862,  7.1113,  0.9987, -5.1418]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.1862,  7.1113,  0.9987, -5.1418]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.1862,  7.1113,  0.9987, -5.1418]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.4589,  7.4774,  1.3383, -5.2901]])\n",
      "On state=9, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.1862,  7.1113,  0.9987, -5.1418]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 490000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 33.\n",
      "Episode 491000 finished after 4 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 492000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 493000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 63.\n",
      "Episode 494000 finished after 14 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 495000 finished after 9 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 496000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 497000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 498000 finished after 9 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 499000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 61.\n",
      "action_probs_orig \n",
      "tensor([[-4.3303,  5.7374, -0.0602, -4.5239]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7015,  6.3606,  0.1710, -4.8603]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7015,  6.3606,  0.1710, -4.8603]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7015,  6.3606,  0.1710, -4.8603]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7015,  6.3606,  0.1710, -4.8603]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 500000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 501000 finished after 11 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 502000 finished after 19 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 503000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 504000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 505000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 506000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 507000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 508000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 62.\n",
      "Episode 509000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 54.\n",
      "action_probs_orig \n",
      "tensor([[-4.3465,  5.9136, -0.2456, -4.5567]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3465,  5.9136, -0.2456, -4.5567]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4079,  6.0095, -0.2233, -4.6222]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 510000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 511000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 512000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 513000 finished after 9 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 514000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 515000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 516000 finished after 6 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 517000 finished after 13 timesteps with r=1.0. Running score: 0.06. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 518000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 519000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 47.\n",
      "action_probs_orig \n",
      "tensor([[-4.3423,  6.1314, -0.4573, -4.5320]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4047,  6.2321, -0.4379, -4.5982]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3423,  6.1314, -0.4573, -4.5320]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3423,  6.1314, -0.4573, -4.5320]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3423,  6.1314, -0.4573, -4.5320]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7036,  6.7586, -0.2591, -4.8596]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 520000 finished after 6 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 521000 finished after 9 timesteps with r=1.0. Running score: 0.02. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 522000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 523000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 524000 finished after 5 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 525000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 526000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 527000 finished after 6 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 528000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 529000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 46.\n",
      "action_probs_orig \n",
      "tensor([[-4.3557,  6.3138, -0.6481, -4.5399]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3557,  6.3138, -0.6481, -4.5399]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3557,  6.3138, -0.6481, -4.5399]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.6428,  6.8109, -0.4844, -4.7941]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 530000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 531000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 532000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 533000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 534000 finished after 3 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 535000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 536000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 537000 finished after 8 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 538000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 539000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 57.\n",
      "action_probs_orig \n",
      "tensor([[-4.2996,  6.4729, -0.7939, -4.5127]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3651,  6.5864, -0.7804, -4.5830]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4305,  6.6999, -0.7668, -4.6533]])\n",
      "On state=2, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3651,  6.5864, -0.7804, -4.5830]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4305,  6.6999, -0.7668, -4.6533]])\n",
      "On state=2, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5183,  6.8590, -0.7317, -4.7369]])\n",
      "On state=3, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4305,  6.6999, -0.7668, -4.6533]])\n",
      "On state=2, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5183,  6.8590, -0.7317, -4.7369]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 540000 finished after 8 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 541000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 542000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 543000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 544000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 545000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 546000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 547000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 48.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 548000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 549000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 44.\n",
      "action_probs_orig \n",
      "tensor([[-4.3246,  6.6193, -0.9134, -4.5176]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7137,  7.3378, -0.7593, -4.8815]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.5216,  8.8193, -0.1083, -5.4524]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 550000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 551000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 61.\n",
      "Episode 552000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 553000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 554000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 555000 finished after 9 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 556000 finished after 10 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 557000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 558000 finished after 6 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 559000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 45.\n",
      "action_probs_orig \n",
      "tensor([[-4.3156,  6.7243, -1.0302, -4.5105]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.6319,  7.3067, -0.9019, -4.8039]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 560000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 561000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 64.\n",
      "Episode 562000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 563000 finished after 5 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 564000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 565000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 566000 finished after 10 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 567000 finished after 3 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 568000 finished after 14 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 569000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 51.\n",
      "action_probs_orig \n",
      "tensor([[-4.2798,  6.7862, -1.1124, -4.4763]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2798,  6.7862, -1.1124, -4.4763]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2798,  6.7862, -1.1124, -4.4763]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3284,  6.8729, -1.1038, -4.5278]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2798,  6.7862, -1.1124, -4.4763]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2798,  6.7862, -1.1124, -4.4763]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5501,  7.2887, -1.0033, -4.7268]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 570000 finished after 7 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 571000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 572000 finished after 6 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 573000 finished after 10 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 574000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 575000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 576000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 577000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 578000 finished after 7 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 579000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 39.\n",
      "action_probs_orig \n",
      "tensor([[-4.2815,  6.8897, -1.2000, -4.4741]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5708,  7.4389, -1.1018, -4.7472]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5708,  7.4389, -1.1018, -4.7472]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5708,  7.4389, -1.1018, -4.7472]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 580000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 61.\n",
      "Episode 581000 finished after 4 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 59.\n",
      "Episode 582000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 583000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 584000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 585000 finished after 8 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 586000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 587000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 588000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 589000 finished after 17 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 44.\n",
      "action_probs_orig \n",
      "tensor([[-4.2825,  6.9676, -1.2758, -4.4688]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5485,  7.4747, -1.1882, -4.7192]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.1989,  8.7519, -0.6692, -5.1658]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 590000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 591000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 592000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 593000 finished after 10 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 594000 finished after 8 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 595000 finished after 8 timesteps with r=1.0. Running score: 0.06. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 596000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 43.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 597000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 598000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 599000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 57.\n",
      "action_probs_orig \n",
      "tensor([[-4.2735,  7.0335, -1.3155, -4.4736]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2735,  7.0335, -1.3155, -4.4736]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2735,  7.0335, -1.3155, -4.4736]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2735,  7.0335, -1.3155, -4.4736]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2735,  7.0335, -1.3155, -4.4736]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2735,  7.0335, -1.3155, -4.4736]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2735,  7.0335, -1.3155, -4.4736]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2735,  7.0335, -1.3155, -4.4736]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2735,  7.0335, -1.3155, -4.4736]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5577,  7.5859, -1.2320, -4.7449]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5577,  7.5859, -1.2320, -4.7449]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5577,  7.5859, -1.2320, -4.7449]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5577,  7.5859, -1.2320, -4.7449]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5577,  7.5859, -1.2320, -4.7449]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 600000 finished after 14 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 601000 finished after 10 timesteps with r=1.0. Running score: 0.04. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 602000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 603000 finished after 8 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 604000 finished after 12 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 605000 finished after 9 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 606000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 607000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 608000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 67.\n",
      "Episode 609000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 39.\n",
      "action_probs_orig \n",
      "tensor([[-4.2855,  7.1115, -1.3747, -4.4878]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3415,  7.2191, -1.3717, -4.5479]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2855,  7.1115, -1.3747, -4.4878]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2855,  7.1115, -1.3747, -4.4878]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5800,  7.6888, -1.2964, -4.7702]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.2796,  9.0578, -0.7754, -5.2639]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.6535,  9.5074, -0.4617, -5.5059]])\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -7.1489,  11.3058,   0.7930,  -6.4740]])\n",
      "On state=13, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -7.1489,  11.3058,   0.7930,  -6.4740]])\n",
      "On state=13, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -7.1489,  11.3058,   0.7930,  -6.4740]])\n",
      "On state=13, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 610000 finished after 10 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 611000 finished after 6 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 612000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 613000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 614000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 615000 finished after 9 timesteps with r=1.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 616000 finished after 4 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 617000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 618000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 64.\n",
      "Episode 619000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 57.\n",
      "action_probs_orig \n",
      "tensor([[-4.2539,  7.1906, -1.4425, -4.4579]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2539,  7.1906, -1.4425, -4.4579]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2539,  7.1906, -1.4425, -4.4579]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2539,  7.1906, -1.4425, -4.4579]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5733,  7.8301, -1.3669, -4.7653]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5733,  7.8301, -1.3669, -4.7653]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 620000 finished after 6 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 621000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 622000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 623000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 624000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 625000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 626000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 627000 finished after 4 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 59.\n",
      "Episode 628000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 629000 finished after 11 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 41.\n",
      "action_probs_orig \n",
      "tensor([[-4.2681,  7.2510, -1.4934, -4.4750]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3197,  7.3522, -1.4918, -4.5304]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3713,  7.4534, -1.4903, -4.5857]])\n",
      "On state=2, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4229,  7.5546, -1.4887, -4.6411]])\n",
      "On state=3, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4229,  7.5546, -1.4887, -4.6411]])\n",
      "On state=3, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4229,  7.5546, -1.4887, -4.6411]])\n",
      "On state=3, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3713,  7.4534, -1.4903, -4.5857]])\n",
      "On state=2, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3197,  7.3522, -1.4918, -4.5304]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3713,  7.4534, -1.4903, -4.5857]])\n",
      "On state=2, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3197,  7.3522, -1.4918, -4.5304]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3713,  7.4534, -1.4903, -4.5857]])\n",
      "On state=2, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3197,  7.3522, -1.4918, -4.5304]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 630000 finished after 12 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 55.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 631000 finished after 6 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 632000 finished after 14 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 633000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 634000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 635000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 636000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 637000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 638000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 639000 finished after 9 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 34.\n",
      "action_probs_orig \n",
      "tensor([[-4.2469,  7.2638, -1.5197, -4.4540]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2469,  7.2638, -1.5197, -4.4540]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2913,  7.3504, -1.5172, -4.5014]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3358,  7.4371, -1.5148, -4.5488]])\n",
      "On state=2, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2913,  7.3504, -1.5172, -4.5014]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3358,  7.4371, -1.5148, -4.5488]])\n",
      "On state=2, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2913,  7.3504, -1.5172, -4.5014]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2469,  7.2638, -1.5197, -4.4540]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2913,  7.3504, -1.5172, -4.5014]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3358,  7.4371, -1.5148, -4.5488]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7764,  8.3381, -1.2359, -4.8841]])\n",
      "On state=6, selected action=1 , \n",
      "new state=10, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.8198,  9.8099, -0.4005, -5.5767]])\n",
      "On state=10, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.4519,  9.3859, -0.7008, -5.3370]])\n",
      "On state=9, selected action=1 , \n",
      "new state=10, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.8198,  9.8099, -0.4005, -5.5767]])\n",
      "On state=10, selected action=1 , \n",
      "new state=14, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -7.2966,  11.7516,   0.9943,  -6.4968]])\n",
      "On state=14, selected action=1 , \n",
      "new state=14, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -7.2966,  11.7516,   0.9943,  -6.4968]])\n",
      "On state=14, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -6.9235,  11.0817,   0.5005,  -6.2957]])\n",
      "On state=13, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -6.9235,  11.0817,   0.5005,  -6.2957]])\n",
      "On state=13, selected action=1 , \n",
      "new state=14, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -7.2966,  11.7516,   0.9943,  -6.4968]])\n",
      "On state=14, selected action=1 , \n",
      "new state=14, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -7.2966,  11.7516,   0.9943,  -6.4968]])\n",
      "On state=14, selected action=1 , \n",
      "new state=15, done=True\n",
      "Episode 640000 finished after 20 timesteps with r=1.0. Running score: 0.04. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 641000 finished after 10 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 642000 finished after 9 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 643000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 644000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 645000 finished after 15 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 646000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 647000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 648000 finished after 4 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 649000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 46.\n",
      "action_probs_orig \n",
      "tensor([[-4.2724,  7.3596, -1.5836, -4.4935]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2724,  7.3596, -1.5836, -4.4935]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3223,  7.4586, -1.5829, -4.5472]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2724,  7.3596, -1.5836, -4.4935]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3223,  7.4586, -1.5829, -4.5472]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3722,  7.5576, -1.5823, -4.6008]])\n",
      "On state=2, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3223,  7.4586, -1.5829, -4.5472]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2724,  7.3596, -1.5836, -4.4935]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5329,  7.8832, -1.5255, -4.7450]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5329,  7.8832, -1.5255, -4.7450]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.1768,  9.1900, -1.0636, -5.2027]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.5566,  9.6326, -0.7658, -5.4545]])\n",
      "On state=9, selected action=1 , \n",
      "new state=10, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -5.9365,  10.0751,  -0.4681,  -5.7064]])\n",
      "On state=10, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.5566,  9.6326, -0.7658, -5.4545]])\n",
      "On state=9, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.1768,  9.1900, -1.0636, -5.2027]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.1768,  9.1900, -1.0636, -5.2027]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 650000 finished after 16 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 651000 finished after 7 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 652000 finished after 6 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 653000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 654000 finished after 5 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 655000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 656000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 657000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 658000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 659000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 62.\n",
      "action_probs_orig \n",
      "tensor([[-4.2449,  7.3803, -1.6178, -4.4675]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2913,  7.4728, -1.6171, -4.5173]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 660000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 55.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 661000 finished after 8 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 662000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 663000 finished after 5 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 61.\n",
      "Episode 664000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 665000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 666000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 667000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 668000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 669000 finished after 4 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 61.\n",
      "action_probs_orig \n",
      "tensor([[-4.2360,  7.4130, -1.6558, -4.4602]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2858,  7.5141, -1.6566, -4.5140]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3357,  7.6152, -1.6573, -4.5678]])\n",
      "On state=2, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2858,  7.5141, -1.6566, -4.5140]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2360,  7.4130, -1.6558, -4.4602]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2858,  7.5141, -1.6566, -4.5140]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 670000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 671000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 672000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 31.\n",
      "Episode 673000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 674000 finished after 2 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 71.\n",
      "Episode 675000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 676000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 677000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 678000 finished after 10 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 679000 finished after 3 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 63.\n",
      "action_probs_orig \n",
      "tensor([[-4.2819,  7.5110, -1.7221, -4.5083]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3429,  7.6370, -1.7265, -4.5747]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4039,  7.7630, -1.7310, -4.6411]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.9294,  8.8788, -1.4675, -5.0651]])\n",
      "On state=6, selected action=1 , \n",
      "new state=10, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -6.1928,  10.6288,  -0.5869,  -5.9487]])\n",
      "On state=10, selected action=1 , \n",
      "new state=14, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -7.8487,  12.8912,   0.8200,  -7.0353]])\n",
      "On state=14, selected action=1 , \n",
      "new state=15, done=True\n",
      "Episode 680000 finished after 6 timesteps with r=1.0. Running score: 0.09. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 681000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 682000 finished after 5 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 683000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 684000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 685000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 686000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 687000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 688000 finished after 2 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 689000 finished after 4 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 48.\n",
      "action_probs_orig \n",
      "tensor([[-4.2691,  7.5344, -1.7689, -4.4952]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2691,  7.5344, -1.7689, -4.4952]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4978,  8.0038, -1.7270, -4.7173]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.1082,  9.2743, -1.2900, -5.1486]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 690000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 691000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 692000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 693000 finished after 18 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 694000 finished after 8 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 695000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 61.\n",
      "Episode 696000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 697000 finished after 10 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 698000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 699000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 47.\n",
      "action_probs_orig \n",
      "tensor([[-4.2684,  7.5670, -1.7948, -4.4978]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2684,  7.5670, -1.7948, -4.4978]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2684,  7.5670, -1.7948, -4.4978]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5162,  8.0791, -1.7536, -4.7388]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 700000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 701000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 702000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 703000 finished after 13 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 704000 finished after 12 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 705000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 706000 finished after 12 timesteps with r=0.0. Running score: 0.12. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 707000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 708000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 61.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 709000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 55.\n",
      "action_probs_orig \n",
      "tensor([[-4.2670,  7.5794, -1.8078, -4.4807]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4899,  8.0380, -1.7674, -4.6957]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 710000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 711000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 712000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 74.\n",
      "Episode 713000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 714000 finished after 13 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 715000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 716000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 717000 finished after 7 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 718000 finished after 14 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 719000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 51.\n",
      "action_probs_orig \n",
      "tensor([[-4.2833,  7.6405, -1.8482, -4.4976]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5722,  8.2467, -1.8140, -4.7803]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.3124,  9.6980, -1.3484, -5.3235]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 720000 finished after 3 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 721000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 722000 finished after 10 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 723000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 724000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 725000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 726000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 84.\n",
      "Episode 727000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 728000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 729000 finished after 9 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 49.\n",
      "action_probs_orig \n",
      "tensor([[-4.2664,  7.6435, -1.8568, -4.4807]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2664,  7.6435, -1.8568, -4.4807]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3102,  7.7338, -1.8593, -4.5278]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 730000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 731000 finished after 10 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 732000 finished after 8 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 733000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 734000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 735000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 736000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 737000 finished after 4 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 738000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 739000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 44.\n",
      "action_probs_orig \n",
      "tensor([[-4.2163,  7.6636, -1.8673, -4.4463]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5478,  8.3789, -1.8360, -4.7720]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5478,  8.3789, -1.8360, -4.7720]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5478,  8.3789, -1.8360, -4.7720]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.3957,  9.9779, -1.3531, -5.4085]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 740000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 70.\n",
      "Episode 741000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 742000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 743000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 744000 finished after 2 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 745000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 746000 finished after 3 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 65.\n",
      "Episode 747000 finished after 2 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 748000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 749000 finished after 3 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 57.\n",
      "action_probs_orig \n",
      "tensor([[-4.2214,  7.6482, -1.8811, -4.4500]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4601,  8.1533, -1.8478, -4.6819]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 750000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 751000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 752000 finished after 11 timesteps with r=0.0. Running score: 0.1. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 753000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 754000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 755000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 756000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 757000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 758000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 759000 finished after 10 timesteps with r=1.0. Running score: 0.02. Times trained: 1000. Times reached goal: 48.\n",
      "action_probs_orig \n",
      "tensor([[-4.2488,  7.6950, -1.9049, -4.4813]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.5052,  8.2377, -1.8755, -4.7317]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 760000 finished after 2 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 61.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 761000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 762000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 763000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 764000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 765000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 766000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 767000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 768000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 769000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 58.\n",
      "action_probs_orig \n",
      "tensor([[-4.2106,  7.6678, -1.9089, -4.4439]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2106,  7.6678, -1.9089, -4.4439]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2411,  7.7302, -1.9093, -4.4764]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2716,  7.7925, -1.9096, -4.5090]])\n",
      "On state=2, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2411,  7.7302, -1.9093, -4.4764]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2716,  7.7925, -1.9096, -4.5090]])\n",
      "On state=2, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2411,  7.7302, -1.9093, -4.4764]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2716,  7.7925, -1.9096, -4.5090]])\n",
      "On state=2, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3021,  7.8549, -1.9100, -4.5416]])\n",
      "On state=3, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3021,  7.8549, -1.9100, -4.5416]])\n",
      "On state=3, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3021,  7.8549, -1.9100, -4.5416]])\n",
      "On state=3, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2716,  7.7925, -1.9096, -4.5090]])\n",
      "On state=2, selected action=1 , \n",
      "new state=6, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.6521,  8.6002, -1.6853, -4.7950]])\n",
      "On state=6, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 770000 finished after 13 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 771000 finished after 4 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 772000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 773000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 774000 finished after 19 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 775000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 776000 finished after 10 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 63.\n",
      "Episode 777000 finished after 9 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 778000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 779000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 38.\n",
      "action_probs_orig \n",
      "tensor([[-4.1912,  7.6500, -1.8927, -4.4517]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3859,  8.0587, -1.8581, -4.6391]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3859,  8.0587, -1.8581, -4.6391]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 780000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 781000 finished after 11 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 782000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 61.\n",
      "Episode 783000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 784000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 785000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 786000 finished after 20 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 787000 finished after 9 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 788000 finished after 8 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 789000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 39.\n",
      "action_probs_orig \n",
      "tensor([[-4.1962,  7.6797, -1.9113, -4.4593]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.4186,  8.1547, -1.8829, -4.6768]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.0786,  9.4811, -1.4702, -5.1581]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.0786,  9.4811, -1.4702, -5.1581]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.5035,  9.9473, -1.2092, -5.4612]])\n",
      "On state=9, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.0786,  9.4811, -1.4702, -5.1581]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.5035,  9.9473, -1.2092, -5.4612]])\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -7.2033,  11.8118,  -0.1650,  -6.6735]])\n",
      "On state=13, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 790000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 791000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 792000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 793000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 794000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 795000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 796000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 797000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 798000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 799000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 44.\n",
      "action_probs_orig \n",
      "tensor([[-4.1863,  7.7138, -1.9390, -4.4644]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1863,  7.7138, -1.9390, -4.4644]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1863,  7.7138, -1.9390, -4.4644]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2264,  7.8000, -1.9434, -4.5086]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 800000 finished after 4 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 67.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 801000 finished after 7 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 802000 finished after 11 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 803000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 804000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 805000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 806000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 807000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 808000 finished after 10 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 809000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 54.\n",
      "action_probs_orig \n",
      "tensor([[-4.1557,  7.6979, -1.9394, -4.4361]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1887,  7.7686, -1.9425, -4.4724]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 810000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 811000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 812000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 813000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 814000 finished after 4 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 815000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 816000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 817000 finished after 11 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 818000 finished after 3 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 819000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 47.\n",
      "action_probs_orig \n",
      "tensor([[-4.1228,  7.6783, -1.9332, -4.4056]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1228,  7.6783, -1.9332, -4.4056]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3170,  8.0939, -1.9042, -4.5934]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.9308,  9.3464, -1.5225, -5.0395]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.3585,  9.7996, -1.2713, -5.3474]])\n",
      "On state=9, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.9308,  9.3464, -1.5225, -5.0395]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.9308,  9.3464, -1.5225, -5.0395]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.9308,  9.3464, -1.5225, -5.0395]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.9308,  9.3464, -1.5225, -5.0395]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.9308,  9.3464, -1.5225, -5.0395]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.3585,  9.7996, -1.2713, -5.3474]])\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -7.0694,  11.6123,  -0.2661,  -6.5789]])\n",
      "On state=13, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 820000 finished after 12 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 821000 finished after 4 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 822000 finished after 9 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 823000 finished after 9 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 824000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 825000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 59.\n",
      "Episode 826000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 827000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 828000 finished after 11 timesteps with r=1.0. Running score: 0.05. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 829000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 49.\n",
      "action_probs_orig \n",
      "tensor([[-4.1250,  7.7147, -1.9748, -4.4097]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2815,  8.0450, -1.9434, -4.5577]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2815,  8.0450, -1.9434, -4.5577]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2815,  8.0450, -1.9434, -4.5577]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 830000 finished after 4 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 831000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 832000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 833000 finished after 6 timesteps with r=1.0. Running score: 0.04. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 834000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 835000 finished after 6 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 836000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 837000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 838000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 839000 finished after 7 timesteps with r=1.0. Running score: 0.1. Times trained: 1000. Times reached goal: 54.\n",
      "action_probs_orig \n",
      "tensor([[-4.1027,  7.7038, -1.9847, -4.3884]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2428,  7.9999, -1.9555, -4.5201]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7546,  9.1113, -1.6101, -4.8794]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 840000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 40.\n",
      "Episode 841000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 842000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 843000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 844000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 845000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 846000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 59.\n",
      "Episode 847000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 59.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 848000 finished after 4 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 849000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 49.\n",
      "action_probs_orig \n",
      "tensor([[-4.1386,  7.7649, -2.0288, -4.4265]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3024,  8.1199, -2.0097, -4.5855]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 850000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 851000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 852000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 853000 finished after 11 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 854000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 855000 finished after 9 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 856000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 857000 finished after 6 timesteps with r=1.0. Running score: 0.05. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 858000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 63.\n",
      "Episode 859000 finished after 3 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 47.\n",
      "action_probs_orig \n",
      "tensor([[-4.1341,  7.7774, -2.0441, -4.4242]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1341,  7.7774, -2.0441, -4.4242]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1341,  7.7774, -2.0441, -4.4242]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3031,  8.1475, -2.0272, -4.5884]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3031,  8.1475, -2.0272, -4.5884]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3031,  8.1475, -2.0272, -4.5884]])\n",
      "On state=4, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.3031,  8.1475, -2.0272, -4.5884]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 860000 finished after 7 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 861000 finished after 8 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 862000 finished after 12 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 863000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 864000 finished after 9 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 865000 finished after 6 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 866000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 867000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 36.\n",
      "Episode 868000 finished after 10 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 869000 finished after 4 timesteps with r=0.0. Running score: 0.0. Times trained: 1000. Times reached goal: 45.\n",
      "action_probs_orig \n",
      "tensor([[-4.1320,  7.7797, -2.0652, -4.4223]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1320,  7.7797, -2.0652, -4.4223]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2418,  8.0157, -2.0427, -4.5242]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7197,  9.0763, -1.7122, -4.8545]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.1381,  9.4960, -1.4812, -5.1570]])\n",
      "On state=9, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7197,  9.0763, -1.7122, -4.8545]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7197,  9.0763, -1.7122, -4.8545]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7197,  9.0763, -1.7122, -4.8545]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.1381,  9.4960, -1.4812, -5.1570]])\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -6.8209,  11.1795,  -0.5529,  -6.3738]])\n",
      "On state=13, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -6.8209,  11.1795,  -0.5529,  -6.3738]])\n",
      "On state=13, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 870000 finished after 11 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 39.\n",
      "Episode 871000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 872000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 873000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 874000 finished after 2 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 875000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 59.\n",
      "Episode 876000 finished after 7 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 877000 finished after 8 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 62.\n",
      "Episode 878000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 879000 finished after 13 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "action_probs_orig \n",
      "tensor([[-4.1165,  7.7977, -2.0885, -4.4101]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1415,  7.8558, -2.0958, -4.4388]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 880000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 881000 finished after 9 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 882000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 61.\n",
      "Episode 883000 finished after 14 timesteps with r=1.0. Running score: 0.09. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 884000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 885000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 886000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 887000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 888000 finished after 7 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 889000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 52.\n",
      "action_probs_orig \n",
      "tensor([[-4.1372,  7.8036, -2.1075, -4.4274]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2299,  8.0068, -2.0868, -4.5113]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7239,  9.0649, -1.7434, -4.8492]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7239,  9.0649, -1.7434, -4.8492]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 890000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 54.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 891000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 892000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 893000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 894000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 895000 finished after 4 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 896000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 897000 finished after 10 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 898000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 899000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 49.\n",
      "action_probs_orig \n",
      "tensor([[-4.1256,  7.8324, -2.1461, -4.4192]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1821,  7.9552, -2.1234, -4.4653]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.6030,  8.9142, -1.7997, -4.7396]])\n",
      "On state=8, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.6030,  8.9142, -1.7997, -4.7396]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.9918,  9.3015, -1.5890, -5.0197]])\n",
      "On state=9, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -6.6550,  10.9064,  -0.6967,  -6.2215]])\n",
      "On state=13, selected action=1 , \n",
      "new state=13, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -6.6550,  10.9064,  -0.6967,  -6.2215]])\n",
      "On state=13, selected action=1 , \n",
      "new state=14, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -7.0776,  11.5224,  -0.3393,  -6.4945]])\n",
      "On state=14, selected action=1 , \n",
      "new state=15, done=True\n",
      "Episode 900000 finished after 8 timesteps with r=1.0. Running score: 0.03. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 901000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 902000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 903000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 904000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 905000 finished after 7 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 906000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 907000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 908000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 909000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 46.\n",
      "action_probs_orig \n",
      "tensor([[-4.0880,  7.8345, -2.1601, -4.3859]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1794,  8.0424, -2.1486, -4.4702]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7119,  9.1398, -1.8062, -4.8416]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 910000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 911000 finished after 9 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 38.\n",
      "Episode 912000 finished after 7 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 42.\n",
      "Episode 913000 finished after 5 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 914000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 915000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 916000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 64.\n",
      "Episode 917000 finished after 15 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 918000 finished after 2 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 919000 finished after 7 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 50.\n",
      "action_probs_orig \n",
      "tensor([[-4.1313,  7.9175, -2.2162, -4.4232]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2427,  8.1711, -2.2112, -4.5302]])\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7720,  9.2975, -1.8891, -4.9082]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.2207,  9.7417, -1.6729, -5.2415]])\n",
      "On state=9, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7720,  9.2975, -1.8891, -4.9082]])\n",
      "On state=8, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.2207,  9.7417, -1.6729, -5.2415]])\n",
      "On state=9, selected action=1 , \n",
      "new state=10, done=False\n",
      "action_probs_orig \n",
      "tensor([[ -5.6694,  10.1859,  -1.4568,  -5.5749]])\n",
      "On state=10, selected action=1 , \n",
      "new state=9, done=False\n",
      "action_probs_orig \n",
      "tensor([[-5.2207,  9.7417, -1.6729, -5.2415]])\n",
      "On state=9, selected action=1 , \n",
      "new state=8, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.7720,  9.2975, -1.8891, -4.9082]])\n",
      "On state=8, selected action=1 , \n",
      "new state=12, done=True\n",
      "Episode 920000 finished after 9 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 921000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 922000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 923000 finished after 3 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 924000 finished after 7 timesteps with r=1.0. Running score: 0.06. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 925000 finished after 8 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 926000 finished after 5 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 927000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 928000 finished after 2 timesteps with r=0.0. Running score: 0.13. Times trained: 1000. Times reached goal: 61.\n",
      "Episode 929000 finished after 5 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 48.\n",
      "action_probs_orig \n",
      "tensor([[-4.1401,  7.9220, -2.2177, -4.4124]])\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.2290,  8.1223, -2.2068, -4.4945]])\n",
      "On state=4, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 930000 finished after 2 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 931000 finished after 9 timesteps with r=1.0. Running score: 0.06. Times trained: 1000. Times reached goal: 41.\n",
      "Episode 932000 finished after 3 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 933000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 934000 finished after 11 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 935000 finished after 6 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 936000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 42.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 937000 finished after 12 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 938000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 939000 finished after 3 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 51.\n",
      "action_probs_orig \n",
      "tensor([[-4.1561,  7.9657, -2.2269, -4.3918]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1561,  7.9657, -2.2269, -4.3918]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1561,  7.9657, -2.2269, -4.3918]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1661,  7.9898, -2.2312, -4.4036]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1760,  8.0138, -2.2355, -4.4154]])\n",
      "On state=2, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1860,  8.0379, -2.2398, -4.4271]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 940000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 941000 finished after 10 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 55.\n",
      "Episode 942000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 943000 finished after 4 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 34.\n",
      "Episode 944000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 60.\n",
      "Episode 945000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 946000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 947000 finished after 3 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 948000 finished after 2 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 949000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 54.\n",
      "action_probs_orig \n",
      "tensor([[-4.1538,  7.9892, -2.2527, -4.4018]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1630,  8.0116, -2.2570, -4.4128]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 950000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 53.\n",
      "Episode 951000 finished after 5 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 952000 finished after 4 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 953000 finished after 3 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 54.\n",
      "Episode 954000 finished after 7 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 67.\n",
      "Episode 955000 finished after 8 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 52.\n",
      "Episode 956000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 957000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 958000 finished after 7 timesteps with r=0.0. Running score: 0.01. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 959000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 46.\n",
      "action_probs_orig \n",
      "tensor([[-4.1463,  8.0153, -2.2794, -4.3972]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1556,  8.0383, -2.2842, -4.4083]])\n",
      "On state=1, selected action=1 , \n",
      "new state=2, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1649,  8.0614, -2.2890, -4.4195]])\n",
      "On state=2, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1741,  8.0844, -2.2939, -4.4307]])\n",
      "On state=3, selected action=1 , \n",
      "new state=3, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1741,  8.0844, -2.2939, -4.4307]])\n",
      "On state=3, selected action=1 , \n",
      "new state=7, done=True\n",
      "Episode 960000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 961000 finished after 6 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 962000 finished after 3 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 49.\n",
      "Episode 963000 finished after 5 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 56.\n",
      "Episode 964000 finished after 14 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 63.\n",
      "Episode 965000 finished after 8 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 966000 finished after 4 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 967000 finished after 4 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 968000 finished after 9 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 37.\n",
      "Episode 969000 finished after 9 timesteps with r=1.0. Running score: 0.07. Times trained: 1000. Times reached goal: 53.\n",
      "action_probs_orig \n",
      "tensor([[-4.1398,  8.0818, -2.3026, -4.4289]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1398,  8.0818, -2.3026, -4.4289]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1623,  8.1375, -2.3132, -4.4553]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 970000 finished after 3 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 62.\n",
      "Episode 971000 finished after 3 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 972000 finished after 2 timesteps with r=0.0. Running score: 0.08. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 973000 finished after 5 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 974000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 63.\n",
      "Episode 975000 finished after 6 timesteps with r=0.0. Running score: 0.07. Times trained: 1000. Times reached goal: 62.\n",
      "Episode 976000 finished after 2 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 44.\n",
      "Episode 977000 finished after 3 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 978000 finished after 12 timesteps with r=1.0. Running score: 0.06. Times trained: 1000. Times reached goal: 48.\n",
      "Episode 979000 finished after 9 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 57.\n",
      "action_probs_orig \n",
      "tensor([[-4.1294,  8.1123, -2.3135, -4.4268]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1294,  8.1123, -2.3135, -4.4268]])\n",
      "On state=0, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1294,  8.1123, -2.3135, -4.4268]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1567,  8.1805, -2.3263, -4.4590]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 980000 finished after 4 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 981000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 982000 finished after 6 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 58.\n",
      "Episode 983000 finished after 11 timesteps with r=1.0. Running score: 0.1. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 984000 finished after 7 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 54.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 985000 finished after 2 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 35.\n",
      "Episode 986000 finished after 2 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 47.\n",
      "Episode 987000 finished after 5 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 45.\n",
      "Episode 988000 finished after 6 timesteps with r=0.0. Running score: 0.02. Times trained: 1000. Times reached goal: 33.\n",
      "Episode 989000 finished after 7 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 58.\n",
      "action_probs_orig \n",
      "tensor([[-4.1373,  8.1354, -2.3332, -4.4445]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1608,  8.1941, -2.3441, -4.4723]])\n",
      "On state=1, selected action=1 , \n",
      "new state=0, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1373,  8.1354, -2.3332, -4.4445]])\n",
      "On state=0, selected action=1 , \n",
      "new state=1, done=False\n",
      "action_probs_orig \n",
      "tensor([[-4.1608,  8.1941, -2.3441, -4.4723]])\n",
      "On state=1, selected action=1 , \n",
      "new state=5, done=True\n",
      "Episode 990000 finished after 4 timesteps with r=0.0. Running score: 0.09. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 991000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 992000 finished after 15 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 57.\n",
      "Episode 993000 finished after 8 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 43.\n",
      "Episode 994000 finished after 4 timesteps with r=0.0. Running score: 0.06. Times trained: 1000. Times reached goal: 51.\n",
      "Episode 995000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 36.\n",
      "Episode 996000 finished after 5 timesteps with r=0.0. Running score: 0.03. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 997000 finished after 2 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 50.\n",
      "Episode 998000 finished after 7 timesteps with r=0.0. Running score: 0.05. Times trained: 1000. Times reached goal: 46.\n",
      "Episode 999000 finished after 3 timesteps with r=0.0. Running score: 0.04. Times trained: 1000. Times reached goal: 47.\n"
     ]
    }
   ],
   "source": [
    "# custom weights initialization \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        #m.weight.data.normal_(0.0, 0.02)\n",
    "        #m.weight.data.uniform_(0.0, 0.02)\n",
    "        m.weight.data.fill_(0.5)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "NUM_EPISODES = 1000000\n",
    "GAMMA = 0.9\n",
    "net = pi_net()\n",
    "net.apply(weights_init)\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    episode_series = []\n",
    "    while not done:\n",
    "        # Get action from pi\n",
    "        # action = env.action_space.sample()\n",
    "        np_observation = np.array(observation)\n",
    "        #np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        observation_tensor = torch.FloatTensor(np_observation) \n",
    "        #print(observation_tensor)\n",
    "        #net.eval()\n",
    "        #print(\"before eval\")\n",
    "        action_probs, value = net(observation_tensor)\n",
    "        action_probs_orig = action_probs\n",
    "        #print(\"action_probs after net\")\n",
    "        #print(action_probs)\n",
    "        #FOR EXPLORATION: \n",
    "        #action_probs = F.dropout(action_probs, p=0.3, training=True)\n",
    "        #print(\"action_probs after dropout\")\n",
    "        #print(action_probs)\n",
    "        action_probs = F.softmax(action_probs, dim=1)\n",
    "        #print(\"action_probs after softmax\")\n",
    "        #print(action_probs)\n",
    "        #action = action_probs.multinomial(num_samples=1)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"after eval\")\n",
    "        #print(\"action_probs\")\n",
    "        #print(action_probs)\n",
    "        log_prob = m.log_prob(action)\n",
    "        #print(\"log_prob\")\n",
    "        #print(log_prob)\n",
    "        #break\n",
    "        #print(\"softmax\")\n",
    "        #print(action_probs)\n",
    "        #print(\"action\")\n",
    "        #print(str(action.item()))\n",
    "        #print(type(prob.multinomial))\n",
    "        \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        \n",
    "        if k%10000 == 0:\n",
    "            print(\"action_probs_orig \")\n",
    "            print(action_probs_orig)\n",
    "            print(\"On state=\"+ str(observation) + \", selected action=\" + str(action.item()) + \" , \")\n",
    "        \n",
    "        observation, reward, done, info = env.step(action.item()) \n",
    "        \n",
    "        if k%10000 == 0:\n",
    "            print(\"new state=\"+ str(observation) + \", done=\"+str(done))\n",
    "        #if done and reward != 1.0:\n",
    "        #    reward = -1.0\n",
    "        step_data = [observation, action,log_prob, value, reward, done, info]\n",
    "        episode_series.append(step_data)\n",
    "        #env.render()\n",
    "        \n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "\n",
    "    if k%1000 == 0:\n",
    "        print(\"Episode {} finished after {} timesteps with r={}. Running score: {}. Times trained: {}. Times reached goal: {}.\".format(k, len(episode_series), reward, np.mean(score), times_trained, times_reach_goal))\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "        #print(\"Game finished. \" + \"-\" * 5)\n",
    "        #print(len(episode_series))\n",
    "#         for param in net.parameters():\n",
    "#             print(param.data)\n",
    "        \n",
    "\n",
    "    \n",
    "    discounted_rewards = []\n",
    "    actions = []\n",
    "    values = []\n",
    "    log_probs = []\n",
    "    \n",
    "    R = 0\n",
    "    for i in reversed(range(len(episode_series))): # Build the different containers: action, value, reward\n",
    "        [observation, action, log_prob, value, reward, done, info] = episode_series[i]\n",
    "        R = reward + R * GAMMA\n",
    "        discounted_rewards.append(R)\n",
    "        actions.append(action)\n",
    "        values.append(value)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "    value_loss = 0\n",
    "    policy_log_loss = 0\n",
    "    for action, value, log_prob, reward in zip(actions, values, log_probs, discounted_rewards):\n",
    "        reward_diff = reward - value.item() # Treat critic value as baseline\n",
    "        #action.reinforce(reward_diff) # Try to perform better than baseline\n",
    "        policy_log_loss += -log_prob * reward_diff\n",
    "        value_loss += mse(value, torch.Tensor([reward])) # Compare with actual reward\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    if R > 0.0 or True: # Optimize only if rewards are non zero.\n",
    "        #print \"Reward list\"\n",
    "        #print rewards_list\n",
    "        optimizer.zero_grad()\n",
    "        nodes = [value_loss] + [policy_log_loss]\n",
    "        gradients = [torch.ones(1)] + [torch.ones(1)] # No gradients for reinforced values\n",
    "        autograd.backward(nodes, gradients)\n",
    "        optimizer.step()\n",
    "        times_trained = times_trained + 1\n",
    "    \n",
    "    if R > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
