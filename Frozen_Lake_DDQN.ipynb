{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "   id='FrozenLakeNotSlippery-v0',\n",
    "   entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "   kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "   max_episode_steps=100,\n",
    "   reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "\n",
    "#env = gym.make('FrozenLake8x8-v0')\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.render()\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "class q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(q_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 20)\n",
    "        self.linear2 = nn.Linear(20, 4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "#         print(x.shape)\n",
    "#         print(x)\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "        x = x.view(-1,1)\n",
    "        x = F.tanh(self.linear1(x))\n",
    "        #x = F.softmax(self.linear2(x), dim=0)\n",
    "        x = F.tanh(self.linear2(x))\n",
    "        x = x.view(-1,4)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1, out_features=20, bias=True)\n",
      "Linear(in_features=20, out_features=4, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "random.seed(1999)\n",
    "import math\n",
    "\n",
    "# custom weights initialization \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print classname\n",
    "    #print q_net\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        if not m.bias is None:\n",
    "            m.bias.data.normal_(0.0, 0.02)\n",
    "        #m.weight.data.uniform_(0.0, 0.02)        \n",
    "        #m.weight.data.fill_(0.1)\n",
    "        #if not m.bias is None:\n",
    "        #    m.bias.data.fill_(0.1)\n",
    "        print m\n",
    "        \n",
    "\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mse = nn.MSELoss(reduce=False)\n",
    "NUM_EPISODES = 1000000\n",
    "BATCH_SIZE = 500\n",
    "GAMMA = 0.9\n",
    "TARGET_UPDATE = 50\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.0\n",
    "EPS_DECAY = 1000000\n",
    "online_net = q_net().to(device)\n",
    "online_net.apply(weights_init)\n",
    "target_net = q_net().to(device)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "memory = ReplayMemory(100000)\n",
    "#optimizer = optim.RMSprop(online_net.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(online_net.parameters(), lr=0.0001)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda d: d is False,\n",
    "    #                                      batch.done)), device=device, dtype=torch.uint8).unsqueeze(1)\n",
    "    # Compute states that are final.\n",
    "    next_state_final_mask = torch.tensor(tuple(map(lambda d: d in [5,7,11,12],\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8).unsqueeze(1) \n",
    "    next_state_finak_list = [d for d in batch.next_state if d in [5,7,11,12] ]\n",
    "    \n",
    "    \n",
    "    #non_final_next_states = torch.cat([FloatTensor([s]) for s,d in zip(batch.next_state,batch.done)\n",
    "    #                                            if d is False])\n",
    "    \n",
    "    #state_batch = torch.cat([torch.FloatTensor([s]) for s in batch.state])\n",
    "    state_batch = FloatTensor(batch.state)\n",
    "    state_batch = state_batch.view(BATCH_SIZE, 1)\n",
    "    next_state_batch = FloatTensor(batch.next_state)\n",
    "    next_state_batch = next_state_batch.view(BATCH_SIZE, 1)\n",
    "    #action_batch = torch.cat([torch.LongTensor([[a.item()]]) for a in batch.action])\n",
    "    action_batch = LongTensor(batch.action).view(BATCH_SIZE,1)\n",
    "    #reward_batch = torch.cat([torch.tensor([r]) for r in batch.reward])\n",
    "    reward_batch = Tensor(batch.reward).view(BATCH_SIZE,1)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "#     print(\"state_batch \"+\"-\" * 10)\n",
    "#     print(state_batch.shape)\n",
    "#     print(\"action_batch \"+\"-\" * 10)\n",
    "#     print(action_batch.shape)\n",
    "    state_action_values = online_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # next_state_values = torch.zeros(BATCH_SIZE, device=device).view(BATCH_SIZE,1)\n",
    "#     print(\"non_final_mask\")\n",
    "#     print(non_final_mask.shape)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # DDQN ------------------------\n",
    "    # Y_ddqn = R_next + GAMMA * Q_target(S_next, argmax_a Q_online(S_next, a) )\n",
    "    # Below is the argmax_a Q_online(S_next,a)\n",
    "    argmax_a_online_net_next_state = online_net(next_state_batch).max(1)[1].detach().view(BATCH_SIZE,1)\n",
    "    \n",
    "    # Below is the actual Q_target\n",
    "    next_state_values = target_net(next_state_batch).detach().gather(1, argmax_a_online_net_next_state)\n",
    "    # DDQN ------------------------\n",
    "#     print \"next_state_values\"\n",
    "#     print next_state_values.shape\n",
    "#     print next_state_values.type()\n",
    "#     print \"final_mask\"\n",
    "#     print final_mask.shape\n",
    "#     print  final_mask.type()\n",
    "#     print \"next_state_values[final_mask]\"\n",
    "#     print next_state_values[final_mask].shape\n",
    "#     print next_state_values[final_mask].type()\n",
    "    next_state_values[next_state_final_mask] = torch.zeros(len(next_state_finak_list), device=device).view(len(next_state_finak_list))\n",
    "#     print(\"next_state_values\")\n",
    "#     print(next_state_values.shape)\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "#     print(\"expected_state_action_values\")\n",
    "#     print(expected_state_action_values.shape)\n",
    "\n",
    "    # Compute Huber loss (this is like MSE , but less sensitive to outliers )\n",
    "    # loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    #loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    loss = mse( state_action_values, expected_state_action_values)\n",
    "    debug = False\n",
    "    if debug:\n",
    "        print(\"-\" * 40)\n",
    "        print(\"States\")\n",
    "        print(state_batch)\n",
    "        print(\"Target Q\")\n",
    "        print(expected_state_action_values)\n",
    "        print(expected_state_action_values.shape)\n",
    "        print(\"Actual Q\")\n",
    "        print(state_action_values)\n",
    "        print(state_action_values.shape)\n",
    "        print(\"Loss\")\n",
    "        print(loss)\n",
    "        print(loss.shape)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(torch.ones(len(state_action_values), device=device).unsqueeze(1))\n",
    "    \n",
    "    # param in online_net.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if debug: \n",
    "        print(\"AFTER OPTIMIZATION:\")\n",
    "        print(\"New actual Q\")\n",
    "        print(online_net(state_batch).gather(1, action_batch))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.1451,  1.4440,  4.2905,  3.2095]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.1451,  1.4440,  4.2905,  3.2095]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.1451,  1.4440,  4.2905,  3.2095]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.1451,  1.4440,  4.2905,  3.2095]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.1451,  1.4440,  4.2905,  3.2095]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 1.0304,  1.3544,  4.7594,  3.2289]], device='cuda:0')\n",
      "On state=1, selected action=1 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'print_q_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-90b7818e16c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mprint_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'print_q_table' is not defined"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "# TO BE DELETED_______\n",
    "#steps_done = 40000000\n",
    "#EPS_END = 0.0\n",
    "\n",
    "# TO BE DELETED_______\n",
    "\n",
    "\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    episode_series = []\n",
    "    reward = 0\n",
    "    episode_step = 0\n",
    "    while not done:\n",
    "        # Get action from pi\n",
    "        # action = env.action_space.sample()\n",
    "        np_observation = np.array(observation)\n",
    "        #np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        observation_tensor = FloatTensor(np_observation)\n",
    "        #print(observation_tensor)\n",
    "        #net.eval()\n",
    "        #print(\"before eval\")\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        steps_done += 1\n",
    "        q_values = online_net(observation_tensor)\n",
    "        if sample >= eps_threshold: \n",
    "            #print \"observation_tensor\"\n",
    "            #print observation_tensor.type()\n",
    "            \n",
    "            action = q_values.max(1)[1] # First 1 is the dimension, second 1 is the index (this is argmax)\n",
    "        else:\n",
    "            action = torch.LongTensor([[random.randint(0,3)]], device=device)\n",
    "        \n",
    "            \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        old_state = observation                    \n",
    "            \n",
    "        \n",
    "        observation, reward, done, info = env.step(action.item()) \n",
    "        new_state = observation\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#         if done and reward != 1.0:\n",
    "#             if episode_step > 50:\n",
    "#                 reward = -0.2\n",
    "#             if episode_step > 80:\n",
    "#                 reward = -0.5\n",
    "#             if new_state in [5,7,11,12]:\n",
    "#                 reward = -1.0\n",
    "\n",
    "        \n",
    "        \n",
    "        # Store the transition in memory\n",
    "        #if old_state != new_state:\n",
    "\n",
    "        memory.push(old_state, action, new_state, reward, done)\n",
    "        if k%5000 == 0:\n",
    "            #print(\"old_state != new_state\")\n",
    "            #print(old_state != new_state)\n",
    "            #print(\"oldstate \" + str(old_state) + \" newstate \" + str(new_state))\n",
    "            print(\"q_values \")\n",
    "            print(q_values)\n",
    "            print(\"On state=\"+ str(old_state) + \", selected action=\" + str(action.item()) + \" , \" + \\\n",
    "              \"Random? \" + str( sample < eps_threshold ))\n",
    "            print(\"new state=\"+ str(new_state) + \", done=\"+str(done) + \\\n",
    "             \". Reward: \" + str(reward))\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        if k > BATCH_SIZE :\n",
    "            optimize_model()\n",
    "            times_trained = times_trained + 1\n",
    "\n",
    "        episode_step += 1\n",
    "        #env.render()\n",
    "    if k % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "    \n",
    "    if k % 1000 ==0:\n",
    "        print_q_table()\n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "\n",
    "    if k%1000 == 0:\n",
    "        print(\"Episode {} finished after {} timesteps with r={}. Running score: {}. Times trained: \\\n",
    "              {}. Times reached goal: {}. \\\n",
    "              Steps done: {}. EPS_DECAY: {}. EPS_THRESHOLD: {}.\".format(k, len(episode_series), \\\n",
    "                                                                    reward, np.mean(score), times_trained, \\\n",
    "                                                                       times_reach_goal, steps_done, \\\n",
    "                                                                       EPS_DECAY, eps_threshold))\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "        #print(\"Game finished. \" + \"-\" * 5)\n",
    "        #print(len(episode_series))\n",
    "#         for param in net.parameters():\n",
    "#             print(param.data)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if reward > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(-0.016590623185) A[1]:(-0.0379296652973) A[2]:(0.00880737882107) A[3]:(0.0203321184963)\n",
      " state (1)  A[0]:(-0.0167372319847) A[1]:(-0.0366442278028) A[2]:(0.0104934358969) A[3]:(0.0212363991886)\n",
      " state (2)  A[0]:(-0.0168837141246) A[1]:(-0.0353712290525) A[2]:(0.0121734226122) A[3]:(0.022135829553)\n",
      " state (3)  A[0]:(-0.0170289520174) A[1]:(-0.0341202318668) A[2]:(0.0138428043574) A[3]:(0.0230262875557)\n",
      " state (4)  A[0]:(-0.0171718932688) A[1]:(-0.0329002924263) A[2]:(0.0154972476885) A[3]:(0.0239038150758)\n",
      " state (5)  A[0]:(-0.0173115506768) A[1]:(-0.0317197963595) A[2]:(0.0171326845884) A[3]:(0.0247646775097)\n",
      " state (6)  A[0]:(-0.0174470487982) A[1]:(-0.0305862799287) A[2]:(0.018745386973) A[3]:(0.0256054457277)\n",
      " state (7)  A[0]:(-0.0175776146352) A[1]:(-0.0295063517988) A[2]:(0.0203320086002) A[3]:(0.0264229979366)\n",
      " state (8)  A[0]:(-0.0177025925368) A[1]:(-0.0284855756909) A[2]:(0.0218896120787) A[3]:(0.0272145923227)\n",
      " state (9)  A[0]:(-0.0178214609623) A[1]:(-0.0275284443051) A[2]:(0.0234157033265) A[3]:(0.0279778521508)\n",
      " state (10)  A[0]:(-0.017933819443) A[1]:(-0.0266383886337) A[2]:(0.0249082185328) A[3]:(0.0287107806653)\n",
      " state (11)  A[0]:(-0.0180393848568) A[1]:(-0.0258177798241) A[2]:(0.0263655278832) A[3]:(0.0294117573649)\n",
      " state (12)  A[0]:(-0.0181379895657) A[1]:(-0.0250680148602) A[2]:(0.0277864057571) A[3]:(0.0300795268267)\n",
      " state (13)  A[0]:(-0.0182295646518) A[1]:(-0.0243895780295) A[2]:(0.0291700139642) A[3]:(0.0307131670415)\n",
      " state (14)  A[0]:(-0.0183141361922) A[1]:(-0.023782145232) A[2]:(0.0305158682168) A[3]:(0.0313120670617)\n",
      " state (15)  A[0]:(-0.0183917991817) A[1]:(-0.023244664073) A[2]:(0.0318237841129) A[3]:(0.0318759083748)\n",
      " state (0)  A[0]:(-0.016590623185) A[1]:(-0.0379296652973) A[2]:(0.00880737882107) A[3]:(0.0203321184963)\n",
      " state (1)  A[0]:(-0.0167372319847) A[1]:(-0.0366442278028) A[2]:(0.0104934358969) A[3]:(0.0212363991886)\n",
      " state (2)  A[0]:(-0.0168837141246) A[1]:(-0.0353712290525) A[2]:(0.0121734226122) A[3]:(0.022135829553)\n",
      " state (3)  A[0]:(-0.0170289520174) A[1]:(-0.0341202318668) A[2]:(0.0138428043574) A[3]:(0.0230262875557)\n",
      " state (4)  A[0]:(-0.0171718932688) A[1]:(-0.0329002924263) A[2]:(0.0154972476885) A[3]:(0.0239038150758)\n",
      " state (5)  A[0]:(-0.0173115506768) A[1]:(-0.0317197963595) A[2]:(0.0171326845884) A[3]:(0.0247646775097)\n",
      " state (6)  A[0]:(-0.0174470487982) A[1]:(-0.0305862799287) A[2]:(0.018745386973) A[3]:(0.0256054457277)\n",
      " state (7)  A[0]:(-0.0175776146352) A[1]:(-0.0295063517988) A[2]:(0.0203320086002) A[3]:(0.0264229979366)\n",
      " state (8)  A[0]:(-0.0177025925368) A[1]:(-0.0284855756909) A[2]:(0.0218896120787) A[3]:(0.0272145923227)\n",
      " state (9)  A[0]:(-0.0178214609623) A[1]:(-0.0275284443051) A[2]:(0.0234157033265) A[3]:(0.0279778521508)\n",
      " state (10)  A[0]:(-0.017933819443) A[1]:(-0.0266383886337) A[2]:(0.0249082185328) A[3]:(0.0287107806653)\n",
      " state (11)  A[0]:(-0.0180393848568) A[1]:(-0.0258177798241) A[2]:(0.0263655278832) A[3]:(0.0294117573649)\n",
      " state (12)  A[0]:(-0.0181379895657) A[1]:(-0.0250680148602) A[2]:(0.0277864057571) A[3]:(0.0300795268267)\n",
      " state (13)  A[0]:(-0.0182295646518) A[1]:(-0.0243895780295) A[2]:(0.0291700139642) A[3]:(0.0307131670415)\n",
      " state (14)  A[0]:(-0.0183141361922) A[1]:(-0.023782145232) A[2]:(0.0305158682168) A[3]:(0.0313120670617)\n",
      " state (15)  A[0]:(-0.0183917991817) A[1]:(-0.023244664073) A[2]:(0.0318237841129) A[3]:(0.0318759083748)\n"
     ]
    }
   ],
   "source": [
    "def print_q_table():\n",
    "    for i in range(16):\n",
    "        st = np.array(i)\n",
    "        st = np.expand_dims(st, axis=0)\n",
    "        q_vals = online_net(FloatTensor(st))\n",
    "        outp = \" state (\" +str(i) + \") \"\n",
    "        n = 0\n",
    "        for tensr in q_vals:\n",
    "            for cell in tensr:\n",
    "                outp = outp + \" A[\" + str(n) + \"]:(\" + str(cell.item()) + \")\"\n",
    "                n += 1\n",
    "        print(outp)\n",
    "        \n",
    "print_q_table()\n",
    "print_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
