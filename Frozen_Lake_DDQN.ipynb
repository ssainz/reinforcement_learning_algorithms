{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "   id='FrozenLakeNotSlippery-v0',\n",
    "   entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "   kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "   max_episode_steps=100,\n",
    "   reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "\n",
    "#env = gym.make('FrozenLake8x8-v0')\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.render()\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "class q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(q_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 20)\n",
    "        self.linear2 = nn.Linear(20, 4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "#         print(x.shape)\n",
    "#         print(x)\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "        x = x.view(-1,1)\n",
    "        x = F.tanh(self.linear1(x))\n",
    "        #x = F.softmax(self.linear2(x), dim=0)\n",
    "        x = F.tanh(self.linear2(x))\n",
    "        x = x.view(-1,4)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1, out_features=20, bias=True)\n",
      "Linear(in_features=20, out_features=4, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "random.seed(1999)\n",
    "import math\n",
    "\n",
    "# custom weights initialization \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print classname\n",
    "    #print q_net\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        if not m.bias is None:\n",
    "            m.bias.data.normal_(0.0, 0.02)\n",
    "        #m.weight.data.uniform_(0.0, 0.02)        \n",
    "        #m.weight.data.fill_(0.1)\n",
    "        #if not m.bias is None:\n",
    "        #    m.bias.data.fill_(0.1)\n",
    "        print m\n",
    "        \n",
    "\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mse = nn.MSELoss(reduce=False)\n",
    "NUM_EPISODES = 1000000\n",
    "BATCH_SIZE = 500\n",
    "GAMMA = 0.9\n",
    "TARGET_UPDATE = 50\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.0\n",
    "EPS_DECAY = 1000000\n",
    "online_net = q_net().to(device)\n",
    "online_net.apply(weights_init)\n",
    "target_net = q_net().to(device)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "memory = ReplayMemory(100000)\n",
    "#optimizer = optim.RMSprop(online_net.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(online_net.parameters(), lr=0.0001)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda d: d is False,\n",
    "    #                                      batch.done)), device=device, dtype=torch.uint8).unsqueeze(1)\n",
    "    # Compute states that are final.\n",
    "    next_state_final_mask = torch.tensor(tuple(map(lambda d: d in [5,7,11,12],\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8).unsqueeze(1) \n",
    "    next_state_finak_list = [d for d in batch.next_state if d in [5,7,11,12] ]\n",
    "    \n",
    "    \n",
    "    #non_final_next_states = torch.cat([FloatTensor([s]) for s,d in zip(batch.next_state,batch.done)\n",
    "    #                                            if d is False])\n",
    "    \n",
    "    #state_batch = torch.cat([torch.FloatTensor([s]) for s in batch.state])\n",
    "    state_batch = FloatTensor(batch.state)\n",
    "    state_batch = state_batch.view(BATCH_SIZE, 1)\n",
    "    next_state_batch = FloatTensor(batch.next_state)\n",
    "    next_state_batch = next_state_batch.view(BATCH_SIZE, 1)\n",
    "    #action_batch = torch.cat([torch.LongTensor([[a.item()]]) for a in batch.action])\n",
    "    action_batch = LongTensor(batch.action).view(BATCH_SIZE,1)\n",
    "    #reward_batch = torch.cat([torch.tensor([r]) for r in batch.reward])\n",
    "    reward_batch = Tensor(batch.reward).view(BATCH_SIZE,1)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "#     print(\"state_batch \"+\"-\" * 10)\n",
    "#     print(state_batch.shape)\n",
    "#     print(\"action_batch \"+\"-\" * 10)\n",
    "#     print(action_batch.shape)\n",
    "    state_action_values = online_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # next_state_values = torch.zeros(BATCH_SIZE, device=device).view(BATCH_SIZE,1)\n",
    "#     print(\"non_final_mask\")\n",
    "#     print(non_final_mask.shape)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # DDQN ------------------------\n",
    "    # Y_ddqn = R_next + GAMMA * Q_target(S_next, argmax_a Q_online(S_next, a) )\n",
    "    # Below is the argmax_a Q_online(S_next,a)\n",
    "    argmax_a_online_net_next_state = online_net(next_state_batch).max(1)[1].detach().view(BATCH_SIZE,1)\n",
    "    \n",
    "    # Below is the actual Q_target\n",
    "    next_state_values = target_net(next_state_batch).detach().gather(1, argmax_a_online_net_next_state)\n",
    "    # DDQN ------------------------\n",
    "#     print \"next_state_values\"\n",
    "#     print next_state_values.shape\n",
    "#     print next_state_values.type()\n",
    "#     print \"final_mask\"\n",
    "#     print final_mask.shape\n",
    "#     print  final_mask.type()\n",
    "#     print \"next_state_values[final_mask]\"\n",
    "#     print next_state_values[final_mask].shape\n",
    "#     print next_state_values[final_mask].type()\n",
    "    next_state_values[next_state_final_mask] = torch.zeros(len(next_state_finak_list), device=device).view(len(next_state_finak_list))\n",
    "#     print(\"next_state_values\")\n",
    "#     print(next_state_values.shape)\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "#     print(\"expected_state_action_values\")\n",
    "#     print(expected_state_action_values.shape)\n",
    "\n",
    "    # Compute Huber loss (this is like MSE , but less sensitive to outliers )\n",
    "    # loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    #loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    loss = mse( state_action_values, expected_state_action_values)\n",
    "    debug = False\n",
    "    if debug:\n",
    "        print(\"-\" * 40)\n",
    "        print(\"States\")\n",
    "        print(state_batch)\n",
    "        print(\"Target Q\")\n",
    "        print(expected_state_action_values)\n",
    "        print(expected_state_action_values.shape)\n",
    "        print(\"Actual Q\")\n",
    "        print(state_action_values)\n",
    "        print(state_action_values.shape)\n",
    "        print(\"Loss\")\n",
    "        print(loss)\n",
    "        print(loss.shape)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(torch.ones(len(state_action_values), device=device).unsqueeze(1))\n",
    "    \n",
    "    # param in online_net.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if debug: \n",
    "        print(\"AFTER OPTIMIZATION:\")\n",
    "        print(\"New actual Q\")\n",
    "        print(online_net(state_batch).gather(1, action_batch))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[-1.6591, -3.7930,  0.8807,  2.0332]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[-1.6591, -3.7930,  0.8807,  2.0332]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[-1.6737, -3.6644,  1.0493,  2.1236]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[-1.6884, -3.5371,  1.2173,  2.2136]], device='cuda:0')\n",
      "On state=2, selected action=3 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[-1.6884, -3.5371,  1.2173,  2.2136]], device='cuda:0')\n",
      "On state=2, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[-1.7029, -3.4120,  1.3843,  2.3026]], device='cuda:0')\n",
      "On state=3, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[-1.7029, -3.4120,  1.3843,  2.3026]], device='cuda:0')\n",
      "On state=3, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[-1.7029, -3.4120,  1.3843,  2.3026]], device='cuda:0')\n",
      "On state=3, selected action=3 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[-1.7029, -3.4120,  1.3843,  2.3026]], device='cuda:0')\n",
      "On state=3, selected action=0 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[-1.6884, -3.5371,  1.2173,  2.2136]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[-1.7447, -3.0586,  1.8745,  2.5605]], device='cuda:0')\n",
      "On state=6, selected action=0 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(-0.016590623185) A[1]:(-0.0379296652973) A[2]:(0.00880737882107) A[3]:(0.0203321184963)\n",
      " state (1)  A[0]:(-0.0167372319847) A[1]:(-0.0366442278028) A[2]:(0.0104934358969) A[3]:(0.0212363991886)\n",
      " state (2)  A[0]:(-0.0168837141246) A[1]:(-0.0353712290525) A[2]:(0.0121734226122) A[3]:(0.022135829553)\n",
      " state (3)  A[0]:(-0.0170289520174) A[1]:(-0.0341202318668) A[2]:(0.0138428043574) A[3]:(0.0230262875557)\n",
      " state (4)  A[0]:(-0.0171718932688) A[1]:(-0.0329002924263) A[2]:(0.0154972476885) A[3]:(0.0239038150758)\n",
      " state (5)  A[0]:(-0.0173115506768) A[1]:(-0.0317197963595) A[2]:(0.0171326845884) A[3]:(0.0247646775097)\n",
      " state (6)  A[0]:(-0.0174470487982) A[1]:(-0.0305862799287) A[2]:(0.018745386973) A[3]:(0.0256054457277)\n",
      " state (7)  A[0]:(-0.0175776146352) A[1]:(-0.0295063517988) A[2]:(0.0203320086002) A[3]:(0.0264229979366)\n",
      " state (8)  A[0]:(-0.0177025925368) A[1]:(-0.0284855756909) A[2]:(0.0218896120787) A[3]:(0.0272145923227)\n",
      " state (9)  A[0]:(-0.0178214609623) A[1]:(-0.0275284443051) A[2]:(0.0234157033265) A[3]:(0.0279778521508)\n",
      " state (10)  A[0]:(-0.017933819443) A[1]:(-0.0266383886337) A[2]:(0.0249082185328) A[3]:(0.0287107806653)\n",
      " state (11)  A[0]:(-0.0180393848568) A[1]:(-0.0258177798241) A[2]:(0.0263655278832) A[3]:(0.0294117573649)\n",
      " state (12)  A[0]:(-0.0181379895657) A[1]:(-0.0250680148602) A[2]:(0.0277864057571) A[3]:(0.0300795268267)\n",
      " state (13)  A[0]:(-0.0182295646518) A[1]:(-0.0243895780295) A[2]:(0.0291700139642) A[3]:(0.0307131670415)\n",
      " state (14)  A[0]:(-0.0183141361922) A[1]:(-0.023782145232) A[2]:(0.0305158682168) A[3]:(0.0313120670617)\n",
      " state (15)  A[0]:(-0.0183917991817) A[1]:(-0.023244664073) A[2]:(0.0318237841129) A[3]:(0.0318759083748)\n",
      "Episode 0 finished after 0 timesteps with r=0.0. Running score: 0.0. Times trained:               0. Times reached goal: 0.               Steps done: 11. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.899991000045.\n",
      " state (0)  A[0]:(0.0525645241141) A[1]:(0.0631292909384) A[2]:(0.0311775356531) A[3]:(0.0549012646079)\n",
      " state (1)  A[0]:(0.0591400489211) A[1]:(0.0698074996471) A[2]:(0.0453330613673) A[3]:(0.057317763567)\n",
      " state (2)  A[0]:(0.0656793788075) A[1]:(0.0764333158731) A[2]:(0.0594564341009) A[3]:(0.0597211420536)\n",
      " state (3)  A[0]:(0.072177387774) A[1]:(0.0830018520355) A[2]:(0.0735313072801) A[3]:(0.0621098801494)\n",
      " state (4)  A[0]:(0.0786290615797) A[1]:(0.0895084291697) A[2]:(0.0875415802002) A[3]:(0.0644825100899)\n",
      " state (5)  A[0]:(0.0850296020508) A[1]:(0.0959486141801) A[2]:(0.101471461356) A[3]:(0.0668376088142)\n",
      " state (6)  A[0]:(0.0913743376732) A[1]:(0.102318122983) A[2]:(0.115305557847) A[3]:(0.0691737905145)\n",
      " state (7)  A[0]:(0.097658842802) A[1]:(0.108612984419) A[2]:(0.129028975964) A[3]:(0.071489751339)\n",
      " state (8)  A[0]:(0.103878863156) A[1]:(0.114829443395) A[2]:(0.142627343535) A[3]:(0.0737842023373)\n",
      " state (9)  A[0]:(0.110030397773) A[1]:(0.120963960886) A[2]:(0.156086936593) A[3]:(0.0760559514165)\n",
      " state (10)  A[0]:(0.116109661758) A[1]:(0.127013340592) A[2]:(0.169394597411) A[3]:(0.0783038511872)\n",
      " state (11)  A[0]:(0.122113093734) A[1]:(0.13297457993) A[2]:(0.182538032532) A[3]:(0.0805268064141)\n",
      " state (12)  A[0]:(0.128037363291) A[1]:(0.138844937086) A[2]:(0.195505544543) A[3]:(0.0827238038182)\n",
      " state (13)  A[0]:(0.133879452944) A[1]:(0.144621953368) A[2]:(0.208286330104) A[3]:(0.0848938897252)\n",
      " state (14)  A[0]:(0.139636516571) A[1]:(0.150303408504) A[2]:(0.220870390534) A[3]:(0.0870361551642)\n",
      " state (15)  A[0]:(0.145305976272) A[1]:(0.155887380242) A[2]:(0.233248472214) A[3]:(0.0891497954726)\n",
      "Episode 1000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               3343. Times reached goal: 16.               Steps done: 7558. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.893224334099.\n",
      " state (0)  A[0]:(0.0685048401356) A[1]:(0.0783124119043) A[2]:(0.0389671735466) A[3]:(0.0696211233735)\n",
      " state (1)  A[0]:(0.0753106474876) A[1]:(0.0853655338287) A[2]:(0.0550697110593) A[3]:(0.0719124823809)\n",
      " state (2)  A[0]:(0.0820922926068) A[1]:(0.0923812389374) A[2]:(0.0711457654834) A[3]:(0.0741935744882)\n",
      " state (3)  A[0]:(0.0888471901417) A[1]:(0.0993577837944) A[2]:(0.0871790423989) A[3]:(0.0764641091228)\n",
      " state (4)  A[0]:(0.0955728292465) A[1]:(0.106293559074) A[2]:(0.103153437376) A[3]:(0.0787238180637)\n",
      " state (5)  A[0]:(0.102266721427) A[1]:(0.113187044859) A[2]:(0.119053065777) A[3]:(0.0809724628925)\n",
      " state (6)  A[0]:(0.108926437795) A[1]:(0.120036736131) A[2]:(0.134862348437) A[3]:(0.0832097828388)\n",
      " state (7)  A[0]:(0.115549646318) A[1]:(0.126841291785) A[2]:(0.150566115975) A[3]:(0.0854355618358)\n",
      " state (8)  A[0]:(0.122134014964) A[1]:(0.133599296212) A[2]:(0.166149571538) A[3]:(0.0876495987177)\n",
      " state (9)  A[0]:(0.128677308559) A[1]:(0.140309616923) A[2]:(0.181598469615) A[3]:(0.0898516848683)\n",
      " state (10)  A[0]:(0.135177373886) A[1]:(0.146970912814) A[2]:(0.19689913094) A[3]:(0.0920416340232)\n",
      " state (11)  A[0]:(0.141632124782) A[1]:(0.153582155704) A[2]:(0.212038472295) A[3]:(0.0942192599177)\n",
      " state (12)  A[0]:(0.148039504886) A[1]:(0.160142213106) A[2]:(0.227004051208) A[3]:(0.0963844209909)\n",
      " state (13)  A[0]:(0.154397591949) A[1]:(0.166650086641) A[2]:(0.241784185171) A[3]:(0.0985369160771)\n",
      " state (14)  A[0]:(0.160704508424) A[1]:(0.173104763031) A[2]:(0.256367832422) A[3]:(0.100676611066)\n",
      " state (15)  A[0]:(0.16695843637) A[1]:(0.179505392909) A[2]:(0.270744770765) A[3]:(0.102803349495)\n",
      "Episode 2000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained:               6215. Times reached goal: 22.               Steps done: 13773. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.887690160122.\n",
      " state (0)  A[0]:(0.081898868084) A[1]:(0.0931971669197) A[2]:(0.0482233278453) A[3]:(0.0838516876101)\n",
      " state (1)  A[0]:(0.090101107955) A[1]:(0.102101810277) A[2]:(0.0672650709748) A[3]:(0.0869821682572)\n",
      " state (2)  A[0]:(0.098277553916) A[1]:(0.110964760184) A[2]:(0.0862691774964) A[3]:(0.0901017859578)\n",
      " state (3)  A[0]:(0.106425002217) A[1]:(0.119783334434) A[2]:(0.105213388801) A[3]:(0.0932100713253)\n",
      " state (4)  A[0]:(0.114540308714) A[1]:(0.128554940224) A[2]:(0.124075703323) A[3]:(0.0963065698743)\n",
      " state (5)  A[0]:(0.122620366514) A[1]:(0.137277111411) A[2]:(0.142834484577) A[3]:(0.0993908122182)\n",
      " state (6)  A[0]:(0.130662128329) A[1]:(0.145947411656) A[2]:(0.161468490958) A[3]:(0.102462373674)\n",
      " state (7)  A[0]:(0.138662666082) A[1]:(0.154563561082) A[2]:(0.179957225919) A[3]:(0.10552085191)\n",
      " state (8)  A[0]:(0.14661899209) A[1]:(0.16312327981) A[2]:(0.198280721903) A[3]:(0.108565807343)\n",
      " state (9)  A[0]:(0.154528349638) A[1]:(0.171624451876) A[2]:(0.216419890523) A[3]:(0.111596882343)\n",
      " state (10)  A[0]:(0.16238796711) A[1]:(0.180065020919) A[2]:(0.234356477857) A[3]:(0.114613674581)\n",
      " state (11)  A[0]:(0.170195147395) A[1]:(0.188443019986) A[2]:(0.252073138952) A[3]:(0.117615833879)\n",
      " state (12)  A[0]:(0.177947327495) A[1]:(0.196756482124) A[2]:(0.269553661346) A[3]:(0.120602980256)\n",
      " state (13)  A[0]:(0.185642004013) A[1]:(0.205003723502) A[2]:(0.286782771349) A[3]:(0.123574793339)\n",
      " state (14)  A[0]:(0.193276762962) A[1]:(0.213182941079) A[2]:(0.303746372461) A[3]:(0.126530900598)\n",
      " state (15)  A[0]:(0.200849294662) A[1]:(0.221292480826) A[2]:(0.320431411266) A[3]:(0.12947101891)\n",
      "Episode 3000 finished after 0 timesteps with r=0.0. Running score: 0.01. Times trained:               6556. Times reached goal: 31.               Steps done: 20329. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.881889498777.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.0870576873422) A[1]:(0.102145478129) A[2]:(0.0535888075829) A[3]:(0.0885259285569)\n",
      " state (1)  A[0]:(0.0964293852448) A[1]:(0.111839033663) A[2]:(0.0747825056314) A[3]:(0.0925482362509)\n",
      " state (2)  A[0]:(0.105774983764) A[1]:(0.121488578618) A[2]:(0.0959315821528) A[3]:(0.096560664475)\n",
      " state (3)  A[0]:(0.115090340376) A[1]:(0.131091117859) A[2]:(0.117007471621) A[3]:(0.100562304258)\n",
      " state (4)  A[0]:(0.124371387064) A[1]:(0.140643745661) A[2]:(0.137981981039) A[3]:(0.104552291334)\n",
      " state (5)  A[0]:(0.133614107966) A[1]:(0.150143578649) A[2]:(0.158827349544) A[3]:(0.108529746532)\n",
      " state (6)  A[0]:(0.142814561725) A[1]:(0.159587994218) A[2]:(0.179516538978) A[3]:(0.11249383539)\n",
      " state (7)  A[0]:(0.151968851686) A[1]:(0.168974280357) A[2]:(0.200023293495) A[3]:(0.116443708539)\n",
      " state (8)  A[0]:(0.161073192954) A[1]:(0.178299933672) A[2]:(0.220322296023) A[3]:(0.120378553867)\n",
      " state (9)  A[0]:(0.17012386024) A[1]:(0.187562480569) A[2]:(0.240389481187) A[3]:(0.124297559261)\n",
      " state (10)  A[0]:(0.179117277265) A[1]:(0.196759626269) A[2]:(0.260201901197) A[3]:(0.128199934959)\n",
      " state (11)  A[0]:(0.188049912453) A[1]:(0.205889046192) A[2]:(0.279737919569) A[3]:(0.132084921002)\n",
      " state (12)  A[0]:(0.196918398142) A[1]:(0.214948579669) A[2]:(0.298977404833) A[3]:(0.13595174253)\n",
      " state (13)  A[0]:(0.205719426274) A[1]:(0.223936185241) A[2]:(0.317901790142) A[3]:(0.139799639583)\n",
      " state (14)  A[0]:(0.214449867606) A[1]:(0.232849881053) A[2]:(0.336493968964) A[3]:(0.143627926707)\n",
      " state (15)  A[0]:(0.223106712103) A[1]:(0.241687744856) A[2]:(0.354738473892) A[3]:(0.147435873747)\n",
      "Episode 4000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained:               6477. Times reached goal: 28.               Steps done: 26806. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.876195958926.\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 7.8722,  8.9023,  4.2600,  7.8851]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 7.8658,  8.8904,  4.2625,  7.8869]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 7.8578,  8.8754,  4.2656,  7.8873]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 8.6134,  9.6744,  6.0687,  8.1988]], device='cuda:0')\n",
      "On state=1, selected action=1 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.0784553214908) A[1]:(0.0885028094053) A[2]:(0.0427735112607) A[3]:(0.078901194036)\n",
      " state (1)  A[0]:(0.0860776230693) A[1]:(0.0966307893395) A[2]:(0.0607546083629) A[3]:(0.0820118412375)\n",
      " state (2)  A[0]:(0.0936914533377) A[1]:(0.104737333953) A[2]:(0.0787302330136) A[3]:(0.0851204991341)\n",
      " state (3)  A[0]:(0.101294323802) A[1]:(0.112820640206) A[2]:(0.0966821610928) A[3]:(0.0882265865803)\n",
      " state (4)  A[0]:(0.10888376087) A[1]:(0.120878979564) A[2]:(0.114592202008) A[3]:(0.0913295000792)\n",
      " state (5)  A[0]:(0.116457268596) A[1]:(0.128910601139) A[2]:(0.132442265749) A[3]:(0.0944286435843)\n",
      " state (6)  A[0]:(0.124012388289) A[1]:(0.136913836002) A[2]:(0.150214448571) A[3]:(0.0975234135985)\n",
      " state (7)  A[0]:(0.131546676159) A[1]:(0.144887015224) A[2]:(0.167891144753) A[3]:(0.100613206625)\n",
      " state (8)  A[0]:(0.139057666063) A[1]:(0.152828529477) A[2]:(0.185455054045) A[3]:(0.103697434068)\n",
      " state (9)  A[0]:(0.146542996168) A[1]:(0.160736784339) A[2]:(0.202889353037) A[3]:(0.106775514781)\n",
      " state (10)  A[0]:(0.154000267386) A[1]:(0.168610170484) A[2]:(0.220177739859) A[3]:(0.109846837819)\n",
      " state (11)  A[0]:(0.161427125335) A[1]:(0.176447242498) A[2]:(0.237304449081) A[3]:(0.112910829484)\n",
      " state (12)  A[0]:(0.168821334839) A[1]:(0.184246495366) A[2]:(0.254254430532) A[3]:(0.115966901183)\n",
      " state (13)  A[0]:(0.176180556417) A[1]:(0.192006424069) A[2]:(0.271013259888) A[3]:(0.119014509022)\n",
      " state (14)  A[0]:(0.183502674103) A[1]:(0.199725702405) A[2]:(0.287567347288) A[3]:(0.122053049505)\n",
      " state (15)  A[0]:(0.190785467625) A[1]:(0.207402899861) A[2]:(0.303903907537) A[3]:(0.125081971288)\n",
      "Episode 5000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained:               6658. Times reached goal: 29.               Steps done: 33464. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.870381623633.\n",
      " state (0)  A[0]:(0.0888869911432) A[1]:(0.10519746691) A[2]:(0.0490958057344) A[3]:(0.0894040614367)\n",
      " state (1)  A[0]:(0.0985438525677) A[1]:(0.115145020187) A[2]:(0.071171104908) A[3]:(0.0932376235723)\n",
      " state (2)  A[0]:(0.108187273145) A[1]:(0.125057756901) A[2]:(0.0932335630059) A[3]:(0.097068823874)\n",
      " state (3)  A[0]:(0.11781270057) A[1]:(0.13493257761) A[2]:(0.115250818431) A[3]:(0.100896641612)\n",
      " state (4)  A[0]:(0.127415522933) A[1]:(0.144766509533) A[2]:(0.137190639973) A[3]:(0.104720041156)\n",
      " state (5)  A[0]:(0.136991217732) A[1]:(0.154556572437) A[2]:(0.159021019936) A[3]:(0.108537986875)\n",
      " state (6)  A[0]:(0.146535217762) A[1]:(0.164299920201) A[2]:(0.180710524321) A[3]:(0.112349435687)\n",
      " state (7)  A[0]:(0.156043052673) A[1]:(0.173993766308) A[2]:(0.202228456736) A[3]:(0.116153351963)\n",
      " state (8)  A[0]:(0.165510326624) A[1]:(0.183635368943) A[2]:(0.223544999957) A[3]:(0.119948685169)\n",
      " state (9)  A[0]:(0.17493262887) A[1]:(0.193222150207) A[2]:(0.244631484151) A[3]:(0.123734422028)\n",
      " state (10)  A[0]:(0.18430569768) A[1]:(0.202751398087) A[2]:(0.26546061039) A[3]:(0.127509519458)\n",
      " state (11)  A[0]:(0.193625301123) A[1]:(0.212220817804) A[2]:(0.286006391048) A[3]:(0.131272986531)\n",
      " state (12)  A[0]:(0.202887415886) A[1]:(0.221627831459) A[2]:(0.306244581938) A[3]:(0.135023787618)\n",
      " state (13)  A[0]:(0.212088003755) A[1]:(0.230970203876) A[2]:(0.326152563095) A[3]:(0.138760969043)\n",
      " state (14)  A[0]:(0.221223250031) A[1]:(0.240245625377) A[2]:(0.345709681511) A[3]:(0.142483532429)\n",
      " state (15)  A[0]:(0.230289444327) A[1]:(0.249451994896) A[2]:(0.364896953106) A[3]:(0.146190524101)\n",
      "Episode 6000 finished after 0 timesteps with r=0.0. Running score: 0.04. Times trained:               6627. Times reached goal: 32.               Steps done: 40091. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.864632674794.\n",
      " state (0)  A[0]:(0.0922831073403) A[1]:(0.102091789246) A[2]:(0.0536244027317) A[3]:(0.091303601861)\n",
      " state (1)  A[0]:(0.10048571974) A[1]:(0.11069766432) A[2]:(0.0736984089017) A[3]:(0.0945765227079)\n",
      " state (2)  A[0]:(0.108687922359) A[1]:(0.11928447336) A[2]:(0.0937891677022) A[3]:(0.0978536456823)\n",
      " state (3)  A[0]:(0.116886101663) A[1]:(0.127849921584) A[2]:(0.113869726658) A[3]:(0.101133838296)\n",
      " state (4)  A[0]:(0.125076577067) A[1]:(0.136391714215) A[2]:(0.133912920952) A[3]:(0.104415953159)\n",
      " state (5)  A[0]:(0.133255645633) A[1]:(0.144907668233) A[2]:(0.153891637921) A[3]:(0.10769880563)\n",
      " state (6)  A[0]:(0.141419634223) A[1]:(0.153395593166) A[2]:(0.17377884686) A[3]:(0.110981203616)\n",
      " state (7)  A[0]:(0.149564757943) A[1]:(0.161853283644) A[2]:(0.193547964096) A[3]:(0.114261940122)\n",
      " state (8)  A[0]:(0.157687410712) A[1]:(0.170278653502) A[2]:(0.213172897696) A[3]:(0.117539800704)\n",
      " state (9)  A[0]:(0.165783882141) A[1]:(0.178669586778) A[2]:(0.232628256083) A[3]:(0.120813533664)\n",
      " state (10)  A[0]:(0.173850506544) A[1]:(0.187024071813) A[2]:(0.251889497042) A[3]:(0.12408195436)\n",
      " state (11)  A[0]:(0.181883722544) A[1]:(0.195340082049) A[2]:(0.270933240652) A[3]:(0.127343803644)\n",
      " state (12)  A[0]:(0.189879998565) A[1]:(0.203615635633) A[2]:(0.289736896753) A[3]:(0.130597889423)\n",
      " state (13)  A[0]:(0.197835877538) A[1]:(0.211848780513) A[2]:(0.308279573917) A[3]:(0.133843004704)\n",
      " state (14)  A[0]:(0.205748006701) A[1]:(0.220037773252) A[2]:(0.326541513205) A[3]:(0.137077942491)\n",
      " state (15)  A[0]:(0.213613077998) A[1]:(0.228180646896) A[2]:(0.344504386187) A[3]:(0.140301585197)\n",
      "Episode 7000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               6559. Times reached goal: 24.               Steps done: 46650. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.858980106941.\n",
      " state (0)  A[0]:(0.0894249528646) A[1]:(0.0985437855124) A[2]:(0.0520811751485) A[3]:(0.0905639752746)\n",
      " state (1)  A[0]:(0.0974217951298) A[1]:(0.107131659985) A[2]:(0.0725013017654) A[3]:(0.0934372544289)\n",
      " state (2)  A[0]:(0.105431817472) A[1]:(0.115710243583) A[2]:(0.0929792672396) A[3]:(0.0963239967823)\n",
      " state (3)  A[0]:(0.113450556993) A[1]:(0.12427662313) A[2]:(0.113483570516) A[3]:(0.0992224663496)\n",
      " state (4)  A[0]:(0.121473453939) A[1]:(0.1328278929) A[2]:(0.133982107043) A[3]:(0.102130837739)\n",
      " state (5)  A[0]:(0.129495859146) A[1]:(0.141361102462) A[2]:(0.154442444444) A[3]:(0.105047225952)\n",
      " state (6)  A[0]:(0.137513071299) A[1]:(0.149873360991) A[2]:(0.174832090735) A[3]:(0.107969686389)\n",
      " state (7)  A[0]:(0.145520359278) A[1]:(0.158361807466) A[2]:(0.195118784904) A[3]:(0.110896244645)\n",
      " state (8)  A[0]:(0.153512910008) A[1]:(0.16682356596) A[2]:(0.215270668268) A[3]:(0.113824859262)\n",
      " state (9)  A[0]:(0.161486014724) A[1]:(0.175255820155) A[2]:(0.235256627202) A[3]:(0.11675349623)\n",
      " state (10)  A[0]:(0.16943487525) A[1]:(0.183655738831) A[2]:(0.25504642725) A[3]:(0.119680076838)\n",
      " state (11)  A[0]:(0.177354797721) A[1]:(0.192020580173) A[2]:(0.27461117506) A[3]:(0.122602552176)\n",
      " state (12)  A[0]:(0.185241177678) A[1]:(0.200347676873) A[2]:(0.293923139572) A[3]:(0.125518843532)\n",
      " state (13)  A[0]:(0.193089440465) A[1]:(0.208634331822) A[2]:(0.31295633316) A[3]:(0.128426939249)\n",
      " state (14)  A[0]:(0.200895175338) A[1]:(0.216877967119) A[2]:(0.331686586142) A[3]:(0.131324842572)\n",
      " state (15)  A[0]:(0.208654046059) A[1]:(0.22507609427) A[2]:(0.350091308355) A[3]:(0.134210541844)\n",
      "Episode 8000 finished after 0 timesteps with r=0.0. Running score: 0.04. Times trained:               6622. Times reached goal: 31.               Steps done: 53272. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.853310732689.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.0895830467343) A[1]:(0.101614668965) A[2]:(0.0496598929167) A[3]:(0.0883281305432)\n",
      " state (1)  A[0]:(0.0970922186971) A[1]:(0.109980262816) A[2]:(0.0693174079061) A[3]:(0.0910073444247)\n",
      " state (2)  A[0]:(0.104633189738) A[1]:(0.118351064622) A[2]:(0.0891004055738) A[3]:(0.0937178358436)\n",
      " state (3)  A[0]:(0.112200655043) A[1]:(0.126723304391) A[2]:(0.108975149691) A[3]:(0.0964566096663)\n",
      " state (4)  A[0]:(0.119789130986) A[1]:(0.135093182325) A[2]:(0.128906697035) A[3]:(0.0992204770446)\n",
      " state (5)  A[0]:(0.12739290297) A[1]:(0.143456757069) A[2]:(0.148859098554) A[3]:(0.102006115019)\n",
      " state (6)  A[0]:(0.135006144643) A[1]:(0.151810184121) A[2]:(0.168795734644) A[3]:(0.104809977114)\n",
      " state (7)  A[0]:(0.142622888088) A[1]:(0.160149440169) A[2]:(0.188679784536) A[3]:(0.107628449798)\n",
      " state (8)  A[0]:(0.150237083435) A[1]:(0.168470561504) A[2]:(0.208474367857) A[3]:(0.110457785428)\n",
      " state (9)  A[0]:(0.157842606306) A[1]:(0.176769584417) A[2]:(0.228143021464) A[3]:(0.113294146955)\n",
      " state (10)  A[0]:(0.165433302522) A[1]:(0.185042530298) A[2]:(0.24765008688) A[3]:(0.116133674979)\n",
      " state (11)  A[0]:(0.173003047705) A[1]:(0.193285509944) A[2]:(0.266960889101) A[3]:(0.118972420692)\n",
      " state (12)  A[0]:(0.180545821786) A[1]:(0.201494649053) A[2]:(0.28604221344) A[3]:(0.121806494892)\n",
      " state (13)  A[0]:(0.188055664301) A[1]:(0.209666147828) A[2]:(0.304862499237) A[3]:(0.124632000923)\n",
      " state (14)  A[0]:(0.195526629686) A[1]:(0.217796236277) A[2]:(0.323392122984) A[3]:(0.12744513154)\n",
      " state (15)  A[0]:(0.20295317471) A[1]:(0.225881412625) A[2]:(0.341603726149) A[3]:(0.130242094398)\n",
      "Episode 9000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               6554. Times reached goal: 26.               Steps done: 59826. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.847736421119.\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 8.7483,  9.8843,  4.3734,  8.7632]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0954,  0.1073,  0.0640,  0.0906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1032,  0.1156,  0.0843,  0.0936]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1347,  0.1491,  0.1679,  0.1060]], device='cuda:0')\n",
      "On state=6, selected action=2 , Random? True\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.0881719067693) A[1]:(0.099343188107) A[2]:(0.0434381961823) A[3]:(0.0880749821663)\n",
      " state (1)  A[0]:(0.095785215497) A[1]:(0.10756739229) A[2]:(0.0636399388313) A[3]:(0.0909075140953)\n",
      " state (2)  A[0]:(0.103467799723) A[1]:(0.115827776492) A[2]:(0.0841059610248) A[3]:(0.0938125774264)\n",
      " state (3)  A[0]:(0.111211635172) A[1]:(0.124118432403) A[2]:(0.104790791869) A[3]:(0.0967842638493)\n",
      " state (4)  A[0]:(0.119008153677) A[1]:(0.132433071733) A[2]:(0.125645756721) A[3]:(0.0998160839081)\n",
      " state (5)  A[0]:(0.126848220825) A[1]:(0.140765175223) A[2]:(0.146619424224) A[3]:(0.102900974452)\n",
      " state (6)  A[0]:(0.134722292423) A[1]:(0.14910800755) A[2]:(0.167658224702) A[3]:(0.106031432748)\n",
      " state (7)  A[0]:(0.142620459199) A[1]:(0.157454535365) A[2]:(0.188707232475) A[3]:(0.109199486673)\n",
      " state (8)  A[0]:(0.150532498956) A[1]:(0.165797650814) A[2]:(0.209710612893) A[3]:(0.112396843731)\n",
      " state (9)  A[0]:(0.158447951078) A[1]:(0.174130141735) A[2]:(0.230612561107) A[3]:(0.115614891052)\n",
      " state (10)  A[0]:(0.166356295347) A[1]:(0.18244472146) A[2]:(0.25135794282) A[3]:(0.118844844401)\n",
      " state (11)  A[0]:(0.174246937037) A[1]:(0.190734162927) A[2]:(0.271893024445) A[3]:(0.122077822685)\n",
      " state (12)  A[0]:(0.182109370828) A[1]:(0.198991253972) A[2]:(0.292166262865) A[3]:(0.125304877758)\n",
      " state (13)  A[0]:(0.189933225513) A[1]:(0.207208916545) A[2]:(0.312128782272) A[3]:(0.128517121077)\n",
      " state (14)  A[0]:(0.197708383203) A[1]:(0.215380206704) A[2]:(0.331735014915) A[3]:(0.131705865264)\n",
      " state (15)  A[0]:(0.205425158143) A[1]:(0.223498553038) A[2]:(0.350943416357) A[3]:(0.134862631559)\n",
      "Episode 10000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               6423. Times reached goal: 33.               Steps done: 66249. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.842308859361.\n",
      " state (0)  A[0]:(0.0942020714283) A[1]:(0.104679569602) A[2]:(0.0528668574989) A[3]:(0.0968302637339)\n",
      " state (1)  A[0]:(0.102337904274) A[1]:(0.113157361746) A[2]:(0.0726344808936) A[3]:(0.0992644950747)\n",
      " state (2)  A[0]:(0.110620543361) A[1]:(0.121739111841) A[2]:(0.0929229855537) A[3]:(0.101861856878)\n",
      " state (3)  A[0]:(0.119036883116) A[1]:(0.130414143205) A[2]:(0.113672032952) A[3]:(0.104611903429)\n",
      " state (4)  A[0]:(0.127572223544) A[1]:(0.139170601964) A[2]:(0.134813502431) A[3]:(0.107502393425)\n",
      " state (5)  A[0]:(0.136210337281) A[1]:(0.14799554646) A[2]:(0.156272143126) A[3]:(0.110519371927)\n",
      " state (6)  A[0]:(0.144933670759) A[1]:(0.156875103712) A[2]:(0.177966579795) A[3]:(0.113647311926)\n",
      " state (7)  A[0]:(0.153723463416) A[1]:(0.165794551373) A[2]:(0.19981059432) A[3]:(0.116869240999)\n",
      " state (8)  A[0]:(0.162560001016) A[1]:(0.174738541245) A[2]:(0.22171458602) A[3]:(0.120166927576)\n",
      " state (9)  A[0]:(0.171422764659) A[1]:(0.183691158891) A[2]:(0.24358715117) A[3]:(0.123521186411)\n",
      " state (10)  A[0]:(0.180290803313) A[1]:(0.192636176944) A[2]:(0.265336871147) A[3]:(0.126912012696)\n",
      " state (11)  A[0]:(0.189142927527) A[1]:(0.201557263732) A[2]:(0.286873877048) A[3]:(0.130318984389)\n",
      " state (12)  A[0]:(0.197957962751) A[1]:(0.210438162088) A[2]:(0.308111846447) A[3]:(0.133721455932)\n",
      " state (13)  A[0]:(0.20671518147) A[1]:(0.219262897968) A[2]:(0.328969091177) A[3]:(0.137098938227)\n",
      " state (14)  A[0]:(0.21539439261) A[1]:(0.228016018867) A[2]:(0.349370330572) A[3]:(0.14043135941)\n",
      " state (15)  A[0]:(0.223976328969) A[1]:(0.236682638526) A[2]:(0.36924764514) A[3]:(0.143699303269)\n",
      "Episode 11000 finished after 0 timesteps with r=0.0. Running score: 0.06. Times trained:               6459. Times reached goal: 34.               Steps done: 72708. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.836885918678.\n",
      " state (0)  A[0]:(0.107460789382) A[1]:(0.118239291012) A[2]:(0.0639231279492) A[3]:(0.110673896968)\n",
      " state (1)  A[0]:(0.115043975413) A[1]:(0.12662294507) A[2]:(0.0808488205075) A[3]:(0.111841671169)\n",
      " state (2)  A[0]:(0.122917316854) A[1]:(0.135263338685) A[2]:(0.0989790558815) A[3]:(0.113457076252)\n",
      " state (3)  A[0]:(0.131060510874) A[1]:(0.144138619304) A[2]:(0.1182372123) A[3]:(0.115504190326)\n",
      " state (4)  A[0]:(0.139448732138) A[1]:(0.153222829103) A[2]:(0.138522684574) A[3]:(0.117959827185)\n",
      " state (5)  A[0]:(0.148052737117) A[1]:(0.162486240268) A[2]:(0.159711405635) A[3]:(0.120793528855)\n",
      " state (6)  A[0]:(0.156839087605) A[1]:(0.171895325184) A[2]:(0.181657359004) A[3]:(0.123967632651)\n",
      " state (7)  A[0]:(0.165770426393) A[1]:(0.181413382292) A[2]:(0.204195037484) A[3]:(0.127437844872)\n",
      " state (8)  A[0]:(0.174806162715) A[1]:(0.191000849009) A[2]:(0.227143540978) A[3]:(0.131153866649)\n",
      " state (9)  A[0]:(0.183903068304) A[1]:(0.200616106391) A[2]:(0.250310987234) A[3]:(0.135060369968)\n",
      " state (10)  A[0]:(0.193016082048) A[1]:(0.210216119885) A[2]:(0.273500323296) A[3]:(0.139098152518)\n",
      " state (11)  A[0]:(0.20209929347) A[1]:(0.219757258892) A[2]:(0.296514958143) A[3]:(0.143205553293)\n",
      " state (12)  A[0]:(0.211106836796) A[1]:(0.229196295142) A[2]:(0.319164872169) A[3]:(0.147319793701)\n",
      " state (13)  A[0]:(0.219993829727) A[1]:(0.238491103053) A[2]:(0.341271907091) A[3]:(0.151378586888)\n",
      " state (14)  A[0]:(0.228717386723) A[1]:(0.247601613402) A[2]:(0.362674504519) A[3]:(0.155321553349)\n",
      " state (15)  A[0]:(0.237237408757) A[1]:(0.256490468979) A[2]:(0.38323161006) A[3]:(0.15909153223)\n",
      "Episode 12000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               6470. Times reached goal: 37.               Steps done: 79178. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.831488745467.\n",
      " state (0)  A[0]:(0.108622200787) A[1]:(0.125625297427) A[2]:(0.125467211008) A[3]:(0.118956357241)\n",
      " state (1)  A[0]:(0.118159525096) A[1]:(0.136325925589) A[2]:(0.0853941738605) A[3]:(0.109429359436)\n",
      " state (2)  A[0]:(0.128115192056) A[1]:(0.148125857115) A[2]:(0.0510899014771) A[3]:(0.101227909327)\n",
      " state (3)  A[0]:(0.138924852014) A[1]:(0.162753984332) A[2]:(0.0356181487441) A[3]:(0.0967185646296)\n",
      " state (4)  A[0]:(0.150740340352) A[1]:(0.180868864059) A[2]:(0.0448554866016) A[3]:(0.0972172543406)\n",
      " state (5)  A[0]:(0.163451552391) A[1]:(0.202122837305) A[2]:(0.0775620862842) A[3]:(0.102866970003)\n",
      " state (6)  A[0]:(0.176835402846) A[1]:(0.225715219975) A[2]:(0.128808677197) A[3]:(0.113039925694)\n",
      " state (7)  A[0]:(0.190666750073) A[1]:(0.250821709633) A[2]:(0.192772388458) A[3]:(0.126796841621)\n",
      " state (8)  A[0]:(0.204757392406) A[1]:(0.276760369539) A[2]:(0.264045804739) A[3]:(0.143182530999)\n",
      " state (9)  A[0]:(0.21895621717) A[1]:(0.303005158901) A[2]:(0.338070869446) A[3]:(0.161357820034)\n",
      " state (10)  A[0]:(0.233139127493) A[1]:(0.329152882099) A[2]:(0.411238312721) A[3]:(0.180633485317)\n",
      " state (11)  A[0]:(0.24720042944) A[1]:(0.354889512062) A[2]:(0.480877846479) A[3]:(0.200460463762)\n",
      " state (12)  A[0]:(0.261048316956) A[1]:(0.379967689514) A[2]:(0.545197665691) A[3]:(0.220407217741)\n",
      " state (13)  A[0]:(0.274603247643) A[1]:(0.404193788767) A[2]:(0.60317927599) A[3]:(0.240137666464)\n",
      " state (14)  A[0]:(0.287797778845) A[1]:(0.42742100358) A[2]:(0.65443778038) A[3]:(0.259393751621)\n",
      " state (15)  A[0]:(0.300577044487) A[1]:(0.449544787407) A[2]:(0.699061870575) A[3]:(0.277982026339)\n",
      "Episode 13000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               6305. Times reached goal: 29.               Steps done: 85483. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.826262701344.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.174612358212) A[1]:(0.186580345035) A[2]:(0.195264488459) A[3]:(0.184555619955)\n",
      " state (1)  A[0]:(0.186041280627) A[1]:(0.203898206353) A[2]:(0.16324698925) A[3]:(0.181176200509)\n",
      " state (2)  A[0]:(0.195140883327) A[1]:(0.217340022326) A[2]:(0.10784560442) A[3]:(0.17414213717)\n",
      " state (3)  A[0]:(0.20440505445) A[1]:(0.231121495366) A[2]:(0.0558212064207) A[3]:(0.166832655668)\n",
      " state (4)  A[0]:(0.216274023056) A[1]:(0.249433472753) A[2]:(0.0352969542146) A[3]:(0.163020163774)\n",
      " state (5)  A[0]:(0.231797680259) A[1]:(0.274264872074) A[2]:(0.060232359916) A[3]:(0.165307968855)\n",
      " state (6)  A[0]:(0.25061866641) A[1]:(0.30517911911) A[2]:(0.128676190972) A[3]:(0.174425572157)\n",
      " state (7)  A[0]:(0.271748840809) A[1]:(0.340451687574) A[2]:(0.229185089469) A[3]:(0.189611285925)\n",
      " state (8)  A[0]:(0.294188082218) A[1]:(0.37814244628) A[2]:(0.346678346395) A[3]:(0.209411203861)\n",
      " state (9)  A[0]:(0.317155390978) A[1]:(0.416610211134) A[2]:(0.466533333063) A[3]:(0.232298463583)\n",
      " state (10)  A[0]:(0.340100109577) A[1]:(0.454631239176) A[2]:(0.577458500862) A[3]:(0.256968617439)\n",
      " state (11)  A[0]:(0.362646877766) A[1]:(0.491355031729) A[2]:(0.672835290432) A[3]:(0.282411634922)\n",
      " state (12)  A[0]:(0.38454118371) A[1]:(0.526221871376) A[2]:(0.750422120094) A[3]:(0.307883888483)\n",
      " state (13)  A[0]:(0.405610442162) A[1]:(0.558890581131) A[2]:(0.811046242714) A[3]:(0.332854062319)\n",
      " state (14)  A[0]:(0.425740361214) A[1]:(0.589181661606) A[2]:(0.857126772404) A[3]:(0.356952667236)\n",
      " state (15)  A[0]:(0.444859772921) A[1]:(0.617035746574) A[2]:(0.891553223133) A[3]:(0.379932612181)\n",
      "Episode 14000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               5936. Times reached goal: 17.               Steps done: 91419. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.821372534323.\n",
      "q_values \n",
      "tensor([[ 0.1757,  0.1865,  0.1993,  0.1834]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1758,  0.1866,  0.1994,  0.1834]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1897,  0.2101,  0.1888,  0.1874]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1999,  0.2281,  0.1363,  0.1853]], device='cuda:0')\n",
      "On state=2, selected action=0 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1901,  0.2106,  0.1893,  0.1875]], device='cuda:0')\n",
      "On state=1, selected action=1 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.176311731339) A[1]:(0.187228739262) A[2]:(0.19993789494) A[3]:(0.183547005057)\n",
      " state (1)  A[0]:(0.190257608891) A[1]:(0.210723564029) A[2]:(0.189480707049) A[3]:(0.187548428774)\n",
      " state (2)  A[0]:(0.200364798307) A[1]:(0.228575706482) A[2]:(0.136741176248) A[3]:(0.18532705307)\n",
      " state (3)  A[0]:(0.209537103772) A[1]:(0.243783175945) A[2]:(0.0698682814837) A[3]:(0.178865075111)\n",
      " state (4)  A[0]:(0.221525326371) A[1]:(0.260609775782) A[2]:(0.0296840462834) A[3]:(0.171688750386)\n",
      " state (5)  A[0]:(0.238175362349) A[1]:(0.282298147678) A[2]:(0.0412661805749) A[3]:(0.167948678136)\n",
      " state (6)  A[0]:(0.258970052004) A[1]:(0.310022681952) A[2]:(0.105957888067) A[3]:(0.170984715223)\n",
      " state (7)  A[0]:(0.282439619303) A[1]:(0.343308538198) A[2]:(0.211500450969) A[3]:(0.182333379984)\n",
      " state (8)  A[0]:(0.307247728109) A[1]:(0.380780726671) A[2]:(0.340341448784) A[3]:(0.201710119843)\n",
      " state (9)  A[0]:(0.332475364208) A[1]:(0.42074906826) A[2]:(0.474175035954) A[3]:(0.22764891386)\n",
      " state (10)  A[0]:(0.357544630766) A[1]:(0.461586892605) A[2]:(0.597849607468) A[3]:(0.258226573467)\n",
      " state (11)  A[0]:(0.38209220767) A[1]:(0.501933395863) A[2]:(0.702170073986) A[3]:(0.291560739279)\n",
      " state (12)  A[0]:(0.405880063772) A[1]:(0.540761113167) A[2]:(0.784206151962) A[3]:(0.326051861048)\n",
      " state (13)  A[0]:(0.428745955229) A[1]:(0.5773665905) A[2]:(0.845503866673) A[3]:(0.360453993082)\n",
      " state (14)  A[0]:(0.450578123331) A[1]:(0.611323952675) A[2]:(0.88975071907) A[3]:(0.393859177828)\n",
      " state (15)  A[0]:(0.471303403378) A[1]:(0.642428278923) A[2]:(0.921028673649) A[3]:(0.425648659468)\n",
      "Episode 15000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               5691. Times reached goal: 23.               Steps done: 97110. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.816711379128.\n",
      " state (0)  A[0]:(0.166389361024) A[1]:(0.177498266101) A[2]:(0.185167536139) A[3]:(0.16771659255)\n",
      " state (1)  A[0]:(0.177584260702) A[1]:(0.200129404664) A[2]:(0.198857828975) A[3]:(0.180772274733)\n",
      " state (2)  A[0]:(0.186376541853) A[1]:(0.219350218773) A[2]:(0.152580171824) A[3]:(0.183591887355)\n",
      " state (3)  A[0]:(0.195176020265) A[1]:(0.236230030656) A[2]:(0.0749227255583) A[3]:(0.177763402462)\n",
      " state (4)  A[0]:(0.207806646824) A[1]:(0.252788543701) A[2]:(0.0221126973629) A[3]:(0.167406320572)\n",
      " state (5)  A[0]:(0.226067617536) A[1]:(0.271943032742) A[2]:(0.0305408295244) A[3]:(0.158372849226)\n",
      " state (6)  A[0]:(0.248975813389) A[1]:(0.296531826258) A[2]:(0.100218147039) A[3]:(0.156329169869)\n",
      " state (7)  A[0]:(0.274657338858) A[1]:(0.328083515167) A[2]:(0.214065045118) A[3]:(0.164873927832)\n",
      " state (8)  A[0]:(0.301633179188) A[1]:(0.366376370192) A[2]:(0.35121396184) A[3]:(0.184854283929)\n",
      " state (9)  A[0]:(0.328998923302) A[1]:(0.40984493494) A[2]:(0.491513699293) A[3]:(0.214910849929)\n",
      " state (10)  A[0]:(0.356231331825) A[1]:(0.456304132938) A[2]:(0.618932843208) A[3]:(0.252510726452)\n",
      " state (11)  A[0]:(0.383005410433) A[1]:(0.503562867641) A[2]:(0.724258184433) A[3]:(0.294839054346)\n",
      " state (12)  A[0]:(0.409092098475) A[1]:(0.549781739712) A[2]:(0.805196881294) A[3]:(0.339338153601)\n",
      " state (13)  A[0]:(0.434313207865) A[1]:(0.593608617783) A[2]:(0.864174187183) A[3]:(0.38394343853)\n",
      " state (14)  A[0]:(0.458526521921) A[1]:(0.634175777435) A[2]:(0.905642390251) A[3]:(0.42713278532)\n",
      " state (15)  A[0]:(0.481622338295) A[1]:(0.671028256416) A[2]:(0.934186637402) A[3]:(0.467880874872)\n",
      "Episode 16000 finished after 0 timesteps with r=0.0. Running score: 0.0. Times trained:               5747. Times reached goal: 15.               Steps done: 102857. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.812031200207.\n",
      " state (0)  A[0]:(0.197927027941) A[1]:(0.207390367985) A[2]:(0.219691887498) A[3]:(0.192749112844)\n",
      " state (1)  A[0]:(0.192127302289) A[1]:(0.136004239321) A[2]:(0.258672446012) A[3]:(0.22793674469)\n",
      " state (2)  A[0]:(0.19712460041) A[1]:(0.156980633736) A[2]:(0.183152109385) A[3]:(0.232111543417)\n",
      " state (3)  A[0]:(0.209231287241) A[1]:(0.221055433154) A[2]:(0.0589250437915) A[3]:(0.215857729316)\n",
      " state (4)  A[0]:(0.224560052156) A[1]:(0.270291507244) A[2]:(-0.00178788416088) A[3]:(0.193996980786)\n",
      " state (5)  A[0]:(0.241736769676) A[1]:(0.296520769596) A[2]:(0.023011257872) A[3]:(0.174978777766)\n",
      " state (6)  A[0]:(0.260449767113) A[1]:(0.314510762691) A[2]:(0.102418288589) A[3]:(0.16372384131)\n",
      " state (7)  A[0]:(0.280476808548) A[1]:(0.335882037878) A[2]:(0.211920231581) A[3]:(0.164249330759)\n",
      " state (8)  A[0]:(0.301508307457) A[1]:(0.365528106689) A[2]:(0.336695760489) A[3]:(0.178717538714)\n",
      " state (9)  A[0]:(0.323190897703) A[1]:(0.403791189194) A[2]:(0.464694648981) A[3]:(0.206790208817)\n",
      " state (10)  A[0]:(0.3451795578) A[1]:(0.448619216681) A[2]:(0.58492231369) A[3]:(0.246163338423)\n",
      " state (11)  A[0]:(0.367166519165) A[1]:(0.497103840113) A[2]:(0.689144968987) A[3]:(0.29354840517)\n",
      " state (12)  A[0]:(0.388893246651) A[1]:(0.54642701149) A[2]:(0.773410439491) A[3]:(0.345514476299)\n",
      " state (13)  A[0]:(0.410151869059) A[1]:(0.594327270985) A[2]:(0.837814509869) A[3]:(0.399013131857)\n",
      " state (14)  A[0]:(0.43078148365) A[1]:(0.639237642288) A[2]:(0.88501739502) A[3]:(0.451625496149)\n",
      " state (15)  A[0]:(0.450663030148) A[1]:(0.680238246918) A[2]:(0.918637633324) A[3]:(0.501619756222)\n",
      "Episode 17000 finished after 0 timesteps with r=0.0. Running score: 0.01. Times trained:               6923. Times reached goal: 28.               Steps done: 109780. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.806428922867.\n",
      " state (0)  A[0]:(0.209699675441) A[1]:(0.238296106458) A[2]:(0.218091800809) A[3]:(0.210121095181)\n",
      " state (1)  A[0]:(0.216806545854) A[1]:(0.0421121940017) A[2]:(0.244560882449) A[3]:(0.214023649693)\n",
      " state (2)  A[0]:(0.218116670847) A[1]:(0.125111266971) A[2]:(0.163750484586) A[3]:(0.229410916567)\n",
      " state (3)  A[0]:(0.218560680747) A[1]:(0.232745930552) A[2]:(0.0610187426209) A[3]:(0.23063954711)\n",
      " state (4)  A[0]:(0.220607832074) A[1]:(0.275887459517) A[2]:(0.0259288679808) A[3]:(0.212192505598)\n",
      " state (5)  A[0]:(0.225960791111) A[1]:(0.277508676052) A[2]:(0.0537427924573) A[3]:(0.184438511729)\n",
      " state (6)  A[0]:(0.235961794853) A[1]:(0.275061547756) A[2]:(0.121474571526) A[3]:(0.162159860134)\n",
      " state (7)  A[0]:(0.251163721085) A[1]:(0.28925973177) A[2]:(0.219162866473) A[3]:(0.155941575766)\n",
      " state (8)  A[0]:(0.271112114191) A[1]:(0.325604498386) A[2]:(0.33842048049) A[3]:(0.169214084744)\n",
      " state (9)  A[0]:(0.294676303864) A[1]:(0.380443274975) A[2]:(0.46655112505) A[3]:(0.199806496501)\n",
      " state (10)  A[0]:(0.320549428463) A[1]:(0.446496456861) A[2]:(0.589333415031) A[3]:(0.243027672172)\n",
      " state (11)  A[0]:(0.347576200962) A[1]:(0.516456723213) A[2]:(0.695926249027) A[3]:(0.293933659792)\n",
      " state (12)  A[0]:(0.374865442514) A[1]:(0.584660887718) A[2]:(0.781341135502) A[3]:(0.348359882832)\n",
      " state (13)  A[0]:(0.401779025793) A[1]:(0.647476732731) A[2]:(0.845742225647) A[3]:(0.403172940016)\n",
      " state (14)  A[0]:(0.427880465984) A[1]:(0.703047454357) A[2]:(0.892241477966) A[3]:(0.456189453602)\n",
      " state (15)  A[0]:(0.452881783247) A[1]:(0.750813364983) A[2]:(0.924872636795) A[3]:(0.506000161171)\n",
      "Episode 18000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained:               7479. Times reached goal: 34.               Steps done: 117259. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.800420138809.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.207345858216) A[1]:(0.231802731752) A[2]:(0.184103310108) A[3]:(0.208010151982)\n",
      " state (1)  A[0]:(0.202408269048) A[1]:(0.0152368303388) A[2]:(0.210252985358) A[3]:(0.175735160708)\n",
      " state (2)  A[0]:(0.204491361976) A[1]:(0.117405869067) A[2]:(0.165985360742) A[3]:(0.205773696303)\n",
      " state (3)  A[0]:(0.204622685909) A[1]:(0.219640597701) A[2]:(0.0796986967325) A[3]:(0.221006885171)\n",
      " state (4)  A[0]:(0.202881395817) A[1]:(0.256819665432) A[2]:(0.0298941750079) A[3]:(0.209459617734)\n",
      " state (5)  A[0]:(0.202706441283) A[1]:(0.246373921633) A[2]:(0.0365951023996) A[3]:(0.180946499109)\n",
      " state (6)  A[0]:(0.208564311266) A[1]:(0.232739269733) A[2]:(0.0904122889042) A[3]:(0.154737278819)\n",
      " state (7)  A[0]:(0.223021402955) A[1]:(0.246266916394) A[2]:(0.185129448771) A[3]:(0.145420446992)\n",
      " state (8)  A[0]:(0.245862036943) A[1]:(0.294919937849) A[2]:(0.311091274023) A[3]:(0.157499194145)\n",
      " state (9)  A[0]:(0.275074362755) A[1]:(0.37118768692) A[2]:(0.451021671295) A[3]:(0.187998458743)\n",
      " state (10)  A[0]:(0.308210581541) A[1]:(0.461733669043) A[2]:(0.585529088974) A[3]:(0.231184244156)\n",
      " state (11)  A[0]:(0.343170255423) A[1]:(0.55400967598) A[2]:(0.700515210629) A[3]:(0.281532764435)\n",
      " state (12)  A[0]:(0.3784096241) A[1]:(0.639237046242) A[2]:(0.790282666683) A[3]:(0.334754824638)\n",
      " state (13)  A[0]:(0.412881046534) A[1]:(0.712821960449) A[2]:(0.855889379978) A[3]:(0.387842237949)\n",
      " state (14)  A[0]:(0.445908069611) A[1]:(0.77344930172) A[2]:(0.901750445366) A[3]:(0.438817381859)\n",
      " state (15)  A[0]:(0.477076679468) A[1]:(0.821829676628) A[2]:(0.932946145535) A[3]:(0.486466884613)\n",
      "Episode 19000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               7381. Times reached goal: 51.               Steps done: 124640. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.79453398733.\n",
      "q_values \n",
      "tensor([[ 0.2102,  0.2346,  0.1822,  0.2113]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2126,  0.2603,  0.0210,  0.2129]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.210283741355) A[1]:(0.234840244055) A[2]:(0.182781800628) A[3]:(0.211339846253)\n",
      " state (1)  A[0]:(0.204767212272) A[1]:(0.0108862463385) A[2]:(0.199257478118) A[3]:(0.172935783863)\n",
      " state (2)  A[0]:(0.210707843304) A[1]:(0.121547013521) A[2]:(0.168256238103) A[3]:(0.205037966371)\n",
      " state (3)  A[0]:(0.213826730847) A[1]:(0.223261237144) A[2]:(0.0803062319756) A[3]:(0.221494138241)\n",
      " state (4)  A[0]:(0.212680801749) A[1]:(0.260383367538) A[2]:(0.0217729359865) A[3]:(0.21291783452)\n",
      " state (5)  A[0]:(0.210604116321) A[1]:(0.243734121323) A[2]:(0.0233521610498) A[3]:(0.186141371727)\n",
      " state (6)  A[0]:(0.213827610016) A[1]:(0.220413595438) A[2]:(0.0744910538197) A[3]:(0.1589114815)\n",
      " state (7)  A[0]:(0.22673201561) A[1]:(0.228383108974) A[2]:(0.167864367366) A[3]:(0.146944016218)\n",
      " state (8)  A[0]:(0.249955549836) A[1]:(0.280389964581) A[2]:(0.294727474451) A[3]:(0.156491056085)\n",
      " state (9)  A[0]:(0.281363934278) A[1]:(0.368306785822) A[2]:(0.437976449728) A[3]:(0.185449644923)\n",
      " state (10)  A[0]:(0.317911177874) A[1]:(0.474521905184) A[2]:(0.576977729797) A[3]:(0.228102639318)\n",
      " state (11)  A[0]:(0.356858044863) A[1]:(0.581563055515) A[2]:(0.696079671383) A[3]:(0.278596758842)\n",
      " state (12)  A[0]:(0.396167099476) A[1]:(0.677384197712) A[2]:(0.788734436035) A[3]:(0.332294046879)\n",
      " state (13)  A[0]:(0.434466570616) A[1]:(0.756440997124) A[2]:(0.855953097343) A[3]:(0.38592800498)\n",
      " state (14)  A[0]:(0.470892965794) A[1]:(0.818134427071) A[2]:(0.902502655983) A[3]:(0.437362343073)\n",
      " state (15)  A[0]:(0.504942834377) A[1]:(0.864559590816) A[2]:(0.933854818344) A[3]:(0.485303968191)\n",
      "Episode 20000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained:               7335. Times reached goal: 47.               Steps done: 131975. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.788727402217.\n",
      " state (0)  A[0]:(0.21519112587) A[1]:(0.238310307264) A[2]:(0.184708938003) A[3]:(0.216844975948)\n",
      " state (1)  A[0]:(0.207691818476) A[1]:(0.00142850633711) A[2]:(0.196976289153) A[3]:(0.178116425872)\n",
      " state (2)  A[0]:(0.212121501565) A[1]:(0.110568098724) A[2]:(0.176830708981) A[3]:(0.208531022072)\n",
      " state (3)  A[0]:(0.214032158256) A[1]:(0.209060639143) A[2]:(0.0832579880953) A[3]:(0.223266124725)\n",
      " state (4)  A[0]:(0.210995718837) A[1]:(0.245118811727) A[2]:(0.0145684750751) A[3]:(0.21472427249)\n",
      " state (5)  A[0]:(0.205708518624) A[1]:(0.223607301712) A[2]:(0.0132985021919) A[3]:(0.187874898314)\n",
      " state (6)  A[0]:(0.205372482538) A[1]:(0.193592682481) A[2]:(0.0647205784917) A[3]:(0.15900914371)\n",
      " state (7)  A[0]:(0.215547651052) A[1]:(0.199043244123) A[2]:(0.158247590065) A[3]:(0.14431360364)\n",
      " state (8)  A[0]:(0.237497389317) A[1]:(0.256755560637) A[2]:(0.285465151072) A[3]:(0.151284590364)\n",
      " state (9)  A[0]:(0.269050389528) A[1]:(0.358037620783) A[2]:(0.429975807667) A[3]:(0.178544864058)\n",
      " state (10)  A[0]:(0.306756675243) A[1]:(0.48091840744) A[2]:(0.570951700211) A[3]:(0.220433011651)\n",
      " state (11)  A[0]:(0.34742218256) A[1]:(0.602660894394) A[2]:(0.692066907883) A[3]:(0.270841509104)\n",
      " state (12)  A[0]:(0.3886526227) A[1]:(0.707944154739) A[2]:(0.786283254623) A[3]:(0.324837952852)\n",
      " state (13)  A[0]:(0.428840219975) A[1]:(0.79079502821) A[2]:(0.854500293732) A[3]:(0.378923267126)\n",
      " state (14)  A[0]:(0.466983705759) A[1]:(0.851997852325) A[2]:(0.901602208614) A[3]:(0.430806368589)\n",
      " state (15)  A[0]:(0.502509951591) A[1]:(0.895463526249) A[2]:(0.933227956295) A[3]:(0.479105293751)\n",
      "Episode 21000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               7428. Times reached goal: 56.               Steps done: 139403. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.782890440388.\n",
      " state (0)  A[0]:(0.212032914162) A[1]:(0.23054201901) A[2]:(0.186460882425) A[3]:(0.210847243667)\n",
      " state (1)  A[0]:(0.205006107688) A[1]:(0.0078528476879) A[2]:(0.198737457395) A[3]:(0.177031755447)\n",
      " state (2)  A[0]:(0.206874996424) A[1]:(0.117234438658) A[2]:(0.193848028779) A[3]:(0.207257732749)\n",
      " state (3)  A[0]:(0.20777694881) A[1]:(0.21665789187) A[2]:(0.0976503267884) A[3]:(0.222559049726)\n",
      " state (4)  A[0]:(0.204772680998) A[1]:(0.257093578577) A[2]:(0.0186656937003) A[3]:(0.216098070145)\n",
      " state (5)  A[0]:(0.199119135737) A[1]:(0.23598074913) A[2]:(0.016119170934) A[3]:(0.190898448229)\n",
      " state (6)  A[0]:(0.197809040546) A[1]:(0.201629459858) A[2]:(0.0699043571949) A[3]:(0.161450833082)\n",
      " state (7)  A[0]:(0.206617221236) A[1]:(0.202912047505) A[2]:(0.164049163461) A[3]:(0.144257187843)\n",
      " state (8)  A[0]:(0.227163746953) A[1]:(0.261904031038) A[2]:(0.290164768696) A[3]:(0.148438498378)\n",
      " state (9)  A[0]:(0.257547050714) A[1]:(0.370986223221) A[2]:(0.433272629976) A[3]:(0.173844531178)\n",
      " state (10)  A[0]:(0.294386416674) A[1]:(0.504612565041) A[2]:(0.573245048523) A[3]:(0.215117320418)\n",
      " state (11)  A[0]:(0.334433704615) A[1]:(0.634993076324) A[2]:(0.693735539913) A[3]:(0.265896290541)\n",
      " state (12)  A[0]:(0.375211894512) A[1]:(0.743937373161) A[2]:(0.787502646446) A[3]:(0.320845752954)\n",
      " state (13)  A[0]:(0.415045022964) A[1]:(0.825710058212) A[2]:(0.855336010456) A[3]:(0.37611156702)\n",
      " state (14)  A[0]:(0.452886074781) A[1]:(0.882938802242) A[2]:(0.902104198933) A[3]:(0.429153174162)\n",
      " state (15)  A[0]:(0.488134771585) A[1]:(0.92137169838) A[2]:(0.933459281921) A[3]:(0.478441298008)\n",
      "Episode 22000 finished after 0 timesteps with r=0.0. Running score: 0.07. Times trained:               7568. Times reached goal: 56.               Steps done: 146971. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.776987888962.\n",
      " state (0)  A[0]:(0.214018672705) A[1]:(0.236229419708) A[2]:(0.187187388539) A[3]:(0.214161664248)\n",
      " state (1)  A[0]:(0.209086865187) A[1]:(0.00744332792237) A[2]:(0.195080637932) A[3]:(0.17712071538)\n",
      " state (2)  A[0]:(0.212707027793) A[1]:(0.122500002384) A[2]:(0.201721444726) A[3]:(0.206795692444)\n",
      " state (3)  A[0]:(0.218250378966) A[1]:(0.226271793246) A[2]:(0.103912003338) A[3]:(0.221928209066)\n",
      " state (4)  A[0]:(0.219942435622) A[1]:(0.269952178001) A[2]:(0.0250934418291) A[3]:(0.217173039913)\n",
      " state (5)  A[0]:(0.217320129275) A[1]:(0.248990416527) A[2]:(0.0310945380479) A[3]:(0.194729506969)\n",
      " state (6)  A[0]:(0.218103200197) A[1]:(0.212096109986) A[2]:(0.090959481895) A[3]:(0.166842877865)\n",
      " state (7)  A[0]:(0.228287518024) A[1]:(0.210972160101) A[2]:(0.185446843505) A[3]:(0.149314016104)\n",
      " state (8)  A[0]:(0.249543637037) A[1]:(0.272036463022) A[2]:(0.308449447155) A[3]:(0.152293488383)\n",
      " state (9)  A[0]:(0.280124098063) A[1]:(0.388812243938) A[2]:(0.447550088167) A[3]:(0.176786154509)\n",
      " state (10)  A[0]:(0.316784530878) A[1]:(0.532108426094) A[2]:(0.583887219429) A[3]:(0.217875525355)\n",
      " state (11)  A[0]:(0.356358230114) A[1]:(0.668989539146) A[2]:(0.701423168182) A[3]:(0.269095182419)\n",
      " state (12)  A[0]:(0.3964227736) A[1]:(0.778921127319) A[2]:(0.792880833149) A[3]:(0.324817091227)\n",
      " state (13)  A[0]:(0.435352057219) A[1]:(0.857287406921) A[2]:(0.858962535858) A[3]:(0.380914419889)\n",
      " state (14)  A[0]:(0.472148835659) A[1]:(0.909097015858) A[2]:(0.904449701309) A[3]:(0.434660762548)\n",
      " state (15)  A[0]:(0.50626128912) A[1]:(0.941950201988) A[2]:(0.93490344286) A[3]:(0.484429687262)\n",
      "Episode 23000 finished after 0 timesteps with r=0.0. Running score: 0.01. Times trained:               7675. Times reached goal: 56.               Steps done: 154646. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.771047332959.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.212674111128) A[1]:(0.245015308261) A[2]:(0.191379517317) A[3]:(0.213176727295)\n",
      " state (1)  A[0]:(0.208905011415) A[1]:(0.00895453896374) A[2]:(0.19505096972) A[3]:(0.17717179656)\n",
      " state (2)  A[0]:(0.212137430906) A[1]:(0.130631238222) A[2]:(0.213142737746) A[3]:(0.207280531526)\n",
      " state (3)  A[0]:(0.219801992178) A[1]:(0.236986473203) A[2]:(0.111816219985) A[3]:(0.222868070006)\n",
      " state (4)  A[0]:(0.223434120417) A[1]:(0.280177295208) A[2]:(0.0294676031917) A[3]:(0.219504147768)\n",
      " state (5)  A[0]:(0.221306309104) A[1]:(0.254992097616) A[2]:(0.0414035879076) A[3]:(0.198946729302)\n",
      " state (6)  A[0]:(0.222797051072) A[1]:(0.212651222944) A[2]:(0.105586007237) A[3]:(0.172223299742)\n",
      " state (7)  A[0]:(0.234193310142) A[1]:(0.209440395236) A[2]:(0.200105324388) A[3]:(0.155096173286)\n",
      " state (8)  A[0]:(0.25676292181) A[1]:(0.275919944048) A[2]:(0.320963859558) A[3]:(0.158502906561)\n",
      " state (9)  A[0]:(0.288442820311) A[1]:(0.404763042927) A[2]:(0.457442998886) A[3]:(0.18392829597)\n",
      " state (10)  A[0]:(0.325857102871) A[1]:(0.560603737831) A[2]:(0.59128344059) A[3]:(0.226378500462)\n",
      " state (11)  A[0]:(0.365827023983) A[1]:(0.704119503498) A[2]:(0.70660597086) A[3]:(0.279077202082)\n",
      " state (12)  A[0]:(0.40597474575) A[1]:(0.813343405724) A[2]:(0.796228706837) A[3]:(0.336113780737)\n",
      " state (13)  A[0]:(0.444734811783) A[1]:(0.886436879635) A[2]:(0.860908210278) A[3]:(0.393187642097)\n",
      " state (14)  A[0]:(0.481169402599) A[1]:(0.931699752808) A[2]:(0.905406355858) A[3]:(0.447502046824)\n",
      " state (15)  A[0]:(0.514778912067) A[1]:(0.958648264408) A[2]:(0.935210227966) A[3]:(0.497436285019)\n",
      "Episode 24000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained:               7680. Times reached goal: 25.               Steps done: 162326. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.765148370452.\n",
      "q_values \n",
      "tensor([[ 0.2102,  0.2362,  0.1901,  0.2128]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2202,  0.2708,  0.0159,  0.2176]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2535,  0.2613,  0.3097,  0.1485]], device='cuda:0')\n",
      "On state=8, selected action=1 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.210267052054) A[1]:(0.236288130283) A[2]:(0.190290868282) A[3]:(0.212947815657)\n",
      " state (1)  A[0]:(0.207761496305) A[1]:(0.00395617820323) A[2]:(0.197171986103) A[3]:(0.180060192943)\n",
      " state (2)  A[0]:(0.208785131574) A[1]:(0.129612192512) A[2]:(0.222429871559) A[3]:(0.208880707622)\n",
      " state (3)  A[0]:(0.216870993376) A[1]:(0.234175816178) A[2]:(0.110213816166) A[3]:(0.222930416465)\n",
      " state (4)  A[0]:(0.220397546887) A[1]:(0.271737992764) A[2]:(0.0164494570345) A[3]:(0.217948675156)\n",
      " state (5)  A[0]:(0.216313824058) A[1]:(0.237365245819) A[2]:(0.0295394379646) A[3]:(0.195370495319)\n",
      " state (6)  A[0]:(0.216869264841) A[1]:(0.187072247267) A[2]:(0.0950796157122) A[3]:(0.166007518768)\n",
      " state (7)  A[0]:(0.229097515345) A[1]:(0.183922156692) A[2]:(0.189232259989) A[3]:(0.146519765258)\n",
      " state (8)  A[0]:(0.253729760647) A[1]:(0.262216776609) A[2]:(0.310194909573) A[3]:(0.148683100939)\n",
      " state (9)  A[0]:(0.288006186485) A[1]:(0.411006808281) A[2]:(0.448005914688) A[3]:(0.173926889896)\n",
      " state (10)  A[0]:(0.328045159578) A[1]:(0.585082530975) A[2]:(0.583798885345) A[3]:(0.216731488705)\n",
      " state (11)  A[0]:(0.370391130447) A[1]:(0.73698836565) A[2]:(0.700937867165) A[3]:(0.269864439964)\n",
      " state (12)  A[0]:(0.412546277046) A[1]:(0.844799041748) A[2]:(0.791907012463) A[3]:(0.327171474695)\n",
      " state (13)  A[0]:(0.4529106915) A[1]:(0.911643922329) A[2]:(0.85748231411) A[3]:(0.38426426053)\n",
      " state (14)  A[0]:(0.490555763245) A[1]:(0.950050413609) A[2]:(0.902567744255) A[3]:(0.438342660666)\n",
      " state (15)  A[0]:(0.525014996529) A[1]:(0.971389591694) A[2]:(0.932778358459) A[3]:(0.48782351613)\n",
      "Episode 25000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               7430. Times reached goal: 36.               Steps done: 169756. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.759484385819.\n",
      " state (0)  A[0]:(0.231419712305) A[1]:(0.256875365973) A[2]:(0.203272640705) A[3]:(0.234254166484)\n",
      " state (1)  A[0]:(0.226907074451) A[1]:(0.0020310850814) A[2]:(0.215724110603) A[3]:(0.199031636119)\n",
      " state (2)  A[0]:(0.227239727974) A[1]:(0.139425307512) A[2]:(0.246053874493) A[3]:(0.22985060513)\n",
      " state (3)  A[0]:(0.237406879663) A[1]:(0.251278936863) A[2]:(0.119547605515) A[3]:(0.244133859873)\n",
      " state (4)  A[0]:(0.241376206279) A[1]:(0.28941270709) A[2]:(0.0158354062587) A[3]:(0.239564731717)\n",
      " state (5)  A[0]:(0.234793558717) A[1]:(0.247893795371) A[2]:(0.0368837974966) A[3]:(0.217618688941)\n",
      " state (6)  A[0]:(0.23355267942) A[1]:(0.186341151595) A[2]:(0.109617762268) A[3]:(0.187253475189)\n",
      " state (7)  A[0]:(0.245351612568) A[1]:(0.17542013526) A[2]:(0.206522092223) A[3]:(0.165716692805)\n",
      " state (8)  A[0]:(0.270278483629) A[1]:(0.256778240204) A[2]:(0.327879875898) A[3]:(0.166515558958)\n",
      " state (9)  A[0]:(0.304980009794) A[1]:(0.41880774498) A[2]:(0.464895933867) A[3]:(0.191813439131)\n",
      " state (10)  A[0]:(0.345261842012) A[1]:(0.606860041618) A[2]:(0.598815441132) A[3]:(0.235703468323)\n",
      " state (11)  A[0]:(0.3875631392) A[1]:(0.764488875866) A[2]:(0.713222026825) A[3]:(0.290244311094)\n",
      " state (12)  A[0]:(0.429388612509) A[1]:(0.869657099247) A[2]:(0.801175057888) A[3]:(0.34873226285)\n",
      " state (13)  A[0]:(0.469184577465) A[1]:(0.930400848389) A[2]:(0.864002883434) A[3]:(0.406488776207)\n",
      " state (14)  A[0]:(0.506082296371) A[1]:(0.962924361229) A[2]:(0.906891703606) A[3]:(0.460632830858)\n",
      " state (15)  A[0]:(0.539675593376) A[1]:(0.979853332043) A[2]:(0.935492813587) A[3]:(0.509634017944)\n",
      "Episode 26000 finished after 0 timesteps with r=0.0. Running score: 0.08. Times trained:               7517. Times reached goal: 63.               Steps done: 177273. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.753796745469.\n",
      " state (0)  A[0]:(0.2497587502) A[1]:(0.280429840088) A[2]:(0.219155609608) A[3]:(0.249503090978)\n",
      " state (1)  A[0]:(0.244656667113) A[1]:(-0.0033097891137) A[2]:(0.239624202251) A[3]:(0.215555876493)\n",
      " state (2)  A[0]:(0.242658257484) A[1]:(0.150120273232) A[2]:(0.281286984682) A[3]:(0.250470787287)\n",
      " state (3)  A[0]:(0.253764122725) A[1]:(0.2729344666) A[2]:(0.144827991724) A[3]:(0.265795975924)\n",
      " state (4)  A[0]:(0.259666502476) A[1]:(0.317623585463) A[2]:(0.0201932154596) A[3]:(0.2617623806)\n",
      " state (5)  A[0]:(0.252177476883) A[1]:(0.273472249508) A[2]:(0.045748077333) A[3]:(0.24099612236)\n",
      " state (6)  A[0]:(0.250575691462) A[1]:(0.202337250113) A[2]:(0.130451530218) A[3]:(0.209940642118)\n",
      " state (7)  A[0]:(0.263168424368) A[1]:(0.184771567583) A[2]:(0.236853286624) A[3]:(0.186715573072)\n",
      " state (8)  A[0]:(0.288954049349) A[1]:(0.271814972162) A[2]:(0.363893955946) A[3]:(0.187214642763)\n",
      " state (9)  A[0]:(0.323822319508) A[1]:(0.449261635542) A[2]:(0.502265274525) A[3]:(0.214401438832)\n",
      " state (10)  A[0]:(0.363440811634) A[1]:(0.648747563362) A[2]:(0.633212685585) A[3]:(0.261558651924)\n",
      " state (11)  A[0]:(0.404422163963) A[1]:(0.804966568947) A[2]:(0.741727471352) A[3]:(0.319566130638)\n",
      " state (12)  A[0]:(0.444505780935) A[1]:(0.900389134884) A[2]:(0.822918355465) A[3]:(0.38086861372)\n",
      " state (13)  A[0]:(0.482337802649) A[1]:(0.950671315193) A[2]:(0.87963283062) A[3]:(0.440380305052)\n",
      " state (14)  A[0]:(0.517197966576) A[1]:(0.975399315357) A[2]:(0.917695760727) A[3]:(0.495158940554)\n",
      " state (15)  A[0]:(0.548786520958) A[1]:(0.987349271774) A[2]:(0.942782878876) A[3]:(0.543823242188)\n",
      "Episode 27000 finished after 0 timesteps with r=0.0. Running score: 0.11. Times trained:               7906. Times reached goal: 78.               Steps done: 185179. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.74786072441.\n",
      " state (0)  A[0]:(0.276319563389) A[1]:(0.310802608728) A[2]:(0.239422276616) A[3]:(0.27657070756)\n",
      " state (1)  A[0]:(0.272010296583) A[1]:(-0.00105071032885) A[2]:(0.264729291201) A[3]:(0.239552274346)\n",
      " state (2)  A[0]:(0.267600595951) A[1]:(0.166750848293) A[2]:(0.310151904821) A[3]:(0.27826783061)\n",
      " state (3)  A[0]:(0.280193597078) A[1]:(0.298126906157) A[2]:(0.159164831042) A[3]:(0.293110460043)\n",
      " state (4)  A[0]:(0.28761446476) A[1]:(0.343685865402) A[2]:(0.0189401805401) A[3]:(0.287640273571)\n",
      " state (5)  A[0]:(0.278833150864) A[1]:(0.290177643299) A[2]:(0.0531984344125) A[3]:(0.266128569841)\n",
      " state (6)  A[0]:(0.277333587408) A[1]:(0.20284640789) A[2]:(0.150002464652) A[3]:(0.232021272182)\n",
      " state (7)  A[0]:(0.291271269321) A[1]:(0.174645125866) A[2]:(0.264650344849) A[3]:(0.205058515072)\n",
      " state (8)  A[0]:(0.317825078964) A[1]:(0.267635256052) A[2]:(0.395493835211) A[3]:(0.204171895981)\n",
      " state (9)  A[0]:(0.351972788572) A[1]:(0.463667690754) A[2]:(0.533070683479) A[3]:(0.232916638255)\n",
      " state (10)  A[0]:(0.389488428831) A[1]:(0.677439749241) A[2]:(0.659472584724) A[3]:(0.283260554075)\n",
      " state (11)  A[0]:(0.427479594946) A[1]:(0.833894968033) A[2]:(0.761732757092) A[3]:(0.344616949558)\n",
      " state (12)  A[0]:(0.464166432619) A[1]:(0.921540737152) A[2]:(0.836897790432) A[3]:(0.408532202244)\n",
      " state (13)  A[0]:(0.498543828726) A[1]:(0.963843822479) A[2]:(0.888801753521) A[3]:(0.469579339027)\n",
      " state (14)  A[0]:(0.530113458633) A[1]:(0.98305529356) A[2]:(0.923426508904) A[3]:(0.52483779192)\n",
      " state (15)  A[0]:(0.558702588081) A[1]:(0.991724312305) A[2]:(0.946213006973) A[3]:(0.573130607605)\n",
      "Episode 28000 finished after 0 timesteps with r=0.0. Running score: 0.04. Times trained:               7380. Times reached goal: 58.               Steps done: 192559. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.742361828149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.310530811548) A[1]:(0.33349570632) A[2]:(0.266886949539) A[3]:(0.310486376286)\n",
      " state (1)  A[0]:(0.303660273552) A[1]:(-0.0106847686693) A[2]:(0.29059278965) A[3]:(0.264854967594)\n",
      " state (2)  A[0]:(0.294640421867) A[1]:(0.172277674079) A[2]:(0.341195613146) A[3]:(0.307176619768)\n",
      " state (3)  A[0]:(0.305604964495) A[1]:(0.316437751055) A[2]:(0.178639143705) A[3]:(0.322537153959)\n",
      " state (4)  A[0]:(0.313603162766) A[1]:(0.367290019989) A[2]:(0.0128240855411) A[3]:(0.315387487411)\n",
      " state (5)  A[0]:(0.303016394377) A[1]:(0.303815066814) A[2]:(0.0512809380889) A[3]:(0.291682779789)\n",
      " state (6)  A[0]:(0.301178187132) A[1]:(0.196068704128) A[2]:(0.162554547191) A[3]:(0.252232432365)\n",
      " state (7)  A[0]:(0.316002011299) A[1]:(0.15475808084) A[2]:(0.289361596107) A[3]:(0.219199106097)\n",
      " state (8)  A[0]:(0.342445999384) A[1]:(0.256191283464) A[2]:(0.426728338003) A[3]:(0.215277731419)\n",
      " state (9)  A[0]:(0.374580204487) A[1]:(0.475910514593) A[2]:(0.564399063587) A[3]:(0.244465813041)\n",
      " state (10)  A[0]:(0.408559262753) A[1]:(0.70592713356) A[2]:(0.686023592949) A[3]:(0.296981275082)\n",
      " state (11)  A[0]:(0.442220300436) A[1]:(0.861061930656) A[2]:(0.781560897827) A[3]:(0.36058473587)\n",
      " state (12)  A[0]:(0.474382787943) A[1]:(0.939755856991) A[2]:(0.850398659706) A[3]:(0.425965189934)\n",
      " state (13)  A[0]:(0.50442814827) A[1]:(0.974256575108) A[2]:(0.897386431694) A[3]:(0.487470835447)\n",
      " state (14)  A[0]:(0.532076358795) A[1]:(0.98866546154) A[2]:(0.928582251072) A[3]:(0.542302846909)\n",
      " state (15)  A[0]:(0.557257413864) A[1]:(0.994730949402) A[2]:(0.949122846127) A[3]:(0.589542031288)\n",
      "Episode 29000 finished after 0 timesteps with r=1.0. Running score: 0.12. Times trained:               7438. Times reached goal: 72.               Steps done: 199997. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.736860625207.\n",
      "q_values \n",
      "tensor([[ 0.3312,  0.3661,  0.2929,  0.3310]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3261, -0.0056,  0.3170,  0.2855]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3154,  0.1885,  0.3716,  0.3314]], device='cuda:0')\n",
      "On state=2, selected action=2 , Random? False\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3292,  0.3466,  0.2023,  0.3479]], device='cuda:0')\n",
      "On state=3, selected action=3 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3289,  0.3456,  0.2022,  0.3479]], device='cuda:0')\n",
      "On state=3, selected action=3 , Random? False\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3287,  0.3453,  0.2023,  0.3480]], device='cuda:0')\n",
      "On state=3, selected action=3 , Random? False\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3285,  0.3456,  0.2026,  0.3483]], device='cuda:0')\n",
      "On state=3, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3285,  0.3461,  0.2030,  0.3485]], device='cuda:0')\n",
      "On state=3, selected action=3 , Random? False\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3286,  0.3468,  0.2034,  0.3487]], device='cuda:0')\n",
      "On state=3, selected action=3 , Random? False\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3286,  0.3478,  0.2039,  0.3489]], device='cuda:0')\n",
      "On state=3, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3288,  0.3491,  0.2045,  0.3492]], device='cuda:0')\n",
      "On state=3, selected action=0 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3150,  0.1909,  0.3731,  0.3323]], device='cuda:0')\n",
      "On state=2, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3290,  0.3515,  0.2053,  0.3496]], device='cuda:0')\n",
      "On state=3, selected action=1 , Random? True\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.330940514803) A[1]:(0.366995871067) A[2]:(0.292712062597) A[3]:(0.330859810114)\n",
      " state (1)  A[0]:(0.325724571943) A[1]:(-0.00323100760579) A[2]:(0.318167179823) A[3]:(0.286220908165)\n",
      " state (2)  A[0]:(0.315264284611) A[1]:(0.192791089416) A[2]:(0.373676985502) A[3]:(0.33262565732)\n",
      " state (3)  A[0]:(0.329256623983) A[1]:(0.352901548147) A[2]:(0.205918952823) A[3]:(0.349830210209)\n",
      " state (4)  A[0]:(0.342647373676) A[1]:(0.409801244736) A[2]:(0.0203003417701) A[3]:(0.342482686043)\n",
      " state (5)  A[0]:(0.333681970835) A[1]:(0.332289904356) A[2]:(0.0662361010909) A[3]:(0.318846911192)\n",
      " state (6)  A[0]:(0.334276586771) A[1]:(0.19662335515) A[2]:(0.194178268313) A[3]:(0.276992857456)\n",
      " state (7)  A[0]:(0.351873934269) A[1]:(0.137657985091) A[2]:(0.333864510059) A[3]:(0.241656601429)\n",
      " state (8)  A[0]:(0.378905326128) A[1]:(0.248827785254) A[2]:(0.476122289896) A[3]:(0.238826811314)\n",
      " state (9)  A[0]:(0.408882826567) A[1]:(0.494463562965) A[2]:(0.610269844532) A[3]:(0.271869510412)\n",
      " state (10)  A[0]:(0.438845843077) A[1]:(0.737945795059) A[2]:(0.723071277142) A[3]:(0.328653544188)\n",
      " state (11)  A[0]:(0.467656612396) A[1]:(0.887218952179) A[2]:(0.80860465765) A[3]:(0.395358026028)\n",
      " state (12)  A[0]:(0.494844704866) A[1]:(0.955235540867) A[2]:(0.868868350983) A[3]:(0.46217700839)\n",
      " state (13)  A[0]:(0.520202279091) A[1]:(0.982262194157) A[2]:(0.909506678581) A[3]:(0.523609876633)\n",
      " state (14)  A[0]:(0.543659508228) A[1]:(0.992653131485) A[2]:(0.936364352703) A[3]:(0.577265024185)\n",
      " state (15)  A[0]:(0.565231919289) A[1]:(0.996743261814) A[2]:(0.954064488411) A[3]:(0.622661352158)\n",
      "Episode 30000 finished after 0 timesteps with r=0.0. Running score: 0.08. Times trained:               7141. Times reached goal: 59.               Steps done: 207138. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.731617446532.\n",
      " state (0)  A[0]:(0.34512680769) A[1]:(0.395099461079) A[2]:(0.311593711376) A[3]:(0.345428079367)\n",
      " state (1)  A[0]:(0.34693852067) A[1]:(-0.00353860878386) A[2]:(0.328788250685) A[3]:(0.299906164408)\n",
      " state (2)  A[0]:(0.332099825144) A[1]:(0.173244401813) A[2]:(0.391572624445) A[3]:(0.34546855092)\n",
      " state (3)  A[0]:(0.344723641872) A[1]:(0.35583999753) A[2]:(0.23453900218) A[3]:(0.366807848215)\n",
      " state (4)  A[0]:(0.365523487329) A[1]:(0.431038796902) A[2]:(0.0124879991636) A[3]:(0.359228610992)\n",
      " state (5)  A[0]:(0.358807533979) A[1]:(0.343704730272) A[2]:(0.0535043142736) A[3]:(0.335581570864)\n",
      " state (6)  A[0]:(0.360867470503) A[1]:(0.177755013108) A[2]:(0.200020357966) A[3]:(0.29165610671)\n",
      " state (7)  A[0]:(0.380103826523) A[1]:(0.101754054427) A[2]:(0.356761485338) A[3]:(0.254396438599)\n",
      " state (8)  A[0]:(0.406303226948) A[1]:(0.230910524726) A[2]:(0.506500482559) A[3]:(0.253166288137)\n",
      " state (9)  A[0]:(0.432609051466) A[1]:(0.513129353523) A[2]:(0.638994216919) A[3]:(0.290316075087)\n",
      " state (10)  A[0]:(0.457217127085) A[1]:(0.772303402424) A[2]:(0.745341658592) A[3]:(0.351142853498)\n",
      " state (11)  A[0]:(0.480116426945) A[1]:(0.912772715092) A[2]:(0.823712348938) A[3]:(0.420372396708)\n",
      " state (12)  A[0]:(0.50154709816) A[1]:(0.968848705292) A[2]:(0.878139913082) A[3]:(0.487932622433)\n",
      " state (13)  A[0]:(0.521685123444) A[1]:(0.988680839539) A[2]:(0.914684712887) A[3]:(0.548652172089)\n",
      " state (14)  A[0]:(0.540638685226) A[1]:(0.995616197586) A[2]:(0.938902199268) A[3]:(0.600643873215)\n",
      " state (15)  A[0]:(0.558473050594) A[1]:(0.998150765896) A[2]:(0.954983830452) A[3]:(0.643895447254)\n",
      "Episode 31000 finished after 0 timesteps with r=0.0. Running score: 0.11. Times trained:               7602. Times reached goal: 70.               Steps done: 214740. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.72607677747.\n",
      " state (0)  A[0]:(0.383033305407) A[1]:(0.43342474103) A[2]:(0.343084454536) A[3]:(0.383885025978)\n",
      " state (1)  A[0]:(0.38351598382) A[1]:(0.0149640003219) A[2]:(0.343841403723) A[3]:(0.327643573284)\n",
      " state (2)  A[0]:(0.366405338049) A[1]:(0.165776669979) A[2]:(0.40631377697) A[3]:(0.368158608675)\n",
      " state (3)  A[0]:(0.377328187227) A[1]:(0.379689753056) A[2]:(0.255373716354) A[3]:(0.393848657608)\n",
      " state (4)  A[0]:(0.402512788773) A[1]:(0.479272216558) A[2]:(0.00392750883475) A[3]:(0.387337237597)\n",
      " state (5)  A[0]:(0.395753502846) A[1]:(0.384204119444) A[2]:(0.0478564351797) A[3]:(0.362705975771)\n",
      " state (6)  A[0]:(0.398577153683) A[1]:(0.181935861707) A[2]:(0.214565113187) A[3]:(0.3130838871)\n",
      " state (7)  A[0]:(0.419128030539) A[1]:(0.0790375396609) A[2]:(0.38613191247) A[3]:(0.269155621529)\n",
      " state (8)  A[0]:(0.443698257208) A[1]:(0.221157357097) A[2]:(0.539872765541) A[3]:(0.265563100576)\n",
      " state (9)  A[0]:(0.465215742588) A[1]:(0.537442088127) A[2]:(0.668155908585) A[3]:(0.304047524929)\n",
      " state (10)  A[0]:(0.483336597681) A[1]:(0.806376338005) A[2]:(0.767079353333) A[3]:(0.367040127516)\n",
      " state (11)  A[0]:(0.499296426773) A[1]:(0.934459209442) A[2]:(0.838354051113) A[3]:(0.43763205409)\n",
      " state (12)  A[0]:(0.514077544212) A[1]:(0.97907269001) A[2]:(0.88735973835) A[3]:(0.505396127701)\n",
      " state (13)  A[0]:(0.528253376484) A[1]:(0.993073701859) A[2]:(0.920201361179) A[3]:(0.565383315086)\n",
      " state (14)  A[0]:(0.542114436626) A[1]:(0.997510254383) A[2]:(0.942035496235) A[3]:(0.616070568562)\n",
      " state (15)  A[0]:(0.555769503117) A[1]:(0.99900841713) A[2]:(0.956629812717) A[3]:(0.65776860714)\n",
      "Episode 32000 finished after 0 timesteps with r=0.0. Running score: 0.08. Times trained:               7880. Times reached goal: 80.               Steps done: 222620. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.720377776019.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.402812182903) A[1]:(0.450937062502) A[2]:(0.368107795715) A[3]:(0.404904156923)\n",
      " state (1)  A[0]:(0.408968657255) A[1]:(0.00680109811947) A[2]:(0.356671959162) A[3]:(0.345533043146)\n",
      " state (2)  A[0]:(0.387815862894) A[1]:(0.131101161242) A[2]:(0.424378812313) A[3]:(0.384244948626)\n",
      " state (3)  A[0]:(0.390322774649) A[1]:(0.364173680544) A[2]:(0.297333091497) A[3]:(0.417070865631)\n",
      " state (4)  A[0]:(0.419869631529) A[1]:(0.493198424578) A[2]:(0.00385962007567) A[3]:(0.413726508617)\n",
      " state (5)  A[0]:(0.414726495743) A[1]:(0.403002411127) A[2]:(0.0298968236893) A[3]:(0.391759604216)\n",
      " state (6)  A[0]:(0.419315934181) A[1]:(0.173937126994) A[2]:(0.214003413916) A[3]:(0.341016918421)\n",
      " state (7)  A[0]:(0.443816035986) A[1]:(0.0412973463535) A[2]:(0.402517974377) A[3]:(0.291902899742)\n",
      " state (8)  A[0]:(0.470168054104) A[1]:(0.187334164977) A[2]:(0.562656641006) A[3]:(0.284687310457)\n",
      " state (9)  A[0]:(0.490216374397) A[1]:(0.537174701691) A[2]:(0.689343631268) A[3]:(0.322810709476)\n",
      " state (10)  A[0]:(0.504874587059) A[1]:(0.823443770409) A[2]:(0.783511638641) A[3]:(0.386817336082)\n",
      " state (11)  A[0]:(0.516653597355) A[1]:(0.946521282196) A[2]:(0.849887609482) A[3]:(0.458114624023)\n",
      " state (12)  A[0]:(0.527321696281) A[1]:(0.984571397305) A[2]:(0.894977629185) A[3]:(0.525671362877)\n",
      " state (13)  A[0]:(0.537865042686) A[1]:(0.995298743248) A[2]:(0.925025701523) A[3]:(0.584607005119)\n",
      " state (14)  A[0]:(0.548750519753) A[1]:(0.998413026333) A[2]:(0.944981873035) A[3]:(0.63371270895)\n",
      " state (15)  A[0]:(0.560114383698) A[1]:(0.999395847321) A[2]:(0.95835351944) A[3]:(0.67362010479)\n",
      "Episode 33000 finished after 0 timesteps with r=1.0. Running score: 0.13. Times trained:               7798. Times reached goal: 70.               Steps done: 230418. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.714782115956.\n",
      " state (0)  A[0]:(0.422644793987) A[1]:(0.468238055706) A[2]:(0.389547288418) A[3]:(0.422815293074)\n",
      " state (1)  A[0]:(0.430040329695) A[1]:(0.0119019495323) A[2]:(0.366430938244) A[3]:(0.357156813145)\n",
      " state (2)  A[0]:(0.404509007931) A[1]:(0.110147513449) A[2]:(0.435233265162) A[3]:(0.392510324717)\n",
      " state (3)  A[0]:(0.398043274879) A[1]:(0.357127159834) A[2]:(0.325197130442) A[3]:(0.430568784475)\n",
      " state (4)  A[0]:(0.428375542164) A[1]:(0.509085536003) A[2]:(-9.1016292572e-05) A[3]:(0.429321676493)\n",
      " state (5)  A[0]:(0.419892460108) A[1]:(0.418440014124) A[2]:(0.0173023361713) A[3]:(0.409476935863)\n",
      " state (6)  A[0]:(0.423816114664) A[1]:(0.160031422973) A[2]:(0.217592388391) A[3]:(0.356590241194)\n",
      " state (7)  A[0]:(0.452160567045) A[1]:(-0.000696390750818) A[2]:(0.41926497221) A[3]:(0.301676034927)\n",
      " state (8)  A[0]:(0.481124520302) A[1]:(0.154361143708) A[2]:(0.583433270454) A[3]:(0.291388988495)\n",
      " state (9)  A[0]:(0.50096976757) A[1]:(0.542163848877) A[2]:(0.70803964138) A[3]:(0.330469846725)\n",
      " state (10)  A[0]:(0.513721942902) A[1]:(0.842652261257) A[2]:(0.798016428947) A[3]:(0.396986454725)\n",
      " state (11)  A[0]:(0.523016810417) A[1]:(0.957651853561) A[2]:(0.860294938087) A[3]:(0.470404714346)\n",
      " state (12)  A[0]:(0.531278789043) A[1]:(0.988988935947) A[2]:(0.902128696442) A[3]:(0.539013206959)\n",
      " state (13)  A[0]:(0.539815664291) A[1]:(0.996911287308) A[2]:(0.929824054241) A[3]:(0.5979860425)\n",
      " state (14)  A[0]:(0.549211382866) A[1]:(0.999019861221) A[2]:(0.948162794113) A[3]:(0.646436095238)\n",
      " state (15)  A[0]:(0.559594035149) A[1]:(0.999642729759) A[2]:(0.960451602936) A[3]:(0.685330450535)\n",
      "Episode 34000 finished after 0 timesteps with r=0.0. Running score: 0.11. Times trained:               8015. Times reached goal: 74.               Steps done: 238433. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.709076034962.\n",
      "q_values \n",
      "tensor([[ 0.4427,  0.4955,  0.4077,  0.4468]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4548,  0.0167,  0.3752,  0.3752]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4281,  0.1054,  0.4487,  0.4095]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4485,  0.2185,  0.2250,  0.3885]], device='cuda:0')\n",
      "On state=6, selected action=3 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4279,  0.1057,  0.4489,  0.4095]], device='cuda:0')\n",
      "On state=2, selected action=0 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4547,  0.0167,  0.3755,  0.3752]], device='cuda:0')\n",
      "On state=1, selected action=1 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.44256362319) A[1]:(0.494967311621) A[2]:(0.407815784216) A[3]:(0.44685330987)\n",
      " state (1)  A[0]:(0.454630792141) A[1]:(0.0162612907588) A[2]:(0.375428676605) A[3]:(0.37507584691)\n",
      " state (2)  A[0]:(0.427787154913) A[1]:(0.105154126883) A[2]:(0.449004471302) A[3]:(0.409377217293)\n",
      " state (3)  A[0]:(0.415382057428) A[1]:(0.369543939829) A[2]:(0.362375050783) A[3]:(0.453326940536)\n",
      " state (4)  A[0]:(0.452843397856) A[1]:(0.555168747902) A[2]:(0.00355692207813) A[3]:(0.455386847258)\n",
      " state (5)  A[0]:(0.444507390261) A[1]:(0.484661310911) A[2]:(0.0056836293079) A[3]:(0.440059244633)\n",
      " state (6)  A[0]:(0.448003768921) A[1]:(0.219832926989) A[2]:(0.225391864777) A[3]:(0.38834297657)\n",
      " state (7)  A[0]:(0.480745643377) A[1]:(0.0318064205348) A[2]:(0.444296002388) A[3]:(0.328541010618)\n",
      " state (8)  A[0]:(0.512712955475) A[1]:(0.18578325212) A[2]:(0.613986968994) A[3]:(0.313989639282)\n",
      " state (9)  A[0]:(0.532103836536) A[1]:(0.588076174259) A[2]:(0.736027061939) A[3]:(0.352758824825)\n",
      " state (10)  A[0]:(0.542301416397) A[1]:(0.874258518219) A[2]:(0.820519924164) A[3]:(0.42085647583)\n",
      " state (11)  A[0]:(0.548366904259) A[1]:(0.9700075984) A[2]:(0.877262413502) A[3]:(0.495516449213)\n",
      " state (12)  A[0]:(0.553553581238) A[1]:(0.992943942547) A[2]:(0.914551258087) A[3]:(0.564208626747)\n",
      " state (13)  A[0]:(0.559560716152) A[1]:(0.998164534569) A[2]:(0.938850879669) A[3]:(0.622215092182)\n",
      " state (14)  A[0]:(0.567091345787) A[1]:(0.999447226524) A[2]:(0.954770684242) A[3]:(0.669070720673)\n",
      " state (15)  A[0]:(0.576233029366) A[1]:(0.999805092812) A[2]:(0.965373516083) A[3]:(0.706143796444)\n",
      "Episode 35000 finished after 0 timesteps with r=0.0. Running score: 0.08. Times trained:               8209. Times reached goal: 85.               Steps done: 246642. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.703279056045.\n",
      " state (0)  A[0]:(0.453712165356) A[1]:(0.514611184597) A[2]:(0.422854274511) A[3]:(0.457580983639)\n",
      " state (1)  A[0]:(0.471881866455) A[1]:(0.0247935801744) A[2]:(0.381751060486) A[3]:(0.381577670574)\n",
      " state (2)  A[0]:(0.443777799606) A[1]:(0.0807213932276) A[2]:(0.453654497862) A[3]:(0.412214398384)\n",
      " state (3)  A[0]:(0.423991620541) A[1]:(0.352320998907) A[2]:(0.38573628664) A[3]:(0.462327867746)\n",
      " state (4)  A[0]:(0.467733085155) A[1]:(0.569760799408) A[2]:(-0.00251706899144) A[3]:(0.4696585536)\n",
      " state (5)  A[0]:(0.456385701895) A[1]:(0.51336979866) A[2]:(-0.0137030845508) A[3]:(0.460764288902)\n",
      " state (6)  A[0]:(0.457106947899) A[1]:(0.238199457526) A[2]:(0.222719818354) A[3]:(0.411445200443)\n",
      " state (7)  A[0]:(0.494878858328) A[1]:(0.0173370745033) A[2]:(0.457134395838) A[3]:(0.346360117197)\n",
      " state (8)  A[0]:(0.531490921974) A[1]:(0.160428538918) A[2]:(0.63278645277) A[3]:(0.326138466597)\n",
      " state (9)  A[0]:(0.551702916622) A[1]:(0.586557030678) A[2]:(0.754035830498) A[3]:(0.363997370005)\n",
      " state (10)  A[0]:(0.560183286667) A[1]:(0.883829534054) A[2]:(0.835264086723) A[3]:(0.434191435575)\n",
      " state (11)  A[0]:(0.563619852066) A[1]:(0.974914133549) A[2]:(0.888519704342) A[3]:(0.511203527451)\n",
      " state (12)  A[0]:(0.566234827042) A[1]:(0.994589090347) A[2]:(0.922892034054) A[3]:(0.581233263016)\n",
      " state (13)  A[0]:(0.570183992386) A[1]:(0.998682379723) A[2]:(0.944990098476) A[3]:(0.639447927475)\n",
      " state (14)  A[0]:(0.576308369637) A[1]:(0.999620735645) A[2]:(0.959331035614) A[3]:(0.685729861259)\n",
      " state (15)  A[0]:(0.584652721882) A[1]:(0.99987000227) A[2]:(0.968828439713) A[3]:(0.721838712692)\n",
      "Episode 36000 finished after 0 timesteps with r=1.0. Running score: 0.16. Times trained:               8083. Times reached goal: 89.               Steps done: 254725. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.697617363989.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.457685649395) A[1]:(0.523553967476) A[2]:(0.431372642517) A[3]:(0.464061379433)\n",
      " state (1)  A[0]:(0.479755938053) A[1]:(0.0182400681078) A[2]:(0.380165427923) A[3]:(0.382126063108)\n",
      " state (2)  A[0]:(0.449205696583) A[1]:(0.0551902614534) A[2]:(0.452287435532) A[3]:(0.410185962915)\n",
      " state (3)  A[0]:(0.420315653086) A[1]:(0.333091288805) A[2]:(0.404689759016) A[3]:(0.465839892626)\n",
      " state (4)  A[0]:(0.469229251146) A[1]:(0.58524274826) A[2]:(-0.00105446542148) A[3]:(0.478764891624)\n",
      " state (5)  A[0]:(0.452352613211) A[1]:(0.546792268753) A[2]:(-0.0222899056971) A[3]:(0.476310789585)\n",
      " state (6)  A[0]:(0.448336035013) A[1]:(0.272005617619) A[2]:(0.228139922023) A[3]:(0.429028481245)\n",
      " state (7)  A[0]:(0.49232968688) A[1]:(0.0234304238111) A[2]:(0.474694043398) A[3]:(0.357152581215)\n",
      " state (8)  A[0]:(0.535461187363) A[1]:(0.151616230607) A[2]:(0.654400467873) A[3]:(0.329124629498)\n",
      " state (9)  A[0]:(0.557392179966) A[1]:(0.593136429787) A[2]:(0.774015784264) A[3]:(0.365076452494)\n",
      " state (10)  A[0]:(0.564218103886) A[1]:(0.894530296326) A[2]:(0.851595580578) A[3]:(0.437670767307)\n",
      " state (11)  A[0]:(0.56468296051) A[1]:(0.979299247265) A[2]:(0.901135444641) A[3]:(0.517972052097)\n",
      " state (12)  A[0]:(0.564256906509) A[1]:(0.995889544487) A[2]:(0.932420253754) A[3]:(0.590396046638)\n",
      " state (13)  A[0]:(0.565697789192) A[1]:(0.99905949831) A[2]:(0.952179074287) A[3]:(0.649773478508)\n",
      " state (14)  A[0]:(0.5700481534) A[1]:(0.999740481377) A[2]:(0.964827775955) A[3]:(0.696277737617)\n",
      " state (15)  A[0]:(0.577323079109) A[1]:(0.999913334846) A[2]:(0.973124444485) A[3]:(0.73206615448)\n",
      "Episode 37000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained:               7849. Times reached goal: 79.               Steps done: 262574. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.692163198174.\n",
      " state (0)  A[0]:(0.490007728338) A[1]:(0.543800115585) A[2]:(0.452096432447) A[3]:(0.492647200823)\n",
      " state (1)  A[0]:(0.509129047394) A[1]:(0.0195079427212) A[2]:(0.394056856632) A[3]:(0.402081519365)\n",
      " state (2)  A[0]:(0.474708378315) A[1]:(0.0498216077685) A[2]:(0.465461313725) A[3]:(0.427779316902)\n",
      " state (3)  A[0]:(0.436665654182) A[1]:(0.32538163662) A[2]:(0.428366035223) A[3]:(0.486065745354)\n",
      " state (4)  A[0]:(0.4875228405) A[1]:(0.599880754948) A[2]:(-0.000702142599039) A[3]:(0.502151370049)\n",
      " state (5)  A[0]:(0.459109514952) A[1]:(0.569446027279) A[2]:(-0.0290322955698) A[3]:(0.505238771439)\n",
      " state (6)  A[0]:(0.447341829538) A[1]:(0.293461710215) A[2]:(0.23122254014) A[3]:(0.459215909243)\n",
      " state (7)  A[0]:(0.497645497322) A[1]:(0.02446051687) A[2]:(0.485865414143) A[3]:(0.37896490097)\n",
      " state (8)  A[0]:(0.54771566391) A[1]:(0.146215200424) A[2]:(0.668547034264) A[3]:(0.34022039175)\n",
      " state (9)  A[0]:(0.571165800095) A[1]:(0.606799721718) A[2]:(0.787204861641) A[3]:(0.371093034744)\n",
      " state (10)  A[0]:(0.575845718384) A[1]:(0.907303154469) A[2]:(0.86236256361) A[3]:(0.4431399405)\n",
      " state (11)  A[0]:(0.572838783264) A[1]:(0.983559072018) A[2]:(0.90937936306) A[3]:(0.524186491966)\n",
      " state (12)  A[0]:(0.569015264511) A[1]:(0.996990144253) A[2]:(0.938550591469) A[3]:(0.597019433975)\n",
      " state (13)  A[0]:(0.567800879478) A[1]:(0.999348759651) A[2]:(0.956711173058) A[3]:(0.656174898148)\n",
      " state (14)  A[0]:(0.570430994034) A[1]:(0.999826133251) A[2]:(0.968215107918) A[3]:(0.702043056488)\n",
      " state (15)  A[0]:(0.576841115952) A[1]:(0.999942839146) A[2]:(0.975712239742) A[3]:(0.737049818039)\n",
      "Episode 38000 finished after 0 timesteps with r=0.0. Running score: 0.06. Times trained:               8045. Times reached goal: 83.               Steps done: 270619. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.6866170844.\n",
      " state (0)  A[0]:(0.48134765029) A[1]:(0.543285965919) A[2]:(0.453024417162) A[3]:(0.488090693951)\n",
      " state (1)  A[0]:(0.504188299179) A[1]:(0.0232005491853) A[2]:(0.394250959158) A[3]:(0.396439760923)\n",
      " state (2)  A[0]:(0.467953383923) A[1]:(0.0563899166882) A[2]:(0.465754777193) A[3]:(0.423408538103)\n",
      " state (3)  A[0]:(0.425108939409) A[1]:(0.324858933687) A[2]:(0.43727388978) A[3]:(0.483655720949)\n",
      " state (4)  A[0]:(0.489432066679) A[1]:(0.607851445675) A[2]:(-0.00270810071379) A[3]:(0.497684657574)\n",
      " state (5)  A[0]:(0.449200063944) A[1]:(0.576866745949) A[2]:(-0.0271193627268) A[3]:(0.508026659489)\n",
      " state (6)  A[0]:(0.42982801795) A[1]:(0.298595607281) A[2]:(0.238388106227) A[3]:(0.467404723167)\n",
      " state (7)  A[0]:(0.492146730423) A[1]:(0.0130898626521) A[2]:(0.49482691288) A[3]:(0.382331728935)\n",
      " state (8)  A[0]:(0.554960608482) A[1]:(0.129485741258) A[2]:(0.677579879761) A[3]:(0.336085110903)\n",
      " state (9)  A[0]:(0.583320021629) A[1]:(0.613690972328) A[2]:(0.794920504093) A[3]:(0.365953207016)\n",
      " state (10)  A[0]:(0.587708353996) A[1]:(0.917396128178) A[2]:(0.868355989456) A[3]:(0.442112237215)\n",
      " state (11)  A[0]:(0.582371711731) A[1]:(0.9868298769) A[2]:(0.913796842098) A[3]:(0.528128445148)\n",
      " state (12)  A[0]:(0.575933694839) A[1]:(0.9977876544) A[2]:(0.941715300083) A[3]:(0.604569911957)\n",
      " state (13)  A[0]:(0.572612404823) A[1]:(0.999549031258) A[2]:(0.958957850933) A[3]:(0.665642380714)\n",
      " state (14)  A[0]:(0.573876857758) A[1]:(0.999883830547) A[2]:(0.969821929932) A[3]:(0.712180733681)\n",
      " state (15)  A[0]:(0.5796007514) A[1]:(0.999962508678) A[2]:(0.976885378361) A[3]:(0.747140169144)\n",
      "Episode 39000 finished after 0 timesteps with r=0.0. Running score: 0.13. Times trained:               8497. Times reached goal: 106.               Steps done: 279116. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.680807615516.\n",
      "q_values \n",
      "tensor([[ 0.4796,  0.5415,  0.4598,  0.4877]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4900,  0.6031, -0.0092,  0.5003]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.479663848877) A[1]:(0.541522145271) A[2]:(0.459790199995) A[3]:(0.487729370594)\n",
      " state (1)  A[0]:(0.504509985447) A[1]:(0.0263225417584) A[2]:(0.399950653315) A[3]:(0.394262164831)\n",
      " state (2)  A[0]:(0.46431311965) A[1]:(0.0659125521779) A[2]:(0.470453351736) A[3]:(0.424124777317)\n",
      " state (3)  A[0]:(0.413121432066) A[1]:(0.318798035383) A[2]:(0.446483701468) A[3]:(0.487418502569)\n",
      " state (4)  A[0]:(0.490137755871) A[1]:(0.603401184082) A[2]:(-0.00912182033062) A[3]:(0.500154912472)\n",
      " state (5)  A[0]:(0.433534860611) A[1]:(0.568769037724) A[2]:(-0.0346206575632) A[3]:(0.519802212715)\n",
      " state (6)  A[0]:(0.405123829842) A[1]:(0.292201250792) A[2]:(0.229904174805) A[3]:(0.486827403307)\n",
      " state (7)  A[0]:(0.48348402977) A[1]:(-0.00117170752492) A[2]:(0.486929625273) A[3]:(0.398149460554)\n",
      " state (8)  A[0]:(0.563089728355) A[1]:(0.110758841038) A[2]:(0.672368764877) A[3]:(0.342758595943)\n",
      " state (9)  A[0]:(0.598154306412) A[1]:(0.617554545403) A[2]:(0.791789054871) A[3]:(0.368955641985)\n",
      " state (10)  A[0]:(0.602838397026) A[1]:(0.925200462341) A[2]:(0.866446197033) A[3]:(0.446986466646)\n",
      " state (11)  A[0]:(0.595334172249) A[1]:(0.989161610603) A[2]:(0.912548661232) A[3]:(0.536347270012)\n",
      " state (12)  A[0]:(0.586334228516) A[1]:(0.998308241367) A[2]:(0.940800070763) A[3]:(0.615300297737)\n",
      " state (13)  A[0]:(0.580966591835) A[1]:(0.999670922756) A[2]:(0.958208322525) A[3]:(0.677606344223)\n",
      " state (14)  A[0]:(0.580960035324) A[1]:(0.999917268753) A[2]:(0.969165503979) A[3]:(0.724425435066)\n",
      " state (15)  A[0]:(0.586115658283) A[1]:(0.999973535538) A[2]:(0.976295053959) A[3]:(0.759136974812)\n",
      "Episode 40000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               8647. Times reached goal: 108.               Steps done: 287763. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.674946051061.\n",
      " state (0)  A[0]:(0.490348726511) A[1]:(0.555926203728) A[2]:(0.459789067507) A[3]:(0.495934396982)\n",
      " state (1)  A[0]:(0.513119399548) A[1]:(0.0280799940228) A[2]:(0.395227402449) A[3]:(0.392654776573)\n",
      " state (2)  A[0]:(0.468751072884) A[1]:(0.0876426398754) A[2]:(0.467668771744) A[3]:(0.425723671913)\n",
      " state (3)  A[0]:(0.414510816336) A[1]:(0.332887977362) A[2]:(0.445550620556) A[3]:(0.490941166878)\n",
      " state (4)  A[0]:(0.506151437759) A[1]:(0.615343391895) A[2]:(-0.011180203408) A[3]:(0.503030180931)\n",
      " state (5)  A[0]:(0.422778129578) A[1]:(0.574855804443) A[2]:(-0.0181907024235) A[3]:(0.534060537815)\n",
      " state (6)  A[0]:(0.374490380287) A[1]:(0.308417439461) A[2]:(0.245491445065) A[3]:(0.509065747261)\n",
      " state (7)  A[0]:(0.463682591915) A[1]:(0.0100617585704) A[2]:(0.497518450022) A[3]:(0.415473043919)\n",
      " state (8)  A[0]:(0.559764027596) A[1]:(0.107265263796) A[2]:(0.679477453232) A[3]:(0.346266508102)\n",
      " state (9)  A[0]:(0.601460456848) A[1]:(0.622339904308) A[2]:(0.796241462231) A[3]:(0.364021748304)\n",
      " state (10)  A[0]:(0.605801403522) A[1]:(0.930786550045) A[2]:(0.869130849838) A[3]:(0.441628426313)\n",
      " state (11)  A[0]:(0.595202803612) A[1]:(0.99067568779) A[2]:(0.914240002632) A[3]:(0.534473896027)\n",
      " state (12)  A[0]:(0.582646727562) A[1]:(0.998624742031) A[2]:(0.941960513592) A[3]:(0.617497444153)\n",
      " state (13)  A[0]:(0.574305593967) A[1]:(0.999742150307) A[2]:(0.959070980549) A[3]:(0.683028936386)\n",
      " state (14)  A[0]:(0.57225048542) A[1]:(0.999936461449) A[2]:(0.96984577179) A[3]:(0.731977343559)\n",
      " state (15)  A[0]:(0.576254725456) A[1]:(0.99997985363) A[2]:(0.976854562759) A[3]:(0.767931818962)\n",
      "Episode 41000 finished after 0 timesteps with r=1.0. Running score: 0.13. Times trained:               8487. Times reached goal: 84.               Steps done: 296250. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.669242023206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.496866077185) A[1]:(0.557475805283) A[2]:(0.464489966631) A[3]:(0.502061903477)\n",
      " state (1)  A[0]:(0.51805305481) A[1]:(0.0334084518254) A[2]:(0.402673989534) A[3]:(0.397457659245)\n",
      " state (2)  A[0]:(0.470015853643) A[1]:(0.0961677283049) A[2]:(0.47464543581) A[3]:(0.432225137949)\n",
      " state (3)  A[0]:(0.408643245697) A[1]:(0.335568726063) A[2]:(0.459234565496) A[3]:(0.5004119277)\n",
      " state (4)  A[0]:(0.517180740833) A[1]:(0.624392271042) A[2]:(-0.00454625347629) A[3]:(0.511067271233)\n",
      " state (5)  A[0]:(0.416586726904) A[1]:(0.586841881275) A[2]:(-0.0134002296254) A[3]:(0.55066883564)\n",
      " state (6)  A[0]:(0.352563500404) A[1]:(0.330083608627) A[2]:(0.251989841461) A[3]:(0.533214092255)\n",
      " state (7)  A[0]:(0.454197794199) A[1]:(0.0322646833956) A[2]:(0.50640141964) A[3]:(0.437970817089)\n",
      " state (8)  A[0]:(0.564474403858) A[1]:(0.136267408729) A[2]:(0.688266217709) A[3]:(0.360603362322)\n",
      " state (9)  A[0]:(0.609418988228) A[1]:(0.659573793411) A[2]:(0.802727639675) A[3]:(0.373370736837)\n",
      " state (10)  A[0]:(0.611155509949) A[1]:(0.943616330624) A[2]:(0.873544156551) A[3]:(0.450802594423)\n",
      " state (11)  A[0]:(0.596343636513) A[1]:(0.992941200733) A[2]:(0.91741669178) A[3]:(0.545923233032)\n",
      " state (12)  A[0]:(0.57985752821) A[1]:(0.999001681805) A[2]:(0.944449305534) A[3]:(0.631645441055)\n",
      " state (13)  A[0]:(0.568575918674) A[1]:(0.999816298485) A[2]:(0.961133718491) A[3]:(0.699238061905)\n",
      " state (14)  A[0]:(0.564676046371) A[1]:(0.999954938889) A[2]:(0.971604466438) A[3]:(0.749364674091)\n",
      " state (15)  A[0]:(0.567761778831) A[1]:(0.999985694885) A[2]:(0.978376865387) A[3]:(0.785779237747)\n",
      "Episode 42000 finished after 0 timesteps with r=0.0. Running score: 0.13. Times trained:               8400. Times reached goal: 109.               Steps done: 304650. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.663643935098.\n",
      " state (0)  A[0]:(0.501958966255) A[1]:(0.559263586998) A[2]:(0.472600221634) A[3]:(0.50598526001)\n",
      " state (1)  A[0]:(0.523223817348) A[1]:(0.0335581600666) A[2]:(0.405680686235) A[3]:(0.397688090801)\n",
      " state (2)  A[0]:(0.472453683615) A[1]:(0.103732861578) A[2]:(0.476633757353) A[3]:(0.434702306986)\n",
      " state (3)  A[0]:(0.402244508266) A[1]:(0.331523925066) A[2]:(0.46742361784) A[3]:(0.505095720291)\n",
      " state (4)  A[0]:(0.523245155811) A[1]:(0.62285721302) A[2]:(-0.00160550931469) A[3]:(0.512454807758)\n",
      " state (5)  A[0]:(0.407820731401) A[1]:(0.583238720894) A[2]:(-0.0193115007132) A[3]:(0.558001995087)\n",
      " state (6)  A[0]:(0.330535709858) A[1]:(0.320678323507) A[2]:(0.244395494461) A[3]:(0.546131908894)\n",
      " state (7)  A[0]:(0.446937084198) A[1]:(0.00710847927257) A[2]:(0.50202691555) A[3]:(0.447900772095)\n",
      " state (8)  A[0]:(0.572340369225) A[1]:(0.107078105211) A[2]:(0.685444831848) A[3]:(0.360799133778)\n",
      " state (9)  A[0]:(0.621276140213) A[1]:(0.654759049416) A[2]:(0.799311101437) A[3]:(0.36604359746)\n",
      " state (10)  A[0]:(0.622124314308) A[1]:(0.945768356323) A[2]:(0.870270013809) A[3]:(0.441771537066)\n",
      " state (11)  A[0]:(0.605387210846) A[1]:(0.993513047695) A[2]:(0.915231108665) A[3]:(0.540276348591)\n",
      " state (12)  A[0]:(0.587171733379) A[1]:(0.99910992384) A[2]:(0.943484008312) A[3]:(0.631778120995)\n",
      " state (13)  A[0]:(0.574641346931) A[1]:(0.999839603901) A[2]:(0.961066067219) A[3]:(0.704942822456)\n",
      " state (14)  A[0]:(0.56997013092) A[1]:(0.999961256981) A[2]:(0.972065091133) A[3]:(0.759210944176)\n",
      " state (15)  A[0]:(0.572687745094) A[1]:(0.999987840652) A[2]:(0.979099571705) A[3]:(0.798247158527)\n",
      "Episode 43000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained:               8116. Times reached goal: 108.               Steps done: 312766. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.658279598842.\n",
      " state (0)  A[0]:(0.501212120056) A[1]:(0.550847411156) A[2]:(0.472142547369) A[3]:(0.505870223045)\n",
      " state (1)  A[0]:(0.522171139717) A[1]:(0.0281569417566) A[2]:(0.401424497366) A[3]:(0.39347037673)\n",
      " state (2)  A[0]:(0.467495232821) A[1]:(0.0895065069199) A[2]:(0.469625890255) A[3]:(0.42966067791)\n",
      " state (3)  A[0]:(0.384335517883) A[1]:(0.304925173521) A[2]:(0.465854734182) A[3]:(0.503217339516)\n",
      " state (4)  A[0]:(0.514811038971) A[1]:(0.60644286871) A[2]:(-0.00865516811609) A[3]:(0.510491013527)\n",
      " state (5)  A[0]:(0.380162030458) A[1]:(0.567800998688) A[2]:(-0.0338339656591) A[3]:(0.562398433685)\n",
      " state (6)  A[0]:(0.288573980331) A[1]:(0.302371889353) A[2]:(0.229108765721) A[3]:(0.555441379547)\n",
      " state (7)  A[0]:(0.423470169306) A[1]:(-0.0238001998514) A[2]:(0.492494523525) A[3]:(0.454198867083)\n",
      " state (8)  A[0]:(0.567476272583) A[1]:(0.0742651373148) A[2]:(0.678357541561) A[3]:(0.356345117092)\n",
      " state (9)  A[0]:(0.621779382229) A[1]:(0.648279249668) A[2]:(0.791368186474) A[3]:(0.35121551156)\n",
      " state (10)  A[0]:(0.622842907906) A[1]:(0.947184979916) A[2]:(0.862946867943) A[3]:(0.422791928053)\n",
      " state (11)  A[0]:(0.605763435364) A[1]:(0.993889391422) A[2]:(0.910194754601) A[3]:(0.52453738451)\n",
      " state (12)  A[0]:(0.587763428688) A[1]:(0.999178528786) A[2]:(0.940877258778) A[3]:(0.623576879501)\n",
      " state (13)  A[0]:(0.575921475887) A[1]:(0.999854385853) A[2]:(0.960203051567) A[3]:(0.704544425011)\n",
      " state (14)  A[0]:(0.572132945061) A[1]:(0.999965369701) A[2]:(0.972212791443) A[3]:(0.76473993063)\n",
      " state (15)  A[0]:(0.57575494051) A[1]:(0.999989271164) A[2]:(0.979751050472) A[3]:(0.807551026344)\n",
      "Episode 44000 finished after 0 timesteps with r=1.0. Running score: 0.1. Times trained:               8420. Times reached goal: 116.               Steps done: 321186. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.652760154092.\n",
      "q_values \n",
      "tensor([[ 0.5029,  0.5649,  0.4776,  0.5093]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5336,  0.6282, -0.0030,  0.5113]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5328,  0.6260, -0.0038,  0.5110]], device='cuda:0')\n",
      "On state=4, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5022,  0.5626,  0.4764,  0.5087]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5020,  0.5619,  0.4761,  0.5085]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.6211, -0.0055,  0.5105]], device='cuda:0')\n",
      "On state=4, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5017,  0.5610,  0.4756,  0.5083]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5015,  0.5607,  0.4754,  0.5081]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5014,  0.5606,  0.4753,  0.5081]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5305,  0.6170, -0.0070,  0.5100]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5304,  0.6167, -0.0070,  0.5098]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5771,  0.0765,  0.6854,  0.3649]], device='cuda:0')\n",
      "On state=8, selected action=3 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5306,  0.6176, -0.0067,  0.5097]], device='cuda:0')\n",
      "On state=4, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5014,  0.5609,  0.4750,  0.5080]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5309,  0.6190, -0.0062,  0.5093]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.501474559307) A[1]:(0.56109380722) A[2]:(0.475040882826) A[3]:(0.507938325405)\n",
      " state (1)  A[0]:(0.525604724884) A[1]:(0.0348615273833) A[2]:(0.401669532061) A[3]:(0.393732130527)\n",
      " state (2)  A[0]:(0.470794767141) A[1]:(0.0933251976967) A[2]:(0.468885660172) A[3]:(0.429366618395)\n",
      " state (3)  A[0]:(0.383052587509) A[1]:(0.306395441294) A[2]:(0.470841646194) A[3]:(0.503977179527)\n",
      " state (4)  A[0]:(0.531081557274) A[1]:(0.619548797607) A[2]:(-0.00609753932804) A[3]:(0.509078621864)\n",
      " state (5)  A[0]:(0.388213962317) A[1]:(0.588945746422) A[2]:(-0.0365626476705) A[3]:(0.5657736063)\n",
      " state (6)  A[0]:(0.282134801149) A[1]:(0.326639533043) A[2]:(0.230381190777) A[3]:(0.565143585205)\n",
      " state (7)  A[0]:(0.425930887461) A[1]:(-0.0159589815885) A[2]:(0.501706421375) A[3]:(0.46613919735)\n",
      " state (8)  A[0]:(0.577099204063) A[1]:(0.0859741196036) A[2]:(0.686336755753) A[3]:(0.363887429237)\n",
      " state (9)  A[0]:(0.629395186901) A[1]:(0.674299776554) A[2]:(0.793523490429) A[3]:(0.350741952658)\n",
      " state (10)  A[0]:(0.626909613609) A[1]:(0.95483058691) A[2]:(0.862384736538) A[3]:(0.417187780142)\n",
      " state (11)  A[0]:(0.607665061951) A[1]:(0.995012640953) A[2]:(0.910110294819) A[3]:(0.520440578461)\n",
      " state (12)  A[0]:(0.589114427567) A[1]:(0.999348104) A[2]:(0.942140221596) A[3]:(0.625501096249)\n",
      " state (13)  A[0]:(0.577751994133) A[1]:(0.999886989594) A[2]:(0.96238553524) A[3]:(0.712849617004)\n",
      " state (14)  A[0]:(0.574886143208) A[1]:(0.999973714352) A[2]:(0.974729120731) A[3]:(0.777527868748)\n",
      " state (15)  A[0]:(0.579522848129) A[1]:(0.999992012978) A[2]:(0.982237756252) A[3]:(0.822749614716)\n",
      "Episode 45000 finished after 0 timesteps with r=0.0. Running score: 0.07. Times trained:               8594. Times reached goal: 106.               Steps done: 329780. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.647174369821.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.495037317276) A[1]:(0.55725902319) A[2]:(0.4672267735) A[3]:(0.499013930559)\n",
      " state (1)  A[0]:(0.520731210709) A[1]:(0.0391028411686) A[2]:(0.395342409611) A[3]:(0.387831181288)\n",
      " state (2)  A[0]:(0.465522289276) A[1]:(0.096972219646) A[2]:(0.462564796209) A[3]:(0.423691093922)\n",
      " state (3)  A[0]:(0.366864651442) A[1]:(0.300272196531) A[2]:(0.474284231663) A[3]:(0.49916100502)\n",
      " state (4)  A[0]:(0.526953577995) A[1]:(0.616537690163) A[2]:(-0.00375841278583) A[3]:(0.498285919428)\n",
      " state (5)  A[0]:(0.3879378438) A[1]:(0.600282490253) A[2]:(-0.0634040609002) A[3]:(0.554068803787)\n",
      " state (6)  A[0]:(0.26544636488) A[1]:(0.33924254775) A[2]:(0.207706898451) A[3]:(0.560958683491)\n",
      " state (7)  A[0]:(0.414010435343) A[1]:(-0.0246568098664) A[2]:(0.49544647336) A[3]:(0.467582255602)\n",
      " state (8)  A[0]:(0.570922195911) A[1]:(0.0794171541929) A[2]:(0.682882547379) A[3]:(0.363815575838)\n",
      " state (9)  A[0]:(0.621198415756) A[1]:(0.688914835453) A[2]:(0.784548401833) A[3]:(0.341952234507)\n",
      " state (10)  A[0]:(0.6163854599) A[1]:(0.959705293179) A[2]:(0.851664423943) A[3]:(0.401596754789)\n",
      " state (11)  A[0]:(0.59768474102) A[1]:(0.995722770691) A[2]:(0.902318835258) A[3]:(0.506636142731)\n",
      " state (12)  A[0]:(0.582314729691) A[1]:(0.999456048012) A[2]:(0.938359618187) A[3]:(0.620021343231)\n",
      " state (13)  A[0]:(0.575355172157) A[1]:(0.99990850687) A[2]:(0.961460888386) A[3]:(0.716505050659)\n",
      " state (14)  A[0]:(0.576878845692) A[1]:(0.999979436398) A[2]:(0.97528731823) A[3]:(0.787723898888)\n",
      " state (15)  A[0]:(0.585246324539) A[1]:(0.999993979931) A[2]:(0.98339921236) A[3]:(0.836550176144)\n",
      "Episode 46000 finished after 0 timesteps with r=0.0. Running score: 0.13. Times trained:               8870. Times reached goal: 109.               Steps done: 338650. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.641459316891.\n",
      " state (0)  A[0]:(0.496350109577) A[1]:(0.546231865883) A[2]:(0.472317963839) A[3]:(0.501246750355)\n",
      " state (1)  A[0]:(0.522009670734) A[1]:(0.038754414767) A[2]:(0.398066550493) A[3]:(0.392927438021)\n",
      " state (2)  A[0]:(0.465341478586) A[1]:(0.111433066428) A[2]:(0.467017024755) A[3]:(0.431469500065)\n",
      " state (3)  A[0]:(0.357753127813) A[1]:(0.307583361864) A[2]:(0.482319146395) A[3]:(0.506240785122)\n",
      " state (4)  A[0]:(0.525070369244) A[1]:(0.618392765522) A[2]:(-0.00304635427892) A[3]:(0.498043745756)\n",
      " state (5)  A[0]:(0.383898317814) A[1]:(0.611184358597) A[2]:(-0.082846224308) A[3]:(0.550899028778)\n",
      " state (6)  A[0]:(0.252237588167) A[1]:(0.358659029007) A[2]:(0.189029023051) A[3]:(0.562396347523)\n",
      " state (7)  A[0]:(0.408581763506) A[1]:(-0.0176583938301) A[2]:(0.489121168852) A[3]:(0.47455739975)\n",
      " state (8)  A[0]:(0.567685246468) A[1]:(0.0969564542174) A[2]:(0.674366950989) A[3]:(0.368031173944)\n",
      " state (9)  A[0]:(0.612247228622) A[1]:(0.71763253212) A[2]:(0.766251444817) A[3]:(0.331923395395)\n",
      " state (10)  A[0]:(0.602955043316) A[1]:(0.966074228287) A[2]:(0.830290436745) A[3]:(0.378191262484)\n",
      " state (11)  A[0]:(0.584204077721) A[1]:(0.996528327465) A[2]:(0.885912775993) A[3]:(0.481284856796)\n",
      " state (12)  A[0]:(0.572603702545) A[1]:(0.999571382999) A[2]:(0.929253160954) A[3]:(0.60336959362)\n",
      " state (13)  A[0]:(0.571205258369) A[1]:(0.999930918217) A[2]:(0.957771897316) A[3]:(0.711761593819)\n",
      " state (14)  A[0]:(0.578338265419) A[1]:(0.999985337257) A[2]:(0.974522411823) A[3]:(0.792412757874)\n",
      " state (15)  A[0]:(0.591442584991) A[1]:(0.999996006489) A[2]:(0.983926713467) A[3]:(0.846879482269)\n",
      "Episode 47000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained:               8754. Times reached goal: 128.               Steps done: 347404. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.635868488789.\n",
      " state (0)  A[0]:(0.491940289736) A[1]:(0.55372852087) A[2]:(0.473891019821) A[3]:(0.497326523066)\n",
      " state (1)  A[0]:(0.521206021309) A[1]:(0.0517291203141) A[2]:(0.39971101284) A[3]:(0.393787115812)\n",
      " state (2)  A[0]:(0.465776622295) A[1]:(0.126096591353) A[2]:(0.469455271959) A[3]:(0.433019995689)\n",
      " state (3)  A[0]:(0.352529376745) A[1]:(0.312858521938) A[2]:(0.48922726512) A[3]:(0.505884051323)\n",
      " state (4)  A[0]:(0.528038084507) A[1]:(0.623167097569) A[2]:(-0.00265633431263) A[3]:(0.491002231836)\n",
      " state (5)  A[0]:(0.402192920446) A[1]:(0.63113963604) A[2]:(-0.118028275669) A[3]:(0.537531733513)\n",
      " state (6)  A[0]:(0.262874752283) A[1]:(0.383145362139) A[2]:(0.156589180231) A[3]:(0.55616402626)\n",
      " state (7)  A[0]:(0.420802950859) A[1]:(-0.0240752063692) A[2]:(0.479469507933) A[3]:(0.479923278093)\n",
      " state (8)  A[0]:(0.576795041561) A[1]:(0.0808160677552) A[2]:(0.667712986469) A[3]:(0.376484781504)\n",
      " state (9)  A[0]:(0.612222194672) A[1]:(0.72385597229) A[2]:(0.749018192291) A[3]:(0.328516930342)\n",
      " state (10)  A[0]:(0.596118628979) A[1]:(0.968722343445) A[2]:(0.807924985886) A[3]:(0.361102372408)\n",
      " state (11)  A[0]:(0.576127171516) A[1]:(0.996903121471) A[2]:(0.868241727352) A[3]:(0.460778057575)\n",
      " state (12)  A[0]:(0.568330705166) A[1]:(0.9996265769) A[2]:(0.919885635376) A[3]:(0.590174853802)\n",
      " state (13)  A[0]:(0.573389947414) A[1]:(0.999941706657) A[2]:(0.954561054707) A[3]:(0.709463953972)\n",
      " state (14)  A[0]:(0.587377905846) A[1]:(0.99998819828) A[2]:(0.974377512932) A[3]:(0.798568129539)\n",
      " state (15)  A[0]:(0.606403291225) A[1]:(0.999996900558) A[2]:(0.98493283987) A[3]:(0.85765516758)\n",
      "Episode 48000 finished after 0 timesteps with r=0.0. Running score: 0.13. Times trained:               8652. Times reached goal: 126.               Steps done: 356056. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.630390685771.\n",
      " state (0)  A[0]:(0.500537157059) A[1]:(0.55502218008) A[2]:(0.476302713156) A[3]:(0.506039440632)\n",
      " state (1)  A[0]:(0.528808712959) A[1]:(0.0347950607538) A[2]:(0.399816542864) A[3]:(0.405753165483)\n",
      " state (2)  A[0]:(0.473770737648) A[1]:(0.109230436385) A[2]:(0.470572263002) A[3]:(0.444579690695)\n",
      " state (3)  A[0]:(0.355754971504) A[1]:(0.287873059511) A[2]:(0.492804855108) A[3]:(0.513578236103)\n",
      " state (4)  A[0]:(0.533064603806) A[1]:(0.61018872261) A[2]:(-0.00683849817142) A[3]:(0.496029734612)\n",
      " state (5)  A[0]:(0.421188741922) A[1]:(0.638882517815) A[2]:(-0.159668788314) A[3]:(0.533955574036)\n",
      " state (6)  A[0]:(0.270671308041) A[1]:(0.403264701366) A[2]:(0.115233309567) A[3]:(0.555672168732)\n",
      " state (7)  A[0]:(0.426364928484) A[1]:(-0.0285144913942) A[2]:(0.466812908649) A[3]:(0.488561660051)\n",
      " state (8)  A[0]:(0.579257845879) A[1]:(0.0591323822737) A[2]:(0.662696182728) A[3]:(0.384829610586)\n",
      " state (9)  A[0]:(0.603960573673) A[1]:(0.726059794426) A[2]:(0.732521653175) A[3]:(0.321363657713)\n",
      " state (10)  A[0]:(0.578174352646) A[1]:(0.970839083195) A[2]:(0.78385579586) A[3]:(0.33795440197)\n",
      " state (11)  A[0]:(0.555061936378) A[1]:(0.997218310833) A[2]:(0.848445057869) A[3]:(0.433666616678)\n",
      " state (12)  A[0]:(0.550998330116) A[1]:(0.999671816826) A[2]:(0.90957480669) A[3]:(0.571295976639)\n",
      " state (13)  A[0]:(0.563763976097) A[1]:(0.999949932098) A[2]:(0.951274931431) A[3]:(0.702808976173)\n",
      " state (14)  A[0]:(0.58640152216) A[1]:(0.999990165234) A[2]:(0.974330961704) A[3]:(0.801122784615)\n",
      " state (15)  A[0]:(0.613129079342) A[1]:(0.99999755621) A[2]:(0.985930263996) A[3]:(0.864969909191)\n",
      "Episode 49000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained:               8781. Times reached goal: 134.               Steps done: 364837. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.624879457618.\n",
      "q_values \n",
      "tensor([[ 0.4940,  0.5545,  0.4722,  0.4984]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4940,  0.5546,  0.4723,  0.4984]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5329,  0.6178, -0.0079,  0.4874]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5921,  0.0375,  0.6823,  0.4017]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5978,  0.7459,  0.7334,  0.3306]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5548,  0.9765,  0.7709,  0.3353]], device='cuda:0')\n",
      "On state=10, selected action=3 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2892,  0.4370,  0.1015,  0.5504]], device='cuda:0')\n",
      "On state=6, selected action=3 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4705,  0.0969,  0.4638,  0.4376]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2900,  0.4413,  0.1024,  0.5512]], device='cuda:0')\n",
      "On state=6, selected action=3 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4708,  0.0993,  0.4645,  0.4382]], device='cuda:0')\n",
      "On state=2, selected action=0 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5232,  0.0379,  0.3955,  0.4028]], device='cuda:0')\n",
      "On state=1, selected action=0 , Random? False\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4940,  0.5580,  0.4736,  0.4995]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4940,  0.5584,  0.4737,  0.4996]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5376,  0.6306, -0.0037,  0.4913]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5973,  0.0745,  0.6867,  0.4093]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6042,  0.7621,  0.7383,  0.3395]], device='cuda:0')\n",
      "On state=9, selected action=3 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.493873924017) A[1]:(0.559353530407) A[2]:(0.474019140005) A[3]:(0.499702006578)\n",
      " state (1)  A[0]:(0.523602128029) A[1]:(0.0414748564363) A[2]:(0.396509379148) A[3]:(0.403457462788)\n",
      " state (2)  A[0]:(0.471888959408) A[1]:(0.105404943228) A[2]:(0.466180950403) A[3]:(0.439486145973)\n",
      " state (3)  A[0]:(0.355289548635) A[1]:(0.286966979504) A[2]:(0.492848157883) A[3]:(0.506337165833)\n",
      " state (4)  A[0]:(0.538720548153) A[1]:(0.631942868233) A[2]:(-0.00280264765024) A[3]:(0.491714179516)\n",
      " state (5)  A[0]:(0.445938915014) A[1]:(0.681623756886) A[2]:(-0.180242970586) A[3]:(0.526197969913)\n",
      " state (6)  A[0]:(0.295508623123) A[1]:(0.454963564873) A[2]:(0.108195587993) A[3]:(0.554646372795)\n",
      " state (7)  A[0]:(0.455729842186) A[1]:(-0.0145245408639) A[2]:(0.493593096733) A[3]:(0.503562986851)\n",
      " state (8)  A[0]:(0.598634660244) A[1]:(0.0753521695733) A[2]:(0.687429189682) A[3]:(0.409886389971)\n",
      " state (9)  A[0]:(0.60466247797) A[1]:(0.762072563171) A[2]:(0.738469719887) A[3]:(0.339570313692)\n",
      " state (10)  A[0]:(0.562444090843) A[1]:(0.978186011314) A[2]:(0.776012599468) A[3]:(0.344793766737)\n",
      " state (11)  A[0]:(0.531172335148) A[1]:(0.998113632202) A[2]:(0.839683532715) A[3]:(0.437548518181)\n",
      " state (12)  A[0]:(0.528096735477) A[1]:(0.999791562557) A[2]:(0.906584739685) A[3]:(0.580774903297)\n",
      " state (13)  A[0]:(0.547602593899) A[1]:(0.999969780445) A[2]:(0.952441692352) A[3]:(0.718843519688)\n",
      " state (14)  A[0]:(0.578828632832) A[1]:(0.999994337559) A[2]:(0.976680517197) A[3]:(0.819770634174)\n",
      " state (15)  A[0]:(0.613462865353) A[1]:(0.999998629093) A[2]:(0.988095879555) A[3]:(0.882842540741)\n",
      "Episode 50000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained:               9007. Times reached goal: 119.               Steps done: 373844. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.619276439415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.518887758255) A[1]:(0.576145648956) A[2]:(0.493344962597) A[3]:(0.524675250053)\n",
      " state (1)  A[0]:(0.546323895454) A[1]:(0.0358411557972) A[2]:(0.41395881772) A[3]:(0.426023066044)\n",
      " state (2)  A[0]:(0.492381304502) A[1]:(0.118089690804) A[2]:(0.487188667059) A[3]:(0.464027911425)\n",
      " state (3)  A[0]:(0.3720946908) A[1]:(0.291913002729) A[2]:(0.5085516572) A[3]:(0.525864958763)\n",
      " state (4)  A[0]:(0.556773543358) A[1]:(0.647681474686) A[2]:(-0.00297582754865) A[3]:(0.510833084583)\n",
      " state (5)  A[0]:(0.446505695581) A[1]:(0.694061756134) A[2]:(-0.17202334106) A[3]:(0.542254686356)\n",
      " state (6)  A[0]:(0.304578095675) A[1]:(0.467809289694) A[2]:(0.12486872077) A[3]:(0.564394712448)\n",
      " state (7)  A[0]:(0.491350591183) A[1]:(-0.037139698863) A[2]:(0.527819812298) A[3]:(0.511684834957)\n",
      " state (8)  A[0]:(0.630960166454) A[1]:(0.0647043138742) A[2]:(0.706631243229) A[3]:(0.410527557135)\n",
      " state (9)  A[0]:(0.618680715561) A[1]:(0.785583913326) A[2]:(0.730135560036) A[3]:(0.322534114122)\n",
      " state (10)  A[0]:(0.556660950184) A[1]:(0.982858598232) A[2]:(0.746740341187) A[3]:(0.313110738993)\n",
      " state (11)  A[0]:(0.511413574219) A[1]:(0.998634040356) A[2]:(0.810013473034) A[3]:(0.406017661095)\n",
      " state (12)  A[0]:(0.502941012383) A[1]:(0.999856233597) A[2]:(0.88963252306) A[3]:(0.562646865845)\n",
      " state (13)  A[0]:(0.523314416409) A[1]:(0.999979913235) A[2]:(0.946165919304) A[3]:(0.716016113758)\n",
      " state (14)  A[0]:(0.558307290077) A[1]:(0.999996364117) A[2]:(0.975217700005) A[3]:(0.825978815556)\n",
      " state (15)  A[0]:(0.597280740738) A[1]:(0.999999165535) A[2]:(0.988150656223) A[3]:(0.892074525356)\n",
      "Episode 51000 finished after 0 timesteps with r=0.0. Running score: 0.08. Times trained:               8396. Times reached goal: 102.               Steps done: 382240. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.614098760741.\n",
      " state (0)  A[0]:(0.530090212822) A[1]:(0.588643312454) A[2]:(0.500192224979) A[3]:(0.535046458244)\n",
      " state (1)  A[0]:(0.554881751537) A[1]:(0.029644086957) A[2]:(0.420865297318) A[3]:(0.437144637108)\n",
      " state (2)  A[0]:(0.50046569109) A[1]:(0.122153081) A[2]:(0.497098714113) A[3]:(0.47575160861)\n",
      " state (3)  A[0]:(0.3772983253) A[1]:(0.288627773523) A[2]:(0.516681194305) A[3]:(0.534131884575)\n",
      " state (4)  A[0]:(0.564253091812) A[1]:(0.652425229549) A[2]:(-0.00389419007115) A[3]:(0.524228096008)\n",
      " state (5)  A[0]:(0.438238799572) A[1]:(0.702226042747) A[2]:(-0.176259174943) A[3]:(0.55547940731)\n",
      " state (6)  A[0]:(0.287462353706) A[1]:(0.496830791235) A[2]:(0.119253270328) A[3]:(0.576709985733)\n",
      " state (7)  A[0]:(0.498427838087) A[1]:(-0.0280077494681) A[2]:(0.54805791378) A[3]:(0.530092120171)\n",
      " state (8)  A[0]:(0.64978903532) A[1]:(0.0427139215171) A[2]:(0.728395700455) A[3]:(0.429405272007)\n",
      " state (9)  A[0]:(0.626585483551) A[1]:(0.794559836388) A[2]:(0.732230484486) A[3]:(0.328787744045)\n",
      " state (10)  A[0]:(0.545278429985) A[1]:(0.98578029871) A[2]:(0.727641284466) A[3]:(0.30642041564)\n",
      " state (11)  A[0]:(0.484849989414) A[1]:(0.99899148941) A[2]:(0.788227677345) A[3]:(0.398971647024)\n",
      " state (12)  A[0]:(0.47120898962) A[1]:(0.999902069569) A[2]:(0.879135668278) A[3]:(0.565965771675)\n",
      " state (13)  A[0]:(0.494114339352) A[1]:(0.999987065792) A[2]:(0.944380402565) A[3]:(0.728826224804)\n",
      " state (14)  A[0]:(0.534957945347) A[1]:(0.999997735023) A[2]:(0.976227104664) A[3]:(0.841418504715)\n",
      " state (15)  A[0]:(0.580035984516) A[1]:(0.999999463558) A[2]:(0.989399671555) A[3]:(0.905796647072)\n",
      "Episode 52000 finished after 0 timesteps with r=1.0. Running score: 0.09. Times trained:               8700. Times reached goal: 114.               Steps done: 390940. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.608779274839.\n",
      " state (0)  A[0]:(0.543595433235) A[1]:(0.610435366631) A[2]:(0.50803399086) A[3]:(0.548152327538)\n",
      " state (1)  A[0]:(0.566305875778) A[1]:(0.0283881425858) A[2]:(0.429593622684) A[3]:(0.449922949076)\n",
      " state (2)  A[0]:(0.512440562248) A[1]:(0.146925762296) A[2]:(0.511347055435) A[3]:(0.491121172905)\n",
      " state (3)  A[0]:(0.392137914896) A[1]:(0.311513125896) A[2]:(0.526287436485) A[3]:(0.544874310493)\n",
      " state (4)  A[0]:(0.5897783041) A[1]:(0.679693818092) A[2]:(-0.0112345702946) A[3]:(0.535295903683)\n",
      " state (5)  A[0]:(0.443219304085) A[1]:(0.720830976963) A[2]:(-0.170966550708) A[3]:(0.564898014069)\n",
      " state (6)  A[0]:(0.293785691261) A[1]:(0.523344635963) A[2]:(0.129142105579) A[3]:(0.582561135292)\n",
      " state (7)  A[0]:(0.53140103817) A[1]:(-0.0349212698638) A[2]:(0.579064249992) A[3]:(0.537781953812)\n",
      " state (8)  A[0]:(0.680403470993) A[1]:(0.038993306458) A[2]:(0.746206104755) A[3]:(0.432770490646)\n",
      " state (9)  A[0]:(0.638914823532) A[1]:(0.818784534931) A[2]:(0.718904972076) A[3]:(0.31521448493)\n",
      " state (10)  A[0]:(0.534750461578) A[1]:(0.989185392857) A[2]:(0.682111144066) A[3]:(0.277465701103)\n",
      " state (11)  A[0]:(0.458324640989) A[1]:(0.999295651913) A[2]:(0.739832997322) A[3]:(0.370084196329)\n",
      " state (12)  A[0]:(0.44096532464) A[1]:(0.999934911728) A[2]:(0.852640330791) A[3]:(0.551530003548)\n",
      " state (13)  A[0]:(0.468690216541) A[1]:(0.999991714954) A[2]:(0.936059892178) A[3]:(0.729889750481)\n",
      " state (14)  A[0]:(0.517526388168) A[1]:(0.999998629093) A[2]:(0.974914193153) A[3]:(0.849891364574)\n",
      " state (15)  A[0]:(0.57028055191) A[1]:(0.999999701977) A[2]:(0.989753782749) A[3]:(0.915383458138)\n",
      "Episode 53000 finished after 0 timesteps with r=0.0. Running score: 0.07. Times trained:               8604. Times reached goal: 89.               Steps done: 399544. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.603563807075.\n",
      " state (0)  A[0]:(0.557459592819) A[1]:(0.623087286949) A[2]:(0.525615572929) A[3]:(0.561420023441)\n",
      " state (1)  A[0]:(0.576706290245) A[1]:(0.0275475997478) A[2]:(0.44736790657) A[3]:(0.462351858616)\n",
      " state (2)  A[0]:(0.523431658745) A[1]:(0.162684068084) A[2]:(0.531675934792) A[3]:(0.505038380623)\n",
      " state (3)  A[0]:(0.401420325041) A[1]:(0.322818338871) A[2]:(0.543628811836) A[3]:(0.55654001236)\n",
      " state (4)  A[0]:(0.6022554636) A[1]:(0.700935006142) A[2]:(-0.00260272040032) A[3]:(0.554456114769)\n",
      " state (5)  A[0]:(0.431545346975) A[1]:(0.738639116287) A[2]:(-0.156846866012) A[3]:(0.585257411003)\n",
      " state (6)  A[0]:(0.278248488903) A[1]:(0.565859675407) A[2]:(0.139321625233) A[3]:(0.602288782597)\n",
      " state (7)  A[0]:(0.549864172935) A[1]:(-0.00317256571725) A[2]:(0.610693156719) A[3]:(0.565404176712)\n",
      " state (8)  A[0]:(0.710526764393) A[1]:(0.0410535149276) A[2]:(0.774449706078) A[3]:(0.467062920332)\n",
      " state (9)  A[0]:(0.659384846687) A[1]:(0.83746355772) A[2]:(0.724870622158) A[3]:(0.344265341759)\n",
      " state (10)  A[0]:(0.535127043724) A[1]:(0.99184268713) A[2]:(0.654669702053) A[3]:(0.297397434711)\n",
      " state (11)  A[0]:(0.442144572735) A[1]:(0.99953609705) A[2]:(0.703179597855) A[3]:(0.38996130228)\n",
      " state (12)  A[0]:(0.421363413334) A[1]:(0.999961018562) A[2]:(0.834146022797) A[3]:(0.578163146973)\n",
      " state (13)  A[0]:(0.455520033836) A[1]:(0.999995350838) A[2]:(0.932665586472) A[3]:(0.758347570896)\n",
      " state (14)  A[0]:(0.514031767845) A[1]:(0.999999284744) A[2]:(0.97583013773) A[3]:(0.873073458672)\n",
      " state (15)  A[0]:(0.575343132019) A[1]:(0.999999821186) A[2]:(0.990917026997) A[3]:(0.931862175465)\n",
      "Episode 54000 finished after 0 timesteps with r=0.0. Running score: 0.15. Times trained:               8732. Times reached goal: 102.               Steps done: 408276. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.598316431296.\n",
      "q_values \n",
      "tensor([[ 0.5694,  0.6317,  0.5256,  0.5718]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6077,  0.6995, -0.0028,  0.5623]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7118,  0.0364,  0.7798,  0.4611]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6488,  0.8438,  0.7105,  0.3194]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4043,  1.0000,  0.9265,  0.7495]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4044,  1.0000,  0.9263,  0.7493]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6488,  0.8436,  0.7095,  0.3179]], device='cuda:0')\n",
      "On state=9, selected action=3 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.569396853447) A[1]:(0.631092309952) A[2]:(0.525261104107) A[3]:(0.571670353413)\n",
      " state (1)  A[0]:(0.584793925285) A[1]:(0.0209199059755) A[2]:(0.448062866926) A[3]:(0.469963490963)\n",
      " state (2)  A[0]:(0.532133758068) A[1]:(0.167379751801) A[2]:(0.536076188087) A[3]:(0.512121975422)\n",
      " state (3)  A[0]:(0.405976116657) A[1]:(0.314911782742) A[2]:(0.548026502132) A[3]:(0.559891700745)\n",
      " state (4)  A[0]:(0.607479989529) A[1]:(0.698719739914) A[2]:(-0.00345033523627) A[3]:(0.561751246452)\n",
      " state (5)  A[0]:(0.416874527931) A[1]:(0.736692845821) A[2]:(-0.168825790286) A[3]:(0.588959991932)\n",
      " state (6)  A[0]:(0.238403126597) A[1]:(0.593848466873) A[2]:(0.102681159973) A[3]:(0.601169705391)\n",
      " state (7)  A[0]:(0.529044389725) A[1]:(0.0463022664189) A[2]:(0.598562002182) A[3]:(0.563583076)\n",
      " state (8)  A[0]:(0.711841702461) A[1]:(0.035749334842) A[2]:(0.778936803341) A[3]:(0.459228843451)\n",
      " state (9)  A[0]:(0.648622632027) A[1]:(0.843356966972) A[2]:(0.708939790726) A[3]:(0.317072093487)\n",
      " state (10)  A[0]:(0.494779795408) A[1]:(0.993000030518) A[2]:(0.5999532938) A[3]:(0.251233369112)\n",
      " state (11)  A[0]:(0.380107045174) A[1]:(0.999634563923) A[2]:(0.641666412354) A[3]:(0.342181712389)\n",
      " state (12)  A[0]:(0.357755571604) A[1]:(0.999970853329) A[2]:(0.80319583416) A[3]:(0.547748863697)\n",
      " state (13)  A[0]:(0.404097765684) A[1]:(0.99999666214) A[2]:(0.92602866888) A[3]:(0.74837064743)\n",
      " state (14)  A[0]:(0.479114055634) A[1]:(0.999999463558) A[2]:(0.976151823997) A[3]:(0.873632490635)\n",
      " state (15)  A[0]:(0.555061936378) A[1]:(0.999999880791) A[2]:(0.99193161726) A[3]:(0.935194671154)\n",
      "Episode 55000 finished after 0 timesteps with r=0.0. Running score: 0.11. Times trained:               8696. Times reached goal: 102.               Steps done: 416972. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.593136028645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.578439891338) A[1]:(0.647041916847) A[2]:(0.535383403301) A[3]:(0.578995764256)\n",
      " state (1)  A[0]:(0.593171000481) A[1]:(0.0214187521487) A[2]:(0.456351608038) A[3]:(0.478124022484)\n",
      " state (2)  A[0]:(0.54394620657) A[1]:(0.187820702791) A[2]:(0.54787927866) A[3]:(0.522284150124)\n",
      " state (3)  A[0]:(0.42114558816) A[1]:(0.329274386168) A[2]:(0.558208227158) A[3]:(0.567956089973)\n",
      " state (4)  A[0]:(0.630087852478) A[1]:(0.71489906311) A[2]:(-0.00287934346125) A[3]:(0.57431614399)\n",
      " state (5)  A[0]:(0.432530343533) A[1]:(0.7467045784) A[2]:(-0.175465449691) A[3]:(0.600596904755)\n",
      " state (6)  A[0]:(0.247104883194) A[1]:(0.617995023727) A[2]:(0.0829739645123) A[3]:(0.612111330032)\n",
      " state (7)  A[0]:(0.556315779686) A[1]:(0.0653860345483) A[2]:(0.60477656126) A[3]:(0.579532146454)\n",
      " state (8)  A[0]:(0.745288550854) A[1]:(0.0107208555564) A[2]:(0.793797492981) A[3]:(0.480658769608)\n",
      " state (9)  A[0]:(0.673237323761) A[1]:(0.847792506218) A[2]:(0.703991174698) A[3]:(0.3345066607)\n",
      " state (10)  A[0]:(0.495130956173) A[1]:(0.994141042233) A[2]:(0.548795223236) A[3]:(0.258024871349)\n",
      " state (11)  A[0]:(0.357760101557) A[1]:(0.99973064661) A[2]:(0.571988224983) A[3]:(0.345408290625)\n",
      " state (12)  A[0]:(0.331664055586) A[1]:(0.999980390072) A[2]:(0.764241337776) A[3]:(0.557408571243)\n",
      " state (13)  A[0]:(0.389143168926) A[1]:(0.999997913837) A[2]:(0.916904389858) A[3]:(0.763391852379)\n",
      " state (14)  A[0]:(0.47988307476) A[1]:(0.999999701977) A[2]:(0.975850999355) A[3]:(0.887271285057)\n",
      " state (15)  A[0]:(0.56892824173) A[1]:(0.999999940395) A[2]:(0.992645442486) A[3]:(0.945033729076)\n",
      "Episode 56000 finished after 0 timesteps with r=0.0. Running score: 0.05. Times trained:               8461. Times reached goal: 102.               Steps done: 425433. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.588138675821.\n",
      " state (0)  A[0]:(0.584055662155) A[1]:(0.655559301376) A[2]:(0.533311009407) A[3]:(0.58290886879)\n",
      " state (1)  A[0]:(0.597211122513) A[1]:(0.0232555065304) A[2]:(0.453000515699) A[3]:(0.476375430822)\n",
      " state (2)  A[0]:(0.549373149872) A[1]:(0.190727695823) A[2]:(0.547508835793) A[3]:(0.519595623016)\n",
      " state (3)  A[0]:(0.422838330269) A[1]:(0.319941401482) A[2]:(0.558644473553) A[3]:(0.56378364563)\n",
      " state (4)  A[0]:(0.635056853294) A[1]:(0.722999751568) A[2]:(-0.00340661569498) A[3]:(0.577621102333)\n",
      " state (5)  A[0]:(0.421754211187) A[1]:(0.752503991127) A[2]:(-0.181520149112) A[3]:(0.603732585907)\n",
      " state (6)  A[0]:(0.215765103698) A[1]:(0.65075969696) A[2]:(0.0496963039041) A[3]:(0.612483084202)\n",
      " state (7)  A[0]:(0.539651155472) A[1]:(0.147787094116) A[2]:(0.587955355644) A[3]:(0.578795015812)\n",
      " state (8)  A[0]:(0.755238771439) A[1]:(0.0280372910202) A[2]:(0.803014039993) A[3]:(0.478772759438)\n",
      " state (9)  A[0]:(0.677362084389) A[1]:(0.856225609779) A[2]:(0.704922318459) A[3]:(0.326022565365)\n",
      " state (10)  A[0]:(0.469842940569) A[1]:(0.995170354843) A[2]:(0.512618243694) A[3]:(0.240463897586)\n",
      " state (11)  A[0]:(0.307834357023) A[1]:(0.999803602695) A[2]:(0.521186470985) A[3]:(0.328149646521)\n",
      " state (12)  A[0]:(0.283211261034) A[1]:(0.999986886978) A[2]:(0.742201805115) A[3]:(0.552287220955)\n",
      " state (13)  A[0]:(0.360600203276) A[1]:(0.999998688698) A[2]:(0.917088150978) A[3]:(0.769206047058)\n",
      " state (14)  A[0]:(0.475327074528) A[1]:(0.999999821186) A[2]:(0.978730380535) A[3]:(0.895160913467)\n",
      " state (15)  A[0]:(0.582688629627) A[1]:(0.999999940395) A[2]:(0.994236826897) A[3]:(0.951048433781)\n",
      "Episode 57000 finished after 0 timesteps with r=0.0. Running score: 0.1. Times trained:               8967. Times reached goal: 130.               Steps done: 434400. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.582888411056.\n",
      " state (0)  A[0]:(0.579937696457) A[1]:(0.654147744179) A[2]:(0.538640916348) A[3]:(0.582741379738)\n",
      " state (1)  A[0]:(0.593708634377) A[1]:(0.0263292714953) A[2]:(0.455057263374) A[3]:(0.475530892611)\n",
      " state (2)  A[0]:(0.548502743244) A[1]:(0.199386581779) A[2]:(0.552516877651) A[3]:(0.518404722214)\n",
      " state (3)  A[0]:(0.418546080589) A[1]:(0.315198332071) A[2]:(0.56466114521) A[3]:(0.561265945435)\n",
      " state (4)  A[0]:(0.638212800026) A[1]:(0.728114187717) A[2]:(-0.00418603932485) A[3]:(0.581204652786)\n",
      " state (5)  A[0]:(0.423684030771) A[1]:(0.756004631519) A[2]:(-0.202556014061) A[3]:(0.606776058674)\n",
      " state (6)  A[0]:(0.202542871237) A[1]:(0.670739650726) A[2]:(0.00042104718159) A[3]:(0.61325442791)\n",
      " state (7)  A[0]:(0.533037483692) A[1]:(0.20719730854) A[2]:(0.553673386574) A[3]:(0.576472640038)\n",
      " state (8)  A[0]:(0.769908428192) A[1]:(0.0261743478477) A[2]:(0.801705598831) A[3]:(0.473117947578)\n",
      " state (9)  A[0]:(0.688070595264) A[1]:(0.858449578285) A[2]:(0.69306665659) A[3]:(0.315727174282)\n",
      " state (10)  A[0]:(0.452243953943) A[1]:(0.995836555958) A[2]:(0.454575657845) A[3]:(0.222445547581)\n",
      " state (11)  A[0]:(0.265339046717) A[1]:(0.999850869179) A[2]:(0.442223280668) A[3]:(0.30808070302)\n",
      " state (12)  A[0]:(0.245539307594) A[1]:(0.999990880489) A[2]:(0.701172709465) A[3]:(0.541765630245)\n",
      " state (13)  A[0]:(0.349112689495) A[1]:(0.999999165535) A[2]:(0.911607325077) A[3]:(0.77020418644)\n",
      " state (14)  A[0]:(0.493253409863) A[1]:(0.999999880791) A[2]:(0.980155944824) A[3]:(0.900046467781)\n",
      " state (15)  A[0]:(0.620530724525) A[1]:(1.0) A[2]:(0.995297670364) A[3]:(0.95535081625)\n",
      "Episode 58000 finished after 0 timesteps with r=0.0. Running score: 0.1. Times trained:               9029. Times reached goal: 134.               Steps done: 443429. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.577649199606.\n",
      " state (0)  A[0]:(0.576956987381) A[1]:(0.649174809456) A[2]:(0.536901652813) A[3]:(0.580428361893)\n",
      " state (1)  A[0]:(0.589984297752) A[1]:(0.0201748050749) A[2]:(0.455243229866) A[3]:(0.476728767157)\n",
      " state (2)  A[0]:(0.545861899853) A[1]:(0.196402519941) A[2]:(0.556754946709) A[3]:(0.520787775517)\n",
      " state (3)  A[0]:(0.409220606089) A[1]:(0.301156878471) A[2]:(0.571100234985) A[3]:(0.565101504326)\n",
      " state (4)  A[0]:(0.634581267834) A[1]:(0.727967262268) A[2]:(-0.000377356976969) A[3]:(0.591751217842)\n",
      " state (5)  A[0]:(0.416864842176) A[1]:(0.755120098591) A[2]:(-0.212514087558) A[3]:(0.620205640793)\n",
      " state (6)  A[0]:(0.181753158569) A[1]:(0.686355829239) A[2]:(-0.0321775712073) A[3]:(0.628027677536)\n",
      " state (7)  A[0]:(0.512290358543) A[1]:(0.278047740459) A[2]:(0.524510622025) A[3]:(0.590489566326)\n",
      " state (8)  A[0]:(0.776370346546) A[1]:(0.0376281626523) A[2]:(0.807894051075) A[3]:(0.486837178469)\n",
      " state (9)  A[0]:(0.691973924637) A[1]:(0.860307753086) A[2]:(0.704501926899) A[3]:(0.331501632929)\n",
      " state (10)  A[0]:(0.420306742191) A[1]:(0.996430158615) A[2]:(0.437725871801) A[3]:(0.233632296324)\n",
      " state (11)  A[0]:(0.197254836559) A[1]:(0.999890446663) A[2]:(0.399867892265) A[3]:(0.313500910997)\n",
      " state (12)  A[0]:(0.177183195949) A[1]:(0.999994039536) A[2]:(0.680112659931) A[3]:(0.548192381859)\n",
      " state (13)  A[0]:(0.309288293123) A[1]:(0.999999463558) A[2]:(0.913171291351) A[3]:(0.779512345791)\n",
      " state (14)  A[0]:(0.489490389824) A[1]:(0.999999940395) A[2]:(0.983056724072) A[3]:(0.908199071884)\n",
      " state (15)  A[0]:(0.64193457365) A[1]:(1.0) A[2]:(0.996526956558) A[3]:(0.960853040218)\n",
      "Episode 59000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               8977. Times reached goal: 151.               Steps done: 452406. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.572486848621.\n",
      "q_values \n",
      "tensor([[ 0.5821,  0.6549,  0.5353,  0.5823]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6385,  0.7251,  0.0016,  0.5963]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7871,  0.0324,  0.8078,  0.4839]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7053,  0.8550,  0.7116,  0.3262]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2764,  1.0000,  0.9106,  0.7764]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.581955909729) A[1]:(0.653784394264) A[2]:(0.534951448441) A[3]:(0.581916034222)\n",
      " state (1)  A[0]:(0.594594836235) A[1]:(0.0218537226319) A[2]:(0.450766831636) A[3]:(0.473881393671)\n",
      " state (2)  A[0]:(0.552724838257) A[1]:(0.198235839605) A[2]:(0.555600106716) A[3]:(0.519151389599)\n",
      " state (3)  A[0]:(0.412068933249) A[1]:(0.289123833179) A[2]:(0.57071018219) A[3]:(0.565487146378)\n",
      " state (4)  A[0]:(0.63752746582) A[1]:(0.722553014755) A[2]:(0.00113829923794) A[3]:(0.59539437294)\n",
      " state (5)  A[0]:(0.416604578495) A[1]:(0.74370354414) A[2]:(-0.219867512584) A[3]:(0.627252697945)\n",
      " state (6)  A[0]:(0.171922802925) A[1]:(0.68436384201) A[2]:(-0.0618094727397) A[3]:(0.637084782124)\n",
      " state (7)  A[0]:(0.498114734888) A[1]:(0.321067392826) A[2]:(0.487203150988) A[3]:(0.595286726952)\n",
      " state (8)  A[0]:(0.787360429764) A[1]:(0.0254451539367) A[2]:(0.80801320076) A[3]:(0.482919454575)\n",
      " state (9)  A[0]:(0.705156743526) A[1]:(0.853635549545) A[2]:(0.711264133453) A[3]:(0.325025528669)\n",
      " state (10)  A[0]:(0.399959385395) A[1]:(0.996749222279) A[2]:(0.414727210999) A[3]:(0.221328064799)\n",
      " state (11)  A[0]:(0.135399654508) A[1]:(0.999915957451) A[2]:(0.343437314034) A[3]:(0.294517815113)\n",
      " state (12)  A[0]:(0.112417750061) A[1]:(0.999996006489) A[2]:(0.645140469074) A[3]:(0.533373951912)\n",
      " state (13)  A[0]:(0.275932133198) A[1]:(0.999999701977) A[2]:(0.910370230675) A[3]:(0.776004374027)\n",
      " state (14)  A[0]:(0.497206002474) A[1]:(0.999999940395) A[2]:(0.984825134277) A[3]:(0.910370528698)\n",
      " state (15)  A[0]:(0.675479352474) A[1]:(1.0) A[2]:(0.997338175774) A[3]:(0.963581800461)\n",
      "Episode 60000 finished after 0 timesteps with r=0.0. Running score: 0.15. Times trained:               9341. Times reached goal: 133.               Steps done: 461747. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.567164147347.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.580909430981) A[1]:(0.643170714378) A[2]:(0.535322546959) A[3]:(0.581912398338)\n",
      " state (1)  A[0]:(0.593162834644) A[1]:(0.0143354842439) A[2]:(0.444406986237) A[3]:(0.468845009804)\n",
      " state (2)  A[0]:(0.552972018719) A[1]:(0.181467711926) A[2]:(0.549066185951) A[3]:(0.513915359974)\n",
      " state (3)  A[0]:(0.407023817301) A[1]:(0.260055035353) A[2]:(0.563621520996) A[3]:(0.563369750977)\n",
      " state (4)  A[0]:(0.630739212036) A[1]:(0.712646782398) A[2]:(-0.00239008199424) A[3]:(0.598051190376)\n",
      " state (5)  A[0]:(0.405621021986) A[1]:(0.732385635376) A[2]:(-0.236164793372) A[3]:(0.634834170341)\n",
      " state (6)  A[0]:(0.14990273118) A[1]:(0.679027080536) A[2]:(-0.102281048894) A[3]:(0.646366119385)\n",
      " state (7)  A[0]:(0.471775949001) A[1]:(0.354861021042) A[2]:(0.431859999895) A[3]:(0.596747338772)\n",
      " state (8)  A[0]:(0.787319421768) A[1]:(0.0331547297537) A[2]:(0.791089057922) A[3]:(0.465373694897)\n",
      " state (9)  A[0]:(0.702439188957) A[1]:(0.856837391853) A[2]:(0.688008666039) A[3]:(0.29501965642)\n",
      " state (10)  A[0]:(0.357905238867) A[1]:(0.997235953808) A[2]:(0.344363242388) A[3]:(0.182990327477)\n",
      " state (11)  A[0]:(0.058424603194) A[1]:(0.999938189983) A[2]:(0.244595900178) A[3]:(0.252783119678)\n",
      " state (12)  A[0]:(0.0430632643402) A[1]:(0.999997317791) A[2]:(0.585348248482) A[3]:(0.502294421196)\n",
      " state (13)  A[0]:(0.246910244226) A[1]:(0.999999821186) A[2]:(0.901978969574) A[3]:(0.763978898525)\n",
      " state (14)  A[0]:(0.514297664165) A[1]:(1.0) A[2]:(0.985553026199) A[3]:(0.908823013306)\n",
      " state (15)  A[0]:(0.715644717216) A[1]:(1.0) A[2]:(0.997817695141) A[3]:(0.964525461197)\n",
      "Episode 61000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               8903. Times reached goal: 145.               Steps done: 470650. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.562137096067.\n",
      " state (0)  A[0]:(0.5796636343) A[1]:(0.64055788517) A[2]:(0.530524849892) A[3]:(0.581623196602)\n",
      " state (1)  A[0]:(0.590968370438) A[1]:(0.0206321999431) A[2]:(0.437085300684) A[3]:(0.463160008192)\n",
      " state (2)  A[0]:(0.553621828556) A[1]:(0.166633635759) A[2]:(0.539549469948) A[3]:(0.50661277771)\n",
      " state (3)  A[0]:(0.408375769854) A[1]:(0.244699969888) A[2]:(0.555365085602) A[3]:(0.561432719231)\n",
      " state (4)  A[0]:(0.630341231823) A[1]:(0.712210297585) A[2]:(-0.00129327108152) A[3]:(0.600900053978)\n",
      " state (5)  A[0]:(0.39625492692) A[1]:(0.726791381836) A[2]:(-0.237571641803) A[3]:(0.645321130753)\n",
      " state (6)  A[0]:(0.125815987587) A[1]:(0.676547348499) A[2]:(-0.118037350476) A[3]:(0.662627398968)\n",
      " state (7)  A[0]:(0.440761804581) A[1]:(0.391618102789) A[2]:(0.403468430042) A[3]:(0.612624883652)\n",
      " state (8)  A[0]:(0.788252472878) A[1]:(0.0428457632661) A[2]:(0.793890058994) A[3]:(0.473207443953)\n",
      " state (9)  A[0]:(0.709309101105) A[1]:(0.856630623341) A[2]:(0.704742312431) A[3]:(0.305133163929)\n",
      " state (10)  A[0]:(0.334055066109) A[1]:(0.997622549534) A[2]:(0.345408290625) A[3]:(0.196324810386)\n",
      " state (11)  A[0]:(-0.00578169059008) A[1]:(0.999956190586) A[2]:(0.210653185844) A[3]:(0.263639181852)\n",
      " state (12)  A[0]:(-0.0225827507675) A[1]:(0.999998390675) A[2]:(0.559844613075) A[3]:(0.511781215668)\n",
      " state (13)  A[0]:(0.216875568032) A[1]:(0.999999880791) A[2]:(0.90231359005) A[3]:(0.77331084013)\n",
      " state (14)  A[0]:(0.530414104462) A[1]:(1.0) A[2]:(0.987409591675) A[3]:(0.915664434433)\n",
      " state (15)  A[0]:(0.752199292183) A[1]:(1.0) A[2]:(0.998360991478) A[3]:(0.968557715416)\n",
      "Episode 62000 finished after 0 timesteps with r=1.0. Running score: 0.2. Times trained:               8995. Times reached goal: 144.               Steps done: 479645. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.557103346118.\n",
      " state (0)  A[0]:(0.579611837864) A[1]:(0.64374089241) A[2]:(0.529038190842) A[3]:(0.580839753151)\n",
      " state (1)  A[0]:(0.58963984251) A[1]:(0.0311222262681) A[2]:(0.434876054525) A[3]:(0.454224824905)\n",
      " state (2)  A[0]:(0.556402027607) A[1]:(0.156826913357) A[2]:(0.534123063087) A[3]:(0.494982779026)\n",
      " state (3)  A[0]:(0.415221720934) A[1]:(0.239072009921) A[2]:(0.552138447762) A[3]:(0.556032180786)\n",
      " state (4)  A[0]:(0.631985902786) A[1]:(0.712562203407) A[2]:(0.00106731022242) A[3]:(0.599346876144)\n",
      " state (5)  A[0]:(0.389339447021) A[1]:(0.721635818481) A[2]:(-0.244323790073) A[3]:(0.652868807316)\n",
      " state (6)  A[0]:(0.0974803864956) A[1]:(0.669992446899) A[2]:(-0.140832871199) A[3]:(0.676512122154)\n",
      " state (7)  A[0]:(0.401670128107) A[1]:(0.419645428658) A[2]:(0.362824380398) A[3]:(0.623045086861)\n",
      " state (8)  A[0]:(0.787294685841) A[1]:(0.0505064353347) A[2]:(0.788368463516) A[3]:(0.464074879885)\n",
      " state (9)  A[0]:(0.71928358078) A[1]:(0.855379879475) A[2]:(0.710012674332) A[3]:(0.28728479147)\n",
      " state (10)  A[0]:(0.321170508862) A[1]:(0.997955858707) A[2]:(0.32579356432) A[3]:(0.177376881242)\n",
      " state (11)  A[0]:(-0.0561423823237) A[1]:(0.999969482422) A[2]:(0.150034636259) A[3]:(0.241552516818)\n",
      " state (12)  A[0]:(-0.073688365519) A[1]:(0.999999046326) A[2]:(0.512022733688) A[3]:(0.493330389261)\n",
      " state (13)  A[0]:(0.202635705471) A[1]:(0.999999940395) A[2]:(0.897260963917) A[3]:(0.766313433647)\n",
      " state (14)  A[0]:(0.560760259628) A[1]:(1.0) A[2]:(0.988471627235) A[3]:(0.915397286415)\n",
      " state (15)  A[0]:(0.794340968132) A[1]:(1.0) A[2]:(0.998716890812) A[3]:(0.969570398331)\n",
      "Episode 63000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               9575. Times reached goal: 136.               Steps done: 489220. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.551794538057.\n",
      " state (0)  A[0]:(0.571334600449) A[1]:(0.63944029808) A[2]:(0.524239242077) A[3]:(0.575480937958)\n",
      " state (1)  A[0]:(0.580188393593) A[1]:(0.0363752022386) A[2]:(0.430102437735) A[3]:(0.445693254471)\n",
      " state (2)  A[0]:(0.550592124462) A[1]:(0.148634910583) A[2]:(0.52755177021) A[3]:(0.485785990953)\n",
      " state (3)  A[0]:(0.413196057081) A[1]:(0.240305706859) A[2]:(0.549773454666) A[3]:(0.555307030678)\n",
      " state (4)  A[0]:(0.624507308006) A[1]:(0.709870994091) A[2]:(0.000386118859751) A[3]:(0.603863835335)\n",
      " state (5)  A[0]:(0.370243757963) A[1]:(0.714264035225) A[2]:(-0.259067118168) A[3]:(0.669227898121)\n",
      " state (6)  A[0]:(0.0467557087541) A[1]:(0.65652525425) A[2]:(-0.17027284205) A[3]:(0.702389597893)\n",
      " state (7)  A[0]:(0.336923986673) A[1]:(0.435505867004) A[2]:(0.315909743309) A[3]:(0.652614474297)\n",
      " state (8)  A[0]:(0.777417242527) A[1]:(0.0501440651715) A[2]:(0.782008767128) A[3]:(0.484848350286)\n",
      " state (9)  A[0]:(0.725911200047) A[1]:(0.848663628101) A[2]:(0.72127199173) A[3]:(0.30787524581)\n",
      " state (10)  A[0]:(0.309143275023) A[1]:(0.998157441616) A[2]:(0.32390281558) A[3]:(0.204244509339)\n",
      " state (11)  A[0]:(-0.108464196324) A[1]:(0.999978125095) A[2]:(0.104911155999) A[3]:(0.266406267881)\n",
      " state (12)  A[0]:(-0.13457608223) A[1]:(0.999999403954) A[2]:(0.468023836613) A[3]:(0.511524796486)\n",
      " state (13)  A[0]:(0.171017900109) A[1]:(1.0) A[2]:(0.892537117004) A[3]:(0.777748465538)\n",
      " state (14)  A[0]:(0.57432103157) A[1]:(1.0) A[2]:(0.989428400993) A[3]:(0.921570360661)\n",
      " state (15)  A[0]:(0.821533560753) A[1]:(1.0) A[2]:(0.998991191387) A[3]:(0.972629368305)\n",
      "Episode 64000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               9566. Times reached goal: 132.               Steps done: 498786. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.5465412381.\n",
      "q_values \n",
      "tensor([[ 0.5722,  0.6352,  0.5211,  0.5746]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6232,  0.7006, -0.0009,  0.5977]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.57212972641) A[1]:(0.635288953781) A[2]:(0.5210750103) A[3]:(0.574617028236)\n",
      " state (1)  A[0]:(0.577364742756) A[1]:(0.0285294689238) A[2]:(0.431361377239) A[3]:(0.441203176975)\n",
      " state (2)  A[0]:(0.549185991287) A[1]:(0.139940991998) A[2]:(0.530113220215) A[3]:(0.482022255659)\n",
      " state (3)  A[0]:(0.414505928755) A[1]:(0.237724781036) A[2]:(0.553946554661) A[3]:(0.556367635727)\n",
      " state (4)  A[0]:(0.622920095921) A[1]:(0.700257301331) A[2]:(-0.00137975718826) A[3]:(0.597634315491)\n",
      " state (5)  A[0]:(0.346952259541) A[1]:(0.691176056862) A[2]:(-0.260786741972) A[3]:(0.671257555485)\n",
      " state (6)  A[0]:(-0.00338144064881) A[1]:(0.62672829628) A[2]:(-0.180912971497) A[3]:(0.712314546108)\n",
      " state (7)  A[0]:(0.276953011751) A[1]:(0.424072563648) A[2]:(0.288512051105) A[3]:(0.660544335842)\n",
      " state (8)  A[0]:(0.766527593136) A[1]:(0.042589455843) A[2]:(0.776521086693) A[3]:(0.47036883235)\n",
      " state (9)  A[0]:(0.725500404835) A[1]:(0.849015414715) A[2]:(0.721249163151) A[3]:(0.281761080027)\n",
      " state (10)  A[0]:(0.286395698786) A[1]:(0.998426318169) A[2]:(0.297703564167) A[3]:(0.184583440423)\n",
      " state (11)  A[0]:(-0.158994212747) A[1]:(0.999984562397) A[2]:(0.0465897172689) A[3]:(0.253520905972)\n",
      " state (12)  A[0]:(-0.182755872607) A[1]:(0.999999642372) A[2]:(0.425397276878) A[3]:(0.504740715027)\n",
      " state (13)  A[0]:(0.154464930296) A[1]:(1.0) A[2]:(0.890892267227) A[3]:(0.77669274807)\n",
      " state (14)  A[0]:(0.595768094063) A[1]:(1.0) A[2]:(0.990639269352) A[3]:(0.922498941422)\n",
      " state (15)  A[0]:(0.846916019917) A[1]:(1.0) A[2]:(0.999224960804) A[3]:(0.973406195641)\n",
      "Episode 65000 finished after 0 timesteps with r=0.0. Running score: 0.11. Times trained:               8937. Times reached goal: 119.               Steps done: 507723. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.541678560296.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.568253397942) A[1]:(0.638300776482) A[2]:(0.514403343201) A[3]:(0.572892606258)\n",
      " state (1)  A[0]:(0.572144389153) A[1]:(0.0425761900842) A[2]:(0.426421880722) A[3]:(0.442167669535)\n",
      " state (2)  A[0]:(0.546818614006) A[1]:(0.163975819945) A[2]:(0.530209779739) A[3]:(0.486350566149)\n",
      " state (3)  A[0]:(0.418243587017) A[1]:(0.274276524782) A[2]:(0.560303926468) A[3]:(0.566244781017)\n",
      " state (4)  A[0]:(0.627835631371) A[1]:(0.709971547127) A[2]:(0.00339044816792) A[3]:(0.597933471203)\n",
      " state (5)  A[0]:(0.34709662199) A[1]:(0.695360302925) A[2]:(-0.258652865887) A[3]:(0.680690586567)\n",
      " state (6)  A[0]:(-0.0312451682985) A[1]:(0.624350726604) A[2]:(-0.183196470141) A[3]:(0.732225775719)\n",
      " state (7)  A[0]:(0.234662339091) A[1]:(0.438639044762) A[2]:(0.272299259901) A[3]:(0.683196485043)\n",
      " state (8)  A[0]:(0.769071280956) A[1]:(0.0513895042241) A[2]:(0.778131008148) A[3]:(0.47659727931)\n",
      " state (9)  A[0]:(0.748215675354) A[1]:(0.847958087921) A[2]:(0.735747516155) A[3]:(0.280507028103)\n",
      " state (10)  A[0]:(0.312807112932) A[1]:(0.998629033566) A[2]:(0.301008105278) A[3]:(0.193657830358)\n",
      " state (11)  A[0]:(-0.168093889952) A[1]:(0.999989211559) A[2]:(0.00531984306872) A[3]:(0.266705513)\n",
      " state (12)  A[0]:(-0.205957710743) A[1]:(0.999999821186) A[2]:(0.378800392151) A[3]:(0.513836801052)\n",
      " state (13)  A[0]:(0.149773702025) A[1]:(1.0) A[2]:(0.886090874672) A[3]:(0.780967473984)\n",
      " state (14)  A[0]:(0.619743824005) A[1]:(1.0) A[2]:(0.991444051266) A[3]:(0.924509584904)\n",
      " state (15)  A[0]:(0.869953632355) A[1]:(1.0) A[2]:(0.999395191669) A[3]:(0.974400281906)\n",
      "Episode 66000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained:               9694. Times reached goal: 135.               Steps done: 517417. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.53645289804.\n",
      " state (0)  A[0]:(0.569463372231) A[1]:(0.629945158958) A[2]:(0.512044072151) A[3]:(0.569766283035)\n",
      " state (1)  A[0]:(0.569844603539) A[1]:(0.0343587994576) A[2]:(0.427031099796) A[3]:(0.44049423933)\n",
      " state (2)  A[0]:(0.545958638191) A[1]:(0.180176407099) A[2]:(0.538574337959) A[3]:(0.49059727788)\n",
      " state (3)  A[0]:(0.418985158205) A[1]:(0.29869940877) A[2]:(0.573029637337) A[3]:(0.574657201767)\n",
      " state (4)  A[0]:(0.624020636082) A[1]:(0.704125761986) A[2]:(0.00510205375031) A[3]:(0.591000556946)\n",
      " state (5)  A[0]:(0.334037214518) A[1]:(0.685097813606) A[2]:(-0.262182086706) A[3]:(0.680931448936)\n",
      " state (6)  A[0]:(-0.0719217434525) A[1]:(0.611921966076) A[2]:(-0.190491855145) A[3]:(0.74311619997)\n",
      " state (7)  A[0]:(0.174457252026) A[1]:(0.445496559143) A[2]:(0.249039188027) A[3]:(0.696741580963)\n",
      " state (8)  A[0]:(0.758806109428) A[1]:(0.0688272938132) A[2]:(0.772279202938) A[3]:(0.472376137972)\n",
      " state (9)  A[0]:(0.752294898033) A[1]:(0.85474550724) A[2]:(0.735509634018) A[3]:(0.266550987959)\n",
      " state (10)  A[0]:(0.303229779005) A[1]:(0.998877465725) A[2]:(0.274393379688) A[3]:(0.193253576756)\n",
      " state (11)  A[0]:(-0.204956948757) A[1]:(0.999992668629) A[2]:(-0.0570067130029) A[3]:(0.276471525431)\n",
      " state (12)  A[0]:(-0.244857028127) A[1]:(0.999999880791) A[2]:(0.323316812515) A[3]:(0.522712409496)\n",
      " state (13)  A[0]:(0.136181384325) A[1]:(1.0) A[2]:(0.881166577339) A[3]:(0.784642219543)\n",
      " state (14)  A[0]:(0.639580130577) A[1]:(1.0) A[2]:(0.992232322693) A[3]:(0.925625920296)\n",
      " state (15)  A[0]:(0.889004409313) A[1]:(1.0) A[2]:(0.999531507492) A[3]:(0.974786937237)\n",
      "Episode 67000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               9457. Times reached goal: 127.               Steps done: 526874. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.531403576333.\n",
      " state (0)  A[0]:(0.568270266056) A[1]:(0.629452526569) A[2]:(0.509287714958) A[3]:(0.571956932545)\n",
      " state (1)  A[0]:(0.567717671394) A[1]:(0.0251976549625) A[2]:(0.436907947063) A[3]:(0.447775781155)\n",
      " state (2)  A[0]:(0.544102430344) A[1]:(0.203823417425) A[2]:(0.557498276234) A[3]:(0.506420791149)\n",
      " state (3)  A[0]:(0.420753002167) A[1]:(0.325924307108) A[2]:(0.590291798115) A[3]:(0.592441797256)\n",
      " state (4)  A[0]:(0.628932595253) A[1]:(0.699919819832) A[2]:(0.0010063048685) A[3]:(0.592379331589)\n",
      " state (5)  A[0]:(0.312842905521) A[1]:(0.659124195576) A[2]:(-0.249300926924) A[3]:(0.691670775414)\n",
      " state (6)  A[0]:(-0.116289362311) A[1]:(0.579213917255) A[2]:(-0.178244963288) A[3]:(0.762169182301)\n",
      " state (7)  A[0]:(0.112881287932) A[1]:(0.426566570997) A[2]:(0.242459222674) A[3]:(0.719833254814)\n",
      " state (8)  A[0]:(0.749993681908) A[1]:(0.0582824088633) A[2]:(0.770360469818) A[3]:(0.481545597315)\n",
      " state (9)  A[0]:(0.759830832481) A[1]:(0.852931201458) A[2]:(0.739323616028) A[3]:(0.262869536877)\n",
      " state (10)  A[0]:(0.2989769876) A[1]:(0.999011993408) A[2]:(0.257998943329) A[3]:(0.200537592173)\n",
      " state (11)  A[0]:(-0.239734053612) A[1]:(0.999994575977) A[2]:(-0.104200206697) A[3]:(0.292305827141)\n",
      " state (12)  A[0]:(-0.285421907902) A[1]:(0.999999940395) A[2]:(0.282819807529) A[3]:(0.535481989384)\n",
      " state (13)  A[0]:(0.115591540933) A[1]:(1.0) A[2]:(0.881059229374) A[3]:(0.789658844471)\n",
      " state (14)  A[0]:(0.651796102524) A[1]:(1.0) A[2]:(0.993273973465) A[3]:(0.926811277866)\n",
      " state (15)  A[0]:(0.902250051498) A[1]:(1.0) A[2]:(0.999653577805) A[3]:(0.975002348423)\n",
      "Episode 68000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               9519. Times reached goal: 128.               Steps done: 536393. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.52636914508.\n",
      " state (0)  A[0]:(0.565286040306) A[1]:(0.630962669849) A[2]:(0.508058965206) A[3]:(0.567905962467)\n",
      " state (1)  A[0]:(0.565301835537) A[1]:(0.0281384792179) A[2]:(0.443797439337) A[3]:(0.445939540863)\n",
      " state (2)  A[0]:(0.542191147804) A[1]:(0.218472033739) A[2]:(0.567554593086) A[3]:(0.508608579636)\n",
      " state (3)  A[0]:(0.423082768917) A[1]:(0.341750979424) A[2]:(0.600291728973) A[3]:(0.596633195877)\n",
      " state (4)  A[0]:(0.626979589462) A[1]:(0.695440530777) A[2]:(0.00403393106535) A[3]:(0.582869529724)\n",
      " state (5)  A[0]:(0.296795904636) A[1]:(0.642907559872) A[2]:(-0.235995739698) A[3]:(0.692351579666)\n",
      " state (6)  A[0]:(-0.15301206708) A[1]:(0.552748978138) A[2]:(-0.166353464127) A[3]:(0.772598624229)\n",
      " state (7)  A[0]:(0.056302882731) A[1]:(0.41090425849) A[2]:(0.238463684916) A[3]:(0.734275102615)\n",
      " state (8)  A[0]:(0.744644284248) A[1]:(0.0558656528592) A[2]:(0.772317230701) A[3]:(0.479664087296)\n",
      " state (9)  A[0]:(0.774943351746) A[1]:(0.851271748543) A[2]:(0.751416623592) A[3]:(0.24407030642)\n",
      " state (10)  A[0]:(0.318463295698) A[1]:(0.999113023281) A[2]:(0.263421833515) A[3]:(0.193153426051)\n",
      " state (11)  A[0]:(-0.248533427715) A[1]:(0.99999588728) A[2]:(-0.129477858543) A[3]:(0.295518189669)\n",
      " state (12)  A[0]:(-0.303134232759) A[1]:(0.999999940395) A[2]:(0.25681000948) A[3]:(0.538645148277)\n",
      " state (13)  A[0]:(0.114239402115) A[1]:(1.0) A[2]:(0.883130669594) A[3]:(0.789229989052)\n",
      " state (14)  A[0]:(0.671563088894) A[1]:(1.0) A[2]:(0.994204163551) A[3]:(0.925642848015)\n",
      " state (15)  A[0]:(0.915430366993) A[1]:(1.0) A[2]:(0.999741077423) A[3]:(0.974247992039)\n",
      "Episode 69000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained:               9177. Times reached goal: 141.               Steps done: 545570. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.521560752491.\n",
      "q_values \n",
      "tensor([[ 0.5661,  0.6252,  0.5078,  0.5668]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5655,  0.0373,  0.4451,  0.4510]], device='cuda:0')\n",
      "On state=1, selected action=1 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.566388726234) A[1]:(0.625763237476) A[2]:(0.508117735386) A[3]:(0.566649317741)\n",
      " state (1)  A[0]:(0.565673470497) A[1]:(0.0380290262401) A[2]:(0.445398181677) A[3]:(0.45088237524)\n",
      " state (2)  A[0]:(0.544174134731) A[1]:(0.243366390467) A[2]:(0.573531150818) A[3]:(0.517740368843)\n",
      " state (3)  A[0]:(0.432524889708) A[1]:(0.365302234888) A[2]:(0.607547700405) A[3]:(0.605953633785)\n",
      " state (4)  A[0]:(0.629851698875) A[1]:(0.701889276505) A[2]:(0.0106830708683) A[3]:(0.581400990486)\n",
      " state (5)  A[0]:(0.292021930218) A[1]:(0.639913678169) A[2]:(-0.214397251606) A[3]:(0.700085997581)\n",
      " state (6)  A[0]:(-0.17869900167) A[1]:(0.533011317253) A[2]:(-0.142941564322) A[3]:(0.788652777672)\n",
      " state (7)  A[0]:(0.010117274709) A[1]:(0.390603333712) A[2]:(0.251957327127) A[3]:(0.755537867546)\n",
      " state (8)  A[0]:(0.742921352386) A[1]:(0.0538889355958) A[2]:(0.782759308815) A[3]:(0.490296691656)\n",
      " state (9)  A[0]:(0.787951290607) A[1]:(0.855500817299) A[2]:(0.766709387302) A[3]:(0.242925196886)\n",
      " state (10)  A[0]:(0.328528702259) A[1]:(0.99924737215) A[2]:(0.266629606485) A[3]:(0.209631949663)\n",
      " state (11)  A[0]:(-0.266220360994) A[1]:(0.999997019768) A[2]:(-0.158986106515) A[3]:(0.326378256083)\n",
      " state (12)  A[0]:(-0.325343668461) A[1]:(0.999999940395) A[2]:(0.230139046907) A[3]:(0.564669430256)\n",
      " state (13)  A[0]:(0.111709646881) A[1]:(1.0) A[2]:(0.886418819427) A[3]:(0.800394535065)\n",
      " state (14)  A[0]:(0.691367089748) A[1]:(1.0) A[2]:(0.995126187801) A[3]:(0.928378641605)\n",
      " state (15)  A[0]:(0.927442967892) A[1]:(1.0) A[2]:(0.999813258648) A[3]:(0.974663376808)\n",
      "Episode 70000 finished after 0 timesteps with r=0.0. Running score: 0.21. Times trained:               9851. Times reached goal: 144.               Steps done: 555421. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.516448081326.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.569467544556) A[1]:(0.635952472687) A[2]:(0.511974453926) A[3]:(0.569501519203)\n",
      " state (1)  A[0]:(0.567012667656) A[1]:(0.0471893288195) A[2]:(0.443266272545) A[3]:(0.450643658638)\n",
      " state (2)  A[0]:(0.547651648521) A[1]:(0.258923143148) A[2]:(0.573521494865) A[3]:(0.518936872482)\n",
      " state (3)  A[0]:(0.445154041052) A[1]:(0.374768048525) A[2]:(0.608814835548) A[3]:(0.606785655022)\n",
      " state (4)  A[0]:(0.636514544487) A[1]:(0.703498601913) A[2]:(0.0107540516183) A[3]:(0.58118891716)\n",
      " state (5)  A[0]:(0.308497816324) A[1]:(0.635656118393) A[2]:(-0.207973986864) A[3]:(0.70711517334)\n",
      " state (6)  A[0]:(-0.183550819755) A[1]:(0.506182312965) A[2]:(-0.139154374599) A[3]:(0.802811861038)\n",
      " state (7)  A[0]:(-0.0378576926887) A[1]:(0.367976009846) A[2]:(0.232854917645) A[3]:(0.776755750179)\n",
      " state (8)  A[0]:(0.731706142426) A[1]:(0.0596773549914) A[2]:(0.775660693645) A[3]:(0.503770112991)\n",
      " state (9)  A[0]:(0.797761797905) A[1]:(0.855285823345) A[2]:(0.768809735775) A[3]:(0.23162740469)\n",
      " state (10)  A[0]:(0.333350718021) A[1]:(0.999313175678) A[2]:(0.244835674763) A[3]:(0.20769636333)\n",
      " state (11)  A[0]:(-0.292932659388) A[1]:(0.99999755621) A[2]:(-0.213730320334) A[3]:(0.335578858852)\n",
      " state (12)  A[0]:(-0.361131995916) A[1]:(1.0) A[2]:(0.174656122923) A[3]:(0.571193516254)\n",
      " state (13)  A[0]:(0.0853403359652) A[1]:(1.0) A[2]:(0.881886959076) A[3]:(0.799858391285)\n",
      " state (14)  A[0]:(0.693513154984) A[1]:(1.0) A[2]:(0.995537936687) A[3]:(0.926138341427)\n",
      " state (15)  A[0]:(0.932778954506) A[1]:(1.0) A[2]:(0.999852120876) A[3]:(0.973134934902)\n",
      "Episode 71000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               9340. Times reached goal: 130.               Steps done: 564761. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.511646912607.\n",
      " state (0)  A[0]:(0.574120402336) A[1]:(0.633070707321) A[2]:(0.50771945715) A[3]:(0.575588107109)\n",
      " state (1)  A[0]:(0.569828391075) A[1]:(0.0349382348359) A[2]:(0.432020157576) A[3]:(0.457676827908)\n",
      " state (2)  A[0]:(0.553348302841) A[1]:(0.244841843843) A[2]:(0.564660429955) A[3]:(0.523543775082)\n",
      " state (3)  A[0]:(0.460995286703) A[1]:(0.356310933828) A[2]:(0.603964269161) A[3]:(0.607974350452)\n",
      " state (4)  A[0]:(0.643229842186) A[1]:(0.696959018707) A[2]:(0.006870072335) A[3]:(0.578754067421)\n",
      " state (5)  A[0]:(0.333187073469) A[1]:(0.627353072166) A[2]:(-0.20162832737) A[3]:(0.707966804504)\n",
      " state (6)  A[0]:(-0.173561528325) A[1]:(0.470851570368) A[2]:(-0.127821773291) A[3]:(0.81069457531)\n",
      " state (7)  A[0]:(-0.0714949592948) A[1]:(0.330932408571) A[2]:(0.225740239024) A[3]:(0.791467905045)\n",
      " state (8)  A[0]:(0.726012468338) A[1]:(0.0560237653553) A[2]:(0.774529099464) A[3]:(0.509927272797)\n",
      " state (9)  A[0]:(0.814311206341) A[1]:(0.853193700314) A[2]:(0.77760964632) A[3]:(0.209529608488)\n",
      " state (10)  A[0]:(0.357885599136) A[1]:(0.999362826347) A[2]:(0.231068819761) A[3]:(0.19840374589)\n",
      " state (11)  A[0]:(-0.299520581961) A[1]:(0.999997973442) A[2]:(-0.267077088356) A[3]:(0.341830044985)\n",
      " state (12)  A[0]:(-0.377435207367) A[1]:(1.0) A[2]:(0.113899856806) A[3]:(0.576307892799)\n",
      " state (13)  A[0]:(0.0802859216928) A[1]:(1.0) A[2]:(0.876006066799) A[3]:(0.797913551331)\n",
      " state (14)  A[0]:(0.706424593925) A[1]:(1.0) A[2]:(0.99588406086) A[3]:(0.922720849514)\n",
      " state (15)  A[0]:(0.940198779106) A[1]:(1.0) A[2]:(0.999882519245) A[3]:(0.970859766006)\n",
      "Episode 72000 finished after 0 timesteps with r=0.0. Running score: 0.15. Times trained:               9765. Times reached goal: 135.               Steps done: 574526. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.506674995399.\n",
      " state (0)  A[0]:(0.566068410873) A[1]:(0.626465320587) A[2]:(0.502162337303) A[3]:(0.567032933235)\n",
      " state (1)  A[0]:(0.558917999268) A[1]:(0.0380604490638) A[2]:(0.410058319569) A[3]:(0.443059682846)\n",
      " state (2)  A[0]:(0.542964696884) A[1]:(0.226270720363) A[2]:(0.540979504585) A[3]:(0.504767179489)\n",
      " state (3)  A[0]:(0.456237345934) A[1]:(0.324162334204) A[2]:(0.58496773243) A[3]:(0.587741971016)\n",
      " state (4)  A[0]:(0.635132014751) A[1]:(0.696433782578) A[2]:(0.00810781214386) A[3]:(0.561517357826)\n",
      " state (5)  A[0]:(0.340338438749) A[1]:(0.628042817116) A[2]:(-0.187696665525) A[3]:(0.696947991848)\n",
      " state (6)  A[0]:(-0.177860885859) A[1]:(0.434713661671) A[2]:(-0.108592219651) A[3]:(0.8093110919)\n",
      " state (7)  A[0]:(-0.110613875091) A[1]:(0.278790712357) A[2]:(0.231863304973) A[3]:(0.796283721924)\n",
      " state (8)  A[0]:(0.717029154301) A[1]:(0.0527690574527) A[2]:(0.778214335442) A[3]:(0.50675535202)\n",
      " state (9)  A[0]:(0.819698989391) A[1]:(0.863681912422) A[2]:(0.781955361366) A[3]:(0.191356927156)\n",
      " state (10)  A[0]:(0.34319037199) A[1]:(0.999478280544) A[2]:(0.197327688336) A[3]:(0.205152705312)\n",
      " state (11)  A[0]:(-0.340017914772) A[1]:(0.999998450279) A[2]:(-0.324972450733) A[3]:(0.370760887861)\n",
      " state (12)  A[0]:(-0.412478536367) A[1]:(1.0) A[2]:(0.0688078403473) A[3]:(0.601618170738)\n",
      " state (13)  A[0]:(0.0647199004889) A[1]:(1.0) A[2]:(0.878111422062) A[3]:(0.807387173176)\n",
      " state (14)  A[0]:(0.716536700726) A[1]:(1.0) A[2]:(0.996518909931) A[3]:(0.923580884933)\n",
      " state (15)  A[0]:(0.946285545826) A[1]:(1.0) A[2]:(0.999914109707) A[3]:(0.969854950905)\n",
      "Episode 73000 finished after 0 timesteps with r=0.0. Running score: 0.13. Times trained:               10063. Times reached goal: 161.               Steps done: 584589. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.501601893045.\n",
      " state (0)  A[0]:(0.574222445488) A[1]:(0.630990684032) A[2]:(0.509941339493) A[3]:(0.574294924736)\n",
      " state (1)  A[0]:(0.566449522972) A[1]:(0.048244189471) A[2]:(0.410562604666) A[3]:(0.452855348587)\n",
      " state (2)  A[0]:(0.551016688347) A[1]:(0.211135432124) A[2]:(0.534297645092) A[3]:(0.506385982037)\n",
      " state (3)  A[0]:(0.469630002975) A[1]:(0.29191339016) A[2]:(0.580157279968) A[3]:(0.584379673004)\n",
      " state (4)  A[0]:(0.640807628632) A[1]:(0.696426510811) A[2]:(0.00782840326428) A[3]:(0.568988144398)\n",
      " state (5)  A[0]:(0.37537547946) A[1]:(0.637437939644) A[2]:(-0.194012746215) A[3]:(0.701596498489)\n",
      " state (6)  A[0]:(-0.147312775254) A[1]:(0.410553753376) A[2]:(-0.108887866139) A[3]:(0.815959870815)\n",
      " state (7)  A[0]:(-0.116509199142) A[1]:(0.234498172998) A[2]:(0.221056833863) A[3]:(0.808578789234)\n",
      " state (8)  A[0]:(0.721997618675) A[1]:(0.0441951565444) A[2]:(0.776990830898) A[3]:(0.518405139446)\n",
      " state (9)  A[0]:(0.838686704636) A[1]:(0.864896893501) A[2]:(0.789066016674) A[3]:(0.183164834976)\n",
      " state (10)  A[0]:(0.371840775013) A[1]:(0.999522387981) A[2]:(0.178269773722) A[3]:(0.215863987803)\n",
      " state (11)  A[0]:(-0.344188183546) A[1]:(0.999998688698) A[2]:(-0.374311178923) A[3]:(0.398052692413)\n",
      " state (12)  A[0]:(-0.42474424839) A[1]:(1.0) A[2]:(0.0157596822828) A[3]:(0.621376752853)\n",
      " state (13)  A[0]:(0.0638678818941) A[1]:(1.0) A[2]:(0.875463783741) A[3]:(0.811847090721)\n",
      " state (14)  A[0]:(0.729372739792) A[1]:(1.0) A[2]:(0.99688410759) A[3]:(0.921489715576)\n",
      " state (15)  A[0]:(0.951883435249) A[1]:(1.0) A[2]:(0.999933123589) A[3]:(0.96725165844)\n",
      "Episode 74000 finished after 0 timesteps with r=0.0. Running score: 0.23. Times trained:               9749. Times reached goal: 178.               Steps done: 594338. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.496735535791.\n",
      "q_values \n",
      "tensor([[ 0.5648,  0.6351,  0.5044,  0.5666]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5581,  0.0669,  0.4091,  0.4517]], device='cuda:0')\n",
      "On state=1, selected action=0 , Random? False\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5649,  0.6359,  0.5046,  0.5668]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5649,  0.6359,  0.5046,  0.5668]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6432,  0.7096,  0.0126,  0.5562]], device='cuda:0')\n",
      "On state=4, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5649,  0.6357,  0.5043,  0.5667]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6434,  0.7096,  0.0124,  0.5560]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6434,  0.7093,  0.0122,  0.5558]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7238,  0.0614,  0.7836,  0.5209]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8554,  0.8709,  0.8061,  0.1724]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4013,  0.9996,  0.1912,  0.2294]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7457,  1.0000,  0.9974,  0.9178]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4026,  0.9996,  0.1919,  0.2275]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7444,  1.0000,  0.9974,  0.9173]], device='cuda:0')\n",
      "On state=14, selected action=0 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0670,  1.0000,  0.8795,  0.8141]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0657,  1.0000,  0.8789,  0.8138]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.563938140869) A[1]:(0.632903933525) A[2]:(0.502009272575) A[3]:(0.565708994865)\n",
      " state (1)  A[0]:(0.557343184948) A[1]:(0.0622952505946) A[2]:(0.406349927187) A[3]:(0.449916243553)\n",
      " state (2)  A[0]:(0.544122755527) A[1]:(0.202547594905) A[2]:(0.522747814655) A[3]:(0.493070632219)\n",
      " state (3)  A[0]:(0.471107244492) A[1]:(0.271790295839) A[2]:(0.571859419346) A[3]:(0.563447237015)\n",
      " state (4)  A[0]:(0.641377925873) A[1]:(0.703910112381) A[2]:(0.00804046355188) A[3]:(0.55363881588)\n",
      " state (5)  A[0]:(0.415572285652) A[1]:(0.66837644577) A[2]:(-0.204623281956) A[3]:(0.683790802956)\n",
      " state (6)  A[0]:(-0.10727981478) A[1]:(0.418483048677) A[2]:(-0.106952011585) A[3]:(0.807264387608)\n",
      " state (7)  A[0]:(-0.121395274997) A[1]:(0.208804741502) A[2]:(0.219812169671) A[3]:(0.808082580566)\n",
      " state (8)  A[0]:(0.724531531334) A[1]:(0.0380644984543) A[2]:(0.78278619051) A[3]:(0.516270399094)\n",
      " state (9)  A[0]:(0.855964004993) A[1]:(0.864221155643) A[2]:(0.805895924568) A[3]:(0.166759088635)\n",
      " state (10)  A[0]:(0.402400225401) A[1]:(0.999549388885) A[2]:(0.189256697893) A[3]:(0.224184542894)\n",
      " state (11)  A[0]:(-0.34564268589) A[1]:(0.999998807907) A[2]:(-0.396843791008) A[3]:(0.424657464027)\n",
      " state (12)  A[0]:(-0.435027062893) A[1]:(1.0) A[2]:(-0.0107845952734) A[3]:(0.638980984688)\n",
      " state (13)  A[0]:(0.064463019371) A[1]:(1.0) A[2]:(0.878329634666) A[3]:(0.813395321369)\n",
      " state (14)  A[0]:(0.742404460907) A[1]:(1.0) A[2]:(0.99733799696) A[3]:(0.916811525822)\n",
      " state (15)  A[0]:(0.957113087177) A[1]:(1.0) A[2]:(0.999950408936) A[3]:(0.962739825249)\n",
      "Episode 75000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained:               10388. Times reached goal: 157.               Steps done: 604726. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.491602155982.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.569539308548) A[1]:(0.634101748466) A[2]:(0.502992630005) A[3]:(0.571620583534)\n",
      " state (1)  A[0]:(0.561627745628) A[1]:(0.0711472555995) A[2]:(0.399710506201) A[3]:(0.461227029562)\n",
      " state (2)  A[0]:(0.548884034157) A[1]:(0.18983630836) A[2]:(0.507482171059) A[3]:(0.493973821402)\n",
      " state (3)  A[0]:(0.479847759008) A[1]:(0.243985041976) A[2]:(0.557924628258) A[3]:(0.556235551834)\n",
      " state (4)  A[0]:(0.637114882469) A[1]:(0.696554303169) A[2]:(0.0050295763649) A[3]:(0.557944893837)\n",
      " state (5)  A[0]:(0.435440301895) A[1]:(0.679250478745) A[2]:(-0.220734968781) A[3]:(0.683219254017)\n",
      " state (6)  A[0]:(-0.090304158628) A[1]:(0.405254989862) A[2]:(-0.112555235624) A[3]:(0.808423995972)\n",
      " state (7)  A[0]:(-0.14866925776) A[1]:(0.171013236046) A[2]:(0.204889655113) A[3]:(0.816089749336)\n",
      " state (8)  A[0]:(0.711330354214) A[1]:(0.0311574786901) A[2]:(0.774964988232) A[3]:(0.531721293926)\n",
      " state (9)  A[0]:(0.86534768343) A[1]:(0.86408662796) A[2]:(0.807739377022) A[3]:(0.167547896504)\n",
      " state (10)  A[0]:(0.415191233158) A[1]:(0.999574780464) A[2]:(0.160326391459) A[3]:(0.248527169228)\n",
      " state (11)  A[0]:(-0.363706886768) A[1]:(0.999998927116) A[2]:(-0.451101154089) A[3]:(0.465605676174)\n",
      " state (12)  A[0]:(-0.458659261465) A[1]:(1.0) A[2]:(-0.0738906562328) A[3]:(0.667256355286)\n",
      " state (13)  A[0]:(0.0501685924828) A[1]:(1.0) A[2]:(0.873353719711) A[3]:(0.821532309055)\n",
      " state (14)  A[0]:(0.748806357384) A[1]:(1.0) A[2]:(0.997572004795) A[3]:(0.914953410625)\n",
      " state (15)  A[0]:(0.9606372118) A[1]:(1.0) A[2]:(0.999960541725) A[3]:(0.958956718445)\n",
      "Episode 76000 finished after 0 timesteps with r=0.0. Running score: 0.19. Times trained:               10064. Times reached goal: 155.               Steps done: 614790. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.486679484317.\n",
      " state (0)  A[0]:(0.570084810257) A[1]:(0.636398077011) A[2]:(0.508113503456) A[3]:(0.568653941154)\n",
      " state (1)  A[0]:(0.559711158276) A[1]:(0.076773904264) A[2]:(0.401758879423) A[3]:(0.461645692587)\n",
      " state (2)  A[0]:(0.547716677189) A[1]:(0.177731394768) A[2]:(0.498905986547) A[3]:(0.483851969242)\n",
      " state (3)  A[0]:(0.484438985586) A[1]:(0.224200427532) A[2]:(0.54754948616) A[3]:(0.537844061852)\n",
      " state (4)  A[0]:(0.634799599648) A[1]:(0.703394293785) A[2]:(0.00583522859961) A[3]:(0.554935455322)\n",
      " state (5)  A[0]:(0.44725343585) A[1]:(0.700798332691) A[2]:(-0.214678198099) A[3]:(0.682136833668)\n",
      " state (6)  A[0]:(-0.08176445961) A[1]:(0.411723524332) A[2]:(-0.0893759205937) A[3]:(0.809148073196)\n",
      " state (7)  A[0]:(-0.172788739204) A[1]:(0.146165907383) A[2]:(0.229821503162) A[3]:(0.8209182024)\n",
      " state (8)  A[0]:(0.702419638634) A[1]:(0.0316484943032) A[2]:(0.784569203854) A[3]:(0.530290961266)\n",
      " state (9)  A[0]:(0.867071449757) A[1]:(0.875617206097) A[2]:(0.8120251894) A[3]:(0.150632441044)\n",
      " state (10)  A[0]:(0.389288932085) A[1]:(0.999651253223) A[2]:(0.120859757066) A[3]:(0.261398226023)\n",
      " state (11)  A[0]:(-0.410344988108) A[1]:(0.99999910593) A[2]:(-0.498881906271) A[3]:(0.496633738279)\n",
      " state (12)  A[0]:(-0.491306692362) A[1]:(1.0) A[2]:(-0.101955227554) A[3]:(0.688060641289)\n",
      " state (13)  A[0]:(0.0417590476573) A[1]:(1.0) A[2]:(0.883304834366) A[3]:(0.825855433941)\n",
      " state (14)  A[0]:(0.761873960495) A[1]:(1.0) A[2]:(0.998121201992) A[3]:(0.910900950432)\n",
      " state (15)  A[0]:(0.964938163757) A[1]:(1.0) A[2]:(0.99997317791) A[3]:(0.953271448612)\n",
      "Episode 77000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained:               10544. Times reached goal: 150.               Steps done: 625334. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.481574894524.\n",
      " state (0)  A[0]:(0.576221346855) A[1]:(0.636457622051) A[2]:(0.509934306145) A[3]:(0.574145674706)\n",
      " state (1)  A[0]:(0.566311836243) A[1]:(0.0861743241549) A[2]:(0.402516722679) A[3]:(0.468941628933)\n",
      " state (2)  A[0]:(0.556000471115) A[1]:(0.17330044508) A[2]:(0.486431747675) A[3]:(0.480466932058)\n",
      " state (3)  A[0]:(0.498692631721) A[1]:(0.211453408003) A[2]:(0.531905651093) A[3]:(0.526013493538)\n",
      " state (4)  A[0]:(0.639804005623) A[1]:(0.704096317291) A[2]:(0.0102573018521) A[3]:(0.563826918602)\n",
      " state (5)  A[0]:(0.484538942575) A[1]:(0.730279922485) A[2]:(-0.232939004898) A[3]:(0.690932393074)\n",
      " state (6)  A[0]:(-0.0362953990698) A[1]:(0.437736481428) A[2]:(-0.0973465815187) A[3]:(0.816425681114)\n",
      " state (7)  A[0]:(-0.16626933217) A[1]:(0.137818023562) A[2]:(0.228112742305) A[3]:(0.832357883453)\n",
      " state (8)  A[0]:(0.697180569172) A[1]:(0.0227414499968) A[2]:(0.789908111095) A[3]:(0.544384121895)\n",
      " state (9)  A[0]:(0.874560415745) A[1]:(0.872290790081) A[2]:(0.826465904713) A[3]:(0.141550719738)\n",
      " state (10)  A[0]:(0.408460438251) A[1]:(0.999653339386) A[2]:(0.134453698993) A[3]:(0.272334456444)\n",
      " state (11)  A[0]:(-0.409848034382) A[1]:(0.999999165535) A[2]:(-0.517090141773) A[3]:(0.522213816643)\n",
      " state (12)  A[0]:(-0.494421958923) A[1]:(1.0) A[2]:(-0.129722207785) A[3]:(0.704023480415)\n",
      " state (13)  A[0]:(0.0511245839298) A[1]:(1.0) A[2]:(0.884838283062) A[3]:(0.827309668064)\n",
      " state (14)  A[0]:(0.775254368782) A[1]:(1.0) A[2]:(0.998351097107) A[3]:(0.905003130436)\n",
      " state (15)  A[0]:(0.968415379524) A[1]:(1.0) A[2]:(0.999978840351) A[3]:(0.946028113365)\n",
      "Episode 78000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               9946. Times reached goal: 137.               Steps done: 635280. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.476808891246.\n",
      " state (0)  A[0]:(0.572593510151) A[1]:(0.634504854679) A[2]:(0.503991663456) A[3]:(0.573045015335)\n",
      " state (1)  A[0]:(0.561235427856) A[1]:(0.0850455760956) A[2]:(0.400743544102) A[3]:(0.474119126797)\n",
      " state (2)  A[0]:(0.552773952484) A[1]:(0.16467627883) A[2]:(0.472085475922) A[3]:(0.476212710142)\n",
      " state (3)  A[0]:(0.500694513321) A[1]:(0.199316471815) A[2]:(0.512746572495) A[3]:(0.513490319252)\n",
      " state (4)  A[0]:(0.635965645313) A[1]:(0.702024281025) A[2]:(0.00863530673087) A[3]:(0.562597870827)\n",
      " state (5)  A[0]:(0.505173206329) A[1]:(0.753154754639) A[2]:(-0.255319535732) A[3]:(0.691934108734)\n",
      " state (6)  A[0]:(-0.00876345019788) A[1]:(0.460176855326) A[2]:(-0.113805435598) A[3]:(0.819883644581)\n",
      " state (7)  A[0]:(-0.173215776682) A[1]:(0.133683219552) A[2]:(0.216917172074) A[3]:(0.840571343899)\n",
      " state (8)  A[0]:(0.675598025322) A[1]:(0.031857136637) A[2]:(0.788037359715) A[3]:(0.556392908096)\n",
      " state (9)  A[0]:(0.867627501488) A[1]:(0.880084276199) A[2]:(0.828468799591) A[3]:(0.142466068268)\n",
      " state (10)  A[0]:(0.380027115345) A[1]:(0.999692678452) A[2]:(0.112409085035) A[3]:(0.300296485424)\n",
      " state (11)  A[0]:(-0.440293580294) A[1]:(0.999999284744) A[2]:(-0.54908233881) A[3]:(0.562216043472)\n",
      " state (12)  A[0]:(-0.515856146812) A[1]:(1.0) A[2]:(-0.164769515395) A[3]:(0.72982698679)\n",
      " state (13)  A[0]:(0.0391656830907) A[1]:(1.0) A[2]:(0.885690152645) A[3]:(0.835129380226)\n",
      " state (14)  A[0]:(0.779277026653) A[1]:(1.0) A[2]:(0.998552560806) A[3]:(0.902357280254)\n",
      " state (15)  A[0]:(0.970201194286) A[1]:(1.0) A[2]:(0.999983429909) A[3]:(0.93989944458)\n",
      "Episode 79000 finished after 0 timesteps with r=1.0. Running score: 0.12. Times trained:               10092. Times reached goal: 150.               Steps done: 645372. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.472021135566.\n",
      "q_values \n",
      "tensor([[ 0.5778,  0.6409,  0.5051,  0.5792]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6453,  0.7134,  0.0032,  0.5647]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6838,  0.0210,  0.7912,  0.5576]], device='cuda:0')\n",
      "On state=8, selected action=1 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.577373504639) A[1]:(0.639912962914) A[2]:(0.504171311855) A[3]:(0.579204082489)\n",
      " state (1)  A[0]:(0.563638210297) A[1]:(0.0938942506909) A[2]:(0.414014816284) A[3]:(0.488469719887)\n",
      " state (2)  A[0]:(0.557160198689) A[1]:(0.171940386295) A[2]:(0.473014801741) A[3]:(0.481187224388)\n",
      " state (3)  A[0]:(0.511061906815) A[1]:(0.208183184266) A[2]:(0.506535351276) A[3]:(0.509399116039)\n",
      " state (4)  A[0]:(0.644461750984) A[1]:(0.711512804031) A[2]:(0.00150281074457) A[3]:(0.564809441566)\n",
      " state (5)  A[0]:(0.535600543022) A[1]:(0.776430308819) A[2]:(-0.280518829823) A[3]:(0.695775747299)\n",
      " state (6)  A[0]:(0.0381384603679) A[1]:(0.486151278019) A[2]:(-0.130415633321) A[3]:(0.824453830719)\n",
      " state (7)  A[0]:(-0.144391700625) A[1]:(0.129549086094) A[2]:(0.212746113539) A[3]:(0.847305893898)\n",
      " state (8)  A[0]:(0.683269143105) A[1]:(0.0193329807371) A[2]:(0.790705680847) A[3]:(0.557852387428)\n",
      " state (9)  A[0]:(0.874170660973) A[1]:(0.877872228622) A[2]:(0.834194600582) A[3]:(0.123734876513)\n",
      " state (10)  A[0]:(0.399217516184) A[1]:(0.999698519707) A[2]:(0.108413651586) A[3]:(0.304959893227)\n",
      " state (11)  A[0]:(-0.435401797295) A[1]:(0.999999344349) A[2]:(-0.573716640472) A[3]:(0.581638455391)\n",
      " state (12)  A[0]:(-0.51856303215) A[1]:(1.0) A[2]:(-0.205246314406) A[3]:(0.741281032562)\n",
      " state (13)  A[0]:(0.0385637097061) A[1]:(1.0) A[2]:(0.883657932281) A[3]:(0.834110081196)\n",
      " state (14)  A[0]:(0.784130692482) A[1]:(1.0) A[2]:(0.998696506023) A[3]:(0.893810331821)\n",
      " state (15)  A[0]:(0.971666753292) A[1]:(1.0) A[2]:(0.999986827374) A[3]:(0.928984582424)\n",
      "Episode 80000 finished after 0 timesteps with r=0.0. Running score: 0.21. Times trained:               10421. Times reached goal: 178.               Steps done: 655793. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.46712774461.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.577368915081) A[1]:(0.642008066177) A[2]:(0.501018464565) A[3]:(0.577291786671)\n",
      " state (1)  A[0]:(0.563668966293) A[1]:(0.0786468014121) A[2]:(0.425058156252) A[3]:(0.498430222273)\n",
      " state (2)  A[0]:(0.558931052685) A[1]:(0.159922033548) A[2]:(0.476430624723) A[3]:(0.485539793968)\n",
      " state (3)  A[0]:(0.517575502396) A[1]:(0.201374575496) A[2]:(0.505555510521) A[3]:(0.507851779461)\n",
      " state (4)  A[0]:(0.646860301495) A[1]:(0.711973905563) A[2]:(-0.00456401519477) A[3]:(0.564387023449)\n",
      " state (5)  A[0]:(0.552767753601) A[1]:(0.789801836014) A[2]:(-0.304697066545) A[3]:(0.696802377701)\n",
      " state (6)  A[0]:(0.0684813931584) A[1]:(0.500732302666) A[2]:(-0.145624428988) A[3]:(0.828601360321)\n",
      " state (7)  A[0]:(-0.134508833289) A[1]:(0.122560545802) A[2]:(0.205194443464) A[3]:(0.855433225632)\n",
      " state (8)  A[0]:(0.672303557396) A[1]:(0.0134143531322) A[2]:(0.786110401154) A[3]:(0.571102499962)\n",
      " state (9)  A[0]:(0.872853219509) A[1]:(0.877362072468) A[2]:(0.832706868649) A[3]:(0.123500518501)\n",
      " state (10)  A[0]:(0.399480700493) A[1]:(0.999707818031) A[2]:(0.0840315520763) A[3]:(0.327800989151)\n",
      " state (11)  A[0]:(-0.442280501127) A[1]:(0.999999344349) A[2]:(-0.609643220901) A[3]:(0.615232348442)\n",
      " state (12)  A[0]:(-0.529879927635) A[1]:(1.0) A[2]:(-0.266200959682) A[3]:(0.76346540451)\n",
      " state (13)  A[0]:(0.0257407054305) A[1]:(1.0) A[2]:(0.875309824944) A[3]:(0.841472625732)\n",
      " state (14)  A[0]:(0.784844636917) A[1]:(1.0) A[2]:(0.998764276505) A[3]:(0.890869259834)\n",
      " state (15)  A[0]:(0.972826004028) A[1]:(1.0) A[2]:(0.999989271164) A[3]:(0.921193301678)\n",
      "Episode 81000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained:               10287. Times reached goal: 151.               Steps done: 666080. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.462347033249.\n",
      " state (0)  A[0]:(0.580980420113) A[1]:(0.641372263432) A[2]:(0.508748531342) A[3]:(0.580903410912)\n",
      " state (1)  A[0]:(0.564781069756) A[1]:(0.0752601027489) A[2]:(0.445038110018) A[3]:(0.511994242668)\n",
      " state (2)  A[0]:(0.562219381332) A[1]:(0.166509643197) A[2]:(0.49316161871) A[3]:(0.494771063328)\n",
      " state (3)  A[0]:(0.524897456169) A[1]:(0.21659052372) A[2]:(0.521590292454) A[3]:(0.511956691742)\n",
      " state (4)  A[0]:(0.646446287632) A[1]:(0.715445637703) A[2]:(0.00882984139025) A[3]:(0.563698768616)\n",
      " state (5)  A[0]:(0.565851926804) A[1]:(0.804446101189) A[2]:(-0.32089984417) A[3]:(0.694618701935)\n",
      " state (6)  A[0]:(0.0969902575016) A[1]:(0.522291302681) A[2]:(-0.147107824683) A[3]:(0.830111384392)\n",
      " state (7)  A[0]:(-0.109465792775) A[1]:(0.129629909992) A[2]:(0.221388399601) A[3]:(0.858883321285)\n",
      " state (8)  A[0]:(0.681398868561) A[1]:(0.0232258979231) A[2]:(0.794185042381) A[3]:(0.566543281078)\n",
      " state (9)  A[0]:(0.877463638783) A[1]:(0.884052634239) A[2]:(0.838312983513) A[3]:(0.103246919811)\n",
      " state (10)  A[0]:(0.416109412909) A[1]:(0.999742209911) A[2]:(0.0875312238932) A[3]:(0.333264857531)\n",
      " state (11)  A[0]:(-0.431116729975) A[1]:(0.999999463558) A[2]:(-0.617263317108) A[3]:(0.634564876556)\n",
      " state (12)  A[0]:(-0.521233797073) A[1]:(1.0) A[2]:(-0.268441855907) A[3]:(0.776093363762)\n",
      " state (13)  A[0]:(0.0462841615081) A[1]:(1.0) A[2]:(0.885613262653) A[3]:(0.843264758587)\n",
      " state (14)  A[0]:(0.799100100994) A[1]:(1.0) A[2]:(0.999036192894) A[3]:(0.884133338928)\n",
      " state (15)  A[0]:(0.975635886192) A[1]:(1.0) A[2]:(0.999992787838) A[3]:(0.909706294537)\n",
      "Episode 82000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               10349. Times reached goal: 152.               Steps done: 676429. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.457586877711.\n",
      " state (0)  A[0]:(0.587300658226) A[1]:(0.648192763329) A[2]:(0.503702759743) A[3]:(0.587426245213)\n",
      " state (1)  A[0]:(0.569270968437) A[1]:(0.079936593771) A[2]:(0.455684810877) A[3]:(0.527872622013)\n",
      " state (2)  A[0]:(0.568792760372) A[1]:(0.18536773324) A[2]:(0.505805194378) A[3]:(0.508526444435)\n",
      " state (3)  A[0]:(0.535735249519) A[1]:(0.247133612633) A[2]:(0.536617517471) A[3]:(0.522610187531)\n",
      " state (4)  A[0]:(0.654207229614) A[1]:(0.719985544682) A[2]:(0.00373558443971) A[3]:(0.568303644657)\n",
      " state (5)  A[0]:(0.580999672413) A[1]:(0.804119884968) A[2]:(-0.340732693672) A[3]:(0.697166204453)\n",
      " state (6)  A[0]:(0.127020701766) A[1]:(0.514276862144) A[2]:(-0.143951743841) A[3]:(0.834978222847)\n",
      " state (7)  A[0]:(-0.0846308171749) A[1]:(0.105244465172) A[2]:(0.243325278163) A[3]:(0.865675568581)\n",
      " state (8)  A[0]:(0.686269044876) A[1]:(0.00733809405938) A[2]:(0.800109326839) A[3]:(0.574621796608)\n",
      " state (9)  A[0]:(0.881055891514) A[1]:(0.883911252022) A[2]:(0.841029047966) A[3]:(0.0970418974757)\n",
      " state (10)  A[0]:(0.428519427776) A[1]:(0.999757111073) A[2]:(0.0784284546971) A[3]:(0.346892565489)\n",
      " state (11)  A[0]:(-0.431444615126) A[1]:(0.999999582767) A[2]:(-0.639656305313) A[3]:(0.657797455788)\n",
      " state (12)  A[0]:(-0.531050801277) A[1]:(1.0) A[2]:(-0.30909216404) A[3]:(0.791358590126)\n",
      " state (13)  A[0]:(0.0323566049337) A[1]:(1.0) A[2]:(0.883253276348) A[3]:(0.84758245945)\n",
      " state (14)  A[0]:(0.79947924614) A[1]:(1.0) A[2]:(0.999154269695) A[3]:(0.879155695438)\n",
      " state (15)  A[0]:(0.976728975773) A[1]:(1.0) A[2]:(0.999994754791) A[3]:(0.898678421974)\n",
      "Episode 83000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               10547. Times reached goal: 170.               Steps done: 686976. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.452786070472.\n",
      " state (0)  A[0]:(0.591063141823) A[1]:(0.653053402901) A[2]:(0.513136923313) A[3]:(0.591150760651)\n",
      " state (1)  A[0]:(0.572987437248) A[1]:(0.0885061919689) A[2]:(0.469192624092) A[3]:(0.539899587631)\n",
      " state (2)  A[0]:(0.574411988258) A[1]:(0.210313871503) A[2]:(0.520621180534) A[3]:(0.518512845039)\n",
      " state (3)  A[0]:(0.545318245888) A[1]:(0.285635560751) A[2]:(0.552966535091) A[3]:(0.529541611671)\n",
      " state (4)  A[0]:(0.65808314085) A[1]:(0.726054430008) A[2]:(0.00675971945748) A[3]:(0.570998430252)\n",
      " state (5)  A[0]:(0.592391371727) A[1]:(0.806544184685) A[2]:(-0.357047557831) A[3]:(0.697400450706)\n",
      " state (6)  A[0]:(0.15746730566) A[1]:(0.520388185978) A[2]:(-0.144993126392) A[3]:(0.836990356445)\n",
      " state (7)  A[0]:(-0.0617657341063) A[1]:(0.106646955013) A[2]:(0.253879636526) A[3]:(0.870506763458)\n",
      " state (8)  A[0]:(0.682779192924) A[1]:(0.0165433101356) A[2]:(0.800512075424) A[3]:(0.585770606995)\n",
      " state (9)  A[0]:(0.882689893246) A[1]:(0.886018157005) A[2]:(0.843788862228) A[3]:(0.0925498828292)\n",
      " state (10)  A[0]:(0.441651016474) A[1]:(0.999771595001) A[2]:(0.0829376205802) A[3]:(0.356256902218)\n",
      " state (11)  A[0]:(-0.428154140711) A[1]:(0.999999642372) A[2]:(-0.649384975433) A[3]:(0.676506996155)\n",
      " state (12)  A[0]:(-0.535184979439) A[1]:(1.0) A[2]:(-0.323543727398) A[3]:(0.803964495659)\n",
      " state (13)  A[0]:(0.0318097472191) A[1]:(1.0) A[2]:(0.889387607574) A[3]:(0.850863933563)\n",
      " state (14)  A[0]:(0.806155920029) A[1]:(1.0) A[2]:(0.999323546886) A[3]:(0.873327851295)\n",
      " state (15)  A[0]:(0.978501617908) A[1]:(1.0) A[2]:(0.999996483326) A[3]:(0.885721921921)\n",
      "Episode 84000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               10904. Times reached goal: 167.               Steps done: 697880. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.447875711091.\n",
      "q_values \n",
      "tensor([[ 0.5857,  0.6460,  0.5116,  0.5874]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6498,  0.7196,  0.0083,  0.5675]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.585860550404) A[1]:(0.645387351513) A[2]:(0.511759281158) A[3]:(0.587246656418)\n",
      " state (1)  A[0]:(0.567697405815) A[1]:(0.0853597819805) A[2]:(0.466612607241) A[3]:(0.541996240616)\n",
      " state (2)  A[0]:(0.570305466652) A[1]:(0.221773922443) A[2]:(0.519277572632) A[3]:(0.518912553787)\n",
      " state (3)  A[0]:(0.543558955193) A[1]:(0.308043628931) A[2]:(0.553925991058) A[3]:(0.52772217989)\n",
      " state (4)  A[0]:(0.6498503685) A[1]:(0.719239711761) A[2]:(0.00849453080446) A[3]:(0.567428708076)\n",
      " state (5)  A[0]:(0.592579722404) A[1]:(0.801950454712) A[2]:(-0.376821517944) A[3]:(0.692866921425)\n",
      " state (6)  A[0]:(0.17446000874) A[1]:(0.513536691666) A[2]:(-0.153831198812) A[3]:(0.835680007935)\n",
      " state (7)  A[0]:(-0.0448829196393) A[1]:(0.0906026437879) A[2]:(0.25777515769) A[3]:(0.872218370438)\n",
      " state (8)  A[0]:(0.680818319321) A[1]:(0.00745890615508) A[2]:(0.800423979759) A[3]:(0.591466248035)\n",
      " state (9)  A[0]:(0.884156286716) A[1]:(0.885354757309) A[2]:(0.844696998596) A[3]:(0.0924992933869)\n",
      " state (10)  A[0]:(0.455068349838) A[1]:(0.999779582024) A[2]:(0.0737307146192) A[3]:(0.37561121583)\n",
      " state (11)  A[0]:(-0.421398609877) A[1]:(0.999999642372) A[2]:(-0.671460211277) A[3]:(0.702249526978)\n",
      " state (12)  A[0]:(-0.537100851536) A[1]:(1.0) A[2]:(-0.369904458523) A[3]:(0.821667909622)\n",
      " state (13)  A[0]:(0.0293067973107) A[1]:(1.0) A[2]:(0.884047150612) A[3]:(0.859053015709)\n",
      " state (14)  A[0]:(0.810765266418) A[1]:(1.0) A[2]:(0.99938583374) A[3]:(0.871911287308)\n",
      " state (15)  A[0]:(0.97998970747) A[1]:(1.0) A[2]:(0.999997317791) A[3]:(0.875878632069)\n",
      "Episode 85000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               11214. Times reached goal: 174.               Steps done: 709094. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.442881288931.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.593225717545) A[1]:(0.652509331703) A[2]:(0.506902992725) A[3]:(0.592671751976)\n",
      " state (1)  A[0]:(0.574922800064) A[1]:(0.0865211188793) A[2]:(0.463903486729) A[3]:(0.549918353558)\n",
      " state (2)  A[0]:(0.578331291676) A[1]:(0.237219676375) A[2]:(0.515408694744) A[3]:(0.524723470211)\n",
      " state (3)  A[0]:(0.554953575134) A[1]:(0.336486876011) A[2]:(0.549108207226) A[3]:(0.53024327755)\n",
      " state (4)  A[0]:(0.65539419651) A[1]:(0.723991036415) A[2]:(-0.00390187907033) A[3]:(0.56959861517)\n",
      " state (5)  A[0]:(0.605709254742) A[1]:(0.80480492115) A[2]:(-0.398962914944) A[3]:(0.692994117737)\n",
      " state (6)  A[0]:(0.213024765253) A[1]:(0.526392102242) A[2]:(-0.173069640994) A[3]:(0.835338056087)\n",
      " state (7)  A[0]:(-0.00916275754571) A[1]:(0.0983427762985) A[2]:(0.246578708291) A[3]:(0.873934090137)\n",
      " state (8)  A[0]:(0.680593132973) A[1]:(0.00814062263817) A[2]:(0.79432618618) A[3]:(0.595984876156)\n",
      " state (9)  A[0]:(0.883483886719) A[1]:(0.883077442646) A[2]:(0.84203261137) A[3]:(0.0820328816772)\n",
      " state (10)  A[0]:(0.459080696106) A[1]:(0.999778687954) A[2]:(0.0573209933937) A[3]:(0.374745041132)\n",
      " state (11)  A[0]:(-0.423637628555) A[1]:(0.999999701977) A[2]:(-0.694326579571) A[3]:(0.710626244545)\n",
      " state (12)  A[0]:(-0.545691132545) A[1]:(1.0) A[2]:(-0.414714574814) A[3]:(0.827260971069)\n",
      " state (13)  A[0]:(0.0192773230374) A[1]:(1.0) A[2]:(0.879162251949) A[3]:(0.858514070511)\n",
      " state (14)  A[0]:(0.812750577927) A[1]:(1.0) A[2]:(0.999443948269) A[3]:(0.863439083099)\n",
      " state (15)  A[0]:(0.980954349041) A[1]:(1.0) A[2]:(0.999997913837) A[3]:(0.859049797058)\n",
      "Episode 86000 finished after 0 timesteps with r=1.0. Running score: 0.2. Times trained:               11149. Times reached goal: 189.               Steps done: 720243. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.437971028549.\n",
      " state (0)  A[0]:(0.587111353874) A[1]:(0.65420794487) A[2]:(0.510096848011) A[3]:(0.587090969086)\n",
      " state (1)  A[0]:(0.570930123329) A[1]:(0.0822276547551) A[2]:(0.477044165134) A[3]:(0.549861490726)\n",
      " state (2)  A[0]:(0.573822855949) A[1]:(0.244190588593) A[2]:(0.528678953648) A[3]:(0.524099230766)\n",
      " state (3)  A[0]:(0.550828456879) A[1]:(0.352001041174) A[2]:(0.561994075775) A[3]:(0.529320180416)\n",
      " state (4)  A[0]:(0.651049971581) A[1]:(0.726357221603) A[2]:(0.00266229477711) A[3]:(0.569061815739)\n",
      " state (5)  A[0]:(0.595699846745) A[1]:(0.793216228485) A[2]:(-0.375417202711) A[3]:(0.694950461388)\n",
      " state (6)  A[0]:(0.208665028214) A[1]:(0.499519884586) A[2]:(-0.136685356498) A[3]:(0.838210463524)\n",
      " state (7)  A[0]:(-0.000140520278364) A[1]:(0.0644478946924) A[2]:(0.284739494324) A[3]:(0.877097725868)\n",
      " state (8)  A[0]:(0.692587375641) A[1]:(-0.000972211069893) A[2]:(0.804865896702) A[3]:(0.600614786148)\n",
      " state (9)  A[0]:(0.891096234322) A[1]:(0.888235151768) A[2]:(0.846631288528) A[3]:(0.0845207497478)\n",
      " state (10)  A[0]:(0.483871370554) A[1]:(0.999803125858) A[2]:(0.0608321614563) A[3]:(0.390955805779)\n",
      " state (11)  A[0]:(-0.41077631712) A[1]:(0.999999761581) A[2]:(-0.700363636017) A[3]:(0.728428602219)\n",
      " state (12)  A[0]:(-0.543738126755) A[1]:(1.0) A[2]:(-0.422663390636) A[3]:(0.83886808157)\n",
      " state (13)  A[0]:(0.0216031558812) A[1]:(1.0) A[2]:(0.88558447361) A[3]:(0.863842487335)\n",
      " state (14)  A[0]:(0.817724466324) A[1]:(1.0) A[2]:(0.999549090862) A[3]:(0.861351788044)\n",
      " state (15)  A[0]:(0.982067763805) A[1]:(1.0) A[2]:(0.999998569489) A[3]:(0.848701357841)\n",
      "Episode 87000 finished after 0 timesteps with r=1.0. Running score: 0.17. Times trained:               10683. Times reached goal: 183.               Steps done: 730926. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.43331708734.\n",
      " state (0)  A[0]:(0.588049292564) A[1]:(0.654658436775) A[2]:(0.512791931629) A[3]:(0.588413715363)\n",
      " state (1)  A[0]:(0.572037220001) A[1]:(0.0743619650602) A[2]:(0.482031345367) A[3]:(0.556067585945)\n",
      " state (2)  A[0]:(0.574130117893) A[1]:(0.243395805359) A[2]:(0.53145980835) A[3]:(0.529300093651)\n",
      " state (3)  A[0]:(0.552894711494) A[1]:(0.360042691231) A[2]:(0.562100291252) A[3]:(0.531415045261)\n",
      " state (4)  A[0]:(0.651595056057) A[1]:(0.723898410797) A[2]:(-0.00307690119371) A[3]:(0.566172778606)\n",
      " state (5)  A[0]:(0.597199380398) A[1]:(0.784969806671) A[2]:(-0.364335507154) A[3]:(0.686677575111)\n",
      " state (6)  A[0]:(0.228911489248) A[1]:(0.499014347792) A[2]:(-0.123034238815) A[3]:(0.830246806145)\n",
      " state (7)  A[0]:(0.012540566735) A[1]:(0.0691367760301) A[2]:(0.293795853853) A[3]:(0.873615145683)\n",
      " state (8)  A[0]:(0.681246936321) A[1]:(0.0118192890659) A[2]:(0.802034914494) A[3]:(0.606726408005)\n",
      " state (9)  A[0]:(0.88861399889) A[1]:(0.891722500324) A[2]:(0.844818413258) A[3]:(0.0917750969529)\n",
      " state (10)  A[0]:(0.474090278149) A[1]:(0.999814450741) A[2]:(0.0426775105298) A[3]:(0.409997880459)\n",
      " state (11)  A[0]:(-0.428396940231) A[1]:(0.999999761581) A[2]:(-0.715819299221) A[3]:(0.746699571609)\n",
      " state (12)  A[0]:(-0.55862736702) A[1]:(1.0) A[2]:(-0.439096450806) A[3]:(0.850132584572)\n",
      " state (13)  A[0]:(0.0122329797596) A[1]:(1.0) A[2]:(0.891068577766) A[3]:(0.869126260281)\n",
      " state (14)  A[0]:(0.822150349617) A[1]:(1.0) A[2]:(0.999634087086) A[3]:(0.859446883202)\n",
      " state (15)  A[0]:(0.983273506165) A[1]:(1.0) A[2]:(0.999998986721) A[3]:(0.838097155094)\n",
      "Episode 88000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               11202. Times reached goal: 171.               Steps done: 742128. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.428490155448.\n",
      " state (0)  A[0]:(0.590125441551) A[1]:(0.653713822365) A[2]:(0.513659834862) A[3]:(0.589088559151)\n",
      " state (1)  A[0]:(0.575265526772) A[1]:(0.0845178440213) A[2]:(0.478140115738) A[3]:(0.560611248016)\n",
      " state (2)  A[0]:(0.576441645622) A[1]:(0.258330613375) A[2]:(0.523404121399) A[3]:(0.532088637352)\n",
      " state (3)  A[0]:(0.556249856949) A[1]:(0.380011826754) A[2]:(0.550790190697) A[3]:(0.53101670742)\n",
      " state (4)  A[0]:(0.649883031845) A[1]:(0.727807879448) A[2]:(-0.000522911490407) A[3]:(0.566380500793)\n",
      " state (5)  A[0]:(0.600619673729) A[1]:(0.788916230202) A[2]:(-0.35831964016) A[3]:(0.68350070715)\n",
      " state (6)  A[0]:(0.253119081259) A[1]:(0.515411257744) A[2]:(-0.12452159822) A[3]:(0.824853181839)\n",
      " state (7)  A[0]:(0.039502825588) A[1]:(0.0836850851774) A[2]:(0.288839578629) A[3]:(0.869795799255)\n",
      " state (8)  A[0]:(0.68243175745) A[1]:(0.0181630551815) A[2]:(0.800390005112) A[3]:(0.599978685379)\n",
      " state (9)  A[0]:(0.888711988926) A[1]:(0.891833424568) A[2]:(0.846686959267) A[3]:(0.0740630030632)\n",
      " state (10)  A[0]:(0.477779656649) A[1]:(0.999816536903) A[2]:(0.0391768477857) A[3]:(0.403749197721)\n",
      " state (11)  A[0]:(-0.426796406507) A[1]:(0.999999761581) A[2]:(-0.728087484837) A[3]:(0.74953109026)\n",
      " state (12)  A[0]:(-0.559496879578) A[1]:(1.0) A[2]:(-0.467215210199) A[3]:(0.85206669569)\n",
      " state (13)  A[0]:(0.0147893354297) A[1]:(1.0) A[2]:(0.888020932674) A[3]:(0.867509901524)\n",
      " state (14)  A[0]:(0.827944219112) A[1]:(1.0) A[2]:(0.999664485455) A[3]:(0.851598441601)\n",
      " state (15)  A[0]:(0.984481453896) A[1]:(1.0) A[2]:(0.99999922514) A[3]:(0.821572959423)\n",
      "Episode 89000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained:               10832. Times reached goal: 162.               Steps done: 752960. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.423873797417.\n",
      "q_values \n",
      "tensor([[ 0.5873,  0.6500,  0.5153,  0.5869]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5714,  0.0738,  0.4819,  0.5612]], device='cuda:0')\n",
      "On state=1, selected action=0 , Random? False\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5873,  0.6502,  0.5160,  0.5868]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6500,  0.7282,  0.0047,  0.5620]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6811,  0.0200,  0.8046,  0.5995]], device='cuda:0')\n",
      "On state=8, selected action=3 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6500,  0.7285,  0.0068,  0.5619]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6820,  0.0219,  0.8062,  0.5993]], device='cuda:0')\n",
      "On state=8, selected action=1 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.587267637253) A[1]:(0.650493860245) A[2]:(0.517190873623) A[3]:(0.586459875107)\n",
      " state (1)  A[0]:(0.571489095688) A[1]:(0.075391612947) A[2]:(0.484373807907) A[3]:(0.560720324516)\n",
      " state (2)  A[0]:(0.571670174599) A[1]:(0.254252940416) A[2]:(0.525045275688) A[3]:(0.530909001827)\n",
      " state (3)  A[0]:(0.553088843822) A[1]:(0.38340896368) A[2]:(0.548245370388) A[3]:(0.527085661888)\n",
      " state (4)  A[0]:(0.65006506443) A[1]:(0.729060053825) A[2]:(0.00894039869308) A[3]:(0.562004446983)\n",
      " state (5)  A[0]:(0.607028603554) A[1]:(0.78856253624) A[2]:(-0.328590124846) A[3]:(0.67636680603)\n",
      " state (6)  A[0]:(0.283403038979) A[1]:(0.525787234306) A[2]:(-0.0959924980998) A[3]:(0.81709843874)\n",
      " state (7)  A[0]:(0.0727540776134) A[1]:(0.0939009264112) A[2]:(0.311573505402) A[3]:(0.865285634995)\n",
      " state (8)  A[0]:(0.682401299477) A[1]:(0.0222793295979) A[2]:(0.806932330132) A[3]:(0.599245429039)\n",
      " state (9)  A[0]:(0.888737082481) A[1]:(0.890745222569) A[2]:(0.854794502258) A[3]:(0.0684062764049)\n",
      " state (10)  A[0]:(0.486226201057) A[1]:(0.999814510345) A[2]:(0.0628967434168) A[3]:(0.405517160892)\n",
      " state (11)  A[0]:(-0.418210148811) A[1]:(0.999999761581) A[2]:(-0.726595759392) A[3]:(0.756476759911)\n",
      " state (12)  A[0]:(-0.55560863018) A[1]:(1.0) A[2]:(-0.473342806101) A[3]:(0.857237398624)\n",
      " state (13)  A[0]:(0.0197381172329) A[1]:(1.0) A[2]:(0.890116810799) A[3]:(0.870005786419)\n",
      " state (14)  A[0]:(0.832582235336) A[1]:(1.0) A[2]:(0.999705970287) A[3]:(0.849693536758)\n",
      " state (15)  A[0]:(0.98538351059) A[1]:(1.0) A[2]:(0.999999403954) A[3]:(0.813166797161)\n",
      "Episode 90000 finished after 0 timesteps with r=0.0. Running score: 0.24. Times trained:               11070. Times reached goal: 182.               Steps done: 764030. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.419207390694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.588252186775) A[1]:(0.655136942863) A[2]:(0.515982747078) A[3]:(0.588504970074)\n",
      " state (1)  A[0]:(0.575096070766) A[1]:(0.0808498188853) A[2]:(0.484996676445) A[3]:(0.562718212605)\n",
      " state (2)  A[0]:(0.573375105858) A[1]:(0.263751775026) A[2]:(0.522545218468) A[3]:(0.532155156136)\n",
      " state (3)  A[0]:(0.554313540459) A[1]:(0.396158635616) A[2]:(0.542524456978) A[3]:(0.52716755867)\n",
      " state (4)  A[0]:(0.64965981245) A[1]:(0.727069497108) A[2]:(0.00149613502435) A[3]:(0.565652310848)\n",
      " state (5)  A[0]:(0.604942917824) A[1]:(0.777151882648) A[2]:(-0.319161683321) A[3]:(0.680432915688)\n",
      " state (6)  A[0]:(0.290321558714) A[1]:(0.506101846695) A[2]:(-0.0871962532401) A[3]:(0.818698883057)\n",
      " state (7)  A[0]:(0.082290738821) A[1]:(0.0688124075532) A[2]:(0.316914856434) A[3]:(0.86727976799)\n",
      " state (8)  A[0]:(0.676955103874) A[1]:(0.0184316616505) A[2]:(0.805883586407) A[3]:(0.608579337597)\n",
      " state (9)  A[0]:(0.884207129478) A[1]:(0.897134900093) A[2]:(0.851645469666) A[3]:(0.0832526162267)\n",
      " state (10)  A[0]:(0.470143228769) A[1]:(0.999835193157) A[2]:(0.0381475239992) A[3]:(0.424972832203)\n",
      " state (11)  A[0]:(-0.431812018156) A[1]:(0.999999821186) A[2]:(-0.739992499352) A[3]:(0.769295930862)\n",
      " state (12)  A[0]:(-0.562844753265) A[1]:(1.0) A[2]:(-0.483450561762) A[3]:(0.864417314529)\n",
      " state (13)  A[0]:(0.0181073788553) A[1]:(1.0) A[2]:(0.896480739117) A[3]:(0.87387830019)\n",
      " state (14)  A[0]:(0.836353600025) A[1]:(1.0) A[2]:(0.999761462212) A[3]:(0.849460065365)\n",
      " state (15)  A[0]:(0.98606300354) A[1]:(1.0) A[2]:(0.999999582767) A[3]:(0.80705344677)\n",
      "Episode 91000 finished after 0 timesteps with r=1.0. Running score: 0.21. Times trained:               11053. Times reached goal: 177.               Steps done: 775083. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.414599404354.\n",
      " state (0)  A[0]:(0.590005040169) A[1]:(0.650019884109) A[2]:(0.519629478455) A[3]:(0.591414690018)\n",
      " state (1)  A[0]:(0.577164649963) A[1]:(0.0733751431108) A[2]:(0.487892389297) A[3]:(0.562898814678)\n",
      " state (2)  A[0]:(0.573984086514) A[1]:(0.264041960239) A[2]:(0.521562576294) A[3]:(0.53038829565)\n",
      " state (3)  A[0]:(0.554396867752) A[1]:(0.403206020594) A[2]:(0.538513422012) A[3]:(0.522846460342)\n",
      " state (4)  A[0]:(0.644119858742) A[1]:(0.722447633743) A[2]:(0.00499590067193) A[3]:(0.564960300922)\n",
      " state (5)  A[0]:(0.601546704769) A[1]:(0.773531198502) A[2]:(-0.321148753166) A[3]:(0.679514169693)\n",
      " state (6)  A[0]:(0.296698689461) A[1]:(0.506618380547) A[2]:(-0.0940433070064) A[3]:(0.815822958946)\n",
      " state (7)  A[0]:(0.0896401852369) A[1]:(0.0653782039881) A[2]:(0.311106175184) A[3]:(0.865031898022)\n",
      " state (8)  A[0]:(0.668466866016) A[1]:(0.00965004973114) A[2]:(0.805356025696) A[3]:(0.60280585289)\n",
      " state (9)  A[0]:(0.880128383636) A[1]:(0.894397854805) A[2]:(0.854781270027) A[3]:(0.0617562420666)\n",
      " state (10)  A[0]:(0.466386884451) A[1]:(0.999831914902) A[2]:(0.043210644275) A[3]:(0.408458590508)\n",
      " state (11)  A[0]:(-0.431842923164) A[1]:(0.999999821186) A[2]:(-0.747205734253) A[3]:(0.765030503273)\n",
      " state (12)  A[0]:(-0.56379878521) A[1]:(1.0) A[2]:(-0.502391457558) A[3]:(0.862567543983)\n",
      " state (13)  A[0]:(0.0187394525856) A[1]:(1.0) A[2]:(0.896224975586) A[3]:(0.870681107044)\n",
      " state (14)  A[0]:(0.839643239975) A[1]:(1.0) A[2]:(0.999787569046) A[3]:(0.842096090317)\n",
      " state (15)  A[0]:(0.986678123474) A[1]:(1.0) A[2]:(0.999999642372) A[3]:(0.792897284031)\n",
      "Episode 92000 finished after 0 timesteps with r=1.0. Running score: 0.18. Times trained:               10978. Times reached goal: 200.               Steps done: 786061. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.410072823953.\n",
      " state (0)  A[0]:(0.590675115585) A[1]:(0.651883244514) A[2]:(0.517891168594) A[3]:(0.58934211731)\n",
      " state (1)  A[0]:(0.578812718391) A[1]:(0.0706358030438) A[2]:(0.491415023804) A[3]:(0.561810493469)\n",
      " state (2)  A[0]:(0.574990928173) A[1]:(0.266777575016) A[2]:(0.523251771927) A[3]:(0.528782010078)\n",
      " state (3)  A[0]:(0.55675393343) A[1]:(0.41252720356) A[2]:(0.537580609322) A[3]:(0.519816160202)\n",
      " state (4)  A[0]:(0.647677183151) A[1]:(0.725916028023) A[2]:(0.00567352632061) A[3]:(0.56276422739)\n",
      " state (5)  A[0]:(0.608892738819) A[1]:(0.774342477322) A[2]:(-0.307865649462) A[3]:(0.675777673721)\n",
      " state (6)  A[0]:(0.324001163244) A[1]:(0.518888056278) A[2]:(-0.0877815335989) A[3]:(0.811434268951)\n",
      " state (7)  A[0]:(0.124626941979) A[1]:(0.079321347177) A[2]:(0.31238052249) A[3]:(0.862769126892)\n",
      " state (8)  A[0]:(0.674048185349) A[1]:(0.0128368241712) A[2]:(0.806640625) A[3]:(0.6038595438)\n",
      " state (9)  A[0]:(0.879913806915) A[1]:(0.893009185791) A[2]:(0.858784914017) A[3]:(0.0626348033547)\n",
      " state (10)  A[0]:(0.473229855299) A[1]:(0.999828755856) A[2]:(0.0530356951058) A[3]:(0.414045810699)\n",
      " state (11)  A[0]:(-0.420708149672) A[1]:(0.999999821186) A[2]:(-0.751306056976) A[3]:(0.771615982056)\n",
      " state (12)  A[0]:(-0.557239890099) A[1]:(1.0) A[2]:(-0.520449638367) A[3]:(0.867435991764)\n",
      " state (13)  A[0]:(0.0232813749462) A[1]:(1.0) A[2]:(0.893236458302) A[3]:(0.874352514744)\n",
      " state (14)  A[0]:(0.842504024506) A[1]:(1.0) A[2]:(0.999802947044) A[3]:(0.843779087067)\n",
      " state (15)  A[0]:(0.987306237221) A[1]:(1.0) A[2]:(0.999999761581) A[3]:(0.790872335434)\n",
      "Episode 93000 finished after 0 timesteps with r=0.0. Running score: 0.22. Times trained:               11530. Times reached goal: 197.               Steps done: 797591. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.405371837559.\n",
      " state (0)  A[0]:(0.590653061867) A[1]:(0.657351434231) A[2]:(0.522032260895) A[3]:(0.590408802032)\n",
      " state (1)  A[0]:(0.578629136086) A[1]:(0.066866196692) A[2]:(0.494162768126) A[3]:(0.564550995827)\n",
      " state (2)  A[0]:(0.573392152786) A[1]:(0.267661958933) A[2]:(0.524355649948) A[3]:(0.531163334846)\n",
      " state (3)  A[0]:(0.555480241776) A[1]:(0.419369399548) A[2]:(0.535810410976) A[3]:(0.521114349365)\n",
      " state (4)  A[0]:(0.647260546684) A[1]:(0.728869259357) A[2]:(0.00284152454697) A[3]:(0.566671550274)\n",
      " state (5)  A[0]:(0.607390105724) A[1]:(0.771451532841) A[2]:(-0.288356274366) A[3]:(0.678329825401)\n",
      " state (6)  A[0]:(0.335728704929) A[1]:(0.521651864052) A[2]:(-0.0745420977473) A[3]:(0.811376452446)\n",
      " state (7)  A[0]:(0.146318987012) A[1]:(0.081285148859) A[2]:(0.31952381134) A[3]:(0.862753987312)\n",
      " state (8)  A[0]:(0.678824543953) A[1]:(0.0109274806455) A[2]:(0.809618771076) A[3]:(0.603900313377)\n",
      " state (9)  A[0]:(0.877447426319) A[1]:(0.894424915314) A[2]:(0.859740197659) A[3]:(0.0647576451302)\n",
      " state (10)  A[0]:(0.464115411043) A[1]:(0.999834597111) A[2]:(0.0429079383612) A[3]:(0.422384917736)\n",
      " state (11)  A[0]:(-0.425748974085) A[1]:(0.999999821186) A[2]:(-0.760588049889) A[3]:(0.777676582336)\n",
      " state (12)  A[0]:(-0.561094045639) A[1]:(1.0) A[2]:(-0.53772354126) A[3]:(0.87104588747)\n",
      " state (13)  A[0]:(0.0160507149994) A[1]:(1.0) A[2]:(0.892564535141) A[3]:(0.876777946949)\n",
      " state (14)  A[0]:(0.842151165009) A[1]:(1.0) A[2]:(0.999823987484) A[3]:(0.844584941864)\n",
      " state (15)  A[0]:(0.987588167191) A[1]:(1.0) A[2]:(0.999999821186) A[3]:(0.788761377335)\n",
      "Episode 94000 finished after 0 timesteps with r=0.0. Running score: 0.19. Times trained:               11842. Times reached goal: 193.               Steps done: 809433. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.400599735641.\n",
      "q_values \n",
      "tensor([[ 0.5896,  0.6509,  0.5194,  0.5927]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5764,  0.0644,  0.4909,  0.5674]], device='cuda:0')\n",
      "On state=1, selected action=0 , Random? False\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5895,  0.6513,  0.5194,  0.5929]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6404,  0.7299,  0.0056,  0.5682]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6714,  0.0198,  0.8081,  0.6099]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8750,  0.8976,  0.8600,  0.0650]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0207,  1.0000,  0.8971,  0.8771]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8750,  0.8967,  0.8599,  0.0647]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0211,  1.0000,  0.8973,  0.8773]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0221,  1.0000,  0.8975,  0.8774]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0239,  1.0000,  0.8981,  0.8775]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0248,  1.0000,  0.8982,  0.8776]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0250,  1.0000,  0.8980,  0.8777]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0250,  1.0000,  0.8978,  0.8777]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.590325057507) A[1]:(0.649182200432) A[2]:(0.51762008667) A[3]:(0.593320310116)\n",
      " state (1)  A[0]:(0.577674746513) A[1]:(0.0609309151769) A[2]:(0.489335566759) A[3]:(0.568342030048)\n",
      " state (2)  A[0]:(0.571057617664) A[1]:(0.272756487131) A[2]:(0.520245194435) A[3]:(0.533467769623)\n",
      " state (3)  A[0]:(0.552687168121) A[1]:(0.433141738176) A[2]:(0.531850337982) A[3]:(0.521378755569)\n",
      " state (4)  A[0]:(0.642390847206) A[1]:(0.727980971336) A[2]:(0.00317381741479) A[3]:(0.569301724434)\n",
      " state (5)  A[0]:(0.60238802433) A[1]:(0.766148090363) A[2]:(-0.279380947351) A[3]:(0.680147051811)\n",
      " state (6)  A[0]:(0.338754922152) A[1]:(0.519381701946) A[2]:(-0.0682764053345) A[3]:(0.811288058758)\n",
      " state (7)  A[0]:(0.155282199383) A[1]:(0.0772266387939) A[2]:(0.323112368584) A[3]:(0.86318552494)\n",
      " state (8)  A[0]:(0.678955435753) A[1]:(0.00431105820462) A[2]:(0.811068534851) A[3]:(0.606684207916)\n",
      " state (9)  A[0]:(0.878257274628) A[1]:(0.892897129059) A[2]:(0.863220572472) A[3]:(0.0609725117683)\n",
      " state (10)  A[0]:(0.475923269987) A[1]:(0.999834775925) A[2]:(0.0562385544181) A[3]:(0.418499857187)\n",
      " state (11)  A[0]:(-0.412060886621) A[1]:(0.999999821186) A[2]:(-0.760122954845) A[3]:(0.778192281723)\n",
      " state (12)  A[0]:(-0.553210020065) A[1]:(1.0) A[2]:(-0.539429306984) A[3]:(0.872252106667)\n",
      " state (13)  A[0]:(0.0250115748495) A[1]:(1.0) A[2]:(0.897398293018) A[3]:(0.877687335014)\n",
      " state (14)  A[0]:(0.846482872963) A[1]:(1.0) A[2]:(0.999852657318) A[3]:(0.84433567524)\n",
      " state (15)  A[0]:(0.988172769547) A[1]:(1.0) A[2]:(0.999999880791) A[3]:(0.786275982857)\n",
      "Episode 95000 finished after 0 timesteps with r=0.0. Running score: 0.18. Times trained:               11147. Times reached goal: 197.               Steps done: 820580. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.39615904655.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.587507605553) A[1]:(0.653080403805) A[2]:(0.516604065895) A[3]:(0.590270876884)\n",
      " state (1)  A[0]:(0.574588418007) A[1]:(0.0584741570055) A[2]:(0.490570485592) A[3]:(0.566824197769)\n",
      " state (2)  A[0]:(0.566323637962) A[1]:(0.279882252216) A[2]:(0.522107005119) A[3]:(0.530157268047)\n",
      " state (3)  A[0]:(0.547912716866) A[1]:(0.448138475418) A[2]:(0.533201396465) A[3]:(0.515606999397)\n",
      " state (4)  A[0]:(0.640936493874) A[1]:(0.725197792053) A[2]:(0.00393740274012) A[3]:(0.566371738911)\n",
      " state (5)  A[0]:(0.603282809258) A[1]:(0.757085502148) A[2]:(-0.265502035618) A[3]:(0.676138758659)\n",
      " state (6)  A[0]:(0.351385712624) A[1]:(0.516031086445) A[2]:(-0.0581462010741) A[3]:(0.806468307972)\n",
      " state (7)  A[0]:(0.169701337814) A[1]:(0.078252337873) A[2]:(0.326132655144) A[3]:(0.861105084419)\n",
      " state (8)  A[0]:(0.672054708004) A[1]:(0.0108086429536) A[2]:(0.808574438095) A[3]:(0.613794207573)\n",
      " state (9)  A[0]:(0.873409986496) A[1]:(0.896274268627) A[2]:(0.86147493124) A[3]:(0.0728205814958)\n",
      " state (10)  A[0]:(0.466912388802) A[1]:(0.999843835831) A[2]:(0.0431888699532) A[3]:(0.426226168871)\n",
      " state (11)  A[0]:(-0.415637791157) A[1]:(0.999999821186) A[2]:(-0.768705248833) A[3]:(0.781775593758)\n",
      " state (12)  A[0]:(-0.555174291134) A[1]:(1.0) A[2]:(-0.552038669586) A[3]:(0.874039888382)\n",
      " state (13)  A[0]:(0.0216818917543) A[1]:(1.0) A[2]:(0.89913880825) A[3]:(0.878920793533)\n",
      " state (14)  A[0]:(0.847424805164) A[1]:(1.0) A[2]:(0.999873280525) A[3]:(0.845049500465)\n",
      " state (15)  A[0]:(0.988524019718) A[1]:(1.0) A[2]:(0.999999880791) A[3]:(0.786036849022)\n",
      "Episode 96000 finished after 0 timesteps with r=0.0. Running score: 0.15. Times trained:               12163. Times reached goal: 178.               Steps done: 832743. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.391369749222.\n",
      " state (0)  A[0]:(0.585602402687) A[1]:(0.651230216026) A[2]:(0.516428470612) A[3]:(0.586528420448)\n",
      " state (1)  A[0]:(0.574554562569) A[1]:(0.0643653273582) A[2]:(0.49089795351) A[3]:(0.568692207336)\n",
      " state (2)  A[0]:(0.563124537468) A[1]:(0.293861180544) A[2]:(0.520770549774) A[3]:(0.529193162918)\n",
      " state (3)  A[0]:(0.543241500854) A[1]:(0.468586415052) A[2]:(0.528417825699) A[3]:(0.510965287685)\n",
      " state (4)  A[0]:(0.638068318367) A[1]:(0.726889789104) A[2]:(-0.00176828913391) A[3]:(0.56309068203)\n",
      " state (5)  A[0]:(0.596967697144) A[1]:(0.749472737312) A[2]:(-0.24332138896) A[3]:(0.669649958611)\n",
      " state (6)  A[0]:(0.353735476732) A[1]:(0.515414059162) A[2]:(-0.0460363440216) A[3]:(0.798654794693)\n",
      " state (7)  A[0]:(0.184714362025) A[1]:(0.0798263773322) A[2]:(0.326889872551) A[3]:(0.855350732803)\n",
      " state (8)  A[0]:(0.688607931137) A[1]:(0.00418875133619) A[2]:(0.808163285255) A[3]:(0.608553051949)\n",
      " state (9)  A[0]:(0.883975565434) A[1]:(0.893851339817) A[2]:(0.862348675728) A[3]:(0.0745837688446)\n",
      " state (10)  A[0]:(0.497575998306) A[1]:(0.999841570854) A[2]:(0.0392757579684) A[3]:(0.430554181337)\n",
      " state (11)  A[0]:(-0.39746260643) A[1]:(0.999999821186) A[2]:(-0.776886224747) A[3]:(0.784423351288)\n",
      " state (12)  A[0]:(-0.552333116531) A[1]:(1.0) A[2]:(-0.572266340256) A[3]:(0.875718653202)\n",
      " state (13)  A[0]:(0.0151099124923) A[1]:(1.0) A[2]:(0.896834075451) A[3]:(0.880359113216)\n",
      " state (14)  A[0]:(0.845978796482) A[1]:(1.0) A[2]:(0.999884724617) A[3]:(0.846280455589)\n",
      " state (15)  A[0]:(0.988602221012) A[1]:(1.0) A[2]:(0.999999940395) A[3]:(0.786733090878)\n",
      "Episode 97000 finished after 0 timesteps with r=0.0. Running score: 0.18. Times trained:               11382. Times reached goal: 199.               Steps done: 844125. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.386940433789.\n",
      " state (0)  A[0]:(0.584943175316) A[1]:(0.662278294563) A[2]:(0.516060352325) A[3]:(0.58908611536)\n",
      " state (1)  A[0]:(0.575204193592) A[1]:(0.0671912431717) A[2]:(0.495354712009) A[3]:(0.568287789822)\n",
      " state (2)  A[0]:(0.559296011925) A[1]:(0.313528299332) A[2]:(0.523887515068) A[3]:(0.523485124111)\n",
      " state (3)  A[0]:(0.537821531296) A[1]:(0.500223577023) A[2]:(0.525376021862) A[3]:(0.500112950802)\n",
      " state (4)  A[0]:(0.640547335148) A[1]:(0.727838754654) A[2]:(-0.00622622063383) A[3]:(0.563966214657)\n",
      " state (5)  A[0]:(0.584977865219) A[1]:(0.730513572693) A[2]:(-0.179031848907) A[3]:(0.666806936264)\n",
      " state (6)  A[0]:(0.338863313198) A[1]:(0.499045073986) A[2]:(0.00424477877095) A[3]:(0.792462944984)\n",
      " state (7)  A[0]:(0.165171951056) A[1]:(0.0679634511471) A[2]:(0.346301972866) A[3]:(0.851970732212)\n",
      " state (8)  A[0]:(0.681411266327) A[1]:(-0.00102817977313) A[2]:(0.807506024837) A[3]:(0.61488878727)\n",
      " state (9)  A[0]:(0.890211701393) A[1]:(0.893883824348) A[2]:(0.86342304945) A[3]:(0.0782645121217)\n",
      " state (10)  A[0]:(0.520633339882) A[1]:(0.999847114086) A[2]:(0.0319916456938) A[3]:(0.425989866257)\n",
      " state (11)  A[0]:(-0.38883972168) A[1]:(0.999999821186) A[2]:(-0.787277340889) A[3]:(0.781201660633)\n",
      " state (12)  A[0]:(-0.555158853531) A[1]:(1.0) A[2]:(-0.590066194534) A[3]:(0.873304903507)\n",
      " state (13)  A[0]:(0.0103381695226) A[1]:(1.0) A[2]:(0.89867401123) A[3]:(0.877750039101)\n",
      " state (14)  A[0]:(0.848151564598) A[1]:(1.0) A[2]:(0.999902665615) A[3]:(0.842872619629)\n",
      " state (15)  A[0]:(0.989053368568) A[1]:(1.0) A[2]:(0.999999940395) A[3]:(0.78241455555)\n",
      "Episode 98000 finished after 0 timesteps with r=1.0. Running score: 0.16. Times trained:               11912. Times reached goal: 178.               Steps done: 856037. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.38235854326.\n",
      " state (0)  A[0]:(0.586668372154) A[1]:(0.654520988464) A[2]:(0.518918156624) A[3]:(0.584441542625)\n",
      " state (1)  A[0]:(0.576981902122) A[1]:(0.0557524599135) A[2]:(0.493733167648) A[3]:(0.553293049335)\n",
      " state (2)  A[0]:(0.556444883347) A[1]:(0.326270282269) A[2]:(0.517666339874) A[3]:(0.499484688044)\n",
      " state (3)  A[0]:(0.535234928131) A[1]:(0.530730545521) A[2]:(0.507632136345) A[3]:(0.469660133123)\n",
      " state (4)  A[0]:(0.646668970585) A[1]:(0.729020118713) A[2]:(-0.00505687436089) A[3]:(0.561993598938)\n",
      " state (5)  A[0]:(0.578090906143) A[1]:(0.72250688076) A[2]:(-0.111352778971) A[3]:(0.659120678902)\n",
      " state (6)  A[0]:(0.339837402105) A[1]:(0.509614109993) A[2]:(0.0367034226656) A[3]:(0.780138909817)\n",
      " state (7)  A[0]:(0.175699144602) A[1]:(0.0884044766426) A[2]:(0.342199683189) A[3]:(0.84095543623)\n",
      " state (8)  A[0]:(0.690663337708) A[1]:(0.00327806500718) A[2]:(0.807604253292) A[3]:(0.593531370163)\n",
      " state (9)  A[0]:(0.894315719604) A[1]:(0.89335501194) A[2]:(0.868565142155) A[3]:(0.0502806045115)\n",
      " state (10)  A[0]:(0.5279763937) A[1]:(0.999849617481) A[2]:(0.0317260213196) A[3]:(0.405750751495)\n",
      " state (11)  A[0]:(-0.387912482023) A[1]:(0.999999821186) A[2]:(-0.797562897205) A[3]:(0.7710262537)\n",
      " state (12)  A[0]:(-0.558215975761) A[1]:(1.0) A[2]:(-0.616965293884) A[3]:(0.86689645052)\n",
      " state (13)  A[0]:(0.0066406307742) A[1]:(1.0) A[2]:(0.892518818378) A[3]:(0.872475862503)\n",
      " state (14)  A[0]:(0.850923299789) A[1]:(1.0) A[2]:(0.999906778336) A[3]:(0.838588476181)\n",
      " state (15)  A[0]:(0.989679336548) A[1]:(1.0) A[2]:(0.999999940395) A[3]:(0.780219078064)\n",
      "Episode 99000 finished after 0 timesteps with r=0.0. Running score: 0.17. Times trained:               12743. Times reached goal: 185.               Steps done: 868780. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.37751706136.\n",
      "q_values \n",
      "tensor([[ 0.5887,  0.6546,  0.5227,  0.5879]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5888,  0.6549,  0.5230,  0.5879]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6501,  0.7282, -0.0005,  0.5702]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.588940858841) A[1]:(0.655208587646) A[2]:(0.523039937019) A[3]:(0.588020920753)\n",
      " state (1)  A[0]:(0.578969240189) A[1]:(0.0401704385877) A[2]:(0.500323534012) A[3]:(0.54495537281)\n",
      " state (2)  A[0]:(0.552703917027) A[1]:(0.338270187378) A[2]:(0.518735349178) A[3]:(0.480766296387)\n",
      " state (3)  A[0]:(0.53794169426) A[1]:(0.562774538994) A[2]:(0.481651842594) A[3]:(0.449918806553)\n",
      " state (4)  A[0]:(0.650089383125) A[1]:(0.728444397449) A[2]:(-0.000833570782561) A[3]:(0.570145666599)\n",
      " state (5)  A[0]:(0.554385960102) A[1]:(0.715994715691) A[2]:(-0.0195895396173) A[3]:(0.651043057442)\n",
      " state (6)  A[0]:(0.316371530294) A[1]:(0.517152309418) A[2]:(0.0963762253523) A[3]:(0.771342515945)\n",
      " state (7)  A[0]:(0.154264301062) A[1]:(0.101444683969) A[2]:(0.358728677034) A[3]:(0.837414860725)\n",
      " state (8)  A[0]:(0.674736022949) A[1]:(0.0110347559676) A[2]:(0.806900084019) A[3]:(0.600496768951)\n",
      " state (9)  A[0]:(0.890808105469) A[1]:(0.895718753338) A[2]:(0.87062984705) A[3]:(0.0585485883057)\n",
      " state (10)  A[0]:(0.522219061852) A[1]:(0.999857723713) A[2]:(0.0306371506304) A[3]:(0.404047220945)\n",
      " state (11)  A[0]:(-0.392359435558) A[1]:(0.999999821186) A[2]:(-0.802315413952) A[3]:(0.767006993294)\n",
      " state (12)  A[0]:(-0.560103714466) A[1]:(1.0) A[2]:(-0.623373985291) A[3]:(0.86355561018)\n",
      " state (13)  A[0]:(0.010268766433) A[1]:(1.0) A[2]:(0.896298646927) A[3]:(0.870142757893)\n",
      " state (14)  A[0]:(0.855682075024) A[1]:(1.0) A[2]:(0.999920487404) A[3]:(0.838533997536)\n",
      " state (15)  A[0]:(0.990268945694) A[1]:(1.0) A[2]:(0.999999940395) A[3]:(0.784709215164)\n",
      "Episode 100000 finished after 0 timesteps with r=0.0. Running score: 0.25. Times trained:               12334. Times reached goal: 194.               Steps done: 881114. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.372889363604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.583828687668) A[1]:(0.648790717125) A[2]:(0.518089056015) A[3]:(0.58414298296)\n",
      " state (1)  A[0]:(0.573616504669) A[1]:(0.0277822036296) A[2]:(0.498718649149) A[3]:(0.536642789841)\n",
      " state (2)  A[0]:(0.542408704758) A[1]:(0.346129745245) A[2]:(0.515142440796) A[3]:(0.462435781956)\n",
      " state (3)  A[0]:(0.536494493484) A[1]:(0.581952691078) A[2]:(0.451738774776) A[3]:(0.433800578117)\n",
      " state (4)  A[0]:(0.643961548805) A[1]:(0.723680734634) A[2]:(-0.00203728396446) A[3]:(0.564382016659)\n",
      " state (5)  A[0]:(0.538953185081) A[1]:(0.718195676804) A[2]:(0.0088129742071) A[3]:(0.63258934021)\n",
      " state (6)  A[0]:(0.309208810329) A[1]:(0.532688140869) A[2]:(0.108733609319) A[3]:(0.75541639328)\n",
      " state (7)  A[0]:(0.156091168523) A[1]:(0.113934397697) A[2]:(0.355354815722) A[3]:(0.828705608845)\n",
      " state (8)  A[0]:(0.669412493706) A[1]:(0.00532997585833) A[2]:(0.805250644684) A[3]:(0.5947291255)\n",
      " state (9)  A[0]:(0.88659709692) A[1]:(0.89472258091) A[2]:(0.871900975704) A[3]:(0.0548040159047)\n",
      " state (10)  A[0]:(0.509205400944) A[1]:(0.999859988689) A[2]:(0.0263298675418) A[3]:(0.398932278156)\n",
      " state (11)  A[0]:(-0.401545077562) A[1]:(0.999999821186) A[2]:(-0.806740283966) A[3]:(0.762562274933)\n",
      " state (12)  A[0]:(-0.564218640327) A[1]:(1.0) A[2]:(-0.629423379898) A[3]:(0.860397100449)\n",
      " state (13)  A[0]:(0.00850765034556) A[1]:(1.0) A[2]:(0.8988519907) A[3]:(0.86866748333)\n",
      " state (14)  A[0]:(0.858023226261) A[1]:(1.0) A[2]:(0.999930739403) A[3]:(0.84061384201)\n",
      " state (15)  A[0]:(0.99066567421) A[1]:(1.0) A[2]:(1.0) A[3]:(0.793006122112)\n",
      "Episode 101000 finished after 0 timesteps with r=1.0. Running score: 0.22. Times trained:               12046. Times reached goal: 200.               Steps done: 893160. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.368424484288.\n",
      " state (0)  A[0]:(0.584455728531) A[1]:(0.654720067978) A[2]:(0.522945404053) A[3]:(0.586214661598)\n",
      " state (1)  A[0]:(0.576796174049) A[1]:(0.0224705059081) A[2]:(0.502587080002) A[3]:(0.541545152664)\n",
      " state (2)  A[0]:(0.54217684269) A[1]:(0.361224919558) A[2]:(0.522656738758) A[3]:(0.458776205778)\n",
      " state (3)  A[0]:(0.549906373024) A[1]:(0.602474689484) A[2]:(0.427178561687) A[3]:(0.440210074186)\n",
      " state (4)  A[0]:(0.645995497704) A[1]:(0.7249584198) A[2]:(-0.00309537863359) A[3]:(0.572702527046)\n",
      " state (5)  A[0]:(0.539135098457) A[1]:(0.729127526283) A[2]:(0.0284519866109) A[3]:(0.628465473652)\n",
      " state (6)  A[0]:(0.321713268757) A[1]:(0.555254101753) A[2]:(0.12582872808) A[3]:(0.750369310379)\n",
      " state (7)  A[0]:(0.178700864315) A[1]:(0.128226920962) A[2]:(0.366033822298) A[3]:(0.828341722488)\n",
      " state (8)  A[0]:(0.674848675728) A[1]:(0.00151479127817) A[2]:(0.807323932648) A[3]:(0.603459239006)\n",
      " state (9)  A[0]:(0.887385725975) A[1]:(0.894758760929) A[2]:(0.874184370041) A[3]:(0.0598266385496)\n",
      " state (10)  A[0]:(0.517419099808) A[1]:(0.99986499548) A[2]:(0.0316050872207) A[3]:(0.392089635134)\n",
      " state (11)  A[0]:(-0.390389144421) A[1]:(0.999999880791) A[2]:(-0.809153676033) A[3]:(0.756460905075)\n",
      " state (12)  A[0]:(-0.558525443077) A[1]:(1.0) A[2]:(-0.639374732971) A[3]:(0.856922864914)\n",
      " state (13)  A[0]:(0.0106438025832) A[1]:(1.0) A[2]:(0.897627174854) A[3]:(0.868011593819)\n",
      " state (14)  A[0]:(0.859543025494) A[1]:(1.0) A[2]:(0.999937176704) A[3]:(0.845121383667)\n",
      " state (15)  A[0]:(0.991007864475) A[1]:(1.0) A[2]:(1.0) A[3]:(0.805662512779)\n",
      "Episode 102000 finished after 0 timesteps with r=0.0. Running score: 0.24. Times trained:               12840. Times reached goal: 207.               Steps done: 906000. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.363724154603.\n",
      " state (0)  A[0]:(0.591424465179) A[1]:(0.6524720788) A[2]:(0.525447845459) A[3]:(0.589273333549)\n",
      " state (1)  A[0]:(0.584272027016) A[1]:(0.0170297846198) A[2]:(0.506624817848) A[3]:(0.546276926994)\n",
      " state (2)  A[0]:(0.546301066875) A[1]:(0.370001852512) A[2]:(0.529413104057) A[3]:(0.45421642065)\n",
      " state (3)  A[0]:(0.559009850025) A[1]:(0.611166417599) A[2]:(0.419058054686) A[3]:(0.438178241253)\n",
      " state (4)  A[0]:(0.650425076485) A[1]:(0.721724271774) A[2]:(-0.00291155953892) A[3]:(0.572372317314)\n",
      " state (5)  A[0]:(0.547764718533) A[1]:(0.737821340561) A[2]:(0.0331708043814) A[3]:(0.616366744041)\n",
      " state (6)  A[0]:(0.344684422016) A[1]:(0.579643547535) A[2]:(0.12746424973) A[3]:(0.73635840416)\n",
      " state (7)  A[0]:(0.20893073082) A[1]:(0.151984155178) A[2]:(0.364252626896) A[3]:(0.820627748966)\n",
      " state (8)  A[0]:(0.676961183548) A[1]:(0.00306748389266) A[2]:(0.806069970131) A[3]:(0.602171778679)\n",
      " state (9)  A[0]:(0.883211433887) A[1]:(0.894955992699) A[2]:(0.87464350462) A[3]:(0.0620149150491)\n",
      " state (10)  A[0]:(0.50744664669) A[1]:(0.999867618084) A[2]:(0.0278086476028) A[3]:(0.389327794313)\n",
      " state (11)  A[0]:(-0.391035139561) A[1]:(0.999999880791) A[2]:(-0.812566876411) A[3]:(0.752060353756)\n",
      " state (12)  A[0]:(-0.556234002113) A[1]:(1.0) A[2]:(-0.649382352829) A[3]:(0.853279054165)\n",
      " state (13)  A[0]:(0.0101496670395) A[1]:(1.0) A[2]:(0.894538760185) A[3]:(0.866006970406)\n",
      " state (14)  A[0]:(0.860186994076) A[1]:(1.0) A[2]:(0.999940335751) A[3]:(0.846561074257)\n",
      " state (15)  A[0]:(0.991291582584) A[1]:(1.0) A[2]:(1.0) A[3]:(0.812492251396)\n",
      "Episode 103000 finished after 0 timesteps with r=0.0. Running score: 0.25. Times trained:               12751. Times reached goal: 221.               Steps done: 918751. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.359115751223.\n",
      " state (0)  A[0]:(0.589169621468) A[1]:(0.656333863735) A[2]:(0.526067495346) A[3]:(0.589201331139)\n",
      " state (1)  A[0]:(0.582371115685) A[1]:(0.0143555672839) A[2]:(0.502862930298) A[3]:(0.546074271202)\n",
      " state (2)  A[0]:(0.539386808872) A[1]:(0.381220459938) A[2]:(0.528182685375) A[3]:(0.442452132702)\n",
      " state (3)  A[0]:(0.575499534607) A[1]:(0.617159128189) A[2]:(0.363183021545) A[3]:(0.45168659091)\n",
      " state (4)  A[0]:(0.648530960083) A[1]:(0.725610435009) A[2]:(0.00111532164738) A[3]:(0.56823939085)\n",
      " state (5)  A[0]:(0.54400241375) A[1]:(0.751392364502) A[2]:(0.0478446893394) A[3]:(0.598905384541)\n",
      " state (6)  A[0]:(0.347270458937) A[1]:(0.605094909668) A[2]:(0.14097571373) A[3]:(0.719206809998)\n",
      " state (7)  A[0]:(0.214601308107) A[1]:(0.173194929957) A[2]:(0.372750520706) A[3]:(0.811995267868)\n",
      " state (8)  A[0]:(0.668857097626) A[1]:(0.00862720049918) A[2]:(0.80615645647) A[3]:(0.598924100399)\n",
      " state (9)  A[0]:(0.877417445183) A[1]:(0.898224949837) A[2]:(0.874594271183) A[3]:(0.053857319057)\n",
      " state (10)  A[0]:(0.494117259979) A[1]:(0.999875962734) A[2]:(0.0253612846136) A[3]:(0.374425143003)\n",
      " state (11)  A[0]:(-0.398103564978) A[1]:(0.999999880791) A[2]:(-0.813289284706) A[3]:(0.741344749928)\n",
      " state (12)  A[0]:(-0.559413552284) A[1]:(1.0) A[2]:(-0.647204637527) A[3]:(0.845653533936)\n",
      " state (13)  A[0]:(0.00644264416769) A[1]:(1.0) A[2]:(0.900002062321) A[3]:(0.860129714012)\n",
      " state (14)  A[0]:(0.860932350159) A[1]:(1.0) A[2]:(0.999949932098) A[3]:(0.843245983124)\n",
      " state (15)  A[0]:(0.991479158401) A[1]:(1.0) A[2]:(1.0) A[3]:(0.81312173605)\n",
      "Episode 104000 finished after 0 timesteps with r=1.0. Running score: 0.21. Times trained:               12705. Times reached goal: 198.               Steps done: 931456. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.354582046945.\n",
      "q_values \n",
      "tensor([[ 0.5853,  0.6511,  0.5271,  0.5846]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5853,  0.6510,  0.5268,  0.5845]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6434,  0.7210, -0.0010,  0.5662]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6653,  0.0014,  0.8021,  0.6017]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8751,  0.8961,  0.8737,  0.0575]], device='cuda:0')\n",
      "On state=9, selected action=3 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.585032045841) A[1]:(0.65135037899) A[2]:(0.526124238968) A[3]:(0.5841370821)\n",
      " state (1)  A[0]:(0.580940246582) A[1]:(0.00571417529136) A[2]:(0.497309744358) A[3]:(0.542058587074)\n",
      " state (2)  A[0]:(0.533678412437) A[1]:(0.393978476524) A[2]:(0.52301710844) A[3]:(0.425607979298)\n",
      " state (3)  A[0]:(0.611168444157) A[1]:(0.604033470154) A[2]:(0.248615860939) A[3]:(0.49607899785)\n",
      " state (4)  A[0]:(0.642778813839) A[1]:(0.721858382225) A[2]:(-0.00211888225749) A[3]:(0.565319359303)\n",
      " state (5)  A[0]:(0.536269187927) A[1]:(0.76169615984) A[2]:(0.0494644157588) A[3]:(0.581487059593)\n",
      " state (6)  A[0]:(0.345766365528) A[1]:(0.629817128181) A[2]:(0.138366281986) A[3]:(0.701674640179)\n",
      " state (7)  A[0]:(0.219652831554) A[1]:(0.195077717304) A[2]:(0.365888267756) A[3]:(0.804291665554)\n",
      " state (8)  A[0]:(0.66502058506) A[1]:(0.00480517046526) A[2]:(0.801880896091) A[3]:(0.600360274315)\n",
      " state (9)  A[0]:(0.874890565872) A[1]:(0.896681368351) A[2]:(0.873424649239) A[3]:(0.0571070872247)\n",
      " state (10)  A[0]:(0.495524078608) A[1]:(0.999876260757) A[2]:(0.0193851739168) A[3]:(0.37013655901)\n",
      " state (11)  A[0]:(-0.390966624022) A[1]:(0.999999880791) A[2]:(-0.819643378258) A[3]:(0.73711669445)\n",
      " state (12)  A[0]:(-0.55648624897) A[1]:(1.0) A[2]:(-0.667585372925) A[3]:(0.843047201633)\n",
      " state (13)  A[0]:(0.00274199293926) A[1]:(1.0) A[2]:(0.892701864243) A[3]:(0.859599351883)\n",
      " state (14)  A[0]:(0.860164284706) A[1]:(1.0) A[2]:(0.999950408936) A[3]:(0.846540391445)\n",
      " state (15)  A[0]:(0.991586565971) A[1]:(1.0) A[2]:(1.0) A[3]:(0.821897089481)\n",
      "Episode 105000 finished after 0 timesteps with r=0.0. Running score: 0.23. Times trained:               12901. Times reached goal: 203.               Steps done: 944357. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.350036965046.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.589686632156) A[1]:(0.653596639633) A[2]:(0.532099187374) A[3]:(0.589158415794)\n",
      " state (1)  A[0]:(0.586107611656) A[1]:(0.0135962925851) A[2]:(0.513058304787) A[3]:(0.552915334702)\n",
      " state (2)  A[0]:(0.537214517593) A[1]:(0.445206701756) A[2]:(0.538919150829) A[3]:(0.423215508461)\n",
      " state (3)  A[0]:(0.667527973652) A[1]:(0.562297999859) A[2]:(0.0981311574578) A[3]:(0.584715008736)\n",
      " state (4)  A[0]:(0.644285440445) A[1]:(0.725782036781) A[2]:(0.00721442094073) A[3]:(0.571089029312)\n",
      " state (5)  A[0]:(0.536341428757) A[1]:(0.785867393017) A[2]:(0.064227797091) A[3]:(0.571282744408)\n",
      " state (6)  A[0]:(0.350737065077) A[1]:(0.673629820347) A[2]:(0.151272714138) A[3]:(0.691902816296)\n",
      " state (7)  A[0]:(0.23498877883) A[1]:(0.235611781478) A[2]:(0.375343978405) A[3]:(0.804268121719)\n",
      " state (8)  A[0]:(0.679266035557) A[1]:(0.0124262133613) A[2]:(0.806244373322) A[3]:(0.607428073883)\n",
      " state (9)  A[0]:(0.880942344666) A[1]:(0.899640858173) A[2]:(0.876038432121) A[3]:(0.0589966326952)\n",
      " state (10)  A[0]:(0.514452815056) A[1]:(0.999887108803) A[2]:(0.0251013953239) A[3]:(0.359942287207)\n",
      " state (11)  A[0]:(-0.375244677067) A[1]:(0.999999880791) A[2]:(-0.821640491486) A[3]:(0.728182077408)\n",
      " state (12)  A[0]:(-0.549934625626) A[1]:(1.0) A[2]:(-0.673564314842) A[3]:(0.837603867054)\n",
      " state (13)  A[0]:(0.00989021919668) A[1]:(1.0) A[2]:(0.896095335484) A[3]:(0.857034862041)\n",
      " state (14)  A[0]:(0.863930761814) A[1]:(1.0) A[2]:(0.999958276749) A[3]:(0.848246693611)\n",
      " state (15)  A[0]:(0.991915106773) A[1]:(1.0) A[2]:(1.0) A[3]:(0.82939517498)\n",
      "Episode 106000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained:               13852. Times reached goal: 199.               Steps done: 958209. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.345221680661.\n",
      " state (0)  A[0]:(0.588417768478) A[1]:(0.654388427734) A[2]:(0.522369146347) A[3]:(0.588127613068)\n",
      " state (1)  A[0]:(0.58591401577) A[1]:(0.0121806794778) A[2]:(0.505741000175) A[3]:(0.560262441635)\n",
      " state (2)  A[0]:(0.537225067616) A[1]:(0.491958588362) A[2]:(0.524847984314) A[3]:(0.42050755024)\n",
      " state (3)  A[0]:(0.696921765804) A[1]:(0.480332791805) A[2]:(-0.00892728660256) A[3]:(0.642776966095)\n",
      " state (4)  A[0]:(0.641790032387) A[1]:(0.726431071758) A[2]:(-0.00502528483048) A[3]:(0.570940732956)\n",
      " state (5)  A[0]:(0.52675318718) A[1]:(0.810975968838) A[2]:(0.0537244863808) A[3]:(0.549920558929)\n",
      " state (6)  A[0]:(0.336765468121) A[1]:(0.720730364323) A[2]:(0.138287708163) A[3]:(0.669604480267)\n",
      " state (7)  A[0]:(0.230714872479) A[1]:(0.281392782927) A[2]:(0.366517931223) A[3]:(0.794845104218)\n",
      " state (8)  A[0]:(0.698474287987) A[1]:(0.00591426156461) A[2]:(0.810975193977) A[3]:(0.597986161709)\n",
      " state (9)  A[0]:(0.892624080181) A[1]:(0.897085428238) A[2]:(0.880250096321) A[3]:(0.0585669726133)\n",
      " state (10)  A[0]:(0.544400513172) A[1]:(0.999889075756) A[2]:(0.0278160329908) A[3]:(0.366445541382)\n",
      " state (11)  A[0]:(-0.355280458927) A[1]:(0.999999880791) A[2]:(-0.827040910721) A[3]:(0.731844544411)\n",
      " state (12)  A[0]:(-0.545352101326) A[1]:(1.0) A[2]:(-0.689126551151) A[3]:(0.840216159821)\n",
      " state (13)  A[0]:(0.00865225493908) A[1]:(1.0) A[2]:(0.893930017948) A[3]:(0.860274672508)\n",
      " state (14)  A[0]:(0.864047706127) A[1]:(1.0) A[2]:(0.999961853027) A[3]:(0.853411793709)\n",
      " state (15)  A[0]:(0.991891741753) A[1]:(1.0) A[2]:(1.0) A[3]:(0.837199211121)\n",
      "Episode 107000 finished after 0 timesteps with r=0.0. Running score: 0.17. Times trained:               12734. Times reached goal: 204.               Steps done: 970943. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.340853499019.\n",
      " state (0)  A[0]:(0.583661258221) A[1]:(0.653755187988) A[2]:(0.522618293762) A[3]:(0.585834383965)\n",
      " state (1)  A[0]:(0.583833098412) A[1]:(0.0112773766741) A[2]:(0.506145000458) A[3]:(0.564026594162)\n",
      " state (2)  A[0]:(0.531882882118) A[1]:(0.536144852638) A[2]:(0.528701663017) A[3]:(0.410802960396)\n",
      " state (3)  A[0]:(0.708121538162) A[1]:(0.412294358015) A[2]:(-0.0182278230786) A[3]:(0.671500563622)\n",
      " state (4)  A[0]:(0.642749726772) A[1]:(0.724629282951) A[2]:(0.00368521921337) A[3]:(0.576861202717)\n",
      " state (5)  A[0]:(0.518883466721) A[1]:(0.831237971783) A[2]:(0.059527091682) A[3]:(0.534942030907)\n",
      " state (6)  A[0]:(0.318731099367) A[1]:(0.759530067444) A[2]:(0.133906885982) A[3]:(0.652395665646)\n",
      " state (7)  A[0]:(0.207946464419) A[1]:(0.325032025576) A[2]:(0.352918237448) A[3]:(0.792462348938)\n",
      " state (8)  A[0]:(0.696603059769) A[1]:(0.0125148911029) A[2]:(0.809171497822) A[3]:(0.611051082611)\n",
      " state (9)  A[0]:(0.895469784737) A[1]:(0.902405202389) A[2]:(0.880719661713) A[3]:(0.0943250060081)\n",
      " state (10)  A[0]:(0.549085021019) A[1]:(0.999902069569) A[2]:(0.016102174297) A[3]:(0.406663000584)\n",
      " state (11)  A[0]:(-0.349920868874) A[1]:(0.999999880791) A[2]:(-0.830155730247) A[3]:(0.751062273979)\n",
      " state (12)  A[0]:(-0.541934251785) A[1]:(1.0) A[2]:(-0.692919850349) A[3]:(0.848950266838)\n",
      " state (13)  A[0]:(0.00890626292676) A[1]:(1.0) A[2]:(0.894126653671) A[3]:(0.865190804005)\n",
      " state (14)  A[0]:(0.864528059959) A[1]:(1.0) A[2]:(0.99996471405) A[3]:(0.856210589409)\n",
      " state (15)  A[0]:(0.99198371172) A[1]:(1.0) A[2]:(1.0) A[3]:(0.838054537773)\n",
      "Episode 108000 finished after 0 timesteps with r=0.0. Running score: 0.18. Times trained:               13001. Times reached goal: 217.               Steps done: 983944. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.336450744797.\n",
      " state (0)  A[0]:(0.587665081024) A[1]:(0.654507637024) A[2]:(0.532207071781) A[3]:(0.589201033115)\n",
      " state (1)  A[0]:(0.588636755943) A[1]:(0.0212040934712) A[2]:(0.541904568672) A[3]:(0.575812160969)\n",
      " state (2)  A[0]:(0.529666304588) A[1]:(0.593113362789) A[2]:(0.568606138229) A[3]:(0.409209936857)\n",
      " state (3)  A[0]:(0.715823292732) A[1]:(0.361495643854) A[2]:(-0.0284011848271) A[3]:(0.691403031349)\n",
      " state (4)  A[0]:(0.640563130379) A[1]:(0.733543515205) A[2]:(-0.00370536535047) A[3]:(0.582053303719)\n",
      " state (5)  A[0]:(0.502461910248) A[1]:(0.85591673851) A[2]:(0.0474917404354) A[3]:(0.524216771126)\n",
      " state (6)  A[0]:(0.283949077129) A[1]:(0.800797224045) A[2]:(0.110847860575) A[3]:(0.642372846603)\n",
      " state (7)  A[0]:(0.169438287616) A[1]:(0.377633213997) A[2]:(0.32914274931) A[3]:(0.791932880878)\n",
      " state (8)  A[0]:(0.697019159794) A[1]:(0.00605569826439) A[2]:(0.813671946526) A[3]:(0.597290098667)\n",
      " state (9)  A[0]:(0.897610664368) A[1]:(0.89908772707) A[2]:(0.88612651825) A[3]:(0.0683945268393)\n",
      " state (10)  A[0]:(0.548804163933) A[1]:(0.999902904034) A[2]:(0.0204831324518) A[3]:(0.406201660633)\n",
      " state (11)  A[0]:(-0.348230957985) A[1]:(0.999999880791) A[2]:(-0.830440342426) A[3]:(0.755733370781)\n",
      " state (12)  A[0]:(-0.540603876114) A[1]:(1.0) A[2]:(-0.692930221558) A[3]:(0.851504027843)\n",
      " state (13)  A[0]:(0.0117017719895) A[1]:(1.0) A[2]:(0.898077368736) A[3]:(0.865999698639)\n",
      " state (14)  A[0]:(0.867988169193) A[1]:(1.0) A[2]:(0.99996972084) A[3]:(0.855486392975)\n",
      " state (15)  A[0]:(0.99236446619) A[1]:(1.0) A[2]:(1.0) A[3]:(0.835648655891)\n",
      "Episode 109000 finished after 0 timesteps with r=0.0. Running score: 0.33. Times trained:               13031. Times reached goal: 224.               Steps done: 996975. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.332094897303.\n",
      "q_values \n",
      "tensor([[ 0.5872,  0.6547,  0.5270,  0.5875]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6419,  0.7313, -0.0049,  0.5859]], device='cuda:0')\n",
      "On state=4, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5871,  0.6540,  0.5265,  0.5876]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5870,  0.6538,  0.5264,  0.5876]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6414,  0.7288, -0.0080,  0.5873]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6951, -0.0031,  0.8095,  0.6084]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8971,  0.9005,  0.8855,  0.0761]], device='cuda:0')\n",
      "On state=9, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6957,  0.0016,  0.8105,  0.6104]], device='cuda:0')\n",
      "On state=8, selected action=1 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.587332606316) A[1]:(0.65513420105) A[2]:(0.528794527054) A[3]:(0.587803006172)\n",
      " state (1)  A[0]:(0.588337242603) A[1]:(0.0203465148807) A[2]:(0.573373436928) A[3]:(0.582430839539)\n",
      " state (2)  A[0]:(0.526769399643) A[1]:(0.63513982296) A[2]:(0.606288909912) A[3]:(0.411744356155)\n",
      " state (3)  A[0]:(0.724704802036) A[1]:(0.288588285446) A[2]:(-0.0336856283247) A[3]:(0.704965949059)\n",
      " state (4)  A[0]:(0.642625212669) A[1]:(0.73076570034) A[2]:(-0.00237017427571) A[3]:(0.588687360287)\n",
      " state (5)  A[0]:(0.493527591228) A[1]:(0.870132923126) A[2]:(0.0447900220752) A[3]:(0.526233553886)\n",
      " state (6)  A[0]:(0.25902813673) A[1]:(0.826070129871) A[2]:(0.0925210118294) A[3]:(0.651730060577)\n",
      " state (7)  A[0]:(0.139360651374) A[1]:(0.412194430828) A[2]:(0.298772513866) A[3]:(0.807394385338)\n",
      " state (8)  A[0]:(0.696190714836) A[1]:(0.00394944287837) A[2]:(0.811279535294) A[3]:(0.611004710197)\n",
      " state (9)  A[0]:(0.897364258766) A[1]:(0.901789307594) A[2]:(0.886174738407) A[3]:(0.079059638083)\n",
      " state (10)  A[0]:(0.545895397663) A[1]:(0.999912500381) A[2]:(0.0141439540312) A[3]:(0.430936217308)\n",
      " state (11)  A[0]:(-0.337431013584) A[1]:(0.999999880791) A[2]:(-0.827767968178) A[3]:(0.76921916008)\n",
      " state (12)  A[0]:(-0.531171858311) A[1]:(1.0) A[2]:(-0.690554261208) A[3]:(0.857617676258)\n",
      " state (13)  A[0]:(0.0097432481125) A[1]:(1.0) A[2]:(0.895196914673) A[3]:(0.869111895561)\n",
      " state (14)  A[0]:(0.865980625153) A[1]:(1.0) A[2]:(0.999970197678) A[3]:(0.856938719749)\n",
      " state (15)  A[0]:(0.992456078529) A[1]:(1.0) A[2]:(1.0) A[3]:(0.835719227791)\n",
      "Episode 110000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained:               13662. Times reached goal: 238.               Steps done: 1010637. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.327588668953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.592566132545) A[1]:(0.655141115189) A[2]:(0.543003559113) A[3]:(0.595449566841)\n",
      " state (1)  A[0]:(0.594478607178) A[1]:(0.0232818368822) A[2]:(0.608646988869) A[3]:(0.593669354916)\n",
      " state (2)  A[0]:(0.530615448952) A[1]:(0.681406676769) A[2]:(0.652503669262) A[3]:(0.421110153198)\n",
      " state (3)  A[0]:(0.733507692814) A[1]:(0.227258354425) A[2]:(-0.0423836559057) A[3]:(0.714984834194)\n",
      " state (4)  A[0]:(0.646639823914) A[1]:(0.731706917286) A[2]:(-0.00265049305744) A[3]:(0.597798585892)\n",
      " state (5)  A[0]:(0.487183958292) A[1]:(0.883494377136) A[2]:(0.045082565397) A[3]:(0.536683797836)\n",
      " state (6)  A[0]:(0.236078903079) A[1]:(0.848030567169) A[2]:(0.079206854105) A[3]:(0.669930517673)\n",
      " state (7)  A[0]:(0.111143760383) A[1]:(0.446980446577) A[2]:(0.273731321096) A[3]:(0.82485884428)\n",
      " state (8)  A[0]:(0.695511341095) A[1]:(-0.00305544375442) A[2]:(0.812058627605) A[3]:(0.618122518063)\n",
      " state (9)  A[0]:(0.895378291607) A[1]:(0.900667369366) A[2]:(0.887234568596) A[3]:(0.0723946094513)\n",
      " state (10)  A[0]:(0.538243591785) A[1]:(0.999916136265) A[2]:(0.0144096771255) A[3]:(0.440055668354)\n",
      " state (11)  A[0]:(-0.329316169024) A[1]:(0.999999880791) A[2]:(-0.822157680988) A[3]:(0.774380266666)\n",
      " state (12)  A[0]:(-0.523446202278) A[1]:(1.0) A[2]:(-0.682620406151) A[3]:(0.858669698238)\n",
      " state (13)  A[0]:(0.00583382043988) A[1]:(1.0) A[2]:(0.895168423653) A[3]:(0.868194460869)\n",
      " state (14)  A[0]:(0.863913178444) A[1]:(1.0) A[2]:(0.999971866608) A[3]:(0.85509455204)\n",
      " state (15)  A[0]:(0.992566525936) A[1]:(1.0) A[2]:(1.0) A[3]:(0.833409130573)\n",
      "Episode 111000 finished after 0 timesteps with r=0.0. Running score: 0.28. Times trained:               13641. Times reached goal: 250.               Steps done: 1024278. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.323150372145.\n",
      " state (0)  A[0]:(0.591813147068) A[1]:(0.660108149052) A[2]:(0.58154630661) A[3]:(0.592912435532)\n",
      " state (1)  A[0]:(0.600932240486) A[1]:(0.0273555796593) A[2]:(0.649168252945) A[3]:(0.63299703598)\n",
      " state (2)  A[0]:(0.53418803215) A[1]:(0.736555695534) A[2]:(0.704472661018) A[3]:(0.461449354887)\n",
      " state (3)  A[0]:(0.738043189049) A[1]:(0.184630274773) A[2]:(-0.0404658950865) A[3]:(0.712164521217)\n",
      " state (4)  A[0]:(0.651420354843) A[1]:(0.729655325413) A[2]:(-0.0117832925171) A[3]:(0.598990559578)\n",
      " state (5)  A[0]:(0.484448760748) A[1]:(0.89225757122) A[2]:(0.0300239063799) A[3]:(0.544416487217)\n",
      " state (6)  A[0]:(0.21914049983) A[1]:(0.8627846241) A[2]:(0.0474375598133) A[3]:(0.687919259071)\n",
      " state (7)  A[0]:(0.0910430327058) A[1]:(0.476108819246) A[2]:(0.234108909965) A[3]:(0.840341866016)\n",
      " state (8)  A[0]:(0.700516104698) A[1]:(-0.00698793493211) A[2]:(0.810114502907) A[3]:(0.622215151787)\n",
      " state (9)  A[0]:(0.895223915577) A[1]:(0.900661945343) A[2]:(0.886399447918) A[3]:(0.0685629248619)\n",
      " state (10)  A[0]:(0.53560090065) A[1]:(0.999920606613) A[2]:(0.00521976966411) A[3]:(0.454287469387)\n",
      " state (11)  A[0]:(-0.316884130239) A[1]:(0.999999880791) A[2]:(-0.819519162178) A[3]:(0.781580924988)\n",
      " state (12)  A[0]:(-0.513240933418) A[1]:(1.0) A[2]:(-0.679530978203) A[3]:(0.860695838928)\n",
      " state (13)  A[0]:(0.00562289403751) A[1]:(1.0) A[2]:(0.89385253191) A[3]:(0.868145346642)\n",
      " state (14)  A[0]:(0.862876474857) A[1]:(1.0) A[2]:(0.99997317791) A[3]:(0.854696750641)\n",
      " state (15)  A[0]:(0.99268245697) A[1]:(1.0) A[2]:(1.0) A[3]:(0.833597421646)\n",
      "Episode 112000 finished after 0 timesteps with r=0.0. Running score: 0.21. Times trained:               13640. Times reached goal: 236.               Steps done: 1037918. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.318772525855.\n",
      " state (0)  A[0]:(0.591876029968) A[1]:(0.655409932137) A[2]:(0.610818743706) A[3]:(0.59300994873)\n",
      " state (1)  A[0]:(0.601838588715) A[1]:(0.0237202849239) A[2]:(0.678185582161) A[3]:(0.673523545265)\n",
      " state (2)  A[0]:(0.535989165306) A[1]:(0.766319513321) A[2]:(0.730405509472) A[3]:(0.515721440315)\n",
      " state (3)  A[0]:(0.738335371017) A[1]:(0.133004620671) A[2]:(-0.0298467408866) A[3]:(0.711980819702)\n",
      " state (4)  A[0]:(0.645743310452) A[1]:(0.730219841003) A[2]:(-0.00516553083435) A[3]:(0.598119616508)\n",
      " state (5)  A[0]:(0.467382609844) A[1]:(0.900171041489) A[2]:(0.0237637124956) A[3]:(0.549917697906)\n",
      " state (6)  A[0]:(0.185264363885) A[1]:(0.877712607384) A[2]:(0.0219869930297) A[3]:(0.70059633255)\n",
      " state (7)  A[0]:(0.0539882741868) A[1]:(0.521763980389) A[2]:(0.200157135725) A[3]:(0.850279331207)\n",
      " state (8)  A[0]:(0.696149230003) A[1]:(0.00859006959945) A[2]:(0.809576511383) A[3]:(0.621323823929)\n",
      " state (9)  A[0]:(0.893778324127) A[1]:(0.900893747807) A[2]:(0.887477636337) A[3]:(0.0619703233242)\n",
      " state (10)  A[0]:(0.536197781563) A[1]:(0.999923467636) A[2]:(0.00994864106178) A[3]:(0.46572458744)\n",
      " state (11)  A[0]:(-0.296994775534) A[1]:(0.999999880791) A[2]:(-0.812376379967) A[3]:(0.786727786064)\n",
      " state (12)  A[0]:(-0.497140347958) A[1]:(1.0) A[2]:(-0.671704053879) A[3]:(0.861174762249)\n",
      " state (13)  A[0]:(0.0119174737483) A[1]:(1.0) A[2]:(0.892913937569) A[3]:(0.866926074028)\n",
      " state (14)  A[0]:(0.863642156124) A[1]:(1.0) A[2]:(0.999974250793) A[3]:(0.854140758514)\n",
      " state (15)  A[0]:(0.992928683758) A[1]:(1.0) A[2]:(1.0) A[3]:(0.835213422775)\n",
      "Episode 113000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               14239. Times reached goal: 255.               Steps done: 1052157. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.314265686449.\n",
      " state (0)  A[0]:(0.59288918972) A[1]:(0.656551837921) A[2]:(0.627384901047) A[3]:(0.592750787735)\n",
      " state (1)  A[0]:(0.602339029312) A[1]:(0.01853402704) A[2]:(0.693396687508) A[3]:(0.68991112709)\n",
      " state (2)  A[0]:(0.53723937273) A[1]:(0.779613852501) A[2]:(0.738649129868) A[3]:(0.543297290802)\n",
      " state (3)  A[0]:(0.746429145336) A[1]:(0.0838451236486) A[2]:(-0.0313174799085) A[3]:(0.712083220482)\n",
      " state (4)  A[0]:(0.645622014999) A[1]:(0.73186391592) A[2]:(-0.00186562316958) A[3]:(0.598041772842)\n",
      " state (5)  A[0]:(0.453050047159) A[1]:(0.904306769371) A[2]:(0.0183371007442) A[3]:(0.559573531151)\n",
      " state (6)  A[0]:(0.150391027331) A[1]:(0.882003068924) A[2]:(0.0033614509739) A[3]:(0.720256209373)\n",
      " state (7)  A[0]:(0.0172516498715) A[1]:(0.527791082859) A[2]:(0.176592186093) A[3]:(0.864516079426)\n",
      " state (8)  A[0]:(0.697070240974) A[1]:(-0.0058403541334) A[2]:(0.810246288776) A[3]:(0.627384781837)\n",
      " state (9)  A[0]:(0.894377052784) A[1]:(0.902249276638) A[2]:(0.885430395603) A[3]:(0.0668597593904)\n",
      " state (10)  A[0]:(0.537058949471) A[1]:(0.999928593636) A[2]:(-0.00170302228071) A[3]:(0.486586898565)\n",
      " state (11)  A[0]:(-0.28299883008) A[1]:(0.999999880791) A[2]:(-0.80649548769) A[3]:(0.794996142387)\n",
      " state (12)  A[0]:(-0.488244920969) A[1]:(1.0) A[2]:(-0.657726168633) A[3]:(0.862879991531)\n",
      " state (13)  A[0]:(0.00766660878435) A[1]:(1.0) A[2]:(0.897202908993) A[3]:(0.866528630257)\n",
      " state (14)  A[0]:(0.861654102802) A[1]:(1.0) A[2]:(0.999977171421) A[3]:(0.854780614376)\n",
      " state (15)  A[0]:(0.992958664894) A[1]:(1.0) A[2]:(1.0) A[3]:(0.838719069958)\n",
      "Episode 114000 finished after 0 timesteps with r=0.0. Running score: 0.25. Times trained:               14068. Times reached goal: 254.               Steps done: 1066225. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.309875549399.\n",
      "q_values \n",
      "tensor([[ 0.5912,  0.6564,  0.6330,  0.5915]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5912,  0.6566,  0.6329,  0.5915]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6445,  0.7314, -0.0022,  0.5946]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6918,  0.0033,  0.8095,  0.6238]], device='cuda:0')\n",
      "On state=8, selected action=3 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6446,  0.7313, -0.0014,  0.5951]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.591042041779) A[1]:(0.657242953777) A[2]:(0.633112072945) A[3]:(0.591638565063)\n",
      " state (1)  A[0]:(0.60251557827) A[1]:(0.0181707404554) A[2]:(0.703050017357) A[3]:(0.697788476944)\n",
      " state (2)  A[0]:(0.542036652565) A[1]:(0.787578523159) A[2]:(0.740011096001) A[3]:(0.56096124649)\n",
      " state (3)  A[0]:(0.752985298634) A[1]:(0.0236107334495) A[2]:(-0.0402158461511) A[3]:(0.711296379566)\n",
      " state (4)  A[0]:(0.644571602345) A[1]:(0.731304526329) A[2]:(-0.000746369245462) A[3]:(0.595489084721)\n",
      " state (5)  A[0]:(0.440376728773) A[1]:(0.90833234787) A[2]:(0.0157690085471) A[3]:(0.56238090992)\n",
      " state (6)  A[0]:(0.122784718871) A[1]:(0.889372885227) A[2]:(-0.00743673462421) A[3]:(0.728921949863)\n",
      " state (7)  A[0]:(-0.0133075602353) A[1]:(0.555951833725) A[2]:(0.159533694386) A[3]:(0.870980739594)\n",
      " state (8)  A[0]:(0.691429078579) A[1]:(0.00428846850991) A[2]:(0.809594988823) A[3]:(0.62570387125)\n",
      " state (9)  A[0]:(0.893621921539) A[1]:(0.902254283428) A[2]:(0.885177612305) A[3]:(0.0576615147293)\n",
      " state (10)  A[0]:(0.541811645031) A[1]:(0.999927759171) A[2]:(0.00410542543977) A[3]:(0.493717461824)\n",
      " state (11)  A[0]:(-0.259757608175) A[1]:(0.999999880791) A[2]:(-0.795088410378) A[3]:(0.796464264393)\n",
      " state (12)  A[0]:(-0.472450315952) A[1]:(1.0) A[2]:(-0.643295764923) A[3]:(0.860269725323)\n",
      " state (13)  A[0]:(0.0080622471869) A[1]:(1.0) A[2]:(0.895674467087) A[3]:(0.862931966782)\n",
      " state (14)  A[0]:(0.860836148262) A[1]:(1.0) A[2]:(0.99997729063) A[3]:(0.854096710682)\n",
      " state (15)  A[0]:(0.993218660355) A[1]:(1.0) A[2]:(1.0) A[3]:(0.843470275402)\n",
      "Episode 115000 finished after 0 timesteps with r=0.0. Running score: 0.25. Times trained:               14142. Times reached goal: 263.               Steps done: 1080367. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.305524130783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.594693481922) A[1]:(0.658638119698) A[2]:(0.639879286289) A[3]:(0.595058560371)\n",
      " state (1)  A[0]:(0.612240672112) A[1]:(0.0190227329731) A[2]:(0.70948433876) A[3]:(0.702900767326)\n",
      " state (2)  A[0]:(0.561105012894) A[1]:(0.790228366852) A[2]:(0.737007081509) A[3]:(0.576473772526)\n",
      " state (3)  A[0]:(0.764588117599) A[1]:(-0.0305528882891) A[2]:(-0.0549395643175) A[3]:(0.712707042694)\n",
      " state (4)  A[0]:(0.65114068985) A[1]:(0.730071544647) A[2]:(-0.00296067330055) A[3]:(0.595828652382)\n",
      " state (5)  A[0]:(0.438952386379) A[1]:(0.910107553005) A[2]:(0.0133108384907) A[3]:(0.565853714943)\n",
      " state (6)  A[0]:(0.111330017447) A[1]:(0.892689287663) A[2]:(-0.0146896997467) A[3]:(0.734221756458)\n",
      " state (7)  A[0]:(-0.0242187771946) A[1]:(0.57100880146) A[2]:(0.147051841021) A[3]:(0.874099969864)\n",
      " state (8)  A[0]:(0.697938203812) A[1]:(0.000515997351613) A[2]:(0.80924987793) A[3]:(0.619716644287)\n",
      " state (9)  A[0]:(0.898140490055) A[1]:(0.899438500404) A[2]:(0.884867370129) A[3]:(0.0483611263335)\n",
      " state (10)  A[0]:(0.55887234211) A[1]:(0.999922573566) A[2]:(0.00886440929025) A[3]:(0.503377020359)\n",
      " state (11)  A[0]:(-0.229945540428) A[1]:(0.999999880791) A[2]:(-0.783108890057) A[3]:(0.798988103867)\n",
      " state (12)  A[0]:(-0.455618739128) A[1]:(1.0) A[2]:(-0.623737454414) A[3]:(0.857893943787)\n",
      " state (13)  A[0]:(0.0112726306543) A[1]:(1.0) A[2]:(0.899350047112) A[3]:(0.859259486198)\n",
      " state (14)  A[0]:(0.862138688564) A[1]:(1.0) A[2]:(0.999979436398) A[3]:(0.853696584702)\n",
      " state (15)  A[0]:(0.993539869785) A[1]:(1.0) A[2]:(1.0) A[3]:(0.848836421967)\n",
      "Episode 116000 finished after 0 timesteps with r=0.0. Running score: 0.24. Times trained:               13956. Times reached goal: 233.               Steps done: 1094323. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.30128985154.\n",
      " state (0)  A[0]:(0.590015053749) A[1]:(0.65609651804) A[2]:(0.641791820526) A[3]:(0.590782999992)\n",
      " state (1)  A[0]:(0.61742401123) A[1]:(0.0193782616407) A[2]:(0.711813926697) A[3]:(0.705741882324)\n",
      " state (2)  A[0]:(0.57600992918) A[1]:(0.792575359344) A[2]:(0.731839597225) A[3]:(0.587811410427)\n",
      " state (3)  A[0]:(0.770640969276) A[1]:(-0.0719278752804) A[2]:(-0.0680583789945) A[3]:(0.712032318115)\n",
      " state (4)  A[0]:(0.652494132519) A[1]:(0.731166481972) A[2]:(-0.00485785957426) A[3]:(0.594346761703)\n",
      " state (5)  A[0]:(0.431952357292) A[1]:(0.911925435066) A[2]:(0.0134568437934) A[3]:(0.567554712296)\n",
      " state (6)  A[0]:(0.0952574014664) A[1]:(0.895010411739) A[2]:(-0.0163501240313) A[3]:(0.739245533943)\n",
      " state (7)  A[0]:(-0.0392950475216) A[1]:(0.582532644272) A[2]:(0.142821520567) A[3]:(0.877851068974)\n",
      " state (8)  A[0]:(0.698239982128) A[1]:(0.00076889974298) A[2]:(0.811588764191) A[3]:(0.617681920528)\n",
      " state (9)  A[0]:(0.89917165041) A[1]:(0.89983779192) A[2]:(0.884478032589) A[3]:(0.054856043309)\n",
      " state (10)  A[0]:(0.570339977741) A[1]:(0.999917626381) A[2]:(0.00956446584314) A[3]:(0.530041694641)\n",
      " state (11)  A[0]:(-0.195209294558) A[1]:(0.999999821186) A[2]:(-0.770676016808) A[3]:(0.808875322342)\n",
      " state (12)  A[0]:(-0.43183773756) A[1]:(1.0) A[2]:(-0.60775077343) A[3]:(0.859803915024)\n",
      " state (13)  A[0]:(0.017758641392) A[1]:(1.0) A[2]:(0.896843254566) A[3]:(0.859083294868)\n",
      " state (14)  A[0]:(0.863103270531) A[1]:(1.0) A[2]:(0.99997895956) A[3]:(0.856955885887)\n",
      " state (15)  A[0]:(0.993915915489) A[1]:(1.0) A[2]:(1.0) A[3]:(0.858397841454)\n",
      "Episode 117000 finished after 0 timesteps with r=0.0. Running score: 0.19. Times trained:               14193. Times reached goal: 241.               Steps done: 1108516. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.297043847719.\n",
      " state (0)  A[0]:(0.592689752579) A[1]:(0.660168707371) A[2]:(0.645676136017) A[3]:(0.590384840965)\n",
      " state (1)  A[0]:(0.634951233864) A[1]:(0.0189882945269) A[2]:(0.717383742332) A[3]:(0.710272669792)\n",
      " state (2)  A[0]:(0.598197579384) A[1]:(0.800456941128) A[2]:(0.734946250916) A[3]:(0.597234129906)\n",
      " state (3)  A[0]:(0.777171969414) A[1]:(-0.102087944746) A[2]:(-0.0687408670783) A[3]:(0.711155056953)\n",
      " state (4)  A[0]:(0.653923869133) A[1]:(0.733599185944) A[2]:(0.000390171975596) A[3]:(0.59255695343)\n",
      " state (5)  A[0]:(0.422626405954) A[1]:(0.913865149021) A[2]:(0.0184034779668) A[3]:(0.569301128387)\n",
      " state (6)  A[0]:(0.0749624371529) A[1]:(0.896095812321) A[2]:(-0.0164697766304) A[3]:(0.746934115887)\n",
      " state (7)  A[0]:(-0.0598368681967) A[1]:(0.587789952755) A[2]:(0.135165482759) A[3]:(0.883972048759)\n",
      " state (8)  A[0]:(0.692035377026) A[1]:(0.000615417899098) A[2]:(0.811262428761) A[3]:(0.61594825983)\n",
      " state (9)  A[0]:(0.898326158524) A[1]:(0.899487376213) A[2]:(0.884306609631) A[3]:(0.039012439549)\n",
      " state (10)  A[0]:(0.577817678452) A[1]:(0.999910950661) A[2]:(0.0156646072865) A[3]:(0.531991541386)\n",
      " state (11)  A[0]:(-0.169929713011) A[1]:(0.999999761581) A[2]:(-0.757120668888) A[3]:(0.806589603424)\n",
      " state (12)  A[0]:(-0.41826274991) A[1]:(1.0) A[2]:(-0.587369501591) A[3]:(0.853232860565)\n",
      " state (13)  A[0]:(0.0176733657718) A[1]:(1.0) A[2]:(0.900231003761) A[3]:(0.85170853138)\n",
      " state (14)  A[0]:(0.86564964056) A[1]:(1.0) A[2]:(0.999981105328) A[3]:(0.855561196804)\n",
      " state (15)  A[0]:(0.994390189648) A[1]:(1.0) A[2]:(1.0) A[3]:(0.865740895271)\n",
      "Episode 118000 finished after 0 timesteps with r=0.0. Running score: 0.28. Times trained:               14941. Times reached goal: 262.               Steps done: 1123457. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.292638706148.\n",
      " state (0)  A[0]:(0.591323256493) A[1]:(0.660473585129) A[2]:(0.645791947842) A[3]:(0.591081976891)\n",
      " state (1)  A[0]:(0.634607613087) A[1]:(0.0169171039015) A[2]:(0.715783715248) A[3]:(0.714720010757)\n",
      " state (2)  A[0]:(0.606486439705) A[1]:(0.801478922367) A[2]:(0.73292195797) A[3]:(0.608353614807)\n",
      " state (3)  A[0]:(0.781311810017) A[1]:(-0.128678306937) A[2]:(-0.0802435427904) A[3]:(0.712195634842)\n",
      " state (4)  A[0]:(0.655562281609) A[1]:(0.733173549175) A[2]:(0.000243186950684) A[3]:(0.593891918659)\n",
      " state (5)  A[0]:(0.418666213751) A[1]:(0.914217531681) A[2]:(0.0227841045707) A[3]:(0.57341837883)\n",
      " state (6)  A[0]:(0.0677936077118) A[1]:(0.895964562893) A[2]:(-0.0138413412496) A[3]:(0.754903137684)\n",
      " state (7)  A[0]:(-0.065276697278) A[1]:(0.591412365437) A[2]:(0.129916518927) A[3]:(0.89067864418)\n",
      " state (8)  A[0]:(0.685480415821) A[1]:(0.00241082441062) A[2]:(0.809349238873) A[3]:(0.624861359596)\n",
      " state (9)  A[0]:(0.894620478153) A[1]:(0.900548279285) A[2]:(0.881033420563) A[3]:(0.0536091849208)\n",
      " state (10)  A[0]:(0.578346848488) A[1]:(0.999901533127) A[2]:(0.00764474272728) A[3]:(0.556556224823)\n",
      " state (11)  A[0]:(-0.148107990623) A[1]:(0.999999642372) A[2]:(-0.74708288908) A[3]:(0.812704265118)\n",
      " state (12)  A[0]:(-0.407801538706) A[1]:(1.0) A[2]:(-0.57818543911) A[3]:(0.851466715336)\n",
      " state (13)  A[0]:(0.00476423790678) A[1]:(1.0) A[2]:(0.894204854965) A[3]:(0.848552167416)\n",
      " state (14)  A[0]:(0.862415671349) A[1]:(1.0) A[2]:(0.999979794025) A[3]:(0.859179258347)\n",
      " state (15)  A[0]:(0.994646012783) A[1]:(1.0) A[2]:(1.0) A[3]:(0.878860890865)\n",
      "Episode 119000 finished after 0 timesteps with r=0.0. Running score: 0.22. Times trained:               15056. Times reached goal: 255.               Steps done: 1138513. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.288265740083.\n",
      "q_values \n",
      "tensor([[ 0.5929,  0.6572,  0.6453,  0.5918]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5928,  0.6571,  0.6454,  0.5918]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6525,  0.7279,  0.0022,  0.5926]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6867, -0.0048,  0.8158,  0.6153]], device='cuda:0')\n",
      "On state=8, selected action=1 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.592589616776) A[1]:(0.656639575958) A[2]:(0.645264029503) A[3]:(0.591794490814)\n",
      " state (1)  A[0]:(0.628572821617) A[1]:(0.0125180501491) A[2]:(0.719110369682) A[3]:(0.703655481339)\n",
      " state (2)  A[0]:(0.609498023987) A[1]:(0.801178276539) A[2]:(0.742081284523) A[3]:(0.602435946465)\n",
      " state (3)  A[0]:(0.780893325806) A[1]:(-0.166052475572) A[2]:(-0.0988060086966) A[3]:(0.710144162178)\n",
      " state (4)  A[0]:(0.652179956436) A[1]:(0.728033542633) A[2]:(0.0015029895585) A[3]:(0.592039227486)\n",
      " state (5)  A[0]:(0.409585118294) A[1]:(0.913937091827) A[2]:(0.0328017771244) A[3]:(0.574820876122)\n",
      " state (6)  A[0]:(0.0588091798127) A[1]:(0.895843088627) A[2]:(-0.00796551536769) A[3]:(0.761700928211)\n",
      " state (7)  A[0]:(-0.0637898594141) A[1]:(0.594183981419) A[2]:(0.127763941884) A[3]:(0.895315825939)\n",
      " state (8)  A[0]:(0.686144053936) A[1]:(-0.00470986450091) A[2]:(0.815504968166) A[3]:(0.614488720894)\n",
      " state (9)  A[0]:(0.89118129015) A[1]:(0.900094211102) A[2]:(0.883137404919) A[3]:(0.0417152419686)\n",
      " state (10)  A[0]:(0.579790711403) A[1]:(0.999888956547) A[2]:(0.0153798852116) A[3]:(0.569656074047)\n",
      " state (11)  A[0]:(-0.12362447381) A[1]:(0.999999463558) A[2]:(-0.731099843979) A[3]:(0.815234422684)\n",
      " state (12)  A[0]:(-0.392066299915) A[1]:(1.0) A[2]:(-0.552308678627) A[3]:(0.847741127014)\n",
      " state (13)  A[0]:(0.00953425653279) A[1]:(1.0) A[2]:(0.900084018707) A[3]:(0.84420889616)\n",
      " state (14)  A[0]:(0.867235183716) A[1]:(1.0) A[2]:(0.999982535839) A[3]:(0.862740814686)\n",
      " state (15)  A[0]:(0.995180249214) A[1]:(1.0) A[2]:(1.0) A[3]:(0.891314387321)\n",
      "Episode 120000 finished after 0 timesteps with r=0.0. Running score: 0.28. Times trained:               15585. Times reached goal: 274.               Steps done: 1154098. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.283807946113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.593530297279) A[1]:(0.658489942551) A[2]:(0.649021565914) A[3]:(0.593325853348)\n",
      " state (1)  A[0]:(0.624971389771) A[1]:(0.0175974089652) A[2]:(0.718305468559) A[3]:(0.703097999096)\n",
      " state (2)  A[0]:(0.61673438549) A[1]:(0.804950714111) A[2]:(0.748448073864) A[3]:(0.608054041862)\n",
      " state (3)  A[0]:(0.784362435341) A[1]:(-0.18160238862) A[2]:(-0.117121845484) A[3]:(0.712380290031)\n",
      " state (4)  A[0]:(0.65624243021) A[1]:(0.732160627842) A[2]:(0.00304638408124) A[3]:(0.595512747765)\n",
      " state (5)  A[0]:(0.41386204958) A[1]:(0.916080653667) A[2]:(0.0406777374446) A[3]:(0.582923173904)\n",
      " state (6)  A[0]:(0.0685250163078) A[1]:(0.896887540817) A[2]:(-0.0105576170608) A[3]:(0.775034189224)\n",
      " state (7)  A[0]:(-0.0477655045688) A[1]:(0.598261356354) A[2]:(0.106145873666) A[3]:(0.904613494873)\n",
      " state (8)  A[0]:(0.684441864491) A[1]:(-0.00143033172935) A[2]:(0.812638282776) A[3]:(0.623544454575)\n",
      " state (9)  A[0]:(0.887197196484) A[1]:(0.899860322475) A[2]:(0.882757008076) A[3]:(0.0467827394605)\n",
      " state (10)  A[0]:(0.585252165794) A[1]:(0.999873101711) A[2]:(0.0145644377917) A[3]:(0.587844312191)\n",
      " state (11)  A[0]:(-0.0940244868398) A[1]:(0.99999922514) A[2]:(-0.723157167435) A[3]:(0.819838404655)\n",
      " state (12)  A[0]:(-0.37505325675) A[1]:(0.999999940395) A[2]:(-0.547004640102) A[3]:(0.845421493053)\n",
      " state (13)  A[0]:(0.0103380279616) A[1]:(1.0) A[2]:(0.897037982941) A[3]:(0.840548694134)\n",
      " state (14)  A[0]:(0.869627833366) A[1]:(1.0) A[2]:(0.999982893467) A[3]:(0.866274178028)\n",
      " state (15)  A[0]:(0.995574891567) A[1]:(1.0) A[2]:(1.0) A[3]:(0.902398705482)\n",
      "Episode 121000 finished after 0 timesteps with r=0.0. Running score: 0.28. Times trained:               15459. Times reached goal: 273.               Steps done: 1169557. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.279454297306.\n",
      " state (0)  A[0]:(0.591357827187) A[1]:(0.65575671196) A[2]:(0.64963132143) A[3]:(0.59158217907)\n",
      " state (1)  A[0]:(0.623024046421) A[1]:(0.0146377356723) A[2]:(0.72158062458) A[3]:(0.712513685226)\n",
      " state (2)  A[0]:(0.61877477169) A[1]:(0.802374362946) A[2]:(0.749442994595) A[3]:(0.620684146881)\n",
      " state (3)  A[0]:(0.785668492317) A[1]:(-0.198374465108) A[2]:(-0.113279439509) A[3]:(0.713852763176)\n",
      " state (4)  A[0]:(0.653849244118) A[1]:(0.732476234436) A[2]:(0.00800652056932) A[3]:(0.595067322254)\n",
      " state (5)  A[0]:(0.405701518059) A[1]:(0.916863381863) A[2]:(0.0428670048714) A[3]:(0.584535181522)\n",
      " state (6)  A[0]:(0.0632390454412) A[1]:(0.896451056004) A[2]:(-0.0159651786089) A[3]:(0.783746182919)\n",
      " state (7)  A[0]:(-0.0418069399893) A[1]:(0.596449613571) A[2]:(0.0934055224061) A[3]:(0.911526560783)\n",
      " state (8)  A[0]:(0.681971669197) A[1]:(-0.00361584033817) A[2]:(0.812554717064) A[3]:(0.626294970512)\n",
      " state (9)  A[0]:(0.883012473583) A[1]:(0.901255905628) A[2]:(0.880901157856) A[3]:(0.050330426544)\n",
      " state (10)  A[0]:(0.589269697666) A[1]:(0.999862611294) A[2]:(0.00598769169301) A[3]:(0.606794118881)\n",
      " state (11)  A[0]:(-0.070405639708) A[1]:(0.999998986721) A[2]:(-0.718049526215) A[3]:(0.825425744057)\n",
      " state (12)  A[0]:(-0.363249778748) A[1]:(0.999999940395) A[2]:(-0.539464473724) A[3]:(0.844214498997)\n",
      " state (13)  A[0]:(0.0106475129724) A[1]:(1.0) A[2]:(0.898735940456) A[3]:(0.838097035885)\n",
      " state (14)  A[0]:(0.873353362083) A[1]:(1.0) A[2]:(0.999984681606) A[3]:(0.870923221111)\n",
      " state (15)  A[0]:(0.995948970318) A[1]:(1.0) A[2]:(1.0) A[3]:(0.913071155548)\n",
      "Episode 122000 finished after 0 timesteps with r=0.0. Running score: 0.29. Times trained:               15358. Times reached goal: 282.               Steps done: 1184915. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.275195227321.\n",
      " state (0)  A[0]:(0.590755462646) A[1]:(0.658748924732) A[2]:(0.647061407566) A[3]:(0.591483712196)\n",
      " state (1)  A[0]:(0.620709657669) A[1]:(0.0132403979078) A[2]:(0.7216360569) A[3]:(0.71407687664)\n",
      " state (2)  A[0]:(0.61631500721) A[1]:(0.80490899086) A[2]:(0.747807085514) A[3]:(0.622144937515)\n",
      " state (3)  A[0]:(0.790122389793) A[1]:(-0.206193700433) A[2]:(-0.108419075608) A[3]:(0.713995218277)\n",
      " state (4)  A[0]:(0.655099987984) A[1]:(0.732078254223) A[2]:(0.00491662835702) A[3]:(0.591317296028)\n",
      " state (5)  A[0]:(0.402065098286) A[1]:(0.917628645897) A[2]:(0.0350252129138) A[3]:(0.57877856493)\n",
      " state (6)  A[0]:(0.066329523921) A[1]:(0.897650659084) A[2]:(-0.0264351759106) A[3]:(0.785083889961)\n",
      " state (7)  A[0]:(-0.0223309528083) A[1]:(0.602844953537) A[2]:(0.0812979415059) A[3]:(0.915170013905)\n",
      " state (8)  A[0]:(0.690564393997) A[1]:(0.000982582219876) A[2]:(0.811116337776) A[3]:(0.624464988708)\n",
      " state (9)  A[0]:(0.886685729027) A[1]:(0.900346517563) A[2]:(0.880635738373) A[3]:(0.036406185478)\n",
      " state (10)  A[0]:(0.612170577049) A[1]:(0.999851286411) A[2]:(0.00568896345794) A[3]:(0.613430440426)\n",
      " state (11)  A[0]:(-0.0319155193865) A[1]:(0.999998748302) A[2]:(-0.713399648666) A[3]:(0.826696276665)\n",
      " state (12)  A[0]:(-0.345868706703) A[1]:(0.999999940395) A[2]:(-0.533504247665) A[3]:(0.83997631073)\n",
      " state (13)  A[0]:(0.0167548172176) A[1]:(1.0) A[2]:(0.902276158333) A[3]:(0.832944512367)\n",
      " state (14)  A[0]:(0.879239976406) A[1]:(1.0) A[2]:(0.999987006187) A[3]:(0.874040007591)\n",
      " state (15)  A[0]:(0.996341943741) A[1]:(1.0) A[2]:(1.0) A[3]:(0.921957731247)\n",
      "Episode 123000 finished after 0 timesteps with r=0.0. Running score: 0.35. Times trained:               16092. Times reached goal: 269.               Steps done: 1201007. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.270802226604.\n",
      " state (0)  A[0]:(0.588860630989) A[1]:(0.656696736813) A[2]:(0.651039004326) A[3]:(0.592485010624)\n",
      " state (1)  A[0]:(0.617137789726) A[1]:(0.014137994498) A[2]:(0.722757577896) A[3]:(0.7166467309)\n",
      " state (2)  A[0]:(0.613999128342) A[1]:(0.803200125694) A[2]:(0.743567943573) A[3]:(0.627448797226)\n",
      " state (3)  A[0]:(0.79106092453) A[1]:(-0.206797540188) A[2]:(-0.0917647257447) A[3]:(0.719550848007)\n",
      " state (4)  A[0]:(0.650423049927) A[1]:(0.730986356735) A[2]:(0.014535238035) A[3]:(0.593404889107)\n",
      " state (5)  A[0]:(0.388627558947) A[1]:(0.91670525074) A[2]:(0.040686070919) A[3]:(0.575717926025)\n",
      " state (6)  A[0]:(0.056112665683) A[1]:(0.896045744419) A[2]:(-0.0218051671982) A[3]:(0.785385549068)\n",
      " state (7)  A[0]:(-0.0179215408862) A[1]:(0.599126517773) A[2]:(0.0837088748813) A[3]:(0.918520987034)\n",
      " state (8)  A[0]:(0.692793130875) A[1]:(-0.00211667688563) A[2]:(0.813025951385) A[3]:(0.630743920803)\n",
      " state (9)  A[0]:(0.8895175457) A[1]:(0.900579273701) A[2]:(0.882141709328) A[3]:(0.0448133125901)\n",
      " state (10)  A[0]:(0.632786989212) A[1]:(0.999843358994) A[2]:(0.0108856428415) A[3]:(0.634627461433)\n",
      " state (11)  A[0]:(0.00198481720872) A[1]:(0.999998509884) A[2]:(-0.707813978195) A[3]:(0.83410269022)\n",
      " state (12)  A[0]:(-0.337352216244) A[1]:(0.999999880791) A[2]:(-0.535595297813) A[3]:(0.839750349522)\n",
      " state (13)  A[0]:(0.000653341296129) A[1]:(1.0) A[2]:(0.89719837904) A[3]:(0.829586982727)\n",
      " state (14)  A[0]:(0.876908957958) A[1]:(1.0) A[2]:(0.999986827374) A[3]:(0.876749992371)\n",
      " state (15)  A[0]:(0.996491909027) A[1]:(1.0) A[2]:(1.0) A[3]:(0.92942738533)\n",
      "Episode 124000 finished after 0 timesteps with r=0.0. Running score: 0.31. Times trained:               16368. Times reached goal: 279.               Steps done: 1217375. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.266405814152.\n",
      "q_values \n",
      "tensor([[ 0.5932,  0.6593,  0.6504,  0.5960]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6496,  0.7307,  0.0086,  0.5987]], device='cuda:0')\n",
      "On state=4, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5935,  0.6592,  0.6505,  0.5957]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6500,  0.7311,  0.0088,  0.5987]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6918, -0.0050,  0.8125,  0.6390]], device='cuda:0')\n",
      "On state=8, selected action=1 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.593774557114) A[1]:(0.659395158291) A[2]:(0.650274157524) A[3]:(0.595232486725)\n",
      " state (1)  A[0]:(0.618038773537) A[1]:(0.0122153647244) A[2]:(0.722653031349) A[3]:(0.71782886982)\n",
      " state (2)  A[0]:(0.614202857018) A[1]:(0.805384039879) A[2]:(0.738798201084) A[3]:(0.630826115608)\n",
      " state (3)  A[0]:(0.793901324272) A[1]:(-0.205824092031) A[2]:(-0.0873163938522) A[3]:(0.72660279274)\n",
      " state (4)  A[0]:(0.649780750275) A[1]:(0.731566786766) A[2]:(0.007947396487) A[3]:(0.598301410675)\n",
      " state (5)  A[0]:(0.381972819567) A[1]:(0.917015373707) A[2]:(0.0289700012654) A[3]:(0.576497554779)\n",
      " state (6)  A[0]:(0.0535209774971) A[1]:(0.895954668522) A[2]:(-0.0333650186658) A[3]:(0.789372801781)\n",
      " state (7)  A[0]:(-0.00989450607449) A[1]:(0.598412871361) A[2]:(0.0736690014601) A[3]:(0.923510670662)\n",
      " state (8)  A[0]:(0.691291630268) A[1]:(-0.00355736911297) A[2]:(0.812100172043) A[3]:(0.638735413551)\n",
      " state (9)  A[0]:(0.888958632946) A[1]:(0.900957167149) A[2]:(0.882248878479) A[3]:(0.0467944853008)\n",
      " state (10)  A[0]:(0.642049431801) A[1]:(0.999840676785) A[2]:(0.0056154136546) A[3]:(0.652292966843)\n",
      " state (11)  A[0]:(0.0199859831482) A[1]:(0.999998390675) A[2]:(-0.707992434502) A[3]:(0.841577887535)\n",
      " state (12)  A[0]:(-0.333952277899) A[1]:(0.999999880791) A[2]:(-0.534209132195) A[3]:(0.841279506683)\n",
      " state (13)  A[0]:(-0.000411998451455) A[1]:(1.0) A[2]:(0.901237428188) A[3]:(0.82950925827)\n",
      " state (14)  A[0]:(0.883061110973) A[1]:(1.0) A[2]:(0.999988973141) A[3]:(0.882513284683)\n",
      " state (15)  A[0]:(0.996868252754) A[1]:(1.0) A[2]:(1.0) A[3]:(0.937406003475)\n",
      "Episode 125000 finished after 0 timesteps with r=0.0. Running score: 0.28. Times trained:               16397. Times reached goal: 275.               Steps done: 1233772. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.262073176223.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.591551423073) A[1]:(0.660705685616) A[2]:(0.652618348598) A[3]:(0.590052902699)\n",
      " state (1)  A[0]:(0.617860913277) A[1]:(0.0123719219118) A[2]:(0.72620254755) A[3]:(0.718870699406)\n",
      " state (2)  A[0]:(0.613763332367) A[1]:(0.806877851486) A[2]:(0.736883938313) A[3]:(0.631463885307)\n",
      " state (3)  A[0]:(0.796937823296) A[1]:(-0.207860589027) A[2]:(-0.0627327039838) A[3]:(0.7260825634)\n",
      " state (4)  A[0]:(0.650765955448) A[1]:(0.730849921703) A[2]:(0.0171707682312) A[3]:(0.590693891048)\n",
      " state (5)  A[0]:(0.379710406065) A[1]:(0.917671442032) A[2]:(0.0296217538416) A[3]:(0.560295581818)\n",
      " state (6)  A[0]:(0.059366710484) A[1]:(0.897688269615) A[2]:(-0.0353325083852) A[3]:(0.781068444252)\n",
      " state (7)  A[0]:(0.00988592393696) A[1]:(0.603597998619) A[2]:(0.0707716792822) A[3]:(0.923388838768)\n",
      " state (8)  A[0]:(0.693990767002) A[1]:(-0.00658889999613) A[2]:(0.81315612793) A[3]:(0.628178238869)\n",
      " state (9)  A[0]:(0.889673411846) A[1]:(0.898766756058) A[2]:(0.884597361088) A[3]:(0.0182419139892)\n",
      " state (10)  A[0]:(0.656429886818) A[1]:(0.999833881855) A[2]:(0.0122517179698) A[3]:(0.654886841774)\n",
      " state (11)  A[0]:(0.0491859763861) A[1]:(0.999998211861) A[2]:(-0.703672170639) A[3]:(0.842455148697)\n",
      " state (12)  A[0]:(-0.32101893425) A[1]:(0.999999880791) A[2]:(-0.532290279865) A[3]:(0.836276888847)\n",
      " state (13)  A[0]:(0.0019983437378) A[1]:(1.0) A[2]:(0.901245415211) A[3]:(0.821446061134)\n",
      " state (14)  A[0]:(0.886928856373) A[1]:(1.0) A[2]:(0.999989688396) A[3]:(0.881708621979)\n",
      " state (15)  A[0]:(0.997137963772) A[1]:(1.0) A[2]:(1.0) A[3]:(0.941197454929)\n",
      "Episode 126000 finished after 0 timesteps with r=0.0. Running score: 0.34. Times trained:               16972. Times reached goal: 258.               Steps done: 1250744. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.257662802573.\n",
      " state (0)  A[0]:(0.592689871788) A[1]:(0.659088134766) A[2]:(0.654955625534) A[3]:(0.59153676033)\n",
      " state (1)  A[0]:(0.619202971458) A[1]:(0.00983933638781) A[2]:(0.727382957935) A[3]:(0.717207431793)\n",
      " state (2)  A[0]:(0.61502045393) A[1]:(0.806478261948) A[2]:(0.729713201523) A[3]:(0.633420467377)\n",
      " state (3)  A[0]:(0.796771168709) A[1]:(-0.211907178164) A[2]:(-0.0475341416895) A[3]:(0.730568766594)\n",
      " state (4)  A[0]:(0.648949027061) A[1]:(0.726814627647) A[2]:(0.0174485612661) A[3]:(0.593150019646)\n",
      " state (5)  A[0]:(0.377035677433) A[1]:(0.916872322559) A[2]:(0.022894911468) A[3]:(0.555860698223)\n",
      " state (6)  A[0]:(0.0682908445597) A[1]:(0.898732185364) A[2]:(-0.0423266589642) A[3]:(0.779113173485)\n",
      " state (7)  A[0]:(0.0335262715816) A[1]:(0.610505878925) A[2]:(0.0650620386004) A[3]:(0.926263451576)\n",
      " state (8)  A[0]:(0.69011092186) A[1]:(0.000834345642943) A[2]:(0.811480522156) A[3]:(0.637129664421)\n",
      " state (9)  A[0]:(0.884246349335) A[1]:(0.899950146675) A[2]:(0.883432090282) A[3]:(0.0290333665907)\n",
      " state (10)  A[0]:(0.658564448357) A[1]:(0.999829113483) A[2]:(0.00822930876166) A[3]:(0.674903035164)\n",
      " state (11)  A[0]:(0.0678853988647) A[1]:(0.999997973442) A[2]:(-0.701793074608) A[3]:(0.850218296051)\n",
      " state (12)  A[0]:(-0.312224626541) A[1]:(0.999999821186) A[2]:(-0.536787629128) A[3]:(0.8380869627)\n",
      " state (13)  A[0]:(-0.00414796033874) A[1]:(1.0) A[2]:(0.895211160183) A[3]:(0.820376992226)\n",
      " state (14)  A[0]:(0.886418163776) A[1]:(1.0) A[2]:(0.99998909235) A[3]:(0.885354161263)\n",
      " state (15)  A[0]:(0.997277975082) A[1]:(1.0) A[2]:(1.0) A[3]:(0.947156429291)\n",
      "Episode 127000 finished after 0 timesteps with r=0.0. Running score: 0.26. Times trained:               17249. Times reached goal: 256.               Steps done: 1267993. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.253256488398.\n",
      " state (0)  A[0]:(0.590443193913) A[1]:(0.656805276871) A[2]:(0.651083827019) A[3]:(0.590533256531)\n",
      " state (1)  A[0]:(0.614397823811) A[1]:(0.00351958023384) A[2]:(0.724439442158) A[3]:(0.713193297386)\n",
      " state (2)  A[0]:(0.612667798996) A[1]:(0.806402564049) A[2]:(0.713601231575) A[3]:(0.634157299995)\n",
      " state (3)  A[0]:(0.795636177063) A[1]:(-0.192225307226) A[2]:(-0.0473469272256) A[3]:(0.732024371624)\n",
      " state (4)  A[0]:(0.643002510071) A[1]:(0.730076909065) A[2]:(0.00623027374968) A[3]:(0.589976072311)\n",
      " state (5)  A[0]:(0.36400398612) A[1]:(0.916713416576) A[2]:(0.00843528751284) A[3]:(0.541373610497)\n",
      " state (6)  A[0]:(0.0618510916829) A[1]:(0.899748623371) A[2]:(-0.0530236288905) A[3]:(0.767629504204)\n",
      " state (7)  A[0]:(0.0452945567667) A[1]:(0.614575386047) A[2]:(0.0598530955613) A[3]:(0.925194859505)\n",
      " state (8)  A[0]:(0.686941385269) A[1]:(0.00115668727085) A[2]:(0.810424089432) A[3]:(0.6313393116)\n",
      " state (9)  A[0]:(0.880831897259) A[1]:(0.900619566441) A[2]:(0.882906675339) A[3]:(0.0144749898463)\n",
      " state (10)  A[0]:(0.663864791393) A[1]:(0.999828934669) A[2]:(0.00588791724294) A[3]:(0.682662844658)\n",
      " state (11)  A[0]:(0.0871516168118) A[1]:(0.999997854233) A[2]:(-0.698486924171) A[3]:(0.853780627251)\n",
      " state (12)  A[0]:(-0.300851076841) A[1]:(0.999999821186) A[2]:(-0.527182936668) A[3]:(0.837263822556)\n",
      " state (13)  A[0]:(0.00454362342134) A[1]:(1.0) A[2]:(0.901111781597) A[3]:(0.817840099335)\n",
      " state (14)  A[0]:(0.891588807106) A[1]:(1.0) A[2]:(0.999990701675) A[3]:(0.888051092625)\n",
      " state (15)  A[0]:(0.997488439083) A[1]:(1.0) A[2]:(1.0) A[3]:(0.951293110847)\n",
      "Episode 128000 finished after 0 timesteps with r=0.0. Running score: 0.28. Times trained:               16810. Times reached goal: 292.               Steps done: 1284803. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.249034829284.\n",
      " state (0)  A[0]:(0.591738402843) A[1]:(0.657685399055) A[2]:(0.654949605465) A[3]:(0.594309687614)\n",
      " state (1)  A[0]:(0.613811790943) A[1]:(0.00530792260543) A[2]:(0.729212164879) A[3]:(0.71149545908)\n",
      " state (2)  A[0]:(0.618163585663) A[1]:(0.809566378593) A[2]:(0.705940246582) A[3]:(0.642452955246)\n",
      " state (3)  A[0]:(0.796773076057) A[1]:(-0.175109282136) A[2]:(-0.0373668670654) A[3]:(0.740703463554)\n",
      " state (4)  A[0]:(0.640301406384) A[1]:(0.731834352016) A[2]:(0.0126276453957) A[3]:(0.597192645073)\n",
      " state (5)  A[0]:(0.35431048274) A[1]:(0.915564119816) A[2]:(0.0160968694836) A[3]:(0.536061644554)\n",
      " state (6)  A[0]:(0.0585098341107) A[1]:(0.899522483349) A[2]:(-0.0413861982524) A[3]:(0.759633481503)\n",
      " state (7)  A[0]:(0.0633048638701) A[1]:(0.614138841629) A[2]:(0.0715777203441) A[3]:(0.926057994366)\n",
      " state (8)  A[0]:(0.689140200615) A[1]:(-0.00459459191188) A[2]:(0.811033964157) A[3]:(0.640472173691)\n",
      " state (9)  A[0]:(0.880622684956) A[1]:(0.900162100792) A[2]:(0.881877303123) A[3]:(0.0302133969963)\n",
      " state (10)  A[0]:(0.677572369576) A[1]:(0.999825000763) A[2]:(0.000357627839549) A[3]:(0.706527709961)\n",
      " state (11)  A[0]:(0.117987014353) A[1]:(0.999997675419) A[2]:(-0.697142362595) A[3]:(0.86461275816)\n",
      " state (12)  A[0]:(-0.285044521093) A[1]:(0.999999821186) A[2]:(-0.524521946907) A[3]:(0.843330144882)\n",
      " state (13)  A[0]:(0.00734006473795) A[1]:(1.0) A[2]:(0.902173638344) A[3]:(0.820983767509)\n",
      " state (14)  A[0]:(0.893189072609) A[1]:(1.0) A[2]:(0.999991357327) A[3]:(0.892929792404)\n",
      " state (15)  A[0]:(0.997596740723) A[1]:(1.0) A[2]:(1.0) A[3]:(0.955798268318)\n",
      "Episode 129000 finished after 0 timesteps with r=1.0. Running score: 0.21. Times trained:               17859. Times reached goal: 327.               Steps done: 1302662. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.244626794969.\n",
      "q_values \n",
      "tensor([[ 0.5876,  0.6567,  0.6556,  0.5933]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6342,  0.7311,  0.0130,  0.5976]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6877, -0.0045,  0.8121,  0.6430]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8817,  0.9002,  0.8844,  0.0261]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0021,  1.0000,  0.8999,  0.8172]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0026,  1.0000,  0.9000,  0.8168]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0036,  1.0000,  0.9002,  0.8162]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0043,  1.0000,  0.9004,  0.8158]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0043,  1.0000,  0.9008,  0.8155]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0013,  1.0000,  0.9016,  0.8159]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0026,  1.0000,  0.9024,  0.8167]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0054,  1.0000,  0.9031,  0.8172]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0089,  1.0000,  0.9035,  0.8180]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0097,  1.0000,  0.9034,  0.8182]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0089,  1.0000,  0.9029,  0.8182]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8820,  0.9012,  0.8838,  0.0274]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0054,  1.0000,  0.9012,  0.8180]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0025,  1.0000,  0.9001,  0.8178]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8816,  0.9014,  0.8824,  0.0307]], device='cuda:0')\n",
      "On state=9, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6874,  0.0017,  0.8097,  0.6448]], device='cuda:0')\n",
      "On state=8, selected action=3 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6360,  0.7293,  0.0158,  0.5960]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6884, -0.0007,  0.8094,  0.6455]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8825,  0.9005,  0.8821,  0.0330]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-6.1743e-05,  1.0000e+00,  8.9688e-01,  8.1824e-01]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  1.0000,  0.8968,  0.8179]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  1.0000,  0.8969,  0.8176]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0014,  1.0000,  0.8972,  0.8175]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.59034126997) A[1]:(0.658599317074) A[2]:(0.655831456184) A[3]:(0.590728282928)\n",
      " state (1)  A[0]:(0.610582232475) A[1]:(0.00616077752784) A[2]:(0.72826397419) A[3]:(0.707195878029)\n",
      " state (2)  A[0]:(0.622999429703) A[1]:(0.809921562672) A[2]:(0.694071173668) A[3]:(0.648658514023)\n",
      " state (3)  A[0]:(0.798222601414) A[1]:(-0.159763470292) A[2]:(-0.0305980276316) A[3]:(0.740858376026)\n",
      " state (4)  A[0]:(0.63823223114) A[1]:(0.731937944889) A[2]:(0.0131824724376) A[3]:(0.594036579132)\n",
      " state (5)  A[0]:(0.343169420958) A[1]:(0.914328813553) A[2]:(0.0152281671762) A[3]:(0.518707871437)\n",
      " state (6)  A[0]:(0.0486579574645) A[1]:(0.900183022022) A[2]:(-0.039038002491) A[3]:(0.741672515869)\n",
      " state (7)  A[0]:(0.0733275488019) A[1]:(0.617748737335) A[2]:(0.0785057619214) A[3]:(0.922847628593)\n",
      " state (8)  A[0]:(0.69311517477) A[1]:(-0.0117483697832) A[2]:(0.814130604267) A[3]:(0.632595419884)\n",
      " state (9)  A[0]:(0.883851647377) A[1]:(0.897997200489) A[2]:(0.884749412537) A[3]:(0.016306117177)\n",
      " state (10)  A[0]:(0.69880014658) A[1]:(0.999821484089) A[2]:(0.00676371296868) A[3]:(0.716975271702)\n",
      " state (11)  A[0]:(0.158757373691) A[1]:(0.99999755621) A[2]:(-0.695651173592) A[3]:(0.870301067829)\n",
      " state (12)  A[0]:(-0.265322417021) A[1]:(0.999999761581) A[2]:(-0.530047118664) A[3]:(0.844598054886)\n",
      " state (13)  A[0]:(0.0048761148937) A[1]:(1.0) A[2]:(0.897767663002) A[3]:(0.817924797535)\n",
      " state (14)  A[0]:(0.891523122787) A[1]:(1.0) A[2]:(0.999991118908) A[3]:(0.892645597458)\n",
      " state (15)  A[0]:(0.997618496418) A[1]:(1.0) A[2]:(1.0) A[3]:(0.957578599453)\n",
      "Episode 130000 finished after 0 timesteps with r=0.0. Running score: 0.31. Times trained:               17421. Times reached goal: 335.               Steps done: 1320083. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.240402057992.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.596628904343) A[1]:(0.657755970955) A[2]:(0.664276361465) A[3]:(0.599373102188)\n",
      " state (1)  A[0]:(0.616171002388) A[1]:(-0.00171422795393) A[2]:(0.732987165451) A[3]:(0.712765216827)\n",
      " state (2)  A[0]:(0.632993519306) A[1]:(0.80974149704) A[2]:(0.695929884911) A[3]:(0.666606724262)\n",
      " state (3)  A[0]:(0.798812747002) A[1]:(-0.143159464002) A[2]:(0.00597684411332) A[3]:(0.746346950531)\n",
      " state (4)  A[0]:(0.636540770531) A[1]:(0.734913468361) A[2]:(0.0358049347997) A[3]:(0.603624165058)\n",
      " state (5)  A[0]:(0.333703577518) A[1]:(0.914597451687) A[2]:(0.0289846509695) A[3]:(0.522391676903)\n",
      " state (6)  A[0]:(0.0414252616465) A[1]:(0.900720119476) A[2]:(-0.0288773328066) A[3]:(0.741787910461)\n",
      " state (7)  A[0]:(0.0825106650591) A[1]:(0.619487047195) A[2]:(0.0853062346578) A[3]:(0.926257550716)\n",
      " state (8)  A[0]:(0.692042589188) A[1]:(-0.000874519115314) A[2]:(0.813223958015) A[3]:(0.648171186447)\n",
      " state (9)  A[0]:(0.884985923767) A[1]:(0.900756120682) A[2]:(0.885429739952) A[3]:(0.025476962328)\n",
      " state (10)  A[0]:(0.714965164661) A[1]:(0.999825596809) A[2]:(0.00632444536313) A[3]:(0.73316514492)\n",
      " state (11)  A[0]:(0.190786719322) A[1]:(0.99999755621) A[2]:(-0.696205496788) A[3]:(0.877657771111)\n",
      " state (12)  A[0]:(-0.25130340457) A[1]:(0.999999761581) A[2]:(-0.53164857626) A[3]:(0.847763895988)\n",
      " state (13)  A[0]:(0.00596951507032) A[1]:(1.0) A[2]:(0.899303793907) A[3]:(0.81768989563)\n",
      " state (14)  A[0]:(0.893379330635) A[1]:(1.0) A[2]:(0.999991953373) A[3]:(0.89460504055)\n",
      " state (15)  A[0]:(0.997707784176) A[1]:(1.0) A[2]:(1.0) A[3]:(0.959848880768)\n",
      "Episode 131000 finished after 0 timesteps with r=1.0. Running score: 0.39. Times trained:               17887. Times reached goal: 360.               Steps done: 1337970. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.236140215796.\n",
      " state (0)  A[0]:(0.591771662235) A[1]:(0.659838199615) A[2]:(0.651774227619) A[3]:(0.592460751534)\n",
      " state (1)  A[0]:(0.608492791653) A[1]:(-0.000898480182514) A[2]:(0.72816836834) A[3]:(0.695790231228)\n",
      " state (2)  A[0]:(0.630943000317) A[1]:(0.811313152313) A[2]:(0.692276477814) A[3]:(0.663656353951)\n",
      " state (3)  A[0]:(0.793471097946) A[1]:(-0.130054816604) A[2]:(0.0270119737834) A[3]:(0.736310243607)\n",
      " state (4)  A[0]:(0.63006234169) A[1]:(0.729276180267) A[2]:(0.0409474186599) A[3]:(0.597533881664)\n",
      " state (5)  A[0]:(0.320789605379) A[1]:(0.91167640686) A[2]:(0.0257034897804) A[3]:(0.51023542881)\n",
      " state (6)  A[0]:(0.027398718521) A[1]:(0.899335622787) A[2]:(-0.0332382023335) A[3]:(0.726852297783)\n",
      " state (7)  A[0]:(0.0771334543824) A[1]:(0.620245456696) A[2]:(0.0829074308276) A[3]:(0.922686398029)\n",
      " state (8)  A[0]:(0.680127739906) A[1]:(-0.00120186747517) A[2]:(0.809640645981) A[3]:(0.640932798386)\n",
      " state (9)  A[0]:(0.881335437298) A[1]:(0.89954406023) A[2]:(0.883662045002) A[3]:(0.0138936042786)\n",
      " state (10)  A[0]:(0.723584115505) A[1]:(0.999818980694) A[2]:(-0.00283407396637) A[3]:(0.742354035378)\n",
      " state (11)  A[0]:(0.21514865756) A[1]:(0.999997258186) A[2]:(-0.699407637119) A[3]:(0.881811738014)\n",
      " state (12)  A[0]:(-0.241055130959) A[1]:(0.999999761581) A[2]:(-0.538856208324) A[3]:(0.847504496574)\n",
      " state (13)  A[0]:(0.000324474647641) A[1]:(1.0) A[2]:(0.896178483963) A[3]:(0.812981188297)\n",
      " state (14)  A[0]:(0.892538785934) A[1]:(1.0) A[2]:(0.999992012978) A[3]:(0.893184602261)\n",
      " state (15)  A[0]:(0.997755289078) A[1]:(1.0) A[2]:(1.0) A[3]:(0.96075284481)\n",
      "Episode 132000 finished after 0 timesteps with r=0.0. Running score: 0.3. Times trained:               17856. Times reached goal: 342.               Steps done: 1355826. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.231961118123.\n",
      " state (0)  A[0]:(0.59505712986) A[1]:(0.661877274513) A[2]:(0.659902989864) A[3]:(0.594080626965)\n",
      " state (1)  A[0]:(0.607884287834) A[1]:(-0.00138366129249) A[2]:(0.732509493828) A[3]:(0.691695272923)\n",
      " state (2)  A[0]:(0.638522982597) A[1]:(0.809900641441) A[2]:(0.692819237709) A[3]:(0.677522301674)\n",
      " state (3)  A[0]:(0.789259076118) A[1]:(-0.1111696437) A[2]:(0.0291104298085) A[3]:(0.729843020439)\n",
      " state (4)  A[0]:(0.627921462059) A[1]:(0.728963971138) A[2]:(0.0367121137679) A[3]:(0.600221991539)\n",
      " state (5)  A[0]:(0.318220674992) A[1]:(0.910685002804) A[2]:(0.024758880958) A[3]:(0.513331592083)\n",
      " state (6)  A[0]:(0.0287140961736) A[1]:(0.898804366589) A[2]:(-0.0240013990551) A[3]:(0.725896418095)\n",
      " state (7)  A[0]:(0.0863247737288) A[1]:(0.619724214077) A[2]:(0.100630424917) A[3]:(0.924105703831)\n",
      " state (8)  A[0]:(0.671047210693) A[1]:(0.000803410832305) A[2]:(0.810981154442) A[3]:(0.649131894112)\n",
      " state (9)  A[0]:(0.878041148186) A[1]:(0.899509012699) A[2]:(0.884624898434) A[3]:(0.0156583208591)\n",
      " state (10)  A[0]:(0.733563899994) A[1]:(0.999819517136) A[2]:(0.00333141046576) A[3]:(0.757165431976)\n",
      " state (11)  A[0]:(0.243131399155) A[1]:(0.999997198582) A[2]:(-0.692129313946) A[3]:(0.8893481493)\n",
      " state (12)  A[0]:(-0.225458011031) A[1]:(0.999999701977) A[2]:(-0.525513410568) A[3]:(0.85218256712)\n",
      " state (13)  A[0]:(0.00468838075176) A[1]:(1.0) A[2]:(0.901084601879) A[3]:(0.814683139324)\n",
      " state (14)  A[0]:(0.89514541626) A[1]:(1.0) A[2]:(0.999992907047) A[3]:(0.895933926105)\n",
      " state (15)  A[0]:(0.997884094715) A[1]:(1.0) A[2]:(1.0) A[3]:(0.963147044182)\n",
      "Episode 133000 finished after 0 timesteps with r=1.0. Running score: 0.34. Times trained:               18371. Times reached goal: 356.               Steps done: 1374197. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.227738664523.\n",
      " state (0)  A[0]:(0.590486228466) A[1]:(0.656157970428) A[2]:(0.655352413654) A[3]:(0.59038579464)\n",
      " state (1)  A[0]:(0.600943386555) A[1]:(-0.00301306531765) A[2]:(0.729917168617) A[3]:(0.681747436523)\n",
      " state (2)  A[0]:(0.639598846436) A[1]:(0.809486508369) A[2]:(0.691306114197) A[3]:(0.687891900539)\n",
      " state (3)  A[0]:(0.780568301678) A[1]:(-0.0833218991756) A[2]:(0.0405565835536) A[3]:(0.715855836868)\n",
      " state (4)  A[0]:(0.620633780956) A[1]:(0.730104446411) A[2]:(0.0368733033538) A[3]:(0.596650958061)\n",
      " state (5)  A[0]:(0.310490489006) A[1]:(0.910366356373) A[2]:(0.023281538859) A[3]:(0.511145710945)\n",
      " state (6)  A[0]:(0.0285324249417) A[1]:(0.89931499958) A[2]:(-0.0207633934915) A[3]:(0.721257090569)\n",
      " state (7)  A[0]:(0.102183707058) A[1]:(0.621185779572) A[2]:(0.110644154251) A[3]:(0.923855602741)\n",
      " state (8)  A[0]:(0.668743610382) A[1]:(-0.000364363164408) A[2]:(0.812346160412) A[3]:(0.646759688854)\n",
      " state (9)  A[0]:(0.875443696976) A[1]:(0.899390101433) A[2]:(0.884976089001) A[3]:(0.00805113185197)\n",
      " state (10)  A[0]:(0.742312073708) A[1]:(0.999818742275) A[2]:(0.005642473232) A[3]:(0.767973601818)\n",
      " state (11)  A[0]:(0.267995595932) A[1]:(0.999997079372) A[2]:(-0.687940061092) A[3]:(0.894545316696)\n",
      " state (12)  A[0]:(-0.213448435068) A[1]:(0.999999701977) A[2]:(-0.52332252264) A[3]:(0.853930532932)\n",
      " state (13)  A[0]:(-0.000960282690357) A[1]:(1.0) A[2]:(0.898338675499) A[3]:(0.812003314495)\n",
      " state (14)  A[0]:(0.893968701363) A[1]:(1.0) A[2]:(0.999992668629) A[3]:(0.89549267292)\n",
      " state (15)  A[0]:(0.99795550108) A[1]:(1.0) A[2]:(1.0) A[3]:(0.964540421963)\n",
      "Episode 134000 finished after 0 timesteps with r=1.0. Running score: 0.39. Times trained:               19162. Times reached goal: 350.               Steps done: 1393359. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.223416281246.\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.6551,  0.6569,  0.5905]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5990, -0.0025,  0.7289,  0.6753]], device='cuda:0')\n",
      "On state=1, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6553,  0.6568,  0.5905]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5985, -0.0021,  0.7287,  0.6753]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6405,  0.8101,  0.6887,  0.7007]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0279,  0.8995, -0.0309,  0.7215]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7459,  0.9998, -0.0001,  0.7783]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8973,  1.0000,  1.0000,  0.8975]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7477,  0.9998,  0.0002,  0.7798]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8975,  1.0000,  1.0000,  0.8976]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8968,  1.0000,  1.0000,  0.8972]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8962,  1.0000,  1.0000,  0.8969]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8955,  1.0000,  1.0000,  0.8965]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8954,  1.0000,  1.0000,  0.8965]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8956,  1.0000,  1.0000,  0.8967]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8959,  1.0000,  1.0000,  0.8969]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8963,  1.0000,  1.0000,  0.8973]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8966,  1.0000,  1.0000,  0.8976]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8964,  1.0000,  1.0000,  0.8976]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8961,  1.0000,  1.0000,  0.8974]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8958,  1.0000,  1.0000,  0.8972]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8955,  1.0000,  1.0000,  0.8971]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8954,  1.0000,  1.0000,  0.8971]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8953,  1.0000,  1.0000,  0.8970]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8953,  1.0000,  1.0000,  0.8970]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8957,  1.0000,  1.0000,  0.8973]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7463,  0.9998, -0.0019,  0.7800]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8968,  1.0000,  1.0000,  0.8980]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8975,  1.0000,  1.0000,  0.8985]], device='cuda:0')\n",
      "On state=14, selected action=0 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0068,  1.0000,  0.9003,  0.8149]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0056,  1.0000,  0.9000,  0.8148]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_values \n",
      "tensor([[ 0.0036,  1.0000,  0.8996,  0.8144]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0026,  1.0000,  0.8992,  0.8143]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0016,  1.0000,  0.8988,  0.8141]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  1.0000,  0.8984,  0.8138]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0006,  1.0000,  0.8981,  0.8136]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0008,  1.0000,  0.8984,  0.8138]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0033,  1.0000,  0.8987,  0.8143]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0059,  1.0000,  0.8990,  0.8148]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0065,  1.0000,  0.8989,  0.8148]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0077,  1.0000,  0.8990,  0.8149]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0063,  1.0000,  0.8986,  0.8145]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0029,  1.0000,  0.8979,  0.8135]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.59239590168) A[1]:(0.65461319685) A[2]:(0.659998953342) A[3]:(0.587442278862)\n",
      " state (1)  A[0]:(0.601321697235) A[1]:(-0.00317280413583) A[2]:(0.732347786427) A[3]:(0.672355532646)\n",
      " state (2)  A[0]:(0.643605351448) A[1]:(0.809575676918) A[2]:(0.693468868732) A[3]:(0.698172926903)\n",
      " state (3)  A[0]:(0.777750909328) A[1]:(-0.0565196499228) A[2]:(0.0657471716404) A[3]:(0.702466845512)\n",
      " state (4)  A[0]:(0.62091255188) A[1]:(0.729025959969) A[2]:(0.0419482439756) A[3]:(0.594323039055)\n",
      " state (5)  A[0]:(0.311349898577) A[1]:(0.909890770912) A[2]:(0.0220731403679) A[3]:(0.511343121529)\n",
      " state (6)  A[0]:(0.0364926308393) A[1]:(0.900139927864) A[2]:(-0.0185226481408) A[3]:(0.720477938652)\n",
      " state (7)  A[0]:(0.126640349627) A[1]:(0.624190330505) A[2]:(0.123121410608) A[3]:(0.925452172756)\n",
      " state (8)  A[0]:(0.671637892723) A[1]:(0.00373474997468) A[2]:(0.816742181778) A[3]:(0.649003982544)\n",
      " state (9)  A[0]:(0.873671650887) A[1]:(0.901442468166) A[2]:(0.886822342873) A[3]:(0.00755102001131)\n",
      " state (10)  A[0]:(0.750636279583) A[1]:(0.999824464321) A[2]:(0.0128983259201) A[3]:(0.780187368393)\n",
      " state (11)  A[0]:(0.292430311441) A[1]:(0.999997079372) A[2]:(-0.681520998478) A[3]:(0.900687932968)\n",
      " state (12)  A[0]:(-0.198422089219) A[1]:(0.999999701977) A[2]:(-0.516637921333) A[3]:(0.858017981052)\n",
      " state (13)  A[0]:(-0.000181958079338) A[1]:(1.0) A[2]:(0.897278189659) A[3]:(0.812726080418)\n",
      " state (14)  A[0]:(0.894087255001) A[1]:(1.0) A[2]:(0.999992609024) A[3]:(0.896594882011)\n",
      " state (15)  A[0]:(0.998042166233) A[1]:(1.0) A[2]:(1.0) A[3]:(0.966160535812)\n",
      "Episode 135000 finished after 0 timesteps with r=0.0. Running score: 0.31. Times trained:               18463. Times reached goal: 339.               Steps done: 1411822. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.219329192505.\n",
      " state (0)  A[0]:(0.591117978096) A[1]:(0.658284723759) A[2]:(0.657489061356) A[3]:(0.591174125671)\n",
      " state (1)  A[0]:(0.600092291832) A[1]:(-0.00349865923636) A[2]:(0.72950142622) A[3]:(0.667602837086)\n",
      " state (2)  A[0]:(0.642750382423) A[1]:(0.810471773148) A[2]:(0.689889252186) A[3]:(0.71192574501)\n",
      " state (3)  A[0]:(0.77144163847) A[1]:(-0.0193618778139) A[2]:(0.0806377083063) A[3]:(0.695529162884)\n",
      " state (4)  A[0]:(0.614023625851) A[1]:(0.729246795177) A[2]:(0.0335976928473) A[3]:(0.596626102924)\n",
      " state (5)  A[0]:(0.299471884966) A[1]:(0.909445881844) A[2]:(0.00602154619992) A[3]:(0.511481046677)\n",
      " state (6)  A[0]:(0.0310763604939) A[1]:(0.900398313999) A[2]:(-0.032987780869) A[3]:(0.715933561325)\n",
      " state (7)  A[0]:(0.140687823296) A[1]:(0.622733592987) A[2]:(0.114345014095) A[3]:(0.925606548786)\n",
      " state (8)  A[0]:(0.662112534046) A[1]:(0.00270288530737) A[2]:(0.809489011765) A[3]:(0.650955438614)\n",
      " state (9)  A[0]:(0.863116323948) A[1]:(0.899774134159) A[2]:(0.881794273853) A[3]:(-0.00103582406882)\n",
      " state (10)  A[0]:(0.743857741356) A[1]:(0.999816536903) A[2]:(-0.00163686124142) A[3]:(0.784783005714)\n",
      " state (11)  A[0]:(0.291216403246) A[1]:(0.999996840954) A[2]:(-0.683682322502) A[3]:(0.903074204922)\n",
      " state (12)  A[0]:(-0.200131222606) A[1]:(0.999999642372) A[2]:(-0.514238119125) A[3]:(0.857865810394)\n",
      " state (13)  A[0]:(-0.00182674534153) A[1]:(0.999999940395) A[2]:(0.899134337902) A[3]:(0.809737622738)\n",
      " state (14)  A[0]:(0.89653724432) A[1]:(1.0) A[2]:(0.999993026257) A[3]:(0.896591365337)\n",
      " state (15)  A[0]:(0.998172402382) A[1]:(1.0) A[2]:(1.0) A[3]:(0.967368960381)\n",
      "Episode 136000 finished after 0 timesteps with r=0.0. Running score: 0.39. Times trained:               19441. Times reached goal: 369.               Steps done: 1431263. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.215106394384.\n",
      " state (0)  A[0]:(0.592927455902) A[1]:(0.657444000244) A[2]:(0.659963786602) A[3]:(0.593097031116)\n",
      " state (1)  A[0]:(0.60212624073) A[1]:(-0.00308333849534) A[2]:(0.732811450958) A[3]:(0.662069916725)\n",
      " state (2)  A[0]:(0.64429551363) A[1]:(0.810351669788) A[2]:(0.693963170052) A[3]:(0.721394002438)\n",
      " state (3)  A[0]:(0.770414829254) A[1]:(0.00928571913391) A[2]:(0.103893384337) A[3]:(0.691940426826)\n",
      " state (4)  A[0]:(0.613362908363) A[1]:(0.730466246605) A[2]:(0.0404386408627) A[3]:(0.600006580353)\n",
      " state (5)  A[0]:(0.295640140772) A[1]:(0.910010695457) A[2]:(0.0112375058234) A[3]:(0.514159679413)\n",
      " state (6)  A[0]:(0.0393590666354) A[1]:(0.900362789631) A[2]:(-0.0208048596978) A[3]:(0.719608187675)\n",
      " state (7)  A[0]:(0.176079630852) A[1]:(0.615047812462) A[2]:(0.137042075396) A[3]:(0.929561674595)\n",
      " state (8)  A[0]:(0.664673447609) A[1]:(0.00204455573112) A[2]:(0.813829481602) A[3]:(0.660763084888)\n",
      " state (9)  A[0]:(0.855513215065) A[1]:(0.90037560463) A[2]:(0.882381260395) A[3]:(0.00307539617643)\n",
      " state (10)  A[0]:(0.74177646637) A[1]:(0.999811649323) A[2]:(0.00898849591613) A[3]:(0.793152272701)\n",
      " state (11)  A[0]:(0.299726396799) A[1]:(0.99999666214) A[2]:(-0.672169089317) A[3]:(0.907538175583)\n",
      " state (12)  A[0]:(-0.192396983504) A[1]:(0.999999642372) A[2]:(-0.495891749859) A[3]:(0.861612856388)\n",
      " state (13)  A[0]:(1.15241855383e-05) A[1]:(0.999999940395) A[2]:(0.901946544647) A[3]:(0.811847448349)\n",
      " state (14)  A[0]:(0.897329807281) A[1]:(1.0) A[2]:(0.999993145466) A[3]:(0.898707270622)\n",
      " state (15)  A[0]:(0.998265087605) A[1]:(1.0) A[2]:(1.0) A[3]:(0.969221234322)\n",
      "Episode 137000 finished after 0 timesteps with r=0.0. Running score: 0.44. Times trained:               19211. Times reached goal: 351.               Steps done: 1450474. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.211013426325.\n",
      " state (0)  A[0]:(0.592168331146) A[1]:(0.661713659763) A[2]:(0.652486681938) A[3]:(0.590866267681)\n",
      " state (1)  A[0]:(0.603132247925) A[1]:(0.000493466795888) A[2]:(0.728145539761) A[3]:(0.657388091087)\n",
      " state (2)  A[0]:(0.64227193594) A[1]:(0.813048839569) A[2]:(0.693359851837) A[3]:(0.726007342339)\n",
      " state (3)  A[0]:(0.770844459534) A[1]:(0.0367218181491) A[2]:(0.124507687986) A[3]:(0.684489369392)\n",
      " state (4)  A[0]:(0.613781809807) A[1]:(0.729646205902) A[2]:(0.0356313474476) A[3]:(0.598911046982)\n",
      " state (5)  A[0]:(0.28727594018) A[1]:(0.910376966) A[2]:(-0.00368498079479) A[3]:(0.512303054333)\n",
      " state (6)  A[0]:(0.0344382040203) A[1]:(0.900908589363) A[2]:(-0.0364478230476) A[3]:(0.7169713974)\n",
      " state (7)  A[0]:(0.198525011539) A[1]:(0.610325276852) A[2]:(0.129080697894) A[3]:(0.930514156818)\n",
      " state (8)  A[0]:(0.671918749809) A[1]:(-0.000258088111877) A[2]:(0.809944987297) A[3]:(0.661344528198)\n",
      " state (9)  A[0]:(0.854341864586) A[1]:(0.90016412735) A[2]:(0.877639353275) A[3]:(0.00465409085155)\n",
      " state (10)  A[0]:(0.749008893967) A[1]:(0.999798834324) A[2]:(-0.00500567536801) A[3]:(0.799442410469)\n",
      " state (11)  A[0]:(0.318960368633) A[1]:(0.999996185303) A[2]:(-0.674978137016) A[3]:(0.910608887672)\n",
      " state (12)  A[0]:(-0.181273534894) A[1]:(0.999999582767) A[2]:(-0.498413324356) A[3]:(0.863965332508)\n",
      " state (13)  A[0]:(-0.000402584875701) A[1]:(0.999999940395) A[2]:(0.899969160557) A[3]:(0.812707901001)\n",
      " state (14)  A[0]:(0.897120118141) A[1]:(1.0) A[2]:(0.999993026257) A[3]:(0.899721264839)\n",
      " state (15)  A[0]:(0.998320758343) A[1]:(1.0) A[2]:(1.0) A[3]:(0.970336854458)\n",
      "Episode 138000 finished after 0 timesteps with r=0.0. Running score: 0.24. Times trained:               19627. Times reached goal: 375.               Steps done: 1470101. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.206912244409.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.592607140541) A[1]:(0.659667372704) A[2]:(0.658275902271) A[3]:(0.592108488083)\n",
      " state (1)  A[0]:(0.603899776936) A[1]:(-0.00427225604653) A[2]:(0.731311321259) A[3]:(0.657797992229)\n",
      " state (2)  A[0]:(0.64143884182) A[1]:(0.811017096043) A[2]:(0.697791814804) A[3]:(0.731782495975)\n",
      " state (3)  A[0]:(0.771126747131) A[1]:(0.0617977455258) A[2]:(0.163367658854) A[3]:(0.678575277328)\n",
      " state (4)  A[0]:(0.614960432053) A[1]:(0.731757104397) A[2]:(0.0496948175132) A[3]:(0.598631620407)\n",
      " state (5)  A[0]:(0.281351357698) A[1]:(0.911965668201) A[2]:(0.000720023992471) A[3]:(0.512097120285)\n",
      " state (6)  A[0]:(0.0340673550963) A[1]:(0.901412904263) A[2]:(-0.0299753732979) A[3]:(0.716241240501)\n",
      " state (7)  A[0]:(0.221611022949) A[1]:(0.602096736431) A[2]:(0.143198117614) A[3]:(0.931622564793)\n",
      " state (8)  A[0]:(0.671100139618) A[1]:(0.000767648045439) A[2]:(0.80952501297) A[3]:(0.667604863644)\n",
      " state (9)  A[0]:(0.848342895508) A[1]:(0.899311304092) A[2]:(0.876414418221) A[3]:(0.00290172477253)\n",
      " state (10)  A[0]:(0.751534700394) A[1]:(0.999786973) A[2]:(-0.00146937265527) A[3]:(0.80024689436)\n",
      " state (11)  A[0]:(0.333632409573) A[1]:(0.999995827675) A[2]:(-0.670142292976) A[3]:(0.911666989326)\n",
      " state (12)  A[0]:(-0.170475944877) A[1]:(0.999999523163) A[2]:(-0.490363508463) A[3]:(0.864607334137)\n",
      " state (13)  A[0]:(0.00359474704601) A[1]:(0.999999940395) A[2]:(0.901292622089) A[3]:(0.812353849411)\n",
      " state (14)  A[0]:(0.898702681065) A[1]:(1.0) A[2]:(0.999993145466) A[3]:(0.900041222572)\n",
      " state (15)  A[0]:(0.998412191868) A[1]:(1.0) A[2]:(1.0) A[3]:(0.971055924892)\n",
      "Episode 139000 finished after 0 timesteps with r=0.0. Running score: 0.3. Times trained:               20144. Times reached goal: 351.               Steps done: 1490245. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.202785904188.\n",
      "q_values \n",
      "tensor([[ 0.5918,  0.6576,  0.6587,  0.5884]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6035, -0.0026,  0.7345,  0.6520]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6429,  0.8110,  0.7021,  0.7315]], device='cuda:0')\n",
      "On state=2, selected action=3 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6427,  0.8115,  0.7023,  0.7313]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0359,  0.9004, -0.0116,  0.7126]], device='cuda:0')\n",
      "On state=6, selected action=2 , Random? True\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.590896785259) A[1]:(0.65944224596) A[2]:(0.658570349216) A[3]:(0.587915062904)\n",
      " state (1)  A[0]:(0.602800250053) A[1]:(0.000802874390502) A[2]:(0.734741091728) A[3]:(0.651486933231)\n",
      " state (2)  A[0]:(0.642305731773) A[1]:(0.812112867832) A[2]:(0.702384710312) A[3]:(0.731157779694)\n",
      " state (3)  A[0]:(0.772057414055) A[1]:(0.0632286071777) A[2]:(0.193693578243) A[3]:(0.671174824238)\n",
      " state (4)  A[0]:(0.617460489273) A[1]:(0.729328989983) A[2]:(0.0627020671964) A[3]:(0.59594476223)\n",
      " state (5)  A[0]:(0.277900308371) A[1]:(0.912523388863) A[2]:(0.0104319844395) A[3]:(0.508380055428)\n",
      " state (6)  A[0]:(0.0360978432) A[1]:(0.900563359261) A[2]:(-0.0113904317841) A[3]:(0.712586164474)\n",
      " state (7)  A[0]:(0.243522658944) A[1]:(0.589815318584) A[2]:(0.174470365047) A[3]:(0.931062221527)\n",
      " state (8)  A[0]:(0.667366147041) A[1]:(0.00292288418859) A[2]:(0.814378023148) A[3]:(0.667721927166)\n",
      " state (9)  A[0]:(0.840708553791) A[1]:(0.899987995625) A[2]:(0.877258777618) A[3]:(0.00027833878994)\n",
      " state (10)  A[0]:(0.752773404121) A[1]:(0.999778389931) A[2]:(0.0110998358577) A[3]:(0.799960017204)\n",
      " state (11)  A[0]:(0.346259891987) A[1]:(0.999995470047) A[2]:(-0.659129321575) A[3]:(0.911671221256)\n",
      " state (12)  A[0]:(-0.162836119533) A[1]:(0.999999463558) A[2]:(-0.476327747107) A[3]:(0.863627910614)\n",
      " state (13)  A[0]:(-0.00137155700941) A[1]:(0.999999940395) A[2]:(0.901275753975) A[3]:(0.80956363678)\n",
      " state (14)  A[0]:(0.896858274937) A[1]:(1.0) A[2]:(0.999992907047) A[3]:(0.898522496223)\n",
      " state (15)  A[0]:(0.998452782631) A[1]:(1.0) A[2]:(1.0) A[3]:(0.971321105957)\n",
      "Episode 140000 finished after 0 timesteps with r=0.0. Running score: 0.32. Times trained:               20969. Times reached goal: 368.               Steps done: 1511214. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.198577958949.\n",
      " state (0)  A[0]:(0.594034552574) A[1]:(0.656414031982) A[2]:(0.658744335175) A[3]:(0.590849936008)\n",
      " state (1)  A[0]:(0.603213191032) A[1]:(-0.00280755013227) A[2]:(0.729723095894) A[3]:(0.653036534786)\n",
      " state (2)  A[0]:(0.641470849514) A[1]:(0.810076355934) A[2]:(0.695825815201) A[3]:(0.73296225071)\n",
      " state (3)  A[0]:(0.76884740591) A[1]:(0.0649823248386) A[2]:(0.19652171433) A[3]:(0.665709257126)\n",
      " state (4)  A[0]:(0.614090323448) A[1]:(0.730394482613) A[2]:(0.0431134290993) A[3]:(0.596837520599)\n",
      " state (5)  A[0]:(0.26778653264) A[1]:(0.914667189121) A[2]:(-0.0155407786369) A[3]:(0.512030541897)\n",
      " state (6)  A[0]:(0.0360128208995) A[1]:(0.90050804615) A[2]:(-0.0290256254375) A[3]:(0.715792417526)\n",
      " state (7)  A[0]:(0.269079267979) A[1]:(0.575773596764) A[2]:(0.173120751977) A[3]:(0.931729912758)\n",
      " state (8)  A[0]:(0.669540882111) A[1]:(0.00191575055942) A[2]:(0.810818791389) A[3]:(0.666684150696)\n",
      " state (9)  A[0]:(0.837532639503) A[1]:(0.90074968338) A[2]:(0.873359501362) A[3]:(-0.00151708605699)\n",
      " state (10)  A[0]:(0.760432720184) A[1]:(0.999771416187) A[2]:(0.000965714163613) A[3]:(0.802043318748)\n",
      " state (11)  A[0]:(0.368547409773) A[1]:(0.999995112419) A[2]:(-0.661956250668) A[3]:(0.913189411163)\n",
      " state (12)  A[0]:(-0.148469641805) A[1]:(0.999999403954) A[2]:(-0.480036020279) A[3]:(0.864953637123)\n",
      " state (13)  A[0]:(-0.000959915749263) A[1]:(0.999999940395) A[2]:(0.898777127266) A[3]:(0.80973392725)\n",
      " state (14)  A[0]:(0.89637774229) A[1]:(1.0) A[2]:(0.999992728233) A[3]:(0.898462951183)\n",
      " state (15)  A[0]:(0.998494982719) A[1]:(1.0) A[2]:(1.0) A[3]:(0.971696019173)\n",
      "Episode 141000 finished after 0 timesteps with r=1.0. Running score: 0.37. Times trained:               22219. Times reached goal: 344.               Steps done: 1533433. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.194214411625.\n",
      " state (0)  A[0]:(0.587811350822) A[1]:(0.657353758812) A[2]:(0.65514087677) A[3]:(0.589322924614)\n",
      " state (1)  A[0]:(0.596076250076) A[1]:(-4.81605529785e-05) A[2]:(0.728614509106) A[3]:(0.651880979538)\n",
      " state (2)  A[0]:(0.636563241482) A[1]:(0.810923874378) A[2]:(0.694498836994) A[3]:(0.734402537346)\n",
      " state (3)  A[0]:(0.764869213104) A[1]:(0.0748713538051) A[2]:(0.204183042049) A[3]:(0.662895202637)\n",
      " state (4)  A[0]:(0.609421372414) A[1]:(0.731138586998) A[2]:(0.0356273017824) A[3]:(0.598077535629)\n",
      " state (5)  A[0]:(0.252905458212) A[1]:(0.915788769722) A[2]:(-0.0242255013436) A[3]:(0.510096848011)\n",
      " state (6)  A[0]:(0.0220167059451) A[1]:(0.900797784328) A[2]:(-0.0287850238383) A[3]:(0.708016753197)\n",
      " state (7)  A[0]:(0.274789184332) A[1]:(0.567518353462) A[2]:(0.18541482091) A[3]:(0.929034411907)\n",
      " state (8)  A[0]:(0.662366628647) A[1]:(-0.00162100652233) A[2]:(0.810884773731) A[3]:(0.664252519608)\n",
      " state (9)  A[0]:(0.829727530479) A[1]:(0.898940324783) A[2]:(0.872471630573) A[3]:(-0.00471730018035)\n",
      " state (10)  A[0]:(0.761482775211) A[1]:(0.99975913763) A[2]:(0.00133776583243) A[3]:(0.801608800888)\n",
      " state (11)  A[0]:(0.37911593914) A[1]:(0.999994754791) A[2]:(-0.660837709904) A[3]:(0.913628160954)\n",
      " state (12)  A[0]:(-0.142854258418) A[1]:(0.999999344349) A[2]:(-0.478237658739) A[3]:(0.865207672119)\n",
      " state (13)  A[0]:(-0.00358998915181) A[1]:(0.999999940395) A[2]:(0.89851385355) A[3]:(0.809124469757)\n",
      " state (14)  A[0]:(0.896439552307) A[1]:(1.0) A[2]:(0.999992787838) A[3]:(0.89804071188)\n",
      " state (15)  A[0]:(0.998547196388) A[1]:(1.0) A[2]:(1.0) A[3]:(0.971823692322)\n",
      "Episode 142000 finished after 0 timesteps with r=1.0. Running score: 0.3. Times trained:               21368. Times reached goal: 357.               Steps done: 1554801. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.190108462267.\n",
      " state (0)  A[0]:(0.59485578537) A[1]:(0.655882120132) A[2]:(0.661518514156) A[3]:(0.59744822979)\n",
      " state (1)  A[0]:(0.602788329124) A[1]:(0.000422656506998) A[2]:(0.727794885635) A[3]:(0.657770752907)\n",
      " state (2)  A[0]:(0.643889188766) A[1]:(0.810601532459) A[2]:(0.691985070705) A[3]:(0.741244316101)\n",
      " state (3)  A[0]:(0.769387245178) A[1]:(0.0803161486983) A[2]:(0.202654480934) A[3]:(0.66648209095)\n",
      " state (4)  A[0]:(0.615317821503) A[1]:(0.734866380692) A[2]:(0.0227331090719) A[3]:(0.60580933094)\n",
      " state (5)  A[0]:(0.253235340118) A[1]:(0.917676210403) A[2]:(-0.0350473560393) A[3]:(0.515353679657)\n",
      " state (6)  A[0]:(0.0321725904942) A[1]:(0.900633990765) A[2]:(-0.0293479301035) A[3]:(0.70942902565)\n",
      " state (7)  A[0]:(0.312021911144) A[1]:(0.555052518845) A[2]:(0.197353258729) A[3]:(0.929592549801)\n",
      " state (8)  A[0]:(0.66915845871) A[1]:(-0.00413987645879) A[2]:(0.813292562962) A[3]:(0.663755655289)\n",
      " state (9)  A[0]:(0.822626650333) A[1]:(0.899754941463) A[2]:(0.872068166733) A[3]:(-0.00240975152701)\n",
      " state (10)  A[0]:(0.759169876575) A[1]:(0.999755620956) A[2]:(0.00110781146213) A[3]:(0.803550004959)\n",
      " state (11)  A[0]:(0.384034872055) A[1]:(0.999994575977) A[2]:(-0.658394217491) A[3]:(0.915023446083)\n",
      " state (12)  A[0]:(-0.136842533946) A[1]:(0.999999284744) A[2]:(-0.471860975027) A[3]:(0.867339611053)\n",
      " state (13)  A[0]:(0.0010030210251) A[1]:(0.999999940395) A[2]:(0.899577975273) A[3]:(0.811566114426)\n",
      " state (14)  A[0]:(0.898539125919) A[1]:(1.0) A[2]:(0.999992907047) A[3]:(0.89905154705)\n",
      " state (15)  A[0]:(0.998638093472) A[1]:(1.0) A[2]:(1.0) A[3]:(0.972263514996)\n",
      "Episode 143000 finished after 0 timesteps with r=1.0. Running score: 0.36. Times trained:               23103. Times reached goal: 365.               Steps done: 1577904. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.185766733063.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.591498553753) A[1]:(0.656080007553) A[2]:(0.655812561512) A[3]:(0.590508162975)\n",
      " state (1)  A[0]:(0.600509643555) A[1]:(-0.0028066560626) A[2]:(0.731170296669) A[3]:(0.649543762207)\n",
      " state (2)  A[0]:(0.641764044762) A[1]:(0.809486627579) A[2]:(0.699042379856) A[3]:(0.736419796944)\n",
      " state (3)  A[0]:(0.770609557629) A[1]:(0.0683954209089) A[2]:(0.213238954544) A[3]:(0.655450820923)\n",
      " state (4)  A[0]:(0.617871284485) A[1]:(0.731046676636) A[2]:(0.0215132422745) A[3]:(0.599964976311)\n",
      " state (5)  A[0]:(0.249538823962) A[1]:(0.918122649193) A[2]:(-0.0355899147689) A[3]:(0.509295582771)\n",
      " state (6)  A[0]:(0.0368422158062) A[1]:(0.899994075298) A[2]:(-0.0242722034454) A[3]:(0.704960346222)\n",
      " state (7)  A[0]:(0.340280950069) A[1]:(0.545038282871) A[2]:(0.205179572105) A[3]:(0.929320037365)\n",
      " state (8)  A[0]:(0.670942544937) A[1]:(-0.000968217558693) A[2]:(0.811742424965) A[3]:(0.66771876812)\n",
      " state (9)  A[0]:(0.814707815647) A[1]:(0.899934530258) A[2]:(0.871370017529) A[3]:(-0.000628337205853)\n",
      " state (10)  A[0]:(0.758594691753) A[1]:(0.999749004841) A[2]:(0.00393138313666) A[3]:(0.802406072617)\n",
      " state (11)  A[0]:(0.393033653498) A[1]:(0.999994277954) A[2]:(-0.654817521572) A[3]:(0.914671242237)\n",
      " state (12)  A[0]:(-0.12839820981) A[1]:(0.99999922514) A[2]:(-0.464301377535) A[3]:(0.866688072681)\n",
      " state (13)  A[0]:(0.00505152158439) A[1]:(0.999999880791) A[2]:(0.901511788368) A[3]:(0.810615897179)\n",
      " state (14)  A[0]:(0.899771571159) A[1]:(1.0) A[2]:(0.999993145466) A[3]:(0.898937523365)\n",
      " state (15)  A[0]:(0.998688399792) A[1]:(1.0) A[2]:(1.0) A[3]:(0.972546994686)\n",
      "Episode 144000 finished after 0 timesteps with r=0.0. Running score: 0.33. Times trained:               22487. Times reached goal: 365.               Steps done: 1600391. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.181636014334.\n",
      "q_values \n",
      "tensor([[ 0.5898,  0.6562,  0.6548,  0.5884]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6163,  0.7325,  0.0297,  0.5973]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6818, -0.0016,  0.8131,  0.6642]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8119,  0.9004,  0.8715,  0.0017]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0061,  1.0000,  0.8959,  0.8089]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0081,  1.0000,  0.8958,  0.8084]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0071,  1.0000,  0.8962,  0.8086]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0030,  1.0000,  0.8970,  0.8096]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0012,  1.0000,  0.8980,  0.8105]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0036,  1.0000,  0.8988,  0.8111]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0045,  1.0000,  0.8994,  0.8112]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0045,  1.0000,  0.8998,  0.8112]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0039,  1.0000,  0.9000,  0.8110]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0014,  1.0000,  0.8998,  0.8104]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8087,  0.8989,  0.8710, -0.0027]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7583,  0.9997,  0.0007,  0.8021]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8983,  1.0000,  1.0000,  0.8985]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8987,  1.0000,  1.0000,  0.8988]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8991,  1.0000,  1.0000,  0.8990]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8989,  1.0000,  1.0000,  0.8990]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8984,  1.0000,  1.0000,  0.8987]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7581,  0.9997, -0.0011,  0.8027]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8981,  1.0000,  1.0000,  0.8986]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8985,  1.0000,  1.0000,  0.8989]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8989,  1.0000,  1.0000,  0.8992]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8994,  1.0000,  1.0000,  0.8996]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8997,  1.0000,  1.0000,  0.8998]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8995,  1.0000,  1.0000,  0.8997]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8989,  1.0000,  1.0000,  0.8994]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8984,  1.0000,  1.0000,  0.8991]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8979,  1.0000,  1.0000,  0.8989]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8976,  1.0000,  1.0000,  0.8988]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8977,  1.0000,  1.0000,  0.8989]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8977,  1.0000,  1.0000,  0.8990]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7596,  0.9997, -0.0009,  0.8037]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8972,  1.0000,  1.0000,  0.8986]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8972,  1.0000,  1.0000,  0.8986]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8975,  1.0000,  1.0000,  0.8988]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8983,  1.0000,  1.0000,  0.8992]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8989,  1.0000,  1.0000,  0.8996]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8992,  1.0000,  1.0000,  0.8998]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8996,  1.0000,  1.0000,  0.9000]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8996,  1.0000,  1.0000,  0.9001]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7592,  0.9997,  0.0007,  0.8039]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8992,  1.0000,  1.0000,  0.8999]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? True\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.591758489609) A[1]:(0.654876112938) A[2]:(0.655241131783) A[3]:(0.589764416218)\n",
      " state (1)  A[0]:(0.602776169777) A[1]:(-0.00442931149155) A[2]:(0.728374123573) A[3]:(0.64923876524)\n",
      " state (2)  A[0]:(0.637367486954) A[1]:(0.808825731277) A[2]:(0.699284434319) A[3]:(0.736824274063)\n",
      " state (3)  A[0]:(0.766713440418) A[1]:(0.0978088378906) A[2]:(0.234810486436) A[3]:(0.650996685028)\n",
      " state (4)  A[0]:(0.615269124508) A[1]:(0.730621933937) A[2]:(0.0208010468632) A[3]:(0.59949362278)\n",
      " state (5)  A[0]:(0.238989681005) A[1]:(0.919124722481) A[2]:(-0.0452164635062) A[3]:(0.510387897491)\n",
      " state (6)  A[0]:(0.0354895405471) A[1]:(0.900420665741) A[2]:(-0.0344398841262) A[3]:(0.703622221947)\n",
      " state (7)  A[0]:(0.368241548538) A[1]:(0.538598656654) A[2]:(0.199880883098) A[3]:(0.929001331329)\n",
      " state (8)  A[0]:(0.678124248981) A[1]:(-0.00340871675871) A[2]:(0.810457766056) A[3]:(0.666373670101)\n",
      " state (9)  A[0]:(0.809692561626) A[1]:(0.899629116058) A[2]:(0.870857775211) A[3]:(0.00181069772225)\n",
      " state (10)  A[0]:(0.759717822075) A[1]:(0.999743402004) A[2]:(-0.000152826309204) A[3]:(0.804727911949)\n",
      " state (11)  A[0]:(0.40219014883) A[1]:(0.999994039536) A[2]:(-0.6580286026) A[3]:(0.916094839573)\n",
      " state (12)  A[0]:(-0.122950181365) A[1]:(0.999999165535) A[2]:(-0.468240261078) A[3]:(0.868360042572)\n",
      " state (13)  A[0]:(0.00291334022768) A[1]:(0.999999880791) A[2]:(0.900690495968) A[3]:(0.812184512615)\n",
      " state (14)  A[0]:(0.899165034294) A[1]:(1.0) A[2]:(0.999993145466) A[3]:(0.899980545044)\n",
      " state (15)  A[0]:(0.998695313931) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973034083843)\n",
      "Episode 145000 finished after 0 timesteps with r=1.0. Running score: 0.43. Times trained:               21543. Times reached goal: 345.               Steps done: 1621934. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.177764877345.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.593494355679) A[1]:(0.658893585205) A[2]:(0.655719399452) A[3]:(0.593339979649)\n",
      " state (1)  A[0]:(0.604947209358) A[1]:(0.0014615644468) A[2]:(0.727880358696) A[3]:(0.650794744492)\n",
      " state (2)  A[0]:(0.642940223217) A[1]:(0.809812366962) A[2]:(0.698744893074) A[3]:(0.739794790745)\n",
      " state (3)  A[0]:(0.772976577282) A[1]:(0.0674466788769) A[2]:(0.232487589121) A[3]:(0.647086679935)\n",
      " state (4)  A[0]:(0.62083029747) A[1]:(0.72972202301) A[2]:(0.0129250241444) A[3]:(0.601139545441)\n",
      " state (5)  A[0]:(0.234938651323) A[1]:(0.920133173466) A[2]:(-0.05110944435) A[3]:(0.507475018501)\n",
      " state (6)  A[0]:(0.0350547768176) A[1]:(0.901357233524) A[2]:(-0.0356419458985) A[3]:(0.694724440575)\n",
      " state (7)  A[0]:(0.39543312788) A[1]:(0.536596775055) A[2]:(0.201674953103) A[3]:(0.926665723324)\n",
      " state (8)  A[0]:(0.684426784515) A[1]:(0.000576615275349) A[2]:(0.810187339783) A[3]:(0.66360604763)\n",
      " state (9)  A[0]:(0.803080976009) A[1]:(0.900204479694) A[2]:(0.871739268303) A[3]:(-0.00223131105304)\n",
      " state (10)  A[0]:(0.759814798832) A[1]:(0.999742865562) A[2]:(0.00137257494498) A[3]:(0.803213953972)\n",
      " state (11)  A[0]:(0.411117196083) A[1]:(0.999993979931) A[2]:(-0.658564448357) A[3]:(0.915754377842)\n",
      " state (12)  A[0]:(-0.115786015987) A[1]:(0.999999165535) A[2]:(-0.467998683453) A[3]:(0.867550075054)\n",
      " state (13)  A[0]:(0.0051992139779) A[1]:(0.999999880791) A[2]:(0.901361405849) A[3]:(0.810567080975)\n",
      " state (14)  A[0]:(0.900126934052) A[1]:(1.0) A[2]:(0.999993383884) A[3]:(0.899186432362)\n",
      " state (15)  A[0]:(0.998728454113) A[1]:(1.0) A[2]:(1.0) A[3]:(0.972901523113)\n",
      "Episode 146000 finished after 0 timesteps with r=1.0. Running score: 0.33. Times trained:               24164. Times reached goal: 351.               Steps done: 1646098. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.173520849696.\n",
      " state (0)  A[0]:(0.594424724579) A[1]:(0.656333386898) A[2]:(0.660119891167) A[3]:(0.592194199562)\n",
      " state (1)  A[0]:(0.606050848961) A[1]:(0.00323765329085) A[2]:(0.727501630783) A[3]:(0.651210904121)\n",
      " state (2)  A[0]:(0.638977885246) A[1]:(0.810965776443) A[2]:(0.703055500984) A[3]:(0.739036202431)\n",
      " state (3)  A[0]:(0.769718110561) A[1]:(0.0786138102412) A[2]:(0.265182375908) A[3]:(0.638662576675)\n",
      " state (4)  A[0]:(0.618705570698) A[1]:(0.732926249504) A[2]:(0.0232213698328) A[3]:(0.601718664169)\n",
      " state (5)  A[0]:(0.227076902986) A[1]:(0.922416329384) A[2]:(-0.0521629415452) A[3]:(0.515987455845)\n",
      " state (6)  A[0]:(0.0472405441105) A[1]:(0.900564789772) A[2]:(-0.0355197899044) A[3]:(0.705245137215)\n",
      " state (7)  A[0]:(0.43871435523) A[1]:(0.519580245018) A[2]:(0.206836342812) A[3]:(0.929826080799)\n",
      " state (8)  A[0]:(0.693330585957) A[1]:(-0.000826537434477) A[2]:(0.810567736626) A[3]:(0.668306529522)\n",
      " state (9)  A[0]:(0.796504795551) A[1]:(0.900551617146) A[2]:(0.872866034508) A[3]:(-0.00366720394231)\n",
      " state (10)  A[0]:(0.761393070221) A[1]:(0.999738812447) A[2]:(0.000831842247862) A[3]:(0.803319573402)\n",
      " state (11)  A[0]:(0.42273658514) A[1]:(0.999993741512) A[2]:(-0.662317872047) A[3]:(0.91636967659)\n",
      " state (12)  A[0]:(-0.1094584167) A[1]:(0.99999910593) A[2]:(-0.476393163204) A[3]:(0.867840528488)\n",
      " state (13)  A[0]:(-0.00174705882091) A[1]:(0.999999880791) A[2]:(0.897766411304) A[3]:(0.809303820133)\n",
      " state (14)  A[0]:(0.897227704525) A[1]:(1.0) A[2]:(0.999993085861) A[3]:(0.897821068764)\n",
      " state (15)  A[0]:(0.998697936535) A[1]:(1.0) A[2]:(1.0) A[3]:(0.972545683384)\n",
      "Episode 147000 finished after 0 timesteps with r=0.0. Running score: 0.4. Times trained:               25229. Times reached goal: 389.               Steps done: 1671327. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.169197853907.\n",
      " state (0)  A[0]:(0.590025961399) A[1]:(0.658580243587) A[2]:(0.652265429497) A[3]:(0.591742694378)\n",
      " state (1)  A[0]:(0.600704848766) A[1]:(0.00178867392242) A[2]:(0.727284908295) A[3]:(0.648078083992)\n",
      " state (2)  A[0]:(0.632726430893) A[1]:(0.811485350132) A[2]:(0.705891370773) A[3]:(0.737224340439)\n",
      " state (3)  A[0]:(0.76651096344) A[1]:(0.0735783651471) A[2]:(0.288721263409) A[3]:(0.62681055069)\n",
      " state (4)  A[0]:(0.615586876869) A[1]:(0.73169118166) A[2]:(0.0273818448186) A[3]:(0.599088668823)\n",
      " state (5)  A[0]:(0.210255101323) A[1]:(0.923593997955) A[2]:(-0.0564872100949) A[3]:(0.516321063042)\n",
      " state (6)  A[0]:(0.0243719629943) A[1]:(0.901802361012) A[2]:(-0.0370110422373) A[3]:(0.697661757469)\n",
      " state (7)  A[0]:(0.435558080673) A[1]:(0.517551600933) A[2]:(0.212056383491) A[3]:(0.926068007946)\n",
      " state (8)  A[0]:(0.684929132462) A[1]:(-0.00380675145425) A[2]:(0.811407446861) A[3]:(0.659799456596)\n",
      " state (9)  A[0]:(0.786903381348) A[1]:(0.898778259754) A[2]:(0.873352646828) A[3]:(-0.0117205232382)\n",
      " state (10)  A[0]:(0.763008713722) A[1]:(0.999726772308) A[2]:(-0.00292789097875) A[3]:(0.802926778793)\n",
      " state (11)  A[0]:(0.43486186862) A[1]:(0.999993264675) A[2]:(-0.665538549423) A[3]:(0.916845083237)\n",
      " state (12)  A[0]:(-0.10014615953) A[1]:(0.999998986721) A[2]:(-0.47758358717) A[3]:(0.868634462357)\n",
      " state (13)  A[0]:(0.00396736478433) A[1]:(0.999999880791) A[2]:(0.89998281002) A[3]:(0.810906529427)\n",
      " state (14)  A[0]:(0.899962186813) A[1]:(1.0) A[2]:(0.999993622303) A[3]:(0.89955163002)\n",
      " state (15)  A[0]:(0.99874907732) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973092973232)\n",
      "Episode 148000 finished after 0 timesteps with r=0.0. Running score: 0.32. Times trained:               24859. Times reached goal: 358.               Steps done: 1696186. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.165043613518.\n",
      " state (0)  A[0]:(0.587703943253) A[1]:(0.658509969711) A[2]:(0.656126499176) A[3]:(0.587210536003)\n",
      " state (1)  A[0]:(0.600544691086) A[1]:(-0.000376701325877) A[2]:(0.732357442379) A[3]:(0.648261785507)\n",
      " state (2)  A[0]:(0.636722028255) A[1]:(0.809525430202) A[2]:(0.711699962616) A[3]:(0.73845499754)\n",
      " state (3)  A[0]:(0.772454977036) A[1]:(0.0424890257418) A[2]:(0.298009425402) A[3]:(0.618159711361)\n",
      " state (4)  A[0]:(0.625059008598) A[1]:(0.729701697826) A[2]:(0.0266800969839) A[3]:(0.600266695023)\n",
      " state (5)  A[0]:(0.217292159796) A[1]:(0.923908889294) A[2]:(-0.0544447824359) A[3]:(0.521898269653)\n",
      " state (6)  A[0]:(0.0414126254618) A[1]:(0.899933636189) A[2]:(-0.0249895267189) A[3]:(0.703091561794)\n",
      " state (7)  A[0]:(0.470631510019) A[1]:(0.503991901875) A[2]:(0.22830478847) A[3]:(0.928307712078)\n",
      " state (8)  A[0]:(0.693113446236) A[1]:(0.00468268524855) A[2]:(0.812449336052) A[3]:(0.673438191414)\n",
      " state (9)  A[0]:(0.783102691174) A[1]:(0.900801897049) A[2]:(0.875227689743) A[3]:(0.00273244897835)\n",
      " state (10)  A[0]:(0.76672232151) A[1]:(0.999724924564) A[2]:(0.00958365667611) A[3]:(0.805536448956)\n",
      " state (11)  A[0]:(0.447771281004) A[1]:(0.999993026257) A[2]:(-0.65728867054) A[3]:(0.917909443378)\n",
      " state (12)  A[0]:(-0.091485068202) A[1]:(0.999998927116) A[2]:(-0.466522961855) A[3]:(0.869941532612)\n",
      " state (13)  A[0]:(0.00189424084965) A[1]:(0.999999880791) A[2]:(0.901089370251) A[3]:(0.812099456787)\n",
      " state (14)  A[0]:(0.898979306221) A[1]:(1.0) A[2]:(0.999993622303) A[3]:(0.900061607361)\n",
      " state (15)  A[0]:(0.998764336109) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973412632942)\n",
      "Episode 149000 finished after 0 timesteps with r=0.0. Running score: 0.51. Times trained:               24942. Times reached goal: 352.               Steps done: 1721128. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.160978008635.\n",
      "q_values \n",
      "tensor([[ 0.5868,  0.6566,  0.6547,  0.5908]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6254,  0.7288,  0.0084,  0.6017]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6902, -0.0019,  0.8105,  0.6688]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7726,  0.8995,  0.8739, -0.0021]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0011,  1.0000,  0.8990,  0.8107]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7718,  0.8997,  0.8740, -0.0051]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0036,  1.0000,  0.8985,  0.8100]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0027,  1.0000,  0.8987,  0.8101]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0008,  1.0000,  0.8990,  0.8104]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  1.0000,  0.8992,  0.8106]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0013,  1.0000,  0.8994,  0.8108]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0013,  1.0000,  0.8996,  0.8109]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0018,  1.0000,  0.8996,  0.8111]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0015,  1.0000,  0.8997,  0.8112]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0005,  1.0000,  0.8996,  0.8111]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0002,  1.0000,  0.8996,  0.8112]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0002,  1.0000,  0.8996,  0.8112]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0005,  1.0000,  0.8998,  0.8114]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0011,  1.0000,  0.9000,  0.8116]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8993,  1.0000,  1.0000,  0.8998]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8993,  1.0000,  1.0000,  0.8998]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8993,  1.0000,  1.0000,  0.8998]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8990,  1.0000,  1.0000,  0.8996]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8989,  1.0000,  1.0000,  0.8996]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8991,  1.0000,  1.0000,  0.8997]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8992,  1.0000,  1.0000,  0.8998]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8991,  1.0000,  1.0000,  0.8998]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8988,  1.0000,  1.0000,  0.8996]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8984,  1.0000,  1.0000,  0.8994]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_values \n",
      "tensor([[ 0.8981,  1.0000,  1.0000,  0.8992]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8977,  1.0000,  1.0000,  0.8989]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8976,  1.0000,  1.0000,  0.8988]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8977,  1.0000,  1.0000,  0.8987]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? True\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.588553190231) A[1]:(0.656483471394) A[2]:(0.655444145203) A[3]:(0.592231869698)\n",
      " state (1)  A[0]:(0.598371267319) A[1]:(0.000270485877991) A[2]:(0.727883696556) A[3]:(0.651096582413)\n",
      " state (2)  A[0]:(0.63635623455) A[1]:(0.809521317482) A[2]:(0.705286860466) A[3]:(0.743558466434)\n",
      " state (3)  A[0]:(0.773667573929) A[1]:(0.0401720479131) A[2]:(0.28854188323) A[3]:(0.617353856564)\n",
      " state (4)  A[0]:(0.627662777901) A[1]:(0.728969752789) A[2]:(0.0103856166825) A[3]:(0.603883385658)\n",
      " state (5)  A[0]:(0.211020410061) A[1]:(0.924072027206) A[2]:(-0.0633919239044) A[3]:(0.52590996027)\n",
      " state (6)  A[0]:(0.0383977182209) A[1]:(0.898713350296) A[2]:(-0.0250480230898) A[3]:(0.702346324921)\n",
      " state (7)  A[0]:(0.48732355237) A[1]:(0.491784721613) A[2]:(0.230826884508) A[3]:(0.927413225174)\n",
      " state (8)  A[0]:(0.692187905312) A[1]:(-0.00280158990063) A[2]:(0.812023639679) A[3]:(0.670369625092)\n",
      " state (9)  A[0]:(0.773326933384) A[1]:(0.899278402328) A[2]:(0.875236988068) A[3]:(-0.0026308237575)\n",
      " state (10)  A[0]:(0.766830801964) A[1]:(0.999715805054) A[2]:(0.00495358230546) A[3]:(0.804831206799)\n",
      " state (11)  A[0]:(0.457452356815) A[1]:(0.999992609024) A[2]:(-0.660470724106) A[3]:(0.917884171009)\n",
      " state (12)  A[0]:(-0.084725394845) A[1]:(0.999998867512) A[2]:(-0.471223175526) A[3]:(0.869461536407)\n",
      " state (13)  A[0]:(-0.00140773039311) A[1]:(0.999999821186) A[2]:(0.89867413044) A[3]:(0.810370266438)\n",
      " state (14)  A[0]:(0.897863268852) A[1]:(1.0) A[2]:(0.999993443489) A[3]:(0.898751854897)\n",
      " state (15)  A[0]:(0.998776197433) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973159909248)\n",
      "Episode 150000 finished after 0 timesteps with r=1.0. Running score: 0.36. Times trained:               26719. Times reached goal: 350.               Steps done: 1747847. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.156733790351.\n",
      " state (0)  A[0]:(0.593424916267) A[1]:(0.657279729843) A[2]:(0.65371465683) A[3]:(0.592938840389)\n",
      " state (1)  A[0]:(0.603210151196) A[1]:(-0.00031042098999) A[2]:(0.727484643459) A[3]:(0.649980902672)\n",
      " state (2)  A[0]:(0.638034939766) A[1]:(0.810700416565) A[2]:(0.710753858089) A[3]:(0.743429064751)\n",
      " state (3)  A[0]:(0.776445865631) A[1]:(0.0602302215993) A[2]:(0.299917697906) A[3]:(0.611536026001)\n",
      " state (4)  A[0]:(0.634495139122) A[1]:(0.729932785034) A[2]:(0.00933524686843) A[3]:(0.602083563805)\n",
      " state (5)  A[0]:(0.209857285023) A[1]:(0.925611019135) A[2]:(-0.0658160150051) A[3]:(0.526735842228)\n",
      " state (6)  A[0]:(0.0344183593988) A[1]:(0.900840044022) A[2]:(-0.028457108885) A[3]:(0.698299646378)\n",
      " state (7)  A[0]:(0.503523826599) A[1]:(0.494803786278) A[2]:(0.222102090716) A[3]:(0.926122367382)\n",
      " state (8)  A[0]:(0.696299791336) A[1]:(0.00285815424286) A[2]:(0.808951377869) A[3]:(0.671423077583)\n",
      " state (9)  A[0]:(0.766542494297) A[1]:(0.899766623974) A[2]:(0.875443041325) A[3]:(-3.41236591339e-05)\n",
      " state (10)  A[0]:(0.766894936562) A[1]:(0.999709010124) A[2]:(0.00268577877432) A[3]:(0.805121839046)\n",
      " state (11)  A[0]:(0.464056491852) A[1]:(0.999992191792) A[2]:(-0.662844419479) A[3]:(0.917827069759)\n",
      " state (12)  A[0]:(-0.0806279331446) A[1]:(0.999998807907) A[2]:(-0.473113715649) A[3]:(0.868935465813)\n",
      " state (13)  A[0]:(-0.00192418834195) A[1]:(0.999999821186) A[2]:(0.899302363396) A[3]:(0.809333920479)\n",
      " state (14)  A[0]:(0.898425042629) A[1]:(1.0) A[2]:(0.999993681908) A[3]:(0.898420631886)\n",
      " state (15)  A[0]:(0.998791873455) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973067164421)\n",
      "Episode 151000 finished after 0 timesteps with r=1.0. Running score: 0.35. Times trained:               26098. Times reached goal: 333.               Steps done: 1773945. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.152696266699.\n",
      " state (0)  A[0]:(0.588466286659) A[1]:(0.655454039574) A[2]:(0.656479299068) A[3]:(0.590476632118)\n",
      " state (1)  A[0]:(0.597133934498) A[1]:(-0.00105202163104) A[2]:(0.727312624454) A[3]:(0.647226572037)\n",
      " state (2)  A[0]:(0.629795372486) A[1]:(0.807934880257) A[2]:(0.713094234467) A[3]:(0.742524623871)\n",
      " state (3)  A[0]:(0.771005094051) A[1]:(0.0681959614158) A[2]:(0.305360376835) A[3]:(0.600696742535)\n",
      " state (4)  A[0]:(0.627873897552) A[1]:(0.727780103683) A[2]:(0.00684655923396) A[3]:(0.59469139576)\n",
      " state (5)  A[0]:(0.190120860934) A[1]:(0.925867915154) A[2]:(-0.0663917735219) A[3]:(0.522814750671)\n",
      " state (6)  A[0]:(0.0196839328855) A[1]:(0.900748848915) A[2]:(-0.0286211259663) A[3]:(0.693836927414)\n",
      " state (7)  A[0]:(0.515293776989) A[1]:(0.49012607336) A[2]:(0.216482296586) A[3]:(0.924213826656)\n",
      " state (8)  A[0]:(0.694520950317) A[1]:(0.00253474176861) A[2]:(0.808327794075) A[3]:(0.663308799267)\n",
      " state (9)  A[0]:(0.7535110116) A[1]:(0.899820208549) A[2]:(0.876888513565) A[3]:(-0.00694514065981)\n",
      " state (10)  A[0]:(0.761573970318) A[1]:(0.999704539776) A[2]:(0.00195085757878) A[3]:(0.805064916611)\n",
      " state (11)  A[0]:(0.462375074625) A[1]:(0.999991893768) A[2]:(-0.664242982864) A[3]:(0.918095648289)\n",
      " state (12)  A[0]:(-0.0838384777308) A[1]:(0.999998688698) A[2]:(-0.470926731825) A[3]:(0.869302392006)\n",
      " state (13)  A[0]:(-0.00442455802113) A[1]:(0.999999821186) A[2]:(0.902536511421) A[3]:(0.810349881649)\n",
      " state (14)  A[0]:(0.89995020628) A[1]:(1.0) A[2]:(0.999994277954) A[3]:(0.899887919426)\n",
      " state (15)  A[0]:(0.99882209301) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973536133766)\n",
      "Episode 152000 finished after 0 timesteps with r=1.0. Running score: 0.34. Times trained:               26802. Times reached goal: 369.               Steps done: 1800747. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.148658059112.\n",
      " state (0)  A[0]:(0.59247225523) A[1]:(0.655567586422) A[2]:(0.656402885914) A[3]:(0.588606476784)\n",
      " state (1)  A[0]:(0.605040848255) A[1]:(0.00232338486239) A[2]:(0.727835297585) A[3]:(0.642235457897)\n",
      " state (2)  A[0]:(0.635019421577) A[1]:(0.809780299664) A[2]:(0.708146870136) A[3]:(0.751380205154)\n",
      " state (3)  A[0]:(0.770603358746) A[1]:(0.215761706233) A[2]:(0.310286611319) A[3]:(0.619277894497)\n",
      " state (4)  A[0]:(0.639898777008) A[1]:(0.730880260468) A[2]:(0.00650456314906) A[3]:(0.596067428589)\n",
      " state (5)  A[0]:(0.204389899969) A[1]:(0.924802660942) A[2]:(-0.056171592325) A[3]:(0.526938319206)\n",
      " state (6)  A[0]:(0.0339132472873) A[1]:(0.899469017982) A[2]:(-0.0156737845391) A[3]:(0.69485938549)\n",
      " state (7)  A[0]:(0.540443181992) A[1]:(0.485731065273) A[2]:(0.22077652812) A[3]:(0.9240899086)\n",
      " state (8)  A[0]:(0.707459509373) A[1]:(-0.000663399579935) A[2]:(0.812322497368) A[3]:(0.662472963333)\n",
      " state (9)  A[0]:(0.758540630341) A[1]:(0.899529457092) A[2]:(0.881108224392) A[3]:(-0.00602911552414)\n",
      " state (10)  A[0]:(0.775136113167) A[1]:(0.999698162079) A[2]:(0.00818353518844) A[3]:(0.807208895683)\n",
      " state (11)  A[0]:(0.491172522306) A[1]:(0.999991476536) A[2]:(-0.66393661499) A[3]:(0.919028520584)\n",
      " state (12)  A[0]:(-0.06076605618) A[1]:(0.999998629093) A[2]:(-0.472829341888) A[3]:(0.869955420494)\n",
      " state (13)  A[0]:(-0.000295989215374) A[1]:(0.999999821186) A[2]:(0.90056592226) A[3]:(0.80971455574)\n",
      " state (14)  A[0]:(0.898476064205) A[1]:(1.0) A[2]:(0.99999409914) A[3]:(0.89890319109)\n",
      " state (15)  A[0]:(0.998805642128) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973276913166)\n",
      "Episode 153000 finished after 0 timesteps with r=1.0. Running score: 0.43. Times trained:               26799. Times reached goal: 354.               Steps done: 1827546. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.144727080199.\n",
      " state (0)  A[0]:(0.590742468834) A[1]:(0.655686080456) A[2]:(0.655549645424) A[3]:(0.590524077415)\n",
      " state (1)  A[0]:(0.60957890749) A[1]:(0.00288163824007) A[2]:(0.728186130524) A[3]:(0.638588666916)\n",
      " state (2)  A[0]:(0.633097887039) A[1]:(0.808773100376) A[2]:(0.703168570995) A[3]:(0.758823275566)\n",
      " state (3)  A[0]:(0.758204758167) A[1]:(0.451616644859) A[2]:(0.328565448523) A[3]:(0.658593833447)\n",
      " state (4)  A[0]:(0.642255306244) A[1]:(0.729355216026) A[2]:(0.00718938838691) A[3]:(0.596328914165)\n",
      " state (5)  A[0]:(0.200900152326) A[1]:(0.922712922096) A[2]:(-0.0523967854679) A[3]:(0.526651978493)\n",
      " state (6)  A[0]:(0.0246029570699) A[1]:(0.899140834808) A[2]:(-0.0155256427824) A[3]:(0.694982767105)\n",
      " state (7)  A[0]:(0.548638939857) A[1]:(0.486087173223) A[2]:(0.206663459539) A[3]:(0.925123989582)\n",
      " state (8)  A[0]:(0.709200680256) A[1]:(0.000682949903421) A[2]:(0.810985028744) A[3]:(0.665603041649)\n",
      " state (9)  A[0]:(0.754246413708) A[1]:(0.899962365627) A[2]:(0.882105529308) A[3]:(0.00218321033753)\n",
      " state (10)  A[0]:(0.779516637325) A[1]:(0.999687492847) A[2]:(0.00408659083769) A[3]:(0.810193419456)\n",
      " state (11)  A[0]:(0.505339920521) A[1]:(0.999990880489) A[2]:(-0.665834307671) A[3]:(0.919697403908)\n",
      " state (12)  A[0]:(-0.0485047996044) A[1]:(0.999998450279) A[2]:(-0.474681615829) A[3]:(0.870513617992)\n",
      " state (13)  A[0]:(0.00184106244706) A[1]:(0.999999761581) A[2]:(0.898915290833) A[3]:(0.80992525816)\n",
      " state (14)  A[0]:(0.898457527161) A[1]:(1.0) A[2]:(0.999993920326) A[3]:(0.898819804192)\n",
      " state (15)  A[0]:(0.998830795288) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973360359669)\n",
      "Episode 154000 finished after 0 timesteps with r=0.0. Running score: 0.33. Times trained:               28716. Times reached goal: 322.               Steps done: 1856262. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.140630201866.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_values \n",
      "tensor([[ 0.5909,  0.6565,  0.6542,  0.5911]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6444,  0.7284,  0.0035,  0.5945]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7105,  0.0036,  0.8090,  0.6654]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7446,  0.8996,  0.8857, -0.0033]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  1.0000,  0.9013,  0.8103]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  1.0000,  0.9013,  0.8103]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7438,  0.8992,  0.8862, -0.0063]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0007,  1.0000,  0.9012,  0.8102]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0005,  1.0000,  0.9012,  0.8103]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  1.0000,  0.9012,  0.8104]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0005,  1.0000,  0.9013,  0.8107]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0009,  1.0000,  0.9012,  0.8108]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0020,  1.0000,  0.9012,  0.8112]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  1.0000,  0.9008,  0.8109]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  1.0000,  0.9004,  0.8109]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0005,  1.0000,  0.9002,  0.8109]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  1.0000,  0.9000,  0.8111]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0013,  1.0000,  0.8998,  0.8113]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0017,  1.0000,  0.8995,  0.8113]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0018,  1.0000,  0.8993,  0.8112]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0010,  1.0000,  0.8989,  0.8109]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0000,  1.0000,  0.8986,  0.8106]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0009,  1.0000,  0.8984,  0.8102]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  1.0000,  0.8984,  0.8102]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  1.0000,  0.8985,  0.8102]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0021,  1.0000,  0.8987,  0.8103]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0027,  1.0000,  0.8988,  0.8102]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0020,  1.0000,  0.8987,  0.8100]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0005,  1.0000,  0.8985,  0.8097]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0000,  1.0000,  0.8984,  0.8096]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0009,  1.0000,  0.8985,  0.8098]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0020,  1.0000,  0.8985,  0.8101]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0025,  1.0000,  0.8986,  0.8103]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0016,  1.0000,  0.8985,  0.8102]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0006,  1.0000,  0.8983,  0.8098]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0015,  1.0000,  0.8983,  0.8096]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0008,  1.0000,  0.8986,  0.8097]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  1.0000,  0.8990,  0.8099]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0010,  1.0000,  0.8993,  0.8101]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0016,  1.0000,  0.8996,  0.8103]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  1.0000,  0.8996,  0.8101]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  1.0000,  0.8996,  0.8101]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0006,  1.0000,  0.8997,  0.8100]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0013,  1.0000,  0.8997,  0.8098]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  1.0000,  0.9000,  0.8099]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0010,  1.0000,  0.9002,  0.8102]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0017,  1.0000,  0.9005,  0.8102]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0015,  1.0000,  0.9005,  0.8101]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0011,  1.0000,  0.9006,  0.8100]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  1.0000,  0.9005,  0.8098]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  1.0000,  0.9004,  0.8097]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7461,  0.8993,  0.8858, -0.0071]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0020,  1.0000,  0.9003,  0.8095]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  1.0000,  0.9006,  0.8101]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0024,  1.0000,  0.9009,  0.8106]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0029,  1.0000,  0.9009,  0.8108]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0018,  1.0000,  0.9008,  0.8107]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  1.0000,  0.9006,  0.8105]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0005,  1.0000,  0.9004,  0.8104]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0012,  1.0000,  0.9003,  0.8103]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0002,  1.0000,  0.9002,  0.8107]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0014,  1.0000,  0.9004,  0.8112]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.590402960777) A[1]:(0.656141579151) A[2]:(0.656506538391) A[3]:(0.590419054031)\n",
      " state (1)  A[0]:(0.608787894249) A[1]:(0.00174957339186) A[2]:(0.728774666786) A[3]:(0.636564016342)\n",
      " state (2)  A[0]:(0.637175559998) A[1]:(0.809123873711) A[2]:(0.700582742691) A[3]:(0.756235599518)\n",
      " state (3)  A[0]:(0.760712921619) A[1]:(0.470921486616) A[2]:(0.318928211927) A[3]:(0.660943388939)\n",
      " state (4)  A[0]:(0.644847154617) A[1]:(0.728906989098) A[2]:(0.00308512663469) A[3]:(0.593299388885)\n",
      " state (5)  A[0]:(0.198440045118) A[1]:(0.922507584095) A[2]:(-0.0429298318923) A[3]:(0.522553086281)\n",
      " state (6)  A[0]:(0.0293847862631) A[1]:(0.899288117886) A[2]:(-0.0123684061691) A[3]:(0.697918534279)\n",
      " state (7)  A[0]:(0.570555210114) A[1]:(0.484458893538) A[2]:(0.183729976416) A[3]:(0.927807629108)\n",
      " state (8)  A[0]:(0.714427351952) A[1]:(0.00116717757192) A[2]:(0.811340689659) A[3]:(0.663207769394)\n",
      " state (9)  A[0]:(0.748767614365) A[1]:(0.900403678417) A[2]:(0.885864138603) A[3]:(0.00115933967754)\n",
      " state (10)  A[0]:(0.780631780624) A[1]:(0.999676048756) A[2]:(0.00215911539271) A[3]:(0.812483072281)\n",
      " state (11)  A[0]:(0.512351155281) A[1]:(0.999990165234) A[2]:(-0.669220149517) A[3]:(0.920515835285)\n",
      " state (12)  A[0]:(-0.0436389371753) A[1]:(0.99999833107) A[2]:(-0.47544157505) A[3]:(0.871590971947)\n",
      " state (13)  A[0]:(0.00282473862171) A[1]:(0.999999761581) A[2]:(0.900420367718) A[3]:(0.811592578888)\n",
      " state (14)  A[0]:(0.899865269661) A[1]:(1.0) A[2]:(0.999994218349) A[3]:(0.900198936462)\n",
      " state (15)  A[0]:(0.998865485191) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973784685135)\n",
      "Episode 155000 finished after 0 timesteps with r=0.0. Running score: 0.35. Times trained:               28000. Times reached goal: 320.               Steps done: 1884262. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.136747172316.\n",
      " state (0)  A[0]:(0.589542031288) A[1]:(0.656916201115) A[2]:(0.655028522015) A[3]:(0.590780854225)\n",
      " state (1)  A[0]:(0.608489871025) A[1]:(0.000750481965952) A[2]:(0.72786706686) A[3]:(0.633376717567)\n",
      " state (2)  A[0]:(0.63910639286) A[1]:(0.808776676655) A[2]:(0.699767827988) A[3]:(0.750175952911)\n",
      " state (3)  A[0]:(0.763670563698) A[1]:(0.498006612062) A[2]:(0.314580470324) A[3]:(0.66305065155)\n",
      " state (4)  A[0]:(0.650582790375) A[1]:(0.729878425598) A[2]:(-0.000204563140869) A[3]:(0.592269897461)\n",
      " state (5)  A[0]:(0.196838572621) A[1]:(0.922141432762) A[2]:(-0.0358916074038) A[3]:(0.519112110138)\n",
      " state (6)  A[0]:(0.0186836812645) A[1]:(0.899740993977) A[2]:(-0.0126446895301) A[3]:(0.693750977516)\n",
      " state (7)  A[0]:(0.576342225075) A[1]:(0.485260993242) A[2]:(0.155665352941) A[3]:(0.92755562067)\n",
      " state (8)  A[0]:(0.717354059219) A[1]:(0.00136864103843) A[2]:(0.809919595718) A[3]:(0.659854948521)\n",
      " state (9)  A[0]:(0.745364129543) A[1]:(0.899947583675) A[2]:(0.889542222023) A[3]:(-0.000487133831484)\n",
      " state (10)  A[0]:(0.782341063023) A[1]:(0.999647080898) A[2]:(0.00232779560611) A[3]:(0.812113165855)\n",
      " state (11)  A[0]:(0.518870711327) A[1]:(0.999988496304) A[2]:(-0.671779036522) A[3]:(0.919831097126)\n",
      " state (12)  A[0]:(-0.0388503745198) A[1]:(0.999997973442) A[2]:(-0.476489752531) A[3]:(0.870607435703)\n",
      " state (13)  A[0]:(0.00331449043006) A[1]:(0.999999701977) A[2]:(0.901058256626) A[3]:(0.810847818851)\n",
      " state (14)  A[0]:(0.900584995747) A[1]:(1.0) A[2]:(0.999994397163) A[3]:(0.900421917439)\n",
      " state (15)  A[0]:(0.998888492584) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973987221718)\n",
      "Episode 156000 finished after 0 timesteps with r=0.0. Running score: 0.27. Times trained:               27899. Times reached goal: 296.               Steps done: 1912161. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.132984790338.\n",
      " state (0)  A[0]:(0.588473021984) A[1]:(0.655547142029) A[2]:(0.6521037817) A[3]:(0.588300943375)\n",
      " state (1)  A[0]:(0.604804396629) A[1]:(0.00170618132688) A[2]:(0.724854588509) A[3]:(0.63765847683)\n",
      " state (2)  A[0]:(0.638700485229) A[1]:(0.806558668613) A[2]:(0.704207777977) A[3]:(0.744206786156)\n",
      " state (3)  A[0]:(0.768797755241) A[1]:(0.39859649539) A[2]:(0.301977843046) A[3]:(0.642998158932)\n",
      " state (4)  A[0]:(0.649322330952) A[1]:(0.727336347103) A[2]:(-0.0055424598977) A[3]:(0.588610053062)\n",
      " state (5)  A[0]:(0.188510015607) A[1]:(0.924262046814) A[2]:(-0.0317666903138) A[3]:(0.523102164268)\n",
      " state (6)  A[0]:(0.0236577801406) A[1]:(0.900495946407) A[2]:(-0.0162726584822) A[3]:(0.704665184021)\n",
      " state (7)  A[0]:(0.599253058434) A[1]:(0.47536277771) A[2]:(0.126871228218) A[3]:(0.93139821291)\n",
      " state (8)  A[0]:(0.721364855766) A[1]:(-0.00431546848267) A[2]:(0.808478295803) A[3]:(0.657672643661)\n",
      " state (9)  A[0]:(0.738335847855) A[1]:(0.897339046001) A[2]:(0.893126547337) A[3]:(-0.00812539458275)\n",
      " state (10)  A[0]:(0.780734121799) A[1]:(0.999599158764) A[2]:(0.00635484233499) A[3]:(0.810698449612)\n",
      " state (11)  A[0]:(0.521587908268) A[1]:(0.999985814095) A[2]:(-0.672580122948) A[3]:(0.919315099716)\n",
      " state (12)  A[0]:(-0.0359525494277) A[1]:(0.999997377396) A[2]:(-0.479511648417) A[3]:(0.870423853397)\n",
      " state (13)  A[0]:(0.000372812122805) A[1]:(0.999999582767) A[2]:(0.898698687553) A[3]:(0.810811758041)\n",
      " state (14)  A[0]:(0.899225533009) A[1]:(1.0) A[2]:(0.999994158745) A[3]:(0.900046169758)\n",
      " state (15)  A[0]:(0.998884856701) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973856627941)\n",
      "Episode 157000 finished after 0 timesteps with r=1.0. Running score: 0.36. Times trained:               29489. Times reached goal: 334.               Steps done: 1941650. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.129120459513.\n",
      " state (0)  A[0]:(0.589212656021) A[1]:(0.656437397003) A[2]:(0.655532896519) A[3]:(0.59021961689)\n",
      " state (1)  A[0]:(0.590485930443) A[1]:(0.00228851637803) A[2]:(0.728902339935) A[3]:(0.649484932423)\n",
      " state (2)  A[0]:(0.649035930634) A[1]:(0.810089170933) A[2]:(0.720387279987) A[3]:(0.735112011433)\n",
      " state (3)  A[0]:(0.787319779396) A[1]:(0.0629835352302) A[2]:(0.275646746159) A[3]:(0.592076539993)\n",
      " state (4)  A[0]:(0.647694826126) A[1]:(0.728396177292) A[2]:(0.00127088953741) A[3]:(0.590537786484)\n",
      " state (5)  A[0]:(0.175283104181) A[1]:(0.92667222023) A[2]:(-0.0141643350944) A[3]:(0.534627377987)\n",
      " state (6)  A[0]:(0.0115912212059) A[1]:(0.900294184685) A[2]:(-0.00278293364681) A[3]:(0.713447093964)\n",
      " state (7)  A[0]:(0.599676966667) A[1]:(0.468377530575) A[2]:(0.124241508543) A[3]:(0.932362794876)\n",
      " state (8)  A[0]:(0.71771800518) A[1]:(0.000785469834227) A[2]:(0.810255765915) A[3]:(0.656627058983)\n",
      " state (9)  A[0]:(0.735164165497) A[1]:(0.899236440659) A[2]:(0.895025014877) A[3]:(-0.00244995462708)\n",
      " state (10)  A[0]:(0.782388150692) A[1]:(0.999564349651) A[2]:(0.0034791091457) A[3]:(0.81052839756)\n",
      " state (11)  A[0]:(0.523391962051) A[1]:(0.999982655048) A[2]:(-0.67470562458) A[3]:(0.917697191238)\n",
      " state (12)  A[0]:(-0.0382137037814) A[1]:(0.999996542931) A[2]:(-0.480940252542) A[3]:(0.868363857269)\n",
      " state (13)  A[0]:(-0.00278966152109) A[1]:(0.999999463558) A[2]:(0.900464296341) A[3]:(0.809771239758)\n",
      " state (14)  A[0]:(0.899401903152) A[1]:(1.0) A[2]:(0.999994516373) A[3]:(0.899787545204)\n",
      " state (15)  A[0]:(0.998872756958) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973395109177)\n",
      "Episode 158000 finished after 0 timesteps with r=0.0. Running score: 0.45. Times trained:               31264. Times reached goal: 345.               Steps done: 1972914. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.125146088554.\n",
      " state (0)  A[0]:(0.588144302368) A[1]:(0.65716356039) A[2]:(0.654602885246) A[3]:(0.589760541916)\n",
      " state (1)  A[0]:(0.587959527969) A[1]:(0.00121682824101) A[2]:(0.728080868721) A[3]:(0.651580810547)\n",
      " state (2)  A[0]:(0.644551217556) A[1]:(0.81050992012) A[2]:(0.712608575821) A[3]:(0.736018419266)\n",
      " state (3)  A[0]:(0.776407182217) A[1]:(0.0431549586356) A[2]:(0.278316557407) A[3]:(0.583086252213)\n",
      " state (4)  A[0]:(0.642977416515) A[1]:(0.729151666164) A[2]:(0.000351309776306) A[3]:(0.592580080032)\n",
      " state (5)  A[0]:(0.182153001428) A[1]:(0.926941514015) A[2]:(-0.0152409197763) A[3]:(0.547973334789)\n",
      " state (6)  A[0]:(0.0125339766964) A[1]:(0.899559557438) A[2]:(-0.00390241551213) A[3]:(0.719390511513)\n",
      " state (7)  A[0]:(0.601250052452) A[1]:(0.45952218771) A[2]:(0.114664308727) A[3]:(0.932733356953)\n",
      " state (8)  A[0]:(0.720063507557) A[1]:(0.00152933481149) A[2]:(0.81005859375) A[3]:(0.657687783241)\n",
      " state (9)  A[0]:(0.737726867199) A[1]:(0.899518668652) A[2]:(0.895411431789) A[3]:(-0.000313729047775)\n",
      " state (10)  A[0]:(0.788918673992) A[1]:(0.999473929405) A[2]:(0.00225805863738) A[3]:(0.809127688408)\n",
      " state (11)  A[0]:(0.534731149673) A[1]:(0.999974548817) A[2]:(-0.670548915863) A[3]:(0.915924906731)\n",
      " state (12)  A[0]:(-0.0278664939106) A[1]:(0.999994337559) A[2]:(-0.474011540413) A[3]:(0.86693495512)\n",
      " state (13)  A[0]:(0.000770583574194) A[1]:(0.999999046326) A[2]:(0.899898409843) A[3]:(0.809790492058)\n",
      " state (14)  A[0]:(0.899659216404) A[1]:(0.999999940395) A[2]:(0.999994337559) A[3]:(0.899665176868)\n",
      " state (15)  A[0]:(0.998905897141) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973302066326)\n",
      "Episode 159000 finished after 0 timesteps with r=0.0. Running score: 0.37. Times trained:               33005. Times reached goal: 341.               Steps done: 2005919. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.121083060841.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_values \n",
      "tensor([[ 0.5910,  0.6565,  0.6564,  0.5903]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6462,  0.7283,  0.0007,  0.5920]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7190,  0.0009,  0.8103,  0.6554]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7386,  0.8993,  0.8937,  0.0008]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  1.0000,  0.8998,  0.8096]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7391,  0.8994,  0.8937,  0.0016]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0005,  1.0000,  0.9001,  0.8095]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0007,  1.0000,  0.9001,  0.8092]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0007,  1.0000,  0.9002,  0.8092]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  1.0000,  0.9003,  0.8094]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  1.0000,  0.9005,  0.8095]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  1.0000,  0.9006,  0.8096]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  1.0000,  0.9007,  0.8097]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  1.0000,  0.9008,  0.8097]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0002,  1.0000,  0.9008,  0.8095]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0009,  1.0000,  0.9007,  0.8093]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0008,  1.0000,  0.9008,  0.8094]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0005,  1.0000,  0.9007,  0.8095]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  1.0000,  0.9008,  0.8098]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  1.0000,  0.9007,  0.8099]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  1.0000,  0.9005,  0.8100]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  1.0000,  0.9003,  0.8100]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0007,  1.0000,  0.9001,  0.8100]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0011,  1.0000,  0.9000,  0.8100]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0012,  1.0000,  0.9000,  0.8100]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0010,  1.0000,  0.9001,  0.8101]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  1.0000,  0.9003,  0.8104]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0019,  1.0000,  0.9004,  0.8106]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0015,  1.0000,  0.9003,  0.8105]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0002,  1.0000,  0.9001,  0.8101]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0017,  1.0000,  0.8999,  0.8097]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0029,  1.0000,  0.8998,  0.8094]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0024,  1.0000,  0.8999,  0.8095]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  1.0000,  0.9004,  0.8101]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0026,  1.0000,  0.9007,  0.8105]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0020,  1.0000,  0.9007,  0.8104]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0010,  1.0000,  0.9005,  0.8097]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0035,  1.0000,  0.9003,  0.8092]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0008,  1.0000,  0.9007,  0.8098]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0018,  1.0000,  0.9010,  0.8104]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.591666936874) A[1]:(0.656925439835) A[2]:(0.656923890114) A[3]:(0.591084480286)\n",
      " state (1)  A[0]:(0.593783855438) A[1]:(0.00253408611752) A[2]:(0.727677762508) A[3]:(0.651639342308)\n",
      " state (2)  A[0]:(0.646330595016) A[1]:(0.809414029121) A[2]:(0.705572187901) A[3]:(0.733735322952)\n",
      " state (3)  A[0]:(0.770913600922) A[1]:(0.0300677362829) A[2]:(0.285286486149) A[3]:(0.578775763512)\n",
      " state (4)  A[0]:(0.646364808083) A[1]:(0.728288829327) A[2]:(0.000915050273761) A[3]:(0.593336343765)\n",
      " state (5)  A[0]:(0.195622622967) A[1]:(0.927330553532) A[2]:(-0.0157227665186) A[3]:(0.55459177494)\n",
      " state (6)  A[0]:(0.00804807711393) A[1]:(0.899181127548) A[2]:(-0.00311695528217) A[3]:(0.720120072365)\n",
      " state (7)  A[0]:(0.59604293108) A[1]:(0.448645561934) A[2]:(0.106015987694) A[3]:(0.933392107487)\n",
      " state (8)  A[0]:(0.718055009842) A[1]:(0.0036709739361) A[2]:(0.809709131718) A[3]:(0.657476484776)\n",
      " state (9)  A[0]:(0.737011730671) A[1]:(0.899573326111) A[2]:(0.893597960472) A[3]:(0.00206884439103)\n",
      " state (10)  A[0]:(0.78989315033) A[1]:(0.99929690361) A[2]:(0.000968098349404) A[3]:(0.807764172554)\n",
      " state (11)  A[0]:(0.533931016922) A[1]:(0.999954581261) A[2]:(-0.657793164253) A[3]:(0.913266777992)\n",
      " state (12)  A[0]:(-0.0273157786578) A[1]:(0.999987840652) A[2]:(-0.452566683292) A[3]:(0.864894628525)\n",
      " state (13)  A[0]:(0.00313055980951) A[1]:(0.999997675419) A[2]:(0.901155948639) A[3]:(0.810772001743)\n",
      " state (14)  A[0]:(0.901003479958) A[1]:(0.999999880791) A[2]:(0.999993979931) A[3]:(0.900628149509)\n",
      " state (15)  A[0]:(0.998980462551) A[1]:(1.0) A[2]:(1.0) A[3]:(0.973736464977)\n",
      "Episode 160000 finished after 0 timesteps with r=0.0. Running score: 0.27. Times trained:               32677. Times reached goal: 308.               Steps done: 2038596. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.117190376656.\n",
      " state (0)  A[0]:(0.589593589306) A[1]:(0.653597831726) A[2]:(0.652992010117) A[3]:(0.589859724045)\n",
      " state (1)  A[0]:(0.592924118042) A[1]:(0.00507815275341) A[2]:(0.724626898766) A[3]:(0.651742458344)\n",
      " state (2)  A[0]:(0.637039780617) A[1]:(0.806468844414) A[2]:(0.713816583157) A[3]:(0.726228237152)\n",
      " state (3)  A[0]:(0.755245625973) A[1]:(0.0415623821318) A[2]:(0.311567723751) A[3]:(0.566826343536)\n",
      " state (4)  A[0]:(0.643610417843) A[1]:(0.728527963161) A[2]:(0.00302659533918) A[3]:(0.588461875916)\n",
      " state (5)  A[0]:(0.206779047847) A[1]:(0.928436160088) A[2]:(-0.0256283171475) A[3]:(0.563677251339)\n",
      " state (6)  A[0]:(-0.00552193587646) A[1]:(0.895870685577) A[2]:(-0.00694800168276) A[3]:(0.722434043884)\n",
      " state (7)  A[0]:(0.585916876793) A[1]:(0.409294098616) A[2]:(0.104362808168) A[3]:(0.93411308527)\n",
      " state (8)  A[0]:(0.713694274426) A[1]:(0.00470861280337) A[2]:(0.81063657999) A[3]:(0.636985778809)\n",
      " state (9)  A[0]:(0.739824652672) A[1]:(0.897455096245) A[2]:(0.881445467472) A[3]:(-0.00178263895214)\n",
      " state (10)  A[0]:(0.780995130539) A[1]:(0.998437464237) A[2]:(0.00844220165163) A[3]:(0.793477654457)\n",
      " state (11)  A[0]:(0.500800490379) A[1]:(0.99977517128) A[2]:(-0.592907547951) A[3]:(0.897289633751)\n",
      " state (12)  A[0]:(-0.0498253330588) A[1]:(0.999893784523) A[2]:(-0.357704401016) A[3]:(0.849165976048)\n",
      " state (13)  A[0]:(-0.00279402942397) A[1]:(0.999966323376) A[2]:(0.90042835474) A[3]:(0.805760979652)\n",
      " state (14)  A[0]:(0.90005671978) A[1]:(0.999997794628) A[2]:(0.999990820885) A[3]:(0.901236474514)\n",
      " state (15)  A[0]:(0.999185442924) A[1]:(0.999999940395) A[2]:(1.0) A[3]:(0.976050376892)\n",
      "Episode 161000 finished after 0 timesteps with r=0.0. Running score: 0.31. Times trained:               33535. Times reached goal: 312.               Steps done: 2072131. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.11332556283.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.514252662659) A[1]:(0.578725814819) A[2]:(0.510241687298) A[3]:(0.503501057625)\n",
      " state (1)  A[0]:(0.469688415527) A[1]:(0.208825528622) A[2]:(0.556010067463) A[3]:(0.530377745628)\n",
      " state (2)  A[0]:(0.51889872551) A[1]:(0.721932053566) A[2]:(0.663114190102) A[3]:(0.609920024872)\n",
      " state (3)  A[0]:(0.648647546768) A[1]:(0.0419912338257) A[2]:(0.338017225266) A[3]:(0.469272166491)\n",
      " state (4)  A[0]:(0.574193477631) A[1]:(0.643284857273) A[2]:(-0.058006066829) A[3]:(0.531984865665)\n",
      " state (5)  A[0]:(0.172083154321) A[1]:(0.896097183228) A[2]:(-0.178422361612) A[3]:(0.570297598839)\n",
      " state (6)  A[0]:(-0.0885983556509) A[1]:(0.816438794136) A[2]:(-0.167498022318) A[3]:(0.721134662628)\n",
      " state (7)  A[0]:(0.411425828934) A[1]:(0.224450021982) A[2]:(-0.0201491266489) A[3]:(0.912954688072)\n",
      " state (8)  A[0]:(0.58929592371) A[1]:(0.0211503244936) A[2]:(0.710571408272) A[3]:(0.540180623531)\n",
      " state (9)  A[0]:(0.651917338371) A[1]:(0.786286592484) A[2]:(0.756007313728) A[3]:(0.0550448484719)\n",
      " state (10)  A[0]:(0.59305024147) A[1]:(0.939834654331) A[2]:(0.239508315921) A[3]:(0.569675147533)\n",
      " state (11)  A[0]:(0.239728078246) A[1]:(0.921806275845) A[2]:(0.00486119743437) A[3]:(0.670496821404)\n",
      " state (12)  A[0]:(-0.121396198869) A[1]:(0.849773645401) A[2]:(0.262744724751) A[3]:(0.653007745743)\n",
      " state (13)  A[0]:(-0.0128640849143) A[1]:(0.797434091568) A[2]:(0.886388778687) A[3]:(0.698610126972)\n",
      " state (14)  A[0]:(0.825838804245) A[1]:(0.900145769119) A[2]:(0.999772369862) A[3]:(0.869764268398)\n",
      " state (15)  A[0]:(0.998856365681) A[1]:(0.98826366663) A[2]:(1.0) A[3]:(0.980041325092)\n",
      "Episode 162000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               17388. Times reached goal: 637.               Steps done: 2089519. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.111372090648.\n",
      " state (0)  A[0]:(0.524446249008) A[1]:(0.576753258705) A[2]:(0.552319049835) A[3]:(0.517684578896)\n",
      " state (1)  A[0]:(0.502750813961) A[1]:(0.356941103935) A[2]:(0.603132843971) A[3]:(0.547199726105)\n",
      " state (2)  A[0]:(0.541623353958) A[1]:(0.72076767683) A[2]:(0.604905247688) A[3]:(0.6260009408)\n",
      " state (3)  A[0]:(0.647378325462) A[1]:(0.0824685096741) A[2]:(0.353793382645) A[3]:(0.491000115871)\n",
      " state (4)  A[0]:(0.579205453396) A[1]:(0.643619775772) A[2]:(0.00734149152413) A[3]:(0.536915659904)\n",
      " state (5)  A[0]:(0.222104594111) A[1]:(0.896046459675) A[2]:(-0.11503392458) A[3]:(0.56808412075)\n",
      " state (6)  A[0]:(1.52066349983e-05) A[1]:(0.825535774231) A[2]:(-0.113896153867) A[3]:(0.71700412035)\n",
      " state (7)  A[0]:(0.486305862665) A[1]:(0.271351188421) A[2]:(0.00887132249773) A[3]:(0.913472056389)\n",
      " state (8)  A[0]:(0.617200493813) A[1]:(0.0514190793037) A[2]:(0.716059923172) A[3]:(0.537934422493)\n",
      " state (9)  A[0]:(0.649985671043) A[1]:(0.793501138687) A[2]:(0.759763538837) A[3]:(0.0236108973622)\n",
      " state (10)  A[0]:(0.603673577309) A[1]:(0.944647073746) A[2]:(0.224030643702) A[3]:(0.574631631374)\n",
      " state (11)  A[0]:(0.264312505722) A[1]:(0.928675472736) A[2]:(-0.00772448908538) A[3]:(0.684621453285)\n",
      " state (12)  A[0]:(-0.100490756333) A[1]:(0.85979694128) A[2]:(0.272726416588) A[3]:(0.666415572166)\n",
      " state (13)  A[0]:(-0.00433524977416) A[1]:(0.805221378803) A[2]:(0.894127130508) A[3]:(0.70486676693)\n",
      " state (14)  A[0]:(0.819903850555) A[1]:(0.900488853455) A[2]:(0.999793410301) A[3]:(0.867382466793)\n",
      " state (15)  A[0]:(0.998696684837) A[1]:(0.987692534924) A[2]:(1.0) A[3]:(0.978184878826)\n",
      "Episode 163000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained:               6248. Times reached goal: 875.               Steps done: 2095767. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.11067840715.\n",
      " state (0)  A[0]:(0.522513151169) A[1]:(0.580775201321) A[2]:(0.554924964905) A[3]:(0.518822193146)\n",
      " state (1)  A[0]:(0.511462807655) A[1]:(0.307289123535) A[2]:(0.604550898075) A[3]:(0.552336215973)\n",
      " state (2)  A[0]:(0.555382072926) A[1]:(0.719501376152) A[2]:(0.595348596573) A[3]:(0.637303352356)\n",
      " state (3)  A[0]:(0.647074699402) A[1]:(0.092189244926) A[2]:(0.35697939992) A[3]:(0.498163580894)\n",
      " state (4)  A[0]:(0.583902955055) A[1]:(0.650262534618) A[2]:(0.0108320051804) A[3]:(0.538509964943)\n",
      " state (5)  A[0]:(0.256242156029) A[1]:(0.898331105709) A[2]:(-0.11808925122) A[3]:(0.567193508148)\n",
      " state (6)  A[0]:(0.0512975677848) A[1]:(0.831871628761) A[2]:(-0.120033495128) A[3]:(0.715410709381)\n",
      " state (7)  A[0]:(0.524465858936) A[1]:(0.283531278372) A[2]:(0.000737428548746) A[3]:(0.917614519596)\n",
      " state (8)  A[0]:(0.634325146675) A[1]:(0.0441604144871) A[2]:(0.72052115202) A[3]:(0.553972601891)\n",
      " state (9)  A[0]:(0.658396065235) A[1]:(0.798738360405) A[2]:(0.763135373592) A[3]:(0.0267370268703)\n",
      " state (10)  A[0]:(0.628633081913) A[1]:(0.949766993523) A[2]:(0.189549177885) A[3]:(0.602979183197)\n",
      " state (11)  A[0]:(0.298713773489) A[1]:(0.934950947762) A[2]:(-0.0555665940046) A[3]:(0.712439894676)\n",
      " state (12)  A[0]:(-0.0833245590329) A[1]:(0.866721630096) A[2]:(0.245336711407) A[3]:(0.689614236355)\n",
      " state (13)  A[0]:(-0.00317242043093) A[1]:(0.808215141296) A[2]:(0.897951841354) A[3]:(0.718207120895)\n",
      " state (14)  A[0]:(0.818662106991) A[1]:(0.900283396244) A[2]:(0.999831736088) A[3]:(0.869878768921)\n",
      " state (15)  A[0]:(0.998622357845) A[1]:(0.987281680107) A[2]:(1.0) A[3]:(0.977352499962)\n",
      "Episode 164000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained:               6239. Times reached goal: 888.               Steps done: 2102006. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.10999003418.\n",
      "q_values \n",
      "tensor([[ 0.5282,  0.5840,  0.5601,  0.5244]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5826,  0.6505,  0.0183,  0.5364]], device='cuda:0')\n",
      "On state=4, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5280,  0.5836,  0.5603,  0.5244]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5837,  0.6518,  0.0222,  0.5366]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6350,  0.0459,  0.7226,  0.5679]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6576,  0.8056,  0.7707,  0.0197]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0007,  0.8117,  0.9010,  0.7175]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8204,  0.9008,  0.9999,  0.8661]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.52795535326) A[1]:(0.583223879337) A[2]:(0.561293900013) A[3]:(0.523976504803)\n",
      " state (1)  A[0]:(0.518305182457) A[1]:(0.226115703583) A[2]:(0.616297721863) A[3]:(0.554196476936)\n",
      " state (2)  A[0]:(0.564396202564) A[1]:(0.721277117729) A[2]:(0.605955123901) A[3]:(0.643694698811)\n",
      " state (3)  A[0]:(0.6469643116) A[1]:(0.0701009556651) A[2]:(0.376048266888) A[3]:(0.497677356005)\n",
      " state (4)  A[0]:(0.583308577538) A[1]:(0.649161934853) A[2]:(0.0253266766667) A[3]:(0.536267459393)\n",
      " state (5)  A[0]:(0.268065035343) A[1]:(0.897272109985) A[2]:(-0.114169590175) A[3]:(0.559022367001)\n",
      " state (6)  A[0]:(0.0570750311017) A[1]:(0.834130167961) A[2]:(-0.121661603451) A[3]:(0.700071036816)\n",
      " state (7)  A[0]:(0.519882977009) A[1]:(0.29595464468) A[2]:(-0.00617830036208) A[3]:(0.915942311287)\n",
      " state (8)  A[0]:(0.633096575737) A[1]:(0.0393656231463) A[2]:(0.722242355347) A[3]:(0.567893981934)\n",
      " state (9)  A[0]:(0.655294537544) A[1]:(0.802786290646) A[2]:(0.770180761814) A[3]:(0.0197094511241)\n",
      " state (10)  A[0]:(0.642431378365) A[1]:(0.953693389893) A[2]:(0.165410667658) A[3]:(0.616282224655)\n",
      " state (11)  A[0]:(0.321195393801) A[1]:(0.939649045467) A[2]:(-0.100481264293) A[3]:(0.725984156132)\n",
      " state (12)  A[0]:(-0.074418336153) A[1]:(0.871323406696) A[2]:(0.212185591459) A[3]:(0.697154581547)\n",
      " state (13)  A[0]:(-0.00569865293801) A[1]:(0.809001505375) A[2]:(0.900073945522) A[3]:(0.717228293419)\n",
      " state (14)  A[0]:(0.81914627552) A[1]:(0.899899721146) A[2]:(0.999863028526) A[3]:(0.865864217281)\n",
      " state (15)  A[0]:(0.998587608337) A[1]:(0.987007558346) A[2]:(1.0) A[3]:(0.975475728512)\n",
      "Episode 165000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6282. Times reached goal: 881.               Steps done: 2108288. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.109301242545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531166136265) A[1]:(0.587851285934) A[2]:(0.573095083237) A[3]:(0.532073736191)\n",
      " state (1)  A[0]:(0.534045100212) A[1]:(0.163152098656) A[2]:(0.630297780037) A[3]:(0.570225834846)\n",
      " state (2)  A[0]:(0.57970225811) A[1]:(0.73512673378) A[2]:(0.607005119324) A[3]:(0.663619816303)\n",
      " state (3)  A[0]:(0.65528357029) A[1]:(0.0653480812907) A[2]:(0.379405468702) A[3]:(0.511705517769)\n",
      " state (4)  A[0]:(0.587686657906) A[1]:(0.650345087051) A[2]:(0.0183564070612) A[3]:(0.544554412365)\n",
      " state (5)  A[0]:(0.276004135609) A[1]:(0.896193861961) A[2]:(-0.133935511112) A[3]:(0.559642374516)\n",
      " state (6)  A[0]:(0.0499554350972) A[1]:(0.83673453331) A[2]:(-0.146664842963) A[3]:(0.686500906944)\n",
      " state (7)  A[0]:(0.504446148872) A[1]:(0.313094764948) A[2]:(-0.0282331760973) A[3]:(0.911815166473)\n",
      " state (8)  A[0]:(0.63017976284) A[1]:(0.0342348180711) A[2]:(0.725304245949) A[3]:(0.570865273476)\n",
      " state (9)  A[0]:(0.653475046158) A[1]:(0.80478811264) A[2]:(0.779683947563) A[3]:(0.0162030234933)\n",
      " state (10)  A[0]:(0.656496226788) A[1]:(0.956645727158) A[2]:(0.145841106772) A[3]:(0.636850953102)\n",
      " state (11)  A[0]:(0.344784766436) A[1]:(0.943395972252) A[2]:(-0.143403604627) A[3]:(0.745734214783)\n",
      " state (12)  A[0]:(-0.0622360631824) A[1]:(0.875306129456) A[2]:(0.176449328661) A[3]:(0.712489783764)\n",
      " state (13)  A[0]:(-0.00326916342601) A[1]:(0.810328960419) A[2]:(0.901171684265) A[3]:(0.724447011948)\n",
      " state (14)  A[0]:(0.821819484234) A[1]:(0.900509655476) A[2]:(0.999887108803) A[3]:(0.866668224335)\n",
      " state (15)  A[0]:(0.998574733734) A[1]:(0.986953914165) A[2]:(1.0) A[3]:(0.974659204483)\n",
      "Episode 166000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6307. Times reached goal: 909.               Steps done: 2114595. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.108614048951.\n",
      " state (0)  A[0]:(0.529976844788) A[1]:(0.587369024754) A[2]:(0.582895994186) A[3]:(0.528859257698)\n",
      " state (1)  A[0]:(0.551049232483) A[1]:(0.095782905817) A[2]:(0.643490791321) A[3]:(0.58013343811)\n",
      " state (2)  A[0]:(0.592303395271) A[1]:(0.74484205246) A[2]:(0.607354640961) A[3]:(0.674674630165)\n",
      " state (3)  A[0]:(0.662947297096) A[1]:(0.0456230193377) A[2]:(0.38845717907) A[3]:(0.510070621967)\n",
      " state (4)  A[0]:(0.589580655098) A[1]:(0.65717792511) A[2]:(0.0192281156778) A[3]:(0.539608240128)\n",
      " state (5)  A[0]:(0.28040522337) A[1]:(0.898016810417) A[2]:(-0.14618422091) A[3]:(0.550935864449)\n",
      " state (6)  A[0]:(0.0630127415061) A[1]:(0.839740753174) A[2]:(-0.167295023799) A[3]:(0.679034292698)\n",
      " state (7)  A[0]:(0.527705311775) A[1]:(0.314398884773) A[2]:(-0.0532572157681) A[3]:(0.914050340652)\n",
      " state (8)  A[0]:(0.646316289902) A[1]:(0.0246943682432) A[2]:(0.724099159241) A[3]:(0.586362600327)\n",
      " state (9)  A[0]:(0.65953296423) A[1]:(0.805614650249) A[2]:(0.788373351097) A[3]:(0.0120242014527)\n",
      " state (10)  A[0]:(0.673551082611) A[1]:(0.958873331547) A[2]:(0.131214380264) A[3]:(0.651009976864)\n",
      " state (11)  A[0]:(0.371942162514) A[1]:(0.946359992027) A[2]:(-0.181000635028) A[3]:(0.759462177753)\n",
      " state (12)  A[0]:(-0.0462571904063) A[1]:(0.878510713577) A[2]:(0.140869379044) A[3]:(0.721480488777)\n",
      " state (13)  A[0]:(-0.00136587687302) A[1]:(0.810837030411) A[2]:(0.900224208832) A[3]:(0.725495100021)\n",
      " state (14)  A[0]:(0.820455789566) A[1]:(0.900076806545) A[2]:(0.999900400639) A[3]:(0.863992929459)\n",
      " state (15)  A[0]:(0.998490393162) A[1]:(0.986681878567) A[2]:(1.0) A[3]:(0.973197162151)\n",
      "Episode 167000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6185. Times reached goal: 885.               Steps done: 2120780. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.107944344255.\n",
      " state (0)  A[0]:(0.53739130497) A[1]:(0.586379170418) A[2]:(0.601231098175) A[3]:(0.538735806942)\n",
      " state (1)  A[0]:(0.576939702034) A[1]:(0.0354274734855) A[2]:(0.668604373932) A[3]:(0.599283099174)\n",
      " state (2)  A[0]:(0.606958270073) A[1]:(0.758481502533) A[2]:(0.618353128433) A[3]:(0.692191004753)\n",
      " state (3)  A[0]:(0.672888159752) A[1]:(0.0234894603491) A[2]:(0.406175494194) A[3]:(0.524161636829)\n",
      " state (4)  A[0]:(0.58942091465) A[1]:(0.654352843761) A[2]:(0.0311692096293) A[3]:(0.54839348793)\n",
      " state (5)  A[0]:(0.268815517426) A[1]:(0.898588657379) A[2]:(-0.143162846565) A[3]:(0.550402402878)\n",
      " state (6)  A[0]:(0.0257403887808) A[1]:(0.847023189068) A[2]:(-0.167544722557) A[3]:(0.65916121006)\n",
      " state (7)  A[0]:(0.491695284843) A[1]:(0.343927741051) A[2]:(-0.059112932533) A[3]:(0.905475795269)\n",
      " state (8)  A[0]:(0.640372753143) A[1]:(0.0228618029505) A[2]:(0.721900463104) A[3]:(0.58695089817)\n",
      " state (9)  A[0]:(0.657200098038) A[1]:(0.801402807236) A[2]:(0.790527641773) A[3]:(0.00990750081837)\n",
      " state (10)  A[0]:(0.682530522346) A[1]:(0.95896756649) A[2]:(0.10226341337) A[3]:(0.664670109749)\n",
      " state (11)  A[0]:(0.386554509401) A[1]:(0.946677207947) A[2]:(-0.227260440588) A[3]:(0.770948648453)\n",
      " state (12)  A[0]:(-0.0415948741138) A[1]:(0.877461493015) A[2]:(0.100896157324) A[3]:(0.727417469025)\n",
      " state (13)  A[0]:(-0.00432959757745) A[1]:(0.807645916939) A[2]:(0.900594949722) A[3]:(0.723803281784)\n",
      " state (14)  A[0]:(0.820552647114) A[1]:(0.899199426174) A[2]:(0.999916613102) A[3]:(0.860659956932)\n",
      " state (15)  A[0]:(0.998421609402) A[1]:(0.986444234848) A[2]:(1.0) A[3]:(0.971713125706)\n",
      "Episode 168000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6315. Times reached goal: 891.               Steps done: 2127095. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.107264823566.\n",
      " state (0)  A[0]:(0.554455280304) A[1]:(0.589523255825) A[2]:(0.620152235031) A[3]:(0.557959973812)\n",
      " state (1)  A[0]:(0.592993497849) A[1]:(0.0103591699153) A[2]:(0.688158392906) A[3]:(0.6165817976)\n",
      " state (2)  A[0]:(0.612154722214) A[1]:(0.768338382244) A[2]:(0.62475425005) A[3]:(0.70801794529)\n",
      " state (3)  A[0]:(0.684637784958) A[1]:(-0.00277666025795) A[2]:(0.41385564208) A[3]:(0.545170307159)\n",
      " state (4)  A[0]:(0.593515872955) A[1]:(0.656672477722) A[2]:(0.0436368472874) A[3]:(0.568723082542)\n",
      " state (5)  A[0]:(0.262354850769) A[1]:(0.901892662048) A[2]:(-0.120773740113) A[3]:(0.565093100071)\n",
      " state (6)  A[0]:(0.0136697618291) A[1]:(0.855609595776) A[2]:(-0.137783586979) A[3]:(0.663360476494)\n",
      " state (7)  A[0]:(0.494872480631) A[1]:(0.362186342478) A[2]:(-0.0376594699919) A[3]:(0.906160891056)\n",
      " state (8)  A[0]:(0.645764827728) A[1]:(0.0321400426328) A[2]:(0.72755074501) A[3]:(0.585043072701)\n",
      " state (9)  A[0]:(0.658295154572) A[1]:(0.804135620594) A[2]:(0.788000643253) A[3]:(0.0112011982128)\n",
      " state (10)  A[0]:(0.69027197361) A[1]:(0.959284067154) A[2]:(0.0725192055106) A[3]:(0.679839134216)\n",
      " state (11)  A[0]:(0.399579644203) A[1]:(0.947294235229) A[2]:(-0.25768622756) A[3]:(0.78284817934)\n",
      " state (12)  A[0]:(-0.0332012549043) A[1]:(0.879226624966) A[2]:(0.0804172977805) A[3]:(0.736291646957)\n",
      " state (13)  A[0]:(-0.00257511902601) A[1]:(0.810633718967) A[2]:(0.901840567589) A[3]:(0.727128922939)\n",
      " state (14)  A[0]:(0.818887472153) A[1]:(0.900907814503) A[2]:(0.999924957752) A[3]:(0.860124349594)\n",
      " state (15)  A[0]:(0.998317837715) A[1]:(0.986421346664) A[2]:(1.0) A[3]:(0.970924019814)\n",
      "Episode 169000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6300. Times reached goal: 887.               Steps done: 2133395. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.106591179385.\n",
      "q_values \n",
      "tensor([[ 0.5571,  0.5886,  0.6210,  0.5586]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5935,  0.0024,  0.6932,  0.6171]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6065,  0.7700,  0.6264,  0.7091]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0047,  0.8561, -0.1042,  0.6661]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6905,  0.9569,  0.0434,  0.6890]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8169,  0.9018,  0.9999,  0.8576]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.557109355927) A[1]:(0.588974773884) A[2]:(0.620749115944) A[3]:(0.559262871742)\n",
      " state (1)  A[0]:(0.593758821487) A[1]:(0.00342624029145) A[2]:(0.693035244942) A[3]:(0.617857456207)\n",
      " state (2)  A[0]:(0.606947302818) A[1]:(0.77057415247) A[2]:(0.62618970871) A[3]:(0.709766089916)\n",
      " state (3)  A[0]:(0.689450442791) A[1]:(-0.0096973720938) A[2]:(0.412689626217) A[3]:(0.551882088184)\n",
      " state (4)  A[0]:(0.593071162701) A[1]:(0.653323352337) A[2]:(0.0529074892402) A[3]:(0.571486651897)\n",
      " state (5)  A[0]:(0.249874547124) A[1]:(0.90298885107) A[2]:(-0.0945614501834) A[3]:(0.564411520958)\n",
      " state (6)  A[0]:(0.00826126523316) A[1]:(0.857207417488) A[2]:(-0.104456782341) A[3]:(0.667755723)\n",
      " state (7)  A[0]:(0.514832913876) A[1]:(0.346620410681) A[2]:(-0.0248190443963) A[3]:(0.911772251129)\n",
      " state (8)  A[0]:(0.649826109409) A[1]:(0.0335049331188) A[2]:(0.723051071167) A[3]:(0.589706301689)\n",
      " state (9)  A[0]:(0.657348394394) A[1]:(0.804641127586) A[2]:(0.778108537197) A[3]:(0.0187711101025)\n",
      " state (10)  A[0]:(0.693340182304) A[1]:(0.957342982292) A[2]:(0.0430294238031) A[3]:(0.691225409508)\n",
      " state (11)  A[0]:(0.404932081699) A[1]:(0.944711446762) A[2]:(-0.27933165431) A[3]:(0.789995074272)\n",
      " state (12)  A[0]:(-0.0285886134952) A[1]:(0.876202583313) A[2]:(0.0654530450702) A[3]:(0.740729212761)\n",
      " state (13)  A[0]:(5.7574827224e-05) A[1]:(0.810316801071) A[2]:(0.901094317436) A[3]:(0.727295041084)\n",
      " state (14)  A[0]:(0.817696809769) A[1]:(0.902240514755) A[2]:(0.999926626682) A[3]:(0.857966423035)\n",
      " state (15)  A[0]:(0.998243272305) A[1]:(0.986601352692) A[2]:(1.0) A[3]:(0.969880759716)\n",
      "Episode 170000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6307. Times reached goal: 884.               Steps done: 2139702. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.105921024372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.557837724686) A[1]:(0.586324453354) A[2]:(0.625078558922) A[3]:(0.562465906143)\n",
      " state (1)  A[0]:(0.590130865574) A[1]:(0.00210025603883) A[2]:(0.694152653217) A[3]:(0.619846880436)\n",
      " state (2)  A[0]:(0.601775527) A[1]:(0.769804418087) A[2]:(0.624762952328) A[3]:(0.710728168488)\n",
      " state (3)  A[0]:(0.688817024231) A[1]:(-0.00886670313776) A[2]:(0.398064523935) A[3]:(0.557760596275)\n",
      " state (4)  A[0]:(0.587075769901) A[1]:(0.650911033154) A[2]:(0.0468801893294) A[3]:(0.571342647076)\n",
      " state (5)  A[0]:(0.229856669903) A[1]:(0.903161525726) A[2]:(-0.0775360912085) A[3]:(0.560008347034)\n",
      " state (6)  A[0]:(0.000611849536654) A[1]:(0.854532778263) A[2]:(-0.0774802789092) A[3]:(0.673850238323)\n",
      " state (7)  A[0]:(0.536066651344) A[1]:(0.310354948044) A[2]:(-0.0152807263657) A[3]:(0.917672097683)\n",
      " state (8)  A[0]:(0.64565038681) A[1]:(0.0271411910653) A[2]:(0.724777817726) A[3]:(0.576054632664)\n",
      " state (9)  A[0]:(0.651319026947) A[1]:(0.800848305225) A[2]:(0.773228704929) A[3]:(0.0110813090578)\n",
      " state (10)  A[0]:(0.691127419472) A[1]:(0.953159153461) A[2]:(0.032468225807) A[3]:(0.697136819363)\n",
      " state (11)  A[0]:(0.402208775282) A[1]:(0.939522504807) A[2]:(-0.280517458916) A[3]:(0.793416202068)\n",
      " state (12)  A[0]:(-0.0300233177841) A[1]:(0.870188713074) A[2]:(0.0675604790449) A[3]:(0.741753816605)\n",
      " state (13)  A[0]:(-0.00193399644922) A[1]:(0.808380723) A[2]:(0.9002571702) A[3]:(0.724398970604)\n",
      " state (14)  A[0]:(0.813613533974) A[1]:(0.902692615986) A[2]:(0.999923050404) A[3]:(0.853466033936)\n",
      " state (15)  A[0]:(0.998149037361) A[1]:(0.986625134945) A[2]:(1.0) A[3]:(0.968234360218)\n",
      "Episode 171000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained:               6291. Times reached goal: 887.               Steps done: 2145993. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.10525676682.\n",
      " state (0)  A[0]:(0.559331893921) A[1]:(0.585472822189) A[2]:(0.620875835419) A[3]:(0.562084674835)\n",
      " state (1)  A[0]:(0.588856816292) A[1]:(-0.000757411005907) A[2]:(0.690527260303) A[3]:(0.61921030283)\n",
      " state (2)  A[0]:(0.601481080055) A[1]:(0.76734572649) A[2]:(0.62297296524) A[3]:(0.707970619202)\n",
      " state (3)  A[0]:(0.690895915031) A[1]:(-0.00291544874199) A[2]:(0.389726251364) A[3]:(0.562751591206)\n",
      " state (4)  A[0]:(0.586839199066) A[1]:(0.648663163185) A[2]:(0.0454426147044) A[3]:(0.570370018482)\n",
      " state (5)  A[0]:(0.216454684734) A[1]:(0.904081404209) A[2]:(-0.0628387406468) A[3]:(0.555240631104)\n",
      " state (6)  A[0]:(-0.00127648399211) A[1]:(0.851915597916) A[2]:(-0.0582999922335) A[3]:(0.683632731438)\n",
      " state (7)  A[0]:(0.559742212296) A[1]:(0.272059768438) A[2]:(-0.0137971229851) A[3]:(0.925650477409)\n",
      " state (8)  A[0]:(0.641063094139) A[1]:(0.0343040712178) A[2]:(0.725537896156) A[3]:(0.570709109306)\n",
      " state (9)  A[0]:(0.646461009979) A[1]:(0.800692796707) A[2]:(0.77008920908) A[3]:(0.0107243759558)\n",
      " state (10)  A[0]:(0.687766909599) A[1]:(0.948505818844) A[2]:(0.0314287133515) A[3]:(0.703519225121)\n",
      " state (11)  A[0]:(0.396610885859) A[1]:(0.933632910252) A[2]:(-0.272600293159) A[3]:(0.797063350677)\n",
      " state (12)  A[0]:(-0.0323864109814) A[1]:(0.86471247673) A[2]:(0.0797057971358) A[3]:(0.744497776031)\n",
      " state (13)  A[0]:(0.00104410259519) A[1]:(0.810182392597) A[2]:(0.902430713177) A[3]:(0.725331902504)\n",
      " state (14)  A[0]:(0.814529657364) A[1]:(0.906591832638) A[2]:(0.999924123287) A[3]:(0.852291703224)\n",
      " state (15)  A[0]:(0.998132050037) A[1]:(0.987251281738) A[2]:(1.0) A[3]:(0.967424571514)\n",
      "Episode 172000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6350. Times reached goal: 890.               Steps done: 2152343. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.104590503974.\n",
      " state (0)  A[0]:(0.551894545555) A[1]:(0.584596097469) A[2]:(0.616282820702) A[3]:(0.552606761456)\n",
      " state (1)  A[0]:(0.576892197132) A[1]:(0.000585049332585) A[2]:(0.686276376247) A[3]:(0.610929489136)\n",
      " state (2)  A[0]:(0.594962418079) A[1]:(0.762619495392) A[2]:(0.62100404501) A[3]:(0.697865903378)\n",
      " state (3)  A[0]:(0.687716424465) A[1]:(-0.0039129354991) A[2]:(0.383123397827) A[3]:(0.558049559593)\n",
      " state (4)  A[0]:(0.584272384644) A[1]:(0.646918654442) A[2]:(0.0438625514507) A[3]:(0.560594916344)\n",
      " state (5)  A[0]:(0.204882368445) A[1]:(0.904613316059) A[2]:(-0.0510042198002) A[3]:(0.540814638138)\n",
      " state (6)  A[0]:(0.00170380703639) A[1]:(0.847168147564) A[2]:(-0.0442325733602) A[3]:(0.686900317669)\n",
      " state (7)  A[0]:(0.586856722832) A[1]:(0.223469614983) A[2]:(-0.0186602864414) A[3]:(0.932439506054)\n",
      " state (8)  A[0]:(0.644494235516) A[1]:(0.0378143712878) A[2]:(0.720249295235) A[3]:(0.577694058418)\n",
      " state (9)  A[0]:(0.648726940155) A[1]:(0.799841105938) A[2]:(0.763254463673) A[3]:(0.0274830199778)\n",
      " state (10)  A[0]:(0.6880813241) A[1]:(0.942755877972) A[2]:(0.0249959602952) A[3]:(0.714226603508)\n",
      " state (11)  A[0]:(0.394648253918) A[1]:(0.925976514816) A[2]:(-0.271237909794) A[3]:(0.802778124809)\n",
      " state (12)  A[0]:(-0.0334601514041) A[1]:(0.856396198273) A[2]:(0.08083833009) A[3]:(0.749104857445)\n",
      " state (13)  A[0]:(-9.47912922129e-05) A[1]:(0.808354258537) A[2]:(0.901201605797) A[3]:(0.727090358734)\n",
      " state (14)  A[0]:(0.811443984509) A[1]:(0.90848749876) A[2]:(0.999920606613) A[3]:(0.850396931171)\n",
      " state (15)  A[0]:(0.998052179813) A[1]:(0.987651288509) A[2]:(1.0) A[3]:(0.966129243374)\n",
      "Episode 173000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6239. Times reached goal: 880.               Steps done: 2158582. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.103939995192.\n",
      " state (0)  A[0]:(0.552382230759) A[1]:(0.581822454929) A[2]:(0.612622559071) A[3]:(0.55492246151)\n",
      " state (1)  A[0]:(0.571245551109) A[1]:(0.00109724653885) A[2]:(0.682249486446) A[3]:(0.611862421036)\n",
      " state (2)  A[0]:(0.595668196678) A[1]:(0.758524775505) A[2]:(0.619130253792) A[3]:(0.694109082222)\n",
      " state (3)  A[0]:(0.683626890182) A[1]:(-0.00296289357357) A[2]:(0.383374065161) A[3]:(0.5621342659)\n",
      " state (4)  A[0]:(0.57926094532) A[1]:(0.649607717991) A[2]:(0.0482879541814) A[3]:(0.558723688126)\n",
      " state (5)  A[0]:(0.190459996462) A[1]:(0.905277013779) A[2]:(-0.038564145565) A[3]:(0.530898690224)\n",
      " state (6)  A[0]:(-0.00284291035496) A[1]:(0.841362118721) A[2]:(-0.03252145648) A[3]:(0.68917798996)\n",
      " state (7)  A[0]:(0.603569149971) A[1]:(0.174534097314) A[2]:(-0.0204821787775) A[3]:(0.937056422234)\n",
      " state (8)  A[0]:(0.642835021019) A[1]:(0.0411203801632) A[2]:(0.721113801003) A[3]:(0.572389185429)\n",
      " state (9)  A[0]:(0.650102496147) A[1]:(0.797358334064) A[2]:(0.760433912277) A[3]:(0.0291571989655)\n",
      " state (10)  A[0]:(0.688906788826) A[1]:(0.935156047344) A[2]:(0.0193895827979) A[3]:(0.719335615635)\n",
      " state (11)  A[0]:(0.392187297344) A[1]:(0.915236771107) A[2]:(-0.274126410484) A[3]:(0.804652154446)\n",
      " state (12)  A[0]:(-0.0370832271874) A[1]:(0.843453168869) A[2]:(0.0791723802686) A[3]:(0.74876999855)\n",
      " state (13)  A[0]:(-0.000855818158016) A[1]:(0.803664028645) A[2]:(0.902666091919) A[3]:(0.723846137524)\n",
      " state (14)  A[0]:(0.812456488609) A[1]:(0.911767363548) A[2]:(0.999925494194) A[3]:(0.846326470375)\n",
      " state (15)  A[0]:(0.998032689095) A[1]:(0.988644063473) A[2]:(1.0) A[3]:(0.964195549488)\n",
      "Episode 174000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6328. Times reached goal: 897.               Steps done: 2164910. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.103284339585.\n",
      "q_values \n",
      "tensor([[ 0.5480,  0.5830,  0.6073,  0.5485]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5606,  0.0007,  0.6753,  0.6050]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5946,  0.7508,  0.6140,  0.6839]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0048,  0.8347, -0.0337,  0.6794]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6898,  0.9282,  0.0134,  0.7156]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8158,  0.9187,  0.9999,  0.8374]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.547706961632) A[1]:(0.583672046661) A[2]:(0.607898831367) A[3]:(0.549564361572)\n",
      " state (1)  A[0]:(0.560768127441) A[1]:(0.00305763422512) A[2]:(0.675814151764) A[3]:(0.60579431057)\n",
      " state (2)  A[0]:(0.595133185387) A[1]:(0.752441108227) A[2]:(0.614364326) A[3]:(0.684671103954)\n",
      " state (3)  A[0]:(0.680357396603) A[1]:(-0.00332671659999) A[2]:(0.380204737186) A[3]:(0.559924185276)\n",
      " state (4)  A[0]:(0.581350207329) A[1]:(0.64860033989) A[2]:(0.0445708185434) A[3]:(0.551359713078)\n",
      " state (5)  A[0]:(0.192847549915) A[1]:(0.904706299305) A[2]:(-0.0394631624222) A[3]:(0.515027880669)\n",
      " state (6)  A[0]:(-0.00210385280661) A[1]:(0.835781693459) A[2]:(-0.0329425297678) A[3]:(0.680266916752)\n",
      " state (7)  A[0]:(0.613336920738) A[1]:(0.135137438774) A[2]:(-0.0285148788244) A[3]:(0.938533306122)\n",
      " state (8)  A[0]:(0.642748892307) A[1]:(0.0434060618281) A[2]:(0.718027114868) A[3]:(0.569186389446)\n",
      " state (9)  A[0]:(0.650105118752) A[1]:(0.796657264233) A[2]:(0.760237932205) A[3]:(0.0143721597269)\n",
      " state (10)  A[0]:(0.691192388535) A[1]:(0.928699851036) A[2]:(0.0143568189815) A[3]:(0.716093301773)\n",
      " state (11)  A[0]:(0.39449390769) A[1]:(0.905789792538) A[2]:(-0.284167587757) A[3]:(0.801773369312)\n",
      " state (12)  A[0]:(-0.0360274426639) A[1]:(0.833325922489) A[2]:(0.0670720338821) A[3]:(0.74245429039)\n",
      " state (13)  A[0]:(0.00305682397448) A[1]:(0.804098367691) A[2]:(0.903255939484) A[3]:(0.71343934536)\n",
      " state (14)  A[0]:(0.816719055176) A[1]:(0.919096589088) A[2]:(0.99993211031) A[3]:(0.837729692459)\n",
      " state (15)  A[0]:(0.99806445837) A[1]:(0.990434885025) A[2]:(1.0) A[3]:(0.960897684097)\n",
      "Episode 175000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6324. Times reached goal: 894.               Steps done: 2171234. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.102633230399.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.542908012867) A[1]:(0.580439209938) A[2]:(0.602602005005) A[3]:(0.542959272861)\n",
      " state (1)  A[0]:(0.54978042841) A[1]:(0.00133657373954) A[2]:(0.670893371105) A[3]:(0.600223183632)\n",
      " state (2)  A[0]:(0.595191121101) A[1]:(0.745546102524) A[2]:(0.61394906044) A[3]:(0.677325367928)\n",
      " state (3)  A[0]:(0.672613620758) A[1]:(-0.00424102367833) A[2]:(0.389624536037) A[3]:(0.55708193779)\n",
      " state (4)  A[0]:(0.580160737038) A[1]:(0.64816904068) A[2]:(0.0571306422353) A[3]:(0.545865058899)\n",
      " state (5)  A[0]:(0.20102699101) A[1]:(0.902744174004) A[2]:(-0.0263309404254) A[3]:(0.505402803421)\n",
      " state (6)  A[0]:(0.00141712650657) A[1]:(0.827823877335) A[2]:(-0.0202661417425) A[3]:(0.672483921051)\n",
      " state (7)  A[0]:(0.617096543312) A[1]:(0.0995282158256) A[2]:(-0.0220229774714) A[3]:(0.939261376858)\n",
      " state (8)  A[0]:(0.643939733505) A[1]:(0.0522898510098) A[2]:(0.718690335751) A[3]:(0.575053691864)\n",
      " state (9)  A[0]:(0.652780175209) A[1]:(0.797768652439) A[2]:(0.75842654705) A[3]:(0.0301079880446)\n",
      " state (10)  A[0]:(0.6921454072) A[1]:(0.918338537216) A[2]:(0.0124464761466) A[3]:(0.722109913826)\n",
      " state (11)  A[0]:(0.389533698559) A[1]:(0.884751617908) A[2]:(-0.288692474365) A[3]:(0.802758872509)\n",
      " state (12)  A[0]:(-0.0469592921436) A[1]:(0.796601474285) A[2]:(0.0575949810445) A[3]:(0.74059176445)\n",
      " state (13)  A[0]:(-0.00622374983504) A[1]:(0.769403815269) A[2]:(0.903834402561) A[3]:(0.708901882172)\n",
      " state (14)  A[0]:(0.818130493164) A[1]:(0.910720705986) A[2]:(0.999939262867) A[3]:(0.834093272686)\n",
      " state (15)  A[0]:(0.998102545738) A[1]:(0.990384817123) A[2]:(1.0) A[3]:(0.959341943264)\n",
      "Episode 176000 finished after 0 timesteps with r=0.0. Running score: 0.88. Times trained:               6322. Times reached goal: 895.               Steps done: 2177556. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.101986429807.\n",
      " state (0)  A[0]:(0.539822638035) A[1]:(0.583602190018) A[2]:(0.600462079048) A[3]:(0.54048538208)\n",
      " state (1)  A[0]:(0.542140603065) A[1]:(0.00127836992033) A[2]:(0.664213299751) A[3]:(0.595989346504)\n",
      " state (2)  A[0]:(0.595912098885) A[1]:(0.740037441254) A[2]:(0.603917658329) A[3]:(0.672485589981)\n",
      " state (3)  A[0]:(0.667044997215) A[1]:(-0.00340648158453) A[2]:(0.3736525774) A[3]:(0.554470658302)\n",
      " state (4)  A[0]:(0.58035826683) A[1]:(0.648960530758) A[2]:(0.0409173071384) A[3]:(0.543274283409)\n",
      " state (5)  A[0]:(0.212085738778) A[1]:(0.900361061096) A[2]:(-0.0351277254522) A[3]:(0.503309607506)\n",
      " state (6)  A[0]:(0.00020803604275) A[1]:(0.822314202785) A[2]:(-0.0211064461619) A[3]:(0.665861725807)\n",
      " state (7)  A[0]:(0.6121789217) A[1]:(0.0794044137001) A[2]:(-0.0196892805398) A[3]:(0.938711047173)\n",
      " state (8)  A[0]:(0.64470243454) A[1]:(0.0546129904687) A[2]:(0.721782386303) A[3]:(0.575381457806)\n",
      " state (9)  A[0]:(0.657463908195) A[1]:(0.801813721657) A[2]:(0.762648165226) A[3]:(0.0202026478946)\n",
      " state (10)  A[0]:(0.70568138361) A[1]:(0.914175868034) A[2]:(0.00405583530664) A[3]:(0.72792327404)\n",
      " state (11)  A[0]:(0.410902738571) A[1]:(0.874145269394) A[2]:(-0.309335023165) A[3]:(0.809146881104)\n",
      " state (12)  A[0]:(-0.0330919288099) A[1]:(0.777820944786) A[2]:(0.031048329547) A[3]:(0.745493710041)\n",
      " state (13)  A[0]:(-0.00178973097354) A[1]:(0.755256474018) A[2]:(0.902832150459) A[3]:(0.708213448524)\n",
      " state (14)  A[0]:(0.820277571678) A[1]:(0.911587953568) A[2]:(0.999946594238) A[3]:(0.829627275467)\n",
      " state (15)  A[0]:(0.998089253902) A[1]:(0.991349577904) A[2]:(1.0) A[3]:(0.956407427788)\n",
      "Episode 177000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6277. Times reached goal: 900.               Steps done: 2183833. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.10134826596.\n",
      " state (0)  A[0]:(0.535128831863) A[1]:(0.580882072449) A[2]:(0.596690356731) A[3]:(0.536293923855)\n",
      " state (1)  A[0]:(0.533731281757) A[1]:(-3.95178794861e-05) A[2]:(0.662311792374) A[3]:(0.591307044029)\n",
      " state (2)  A[0]:(0.595644772053) A[1]:(0.736008048058) A[2]:(0.604015946388) A[3]:(0.667268157005)\n",
      " state (3)  A[0]:(0.663848459721) A[1]:(-0.0142480032519) A[2]:(0.354313135147) A[3]:(0.547183215618)\n",
      " state (4)  A[0]:(0.580863952637) A[1]:(0.652910590172) A[2]:(0.0216580126435) A[3]:(0.536432385445)\n",
      " state (5)  A[0]:(0.216623529792) A[1]:(0.899482727051) A[2]:(-0.0386019982398) A[3]:(0.497629195452)\n",
      " state (6)  A[0]:(-0.005020421464) A[1]:(0.817809939384) A[2]:(-0.0132436165586) A[3]:(0.660775959492)\n",
      " state (7)  A[0]:(0.608947455883) A[1]:(0.0531640797853) A[2]:(-0.0110053149983) A[3]:(0.938791155815)\n",
      " state (8)  A[0]:(0.647218048573) A[1]:(0.0462919771671) A[2]:(0.723033964634) A[3]:(0.577220439911)\n",
      " state (9)  A[0]:(0.662289142609) A[1]:(0.801324307919) A[2]:(0.764251053333) A[3]:(0.00769164506346)\n",
      " state (10)  A[0]:(0.713840901852) A[1]:(0.909383058548) A[2]:(0.00183915882371) A[3]:(0.726858854294)\n",
      " state (11)  A[0]:(0.423085570335) A[1]:(0.865555167198) A[2]:(-0.317758262157) A[3]:(0.810334563255)\n",
      " state (12)  A[0]:(-0.0265448242426) A[1]:(0.767844498158) A[2]:(0.016603494063) A[3]:(0.7464620471)\n",
      " state (13)  A[0]:(-0.00653528235853) A[1]:(0.754850149155) A[2]:(0.901519536972) A[3]:(0.705550312996)\n",
      " state (14)  A[0]:(0.814929544926) A[1]:(0.917898654938) A[2]:(0.999949872494) A[3]:(0.823662161827)\n",
      " state (15)  A[0]:(0.997934401035) A[1]:(0.992716372013) A[2]:(1.0) A[3]:(0.952715814114)\n",
      "Episode 178000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6242. Times reached goal: 882.               Steps done: 2190075. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.100717620376.\n",
      " state (0)  A[0]:(0.535738945007) A[1]:(0.588066935539) A[2]:(0.594908237457) A[3]:(0.53483736515)\n",
      " state (1)  A[0]:(0.532723546028) A[1]:(0.00184033601545) A[2]:(0.660198867321) A[3]:(0.591115534306)\n",
      " state (2)  A[0]:(0.597329616547) A[1]:(0.73382383585) A[2]:(0.605060458183) A[3]:(0.664804577827)\n",
      " state (3)  A[0]:(0.664304971695) A[1]:(-0.00640721805394) A[2]:(0.348651796579) A[3]:(0.548008799553)\n",
      " state (4)  A[0]:(0.584015250206) A[1]:(0.652730226517) A[2]:(0.018313864246) A[3]:(0.534398913383)\n",
      " state (5)  A[0]:(0.224194407463) A[1]:(0.898774623871) A[2]:(-0.0352099947631) A[3]:(0.495499134064)\n",
      " state (6)  A[0]:(0.000427254912211) A[1]:(0.813889741898) A[2]:(-0.00684882421046) A[3]:(0.660482168198)\n",
      " state (7)  A[0]:(0.614444971085) A[1]:(0.0247889179736) A[2]:(-0.00871552806348) A[3]:(0.94015109539)\n",
      " state (8)  A[0]:(0.649422705173) A[1]:(0.0393164791167) A[2]:(0.722982406616) A[3]:(0.583546161652)\n",
      " state (9)  A[0]:(0.662167727947) A[1]:(0.802190303802) A[2]:(0.766138076782) A[3]:(0.00668726954609)\n",
      " state (10)  A[0]:(0.716560661793) A[1]:(0.905144870281) A[2]:(0.00231349049136) A[3]:(0.729795098305)\n",
      " state (11)  A[0]:(0.428633660078) A[1]:(0.85617095232) A[2]:(-0.322835385799) A[3]:(0.813803732395)\n",
      " state (12)  A[0]:(-0.0220141801983) A[1]:(0.752786040306) A[2]:(0.00805992353708) A[3]:(0.750829160213)\n",
      " state (13)  A[0]:(-0.00380501407199) A[1]:(0.743927001953) A[2]:(0.902796447277) A[3]:(0.708183407784)\n",
      " state (14)  A[0]:(0.816745996475) A[1]:(0.917709231377) A[2]:(0.999955713749) A[3]:(0.822060108185)\n",
      " state (15)  A[0]:(0.997906804085) A[1]:(0.993055641651) A[2]:(1.0) A[3]:(0.950171470642)\n",
      "Episode 179000 finished after 0 timesteps with r=0.0. Running score: 0.9. Times trained:               6287. Times reached goal: 888.               Steps done: 2196362. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.100086395033.\n",
      "q_values \n",
      "tensor([[ 0.5328,  0.5854,  0.5923,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5288,  0.0021,  0.6582,  0.5893]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5942,  0.7323,  0.6019,  0.6606]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0019,  0.8132, -0.0079,  0.6573]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7196,  0.9040, -0.0017,  0.7326]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8157,  0.9219,  1.0000,  0.8173]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532206952572) A[1]:(0.585095465183) A[2]:(0.59214413166) A[3]:(0.531778931618)\n",
      " state (1)  A[0]:(0.528238296509) A[1]:(0.00163937953766) A[2]:(0.658217072487) A[3]:(0.589533865452)\n",
      " state (2)  A[0]:(0.59381711483) A[1]:(0.732295274734) A[2]:(0.602129101753) A[3]:(0.660808146)\n",
      " state (3)  A[0]:(0.66070830822) A[1]:(0.00899873208255) A[2]:(0.347664445639) A[3]:(0.553065419197)\n",
      " state (4)  A[0]:(0.581860303879) A[1]:(0.650746822357) A[2]:(0.018673395738) A[3]:(0.535669565201)\n",
      " state (5)  A[0]:(0.224591746926) A[1]:(0.898598074913) A[2]:(-0.0350204482675) A[3]:(0.494114130735)\n",
      " state (6)  A[0]:(-0.0021347347647) A[1]:(0.814042210579) A[2]:(-0.00705409282818) A[3]:(0.657608628273)\n",
      " state (7)  A[0]:(0.613740742207) A[1]:(0.0153286820278) A[2]:(-0.0113162929192) A[3]:(0.940336763859)\n",
      " state (8)  A[0]:(0.649479269981) A[1]:(0.0405857861042) A[2]:(0.723053216934) A[3]:(0.586855888367)\n",
      " state (9)  A[0]:(0.661241173744) A[1]:(0.805697858334) A[2]:(0.768958330154) A[3]:(0.00338232354261)\n",
      " state (10)  A[0]:(0.720111370087) A[1]:(0.904597401619) A[2]:(-0.00135028280783) A[3]:(0.733267188072)\n",
      " state (11)  A[0]:(0.436116188765) A[1]:(0.854403376579) A[2]:(-0.334791123867) A[3]:(0.817773878574)\n",
      " state (12)  A[0]:(-0.017140943557) A[1]:(0.751760363579) A[2]:(-0.00860502943397) A[3]:(0.754676997662)\n",
      " state (13)  A[0]:(-0.00426927348599) A[1]:(0.747882723808) A[2]:(0.902591824532) A[3]:(0.70834004879)\n",
      " state (14)  A[0]:(0.815860927105) A[1]:(0.922323048115) A[2]:(0.999960124493) A[3]:(0.817336559296)\n",
      " state (15)  A[0]:(0.997820913792) A[1]:(0.993715643883) A[2]:(1.0) A[3]:(0.946059942245)\n",
      "Episode 180000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained:               6319. Times reached goal: 876.               Steps done: 2202681. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0994559431131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.535606265068) A[1]:(0.589839160442) A[2]:(0.593212664127) A[3]:(0.534750819206)\n",
      " state (1)  A[0]:(0.531445145607) A[1]:(0.000657155993395) A[2]:(0.657997906208) A[3]:(0.593215465546)\n",
      " state (2)  A[0]:(0.596119701862) A[1]:(0.732314586639) A[2]:(0.600013375282) A[3]:(0.660535335541)\n",
      " state (3)  A[0]:(0.6620221138) A[1]:(0.00810943637043) A[2]:(0.352285385132) A[3]:(0.557080984116)\n",
      " state (4)  A[0]:(0.582572937012) A[1]:(0.656087219715) A[2]:(0.0243560764939) A[3]:(0.537357091904)\n",
      " state (5)  A[0]:(0.225174382329) A[1]:(0.89982676506) A[2]:(-0.0310796499252) A[3]:(0.49369764328)\n",
      " state (6)  A[0]:(0.00224170740694) A[1]:(0.813279926777) A[2]:(-0.00546223437414) A[3]:(0.660003662109)\n",
      " state (7)  A[0]:(0.623269081116) A[1]:(-0.00247443700209) A[2]:(-0.012055051513) A[3]:(0.942068517208)\n",
      " state (8)  A[0]:(0.655262947083) A[1]:(0.0376784428954) A[2]:(0.72463285923) A[3]:(0.591212213039)\n",
      " state (9)  A[0]:(0.665007352829) A[1]:(0.807364940643) A[2]:(0.771179258823) A[3]:(0.00213889451697)\n",
      " state (10)  A[0]:(0.724801540375) A[1]:(0.903224229813) A[2]:(-0.00419518863782) A[3]:(0.735015392303)\n",
      " state (11)  A[0]:(0.443154603243) A[1]:(0.850884258747) A[2]:(-0.344052404165) A[3]:(0.820070922375)\n",
      " state (12)  A[0]:(-0.0129870222881) A[1]:(0.746116876602) A[2]:(-0.0213401094079) A[3]:(0.757571399212)\n",
      " state (13)  A[0]:(-0.00420811260119) A[1]:(0.744339227676) A[2]:(0.903227865696) A[3]:(0.709883272648)\n",
      " state (14)  A[0]:(0.81586176157) A[1]:(0.922898888588) A[2]:(0.999964356422) A[3]:(0.815755009651)\n",
      " state (15)  A[0]:(0.997754395008) A[1]:(0.993861436844) A[2]:(1.0) A[3]:(0.943641066551)\n",
      "Episode 181000 finished after 0 timesteps with r=0.0. Running score: 0.92. Times trained:               6343. Times reached goal: 912.               Steps done: 2209024. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0988270905802.\n",
      " state (0)  A[0]:(0.533545017242) A[1]:(0.587572574615) A[2]:(0.591806352139) A[3]:(0.532561421394)\n",
      " state (1)  A[0]:(0.529767513275) A[1]:(0.00174897734541) A[2]:(0.657212913036) A[3]:(0.591409504414)\n",
      " state (2)  A[0]:(0.59337246418) A[1]:(0.730082452297) A[2]:(0.59801530838) A[3]:(0.656521499157)\n",
      " state (3)  A[0]:(0.658661782742) A[1]:(0.00416783057153) A[2]:(0.358614325523) A[3]:(0.557207942009)\n",
      " state (4)  A[0]:(0.578420639038) A[1]:(0.65271282196) A[2]:(0.0325022861362) A[3]:(0.535491585732)\n",
      " state (5)  A[0]:(0.220005199313) A[1]:(0.898965358734) A[2]:(-0.0252364929765) A[3]:(0.489567399025)\n",
      " state (6)  A[0]:(-0.00304470956326) A[1]:(0.811471283436) A[2]:(-0.0011515612714) A[3]:(0.656608343124)\n",
      " state (7)  A[0]:(0.621388733387) A[1]:(-0.0139484899119) A[2]:(-0.00854614470154) A[3]:(0.94192057848)\n",
      " state (8)  A[0]:(0.653721451759) A[1]:(0.0323473587632) A[2]:(0.727334022522) A[3]:(0.592512011528)\n",
      " state (9)  A[0]:(0.662798643112) A[1]:(0.80617916584) A[2]:(0.775460898876) A[3]:(-0.00316890375689)\n",
      " state (10)  A[0]:(0.725485086441) A[1]:(0.900622904301) A[2]:(-0.000711083295755) A[3]:(0.733504295349)\n",
      " state (11)  A[0]:(0.446319907904) A[1]:(0.84506291151) A[2]:(-0.348241746426) A[3]:(0.819959878922)\n",
      " state (12)  A[0]:(-0.0104919346049) A[1]:(0.734503388405) A[2]:(-0.0303201694041) A[3]:(0.757639408112)\n",
      " state (13)  A[0]:(-0.00431999284774) A[1]:(0.730956435204) A[2]:(0.90334635973) A[3]:(0.708990216255)\n",
      " state (14)  A[0]:(0.815560996532) A[1]:(0.918313443661) A[2]:(0.999967098236) A[3]:(0.813654303551)\n",
      " state (15)  A[0]:(0.997694849968) A[1]:(0.993413627148) A[2]:(1.0) A[3]:(0.941736638546)\n",
      "Episode 182000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6323. Times reached goal: 893.               Steps done: 2215347. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0982041782989.\n",
      " state (0)  A[0]:(0.529220581055) A[1]:(0.593466997147) A[2]:(0.591603636742) A[3]:(0.528311312199)\n",
      " state (1)  A[0]:(0.52848982811) A[1]:(0.00200426299125) A[2]:(0.658002912998) A[3]:(0.591403305531)\n",
      " state (2)  A[0]:(0.590988337994) A[1]:(0.729142427444) A[2]:(0.599697649479) A[3]:(0.655278205872)\n",
      " state (3)  A[0]:(0.657873392105) A[1]:(0.000208139419556) A[2]:(0.375816434622) A[3]:(0.557488620281)\n",
      " state (4)  A[0]:(0.577215731144) A[1]:(0.653273880482) A[2]:(0.0505863986909) A[3]:(0.534502983093)\n",
      " state (5)  A[0]:(0.217801779509) A[1]:(0.899430334568) A[2]:(-0.0157616194338) A[3]:(0.487334609032)\n",
      " state (6)  A[0]:(0.000685723382048) A[1]:(0.810350596905) A[2]:(0.000692605855875) A[3]:(0.658006668091)\n",
      " state (7)  A[0]:(0.627995252609) A[1]:(-0.0234520938247) A[2]:(-0.00994244217873) A[3]:(0.942862391472)\n",
      " state (8)  A[0]:(0.6544226408) A[1]:(0.0373890660703) A[2]:(0.730024576187) A[3]:(0.590274333954)\n",
      " state (9)  A[0]:(0.662758708) A[1]:(0.809014499187) A[2]:(0.778936624527) A[3]:(-0.0103044882417)\n",
      " state (10)  A[0]:(0.726417064667) A[1]:(0.900469660759) A[2]:(8.4400177002e-05) A[3]:(0.730894982815)\n",
      " state (11)  A[0]:(0.447126954794) A[1]:(0.842760205269) A[2]:(-0.354444175959) A[3]:(0.818531632423)\n",
      " state (12)  A[0]:(-0.0121138114482) A[1]:(0.728033304214) A[2]:(-0.038586165756) A[3]:(0.755886554718)\n",
      " state (13)  A[0]:(-0.00534153683111) A[1]:(0.722727656364) A[2]:(0.905401527882) A[3]:(0.707136631012)\n",
      " state (14)  A[0]:(0.817764699459) A[1]:(0.915608882904) A[2]:(0.999971330166) A[3]:(0.812883019447)\n",
      " state (15)  A[0]:(0.997689664364) A[1]:(0.993079900742) A[2]:(1.0) A[3]:(0.940951526165)\n",
      "Episode 183000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6273. Times reached goal: 908.               Steps done: 2221620. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0975900716478.\n",
      " state (0)  A[0]:(0.533352971077) A[1]:(0.588192939758) A[2]:(0.593030810356) A[3]:(0.532754778862)\n",
      " state (1)  A[0]:(0.53195643425) A[1]:(0.000428155035479) A[2]:(0.657323718071) A[3]:(0.593321919441)\n",
      " state (2)  A[0]:(0.593196749687) A[1]:(0.728929102421) A[2]:(0.59595054388) A[3]:(0.655623137951)\n",
      " state (3)  A[0]:(0.657190799713) A[1]:(-0.00163195887581) A[2]:(0.383238822222) A[3]:(0.560104012489)\n",
      " state (4)  A[0]:(0.575255274773) A[1]:(0.655678153038) A[2]:(0.0596403554082) A[3]:(0.536654651165)\n",
      " state (5)  A[0]:(0.214870542288) A[1]:(0.899744689465) A[2]:(-0.0113376285881) A[3]:(0.487373292446)\n",
      " state (6)  A[0]:(-0.00265830126591) A[1]:(0.810437679291) A[2]:(0.00136005796958) A[3]:(0.655989170074)\n",
      " state (7)  A[0]:(0.627141594887) A[1]:(-0.025166189298) A[2]:(-0.0110498936847) A[3]:(0.942518353462)\n",
      " state (8)  A[0]:(0.654016017914) A[1]:(0.0332930535078) A[2]:(0.730302095413) A[3]:(0.595539689064)\n",
      " state (9)  A[0]:(0.658980250359) A[1]:(0.80794018507) A[2]:(0.783998966217) A[3]:(-0.0112308608368)\n",
      " state (10)  A[0]:(0.72637963295) A[1]:(0.90004336834) A[2]:(0.00264798966236) A[3]:(0.733117699623)\n",
      " state (11)  A[0]:(0.450103551149) A[1]:(0.842155814171) A[2]:(-0.363164454699) A[3]:(0.822011709213)\n",
      " state (12)  A[0]:(-0.0105587793514) A[1]:(0.726293087006) A[2]:(-0.0548245199025) A[3]:(0.760341107845)\n",
      " state (13)  A[0]:(-0.00878621358424) A[1]:(0.719828128815) A[2]:(0.903527617455) A[3]:(0.710671782494)\n",
      " state (14)  A[0]:(0.814744055271) A[1]:(0.914298355579) A[2]:(0.999972462654) A[3]:(0.813579857349)\n",
      " state (15)  A[0]:(0.997572541237) A[1]:(0.992869138718) A[2]:(1.0) A[3]:(0.940123856068)\n",
      "Episode 184000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6296. Times reached goal: 897.               Steps done: 2227916. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0969775747203.\n",
      "q_values \n",
      "tensor([[ 0.5278,  0.5894,  0.5885,  0.5271]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5805,  0.6547,  0.0605,  0.5410]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6584,  0.0364,  0.7297,  0.5988]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6602,  0.8093,  0.7849, -0.0083]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0029,  0.7402,  0.9039,  0.7114]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8160,  0.9233,  1.0000,  0.8130]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.527777075768) A[1]:(0.589205980301) A[2]:(0.589271128178) A[3]:(0.528177499771)\n",
      " state (1)  A[0]:(0.525700449944) A[1]:(-0.000601679028478) A[2]:(0.655874729156) A[3]:(0.591322183609)\n",
      " state (2)  A[0]:(0.588967740536) A[1]:(0.728744268417) A[2]:(0.593517124653) A[3]:(0.655327916145)\n",
      " state (3)  A[0]:(0.657486081123) A[1]:(-0.000735551002435) A[2]:(0.383346527815) A[3]:(0.563364684582)\n",
      " state (4)  A[0]:(0.578630208969) A[1]:(0.653945326805) A[2]:(0.0572205893695) A[3]:(0.541657924652)\n",
      " state (5)  A[0]:(0.219713270664) A[1]:(0.899533569813) A[2]:(-0.0168640129268) A[3]:(0.493013560772)\n",
      " state (6)  A[0]:(-0.00236447458155) A[1]:(0.811335384846) A[2]:(-0.00465282425284) A[3]:(0.656837940216)\n",
      " state (7)  A[0]:(0.626277804375) A[1]:(-0.0229267533869) A[2]:(-0.0159933045506) A[3]:(0.941902518272)\n",
      " state (8)  A[0]:(0.65597319603) A[1]:(0.0312989316881) A[2]:(0.727259099483) A[3]:(0.600038528442)\n",
      " state (9)  A[0]:(0.657828629017) A[1]:(0.807988643646) A[2]:(0.783743679523) A[3]:(-0.00888812262565)\n",
      " state (10)  A[0]:(0.726901471615) A[1]:(0.901271045208) A[2]:(-0.00272976676933) A[3]:(0.733967900276)\n",
      " state (11)  A[0]:(0.454024523497) A[1]:(0.847132623196) A[2]:(-0.373367935419) A[3]:(0.823283553123)\n",
      " state (12)  A[0]:(-0.00521925557405) A[1]:(0.739945590496) A[2]:(-0.0672799497843) A[3]:(0.761680424213)\n",
      " state (13)  A[0]:(-0.00432666670531) A[1]:(0.739843904972) A[2]:(0.90405601263) A[3]:(0.710995316505)\n",
      " state (14)  A[0]:(0.816021800041) A[1]:(0.923270821571) A[2]:(0.999974966049) A[3]:(0.812880039215)\n",
      " state (15)  A[0]:(0.997508823872) A[1]:(0.993730902672) A[2]:(1.0) A[3]:(0.938955783844)\n",
      "Episode 185000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6194. Times reached goal: 877.               Steps done: 2234110. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0963787520906.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.535318374634) A[1]:(0.589852929115) A[2]:(0.59325492382) A[3]:(0.533318459988)\n",
      " state (1)  A[0]:(0.532434880733) A[1]:(-0.000378385157092) A[2]:(0.656500399113) A[3]:(0.593134880066)\n",
      " state (2)  A[0]:(0.594356060028) A[1]:(0.729999542236) A[2]:(0.593383133411) A[3]:(0.655579328537)\n",
      " state (3)  A[0]:(0.659132122993) A[1]:(-0.0052625797689) A[2]:(0.378064453602) A[3]:(0.560526907444)\n",
      " state (4)  A[0]:(0.578789353371) A[1]:(0.657798588276) A[2]:(0.0497419051826) A[3]:(0.538776218891)\n",
      " state (5)  A[0]:(0.218189433217) A[1]:(0.900479793549) A[2]:(-0.019601456821) A[3]:(0.490715146065)\n",
      " state (6)  A[0]:(-3.96072864532e-05) A[1]:(0.810435473919) A[2]:(-0.0028675715439) A[3]:(0.656720638275)\n",
      " state (7)  A[0]:(0.631394982338) A[1]:(-0.0359579361975) A[2]:(-0.0116913951933) A[3]:(0.941413402557)\n",
      " state (8)  A[0]:(0.657112002373) A[1]:(0.0309890639037) A[2]:(0.72794675827) A[3]:(0.595279335976)\n",
      " state (9)  A[0]:(0.657591104507) A[1]:(0.809796929359) A[2]:(0.781988382339) A[3]:(-0.00809713639319)\n",
      " state (10)  A[0]:(0.725891232491) A[1]:(0.901352703571) A[2]:(-0.00416336022317) A[3]:(0.731785595417)\n",
      " state (11)  A[0]:(0.45359224081) A[1]:(0.846731185913) A[2]:(-0.374548852444) A[3]:(0.821100175381)\n",
      " state (12)  A[0]:(-0.00482016056776) A[1]:(0.738411962986) A[2]:(-0.0716297850013) A[3]:(0.759223341942)\n",
      " state (13)  A[0]:(-0.00627491576597) A[1]:(0.73611164093) A[2]:(0.903452157974) A[3]:(0.70822429657)\n",
      " state (14)  A[0]:(0.813572704792) A[1]:(0.92081451416) A[2]:(0.9999756217) A[3]:(0.810543060303)\n",
      " state (15)  A[0]:(0.997408092022) A[1]:(0.993344783783) A[2]:(1.0) A[3]:(0.937635242939)\n",
      "Episode 186000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6278. Times reached goal: 895.               Steps done: 2240388. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0957755816182.\n",
      " state (0)  A[0]:(0.531412601471) A[1]:(0.590573191643) A[2]:(0.591063022614) A[3]:(0.531393170357)\n",
      " state (1)  A[0]:(0.528805077076) A[1]:(6.15417957306e-06) A[2]:(0.657139778137) A[3]:(0.592981696129)\n",
      " state (2)  A[0]:(0.591318309307) A[1]:(0.730417788029) A[2]:(0.596675395966) A[3]:(0.655923485756)\n",
      " state (3)  A[0]:(0.657906889915) A[1]:(0.00145964219701) A[2]:(0.379919975996) A[3]:(0.561974406242)\n",
      " state (4)  A[0]:(0.579270899296) A[1]:(0.657045960426) A[2]:(0.0497763901949) A[3]:(0.540541172028)\n",
      " state (5)  A[0]:(0.219264551997) A[1]:(0.90118932724) A[2]:(-0.0184237379581) A[3]:(0.493438720703)\n",
      " state (6)  A[0]:(-0.000856604194269) A[1]:(0.812309503555) A[2]:(0.00157856813166) A[3]:(0.656919419765)\n",
      " state (7)  A[0]:(0.630789637566) A[1]:(-0.0361337773502) A[2]:(-0.00549918832257) A[3]:(0.940724372864)\n",
      " state (8)  A[0]:(0.659278035164) A[1]:(0.0319576337934) A[2]:(0.72802066803) A[3]:(0.598890542984)\n",
      " state (9)  A[0]:(0.659259557724) A[1]:(0.811400532722) A[2]:(0.78261166811) A[3]:(-0.00178869254887)\n",
      " state (10)  A[0]:(0.727752745152) A[1]:(0.901962518692) A[2]:(-0.00143110658973) A[3]:(0.733036160469)\n",
      " state (11)  A[0]:(0.45876416564) A[1]:(0.847700715065) A[2]:(-0.374171257019) A[3]:(0.822225749493)\n",
      " state (12)  A[0]:(0.00263930624351) A[1]:(0.739756584167) A[2]:(-0.0730120167136) A[3]:(0.761286318302)\n",
      " state (13)  A[0]:(0.000746805104427) A[1]:(0.736386299133) A[2]:(0.904536604881) A[3]:(0.710869967937)\n",
      " state (14)  A[0]:(0.815979480743) A[1]:(0.920244514942) A[2]:(0.999977231026) A[3]:(0.812158465385)\n",
      " state (15)  A[0]:(0.997397482395) A[1]:(0.993141651154) A[2]:(1.0) A[3]:(0.937636256218)\n",
      "Episode 187000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6255. Times reached goal: 900.               Steps done: 2246643. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0951783750658.\n",
      " state (0)  A[0]:(0.534392237663) A[1]:(0.589192211628) A[2]:(0.591860532761) A[3]:(0.534425139427)\n",
      " state (1)  A[0]:(0.531116783619) A[1]:(-0.000768229190726) A[2]:(0.656754374504) A[3]:(0.593151807785)\n",
      " state (2)  A[0]:(0.593208909035) A[1]:(0.729214370251) A[2]:(0.594954609871) A[3]:(0.655620455742)\n",
      " state (3)  A[0]:(0.659290015697) A[1]:(-0.00462762685493) A[2]:(0.372589707375) A[3]:(0.559828400612)\n",
      " state (4)  A[0]:(0.581397473812) A[1]:(0.655149936676) A[2]:(0.0381088405848) A[3]:(0.538965284824)\n",
      " state (5)  A[0]:(0.221753329039) A[1]:(0.901071369648) A[2]:(-0.027869515121) A[3]:(0.493111491203)\n",
      " state (6)  A[0]:(0.000632282288279) A[1]:(0.811860859394) A[2]:(-0.00350140105002) A[3]:(0.655797719955)\n",
      " state (7)  A[0]:(0.631121218204) A[1]:(-0.0441917218268) A[2]:(-0.00624875025824) A[3]:(0.939594447613)\n",
      " state (8)  A[0]:(0.658675789833) A[1]:(0.0260049328208) A[2]:(0.727959871292) A[3]:(0.594107568264)\n",
      " state (9)  A[0]:(0.65767455101) A[1]:(0.810785412788) A[2]:(0.783500015736) A[3]:(-0.0075437896885)\n",
      " state (10)  A[0]:(0.728182792664) A[1]:(0.901980102062) A[2]:(-0.00146317377221) A[3]:(0.731973409653)\n",
      " state (11)  A[0]:(0.46231842041) A[1]:(0.848776638508) A[2]:(-0.377863377333) A[3]:(0.822900056839)\n",
      " state (12)  A[0]:(0.00700116250664) A[1]:(0.742587566376) A[2]:(-0.0816944018006) A[3]:(0.76306271553)\n",
      " state (13)  A[0]:(0.000878121471033) A[1]:(0.738929033279) A[2]:(0.902751147747) A[3]:(0.712521314621)\n",
      " state (14)  A[0]:(0.813877105713) A[1]:(0.920469999313) A[2]:(0.999977350235) A[3]:(0.811948418617)\n",
      " state (15)  A[0]:(0.997309207916) A[1]:(0.993048131466) A[2]:(1.0) A[3]:(0.936679244041)\n",
      "Episode 188000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6201. Times reached goal: 891.               Steps done: 2252844. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.094590000104.\n",
      " state (0)  A[0]:(0.532589316368) A[1]:(0.588310480118) A[2]:(0.589842677116) A[3]:(0.531199216843)\n",
      " state (1)  A[0]:(0.529722929001) A[1]:(0.00107879890129) A[2]:(0.656925916672) A[3]:(0.592334866524)\n",
      " state (2)  A[0]:(0.591888666153) A[1]:(0.729765534401) A[2]:(0.595475673676) A[3]:(0.655864536762)\n",
      " state (3)  A[0]:(0.658309221268) A[1]:(0.000230029225349) A[2]:(0.372437149286) A[3]:(0.559514403343)\n",
      " state (4)  A[0]:(0.580661654472) A[1]:(0.654243648052) A[2]:(0.034662541002) A[3]:(0.538819491863)\n",
      " state (5)  A[0]:(0.21930103004) A[1]:(0.901248693466) A[2]:(-0.0321564637125) A[3]:(0.494459867477)\n",
      " state (6)  A[0]:(-0.0019858230371) A[1]:(0.810783147812) A[2]:(-0.00612644618377) A[3]:(0.657502055168)\n",
      " state (7)  A[0]:(0.62878036499) A[1]:(-0.0582062415779) A[2]:(-0.00560206267983) A[3]:(0.939283788204)\n",
      " state (8)  A[0]:(0.65165746212) A[1]:(0.0141921797767) A[2]:(0.729702413082) A[3]:(0.589936852455)\n",
      " state (9)  A[0]:(0.649323940277) A[1]:(0.808404684067) A[2]:(0.786581158638) A[3]:(-0.0152262374759)\n",
      " state (10)  A[0]:(0.724708080292) A[1]:(0.901835918427) A[2]:(0.000393390626414) A[3]:(0.73169618845)\n",
      " state (11)  A[0]:(0.461276739836) A[1]:(0.850952923298) A[2]:(-0.382089316845) A[3]:(0.824882507324)\n",
      " state (12)  A[0]:(0.00720387883484) A[1]:(0.749596476555) A[2]:(-0.0904044881463) A[3]:(0.766714215279)\n",
      " state (13)  A[0]:(-0.000648192944936) A[1]:(0.748692989349) A[2]:(0.901819586754) A[3]:(0.716448545456)\n",
      " state (14)  A[0]:(0.812453269958) A[1]:(0.924261033535) A[2]:(0.999978125095) A[3]:(0.813481688499)\n",
      " state (15)  A[0]:(0.997232615948) A[1]:(0.993362128735) A[2]:(1.0) A[3]:(0.936283051968)\n",
      "Episode 189000 finished after 0 timesteps with r=0.0. Running score: 0.84. Times trained:               6210. Times reached goal: 874.               Steps done: 2259054. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0940044163228.\n",
      "q_values \n",
      "tensor([[ 0.5335,  0.5906,  0.5941,  0.5324]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.0007,  0.6573,  0.5937]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5940,  0.7310,  0.5961,  0.6568]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0007,  0.8109, -0.0039,  0.6588]], device='cuda:0')\n",
      "On state=6, selected action=2 , Random? True\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.533691823483) A[1]:(0.590527534485) A[2]:(0.593021273613) A[3]:(0.531949043274)\n",
      " state (1)  A[0]:(0.532190620899) A[1]:(0.000780880276579) A[2]:(0.656696200371) A[3]:(0.593655586243)\n",
      " state (2)  A[0]:(0.594283461571) A[1]:(0.730923295021) A[2]:(0.595758676529) A[3]:(0.656810998917)\n",
      " state (3)  A[0]:(0.659466862679) A[1]:(-0.00216537364759) A[2]:(0.371794432402) A[3]:(0.557901144028)\n",
      " state (4)  A[0]:(0.581897139549) A[1]:(0.655910968781) A[2]:(0.0320016518235) A[3]:(0.537326574326)\n",
      " state (5)  A[0]:(0.220062926412) A[1]:(0.902139008045) A[2]:(-0.0330579169095) A[3]:(0.4942086339)\n",
      " state (6)  A[0]:(0.0012301945826) A[1]:(0.810927689075) A[2]:(-0.00408969027922) A[3]:(0.659010112286)\n",
      " state (7)  A[0]:(0.634362220764) A[1]:(-0.066820025444) A[2]:(-0.00210952456109) A[3]:(0.939724087715)\n",
      " state (8)  A[0]:(0.657099425793) A[1]:(0.0129603641108) A[2]:(0.729113817215) A[3]:(0.594899892807)\n",
      " state (9)  A[0]:(0.653901576996) A[1]:(0.809075474739) A[2]:(0.785733580589) A[3]:(-0.00740724662319)\n",
      " state (10)  A[0]:(0.727006077766) A[1]:(0.901528835297) A[2]:(1.80006027222e-05) A[3]:(0.733019828796)\n",
      " state (11)  A[0]:(0.464303761721) A[1]:(0.85049366951) A[2]:(-0.383364021778) A[3]:(0.825699150562)\n",
      " state (12)  A[0]:(0.00974810495973) A[1]:(0.749103844166) A[2]:(-0.0939730629325) A[3]:(0.768315434456)\n",
      " state (13)  A[0]:(7.70837068558e-05) A[1]:(0.74763405323) A[2]:(0.901924669743) A[3]:(0.718339800835)\n",
      " state (14)  A[0]:(0.812770605087) A[1]:(0.923341691494) A[2]:(0.99997907877) A[3]:(0.8139128685)\n",
      " state (15)  A[0]:(0.997210741043) A[1]:(0.993145823479) A[2]:(1.0) A[3]:(0.935543894768)\n",
      "Episode 190000 finished after 0 timesteps with r=0.0. Running score: 0.89. Times trained:               6286. Times reached goal: 902.               Steps done: 2265340. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0934153579121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532343268394) A[1]:(0.589521408081) A[2]:(0.590957283974) A[3]:(0.531645894051)\n",
      " state (1)  A[0]:(0.530018806458) A[1]:(-0.00187872129027) A[2]:(0.656912565231) A[3]:(0.592746317387)\n",
      " state (2)  A[0]:(0.592543125153) A[1]:(0.729002356529) A[2]:(0.596431732178) A[3]:(0.655566692352)\n",
      " state (3)  A[0]:(0.659508407116) A[1]:(-0.00625067250803) A[2]:(0.373631864786) A[3]:(0.557400345802)\n",
      " state (4)  A[0]:(0.58298265934) A[1]:(0.655687093735) A[2]:(0.031271032989) A[3]:(0.537019729614)\n",
      " state (5)  A[0]:(0.219778165221) A[1]:(0.902249276638) A[2]:(-0.0337782092392) A[3]:(0.49309566617)\n",
      " state (6)  A[0]:(-0.000765092496295) A[1]:(0.810101091862) A[2]:(-0.00302349589765) A[3]:(0.656854391098)\n",
      " state (7)  A[0]:(0.633814573288) A[1]:(-0.0755172669888) A[2]:(-0.0015473352978) A[3]:(0.938869595528)\n",
      " state (8)  A[0]:(0.658165216446) A[1]:(0.00769985886291) A[2]:(0.725764214993) A[3]:(0.596577763557)\n",
      " state (9)  A[0]:(0.653916239738) A[1]:(0.808417677879) A[2]:(0.784029364586) A[3]:(-0.00698779523373)\n",
      " state (10)  A[0]:(0.726425170898) A[1]:(0.901202201843) A[2]:(-0.00267731514759) A[3]:(0.730916082859)\n",
      " state (11)  A[0]:(0.464020520449) A[1]:(0.85115903616) A[2]:(-0.387094199657) A[3]:(0.824188172817)\n",
      " state (12)  A[0]:(0.00955373048782) A[1]:(0.752422630787) A[2]:(-0.0990816354752) A[3]:(0.766532838345)\n",
      " state (13)  A[0]:(-0.000114358961582) A[1]:(0.75277274847) A[2]:(0.902512490749) A[3]:(0.71578347683)\n",
      " state (14)  A[0]:(0.81338763237) A[1]:(0.92532324791) A[2]:(0.999980390072) A[3]:(0.81108546257)\n",
      " state (15)  A[0]:(0.997186422348) A[1]:(0.993255317211) A[2]:(1.0) A[3]:(0.933461546898)\n",
      "Episode 191000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6230. Times reached goal: 896.               Steps done: 2271570. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0928351893339.\n",
      " state (0)  A[0]:(0.532794356346) A[1]:(0.59059715271) A[2]:(0.58968436718) A[3]:(0.530985713005)\n",
      " state (1)  A[0]:(0.529610037804) A[1]:(-5.83231449127e-05) A[2]:(0.655827403069) A[3]:(0.591802299023)\n",
      " state (2)  A[0]:(0.591715574265) A[1]:(0.730180561543) A[2]:(0.596938729286) A[3]:(0.654257774353)\n",
      " state (3)  A[0]:(0.658134818077) A[1]:(-0.00259649171494) A[2]:(0.37650129199) A[3]:(0.55776488781)\n",
      " state (4)  A[0]:(0.582943677902) A[1]:(0.655322551727) A[2]:(0.0318816117942) A[3]:(0.537824869156)\n",
      " state (5)  A[0]:(0.221273228526) A[1]:(0.903108477592) A[2]:(-0.0342644974589) A[3]:(0.494495481253)\n",
      " state (6)  A[0]:(0.00274685816839) A[1]:(0.811603426933) A[2]:(-0.00154888501856) A[3]:(0.657865166664)\n",
      " state (7)  A[0]:(0.636078953743) A[1]:(-0.0751076638699) A[2]:(0.00395546294749) A[3]:(0.938383638859)\n",
      " state (8)  A[0]:(0.658619165421) A[1]:(0.0123104788363) A[2]:(0.72973716259) A[3]:(0.592427015305)\n",
      " state (9)  A[0]:(0.655140399933) A[1]:(0.809490680695) A[2]:(0.786298453808) A[3]:(-0.00552880344912)\n",
      " state (10)  A[0]:(0.728211641312) A[1]:(0.901102304459) A[2]:(3.00407409668e-05) A[3]:(0.733323812485)\n",
      " state (11)  A[0]:(0.467903524637) A[1]:(0.850946009159) A[2]:(-0.387733817101) A[3]:(0.826777935028)\n",
      " state (12)  A[0]:(0.0133270630613) A[1]:(0.751703262329) A[2]:(-0.103872515261) A[3]:(0.770682096481)\n",
      " state (13)  A[0]:(-0.000518485845532) A[1]:(0.750094115734) A[2]:(0.901198267937) A[3]:(0.720612227917)\n",
      " state (14)  A[0]:(0.811313092709) A[1]:(0.922983586788) A[2]:(0.999980390072) A[3]:(0.813430249691)\n",
      " state (15)  A[0]:(0.997107326984) A[1]:(0.992825329304) A[2]:(1.0) A[3]:(0.933591961861)\n",
      "Episode 192000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6325. Times reached goal: 907.               Steps done: 2277895. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0922498598173.\n",
      " state (0)  A[0]:(0.532023668289) A[1]:(0.587645888329) A[2]:(0.590094566345) A[3]:(0.532161474228)\n",
      " state (1)  A[0]:(0.530268073082) A[1]:(0.00100368226413) A[2]:(0.656513571739) A[3]:(0.593650281429)\n",
      " state (2)  A[0]:(0.592679560184) A[1]:(0.729619145393) A[2]:(0.595752477646) A[3]:(0.656371951103)\n",
      " state (3)  A[0]:(0.659586071968) A[1]:(-0.00350220571272) A[2]:(0.376237720251) A[3]:(0.559215068817)\n",
      " state (4)  A[0]:(0.584780752659) A[1]:(0.655338287354) A[2]:(0.0277680289) A[3]:(0.539175271988)\n",
      " state (5)  A[0]:(0.221023753285) A[1]:(0.903196930885) A[2]:(-0.0393703244627) A[3]:(0.49556684494)\n",
      " state (6)  A[0]:(-0.000228352844715) A[1]:(0.810690641403) A[2]:(-0.00457474356517) A[3]:(0.657837510109)\n",
      " state (7)  A[0]:(0.633804917336) A[1]:(-0.0835971608758) A[2]:(0.00212645204738) A[3]:(0.938238084316)\n",
      " state (8)  A[0]:(0.657594561577) A[1]:(0.00394875742495) A[2]:(0.726255059242) A[3]:(0.598900258541)\n",
      " state (9)  A[0]:(0.653043985367) A[1]:(0.807878315449) A[2]:(0.787113308907) A[3]:(-0.00557732582092)\n",
      " state (10)  A[0]:(0.727917015553) A[1]:(0.901042282581) A[2]:(0.00110673857853) A[3]:(0.732423305511)\n",
      " state (11)  A[0]:(0.469935148954) A[1]:(0.852574169636) A[2]:(-0.390891849995) A[3]:(0.827235221863)\n",
      " state (12)  A[0]:(0.0162711758167) A[1]:(0.756675004959) A[2]:(-0.109744220972) A[3]:(0.77183842659)\n",
      " state (13)  A[0]:(0.00148824497592) A[1]:(0.756582736969) A[2]:(0.90124553442) A[3]:(0.721490323544)\n",
      " state (14)  A[0]:(0.812091171741) A[1]:(0.925177395344) A[2]:(0.999981343746) A[3]:(0.812791824341)\n",
      " state (15)  A[0]:(0.997081398964) A[1]:(0.992935121059) A[2]:(1.0) A[3]:(0.932248473167)\n",
      "Episode 193000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6248. Times reached goal: 906.               Steps done: 2284143. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0916752795511.\n",
      " state (0)  A[0]:(0.533873677254) A[1]:(0.591621696949) A[2]:(0.592039942741) A[3]:(0.534576773643)\n",
      " state (1)  A[0]:(0.531323730946) A[1]:(0.000599667371716) A[2]:(0.657928943634) A[3]:(0.594665050507)\n",
      " state (2)  A[0]:(0.592969596386) A[1]:(0.730471909046) A[2]:(0.599564790726) A[3]:(0.656540036201)\n",
      " state (3)  A[0]:(0.659423589706) A[1]:(-0.00639267498627) A[2]:(0.385035246611) A[3]:(0.557748436928)\n",
      " state (4)  A[0]:(0.585047602654) A[1]:(0.656058967113) A[2]:(0.0345953889191) A[3]:(0.538097441196)\n",
      " state (5)  A[0]:(0.220280244946) A[1]:(0.904103934765) A[2]:(-0.0341174453497) A[3]:(0.495251625776)\n",
      " state (6)  A[0]:(0.000543557049241) A[1]:(0.811394751072) A[2]:(0.00185656337999) A[3]:(0.658391237259)\n",
      " state (7)  A[0]:(0.635966479778) A[1]:(-0.0861426591873) A[2]:(0.0105915879831) A[3]:(0.93790358305)\n",
      " state (8)  A[0]:(0.658903717995) A[1]:(0.00931320991367) A[2]:(0.729710817337) A[3]:(0.596051216125)\n",
      " state (9)  A[0]:(0.656117737293) A[1]:(0.80937474966) A[2]:(0.786781251431) A[3]:(0.0013913968578)\n",
      " state (10)  A[0]:(0.729652047157) A[1]:(0.90114402771) A[2]:(-0.000351071357727) A[3]:(0.734688878059)\n",
      " state (11)  A[0]:(0.472389549017) A[1]:(0.853302001953) A[2]:(-0.392870932817) A[3]:(0.828270435333)\n",
      " state (12)  A[0]:(0.0184994172305) A[1]:(0.759187698364) A[2]:(-0.114569604397) A[3]:(0.773200631142)\n",
      " state (13)  A[0]:(0.00107426149771) A[1]:(0.759326219559) A[2]:(0.900429010391) A[3]:(0.722643613815)\n",
      " state (14)  A[0]:(0.81092363596) A[1]:(0.925459861755) A[2]:(0.999981641769) A[3]:(0.81249243021)\n",
      " state (15)  A[0]:(0.997022509575) A[1]:(0.992812871933) A[2]:(1.0) A[3]:(0.931281507015)\n",
      "Episode 194000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6279. Times reached goal: 917.               Steps done: 2290422. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0911014538818.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5909,  0.5897,  0.5296]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5865,  0.6543,  0.0268,  0.5354]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6562,  0.0066,  0.7287,  0.5910]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6534,  0.8096,  0.7876, -0.0063]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0019,  0.7633,  0.9018,  0.7218]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6535,  0.8103,  0.7882, -0.0076]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0020,  0.7638,  0.9022,  0.7213]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8125,  0.9262,  1.0000,  0.8112]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.529780745506) A[1]:(0.591040968895) A[2]:(0.590394079685) A[3]:(0.529639005661)\n",
      " state (1)  A[0]:(0.527614116669) A[1]:(-0.000210896134377) A[2]:(0.657224953175) A[3]:(0.591062903404)\n",
      " state (2)  A[0]:(0.589586734772) A[1]:(0.728316545486) A[2]:(0.598556697369) A[3]:(0.653722643852)\n",
      " state (3)  A[0]:(0.657762885094) A[1]:(-0.00983568560332) A[2]:(0.385913491249) A[3]:(0.553496837616)\n",
      " state (4)  A[0]:(0.584583997726) A[1]:(0.653525710106) A[2]:(0.0314904004335) A[3]:(0.533864378929)\n",
      " state (5)  A[0]:(0.218597695231) A[1]:(0.904017388821) A[2]:(-0.0388835035264) A[3]:(0.490909308195)\n",
      " state (6)  A[0]:(-0.00216703512706) A[1]:(0.81078016758) A[2]:(-0.00052082532784) A[3]:(0.65435552597)\n",
      " state (7)  A[0]:(0.633493900299) A[1]:(-0.0923415273428) A[2]:(0.0113817304373) A[3]:(0.936491250992)\n",
      " state (8)  A[0]:(0.655574083328) A[1]:(0.00579858757555) A[2]:(0.729308307171) A[3]:(0.591540694237)\n",
      " state (9)  A[0]:(0.651924788952) A[1]:(0.809014499187) A[2]:(0.788265407085) A[3]:(-0.00806037243456)\n",
      " state (10)  A[0]:(0.727083206177) A[1]:(0.901298463345) A[2]:(0.00229787430726) A[3]:(0.730437338352)\n",
      " state (11)  A[0]:(0.470014095306) A[1]:(0.854496538639) A[2]:(-0.39313262701) A[3]:(0.826323986053)\n",
      " state (12)  A[0]:(0.0163603052497) A[1]:(0.76216417551) A[2]:(-0.115213893354) A[3]:(0.771533787251)\n",
      " state (13)  A[0]:(0.000258050858974) A[1]:(0.762385725975) A[2]:(0.902028918266) A[3]:(0.721080601215)\n",
      " state (14)  A[0]:(0.812136232853) A[1]:(0.925939142704) A[2]:(0.999982953072) A[3]:(0.811212301254)\n",
      " state (15)  A[0]:(0.99702423811) A[1]:(0.99267822504) A[2]:(1.0) A[3]:(0.930133283138)\n",
      "Episode 195000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6159. Times reached goal: 889.               Steps done: 2296581. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.090542084374.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532330393791) A[1]:(0.590773522854) A[2]:(0.59095698595) A[3]:(0.531809806824)\n",
      " state (1)  A[0]:(0.53033208847) A[1]:(1.08927488327e-05) A[2]:(0.656826615334) A[3]:(0.593585431576)\n",
      " state (2)  A[0]:(0.592565774918) A[1]:(0.730794072151) A[2]:(0.596066951752) A[3]:(0.656633138657)\n",
      " state (3)  A[0]:(0.65951949358) A[1]:(-0.00402241246775) A[2]:(0.381134569645) A[3]:(0.556628346443)\n",
      " state (4)  A[0]:(0.586490213871) A[1]:(0.6600061059) A[2]:(0.0215501785278) A[3]:(0.537771224976)\n",
      " state (5)  A[0]:(0.219699382782) A[1]:(0.905656039715) A[2]:(-0.048320543021) A[3]:(0.495008826256)\n",
      " state (6)  A[0]:(0.00102238322143) A[1]:(0.811695992947) A[2]:(-0.0068867309019) A[3]:(0.657708883286)\n",
      " state (7)  A[0]:(0.637807250023) A[1]:(-0.095067679882) A[2]:(0.00919995550066) A[3]:(0.936678946018)\n",
      " state (8)  A[0]:(0.658417522907) A[1]:(0.00893299374729) A[2]:(0.729176878929) A[3]:(0.589673519135)\n",
      " state (9)  A[0]:(0.654992759228) A[1]:(0.809814870358) A[2]:(0.787577986717) A[3]:(-0.00834109541029)\n",
      " state (10)  A[0]:(0.729290127754) A[1]:(0.90132522583) A[2]:(-0.00236165081151) A[3]:(0.730710148811)\n",
      " state (11)  A[0]:(0.473365664482) A[1]:(0.855098664761) A[2]:(-0.39949041605) A[3]:(0.826987028122)\n",
      " state (12)  A[0]:(0.0192516651005) A[1]:(0.764106094837) A[2]:(-0.124630331993) A[3]:(0.772866129875)\n",
      " state (13)  A[0]:(0.00112993968651) A[1]:(0.76431286335) A[2]:(0.90090662241) A[3]:(0.722636580467)\n",
      " state (14)  A[0]:(0.812234282494) A[1]:(0.92594563961) A[2]:(0.999983370304) A[3]:(0.811690449715)\n",
      " state (15)  A[0]:(0.996996939182) A[1]:(0.992512524128) A[2]:(1.0) A[3]:(0.929630041122)\n",
      "Episode 196000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6334. Times reached goal: 924.               Steps done: 2302915. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.089970403237.\n",
      " state (0)  A[0]:(0.53170311451) A[1]:(0.588547587395) A[2]:(0.590525209904) A[3]:(0.531226277351)\n",
      " state (1)  A[0]:(0.529116034508) A[1]:(-0.000572428049054) A[2]:(0.655620455742) A[3]:(0.591822087765)\n",
      " state (2)  A[0]:(0.590452492237) A[1]:(0.729105889797) A[2]:(0.596190810204) A[3]:(0.654087662697)\n",
      " state (3)  A[0]:(0.656201004982) A[1]:(-0.00978368613869) A[2]:(0.384777516127) A[3]:(0.553717315197)\n",
      " state (4)  A[0]:(0.583787202835) A[1]:(0.653979063034) A[2]:(0.0225366335362) A[3]:(0.534434914589)\n",
      " state (5)  A[0]:(0.216671302915) A[1]:(0.905102431774) A[2]:(-0.0489352792501) A[3]:(0.491431832314)\n",
      " state (6)  A[0]:(-0.00160749116912) A[1]:(0.810964405537) A[2]:(-0.00507874879986) A[3]:(0.654799580574)\n",
      " state (7)  A[0]:(0.63488972187) A[1]:(-0.0998938158154) A[2]:(0.0130390878767) A[3]:(0.935812294483)\n",
      " state (8)  A[0]:(0.655161857605) A[1]:(0.00823722034693) A[2]:(0.72884786129) A[3]:(0.590458989143)\n",
      " state (9)  A[0]:(0.651973366737) A[1]:(0.809924721718) A[2]:(0.788196563721) A[3]:(-0.00627267593518)\n",
      " state (10)  A[0]:(0.727432012558) A[1]:(0.901098489761) A[2]:(-0.000271916389465) A[3]:(0.731030285358)\n",
      " state (11)  A[0]:(0.472014546394) A[1]:(0.854796230793) A[2]:(-0.399200737476) A[3]:(0.827615559101)\n",
      " state (12)  A[0]:(0.0184397213161) A[1]:(0.763097524643) A[2]:(-0.125615075231) A[3]:(0.774220466614)\n",
      " state (13)  A[0]:(0.000430956453783) A[1]:(0.761212825775) A[2]:(0.901495933533) A[3]:(0.72445166111)\n",
      " state (14)  A[0]:(0.812388896942) A[1]:(0.923318088055) A[2]:(0.999984025955) A[3]:(0.812586784363)\n",
      " state (15)  A[0]:(0.996976494789) A[1]:(0.991947591305) A[2]:(1.0) A[3]:(0.929393470287)\n",
      "Episode 197000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6273. Times reached goal: 911.               Steps done: 2309188. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0894077853933.\n",
      " state (0)  A[0]:(0.532879471779) A[1]:(0.588314890862) A[2]:(0.589156150818) A[3]:(0.529562473297)\n",
      " state (1)  A[0]:(0.530138969421) A[1]:(7.264316082e-05) A[2]:(0.655363917351) A[3]:(0.589854776859)\n",
      " state (2)  A[0]:(0.591537833214) A[1]:(0.729101002216) A[2]:(0.595282375813) A[3]:(0.653048038483)\n",
      " state (3)  A[0]:(0.658216357231) A[1]:(-0.00857208576053) A[2]:(0.384760051966) A[3]:(0.554307818413)\n",
      " state (4)  A[0]:(0.58670592308) A[1]:(0.65437746048) A[2]:(0.0200380682945) A[3]:(0.536143541336)\n",
      " state (5)  A[0]:(0.218673065305) A[1]:(0.905435442924) A[2]:(-0.050771176815) A[3]:(0.493015944958)\n",
      " state (6)  A[0]:(-0.00209556217305) A[1]:(0.811218500137) A[2]:(-0.00373266404495) A[3]:(0.654390931129)\n",
      " state (7)  A[0]:(0.633935332298) A[1]:(-0.101840339601) A[2]:(0.0163092464209) A[3]:(0.935133099556)\n",
      " state (8)  A[0]:(0.656219601631) A[1]:(0.00646258844063) A[2]:(0.728518307209) A[3]:(0.592904269695)\n",
      " state (9)  A[0]:(0.653269886971) A[1]:(0.809232890606) A[2]:(0.788908243179) A[3]:(-0.001769420458)\n",
      " state (10)  A[0]:(0.728561580181) A[1]:(0.90086466074) A[2]:(0.000234842300415) A[3]:(0.732666969299)\n",
      " state (11)  A[0]:(0.47431114316) A[1]:(0.855565905571) A[2]:(-0.401655077934) A[3]:(0.829003214836)\n",
      " state (12)  A[0]:(0.0201485157013) A[1]:(0.765876829624) A[2]:(-0.131326839328) A[3]:(0.775991439819)\n",
      " state (13)  A[0]:(-0.000659942510538) A[1]:(0.764438152313) A[2]:(0.900695860386) A[3]:(0.725835442543)\n",
      " state (14)  A[0]:(0.811192572117) A[1]:(0.923916220665) A[2]:(0.999984383583) A[3]:(0.812469959259)\n",
      " state (15)  A[0]:(0.996918082237) A[1]:(0.991858899593) A[2]:(1.0) A[3]:(0.928535163403)\n",
      "Episode 198000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6313. Times reached goal: 911.               Steps done: 2315501. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0888451319285.\n",
      " state (0)  A[0]:(0.532984554768) A[1]:(0.592685639858) A[2]:(0.591505765915) A[3]:(0.533526301384)\n",
      " state (1)  A[0]:(0.531114697456) A[1]:(0.00125976582058) A[2]:(0.658452510834) A[3]:(0.594143867493)\n",
      " state (2)  A[0]:(0.59247559309) A[1]:(0.730723023415) A[2]:(0.597347795963) A[3]:(0.657468318939)\n",
      " state (3)  A[0]:(0.660905957222) A[1]:(-0.00193325185683) A[2]:(0.385465145111) A[3]:(0.558753609657)\n",
      " state (4)  A[0]:(0.589969992638) A[1]:(0.655841350555) A[2]:(0.0159930661321) A[3]:(0.540300369263)\n",
      " state (5)  A[0]:(0.219863861799) A[1]:(0.90587502718) A[2]:(-0.0554685480893) A[3]:(0.497024536133)\n",
      " state (6)  A[0]:(-0.00337955565192) A[1]:(0.81121635437) A[2]:(-0.00561338709667) A[3]:(0.656597435474)\n",
      " state (7)  A[0]:(0.633097827435) A[1]:(-0.105513580143) A[2]:(0.0174423642457) A[3]:(0.935334324837)\n",
      " state (8)  A[0]:(0.656234264374) A[1]:(0.00315256114118) A[2]:(0.728477835655) A[3]:(0.597196757793)\n",
      " state (9)  A[0]:(0.652926206589) A[1]:(0.80854922533) A[2]:(0.790845632553) A[3]:(-0.000614940596279)\n",
      " state (10)  A[0]:(0.729101061821) A[1]:(0.90100479126) A[2]:(0.00152289750986) A[3]:(0.733464717865)\n",
      " state (11)  A[0]:(0.47626337409) A[1]:(0.856935739517) A[2]:(-0.404920220375) A[3]:(0.830374240875)\n",
      " state (12)  A[0]:(0.0214372370392) A[1]:(0.769219994545) A[2]:(-0.138105407357) A[3]:(0.777970314026)\n",
      " state (13)  A[0]:(-0.00227079889737) A[1]:(0.767609715462) A[2]:(0.899863898754) A[3]:(0.72765392065)\n",
      " state (14)  A[0]:(0.809657752514) A[1]:(0.924144744873) A[2]:(0.999984741211) A[3]:(0.812878012657)\n",
      " state (15)  A[0]:(0.996843397617) A[1]:(0.991669178009) A[2]:(1.0) A[3]:(0.927975535393)\n",
      "Episode 199000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6284. Times reached goal: 910.               Steps done: 2321785. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0882885796382.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5923,  0.5894,  0.5307]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5879,  0.6575,  0.0263,  0.5354]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6583, -0.0005,  0.7276,  0.5961]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6540,  0.8083,  0.7910, -0.0050]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0021,  0.7689,  0.9001,  0.7255]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8120,  0.9240,  1.0000,  0.8114]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530882835388) A[1]:(0.591526389122) A[2]:(0.591136574745) A[3]:(0.531033873558)\n",
      " state (1)  A[0]:(0.530050992966) A[1]:(-0.000788494769949) A[2]:(0.657146334648) A[3]:(0.593098342419)\n",
      " state (2)  A[0]:(0.59083032608) A[1]:(0.730345487595) A[2]:(0.598782777786) A[3]:(0.65568780899)\n",
      " state (3)  A[0]:(0.658660292625) A[1]:(-0.0086130304262) A[2]:(0.396241158247) A[3]:(0.555065631866)\n",
      " state (4)  A[0]:(0.587826609612) A[1]:(0.657168149948) A[2]:(0.0248457305133) A[3]:(0.536490559578)\n",
      " state (5)  A[0]:(0.216422364116) A[1]:(0.907109975815) A[2]:(-0.0510807931423) A[3]:(0.493222773075)\n",
      " state (6)  A[0]:(0.00184163241647) A[1]:(0.810165703297) A[2]:(-0.00279509299435) A[3]:(0.657350182533)\n",
      " state (7)  A[0]:(0.641072392464) A[1]:(-0.119878768921) A[2]:(0.0187856499106) A[3]:(0.935735523701)\n",
      " state (8)  A[0]:(0.658730864525) A[1]:(0.000248298048973) A[2]:(0.727260112762) A[3]:(0.596494793892)\n",
      " state (9)  A[0]:(0.654905319214) A[1]:(0.808449804783) A[2]:(0.790361762047) A[3]:(-0.00281751342118)\n",
      " state (10)  A[0]:(0.73060464859) A[1]:(0.900818824768) A[2]:(-0.00127828051336) A[3]:(0.731696307659)\n",
      " state (11)  A[0]:(0.479366511106) A[1]:(0.857204318047) A[2]:(-0.409784406424) A[3]:(0.829290390015)\n",
      " state (12)  A[0]:(0.0257671494037) A[1]:(0.770495176315) A[2]:(-0.144999086857) A[3]:(0.776723504066)\n",
      " state (13)  A[0]:(0.0025623831898) A[1]:(0.768887042999) A[2]:(0.899813234806) A[3]:(0.726146817207)\n",
      " state (14)  A[0]:(0.811961293221) A[1]:(0.923918783665) A[2]:(0.999985396862) A[3]:(0.811610758305)\n",
      " state (15)  A[0]:(0.9968521595) A[1]:(0.991429924965) A[2]:(1.0) A[3]:(0.926968872547)\n",
      "Episode 200000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6223. Times reached goal: 914.               Steps done: 2328008. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0877408657863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532239079475) A[1]:(0.589134454727) A[2]:(0.591595232487) A[3]:(0.531978070736)\n",
      " state (1)  A[0]:(0.53179371357) A[1]:(0.00114647252485) A[2]:(0.658086061478) A[3]:(0.59423995018)\n",
      " state (2)  A[0]:(0.592762708664) A[1]:(0.730177044868) A[2]:(0.597394943237) A[3]:(0.657977342606)\n",
      " state (3)  A[0]:(0.658832013607) A[1]:(-0.00856598373502) A[2]:(0.393777132034) A[3]:(0.556342720985)\n",
      " state (4)  A[0]:(0.588114261627) A[1]:(0.657193779945) A[2]:(0.019328808412) A[3]:(0.538483977318)\n",
      " state (5)  A[0]:(0.216018408537) A[1]:(0.907454133034) A[2]:(-0.055329978466) A[3]:(0.496121257544)\n",
      " state (6)  A[0]:(-0.00120767147746) A[1]:(0.811307668686) A[2]:(-0.0031648769509) A[3]:(0.657987296581)\n",
      " state (7)  A[0]:(0.637413978577) A[1]:(-0.116531930864) A[2]:(0.0219267029315) A[3]:(0.935050070286)\n",
      " state (8)  A[0]:(0.656581640244) A[1]:(0.00374977779575) A[2]:(0.72862380743) A[3]:(0.595538973808)\n",
      " state (9)  A[0]:(0.653128623962) A[1]:(0.80916684866) A[2]:(0.791595757008) A[3]:(-0.00178909488022)\n",
      " state (10)  A[0]:(0.729098916054) A[1]:(0.901100099087) A[2]:(0.000178933143616) A[3]:(0.732452511787)\n",
      " state (11)  A[0]:(0.477278202772) A[1]:(0.858131170273) A[2]:(-0.41031524539) A[3]:(0.830156385899)\n",
      " state (12)  A[0]:(0.0227155275643) A[1]:(0.772372186184) A[2]:(-0.146127641201) A[3]:(0.7780636549)\n",
      " state (13)  A[0]:(-0.000776208762545) A[1]:(0.770033657551) A[2]:(0.9008461833) A[3]:(0.727716445923)\n",
      " state (14)  A[0]:(0.811363816261) A[1]:(0.923341572285) A[2]:(0.999986171722) A[3]:(0.812476813793)\n",
      " state (15)  A[0]:(0.996811389923) A[1]:(0.991108953953) A[2]:(1.0) A[3]:(0.9268014431)\n",
      "Episode 201000 finished after 0 timesteps with r=0.0. Running score: 0.86. Times trained:               6195. Times reached goal: 896.               Steps done: 2334203. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0871989913119.\n",
      " state (0)  A[0]:(0.532559037209) A[1]:(0.590399980545) A[2]:(0.591114580631) A[3]:(0.532246351242)\n",
      " state (1)  A[0]:(0.531387329102) A[1]:(0.000195443630219) A[2]:(0.656818687916) A[3]:(0.593283951283)\n",
      " state (2)  A[0]:(0.5910166502) A[1]:(0.729742765427) A[2]:(0.59737688303) A[3]:(0.656244695187)\n",
      " state (3)  A[0]:(0.657396495342) A[1]:(-0.0107717225328) A[2]:(0.399540901184) A[3]:(0.553781449795)\n",
      " state (4)  A[0]:(0.586543798447) A[1]:(0.656039655209) A[2]:(0.0222401898354) A[3]:(0.535759449005)\n",
      " state (5)  A[0]:(0.211656555533) A[1]:(0.907727301121) A[2]:(-0.0561505593359) A[3]:(0.49306845665)\n",
      " state (6)  A[0]:(-0.00278491550125) A[1]:(0.809853672981) A[2]:(-0.00401184801012) A[3]:(0.65681552887)\n",
      " state (7)  A[0]:(0.637869715691) A[1]:(-0.126897290349) A[2]:(0.0223300289363) A[3]:(0.934481680393)\n",
      " state (8)  A[0]:(0.653611660004) A[1]:(0.00141182448715) A[2]:(0.728403091431) A[3]:(0.591250956059)\n",
      " state (9)  A[0]:(0.649848222733) A[1]:(0.809072494507) A[2]:(0.792128503323) A[3]:(-0.00842069089413)\n",
      " state (10)  A[0]:(0.726518511772) A[1]:(0.900998175144) A[2]:(-0.000515818537679) A[3]:(0.729016363621)\n",
      " state (11)  A[0]:(0.473657995462) A[1]:(0.858350455761) A[2]:(-0.413499325514) A[3]:(0.827988505363)\n",
      " state (12)  A[0]:(0.0181066859514) A[1]:(0.772943198681) A[2]:(-0.151103690267) A[3]:(0.775404751301)\n",
      " state (13)  A[0]:(-0.00523915560916) A[1]:(0.769713878632) A[2]:(0.901044070721) A[3]:(0.72457742691)\n",
      " state (14)  A[0]:(0.810333132744) A[1]:(0.922134399414) A[2]:(0.999986767769) A[3]:(0.810205817223)\n",
      " state (15)  A[0]:(0.99675911665) A[1]:(0.990686416626) A[2]:(1.0) A[3]:(0.925522089005)\n",
      "Episode 202000 finished after 0 timesteps with r=0.0. Running score: 0.88. Times trained:               6255. Times reached goal: 913.               Steps done: 2340458. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0866552639015.\n",
      " state (0)  A[0]:(0.532860338688) A[1]:(0.590720057487) A[2]:(0.591597914696) A[3]:(0.531363010406)\n",
      " state (1)  A[0]:(0.532059013844) A[1]:(9.07480716705e-06) A[2]:(0.656619846821) A[3]:(0.592427194118)\n",
      " state (2)  A[0]:(0.592082858086) A[1]:(0.730506062508) A[2]:(0.596672654152) A[3]:(0.655828475952)\n",
      " state (3)  A[0]:(0.657925128937) A[1]:(-0.00811456236988) A[2]:(0.401767045259) A[3]:(0.553324878216)\n",
      " state (4)  A[0]:(0.588103652) A[1]:(0.656167507172) A[2]:(0.022379476577) A[3]:(0.535584330559)\n",
      " state (5)  A[0]:(0.213773399591) A[1]:(0.908162713051) A[2]:(-0.0570303574204) A[3]:(0.493006527424)\n",
      " state (6)  A[0]:(-0.00130718876608) A[1]:(0.810702383518) A[2]:(-0.00276469485834) A[3]:(0.655992686749)\n",
      " state (7)  A[0]:(0.638800978661) A[1]:(-0.125885426998) A[2]:(0.0252734236419) A[3]:(0.934087753296)\n",
      " state (8)  A[0]:(0.656110644341) A[1]:(0.00303805433214) A[2]:(0.727951407433) A[3]:(0.594710230827)\n",
      " state (9)  A[0]:(0.651877701283) A[1]:(0.8093495965) A[2]:(0.792294323444) A[3]:(-0.00316401827149)\n",
      " state (10)  A[0]:(0.727854251862) A[1]:(0.900982677937) A[2]:(-0.001249312656) A[3]:(0.73070538044)\n",
      " state (11)  A[0]:(0.476372748613) A[1]:(0.858743965626) A[2]:(-0.416448801756) A[3]:(0.829276442528)\n",
      " state (12)  A[0]:(0.0211859829724) A[1]:(0.773875117302) A[2]:(-0.157124370337) A[3]:(0.777156352997)\n",
      " state (13)  A[0]:(-0.00401922408491) A[1]:(0.769599914551) A[2]:(0.900017619133) A[3]:(0.726456582546)\n",
      " state (14)  A[0]:(0.810026407242) A[1]:(0.920818150043) A[2]:(0.999986946583) A[3]:(0.811178922653)\n",
      " state (15)  A[0]:(0.996719062328) A[1]:(0.990244328976) A[2]:(1.0) A[3]:(0.925562977791)\n",
      "Episode 203000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6204. Times reached goal: 904.               Steps done: 2346662. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0861193188648.\n",
      " state (0)  A[0]:(0.531199455261) A[1]:(0.590685248375) A[2]:(0.591695189476) A[3]:(0.531215786934)\n",
      " state (1)  A[0]:(0.531852841377) A[1]:(0.000882550841197) A[2]:(0.655802369118) A[3]:(0.593079209328)\n",
      " state (2)  A[0]:(0.591736853123) A[1]:(0.72912466526) A[2]:(0.596408128738) A[3]:(0.656139135361)\n",
      " state (3)  A[0]:(0.657761454582) A[1]:(-0.0242995005101) A[2]:(0.407507419586) A[3]:(0.552817106247)\n",
      " state (4)  A[0]:(0.58889901638) A[1]:(0.654355823994) A[2]:(0.0236088261008) A[3]:(0.536390542984)\n",
      " state (5)  A[0]:(0.213697686791) A[1]:(0.908396422863) A[2]:(-0.0595605298877) A[3]:(0.493841409683)\n",
      " state (6)  A[0]:(-0.00160434702411) A[1]:(0.810460329056) A[2]:(-0.00439807912335) A[3]:(0.656111121178)\n",
      " state (7)  A[0]:(0.638212263584) A[1]:(-0.129365250468) A[2]:(0.026535121724) A[3]:(0.933568120003)\n",
      " state (8)  A[0]:(0.65525662899) A[1]:(0.00288460357115) A[2]:(0.728811323643) A[3]:(0.593640267849)\n",
      " state (9)  A[0]:(0.6520408988) A[1]:(0.809340059757) A[2]:(0.794170200825) A[3]:(-0.00306134833954)\n",
      " state (10)  A[0]:(0.728950798512) A[1]:(0.90091586113) A[2]:(0.00150728109293) A[3]:(0.731542348862)\n",
      " state (11)  A[0]:(0.479717731476) A[1]:(0.85911154747) A[2]:(-0.417141675949) A[3]:(0.83041036129)\n",
      " state (12)  A[0]:(0.0258971955627) A[1]:(0.774799644947) A[2]:(-0.160493895411) A[3]:(0.778847694397)\n",
      " state (13)  A[0]:(-0.000341728329659) A[1]:(0.769586980343) A[2]:(0.899639964104) A[3]:(0.728313028812)\n",
      " state (14)  A[0]:(0.81084138155) A[1]:(0.919591546059) A[2]:(0.999987185001) A[3]:(0.812132239342)\n",
      " state (15)  A[0]:(0.996701359749) A[1]:(0.989803254604) A[2]:(1.0) A[3]:(0.925561904907)\n",
      "Episode 204000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained:               6306. Times reached goal: 910.               Steps done: 2352968. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0855779591412.\n",
      "q_values \n",
      "tensor([[ 0.5328,  0.5909,  0.5910,  0.5324]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5324,  0.0020,  0.6563,  0.5930]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5921,  0.7305,  0.5949,  0.6566]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  0.8106, -0.0070,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=3 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5912,  0.7296,  0.5948,  0.6564]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0015,  0.8100, -0.0079,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9009, -0.0014,  0.7323]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8118,  0.9203,  1.0000,  0.8124]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531271457672) A[1]:(0.590145230293) A[2]:(0.591484725475) A[3]:(0.531715333462)\n",
      " state (1)  A[0]:(0.53063082695) A[1]:(0.000510372163262) A[2]:(0.656564116478) A[3]:(0.592823326588)\n",
      " state (2)  A[0]:(0.590538680553) A[1]:(0.729656457901) A[2]:(0.594999134541) A[3]:(0.656713664532)\n",
      " state (3)  A[0]:(0.657214641571) A[1]:(-0.0163239259273) A[2]:(0.404969453812) A[3]:(0.552990436554)\n",
      " state (4)  A[0]:(0.588677883148) A[1]:(0.656825065613) A[2]:(0.0170916356146) A[3]:(0.536833107471)\n",
      " state (5)  A[0]:(0.21198451519) A[1]:(0.90904468298) A[2]:(-0.0657386258245) A[3]:(0.494668424129)\n",
      " state (6)  A[0]:(-0.0026548889) A[1]:(0.810740053654) A[2]:(-0.00717591820285) A[3]:(0.656208634377)\n",
      " state (7)  A[0]:(0.638578176498) A[1]:(-0.130684763193) A[2]:(0.02675216645) A[3]:(0.932967960835)\n",
      " state (8)  A[0]:(0.656363725662) A[1]:(0.00467884866521) A[2]:(0.72763299942) A[3]:(0.593848347664)\n",
      " state (9)  A[0]:(0.653203368187) A[1]:(0.809891879559) A[2]:(0.793437838554) A[3]:(0.00017474591732)\n",
      " state (10)  A[0]:(0.729260325432) A[1]:(0.901279807091) A[2]:(-0.000426411599619) A[3]:(0.732327461243)\n",
      " state (11)  A[0]:(0.480139285326) A[1]:(0.860471010208) A[2]:(-0.419401615858) A[3]:(0.830783009529)\n",
      " state (12)  A[0]:(0.0260884556919) A[1]:(0.778081715107) A[2]:(-0.162287831306) A[3]:(0.779179334641)\n",
      " state (13)  A[0]:(4.2736530304e-05) A[1]:(0.773222386837) A[2]:(0.901073336601) A[3]:(0.728561043739)\n",
      " state (14)  A[0]:(0.811651885509) A[1]:(0.920371592045) A[2]:(0.999988019466) A[3]:(0.812294185162)\n",
      " state (15)  A[0]:(0.996674954891) A[1]:(0.989657998085) A[2]:(1.0) A[3]:(0.925229907036)\n",
      "Episode 205000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained:               6262. Times reached goal: 913.               Steps done: 2359230. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0850437443333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531569123268) A[1]:(0.589554488659) A[2]:(0.591626465321) A[3]:(0.53003692627)\n",
      " state (1)  A[0]:(0.531519711018) A[1]:(0.000815197650809) A[2]:(0.656623959541) A[3]:(0.591187238693)\n",
      " state (2)  A[0]:(0.591332197189) A[1]:(0.729801058769) A[2]:(0.595667719841) A[3]:(0.655558586121)\n",
      " state (3)  A[0]:(0.656972229481) A[1]:(-0.0180432442576) A[2]:(0.410769790411) A[3]:(0.550210177898)\n",
      " state (4)  A[0]:(0.589155197144) A[1]:(0.65648329258) A[2]:(0.0215692445636) A[3]:(0.534734010696)\n",
      " state (5)  A[0]:(0.213055804372) A[1]:(0.909523248672) A[2]:(-0.0626888871193) A[3]:(0.493500679731)\n",
      " state (6)  A[0]:(0.000356033415301) A[1]:(0.811024367809) A[2]:(-0.00246953452006) A[3]:(0.655321121216)\n",
      " state (7)  A[0]:(0.640612781048) A[1]:(-0.132867217064) A[2]:(0.0323067493737) A[3]:(0.932147979736)\n",
      " state (8)  A[0]:(0.658164262772) A[1]:(0.00469071697444) A[2]:(0.728002488613) A[3]:(0.592331767082)\n",
      " state (9)  A[0]:(0.654955387115) A[1]:(0.809390902519) A[2]:(0.79393106699) A[3]:(-0.00194634811487)\n",
      " state (10)  A[0]:(0.730547189713) A[1]:(0.900738894939) A[2]:(-0.000248670578003) A[3]:(0.730741083622)\n",
      " state (11)  A[0]:(0.482821136713) A[1]:(0.859976530075) A[2]:(-0.421320170164) A[3]:(0.829890370369)\n",
      " state (12)  A[0]:(0.0293282326311) A[1]:(0.777394175529) A[2]:(-0.166798919439) A[3]:(0.778209984303)\n",
      " state (13)  A[0]:(0.00171337858774) A[1]:(0.771148264408) A[2]:(0.900320529938) A[3]:(0.727383375168)\n",
      " state (14)  A[0]:(0.81155705452) A[1]:(0.918084144592) A[2]:(0.99998819828) A[3]:(0.811347603798)\n",
      " state (15)  A[0]:(0.996640443802) A[1]:(0.989009857178) A[2]:(1.0) A[3]:(0.924652397633)\n",
      "Episode 206000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6225. Times reached goal: 908.               Steps done: 2365455. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0845159913602.\n",
      " state (0)  A[0]:(0.533383846283) A[1]:(0.590220332146) A[2]:(0.592289805412) A[3]:(0.532846808434)\n",
      " state (1)  A[0]:(0.533325433731) A[1]:(-0.00179967086297) A[2]:(0.656878829002) A[3]:(0.593250095844)\n",
      " state (2)  A[0]:(0.592705368996) A[1]:(0.72981107235) A[2]:(0.596203744411) A[3]:(0.657267332077)\n",
      " state (3)  A[0]:(0.658732712269) A[1]:(-0.023558517918) A[2]:(0.414012700319) A[3]:(0.550557732582)\n",
      " state (4)  A[0]:(0.591603398323) A[1]:(0.657192349434) A[2]:(0.0198416896164) A[3]:(0.535979986191)\n",
      " state (5)  A[0]:(0.214343592525) A[1]:(0.909990131855) A[2]:(-0.0669994056225) A[3]:(0.495337307453)\n",
      " state (6)  A[0]:(0.00114479614422) A[1]:(0.810739815235) A[2]:(-0.00522847194225) A[3]:(0.656808614731)\n",
      " state (7)  A[0]:(0.641147315502) A[1]:(-0.137457549572) A[2]:(0.0325152650476) A[3]:(0.932152509689)\n",
      " state (8)  A[0]:(0.657856345177) A[1]:(0.00349052320234) A[2]:(0.727839529514) A[3]:(0.593224704266)\n",
      " state (9)  A[0]:(0.654264330864) A[1]:(0.809311330318) A[2]:(0.794356107712) A[3]:(-0.00150459888391)\n",
      " state (10)  A[0]:(0.72977244854) A[1]:(0.900742769241) A[2]:(-0.00235318695195) A[3]:(0.731067538261)\n",
      " state (11)  A[0]:(0.481614679098) A[1]:(0.860614895821) A[2]:(-0.42579934001) A[3]:(0.830359637737)\n",
      " state (12)  A[0]:(0.0272689945996) A[1]:(0.779192328453) A[2]:(-0.172750234604) A[3]:(0.778760313988)\n",
      " state (13)  A[0]:(-0.000157698988914) A[1]:(0.772886812687) A[2]:(0.90059030056) A[3]:(0.727825164795)\n",
      " state (14)  A[0]:(0.811707377434) A[1]:(0.917941153049) A[2]:(0.999988794327) A[3]:(0.811514019966)\n",
      " state (15)  A[0]:(0.996612370014) A[1]:(0.988699376583) A[2]:(1.0) A[3]:(0.924293577671)\n",
      "Episode 207000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6296. Times reached goal: 934.               Steps done: 2371751. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0839855502594.\n",
      " state (0)  A[0]:(0.532450318336) A[1]:(0.591765999794) A[2]:(0.59133040905) A[3]:(0.531258702278)\n",
      " state (1)  A[0]:(0.532522201538) A[1]:(0.00194271409418) A[2]:(0.65711915493) A[3]:(0.591954410076)\n",
      " state (2)  A[0]:(0.591840386391) A[1]:(0.731294989586) A[2]:(0.596493244171) A[3]:(0.656902968884)\n",
      " state (3)  A[0]:(0.65848171711) A[1]:(-0.0195573437959) A[2]:(0.4174477458) A[3]:(0.549037694931)\n",
      " state (4)  A[0]:(0.592274725437) A[1]:(0.658466994762) A[2]:(0.0200583245605) A[3]:(0.535450220108)\n",
      " state (5)  A[0]:(0.214801132679) A[1]:(0.910942137241) A[2]:(-0.0691422298551) A[3]:(0.496334910393)\n",
      " state (6)  A[0]:(0.00189994042739) A[1]:(0.812425255775) A[2]:(-0.00561362551525) A[3]:(0.657509505749)\n",
      " state (7)  A[0]:(0.641479372978) A[1]:(-0.133883744478) A[2]:(0.0352932177484) A[3]:(0.931704223156)\n",
      " state (8)  A[0]:(0.658522725105) A[1]:(0.00937943253666) A[2]:(0.729227900505) A[3]:(0.593062162399)\n",
      " state (9)  A[0]:(0.655750930309) A[1]:(0.811156988144) A[2]:(0.796279668808) A[3]:(0.000152885913849)\n",
      " state (10)  A[0]:(0.73157697916) A[1]:(0.901831686497) A[2]:(-0.000117659568787) A[3]:(0.732596457005)\n",
      " state (11)  A[0]:(0.485415816307) A[1]:(0.862905085087) A[2]:(-0.426855355501) A[3]:(0.83180642128)\n",
      " state (12)  A[0]:(0.0317643247545) A[1]:(0.783679962158) A[2]:(-0.175869241357) A[3]:(0.780573666096)\n",
      " state (13)  A[0]:(0.00278662913479) A[1]:(0.777384519577) A[2]:(0.900568902493) A[3]:(0.72958278656)\n",
      " state (14)  A[0]:(0.812161564827) A[1]:(0.918808698654) A[2]:(0.99998909235) A[3]:(0.812416255474)\n",
      " state (15)  A[0]:(0.996577739716) A[1]:(0.988548696041) A[2]:(1.0) A[3]:(0.924296379089)\n",
      "Episode 208000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6290. Times reached goal: 924.               Steps done: 2378041. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0834589390767.\n",
      " state (0)  A[0]:(0.530154705048) A[1]:(0.590854883194) A[2]:(0.592969655991) A[3]:(0.530512213707)\n",
      " state (1)  A[0]:(0.531574249268) A[1]:(-0.000604160071816) A[2]:(0.65786921978) A[3]:(0.592304229736)\n",
      " state (2)  A[0]:(0.590698480606) A[1]:(0.729772090912) A[2]:(0.598738014698) A[3]:(0.65701597929)\n",
      " state (3)  A[0]:(0.65663844347) A[1]:(-0.022608069703) A[2]:(0.424547642469) A[3]:(0.547341585159)\n",
      " state (4)  A[0]:(0.590274512768) A[1]:(0.65709233284) A[2]:(0.0258141607046) A[3]:(0.53331387043)\n",
      " state (5)  A[0]:(0.210954472423) A[1]:(0.911062717438) A[2]:(-0.0652254968882) A[3]:(0.49434825778)\n",
      " state (6)  A[0]:(0.000966593332123) A[1]:(0.811250865459) A[2]:(-0.000628948153462) A[3]:(0.656705975533)\n",
      " state (7)  A[0]:(0.642385721207) A[1]:(-0.141316100955) A[2]:(0.0403471179307) A[3]:(0.931149244308)\n",
      " state (8)  A[0]:(0.658339202404) A[1]:(0.00792396627367) A[2]:(0.729027748108) A[3]:(0.592733025551)\n",
      " state (9)  A[0]:(0.655103564262) A[1]:(0.810892820358) A[2]:(0.796519160271) A[3]:(-0.000598482729401)\n",
      " state (10)  A[0]:(0.730321347713) A[1]:(0.9015609622) A[2]:(-9.83476638794e-05) A[3]:(0.730862617493)\n",
      " state (11)  A[0]:(0.483383536339) A[1]:(0.863151073456) A[2]:(-0.428664475679) A[3]:(0.830349862576)\n",
      " state (12)  A[0]:(0.0290287658572) A[1]:(0.785055875778) A[2]:(-0.179161131382) A[3]:(0.778459668159)\n",
      " state (13)  A[0]:(8.76188278198e-05) A[1]:(0.778887271881) A[2]:(0.90084451437) A[3]:(0.726782083511)\n",
      " state (14)  A[0]:(0.811669826508) A[1]:(0.918647825718) A[2]:(0.999989509583) A[3]:(0.810331463814)\n",
      " state (15)  A[0]:(0.996538698673) A[1]:(0.988254666328) A[2]:(1.0) A[3]:(0.923150002956)\n",
      "Episode 209000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6282. Times reached goal: 915.               Steps done: 2384323. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0829362933703.\n",
      "q_values \n",
      "tensor([[ 0.5321,  0.5901,  0.5903,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.0010,  0.6557,  0.5909]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5911,  0.7293,  0.5933,  0.6563]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0013,  0.8104, -0.0070,  0.6552]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7286,  0.9003, -0.0004,  0.7302]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8115,  0.9158,  1.0000,  0.8114]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532224535942) A[1]:(0.590437531471) A[2]:(0.590942502022) A[3]:(0.531827509403)\n",
      " state (1)  A[0]:(0.532036960125) A[1]:(0.00202364940196) A[2]:(0.656392753124) A[3]:(0.591312646866)\n",
      " state (2)  A[0]:(0.591353535652) A[1]:(0.729972302914) A[2]:(0.593997240067) A[3]:(0.656666517258)\n",
      " state (3)  A[0]:(0.656928777695) A[1]:(-0.0130416955799) A[2]:(0.415781021118) A[3]:(0.548094213009)\n",
      " state (4)  A[0]:(0.591641068459) A[1]:(0.654925644398) A[2]:(0.0135711440817) A[3]:(0.534138739109)\n",
      " state (5)  A[0]:(0.213662430644) A[1]:(0.910605430603) A[2]:(-0.0755583867431) A[3]:(0.495797455311)\n",
      " state (6)  A[0]:(0.000322967767715) A[1]:(0.811313807964) A[2]:(-0.00560683105141) A[3]:(0.655389308929)\n",
      " state (7)  A[0]:(0.638378739357) A[1]:(-0.14042288065) A[2]:(0.0412602871656) A[3]:(0.92972111702)\n",
      " state (8)  A[0]:(0.65550506115) A[1]:(0.00566489901394) A[2]:(0.730064153671) A[3]:(0.588861107826)\n",
      " state (9)  A[0]:(0.652439296246) A[1]:(0.809546291828) A[2]:(0.798266291618) A[3]:(-0.00581522425637)\n",
      " state (10)  A[0]:(0.729477286339) A[1]:(0.900829315186) A[2]:(0.00129055907018) A[3]:(0.730135321617)\n",
      " state (11)  A[0]:(0.48412540555) A[1]:(0.862530469894) A[2]:(-0.429997205734) A[3]:(0.830871701241)\n",
      " state (12)  A[0]:(0.0307938810438) A[1]:(0.784076690674) A[2]:(-0.181931197643) A[3]:(0.779614508152)\n",
      " state (13)  A[0]:(0.00133176066447) A[1]:(0.776356220245) A[2]:(0.900881528854) A[3]:(0.728322148323)\n",
      " state (14)  A[0]:(0.811916768551) A[1]:(0.916069090366) A[2]:(0.999989807606) A[3]:(0.811510622501)\n",
      " state (15)  A[0]:(0.996511816978) A[1]:(0.987467229366) A[2]:(1.0) A[3]:(0.923501014709)\n",
      "Episode 210000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6230. Times reached goal: 917.               Steps done: 2390553. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0824212064245.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532241344452) A[1]:(0.590402662754) A[2]:(0.589688539505) A[3]:(0.532734811306)\n",
      " state (1)  A[0]:(0.531806409359) A[1]:(0.000222407281399) A[2]:(0.65783393383) A[3]:(0.592177569866)\n",
      " state (2)  A[0]:(0.590368151665) A[1]:(0.729177832603) A[2]:(0.597229540348) A[3]:(0.656656861305)\n",
      " state (3)  A[0]:(0.656191587448) A[1]:(-0.0135399540886) A[2]:(0.422542452812) A[3]:(0.548422813416)\n",
      " state (4)  A[0]:(0.589957118034) A[1]:(0.656067252159) A[2]:(0.0193044990301) A[3]:(0.533730983734)\n",
      " state (5)  A[0]:(0.208381801844) A[1]:(0.911020517349) A[2]:(-0.0706096515059) A[3]:(0.494443118572)\n",
      " state (6)  A[0]:(-0.00152282300405) A[1]:(0.809655070305) A[2]:(7.83205032349e-05) A[3]:(0.655803024769)\n",
      " state (7)  A[0]:(0.639622330666) A[1]:(-0.15103545785) A[2]:(0.0449230894446) A[3]:(0.929942965508)\n",
      " state (8)  A[0]:(0.6555788517) A[1]:(0.00360322673805) A[2]:(0.727657914162) A[3]:(0.593671917915)\n",
      " state (9)  A[0]:(0.651232719421) A[1]:(0.810094892979) A[2]:(0.79820972681) A[3]:(-0.00516069959849)\n",
      " state (10)  A[0]:(0.727028787136) A[1]:(0.901139378548) A[2]:(0.0029364740476) A[3]:(0.727161407471)\n",
      " state (11)  A[0]:(0.479510873556) A[1]:(0.863588690758) A[2]:(-0.430568367243) A[3]:(0.828309953213)\n",
      " state (12)  A[0]:(0.0240891613066) A[1]:(0.786776781082) A[2]:(-0.184788882732) A[3]:(0.776022255421)\n",
      " state (13)  A[0]:(-0.00615571113303) A[1]:(0.779458940029) A[2]:(0.90080589056) A[3]:(0.723675131798)\n",
      " state (14)  A[0]:(0.809441268444) A[1]:(0.916721999645) A[2]:(0.999990105629) A[3]:(0.808021068573)\n",
      " state (15)  A[0]:(0.996434032917) A[1]:(0.987331330776) A[2]:(1.0) A[3]:(0.921776175499)\n",
      "Episode 211000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6226. Times reached goal: 908.               Steps done: 2396779. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0819096461329.\n",
      " state (0)  A[0]:(0.53007119894) A[1]:(0.58991932869) A[2]:(0.591465115547) A[3]:(0.53212416172)\n",
      " state (1)  A[0]:(0.531351804733) A[1]:(0.00048758086632) A[2]:(0.656026363373) A[3]:(0.592859864235)\n",
      " state (2)  A[0]:(0.589885115623) A[1]:(0.729269742966) A[2]:(0.594962239265) A[3]:(0.656955838203)\n",
      " state (3)  A[0]:(0.656091690063) A[1]:(-0.0209725890309) A[2]:(0.425155490637) A[3]:(0.549004554749)\n",
      " state (4)  A[0]:(0.590783238411) A[1]:(0.65526330471) A[2]:(0.0177256371826) A[3]:(0.535083651543)\n",
      " state (5)  A[0]:(0.208975508809) A[1]:(0.91149866581) A[2]:(-0.0765565559268) A[3]:(0.495931118727)\n",
      " state (6)  A[0]:(-0.000265166163445) A[1]:(0.809840798378) A[2]:(-0.00585930794477) A[3]:(0.656587779522)\n",
      " state (7)  A[0]:(0.640474915504) A[1]:(-0.15535582602) A[2]:(0.0419173054397) A[3]:(0.929523527622)\n",
      " state (8)  A[0]:(0.655652284622) A[1]:(-0.000205054879189) A[2]:(0.727600574493) A[3]:(0.592089533806)\n",
      " state (9)  A[0]:(0.652354776859) A[1]:(0.808616101742) A[2]:(0.799075484276) A[3]:(-0.00381363951601)\n",
      " state (10)  A[0]:(0.72923964262) A[1]:(0.900411248207) A[2]:(-0.000671505811624) A[3]:(0.730557024479)\n",
      " state (11)  A[0]:(0.484287619591) A[1]:(0.863150298595) A[2]:(-0.437622994184) A[3]:(0.831537663937)\n",
      " state (12)  A[0]:(0.0299907810986) A[1]:(0.78670579195) A[2]:(-0.194857880473) A[3]:(0.780219197273)\n",
      " state (13)  A[0]:(-0.00124959577806) A[1]:(0.779033184052) A[2]:(0.899782836437) A[3]:(0.728363633156)\n",
      " state (14)  A[0]:(0.810769855976) A[1]:(0.915604114532) A[2]:(0.999990344048) A[3]:(0.811140120029)\n",
      " state (15)  A[0]:(0.996409356594) A[1]:(0.986833274364) A[2]:(1.0) A[3]:(0.922726392746)\n",
      "Episode 212000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6206. Times reached goal: 917.               Steps done: 2402985. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.081402888963.\n",
      " state (0)  A[0]:(0.5314412117) A[1]:(0.590738832951) A[2]:(0.59109389782) A[3]:(0.531947135925)\n",
      " state (1)  A[0]:(0.531269431114) A[1]:(0.00053393834969) A[2]:(0.657384037971) A[3]:(0.591491043568)\n",
      " state (2)  A[0]:(0.59040415287) A[1]:(0.729670763016) A[2]:(0.593444943428) A[3]:(0.65710747242)\n",
      " state (3)  A[0]:(0.656825959682) A[1]:(-0.00881160330027) A[2]:(0.419833898544) A[3]:(0.548357069492)\n",
      " state (4)  A[0]:(0.591884374619) A[1]:(0.656290769577) A[2]:(0.0106606017798) A[3]:(0.534555673599)\n",
      " state (5)  A[0]:(0.209813863039) A[1]:(0.911615729332) A[2]:(-0.0803636461496) A[3]:(0.496745646)\n",
      " state (6)  A[0]:(-0.00233159540221) A[1]:(0.811050057411) A[2]:(-0.00453027943149) A[3]:(0.655920088291)\n",
      " state (7)  A[0]:(0.638471603394) A[1]:(-0.14951634407) A[2]:(0.047088701278) A[3]:(0.928821206093)\n",
      " state (8)  A[0]:(0.657377123833) A[1]:(0.00517184892669) A[2]:(0.728893876076) A[3]:(0.59399163723)\n",
      " state (9)  A[0]:(0.654544472694) A[1]:(0.810362160206) A[2]:(0.799992442131) A[3]:(0.000319007784128)\n",
      " state (10)  A[0]:(0.730631113052) A[1]:(0.9015635252) A[2]:(0.00135433592368) A[3]:(0.731678545475)\n",
      " state (11)  A[0]:(0.486862123013) A[1]:(0.865659415722) A[2]:(-0.436942577362) A[3]:(0.832212686539)\n",
      " state (12)  A[0]:(0.0329412519932) A[1]:(0.791663527489) A[2]:(-0.194802135229) A[3]:(0.780979335308)\n",
      " state (13)  A[0]:(0.000285357236862) A[1]:(0.784194946289) A[2]:(0.900472640991) A[3]:(0.728989005089)\n",
      " state (14)  A[0]:(0.8108522892) A[1]:(0.916880488396) A[2]:(0.999990701675) A[3]:(0.811401128769)\n",
      " state (15)  A[0]:(0.99636733532) A[1]:(0.986738264561) A[2]:(1.0) A[3]:(0.922543823719)\n",
      "Episode 213000 finished after 0 timesteps with r=0.0. Running score: 0.92. Times trained:               6267. Times reached goal: 940.               Steps done: 2409252. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0808943322847.\n",
      " state (0)  A[0]:(0.53048324585) A[1]:(0.591683387756) A[2]:(0.59088408947) A[3]:(0.531227350235)\n",
      " state (1)  A[0]:(0.530758619308) A[1]:(-0.00160023430362) A[2]:(0.656312465668) A[3]:(0.591400444508)\n",
      " state (2)  A[0]:(0.589108467102) A[1]:(0.729859888554) A[2]:(0.594942450523) A[3]:(0.656879127026)\n",
      " state (3)  A[0]:(0.654465913773) A[1]:(-0.0115989008918) A[2]:(0.428412437439) A[3]:(0.546371221542)\n",
      " state (4)  A[0]:(0.589332997799) A[1]:(0.655951738358) A[2]:(0.0174355711788) A[3]:(0.533325850964)\n",
      " state (5)  A[0]:(0.205781057477) A[1]:(0.912075102329) A[2]:(-0.0777919143438) A[3]:(0.496905863285)\n",
      " state (6)  A[0]:(-0.0027005309239) A[1]:(0.810138881207) A[2]:(-0.00223016366363) A[3]:(0.657228112221)\n",
      " state (7)  A[0]:(0.639171600342) A[1]:(-0.157212153077) A[2]:(0.050284974277) A[3]:(0.928546309471)\n",
      " state (8)  A[0]:(0.655241012573) A[1]:(0.00231426535174) A[2]:(0.729133248329) A[3]:(0.59208637476)\n",
      " state (9)  A[0]:(0.651768445969) A[1]:(0.809125363827) A[2]:(0.800394654274) A[3]:(-0.00282339006662)\n",
      " state (10)  A[0]:(0.728348016739) A[1]:(0.90042078495) A[2]:(-0.000859260326251) A[3]:(0.730280339718)\n",
      " state (11)  A[0]:(0.483904540539) A[1]:(0.863987565041) A[2]:(-0.441692531109) A[3]:(0.831601321697)\n",
      " state (12)  A[0]:(0.0298689398915) A[1]:(0.788878977299) A[2]:(-0.201724126935) A[3]:(0.780425190926)\n",
      " state (13)  A[0]:(-0.00147168233525) A[1]:(0.780120015144) A[2]:(0.900252580643) A[3]:(0.728639781475)\n",
      " state (14)  A[0]:(0.811157941818) A[1]:(0.913874745369) A[2]:(0.999991059303) A[3]:(0.81158208847)\n",
      " state (15)  A[0]:(0.996346175671) A[1]:(0.985823810101) A[2]:(1.0) A[3]:(0.922556936741)\n",
      "Episode 214000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6273. Times reached goal: 927.               Steps done: 2415525. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0803884704328.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5895,  0.5903,  0.5324]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5319, -0.0005,  0.6562,  0.5917]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.7296,  0.5932,  0.6573]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0025,  0.8108, -0.0031,  0.6570]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 7.3080e-01,  9.0156e-01,  8.3447e-07,  7.3046e-01]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8126,  0.9147,  1.0000,  0.8107]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7299,  0.9014, -0.0006,  0.7305]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8115,  0.9146,  1.0000,  0.8104]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531805276871) A[1]:(0.589647650719) A[2]:(0.590809822083) A[3]:(0.531573534012)\n",
      " state (1)  A[0]:(0.53182554245) A[1]:(-0.000675149145536) A[2]:(0.656471610069) A[3]:(0.590857505798)\n",
      " state (2)  A[0]:(0.59009039402) A[1]:(0.728915691376) A[2]:(0.593383193016) A[3]:(0.656556844711)\n",
      " state (3)  A[0]:(0.655138671398) A[1]:(-0.0164454728365) A[2]:(0.427079737186) A[3]:(0.545127093792)\n",
      " state (4)  A[0]:(0.590123832226) A[1]:(0.655748605728) A[2]:(0.0128936776891) A[3]:(0.532776236534)\n",
      " state (5)  A[0]:(0.205784127116) A[1]:(0.912149250507) A[2]:(-0.0819211602211) A[3]:(0.496680766344)\n",
      " state (6)  A[0]:(-0.00262016942725) A[1]:(0.809748411179) A[2]:(-0.00363086047582) A[3]:(0.656206488609)\n",
      " state (7)  A[0]:(0.638787329197) A[1]:(-0.159277513623) A[2]:(0.0512171648443) A[3]:(0.927467465401)\n",
      " state (8)  A[0]:(0.6552272439) A[1]:(0.00379196228459) A[2]:(0.728514790535) A[3]:(0.590484857559)\n",
      " state (9)  A[0]:(0.651771664619) A[1]:(0.810005784035) A[2]:(0.800653457642) A[3]:(-0.002845734125)\n",
      " state (10)  A[0]:(0.727855443954) A[1]:(0.900977373123) A[2]:(-0.000940203375649) A[3]:(0.729742407799)\n",
      " state (11)  A[0]:(0.483020603657) A[1]:(0.865397036076) A[2]:(-0.443326383829) A[3]:(0.83108818531)\n",
      " state (12)  A[0]:(0.0281923338771) A[1]:(0.791902899742) A[2]:(-0.204134956002) A[3]:(0.779443740845)\n",
      " state (13)  A[0]:(-0.00358928693458) A[1]:(0.783340334892) A[2]:(0.900981009007) A[3]:(0.7270180583)\n",
      " state (14)  A[0]:(0.81064593792) A[1]:(0.914515793324) A[2]:(0.999991476536) A[3]:(0.810188531876)\n",
      " state (15)  A[0]:(0.996290922165) A[1]:(0.985622525215) A[2]:(1.0) A[3]:(0.921563148499)\n",
      "Episode 215000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6271. Times reached goal: 930.               Steps done: 2421796. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0798859316918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.533546686172) A[1]:(0.589495837688) A[2]:(0.591563820839) A[3]:(0.53317284584)\n",
      " state (1)  A[0]:(0.53350764513) A[1]:(-0.00109655363485) A[2]:(0.656650424004) A[3]:(0.591416358948)\n",
      " state (2)  A[0]:(0.59160643816) A[1]:(0.729385733604) A[2]:(0.594235360622) A[3]:(0.656803905964)\n",
      " state (3)  A[0]:(0.656038284302) A[1]:(-0.015805484727) A[2]:(0.430008172989) A[3]:(0.545545458794)\n",
      " state (4)  A[0]:(0.591846346855) A[1]:(0.657415091991) A[2]:(0.0136537412181) A[3]:(0.534039974213)\n",
      " state (5)  A[0]:(0.208670675755) A[1]:(0.912996292114) A[2]:(-0.0821633040905) A[3]:(0.498651862144)\n",
      " state (6)  A[0]:(0.000605881155934) A[1]:(0.811109662056) A[2]:(-0.00215088995174) A[3]:(0.656772613525)\n",
      " state (7)  A[0]:(0.640703141689) A[1]:(-0.158495828509) A[2]:(0.0546342357993) A[3]:(0.926872134209)\n",
      " state (8)  A[0]:(0.658289194107) A[1]:(0.00453712651506) A[2]:(0.729118108749) A[3]:(0.590915501118)\n",
      " state (9)  A[0]:(0.655089259148) A[1]:(0.809819459915) A[2]:(0.801664829254) A[3]:(-0.00116869376507)\n",
      " state (10)  A[0]:(0.730435490608) A[1]:(0.900418639183) A[2]:(-0.000697851064615) A[3]:(0.730671346188)\n",
      " state (11)  A[0]:(0.487539499998) A[1]:(0.864581942558) A[2]:(-0.44577524066) A[3]:(0.832084357738)\n",
      " state (12)  A[0]:(0.0339551083744) A[1]:(0.790552675724) A[2]:(-0.208474829793) A[3]:(0.780969202518)\n",
      " state (13)  A[0]:(0.0021249470301) A[1]:(0.780961275101) A[2]:(0.901110589504) A[3]:(0.729073643684)\n",
      " state (14)  A[0]:(0.81298494339) A[1]:(0.912350893021) A[2]:(0.999991834164) A[3]:(0.811962604523)\n",
      " state (15)  A[0]:(0.996305882931) A[1]:(0.984854221344) A[2]:(1.0) A[3]:(0.922198295593)\n",
      "Episode 216000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6264. Times reached goal: 936.               Steps done: 2428060. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0793870902183.\n",
      " state (0)  A[0]:(0.532778322697) A[1]:(0.590255260468) A[2]:(0.589524447918) A[3]:(0.531831145287)\n",
      " state (1)  A[0]:(0.533026218414) A[1]:(-0.0004518776841) A[2]:(0.655510425568) A[3]:(0.592060089111)\n",
      " state (2)  A[0]:(0.590711236) A[1]:(0.728665709496) A[2]:(0.593359947205) A[3]:(0.657818198204)\n",
      " state (3)  A[0]:(0.65583384037) A[1]:(-0.0245135631412) A[2]:(0.431787192822) A[3]:(0.546425402164)\n",
      " state (4)  A[0]:(0.592094957829) A[1]:(0.655016362667) A[2]:(0.0113743403926) A[3]:(0.536089301109)\n",
      " state (5)  A[0]:(0.207870662212) A[1]:(0.912727415562) A[2]:(-0.0870712697506) A[3]:(0.501582205296)\n",
      " state (6)  A[0]:(-0.000254794955254) A[1]:(0.809957146645) A[2]:(-0.00600867206231) A[3]:(0.658978819847)\n",
      " state (7)  A[0]:(0.639005899429) A[1]:(-0.163601726294) A[2]:(0.0534525178373) A[3]:(0.926856637001)\n",
      " state (8)  A[0]:(0.655534863472) A[1]:(0.00176574103534) A[2]:(0.728336334229) A[3]:(0.592376172543)\n",
      " state (9)  A[0]:(0.652443110943) A[1]:(0.809109807014) A[2]:(0.801990032196) A[3]:(0.00108241615817)\n",
      " state (10)  A[0]:(0.729089021683) A[1]:(0.90025895834) A[2]:(-0.00332676130347) A[3]:(0.732436299324)\n",
      " state (11)  A[0]:(0.486549675465) A[1]:(0.865112900734) A[2]:(-0.451136320829) A[3]:(0.83371090889)\n",
      " state (12)  A[0]:(0.0325416587293) A[1]:(0.792277812958) A[2]:(-0.216910734773) A[3]:(0.782937705517)\n",
      " state (13)  A[0]:(-0.000774890009779) A[1]:(0.78281211853) A[2]:(0.900003552437) A[3]:(0.730877995491)\n",
      " state (14)  A[0]:(0.811230063438) A[1]:(0.912395596504) A[2]:(0.999992012978) A[3]:(0.812752842903)\n",
      " state (15)  A[0]:(0.996212184429) A[1]:(0.98456376791) A[2]:(1.0) A[3]:(0.922087669373)\n",
      "Episode 217000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6183. Times reached goal: 924.               Steps done: 2434243. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0788977541807.\n",
      " state (0)  A[0]:(0.531512618065) A[1]:(0.590140104294) A[2]:(0.591745913029) A[3]:(0.53182965517)\n",
      " state (1)  A[0]:(0.531862974167) A[1]:(-0.000118542462587) A[2]:(0.657310783863) A[3]:(0.590830266476)\n",
      " state (2)  A[0]:(0.589940071106) A[1]:(0.729235768318) A[2]:(0.594241559505) A[3]:(0.65668118)\n",
      " state (3)  A[0]:(0.654728889465) A[1]:(-0.0132828475907) A[2]:(0.432429075241) A[3]:(0.544510900974)\n",
      " state (4)  A[0]:(0.590795636177) A[1]:(0.655598163605) A[2]:(0.0118852024898) A[3]:(0.533091783524)\n",
      " state (5)  A[0]:(0.205479621887) A[1]:(0.912971377373) A[2]:(-0.0855113193393) A[3]:(0.498660802841)\n",
      " state (6)  A[0]:(-0.00340585573576) A[1]:(0.81065505743) A[2]:(-0.00135624327231) A[3]:(0.655790686607)\n",
      " state (7)  A[0]:(0.637367963791) A[1]:(-0.161839082837) A[2]:(0.059890512377) A[3]:(0.925605356693)\n",
      " state (8)  A[0]:(0.656764924526) A[1]:(0.00347891170532) A[2]:(0.729052603245) A[3]:(0.592619776726)\n",
      " state (9)  A[0]:(0.653698682785) A[1]:(0.809557199478) A[2]:(0.803504347801) A[3]:(0.000210586935282)\n",
      " state (10)  A[0]:(0.729508876801) A[1]:(0.900436401367) A[2]:(0.00131666578818) A[3]:(0.73065841198)\n",
      " state (11)  A[0]:(0.487437307835) A[1]:(0.865731835365) A[2]:(-0.449119150639) A[3]:(0.832475423813)\n",
      " state (12)  A[0]:(0.0332927852869) A[1]:(0.793534636497) A[2]:(-0.216886758804) A[3]:(0.781324028969)\n",
      " state (13)  A[0]:(-0.00179007463157) A[1]:(0.783311367035) A[2]:(0.899958252907) A[3]:(0.728727698326)\n",
      " state (14)  A[0]:(0.810133159161) A[1]:(0.911379039288) A[2]:(0.999992132187) A[3]:(0.811059653759)\n",
      " state (15)  A[0]:(0.996152997017) A[1]:(0.983999371529) A[2]:(1.0) A[3]:(0.921204268932)\n",
      "Episode 218000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6169. Times reached goal: 922.               Steps done: 2440412. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0784125321414.\n",
      " state (0)  A[0]:(0.53157299757) A[1]:(0.590202093124) A[2]:(0.590224385262) A[3]:(0.531559586525)\n",
      " state (1)  A[0]:(0.532412052155) A[1]:(0.00145900889765) A[2]:(0.656914234161) A[3]:(0.590820670128)\n",
      " state (2)  A[0]:(0.590738415718) A[1]:(0.72988820076) A[2]:(0.594362556934) A[3]:(0.656621098518)\n",
      " state (3)  A[0]:(0.656170368195) A[1]:(-0.0230563171208) A[2]:(0.435445249081) A[3]:(0.543137967587)\n",
      " state (4)  A[0]:(0.593531787395) A[1]:(0.655970096588) A[2]:(0.0113104525954) A[3]:(0.533042669296)\n",
      " state (5)  A[0]:(0.209300845861) A[1]:(0.913537085056) A[2]:(-0.087872967124) A[3]:(0.499391227961)\n",
      " state (6)  A[0]:(0.000299125909805) A[1]:(0.811497986317) A[2]:(-0.00240754615515) A[3]:(0.656139910221)\n",
      " state (7)  A[0]:(0.63856357336) A[1]:(-0.161537885666) A[2]:(0.0616454444826) A[3]:(0.92518645525)\n",
      " state (8)  A[0]:(0.658118486404) A[1]:(0.00536917196587) A[2]:(0.729876041412) A[3]:(0.592603087425)\n",
      " state (9)  A[0]:(0.655940294266) A[1]:(0.810380518436) A[2]:(0.804710149765) A[3]:(0.00144517316949)\n",
      " state (10)  A[0]:(0.731503486633) A[1]:(0.90091586113) A[2]:(0.00144815340172) A[3]:(0.731769442558)\n",
      " state (11)  A[0]:(0.490594923496) A[1]:(0.866814374924) A[2]:(-0.451809346676) A[3]:(0.833447217941)\n",
      " state (12)  A[0]:(0.0363431163132) A[1]:(0.795735478401) A[2]:(-0.221185073256) A[3]:(0.782364070415)\n",
      " state (13)  A[0]:(0.000219017267227) A[1]:(0.785559654236) A[2]:(0.900313436985) A[3]:(0.729535698891)\n",
      " state (14)  A[0]:(0.810811936855) A[1]:(0.911700129509) A[2]:(0.999992489815) A[3]:(0.811362862587)\n",
      " state (15)  A[0]:(0.996114373207) A[1]:(0.983743369579) A[2]:(1.0) A[3]:(0.920884907246)\n",
      "Episode 219000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6243. Times reached goal: 921.               Steps done: 2446655. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0779245275944.\n",
      "q_values \n",
      "tensor([[ 0.5304,  0.5894,  0.5904,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5311, -0.0011,  0.6554,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5898,  0.7295,  0.5904,  0.6564]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0008,  0.8107, -0.0039,  0.6547]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7299,  0.9008, -0.0004,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8121,  0.9098,  1.0000,  0.8106]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531540453434) A[1]:(0.589828908443) A[2]:(0.591777443886) A[3]:(0.532203137875)\n",
      " state (1)  A[0]:(0.532043397427) A[1]:(-0.000551056058612) A[2]:(0.656197309494) A[3]:(0.591035723686)\n",
      " state (2)  A[0]:(0.590500116348) A[1]:(0.729385018349) A[2]:(0.590791225433) A[3]:(0.656862139702)\n",
      " state (3)  A[0]:(0.654916584492) A[1]:(-0.00798623822629) A[2]:(0.429796218872) A[3]:(0.544790804386)\n",
      " state (4)  A[0]:(0.591550350189) A[1]:(0.655311703682) A[2]:(0.00544435344636) A[3]:(0.533016204834)\n",
      " state (5)  A[0]:(0.206496849656) A[1]:(0.913183569908) A[2]:(-0.0924749746919) A[3]:(0.499076068401)\n",
      " state (6)  A[0]:(-0.000558972300496) A[1]:(0.809944033623) A[2]:(-0.00435123080388) A[3]:(0.655668497086)\n",
      " state (7)  A[0]:(0.638031363487) A[1]:(-0.16756272316) A[2]:(0.0616671778262) A[3]:(0.924422740936)\n",
      " state (8)  A[0]:(0.656652569771) A[1]:(0.00331009086221) A[2]:(0.728839635849) A[3]:(0.589890122414)\n",
      " state (9)  A[0]:(0.653784275055) A[1]:(0.809801518917) A[2]:(0.804037213326) A[3]:(-0.00252183736302)\n",
      " state (10)  A[0]:(0.729573130608) A[1]:(0.900348603725) A[2]:(-0.0012458555866) A[3]:(0.729556679726)\n",
      " state (11)  A[0]:(0.4885828197) A[1]:(0.866144657135) A[2]:(-0.455145150423) A[3]:(0.83215624094)\n",
      " state (12)  A[0]:(0.0351218022406) A[1]:(0.794672369957) A[2]:(-0.225687742233) A[3]:(0.780984759331)\n",
      " state (13)  A[0]:(0.000456944078906) A[1]:(0.783424437046) A[2]:(0.900445640087) A[3]:(0.728251814842)\n",
      " state (14)  A[0]:(0.811768352985) A[1]:(0.909535944462) A[2]:(0.999992787838) A[3]:(0.810894906521)\n",
      " state (15)  A[0]:(0.996110916138) A[1]:(0.982891798019) A[2]:(1.0) A[3]:(0.920651614666)\n",
      "Episode 220000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6216. Times reached goal: 934.               Steps done: 2452871. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.077441651066.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530046224594) A[1]:(0.589998602867) A[2]:(0.590445637703) A[3]:(0.531916975975)\n",
      " state (1)  A[0]:(0.530296564102) A[1]:(0.000229883939028) A[2]:(0.656821727753) A[3]:(0.590892314911)\n",
      " state (2)  A[0]:(0.588047146797) A[1]:(0.728731870651) A[2]:(0.594328284264) A[3]:(0.656111121178)\n",
      " state (3)  A[0]:(0.652980864048) A[1]:(-0.0213772971183) A[2]:(0.439300954342) A[3]:(0.543967247009)\n",
      " state (4)  A[0]:(0.58962905407) A[1]:(0.654479026794) A[2]:(0.0140116577968) A[3]:(0.533481836319)\n",
      " state (5)  A[0]:(0.203094169497) A[1]:(0.913643658161) A[2]:(-0.0863206908107) A[3]:(0.49979364872)\n",
      " state (6)  A[0]:(-0.00157132616732) A[1]:(0.809848725796) A[2]:(0.00126683642156) A[3]:(0.65662419796)\n",
      " state (7)  A[0]:(0.637747526169) A[1]:(-0.172280490398) A[2]:(0.0657001659274) A[3]:(0.924341022968)\n",
      " state (8)  A[0]:(0.655507266521) A[1]:(0.00096355722053) A[2]:(0.72794508934) A[3]:(0.593717396259)\n",
      " state (9)  A[0]:(0.651883244514) A[1]:(0.809301793575) A[2]:(0.804958939552) A[3]:(0.00110797537491)\n",
      " state (10)  A[0]:(0.727824032307) A[1]:(0.900150358677) A[2]:(0.00028133392334) A[3]:(0.730271458626)\n",
      " state (11)  A[0]:(0.486151963472) A[1]:(0.866365373135) A[2]:(-0.456987261772) A[3]:(0.832650661469)\n",
      " state (12)  A[0]:(0.031687092036) A[1]:(0.795692205429) A[2]:(-0.229839488864) A[3]:(0.781354546547)\n",
      " state (13)  A[0]:(-0.00340357609093) A[1]:(0.784587144852) A[2]:(0.900622189045) A[3]:(0.72823548317)\n",
      " state (14)  A[0]:(0.810530781746) A[1]:(0.909462928772) A[2]:(0.999993145466) A[3]:(0.810645520687)\n",
      " state (15)  A[0]:(0.996030390263) A[1]:(0.982567667961) A[2]:(1.0) A[3]:(0.920138061047)\n",
      "Episode 221000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6235. Times reached goal: 916.               Steps done: 2459106. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0769603045288.\n",
      " state (0)  A[0]:(0.531575381756) A[1]:(0.589880526066) A[2]:(0.589826583862) A[3]:(0.531189799309)\n",
      " state (1)  A[0]:(0.531894564629) A[1]:(0.000410798907978) A[2]:(0.655983328819) A[3]:(0.590323388577)\n",
      " state (2)  A[0]:(0.589879274368) A[1]:(0.729032278061) A[2]:(0.590844035149) A[3]:(0.656098246574)\n",
      " state (3)  A[0]:(0.655001759529) A[1]:(-0.0172801055014) A[2]:(0.43281930685) A[3]:(0.5425106287)\n",
      " state (4)  A[0]:(0.591379404068) A[1]:(0.65535402298) A[2]:(0.0048516606912) A[3]:(0.531456589699)\n",
      " state (5)  A[0]:(0.203787878156) A[1]:(0.913714230061) A[2]:(-0.0937915667892) A[3]:(0.498011648655)\n",
      " state (6)  A[0]:(-0.00173418049235) A[1]:(0.809808135033) A[2]:(-0.00340937240981) A[3]:(0.654835939407)\n",
      " state (7)  A[0]:(0.637631297112) A[1]:(-0.172598034143) A[2]:(0.0639495775104) A[3]:(0.923367083073)\n",
      " state (8)  A[0]:(0.656507492065) A[1]:(0.00273108179681) A[2]:(0.727438211441) A[3]:(0.59122812748)\n",
      " state (9)  A[0]:(0.653395354748) A[1]:(0.810036420822) A[2]:(0.805189192295) A[3]:(-0.000863842433318)\n",
      " state (10)  A[0]:(0.728929877281) A[1]:(0.90075981617) A[2]:(-0.000247120857239) A[3]:(0.729399204254)\n",
      " state (11)  A[0]:(0.488094687462) A[1]:(0.868073999882) A[2]:(-0.459119230509) A[3]:(0.832175970078)\n",
      " state (12)  A[0]:(0.0335221849382) A[1]:(0.799534499645) A[2]:(-0.233685046434) A[3]:(0.780630230904)\n",
      " state (13)  A[0]:(-0.00322614982724) A[1]:(0.789223134518) A[2]:(0.900543987751) A[3]:(0.727004289627)\n",
      " state (14)  A[0]:(0.809951424599) A[1]:(0.911120951176) A[2]:(0.99999332428) A[3]:(0.80953425169)\n",
      " state (15)  A[0]:(0.995959281921) A[1]:(0.982641875744) A[2]:(1.0) A[3]:(0.919320702553)\n",
      "Episode 222000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6210. Times reached goal: 915.               Steps done: 2465316. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0764838619231.\n",
      " state (0)  A[0]:(0.531229734421) A[1]:(0.591075539589) A[2]:(0.590740442276) A[3]:(0.531195521355)\n",
      " state (1)  A[0]:(0.531776785851) A[1]:(0.000644359621219) A[2]:(0.656971335411) A[3]:(0.590388178825)\n",
      " state (2)  A[0]:(0.589994311333) A[1]:(0.729432821274) A[2]:(0.591756701469) A[3]:(0.656159520149)\n",
      " state (3)  A[0]:(0.655908823013) A[1]:(-0.0110786156729) A[2]:(0.432346999645) A[3]:(0.543836355209)\n",
      " state (4)  A[0]:(0.592801332474) A[1]:(0.655548334122) A[2]:(0.00330709200352) A[3]:(0.53249257803)\n",
      " state (5)  A[0]:(0.205438733101) A[1]:(0.914012491703) A[2]:(-0.0949686691165) A[3]:(0.499133080244)\n",
      " state (6)  A[0]:(-0.00122873426881) A[1]:(0.810555636883) A[2]:(-0.00285898870789) A[3]:(0.655177175999)\n",
      " state (7)  A[0]:(0.636700868607) A[1]:(-0.172500178218) A[2]:(0.0665688440204) A[3]:(0.923067092896)\n",
      " state (8)  A[0]:(0.65628695488) A[1]:(0.00253182486631) A[2]:(0.728967666626) A[3]:(0.590511083603)\n",
      " state (9)  A[0]:(0.653976082802) A[1]:(0.809814929962) A[2]:(0.806684613228) A[3]:(-0.00257051922381)\n",
      " state (10)  A[0]:(0.730007648468) A[1]:(0.900580406189) A[2]:(0.000168442726135) A[3]:(0.729414343834)\n",
      " state (11)  A[0]:(0.490741759539) A[1]:(0.868118286133) A[2]:(-0.461795151234) A[3]:(0.832711338997)\n",
      " state (12)  A[0]:(0.0370792709291) A[1]:(0.799788832664) A[2]:(-0.238470628858) A[3]:(0.781480669975)\n",
      " state (13)  A[0]:(1.05649232864e-05) A[1]:(0.7888225317) A[2]:(0.90053743124) A[3]:(0.727981925011)\n",
      " state (14)  A[0]:(0.81109982729) A[1]:(0.909888505936) A[2]:(0.999993562698) A[3]:(0.810210347176)\n",
      " state (15)  A[0]:(0.995935857296) A[1]:(0.98197132349) A[2]:(1.0) A[3]:(0.919308125973)\n",
      "Episode 223000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6203. Times reached goal: 923.               Steps done: 2471519. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0760109009326.\n",
      " state (0)  A[0]:(0.529867887497) A[1]:(0.589488863945) A[2]:(0.590355873108) A[3]:(0.531214594841)\n",
      " state (1)  A[0]:(0.530712485313) A[1]:(-0.00165863183793) A[2]:(0.655749380589) A[3]:(0.590906739235)\n",
      " state (2)  A[0]:(0.588754892349) A[1]:(0.728677928448) A[2]:(0.591149926186) A[3]:(0.656235933304)\n",
      " state (3)  A[0]:(0.653661489487) A[1]:(-0.011026635766) A[2]:(0.43292042613) A[3]:(0.544061899185)\n",
      " state (4)  A[0]:(0.589533567429) A[1]:(0.655483543873) A[2]:(0.00432333629578) A[3]:(0.532059550285)\n",
      " state (5)  A[0]:(0.200600922108) A[1]:(0.914339661598) A[2]:(-0.0936700999737) A[3]:(0.498602360487)\n",
      " state (6)  A[0]:(-0.00197557848878) A[1]:(0.810081243515) A[2]:(-0.00139570143074) A[3]:(0.655835032463)\n",
      " state (7)  A[0]:(0.637318253517) A[1]:(-0.177275627851) A[2]:(0.0670582726598) A[3]:(0.92291456461)\n",
      " state (8)  A[0]:(0.655040323734) A[1]:(0.00311285746284) A[2]:(0.728494167328) A[3]:(0.589198887348)\n",
      " state (9)  A[0]:(0.652509689331) A[1]:(0.810038745403) A[2]:(0.806694984436) A[3]:(-0.00376045424491)\n",
      " state (10)  A[0]:(0.728653073311) A[1]:(0.900585353374) A[2]:(-0.00146520033013) A[3]:(0.728650569916)\n",
      " state (11)  A[0]:(0.48911434412) A[1]:(0.86858099699) A[2]:(-0.465353101492) A[3]:(0.832277655602)\n",
      " state (12)  A[0]:(0.0353323742747) A[1]:(0.801151454449) A[2]:(-0.244289010763) A[3]:(0.780937254429)\n",
      " state (13)  A[0]:(-0.00146748020779) A[1]:(0.790210664272) A[2]:(0.90035456419) A[3]:(0.727319359779)\n",
      " state (14)  A[0]:(0.810919463634) A[1]:(0.909753203392) A[2]:(0.999993860722) A[3]:(0.809896111488)\n",
      " state (15)  A[0]:(0.995885193348) A[1]:(0.981569826603) A[2]:(1.0) A[3]:(0.918968260288)\n",
      "Episode 224000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6194. Times reached goal: 926.               Steps done: 2477713. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0755415445097.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5891,  0.5900,  0.5324]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5322,  0.0001,  0.6554,  0.5913]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7285,  0.5907,  0.6563]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0008,  0.8102,  0.0010,  0.6558]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7303,  0.9007, -0.0002,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8108,  0.9100,  1.0000,  0.8094]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532067716122) A[1]:(0.588410019875) A[2]:(0.590428709984) A[3]:(0.532407045364)\n",
      " state (1)  A[0]:(0.532714188099) A[1]:(-0.000204630196095) A[2]:(0.655723869801) A[3]:(0.591157913208)\n",
      " state (2)  A[0]:(0.590750932693) A[1]:(0.728461384773) A[2]:(0.59050488472) A[3]:(0.656106054783)\n",
      " state (3)  A[0]:(0.654794692993) A[1]:(-0.016466865316) A[2]:(0.433575809002) A[3]:(0.5440710783)\n",
      " state (4)  A[0]:(0.590496063232) A[1]:(0.655887246132) A[2]:(0.00450023962185) A[3]:(0.53228533268)\n",
      " state (5)  A[0]:(0.20141659677) A[1]:(0.914672017097) A[2]:(-0.0930701494217) A[3]:(0.498396128416)\n",
      " state (6)  A[0]:(-0.00083181244554) A[1]:(0.810402870178) A[2]:(-0.000393152207835) A[3]:(0.655324816704)\n",
      " state (7)  A[0]:(0.638119697571) A[1]:(-0.179747626185) A[2]:(0.0674361139536) A[3]:(0.922576904297)\n",
      " state (8)  A[0]:(0.656438946724) A[1]:(0.00153564545326) A[2]:(0.727725684643) A[3]:(0.590245723724)\n",
      " state (9)  A[0]:(0.653573274612) A[1]:(0.809822559357) A[2]:(0.807202100754) A[3]:(-0.0033596851863)\n",
      " state (10)  A[0]:(0.7291431427) A[1]:(0.900578796864) A[2]:(-0.00161647656932) A[3]:(0.728462338448)\n",
      " state (11)  A[0]:(0.489990383387) A[1]:(0.869144916534) A[2]:(-0.468244343996) A[3]:(0.832280099392)\n",
      " state (12)  A[0]:(0.0358409620821) A[1]:(0.802760839462) A[2]:(-0.249982118607) A[3]:(0.780825793743)\n",
      " state (13)  A[0]:(-0.00228520832025) A[1]:(0.791967213154) A[2]:(0.899896681309) A[3]:(0.726806640625)\n",
      " state (14)  A[0]:(0.810176849365) A[1]:(0.909858226776) A[2]:(0.999994039536) A[3]:(0.809234023094)\n",
      " state (15)  A[0]:(0.995807170868) A[1]:(0.981248140335) A[2]:(1.0) A[3]:(0.918262898922)\n",
      "Episode 225000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6252. Times reached goal: 938.               Steps done: 2483965. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0750707320666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531977772713) A[1]:(0.590284943581) A[2]:(0.590889573097) A[3]:(0.532219171524)\n",
      " state (1)  A[0]:(0.532339572906) A[1]:(0.000581946165767) A[2]:(0.65722990036) A[3]:(0.591815233231)\n",
      " state (2)  A[0]:(0.590612888336) A[1]:(0.729533672333) A[2]:(0.59265434742) A[3]:(0.657105624676)\n",
      " state (3)  A[0]:(0.655892252922) A[1]:(-0.01777122356) A[2]:(0.434748262167) A[3]:(0.545217335224)\n",
      " state (4)  A[0]:(0.592108011246) A[1]:(0.656427979469) A[2]:(0.0039638071321) A[3]:(0.534006595612)\n",
      " state (5)  A[0]:(0.2028940171) A[1]:(0.914956569672) A[2]:(-0.0936384275556) A[3]:(0.500714719296)\n",
      " state (6)  A[0]:(-0.000145584344864) A[1]:(0.811009764671) A[2]:(-0.00054740899941) A[3]:(0.657344937325)\n",
      " state (7)  A[0]:(0.638072729111) A[1]:(-0.178950503469) A[2]:(0.0684760436416) A[3]:(0.922982215881)\n",
      " state (8)  A[0]:(0.657165408134) A[1]:(0.00478662969545) A[2]:(0.729734659195) A[3]:(0.590673983097)\n",
      " state (9)  A[0]:(0.65555280447) A[1]:(0.810529410839) A[2]:(0.808536171913) A[3]:(-0.000638462544885)\n",
      " state (10)  A[0]:(0.730509102345) A[1]:(0.900531291962) A[2]:(8.52346420288e-05) A[3]:(0.729800701141)\n",
      " state (11)  A[0]:(0.492212772369) A[1]:(0.869084656239) A[2]:(-0.468785375357) A[3]:(0.833159089088)\n",
      " state (12)  A[0]:(0.0387958362699) A[1]:(0.802627444267) A[2]:(-0.251921027899) A[3]:(0.782053828239)\n",
      " state (13)  A[0]:(0.000870764022693) A[1]:(0.79093837738) A[2]:(0.900560200214) A[3]:(0.728501081467)\n",
      " state (14)  A[0]:(0.81158554554) A[1]:(0.908154129982) A[2]:(0.999994337559) A[3]:(0.810750365257)\n",
      " state (15)  A[0]:(0.995792388916) A[1]:(0.980375170708) A[2]:(1.0) A[3]:(0.918771386147)\n",
      "Episode 226000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6200. Times reached goal: 938.               Steps done: 2490165. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.07460673341.\n",
      " state (0)  A[0]:(0.531983613968) A[1]:(0.590989947319) A[2]:(0.591037333012) A[3]:(0.532147109509)\n",
      " state (1)  A[0]:(0.532484412193) A[1]:(-0.000860687112436) A[2]:(0.656207978725) A[3]:(0.591148376465)\n",
      " state (2)  A[0]:(0.591300368309) A[1]:(0.729263186455) A[2]:(0.591858744621) A[3]:(0.656123518944)\n",
      " state (3)  A[0]:(0.656436443329) A[1]:(-0.0410732068121) A[2]:(0.436621457338) A[3]:(0.541318893433)\n",
      " state (4)  A[0]:(0.593176484108) A[1]:(0.656801700592) A[2]:(0.00369797460735) A[3]:(0.531932830811)\n",
      " state (5)  A[0]:(0.20464797318) A[1]:(0.91525888443) A[2]:(-0.0931762680411) A[3]:(0.498943567276)\n",
      " state (6)  A[0]:(0.00310799968429) A[1]:(0.811255633831) A[2]:(0.000243544578552) A[3]:(0.656491041183)\n",
      " state (7)  A[0]:(0.64005613327) A[1]:(-0.180403068662) A[2]:(0.0688323378563) A[3]:(0.922768175602)\n",
      " state (8)  A[0]:(0.658589720726) A[1]:(0.00498660607263) A[2]:(0.729451060295) A[3]:(0.590952038765)\n",
      " state (9)  A[0]:(0.656143069267) A[1]:(0.811039447784) A[2]:(0.809315979481) A[3]:(-0.00252318591811)\n",
      " state (10)  A[0]:(0.731200158596) A[1]:(0.901139557362) A[2]:(-0.000347137451172) A[3]:(0.729182600975)\n",
      " state (11)  A[0]:(0.494335740805) A[1]:(0.87069606781) A[2]:(-0.472205251455) A[3]:(0.833208203316)\n",
      " state (12)  A[0]:(0.041798543185) A[1]:(0.806096971035) A[2]:(-0.257850944996) A[3]:(0.782169282436)\n",
      " state (13)  A[0]:(0.00342996558174) A[1]:(0.79511475563) A[2]:(0.900365948677) A[3]:(0.728323101997)\n",
      " state (14)  A[0]:(0.81237000227) A[1]:(0.909698009491) A[2]:(0.999994516373) A[3]:(0.810323953629)\n",
      " state (15)  A[0]:(0.995749533176) A[1]:(0.98042011261) A[2]:(1.0) A[3]:(0.918084323406)\n",
      "Episode 227000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6171. Times reached goal: 919.               Steps done: 2496336. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.074147752899.\n",
      " state (0)  A[0]:(0.532429099083) A[1]:(0.591510415077) A[2]:(0.590327501297) A[3]:(0.531824231148)\n",
      " state (1)  A[0]:(0.532199084759) A[1]:(0.000558964849915) A[2]:(0.656985878944) A[3]:(0.591212451458)\n",
      " state (2)  A[0]:(0.591164708138) A[1]:(0.729575157166) A[2]:(0.591466546059) A[3]:(0.656531214714)\n",
      " state (3)  A[0]:(0.656717896461) A[1]:(-0.0833354592323) A[2]:(0.438441067934) A[3]:(0.538291573524)\n",
      " state (4)  A[0]:(0.593001484871) A[1]:(0.656142830849) A[2]:(0.000351786613464) A[3]:(0.532755613327)\n",
      " state (5)  A[0]:(0.202219635248) A[1]:(0.914783060551) A[2]:(-0.095099799335) A[3]:(0.499772727489)\n",
      " state (6)  A[0]:(-0.00149205210619) A[1]:(0.809885859489) A[2]:(-0.00106692279223) A[3]:(0.656313359737)\n",
      " state (7)  A[0]:(0.636271476746) A[1]:(-0.185751214623) A[2]:(0.0675894320011) A[3]:(0.922499835491)\n",
      " state (8)  A[0]:(0.656191825867) A[1]:(-0.00108534796163) A[2]:(0.728780269623) A[3]:(0.591883540154)\n",
      " state (9)  A[0]:(0.653881847858) A[1]:(0.808836698532) A[2]:(0.809469521046) A[3]:(-0.000909402733669)\n",
      " state (10)  A[0]:(0.728965580463) A[1]:(0.899869918823) A[2]:(-0.00122654379811) A[3]:(0.729568481445)\n",
      " state (11)  A[0]:(0.490775465965) A[1]:(0.86939740181) A[2]:(-0.474977731705) A[3]:(0.833516836166)\n",
      " state (12)  A[0]:(0.0368934087455) A[1]:(0.804669380188) A[2]:(-0.262705117464) A[3]:(0.782576560974)\n",
      " state (13)  A[0]:(-0.00198019784875) A[1]:(0.793437302113) A[2]:(0.900251150131) A[3]:(0.728789925575)\n",
      " state (14)  A[0]:(0.810403466225) A[1]:(0.908171892166) A[2]:(0.999994754791) A[3]:(0.810766577721)\n",
      " state (15)  A[0]:(0.995643079281) A[1]:(0.979709088802) A[2]:(1.0) A[3]:(0.918068647385)\n",
      "Episode 228000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6188. Times reached goal: 908.               Steps done: 2502524. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.073690343289.\n",
      " state (0)  A[0]:(0.531437039375) A[1]:(0.59085637331) A[2]:(0.590583920479) A[3]:(0.531969666481)\n",
      " state (1)  A[0]:(0.531475067139) A[1]:(0.000765980978031) A[2]:(0.656607627869) A[3]:(0.591308653355)\n",
      " state (2)  A[0]:(0.590244412422) A[1]:(0.729626238346) A[2]:(0.592528641224) A[3]:(0.655799806118)\n",
      " state (3)  A[0]:(0.655216693878) A[1]:(-0.129949271679) A[2]:(0.44727897644) A[3]:(0.533309817314)\n",
      " state (4)  A[0]:(0.591633558273) A[1]:(0.657127380371) A[2]:(0.00451621320099) A[3]:(0.531909108162)\n",
      " state (5)  A[0]:(0.200891554356) A[1]:(0.915605247021) A[2]:(-0.092663012445) A[3]:(0.499169588089)\n",
      " state (6)  A[0]:(0.000732272746973) A[1]:(0.810829520226) A[2]:(0.0001540184021) A[3]:(0.656455993652)\n",
      " state (7)  A[0]:(0.638140082359) A[1]:(-0.185221776366) A[2]:(0.0680283606052) A[3]:(0.922324061394)\n",
      " state (8)  A[0]:(0.657302260399) A[1]:(0.00382123701274) A[2]:(0.728879034519) A[3]:(0.59067517519)\n",
      " state (9)  A[0]:(0.655586361885) A[1]:(0.810467362404) A[2]:(0.809799075127) A[3]:(-0.00106318260077)\n",
      " state (10)  A[0]:(0.729896187782) A[1]:(0.900386333466) A[2]:(-0.00056576723) A[3]:(0.728883504868)\n",
      " state (11)  A[0]:(0.492251873016) A[1]:(0.869879961014) A[2]:(-0.475871145725) A[3]:(0.832870006561)\n",
      " state (12)  A[0]:(0.038949161768) A[1]:(0.805012106895) A[2]:(-0.265399038792) A[3]:(0.78172069788)\n",
      " state (13)  A[0]:(-5.45382499695e-05) A[1]:(0.792700111866) A[2]:(0.900302410126) A[3]:(0.727814733982)\n",
      " state (14)  A[0]:(0.811021924019) A[1]:(0.906622350216) A[2]:(0.999994874001) A[3]:(0.810215890408)\n",
      " state (15)  A[0]:(0.995611965656) A[1]:(0.978882312775) A[2]:(1.0) A[3]:(0.917678773403)\n",
      "Episode 229000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6204. Times reached goal: 920.               Steps done: 2508728. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0732345836275.\n",
      "q_values \n",
      "tensor([[ 0.5305,  0.5906,  0.5910,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.0003,  0.6570,  0.5915]], device='cuda:0')\n",
      "On state=1, selected action=3 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.3129e-01,  2.1700e-06,  6.5689e-01,  5.9154e-01]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7292,  0.5911,  0.6567]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0006,  0.8103,  0.0002,  0.6562]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7298,  0.9007, -0.0000,  0.7297]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9071,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531162858009) A[1]:(0.59034883976) A[2]:(0.590960979462) A[3]:(0.531448841095)\n",
      " state (1)  A[0]:(0.531806468964) A[1]:(-0.000183386728168) A[2]:(0.656850814819) A[3]:(0.591362297535)\n",
      " state (2)  A[0]:(0.590716362) A[1]:(0.729175329208) A[2]:(0.59096121788) A[3]:(0.6565284729)\n",
      " state (3)  A[0]:(0.655978918076) A[1]:(-0.14403359592) A[2]:(0.445319771767) A[3]:(0.531811237335)\n",
      " state (4)  A[0]:(0.591836154461) A[1]:(0.656161546707) A[2]:(0.000898837810382) A[3]:(0.531521499157)\n",
      " state (5)  A[0]:(0.200213044882) A[1]:(0.915287137032) A[2]:(-0.0945070981979) A[3]:(0.499020636082)\n",
      " state (6)  A[0]:(-0.000124335289001) A[1]:(0.810162484646) A[2]:(-0.000540971697774) A[3]:(0.656236171722)\n",
      " state (7)  A[0]:(0.637088179588) A[1]:(-0.188259974122) A[2]:(0.0679470822215) A[3]:(0.922174394131)\n",
      " state (8)  A[0]:(0.656770408154) A[1]:(0.00224743597209) A[2]:(0.728827595711) A[3]:(0.591292500496)\n",
      " state (9)  A[0]:(0.655215442181) A[1]:(0.810383915901) A[2]:(0.810103058815) A[3]:(0.000552929879632)\n",
      " state (10)  A[0]:(0.729618549347) A[1]:(0.900616288185) A[2]:(-0.00114679289982) A[3]:(0.72974729538)\n",
      " state (11)  A[0]:(0.492204397917) A[1]:(0.870851516724) A[2]:(-0.478232979774) A[3]:(0.833545863628)\n",
      " state (12)  A[0]:(0.0385474637151) A[1]:(0.807224810123) A[2]:(-0.269838839769) A[3]:(0.782440900803)\n",
      " state (13)  A[0]:(-0.00161510566249) A[1]:(0.795090377331) A[2]:(0.900097489357) A[3]:(0.728211343288)\n",
      " state (14)  A[0]:(0.810092449188) A[1]:(0.907067239285) A[2]:(0.999995052814) A[3]:(0.810017347336)\n",
      " state (15)  A[0]:(0.995528578758) A[1]:(0.978616416454) A[2]:(1.0) A[3]:(0.917056500912)\n",
      "Episode 230000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6250. Times reached goal: 935.               Steps done: 2514978. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0727782948676.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531370520592) A[1]:(0.590659976006) A[2]:(0.59100228548) A[3]:(0.53188765049)\n",
      " state (1)  A[0]:(0.531772434711) A[1]:(0.000225648283958) A[2]:(0.656692028046) A[3]:(0.591405928135)\n",
      " state (2)  A[0]:(0.590589165688) A[1]:(0.729239106178) A[2]:(0.591422438622) A[3]:(0.656022369862)\n",
      " state (3)  A[0]:(0.65581959486) A[1]:(-0.169720634818) A[2]:(0.448795557022) A[3]:(0.529934167862)\n",
      " state (4)  A[0]:(0.591654002666) A[1]:(0.657556951046) A[2]:(0.000925540633034) A[3]:(0.532109260559)\n",
      " state (5)  A[0]:(0.199380993843) A[1]:(0.915722191334) A[2]:(-0.0945968925953) A[3]:(0.499156355858)\n",
      " state (6)  A[0]:(-0.000398486823542) A[1]:(0.810503304005) A[2]:(-0.000430345506174) A[3]:(0.656178474426)\n",
      " state (7)  A[0]:(0.637233495712) A[1]:(-0.190177410841) A[2]:(0.0695372894406) A[3]:(0.92204529047)\n",
      " state (8)  A[0]:(0.657317876816) A[1]:(0.00209550256841) A[2]:(0.730325698853) A[3]:(0.589289963245)\n",
      " state (9)  A[0]:(0.656484186649) A[1]:(0.810307025909) A[2]:(0.810727238655) A[3]:(-0.00104173237924)\n",
      " state (10)  A[0]:(0.730542063713) A[1]:(0.900480091572) A[2]:(-0.0010864729993) A[3]:(0.729346394539)\n",
      " state (11)  A[0]:(0.493852853775) A[1]:(0.871033787727) A[2]:(-0.479929685593) A[3]:(0.833511471748)\n",
      " state (12)  A[0]:(0.0405730828643) A[1]:(0.808004260063) A[2]:(-0.273193538189) A[3]:(0.782655000687)\n",
      " state (13)  A[0]:(5.76972961426e-05) A[1]:(0.795809805393) A[2]:(0.900448918343) A[3]:(0.728639125824)\n",
      " state (14)  A[0]:(0.81071460247) A[1]:(0.906676232815) A[2]:(0.999995291233) A[3]:(0.810376942158)\n",
      " state (15)  A[0]:(0.995483458042) A[1]:(0.978117525578) A[2]:(1.0) A[3]:(0.916860163212)\n",
      "Episode 231000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6176. Times reached goal: 932.               Steps done: 2521154. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0723302012558.\n",
      " state (0)  A[0]:(0.531600594521) A[1]:(0.591399431229) A[2]:(0.591210007668) A[3]:(0.531355261803)\n",
      " state (1)  A[0]:(0.531890749931) A[1]:(0.000212162733078) A[2]:(0.656470239162) A[3]:(0.59127342701)\n",
      " state (2)  A[0]:(0.590542316437) A[1]:(0.729551911354) A[2]:(0.591328382492) A[3]:(0.655918955803)\n",
      " state (3)  A[0]:(0.655892908573) A[1]:(-0.192133069038) A[2]:(0.451714754105) A[3]:(0.527450203896)\n",
      " state (4)  A[0]:(0.591626524925) A[1]:(0.657212615013) A[2]:(0.00114905787632) A[3]:(0.53178191185)\n",
      " state (5)  A[0]:(0.198920696974) A[1]:(0.915768921375) A[2]:(-0.0945837721229) A[3]:(0.499130159616)\n",
      " state (6)  A[0]:(-0.000799551431555) A[1]:(0.810485363007) A[2]:(-0.000286340713501) A[3]:(0.656362116337)\n",
      " state (7)  A[0]:(0.63635712862) A[1]:(-0.191782191396) A[2]:(0.0699182078242) A[3]:(0.922166824341)\n",
      " state (8)  A[0]:(0.656744599342) A[1]:(0.00140905287117) A[2]:(0.729735732079) A[3]:(0.591035246849)\n",
      " state (9)  A[0]:(0.655633568764) A[1]:(0.810156702995) A[2]:(0.810811460018) A[3]:(-0.000220380723476)\n",
      " state (10)  A[0]:(0.729652106762) A[1]:(0.900465667248) A[2]:(-0.0003342628479) A[3]:(0.729173660278)\n",
      " state (11)  A[0]:(0.492946505547) A[1]:(0.871512174606) A[2]:(-0.480704069138) A[3]:(0.833545744419)\n",
      " state (12)  A[0]:(0.0395419709384) A[1]:(0.809368014336) A[2]:(-0.276001304388) A[3]:(0.78285753727)\n",
      " state (13)  A[0]:(-0.00155281892512) A[1]:(0.797270298004) A[2]:(0.90034455061) A[3]:(0.728849291801)\n",
      " state (14)  A[0]:(0.810130059719) A[1]:(0.906733691692) A[2]:(0.999995410442) A[3]:(0.810442864895)\n",
      " state (15)  A[0]:(0.995426058769) A[1]:(0.977786123753) A[2]:(1.0) A[3]:(0.916591227055)\n",
      "Episode 232000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6132. Times reached goal: 911.               Steps done: 2527286. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0718880295457.\n",
      " state (0)  A[0]:(0.53150510788) A[1]:(0.590514779091) A[2]:(0.59087729454) A[3]:(0.529920518398)\n",
      " state (1)  A[0]:(0.531998217106) A[1]:(0.000240871682763) A[2]:(0.65600925684) A[3]:(0.589900195599)\n",
      " state (2)  A[0]:(0.590705752373) A[1]:(0.729150533676) A[2]:(0.590066671371) A[3]:(0.654801368713)\n",
      " state (3)  A[0]:(0.655789256096) A[1]:(-0.201575994492) A[2]:(0.451842457056) A[3]:(0.526777982712)\n",
      " state (4)  A[0]:(0.591546535492) A[1]:(0.65622407198) A[2]:(0.000288128852844) A[3]:(0.532056570053)\n",
      " state (5)  A[0]:(0.199339091778) A[1]:(0.91568762064) A[2]:(-0.0954337418079) A[3]:(0.499070823193)\n",
      " state (6)  A[0]:(0.000420242518885) A[1]:(0.810323655605) A[2]:(-0.000801086251158) A[3]:(0.655910134315)\n",
      " state (7)  A[0]:(0.636480808258) A[1]:(-0.193596914411) A[2]:(0.070163525641) A[3]:(0.921792507172)\n",
      " state (8)  A[0]:(0.656851232052) A[1]:(0.0012106684735) A[2]:(0.729613304138) A[3]:(0.589089274406)\n",
      " state (9)  A[0]:(0.656000614166) A[1]:(0.810183644295) A[2]:(0.810675740242) A[3]:(-0.00272023002617)\n",
      " state (10)  A[0]:(0.730115413666) A[1]:(0.900456249714) A[2]:(-0.000718355062418) A[3]:(0.728102624416)\n",
      " state (11)  A[0]:(0.494502991438) A[1]:(0.871757805347) A[2]:(-0.482139229774) A[3]:(0.833019852638)\n",
      " state (12)  A[0]:(0.0420268587768) A[1]:(0.809940934181) A[2]:(-0.279449701309) A[3]:(0.782327473164)\n",
      " state (13)  A[0]:(0.000437945098383) A[1]:(0.797358155251) A[2]:(0.900107383728) A[3]:(0.728152871132)\n",
      " state (14)  A[0]:(0.810687482357) A[1]:(0.905861854553) A[2]:(0.999995589256) A[3]:(0.809804975986)\n",
      " state (15)  A[0]:(0.995395123959) A[1]:(0.977154016495) A[2]:(1.0) A[3]:(0.915976464748)\n",
      "Episode 233000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6219. Times reached goal: 934.               Steps done: 2533505. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0714423451819.\n",
      " state (0)  A[0]:(0.531134605408) A[1]:(0.589857041836) A[2]:(0.59077501297) A[3]:(0.531761527061)\n",
      " state (1)  A[0]:(0.531590044498) A[1]:(-0.00164464337286) A[2]:(0.655696928501) A[3]:(0.591633319855)\n",
      " state (2)  A[0]:(0.590024709702) A[1]:(0.729054808617) A[2]:(0.590005218983) A[3]:(0.655723690987)\n",
      " state (3)  A[0]:(0.655284285545) A[1]:(-0.199242264032) A[2]:(0.451699107885) A[3]:(0.528118848801)\n",
      " state (4)  A[0]:(0.590536594391) A[1]:(0.656346678734) A[2]:(-0.000232577323914) A[3]:(0.531908810139)\n",
      " state (5)  A[0]:(0.197285756469) A[1]:(0.915833234787) A[2]:(-0.0964064002037) A[3]:(0.498019248247)\n",
      " state (6)  A[0]:(0.000317841768265) A[1]:(0.809716582298) A[2]:(-0.00190555816516) A[3]:(0.655894935131)\n",
      " state (7)  A[0]:(0.6368496418) A[1]:(-0.198306411505) A[2]:(0.0694771409035) A[3]:(0.921908557415)\n",
      " state (8)  A[0]:(0.656060993671) A[1]:(0.000352706731064) A[2]:(0.729262709618) A[3]:(0.588132739067)\n",
      " state (9)  A[0]:(0.655410945415) A[1]:(0.810022473335) A[2]:(0.810045838356) A[3]:(-0.00281328707933)\n",
      " state (10)  A[0]:(0.729493379593) A[1]:(0.900269269943) A[2]:(-0.00304078124464) A[3]:(0.728216409683)\n",
      " state (11)  A[0]:(0.493742793798) A[1]:(0.871762752533) A[2]:(-0.485311061144) A[3]:(0.833132743835)\n",
      " state (12)  A[0]:(0.0408764891326) A[1]:(0.810251951218) A[2]:(-0.285479992628) A[3]:(0.78247821331)\n",
      " state (13)  A[0]:(-0.0017195326509) A[1]:(0.797365546227) A[2]:(0.899170875549) A[3]:(0.728167176247)\n",
      " state (14)  A[0]:(0.80969029665) A[1]:(0.9051040411) A[2]:(0.999995648861) A[3]:(0.809653520584)\n",
      " state (15)  A[0]:(0.99532365799) A[1]:(0.976598739624) A[2]:(1.0) A[3]:(0.915607511997)\n",
      "Episode 234000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6189. Times reached goal: 906.               Steps done: 2539694. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0710015539431.\n",
      "q_values \n",
      "tensor([[ 0.5310,  0.5905,  0.5904,  0.5299]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5912,  0.6561,  0.0007,  0.5318]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6569,  0.0009,  0.7292,  0.5908]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558,  0.8103,  0.8106, -0.0000]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9002, -0.0003,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9045,  1.0000,  0.8105]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? True\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531093358994) A[1]:(0.590380549431) A[2]:(0.590422272682) A[3]:(0.530845165253)\n",
      " state (1)  A[0]:(0.531448125839) A[1]:(-0.000133751891553) A[2]:(0.655406117439) A[3]:(0.591144323349)\n",
      " state (2)  A[0]:(0.590091586113) A[1]:(0.729152321815) A[2]:(0.590035021305) A[3]:(0.655117154121)\n",
      " state (3)  A[0]:(0.655108690262) A[1]:(-0.225323319435) A[2]:(0.455773174763) A[3]:(0.525916934013)\n",
      " state (4)  A[0]:(0.590601503849) A[1]:(0.656019032001) A[2]:(0.000619292201009) A[3]:(0.532219231129)\n",
      " state (5)  A[0]:(0.196902692318) A[1]:(0.915893495083) A[2]:(-0.0958566069603) A[3]:(0.498170226812)\n",
      " state (6)  A[0]:(-0.00147342577111) A[1]:(0.810080647469) A[2]:(-0.000881790881976) A[3]:(0.655543208122)\n",
      " state (7)  A[0]:(0.635216653347) A[1]:(-0.197959557176) A[2]:(0.071145772934) A[3]:(0.921919286251)\n",
      " state (8)  A[0]:(0.655968546867) A[1]:(0.000150711275637) A[2]:(0.729065537453) A[3]:(0.590715527534)\n",
      " state (9)  A[0]:(0.65544295311) A[1]:(0.809758007526) A[2]:(0.810132622719) A[3]:(0.000919639831409)\n",
      " state (10)  A[0]:(0.729166984558) A[1]:(0.899992763996) A[2]:(-0.00113534880802) A[3]:(0.7291918993)\n",
      " state (11)  A[0]:(0.493575960398) A[1]:(0.871581375599) A[2]:(-0.484244197607) A[3]:(0.833675026894)\n",
      " state (12)  A[0]:(0.0412216410041) A[1]:(0.810239255428) A[2]:(-0.284741282463) A[3]:(0.783230543137)\n",
      " state (13)  A[0]:(-0.000562429369893) A[1]:(0.797152876854) A[2]:(0.900496721268) A[3]:(0.729224979877)\n",
      " state (14)  A[0]:(0.810841798782) A[1]:(0.904428124428) A[2]:(0.999995946884) A[3]:(0.810692250729)\n",
      " state (15)  A[0]:(0.995311081409) A[1]:(0.976067900658) A[2]:(1.0) A[3]:(0.915882110596)\n",
      "Episode 235000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6221. Times reached goal: 927.               Steps done: 2545915. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0705612243413.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532346785069) A[1]:(0.590713322163) A[2]:(0.591176748276) A[3]:(0.532038927078)\n",
      " state (1)  A[0]:(0.532617211342) A[1]:(0.000195869244635) A[2]:(0.656004905701) A[3]:(0.591762244701)\n",
      " state (2)  A[0]:(0.591039955616) A[1]:(0.729430139065) A[2]:(0.590523838997) A[3]:(0.655236244202)\n",
      " state (3)  A[0]:(0.656035423279) A[1]:(-0.245839506388) A[2]:(0.45921972394) A[3]:(0.524439275265)\n",
      " state (4)  A[0]:(0.591437458992) A[1]:(0.657154798508) A[2]:(0.000666975858621) A[3]:(0.532607972622)\n",
      " state (5)  A[0]:(0.197355464101) A[1]:(0.916282653809) A[2]:(-0.0961286202073) A[3]:(0.498382389545)\n",
      " state (6)  A[0]:(0.000145226716995) A[1]:(0.810406148434) A[2]:(-0.000632047594991) A[3]:(0.655918300152)\n",
      " state (7)  A[0]:(0.636994838715) A[1]:(-0.198615327477) A[2]:(0.0726674348116) A[3]:(0.921984016895)\n",
      " state (8)  A[0]:(0.657508671284) A[1]:(0.00128258042969) A[2]:(0.729652047157) A[3]:(0.590463876724)\n",
      " state (9)  A[0]:(0.656958401203) A[1]:(0.810122251511) A[2]:(0.810326933861) A[3]:(0.00114769442007)\n",
      " state (10)  A[0]:(0.730260908604) A[1]:(0.900173485279) A[2]:(-0.000753283384256) A[3]:(0.729407310486)\n",
      " state (11)  A[0]:(0.495486140251) A[1]:(0.87206351757) A[2]:(-0.48502022028) A[3]:(0.833879470825)\n",
      " state (12)  A[0]:(0.0433427467942) A[1]:(0.811202347279) A[2]:(-0.287500828505) A[3]:(0.783473789692)\n",
      " state (13)  A[0]:(0.000198096036911) A[1]:(0.797879636288) A[2]:(0.900255084038) A[3]:(0.729348301888)\n",
      " state (14)  A[0]:(0.810686469078) A[1]:(0.904130637646) A[2]:(0.999996006489) A[3]:(0.810709357262)\n",
      " state (15)  A[0]:(0.99525731802) A[1]:(0.975659251213) A[2]:(1.0) A[3]:(0.915667295456)\n",
      "Episode 236000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6153. Times reached goal: 926.               Steps done: 2552068. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0701283940958.\n",
      " state (0)  A[0]:(0.532212615013) A[1]:(0.591558814049) A[2]:(0.591017365456) A[3]:(0.534348845482)\n",
      " state (1)  A[0]:(0.532148957253) A[1]:(0.00012438558042) A[2]:(0.656454205513) A[3]:(0.594156265259)\n",
      " state (2)  A[0]:(0.590556204319) A[1]:(0.72936218977) A[2]:(0.590444922447) A[3]:(0.657652020454)\n",
      " state (3)  A[0]:(0.656125307083) A[1]:(-0.25790899992) A[2]:(0.460860401392) A[3]:(0.526276469231)\n",
      " state (4)  A[0]:(0.591567993164) A[1]:(0.65638589859) A[2]:(0.000100493431091) A[3]:(0.53552532196)\n",
      " state (5)  A[0]:(0.196993097663) A[1]:(0.916228652) A[2]:(-0.0972126126289) A[3]:(0.501408934593)\n",
      " state (6)  A[0]:(-0.00119677127805) A[1]:(0.810492634773) A[2]:(-0.0013802042231) A[3]:(0.657768368721)\n",
      " state (7)  A[0]:(0.6353110075) A[1]:(-0.198933795094) A[2]:(0.0726649463177) A[3]:(0.922350883484)\n",
      " state (8)  A[0]:(0.656149625778) A[1]:(0.000783165742178) A[2]:(0.728949189186) A[3]:(0.592411637306)\n",
      " state (9)  A[0]:(0.655283808708) A[1]:(0.809955656528) A[2]:(0.810218155384) A[3]:(0.00111240847036)\n",
      " state (10)  A[0]:(0.72883361578) A[1]:(0.900111854076) A[2]:(-0.00104391539935) A[3]:(0.729010760784)\n",
      " state (11)  A[0]:(0.493624657393) A[1]:(0.872233808041) A[2]:(-0.486526846886) A[3]:(0.833714604378)\n",
      " state (12)  A[0]:(0.0409864522517) A[1]:(0.811726093292) A[2]:(-0.290663868189) A[3]:(0.783236980438)\n",
      " state (13)  A[0]:(-0.00227251253091) A[1]:(0.798271596432) A[2]:(0.900395750999) A[3]:(0.728911817074)\n",
      " state (14)  A[0]:(0.810208439827) A[1]:(0.903811395168) A[2]:(0.999996185303) A[3]:(0.810385167599)\n",
      " state (15)  A[0]:(0.995202481747) A[1]:(0.975258409977) A[2]:(1.0) A[3]:(0.915242731571)\n",
      "Episode 237000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6122. Times reached goal: 919.               Steps done: 2558190. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0697003795564.\n",
      " state (0)  A[0]:(0.532303631306) A[1]:(0.591272592545) A[2]:(0.590898275375) A[3]:(0.532477140427)\n",
      " state (1)  A[0]:(0.532546818256) A[1]:(0.000487673998578) A[2]:(0.656497001648) A[3]:(0.59236562252)\n",
      " state (2)  A[0]:(0.590977191925) A[1]:(0.729240357876) A[2]:(0.590398550034) A[3]:(0.656075239182)\n",
      " state (3)  A[0]:(0.65617364645) A[1]:(-0.272355079651) A[2]:(0.4633063972) A[3]:(0.522664189339)\n",
      " state (4)  A[0]:(0.591390490532) A[1]:(0.656954884529) A[2]:(0.000368356675608) A[3]:(0.5334597826)\n",
      " state (5)  A[0]:(0.196397766471) A[1]:(0.916435062885) A[2]:(-0.0969604998827) A[3]:(0.49957844615)\n",
      " state (6)  A[0]:(0.000122129917145) A[1]:(0.81007117033) A[2]:(-0.000572442950215) A[3]:(0.657298207283)\n",
      " state (7)  A[0]:(0.636671304703) A[1]:(-0.202116414905) A[2]:(0.0742721036077) A[3]:(0.92234236002)\n",
      " state (8)  A[0]:(0.656947374344) A[1]:(0.00103704968933) A[2]:(0.729129552841) A[3]:(0.592206001282)\n",
      " state (9)  A[0]:(0.656370639801) A[1]:(0.810256123543) A[2]:(0.81010222435) A[3]:(0.0017978156684)\n",
      " state (10)  A[0]:(0.729587554932) A[1]:(0.900143682957) A[2]:(-0.000842094246764) A[3]:(0.729241132736)\n",
      " state (11)  A[0]:(0.495063275099) A[1]:(0.872356653214) A[2]:(-0.487226873636) A[3]:(0.833882570267)\n",
      " state (12)  A[0]:(0.0427907928824) A[1]:(0.811989784241) A[2]:(-0.293410331011) A[3]:(0.783499956131)\n",
      " state (13)  A[0]:(-0.00133046426345) A[1]:(0.798147439957) A[2]:(0.900137722492) A[3]:(0.729196667671)\n",
      " state (14)  A[0]:(0.810407876968) A[1]:(0.903094768524) A[2]:(0.999996304512) A[3]:(0.810647845268)\n",
      " state (15)  A[0]:(0.995166897774) A[1]:(0.974740207195) A[2]:(1.0) A[3]:(0.915202498436)\n",
      "Episode 238000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6204. Times reached goal: 940.               Steps done: 2564394. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0692692970024.\n",
      " state (0)  A[0]:(0.531234145164) A[1]:(0.590570092201) A[2]:(0.590837717056) A[3]:(0.531252861023)\n",
      " state (1)  A[0]:(0.531940698624) A[1]:(-4.82890754938e-06) A[2]:(0.656126737595) A[3]:(0.591849684715)\n",
      " state (2)  A[0]:(0.59043776989) A[1]:(0.72911965847) A[2]:(0.590203344822) A[3]:(0.655248463154)\n",
      " state (3)  A[0]:(0.655686378479) A[1]:(-0.287291288376) A[2]:(0.465844005346) A[3]:(0.520689129829)\n",
      " state (4)  A[0]:(0.590928792953) A[1]:(0.656133890152) A[2]:(0.000436186761362) A[3]:(0.532138228416)\n",
      " state (5)  A[0]:(0.195588678122) A[1]:(0.916461765766) A[2]:(-0.097500257194) A[3]:(0.497567296028)\n",
      " state (6)  A[0]:(0.000245183706284) A[1]:(0.80985057354) A[2]:(-0.00054454797646) A[3]:(0.655725002289)\n",
      " state (7)  A[0]:(0.636521220207) A[1]:(-0.203940331936) A[2]:(0.0754680633545) A[3]:(0.921834945679)\n",
      " state (8)  A[0]:(0.656231820583) A[1]:(0.000518440210726) A[2]:(0.729167103767) A[3]:(0.590455412865)\n",
      " state (9)  A[0]:(0.65565353632) A[1]:(0.81003344059) A[2]:(0.810086011887) A[3]:(0.000247225165367)\n",
      " state (10)  A[0]:(0.729280948639) A[1]:(0.900096654892) A[2]:(-0.000757694127969) A[3]:(0.728801250458)\n",
      " state (11)  A[0]:(0.495407611132) A[1]:(0.872627735138) A[2]:(-0.488165438175) A[3]:(0.833733797073)\n",
      " state (12)  A[0]:(0.0436428263783) A[1]:(0.812763810158) A[2]:(-0.295991301537) A[3]:(0.783301591873)\n",
      " state (13)  A[0]:(-0.00089481449686) A[1]:(0.79893219471) A[2]:(0.900225043297) A[3]:(0.728795528412)\n",
      " state (14)  A[0]:(0.810535490513) A[1]:(0.903086364269) A[2]:(0.999996423721) A[3]:(0.810321569443)\n",
      " state (15)  A[0]:(0.995118677616) A[1]:(0.974464416504) A[2]:(1.0) A[3]:(0.914786100388)\n",
      "Episode 239000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6158. Times reached goal: 921.               Steps done: 2570552. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0688440473589.\n",
      "q_values \n",
      "tensor([[ 0.5321,  0.5921,  0.5903,  0.5328]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5911,  0.6565,  0.0019,  0.5333]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6538, -0.0057,  0.7282,  0.5911]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6532,  0.8079,  0.8100, -0.0010]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.8995, -0.0004,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8134,  0.9035,  1.0000,  0.8116]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532253026962) A[1]:(0.590177774429) A[2]:(0.590962171555) A[3]:(0.532281517982)\n",
      " state (1)  A[0]:(0.532111763954) A[1]:(-0.00234452681616) A[2]:(0.656993508339) A[3]:(0.591635107994)\n",
      " state (2)  A[0]:(0.590353786945) A[1]:(0.730236053467) A[2]:(0.590542376041) A[3]:(0.655510306358)\n",
      " state (3)  A[0]:(0.656211555004) A[1]:(-0.281984657049) A[2]:(0.468617677689) A[3]:(0.521769702435)\n",
      " state (4)  A[0]:(0.591745257378) A[1]:(0.656014919281) A[2]:(0.0029646072071) A[3]:(0.533537149429)\n",
      " state (5)  A[0]:(0.196927323937) A[1]:(0.916628658772) A[2]:(-0.0970059633255) A[3]:(0.499560981989)\n",
      " state (6)  A[0]:(0.0010350641096) A[1]:(0.81065517664) A[2]:(-0.000143766403198) A[3]:(0.656931877136)\n",
      " state (7)  A[0]:(0.636547088623) A[1]:(-0.201328665018) A[2]:(0.0770048648119) A[3]:(0.922054946423)\n",
      " state (8)  A[0]:(0.657366037369) A[1]:(0.00386515096761) A[2]:(0.72917342186) A[3]:(0.59283554554)\n",
      " state (9)  A[0]:(0.65729367733) A[1]:(0.811655461788) A[2]:(0.810392618179) A[3]:(0.00295949610882)\n",
      " state (10)  A[0]:(0.731127440929) A[1]:(0.901172578335) A[2]:(0.000506043375935) A[3]:(0.730078577995)\n",
      " state (11)  A[0]:(0.49953648448) A[1]:(0.874265551567) A[2]:(-0.488195300102) A[3]:(0.834764838219)\n",
      " state (12)  A[0]:(0.0499799400568) A[1]:(0.815407216549) A[2]:(-0.296954989433) A[3]:(0.784668147564)\n",
      " state (13)  A[0]:(0.0059836092405) A[1]:(0.801795959473) A[2]:(0.901064813137) A[3]:(0.730349719524)\n",
      " state (14)  A[0]:(0.813387155533) A[1]:(0.904336810112) A[2]:(0.99999666214) A[3]:(0.811424016953)\n",
      " state (15)  A[0]:(0.995148479939) A[1]:(0.974582076073) A[2]:(1.0) A[3]:(0.914939641953)\n",
      "Episode 240000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6128. Times reached goal: 914.               Steps done: 2576680. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0684234610294.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531501710415) A[1]:(0.590380072594) A[2]:(0.590139269829) A[3]:(0.531032800674)\n",
      " state (1)  A[0]:(0.532582998276) A[1]:(-0.000548327283468) A[2]:(0.656091332436) A[3]:(0.590609550476)\n",
      " state (2)  A[0]:(0.589560449123) A[1]:(0.729088544846) A[2]:(0.587600827217) A[3]:(0.656152784824)\n",
      " state (3)  A[0]:(0.656017839909) A[1]:(-0.175256848335) A[2]:(0.457294672728) A[3]:(0.531410455704)\n",
      " state (4)  A[0]:(0.590451538563) A[1]:(0.655850708485) A[2]:(0.00312029314227) A[3]:(0.532711267471)\n",
      " state (5)  A[0]:(0.193947851658) A[1]:(0.916868686676) A[2]:(-0.0993585512042) A[3]:(0.498188108206)\n",
      " state (6)  A[0]:(-4.24683094025e-05) A[1]:(0.809841990471) A[2]:(-0.00113308383152) A[3]:(0.656412005424)\n",
      " state (7)  A[0]:(0.636497378349) A[1]:(-0.207709118724) A[2]:(0.0795820131898) A[3]:(0.921571493149)\n",
      " state (8)  A[0]:(0.65589094162) A[1]:(3.97022813559e-05) A[2]:(0.729129314423) A[3]:(0.590734481812)\n",
      " state (9)  A[0]:(0.655639767647) A[1]:(0.810062050819) A[2]:(0.810113549232) A[3]:(0.000634968222585)\n",
      " state (10)  A[0]:(0.729134202003) A[1]:(0.900054693222) A[2]:(-0.000459194154246) A[3]:(0.728885054588)\n",
      " state (11)  A[0]:(0.495723992586) A[1]:(0.872656702995) A[2]:(-0.490221709013) A[3]:(0.833920836449)\n",
      " state (12)  A[0]:(0.0439219549298) A[1]:(0.812831223011) A[2]:(-0.301249533892) A[3]:(0.7834405303)\n",
      " state (13)  A[0]:(-0.00144436850678) A[1]:(0.798669874668) A[2]:(0.900429010391) A[3]:(0.728638529778)\n",
      " state (14)  A[0]:(0.810620188713) A[1]:(0.902533888817) A[2]:(0.999996721745) A[3]:(0.810085475445)\n",
      " state (15)  A[0]:(0.995035767555) A[1]:(0.973974764347) A[2]:(1.0) A[3]:(0.91413551569)\n",
      "Episode 241000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6159. Times reached goal: 924.               Steps done: 2582839. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0680033360359.\n",
      " state (0)  A[0]:(0.532099664211) A[1]:(0.59087562561) A[2]:(0.589615523815) A[3]:(0.531798362732)\n",
      " state (1)  A[0]:(0.532624721527) A[1]:(-0.00120725331362) A[2]:(0.656833648682) A[3]:(0.590104222298)\n",
      " state (2)  A[0]:(0.589421868324) A[1]:(0.729721784592) A[2]:(0.58928745985) A[3]:(0.656207978725)\n",
      " state (3)  A[0]:(0.656334638596) A[1]:(-0.089145898819) A[2]:(0.44881105423) A[3]:(0.53824287653)\n",
      " state (4)  A[0]:(0.591153204441) A[1]:(0.655432343483) A[2]:(0.000871896510944) A[3]:(0.532149553299)\n",
      " state (5)  A[0]:(0.194786608219) A[1]:(0.916889190674) A[2]:(-0.103139802814) A[3]:(0.497889608145)\n",
      " state (6)  A[0]:(-0.00074160087388) A[1]:(0.810162067413) A[2]:(-0.00286661786959) A[3]:(0.655803203583)\n",
      " state (7)  A[0]:(0.635588407516) A[1]:(-0.207136422396) A[2]:(0.081531226635) A[3]:(0.921052336693)\n",
      " state (8)  A[0]:(0.655808806419) A[1]:(-0.000477842026157) A[2]:(0.728819012642) A[3]:(0.591078460217)\n",
      " state (9)  A[0]:(0.655393242836) A[1]:(0.809770524502) A[2]:(0.810469686985) A[3]:(-0.000165693461895)\n",
      " state (10)  A[0]:(0.729359745979) A[1]:(0.899939954281) A[2]:(-0.000115871429443) A[3]:(0.7291918993)\n",
      " state (11)  A[0]:(0.497108578682) A[1]:(0.872541129589) A[2]:(-0.491864115) A[3]:(0.834703326225)\n",
      " state (12)  A[0]:(0.0461618416011) A[1]:(0.812562227249) A[2]:(-0.304714053869) A[3]:(0.784678280354)\n",
      " state (13)  A[0]:(0.000873148208484) A[1]:(0.798119664192) A[2]:(0.900612592697) A[3]:(0.730155706406)\n",
      " state (14)  A[0]:(0.81181704998) A[1]:(0.902075469494) A[2]:(0.999996840954) A[3]:(0.81119787693)\n",
      " state (15)  A[0]:(0.995023369789) A[1]:(0.973675906658) A[2]:(1.0) A[3]:(0.914345622063)\n",
      "Episode 242000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6243. Times reached goal: 943.               Steps done: 2589082. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0675801136722.\n",
      " state (0)  A[0]:(0.530699729919) A[1]:(0.590672254562) A[2]:(0.589992940426) A[3]:(0.53112077713)\n",
      " state (1)  A[0]:(0.531898200512) A[1]:(-0.000133424066007) A[2]:(0.655972957611) A[3]:(0.590145885944)\n",
      " state (2)  A[0]:(0.589018642902) A[1]:(0.728824138641) A[2]:(0.590047299862) A[3]:(0.655984222889)\n",
      " state (3)  A[0]:(0.655784845352) A[1]:(-0.042825832963) A[2]:(0.445826411247) A[3]:(0.541799902916)\n",
      " state (4)  A[0]:(0.591062307358) A[1]:(0.656378626823) A[2]:(0.000944852537941) A[3]:(0.531429052353)\n",
      " state (5)  A[0]:(0.194332540035) A[1]:(0.917295277119) A[2]:(-0.103960953653) A[3]:(0.496862858534)\n",
      " state (6)  A[0]:(-0.000428765983088) A[1]:(0.810136914253) A[2]:(-0.00157439580653) A[3]:(0.655318379402)\n",
      " state (7)  A[0]:(0.636341929436) A[1]:(-0.209150522947) A[2]:(0.0856662169099) A[3]:(0.920676648617)\n",
      " state (8)  A[0]:(0.65673327446) A[1]:(-5.23319467902e-05) A[2]:(0.728751540184) A[3]:(0.591279268265)\n",
      " state (9)  A[0]:(0.656504750252) A[1]:(0.8099796772) A[2]:(0.809932172298) A[3]:(0.00113210035488)\n",
      " state (10)  A[0]:(0.729816198349) A[1]:(0.900042355061) A[2]:(-0.000285387039185) A[3]:(0.729046583176)\n",
      " state (11)  A[0]:(0.497812181711) A[1]:(0.872807383537) A[2]:(-0.492493718863) A[3]:(0.834384918213)\n",
      " state (12)  A[0]:(0.0467059016228) A[1]:(0.8131326437) A[2]:(-0.30713775754) A[3]:(0.784105062485)\n",
      " state (13)  A[0]:(-5.44190406799e-05) A[1]:(0.798736929893) A[2]:(0.900389015675) A[3]:(0.729183077812)\n",
      " state (14)  A[0]:(0.810896277428) A[1]:(0.902290046215) A[2]:(0.999996960163) A[3]:(0.810316503048)\n",
      " state (15)  A[0]:(0.994938790798) A[1]:(0.973623514175) A[2]:(1.0) A[3]:(0.913658022881)\n",
      "Episode 243000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6215. Times reached goal: 940.               Steps done: 2595297. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0671614057484.\n",
      " state (0)  A[0]:(0.532572448254) A[1]:(0.59039413929) A[2]:(0.59075987339) A[3]:(0.533105015755)\n",
      " state (1)  A[0]:(0.533253252506) A[1]:(-0.000289095100015) A[2]:(0.655712962151) A[3]:(0.591980338097)\n",
      " state (2)  A[0]:(0.590730547905) A[1]:(0.72864317894) A[2]:(0.589782834053) A[3]:(0.657306373119)\n",
      " state (3)  A[0]:(0.655715107918) A[1]:(-0.038987249136) A[2]:(0.445921748877) A[3]:(0.544859409332)\n",
      " state (4)  A[0]:(0.59113317728) A[1]:(0.655930221081) A[2]:(0.000196933746338) A[3]:(0.534253060818)\n",
      " state (5)  A[0]:(0.194561257958) A[1]:(0.917220652103) A[2]:(-0.10519990325) A[3]:(0.499570041895)\n",
      " state (6)  A[0]:(-0.000465929479105) A[1]:(0.809688746929) A[2]:(-0.0013927211985) A[3]:(0.656845927238)\n",
      " state (7)  A[0]:(0.635815143585) A[1]:(-0.210510879755) A[2]:(0.0879091620445) A[3]:(0.920619368553)\n",
      " state (8)  A[0]:(0.655989170074) A[1]:(-0.000934136274736) A[2]:(0.728733420372) A[3]:(0.591661155224)\n",
      " state (9)  A[0]:(0.655382633209) A[1]:(0.809661448002) A[2]:(0.809887230396) A[3]:(0.000994518049993)\n",
      " state (10)  A[0]:(0.728901326656) A[1]:(0.899936914444) A[2]:(-0.000619172991719) A[3]:(0.729060351849)\n",
      " state (11)  A[0]:(0.49674397707) A[1]:(0.872880935669) A[2]:(-0.493915915489) A[3]:(0.83454144001)\n",
      " state (12)  A[0]:(0.0451609045267) A[1]:(0.813497781754) A[2]:(-0.310269594193) A[3]:(0.784236133099)\n",
      " state (13)  A[0]:(-0.00239261519164) A[1]:(0.799227595329) A[2]:(0.900319218636) A[3]:(0.729078531265)\n",
      " state (14)  A[0]:(0.80999994278) A[1]:(0.902483344078) A[2]:(0.999997079372) A[3]:(0.81012403965)\n",
      " state (15)  A[0]:(0.994859755039) A[1]:(0.973551273346) A[2]:(1.0) A[3]:(0.913272619247)\n",
      "Episode 244000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6139. Times reached goal: 940.               Steps done: 2601436. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0667503648594.\n",
      "q_values \n",
      "tensor([[ 0.5322,  0.5898,  0.5917,  0.5321]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5334, -0.0001,  0.6554,  0.5919]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5911,  0.7289,  0.5899,  0.6567]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  0.8099, -0.0012,  0.6554]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0005,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9026,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531552433968) A[1]:(0.589624047279) A[2]:(0.592044234276) A[3]:(0.531787753105)\n",
      " state (1)  A[0]:(0.532839000225) A[1]:(4.78161964566e-05) A[2]:(0.655646145344) A[3]:(0.591444849968)\n",
      " state (2)  A[0]:(0.590584337711) A[1]:(0.728995919228) A[2]:(0.590090036392) A[3]:(0.656287670135)\n",
      " state (3)  A[0]:(0.654917359352) A[1]:(-0.037752840668) A[2]:(0.447533547878) A[3]:(0.543420255184)\n",
      " state (4)  A[0]:(0.590322256088) A[1]:(0.65554612875) A[2]:(0.000995397218503) A[3]:(0.53189098835)\n",
      " state (5)  A[0]:(0.193256095052) A[1]:(0.917426943779) A[2]:(-0.105654940009) A[3]:(0.49675783515)\n",
      " state (6)  A[0]:(-0.00185143738054) A[1]:(0.81003344059) A[2]:(-0.00120449007954) A[3]:(0.654502272606)\n",
      " state (7)  A[0]:(0.634792625904) A[1]:(-0.21047565341) A[2]:(0.0896044522524) A[3]:(0.919753909111)\n",
      " state (8)  A[0]:(0.655272603035) A[1]:(-0.000194774707779) A[2]:(0.728770375252) A[3]:(0.589702248573)\n",
      " state (9)  A[0]:(0.655048847198) A[1]:(0.809884309769) A[2]:(0.809898853302) A[3]:(1.61901116371e-05)\n",
      " state (10)  A[0]:(0.72877818346) A[1]:(0.900039494038) A[2]:(-0.000559568346944) A[3]:(0.728789746761)\n",
      " state (11)  A[0]:(0.497036337852) A[1]:(0.873145341873) A[2]:(-0.494934767485) A[3]:(0.834403038025)\n",
      " state (12)  A[0]:(0.0456766709685) A[1]:(0.814077019691) A[2]:(-0.313069880009) A[3]:(0.783937573433)\n",
      " state (13)  A[0]:(-0.00226199231111) A[1]:(0.799896121025) A[2]:(0.900250971317) A[3]:(0.728459239006)\n",
      " state (14)  A[0]:(0.810189545155) A[1]:(0.902741730213) A[2]:(0.999997138977) A[3]:(0.80962407589)\n",
      " state (15)  A[0]:(0.994822144508) A[1]:(0.97349178791) A[2]:(1.0) A[3]:(0.912792801857)\n",
      "Episode 245000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6208. Times reached goal: 942.               Steps done: 2607644. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0663372621918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531645834446) A[1]:(0.59093695879) A[2]:(0.590707778931) A[3]:(0.533830344677)\n",
      " state (1)  A[0]:(0.532299339771) A[1]:(3.71942296624e-05) A[2]:(0.655991792679) A[3]:(0.593147754669)\n",
      " state (2)  A[0]:(0.590244531631) A[1]:(0.729167103767) A[2]:(0.590009689331) A[3]:(0.658312499523)\n",
      " state (3)  A[0]:(0.655643105507) A[1]:(-0.0191803593189) A[2]:(0.446289271116) A[3]:(0.547037363052)\n",
      " state (4)  A[0]:(0.59149992466) A[1]:(0.655978560448) A[2]:(0.000333189964294) A[3]:(0.534541964531)\n",
      " state (5)  A[0]:(0.194682180882) A[1]:(0.91759032011) A[2]:(-0.107190996408) A[3]:(0.500387072563)\n",
      " state (6)  A[0]:(-0.00050935143372) A[1]:(0.810257017612) A[2]:(-0.00157296529505) A[3]:(0.657478153706)\n",
      " state (7)  A[0]:(0.635495185852) A[1]:(-0.210066825151) A[2]:(0.091568030417) A[3]:(0.920277297497)\n",
      " state (8)  A[0]:(0.655832111835) A[1]:(0.000790030520875) A[2]:(0.729137003422) A[3]:(0.591659724712)\n",
      " state (9)  A[0]:(0.655505776405) A[1]:(0.81018280983) A[2]:(0.810198187828) A[3]:(0.00102376902942)\n",
      " state (10)  A[0]:(0.729289531708) A[1]:(0.900211513042) A[2]:(-0.000227212905884) A[3]:(0.729315161705)\n",
      " state (11)  A[0]:(0.498290032148) A[1]:(0.873475313187) A[2]:(-0.495892912149) A[3]:(0.834941744804)\n",
      " state (12)  A[0]:(0.0472660176456) A[1]:(0.814666211605) A[2]:(-0.315500587225) A[3]:(0.784633159637)\n",
      " state (13)  A[0]:(-0.00122144760098) A[1]:(0.800460457802) A[2]:(0.900415956974) A[3]:(0.72916829586)\n",
      " state (14)  A[0]:(0.810612380505) A[1]:(0.902863264084) A[2]:(0.999997258186) A[3]:(0.810153603554)\n",
      " state (15)  A[0]:(0.994783997536) A[1]:(0.973357319832) A[2]:(1.0) A[3]:(0.912806332111)\n",
      "Episode 246000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6233. Times reached goal: 933.               Steps done: 2613877. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0659250679743.\n",
      " state (0)  A[0]:(0.53068625927) A[1]:(0.590187728405) A[2]:(0.588919639587) A[3]:(0.530741214752)\n",
      " state (1)  A[0]:(0.531776785851) A[1]:(8.0531812273e-05) A[2]:(0.65560823679) A[3]:(0.590164065361)\n",
      " state (2)  A[0]:(0.589393854141) A[1]:(0.728322267532) A[2]:(0.591863751411) A[3]:(0.65490591526)\n",
      " state (3)  A[0]:(0.65441942215) A[1]:(-0.0385594964027) A[2]:(0.453620165586) A[3]:(0.541430234909)\n",
      " state (4)  A[0]:(0.5903018713) A[1]:(0.655710339546) A[2]:(0.0053840354085) A[3]:(0.530290842056)\n",
      " state (5)  A[0]:(0.192891344428) A[1]:(0.917857408524) A[2]:(-0.104495108128) A[3]:(0.496173709631)\n",
      " state (6)  A[0]:(0.00149464490823) A[1]:(0.809290647507) A[2]:(0.00097179383738) A[3]:(0.655384242535)\n",
      " state (7)  A[0]:(0.6377145648) A[1]:(-0.214959636331) A[2]:(0.0944186076522) A[3]:(0.919609129429)\n",
      " state (8)  A[0]:(0.656603217125) A[1]:(-0.000391119712731) A[2]:(0.728405475616) A[3]:(0.590264737606)\n",
      " state (9)  A[0]:(0.655922710896) A[1]:(0.809832215309) A[2]:(0.809819340706) A[3]:(-0.000929459638428)\n",
      " state (10)  A[0]:(0.729214191437) A[1]:(0.899826347828) A[2]:(0.000161528587341) A[3]:(0.727562129498)\n",
      " state (11)  A[0]:(0.498345255852) A[1]:(0.872828245163) A[2]:(-0.496399402618) A[3]:(0.833694458008)\n",
      " state (12)  A[0]:(0.0474346466362) A[1]:(0.81350761652) A[2]:(-0.318324238062) A[3]:(0.783071160316)\n",
      " state (13)  A[0]:(-0.00197398406453) A[1]:(0.798741340637) A[2]:(0.899729609489) A[3]:(0.727321505547)\n",
      " state (14)  A[0]:(0.809970140457) A[1]:(0.901586294174) A[2]:(0.999997317791) A[3]:(0.809056580067)\n",
      " state (15)  A[0]:(0.994726836681) A[1]:(0.97281485796) A[2]:(1.0) A[3]:(0.912338495255)\n",
      "Episode 247000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6158. Times reached goal: 937.               Steps done: 2620035. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.065520348815.\n",
      " state (0)  A[0]:(0.531671524048) A[1]:(0.590674698353) A[2]:(0.590391516685) A[3]:(0.53158390522)\n",
      " state (1)  A[0]:(0.532374799252) A[1]:(0.000158167138579) A[2]:(0.656486988068) A[3]:(0.590740799904)\n",
      " state (2)  A[0]:(0.590342998505) A[1]:(0.729278087616) A[2]:(0.590990066528) A[3]:(0.655847549438)\n",
      " state (3)  A[0]:(0.655820429325) A[1]:(-0.0242909919471) A[2]:(0.449662089348) A[3]:(0.544036984444)\n",
      " state (4)  A[0]:(0.592022299767) A[1]:(0.655964970589) A[2]:(0.000689148786478) A[3]:(0.53208398819)\n",
      " state (5)  A[0]:(0.194816023111) A[1]:(0.91787481308) A[2]:(-0.108768835664) A[3]:(0.497982442379)\n",
      " state (6)  A[0]:(0.000583767832723) A[1]:(0.810088396072) A[2]:(-0.00122189463582) A[3]:(0.65565943718)\n",
      " state (7)  A[0]:(0.636078476906) A[1]:(-0.212200701237) A[2]:(0.0959213376045) A[3]:(0.919344425201)\n",
      " state (8)  A[0]:(0.656057715416) A[1]:(-0.000165068850038) A[2]:(0.72967761755) A[3]:(0.58954590559)\n",
      " state (9)  A[0]:(0.655743122101) A[1]:(0.809716105461) A[2]:(0.810525596142) A[3]:(-0.00139823462814)\n",
      " state (10)  A[0]:(0.729568481445) A[1]:(0.900014638901) A[2]:(4.08887863159e-05) A[3]:(0.728663682938)\n",
      " state (11)  A[0]:(0.499498516321) A[1]:(0.8735435009) A[2]:(-0.497903078794) A[3]:(0.834866344929)\n",
      " state (12)  A[0]:(0.0487919449806) A[1]:(0.815147817135) A[2]:(-0.32063922286) A[3]:(0.784549474716)\n",
      " state (13)  A[0]:(-0.00119852961507) A[1]:(0.800984799862) A[2]:(0.900344908237) A[3]:(0.728825092316)\n",
      " state (14)  A[0]:(0.810171484947) A[1]:(0.902840077877) A[2]:(0.999997437) A[3]:(0.80995464325)\n",
      " state (15)  A[0]:(0.99465918541) A[1]:(0.973048865795) A[2]:(1.0) A[3]:(0.912323474884)\n",
      "Episode 248000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6197. Times reached goal: 929.               Steps done: 2626232. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0651155747013.\n",
      " state (0)  A[0]:(0.531698703766) A[1]:(0.59096711874) A[2]:(0.590954065323) A[3]:(0.532142937183)\n",
      " state (1)  A[0]:(0.532302737236) A[1]:(-0.000103554804809) A[2]:(0.656269013882) A[3]:(0.591336190701)\n",
      " state (2)  A[0]:(0.590349256992) A[1]:(0.729150056839) A[2]:(0.589958786964) A[3]:(0.656548857689)\n",
      " state (3)  A[0]:(0.655702710152) A[1]:(-0.00733799627051) A[2]:(0.447477310896) A[3]:(0.546075820923)\n",
      " state (4)  A[0]:(0.591416478157) A[1]:(0.65663921833) A[2]:(0.000135660171509) A[3]:(0.532542228699)\n",
      " state (5)  A[0]:(0.193498685956) A[1]:(0.918035745621) A[2]:(-0.109044358134) A[3]:(0.498337268829)\n",
      " state (6)  A[0]:(0.000385463208659) A[1]:(0.809976816177) A[2]:(-0.000477194756968) A[3]:(0.6560562253)\n",
      " state (7)  A[0]:(0.636788725853) A[1]:(-0.213380977511) A[2]:(0.0973089709878) A[3]:(0.919317245483)\n",
      " state (8)  A[0]:(0.657070338726) A[1]:(0.000464119802928) A[2]:(0.728823184967) A[3]:(0.590800881386)\n",
      " state (9)  A[0]:(0.656466960907) A[1]:(0.809993445873) A[2]:(0.809852480888) A[3]:(0.000169716775417)\n",
      " state (10)  A[0]:(0.729660153389) A[1]:(0.900020897388) A[2]:(-0.000752091291361) A[3]:(0.72863805294)\n",
      " state (11)  A[0]:(0.499841570854) A[1]:(0.873463630676) A[2]:(-0.499072194099) A[3]:(0.834715306759)\n",
      " state (12)  A[0]:(0.0494964644313) A[1]:(0.814844191074) A[2]:(-0.323733359575) A[3]:(0.784324645996)\n",
      " state (13)  A[0]:(-0.00100755656604) A[1]:(0.80010509491) A[2]:(0.899814486504) A[3]:(0.728489875793)\n",
      " state (14)  A[0]:(0.810091137886) A[1]:(0.901827871799) A[2]:(0.999997496605) A[3]:(0.80975651741)\n",
      " state (15)  A[0]:(0.994620084763) A[1]:(0.97249341011) A[2]:(1.0) A[3]:(0.912122547626)\n",
      "Episode 249000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6205. Times reached goal: 931.               Steps done: 2632437. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0647127835123.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5898,  0.5908,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.3228e-01, -1.4396e-06,  6.5603e-01,  5.9078e-01]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.7289,  0.5907,  0.6559]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0010,  0.8102, -0.0006,  0.6552]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9001, -0.0004,  0.7284]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8110,  0.9024,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53111743927) A[1]:(0.590075135231) A[2]:(0.590788125992) A[3]:(0.531573057175)\n",
      " state (1)  A[0]:(0.531917929649) A[1]:(0.000466616096674) A[2]:(0.656037390232) A[3]:(0.590948581696)\n",
      " state (2)  A[0]:(0.590038657188) A[1]:(0.728824257851) A[2]:(0.590615332127) A[3]:(0.656079292297)\n",
      " state (3)  A[0]:(0.654523134232) A[1]:(-0.0349235013127) A[2]:(0.452627539635) A[3]:(0.54214322567)\n",
      " state (4)  A[0]:(0.590527713299) A[1]:(0.655690848827) A[2]:(0.00122737826314) A[3]:(0.531003415585)\n",
      " state (5)  A[0]:(0.192402645946) A[1]:(0.918147444725) A[2]:(-0.109731733799) A[3]:(0.497663497925)\n",
      " state (6)  A[0]:(-0.000338047742844) A[1]:(0.809937953949) A[2]:(-0.000892162090167) A[3]:(0.655601859093)\n",
      " state (7)  A[0]:(0.635867118835) A[1]:(-0.214326143265) A[2]:(0.0985805392265) A[3]:(0.918889045715)\n",
      " state (8)  A[0]:(0.655871152878) A[1]:(-0.000270156888291) A[2]:(0.728877604008) A[3]:(0.589772462845)\n",
      " state (9)  A[0]:(0.655432105064) A[1]:(0.809727966785) A[2]:(0.810063540936) A[3]:(-0.000978730269708)\n",
      " state (10)  A[0]:(0.72920358181) A[1]:(0.899953305721) A[2]:(-0.000648021581583) A[3]:(0.728587329388)\n",
      " state (11)  A[0]:(0.499863922596) A[1]:(0.873589575291) A[2]:(-0.500121057034) A[3]:(0.834944725037)\n",
      " state (12)  A[0]:(0.0500532798469) A[1]:(0.81534332037) A[2]:(-0.325540602207) A[3]:(0.784653306007)\n",
      " state (13)  A[0]:(0.00011733174324) A[1]:(0.800919413567) A[2]:(0.900549709797) A[3]:(0.72884106636)\n",
      " state (14)  A[0]:(0.810937047005) A[1]:(0.90230602026) A[2]:(0.999997615814) A[3]:(0.810109913349)\n",
      " state (15)  A[0]:(0.994587957859) A[1]:(0.972503185272) A[2]:(1.0) A[3]:(0.912007749081)\n",
      "Episode 250000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6127. Times reached goal: 925.               Steps done: 2638564. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0643175004739.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531107008457) A[1]:(0.590995073318) A[2]:(0.591034650803) A[3]:(0.531790614128)\n",
      " state (1)  A[0]:(0.532091617584) A[1]:(0.000229771481827) A[2]:(0.656554341316) A[3]:(0.591394543648)\n",
      " state (2)  A[0]:(0.590297579765) A[1]:(0.729068994522) A[2]:(0.591072559357) A[3]:(0.656495928764)\n",
      " state (3)  A[0]:(0.655385017395) A[1]:(-0.0326024070382) A[2]:(0.453349530697) A[3]:(0.543179750443)\n",
      " state (4)  A[0]:(0.591392040253) A[1]:(0.656136393547) A[2]:(0.00106823409442) A[3]:(0.531864881516)\n",
      " state (5)  A[0]:(0.192661508918) A[1]:(0.918304741383) A[2]:(-0.110560201108) A[3]:(0.498400837183)\n",
      " state (6)  A[0]:(-0.000245690345764) A[1]:(0.809988856316) A[2]:(-0.000878691440448) A[3]:(0.656189978123)\n",
      " state (7)  A[0]:(0.636146187782) A[1]:(-0.214849427342) A[2]:(0.100440077484) A[3]:(0.919005572796)\n",
      " state (8)  A[0]:(0.656429648399) A[1]:(0.000499214220326) A[2]:(0.729200899601) A[3]:(0.590800404549)\n",
      " state (9)  A[0]:(0.656153440475) A[1]:(0.810176372528) A[2]:(0.810242295265) A[3]:(-2.04145908356e-05)\n",
      " state (10)  A[0]:(0.729657530785) A[1]:(0.900178194046) A[2]:(-0.000383257836802) A[3]:(0.728857219219)\n",
      " state (11)  A[0]:(0.500638008118) A[1]:(0.873962879181) A[2]:(-0.501012086868) A[3]:(0.835129201412)\n",
      " state (12)  A[0]:(0.0508248284459) A[1]:(0.816015958786) A[2]:(-0.328126758337) A[3]:(0.784822285175)\n",
      " state (13)  A[0]:(0.000298798084259) A[1]:(0.801611185074) A[2]:(0.900437355042) A[3]:(0.72890162468)\n",
      " state (14)  A[0]:(0.811074137688) A[1]:(0.902490377426) A[2]:(0.999997675419) A[3]:(0.810170352459)\n",
      " state (15)  A[0]:(0.994551062584) A[1]:(0.972398459911) A[2]:(1.0) A[3]:(0.911847651005)\n",
      "Episode 251000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6149. Times reached goal: 933.               Steps done: 2644713. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0639232256039.\n",
      " state (0)  A[0]:(0.531567633152) A[1]:(0.589969933033) A[2]:(0.589772939682) A[3]:(0.530707418919)\n",
      " state (1)  A[0]:(0.532294869423) A[1]:(0.000485853262944) A[2]:(0.656367182732) A[3]:(0.590107142925)\n",
      " state (2)  A[0]:(0.59014582634) A[1]:(0.729153871536) A[2]:(0.592487514019) A[3]:(0.65506029129)\n",
      " state (3)  A[0]:(0.655019402504) A[1]:(-0.0352895446122) A[2]:(0.457442581654) A[3]:(0.541686177254)\n",
      " state (4)  A[0]:(0.590961992741) A[1]:(0.65648663044) A[2]:(0.0051095043309) A[3]:(0.530787587166)\n",
      " state (5)  A[0]:(0.19171115756) A[1]:(0.918659448624) A[2]:(-0.107487119734) A[3]:(0.49725034833)\n",
      " state (6)  A[0]:(-0.000432074040873) A[1]:(0.81040585041) A[2]:(0.00269948784262) A[3]:(0.655169963837)\n",
      " state (7)  A[0]:(0.636442899704) A[1]:(-0.214848220348) A[2]:(0.104461975396) A[3]:(0.918564260006)\n",
      " state (8)  A[0]:(0.65741109848) A[1]:(0.00147126603406) A[2]:(0.729088008404) A[3]:(0.591858506203)\n",
      " state (9)  A[0]:(0.657206952572) A[1]:(0.810570776463) A[2]:(0.810193181038) A[3]:(0.00234013376758)\n",
      " state (10)  A[0]:(0.730046510696) A[1]:(0.900385916233) A[2]:(0.00110876513645) A[3]:(0.729132771492)\n",
      " state (11)  A[0]:(0.501159250736) A[1]:(0.874341845512) A[2]:(-0.500373184681) A[3]:(0.835105776787)\n",
      " state (12)  A[0]:(0.0511811412871) A[1]:(0.816739797592) A[2]:(-0.328918814659) A[3]:(0.78465116024)\n",
      " state (13)  A[0]:(-0.000581294239964) A[1]:(0.802362620831) A[2]:(0.900462448597) A[3]:(0.728471636772)\n",
      " state (14)  A[0]:(0.810240983963) A[1]:(0.902678966522) A[2]:(0.999997735023) A[3]:(0.809759020805)\n",
      " state (15)  A[0]:(0.994469106197) A[1]:(0.972302496433) A[2]:(1.0) A[3]:(0.911440253258)\n",
      "Episode 252000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6243. Times reached goal: 942.               Steps done: 2650956. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0635253960236.\n",
      " state (0)  A[0]:(0.531891584396) A[1]:(0.590478301048) A[2]:(0.590232729912) A[3]:(0.531642854214)\n",
      " state (1)  A[0]:(0.532510280609) A[1]:(0.000691530643962) A[2]:(0.656041741371) A[3]:(0.590790748596)\n",
      " state (2)  A[0]:(0.590297698975) A[1]:(0.729088902473) A[2]:(0.590996086597) A[3]:(0.655834197998)\n",
      " state (3)  A[0]:(0.6556173563) A[1]:(-0.0150766884908) A[2]:(0.453000754118) A[3]:(0.544503808022)\n",
      " state (4)  A[0]:(0.59154176712) A[1]:(0.656150460243) A[2]:(0.00032651424408) A[3]:(0.531967222691)\n",
      " state (5)  A[0]:(0.192035987973) A[1]:(0.918646097183) A[2]:(-0.113381110132) A[3]:(0.498477429152)\n",
      " state (6)  A[0]:(-0.000553965510335) A[1]:(0.810161828995) A[2]:(-0.00215410860255) A[3]:(0.655988931656)\n",
      " state (7)  A[0]:(0.635744154453) A[1]:(-0.216489508748) A[2]:(0.102728992701) A[3]:(0.918497741222)\n",
      " state (8)  A[0]:(0.656097769737) A[1]:(0.000492153165396) A[2]:(0.728830933571) A[3]:(0.590531766415)\n",
      " state (9)  A[0]:(0.656130313873) A[1]:(0.810206472874) A[2]:(0.810019910336) A[3]:(0.000481635303004)\n",
      " state (10)  A[0]:(0.729635357857) A[1]:(0.900175988674) A[2]:(-0.000852346187457) A[3]:(0.729190349579)\n",
      " state (11)  A[0]:(0.50116610527) A[1]:(0.8741787076) A[2]:(-0.503190875053) A[3]:(0.835503697395)\n",
      " state (12)  A[0]:(0.0513951517642) A[1]:(0.816603660583) A[2]:(-0.333448171616) A[3]:(0.785230576992)\n",
      " state (13)  A[0]:(-0.000568807066884) A[1]:(0.802099049091) A[2]:(0.900081276894) A[3]:(0.729070544243)\n",
      " state (14)  A[0]:(0.810361385345) A[1]:(0.902282595634) A[2]:(0.999997794628) A[3]:(0.81015008688)\n",
      " state (15)  A[0]:(0.994428217411) A[1]:(0.971990466118) A[2]:(1.0) A[3]:(0.911379992962)\n",
      "Episode 253000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6225. Times reached goal: 963.               Steps done: 2657181. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0631311787077.\n",
      " state (0)  A[0]:(0.531432390213) A[1]:(0.590603649616) A[2]:(0.590611636639) A[3]:(0.529957532883)\n",
      " state (1)  A[0]:(0.53195297718) A[1]:(-1.91847793758e-05) A[2]:(0.656236767769) A[3]:(0.589606225491)\n",
      " state (2)  A[0]:(0.590116024017) A[1]:(0.729205965996) A[2]:(0.590242087841) A[3]:(0.655215322971)\n",
      " state (3)  A[0]:(0.655366420746) A[1]:(-0.000132582616061) A[2]:(0.452039301395) A[3]:(0.545602202415)\n",
      " state (4)  A[0]:(0.591196119785) A[1]:(0.656304597855) A[2]:(0.00110673857853) A[3]:(0.532039999962)\n",
      " state (5)  A[0]:(0.191776558757) A[1]:(0.918713331223) A[2]:(-0.112747378647) A[3]:(0.49839541316)\n",
      " state (6)  A[0]:(-0.00062680238625) A[1]:(0.810307562351) A[2]:(-0.000267028808594) A[3]:(0.655547380447)\n",
      " state (7)  A[0]:(0.635725200176) A[1]:(-0.21629999578) A[2]:(0.106397986412) A[3]:(0.918146014214)\n",
      " state (8)  A[0]:(0.656253814697) A[1]:(0.00077537476318) A[2]:(0.729728579521) A[3]:(0.59005266428)\n",
      " state (9)  A[0]:(0.656093835831) A[1]:(0.810149610043) A[2]:(0.810509026051) A[3]:(-7.32690095901e-05)\n",
      " state (10)  A[0]:(0.729588091373) A[1]:(0.90003067255) A[2]:(0.000415325135691) A[3]:(0.729162454605)\n",
      " state (11)  A[0]:(0.501537084579) A[1]:(0.87394708395) A[2]:(-0.503085613251) A[3]:(0.835675299168)\n",
      " state (12)  A[0]:(0.0521693639457) A[1]:(0.816151022911) A[2]:(-0.334593385458) A[3]:(0.785541415215)\n",
      " state (13)  A[0]:(1.89244747162e-05) A[1]:(0.801229119301) A[2]:(0.900203347206) A[3]:(0.729484200478)\n",
      " state (14)  A[0]:(0.810680866241) A[1]:(0.901437878609) A[2]:(0.999997854233) A[3]:(0.810599446297)\n",
      " state (15)  A[0]:(0.994402468204) A[1]:(0.97151696682) A[2]:(1.0) A[3]:(0.911509215832)\n",
      "Episode 254000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6163. Times reached goal: 934.               Steps done: 2663344. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0627432977363.\n",
      "q_values \n",
      "tensor([[ 0.5327,  0.5909,  0.5903,  0.5321]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5916,  0.6568,  0.0004,  0.5319]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6570,  0.0002,  0.7294,  0.5899]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.8102,  0.8102, -0.0006]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7299,  0.9000, -0.0005,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8108,  0.9017,  1.0000,  0.8104]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53220975399) A[1]:(0.590778589249) A[2]:(0.590079784393) A[3]:(0.531745672226)\n",
      " state (1)  A[0]:(0.53216946125) A[1]:(3.28989699483e-06) A[2]:(0.656147360802) A[3]:(0.590208888054)\n",
      " state (2)  A[0]:(0.590175628662) A[1]:(0.729571938515) A[2]:(0.590427279472) A[3]:(0.655301690102)\n",
      " state (3)  A[0]:(0.655808568001) A[1]:(-0.00499317701906) A[2]:(0.452923804522) A[3]:(0.545221805573)\n",
      " state (4)  A[0]:(0.591599225998) A[1]:(0.657004833221) A[2]:(0.00015127658844) A[3]:(0.532037138939)\n",
      " state (5)  A[0]:(0.191424831748) A[1]:(0.91889744997) A[2]:(-0.114302307367) A[3]:(0.498393416405)\n",
      " state (6)  A[0]:(-5.51342964172e-05) A[1]:(0.809958577156) A[2]:(-0.00125062395819) A[3]:(0.655889153481)\n",
      " state (7)  A[0]:(0.636894822121) A[1]:(-0.218781456351) A[2]:(0.106978647411) A[3]:(0.918180525303)\n",
      " state (8)  A[0]:(0.657136917114) A[1]:(-3.98936681449e-05) A[2]:(0.729580640793) A[3]:(0.590171873569)\n",
      " state (9)  A[0]:(0.656837701797) A[1]:(0.810039460659) A[2]:(0.810296177864) A[3]:(2.83494591713e-05)\n",
      " state (10)  A[0]:(0.729929089546) A[1]:(0.899954855442) A[2]:(-0.000515937746968) A[3]:(0.72911632061)\n",
      " state (11)  A[0]:(0.502054870129) A[1]:(0.873962044716) A[2]:(-0.504770994186) A[3]:(0.835656583309)\n",
      " state (12)  A[0]:(0.0526959188282) A[1]:(0.816384375095) A[2]:(-0.337640732527) A[3]:(0.785467743874)\n",
      " state (13)  A[0]:(0.000308036804199) A[1]:(0.801629424095) A[2]:(0.900278151035) A[3]:(0.729299783707)\n",
      " state (14)  A[0]:(0.810926318169) A[1]:(0.901625394821) A[2]:(0.999997973442) A[3]:(0.81050992012)\n",
      " state (15)  A[0]:(0.994357287884) A[1]:(0.971448600292) A[2]:(1.0) A[3]:(0.911227703094)\n",
      "Episode 255000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6118. Times reached goal: 923.               Steps done: 2669462. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0623606060882.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531627297401) A[1]:(0.590176939964) A[2]:(0.590276479721) A[3]:(0.531526446342)\n",
      " state (1)  A[0]:(0.532098770142) A[1]:(-0.000142767094076) A[2]:(0.655961096287) A[3]:(0.590723156929)\n",
      " state (2)  A[0]:(0.590209901333) A[1]:(0.728686213493) A[2]:(0.590130925179) A[3]:(0.655778944492)\n",
      " state (3)  A[0]:(0.655288159847) A[1]:(-0.00280020316131) A[2]:(0.452835202217) A[3]:(0.545881927013)\n",
      " state (4)  A[0]:(0.590850293636) A[1]:(0.65569126606) A[2]:(0.000259637832642) A[3]:(0.53193962574)\n",
      " state (5)  A[0]:(0.190463542938) A[1]:(0.918864548206) A[2]:(-0.114462547004) A[3]:(0.498256295919)\n",
      " state (6)  A[0]:(-0.00023427605629) A[1]:(0.809810459614) A[2]:(-0.000568151415791) A[3]:(0.655852437019)\n",
      " state (7)  A[0]:(0.636226415634) A[1]:(-0.21966958046) A[2]:(0.108287833631) A[3]:(0.917960047722)\n",
      " state (8)  A[0]:(0.656136095524) A[1]:(-0.000208898447454) A[2]:(0.728728055954) A[3]:(0.590816259384)\n",
      " state (9)  A[0]:(0.655707836151) A[1]:(0.809976518154) A[2]:(0.80995953083) A[3]:(0.000113777816296)\n",
      " state (10)  A[0]:(0.72923129797) A[1]:(0.900060355663) A[2]:(-0.000661492231302) A[3]:(0.728809475899)\n",
      " state (11)  A[0]:(0.501727819443) A[1]:(0.874359369278) A[2]:(-0.505571603775) A[3]:(0.835517883301)\n",
      " state (12)  A[0]:(0.0526583231986) A[1]:(0.817237377167) A[2]:(-0.339890837669) A[3]:(0.785252213478)\n",
      " state (13)  A[0]:(-0.000353932351572) A[1]:(0.802578747272) A[2]:(0.900057196617) A[3]:(0.728843808174)\n",
      " state (14)  A[0]:(0.810354650021) A[1]:(0.90193772316) A[2]:(0.999997973442) A[3]:(0.810077309608)\n",
      " state (15)  A[0]:(0.994286835194) A[1]:(0.971389651299) A[2]:(1.0) A[3]:(0.910804331303)\n",
      "Episode 256000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6167. Times reached goal: 930.               Steps done: 2675629. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0619772116423.\n",
      " state (0)  A[0]:(0.531347155571) A[1]:(0.590505123138) A[2]:(0.590730071068) A[3]:(0.529827833176)\n",
      " state (1)  A[0]:(0.531887412071) A[1]:(-0.000366445601685) A[2]:(0.656068325043) A[3]:(0.589556694031)\n",
      " state (2)  A[0]:(0.590323269367) A[1]:(0.728894591331) A[2]:(0.589775085449) A[3]:(0.654943943024)\n",
      " state (3)  A[0]:(0.655554354191) A[1]:(0.00154230243061) A[2]:(0.452588319778) A[3]:(0.546095132828)\n",
      " state (4)  A[0]:(0.591065764427) A[1]:(0.655976653099) A[2]:(0.000551819743123) A[3]:(0.531925737858)\n",
      " state (5)  A[0]:(0.190583899617) A[1]:(0.919032156467) A[2]:(-0.1142424196) A[3]:(0.498060375452)\n",
      " state (6)  A[0]:(0.000473827094538) A[1]:(0.810049414635) A[2]:(0.000137209892273) A[3]:(0.655765295029)\n",
      " state (7)  A[0]:(0.637102067471) A[1]:(-0.219736337662) A[2]:(0.109893672168) A[3]:(0.917870938778)\n",
      " state (8)  A[0]:(0.657150626183) A[1]:(0.000414404988987) A[2]:(0.729089975357) A[3]:(0.590569257736)\n",
      " state (9)  A[0]:(0.656836271286) A[1]:(0.81013572216) A[2]:(0.810168921947) A[3]:(-0.000389926106436)\n",
      " state (10)  A[0]:(0.730123758316) A[1]:(0.900111377239) A[2]:(-0.000346660614014) A[3]:(0.728641748428)\n",
      " state (11)  A[0]:(0.503365278244) A[1]:(0.874478340149) A[2]:(-0.506299614906) A[3]:(0.835491240025)\n",
      " state (12)  A[0]:(0.0548458211124) A[1]:(0.817447662354) A[2]:(-0.341899335384) A[3]:(0.785222232342)\n",
      " state (13)  A[0]:(0.00157499185298) A[1]:(0.802646040916) A[2]:(0.900205254555) A[3]:(0.728765487671)\n",
      " state (14)  A[0]:(0.811083912849) A[1]:(0.901706814766) A[2]:(0.999998092651) A[3]:(0.810094118118)\n",
      " state (15)  A[0]:(0.994261145592) A[1]:(0.971117377281) A[2]:(1.0) A[3]:(0.910627186298)\n",
      "Episode 257000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6215. Times reached goal: 943.               Steps done: 2681844. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0615932177689.\n",
      " state (0)  A[0]:(0.53073656559) A[1]:(0.59053593874) A[2]:(0.590498924255) A[3]:(0.531272053719)\n",
      " state (1)  A[0]:(0.53158313036) A[1]:(-0.000892216106877) A[2]:(0.656025648117) A[3]:(0.590959668159)\n",
      " state (2)  A[0]:(0.589715659618) A[1]:(0.729139566422) A[2]:(0.592021644115) A[3]:(0.655356585979)\n",
      " state (3)  A[0]:(0.655299127102) A[1]:(-0.0103057492524) A[2]:(0.45756945014) A[3]:(0.544839799404)\n",
      " state (4)  A[0]:(0.590977430344) A[1]:(0.656704306602) A[2]:(0.00297152111307) A[3]:(0.530908226967)\n",
      " state (5)  A[0]:(0.189673259854) A[1]:(0.919589340687) A[2]:(-0.114139825106) A[3]:(0.496801495552)\n",
      " state (6)  A[0]:(0.00106218422297) A[1]:(0.810255169868) A[2]:(-0.000399470300181) A[3]:(0.655563354492)\n",
      " state (7)  A[0]:(0.637430667877) A[1]:(-0.221388965845) A[2]:(0.110053248703) A[3]:(0.917749464512)\n",
      " state (8)  A[0]:(0.655835747719) A[1]:(0.00178969278932) A[2]:(0.728793740273) A[3]:(0.588903844357)\n",
      " state (9)  A[0]:(0.655330896378) A[1]:(0.810641169548) A[2]:(0.809639573097) A[3]:(-0.00221687182784)\n",
      " state (10)  A[0]:(0.728337585926) A[1]:(0.900313735008) A[2]:(-0.00207388098352) A[3]:(0.727467656136)\n",
      " state (11)  A[0]:(0.499972850084) A[1]:(0.874806284904) A[2]:(-0.508520126343) A[3]:(0.834525763988)\n",
      " state (12)  A[0]:(0.049699600786) A[1]:(0.818100750446) A[2]:(-0.345883786678) A[3]:(0.783796608448)\n",
      " state (13)  A[0]:(-0.00432414095849) A[1]:(0.803440272808) A[2]:(0.899804115295) A[3]:(0.726804435253)\n",
      " state (14)  A[0]:(0.808910906315) A[1]:(0.902011811733) A[2]:(0.999998092651) A[3]:(0.808583021164)\n",
      " state (15)  A[0]:(0.994135856628) A[1]:(0.971072077751) A[2]:(1.0) A[3]:(0.909635841846)\n",
      "Episode 258000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6133. Times reached goal: 938.               Steps done: 2687977. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.061216622574.\n",
      " state (0)  A[0]:(0.530043184757) A[1]:(0.590581536293) A[2]:(0.590453147888) A[3]:(0.529695630074)\n",
      " state (1)  A[0]:(0.530396103859) A[1]:(0.000336723402143) A[2]:(0.656275391579) A[3]:(0.590101480484)\n",
      " state (2)  A[0]:(0.589083552361) A[1]:(0.729278981686) A[2]:(0.589930891991) A[3]:(0.655328989029)\n",
      " state (3)  A[0]:(0.654827058315) A[1]:(0.00597703969106) A[2]:(0.452742993832) A[3]:(0.547265172005)\n",
      " state (4)  A[0]:(0.589943647385) A[1]:(0.656426072121) A[2]:(0.000214576721191) A[3]:(0.532041370869)\n",
      " state (5)  A[0]:(0.187897607684) A[1]:(0.919346153736) A[2]:(-0.115160137415) A[3]:(0.497472345829)\n",
      " state (6)  A[0]:(-0.00110530806705) A[1]:(0.809960186481) A[2]:(0.00012743473053) A[3]:(0.655607938766)\n",
      " state (7)  A[0]:(0.63654345274) A[1]:(-0.222392812371) A[2]:(0.111695185304) A[3]:(0.917712450027)\n",
      " state (8)  A[0]:(0.65630030632) A[1]:(5.53894788027e-05) A[2]:(0.728959619999) A[3]:(0.590469837189)\n",
      " state (9)  A[0]:(0.656240463257) A[1]:(0.810073494911) A[2]:(0.810037016869) A[3]:(2.18152999878e-05)\n",
      " state (10)  A[0]:(0.729563117027) A[1]:(0.900111079216) A[2]:(-0.000429868669016) A[3]:(0.728728890419)\n",
      " state (11)  A[0]:(0.502914071083) A[1]:(0.874792337418) A[2]:(-0.50788128376) A[3]:(0.835589647293)\n",
      " state (12)  A[0]:(0.0543122850358) A[1]:(0.818351924419) A[2]:(-0.345941722393) A[3]:(0.785286962986)\n",
      " state (13)  A[0]:(0.000382691592677) A[1]:(0.803770780563) A[2]:(0.900317370892) A[3]:(0.72863650322)\n",
      " state (14)  A[0]:(0.810629785061) A[1]:(0.902040541172) A[2]:(0.999998152256) A[3]:(0.809954822063)\n",
      " state (15)  A[0]:(0.994148373604) A[1]:(0.97092461586) A[2]:(1.0) A[3]:(0.91010928154)\n",
      "Episode 259000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6143. Times reached goal: 941.               Steps done: 2694120. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0608417215489.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5907,  0.5905,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5912,  0.6563,  0.0002,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6567,  0.0005,  0.7291,  0.5909]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6567,  0.8103,  0.8102,  0.0008]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0008,  0.8034,  0.9003,  0.7291]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8107,  0.9014,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531314969063) A[1]:(0.59073472023) A[2]:(0.590453863144) A[3]:(0.531193852425)\n",
      " state (1)  A[0]:(0.531681418419) A[1]:(5.37168234587e-05) A[2]:(0.656220078468) A[3]:(0.590628862381)\n",
      " state (2)  A[0]:(0.590161085129) A[1]:(0.729026317596) A[2]:(0.590717017651) A[3]:(0.655529022217)\n",
      " state (3)  A[0]:(0.655816435814) A[1]:(-0.00799470674247) A[2]:(0.455288439989) A[3]:(0.545315027237)\n",
      " state (4)  A[0]:(0.591124296188) A[1]:(0.656147420406) A[2]:(0.000214457511902) A[3]:(0.531176805496)\n",
      " state (5)  A[0]:(0.188797339797) A[1]:(0.919522225857) A[2]:(-0.1160473153) A[3]:(0.497324943542)\n",
      " state (6)  A[0]:(-6.79194927216e-05) A[1]:(0.810065507889) A[2]:(-0.000450968713267) A[3]:(0.6559664011)\n",
      " state (7)  A[0]:(0.637214064598) A[1]:(-0.222870647907) A[2]:(0.112393572927) A[3]:(0.917742788792)\n",
      " state (8)  A[0]:(0.656462788582) A[1]:(0.000478101865156) A[2]:(0.729180335999) A[3]:(0.590321063995)\n",
      " state (9)  A[0]:(0.6562281847) A[1]:(0.810139536858) A[2]:(0.81016254425) A[3]:(-0.000231996178627)\n",
      " state (10)  A[0]:(0.729446172714) A[1]:(0.900063037872) A[2]:(-0.000570893229451) A[3]:(0.728746592999)\n",
      " state (11)  A[0]:(0.502832531929) A[1]:(0.87468957901) A[2]:(-0.509039282799) A[3]:(0.835696041584)\n",
      " state (12)  A[0]:(0.0541068091989) A[1]:(0.818089604378) A[2]:(-0.348696827888) A[3]:(0.785440921783)\n",
      " state (13)  A[0]:(-0.000206589698792) A[1]:(0.803149938583) A[2]:(0.900097370148) A[3]:(0.728778064251)\n",
      " state (14)  A[0]:(0.810472786427) A[1]:(0.901354670525) A[2]:(0.999998211861) A[3]:(0.810089945793)\n",
      " state (15)  A[0]:(0.994102060795) A[1]:(0.970497965813) A[2]:(1.0) A[3]:(0.910008609295)\n",
      "Episode 260000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6139. Times reached goal: 931.               Steps done: 2700259. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0604693583586.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531674623489) A[1]:(0.59073472023) A[2]:(0.590653181076) A[3]:(0.532575488091)\n",
      " state (1)  A[0]:(0.531772315502) A[1]:(-0.000180548988283) A[2]:(0.656071782112) A[3]:(0.591747283936)\n",
      " state (2)  A[0]:(0.589987814426) A[1]:(0.729067921638) A[2]:(0.590876579285) A[3]:(0.656324744225)\n",
      " state (3)  A[0]:(0.655685901642) A[1]:(-0.00608958862722) A[2]:(0.4563395679) A[3]:(0.546905398369)\n",
      " state (4)  A[0]:(0.590881228447) A[1]:(0.656400442123) A[2]:(0.000602006854024) A[3]:(0.532535791397)\n",
      " state (5)  A[0]:(0.187942236662) A[1]:(0.919691205025) A[2]:(-0.116870343685) A[3]:(0.498134404421)\n",
      " state (6)  A[0]:(-0.00060421222588) A[1]:(0.81008285284) A[2]:(-0.000896930461749) A[3]:(0.656156301498)\n",
      " state (7)  A[0]:(0.637032032013) A[1]:(-0.223578736186) A[2]:(0.113499261439) A[3]:(0.917600333691)\n",
      " state (8)  A[0]:(0.656300961971) A[1]:(0.000819944601972) A[2]:(0.729198813438) A[3]:(0.590244174004)\n",
      " state (9)  A[0]:(0.656162858009) A[1]:(0.810293018818) A[2]:(0.810118198395) A[3]:(0.000301070511341)\n",
      " state (10)  A[0]:(0.729299604893) A[1]:(0.900149106979) A[2]:(-0.000356316537363) A[3]:(0.728888034821)\n",
      " state (11)  A[0]:(0.502816438675) A[1]:(0.874902963638) A[2]:(-0.509495139122) A[3]:(0.83576375246)\n",
      " state (12)  A[0]:(0.0542237050831) A[1]:(0.818538784981) A[2]:(-0.350150167942) A[3]:(0.785473823547)\n",
      " state (13)  A[0]:(-0.000151216983795) A[1]:(0.803627848625) A[2]:(0.900365054607) A[3]:(0.728717565536)\n",
      " state (14)  A[0]:(0.810650348663) A[1]:(0.901443958282) A[2]:(0.999998271465) A[3]:(0.81003266573)\n",
      " state (15)  A[0]:(0.994061410427) A[1]:(0.970358848572) A[2]:(1.0) A[3]:(0.909740686417)\n",
      "Episode 261000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6136. Times reached goal: 924.               Steps done: 2706395. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0600994544016.\n",
      " state (0)  A[0]:(0.531470417976) A[1]:(0.590435266495) A[2]:(0.590501606464) A[3]:(0.531358599663)\n",
      " state (1)  A[0]:(0.53191447258) A[1]:(0.00022148899734) A[2]:(0.656058967113) A[3]:(0.590791225433)\n",
      " state (2)  A[0]:(0.590493202209) A[1]:(0.72909784317) A[2]:(0.590646326542) A[3]:(0.655730724335)\n",
      " state (3)  A[0]:(0.655829191208) A[1]:(-0.00458633853123) A[2]:(0.45652538538) A[3]:(0.546255111694)\n",
      " state (4)  A[0]:(0.591223537922) A[1]:(0.656011283398) A[2]:(0.000827669922728) A[3]:(0.531883358955)\n",
      " state (5)  A[0]:(0.188891649246) A[1]:(0.919741094112) A[2]:(-0.116972997785) A[3]:(0.497817099094)\n",
      " state (6)  A[0]:(0.000552326382603) A[1]:(0.810223758221) A[2]:(-0.000463008851511) A[3]:(0.65601348877)\n",
      " state (7)  A[0]:(0.637359619141) A[1]:(-0.223636671901) A[2]:(0.114842765033) A[3]:(0.91749984026)\n",
      " state (8)  A[0]:(0.656656682491) A[1]:(0.000653354334645) A[2]:(0.729129552841) A[3]:(0.590878009796)\n",
      " state (9)  A[0]:(0.656465291977) A[1]:(0.810238838196) A[2]:(0.810278654099) A[3]:(0.000679150107317)\n",
      " state (10)  A[0]:(0.729760289192) A[1]:(0.900188446045) A[2]:(-8.52346420288e-05) A[3]:(0.729382634163)\n",
      " state (11)  A[0]:(0.504076600075) A[1]:(0.87513011694) A[2]:(-0.510276317596) A[3]:(0.836350023746)\n",
      " state (12)  A[0]:(0.056075040251) A[1]:(0.819104671478) A[2]:(-0.351972430944) A[3]:(0.786289215088)\n",
      " state (13)  A[0]:(0.00160533050075) A[1]:(0.804373562336) A[2]:(0.900679171085) A[3]:(0.729606866837)\n",
      " state (14)  A[0]:(0.811421871185) A[1]:(0.901783466339) A[2]:(0.99999833107) A[3]:(0.810596704483)\n",
      " state (15)  A[0]:(0.994035005569) A[1]:(0.970327436924) A[2]:(1.0) A[3]:(0.909693956375)\n",
      "Episode 262000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6161. Times reached goal: 943.               Steps done: 2712556. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0597303199494.\n",
      " state (0)  A[0]:(0.531480252743) A[1]:(0.590444564819) A[2]:(0.590366005898) A[3]:(0.531509280205)\n",
      " state (1)  A[0]:(0.531876802444) A[1]:(0.00133377790917) A[2]:(0.655915021896) A[3]:(0.590714335442)\n",
      " state (2)  A[0]:(0.590202689171) A[1]:(0.729291319847) A[2]:(0.590843319893) A[3]:(0.655535101891)\n",
      " state (3)  A[0]:(0.655400693417) A[1]:(-0.0109052751213) A[2]:(0.458142310381) A[3]:(0.545153319836)\n",
      " state (4)  A[0]:(0.590607404709) A[1]:(0.656085014343) A[2]:(0.00109457923099) A[3]:(0.531278014183)\n",
      " state (5)  A[0]:(0.187554046512) A[1]:(0.919907808304) A[2]:(-0.117542035878) A[3]:(0.497357696295)\n",
      " state (6)  A[0]:(-0.000348895788193) A[1]:(0.810097813606) A[2]:(-0.000785589043517) A[3]:(0.65576171875)\n",
      " state (7)  A[0]:(0.636397957802) A[1]:(-0.225085958838) A[2]:(0.115454085171) A[3]:(0.917233347893)\n",
      " state (8)  A[0]:(0.655405342579) A[1]:(0.000451480940683) A[2]:(0.72882437706) A[3]:(0.590151429176)\n",
      " state (9)  A[0]:(0.6555134058) A[1]:(0.810163915157) A[2]:(0.810040831566) A[3]:(8.97943973541e-05)\n",
      " state (10)  A[0]:(0.729008793831) A[1]:(0.900098323822) A[2]:(-0.000298738479614) A[3]:(0.728819966316)\n",
      " state (11)  A[0]:(0.503139615059) A[1]:(0.87507545948) A[2]:(-0.511074662209) A[3]:(0.835908591747)\n",
      " state (12)  A[0]:(0.054842017591) A[1]:(0.819127142429) A[2]:(-0.35426184535) A[3]:(0.785646796227)\n",
      " state (13)  A[0]:(-0.000384896964533) A[1]:(0.804350733757) A[2]:(0.900372505188) A[3]:(0.728670835495)\n",
      " state (14)  A[0]:(0.810363888741) A[1]:(0.901609659195) A[2]:(0.999998390675) A[3]:(0.809874534607)\n",
      " state (15)  A[0]:(0.993947207928) A[1]:(0.970144331455) A[2]:(1.0) A[3]:(0.909171462059)\n",
      "Episode 263000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6124. Times reached goal: 938.               Steps done: 2718680. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0593656492315.\n",
      " state (0)  A[0]:(0.531451821327) A[1]:(0.590393900871) A[2]:(0.590383887291) A[3]:(0.531662940979)\n",
      " state (1)  A[0]:(0.531706571579) A[1]:(-0.000107789412141) A[2]:(0.655990839005) A[3]:(0.590902149677)\n",
      " state (2)  A[0]:(0.590257465839) A[1]:(0.728941917419) A[2]:(0.590104222298) A[3]:(0.656004428864)\n",
      " state (3)  A[0]:(0.655641555786) A[1]:(-0.00186889583711) A[2]:(0.456232339144) A[3]:(0.546587944031)\n",
      " state (4)  A[0]:(0.590644240379) A[1]:(0.655970454216) A[2]:(0.000174880027771) A[3]:(0.531940162182)\n",
      " state (5)  A[0]:(0.187371969223) A[1]:(0.919805109501) A[2]:(-0.117936551571) A[3]:(0.497928053141)\n",
      " state (6)  A[0]:(-0.000240802764893) A[1]:(0.809851884842) A[2]:(-0.000242352485657) A[3]:(0.656007766724)\n",
      " state (7)  A[0]:(0.636862039566) A[1]:(-0.225791603327) A[2]:(0.117155119777) A[3]:(0.917173802853)\n",
      " state (8)  A[0]:(0.656049847603) A[1]:(-0.000290447846055) A[2]:(0.729080915451) A[3]:(0.590392708778)\n",
      " state (9)  A[0]:(0.655789613724) A[1]:(0.80981272459) A[2]:(0.810132265091) A[3]:(0.000127837061882)\n",
      " state (10)  A[0]:(0.729155361652) A[1]:(0.899913966656) A[2]:(-0.000249743461609) A[3]:(0.728937029839)\n",
      " state (11)  A[0]:(0.50370657444) A[1]:(0.874889135361) A[2]:(-0.511892795563) A[3]:(0.836126804352)\n",
      " state (12)  A[0]:(0.0557172223926) A[1]:(0.818809628487) A[2]:(-0.356410086155) A[3]:(0.785985946655)\n",
      " state (13)  A[0]:(0.000135391950607) A[1]:(0.803682804108) A[2]:(0.900309622288) A[3]:(0.729076504707)\n",
      " state (14)  A[0]:(0.810522735119) A[1]:(0.900875329971) A[2]:(0.999998450279) A[3]:(0.810239136219)\n",
      " state (15)  A[0]:(0.99390655756) A[1]:(0.969681084156) A[2]:(1.0) A[3]:(0.909203648567)\n",
      "Episode 264000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6192. Times reached goal: 950.               Steps done: 2724872. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0589991928512.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5910,  0.5905,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5915,  0.6563,  0.0001,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6572,  0.0006,  0.7292,  0.5905]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6573,  0.8102,  0.8103,  0.0001]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7300,  0.9000, -0.0003,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9010,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531875610352) A[1]:(0.591018676758) A[2]:(0.590874612331) A[3]:(0.531642079353)\n",
      " state (1)  A[0]:(0.531992316246) A[1]:(0.000522350834217) A[2]:(0.656555354595) A[3]:(0.590762376785)\n",
      " state (2)  A[0]:(0.590515971184) A[1]:(0.729247093201) A[2]:(0.591084122658) A[3]:(0.655780434608)\n",
      " state (3)  A[0]:(0.65591609478) A[1]:(-0.00513602187857) A[2]:(0.45751658082) A[3]:(0.545556843281)\n",
      " state (4)  A[0]:(0.590903699398) A[1]:(0.656385183334) A[2]:(0.000259637832642) A[3]:(0.531068742275)\n",
      " state (5)  A[0]:(0.187196180224) A[1]:(0.920004308224) A[2]:(-0.118426598608) A[3]:(0.497181147337)\n",
      " state (6)  A[0]:(-0.000448405713541) A[1]:(0.810079991817) A[2]:(-0.000585555972066) A[3]:(0.655621528625)\n",
      " state (7)  A[0]:(0.636903166771) A[1]:(-0.225906029344) A[2]:(0.117712497711) A[3]:(0.917046546936)\n",
      " state (8)  A[0]:(0.656358838081) A[1]:(0.000461405114038) A[2]:(0.729076147079) A[3]:(0.590010046959)\n",
      " state (9)  A[0]:(0.656329274178) A[1]:(0.810132026672) A[2]:(0.810122847557) A[3]:(-0.000589817704167)\n",
      " state (10)  A[0]:(0.729460000992) A[1]:(0.900057077408) A[2]:(-0.00040507313679) A[3]:(0.728545665741)\n",
      " state (11)  A[0]:(0.504137516022) A[1]:(0.875156402588) A[2]:(-0.512995123863) A[3]:(0.835899949074)\n",
      " state (12)  A[0]:(0.0559362992644) A[1]:(0.819366276264) A[2]:(-0.359164237976) A[3]:(0.785659253597)\n",
      " state (13)  A[0]:(-0.000373721093638) A[1]:(0.804381728172) A[2]:(0.900071978569) A[3]:(0.728533029556)\n",
      " state (14)  A[0]:(0.810247302055) A[1]:(0.901195466518) A[2]:(0.999998450279) A[3]:(0.809788167477)\n",
      " state (15)  A[0]:(0.993847429752) A[1]:(0.969675779343) A[2]:(1.0) A[3]:(0.908744394779)\n",
      "Episode 265000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6116. Times reached goal: 935.               Steps done: 2730988. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0586394549875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53157389164) A[1]:(0.590565860271) A[2]:(0.59036976099) A[3]:(0.531764805317)\n",
      " state (1)  A[0]:(0.531534552574) A[1]:(-0.000272138044238) A[2]:(0.655970871449) A[3]:(0.590740501881)\n",
      " state (2)  A[0]:(0.590193986893) A[1]:(0.728889405727) A[2]:(0.590235114098) A[3]:(0.655758500099)\n",
      " state (3)  A[0]:(0.655546426773) A[1]:(0.00190107477829) A[2]:(0.456204205751) A[3]:(0.546868979931)\n",
      " state (4)  A[0]:(0.590387284756) A[1]:(0.655935525894) A[2]:(9.05990600586e-05) A[3]:(0.53171312809)\n",
      " state (5)  A[0]:(0.186699658632) A[1]:(0.919956922531) A[2]:(-0.118493005633) A[3]:(0.497540831566)\n",
      " state (6)  A[0]:(-0.000705748680048) A[1]:(0.81000494957) A[2]:(-0.000196814537048) A[3]:(0.655703902245)\n",
      " state (7)  A[0]:(0.636508047581) A[1]:(-0.226609677076) A[2]:(0.118980281055) A[3]:(0.916971027851)\n",
      " state (8)  A[0]:(0.655920028687) A[1]:(0.000287281349301) A[2]:(0.729173064232) A[3]:(0.590193867683)\n",
      " state (9)  A[0]:(0.6556828022) A[1]:(0.810076832771) A[2]:(0.810131788254) A[3]:(-0.00022067129612)\n",
      " state (10)  A[0]:(0.728967547417) A[1]:(0.89997625351) A[2]:(-0.000419974297984) A[3]:(0.728773236275)\n",
      " state (11)  A[0]:(0.503804922104) A[1]:(0.875116467476) A[2]:(-0.513930916786) A[3]:(0.836147725582)\n",
      " state (12)  A[0]:(0.0557484775782) A[1]:(0.819410920143) A[2]:(-0.361574977636) A[3]:(0.786012887955)\n",
      " state (13)  A[0]:(-0.000727921607904) A[1]:(0.804386734962) A[2]:(0.900013327599) A[3]:(0.728906273842)\n",
      " state (14)  A[0]:(0.810199916363) A[1]:(0.901023983955) A[2]:(0.999998509884) A[3]:(0.810059309006)\n",
      " state (15)  A[0]:(0.993796169758) A[1]:(0.969456911087) A[2]:(1.0) A[3]:(0.908647477627)\n",
      "Episode 266000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6149. Times reached goal: 936.               Steps done: 2737137. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0582799872948.\n",
      " state (0)  A[0]:(0.530810654163) A[1]:(0.590499639511) A[2]:(0.590409994125) A[3]:(0.530393004417)\n",
      " state (1)  A[0]:(0.531196713448) A[1]:(-6.91339373589e-05) A[2]:(0.65606200695) A[3]:(0.590220868587)\n",
      " state (2)  A[0]:(0.590175628662) A[1]:(0.729083418846) A[2]:(0.590268850327) A[3]:(0.655497968197)\n",
      " state (3)  A[0]:(0.655995726585) A[1]:(0.00415213266388) A[2]:(0.455682307482) A[3]:(0.547078490257)\n",
      " state (4)  A[0]:(0.590709507465) A[1]:(0.656156182289) A[2]:(-4.8041343689e-05) A[3]:(0.531554579735)\n",
      " state (5)  A[0]:(0.186704665422) A[1]:(0.919996619225) A[2]:(-0.118389569223) A[3]:(0.497118771076)\n",
      " state (6)  A[0]:(0.000367075175745) A[1]:(0.809705376625) A[2]:(-2.37226486206e-05) A[3]:(0.655703604221)\n",
      " state (7)  A[0]:(0.637493371964) A[1]:(-0.228388532996) A[2]:(0.119670897722) A[3]:(0.916935682297)\n",
      " state (8)  A[0]:(0.656691193581) A[1]:(-0.000608347298112) A[2]:(0.729125380516) A[3]:(0.589707374573)\n",
      " state (9)  A[0]:(0.656473457813) A[1]:(0.809773087502) A[2]:(0.81016522646) A[3]:(-0.00148530199658)\n",
      " state (10)  A[0]:(0.729689478874) A[1]:(0.899936437607) A[2]:(-0.000583410204854) A[3]:(0.728223085403)\n",
      " state (11)  A[0]:(0.505317568779) A[1]:(0.87529784441) A[2]:(-0.515404343605) A[3]:(0.835902810097)\n",
      " state (12)  A[0]:(0.0578320100904) A[1]:(0.819908201694) A[2]:(-0.364731043577) A[3]:(0.785724818707)\n",
      " state (13)  A[0]:(0.00118896306958) A[1]:(0.804969191551) A[2]:(0.900168299675) A[3]:(0.728537082672)\n",
      " state (14)  A[0]:(0.810923337936) A[1]:(0.901152014732) A[2]:(0.999998569489) A[3]:(0.809892058372)\n",
      " state (15)  A[0]:(0.993746221066) A[1]:(0.969278812408) A[2]:(1.0) A[3]:(0.908292531967)\n",
      "Episode 267000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6185. Times reached goal: 944.               Steps done: 2743322. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0579206380066.\n",
      " state (0)  A[0]:(0.531165599823) A[1]:(0.590302765369) A[2]:(0.590298056602) A[3]:(0.531387925148)\n",
      " state (1)  A[0]:(0.531344771385) A[1]:(-7.18366354704e-05) A[2]:(0.656042635441) A[3]:(0.590634584427)\n",
      " state (2)  A[0]:(0.590268135071) A[1]:(0.72885119915) A[2]:(0.590285122395) A[3]:(0.655809521675)\n",
      " state (3)  A[0]:(0.655818939209) A[1]:(-0.000476999179227) A[2]:(0.455986976624) A[3]:(0.54653352499)\n",
      " state (4)  A[0]:(0.590327620506) A[1]:(0.656218707561) A[2]:(-0.000203251838684) A[3]:(0.531240224838)\n",
      " state (5)  A[0]:(0.18599678576) A[1]:(0.920103788376) A[2]:(-0.118549071252) A[3]:(0.497075766325)\n",
      " state (6)  A[0]:(-0.000248491764069) A[1]:(0.809882819653) A[2]:(-0.00025486946106) A[3]:(0.655882716179)\n",
      " state (7)  A[0]:(0.636922121048) A[1]:(-0.228575378656) A[2]:(0.119974978268) A[3]:(0.91696536541)\n",
      " state (8)  A[0]:(0.656123757362) A[1]:(0.000169949606061) A[2]:(0.729036450386) A[3]:(0.589849233627)\n",
      " state (9)  A[0]:(0.655830144882) A[1]:(0.810005068779) A[2]:(0.809884905815) A[3]:(-0.000362858147128)\n",
      " state (10)  A[0]:(0.728847265244) A[1]:(0.899950563908) A[2]:(-0.00102364982013) A[3]:(0.728542506695)\n",
      " state (11)  A[0]:(0.50394141674) A[1]:(0.875297725201) A[2]:(-0.516567707062) A[3]:(0.836013615131)\n",
      " state (12)  A[0]:(0.0558982118964) A[1]:(0.819894850254) A[2]:(-0.367524832487) A[3]:(0.785821974277)\n",
      " state (13)  A[0]:(-0.00112089468166) A[1]:(0.804781794548) A[2]:(0.900153338909) A[3]:(0.728578329086)\n",
      " state (14)  A[0]:(0.810067355633) A[1]:(0.900787889957) A[2]:(0.999998629093) A[3]:(0.809894442558)\n",
      " state (15)  A[0]:(0.993646800518) A[1]:(0.968945086002) A[2]:(1.0) A[3]:(0.907993197441)\n",
      "Episode 268000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6123. Times reached goal: 939.               Steps done: 2749445. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0575670734825.\n",
      " state (0)  A[0]:(0.531736850739) A[1]:(0.590335488319) A[2]:(0.590420305729) A[3]:(0.531647086143)\n",
      " state (1)  A[0]:(0.531752049923) A[1]:(-9.93460416794e-05) A[2]:(0.655808806419) A[3]:(0.590679526329)\n",
      " state (2)  A[0]:(0.590373754501) A[1]:(0.728793382645) A[2]:(0.590056777) A[3]:(0.655664801598)\n",
      " state (3)  A[0]:(0.656099319458) A[1]:(0.0258726682514) A[2]:(0.453059762716) A[3]:(0.549361944199)\n",
      " state (4)  A[0]:(0.590361952782) A[1]:(0.65574246645) A[2]:(-4.44650650024e-05) A[3]:(0.531544327736)\n",
      " state (5)  A[0]:(0.186334446073) A[1]:(0.920055091381) A[2]:(-0.118840426207) A[3]:(0.496680170298)\n",
      " state (6)  A[0]:(0.000538170279469) A[1]:(0.809837222099) A[2]:(-0.000514745654073) A[3]:(0.655593574047)\n",
      " state (7)  A[0]:(0.636991620064) A[1]:(-0.229320183396) A[2]:(0.120402984321) A[3]:(0.916925251484)\n",
      " state (8)  A[0]:(0.656175255775) A[1]:(-0.00028495490551) A[2]:(0.728646039963) A[3]:(0.590511083603)\n",
      " state (9)  A[0]:(0.655879855156) A[1]:(0.809793114662) A[2]:(0.809875965118) A[3]:(0.000284537672997)\n",
      " state (10)  A[0]:(0.729074478149) A[1]:(0.899900615215) A[2]:(-0.000676870229654) A[3]:(0.728915810585)\n",
      " state (11)  A[0]:(0.504864275455) A[1]:(0.875422954559) A[2]:(-0.517436623573) A[3]:(0.836401104927)\n",
      " state (12)  A[0]:(0.05731196329) A[1]:(0.820296406746) A[2]:(-0.370072513819) A[3]:(0.786338984966)\n",
      " state (13)  A[0]:(-0.000101625919342) A[1]:(0.805258512497) A[2]:(0.900104105473) A[3]:(0.729065358639)\n",
      " state (14)  A[0]:(0.810312211514) A[1]:(0.900870680809) A[2]:(0.999998688698) A[3]:(0.81011235714)\n",
      " state (15)  A[0]:(0.993591368198) A[1]:(0.968791425228) A[2]:(1.0) A[3]:(0.907758891582)\n",
      "Episode 269000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6130. Times reached goal: 939.               Steps done: 2755575. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0572152667115.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5908,  0.5904,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.6562,  0.0004,  0.5321]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565,  0.0001,  0.7291,  0.5912]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565,  0.8100,  0.8098,  0.0018]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0006,  0.8064,  0.8999,  0.7290]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8097,  0.9013,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531498789787) A[1]:(0.590815901756) A[2]:(0.590484619141) A[3]:(0.531399488449)\n",
      " state (1)  A[0]:(0.531556010246) A[1]:(-0.000166406854987) A[2]:(0.656207501888) A[3]:(0.59058368206)\n",
      " state (2)  A[0]:(0.590336203575) A[1]:(0.729069590569) A[2]:(0.590485215187) A[3]:(0.655951857567)\n",
      " state (3)  A[0]:(0.656529068947) A[1]:(0.0610899217427) A[2]:(0.449081391096) A[3]:(0.552311122417)\n",
      " state (4)  A[0]:(0.590494513512) A[1]:(0.656035542488) A[2]:(-0.000221014022827) A[3]:(0.531639158726)\n",
      " state (5)  A[0]:(0.186232551932) A[1]:(0.920009016991) A[2]:(-0.118815861642) A[3]:(0.496839016676)\n",
      " state (6)  A[0]:(-0.000114649534225) A[1]:(0.809724748135) A[2]:(-0.000170230865479) A[3]:(0.65584897995)\n",
      " state (7)  A[0]:(0.63640999794) A[1]:(-0.230512604117) A[2]:(0.121552497149) A[3]:(0.916941702366)\n",
      " state (8)  A[0]:(0.655758142471) A[1]:(-0.000751426327042) A[2]:(0.728961825371) A[3]:(0.590239644051)\n",
      " state (9)  A[0]:(0.65551507473) A[1]:(0.809756398201) A[2]:(0.809896588326) A[3]:(-0.000307470560074)\n",
      " state (10)  A[0]:(0.72865664959) A[1]:(0.899882018566) A[2]:(-0.000959157652687) A[3]:(0.728610873222)\n",
      " state (11)  A[0]:(0.504195928574) A[1]:(0.87552946806) A[2]:(-0.518775701523) A[3]:(0.836208105087)\n",
      " state (12)  A[0]:(0.0561084337533) A[1]:(0.820677936077) A[2]:(-0.373099416494) A[3]:(0.786042392254)\n",
      " state (13)  A[0]:(-0.00187805073801) A[1]:(0.805859804153) A[2]:(0.900054812431) A[3]:(0.72857773304)\n",
      " state (14)  A[0]:(0.809653759003) A[1]:(0.901216506958) A[2]:(0.999998688698) A[3]:(0.809737741947)\n",
      " state (15)  A[0]:(0.993499159813) A[1]:(0.968799471855) A[2]:(1.0) A[3]:(0.907278418541)\n",
      "Episode 270000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6192. Times reached goal: 951.               Steps done: 2761767. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.056862084361.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531415581703) A[1]:(0.590597331524) A[2]:(0.590607643127) A[3]:(0.531526565552)\n",
      " state (1)  A[0]:(0.53152513504) A[1]:(0.000168586149812) A[2]:(0.656189203262) A[3]:(0.590663790703)\n",
      " state (2)  A[0]:(0.590472817421) A[1]:(0.729136228561) A[2]:(0.590744495392) A[3]:(0.656067430973)\n",
      " state (3)  A[0]:(0.656511664391) A[1]:(0.0900432020426) A[2]:(0.446291178465) A[3]:(0.554609954357)\n",
      " state (4)  A[0]:(0.590404033661) A[1]:(0.656282305717) A[2]:(0.000161409378052) A[3]:(0.531491875648)\n",
      " state (5)  A[0]:(0.186714559793) A[1]:(0.920081138611) A[2]:(-0.118678942323) A[3]:(0.496721714735)\n",
      " state (6)  A[0]:(0.00029644370079) A[1]:(0.810043811798) A[2]:(6.43730163574e-05) A[3]:(0.655889034271)\n",
      " state (7)  A[0]:(0.63649892807) A[1]:(-0.230050221086) A[2]:(0.122495025396) A[3]:(0.916944622993)\n",
      " state (8)  A[0]:(0.65633648634) A[1]:(0.000303024426103) A[2]:(0.729187905788) A[3]:(0.59057289362)\n",
      " state (9)  A[0]:(0.656363844872) A[1]:(0.810122489929) A[2]:(0.810106873512) A[3]:(0.000355601281626)\n",
      " state (10)  A[0]:(0.729360580444) A[1]:(0.89998292923) A[2]:(-9.85860824585e-05) A[3]:(0.728931903839)\n",
      " state (11)  A[0]:(0.505632758141) A[1]:(0.875617325306) A[2]:(-0.519104957581) A[3]:(0.83650046587)\n",
      " state (12)  A[0]:(0.0582401454449) A[1]:(0.820741891861) A[2]:(-0.374877601862) A[3]:(0.786500990391)\n",
      " state (13)  A[0]:(0.000246465206146) A[1]:(0.805694699287) A[2]:(0.900203943253) A[3]:(0.729171574116)\n",
      " state (14)  A[0]:(0.810532450676) A[1]:(0.900816380978) A[2]:(0.999998748302) A[3]:(0.810215651989)\n",
      " state (15)  A[0]:(0.993477523327) A[1]:(0.968441784382) A[2]:(1.0) A[3]:(0.907278060913)\n",
      "Episode 271000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6086. Times reached goal: 937.               Steps done: 2767853. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0565170726511.\n",
      " state (0)  A[0]:(0.531368196011) A[1]:(0.590105473995) A[2]:(0.590443193913) A[3]:(0.530851840973)\n",
      " state (1)  A[0]:(0.531257629395) A[1]:(-0.000356499076588) A[2]:(0.655863761902) A[3]:(0.590060353279)\n",
      " state (2)  A[0]:(0.59029340744) A[1]:(0.72870850563) A[2]:(0.590473592281) A[3]:(0.655759096146)\n",
      " state (3)  A[0]:(0.655836462975) A[1]:(0.0852456986904) A[2]:(0.446527123451) A[3]:(0.553434371948)\n",
      " state (4)  A[0]:(0.589750647545) A[1]:(0.655452609062) A[2]:(-3.30209732056e-05) A[3]:(0.530897021294)\n",
      " state (5)  A[0]:(0.185988157988) A[1]:(0.919887185097) A[2]:(-0.118975460529) A[3]:(0.496677428484)\n",
      " state (6)  A[0]:(-0.00154331198428) A[1]:(0.810011267662) A[2]:(-0.000271439552307) A[3]:(0.655401647091)\n",
      " state (7)  A[0]:(0.634431004524) A[1]:(-0.229597225785) A[2]:(0.12258156389) A[3]:(0.916569113731)\n",
      " state (8)  A[0]:(0.654777646065) A[1]:(-0.000708166393451) A[2]:(0.728584766388) A[3]:(0.590463042259)\n",
      " state (9)  A[0]:(0.654464006424) A[1]:(0.809661209583) A[2]:(0.809846997261) A[3]:(0.000287145376205)\n",
      " state (10)  A[0]:(0.727861166) A[1]:(0.899897634983) A[2]:(-0.00111103011295) A[3]:(0.729066967964)\n",
      " state (11)  A[0]:(0.503663539886) A[1]:(0.875767290592) A[2]:(-0.521162211895) A[3]:(0.836673021317)\n",
      " state (12)  A[0]:(0.0555594936013) A[1]:(0.82123708725) A[2]:(-0.378790348768) A[3]:(0.786576628685)\n",
      " state (13)  A[0]:(-0.0031595423352) A[1]:(0.806369304657) A[2]:(0.899928629398) A[3]:(0.728943586349)\n",
      " state (14)  A[0]:(0.809034347534) A[1]:(0.901113331318) A[2]:(0.999998807907) A[3]:(0.809893727303)\n",
      " state (15)  A[0]:(0.993337750435) A[1]:(0.968392670155) A[2]:(1.0) A[3]:(0.906766593456)\n",
      "Episode 272000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6153. Times reached goal: 943.               Steps done: 2774006. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0561703907637.\n",
      " state (0)  A[0]:(0.531195163727) A[1]:(0.590224325657) A[2]:(0.59052491188) A[3]:(0.531103491783)\n",
      " state (1)  A[0]:(0.531085431576) A[1]:(-6.84764236212e-05) A[2]:(0.656291604042) A[3]:(0.590277314186)\n",
      " state (2)  A[0]:(0.590193986893) A[1]:(0.729016542435) A[2]:(0.59080684185) A[3]:(0.65608716011)\n",
      " state (3)  A[0]:(0.655962228775) A[1]:(0.0873911678791) A[2]:(0.446555763483) A[3]:(0.553638100624)\n",
      " state (4)  A[0]:(0.589954257011) A[1]:(0.655822038651) A[2]:(0.000117659568787) A[3]:(0.531122744083)\n",
      " state (5)  A[0]:(0.186469823122) A[1]:(0.919901132584) A[2]:(-0.118908472359) A[3]:(0.497355371714)\n",
      " state (6)  A[0]:(-0.000733435037546) A[1]:(0.810007572174) A[2]:(-0.000115752220154) A[3]:(0.655890345573)\n",
      " state (7)  A[0]:(0.634938657284) A[1]:(-0.229516208172) A[2]:(0.123677507043) A[3]:(0.916496872902)\n",
      " state (8)  A[0]:(0.655794620514) A[1]:(5.83920627832e-05) A[2]:(0.728797078133) A[3]:(0.590217709541)\n",
      " state (9)  A[0]:(0.655698537827) A[1]:(0.809925913811) A[2]:(0.809904575348) A[3]:(0.000191658735275)\n",
      " state (10)  A[0]:(0.728851318359) A[1]:(0.899968445301) A[2]:(-0.00077950936975) A[3]:(0.729010999203)\n",
      " state (11)  A[0]:(0.505488216877) A[1]:(0.875843524933) A[2]:(-0.521893978119) A[3]:(0.836680233479)\n",
      " state (12)  A[0]:(0.0581201203167) A[1]:(0.821326613426) A[2]:(-0.381051748991) A[3]:(0.786626458168)\n",
      " state (13)  A[0]:(-0.000683784368448) A[1]:(0.806323885918) A[2]:(0.900059401989) A[3]:(0.729018449783)\n",
      " state (14)  A[0]:(0.810013711452) A[1]:(0.900887310505) A[2]:(0.999998807907) A[3]:(0.810046851635)\n",
      " state (15)  A[0]:(0.993309617043) A[1]:(0.968121945858) A[2]:(1.0) A[3]:(0.906605064869)\n",
      "Episode 273000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6163. Times reached goal: 943.               Steps done: 2780169. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0558252772052.\n",
      " state (0)  A[0]:(0.531424999237) A[1]:(0.590494990349) A[2]:(0.590307712555) A[3]:(0.530080914497)\n",
      " state (1)  A[0]:(0.531609714031) A[1]:(0.000545464397874) A[2]:(0.656042695045) A[3]:(0.590492248535)\n",
      " state (2)  A[0]:(0.590961575508) A[1]:(0.729027509689) A[2]:(0.591108083725) A[3]:(0.655968427658)\n",
      " state (3)  A[0]:(0.656310379505) A[1]:(0.0555842928588) A[2]:(0.450789898634) A[3]:(0.551503777504)\n",
      " state (4)  A[0]:(0.590930998325) A[1]:(0.656096279621) A[2]:(0.000230669975281) A[3]:(0.531348586082)\n",
      " state (5)  A[0]:(0.187671303749) A[1]:(0.920056641102) A[2]:(-0.119269020855) A[3]:(0.497177511454)\n",
      " state (6)  A[0]:(0.000631064118352) A[1]:(0.809895157814) A[2]:(-0.000331521034241) A[3]:(0.655696153641)\n",
      " state (7)  A[0]:(0.63575220108) A[1]:(-0.230417057872) A[2]:(0.124408990145) A[3]:(0.916455388069)\n",
      " state (8)  A[0]:(0.656727552414) A[1]:(-0.000153489410877) A[2]:(0.728806912899) A[3]:(0.590460181236)\n",
      " state (9)  A[0]:(0.656792521477) A[1]:(0.809963226318) A[2]:(0.809955477715) A[3]:(0.000406116218073)\n",
      " state (10)  A[0]:(0.729693770409) A[1]:(0.899970889091) A[2]:(-0.000348448753357) A[3]:(0.72899377346)\n",
      " state (11)  A[0]:(0.506905257702) A[1]:(0.875889658928) A[2]:(-0.522327303886) A[3]:(0.83671683073)\n",
      " state (12)  A[0]:(0.0598846040666) A[1]:(0.821481406689) A[2]:(-0.382785052061) A[3]:(0.786718666553)\n",
      " state (13)  A[0]:(0.000557839812245) A[1]:(0.806514322758) A[2]:(0.900019884109) A[3]:(0.729110360146)\n",
      " state (14)  A[0]:(0.810302495956) A[1]:(0.900922417641) A[2]:(0.999998867512) A[3]:(0.810128927231)\n",
      " state (15)  A[0]:(0.993266820908) A[1]:(0.96802687645) A[2]:(1.0) A[3]:(0.906448483467)\n",
      "Episode 274000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6126. Times reached goal: 944.               Steps done: 2786295. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0554843369234.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5909,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.9117e-01,  6.5652e-01, -1.1921e-07,  5.3168e-01]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.531432151794) A[1]:(0.590942323208) A[2]:(0.590475320816) A[3]:(0.531507492065)\n",
      " state (1)  A[0]:(0.531319320202) A[1]:(0.000265743583441) A[2]:(0.656018197536) A[3]:(0.59103167057)\n",
      " state (2)  A[0]:(0.590670049191) A[1]:(0.729132056236) A[2]:(0.591207504272) A[3]:(0.655883252621)\n",
      " state (3)  A[0]:(0.656054198742) A[1]:(0.0110920853913) A[2]:(0.456011891365) A[3]:(0.548129022121)\n",
      " state (4)  A[0]:(0.591133892536) A[1]:(0.656497716904) A[2]:(-3.68356704712e-05) A[3]:(0.531708359718)\n",
      " state (5)  A[0]:(0.187237143517) A[1]:(0.920268535614) A[2]:(-0.119620956481) A[3]:(0.497663617134)\n",
      " state (6)  A[0]:(-7.80820846558e-06) A[1]:(0.810089707375) A[2]:(-0.000358462304575) A[3]:(0.656055867672)\n",
      " state (7)  A[0]:(0.635550200939) A[1]:(-0.230205565691) A[2]:(0.125322923064) A[3]:(0.91658449173)\n",
      " state (8)  A[0]:(0.656509995461) A[1]:(-0.000156048685312) A[2]:(0.729112386703) A[3]:(0.590936303139)\n",
      " state (9)  A[0]:(0.656408309937) A[1]:(0.809939205647) A[2]:(0.810072422028) A[3]:(0.000384613842471)\n",
      " state (10)  A[0]:(0.729386091232) A[1]:(0.899950265884) A[2]:(-0.000490307749715) A[3]:(0.729005157948)\n",
      " state (11)  A[0]:(0.506526589394) A[1]:(0.875890135765) A[2]:(-0.523166835308) A[3]:(0.836802840233)\n",
      " state (12)  A[0]:(0.0593152344227) A[1]:(0.821506261826) A[2]:(-0.384586662054) A[3]:(0.786837995052)\n",
      " state (13)  A[0]:(-0.000157952308655) A[1]:(0.806469023228) A[2]:(0.900021672249) A[3]:(0.72920024395)\n",
      " state (14)  A[0]:(0.810189366341) A[1]:(0.900757849216) A[2]:(0.999998867512) A[3]:(0.810193419456)\n",
      " state (15)  A[0]:(0.993224322796) A[1]:(0.96783965826) A[2]:(1.0) A[3]:(0.906297147274)\n",
      "Episode 275000 finished after 0 timesteps with r=0.0. Running score: 0.92. Times trained:               6124. Times reached goal: 957.               Steps done: 2792419. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0551455891484.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.5304043293) A[1]:(0.58894109726) A[2]:(0.590379357338) A[3]:(0.53058719635)\n",
      " state (1)  A[0]:(0.530928313732) A[1]:(0.000185459852219) A[2]:(0.655433535576) A[3]:(0.590620994568)\n",
      " state (2)  A[0]:(0.590225040913) A[1]:(0.728226840496) A[2]:(0.590251684189) A[3]:(0.65539085865)\n",
      " state (3)  A[0]:(0.654562592506) A[1]:(-0.0465975664556) A[2]:(0.462688744068) A[3]:(0.542697548866)\n",
      " state (4)  A[0]:(0.589659452438) A[1]:(0.655304312706) A[2]:(0.000384330720408) A[3]:(0.530812501907)\n",
      " state (5)  A[0]:(0.185295417905) A[1]:(0.920289874077) A[2]:(-0.119896732271) A[3]:(0.496975392103)\n",
      " state (6)  A[0]:(-0.000120252370834) A[1]:(0.809613108635) A[2]:(-0.000748634221964) A[3]:(0.655327618122)\n",
      " state (7)  A[0]:(0.635250091553) A[1]:(-0.231479778886) A[2]:(0.125004798174) A[3]:(0.916008293629)\n",
      " state (8)  A[0]:(0.655298352242) A[1]:(8.35061073303e-05) A[2]:(0.728335082531) A[3]:(0.589370369911)\n",
      " state (9)  A[0]:(0.655137658119) A[1]:(0.810006558895) A[2]:(0.80971634388) A[3]:(-0.000194638967514)\n",
      " state (10)  A[0]:(0.728539288044) A[1]:(0.899986088276) A[2]:(-0.000336289405823) A[3]:(0.728583097458)\n",
      " state (11)  A[0]:(0.505769014359) A[1]:(0.875993072987) A[2]:(-0.523137509823) A[3]:(0.836440503597)\n",
      " state (12)  A[0]:(0.058938331902) A[1]:(0.821728527546) A[2]:(-0.385211378336) A[3]:(0.786278545856)\n",
      " state (13)  A[0]:(-0.000291913747787) A[1]:(0.806678116322) A[2]:(0.900132238865) A[3]:(0.728394508362)\n",
      " state (14)  A[0]:(0.810233294964) A[1]:(0.900760710239) A[2]:(0.999998927116) A[3]:(0.809621453285)\n",
      " state (15)  A[0]:(0.993192374706) A[1]:(0.967731356621) A[2]:(1.0) A[3]:(0.905883967876)\n",
      "Episode 276000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6095. Times reached goal: 935.               Steps done: 2798514. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0548104990071.\n",
      " state (0)  A[0]:(0.531489253044) A[1]:(0.590366721153) A[2]:(0.590707063675) A[3]:(0.531268119812)\n",
      " state (1)  A[0]:(0.531716346741) A[1]:(0.000106966122985) A[2]:(0.65612745285) A[3]:(0.590921580791)\n",
      " state (2)  A[0]:(0.590824604034) A[1]:(0.728927671909) A[2]:(0.590689957142) A[3]:(0.655577898026)\n",
      " state (3)  A[0]:(0.655676782131) A[1]:(-0.0822849273682) A[2]:(0.467135131359) A[3]:(0.539972782135)\n",
      " state (4)  A[0]:(0.591022968292) A[1]:(0.655828595161) A[2]:(0.000235915184021) A[3]:(0.531276464462)\n",
      " state (5)  A[0]:(0.186224848032) A[1]:(0.920433282852) A[2]:(-0.119890853763) A[3]:(0.497560530901)\n",
      " state (6)  A[0]:(0.000118613243103) A[1]:(0.809975504875) A[2]:(-0.000107645988464) A[3]:(0.655614733696)\n",
      " state (7)  A[0]:(0.635575890541) A[1]:(-0.230340778828) A[2]:(0.126980066299) A[3]:(0.91617667675)\n",
      " state (8)  A[0]:(0.655958116055) A[1]:(2.32215970755e-05) A[2]:(0.729030489922) A[3]:(0.59022462368)\n",
      " state (9)  A[0]:(0.655624508858) A[1]:(0.809883475304) A[2]:(0.810070335865) A[3]:(-0.000532537640538)\n",
      " state (10)  A[0]:(0.728969573975) A[1]:(0.899980306625) A[2]:(-0.000142335891724) A[3]:(0.728569328785)\n",
      " state (11)  A[0]:(0.50648188591) A[1]:(0.876077234745) A[2]:(-0.523833394051) A[3]:(0.836624026299)\n",
      " state (12)  A[0]:(0.0595475248992) A[1]:(0.821931183338) A[2]:(-0.386813521385) A[3]:(0.786586999893)\n",
      " state (13)  A[0]:(-0.000329047441483) A[1]:(0.806889653206) A[2]:(0.900174558163) A[3]:(0.72876060009)\n",
      " state (14)  A[0]:(0.810033559799) A[1]:(0.900793075562) A[2]:(0.999998927116) A[3]:(0.809911429882)\n",
      " state (15)  A[0]:(0.993131995201) A[1]:(0.967633426189) A[2]:(1.0) A[3]:(0.90586566925)\n",
      "Episode 277000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6106. Times reached goal: 926.               Steps done: 2804620. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0544768457801.\n",
      " state (0)  A[0]:(0.532431364059) A[1]:(0.590503454208) A[2]:(0.590619802475) A[3]:(0.531822800636)\n",
      " state (1)  A[0]:(0.532577395439) A[1]:(-0.000198813155293) A[2]:(0.655920267105) A[3]:(0.591795027256)\n",
      " state (2)  A[0]:(0.59135890007) A[1]:(0.729103326797) A[2]:(0.590454876423) A[3]:(0.656464934349)\n",
      " state (3)  A[0]:(0.656151771545) A[1]:(-0.106090687215) A[2]:(0.469971001148) A[3]:(0.539412856102)\n",
      " state (4)  A[0]:(0.591323852539) A[1]:(0.656180024147) A[2]:(-0.000285625457764) A[3]:(0.532932758331)\n",
      " state (5)  A[0]:(0.185903444886) A[1]:(0.920648276806) A[2]:(-0.120933845639) A[3]:(0.49929562211)\n",
      " state (6)  A[0]:(0.000270813703537) A[1]:(0.810112893581) A[2]:(-0.00105726683978) A[3]:(0.656737864017)\n",
      " state (7)  A[0]:(0.635911464691) A[1]:(-0.230691999197) A[2]:(0.126969397068) A[3]:(0.916291475296)\n",
      " state (8)  A[0]:(0.655920088291) A[1]:(0.000405458704336) A[2]:(0.729059755802) A[3]:(0.590204179287)\n",
      " state (9)  A[0]:(0.655541360378) A[1]:(0.810052573681) A[2]:(0.810083150864) A[3]:(-0.000293493270874)\n",
      " state (10)  A[0]:(0.728813827038) A[1]:(0.900024116039) A[2]:(-0.00060844415566) A[3]:(0.728841543198)\n",
      " state (11)  A[0]:(0.506135702133) A[1]:(0.876118659973) A[2]:(-0.524966478348) A[3]:(0.836815774441)\n",
      " state (12)  A[0]:(0.0588682442904) A[1]:(0.822001993656) A[2]:(-0.388913393021) A[3]:(0.786752998829)\n",
      " state (13)  A[0]:(-0.00103816355113) A[1]:(0.806967616081) A[2]:(0.900256752968) A[3]:(0.728879034519)\n",
      " state (14)  A[0]:(0.81005769968) A[1]:(0.900810956955) A[2]:(0.999998986721) A[3]:(0.810061812401)\n",
      " state (15)  A[0]:(0.993092060089) A[1]:(0.967545986176) A[2]:(1.0) A[3]:(0.905780136585)\n",
      "Episode 278000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6151. Times reached goal: 948.               Steps done: 2810771. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0541427871523.\n",
      " state (0)  A[0]:(0.530719041824) A[1]:(0.590866804123) A[2]:(0.590512394905) A[3]:(0.529537200928)\n",
      " state (1)  A[0]:(0.531449079514) A[1]:(-0.000406371400459) A[2]:(0.656994342804) A[3]:(0.590071499348)\n",
      " state (2)  A[0]:(0.589818716049) A[1]:(0.729018509388) A[2]:(0.588722109795) A[3]:(0.655775547028)\n",
      " state (3)  A[0]:(0.656664848328) A[1]:(-0.0393742956221) A[2]:(0.463274747133) A[3]:(0.544046342373)\n",
      " state (4)  A[0]:(0.590525507927) A[1]:(0.65758228302) A[2]:(0.00144994154107) A[3]:(0.531304180622)\n",
      " state (5)  A[0]:(0.183519750834) A[1]:(0.920920133591) A[2]:(-0.120124310255) A[3]:(0.496647387743)\n",
      " state (6)  A[0]:(5.91576099396e-05) A[1]:(0.810086607933) A[2]:(0.000219225883484) A[3]:(0.655479967594)\n",
      " state (7)  A[0]:(0.637680470943) A[1]:(-0.23251876235) A[2]:(0.129138946533) A[3]:(0.91620862484)\n",
      " state (8)  A[0]:(0.657912313938) A[1]:(0.000488115445478) A[2]:(0.728984713554) A[3]:(0.590890407562)\n",
      " state (9)  A[0]:(0.657895565033) A[1]:(0.810195088387) A[2]:(0.809968829155) A[3]:(0.000368654698832)\n",
      " state (10)  A[0]:(0.730555176735) A[1]:(0.900092601776) A[2]:(-0.000161290168762) A[3]:(0.728627800941)\n",
      " state (11)  A[0]:(0.508737146854) A[1]:(0.876190781593) A[2]:(-0.524900496006) A[3]:(0.836582124233)\n",
      " state (12)  A[0]:(0.0620378926396) A[1]:(0.822031974792) A[2]:(-0.389694720507) A[3]:(0.786436796188)\n",
      " state (13)  A[0]:(0.00150859239511) A[1]:(0.806776165962) A[2]:(0.900129914284) A[3]:(0.728526115417)\n",
      " state (14)  A[0]:(0.810790240765) A[1]:(0.900483965874) A[2]:(0.999998986721) A[3]:(0.809949338436)\n",
      " state (15)  A[0]:(0.993097543716) A[1]:(0.967313230038) A[2]:(1.0) A[3]:(0.905753493309)\n",
      "Episode 279000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6135. Times reached goal: 938.               Steps done: 2816906. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0538116379918.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5903,  0.5905,  0.5301]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5320, -0.0011,  0.6560,  0.5902]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5896,  0.7288,  0.5922,  0.6551]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0007,  0.8098,  0.0001,  0.6567]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.8998, -0.0001,  0.7299]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9007,  1.0000,  0.8110]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531701803207) A[1]:(0.590329408646) A[2]:(0.590251147747) A[3]:(0.529923796654)\n",
      " state (1)  A[0]:(0.532523989677) A[1]:(-0.000595973746385) A[2]:(0.655917108059) A[3]:(0.590224981308)\n",
      " state (2)  A[0]:(0.590386509895) A[1]:(0.728837490082) A[2]:(0.591831088066) A[3]:(0.655073642731)\n",
      " state (3)  A[0]:(0.655993580818) A[1]:(-0.025217268616) A[2]:(0.465632200241) A[3]:(0.546018838882)\n",
      " state (4)  A[0]:(0.590423941612) A[1]:(0.655948340893) A[2]:(0.00354681909084) A[3]:(0.532276988029)\n",
      " state (5)  A[0]:(0.184136778116) A[1]:(0.921134650707) A[2]:(-0.120872296393) A[3]:(0.497603088617)\n",
      " state (6)  A[0]:(0.00131738104392) A[1]:(0.810114204884) A[2]:(-0.000181674957275) A[3]:(0.656467318535)\n",
      " state (7)  A[0]:(0.637286186218) A[1]:(-0.234087750316) A[2]:(0.130475506186) A[3]:(0.916302144527)\n",
      " state (8)  A[0]:(0.65627348423) A[1]:(-6.48070126772e-05) A[2]:(0.728772521019) A[3]:(0.592012465)\n",
      " state (9)  A[0]:(0.656212091446) A[1]:(0.810052990913) A[2]:(0.810211122036) A[3]:(0.00150279584341)\n",
      " state (10)  A[0]:(0.729512095451) A[1]:(0.90005928278) A[2]:(9.94205474854e-05) A[3]:(0.729584574699)\n",
      " state (11)  A[0]:(0.507668614388) A[1]:(0.876228272915) A[2]:(-0.525898098946) A[3]:(0.837498724461)\n",
      " state (12)  A[0]:(0.0608589425683) A[1]:(0.822230935097) A[2]:(-0.391938865185) A[3]:(0.787665247917)\n",
      " state (13)  A[0]:(0.000243753194809) A[1]:(0.807201087475) A[2]:(0.89997446537) A[3]:(0.729901909828)\n",
      " state (14)  A[0]:(0.810366153717) A[1]:(0.900878190994) A[2]:(0.999998986721) A[3]:(0.810791909695)\n",
      " state (15)  A[0]:(0.993038594723) A[1]:(0.967455625534) A[2]:(1.0) A[3]:(0.90591776371)\n",
      "Episode 280000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6123. Times reached goal: 953.               Steps done: 2823029. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0534831560062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531505465508) A[1]:(0.590517997742) A[2]:(0.590499520302) A[3]:(0.531191408634)\n",
      " state (1)  A[0]:(0.531854629517) A[1]:(-3.08826565742e-06) A[2]:(0.655995368958) A[3]:(0.590814113617)\n",
      " state (2)  A[0]:(0.590305864811) A[1]:(0.72905421257) A[2]:(0.590503811836) A[3]:(0.655638575554)\n",
      " state (3)  A[0]:(0.656257033348) A[1]:(0.00155998265836) A[2]:(0.460121184587) A[3]:(0.548366546631)\n",
      " state (4)  A[0]:(0.59061652422) A[1]:(0.656080365181) A[2]:(0.000355601281626) A[3]:(0.531852066517)\n",
      " state (5)  A[0]:(0.184012070298) A[1]:(0.921020328999) A[2]:(-0.122720114887) A[3]:(0.496576905251)\n",
      " state (6)  A[0]:(0.000418066949351) A[1]:(0.809986293316) A[2]:(-0.000355482072337) A[3]:(0.655245900154)\n",
      " state (7)  A[0]:(0.636831343174) A[1]:(-0.233998924494) A[2]:(0.13168297708) A[3]:(0.915787518024)\n",
      " state (8)  A[0]:(0.656206846237) A[1]:(0.000207375735044) A[2]:(0.728950858116) A[3]:(0.590569496155)\n",
      " state (9)  A[0]:(0.656014680862) A[1]:(0.810038328171) A[2]:(0.810047030449) A[3]:(0.000109165906906)\n",
      " state (10)  A[0]:(0.729202151299) A[1]:(0.900009572506) A[2]:(-6.66379928589e-05) A[3]:(0.728883624077)\n",
      " state (11)  A[0]:(0.507351100445) A[1]:(0.876157641411) A[2]:(-0.52615737915) A[3]:(0.837019205093)\n",
      " state (12)  A[0]:(0.0606328360736) A[1]:(0.822050988674) A[2]:(-0.392771244049) A[3]:(0.787033677101)\n",
      " state (13)  A[0]:(-6.71446323395e-05) A[1]:(0.806762099266) A[2]:(0.900065839291) A[3]:(0.729102134705)\n",
      " state (14)  A[0]:(0.810268878937) A[1]:(0.900408089161) A[2]:(0.999999046326) A[3]:(0.810236215591)\n",
      " state (15)  A[0]:(0.993001043797) A[1]:(0.967147290707) A[2]:(1.0) A[3]:(0.905524611473)\n",
      "Episode 281000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6181. Times reached goal: 955.               Steps done: 2829210. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0531535961729.\n",
      " state (0)  A[0]:(0.529750645161) A[1]:(0.590701520443) A[2]:(0.589598119259) A[3]:(0.531580984592)\n",
      " state (1)  A[0]:(0.530050635338) A[1]:(-0.00121306569781) A[2]:(0.654821932316) A[3]:(0.590966403484)\n",
      " state (2)  A[0]:(0.588808000088) A[1]:(0.72885286808) A[2]:(0.590452075005) A[3]:(0.65542280674)\n",
      " state (3)  A[0]:(0.654688715935) A[1]:(-0.00689989561215) A[2]:(0.462150156498) A[3]:(0.547414898872)\n",
      " state (4)  A[0]:(0.58925640583) A[1]:(0.655968427658) A[2]:(0.000960230536293) A[3]:(0.531546592712)\n",
      " state (5)  A[0]:(0.18214532733) A[1]:(0.921207249165) A[2]:(-0.123236820102) A[3]:(0.496710449457)\n",
      " state (6)  A[0]:(0.000170737504959) A[1]:(0.809663653374) A[2]:(-0.000892042880878) A[3]:(0.656072795391)\n",
      " state (7)  A[0]:(0.637206852436) A[1]:(-0.236279338598) A[2]:(0.131711676717) A[3]:(0.915887713432)\n",
      " state (8)  A[0]:(0.655635237694) A[1]:(-0.00100284034852) A[2]:(0.728725552559) A[3]:(0.58950316906)\n",
      " state (9)  A[0]:(0.65557539463) A[1]:(0.809468865395) A[2]:(0.809672415257) A[3]:(-0.00220310338773)\n",
      " state (10)  A[0]:(0.729104816914) A[1]:(0.8996732831) A[2]:(-0.00222944840789) A[3]:(0.728161931038)\n",
      " state (11)  A[0]:(0.50758934021) A[1]:(0.87579190731) A[2]:(-0.528677999973) A[3]:(0.836747944355)\n",
      " state (12)  A[0]:(0.0611605755985) A[1]:(0.821617543697) A[2]:(-0.396625459194) A[3]:(0.786742925644)\n",
      " state (13)  A[0]:(0.00036588308285) A[1]:(0.806332945824) A[2]:(0.89950966835) A[3]:(0.728700459003)\n",
      " state (14)  A[0]:(0.810381889343) A[1]:(0.900140881538) A[2]:(0.999999046326) A[3]:(0.809924006462)\n",
      " state (15)  A[0]:(0.992964923382) A[1]:(0.966981887817) A[2]:(1.0) A[3]:(0.905198395252)\n",
      "Episode 282000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6136. Times reached goal: 953.               Steps done: 2835346. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0528284442929.\n",
      " state (0)  A[0]:(0.531438350677) A[1]:(0.590568959713) A[2]:(0.590681731701) A[3]:(0.53256213665)\n",
      " state (1)  A[0]:(0.531642317772) A[1]:(-0.00014166161418) A[2]:(0.656132221222) A[3]:(0.592378020287)\n",
      " state (2)  A[0]:(0.590404272079) A[1]:(0.728980302811) A[2]:(0.590448498726) A[3]:(0.657150268555)\n",
      " state (3)  A[0]:(0.656112074852) A[1]:(-0.00116125820205) A[2]:(0.461111545563) A[3]:(0.550094425678)\n",
      " state (4)  A[0]:(0.590511679649) A[1]:(0.655881047249) A[2]:(-0.000145316123962) A[3]:(0.533683657646)\n",
      " state (5)  A[0]:(0.182987183332) A[1]:(0.92120975256) A[2]:(-0.124351359904) A[3]:(0.498558372259)\n",
      " state (6)  A[0]:(-0.000957339711022) A[1]:(0.810059547424) A[2]:(-0.00125372339971) A[3]:(0.656607747078)\n",
      " state (7)  A[0]:(0.635979652405) A[1]:(-0.234849274158) A[2]:(0.132699504495) A[3]:(0.915813088417)\n",
      " state (8)  A[0]:(0.654826521873) A[1]:(0.000118810683489) A[2]:(0.729126811028) A[3]:(0.590037941933)\n",
      " state (9)  A[0]:(0.654519021511) A[1]:(0.810020387173) A[2]:(0.810073554516) A[3]:(-0.00118863524403)\n",
      " state (10)  A[0]:(0.727935791016) A[1]:(0.900064468384) A[2]:(-0.000742554548196) A[3]:(0.728398561478)\n",
      " state (11)  A[0]:(0.505500793457) A[1]:(0.876419603825) A[2]:(-0.527861595154) A[3]:(0.836759746075)\n",
      " state (12)  A[0]:(0.0581604279578) A[1]:(0.822727620602) A[2]:(-0.395770847797) A[3]:(0.786572277546)\n",
      " state (13)  A[0]:(-0.00236814771779) A[1]:(0.807792723179) A[2]:(0.900441646576) A[3]:(0.728325247765)\n",
      " state (14)  A[0]:(0.809864878654) A[1]:(0.90107023716) A[2]:(0.999999046326) A[3]:(0.809632182121)\n",
      " state (15)  A[0]:(0.992904365063) A[1]:(0.967256903648) A[2]:(1.0) A[3]:(0.904816269875)\n",
      "Episode 283000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6139. Times reached goal: 947.               Steps done: 2841485. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0525051239207.\n",
      " state (0)  A[0]:(0.531312406063) A[1]:(0.590890645981) A[2]:(0.590746760368) A[3]:(0.530784070492)\n",
      " state (1)  A[0]:(0.531287908554) A[1]:(0.000232625752687) A[2]:(0.656327188015) A[3]:(0.590505361557)\n",
      " state (2)  A[0]:(0.589908003807) A[1]:(0.729156017303) A[2]:(0.591543495655) A[3]:(0.655249476433)\n",
      " state (3)  A[0]:(0.655809640884) A[1]:(-0.0114325182512) A[2]:(0.464112639427) A[3]:(0.547628760338)\n",
      " state (4)  A[0]:(0.590251863003) A[1]:(0.656398177147) A[2]:(0.00114119006321) A[3]:(0.532262086868)\n",
      " state (5)  A[0]:(0.181800842285) A[1]:(0.92144715786) A[2]:(-0.123580545187) A[3]:(0.497150599957)\n",
      " state (6)  A[0]:(-0.00149014487397) A[1]:(0.810086250305) A[2]:(2.64644622803e-05) A[3]:(0.655917108059)\n",
      " state (7)  A[0]:(0.636547923088) A[1]:(-0.235553428531) A[2]:(0.134593635798) A[3]:(0.915713906288)\n",
      " state (8)  A[0]:(0.656034111977) A[1]:(0.000104177743196) A[2]:(0.729088246822) A[3]:(0.59120208025)\n",
      " state (9)  A[0]:(0.656296253204) A[1]:(0.810046553612) A[2]:(0.810049474239) A[3]:(0.00111038936302)\n",
      " state (10)  A[0]:(0.729398608208) A[1]:(0.900080800056) A[2]:(-0.000258564949036) A[3]:(0.729286789894)\n",
      " state (11)  A[0]:(0.507864713669) A[1]:(0.876465082169) A[2]:(-0.527953267097) A[3]:(0.837345123291)\n",
      " state (12)  A[0]:(0.0611383989453) A[1]:(0.822813570499) A[2]:(-0.39685228467) A[3]:(0.787338495255)\n",
      " state (13)  A[0]:(-9.62316989899e-05) A[1]:(0.807819843292) A[2]:(0.900313317776) A[3]:(0.72922372818)\n",
      " state (14)  A[0]:(0.810339450836) A[1]:(0.900986790657) A[2]:(0.99999910593) A[3]:(0.810258686543)\n",
      " state (15)  A[0]:(0.9928804636) A[1]:(0.967143297195) A[2]:(1.0) A[3]:(0.905008792877)\n",
      "Episode 284000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6179. Times reached goal: 954.               Steps done: 2847664. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0521816950226.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5907,  0.5905,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.6566,  0.0003,  0.5318]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565,  0.0010,  0.7291,  0.5907]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.8103,  0.8101,  0.0003]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0005,  0.8072,  0.8999,  0.7290]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9004,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531399011612) A[1]:(0.590704143047) A[2]:(0.590522706509) A[3]:(0.531219244003)\n",
      " state (1)  A[0]:(0.531569778919) A[1]:(0.000191852450371) A[2]:(0.656335830688) A[3]:(0.590887486935)\n",
      " state (2)  A[0]:(0.590480744839) A[1]:(0.729270458221) A[2]:(0.590923249722) A[3]:(0.655639827251)\n",
      " state (3)  A[0]:(0.656204402447) A[1]:(-0.0039980346337) A[2]:(0.463166236877) A[3]:(0.548147082329)\n",
      " state (4)  A[0]:(0.590755581856) A[1]:(0.656489372253) A[2]:(0.000546336115804) A[3]:(0.531730175018)\n",
      " state (5)  A[0]:(0.182734966278) A[1]:(0.921537339687) A[2]:(-0.124619416893) A[3]:(0.496428251266)\n",
      " state (6)  A[0]:(-7.07805156708e-05) A[1]:(0.810090124607) A[2]:(-0.000472426385386) A[3]:(0.65545886755)\n",
      " state (7)  A[0]:(0.637443184853) A[1]:(-0.235796555877) A[2]:(0.135139629245) A[3]:(0.915547013283)\n",
      " state (8)  A[0]:(0.656403541565) A[1]:(0.000743672135286) A[2]:(0.729098320007) A[3]:(0.590687513351)\n",
      " state (9)  A[0]:(0.65633559227) A[1]:(0.810258030891) A[2]:(0.810044109821) A[3]:(0.000366896361811)\n",
      " state (10)  A[0]:(0.729271352291) A[1]:(0.900065481663) A[2]:(-0.000292897224426) A[3]:(0.729009091854)\n",
      " state (11)  A[0]:(0.507721066475) A[1]:(0.876309931278) A[2]:(-0.528478264809) A[3]:(0.837216854095)\n",
      " state (12)  A[0]:(0.0609937123954) A[1]:(0.822408556938) A[2]:(-0.398440748453) A[3]:(0.787182033062)\n",
      " state (13)  A[0]:(-0.000440329284174) A[1]:(0.807105720043) A[2]:(0.899977207184) A[3]:(0.72899132967)\n",
      " state (14)  A[0]:(0.810263991356) A[1]:(0.900404214859) A[2]:(0.99999910593) A[3]:(0.810100972652)\n",
      " state (15)  A[0]:(0.992859780788) A[1]:(0.966838598251) A[2]:(1.0) A[3]:(0.904862642288)\n",
      "Episode 285000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6169. Times reached goal: 955.               Steps done: 2853833. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0518607770353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531976759434) A[1]:(0.59085714817) A[2]:(0.590991139412) A[3]:(0.532049417496)\n",
      " state (1)  A[0]:(0.531863868237) A[1]:(-0.000103194266558) A[2]:(0.655667304993) A[3]:(0.591028988361)\n",
      " state (2)  A[0]:(0.590495944023) A[1]:(0.728820085526) A[2]:(0.590483546257) A[3]:(0.65526330471)\n",
      " state (3)  A[0]:(0.655965149403) A[1]:(-0.012165739201) A[2]:(0.464401602745) A[3]:(0.547888278961)\n",
      " state (4)  A[0]:(0.590580523014) A[1]:(0.656161665916) A[2]:(0.000410675973399) A[3]:(0.531987369061)\n",
      " state (5)  A[0]:(0.182159736753) A[1]:(0.921690225601) A[2]:(-0.125357761979) A[3]:(0.49639159441)\n",
      " state (6)  A[0]:(-7.77840614319e-05) A[1]:(0.810148179531) A[2]:(-0.000609040202107) A[3]:(0.655323922634)\n",
      " state (7)  A[0]:(0.637677609921) A[1]:(-0.236634328961) A[2]:(0.135951146483) A[3]:(0.915375471115)\n",
      " state (8)  A[0]:(0.656294822693) A[1]:(-0.000367958069546) A[2]:(0.72881603241) A[3]:(0.590786099434)\n",
      " state (9)  A[0]:(0.65587747097) A[1]:(0.809890687466) A[2]:(0.810155749321) A[3]:(-0.000499516667333)\n",
      " state (10)  A[0]:(0.729059994221) A[1]:(0.900007426739) A[2]:(-8.20159912109e-05) A[3]:(0.728703260422)\n",
      " state (11)  A[0]:(0.50782597065) A[1]:(0.876399755478) A[2]:(-0.529058277607) A[3]:(0.837207734585)\n",
      " state (12)  A[0]:(0.0612848326564) A[1]:(0.822697520256) A[2]:(-0.399781405926) A[3]:(0.787206590176)\n",
      " state (13)  A[0]:(-0.000475496024592) A[1]:(0.807513833046) A[2]:(0.900045454502) A[3]:(0.728974938393)\n",
      " state (14)  A[0]:(0.810016512871) A[1]:(0.900627970695) A[2]:(0.99999910593) A[3]:(0.810112476349)\n",
      " state (15)  A[0]:(0.992795288563) A[1]:(0.966848492622) A[2]:(1.0) A[3]:(0.904733002186)\n",
      "Episode 286000 finished after 0 timesteps with r=0.0. Running score: 0.92. Times trained:               6092. Times reached goal: 938.               Steps done: 2859925. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.051545801571.\n",
      " state (0)  A[0]:(0.531799674034) A[1]:(0.59052336216) A[2]:(0.59044533968) A[3]:(0.531872034073)\n",
      " state (1)  A[0]:(0.531922459602) A[1]:(-0.000150244683027) A[2]:(0.655941367149) A[3]:(0.591607451439)\n",
      " state (2)  A[0]:(0.590356945992) A[1]:(0.728380084038) A[2]:(0.590935945511) A[3]:(0.655765652657)\n",
      " state (3)  A[0]:(0.65604364872) A[1]:(-0.00867694802582) A[2]:(0.46512556076) A[3]:(0.54916703701)\n",
      " state (4)  A[0]:(0.590487420559) A[1]:(0.655688285828) A[2]:(0.000490069331136) A[3]:(0.532561182976)\n",
      " state (5)  A[0]:(0.181306287646) A[1]:(0.921711623669) A[2]:(-0.12673458457) A[3]:(0.496614426374)\n",
      " state (6)  A[0]:(-9.95397567749e-05) A[1]:(0.809463799) A[2]:(-0.00179588596802) A[3]:(0.655781030655)\n",
      " state (7)  A[0]:(0.637488484383) A[1]:(-0.239204093814) A[2]:(0.135813906789) A[3]:(0.915339231491)\n",
      " state (8)  A[0]:(0.655363321304) A[1]:(-3.62806022167e-05) A[2]:(0.728422760963) A[3]:(0.590072035789)\n",
      " state (9)  A[0]:(0.655578196049) A[1]:(0.810127019882) A[2]:(0.809512019157) A[3]:(-6.09457492828e-05)\n",
      " state (10)  A[0]:(0.728817343712) A[1]:(0.899992287159) A[2]:(-0.00144326582085) A[3]:(0.728739440441)\n",
      " state (11)  A[0]:(0.507519245148) A[1]:(0.8762794137) A[2]:(-0.53011482954) A[3]:(0.837086439133)\n",
      " state (12)  A[0]:(0.0611250959337) A[1]:(0.822446763515) A[2]:(-0.401591688395) A[3]:(0.786998689175)\n",
      " state (13)  A[0]:(-0.000296115875244) A[1]:(0.807147026062) A[2]:(0.899924576283) A[3]:(0.728718042374)\n",
      " state (14)  A[0]:(0.810440480709) A[1]:(0.900375723839) A[2]:(0.99999910593) A[3]:(0.809999883175)\n",
      " state (15)  A[0]:(0.992794156075) A[1]:(0.96668946743) A[2]:(1.0) A[3]:(0.904581189156)\n",
      "Episode 287000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6158. Times reached goal: 954.               Steps done: 2866083. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0512293578551.\n",
      " state (0)  A[0]:(0.531417965889) A[1]:(0.590444624424) A[2]:(0.590577483177) A[3]:(0.531400859356)\n",
      " state (1)  A[0]:(0.531728923321) A[1]:(9.72338020802e-05) A[2]:(0.656085729599) A[3]:(0.591214895248)\n",
      " state (2)  A[0]:(0.590540766716) A[1]:(0.72901058197) A[2]:(0.59042930603) A[3]:(0.655645012856)\n",
      " state (3)  A[0]:(0.656160116196) A[1]:(0.00193365046289) A[2]:(0.463640093803) A[3]:(0.549633026123)\n",
      " state (4)  A[0]:(0.590566515923) A[1]:(0.656127870083) A[2]:(0.000295042991638) A[3]:(0.53199416399)\n",
      " state (5)  A[0]:(0.181480571628) A[1]:(0.921900093555) A[2]:(-0.126258209348) A[3]:(0.49590793252)\n",
      " state (6)  A[0]:(0.000433057517512) A[1]:(0.809955596924) A[2]:(-2.5749206543e-05) A[3]:(0.655216097832)\n",
      " state (7)  A[0]:(0.638127803802) A[1]:(-0.238084211946) A[2]:(0.138667359948) A[3]:(0.915128350258)\n",
      " state (8)  A[0]:(0.655882894993) A[1]:(0.000213164836168) A[2]:(0.72911375761) A[3]:(0.590132653713)\n",
      " state (9)  A[0]:(0.65563929081) A[1]:(0.810025334358) A[2]:(0.81012147665) A[3]:(-0.000937670178246)\n",
      " state (10)  A[0]:(0.728942513466) A[1]:(0.900049269199) A[2]:(6.52074813843e-05) A[3]:(0.728547334671)\n",
      " state (11)  A[0]:(0.507993578911) A[1]:(0.876499891281) A[2]:(-0.529656767845) A[3]:(0.837182164192)\n",
      " state (12)  A[0]:(0.0616578236222) A[1]:(0.822886109352) A[2]:(-0.401782721281) A[3]:(0.787166237831)\n",
      " state (13)  A[0]:(-0.000464469165308) A[1]:(0.807639718056) A[2]:(0.900054216385) A[3]:(0.728813171387)\n",
      " state (14)  A[0]:(0.810023665428) A[1]:(0.900587797165) A[2]:(0.999999165535) A[3]:(0.809945881367)\n",
      " state (15)  A[0]:(0.992730617523) A[1]:(0.966690421104) A[2]:(1.0) A[3]:(0.904370188713)\n",
      "Episode 288000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6129. Times reached goal: 948.               Steps done: 2872212. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0509163333642.\n",
      " state (0)  A[0]:(0.53113090992) A[1]:(0.590774059296) A[2]:(0.590434551239) A[3]:(0.531814932823)\n",
      " state (1)  A[0]:(0.531228899956) A[1]:(-0.00101232121233) A[2]:(0.65496724844) A[3]:(0.591256022453)\n",
      " state (2)  A[0]:(0.589963257313) A[1]:(0.729215860367) A[2]:(0.590281605721) A[3]:(0.6554210186)\n",
      " state (3)  A[0]:(0.65553933382) A[1]:(-0.00362844299525) A[2]:(0.464984744787) A[3]:(0.549021482468)\n",
      " state (4)  A[0]:(0.59007537365) A[1]:(0.656586527824) A[2]:(-8.27312469482e-05) A[3]:(0.531966090202)\n",
      " state (5)  A[0]:(0.180688798428) A[1]:(0.922273874283) A[2]:(-0.128114074469) A[3]:(0.496266305447)\n",
      " state (6)  A[0]:(0.00151321175508) A[1]:(0.81002753973) A[2]:(-0.00212132604793) A[3]:(0.656226754189)\n",
      " state (7)  A[0]:(0.63933712244) A[1]:(-0.238849177957) A[2]:(0.137344658375) A[3]:(0.915279865265)\n",
      " state (8)  A[0]:(0.65590441227) A[1]:(0.00210884283297) A[2]:(0.728845000267) A[3]:(0.588947713375)\n",
      " state (9)  A[0]:(0.655952215195) A[1]:(0.810534119606) A[2]:(0.809646129608) A[3]:(-0.00145672156941)\n",
      " state (10)  A[0]:(0.729169309139) A[1]:(0.90004324913) A[2]:(-0.00212204130366) A[3]:(0.728609204292)\n",
      " state (11)  A[0]:(0.508349239826) A[1]:(0.876226902008) A[2]:(-0.531858980656) A[3]:(0.83721280098)\n",
      " state (12)  A[0]:(0.062431037426) A[1]:(0.822208046913) A[2]:(-0.404816955328) A[3]:(0.787204146385)\n",
      " state (13)  A[0]:(0.00138017442077) A[1]:(0.806632995605) A[2]:(0.900004267693) A[3]:(0.728985667229)\n",
      " state (14)  A[0]:(0.811524987221) A[1]:(0.899900376797) A[2]:(0.999999165535) A[3]:(0.810333371162)\n",
      " state (15)  A[0]:(0.992780268192) A[1]:(0.966322541237) A[2]:(1.0) A[3]:(0.904527902603)\n",
      "Episode 289000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6181. Times reached goal: 950.               Steps done: 2878393. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0506025901301.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5904,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.0000,  0.6559,  0.5909]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.7287,  0.5900,  0.6555]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0009,  0.8098,  0.0000,  0.6552]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7284,  0.9000, -0.0005,  0.7286]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8095,  0.9006,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531810879707) A[1]:(0.590344190598) A[2]:(0.590526998043) A[3]:(0.531290650368)\n",
      " state (1)  A[0]:(0.531981050968) A[1]:(-6.41494989395e-06) A[2]:(0.655867576599) A[3]:(0.590792298317)\n",
      " state (2)  A[0]:(0.590819001198) A[1]:(0.728773832321) A[2]:(0.590022325516) A[3]:(0.655461788177)\n",
      " state (3)  A[0]:(0.656324148178) A[1]:(0.0015572818229) A[2]:(0.463944852352) A[3]:(0.549049913883)\n",
      " state (4)  A[0]:(0.590641379356) A[1]:(0.655934691429) A[2]:(-0.000168085098267) A[3]:(0.531322896481)\n",
      " state (5)  A[0]:(0.180822432041) A[1]:(0.922096014023) A[2]:(-0.127347394824) A[3]:(0.49558326602)\n",
      " state (6)  A[0]:(0.000789284531493) A[1]:(0.809805452824) A[2]:(-0.000251293182373) A[3]:(0.655393838882)\n",
      " state (7)  A[0]:(0.638847053051) A[1]:(-0.239654958248) A[2]:(0.139596715569) A[3]:(0.915020644665)\n",
      " state (8)  A[0]:(0.656126856804) A[1]:(1.27479434013e-05) A[2]:(0.728575587273) A[3]:(0.590463161469)\n",
      " state (9)  A[0]:(0.655752837658) A[1]:(0.809927999973) A[2]:(0.809689223766) A[3]:(-1.58399343491e-05)\n",
      " state (10)  A[0]:(0.728806734085) A[1]:(0.90001565218) A[2]:(-0.000661611440592) A[3]:(0.728688836098)\n",
      " state (11)  A[0]:(0.50790476799) A[1]:(0.876552820206) A[2]:(-0.530879259109) A[3]:(0.837199211121)\n",
      " state (12)  A[0]:(0.0616048872471) A[1]:(0.82307446003) A[2]:(-0.404337197542) A[3]:(0.787079632282)\n",
      " state (13)  A[0]:(-0.000825911585707) A[1]:(0.80786550045) A[2]:(0.900136530399) A[3]:(0.728575825691)\n",
      " state (14)  A[0]:(0.809807121754) A[1]:(0.900657057762) A[2]:(0.999999165535) A[3]:(0.809794723988)\n",
      " state (15)  A[0]:(0.992638170719) A[1]:(0.966573953629) A[2]:(1.0) A[3]:(0.904028773308)\n",
      "Episode 290000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6188. Times reached goal: 955.               Steps done: 2884581. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0502904281277.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531789362431) A[1]:(0.590592980385) A[2]:(0.590443611145) A[3]:(0.531683802605)\n",
      " state (1)  A[0]:(0.531571269035) A[1]:(-6.83404505253e-05) A[2]:(0.6560125947) A[3]:(0.591054975986)\n",
      " state (2)  A[0]:(0.590515732765) A[1]:(0.728880167007) A[2]:(0.590567052364) A[3]:(0.655535936356)\n",
      " state (3)  A[0]:(0.656422793865) A[1]:(0.00174340058584) A[2]:(0.464109390974) A[3]:(0.550004541874)\n",
      " state (4)  A[0]:(0.590773820877) A[1]:(0.65600925684) A[2]:(3.86238098145e-05) A[3]:(0.532044291496)\n",
      " state (5)  A[0]:(0.180562064052) A[1]:(0.922217130661) A[2]:(-0.127091959119) A[3]:(0.495911836624)\n",
      " state (6)  A[0]:(0.00051671260735) A[1]:(0.809988379478) A[2]:(-4.33921813965e-05) A[3]:(0.655752778053)\n",
      " state (7)  A[0]:(0.638914048672) A[1]:(-0.240152180195) A[2]:(0.140138909221) A[3]:(0.915188729763)\n",
      " state (8)  A[0]:(0.656358659267) A[1]:(-0.000155542045832) A[2]:(0.728814601898) A[3]:(0.590904772282)\n",
      " state (9)  A[0]:(0.656214475632) A[1]:(0.809928178787) A[2]:(0.809950590134) A[3]:(0.000239059329033)\n",
      " state (10)  A[0]:(0.72926735878) A[1]:(0.89999973774) A[2]:(-0.000387310952647) A[3]:(0.728898525238)\n",
      " state (11)  A[0]:(0.508767485619) A[1]:(0.876540720463) A[2]:(-0.531828641891) A[3]:(0.83741235733)\n",
      " state (12)  A[0]:(0.0626335814595) A[1]:(0.823067426682) A[2]:(-0.406918436289) A[3]:(0.787365019321)\n",
      " state (13)  A[0]:(-0.000242680311203) A[1]:(0.807838499546) A[2]:(0.899806439877) A[3]:(0.728886842728)\n",
      " state (14)  A[0]:(0.809898972511) A[1]:(0.900616049767) A[2]:(0.999999165535) A[3]:(0.810052990913)\n",
      " state (15)  A[0]:(0.992590904236) A[1]:(0.966485619545) A[2]:(1.0) A[3]:(0.904011189938)\n",
      "Episode 291000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6130. Times reached goal: 948.               Steps done: 2890711. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0499830907547.\n",
      " state (0)  A[0]:(0.531726062298) A[1]:(0.590737819672) A[2]:(0.590581893921) A[3]:(0.531381130219)\n",
      " state (1)  A[0]:(0.531579375267) A[1]:(5.27501106262e-06) A[2]:(0.656017184258) A[3]:(0.590843319893)\n",
      " state (2)  A[0]:(0.590669393539) A[1]:(0.729046583176) A[2]:(0.590599060059) A[3]:(0.655448019505)\n",
      " state (3)  A[0]:(0.656497240067) A[1]:(0.000780492846388) A[2]:(0.46386757493) A[3]:(0.549905359745)\n",
      " state (4)  A[0]:(0.590613365173) A[1]:(0.656315207481) A[2]:(-0.000351786613464) A[3]:(0.531846761703)\n",
      " state (5)  A[0]:(0.179811879992) A[1]:(0.922309041023) A[2]:(-0.127236917615) A[3]:(0.495584517717)\n",
      " state (6)  A[0]:(0.000293046236038) A[1]:(0.809872746468) A[2]:(-0.000164151191711) A[3]:(0.655848681927)\n",
      " state (7)  A[0]:(0.639302253723) A[1]:(-0.241412892938) A[2]:(0.140296444297) A[3]:(0.91531264782)\n",
      " state (8)  A[0]:(0.656643986702) A[1]:(-0.000474542350275) A[2]:(0.728802859783) A[3]:(0.591174066067)\n",
      " state (9)  A[0]:(0.656456232071) A[1]:(0.809885680676) A[2]:(0.809889256954) A[3]:(0.000884890323505)\n",
      " state (10)  A[0]:(0.729261636734) A[1]:(0.89993429184) A[2]:(-0.00052869314095) A[3]:(0.729054927826)\n",
      " state (11)  A[0]:(0.508629441261) A[1]:(0.876441895962) A[2]:(-0.532659053802) A[3]:(0.837443351746)\n",
      " state (12)  A[0]:(0.0623141936958) A[1]:(0.82291084528) A[2]:(-0.408736616373) A[3]:(0.787363827229)\n",
      " state (13)  A[0]:(-0.000454187364085) A[1]:(0.807640612125) A[2]:(0.900051295757) A[3]:(0.728890061378)\n",
      " state (14)  A[0]:(0.810084223747) A[1]:(0.900473773479) A[2]:(0.99999922514) A[3]:(0.810139119625)\n",
      " state (15)  A[0]:(0.992543876171) A[1]:(0.966325342655) A[2]:(1.0) A[3]:(0.903851389885)\n",
      "Episode 292000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6166. Times reached goal: 957.               Steps done: 2896877. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0496758432346.\n",
      " state (0)  A[0]:(0.531223475933) A[1]:(0.590505540371) A[2]:(0.590561270714) A[3]:(0.53135073185)\n",
      " state (1)  A[0]:(0.531001746655) A[1]:(-1.47931277752e-05) A[2]:(0.656164646149) A[3]:(0.591100335121)\n",
      " state (2)  A[0]:(0.590108215809) A[1]:(0.728924274445) A[2]:(0.590768098831) A[3]:(0.655718564987)\n",
      " state (3)  A[0]:(0.655976891518) A[1]:(0.00149436551146) A[2]:(0.463974297047) A[3]:(0.550231456757)\n",
      " state (4)  A[0]:(0.589874386787) A[1]:(0.656017541885) A[2]:(0.000110626220703) A[3]:(0.531623721123)\n",
      " state (5)  A[0]:(0.178472191095) A[1]:(0.922334194183) A[2]:(-0.126899495721) A[3]:(0.495012104511)\n",
      " state (6)  A[0]:(-0.00115859461948) A[1]:(0.80997222662) A[2]:(-4.31537628174e-05) A[3]:(0.6553992033)\n",
      " state (7)  A[0]:(0.638238012791) A[1]:(-0.241311475635) A[2]:(0.140548273921) A[3]:(0.915167152882)\n",
      " state (8)  A[0]:(0.655544579029) A[1]:(0.000298280268908) A[2]:(0.728951931) A[3]:(0.59025478363)\n",
      " state (9)  A[0]:(0.655535101891) A[1]:(0.810143470764) A[2]:(0.809977650642) A[3]:(-0.000117495656013)\n",
      " state (10)  A[0]:(0.728611826897) A[1]:(0.900030016899) A[2]:(-0.000332117080688) A[3]:(0.728704869747)\n",
      " state (11)  A[0]:(0.507861554623) A[1]:(0.876543939114) A[2]:(-0.533257842064) A[3]:(0.837260365486)\n",
      " state (12)  A[0]:(0.0614847950637) A[1]:(0.82303416729) A[2]:(-0.410343885422) A[3]:(0.787148833275)\n",
      " state (13)  A[0]:(-0.00105515087489) A[1]:(0.807742297649) A[2]:(0.900254487991) A[3]:(0.728624939919)\n",
      " state (14)  A[0]:(0.810135483742) A[1]:(0.90050983429) A[2]:(0.99999922514) A[3]:(0.809974133968)\n",
      " state (15)  A[0]:(0.992494821548) A[1]:(0.966241657734) A[2]:(1.0) A[3]:(0.903532981873)\n",
      "Episode 293000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6154. Times reached goal: 956.               Steps done: 2903031. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0493710768235.\n",
      " state (0)  A[0]:(0.531395316124) A[1]:(0.590517163277) A[2]:(0.590410470963) A[3]:(0.531434059143)\n",
      " state (1)  A[0]:(0.531290888786) A[1]:(0.00023015588522) A[2]:(0.65614593029) A[3]:(0.591097772121)\n",
      " state (2)  A[0]:(0.590484857559) A[1]:(0.729049563408) A[2]:(0.590808033943) A[3]:(0.655732512474)\n",
      " state (3)  A[0]:(0.65640515089) A[1]:(0.00176893733442) A[2]:(0.463864296675) A[3]:(0.550422549248)\n",
      " state (4)  A[0]:(0.590469479561) A[1]:(0.656203329563) A[2]:(1.060962677e-05) A[3]:(0.531702399254)\n",
      " state (5)  A[0]:(0.17940428853) A[1]:(0.922438085079) A[2]:(-0.126976311207) A[3]:(0.495087563992)\n",
      " state (6)  A[0]:(0.000333607196808) A[1]:(0.809991598129) A[2]:(-4.02927398682e-05) A[3]:(0.65577852726)\n",
      " state (7)  A[0]:(0.639177143574) A[1]:(-0.242019712925) A[2]:(0.140994057059) A[3]:(0.915323019028)\n",
      " state (8)  A[0]:(0.656112313271) A[1]:(5.07161021233e-05) A[2]:(0.729056715965) A[3]:(0.59047973156)\n",
      " state (9)  A[0]:(0.656001210213) A[1]:(0.810066103935) A[2]:(0.810051381588) A[3]:(-0.000109598040581)\n",
      " state (10)  A[0]:(0.729050159454) A[1]:(0.899990677834) A[2]:(-0.000288009643555) A[3]:(0.728890538216)\n",
      " state (11)  A[0]:(0.508732795715) A[1]:(0.876531779766) A[2]:(-0.534058690071) A[3]:(0.837503373623)\n",
      " state (12)  A[0]:(0.0625212267041) A[1]:(0.823056817055) A[2]:(-0.412528187037) A[3]:(0.787496209145)\n",
      " state (13)  A[0]:(-0.000706434133463) A[1]:(0.807763695717) A[2]:(0.899935781956) A[3]:(0.728933811188)\n",
      " state (14)  A[0]:(0.809952497482) A[1]:(0.900502920151) A[2]:(0.99999922514) A[3]:(0.810028553009)\n",
      " state (15)  A[0]:(0.992431819439) A[1]:(0.96617692709) A[2]:(1.0) A[3]:(0.903291344643)\n",
      "Episode 294000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6092. Times reached goal: 936.               Steps done: 2909123. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0490712225071.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5905,  0.5906,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.3127e-01, -7.1600e-06,  6.5606e-01,  5.9092e-01]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7289,  0.5906,  0.6558]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0002,  0.8100,  0.0002,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0002,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=2 , Random? True\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.53162252903) A[1]:(0.590351581573) A[2]:(0.590492725372) A[3]:(0.531421780586)\n",
      " state (1)  A[0]:(0.53153693676) A[1]:(-7.05495476723e-05) A[2]:(0.655980587006) A[3]:(0.590873360634)\n",
      " state (2)  A[0]:(0.590801000595) A[1]:(0.728952825069) A[2]:(0.590482115746) A[3]:(0.655844569206)\n",
      " state (3)  A[0]:(0.656445860863) A[1]:(0.000127010047436) A[2]:(0.463733762503) A[3]:(0.549996435642)\n",
      " state (4)  A[0]:(0.59045279026) A[1]:(0.656187593937) A[2]:(-0.000122666358948) A[3]:(0.531551241875)\n",
      " state (5)  A[0]:(0.179420903325) A[1]:(0.922451734543) A[2]:(-0.126998364925) A[3]:(0.495350539684)\n",
      " state (6)  A[0]:(0.000682115438394) A[1]:(0.809922397137) A[2]:(-0.000102519989014) A[3]:(0.656104922295)\n",
      " state (7)  A[0]:(0.639459073544) A[1]:(-0.242511034012) A[2]:(0.141103878617) A[3]:(0.915352106094)\n",
      " state (8)  A[0]:(0.656456530094) A[1]:(-5.4843723774e-05) A[2]:(0.72893679142) A[3]:(0.590628623962)\n",
      " state (9)  A[0]:(0.656360030174) A[1]:(0.810048937798) A[2]:(0.809973597527) A[3]:(0.00036051866482)\n",
      " state (10)  A[0]:(0.729284286499) A[1]:(0.899997293949) A[2]:(-0.000390529603465) A[3]:(0.729013323784)\n",
      " state (11)  A[0]:(0.509211719036) A[1]:(0.876592278481) A[2]:(-0.534800887108) A[3]:(0.837559759617)\n",
      " state (12)  A[0]:(0.0632259920239) A[1]:(0.823209583759) A[2]:(-0.414229810238) A[3]:(0.78754645586)\n",
      " state (13)  A[0]:(2.39908695221e-05) A[1]:(0.807974815369) A[2]:(0.900096297264) A[3]:(0.728975594044)\n",
      " state (14)  A[0]:(0.810289800167) A[1]:(0.900614619255) A[2]:(0.999999284744) A[3]:(0.810102045536)\n",
      " state (15)  A[0]:(0.992388427258) A[1]:(0.966125607491) A[2]:(1.0) A[3]:(0.903124451637)\n",
      "Episode 295000 finished after 0 timesteps with r=0.0. Running score: 0.93. Times trained:               6179. Times reached goal: 963.               Steps done: 2915302. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0487689462674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531563639641) A[1]:(0.590232253075) A[2]:(0.590712785721) A[3]:(0.531493425369)\n",
      " state (1)  A[0]:(0.531584978104) A[1]:(-0.000185612589121) A[2]:(0.656197667122) A[3]:(0.590962350368)\n",
      " state (2)  A[0]:(0.590767979622) A[1]:(0.728836894035) A[2]:(0.590990543365) A[3]:(0.65595138073)\n",
      " state (3)  A[0]:(0.656289219856) A[1]:(0.00458359112963) A[2]:(0.463512539864) A[3]:(0.550615906715)\n",
      " state (4)  A[0]:(0.590201377869) A[1]:(0.65590941906) A[2]:(3.81469726562e-05) A[3]:(0.53167372942)\n",
      " state (5)  A[0]:(0.179061681032) A[1]:(0.922429978848) A[2]:(-0.126987099648) A[3]:(0.49524217844)\n",
      " state (6)  A[0]:(0.000149458646774) A[1]:(0.80991590023) A[2]:(2.3365020752e-05) A[3]:(0.65577596426)\n",
      " state (7)  A[0]:(0.638769745827) A[1]:(-0.242736235261) A[2]:(0.141818717122) A[3]:(0.915153503418)\n",
      " state (8)  A[0]:(0.655886411667) A[1]:(-0.000387642503483) A[2]:(0.728969991207) A[3]:(0.590161144733)\n",
      " state (9)  A[0]:(0.655873298645) A[1]:(0.809920847416) A[2]:(0.809994459152) A[3]:(-0.000275045633316)\n",
      " state (10)  A[0]:(0.729029297829) A[1]:(0.899981498718) A[2]:(-0.000442385644419) A[3]:(0.728779435158)\n",
      " state (11)  A[0]:(0.509111940861) A[1]:(0.876663267612) A[2]:(-0.535640716553) A[3]:(0.837476670742)\n",
      " state (12)  A[0]:(0.0631403252482) A[1]:(0.823411405087) A[2]:(-0.416202455759) A[3]:(0.787437677383)\n",
      " state (13)  A[0]:(-0.000390499801142) A[1]:(0.808261036873) A[2]:(0.900062322617) A[3]:(0.728762507439)\n",
      " state (14)  A[0]:(0.809994697571) A[1]:(0.900784432888) A[2]:(0.999999284744) A[3]:(0.80990165472)\n",
      " state (15)  A[0]:(0.992312073708) A[1]:(0.966112911701) A[2]:(1.0) A[3]:(0.902783095837)\n",
      "Episode 296000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6098. Times reached goal: 944.               Steps done: 2921400. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0484724581441.\n",
      " state (0)  A[0]:(0.531663060188) A[1]:(0.590530157089) A[2]:(0.590502381325) A[3]:(0.531498074532)\n",
      " state (1)  A[0]:(0.531505703926) A[1]:(-8.59051942825e-05) A[2]:(0.656212449074) A[3]:(0.590716481209)\n",
      " state (2)  A[0]:(0.590651631355) A[1]:(0.729086041451) A[2]:(0.590753078461) A[3]:(0.655873417854)\n",
      " state (3)  A[0]:(0.656512618065) A[1]:(0.00922604557127) A[2]:(0.462938308716) A[3]:(0.550867438316)\n",
      " state (4)  A[0]:(0.590435147285) A[1]:(0.656184077263) A[2]:(6.61611557007e-05) A[3]:(0.531683683395)\n",
      " state (5)  A[0]:(0.179223075509) A[1]:(0.922466099262) A[2]:(-0.127064168453) A[3]:(0.495369911194)\n",
      " state (6)  A[0]:(9.55164432526e-05) A[1]:(0.810083210468) A[2]:(-0.000110268592834) A[3]:(0.655923128128)\n",
      " state (7)  A[0]:(0.638782858849) A[1]:(-0.242130666971) A[2]:(0.142171829939) A[3]:(0.915221214294)\n",
      " state (8)  A[0]:(0.656163215637) A[1]:(0.000525519193616) A[2]:(0.729061245918) A[3]:(0.590478181839)\n",
      " state (9)  A[0]:(0.656136214733) A[1]:(0.810258865356) A[2]:(0.810030460358) A[3]:(3.99947166443e-05)\n",
      " state (10)  A[0]:(0.729162693024) A[1]:(0.900114536285) A[2]:(-0.000415325135691) A[3]:(0.728907823563)\n",
      " state (11)  A[0]:(0.509393811226) A[1]:(0.8767786026) A[2]:(-0.536311030388) A[3]:(0.83759701252)\n",
      " state (12)  A[0]:(0.0635285750031) A[1]:(0.823505342007) A[2]:(-0.417850464582) A[3]:(0.787621974945)\n",
      " state (13)  A[0]:(-2.08914279938e-05) A[1]:(0.808270454407) A[2]:(0.900164067745) A[3]:(0.728981256485)\n",
      " state (14)  A[0]:(0.810246825218) A[1]:(0.900726795197) A[2]:(0.999999284744) A[3]:(0.810050964355)\n",
      " state (15)  A[0]:(0.992272019386) A[1]:(0.965990185738) A[2]:(1.0) A[3]:(0.902630269527)\n",
      "Episode 297000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6093. Times reached goal: 950.               Steps done: 2927493. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0481780133935.\n",
      " state (0)  A[0]:(0.5315721035) A[1]:(0.590353190899) A[2]:(0.590482115746) A[3]:(0.531470358372)\n",
      " state (1)  A[0]:(0.531444787979) A[1]:(9.1128051281e-05) A[2]:(0.656033158302) A[3]:(0.590654909611)\n",
      " state (2)  A[0]:(0.590584993362) A[1]:(0.72896528244) A[2]:(0.590710639954) A[3]:(0.655743122101)\n",
      " state (3)  A[0]:(0.656306862831) A[1]:(0.00422584731132) A[2]:(0.463551551104) A[3]:(0.550439715385)\n",
      " state (4)  A[0]:(0.590293645859) A[1]:(0.656175971031) A[2]:(0.000171184539795) A[3]:(0.531546235085)\n",
      " state (5)  A[0]:(0.179112881422) A[1]:(0.922513127327) A[2]:(-0.126997306943) A[3]:(0.495183229446)\n",
      " state (6)  A[0]:(0.000420510739787) A[1]:(0.810025334358) A[2]:(-1.44243240356e-05) A[3]:(0.655830264091)\n",
      " state (7)  A[0]:(0.63899075985) A[1]:(-0.242718085647) A[2]:(0.142621129751) A[3]:(0.915146052837)\n",
      " state (8)  A[0]:(0.656330347061) A[1]:(0.000350706279278) A[2]:(0.729076683521) A[3]:(0.590223968029)\n",
      " state (9)  A[0]:(0.656294047832) A[1]:(0.810190081596) A[2]:(0.810058712959) A[3]:(-0.000138714909554)\n",
      " state (10)  A[0]:(0.729281008244) A[1]:(0.900051116943) A[2]:(-0.00011157989502) A[3]:(0.728815853596)\n",
      " state (11)  A[0]:(0.50977653265) A[1]:(0.876697421074) A[2]:(-0.536717236042) A[3]:(0.837576389313)\n",
      " state (12)  A[0]:(0.0641546919942) A[1]:(0.823372364044) A[2]:(-0.419332057238) A[3]:(0.787627637386)\n",
      " state (13)  A[0]:(0.000484794349177) A[1]:(0.808042347431) A[2]:(0.900114655495) A[3]:(0.728977382183)\n",
      " state (14)  A[0]:(0.810439825058) A[1]:(0.900507807732) A[2]:(0.999999344349) A[3]:(0.810052394867)\n",
      " state (15)  A[0]:(0.992235660553) A[1]:(0.965809524059) A[2]:(1.0) A[3]:(0.902456343174)\n",
      "Episode 298000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6146. Times reached goal: 957.               Steps done: 2933639. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0478828193835.\n",
      " state (0)  A[0]:(0.531551599503) A[1]:(0.590457677841) A[2]:(0.590464830399) A[3]:(0.531349897385)\n",
      " state (1)  A[0]:(0.531495392323) A[1]:(-7.30790197849e-05) A[2]:(0.656110286713) A[3]:(0.590672850609)\n",
      " state (2)  A[0]:(0.590623259544) A[1]:(0.728901982307) A[2]:(0.590882182121) A[3]:(0.655792415142)\n",
      " state (3)  A[0]:(0.656409502029) A[1]:(0.00345201534219) A[2]:(0.463553696871) A[3]:(0.550487101078)\n",
      " state (4)  A[0]:(0.590355277061) A[1]:(0.656106054783) A[2]:(-1.22785568237e-05) A[3]:(0.531536579132)\n",
      " state (5)  A[0]:(0.178890898824) A[1]:(0.922497212887) A[2]:(-0.127129718661) A[3]:(0.495113223791)\n",
      " state (6)  A[0]:(1.91032886505e-05) A[1]:(0.809949457645) A[2]:(-6.50882720947e-05) A[3]:(0.655804276466)\n",
      " state (7)  A[0]:(0.638640522957) A[1]:(-0.243149846792) A[2]:(0.142905369401) A[3]:(0.915146827698)\n",
      " state (8)  A[0]:(0.656121730804) A[1]:(-0.000217381864786) A[2]:(0.728880286217) A[3]:(0.590615510941)\n",
      " state (9)  A[0]:(0.656017959118) A[1]:(0.809972345829) A[2]:(0.809925794601) A[3]:(0.000184625387192)\n",
      " state (10)  A[0]:(0.728954434395) A[1]:(0.89997112751) A[2]:(-0.000552773417439) A[3]:(0.728849589825)\n",
      " state (11)  A[0]:(0.509237527847) A[1]:(0.876664280891) A[2]:(-0.537731528282) A[3]:(0.837590515614)\n",
      " state (12)  A[0]:(0.063242085278) A[1]:(0.823395371437) A[2]:(-0.421346545219) A[3]:(0.787603497505)\n",
      " state (13)  A[0]:(-0.000723153236322) A[1]:(0.808102309704) A[2]:(0.9000659585) A[3]:(0.72888314724)\n",
      " state (14)  A[0]:(0.80998980999) A[1]:(0.900528848171) A[2]:(0.999999344349) A[3]:(0.809995889664)\n",
      " state (15)  A[0]:(0.992162227631) A[1]:(0.965738475323) A[2]:(1.0) A[3]:(0.902250230312)\n",
      "Episode 299000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6064. Times reached goal: 945.               Steps done: 2939703. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0475933365658.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5905,  0.5906,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.0001,  0.6561,  0.5908]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7288,  0.5907,  0.6558]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8100,  0.0001,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0005,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9006,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531523942947) A[1]:(0.590441465378) A[2]:(0.590537786484) A[3]:(0.531577348709)\n",
      " state (1)  A[0]:(0.531396925449) A[1]:(7.34850764275e-05) A[2]:(0.656099438667) A[3]:(0.590790808201)\n",
      " state (2)  A[0]:(0.590584874153) A[1]:(0.728940248489) A[2]:(0.590772747993) A[3]:(0.655871629715)\n",
      " state (3)  A[0]:(0.65637165308) A[1]:(0.00060483062407) A[2]:(0.46384960413) A[3]:(0.550487995148)\n",
      " state (4)  A[0]:(0.59043264389) A[1]:(0.656206846237) A[2]:(8.0943107605e-05) A[3]:(0.53173148632)\n",
      " state (5)  A[0]:(0.179110601544) A[1]:(0.922528207302) A[2]:(-0.126976668835) A[3]:(0.495292186737)\n",
      " state (6)  A[0]:(0.000331997871399) A[1]:(0.809986054897) A[2]:(0.00018048286438) A[3]:(0.65594804287)\n",
      " state (7)  A[0]:(0.63868200779) A[1]:(-0.243151217699) A[2]:(0.143662810326) A[3]:(0.915150702)\n",
      " state (8)  A[0]:(0.656167209148) A[1]:(8.86544585228e-05) A[2]:(0.729214668274) A[3]:(0.590412020683)\n",
      " state (9)  A[0]:(0.656145095825) A[1]:(0.810104131699) A[2]:(0.810145497322) A[3]:(-0.000143617391586)\n",
      " state (10)  A[0]:(0.729214906693) A[1]:(0.900048851967) A[2]:(-8.70227813721e-05) A[3]:(0.728896856308)\n",
      " state (11)  A[0]:(0.509956657887) A[1]:(0.876802384853) A[2]:(-0.538090229034) A[3]:(0.837729156017)\n",
      " state (12)  A[0]:(0.0643275752664) A[1]:(0.823650300503) A[2]:(-0.42266279459) A[3]:(0.787821292877)\n",
      " state (13)  A[0]:(0.000204235315323) A[1]:(0.808421373367) A[2]:(0.90014141798) A[3]:(0.729108214378)\n",
      " state (14)  A[0]:(0.810285568237) A[1]:(0.900715172291) A[2]:(0.999999344349) A[3]:(0.810118556023)\n",
      " state (15)  A[0]:(0.992121815681) A[1]:(0.965738832951) A[2]:(1.0) A[3]:(0.902086734772)\n",
      "Episode 300000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6120. Times reached goal: 950.               Steps done: 2945823. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0473029548205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531475305557) A[1]:(0.590453147888) A[2]:(0.590672969818) A[3]:(0.531428337097)\n",
      " state (1)  A[0]:(0.531471252441) A[1]:(9.91262495518e-05) A[2]:(0.656204581261) A[3]:(0.590875446796)\n",
      " state (2)  A[0]:(0.590678751469) A[1]:(0.729053378105) A[2]:(0.59082275629) A[3]:(0.656008541584)\n",
      " state (3)  A[0]:(0.656391382217) A[1]:(0.000518295855727) A[2]:(0.46372449398) A[3]:(0.550544381142)\n",
      " state (4)  A[0]:(0.590350151062) A[1]:(0.656289100647) A[2]:(-0.00010097026825) A[3]:(0.531634569168)\n",
      " state (5)  A[0]:(0.178800106049) A[1]:(0.922560751438) A[2]:(-0.127096056938) A[3]:(0.495194792747)\n",
      " state (6)  A[0]:(0.00022080540657) A[1]:(0.809953093529) A[2]:(5.55515289307e-05) A[3]:(0.655997633934)\n",
      " state (7)  A[0]:(0.638791799545) A[1]:(-0.243671178818) A[2]:(0.143705770373) A[3]:(0.915159583092)\n",
      " state (8)  A[0]:(0.656289577484) A[1]:(-0.000197794288397) A[2]:(0.729056835175) A[3]:(0.590445995331)\n",
      " state (9)  A[0]:(0.656201481819) A[1]:(0.809999704361) A[2]:(0.810006022453) A[3]:(-1.85668468475e-05)\n",
      " state (10)  A[0]:(0.729179918766) A[1]:(0.899982094765) A[2]:(-0.000468850106699) A[3]:(0.728955149651)\n",
      " state (11)  A[0]:(0.509907126427) A[1]:(0.876737654209) A[2]:(-0.538954079151) A[3]:(0.837775409222)\n",
      " state (12)  A[0]:(0.0641217529774) A[1]:(0.823574662209) A[2]:(-0.424541890621) A[3]:(0.787857055664)\n",
      " state (13)  A[0]:(-0.000328004360199) A[1]:(0.808316469193) A[2]:(0.900013566017) A[3]:(0.729075431824)\n",
      " state (14)  A[0]:(0.810042500496) A[1]:(0.900619924068) A[2]:(0.999999344349) A[3]:(0.810043632984)\n",
      " state (15)  A[0]:(0.992062687874) A[1]:(0.96562731266) A[2]:(1.0) A[3]:(0.901848375797)\n",
      "Episode 301000 finished after 0 timesteps with r=0.0. Running score: 0.96. Times trained:               6108. Times reached goal: 946.               Steps done: 2951931. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.04701490896.\n",
      " state (0)  A[0]:(0.53145647049) A[1]:(0.590564966202) A[2]:(0.590566515923) A[3]:(0.531383037567)\n",
      " state (1)  A[0]:(0.531343817711) A[1]:(6.49318099022e-05) A[2]:(0.656156778336) A[3]:(0.59061217308)\n",
      " state (2)  A[0]:(0.590569376945) A[1]:(0.729035496712) A[2]:(0.590790748596) A[3]:(0.655864953995)\n",
      " state (3)  A[0]:(0.656425893307) A[1]:(-0.000232018530369) A[2]:(0.463888436556) A[3]:(0.550292611122)\n",
      " state (4)  A[0]:(0.590510368347) A[1]:(0.656083583832) A[2]:(7.54594802856e-05) A[3]:(0.53155630827)\n",
      " state (5)  A[0]:(0.179043442011) A[1]:(0.922535479069) A[2]:(-0.127049505711) A[3]:(0.495315670967)\n",
      " state (6)  A[0]:(0.000209361314774) A[1]:(0.81001830101) A[2]:(8.01086425781e-05) A[3]:(0.656061470509)\n",
      " state (7)  A[0]:(0.638508200645) A[1]:(-0.243251010776) A[2]:(0.144152522087) A[3]:(0.915135204792)\n",
      " state (8)  A[0]:(0.656263113022) A[1]:(0.000174455344677) A[2]:(0.729123234749) A[3]:(0.590615093708)\n",
      " state (9)  A[0]:(0.656277179718) A[1]:(0.810079097748) A[2]:(0.810097455978) A[3]:(0.000270396471024)\n",
      " state (10)  A[0]:(0.729264616966) A[1]:(0.900034189224) A[2]:(-9.48905944824e-05) A[3]:(0.729059755802)\n",
      " state (11)  A[0]:(0.510234117508) A[1]:(0.876820862293) A[2]:(-0.539303183556) A[3]:(0.837861418724)\n",
      " state (12)  A[0]:(0.0646761208773) A[1]:(0.823678731918) A[2]:(-0.425787240267) A[3]:(0.787963867188)\n",
      " state (13)  A[0]:(0.000211328268051) A[1]:(0.808339297771) A[2]:(0.900113046169) A[3]:(0.729183614254)\n",
      " state (14)  A[0]:(0.810295522213) A[1]:(0.900522768497) A[2]:(0.999999403954) A[3]:(0.810153901577)\n",
      " state (15)  A[0]:(0.992026090622) A[1]:(0.965474545956) A[2]:(1.0) A[3]:(0.901742398739)\n",
      "Episode 302000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6099. Times reached goal: 949.               Steps done: 2958030. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0467290376809.\n",
      " state (0)  A[0]:(0.531596839428) A[1]:(0.590646088123) A[2]:(0.590608358383) A[3]:(0.531496286392)\n",
      " state (1)  A[0]:(0.53154861927) A[1]:(0.000212654471397) A[2]:(0.656235575676) A[3]:(0.590823352337)\n",
      " state (2)  A[0]:(0.590779781342) A[1]:(0.729125678539) A[2]:(0.590844035149) A[3]:(0.655978560448)\n",
      " state (3)  A[0]:(0.656715035439) A[1]:(0.0011351252906) A[2]:(0.463770449162) A[3]:(0.550748586655)\n",
      " state (4)  A[0]:(0.590802907944) A[1]:(0.656268835068) A[2]:(0.000146150588989) A[3]:(0.531658530235)\n",
      " state (5)  A[0]:(0.179325401783) A[1]:(0.922603011131) A[2]:(-0.12696364522) A[3]:(0.495129644871)\n",
      " state (6)  A[0]:(0.000562429369893) A[1]:(0.810107707977) A[2]:(0.000148892402649) A[3]:(0.655970335007)\n",
      " state (7)  A[0]:(0.638675928116) A[1]:(-0.2433706671) A[2]:(0.144420892) A[3]:(0.915143668652)\n",
      " state (8)  A[0]:(0.656510710716) A[1]:(0.000313773751259) A[2]:(0.729070782661) A[3]:(0.59068775177)\n",
      " state (9)  A[0]:(0.656586289406) A[1]:(0.810123443604) A[2]:(0.810073494911) A[3]:(0.000231221318245)\n",
      " state (10)  A[0]:(0.729519248009) A[1]:(0.900064468384) A[2]:(-0.000102996826172) A[3]:(0.729013681412)\n",
      " state (11)  A[0]:(0.510788798332) A[1]:(0.876909434795) A[2]:(-0.539927721024) A[3]:(0.837853550911)\n",
      " state (12)  A[0]:(0.0654766634107) A[1]:(0.823871731758) A[2]:(-0.427386105061) A[3]:(0.787951052189)\n",
      " state (13)  A[0]:(0.000855475431308) A[1]:(0.808559775352) A[2]:(0.900043964386) A[3]:(0.729126572609)\n",
      " state (14)  A[0]:(0.810470461845) A[1]:(0.900582492352) A[2]:(0.999999403954) A[3]:(0.810106992722)\n",
      " state (15)  A[0]:(0.991986811161) A[1]:(0.965406239033) A[2]:(1.0) A[3]:(0.901550292969)\n",
      "Episode 303000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6116. Times reached goal: 959.               Steps done: 2964146. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0464441150679.\n",
      " state (0)  A[0]:(0.531441092491) A[1]:(0.590616583824) A[2]:(0.590527057648) A[3]:(0.531507015228)\n",
      " state (1)  A[0]:(0.531463265419) A[1]:(-0.000121537595987) A[2]:(0.656069517136) A[3]:(0.590713500977)\n",
      " state (2)  A[0]:(0.590811192989) A[1]:(0.729094147682) A[2]:(0.590708732605) A[3]:(0.656227588654)\n",
      " state (3)  A[0]:(0.656774401665) A[1]:(0.000376846612198) A[2]:(0.463903665543) A[3]:(0.550067067146)\n",
      " state (4)  A[0]:(0.591055750847) A[1]:(0.656544327736) A[2]:(0.000152230262756) A[3]:(0.531348764896)\n",
      " state (5)  A[0]:(0.17989204824) A[1]:(0.922609090805) A[2]:(-0.127037540078) A[3]:(0.495468378067)\n",
      " state (6)  A[0]:(0.00122138799634) A[1]:(0.810045957565) A[2]:(0.000222325325012) A[3]:(0.656192302704)\n",
      " state (7)  A[0]:(0.639005303383) A[1]:(-0.243487864733) A[2]:(0.14519816637) A[3]:(0.915069401264)\n",
      " state (8)  A[0]:(0.657006025314) A[1]:(0.0002163015306) A[2]:(0.729010879993) A[3]:(0.590893447399)\n",
      " state (9)  A[0]:(0.65684658289) A[1]:(0.810150444508) A[2]:(0.810018718243) A[3]:(0.000368103355868)\n",
      " state (10)  A[0]:(0.729539036751) A[1]:(0.900098443031) A[2]:(-0.000325679779053) A[3]:(0.728982567787)\n",
      " state (11)  A[0]:(0.51077401638) A[1]:(0.876973748207) A[2]:(-0.540848195553) A[3]:(0.837846457958)\n",
      " state (12)  A[0]:(0.065309651196) A[1]:(0.823989152908) A[2]:(-0.429322510958) A[3]:(0.787914872169)\n",
      " state (13)  A[0]:(0.000449836225016) A[1]:(0.808694958687) A[2]:(0.900038301945) A[3]:(0.729010820389)\n",
      " state (14)  A[0]:(0.810266852379) A[1]:(0.900627374649) A[2]:(0.999999403954) A[3]:(0.809986948967)\n",
      " state (15)  A[0]:(0.991917669773) A[1]:(0.965335190296) A[2]:(1.0) A[3]:(0.901259303093)\n",
      "Episode 304000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6146. Times reached goal: 951.               Steps done: 2970292. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0461595449165.\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.5905,  0.5906,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.0000,  0.6561,  0.5907]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5909,  0.7290,  0.5907,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0000,  0.8100, -0.0001,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0003,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9005,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531287908554) A[1]:(0.590573310852) A[2]:(0.590626657009) A[3]:(0.531278848648)\n",
      " state (1)  A[0]:(0.531251490116) A[1]:(2.06455588341e-05) A[2]:(0.656127333641) A[3]:(0.590315580368)\n",
      " state (2)  A[0]:(0.590499162674) A[1]:(0.72901391983) A[2]:(0.590714216232) A[3]:(0.655912518501)\n",
      " state (3)  A[0]:(0.656243801117) A[1]:(-0.000295080244541) A[2]:(0.463909000158) A[3]:(0.549749732018)\n",
      " state (4)  A[0]:(0.590426385403) A[1]:(0.65632390976) A[2]:(-9.53674316406e-05) A[3]:(0.531265497208)\n",
      " state (5)  A[0]:(0.179117664695) A[1]:(0.922553300858) A[2]:(-0.127508178353) A[3]:(0.495501548052)\n",
      " state (6)  A[0]:(-2.0444393158e-05) A[1]:(0.810050725937) A[2]:(-0.000104904174805) A[3]:(0.65584397316)\n",
      " state (7)  A[0]:(0.637854993343) A[1]:(-0.243408590555) A[2]:(0.145790114999) A[3]:(0.91479241848)\n",
      " state (8)  A[0]:(0.656307339668) A[1]:(-7.65398144722e-05) A[2]:(0.728996515274) A[3]:(0.59017932415)\n",
      " state (9)  A[0]:(0.656308293343) A[1]:(0.810026407242) A[2]:(0.809995830059) A[3]:(-0.000717237475328)\n",
      " state (10)  A[0]:(0.7291918993) A[1]:(0.899993300438) A[2]:(-0.000509023608174) A[3]:(0.728665351868)\n",
      " state (11)  A[0]:(0.510421276093) A[1]:(0.876828014851) A[2]:(-0.541744709015) A[3]:(0.837750971317)\n",
      " state (12)  A[0]:(0.0648761466146) A[1]:(0.823779940605) A[2]:(-0.431272625923) A[3]:(0.787820518017)\n",
      " state (13)  A[0]:(-2.36332416534e-05) A[1]:(0.808462262154) A[2]:(0.900018930435) A[3]:(0.72886890173)\n",
      " state (14)  A[0]:(0.810201406479) A[1]:(0.900489211082) A[2]:(0.999999403954) A[3]:(0.809904813766)\n",
      " state (15)  A[0]:(0.991862535477) A[1]:(0.965206921101) A[2]:(1.0) A[3]:(0.901017010212)\n",
      "Episode 305000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6121. Times reached goal: 947.               Steps done: 2976413. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.045877865302.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531428456306) A[1]:(0.590413928032) A[2]:(0.590462684631) A[3]:(0.531134486198)\n",
      " state (1)  A[0]:(0.531408131123) A[1]:(6.01597130299e-05) A[2]:(0.656103014946) A[3]:(0.59022206068)\n",
      " state (2)  A[0]:(0.590570807457) A[1]:(0.728950560093) A[2]:(0.590697050095) A[3]:(0.655910491943)\n",
      " state (3)  A[0]:(0.656222760677) A[1]:(1.47707760334e-05) A[2]:(0.463948577642) A[3]:(0.54967880249)\n",
      " state (4)  A[0]:(0.590437531471) A[1]:(0.656141638756) A[2]:(5.96046447754e-06) A[3]:(0.531283915043)\n",
      " state (5)  A[0]:(0.179333001375) A[1]:(0.922491192818) A[2]:(-0.127505004406) A[3]:(0.495716392994)\n",
      " state (6)  A[0]:(0.000100672245026) A[1]:(0.809974730015) A[2]:(0.000126242637634) A[3]:(0.6558355093)\n",
      " state (7)  A[0]:(0.637589514256) A[1]:(-0.243412747979) A[2]:(0.146821603179) A[3]:(0.91466742754)\n",
      " state (8)  A[0]:(0.656334459782) A[1]:(-0.000184785574675) A[2]:(0.729038536549) A[3]:(0.590435922146)\n",
      " state (9)  A[0]:(0.65636408329) A[1]:(0.809994578362) A[2]:(0.810030937195) A[3]:(-0.00016550719738)\n",
      " state (10)  A[0]:(0.729305326939) A[1]:(0.899999082088) A[2]:(-0.000244736671448) A[3]:(0.728930950165)\n",
      " state (11)  A[0]:(0.510851264) A[1]:(0.876893401146) A[2]:(-0.542185127735) A[3]:(0.837950110435)\n",
      " state (12)  A[0]:(0.0655535459518) A[1]:(0.823958873749) A[2]:(-0.432584047318) A[3]:(0.788038134575)\n",
      " state (13)  A[0]:(0.000539034546819) A[1]:(0.808735728264) A[2]:(0.900164842606) A[3]:(0.729037284851)\n",
      " state (14)  A[0]:(0.810368597507) A[1]:(0.900667607784) A[2]:(0.999999403954) A[3]:(0.809986591339)\n",
      " state (15)  A[0]:(0.991809725761) A[1]:(0.965204834938) A[2]:(1.0) A[3]:(0.900824666023)\n",
      "Episode 306000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6141. Times reached goal: 955.               Steps done: 2982554. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0455969926334.\n",
      " state (0)  A[0]:(0.531818509102) A[1]:(0.590733468533) A[2]:(0.590550303459) A[3]:(0.532157063484)\n",
      " state (1)  A[0]:(0.531639814377) A[1]:(3.61017882824e-05) A[2]:(0.656352639198) A[3]:(0.590999245644)\n",
      " state (2)  A[0]:(0.590733528137) A[1]:(0.729128718376) A[2]:(0.590984284878) A[3]:(0.656580150127)\n",
      " state (3)  A[0]:(0.656466245651) A[1]:(0.00128496740945) A[2]:(0.463968515396) A[3]:(0.550410211086)\n",
      " state (4)  A[0]:(0.590734481812) A[1]:(0.656360864639) A[2]:(-0.000104427337646) A[3]:(0.532019853592)\n",
      " state (5)  A[0]:(0.17968454957) A[1]:(0.922491550446) A[2]:(-0.127787739038) A[3]:(0.496654212475)\n",
      " state (6)  A[0]:(-1.06692314148e-05) A[1]:(0.810047268867) A[2]:(-6.47306442261e-05) A[3]:(0.656430602074)\n",
      " state (7)  A[0]:(0.63721626997) A[1]:(-0.243002444506) A[2]:(0.147420957685) A[3]:(0.914754450321)\n",
      " state (8)  A[0]:(0.656300544739) A[1]:(0.00018860027194) A[2]:(0.729143381119) A[3]:(0.590910792351)\n",
      " state (9)  A[0]:(0.656352162361) A[1]:(0.810117781162) A[2]:(0.810090363026) A[3]:(0.00018173456192)\n",
      " state (10)  A[0]:(0.729183673859) A[1]:(0.900033891201) A[2]:(-8.65459442139e-05) A[3]:(0.729072451591)\n",
      " state (11)  A[0]:(0.510648965836) A[1]:(0.876911342144) A[2]:(-0.542743206024) A[3]:(0.838075876236)\n",
      " state (12)  A[0]:(0.0651652216911) A[1]:(0.823940634727) A[2]:(-0.4342187047) A[3]:(0.788206458092)\n",
      " state (13)  A[0]:(-0.000142097473145) A[1]:(0.80860376358) A[2]:(0.900007247925) A[3]:(0.729189157486)\n",
      " state (14)  A[0]:(0.810083627701) A[1]:(0.900469183922) A[2]:(0.999999463558) A[3]:(0.810040056705)\n",
      " state (15)  A[0]:(0.991752624512) A[1]:(0.965024828911) A[2]:(1.0) A[3]:(0.900663733482)\n",
      "Episode 307000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6107. Times reached goal: 958.               Steps done: 2988661. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0453193803513.\n",
      " state (0)  A[0]:(0.531762003899) A[1]:(0.590534329414) A[2]:(0.590535283089) A[3]:(0.531492471695)\n",
      " state (1)  A[0]:(0.531647443771) A[1]:(-4.02145087719e-05) A[2]:(0.65617364645) A[3]:(0.590438723564)\n",
      " state (2)  A[0]:(0.590688705444) A[1]:(0.728982269764) A[2]:(0.590718865395) A[3]:(0.656112074852)\n",
      " state (3)  A[0]:(0.656339287758) A[1]:(-7.57984817028e-05) A[2]:(0.464051663876) A[3]:(0.549703001976)\n",
      " state (4)  A[0]:(0.590609073639) A[1]:(0.656045913696) A[2]:(-2.64644622803e-05) A[3]:(0.531476259232)\n",
      " state (5)  A[0]:(0.179686889052) A[1]:(0.922431230545) A[2]:(-0.127851769328) A[3]:(0.496323525906)\n",
      " state (6)  A[0]:(0.000114798545837) A[1]:(0.809913933277) A[2]:(-3.48091125488e-05) A[3]:(0.656080186367)\n",
      " state (7)  A[0]:(0.636987924576) A[1]:(-0.243419781327) A[2]:(0.147954449058) A[3]:(0.914503872395)\n",
      " state (8)  A[0]:(0.656113028526) A[1]:(-0.000368159235222) A[2]:(0.728927075863) A[3]:(0.590732455254)\n",
      " state (9)  A[0]:(0.656039178371) A[1]:(0.80990165472) A[2]:(0.809972643852) A[3]:(0.000258848071098)\n",
      " state (10)  A[0]:(0.728976249695) A[1]:(0.899941205978) A[2]:(-0.00035727021168) A[3]:(0.729065239429)\n",
      " state (11)  A[0]:(0.510553479195) A[1]:(0.876855492592) A[2]:(-0.543573260307) A[3]:(0.838084816933)\n",
      " state (12)  A[0]:(0.0651497617364) A[1]:(0.823937118053) A[2]:(-0.43590721488) A[3]:(0.788192451)\n",
      " state (13)  A[0]:(-0.000185906887054) A[1]:(0.80864739418) A[2]:(0.900045216084) A[3]:(0.729158163071)\n",
      " state (14)  A[0]:(0.810062050819) A[1]:(0.900479018688) A[2]:(0.999999463558) A[3]:(0.81012815237)\n",
      " state (15)  A[0]:(0.991695821285) A[1]:(0.9649477005) A[2]:(1.0) A[3]:(0.900596737862)\n",
      "Episode 308000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6131. Times reached goal: 964.               Steps done: 2994792. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0450423772511.\n",
      " state (0)  A[0]:(0.530835270882) A[1]:(0.590325713158) A[2]:(0.590135216713) A[3]:(0.531125307083)\n",
      " state (1)  A[0]:(0.530840277672) A[1]:(-0.000153962522745) A[2]:(0.655825018883) A[3]:(0.590192437172)\n",
      " state (2)  A[0]:(0.590003490448) A[1]:(0.728786587715) A[2]:(0.590425133705) A[3]:(0.655910968781)\n",
      " state (3)  A[0]:(0.655854821205) A[1]:(0.000222351402044) A[2]:(0.463908076286) A[3]:(0.549327790737)\n",
      " state (4)  A[0]:(0.590279519558) A[1]:(0.655573248863) A[2]:(-6.43730163574e-05) A[3]:(0.531050682068)\n",
      " state (5)  A[0]:(0.179580032825) A[1]:(0.922371268272) A[2]:(-0.128168001771) A[3]:(0.496115505695)\n",
      " state (6)  A[0]:(0.000129669904709) A[1]:(0.809880137444) A[2]:(-0.000264883041382) A[3]:(0.655813455582)\n",
      " state (7)  A[0]:(0.636615991592) A[1]:(-0.243298739195) A[2]:(0.148354128003) A[3]:(0.91426640749)\n",
      " state (8)  A[0]:(0.655896663666) A[1]:(-0.000443402648671) A[2]:(0.728677034378) A[3]:(0.590581417084)\n",
      " state (9)  A[0]:(0.655863404274) A[1]:(0.809804499149) A[2]:(0.809887051582) A[3]:(0.000130489468575)\n",
      " state (10)  A[0]:(0.72894436121) A[1]:(0.899912118912) A[2]:(-0.00050747388741) A[3]:(0.728927493095)\n",
      " state (11)  A[0]:(0.510760188103) A[1]:(0.876857995987) A[2]:(-0.544358372688) A[3]:(0.838017106056)\n",
      " state (12)  A[0]:(0.0655061602592) A[1]:(0.823961913586) A[2]:(-0.437683224678) A[3]:(0.788082361221)\n",
      " state (13)  A[0]:(-1.37090682983e-06) A[1]:(0.80862724781) A[2]:(0.899941205978) A[3]:(0.728980183601)\n",
      " state (14)  A[0]:(0.810047745705) A[1]:(0.90038061142) A[2]:(0.999999463558) A[3]:(0.810046076775)\n",
      " state (15)  A[0]:(0.991643249989) A[1]:(0.964810490608) A[2]:(1.0) A[3]:(0.900425255299)\n",
      "Episode 309000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6086. Times reached goal: 951.               Steps done: 3000878. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0447690818247.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5904,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.0001,  0.6561,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.7290,  0.5904,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8100, -0.0003,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9001, -0.0003,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.9004,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531409740448) A[1]:(0.590422987938) A[2]:(0.590506672859) A[3]:(0.531519293785)\n",
      " state (1)  A[0]:(0.5313975811) A[1]:(2.49706208706e-05) A[2]:(0.656026601791) A[3]:(0.590518951416)\n",
      " state (2)  A[0]:(0.590511441231) A[1]:(0.728976011276) A[2]:(0.590311527252) A[3]:(0.656201779842)\n",
      " state (3)  A[0]:(0.656194448471) A[1]:(-0.000110480934381) A[2]:(0.463960647583) A[3]:(0.549700260162)\n",
      " state (4)  A[0]:(0.590443372726) A[1]:(0.655982851982) A[2]:(-5.63859939575e-05) A[3]:(0.531556665897)\n",
      " state (5)  A[0]:(0.179526731372) A[1]:(0.922400176525) A[2]:(-0.128208100796) A[3]:(0.496673583984)\n",
      " state (6)  A[0]:(-8.99732112885e-05) A[1]:(0.809931874275) A[2]:(-0.000253915786743) A[3]:(0.656085908413)\n",
      " state (7)  A[0]:(0.636451363564) A[1]:(-0.242952659726) A[2]:(0.148993656039) A[3]:(0.914223253727)\n",
      " state (8)  A[0]:(0.655995130539) A[1]:(0.000169392675161) A[2]:(0.728789567947) A[3]:(0.590529680252)\n",
      " state (9)  A[0]:(0.655943036079) A[1]:(0.810029745102) A[2]:(0.809918940067) A[3]:(0.000136166810989)\n",
      " state (10)  A[0]:(0.728915035725) A[1]:(0.900006592274) A[2]:(-0.000404477090342) A[3]:(0.728919148445)\n",
      " state (11)  A[0]:(0.510756731033) A[1]:(0.876947402954) A[2]:(-0.544898867607) A[3]:(0.83802986145)\n",
      " state (12)  A[0]:(0.0655197501183) A[1]:(0.824036836624) A[2]:(-0.43911075592) A[3]:(0.788106799126)\n",
      " state (13)  A[0]:(5.69224357605e-05) A[1]:(0.808605730534) A[2]:(0.900007247925) A[3]:(0.728989124298)\n",
      " state (14)  A[0]:(0.810210943222) A[1]:(0.900261044502) A[2]:(0.999999463558) A[3]:(0.810034513474)\n",
      " state (15)  A[0]:(0.99160528183) A[1]:(0.964650511742) A[2]:(1.0) A[3]:(0.900210380554)\n",
      "Episode 310000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6126. Times reached goal: 951.               Steps done: 3007004. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.044495664761.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531684696674) A[1]:(0.590525388718) A[2]:(0.590484380722) A[3]:(0.531517744064)\n",
      " state (1)  A[0]:(0.531416535378) A[1]:(-7.41071999073e-05) A[2]:(0.656123578548) A[3]:(0.590538740158)\n",
      " state (2)  A[0]:(0.590443491936) A[1]:(0.728903353214) A[2]:(0.590559124947) A[3]:(0.656235635281)\n",
      " state (3)  A[0]:(0.656041264534) A[1]:(-0.000205386430025) A[2]:(0.464151650667) A[3]:(0.549814343452)\n",
      " state (4)  A[0]:(0.590320408344) A[1]:(0.655895233154) A[2]:(0.000106692314148) A[3]:(0.531786561012)\n",
      " state (5)  A[0]:(0.179493710399) A[1]:(0.92237752676) A[2]:(-0.128078535199) A[3]:(0.497066348791)\n",
      " state (6)  A[0]:(-0.000365316838725) A[1]:(0.80992347002) A[2]:(0.000114560127258) A[3]:(0.656281232834)\n",
      " state (7)  A[0]:(0.636051774025) A[1]:(-0.243291094899) A[2]:(0.150089621544) A[3]:(0.914221048355)\n",
      " state (8)  A[0]:(0.655853331089) A[1]:(-0.000517118663993) A[2]:(0.728888034821) A[3]:(0.590928554535)\n",
      " state (9)  A[0]:(0.655738592148) A[1]:(0.809873104095) A[2]:(0.810012876987) A[3]:(-2.15768814087e-05)\n",
      " state (10)  A[0]:(0.728785037994) A[1]:(0.899954378605) A[2]:(-0.00029194355011) A[3]:(0.728933215141)\n",
      " state (11)  A[0]:(0.510663032532) A[1]:(0.876948535442) A[2]:(-0.54555273056) A[3]:(0.838149011135)\n",
      " state (12)  A[0]:(0.0652884617448) A[1]:(0.824156403542) A[2]:(-0.440632134676) A[3]:(0.788279414177)\n",
      " state (13)  A[0]:(-0.000429749459727) A[1]:(0.808879256248) A[2]:(0.900060772896) A[3]:(0.729141116142)\n",
      " state (14)  A[0]:(0.809963703156) A[1]:(0.900487780571) A[2]:(0.999999463558) A[3]:(0.810094833374)\n",
      " state (15)  A[0]:(0.991535007954) A[1]:(0.964696764946) A[2]:(1.0) A[3]:(0.900014162064)\n",
      "Episode 311000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6157. Times reached goal: 962.               Steps done: 3013161. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.04422254661.\n",
      " state (0)  A[0]:(0.531596541405) A[1]:(0.590580642223) A[2]:(0.59043443203) A[3]:(0.531740665436)\n",
      " state (1)  A[0]:(0.531692326069) A[1]:(0.000159732997417) A[2]:(0.656044781208) A[3]:(0.590738356113)\n",
      " state (2)  A[0]:(0.590852379799) A[1]:(0.72916662693) A[2]:(0.590591788292) A[3]:(0.656378030777)\n",
      " state (3)  A[0]:(0.656597137451) A[1]:(0.00062330806395) A[2]:(0.464019209146) A[3]:(0.549841046333)\n",
      " state (4)  A[0]:(0.591068863869) A[1]:(0.656534075737) A[2]:(-0.000241160392761) A[3]:(0.531809270382)\n",
      " state (5)  A[0]:(0.180661708117) A[1]:(0.922511577606) A[2]:(-0.128465101123) A[3]:(0.497189044952)\n",
      " state (6)  A[0]:(0.00119635404553) A[1]:(0.810108244419) A[2]:(-2.76565551758e-05) A[3]:(0.656405031681)\n",
      " state (7)  A[0]:(0.637065291405) A[1]:(-0.242788091302) A[2]:(0.15084682405) A[3]:(0.914179027081)\n",
      " state (8)  A[0]:(0.656937479973) A[1]:(0.000660113873892) A[2]:(0.729105889797) A[3]:(0.590912699699)\n",
      " state (9)  A[0]:(0.656888127327) A[1]:(0.810282766819) A[2]:(0.8100887537) A[3]:(0.000564455927815)\n",
      " state (10)  A[0]:(0.729638338089) A[1]:(0.900137245655) A[2]:(0.000104904174805) A[3]:(0.729187726974)\n",
      " state (11)  A[0]:(0.511965036392) A[1]:(0.877157688141) A[2]:(-0.545809626579) A[3]:(0.838265240192)\n",
      " state (12)  A[0]:(0.0668878853321) A[1]:(0.824437737465) A[2]:(-0.441825807095) A[3]:(0.788368225098)\n",
      " state (13)  A[0]:(0.000880658393726) A[1]:(0.809130132198) A[2]:(0.900068998337) A[3]:(0.729164719582)\n",
      " state (14)  A[0]:(0.810369133949) A[1]:(0.900553047657) A[2]:(0.999999523163) A[3]:(0.810082614422)\n",
      " state (15)  A[0]:(0.991505265236) A[1]:(0.964628458023) A[2]:(1.0) A[3]:(0.899825036526)\n",
      "Episode 312000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6147. Times reached goal: 957.               Steps done: 3019308. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0439515443947.\n",
      " state (0)  A[0]:(0.531562805176) A[1]:(0.590615868568) A[2]:(0.590656757355) A[3]:(0.531453132629)\n",
      " state (1)  A[0]:(0.531529784203) A[1]:(7.91549682617e-05) A[2]:(0.656232297421) A[3]:(0.590423464775)\n",
      " state (2)  A[0]:(0.590628027916) A[1]:(0.72901237011) A[2]:(0.590717494488) A[3]:(0.656077980995)\n",
      " state (3)  A[0]:(0.656247794628) A[1]:(-0.000537455023732) A[2]:(0.464381217957) A[3]:(0.549360513687)\n",
      " state (4)  A[0]:(0.590569019318) A[1]:(0.656218886375) A[2]:(5.79357147217e-05) A[3]:(0.531381130219)\n",
      " state (5)  A[0]:(0.179790124297) A[1]:(0.922417938709) A[2]:(-0.128401324153) A[3]:(0.496777415276)\n",
      " state (6)  A[0]:(-0.00023490190506) A[1]:(0.809987783432) A[2]:(1.07288360596e-06) A[3]:(0.655881285667)\n",
      " state (7)  A[0]:(0.635827183723) A[1]:(-0.243109017611) A[2]:(0.151415750384) A[3]:(0.913926541805)\n",
      " state (8)  A[0]:(0.656027913094) A[1]:(-0.000102162361145) A[2]:(0.729130446911) A[3]:(0.590193510056)\n",
      " state (9)  A[0]:(0.656071662903) A[1]:(0.809974193573) A[2]:(0.810048222542) A[3]:(-0.000611454190221)\n",
      " state (10)  A[0]:(0.728996276855) A[1]:(0.899972558022) A[2]:(-0.000288128852844) A[3]:(0.728720963001)\n",
      " state (11)  A[0]:(0.511055350304) A[1]:(0.876984000206) A[2]:(-0.546777904034) A[3]:(0.838028609753)\n",
      " state (12)  A[0]:(0.0656669661403) A[1]:(0.824230015278) A[2]:(-0.443678408861) A[3]:(0.788076758385)\n",
      " state (13)  A[0]:(-0.000338166952133) A[1]:(0.808915793896) A[2]:(0.900011897087) A[3]:(0.728790283203)\n",
      " state (14)  A[0]:(0.810037851334) A[1]:(0.90040242672) A[2]:(0.999999523163) A[3]:(0.809833049774)\n",
      " state (15)  A[0]:(0.991443932056) A[1]:(0.964493274689) A[2]:(1.0) A[3]:(0.899527609348)\n",
      "Episode 313000 finished after 0 timesteps with r=0.0. Running score: 0.93. Times trained:               6084. Times reached goal: 960.               Steps done: 3025392. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0436849549859.\n",
      " state (0)  A[0]:(0.531608343124) A[1]:(0.590212106705) A[2]:(0.590582787991) A[3]:(0.531595170498)\n",
      " state (1)  A[0]:(0.531527578831) A[1]:(-0.000158116221428) A[2]:(0.656197309494) A[3]:(0.590498566628)\n",
      " state (2)  A[0]:(0.590620875359) A[1]:(0.728887438774) A[2]:(0.59067261219) A[3]:(0.656181812286)\n",
      " state (3)  A[0]:(0.656147658825) A[1]:(-1.47372484207e-05) A[2]:(0.464288562536) A[3]:(0.549305737019)\n",
      " state (4)  A[0]:(0.590548574924) A[1]:(0.655986905098) A[2]:(3.6358833313e-05) A[3]:(0.531311750412)\n",
      " state (5)  A[0]:(0.180120810866) A[1]:(0.922385811806) A[2]:(-0.128583863378) A[3]:(0.496972233057)\n",
      " state (6)  A[0]:(0.000240057706833) A[1]:(0.809893846512) A[2]:(-0.00011134147644) A[3]:(0.655947268009)\n",
      " state (7)  A[0]:(0.635845124722) A[1]:(-0.243633687496) A[2]:(0.151783436537) A[3]:(0.913805246353)\n",
      " state (8)  A[0]:(0.655952095985) A[1]:(-0.000392243237002) A[2]:(0.728949308395) A[3]:(0.590198278427)\n",
      " state (9)  A[0]:(0.655786156654) A[1]:(0.809928774834) A[2]:(0.809880077839) A[3]:(-0.000129252672195)\n",
      " state (10)  A[0]:(0.72863483429) A[1]:(0.899899721146) A[2]:(-0.000661492231302) A[3]:(0.728933513165)\n",
      " state (11)  A[0]:(0.510461449623) A[1]:(0.876860380173) A[2]:(-0.547554373741) A[3]:(0.838153243065)\n",
      " state (12)  A[0]:(0.0647963732481) A[1]:(0.824039518833) A[2]:(-0.445273697376) A[3]:(0.788204669952)\n",
      " state (13)  A[0]:(-0.00129187037237) A[1]:(0.80869859457) A[2]:(0.899979293346) A[3]:(0.728903770447)\n",
      " state (14)  A[0]:(0.809750556946) A[1]:(0.900272071362) A[2]:(0.999999523163) A[3]:(0.809921622276)\n",
      " state (15)  A[0]:(0.991384208202) A[1]:(0.964382171631) A[2]:(1.0) A[3]:(0.89941495657)\n",
      "Episode 314000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6100. Times reached goal: 961.               Steps done: 3031492. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.043419287869.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5902,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.0001,  0.6560,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7288,  0.5902,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8099, -0.0002,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0002,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9004,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531193912029) A[1]:(0.590235054493) A[2]:(0.590389966965) A[3]:(0.531389832497)\n",
      " state (1)  A[0]:(0.531182587147) A[1]:(-1.87382102013e-06) A[2]:(0.655876636505) A[3]:(0.590395450592)\n",
      " state (2)  A[0]:(0.590316057205) A[1]:(0.728786110878) A[2]:(0.590134263039) A[3]:(0.656104385853)\n",
      " state (3)  A[0]:(0.655867218971) A[1]:(-0.000459399045212) A[2]:(0.463915467262) A[3]:(0.549378752708)\n",
      " state (4)  A[0]:(0.590233206749) A[1]:(0.655627429485) A[2]:(-0.000260353088379) A[3]:(0.531469166279)\n",
      " state (5)  A[0]:(0.179790362716) A[1]:(0.92231631279) A[2]:(-0.128919512033) A[3]:(0.497216492891)\n",
      " state (6)  A[0]:(-9.61720943451e-05) A[1]:(0.809856891632) A[2]:(-0.000291466712952) A[3]:(0.655952095985)\n",
      " state (7)  A[0]:(0.635354757309) A[1]:(-0.243415474892) A[2]:(0.152067139745) A[3]:(0.913670003414)\n",
      " state (8)  A[0]:(0.655878067017) A[1]:(-0.000494416744914) A[2]:(0.728587448597) A[3]:(0.590630412102)\n",
      " state (9)  A[0]:(0.655929982662) A[1]:(0.809839427471) A[2]:(0.8097666502) A[3]:(0.000476270884974)\n",
      " state (10)  A[0]:(0.728984594345) A[1]:(0.899912297726) A[2]:(-0.000617980898824) A[3]:(0.729076087475)\n",
      " state (11)  A[0]:(0.511417627335) A[1]:(0.876956582069) A[2]:(-0.548001050949) A[3]:(0.838266074657)\n",
      " state (12)  A[0]:(0.0663247704506) A[1]:(0.82424813509) A[2]:(-0.446472913027) A[3]:(0.788343966007)\n",
      " state (13)  A[0]:(0.000134110450745) A[1]:(0.80893611908) A[2]:(0.899960517883) A[3]:(0.729036033154)\n",
      " state (14)  A[0]:(0.810116291046) A[1]:(0.900354206562) A[2]:(0.999999523163) A[3]:(0.810020744801)\n",
      " state (15)  A[0]:(0.991352796555) A[1]:(0.964334845543) A[2]:(1.0) A[3]:(0.89933013916)\n",
      "Episode 315000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6099. Times reached goal: 954.               Steps done: 3037591. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.043155279544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531038403511) A[1]:(0.590633749962) A[2]:(0.590518116951) A[3]:(0.531464099884)\n",
      " state (1)  A[0]:(0.531078696251) A[1]:(0.00010346993804) A[2]:(0.656113505363) A[3]:(0.590482473373)\n",
      " state (2)  A[0]:(0.590266466141) A[1]:(0.729044675827) A[2]:(0.590558290482) A[3]:(0.656166255474)\n",
      " state (3)  A[0]:(0.65604364872) A[1]:(0.000182624906301) A[2]:(0.464322984219) A[3]:(0.549268066883)\n",
      " state (4)  A[0]:(0.590414226055) A[1]:(0.65622895956) A[2]:(0.00010073184967) A[3]:(0.531365394592)\n",
      " state (5)  A[0]:(0.179807141423) A[1]:(0.922422468662) A[2]:(-0.128618106246) A[3]:(0.497257053852)\n",
      " state (6)  A[0]:(-4.91738319397e-05) A[1]:(0.810044884682) A[2]:(0.000179529190063) A[3]:(0.656052529812)\n",
      " state (7)  A[0]:(0.635555446148) A[1]:(-0.242934346199) A[2]:(0.153366833925) A[3]:(0.913654386997)\n",
      " state (8)  A[0]:(0.65628516674) A[1]:(0.000101830810308) A[2]:(0.729146838188) A[3]:(0.590428709984)\n",
      " state (9)  A[0]:(0.656368494034) A[1]:(0.81007784605) A[2]:(0.810142040253) A[3]:(-0.000328227877617)\n",
      " state (10)  A[0]:(0.72933113575) A[1]:(0.900058805943) A[2]:(-2.89678573608e-05) A[3]:(0.728779554367)\n",
      " state (11)  A[0]:(0.511968255043) A[1]:(0.877145588398) A[2]:(-0.548373758793) A[3]:(0.838152289391)\n",
      " state (12)  A[0]:(0.0669556185603) A[1]:(0.824510037899) A[2]:(-0.447626411915) A[3]:(0.788230419159)\n",
      " state (13)  A[0]:(0.000787347380538) A[1]:(0.809207081795) A[2]:(0.900141179562) A[3]:(0.728925049305)\n",
      " state (14)  A[0]:(0.810493171215) A[1]:(0.900480985641) A[2]:(0.999999523163) A[3]:(0.81000828743)\n",
      " state (15)  A[0]:(0.991324543953) A[1]:(0.964301645756) A[2]:(1.0) A[3]:(0.899170637131)\n",
      "Episode 316000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6110. Times reached goal: 949.               Steps done: 3043701. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0428924046865.\n",
      " state (0)  A[0]:(0.531158208847) A[1]:(0.59021127224) A[2]:(0.590428471565) A[3]:(0.531469285488)\n",
      " state (1)  A[0]:(0.531174123287) A[1]:(-1.29565596581e-05) A[2]:(0.656059145927) A[3]:(0.59049487114)\n",
      " state (2)  A[0]:(0.590353250504) A[1]:(0.729052484035) A[2]:(0.590428352356) A[3]:(0.656228423119)\n",
      " state (3)  A[0]:(0.656011939049) A[1]:(0.000400878459914) A[2]:(0.464215815067) A[3]:(0.549344480038)\n",
      " state (4)  A[0]:(0.590372145176) A[1]:(0.656210184097) A[2]:(5.31673431396e-05) A[3]:(0.531519412994)\n",
      " state (5)  A[0]:(0.179904535413) A[1]:(0.922410130501) A[2]:(-0.128709316254) A[3]:(0.497556090355)\n",
      " state (6)  A[0]:(8.62777233124e-05) A[1]:(0.810007810593) A[2]:(0.0002601146698) A[3]:(0.656125545502)\n",
      " state (7)  A[0]:(0.635340511799) A[1]:(-0.24300840497) A[2]:(0.154127329588) A[3]:(0.913512706757)\n",
      " state (8)  A[0]:(0.656039237976) A[1]:(0.000145383179188) A[2]:(0.72924387455) A[3]:(0.590193808079)\n",
      " state (9)  A[0]:(0.656061410904) A[1]:(0.8100451231) A[2]:(0.810080707073) A[3]:(6.43134117126e-05)\n",
      " state (10)  A[0]:(0.729051232338) A[1]:(0.90000975132) A[2]:(-0.000226020812988) A[3]:(0.72902148962)\n",
      " state (11)  A[0]:(0.511586248875) A[1]:(0.877076566219) A[2]:(-0.549050927162) A[3]:(0.838288724422)\n",
      " state (12)  A[0]:(0.0663617104292) A[1]:(0.824408113956) A[2]:(-0.449200719595) A[3]:(0.788344740868)\n",
      " state (13)  A[0]:(-0.000167459249496) A[1]:(0.809076547623) A[2]:(0.900035023689) A[3]:(0.728970885277)\n",
      " state (14)  A[0]:(0.809970498085) A[1]:(0.900390684605) A[2]:(0.999999523163) A[3]:(0.81000995636)\n",
      " state (15)  A[0]:(0.99124199152) A[1]:(0.964206039906) A[2]:(1.0) A[3]:(0.899005055428)\n",
      "Episode 317000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6103. Times reached goal: 949.               Steps done: 3049804. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0426314295165.\n",
      " state (0)  A[0]:(0.531988978386) A[1]:(0.590700984001) A[2]:(0.590479850769) A[3]:(0.530470311642)\n",
      " state (1)  A[0]:(0.532012343407) A[1]:(0.000167138874531) A[2]:(0.656026721001) A[3]:(0.589793264866)\n",
      " state (2)  A[0]:(0.59114086628) A[1]:(0.72905254364) A[2]:(0.590371668339) A[3]:(0.655708432198)\n",
      " state (3)  A[0]:(0.656863570213) A[1]:(8.81031155586e-05) A[2]:(0.464164942503) A[3]:(0.549057483673)\n",
      " state (4)  A[0]:(0.591455340385) A[1]:(0.656346082687) A[2]:(-0.000301241874695) A[3]:(0.531522333622)\n",
      " state (5)  A[0]:(0.181498095393) A[1]:(0.922420501709) A[2]:(-0.12930200994) A[3]:(0.497731208801)\n",
      " state (6)  A[0]:(0.00166064349469) A[1]:(0.809950351715) A[2]:(-0.000225186347961) A[3]:(0.656362712383)\n",
      " state (7)  A[0]:(0.63598793745) A[1]:(-0.243198245764) A[2]:(0.154404178262) A[3]:(0.91356921196)\n",
      " state (8)  A[0]:(0.656615674496) A[1]:(0.000151224434376) A[2]:(0.729134976864) A[3]:(0.59049654007)\n",
      " state (9)  A[0]:(0.656482279301) A[1]:(0.810093939304) A[2]:(0.810060381889) A[3]:(-0.000138789415359)\n",
      " state (10)  A[0]:(0.729241728783) A[1]:(0.900043904781) A[2]:(-0.000271201133728) A[3]:(0.728967368603)\n",
      " state (11)  A[0]:(0.511775672436) A[1]:(0.87713688612) A[2]:(-0.549731731415) A[3]:(0.838329792023)\n",
      " state (12)  A[0]:(0.0663355737925) A[1]:(0.824523746967) A[2]:(-0.450904637575) A[3]:(0.788417875767)\n",
      " state (13)  A[0]:(-0.000639885547571) A[1]:(0.809226214886) A[2]:(0.899846434593) A[3]:(0.729012846947)\n",
      " state (14)  A[0]:(0.80970788002) A[1]:(0.900483191013) A[2]:(0.999999523163) A[3]:(0.809995472431)\n",
      " state (15)  A[0]:(0.991181850433) A[1]:(0.964194118977) A[2]:(1.0) A[3]:(0.89881914854)\n",
      "Episode 318000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6096. Times reached goal: 959.               Steps done: 3055900. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0423723388329.\n",
      " state (0)  A[0]:(0.531380057335) A[1]:(0.590417504311) A[2]:(0.590467810631) A[3]:(0.531452357769)\n",
      " state (1)  A[0]:(0.53130531311) A[1]:(0.000212270766497) A[2]:(0.65608215332) A[3]:(0.590457320213)\n",
      " state (2)  A[0]:(0.590302288532) A[1]:(0.729020476341) A[2]:(0.591056227684) A[3]:(0.655971288681)\n",
      " state (3)  A[0]:(0.655818641186) A[1]:(-0.00765589950606) A[2]:(0.465658634901) A[3]:(0.54830634594)\n",
      " state (4)  A[0]:(0.590193033218) A[1]:(0.656151890755) A[2]:(5.9962272644e-05) A[3]:(0.531131803989)\n",
      " state (5)  A[0]:(0.179406628013) A[1]:(0.922420799732) A[2]:(-0.129265084863) A[3]:(0.497342675924)\n",
      " state (6)  A[0]:(-0.000746995094232) A[1]:(0.809989631176) A[2]:(-0.000171065330505) A[3]:(0.655754327774)\n",
      " state (7)  A[0]:(0.634493947029) A[1]:(-0.243160679936) A[2]:(0.154991090298) A[3]:(0.913219153881)\n",
      " state (8)  A[0]:(0.655854225159) A[1]:(-0.000260781496763) A[2]:(0.728970348835) A[3]:(0.590008020401)\n",
      " state (9)  A[0]:(0.656124591827) A[1]:(0.809906959534) A[2]:(0.80995541811) A[3]:(-0.000524416507687)\n",
      " state (10)  A[0]:(0.729122042656) A[1]:(0.899960041046) A[2]:(-0.000430941552622) A[3]:(0.728647828102)\n",
      " state (11)  A[0]:(0.511875331402) A[1]:(0.877059996128) A[2]:(-0.550393462181) A[3]:(0.838108837605)\n",
      " state (12)  A[0]:(0.0666926577687) A[1]:(0.824439406395) A[2]:(-0.452277332544) A[3]:(0.788110017776)\n",
      " state (13)  A[0]:(-9.80496406555e-05) A[1]:(0.809153795242) A[2]:(0.899991512299) A[3]:(0.728600502014)\n",
      " state (14)  A[0]:(0.809991776943) A[1]:(0.900450050831) A[2]:(0.999999582767) A[3]:(0.809705078602)\n",
      " state (15)  A[0]:(0.991141676903) A[1]:(0.964110851288) A[2]:(1.0) A[3]:(0.89846342802)\n",
      "Episode 319000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6134. Times reached goal: 959.               Steps done: 3062034. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0421132224289.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5906,  0.5906,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.6561,  0.0007,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6563, -0.0004,  0.7287,  0.5911]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.8101,  0.8100,  0.0003]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8094,  0.9001,  0.7289]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9005,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531237483025) A[1]:(0.590459465981) A[2]:(0.590516507626) A[3]:(0.531338095665)\n",
      " state (1)  A[0]:(0.531490266323) A[1]:(0.000314101576805) A[2]:(0.656079888344) A[3]:(0.590606093407)\n",
      " state (2)  A[0]:(0.590368032455) A[1]:(0.728922247887) A[2]:(0.591570854187) A[3]:(0.656028091908)\n",
      " state (3)  A[0]:(0.655754327774) A[1]:(-0.0178071949631) A[2]:(0.468299895525) A[3]:(0.547541677952)\n",
      " state (4)  A[0]:(0.590486168861) A[1]:(0.655997574329) A[2]:(7.35521316528e-05) A[3]:(0.531319081783)\n",
      " state (5)  A[0]:(0.179607376456) A[1]:(0.9225435853) A[2]:(-0.131107985973) A[3]:(0.49766305089)\n",
      " state (6)  A[0]:(-0.000428408355219) A[1]:(0.809913694859) A[2]:(-0.00166404095944) A[3]:(0.655960083008)\n",
      " state (7)  A[0]:(0.634749591351) A[1]:(-0.243532717228) A[2]:(0.15533567965) A[3]:(0.913195252419)\n",
      " state (8)  A[0]:(0.655776977539) A[1]:(-0.000131592154503) A[2]:(0.728813171387) A[3]:(0.59038066864)\n",
      " state (9)  A[0]:(0.655987381935) A[1]:(0.809973955154) A[2]:(0.809872150421) A[3]:(-1.74194574356e-05)\n",
      " state (10)  A[0]:(0.728886663914) A[1]:(0.899951219559) A[2]:(-0.000586271227803) A[3]:(0.728971719742)\n",
      " state (11)  A[0]:(0.511400222778) A[1]:(0.8769967556) A[2]:(-0.55080139637) A[3]:(0.838375329971)\n",
      " state (12)  A[0]:(0.0659712925553) A[1]:(0.82431614399) A[2]:(-0.453100025654) A[3]:(0.788436055183)\n",
      " state (13)  A[0]:(-0.000777631823439) A[1]:(0.809006273746) A[2]:(0.899992644787) A[3]:(0.728913009167)\n",
      " state (14)  A[0]:(0.809899568558) A[1]:(0.9003662467) A[2]:(0.999999582767) A[3]:(0.809810161591)\n",
      " state (15)  A[0]:(0.991118490696) A[1]:(0.964047431946) A[2]:(1.0) A[3]:(0.898346722126)\n",
      "Episode 320000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6069. Times reached goal: 943.               Steps done: 3068103. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0418584112885.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531640052795) A[1]:(0.589882969856) A[2]:(0.590597391129) A[3]:(0.532052516937)\n",
      " state (1)  A[0]:(0.531754136086) A[1]:(-0.000363368511898) A[2]:(0.656399190426) A[3]:(0.59082955122)\n",
      " state (2)  A[0]:(0.590568304062) A[1]:(0.728979110718) A[2]:(0.591632246971) A[3]:(0.656326830387)\n",
      " state (3)  A[0]:(0.655530750751) A[1]:(-0.0163479074836) A[2]:(0.468675345182) A[3]:(0.547982096672)\n",
      " state (4)  A[0]:(0.590064287186) A[1]:(0.6559227705) A[2]:(0.000332593917847) A[3]:(0.531962633133)\n",
      " state (5)  A[0]:(0.178990796208) A[1]:(0.922576546669) A[2]:(-0.131023049355) A[3]:(0.49860727787)\n",
      " state (6)  A[0]:(-0.000679641845636) A[1]:(0.809945344925) A[2]:(-0.000679254415445) A[3]:(0.65614771843)\n",
      " state (7)  A[0]:(0.634927749634) A[1]:(-0.243212074041) A[2]:(0.157375842333) A[3]:(0.912827014923)\n",
      " state (8)  A[0]:(0.655935883522) A[1]:(0.000102650374174) A[2]:(0.728939175606) A[3]:(0.590070843697)\n",
      " state (9)  A[0]:(0.655859470367) A[1]:(0.810039639473) A[2]:(0.80988240242) A[3]:(-1.30534172058e-05)\n",
      " state (10)  A[0]:(0.728824138641) A[1]:(0.90000295639) A[2]:(-0.000455737084849) A[3]:(0.728965878487)\n",
      " state (11)  A[0]:(0.51151227951) A[1]:(0.877065777779) A[2]:(-0.550978779793) A[3]:(0.838402450085)\n",
      " state (12)  A[0]:(0.0662218108773) A[1]:(0.824400961399) A[2]:(-0.453741252422) A[3]:(0.7884298563)\n",
      " state (13)  A[0]:(-0.000748157384805) A[1]:(0.809038937092) A[2]:(0.899891018867) A[3]:(0.728847026825)\n",
      " state (14)  A[0]:(0.809757530689) A[1]:(0.900327980518) A[2]:(0.999999582767) A[3]:(0.809819459915)\n",
      " state (15)  A[0]:(0.99108427763) A[1]:(0.963986456394) A[2]:(1.0) A[3]:(0.898348510265)\n",
      "Episode 321000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6091. Times reached goal: 953.               Steps done: 3074194. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0416042266107.\n",
      " state (0)  A[0]:(0.531338572502) A[1]:(0.590505540371) A[2]:(0.590440869331) A[3]:(0.531214118004)\n",
      " state (1)  A[0]:(0.531626105309) A[1]:(8.26939940453e-05) A[2]:(0.656039774418) A[3]:(0.590236783028)\n",
      " state (2)  A[0]:(0.590342998505) A[1]:(0.72899377346) A[2]:(0.590736865997) A[3]:(0.656011581421)\n",
      " state (3)  A[0]:(0.656048476696) A[1]:(-0.0109276073053) A[2]:(0.468355089426) A[3]:(0.547518253326)\n",
      " state (4)  A[0]:(0.590738415718) A[1]:(0.656128525734) A[2]:(0.000215172767639) A[3]:(0.531166315079)\n",
      " state (5)  A[0]:(0.17953273654) A[1]:(0.922712564468) A[2]:(-0.13190998137) A[3]:(0.498140245676)\n",
      " state (6)  A[0]:(8.86619091034e-05) A[1]:(0.810026347637) A[2]:(-0.000759005371947) A[3]:(0.655951976776)\n",
      " state (7)  A[0]:(0.635413765907) A[1]:(-0.243240475655) A[2]:(0.158858463168) A[3]:(0.912678778172)\n",
      " state (8)  A[0]:(0.656222462654) A[1]:(1.11386179924e-05) A[2]:(0.729076504707) A[3]:(0.590379714966)\n",
      " state (9)  A[0]:(0.65616106987) A[1]:(0.809968113899) A[2]:(0.810075938702) A[3]:(-0.000118732452393)\n",
      " state (10)  A[0]:(0.729189276695) A[1]:(0.900009155273) A[2]:(-3.57627868652e-07) A[3]:(0.72890740633)\n",
      " state (11)  A[0]:(0.512221932411) A[1]:(0.877097547054) A[2]:(-0.551063656807) A[3]:(0.838456273079)\n",
      " state (12)  A[0]:(0.0671376287937) A[1]:(0.824441611767) A[2]:(-0.454182267189) A[3]:(0.788504719734)\n",
      " state (13)  A[0]:(5.4270029068e-05) A[1]:(0.809042751789) A[2]:(0.899991571903) A[3]:(0.728947401047)\n",
      " state (14)  A[0]:(0.810040891171) A[1]:(0.900290071964) A[2]:(0.999999582767) A[3]:(0.810009598732)\n",
      " state (15)  A[0]:(0.991072416306) A[1]:(0.963919878006) A[2]:(1.0) A[3]:(0.898454427719)\n",
      "Episode 322000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6073. Times reached goal: 954.               Steps done: 3080267. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0413523298013.\n",
      " state (0)  A[0]:(0.531611382961) A[1]:(0.590474069118) A[2]:(0.590418159962) A[3]:(0.531623244286)\n",
      " state (1)  A[0]:(0.531684875488) A[1]:(0.00023090839386) A[2]:(0.656047999859) A[3]:(0.590457558632)\n",
      " state (2)  A[0]:(0.590359807014) A[1]:(0.728996276855) A[2]:(0.591030716896) A[3]:(0.656051397324)\n",
      " state (3)  A[0]:(0.655878305435) A[1]:(-0.0134368799627) A[2]:(0.469331651926) A[3]:(0.547620654106)\n",
      " state (4)  A[0]:(0.590661108494) A[1]:(0.656022548676) A[2]:(0.000190615653992) A[3]:(0.531586170197)\n",
      " state (5)  A[0]:(0.179219573736) A[1]:(0.922758579254) A[2]:(-0.132487177849) A[3]:(0.498560070992)\n",
      " state (6)  A[0]:(-0.000283569097519) A[1]:(0.809958994389) A[2]:(-0.000627875269856) A[3]:(0.655983448029)\n",
      " state (7)  A[0]:(0.635273396969) A[1]:(-0.243542596698) A[2]:(0.16016125679) A[3]:(0.912498772144)\n",
      " state (8)  A[0]:(0.656227707863) A[1]:(-0.000186622142792) A[2]:(0.729009628296) A[3]:(0.590662002563)\n",
      " state (9)  A[0]:(0.656149804592) A[1]:(0.809942722321) A[2]:(0.810005903244) A[3]:(0.000226020812988)\n",
      " state (10)  A[0]:(0.729121387005) A[1]:(0.899989306927) A[2]:(-5.76972961426e-05) A[3]:(0.728979825974)\n",
      " state (11)  A[0]:(0.512172341347) A[1]:(0.877075135708) A[2]:(-0.551388919353) A[3]:(0.838532865047)\n",
      " state (12)  A[0]:(0.0671255588531) A[1]:(0.824442028999) A[2]:(-0.454899400473) A[3]:(0.788590729237)\n",
      " state (13)  A[0]:(5.81443309784e-05) A[1]:(0.809101700783) A[2]:(0.900029659271) A[3]:(0.728998064995)\n",
      " state (14)  A[0]:(0.81007963419) A[1]:(0.900374233723) A[2]:(0.999999582767) A[3]:(0.809998095036)\n",
      " state (15)  A[0]:(0.991047441959) A[1]:(0.963939905167) A[2]:(1.0) A[3]:(0.898319482803)\n",
      "Episode 323000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6111. Times reached goal: 950.               Steps done: 3086378. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0411003962808.\n",
      " state (0)  A[0]:(0.531759977341) A[1]:(0.590419769287) A[2]:(0.590461492538) A[3]:(0.531387329102)\n",
      " state (1)  A[0]:(0.531974077225) A[1]:(0.000236704945564) A[2]:(0.656162202358) A[3]:(0.590396165848)\n",
      " state (2)  A[0]:(0.590582549572) A[1]:(0.729076325893) A[2]:(0.591094136238) A[3]:(0.656072974205)\n",
      " state (3)  A[0]:(0.655933499336) A[1]:(-0.0113852322102) A[2]:(0.46989902854) A[3]:(0.547501683235)\n",
      " state (4)  A[0]:(0.590651392937) A[1]:(0.656340181828) A[2]:(0.00017523765564) A[3]:(0.53138756752)\n",
      " state (5)  A[0]:(0.178934395313) A[1]:(0.922920167446) A[2]:(-0.133472010493) A[3]:(0.498529046774)\n",
      " state (6)  A[0]:(-0.000279009342194) A[1]:(0.80998313427) A[2]:(-0.00109195662662) A[3]:(0.65586745739)\n",
      " state (7)  A[0]:(0.635313391685) A[1]:(-0.243893310428) A[2]:(0.161039695144) A[3]:(0.912227153778)\n",
      " state (8)  A[0]:(0.655977129936) A[1]:(-9.76026058197e-06) A[2]:(0.729019403458) A[3]:(0.590005576611)\n",
      " state (9)  A[0]:(0.655960917473) A[1]:(0.810019731522) A[2]:(0.809957146645) A[3]:(-0.000325411558151)\n",
      " state (10)  A[0]:(0.728903532028) A[1]:(0.900012254715) A[2]:(-0.000342011451721) A[3]:(0.728804826736)\n",
      " state (11)  A[0]:(0.511733651161) A[1]:(0.877092003822) A[2]:(-0.551946222782) A[3]:(0.838434398174)\n",
      " state (12)  A[0]:(0.066397048533) A[1]:(0.824482321739) A[2]:(-0.455883175135) A[3]:(0.788402497768)\n",
      " state (13)  A[0]:(-0.000717252376489) A[1]:(0.80919033289) A[2]:(0.900034606457) A[3]:(0.728697359562)\n",
      " state (14)  A[0]:(0.80988997221) A[1]:(0.900459766388) A[2]:(0.999999582767) A[3]:(0.809808492661)\n",
      " state (15)  A[0]:(0.991011977196) A[1]:(0.963951826096) A[2]:(1.0) A[3]:(0.898144304752)\n",
      "Episode 324000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6102. Times reached goal: 959.               Steps done: 3092480. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0408503652831.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5905,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.6561,  0.0002,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.0001,  0.7291,  0.5905]], device='cuda:0')\n",
      "On state=8, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6562, -0.0000,  0.7290,  0.5906]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6563,  0.8100,  0.8100,  0.0002]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  0.8090,  0.9000,  0.7291]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.531455755234) A[1]:(0.590501189232) A[2]:(0.590464115143) A[3]:(0.531415462494)\n",
      " state (1)  A[0]:(0.531695961952) A[1]:(9.61944460869e-05) A[2]:(0.65613090992) A[3]:(0.590335130692)\n",
      " state (2)  A[0]:(0.590415418148) A[1]:(0.729073047638) A[2]:(0.59064912796) A[3]:(0.65616941452)\n",
      " state (3)  A[0]:(0.656002700329) A[1]:(-0.00551481172442) A[2]:(0.469383984804) A[3]:(0.547969996929)\n",
      " state (4)  A[0]:(0.590789556503) A[1]:(0.656165242195) A[2]:(0.000162839889526) A[3]:(0.531536817551)\n",
      " state (5)  A[0]:(0.179112941027) A[1]:(0.922933340073) A[2]:(-0.133673027158) A[3]:(0.498917639256)\n",
      " state (6)  A[0]:(2.00569629669e-05) A[1]:(0.809995293617) A[2]:(-0.00037324425648) A[3]:(0.65598988533)\n",
      " state (7)  A[0]:(0.63556009531) A[1]:(-0.24370470643) A[2]:(0.162859871984) A[3]:(0.912085771561)\n",
      " state (8)  A[0]:(0.656378865242) A[1]:(1.8797814846e-05) A[2]:(0.729033589363) A[3]:(0.590664148331)\n",
      " state (9)  A[0]:(0.656318902969) A[1]:(0.810034990311) A[2]:(0.810023486614) A[3]:(0.000134542584419)\n",
      " state (10)  A[0]:(0.729279279709) A[1]:(0.900025069714) A[2]:(-3.17096710205e-05) A[3]:(0.729034721851)\n",
      " state (11)  A[0]:(0.512539029121) A[1]:(0.877083420753) A[2]:(-0.55207657814) A[3]:(0.838689386845)\n",
      " state (12)  A[0]:(0.0675796121359) A[1]:(0.824413776398) A[2]:(-0.456498205662) A[3]:(0.788745045662)\n",
      " state (13)  A[0]:(0.000347256660461) A[1]:(0.809016406536) A[2]:(0.899973213673) A[3]:(0.729078888893)\n",
      " state (14)  A[0]:(0.810209989548) A[1]:(0.900296390057) A[2]:(0.999999582767) A[3]:(0.810075640678)\n",
      " state (15)  A[0]:(0.991005480289) A[1]:(0.963837921619) A[2]:(1.0) A[3]:(0.898226737976)\n",
      "Episode 325000 finished after 0 timesteps with r=0.0. Running score: 0.97. Times trained:               6047. Times reached goal: 955.               Steps done: 3098527. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0406040884925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532014429569) A[1]:(0.590880274773) A[2]:(0.590498566628) A[3]:(0.531889796257)\n",
      " state (1)  A[0]:(0.532324194908) A[1]:(0.000398401141865) A[2]:(0.656137824059) A[3]:(0.590722680092)\n",
      " state (2)  A[0]:(0.590840399265) A[1]:(0.728875696659) A[2]:(0.591499090195) A[3]:(0.656268000603)\n",
      " state (3)  A[0]:(0.656333088875) A[1]:(-0.0226394850761) A[2]:(0.472628355026) A[3]:(0.546718716621)\n",
      " state (4)  A[0]:(0.591588616371) A[1]:(0.656239509583) A[2]:(-9.09566879272e-05) A[3]:(0.531930088997)\n",
      " state (5)  A[0]:(0.179936915636) A[1]:(0.923136949539) A[2]:(-0.135317400098) A[3]:(0.499729931355)\n",
      " state (6)  A[0]:(0.00141307618469) A[1]:(0.809980392456) A[2]:(-0.00150728109293) A[3]:(0.656709849834)\n",
      " state (7)  A[0]:(0.636533856392) A[1]:(-0.244056656957) A[2]:(0.163369521499) A[3]:(0.912092268467)\n",
      " state (8)  A[0]:(0.65690612793) A[1]:(5.37969172001e-05) A[2]:(0.728929638863) A[3]:(0.590848147869)\n",
      " state (9)  A[0]:(0.656916558743) A[1]:(0.809964478016) A[2]:(0.809957146645) A[3]:(0.000272244215012)\n",
      " state (10)  A[0]:(0.729773879051) A[1]:(0.899981796741) A[2]:(-0.000450849503977) A[3]:(0.729177832603)\n",
      " state (11)  A[0]:(0.513325691223) A[1]:(0.877015054226) A[2]:(-0.552793979645) A[3]:(0.838851094246)\n",
      " state (12)  A[0]:(0.0685763582587) A[1]:(0.824289143085) A[2]:(-0.457702338696) A[3]:(0.788939416409)\n",
      " state (13)  A[0]:(0.00121933163609) A[1]:(0.808843374252) A[2]:(0.899877905846) A[3]:(0.729270219803)\n",
      " state (14)  A[0]:(0.810458064079) A[1]:(0.900175571442) A[2]:(0.999999582767) A[3]:(0.810206949711)\n",
      " state (15)  A[0]:(0.990988016129) A[1]:(0.963748812675) A[2]:(1.0) A[3]:(0.898211956024)\n",
      "Episode 326000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6111. Times reached goal: 956.               Steps done: 3104638. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0403567135318.\n",
      " state (0)  A[0]:(0.531564950943) A[1]:(0.590641617775) A[2]:(0.590699315071) A[3]:(0.531157970428)\n",
      " state (1)  A[0]:(0.531944215298) A[1]:(-0.000103555619717) A[2]:(0.655959010124) A[3]:(0.590572595596)\n",
      " state (2)  A[0]:(0.590600132942) A[1]:(0.729211807251) A[2]:(0.590903818607) A[3]:(0.656282603741)\n",
      " state (3)  A[0]:(0.655876636505) A[1]:(-0.0152350720018) A[2]:(0.471887737513) A[3]:(0.54688513279)\n",
      " state (4)  A[0]:(0.590843558311) A[1]:(0.655844092369) A[2]:(-9.65595245361e-06) A[3]:(0.531212449074)\n",
      " state (5)  A[0]:(0.17868052423) A[1]:(0.923082053661) A[2]:(-0.135568752885) A[3]:(0.498896002769)\n",
      " state (6)  A[0]:(-0.000515729130711) A[1]:(0.809965312481) A[2]:(-0.00113916350529) A[3]:(0.655774831772)\n",
      " state (7)  A[0]:(0.635233998299) A[1]:(-0.244215756655) A[2]:(0.164705038071) A[3]:(0.911744475365)\n",
      " state (8)  A[0]:(0.656006932259) A[1]:(-0.0003834888048) A[2]:(0.728827416897) A[3]:(0.590933322906)\n",
      " state (9)  A[0]:(0.656006455421) A[1]:(0.809895157814) A[2]:(0.809818506241) A[3]:(0.000329166650772)\n",
      " state (10)  A[0]:(0.728944003582) A[1]:(0.899887025356) A[2]:(-0.000884294277057) A[3]:(0.729098677635)\n",
      " state (11)  A[0]:(0.511964559555) A[1]:(0.876844644547) A[2]:(-0.553451418877) A[3]:(0.838801622391)\n",
      " state (12)  A[0]:(0.0666746795177) A[1]:(0.824042081833) A[2]:(-0.458840340376) A[3]:(0.78883266449)\n",
      " state (13)  A[0]:(-0.000620663107838) A[1]:(0.808646440506) A[2]:(0.899822413921) A[3]:(0.729089200497)\n",
      " state (14)  A[0]:(0.809988558292) A[1]:(0.900174200535) A[2]:(0.999999582767) A[3]:(0.810098052025)\n",
      " state (15)  A[0]:(0.990944385529) A[1]:(0.963765501976) A[2]:(1.0) A[3]:(0.898077368736)\n",
      "Episode 327000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6131. Times reached goal: 963.               Steps done: 3110769. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0401100434609.\n",
      " state (0)  A[0]:(0.531362295151) A[1]:(0.590549349785) A[2]:(0.590421497822) A[3]:(0.531294941902)\n",
      " state (1)  A[0]:(0.531668424606) A[1]:(0.000204883515835) A[2]:(0.656282067299) A[3]:(0.590378940105)\n",
      " state (2)  A[0]:(0.590204715729) A[1]:(0.728832483292) A[2]:(0.591170191765) A[3]:(0.656113743782)\n",
      " state (3)  A[0]:(0.655839204788) A[1]:(-0.0111858723685) A[2]:(0.472230434418) A[3]:(0.547003030777)\n",
      " state (4)  A[0]:(0.590854644775) A[1]:(0.655837774277) A[2]:(0.000314712524414) A[3]:(0.531221747398)\n",
      " state (5)  A[0]:(0.178494259715) A[1]:(0.92318636179) A[2]:(-0.135825023055) A[3]:(0.499223738909)\n",
      " state (6)  A[0]:(-0.000103235244751) A[1]:(0.809911131859) A[2]:(-0.000548005045857) A[3]:(0.655856013298)\n",
      " state (7)  A[0]:(0.63541162014) A[1]:(-0.244453981519) A[2]:(0.166487500072) A[3]:(0.911376178265)\n",
      " state (8)  A[0]:(0.655901432037) A[1]:(-8.34837555885e-05) A[2]:(0.728899478912) A[3]:(0.590376079082)\n",
      " state (9)  A[0]:(0.655904531479) A[1]:(0.809992730618) A[2]:(0.809938132763) A[3]:(-0.000370740861399)\n",
      " state (10)  A[0]:(0.728954434395) A[1]:(0.899992585182) A[2]:(-7.43865966797e-05) A[3]:(0.728600144386)\n",
      " state (11)  A[0]:(0.512258708477) A[1]:(0.877021968365) A[2]:(-0.553047299385) A[3]:(0.838512539864)\n",
      " state (12)  A[0]:(0.0672881379724) A[1]:(0.824323117733) A[2]:(-0.458658963442) A[3]:(0.788474082947)\n",
      " state (13)  A[0]:(-7.18235969543e-05) A[1]:(0.808936178684) A[2]:(0.900054514408) A[3]:(0.728646278381)\n",
      " state (14)  A[0]:(0.810044884682) A[1]:(0.90029758215) A[2]:(0.999999582767) A[3]:(0.809818446636)\n",
      " state (15)  A[0]:(0.9909106493) A[1]:(0.963759541512) A[2]:(1.0) A[3]:(0.897863745689)\n",
      "Episode 328000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6112. Times reached goal: 961.               Steps done: 3116881. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0398656385376.\n",
      " state (0)  A[0]:(0.532378137112) A[1]:(0.590507566929) A[2]:(0.590830564499) A[3]:(0.531756401062)\n",
      " state (1)  A[0]:(0.532436013222) A[1]:(0.000620175094809) A[2]:(0.656419634819) A[3]:(0.590789139271)\n",
      " state (2)  A[0]:(0.590689539909) A[1]:(0.729125738144) A[2]:(0.591597139835) A[3]:(0.656452775002)\n",
      " state (3)  A[0]:(0.655513644218) A[1]:(-0.0170493405312) A[2]:(0.474340617657) A[3]:(0.546995997429)\n",
      " state (4)  A[0]:(0.59021961689) A[1]:(0.656760215759) A[2]:(0.000646352651529) A[3]:(0.532081484795)\n",
      " state (5)  A[0]:(0.176975309849) A[1]:(0.923530340195) A[2]:(-0.136675819755) A[3]:(0.500363051891)\n",
      " state (6)  A[0]:(-0.000753670814447) A[1]:(0.810119330883) A[2]:(-0.000747442129068) A[3]:(0.656612277031)\n",
      " state (7)  A[0]:(0.63579583168) A[1]:(-0.244707256556) A[2]:(0.168051913381) A[3]:(0.911237299442)\n",
      " state (8)  A[0]:(0.655550837517) A[1]:(-0.000131141394377) A[2]:(0.729361534119) A[3]:(0.589433193207)\n",
      " state (9)  A[0]:(0.655263900757) A[1]:(0.809922218323) A[2]:(0.810100436211) A[3]:(-0.00157070031855)\n",
      " state (10)  A[0]:(0.728369772434) A[1]:(0.899972617626) A[2]:(-0.00119566859212) A[3]:(0.728848457336)\n",
      " state (11)  A[0]:(0.511018097401) A[1]:(0.877016723156) A[2]:(-0.55481082201) A[3]:(0.838940799236)\n",
      " state (12)  A[0]:(0.0650044381618) A[1]:(0.824375092983) A[2]:(-0.461135745049) A[3]:(0.789022386074)\n",
      " state (13)  A[0]:(-0.00264500966296) A[1]:(0.809142827988) A[2]:(0.89985626936) A[3]:(0.729229688644)\n",
      " state (14)  A[0]:(0.80927503109) A[1]:(0.900555670261) A[2]:(0.999999582767) A[3]:(0.810192167759)\n",
      " state (15)  A[0]:(0.990836441517) A[1]:(0.963870882988) A[2]:(1.0) A[3]:(0.897908210754)\n",
      "Episode 329000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6062. Times reached goal: 955.               Steps done: 3122943. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.039624704047.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5905,  0.5904,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6556,  0.0016,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6547, -0.0002,  0.7288,  0.5905]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6553,  0.8100,  0.8099,  0.0004]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0011,  0.8090,  0.9001,  0.7287]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9004,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531243383884) A[1]:(0.590455770493) A[2]:(0.590553104877) A[3]:(0.531177580357)\n",
      " state (1)  A[0]:(0.531817674637) A[1]:(0.00155608973) A[2]:(0.656379461288) A[3]:(0.590791583061)\n",
      " state (2)  A[0]:(0.590069532394) A[1]:(0.729012191296) A[2]:(0.59236562252) A[3]:(0.65634137392)\n",
      " state (3)  A[0]:(0.655517697334) A[1]:(-0.0238723773509) A[2]:(0.47677847743) A[3]:(0.545986294746)\n",
      " state (4)  A[0]:(0.590667963028) A[1]:(0.655600190163) A[2]:(0.00143849744927) A[3]:(0.531457066536)\n",
      " state (5)  A[0]:(0.17720285058) A[1]:(0.923489570618) A[2]:(-0.137494131923) A[3]:(0.500004589558)\n",
      " state (6)  A[0]:(-0.000929921574425) A[1]:(0.809806168079) A[2]:(-0.00116252852604) A[3]:(0.65639436245)\n",
      " state (7)  A[0]:(0.634882450104) A[1]:(-0.245558917522) A[2]:(0.168813019991) A[3]:(0.911055266857)\n",
      " state (8)  A[0]:(0.654994249344) A[1]:(-6.03497028351e-07) A[2]:(0.728837609291) A[3]:(0.590548217297)\n",
      " state (9)  A[0]:(0.655581712723) A[1]:(0.810087621212) A[2]:(0.809919595718) A[3]:(0.000446677178843)\n",
      " state (10)  A[0]:(0.728781938553) A[1]:(0.900040745735) A[2]:(-0.000234127044678) A[3]:(0.728995740414)\n",
      " state (11)  A[0]:(0.511959373951) A[1]:(0.877058148384) A[2]:(-0.553975582123) A[3]:(0.838788866997)\n",
      " state (12)  A[0]:(0.0667117387056) A[1]:(0.82438069582) A[2]:(-0.460506469011) A[3]:(0.788687348366)\n",
      " state (13)  A[0]:(-0.000662922742777) A[1]:(0.809064745903) A[2]:(0.900091588497) A[3]:(0.728726863861)\n",
      " state (14)  A[0]:(0.810029268265) A[1]:(0.900452375412) A[2]:(0.999999582767) A[3]:(0.809855818748)\n",
      " state (15)  A[0]:(0.990857958794) A[1]:(0.963781177998) A[2]:(1.0) A[3]:(0.897688269615)\n",
      "Episode 330000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6157. Times reached goal: 969.               Steps done: 3129100. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0393814842646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532793045044) A[1]:(0.590832650661) A[2]:(0.591427206993) A[3]:(0.529354095459)\n",
      " state (1)  A[0]:(0.532965064049) A[1]:(-0.0028087310493) A[2]:(0.655663967133) A[3]:(0.589426398277)\n",
      " state (2)  A[0]:(0.59108376503) A[1]:(0.729190468788) A[2]:(0.592304229736) A[3]:(0.655495524406)\n",
      " state (3)  A[0]:(0.655654609203) A[1]:(-0.0209857001901) A[2]:(0.478274732828) A[3]:(0.546121954918)\n",
      " state (4)  A[0]:(0.590900301933) A[1]:(0.656413435936) A[2]:(0.00284909433685) A[3]:(0.532525360584)\n",
      " state (5)  A[0]:(0.178212895989) A[1]:(0.92381644249) A[2]:(-0.137327119708) A[3]:(0.501894831657)\n",
      " state (6)  A[0]:(0.00171497301199) A[1]:(0.810236096382) A[2]:(-0.000129580497742) A[3]:(0.658016085625)\n",
      " state (7)  A[0]:(0.637133300304) A[1]:(-0.24432399869) A[2]:(0.171754524112) A[3]:(0.911265254021)\n",
      " state (8)  A[0]:(0.656073391438) A[1]:(0.00139234494418) A[2]:(0.72974395752) A[3]:(0.591271698475)\n",
      " state (9)  A[0]:(0.655997276306) A[1]:(0.810348033905) A[2]:(0.810137987137) A[3]:(0.00242544244975)\n",
      " state (10)  A[0]:(0.729395151138) A[1]:(0.900127232075) A[2]:(-0.00136506475974) A[3]:(0.731295824051)\n",
      " state (11)  A[0]:(0.513405442238) A[1]:(0.877081632614) A[2]:(-0.555645108223) A[3]:(0.840831279755)\n",
      " state (12)  A[0]:(0.0690146014094) A[1]:(0.824287354946) A[2]:(-0.462866932154) A[3]:(0.791621267796)\n",
      " state (13)  A[0]:(0.00192519789562) A[1]:(0.808830857277) A[2]:(0.899757623672) A[3]:(0.732565701008)\n",
      " state (14)  A[0]:(0.811043739319) A[1]:(0.900261640549) A[2]:(0.999999642372) A[3]:(0.8127348423)\n",
      " state (15)  A[0]:(0.990887582302) A[1]:(0.96365070343) A[2]:(1.0) A[3]:(0.89922696352)\n",
      "Episode 331000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6079. Times reached goal: 956.               Steps done: 3135179. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.039142810406.\n",
      " state (0)  A[0]:(0.532237410545) A[1]:(0.589858293533) A[2]:(0.590421140194) A[3]:(0.531746804714)\n",
      " state (1)  A[0]:(0.532436966896) A[1]:(0.000587675662246) A[2]:(0.655723273754) A[3]:(0.590416789055)\n",
      " state (2)  A[0]:(0.590466976166) A[1]:(0.728718340397) A[2]:(0.5915402174) A[3]:(0.655841767788)\n",
      " state (3)  A[0]:(0.655132472515) A[1]:(-0.027456946671) A[2]:(0.47898748517) A[3]:(0.545723438263)\n",
      " state (4)  A[0]:(0.590306520462) A[1]:(0.656088352203) A[2]:(0.000990152009763) A[3]:(0.532188117504)\n",
      " state (5)  A[0]:(0.176209077239) A[1]:(0.923862576485) A[2]:(-0.140354529023) A[3]:(0.500841498375)\n",
      " state (6)  A[0]:(-0.00050222867867) A[1]:(0.80989074707) A[2]:(-0.00160503247753) A[3]:(0.656184911728)\n",
      " state (7)  A[0]:(0.63637816906) A[1]:(-0.245784506202) A[2]:(0.172767341137) A[3]:(0.910317182541)\n",
      " state (8)  A[0]:(0.655908048153) A[1]:(-0.000262137502432) A[2]:(0.72881680727) A[3]:(0.590648412704)\n",
      " state (9)  A[0]:(0.655775666237) A[1]:(0.809952557087) A[2]:(0.809875369072) A[3]:(0.0013199589448)\n",
      " state (10)  A[0]:(0.7287992239) A[1]:(0.900005698204) A[2]:(-0.000556707323994) A[3]:(0.72989654541)\n",
      " state (11)  A[0]:(0.512063741684) A[1]:(0.877010822296) A[2]:(-0.555000066757) A[3]:(0.839656829834)\n",
      " state (12)  A[0]:(0.0668035596609) A[1]:(0.824324131012) A[2]:(-0.462264239788) A[3]:(0.789794266224)\n",
      " state (13)  A[0]:(-0.000668227556162) A[1]:(0.809056699276) A[2]:(0.900065600872) A[3]:(0.729971528053)\n",
      " state (14)  A[0]:(0.810006260872) A[1]:(0.900521814823) A[2]:(0.999999642372) A[3]:(0.810712635517)\n",
      " state (15)  A[0]:(0.990804255009) A[1]:(0.963781058788) A[2]:(1.0) A[3]:(0.897989034653)\n",
      "Episode 332000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6089. Times reached goal: 962.               Steps done: 3141268. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0389051939908.\n",
      " state (0)  A[0]:(0.531408667564) A[1]:(0.590370178223) A[2]:(0.590117454529) A[3]:(0.531242847443)\n",
      " state (1)  A[0]:(0.531864881516) A[1]:(0.000432092667324) A[2]:(0.655947804451) A[3]:(0.590266466141)\n",
      " state (2)  A[0]:(0.590210080147) A[1]:(0.729118704796) A[2]:(0.591493606567) A[3]:(0.656045556068)\n",
      " state (3)  A[0]:(0.65571898222) A[1]:(-0.0228139013052) A[2]:(0.479068428278) A[3]:(0.545221090317)\n",
      " state (4)  A[0]:(0.591283321381) A[1]:(0.655895471573) A[2]:(0.000776290718932) A[3]:(0.531369328499)\n",
      " state (5)  A[0]:(0.177286297083) A[1]:(0.923888742924) A[2]:(-0.141174674034) A[3]:(0.500572323799)\n",
      " state (6)  A[0]:(0.000101715326309) A[1]:(0.809964478016) A[2]:(-0.00109744025394) A[3]:(0.655713915825)\n",
      " state (7)  A[0]:(0.6365711689) A[1]:(-0.245179906487) A[2]:(0.175217837095) A[3]:(0.909867882729)\n",
      " state (8)  A[0]:(0.656294941902) A[1]:(-0.000119280070066) A[2]:(0.729021072388) A[3]:(0.590479493141)\n",
      " state (9)  A[0]:(0.656225800514) A[1]:(0.809964537621) A[2]:(0.810060143471) A[3]:(0.000105544924736)\n",
      " state (10)  A[0]:(0.729249119759) A[1]:(0.900007605553) A[2]:(0.000107288360596) A[3]:(0.728980004787)\n",
      " state (11)  A[0]:(0.512887358665) A[1]:(0.876971483231) A[2]:(-0.55484610796) A[3]:(0.839088082314)\n",
      " state (12)  A[0]:(0.0679307952523) A[1]:(0.824195504189) A[2]:(-0.462434917688) A[3]:(0.789045035839)\n",
      " state (13)  A[0]:(0.000366777152522) A[1]:(0.808812260628) A[2]:(0.900094985962) A[3]:(0.729041695595)\n",
      " state (14)  A[0]:(0.810313642025) A[1]:(0.900320470333) A[2]:(0.999999642372) A[3]:(0.810140728951)\n",
      " state (15)  A[0]:(0.990802526474) A[1]:(0.963658630848) A[2]:(1.0) A[3]:(0.897709071636)\n",
      "Episode 333000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6115. Times reached goal: 962.               Steps done: 3147383. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0386680146444.\n",
      " state (0)  A[0]:(0.531279563904) A[1]:(0.591443657875) A[2]:(0.59059882164) A[3]:(0.531986713409)\n",
      " state (1)  A[0]:(0.531933903694) A[1]:(0.000540245266166) A[2]:(0.656173706055) A[3]:(0.590986192226)\n",
      " state (2)  A[0]:(0.590202689171) A[1]:(0.729337453842) A[2]:(0.59258055687) A[3]:(0.656376481056)\n",
      " state (3)  A[0]:(0.655932545662) A[1]:(-0.034312851727) A[2]:(0.482450902462) A[3]:(0.544535040855)\n",
      " state (4)  A[0]:(0.591750562191) A[1]:(0.656581223011) A[2]:(0.00101888144854) A[3]:(0.531817913055)\n",
      " state (5)  A[0]:(0.176906079054) A[1]:(0.924206614494) A[2]:(-0.142622992396) A[3]:(0.501307368279)\n",
      " state (6)  A[0]:(-0.000234603881836) A[1]:(0.810120463371) A[2]:(-0.00172555272002) A[3]:(0.656448125839)\n",
      " state (7)  A[0]:(0.636858820915) A[1]:(-0.245411425829) A[2]:(0.176807805896) A[3]:(0.909976363182)\n",
      " state (8)  A[0]:(0.656556010246) A[1]:(0.000372577429516) A[2]:(0.729383111) A[3]:(0.590950667858)\n",
      " state (9)  A[0]:(0.656944274902) A[1]:(0.810164451599) A[2]:(0.810168385506) A[3]:(0.000194430351257)\n",
      " state (10)  A[0]:(0.729750216007) A[1]:(0.900041520596) A[2]:(6.12735748291e-05) A[3]:(0.728939175606)\n",
      " state (11)  A[0]:(0.513361215591) A[1]:(0.876925170422) A[2]:(-0.555305063725) A[3]:(0.839061617851)\n",
      " state (12)  A[0]:(0.0681894943118) A[1]:(0.824059605598) A[2]:(-0.463423609734) A[3]:(0.788958311081)\n",
      " state (13)  A[0]:(0.000346720218658) A[1]:(0.808630406857) A[2]:(0.89994096756) A[3]:(0.728829503059)\n",
      " state (14)  A[0]:(0.810291051865) A[1]:(0.900224983692) A[2]:(0.999999642372) A[3]:(0.809876263142)\n",
      " state (15)  A[0]:(0.990783989429) A[1]:(0.963608503342) A[2]:(1.0) A[3]:(0.897437214851)\n",
      "Episode 334000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6078. Times reached goal: 963.               Steps done: 3153461. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0384337032451.\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.5902,  0.5906,  0.5319]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5325,  0.0005,  0.6559,  0.5908]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7289,  0.5918,  0.6564]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8100, -0.0011,  0.6565]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7297,  0.9000, -0.0002,  0.7296]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9006,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531556665897) A[1]:(0.590207993984) A[2]:(0.590519011021) A[3]:(0.531764984131)\n",
      " state (1)  A[0]:(0.532244324684) A[1]:(0.000596642435994) A[2]:(0.655913114548) A[3]:(0.590484142303)\n",
      " state (2)  A[0]:(0.590341746807) A[1]:(0.728944659233) A[2]:(0.591776847839) A[3]:(0.656040847301)\n",
      " state (3)  A[0]:(0.655163049698) A[1]:(-0.0410378053784) A[2]:(0.484387636185) A[3]:(0.543549954891)\n",
      " state (4)  A[0]:(0.590691685677) A[1]:(0.656051874161) A[2]:(0.0016802533064) A[3]:(0.531728208065)\n",
      " state (5)  A[0]:(0.175362601876) A[1]:(0.92430961132) A[2]:(-0.143307641149) A[3]:(0.501591384411)\n",
      " state (6)  A[0]:(-0.000600844563451) A[1]:(0.810013532639) A[2]:(-0.00114774657413) A[3]:(0.655920863152)\n",
      " state (7)  A[0]:(0.636981964111) A[1]:(-0.245936334133) A[2]:(0.179335355759) A[3]:(0.909118711948)\n",
      " state (8)  A[0]:(0.656163275242) A[1]:(-0.000844001595397) A[2]:(0.729088246822) A[3]:(0.590183734894)\n",
      " state (9)  A[0]:(0.656102895737) A[1]:(0.80967104435) A[2]:(0.810122966766) A[3]:(-5.39422035217e-06)\n",
      " state (10)  A[0]:(0.729254245758) A[1]:(0.899935364723) A[2]:(-0.000246047973633) A[3]:(0.729208350182)\n",
      " state (11)  A[0]:(0.512910604477) A[1]:(0.876953005791) A[2]:(-0.556097328663) A[3]:(0.839425206184)\n",
      " state (12)  A[0]:(0.0676292106509) A[1]:(0.824298739433) A[2]:(-0.464567184448) A[3]:(0.789332866669)\n",
      " state (13)  A[0]:(-0.000491559447255) A[1]:(0.809127748013) A[2]:(0.899909973145) A[3]:(0.729085147381)\n",
      " state (14)  A[0]:(0.809678137302) A[1]:(0.900654733181) A[2]:(0.999999642372) A[3]:(0.809965252876)\n",
      " state (15)  A[0]:(0.990702569485) A[1]:(0.963801205158) A[2]:(1.0) A[3]:(0.897364199162)\n",
      "Episode 335000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6109. Times reached goal: 965.               Steps done: 3159570. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0381996274644.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.533128261566) A[1]:(0.590194702148) A[2]:(0.589761257172) A[3]:(0.530796408653)\n",
      " state (1)  A[0]:(0.533662796021) A[1]:(-0.00156152120326) A[2]:(0.654379248619) A[3]:(0.589979887009)\n",
      " state (2)  A[0]:(0.591166853905) A[1]:(0.72926980257) A[2]:(0.592306733131) A[3]:(0.655418634415)\n",
      " state (3)  A[0]:(0.654918789864) A[1]:(-0.0425199642777) A[2]:(0.488576024771) A[3]:(0.542763948441)\n",
      " state (4)  A[0]:(0.590398430824) A[1]:(0.656322002411) A[2]:(0.00573461921886) A[3]:(0.531528830528)\n",
      " state (5)  A[0]:(0.175353750587) A[1]:(0.924770116806) A[2]:(-0.141833215952) A[3]:(0.502080559731)\n",
      " state (6)  A[0]:(0.00185769586824) A[1]:(0.809978723526) A[2]:(0.000830292527098) A[3]:(0.656635224819)\n",
      " state (7)  A[0]:(0.63741093874) A[1]:(-0.246920719743) A[2]:(0.182117462158) A[3]:(0.908712744713)\n",
      " state (8)  A[0]:(0.654153585434) A[1]:(-0.000628769339528) A[2]:(0.728372633457) A[3]:(0.589448273182)\n",
      " state (9)  A[0]:(0.653384566307) A[1]:(0.809544324875) A[2]:(0.809743583202) A[3]:(-0.00269303563982)\n",
      " state (10)  A[0]:(0.727151572704) A[1]:(0.899774909019) A[2]:(-0.00219726213254) A[3]:(0.728194653988)\n",
      " state (11)  A[0]:(0.509897708893) A[1]:(0.876571595669) A[2]:(-0.558397054672) A[3]:(0.839133381844)\n",
      " state (12)  A[0]:(0.0640535503626) A[1]:(0.823537111282) A[2]:(-0.46680700779) A[3]:(0.789136528969)\n",
      " state (13)  A[0]:(-0.00219797738828) A[1]:(0.808233439922) A[2]:(0.900613188744) A[3]:(0.729165077209)\n",
      " state (14)  A[0]:(0.810282409191) A[1]:(0.900232791901) A[2]:(0.999999642372) A[3]:(0.810460925102)\n",
      " state (15)  A[0]:(0.99070674181) A[1]:(0.96356344223) A[2]:(1.0) A[3]:(0.897608816624)\n",
      "Episode 336000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6113. Times reached goal: 964.               Steps done: 3165683. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0379668254261.\n",
      " state (0)  A[0]:(0.531661391258) A[1]:(0.590460181236) A[2]:(0.590524315834) A[3]:(0.531271100044)\n",
      " state (1)  A[0]:(0.532299041748) A[1]:(0.000602010579314) A[2]:(0.656027257442) A[3]:(0.590223908424)\n",
      " state (2)  A[0]:(0.590382933617) A[1]:(0.72898888588) A[2]:(0.591222345829) A[3]:(0.656087398529)\n",
      " state (3)  A[0]:(0.655770599842) A[1]:(-0.0201715547591) A[2]:(0.483349204063) A[3]:(0.54467022419)\n",
      " state (4)  A[0]:(0.591161727905) A[1]:(0.656137406826) A[2]:(0.001319884439) A[3]:(0.531372666359)\n",
      " state (5)  A[0]:(0.174971744418) A[1]:(0.924560129642) A[2]:(-0.145574733615) A[3]:(0.501841962337)\n",
      " state (6)  A[0]:(-0.000346064567566) A[1]:(0.809981584549) A[2]:(-0.000688791158609) A[3]:(0.655855774879)\n",
      " state (7)  A[0]:(0.637241542339) A[1]:(-0.245911434293) A[2]:(0.183799430728) A[3]:(0.908415019512)\n",
      " state (8)  A[0]:(0.656059622765) A[1]:(6.55986368656e-05) A[2]:(0.728963077068) A[3]:(0.590599179268)\n",
      " state (9)  A[0]:(0.655979394913) A[1]:(0.80990087986) A[2]:(0.809987783432) A[3]:(0.000508442462888)\n",
      " state (10)  A[0]:(0.728952527046) A[1]:(0.899989187717) A[2]:(-0.000162839889526) A[3]:(0.729137301445)\n",
      " state (11)  A[0]:(0.512324213982) A[1]:(0.876872777939) A[2]:(-0.556573808193) A[3]:(0.839438438416)\n",
      " state (12)  A[0]:(0.0667516738176) A[1]:(0.823979139328) A[2]:(-0.465664952993) A[3]:(0.789300203323)\n",
      " state (13)  A[0]:(-0.00118908227887) A[1]:(0.8085719347) A[2]:(0.899995684624) A[3]:(0.729006230831)\n",
      " state (14)  A[0]:(0.809714436531) A[1]:(0.900277256966) A[2]:(0.999999642372) A[3]:(0.809985041618)\n",
      " state (15)  A[0]:(0.990676641464) A[1]:(0.963578939438) A[2]:(1.0) A[3]:(0.897312879562)\n",
      "Episode 337000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6090. Times reached goal: 960.               Steps done: 3171773. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0377363100909.\n",
      " state (0)  A[0]:(0.531737625599) A[1]:(0.59056609869) A[2]:(0.590486645699) A[3]:(0.531880736351)\n",
      " state (1)  A[0]:(0.532280683517) A[1]:(3.56137752533e-06) A[2]:(0.655968785286) A[3]:(0.59060716629)\n",
      " state (2)  A[0]:(0.590489566326) A[1]:(0.729098796844) A[2]:(0.591152608395) A[3]:(0.656329035759)\n",
      " state (3)  A[0]:(0.655570030212) A[1]:(-0.0285406820476) A[2]:(0.484929770231) A[3]:(0.544412016869)\n",
      " state (4)  A[0]:(0.591066420078) A[1]:(0.656259298325) A[2]:(0.000570893229451) A[3]:(0.532091677189)\n",
      " state (5)  A[0]:(0.174639835954) A[1]:(0.924715697765) A[2]:(-0.14734493196) A[3]:(0.502825438976)\n",
      " state (6)  A[0]:(-0.000446677178843) A[1]:(0.810120642185) A[2]:(-0.00137185968924) A[3]:(0.656175851822)\n",
      " state (7)  A[0]:(0.637593567371) A[1]:(-0.24585904181) A[2]:(0.185330435634) A[3]:(0.9081376791)\n",
      " state (8)  A[0]:(0.656254172325) A[1]:(-0.000191584229469) A[2]:(0.72913479805) A[3]:(0.59042942524)\n",
      " state (9)  A[0]:(0.65593624115) A[1]:(0.809835612774) A[2]:(0.810162305832) A[3]:(2.57045030594e-05)\n",
      " state (10)  A[0]:(0.728956758976) A[1]:(0.899956166744) A[2]:(7.52210617065e-05) A[3]:(0.729147434235)\n",
      " state (11)  A[0]:(0.512466907501) A[1]:(0.876820683479) A[2]:(-0.556874513626) A[3]:(0.839605569839)\n",
      " state (12)  A[0]:(0.067011334002) A[1]:(0.823920428753) A[2]:(-0.466355443001) A[3]:(0.789534389973)\n",
      " state (13)  A[0]:(-0.000937670178246) A[1]:(0.808575868607) A[2]:(0.900024831295) A[3]:(0.729252517223)\n",
      " state (14)  A[0]:(0.809763133526) A[1]:(0.900363743305) A[2]:(0.999999642372) A[3]:(0.8101516366)\n",
      " state (15)  A[0]:(0.990649163723) A[1]:(0.963619410992) A[2]:(1.0) A[3]:(0.897315979004)\n",
      "Episode 338000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6126. Times reached goal: 967.               Steps done: 3177899. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0375058440934.\n",
      " state (0)  A[0]:(0.531673789024) A[1]:(0.590408563614) A[2]:(0.590645313263) A[3]:(0.531959652901)\n",
      " state (1)  A[0]:(0.532158493996) A[1]:(2.1867454052e-05) A[2]:(0.656075358391) A[3]:(0.59052491188)\n",
      " state (2)  A[0]:(0.590184926987) A[1]:(0.729194998741) A[2]:(0.590573012829) A[3]:(0.656338214874)\n",
      " state (3)  A[0]:(0.655302286148) A[1]:(-0.00819555204362) A[2]:(0.483712911606) A[3]:(0.545482754707)\n",
      " state (4)  A[0]:(0.590301096439) A[1]:(0.656042277813) A[2]:(0.00165176240262) A[3]:(0.531345486641)\n",
      " state (5)  A[0]:(0.173282727599) A[1]:(0.924806475639) A[2]:(-0.147616177797) A[3]:(0.502253592014)\n",
      " state (6)  A[0]:(-0.000978707917966) A[1]:(0.810043871403) A[2]:(-0.000617027224507) A[3]:(0.65575248003)\n",
      " state (7)  A[0]:(0.637376904488) A[1]:(-0.246122047305) A[2]:(0.187652647495) A[3]:(0.90773665905)\n",
      " state (8)  A[0]:(0.656024336815) A[1]:(0.000281177461147) A[2]:(0.728929102421) A[3]:(0.590652346611)\n",
      " state (9)  A[0]:(0.656091868877) A[1]:(0.810047447681) A[2]:(0.810018777847) A[3]:(0.000286504626274)\n",
      " state (10)  A[0]:(0.729265451431) A[1]:(0.900071978569) A[2]:(2.45571136475e-05) A[3]:(0.729107737541)\n",
      " state (11)  A[0]:(0.513195157051) A[1]:(0.876906037331) A[2]:(-0.557170510292) A[3]:(0.839598298073)\n",
      " state (12)  A[0]:(0.0681471079588) A[1]:(0.823933005333) A[2]:(-0.467142760754) A[3]:(0.789453148842)\n",
      " state (13)  A[0]:(0.00012469291687) A[1]:(0.808439970016) A[2]:(0.899813294411) A[3]:(0.729041516781)\n",
      " state (14)  A[0]:(0.810078024864) A[1]:(0.900208353996) A[2]:(0.999999642372) A[3]:(0.809978485107)\n",
      " state (15)  A[0]:(0.990653276443) A[1]:(0.963516950607) A[2]:(1.0) A[3]:(0.897201895714)\n",
      "Episode 339000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6090. Times reached goal: 959.               Steps done: 3183989. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0372781276034.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5899,  0.5909,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5323,  0.0006,  0.6562,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7289,  0.5913,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0005,  0.8101, -0.0014,  0.6557]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0001,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9004,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531157016754) A[1]:(0.589877605438) A[2]:(0.590879678726) A[3]:(0.530496120453)\n",
      " state (1)  A[0]:(0.532353997231) A[1]:(0.000652465852909) A[2]:(0.656101644039) A[3]:(0.590103268623)\n",
      " state (2)  A[0]:(0.590607047081) A[1]:(0.728937268257) A[2]:(0.591265678406) A[3]:(0.656079888344)\n",
      " state (3)  A[0]:(0.655406117439) A[1]:(-0.0174458473921) A[2]:(0.485670506954) A[3]:(0.54422134161)\n",
      " state (4)  A[0]:(0.590716838837) A[1]:(0.655788242817) A[2]:(0.000787496392149) A[3]:(0.531056702137)\n",
      " state (5)  A[0]:(0.173562884331) A[1]:(0.925023972988) A[2]:(-0.149624198675) A[3]:(0.502536475658)\n",
      " state (6)  A[0]:(-0.000102251768112) A[1]:(0.810066342354) A[2]:(-0.00131225516088) A[3]:(0.655783176422)\n",
      " state (7)  A[0]:(0.637715101242) A[1]:(-0.24652980268) A[2]:(0.189168050885) A[3]:(0.907193362713)\n",
      " state (8)  A[0]:(0.655660748482) A[1]:(0.000256512314081) A[2]:(0.728987216949) A[3]:(0.589721381664)\n",
      " state (9)  A[0]:(0.655773639679) A[1]:(0.809942245483) A[2]:(0.81014996767) A[3]:(-3.22014093399e-05)\n",
      " state (10)  A[0]:(0.729089200497) A[1]:(0.90002810955) A[2]:(0.000175356864929) A[3]:(0.729167222977)\n",
      " state (11)  A[0]:(0.512946724892) A[1]:(0.876873195171) A[2]:(-0.557531416416) A[3]:(0.839677095413)\n",
      " state (12)  A[0]:(0.0677421838045) A[1]:(0.8239402771) A[2]:(-0.467685341835) A[3]:(0.789440214634)\n",
      " state (13)  A[0]:(-0.00017261505127) A[1]:(0.808577120304) A[2]:(0.900134086609) A[3]:(0.728961825371)\n",
      " state (14)  A[0]:(0.810036301613) A[1]:(0.900417506695) A[2]:(0.999999642372) A[3]:(0.810075879097)\n",
      " state (15)  A[0]:(0.990609407425) A[1]:(0.963601827621) A[2]:(1.0) A[3]:(0.897249400616)\n",
      "Episode 340000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6087. Times reached goal: 960.               Steps done: 3190076. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0370519048481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53086745739) A[1]:(0.590659320354) A[2]:(0.590151309967) A[3]:(0.531038820744)\n",
      " state (1)  A[0]:(0.531366586685) A[1]:(0.000338304787874) A[2]:(0.655932188034) A[3]:(0.590008616447)\n",
      " state (2)  A[0]:(0.589781343937) A[1]:(0.728927850723) A[2]:(0.590594112873) A[3]:(0.655907332897)\n",
      " state (3)  A[0]:(0.65554690361) A[1]:(-0.00207916717045) A[2]:(0.484359055758) A[3]:(0.545470476151)\n",
      " state (4)  A[0]:(0.591058254242) A[1]:(0.656045496464) A[2]:(0.000954150862526) A[3]:(0.531098425388)\n",
      " state (5)  A[0]:(0.17372842133) A[1]:(0.925061762333) A[2]:(-0.150074362755) A[3]:(0.502435147762)\n",
      " state (6)  A[0]:(8.52346420288e-06) A[1]:(0.809976875782) A[2]:(-0.000489354075398) A[3]:(0.655534148216)\n",
      " state (7)  A[0]:(0.638130545616) A[1]:(-0.246796011925) A[2]:(0.191733330488) A[3]:(0.907042622566)\n",
      " state (8)  A[0]:(0.656447649002) A[1]:(0.000107813626528) A[2]:(0.728934407234) A[3]:(0.590809345245)\n",
      " state (9)  A[0]:(0.656386137009) A[1]:(0.809999763966) A[2]:(0.80999070406) A[3]:(0.000232666730881)\n",
      " state (10)  A[0]:(0.729357838631) A[1]:(0.900041103363) A[2]:(0.000365972489817) A[3]:(0.728758335114)\n",
      " state (11)  A[0]:(0.513405919075) A[1]:(0.876836299896) A[2]:(-0.557505846024) A[3]:(0.839446187019)\n",
      " state (12)  A[0]:(0.0685220211744) A[1]:(0.823826730251) A[2]:(-0.468118220568) A[3]:(0.789242386818)\n",
      " state (13)  A[0]:(0.000580638588872) A[1]:(0.808377146721) A[2]:(0.899972319603) A[3]:(0.728793859482)\n",
      " state (14)  A[0]:(0.810278177261) A[1]:(0.900274336338) A[2]:(0.999999642372) A[3]:(0.809962034225)\n",
      " state (15)  A[0]:(0.990612983704) A[1]:(0.963527083397) A[2]:(1.0) A[3]:(0.897153556347)\n",
      "Episode 341000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6098. Times reached goal: 949.               Steps done: 3196174. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0368266498329.\n",
      " state (0)  A[0]:(0.53047978878) A[1]:(0.590412318707) A[2]:(0.590334057808) A[3]:(0.531631052494)\n",
      " state (1)  A[0]:(0.530810117722) A[1]:(-0.000895913457498) A[2]:(0.656926333904) A[3]:(0.590357780457)\n",
      " state (2)  A[0]:(0.588614940643) A[1]:(0.728670358658) A[2]:(0.595180869102) A[3]:(0.655677199364)\n",
      " state (3)  A[0]:(0.65331274271) A[1]:(-0.0111049581319) A[2]:(0.494253367186) A[3]:(0.544001817703)\n",
      " state (4)  A[0]:(0.588508486748) A[1]:(0.655756413937) A[2]:(0.0112338112667) A[3]:(0.530412375927)\n",
      " state (5)  A[0]:(0.17016941309) A[1]:(0.925512313843) A[2]:(-0.143112048507) A[3]:(0.502422213554)\n",
      " state (6)  A[0]:(-0.00124126614537) A[1]:(0.810425281525) A[2]:(0.00604502949864) A[3]:(0.655968308449)\n",
      " state (7)  A[0]:(0.63837569952) A[1]:(-0.24639827013) A[2]:(0.197410538793) A[3]:(0.907032012939)\n",
      " state (8)  A[0]:(0.656485319138) A[1]:(0.00140840094537) A[2]:(0.728865623474) A[3]:(0.593512892723)\n",
      " state (9)  A[0]:(0.656090378761) A[1]:(0.810469448566) A[2]:(0.809927880764) A[3]:(0.00276929931715)\n",
      " state (10)  A[0]:(0.72858953476) A[1]:(0.900205731392) A[2]:(0.000543355883565) A[3]:(0.729047179222)\n",
      " state (11)  A[0]:(0.511678218842) A[1]:(0.876894593239) A[2]:(-0.558111310005) A[3]:(0.839505910873)\n",
      " state (12)  A[0]:(0.0657175555825) A[1]:(0.823816716671) A[2]:(-0.469434082508) A[3]:(0.789175033569)\n",
      " state (13)  A[0]:(-0.00228258571588) A[1]:(0.808419108391) A[2]:(0.899964809418) A[3]:(0.728536486626)\n",
      " state (14)  A[0]:(0.8094419837) A[1]:(0.900419354439) A[2]:(0.999999642372) A[3]:(0.809633731842)\n",
      " state (15)  A[0]:(0.990537524223) A[1]:(0.963599801064) A[2]:(1.0) A[3]:(0.896758794785)\n",
      "Episode 342000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6048. Times reached goal: 956.               Steps done: 3202222. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0366045944271.\n",
      " state (0)  A[0]:(0.530954062939) A[1]:(0.590317726135) A[2]:(0.590113341808) A[3]:(0.53137421608)\n",
      " state (1)  A[0]:(0.531457066536) A[1]:(0.000477820605738) A[2]:(0.655949115753) A[3]:(0.590431928635)\n",
      " state (2)  A[0]:(0.589650988579) A[1]:(0.728876709938) A[2]:(0.591542184353) A[3]:(0.656152248383)\n",
      " state (3)  A[0]:(0.654759883881) A[1]:(-0.00744919106364) A[2]:(0.488149166107) A[3]:(0.545197844505)\n",
      " state (4)  A[0]:(0.59021115303) A[1]:(0.655737996101) A[2]:(0.00244926917367) A[3]:(0.531634926796)\n",
      " state (5)  A[0]:(0.172194063663) A[1]:(0.925401985645) A[2]:(-0.151283547282) A[3]:(0.503711223602)\n",
      " state (6)  A[0]:(0.000274062156677) A[1]:(0.80991101265) A[2]:(5.65052032471e-05) A[3]:(0.656235337257)\n",
      " state (7)  A[0]:(0.638575077057) A[1]:(-0.247851803899) A[2]:(0.195370569825) A[3]:(0.906476020813)\n",
      " state (8)  A[0]:(0.656046867371) A[1]:(-0.000254124403) A[2]:(0.728706777096) A[3]:(0.591101288795)\n",
      " state (9)  A[0]:(0.655943632126) A[1]:(0.809939324856) A[2]:(0.810058474541) A[3]:(0.000514537037816)\n",
      " state (10)  A[0]:(0.729271888733) A[1]:(0.900057375431) A[2]:(0.000256061553955) A[3]:(0.729207515717)\n",
      " state (11)  A[0]:(0.513704895973) A[1]:(0.876865267754) A[2]:(-0.558676838875) A[3]:(0.840026915073)\n",
      " state (12)  A[0]:(0.0690681412816) A[1]:(0.823910951614) A[2]:(-0.470362573862) A[3]:(0.789967238903)\n",
      " state (13)  A[0]:(0.00090330815874) A[1]:(0.808574140072) A[2]:(0.89972782135) A[3]:(0.729488670826)\n",
      " state (14)  A[0]:(0.810176074505) A[1]:(0.900496006012) A[2]:(0.999999642372) A[3]:(0.810317635536)\n",
      " state (15)  A[0]:(0.990543663502) A[1]:(0.963607609272) A[2]:(1.0) A[3]:(0.897116363049)\n",
      "Episode 343000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6050. Times reached goal: 951.               Steps done: 3208272. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0363838051917.\n",
      " state (0)  A[0]:(0.531696200371) A[1]:(0.590315818787) A[2]:(0.590005278587) A[3]:(0.530588150024)\n",
      " state (1)  A[0]:(0.532148897648) A[1]:(1.54264271259e-05) A[2]:(0.655852198601) A[3]:(0.589497685432)\n",
      " state (2)  A[0]:(0.590249538422) A[1]:(0.729131221771) A[2]:(0.591220676899) A[3]:(0.655256152153)\n",
      " state (3)  A[0]:(0.655458927155) A[1]:(-0.0082347728312) A[2]:(0.488134741783) A[3]:(0.543636023998)\n",
      " state (4)  A[0]:(0.590826153755) A[1]:(0.656211614609) A[2]:(0.000706910970621) A[3]:(0.530190706253)\n",
      " state (5)  A[0]:(0.17207147181) A[1]:(0.925567626953) A[2]:(-0.154021546245) A[3]:(0.502432823181)\n",
      " state (6)  A[0]:(-2.12788581848e-05) A[1]:(0.809949994087) A[2]:(-0.00184261589311) A[3]:(0.655231535435)\n",
      " state (7)  A[0]:(0.638685464859) A[1]:(-0.248059183359) A[2]:(0.195718199015) A[3]:(0.905946493149)\n",
      " state (8)  A[0]:(0.655960917473) A[1]:(5.46760857105e-05) A[2]:(0.728942036629) A[3]:(0.589357972145)\n",
      " state (9)  A[0]:(0.656182289124) A[1]:(0.810066521168) A[2]:(0.810004711151) A[3]:(-0.000573232711758)\n",
      " state (10)  A[0]:(0.729350328445) A[1]:(0.900118887424) A[2]:(-0.000725030782633) A[3]:(0.729215979576)\n",
      " state (11)  A[0]:(0.513377547264) A[1]:(0.876959621906) A[2]:(-0.55977666378) A[3]:(0.840097367764)\n",
      " state (12)  A[0]:(0.0681273192167) A[1]:(0.824133515358) A[2]:(-0.471489757299) A[3]:(0.789993524551)\n",
      " state (13)  A[0]:(0.000134378671646) A[1]:(0.809030890465) A[2]:(0.900170922279) A[3]:(0.729506134987)\n",
      " state (14)  A[0]:(0.810223758221) A[1]:(0.900934815407) A[2]:(0.999999642372) A[3]:(0.810426950455)\n",
      " state (15)  A[0]:(0.990502953529) A[1]:(0.963780879974) A[2]:(1.0) A[3]:(0.897048354149)\n",
      "Episode 344000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6108. Times reached goal: 964.               Steps done: 3214380. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0361622502273.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5906,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5913,  0.6563,  0.0009,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6568,  0.0006,  0.7291,  0.5906]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 6.5647e-01,  8.1007e-01,  8.0991e-01, -7.2122e-06]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0019,  0.8084,  0.9000,  0.7284]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8097,  0.9006,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531758368015) A[1]:(0.590621352196) A[2]:(0.590601444244) A[3]:(0.531441807747)\n",
      " state (1)  A[0]:(0.532433629036) A[1]:(0.00052432337543) A[2]:(0.65623486042) A[3]:(0.59049642086)\n",
      " state (2)  A[0]:(0.590733885765) A[1]:(0.728978395462) A[2]:(0.591447234154) A[3]:(0.65621316433)\n",
      " state (3)  A[0]:(0.655740916729) A[1]:(-0.00908932741731) A[2]:(0.489161521196) A[3]:(0.544741511345)\n",
      " state (4)  A[0]:(0.591176271439) A[1]:(0.65614926815) A[2]:(0.000921010738239) A[3]:(0.531505823135)\n",
      " state (5)  A[0]:(0.172114044428) A[1]:(0.925664067268) A[2]:(-0.154315978289) A[3]:(0.503881216049)\n",
      " state (6)  A[0]:(4.47928905487e-05) A[1]:(0.809998512268) A[2]:(-0.000615835131612) A[3]:(0.655926823616)\n",
      " state (7)  A[0]:(0.638874053955) A[1]:(-0.24766972661) A[2]:(0.198796138167) A[3]:(0.905828773975)\n",
      " state (8)  A[0]:(0.656174778938) A[1]:(0.000263404101133) A[2]:(0.729229211807) A[3]:(0.590190649033)\n",
      " state (9)  A[0]:(0.656155586243) A[1]:(0.80999147892) A[2]:(0.810163974762) A[3]:(-0.000546365918126)\n",
      " state (10)  A[0]:(0.729124069214) A[1]:(0.90001732111) A[2]:(0.000207185745239) A[3]:(0.728785037994)\n",
      " state (11)  A[0]:(0.51294118166) A[1]:(0.876745462418) A[2]:(-0.559336781502) A[3]:(0.839792609215)\n",
      " state (12)  A[0]:(0.067474655807) A[1]:(0.823697984219) A[2]:(-0.471381664276) A[3]:(0.789548039436)\n",
      " state (13)  A[0]:(-0.000594675482716) A[1]:(0.808388888836) A[2]:(0.900278508663) A[3]:(0.728880167007)\n",
      " state (14)  A[0]:(0.810013115406) A[1]:(0.900494754314) A[2]:(0.999999642372) A[3]:(0.809935331345)\n",
      " state (15)  A[0]:(0.990475654602) A[1]:(0.963556110859) A[2]:(1.0) A[3]:(0.896707117558)\n",
      "Episode 345000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6149. Times reached goal: 973.               Steps done: 3220529. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0359405708025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530538737774) A[1]:(0.589878380299) A[2]:(0.589913666248) A[3]:(0.53107047081)\n",
      " state (1)  A[0]:(0.531115651131) A[1]:(0.000715967151336) A[2]:(0.655992209911) A[3]:(0.590179085732)\n",
      " state (2)  A[0]:(0.589492797852) A[1]:(0.728591144085) A[2]:(0.591187119484) A[3]:(0.655937254429)\n",
      " state (3)  A[0]:(0.654724597931) A[1]:(-0.0127040203661) A[2]:(0.49042609334) A[3]:(0.543979287148)\n",
      " state (4)  A[0]:(0.590287804604) A[1]:(0.655528068542) A[2]:(0.000846385781188) A[3]:(0.531135201454)\n",
      " state (5)  A[0]:(0.170738011599) A[1]:(0.925722897053) A[2]:(-0.155898794532) A[3]:(0.503866434097)\n",
      " state (6)  A[0]:(-0.0011501009576) A[1]:(0.809855997562) A[2]:(-0.00144636526238) A[3]:(0.655583441257)\n",
      " state (7)  A[0]:(0.638038396835) A[1]:(-0.248582333326) A[2]:(0.199696034193) A[3]:(0.905332446098)\n",
      " state (8)  A[0]:(0.65532118082) A[1]:(-7.88904726505e-05) A[2]:(0.728819727898) A[3]:(0.590043604374)\n",
      " state (9)  A[0]:(0.655521869659) A[1]:(0.810037255287) A[2]:(0.809935927391) A[3]:(0.000181540846825)\n",
      " state (10)  A[0]:(0.728695631027) A[1]:(0.90001308918) A[2]:(0.00029730796814) A[3]:(0.728896379471)\n",
      " state (11)  A[0]:(0.512545228004) A[1]:(0.876706242561) A[2]:(-0.559371232986) A[3]:(0.83980602026)\n",
      " state (12)  A[0]:(0.0672924369574) A[1]:(0.823651850224) A[2]:(-0.471947044134) A[3]:(0.789497613907)\n",
      " state (13)  A[0]:(-0.0007371007232) A[1]:(0.808377087116) A[2]:(0.900036036968) A[3]:(0.728718042374)\n",
      " state (14)  A[0]:(0.809906244278) A[1]:(0.900541245937) A[2]:(0.999999642372) A[3]:(0.809787631035)\n",
      " state (15)  A[0]:(0.990458369255) A[1]:(0.963592886925) A[2]:(1.0) A[3]:(0.896594405174)\n",
      "Episode 346000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6092. Times reached goal: 956.               Steps done: 3226621. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0357222864145.\n",
      " state (0)  A[0]:(0.530520021915) A[1]:(0.590671241283) A[2]:(0.590740561485) A[3]:(0.532908797264)\n",
      " state (1)  A[0]:(0.531342387199) A[1]:(0.00145753740799) A[2]:(0.656146407127) A[3]:(0.591688275337)\n",
      " state (2)  A[0]:(0.589890837669) A[1]:(0.729268908501) A[2]:(0.591160356998) A[3]:(0.657050251961)\n",
      " state (3)  A[0]:(0.655012011528) A[1]:(-0.013414863497) A[2]:(0.49136826396) A[3]:(0.544401526451)\n",
      " state (4)  A[0]:(0.590501964092) A[1]:(0.656571090221) A[2]:(0.00147938623559) A[3]:(0.531387507915)\n",
      " state (5)  A[0]:(0.170730367303) A[1]:(0.926050007343) A[2]:(-0.15563800931) A[3]:(0.50427544117)\n",
      " state (6)  A[0]:(-0.000314712524414) A[1]:(0.810319185257) A[2]:(-0.000628590525594) A[3]:(0.655984401703)\n",
      " state (7)  A[0]:(0.638989031315) A[1]:(-0.247585505247) A[2]:(0.201344549656) A[3]:(0.90523827076)\n",
      " state (8)  A[0]:(0.655752897263) A[1]:(0.00111266551539) A[2]:(0.729272484779) A[3]:(0.589592456818)\n",
      " state (9)  A[0]:(0.655788183212) A[1]:(0.810339570045) A[2]:(0.810184180737) A[3]:(-0.000637978257146)\n",
      " state (10)  A[0]:(0.728914499283) A[1]:(0.90011203289) A[2]:(-0.000444650620921) A[3]:(0.729159712791)\n",
      " state (11)  A[0]:(0.512731254101) A[1]:(0.876737058163) A[2]:(-0.560705304146) A[3]:(0.840189754963)\n",
      " state (12)  A[0]:(0.0673915818334) A[1]:(0.823603630066) A[2]:(-0.473095655441) A[3]:(0.789986252785)\n",
      " state (13)  A[0]:(0.000359445781214) A[1]:(0.808350026608) A[2]:(0.900952219963) A[3]:(0.729369521141)\n",
      " state (14)  A[0]:(0.811029672623) A[1]:(0.900589466095) A[2]:(0.999999701977) A[3]:(0.81038749218)\n",
      " state (15)  A[0]:(0.990466952324) A[1]:(0.963525116444) A[2]:(1.0) A[3]:(0.896703839302)\n",
      "Episode 347000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6127. Times reached goal: 974.               Steps done: 3232748. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.035504085108.\n",
      " state (0)  A[0]:(0.531656682491) A[1]:(0.590529561043) A[2]:(0.590526342392) A[3]:(0.531595528126)\n",
      " state (1)  A[0]:(0.531931698322) A[1]:(0.000100631266832) A[2]:(0.656164050102) A[3]:(0.590499341488)\n",
      " state (2)  A[0]:(0.590312957764) A[1]:(0.729202270508) A[2]:(0.590789318085) A[3]:(0.656199455261)\n",
      " state (3)  A[0]:(0.655593633652) A[1]:(-0.00604739505798) A[2]:(0.489869713783) A[3]:(0.54477596283)\n",
      " state (4)  A[0]:(0.590873062611) A[1]:(0.656439900398) A[2]:(0.000244140625) A[3]:(0.531643748283)\n",
      " state (5)  A[0]:(0.170758590102) A[1]:(0.925974607468) A[2]:(-0.156677752733) A[3]:(0.504700899124)\n",
      " state (6)  A[0]:(-0.000269383192062) A[1]:(0.810019254684) A[2]:(-0.000309705734253) A[3]:(0.656128168106)\n",
      " state (7)  A[0]:(0.639357686043) A[1]:(-0.2485204041) A[2]:(0.203206211329) A[3]:(0.90512239933)\n",
      " state (8)  A[0]:(0.656455278397) A[1]:(-9.66303050518e-05) A[2]:(0.72921705246) A[3]:(0.590608298779)\n",
      " state (9)  A[0]:(0.656299829483) A[1]:(0.809979200363) A[2]:(0.81004601717) A[3]:(0.000281810760498)\n",
      " state (10)  A[0]:(0.729197740555) A[1]:(0.900006055832) A[2]:(-0.00014853477478) A[3]:(0.729115664959)\n",
      " state (11)  A[0]:(0.513271927834) A[1]:(0.876705348492) A[2]:(-0.560552716255) A[3]:(0.840124487877)\n",
      " state (12)  A[0]:(0.0680272951722) A[1]:(0.823645234108) A[2]:(-0.47379848361) A[3]:(0.789889931679)\n",
      " state (13)  A[0]:(-0.000190794467926) A[1]:(0.808344960213) A[2]:(0.900146484375) A[3]:(0.729153513908)\n",
      " state (14)  A[0]:(0.810051620007) A[1]:(0.900482356548) A[2]:(0.999999701977) A[3]:(0.810167789459)\n",
      " state (15)  A[0]:(0.990398108959) A[1]:(0.963474571705) A[2]:(1.0) A[3]:(0.896662533283)\n",
      "Episode 348000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6044. Times reached goal: 958.               Steps done: 3238792. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.035290145594.\n",
      " state (0)  A[0]:(0.53115260601) A[1]:(0.590528607368) A[2]:(0.590134978294) A[3]:(0.530963242054)\n",
      " state (1)  A[0]:(0.531532585621) A[1]:(0.000247377902269) A[2]:(0.656585097313) A[3]:(0.590364933014)\n",
      " state (2)  A[0]:(0.590285897255) A[1]:(0.729147791862) A[2]:(0.591112732887) A[3]:(0.656305074692)\n",
      " state (3)  A[0]:(0.655810773373) A[1]:(-0.00287285773084) A[2]:(0.489879757166) A[3]:(0.544785618782)\n",
      " state (4)  A[0]:(0.591356873512) A[1]:(0.65610063076) A[2]:(0.000158190727234) A[3]:(0.531349301338)\n",
      " state (5)  A[0]:(0.17158074677) A[1]:(0.925970196724) A[2]:(-0.157260164618) A[3]:(0.504690408707)\n",
      " state (6)  A[0]:(0.000362396211131) A[1]:(0.809970140457) A[2]:(-0.000370264024241) A[3]:(0.656062602997)\n",
      " state (7)  A[0]:(0.639213204384) A[1]:(-0.248528659344) A[2]:(0.20396655798) A[3]:(0.904922008514)\n",
      " state (8)  A[0]:(0.656139373779) A[1]:(1.51582062244e-05) A[2]:(0.729040622711) A[3]:(0.590585947037)\n",
      " state (9)  A[0]:(0.655999302864) A[1]:(0.810012698174) A[2]:(0.810001015663) A[3]:(-0.000133335590363)\n",
      " state (10)  A[0]:(0.728954136372) A[1]:(0.899997055531) A[2]:(-0.00020170211792) A[3]:(0.72891908884)\n",
      " state (11)  A[0]:(0.512962818146) A[1]:(0.876676380634) A[2]:(-0.560912966728) A[3]:(0.840059936047)\n",
      " state (12)  A[0]:(0.0676631778479) A[1]:(0.82360470295) A[2]:(-0.474646896124) A[3]:(0.78979831934)\n",
      " state (13)  A[0]:(-0.000571310461964) A[1]:(0.808292090893) A[2]:(0.899995326996) A[3]:(0.728980541229)\n",
      " state (14)  A[0]:(0.809955537319) A[1]:(0.900433838367) A[2]:(0.999999701977) A[3]:(0.810003995895)\n",
      " state (15)  A[0]:(0.990378379822) A[1]:(0.963428914547) A[2]:(1.0) A[3]:(0.896498858929)\n",
      "Episode 349000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6086. Times reached goal: 962.               Steps done: 3244878. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0350760220069.\n",
      "q_values \n",
      "tensor([[ 0.5320,  0.5904,  0.5905,  0.5298]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5905,  0.5905,  0.5301]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6563,  0.0007,  0.5311]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6555,  0.0003,  0.7286,  0.5908]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6554,  0.8103,  0.8102, -0.0007]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0005,  0.8085,  0.9005,  0.7287]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9004,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530922651291) A[1]:(0.590551733971) A[2]:(0.59049397707) A[3]:(0.532705545425)\n",
      " state (1)  A[0]:(0.531368136406) A[1]:(0.000496167631354) A[2]:(0.65612745285) A[3]:(0.591532111168)\n",
      " state (2)  A[0]:(0.589852631092) A[1]:(0.729102909565) A[2]:(0.590921878815) A[3]:(0.657022058964)\n",
      " state (3)  A[0]:(0.654954493046) A[1]:(-0.00537898065522) A[2]:(0.491275846958) A[3]:(0.545802474022)\n",
      " state (4)  A[0]:(0.590114474297) A[1]:(0.656475663185) A[2]:(0.000974535651039) A[3]:(0.532733976841)\n",
      " state (5)  A[0]:(0.169476985931) A[1]:(0.926244854927) A[2]:(-0.157454550266) A[3]:(0.5060942173)\n",
      " state (6)  A[0]:(-0.000590741576161) A[1]:(0.810223579407) A[2]:(-5.13792037964e-05) A[3]:(0.65701508522)\n",
      " state (7)  A[0]:(0.639209389687) A[1]:(-0.248478293419) A[2]:(0.205215543509) A[3]:(0.904968380928)\n",
      " state (8)  A[0]:(0.656133413315) A[1]:(0.000516820640769) A[2]:(0.7289275527) A[3]:(0.591269254684)\n",
      " state (9)  A[0]:(0.656200706959) A[1]:(0.810164690018) A[2]:(0.810025334358) A[3]:(0.000287890434265)\n",
      " state (10)  A[0]:(0.729172945023) A[1]:(0.900092840195) A[2]:(0.000110268592834) A[3]:(0.728874921799)\n",
      " state (11)  A[0]:(0.513414859772) A[1]:(0.876813173294) A[2]:(-0.560972929001) A[3]:(0.840003252029)\n",
      " state (12)  A[0]:(0.0683377236128) A[1]:(0.823828935623) A[2]:(-0.474980592728) A[3]:(0.789643704891)\n",
      " state (13)  A[0]:(0.000120967626572) A[1]:(0.80855089426) A[2]:(0.900126099586) A[3]:(0.728676259518)\n",
      " state (14)  A[0]:(0.81018358469) A[1]:(0.900551438332) A[2]:(0.999999701977) A[3]:(0.809724867344)\n",
      " state (15)  A[0]:(0.990361988544) A[1]:(0.963428974152) A[2]:(1.0) A[3]:(0.896227300167)\n",
      "Episode 350000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6117. Times reached goal: 968.               Steps done: 3250995. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0348621168762.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531154751778) A[1]:(0.590676546097) A[2]:(0.590463280678) A[3]:(0.531094670296)\n",
      " state (1)  A[0]:(0.53163421154) A[1]:(-0.000117935240269) A[2]:(0.655997514725) A[3]:(0.59023296833)\n",
      " state (2)  A[0]:(0.590402960777) A[1]:(0.728895068169) A[2]:(0.589864730835) A[3]:(0.656066000462)\n",
      " state (3)  A[0]:(0.655662715435) A[1]:(-2.88262963295e-05) A[2]:(0.489131867886) A[3]:(0.544675707817)\n",
      " state (4)  A[0]:(0.590686678886) A[1]:(0.655717134476) A[2]:(-0.000125408172607) A[3]:(0.530882358551)\n",
      " state (5)  A[0]:(0.170213103294) A[1]:(0.926023423672) A[2]:(-0.157987877727) A[3]:(0.504416465759)\n",
      " state (6)  A[0]:(-9.07480716705e-05) A[1]:(0.809874355793) A[2]:(-0.000286221504211) A[3]:(0.655874729156)\n",
      " state (7)  A[0]:(0.639241695404) A[1]:(-0.249093368649) A[2]:(0.205211311579) A[3]:(0.904605031013)\n",
      " state (8)  A[0]:(0.656301677227) A[1]:(-9.82955098152e-05) A[2]:(0.728721439838) A[3]:(0.59085226059)\n",
      " state (9)  A[0]:(0.656388401985) A[1]:(0.809928119183) A[2]:(0.809886515141) A[3]:(0.000560998858418)\n",
      " state (10)  A[0]:(0.729373753071) A[1]:(0.899957597256) A[2]:(-0.000276327133179) A[3]:(0.729150950909)\n",
      " state (11)  A[0]:(0.513925135136) A[1]:(0.876654744148) A[2]:(-0.561509370804) A[3]:(0.840239048004)\n",
      " state (12)  A[0]:(0.0692349746823) A[1]:(0.8235937953) A[2]:(-0.475879907608) A[3]:(0.789995551109)\n",
      " state (13)  A[0]:(0.00121223868337) A[1]:(0.808222830296) A[2]:(0.900204479694) A[3]:(0.729201436043)\n",
      " state (14)  A[0]:(0.810648322105) A[1]:(0.900274574757) A[2]:(0.999999701977) A[3]:(0.810245990753)\n",
      " state (15)  A[0]:(0.990354955196) A[1]:(0.963234007359) A[2]:(1.0) A[3]:(0.896494328976)\n",
      "Episode 351000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6099. Times reached goal: 971.               Steps done: 3257094. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0346501399062.\n",
      " state (0)  A[0]:(0.530846357346) A[1]:(0.590449213982) A[2]:(0.590692102909) A[3]:(0.531659066677)\n",
      " state (1)  A[0]:(0.531355023384) A[1]:(-0.000212181359529) A[2]:(0.656156301498) A[3]:(0.590569317341)\n",
      " state (2)  A[0]:(0.590271234512) A[1]:(0.728860259056) A[2]:(0.590247631073) A[3]:(0.656168818474)\n",
      " state (3)  A[0]:(0.65579777956) A[1]:(-0.00092114857398) A[2]:(0.489597946405) A[3]:(0.545096278191)\n",
      " state (4)  A[0]:(0.590928316116) A[1]:(0.655833482742) A[2]:(0.000198602676392) A[3]:(0.531338810921)\n",
      " state (5)  A[0]:(0.170590385795) A[1]:(0.926104307175) A[2]:(-0.157840132713) A[3]:(0.504687547684)\n",
      " state (6)  A[0]:(0.000853061443195) A[1]:(0.809903204441) A[2]:(3.62396240234e-05) A[3]:(0.655920743942)\n",
      " state (7)  A[0]:(0.639902412891) A[1]:(-0.249493330717) A[2]:(0.206050232053) A[3]:(0.904448032379)\n",
      " state (8)  A[0]:(0.656733095646) A[1]:(-0.000228215008974) A[2]:(0.728832483292) A[3]:(0.59028083086)\n",
      " state (9)  A[0]:(0.65659558773) A[1]:(0.809865236282) A[2]:(0.809904038906) A[3]:(-0.000273108482361)\n",
      " state (10)  A[0]:(0.729380369186) A[1]:(0.899945139885) A[2]:(-0.000217795372009) A[3]:(0.728702664375)\n",
      " state (11)  A[0]:(0.513834655285) A[1]:(0.876702010632) A[2]:(-0.561895191669) A[3]:(0.839930891991)\n",
      " state (12)  A[0]:(0.0688966736197) A[1]:(0.823753476143) A[2]:(-0.477009922266) A[3]:(0.789540946484)\n",
      " state (13)  A[0]:(0.000390768022044) A[1]:(0.808454036713) A[2]:(0.900001764297) A[3]:(0.728513836861)\n",
      " state (14)  A[0]:(0.810139775276) A[1]:(0.900387942791) A[2]:(0.999999701977) A[3]:(0.809634685516)\n",
      " state (15)  A[0]:(0.990286290646) A[1]:(0.963238358498) A[2]:(1.0) A[3]:(0.895996034145)\n",
      "Episode 352000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6104. Times reached goal: 964.               Steps done: 3263198. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0344392796524.\n",
      " state (0)  A[0]:(0.531626939774) A[1]:(0.590372681618) A[2]:(0.590557456017) A[3]:(0.531402051449)\n",
      " state (1)  A[0]:(0.531693935394) A[1]:(-0.000489890517201) A[2]:(0.655909836292) A[3]:(0.590313434601)\n",
      " state (2)  A[0]:(0.590505957603) A[1]:(0.728791058064) A[2]:(0.59009540081) A[3]:(0.655914723873)\n",
      " state (3)  A[0]:(0.655675053596) A[1]:(-0.00154116633348) A[2]:(0.489362359047) A[3]:(0.544952511787)\n",
      " state (4)  A[0]:(0.590497493744) A[1]:(0.655589103699) A[2]:(-0.000200390815735) A[3]:(0.531216800213)\n",
      " state (5)  A[0]:(0.169671297073) A[1]:(0.926057457924) A[2]:(-0.158343300223) A[3]:(0.504504740238)\n",
      " state (6)  A[0]:(-0.000512063445058) A[1]:(0.809797048569) A[2]:(-0.000482320756419) A[3]:(0.655753850937)\n",
      " state (7)  A[0]:(0.638876497746) A[1]:(-0.250093698502) A[2]:(0.205858334899) A[3]:(0.904433250427)\n",
      " state (8)  A[0]:(0.65591365099) A[1]:(-0.000818371598143) A[2]:(0.728444457054) A[3]:(0.590659499168)\n",
      " state (9)  A[0]:(0.655756115913) A[1]:(0.80976164341) A[2]:(0.809677600861) A[3]:(-0.000183522701263)\n",
      " state (10)  A[0]:(0.728629112244) A[1]:(0.899869859219) A[2]:(-0.000989198335446) A[3]:(0.728698790073)\n",
      " state (11)  A[0]:(0.51263654232) A[1]:(0.876597642899) A[2]:(-0.562941670418) A[3]:(0.839968383312)\n",
      " state (12)  A[0]:(0.0672437548637) A[1]:(0.82362395525) A[2]:(-0.478684484959) A[3]:(0.789615750313)\n",
      " state (13)  A[0]:(-0.00113320304081) A[1]:(0.808352649212) A[2]:(0.899923801422) A[3]:(0.728629231453)\n",
      " state (14)  A[0]:(0.809790611267) A[1]:(0.900343596935) A[2]:(0.999999701977) A[3]:(0.809738218784)\n",
      " state (15)  A[0]:(0.990234017372) A[1]:(0.963177680969) A[2]:(1.0) A[3]:(0.895920753479)\n",
      "Episode 353000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6095. Times reached goal: 965.               Steps done: 3269293. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0342300106381.\n",
      " state (0)  A[0]:(0.531515359879) A[1]:(0.590382397175) A[2]:(0.590419173241) A[3]:(0.531634569168)\n",
      " state (1)  A[0]:(0.5316593647) A[1]:(0.000173036009073) A[2]:(0.65599501133) A[3]:(0.590597271919)\n",
      " state (2)  A[0]:(0.5905148983) A[1]:(0.728954315186) A[2]:(0.59009206295) A[3]:(0.65618211031)\n",
      " state (3)  A[0]:(0.655806541443) A[1]:(-0.000140931457281) A[2]:(0.489371329546) A[3]:(0.54543697834)\n",
      " state (4)  A[0]:(0.590567708015) A[1]:(0.656137824059) A[2]:(4.52995300293e-05) A[3]:(0.531647205353)\n",
      " state (5)  A[0]:(0.169773817062) A[1]:(0.926216900349) A[2]:(-0.158079579473) A[3]:(0.504964947701)\n",
      " state (6)  A[0]:(0.000175774097443) A[1]:(0.810030221939) A[2]:(0.000154256820679) A[3]:(0.656083405018)\n",
      " state (7)  A[0]:(0.639273524284) A[1]:(-0.249707475305) A[2]:(0.20730677247) A[3]:(0.904358863831)\n",
      " state (8)  A[0]:(0.656018853188) A[1]:(-5.96903264523e-05) A[2]:(0.72918599844) A[3]:(0.590069711208)\n",
      " state (9)  A[0]:(0.655923962593) A[1]:(0.80998057127) A[2]:(0.81014919281) A[3]:(-0.000596582831349)\n",
      " state (10)  A[0]:(0.729023694992) A[1]:(0.900018334389) A[2]:(3.96966934204e-05) A[3]:(0.728846669197)\n",
      " state (11)  A[0]:(0.513611197472) A[1]:(0.876825273037) A[2]:(-0.562649905682) A[3]:(0.840183973312)\n",
      " state (12)  A[0]:(0.0687829852104) A[1]:(0.823972642422) A[2]:(-0.478697359562) A[3]:(0.789944350719)\n",
      " state (13)  A[0]:(0.00049275154015) A[1]:(0.808708310127) A[2]:(0.900279819965) A[3]:(0.729069054127)\n",
      " state (14)  A[0]:(0.810344934464) A[1]:(0.900473892689) A[2]:(0.999999701977) A[3]:(0.810110211372)\n",
      " state (15)  A[0]:(0.990217626095) A[1]:(0.96314239502) A[2]:(1.0) A[3]:(0.896015584469)\n",
      "Episode 354000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6106. Times reached goal: 965.               Steps done: 3275399. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0340216389991.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5904,  0.5904,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.0000,  0.6560,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7290,  0.5903,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8099, -0.0000,  0.6562]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0003,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9005,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531430542469) A[1]:(0.590403914452) A[2]:(0.590464651585) A[3]:(0.531634449959)\n",
      " state (1)  A[0]:(0.531481385231) A[1]:(6.49206340313e-05) A[2]:(0.656055927277) A[3]:(0.590602457523)\n",
      " state (2)  A[0]:(0.590352475643) A[1]:(0.728982925415) A[2]:(0.590172171593) A[3]:(0.656223595142)\n",
      " state (3)  A[0]:(0.655731201172) A[1]:(0.000235240906477) A[2]:(0.489205688238) A[3]:(0.545260429382)\n",
      " state (4)  A[0]:(0.590368390083) A[1]:(0.656082034111) A[2]:(-0.000218033790588) A[3]:(0.531395435333)\n",
      " state (5)  A[0]:(0.169331789017) A[1]:(0.926210284233) A[2]:(-0.158436387777) A[3]:(0.504843533039)\n",
      " state (6)  A[0]:(-0.000224232673645) A[1]:(0.809973955154) A[2]:(-0.000140190124512) A[3]:(0.656021654606)\n",
      " state (7)  A[0]:(0.63896971941) A[1]:(-0.249941244721) A[2]:(0.207361876965) A[3]:(0.904289186001)\n",
      " state (8)  A[0]:(0.655839562416) A[1]:(-3.2264739275e-05) A[2]:(0.728975653648) A[3]:(0.590066611767)\n",
      " state (9)  A[0]:(0.655782580376) A[1]:(0.809979975224) A[2]:(0.809993505478) A[3]:(-0.000645533087663)\n",
      " state (10)  A[0]:(0.728822231293) A[1]:(0.899993479252) A[2]:(-0.000237584114075) A[3]:(0.728645443916)\n",
      " state (11)  A[0]:(0.513262629509) A[1]:(0.8768004179) A[2]:(-0.563192367554) A[3]:(0.839993953705)\n",
      " state (12)  A[0]:(0.0682035833597) A[1]:(0.823961257935) A[2]:(-0.480031907558) A[3]:(0.789642691612)\n",
      " state (13)  A[0]:(-0.000444084376795) A[1]:(0.808679819107) A[2]:(0.899940550327) A[3]:(0.728599309921)\n",
      " state (14)  A[0]:(0.809895694256) A[1]:(0.900408267975) A[2]:(0.999999701977) A[3]:(0.809709310532)\n",
      " state (15)  A[0]:(0.990165054798) A[1]:(0.963073253632) A[2]:(1.0) A[3]:(0.89569234848)\n",
      "Episode 355000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6072. Times reached goal: 970.               Steps done: 3281471. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0338156855147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531277298927) A[1]:(0.590542018414) A[2]:(0.590567588806) A[3]:(0.5313808918)\n",
      " state (1)  A[0]:(0.531427145004) A[1]:(5.46164810658e-05) A[2]:(0.656152784824) A[3]:(0.590513408184)\n",
      " state (2)  A[0]:(0.590413212776) A[1]:(0.729008674622) A[2]:(0.590432226658) A[3]:(0.656204342842)\n",
      " state (3)  A[0]:(0.656020283699) A[1]:(5.87590038776e-05) A[2]:(0.489484101534) A[3]:(0.545298993587)\n",
      " state (4)  A[0]:(0.590761184692) A[1]:(0.656159281731) A[2]:(-1.72853469849e-05) A[3]:(0.531548082829)\n",
      " state (5)  A[0]:(0.169822350144) A[1]:(0.926239073277) A[2]:(-0.158379435539) A[3]:(0.505132496357)\n",
      " state (6)  A[0]:(0.000397771567805) A[1]:(0.81001240015) A[2]:(4.33921813965e-05) A[3]:(0.656341075897)\n",
      " state (7)  A[0]:(0.63938164711) A[1]:(-0.249877616763) A[2]:(0.208053231239) A[3]:(0.904393672943)\n",
      " state (8)  A[0]:(0.656361043453) A[1]:(9.18991863728e-05) A[2]:(0.729100227356) A[3]:(0.590745806694)\n",
      " state (9)  A[0]:(0.656321167946) A[1]:(0.809989273548) A[2]:(0.810072302818) A[3]:(0.000426709622843)\n",
      " state (10)  A[0]:(0.729258537292) A[1]:(0.9000030756) A[2]:(-5.57899475098e-05) A[3]:(0.729231774807)\n",
      " state (11)  A[0]:(0.514012336731) A[1]:(0.876828789711) A[2]:(-0.563477635384) A[3]:(0.840424954891)\n",
      " state (12)  A[0]:(0.0691999793053) A[1]:(0.824008226395) A[2]:(-0.480826348066) A[3]:(0.790225505829)\n",
      " state (13)  A[0]:(0.000468075246317) A[1]:(0.808692634106) A[2]:(0.900016129017) A[3]:(0.729331374168)\n",
      " state (14)  A[0]:(0.810190975666) A[1]:(0.900340735912) A[2]:(0.999999701977) A[3]:(0.810248494148)\n",
      " state (15)  A[0]:(0.990140557289) A[1]:(0.962967038155) A[2]:(1.0) A[3]:(0.895872473717)\n",
      "Episode 356000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6055. Times reached goal: 956.               Steps done: 3287526. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0336115501823.\n",
      " state (0)  A[0]:(0.531668901443) A[1]:(0.59051579237) A[2]:(0.590496897697) A[3]:(0.531243801117)\n",
      " state (1)  A[0]:(0.531626343727) A[1]:(2.23144888878e-05) A[2]:(0.656055212021) A[3]:(0.59046369791)\n",
      " state (2)  A[0]:(0.59053838253) A[1]:(0.729023694992) A[2]:(0.590251326561) A[3]:(0.656192302704)\n",
      " state (3)  A[0]:(0.655972778797) A[1]:(-0.00130239804275) A[2]:(0.4895388484) A[3]:(0.545219182968)\n",
      " state (4)  A[0]:(0.590544700623) A[1]:(0.656091451645) A[2]:(-7.92741775513e-05) A[3]:(0.53159737587)\n",
      " state (5)  A[0]:(0.169462531805) A[1]:(0.926229417324) A[2]:(-0.158541217446) A[3]:(0.505257248878)\n",
      " state (6)  A[0]:(0.000153869390488) A[1]:(0.809898257256) A[2]:(-8.02278518677e-05) A[3]:(0.656468510628)\n",
      " state (7)  A[0]:(0.639037132263) A[1]:(-0.250335365534) A[2]:(0.208278685808) A[3]:(0.904381155968)\n",
      " state (8)  A[0]:(0.655964136124) A[1]:(9.94727015495e-05) A[2]:(0.729047417641) A[3]:(0.590801596642)\n",
      " state (9)  A[0]:(0.655950903893) A[1]:(0.810018777847) A[2]:(0.810006797314) A[3]:(0.000612720788922)\n",
      " state (10)  A[0]:(0.728927850723) A[1]:(0.899966716766) A[2]:(-0.000160932540894) A[3]:(0.729189217091)\n",
      " state (11)  A[0]:(0.513505101204) A[1]:(0.876748800278) A[2]:(-0.563928484917) A[3]:(0.840348482132)\n",
      " state (12)  A[0]:(0.0684322565794) A[1]:(0.823866903782) A[2]:(-0.48210939765) A[3]:(0.790091097355)\n",
      " state (13)  A[0]:(-0.000641077640466) A[1]:(0.808471024036) A[2]:(0.899665772915) A[3]:(0.729102790356)\n",
      " state (14)  A[0]:(0.809704363346) A[1]:(0.900147736073) A[2]:(0.999999701977) A[3]:(0.810061514378)\n",
      " state (15)  A[0]:(0.990087747574) A[1]:(0.962842106819) A[2]:(1.0) A[3]:(0.895696043968)\n",
      "Episode 357000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6073. Times reached goal: 962.               Steps done: 3293599. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0334080458045.\n",
      " state (0)  A[0]:(0.531036496162) A[1]:(0.590127646923) A[2]:(0.59040004015) A[3]:(0.53173583746)\n",
      " state (1)  A[0]:(0.531226634979) A[1]:(0.000195484608412) A[2]:(0.655958175659) A[3]:(0.590601563454)\n",
      " state (2)  A[0]:(0.590345621109) A[1]:(0.728999912739) A[2]:(0.59013569355) A[3]:(0.656156659126)\n",
      " state (3)  A[0]:(0.655844867229) A[1]:(-0.00117354409304) A[2]:(0.489450186491) A[3]:(0.544989824295)\n",
      " state (4)  A[0]:(0.590512990952) A[1]:(0.656144440174) A[2]:(-0.000223278999329) A[3]:(0.53117787838)\n",
      " state (5)  A[0]:(0.169631838799) A[1]:(0.926283359528) A[2]:(-0.158826038241) A[3]:(0.504676878452)\n",
      " state (6)  A[0]:(0.000468969315989) A[1]:(0.810105681419) A[2]:(-0.000259280204773) A[3]:(0.655639588833)\n",
      " state (7)  A[0]:(0.639295399189) A[1]:(-0.249749332666) A[2]:(0.208561375737) A[3]:(0.903909564018)\n",
      " state (8)  A[0]:(0.656651735306) A[1]:(0.000449161947472) A[2]:(0.728863298893) A[3]:(0.589686989784)\n",
      " state (9)  A[0]:(0.656839728355) A[1]:(0.810092508793) A[2]:(0.80994194746) A[3]:(-0.000878050690517)\n",
      " state (10)  A[0]:(0.729700148106) A[1]:(0.900034725666) A[2]:(-0.000191330909729) A[3]:(0.728510022163)\n",
      " state (11)  A[0]:(0.514829516411) A[1]:(0.876880764961) A[2]:(-0.56430888176) A[3]:(0.839970946312)\n",
      " state (12)  A[0]:(0.0702741667628) A[1]:(0.824114322662) A[2]:(-0.482904791832) A[3]:(0.789627492428)\n",
      " state (13)  A[0]:(0.00125881959684) A[1]:(0.80878174305) A[2]:(0.899815618992) A[3]:(0.728521645069)\n",
      " state (14)  A[0]:(0.810404419899) A[1]:(0.900301814079) A[2]:(0.999999701977) A[3]:(0.809631586075)\n",
      " state (15)  A[0]:(0.990085184574) A[1]:(0.962839841843) A[2]:(1.0) A[3]:(0.895297884941)\n",
      "Episode 358000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6063. Times reached goal: 959.               Steps done: 3299662. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0332061056231.\n",
      " state (0)  A[0]:(0.531178236008) A[1]:(0.590404987335) A[2]:(0.590391397476) A[3]:(0.531009793282)\n",
      " state (1)  A[0]:(0.531205117702) A[1]:(-3.47718596458e-05) A[2]:(0.656073510647) A[3]:(0.590124964714)\n",
      " state (2)  A[0]:(0.590242922306) A[1]:(0.728989958763) A[2]:(0.590420544147) A[3]:(0.655853450298)\n",
      " state (3)  A[0]:(0.655950307846) A[1]:(0.000673301401548) A[2]:(0.489421993494) A[3]:(0.544853448868)\n",
      " state (4)  A[0]:(0.590576291084) A[1]:(0.656083226204) A[2]:(-3.32593917847e-05) A[3]:(0.531028032303)\n",
      " state (5)  A[0]:(0.169548243284) A[1]:(0.926260232925) A[2]:(-0.158709138632) A[3]:(0.50473511219)\n",
      " state (6)  A[0]:(0.00025400519371) A[1]:(0.809994220734) A[2]:(3.06367874146e-05) A[3]:(0.655856609344)\n",
      " state (7)  A[0]:(0.63895213604) A[1]:(-0.250300884247) A[2]:(0.209338203073) A[3]:(0.904023051262)\n",
      " state (8)  A[0]:(0.656209647655) A[1]:(-5.95301389694e-06) A[2]:(0.728996515274) A[3]:(0.590464532375)\n",
      " state (9)  A[0]:(0.656209111214) A[1]:(0.809991240501) A[2]:(0.810007572174) A[3]:(0.000233441591263)\n",
      " state (10)  A[0]:(0.729141175747) A[1]:(0.899988949299) A[2]:(-0.00012218952179) A[3]:(0.729076623917)\n",
      " state (11)  A[0]:(0.513989567757) A[1]:(0.876849770546) A[2]:(-0.564700245857) A[3]:(0.840367555618)\n",
      " state (12)  A[0]:(0.0690787881613) A[1]:(0.824114263058) A[2]:(-0.483848124743) A[3]:(0.790137410164)\n",
      " state (13)  A[0]:(-4.88758087158e-05) A[1]:(0.808819890022) A[2]:(0.899853825569) A[3]:(0.729141712189)\n",
      " state (14)  A[0]:(0.809967815876) A[1]:(0.900324821472) A[2]:(0.999999701977) A[3]:(0.810114860535)\n",
      " state (15)  A[0]:(0.99001955986) A[1]:(0.962802708149) A[2]:(1.0) A[3]:(0.895463943481)\n",
      "Episode 359000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6078. Times reached goal: 965.               Steps done: 3305740. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0330048910237.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5903,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.0001,  0.6561,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.7289,  0.5903,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  0.8099, -0.0001,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9001,  0.0001,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9003,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53132635355) A[1]:(0.590397119522) A[2]:(0.590441465378) A[3]:(0.531442284584)\n",
      " state (1)  A[0]:(0.531410813332) A[1]:(4.60669398308e-05) A[2]:(0.656034171581) A[3]:(0.59046959877)\n",
      " state (2)  A[0]:(0.590434670448) A[1]:(0.728894114494) A[2]:(0.590298175812) A[3]:(0.656143307686)\n",
      " state (3)  A[0]:(0.655966579914) A[1]:(-1.32694840431e-05) A[2]:(0.489479571581) A[3]:(0.545172333717)\n",
      " state (4)  A[0]:(0.590482115746) A[1]:(0.656022667885) A[2]:(-4.60147857666e-05) A[3]:(0.531416654587)\n",
      " state (5)  A[0]:(0.169336363673) A[1]:(0.926256358624) A[2]:(-0.158857181668) A[3]:(0.50515627861)\n",
      " state (6)  A[0]:(-7.12275505066e-05) A[1]:(0.809997975826) A[2]:(-1.66893005371e-06) A[3]:(0.656009793282)\n",
      " state (7)  A[0]:(0.638556241989) A[1]:(-0.250252962112) A[2]:(0.209853693843) A[3]:(0.90393435955)\n",
      " state (8)  A[0]:(0.655879259109) A[1]:(0.000103883445263) A[2]:(0.729082942009) A[3]:(0.590213656425)\n",
      " state (9)  A[0]:(0.655860066414) A[1]:(0.810027301311) A[2]:(0.810073435307) A[3]:(-0.000344380736351)\n",
      " state (10)  A[0]:(0.728870511055) A[1]:(0.900014281273) A[2]:(-2.25305557251e-05) A[3]:(0.728809893131)\n",
      " state (11)  A[0]:(0.513670086861) A[1]:(0.876903891563) A[2]:(-0.565082788467) A[3]:(0.840228378773)\n",
      " state (12)  A[0]:(0.0686794966459) A[1]:(0.824222445488) A[2]:(-0.484770357609) A[3]:(0.789951562881)\n",
      " state (13)  A[0]:(-0.000438749761088) A[1]:(0.808950662613) A[2]:(0.899948179722) A[3]:(0.728870153427)\n",
      " state (14)  A[0]:(0.809900999069) A[1]:(0.900373756886) A[2]:(0.999999701977) A[3]:(0.809895634651)\n",
      " state (15)  A[0]:(0.989972949028) A[1]:(0.962757766247) A[2]:(1.0) A[3]:(0.895175457001)\n",
      "Episode 360000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6108. Times reached goal: 980.               Steps done: 3311848. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0328039115655.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53126180172) A[1]:(0.590650320053) A[2]:(0.590613365173) A[3]:(0.53135830164)\n",
      " state (1)  A[0]:(0.531435668468) A[1]:(3.80352139473e-06) A[2]:(0.656259775162) A[3]:(0.590609490871)\n",
      " state (2)  A[0]:(0.590541124344) A[1]:(0.729084253311) A[2]:(0.590469658375) A[3]:(0.656391859055)\n",
      " state (3)  A[0]:(0.656225562096) A[1]:(-9.47825610638e-05) A[2]:(0.489458441734) A[3]:(0.545091509819)\n",
      " state (4)  A[0]:(0.590740919113) A[1]:(0.656227111816) A[2]:(-0.000234365463257) A[3]:(0.531380414963)\n",
      " state (5)  A[0]:(0.169548884034) A[1]:(0.92626285553) A[2]:(-0.159006267786) A[3]:(0.505387961864)\n",
      " state (6)  A[0]:(0.000114977359772) A[1]:(0.809993267059) A[2]:(-6.71148300171e-05) A[3]:(0.6562961936)\n",
      " state (7)  A[0]:(0.638866662979) A[1]:(-0.250428080559) A[2]:(0.210139840841) A[3]:(0.904035210609)\n",
      " state (8)  A[0]:(0.656467795372) A[1]:(-0.00020269677043) A[2]:(0.729001760483) A[3]:(0.590861916542)\n",
      " state (9)  A[0]:(0.656454205513) A[1]:(0.809923052788) A[2]:(0.809989213943) A[3]:(0.000455260247691)\n",
      " state (10)  A[0]:(0.729319453239) A[1]:(0.899953424931) A[2]:(-0.000288486480713) A[3]:(0.72918856144)\n",
      " state (11)  A[0]:(0.51436650753) A[1]:(0.876845240593) A[2]:(-0.565649867058) A[3]:(0.840491056442)\n",
      " state (12)  A[0]:(0.0695156678557) A[1]:(0.82417267561) A[2]:(-0.485836446285) A[3]:(0.790277600288)\n",
      " state (13)  A[0]:(0.000279664993286) A[1]:(0.80892264843) A[2]:(0.90001475811) A[3]:(0.729231715202)\n",
      " state (14)  A[0]:(0.810149788857) A[1]:(0.900350689888) A[2]:(0.999999701977) A[3]:(0.81012660265)\n",
      " state (15)  A[0]:(0.989941000938) A[1]:(0.962692737579) A[2]:(1.0) A[3]:(0.895137012005)\n",
      "Episode 361000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6083. Times reached goal: 969.               Steps done: 3317931. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0326049710624.\n",
      " state (0)  A[0]:(0.531497359276) A[1]:(0.590455651283) A[2]:(0.590515971184) A[3]:(0.531399726868)\n",
      " state (1)  A[0]:(0.531554877758) A[1]:(4.7579407692e-05) A[2]:(0.656100153923) A[3]:(0.59036052227)\n",
      " state (2)  A[0]:(0.59052747488) A[1]:(0.728980362415) A[2]:(0.590478658676) A[3]:(0.65597987175)\n",
      " state (3)  A[0]:(0.656047821045) A[1]:(-0.000122264027596) A[2]:(0.489670902491) A[3]:(0.545032858849)\n",
      " state (4)  A[0]:(0.590439200401) A[1]:(0.65610909462) A[2]:(-1.32322311401e-05) A[3]:(0.531373381615)\n",
      " state (5)  A[0]:(0.16911585629) A[1]:(0.926290929317) A[2]:(-0.159082368016) A[3]:(0.50526368618)\n",
      " state (6)  A[0]:(-0.000183045864105) A[1]:(0.810050368309) A[2]:(-3.93390655518e-06) A[3]:(0.655984997749)\n",
      " state (7)  A[0]:(0.638493537903) A[1]:(-0.250252097845) A[2]:(0.210775211453) A[3]:(0.903770208359)\n",
      " state (8)  A[0]:(0.656084001064) A[1]:(0.000125750899315) A[2]:(0.728991389275) A[3]:(0.590314984322)\n",
      " state (9)  A[0]:(0.656012296677) A[1]:(0.810044169426) A[2]:(0.809982180595) A[3]:(-0.000299260020256)\n",
      " state (10)  A[0]:(0.72899389267) A[1]:(0.900035977364) A[2]:(-0.000313639640808) A[3]:(0.728771388531)\n",
      " state (11)  A[0]:(0.514036297798) A[1]:(0.876950740814) A[2]:(-0.566101551056) A[3]:(0.840240299702)\n",
      " state (12)  A[0]:(0.0691855326295) A[1]:(0.824299752712) A[2]:(-0.486880630255) A[3]:(0.789954423904)\n",
      " state (13)  A[0]:(-5.71608543396e-05) A[1]:(0.809002757072) A[2]:(0.900006830692) A[3]:(0.728850722313)\n",
      " state (14)  A[0]:(0.810044586658) A[1]:(0.90033864975) A[2]:(0.999999701977) A[3]:(0.809960782528)\n",
      " state (15)  A[0]:(0.989893615246) A[1]:(0.962616860867) A[2]:(1.0) A[3]:(0.894992887974)\n",
      "Episode 362000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6109. Times reached goal: 968.               Steps done: 3324040. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.032406394464.\n",
      " state (0)  A[0]:(0.532018661499) A[1]:(0.590394258499) A[2]:(0.590510964394) A[3]:(0.530855774879)\n",
      " state (1)  A[0]:(0.531922698021) A[1]:(0.000213213264942) A[2]:(0.656185805798) A[3]:(0.590033590794)\n",
      " state (2)  A[0]:(0.590818047523) A[1]:(0.7291213274) A[2]:(0.590570569038) A[3]:(0.65579611063)\n",
      " state (3)  A[0]:(0.656090021133) A[1]:(0.000516064406838) A[2]:(0.489790260792) A[3]:(0.544727742672)\n",
      " state (4)  A[0]:(0.590346693993) A[1]:(0.656310975552) A[2]:(0.000287890434265) A[3]:(0.531072616577)\n",
      " state (5)  A[0]:(0.169057607651) A[1]:(0.926322698593) A[2]:(-0.158755272627) A[3]:(0.50514870882)\n",
      " state (6)  A[0]:(-0.000165551900864) A[1]:(0.810046195984) A[2]:(0.000476479501231) A[3]:(0.656106948853)\n",
      " state (7)  A[0]:(0.638353466988) A[1]:(-0.250419348478) A[2]:(0.211611181498) A[3]:(0.903896272182)\n",
      " state (8)  A[0]:(0.656162858009) A[1]:(0.000429846317274) A[2]:(0.729198932648) A[3]:(0.591415047646)\n",
      " state (9)  A[0]:(0.656333506107) A[1]:(0.810221076012) A[2]:(0.810121417046) A[3]:(0.00215102406219)\n",
      " state (10)  A[0]:(0.729323267937) A[1]:(0.900093138218) A[2]:(0.000352263421519) A[3]:(0.730048418045)\n",
      " state (11)  A[0]:(0.514687895775) A[1]:(0.877013742924) A[2]:(-0.565909266472) A[3]:(0.841088950634)\n",
      " state (12)  A[0]:(0.0701707601547) A[1]:(0.824423015118) A[2]:(-0.487149149179) A[3]:(0.791072368622)\n",
      " state (13)  A[0]:(0.000994503148831) A[1]:(0.809185862541) A[2]:(0.900158524513) A[3]:(0.730208992958)\n",
      " state (14)  A[0]:(0.810519456863) A[1]:(0.900460660458) A[2]:(0.999999761581) A[3]:(0.810866892338)\n",
      " state (15)  A[0]:(0.989890038967) A[1]:(0.96263229847) A[2]:(1.0) A[3]:(0.895320653915)\n",
      "Episode 363000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6043. Times reached goal: 960.               Steps done: 3330083. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.032211153138.\n",
      " state (0)  A[0]:(0.531747937202) A[1]:(0.590491771698) A[2]:(0.590613484383) A[3]:(0.531651556492)\n",
      " state (1)  A[0]:(0.531859874725) A[1]:(-0.00013479590416) A[2]:(0.655991077423) A[3]:(0.590682029724)\n",
      " state (2)  A[0]:(0.590850234032) A[1]:(0.729010939598) A[2]:(0.590456366539) A[3]:(0.656270742416)\n",
      " state (3)  A[0]:(0.656323432922) A[1]:(-0.000713258865289) A[2]:(0.489872425795) A[3]:(0.545147776604)\n",
      " state (4)  A[0]:(0.590713977814) A[1]:(0.656176924706) A[2]:(2.43186950684e-05) A[3]:(0.531543374062)\n",
      " state (5)  A[0]:(0.169501215219) A[1]:(0.926301419735) A[2]:(-0.159376084805) A[3]:(0.505622804165)\n",
      " state (6)  A[0]:(0.000368028850062) A[1]:(0.809958815575) A[2]:(-0.000269532203674) A[3]:(0.656290888786)\n",
      " state (7)  A[0]:(0.638662338257) A[1]:(-0.250576138496) A[2]:(0.211212947965) A[3]:(0.903772711754)\n",
      " state (8)  A[0]:(0.656299710274) A[1]:(0.000289857387543) A[2]:(0.728996157646) A[3]:(0.590546369553)\n",
      " state (9)  A[0]:(0.656200051308) A[1]:(0.810049176216) A[2]:(0.810000360012) A[3]:(0.000342786312103)\n",
      " state (10)  A[0]:(0.729005336761) A[1]:(0.899951815605) A[2]:(-0.000126719474792) A[3]:(0.729044318199)\n",
      " state (11)  A[0]:(0.514042377472) A[1]:(0.876807034016) A[2]:(-0.566639780998) A[3]:(0.840394437313)\n",
      " state (12)  A[0]:(0.069131731987) A[1]:(0.824076414108) A[2]:(-0.488535434008) A[3]:(0.790119230747)\n",
      " state (13)  A[0]:(-0.000246047973633) A[1]:(0.808694779873) A[2]:(0.899916410446) A[3]:(0.729006052017)\n",
      " state (14)  A[0]:(0.810080885887) A[1]:(0.900072216988) A[2]:(0.999999761581) A[3]:(0.810111343861)\n",
      " state (15)  A[0]:(0.989838719368) A[1]:(0.962400376797) A[2]:(1.0) A[3]:(0.894904851913)\n",
      "Episode 364000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6085. Times reached goal: 969.               Steps done: 3336168. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0320157434082.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5904,  0.5904,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.6561,  0.0002,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.0000,  0.7289,  0.5906]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.8100,  0.8100, -0.0003]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0001,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9003,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531596720219) A[1]:(0.590476632118) A[2]:(0.590433120728) A[3]:(0.531597137451)\n",
      " state (1)  A[0]:(0.53170144558) A[1]:(4.84958291054e-05) A[2]:(0.65608382225) A[3]:(0.590600728989)\n",
      " state (2)  A[0]:(0.590737819672) A[1]:(0.729015350342) A[2]:(0.590437293053) A[3]:(0.656197190285)\n",
      " state (3)  A[0]:(0.656221747398) A[1]:(-0.000193886458874) A[2]:(0.48965677619) A[3]:(0.545064687729)\n",
      " state (4)  A[0]:(0.590601682663) A[1]:(0.656097173691) A[2]:(-0.000135183334351) A[3]:(0.531379580498)\n",
      " state (5)  A[0]:(0.1693918854) A[1]:(0.926285743713) A[2]:(-0.159447535872) A[3]:(0.505426108837)\n",
      " state (6)  A[0]:(-2.19643115997e-05) A[1]:(0.810039281845) A[2]:(-0.000121116638184) A[3]:(0.655903995037)\n",
      " state (7)  A[0]:(0.638242006302) A[1]:(-0.250383347273) A[2]:(0.211808398366) A[3]:(0.903543531895)\n",
      " state (8)  A[0]:(0.656180620193) A[1]:(1.53854489326e-05) A[2]:(0.728989005089) A[3]:(0.590183615685)\n",
      " state (9)  A[0]:(0.656165003777) A[1]:(0.80998390913) A[2]:(0.810021221638) A[3]:(-0.000498250068631)\n",
      " state (10)  A[0]:(0.729101896286) A[1]:(0.899993956089) A[2]:(-0.000167489051819) A[3]:(0.728786468506)\n",
      " state (11)  A[0]:(0.514373362064) A[1]:(0.876949429512) A[2]:(-0.567039310932) A[3]:(0.840334773064)\n",
      " state (12)  A[0]:(0.0695917755365) A[1]:(0.824384331703) A[2]:(-0.489278376102) A[3]:(0.790057301521)\n",
      " state (13)  A[0]:(9.33408737183e-05) A[1]:(0.809118807316) A[2]:(0.900047183037) A[3]:(0.728864192963)\n",
      " state (14)  A[0]:(0.81011813879) A[1]:(0.900321602821) A[2]:(0.999999761581) A[3]:(0.809922337532)\n",
      " state (15)  A[0]:(0.989793717861) A[1]:(0.962454795837) A[2]:(1.0) A[3]:(0.894605517387)\n",
      "Episode 365000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6123. Times reached goal: 975.               Steps done: 3342291. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0318203099415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531544625759) A[1]:(0.590454876423) A[2]:(0.590454757214) A[3]:(0.531343638897)\n",
      " state (1)  A[0]:(0.5316644907) A[1]:(0.000152714550495) A[2]:(0.656157195568) A[3]:(0.590408146381)\n",
      " state (2)  A[0]:(0.590704381466) A[1]:(0.729081690311) A[2]:(0.590673685074) A[3]:(0.656093895435)\n",
      " state (3)  A[0]:(0.656239032745) A[1]:(0.000416703493102) A[2]:(0.489876866341) A[3]:(0.545027732849)\n",
      " state (4)  A[0]:(0.590652108192) A[1]:(0.656210422516) A[2]:(0.000120162963867) A[3]:(0.531457781792)\n",
      " state (5)  A[0]:(0.169517531991) A[1]:(0.926330983639) A[2]:(-0.159325093031) A[3]:(0.505680441856)\n",
      " state (6)  A[0]:(0.000190675258636) A[1]:(0.810138225555) A[2]:(0.000146746635437) A[3]:(0.656072318554)\n",
      " state (7)  A[0]:(0.638268470764) A[1]:(-0.250108093023) A[2]:(0.21254530549) A[3]:(0.903514385223)\n",
      " state (8)  A[0]:(0.65624409914) A[1]:(0.000391639739973) A[2]:(0.729152619839) A[3]:(0.590390384197)\n",
      " state (9)  A[0]:(0.656218707561) A[1]:(0.810095787048) A[2]:(0.81009221077) A[3]:(8.17179679871e-05)\n",
      " state (10)  A[0]:(0.729164361954) A[1]:(0.900055348873) A[2]:(1.20401382446e-05) A[3]:(0.729047060013)\n",
      " state (11)  A[0]:(0.514568567276) A[1]:(0.877032876015) A[2]:(-0.567254185677) A[3]:(0.840508282185)\n",
      " state (12)  A[0]:(0.0698794797063) A[1]:(0.824502825737) A[2]:(-0.489978522062) A[3]:(0.790277898312)\n",
      " state (13)  A[0]:(0.000302881002426) A[1]:(0.809219896793) A[2]:(0.900059819221) A[3]:(0.729124486446)\n",
      " state (14)  A[0]:(0.810191452503) A[1]:(0.900336265564) A[2]:(0.999999761581) A[3]:(0.810126781464)\n",
      " state (15)  A[0]:(0.989764392376) A[1]:(0.96240490675) A[2]:(1.0) A[3]:(0.89462864399)\n",
      "Episode 366000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6030. Times reached goal: 964.               Steps done: 3348321. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0316290108191.\n",
      " state (0)  A[0]:(0.531621932983) A[1]:(0.590768098831) A[2]:(0.590756654739) A[3]:(0.531627595425)\n",
      " state (1)  A[0]:(0.531672954559) A[1]:(7.55675137043e-05) A[2]:(0.656358599663) A[3]:(0.590636849403)\n",
      " state (2)  A[0]:(0.590753376484) A[1]:(0.729194104671) A[2]:(0.590900540352) A[3]:(0.656291484833)\n",
      " state (3)  A[0]:(0.656398534775) A[1]:(-0.000428210914833) A[2]:(0.490210831165) A[3]:(0.545104444027)\n",
      " state (4)  A[0]:(0.590878367424) A[1]:(0.656532406807) A[2]:(0.000287890434265) A[3]:(0.531651854515)\n",
      " state (5)  A[0]:(0.169754251838) A[1]:(0.926353752613) A[2]:(-0.159133955836) A[3]:(0.505930364132)\n",
      " state (6)  A[0]:(0.000311017036438) A[1]:(0.810146987438) A[2]:(0.000449418992503) A[3]:(0.656313121319)\n",
      " state (7)  A[0]:(0.638351023197) A[1]:(-0.250071555376) A[2]:(0.213230416179) A[3]:(0.903625488281)\n",
      " state (8)  A[0]:(0.656529009342) A[1]:(0.000613350362983) A[2]:(0.729399085045) A[3]:(0.590748846531)\n",
      " state (9)  A[0]:(0.656551003456) A[1]:(0.810228466988) A[2]:(0.81022387743) A[3]:(0.000277623534203)\n",
      " state (10)  A[0]:(0.729352116585) A[1]:(0.900119185448) A[2]:(0.000310659408569) A[3]:(0.729160904884)\n",
      " state (11)  A[0]:(0.51478266716) A[1]:(0.877113580704) A[2]:(-0.567363142967) A[3]:(0.840603411198)\n",
      " state (12)  A[0]:(0.0700526162982) A[1]:(0.824635207653) A[2]:(-0.490452975035) A[3]:(0.790401816368)\n",
      " state (13)  A[0]:(0.00045263764332) A[1]:(0.809386491776) A[2]:(0.900200843811) A[3]:(0.72925555706)\n",
      " state (14)  A[0]:(0.810339748859) A[1]:(0.900428414345) A[2]:(0.999999761581) A[3]:(0.810197114944)\n",
      " state (15)  A[0]:(0.989742934704) A[1]:(0.962400376797) A[2]:(1.0) A[3]:(0.894532561302)\n",
      "Episode 367000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6048. Times reached goal: 964.               Steps done: 3354369. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.031438295865.\n",
      " state (0)  A[0]:(0.531556248665) A[1]:(0.590424776077) A[2]:(0.590457737446) A[3]:(0.531511247158)\n",
      " state (1)  A[0]:(0.53152859211) A[1]:(-6.49727880955e-05) A[2]:(0.656085014343) A[3]:(0.590406119823)\n",
      " state (2)  A[0]:(0.5904687047) A[1]:(0.72898876667) A[2]:(0.590771913528) A[3]:(0.656002044678)\n",
      " state (3)  A[0]:(0.656014680862) A[1]:(0.000102963298559) A[2]:(0.490097910166) A[3]:(0.545047044754)\n",
      " state (4)  A[0]:(0.590409398079) A[1]:(0.656124949455) A[2]:(0.000240683555603) A[3]:(0.531585812569)\n",
      " state (5)  A[0]:(0.169250145555) A[1]:(0.926309466362) A[2]:(-0.159460559487) A[3]:(0.5058811903)\n",
      " state (6)  A[0]:(-6.24656677246e-05) A[1]:(0.810017824173) A[2]:(0.000118970870972) A[3]:(0.656066656113)\n",
      " state (7)  A[0]:(0.637849509716) A[1]:(-0.250575393438) A[2]:(0.213101491332) A[3]:(0.903375148773)\n",
      " state (8)  A[0]:(0.656020104885) A[1]:(0.00015988573432) A[2]:(0.728978157043) A[3]:(0.590432167053)\n",
      " state (9)  A[0]:(0.655996918678) A[1]:(0.810085535049) A[2]:(0.810015022755) A[3]:(2.13086605072e-05)\n",
      " state (10)  A[0]:(0.728970170021) A[1]:(0.900027692318) A[2]:(-3.91006469727e-05) A[3]:(0.728993952274)\n",
      " state (11)  A[0]:(0.514404296875) A[1]:(0.877000331879) A[2]:(-0.567893326283) A[3]:(0.840503573418)\n",
      " state (12)  A[0]:(0.0696994885802) A[1]:(0.824490010738) A[2]:(-0.491532385349) A[3]:(0.790248990059)\n",
      " state (13)  A[0]:(2.57790088654e-05) A[1]:(0.809236705303) A[2]:(0.900035202503) A[3]:(0.729001760483)\n",
      " state (14)  A[0]:(0.81012904644) A[1]:(0.900342404842) A[2]:(0.999999761581) A[3]:(0.809986829758)\n",
      " state (15)  A[0]:(0.989699423313) A[1]:(0.962338268757) A[2]:(1.0) A[3]:(0.894315600395)\n",
      "Episode 368000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6072. Times reached goal: 968.               Steps done: 3360441. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0312479809134.\n",
      " state (0)  A[0]:(0.531523823738) A[1]:(0.590544104576) A[2]:(0.590580701828) A[3]:(0.531396090984)\n",
      " state (1)  A[0]:(0.53160238266) A[1]:(1.8235296011e-05) A[2]:(0.656172633171) A[3]:(0.590512275696)\n",
      " state (2)  A[0]:(0.590679883957) A[1]:(0.729015946388) A[2]:(0.590533077717) A[3]:(0.65620970726)\n",
      " state (3)  A[0]:(0.656258165836) A[1]:(7.10152089596e-05) A[2]:(0.489866912365) A[3]:(0.544949710369)\n",
      " state (4)  A[0]:(0.590716183186) A[1]:(0.656180024147) A[2]:(1.1682510376e-05) A[3]:(0.531431376934)\n",
      " state (5)  A[0]:(0.16971847415) A[1]:(0.926294326782) A[2]:(-0.15968349576) A[3]:(0.505896687508)\n",
      " state (6)  A[0]:(0.00026261806488) A[1]:(0.810026347637) A[2]:(-7.34329223633e-05) A[3]:(0.656134963036)\n",
      " state (7)  A[0]:(0.638021588326) A[1]:(-0.25052651763) A[2]:(0.21323736012) A[3]:(0.903393924236)\n",
      " state (8)  A[0]:(0.65633392334) A[1]:(0.000141348689795) A[2]:(0.728975772858) A[3]:(0.590574622154)\n",
      " state (9)  A[0]:(0.656278252602) A[1]:(0.810066461563) A[2]:(0.809976756573) A[3]:(6.27487897873e-05)\n",
      " state (10)  A[0]:(0.729110360146) A[1]:(0.900000154972) A[2]:(-0.000209450721741) A[3]:(0.728974223137)\n",
      " state (11)  A[0]:(0.514545321465) A[1]:(0.876965403557) A[2]:(-0.568299472332) A[3]:(0.84049731493)\n",
      " state (12)  A[0]:(0.0697680860758) A[1]:(0.824451446533) A[2]:(-0.492333173752) A[3]:(0.790248990059)\n",
      " state (13)  A[0]:(2.121925354e-05) A[1]:(0.809202253819) A[2]:(0.900076985359) A[3]:(0.729013383389)\n",
      " state (14)  A[0]:(0.810174584389) A[1]:(0.900313317776) A[2]:(0.999999761581) A[3]:(0.810031235218)\n",
      " state (15)  A[0]:(0.989670097828) A[1]:(0.962283670902) A[2]:(1.0) A[3]:(0.894252181053)\n",
      "Episode 369000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6091. Times reached goal: 972.               Steps done: 3366532. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.031058227941.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5908,  0.5906,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6563, -0.0001,  0.5317]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561, -0.0005,  0.7291,  0.5909]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6563,  0.8099,  0.8100,  0.0003]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7294,  0.9000, -0.0001,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9004,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531561493874) A[1]:(0.590798139572) A[2]:(0.59059882164) A[3]:(0.531688034534)\n",
      " state (1)  A[0]:(0.531481385231) A[1]:(3.50624322891e-05) A[2]:(0.656286716461) A[3]:(0.590579509735)\n",
      " state (2)  A[0]:(0.590498447418) A[1]:(0.729062080383) A[2]:(0.590637207031) A[3]:(0.656204819679)\n",
      " state (3)  A[0]:(0.65614426136) A[1]:(0.000315949320793) A[2]:(0.48990213871) A[3]:(0.545134186745)\n",
      " state (4)  A[0]:(0.59052836895) A[1]:(0.656324386597) A[2]:(2.01463699341e-05) A[3]:(0.531688690186)\n",
      " state (5)  A[0]:(0.169272810221) A[1]:(0.926284492016) A[2]:(-0.159612163901) A[3]:(0.506133377552)\n",
      " state (6)  A[0]:(-0.000396877498133) A[1]:(0.810006022453) A[2]:(0.000258088111877) A[3]:(0.656224727631)\n",
      " state (7)  A[0]:(0.637635111809) A[1]:(-0.250665515661) A[2]:(0.214161708951) A[3]:(0.903399407864)\n",
      " state (8)  A[0]:(0.656290769577) A[1]:(-0.000387087435229) A[2]:(0.729219079018) A[3]:(0.590841174126)\n",
      " state (9)  A[0]:(0.65639090538) A[1]:(0.809905648232) A[2]:(0.810124576092) A[3]:(7.23302364349e-05)\n",
      " state (10)  A[0]:(0.729345440865) A[1]:(0.899979531765) A[2]:(1.31130218506e-05) A[3]:(0.729122281075)\n",
      " state (11)  A[0]:(0.515081942081) A[1]:(0.877011477947) A[2]:(-0.568546354771) A[3]:(0.840691030025)\n",
      " state (12)  A[0]:(0.0704628005624) A[1]:(0.824586212635) A[2]:(-0.493022143841) A[3]:(0.79051566124)\n",
      " state (13)  A[0]:(0.000462651223643) A[1]:(0.809386610985) A[2]:(0.900088131428) A[3]:(0.729286670685)\n",
      " state (14)  A[0]:(0.810192227364) A[1]:(0.900405704975) A[2]:(0.999999761581) A[3]:(0.81016176939)\n",
      " state (15)  A[0]:(0.98963034153) A[1]:(0.962278664112) A[2]:(1.0) A[3]:(0.894181311131)\n",
      "Episode 370000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6101. Times reached goal: 975.               Steps done: 3372633. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0308693185464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531401634216) A[1]:(0.590524792671) A[2]:(0.590512275696) A[3]:(0.531538665295)\n",
      " state (1)  A[0]:(0.531690776348) A[1]:(0.00022654607892) A[2]:(0.656063318253) A[3]:(0.590524077415)\n",
      " state (2)  A[0]:(0.590561211109) A[1]:(0.728962659836) A[2]:(0.590936481953) A[3]:(0.656026244164)\n",
      " state (3)  A[0]:(0.656278014183) A[1]:(-0.00711674522609) A[2]:(0.491943985224) A[3]:(0.5442456007)\n",
      " state (4)  A[0]:(0.590934813023) A[1]:(0.656087458134) A[2]:(0.000368237466319) A[3]:(0.531454980373)\n",
      " state (5)  A[0]:(0.169718533754) A[1]:(0.926384747028) A[2]:(-0.160718277097) A[3]:(0.50600361824)\n",
      " state (6)  A[0]:(0.000469714374049) A[1]:(0.810000121593) A[2]:(-0.000775575463194) A[3]:(0.65601670742)\n",
      " state (7)  A[0]:(0.638099789619) A[1]:(-0.250747829676) A[2]:(0.214187413454) A[3]:(0.903141498566)\n",
      " state (8)  A[0]:(0.656409621239) A[1]:(3.59490513802e-06) A[2]:(0.728959202766) A[3]:(0.590330481529)\n",
      " state (9)  A[0]:(0.65647816658) A[1]:(0.810010552406) A[2]:(0.809979617596) A[3]:(-0.000320732593536)\n",
      " state (10)  A[0]:(0.72937810421) A[1]:(0.900015413761) A[2]:(-0.000227093696594) A[3]:(0.728843212128)\n",
      " state (11)  A[0]:(0.515175640583) A[1]:(0.877023756504) A[2]:(-0.568942308426) A[3]:(0.840491652489)\n",
      " state (12)  A[0]:(0.0706688985229) A[1]:(0.824561834335) A[2]:(-0.493793785572) A[3]:(0.790227293968)\n",
      " state (13)  A[0]:(0.00073686230462) A[1]:(0.809304773808) A[2]:(0.900031685829) A[3]:(0.728887081146)\n",
      " state (14)  A[0]:(0.810323774815) A[1]:(0.900313735008) A[2]:(0.999999761581) A[3]:(0.809856116772)\n",
      " state (15)  A[0]:(0.989616274834) A[1]:(0.962196528912) A[2]:(1.0) A[3]:(0.89392632246)\n",
      "Episode 371000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6080. Times reached goal: 969.               Steps done: 3378713. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0306822024989.\n",
      " state (0)  A[0]:(0.532117128372) A[1]:(0.590141236782) A[2]:(0.590873241425) A[3]:(0.531408905983)\n",
      " state (1)  A[0]:(0.532342731953) A[1]:(0.000155296176672) A[2]:(0.656734883785) A[3]:(0.590858578682)\n",
      " state (2)  A[0]:(0.590687036514) A[1]:(0.728387892246) A[2]:(0.594218254089) A[3]:(0.655938327312)\n",
      " state (3)  A[0]:(0.655568242073) A[1]:(-0.0288373157382) A[2]:(0.499985903502) A[3]:(0.541985154152)\n",
      " state (4)  A[0]:(0.590583443642) A[1]:(0.655750095844) A[2]:(0.00573914917186) A[3]:(0.530950069427)\n",
      " state (5)  A[0]:(0.169564574957) A[1]:(0.926654875278) A[2]:(-0.157830834389) A[3]:(0.506015956402)\n",
      " state (6)  A[0]:(0.00321688130498) A[1]:(0.809552073479) A[2]:(0.00212084921077) A[3]:(0.656659841537)\n",
      " state (7)  A[0]:(0.639837622643) A[1]:(-0.252486765385) A[2]:(0.217027172446) A[3]:(0.903127074242)\n",
      " state (8)  A[0]:(0.656597435474) A[1]:(7.67074525356e-05) A[2]:(0.728745818138) A[3]:(0.590854644775)\n",
      " state (9)  A[0]:(0.656189084053) A[1]:(0.810023903847) A[2]:(0.809840023518) A[3]:(-0.000783592287917)\n",
      " state (10)  A[0]:(0.728799641132) A[1]:(0.899977445602) A[2]:(-0.000544071139302) A[3]:(0.728129863739)\n",
      " state (11)  A[0]:(0.513953447342) A[1]:(0.876876056194) A[2]:(-0.569636821747) A[3]:(0.840003371239)\n",
      " state (12)  A[0]:(0.0687906071544) A[1]:(0.824241101742) A[2]:(-0.494687378407) A[3]:(0.789552271366)\n",
      " state (13)  A[0]:(-0.00083506089868) A[1]:(0.808912217617) A[2]:(0.90034776926) A[3]:(0.728077590466)\n",
      " state (14)  A[0]:(0.810039043427) A[1]:(0.900110006332) A[2]:(0.999999761581) A[3]:(0.809394955635)\n",
      " state (15)  A[0]:(0.989561378956) A[1]:(0.962067425251) A[2]:(1.0) A[3]:(0.893590629101)\n",
      "Episode 372000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6089. Times reached goal: 971.               Steps done: 3384802. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0304959462006.\n",
      " state (0)  A[0]:(0.53215098381) A[1]:(0.59046626091) A[2]:(0.59028083086) A[3]:(0.531242609024)\n",
      " state (1)  A[0]:(0.532215893269) A[1]:(4.34964895248e-05) A[2]:(0.656014442444) A[3]:(0.590162038803)\n",
      " state (2)  A[0]:(0.590952217579) A[1]:(0.729019999504) A[2]:(0.590792298317) A[3]:(0.655863344669)\n",
      " state (3)  A[0]:(0.656496524811) A[1]:(-0.0087954364717) A[2]:(0.492874503136) A[3]:(0.543658554554)\n",
      " state (4)  A[0]:(0.591231226921) A[1]:(0.656045496464) A[2]:(-3.32593917847e-05) A[3]:(0.531392455101)\n",
      " state (5)  A[0]:(0.169749617577) A[1]:(0.926486909389) A[2]:(-0.162136122584) A[3]:(0.506420552731)\n",
      " state (6)  A[0]:(0.000749826314859) A[1]:(0.809956610203) A[2]:(-0.000730633619241) A[3]:(0.65589427948)\n",
      " state (7)  A[0]:(0.638367831707) A[1]:(-0.250664055347) A[2]:(0.216550469398) A[3]:(0.902619838715)\n",
      " state (8)  A[0]:(0.65646559) A[1]:(3.2976269722e-05) A[2]:(0.728947758675) A[3]:(0.590331435204)\n",
      " state (9)  A[0]:(0.656283557415) A[1]:(0.809982657433) A[2]:(0.809949457645) A[3]:(-4.86969947815e-05)\n",
      " state (10)  A[0]:(0.729122757912) A[1]:(0.899989426136) A[2]:(-0.000291466712952) A[3]:(0.728994250298)\n",
      " state (11)  A[0]:(0.514784812927) A[1]:(0.876944005489) A[2]:(-0.569511771202) A[3]:(0.840681552887)\n",
      " state (12)  A[0]:(0.0700704082847) A[1]:(0.82438147068) A[2]:(-0.494953751564) A[3]:(0.790437340736)\n",
      " state (13)  A[0]:(7.43567943573e-05) A[1]:(0.809038937092) A[2]:(0.899996519089) A[3]:(0.729096353054)\n",
      " state (14)  A[0]:(0.810159683228) A[1]:(0.900144040585) A[2]:(0.999999761581) A[3]:(0.810088813305)\n",
      " state (15)  A[0]:(0.989569664001) A[1]:(0.962076604366) A[2]:(1.0) A[3]:(0.894010365009)\n",
      "Episode 373000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6070. Times reached goal: 967.               Steps done: 3390872. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0303113964822.\n",
      " state (0)  A[0]:(0.531183719635) A[1]:(0.590288162231) A[2]:(0.59063243866) A[3]:(0.531611084938)\n",
      " state (1)  A[0]:(0.531450986862) A[1]:(0.000437423557742) A[2]:(0.656063735485) A[3]:(0.590589642525)\n",
      " state (2)  A[0]:(0.590136229992) A[1]:(0.729053258896) A[2]:(0.590783596039) A[3]:(0.656165242195)\n",
      " state (3)  A[0]:(0.655407607555) A[1]:(-0.0090152118355) A[2]:(0.49408814311) A[3]:(0.543660283089)\n",
      " state (4)  A[0]:(0.589767932892) A[1]:(0.656589388847) A[2]:(0.000726580503397) A[3]:(0.531328856945)\n",
      " state (5)  A[0]:(0.167185455561) A[1]:(0.926734209061) A[2]:(-0.162464588881) A[3]:(0.506367266178)\n",
      " state (6)  A[0]:(-0.00157418719027) A[1]:(0.810296773911) A[2]:(-0.000657677534036) A[3]:(0.655732691288)\n",
      " state (7)  A[0]:(0.637362957001) A[1]:(-0.250060468912) A[2]:(0.21779538691) A[3]:(0.90239405632)\n",
      " state (8)  A[0]:(0.655803263187) A[1]:(0.00136490084697) A[2]:(0.729301869869) A[3]:(0.589871942997)\n",
      " state (9)  A[0]:(0.656254470348) A[1]:(0.810514211655) A[2]:(0.810171365738) A[3]:(-0.00039209422539)\n",
      " state (10)  A[0]:(0.729089736938) A[1]:(0.900207579136) A[2]:(0.000287652015686) A[3]:(0.728790640831)\n",
      " state (11)  A[0]:(0.514478206635) A[1]:(0.877138972282) A[2]:(-0.569404959679) A[3]:(0.840504527092)\n",
      " state (12)  A[0]:(0.0693464055657) A[1]:(0.82462400198) A[2]:(-0.495188444853) A[3]:(0.790130257607)\n",
      " state (13)  A[0]:(-0.00073635566514) A[1]:(0.809344828129) A[2]:(0.900072991848) A[3]:(0.728617429733)\n",
      " state (14)  A[0]:(0.810025811195) A[1]:(0.900390505791) A[2]:(0.999999761581) A[3]:(0.809680104256)\n",
      " state (15)  A[0]:(0.989549338818) A[1]:(0.962188303471) A[2]:(1.0) A[3]:(0.893670618534)\n",
      "Episode 374000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6068. Times reached goal: 969.               Steps done: 3396940. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0301280238436.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5904,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.0001,  0.6561,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7291,  0.5914,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8100, -0.0002,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0001,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9002,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53164255619) A[1]:(0.590453386307) A[2]:(0.590537190437) A[3]:(0.531445205212)\n",
      " state (1)  A[0]:(0.531867861748) A[1]:(9.26032662392e-05) A[2]:(0.656130313873) A[3]:(0.590470910072)\n",
      " state (2)  A[0]:(0.590752720833) A[1]:(0.729013800621) A[2]:(0.591440618038) A[3]:(0.656027078629)\n",
      " state (3)  A[0]:(0.65614759922) A[1]:(-0.0244095027447) A[2]:(0.496284127235) A[3]:(0.542077660561)\n",
      " state (4)  A[0]:(0.591183900833) A[1]:(0.656173050404) A[2]:(0.000226497650146) A[3]:(0.531281352043)\n",
      " state (5)  A[0]:(0.169246524572) A[1]:(0.926690518856) A[2]:(-0.162942036986) A[3]:(0.506834566593)\n",
      " state (6)  A[0]:(0.000534743012395) A[1]:(0.810006558895) A[2]:(-0.000224828720093) A[3]:(0.655892312527)\n",
      " state (7)  A[0]:(0.638309836388) A[1]:(-0.250787615776) A[2]:(0.219115972519) A[3]:(0.902152478695)\n",
      " state (8)  A[0]:(0.656343102455) A[1]:(7.16522336006e-05) A[2]:(0.729097127914) A[3]:(0.590336382389)\n",
      " state (9)  A[0]:(0.656290411949) A[1]:(0.810043334961) A[2]:(0.810065686703) A[3]:(0.000140845775604)\n",
      " state (10)  A[0]:(0.729190289974) A[1]:(0.900019407272) A[2]:(-4.25577163696e-05) A[3]:(0.72906088829)\n",
      " state (11)  A[0]:(0.514960289001) A[1]:(0.876960158348) A[2]:(-0.569977879524) A[3]:(0.840782999992)\n",
      " state (12)  A[0]:(0.070224493742) A[1]:(0.824414253235) A[2]:(-0.496081590652) A[3]:(0.790492415428)\n",
      " state (13)  A[0]:(6.70552253723e-05) A[1]:(0.809138953686) A[2]:(0.900030016899) A[3]:(0.729035973549)\n",
      " state (14)  A[0]:(0.810096621513) A[1]:(0.900278925896) A[2]:(0.999999761581) A[3]:(0.81002253294)\n",
      " state (15)  A[0]:(0.98951125145) A[1]:(0.962114393711) A[2]:(1.0) A[3]:(0.893822550774)\n",
      "Episode 375000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6066. Times reached goal: 970.               Steps done: 3403006. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0299458204326.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531166613102) A[1]:(0.590447664261) A[2]:(0.59042930603) A[3]:(0.531312227249)\n",
      " state (1)  A[0]:(0.531022071838) A[1]:(0.000523675174918) A[2]:(0.656063556671) A[3]:(0.590578794479)\n",
      " state (2)  A[0]:(0.590338230133) A[1]:(0.729052782059) A[2]:(0.591599822044) A[3]:(0.655931591988)\n",
      " state (3)  A[0]:(0.655117034912) A[1]:(-0.0954115837812) A[2]:(0.505508422852) A[3]:(0.535911679268)\n",
      " state (4)  A[0]:(0.590802967548) A[1]:(0.656288325787) A[2]:(-0.000136613845825) A[3]:(0.531433343887)\n",
      " state (5)  A[0]:(0.168207198381) A[1]:(0.926724791527) A[2]:(-0.163130953908) A[3]:(0.507467150688)\n",
      " state (6)  A[0]:(-0.00106906855945) A[1]:(0.810057401657) A[2]:(-0.000414133042796) A[3]:(0.655981898308)\n",
      " state (7)  A[0]:(0.637645959854) A[1]:(-0.249821826816) A[2]:(0.219320341945) A[3]:(0.90202909708)\n",
      " state (8)  A[0]:(0.656096637249) A[1]:(0.000329751521349) A[2]:(0.729180276394) A[3]:(0.590123236179)\n",
      " state (9)  A[0]:(0.656010985374) A[1]:(0.810087561607) A[2]:(0.810061752796) A[3]:(-0.000231862068176)\n",
      " state (10)  A[0]:(0.728866577148) A[1]:(0.900024414062) A[2]:(-7.02142715454e-05) A[3]:(0.72883361578)\n",
      " state (11)  A[0]:(0.514451622963) A[1]:(0.876937389374) A[2]:(-0.570201635361) A[3]:(0.840666294098)\n",
      " state (12)  A[0]:(0.0696880742908) A[1]:(0.824353575706) A[2]:(-0.496653676033) A[3]:(0.790396749973)\n",
      " state (13)  A[0]:(-0.000287443399429) A[1]:(0.809039711952) A[2]:(0.89998716116) A[3]:(0.728992819786)\n",
      " state (14)  A[0]:(0.810042858124) A[1]:(0.900194346905) A[2]:(0.999999761581) A[3]:(0.810081958771)\n",
      " state (15)  A[0]:(0.989488601685) A[1]:(0.962041854858) A[2]:(1.0) A[3]:(0.893840312958)\n",
      "Episode 376000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6112. Times reached goal: 977.               Steps done: 3409118. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0297633497765.\n",
      " state (0)  A[0]:(0.53095895052) A[1]:(0.590186476707) A[2]:(0.590257644653) A[3]:(0.531314373016)\n",
      " state (1)  A[0]:(0.53093457222) A[1]:(0.000393856287701) A[2]:(0.655847966671) A[3]:(0.590499341488)\n",
      " state (2)  A[0]:(0.59005010128) A[1]:(0.728815674782) A[2]:(0.591331601143) A[3]:(0.655661404133)\n",
      " state (3)  A[0]:(0.654718518257) A[1]:(-0.147053465247) A[2]:(0.512494683266) A[3]:(0.531206190586)\n",
      " state (4)  A[0]:(0.59060549736) A[1]:(0.655907809734) A[2]:(0.000109553337097) A[3]:(0.531382203102)\n",
      " state (5)  A[0]:(0.16783349216) A[1]:(0.926751196384) A[2]:(-0.163335055113) A[3]:(0.507695257664)\n",
      " state (6)  A[0]:(-0.00060096377274) A[1]:(0.809857010841) A[2]:(-0.000640630605631) A[3]:(0.655905127525)\n",
      " state (7)  A[0]:(0.637763082981) A[1]:(-0.250376671553) A[2]:(0.219446644187) A[3]:(0.901797890663)\n",
      " state (8)  A[0]:(0.655712783337) A[1]:(-0.000356953562004) A[2]:(0.728751420975) A[3]:(0.589971423149)\n",
      " state (9)  A[0]:(0.655372560024) A[1]:(0.809880077839) A[2]:(0.809849262238) A[3]:(-0.000555112899747)\n",
      " state (10)  A[0]:(0.728311419487) A[1]:(0.899934828281) A[2]:(-0.00045537945698) A[3]:(0.728541076183)\n",
      " state (11)  A[0]:(0.51361143589) A[1]:(0.876854777336) A[2]:(-0.570665538311) A[3]:(0.840465843678)\n",
      " state (12)  A[0]:(0.0685970634222) A[1]:(0.824311971664) A[2]:(-0.497362524271) A[3]:(0.790067434311)\n",
      " state (13)  A[0]:(-0.00128814508207) A[1]:(0.809119462967) A[2]:(0.900053501129) A[3]:(0.728473067284)\n",
      " state (14)  A[0]:(0.809741437435) A[1]:(0.900333225727) A[2]:(0.999999761581) A[3]:(0.809651136398)\n",
      " state (15)  A[0]:(0.989438295364) A[1]:(0.962100863457) A[2]:(1.0) A[3]:(0.893456041813)\n",
      "Episode 377000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6081. Times reached goal: 974.               Steps done: 3415199. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0295829080356.\n",
      " state (0)  A[0]:(0.531212449074) A[1]:(0.589973628521) A[2]:(0.590381741524) A[3]:(0.531438350677)\n",
      " state (1)  A[0]:(0.531453847885) A[1]:(0.00099298695568) A[2]:(0.655616998672) A[3]:(0.590607523918)\n",
      " state (2)  A[0]:(0.5903236866) A[1]:(0.728668451309) A[2]:(0.591186285019) A[3]:(0.655513107777)\n",
      " state (3)  A[0]:(0.654728055) A[1]:(-0.193784818053) A[2]:(0.519106030464) A[3]:(0.526936709881)\n",
      " state (4)  A[0]:(0.590584397316) A[1]:(0.656077086926) A[2]:(0.000433683366282) A[3]:(0.531416416168)\n",
      " state (5)  A[0]:(0.167326018214) A[1]:(0.926868975163) A[2]:(-0.163299322128) A[3]:(0.507854819298)\n",
      " state (6)  A[0]:(-0.000429600448115) A[1]:(0.809896230698) A[2]:(-0.000454187364085) A[3]:(0.655831634998)\n",
      " state (7)  A[0]:(0.638146817684) A[1]:(-0.250319629908) A[2]:(0.220298498869) A[3]:(0.901670277119)\n",
      " state (8)  A[0]:(0.656177043915) A[1]:(-0.00030704960227) A[2]:(0.728944897652) A[3]:(0.590086579323)\n",
      " state (9)  A[0]:(0.656187653542) A[1]:(0.809915304184) A[2]:(0.809980213642) A[3]:(0.000247582793236)\n",
      " state (10)  A[0]:(0.729095220566) A[1]:(0.899951517582) A[2]:(-0.000134825706482) A[3]:(0.729024887085)\n",
      " state (11)  A[0]:(0.514815330505) A[1]:(0.876860439777) A[2]:(-0.570732414722) A[3]:(0.840790629387)\n",
      " state (12)  A[0]:(0.0700626373291) A[1]:(0.824309647083) A[2]:(-0.497828215361) A[3]:(0.790440917015)\n",
      " state (13)  A[0]:(-0.000127732753754) A[1]:(0.809107363224) A[2]:(0.900008797646) A[3]:(0.728863358498)\n",
      " state (14)  A[0]:(0.809980928898) A[1]:(0.900321722031) A[2]:(0.999999761581) A[3]:(0.809917211533)\n",
      " state (15)  A[0]:(0.989421010017) A[1]:(0.962071359158) A[2]:(1.0) A[3]:(0.893541276455)\n",
      "Episode 378000 finished after 0 timesteps with r=0.0. Running score: 0.97. Times trained:               6099. Times reached goal: 977.               Steps done: 3421298. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0294030309722.\n",
      " state (0)  A[0]:(0.531823158264) A[1]:(0.590476930141) A[2]:(0.590490341187) A[3]:(0.531618952751)\n",
      " state (1)  A[0]:(0.532128334045) A[1]:(0.000649496796541) A[2]:(0.656046628952) A[3]:(0.590873062611)\n",
      " state (2)  A[0]:(0.590972006321) A[1]:(0.728993713856) A[2]:(0.590945720673) A[3]:(0.656004786491)\n",
      " state (3)  A[0]:(0.655590295792) A[1]:(-0.221309885383) A[2]:(0.522453427315) A[3]:(0.52448785305)\n",
      " state (4)  A[0]:(0.591456055641) A[1]:(0.656195938587) A[2]:(-9.25064086914e-05) A[3]:(0.531791985035)\n",
      " state (5)  A[0]:(0.16830791533) A[1]:(0.926870584488) A[2]:(-0.163505271077) A[3]:(0.50867497921)\n",
      " state (6)  A[0]:(0.000200867652893) A[1]:(0.810060977936) A[2]:(-0.000321865081787) A[3]:(0.656262695789)\n",
      " state (7)  A[0]:(0.638116657734) A[1]:(-0.249481365085) A[2]:(0.221115112305) A[3]:(0.901711404324)\n",
      " state (8)  A[0]:(0.655884981155) A[1]:(-0.000366076797945) A[2]:(0.729091405869) A[3]:(0.590535879135)\n",
      " state (9)  A[0]:(0.655503034592) A[1]:(0.809847772121) A[2]:(0.810087621212) A[3]:(2.51829624176e-06)\n",
      " state (10)  A[0]:(0.728530168533) A[1]:(0.899959743023) A[2]:(-9.19103622437e-05) A[3]:(0.729009985924)\n",
      " state (11)  A[0]:(0.513950586319) A[1]:(0.876918911934) A[2]:(-0.571030497551) A[3]:(0.840901136398)\n",
      " state (12)  A[0]:(0.068782389164) A[1]:(0.8244535923) A[2]:(-0.498473465443) A[3]:(0.7906036973)\n",
      " state (13)  A[0]:(-0.00166204420384) A[1]:(0.809318423271) A[2]:(0.899928867817) A[3]:(0.72900056839)\n",
      " state (14)  A[0]:(0.809330165386) A[1]:(0.900460124016) A[2]:(0.999999761581) A[3]:(0.809932947159)\n",
      " state (15)  A[0]:(0.989353656769) A[1]:(0.962111949921) A[2]:(1.0) A[3]:(0.893435657024)\n",
      "Episode 379000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6092. Times reached goal: 973.               Steps done: 3427390. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0292244522107.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5907,  0.5904,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5913,  0.6565,  0.0024,  0.5311]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.0001,  0.7297,  0.5892]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.8097,  0.8098, -0.0008]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7295,  0.9001, -0.0009,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8106,  0.9006,  1.0000,  0.8105]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531211614609) A[1]:(0.590367555618) A[2]:(0.590759277344) A[3]:(0.531176924706)\n",
      " state (1)  A[0]:(0.531613230705) A[1]:(-0.00136595510412) A[2]:(0.655654430389) A[3]:(0.59085214138)\n",
      " state (2)  A[0]:(0.590338110924) A[1]:(0.728769779205) A[2]:(0.591139733791) A[3]:(0.656055212021)\n",
      " state (3)  A[0]:(0.654987215996) A[1]:(-0.257053494453) A[2]:(0.52865511179) A[3]:(0.521016120911)\n",
      " state (4)  A[0]:(0.590638041496) A[1]:(0.655825734138) A[2]:(0.00174045388121) A[3]:(0.5320571661)\n",
      " state (5)  A[0]:(0.166829407215) A[1]:(0.926832199097) A[2]:(-0.162311032414) A[3]:(0.509476423264)\n",
      " state (6)  A[0]:(-0.000529468001332) A[1]:(0.809897065163) A[2]:(0.000623464526143) A[3]:(0.656824469566)\n",
      " state (7)  A[0]:(0.638160705566) A[1]:(-0.249572619796) A[2]:(0.222081810236) A[3]:(0.90180337429)\n",
      " state (8)  A[0]:(0.656258881092) A[1]:(-0.000664140912704) A[2]:(0.728946328163) A[3]:(0.591584920883)\n",
      " state (9)  A[0]:(0.656161785126) A[1]:(0.809742450714) A[2]:(0.809962451458) A[3]:(0.00132942118216)\n",
      " state (10)  A[0]:(0.729269742966) A[1]:(0.899868786335) A[2]:(-0.000597476901021) A[3]:(0.729685604572)\n",
      " state (11)  A[0]:(0.515378177166) A[1]:(0.876719474792) A[2]:(-0.571737647057) A[3]:(0.841448366642)\n",
      " state (12)  A[0]:(0.0710626244545) A[1]:(0.824046075344) A[2]:(-0.499473035336) A[3]:(0.791405856609)\n",
      " state (13)  A[0]:(0.0012027019402) A[1]:(0.808756649494) A[2]:(0.900006830692) A[3]:(0.730124473572)\n",
      " state (14)  A[0]:(0.810611724854) A[1]:(0.900094568729) A[2]:(0.999999761581) A[3]:(0.810870468616)\n",
      " state (15)  A[0]:(0.989410459995) A[1]:(0.961901545525) A[2]:(1.0) A[3]:(0.893930256367)\n",
      "Episode 380000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6063. Times reached goal: 971.               Steps done: 3433453. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.029047800418.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531338095665) A[1]:(0.590490102768) A[2]:(0.590729355812) A[3]:(0.531184673309)\n",
      " state (1)  A[0]:(0.531867265701) A[1]:(0.000303335487843) A[2]:(0.655887007713) A[3]:(0.590705871582)\n",
      " state (2)  A[0]:(0.590433239937) A[1]:(0.728968918324) A[2]:(0.591196775436) A[3]:(0.65581870079)\n",
      " state (3)  A[0]:(0.655420005322) A[1]:(-0.287542521954) A[2]:(0.532710671425) A[3]:(0.517208099365)\n",
      " state (4)  A[0]:(0.59102088213) A[1]:(0.656186103821) A[2]:(0.000467181176646) A[3]:(0.53139424324)\n",
      " state (5)  A[0]:(0.166885823011) A[1]:(0.926977515221) A[2]:(-0.164044260979) A[3]:(0.509024560452)\n",
      " state (6)  A[0]:(-6.82771205902e-05) A[1]:(0.810056746006) A[2]:(-0.000860810047016) A[3]:(0.656222760677)\n",
      " state (7)  A[0]:(0.638153016567) A[1]:(-0.248981237411) A[2]:(0.221916571259) A[3]:(0.901303350925)\n",
      " state (8)  A[0]:(0.655578434467) A[1]:(0.000185586512089) A[2]:(0.729115366936) A[3]:(0.589486896992)\n",
      " state (9)  A[0]:(0.655435800552) A[1]:(0.809971392155) A[2]:(0.810055553913) A[3]:(-0.000909596448764)\n",
      " state (10)  A[0]:(0.728683412075) A[1]:(0.899999320507) A[2]:(-0.000830650154967) A[3]:(0.729023754597)\n",
      " state (11)  A[0]:(0.514342665672) A[1]:(0.876879096031) A[2]:(-0.572237968445) A[3]:(0.841083228588)\n",
      " state (12)  A[0]:(0.0695184543729) A[1]:(0.824263334274) A[2]:(-0.500147819519) A[3]:(0.790880560875)\n",
      " state (13)  A[0]:(-0.000216990709305) A[1]:(0.808997929096) A[2]:(0.900194644928) A[3]:(0.729429006577)\n",
      " state (14)  A[0]:(0.810252785683) A[1]:(0.900230050087) A[2]:(0.999999761581) A[3]:(0.810413002968)\n",
      " state (15)  A[0]:(0.989357173443) A[1]:(0.961910605431) A[2]:(1.0) A[3]:(0.893592894077)\n",
      "Episode 381000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6059. Times reached goal: 963.               Steps done: 3439512. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.028872331914.\n",
      " state (0)  A[0]:(0.531497240067) A[1]:(0.590315461159) A[2]:(0.590481817722) A[3]:(0.531223356724)\n",
      " state (1)  A[0]:(0.531741142273) A[1]:(0.000494953186717) A[2]:(0.655969619751) A[3]:(0.590318799019)\n",
      " state (2)  A[0]:(0.590376019478) A[1]:(0.729025304317) A[2]:(0.590798199177) A[3]:(0.655554056168)\n",
      " state (3)  A[0]:(0.655576884747) A[1]:(-0.307510644197) A[2]:(0.535403013229) A[3]:(0.514926433563)\n",
      " state (4)  A[0]:(0.591168701649) A[1]:(0.656126976013) A[2]:(0.000522732676473) A[3]:(0.531414926052)\n",
      " state (5)  A[0]:(0.167198270559) A[1]:(0.926966845989) A[2]:(-0.16364762187) A[3]:(0.509161233902)\n",
      " state (6)  A[0]:(0.000366359920008) A[1]:(0.810219347477) A[2]:(1.22785568237e-05) A[3]:(0.65585309267)\n",
      " state (7)  A[0]:(0.638403773308) A[1]:(-0.248010009527) A[2]:(0.223317205906) A[3]:(0.901023924351)\n",
      " state (8)  A[0]:(0.656249403954) A[1]:(0.000900950049981) A[2]:(0.729183316231) A[3]:(0.589812517166)\n",
      " state (9)  A[0]:(0.65606969595) A[1]:(0.810264825821) A[2]:(0.810236096382) A[3]:(-0.000756606343202)\n",
      " state (10)  A[0]:(0.729098975658) A[1]:(0.900184988976) A[2]:(0.000657200696878) A[3]:(0.728570461273)\n",
      " state (11)  A[0]:(0.515023708344) A[1]:(0.877136528492) A[2]:(-0.571190237999) A[3]:(0.84067428112)\n",
      " state (12)  A[0]:(0.0704884231091) A[1]:(0.824677467346) A[2]:(-0.499350100756) A[3]:(0.790270805359)\n",
      " state (13)  A[0]:(0.000506371201482) A[1]:(0.809490799904) A[2]:(0.900346040726) A[3]:(0.72856760025)\n",
      " state (14)  A[0]:(0.810302555561) A[1]:(0.900527060032) A[2]:(0.999999761581) A[3]:(0.809736311436)\n",
      " state (15)  A[0]:(0.989334881306) A[1]:(0.962025463581) A[2]:(1.0) A[3]:(0.893141329288)\n",
      "Episode 382000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6087. Times reached goal: 971.               Steps done: 3445599. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0286971198286.\n",
      " state (0)  A[0]:(0.531501471996) A[1]:(0.591094136238) A[2]:(0.590530991554) A[3]:(0.531213283539)\n",
      " state (1)  A[0]:(0.531552255154) A[1]:(9.06512141228e-05) A[2]:(0.656217813492) A[3]:(0.590481817722)\n",
      " state (2)  A[0]:(0.590148568153) A[1]:(0.729368209839) A[2]:(0.592047214508) A[3]:(0.655630588531)\n",
      " state (3)  A[0]:(0.65553176403) A[1]:(-0.334892779589) A[2]:(0.54074382782) A[3]:(0.51161557436)\n",
      " state (4)  A[0]:(0.591009557247) A[1]:(0.656532168388) A[2]:(0.00147104158532) A[3]:(0.531246423721)\n",
      " state (5)  A[0]:(0.166383996606) A[1]:(0.926995694637) A[2]:(-0.163288414478) A[3]:(0.509390354156)\n",
      " state (6)  A[0]:(-0.000486075849039) A[1]:(0.810087680817) A[2]:(0.000144958496094) A[3]:(0.656105518341)\n",
      " state (7)  A[0]:(0.638116300106) A[1]:(-0.248579740524) A[2]:(0.223807692528) A[3]:(0.901146411896)\n",
      " state (8)  A[0]:(0.656010150909) A[1]:(-8.06525349617e-05) A[2]:(0.729021787643) A[3]:(0.590292751789)\n",
      " state (9)  A[0]:(0.65576338768) A[1]:(0.809960901737) A[2]:(0.80992525816) A[3]:(-0.00100490415934)\n",
      " state (10)  A[0]:(0.728719592094) A[1]:(0.89997446537) A[2]:(-0.00108683062717) A[3]:(0.728541493416)\n",
      " state (11)  A[0]:(0.514192819595) A[1]:(0.876812577248) A[2]:(-0.572974801064) A[3]:(0.840764760971)\n",
      " state (12)  A[0]:(0.0691912919283) A[1]:(0.824189424515) A[2]:(-0.501483500004) A[3]:(0.790454506874)\n",
      " state (13)  A[0]:(-0.000347018241882) A[1]:(0.809034883976) A[2]:(0.900403618813) A[3]:(0.728899717331)\n",
      " state (14)  A[0]:(0.810383081436) A[1]:(0.900372803211) A[2]:(0.999999761581) A[3]:(0.810069143772)\n",
      " state (15)  A[0]:(0.989302992821) A[1]:(0.961945474148) A[2]:(1.0) A[3]:(0.893198311329)\n",
      "Episode 383000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6027. Times reached goal: 974.               Steps done: 3451626. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0285246824494.\n",
      " state (0)  A[0]:(0.53109151125) A[1]:(0.590672850609) A[2]:(0.590613245964) A[3]:(0.531746387482)\n",
      " state (1)  A[0]:(0.531396627426) A[1]:(1.37314200401e-05) A[2]:(0.656023323536) A[3]:(0.590555906296)\n",
      " state (2)  A[0]:(0.590216219425) A[1]:(0.729069471359) A[2]:(0.59032201767) A[3]:(0.65575492382)\n",
      " state (3)  A[0]:(0.655560135841) A[1]:(-0.347858667374) A[2]:(0.54087305069) A[3]:(0.509323358536)\n",
      " state (4)  A[0]:(0.590781629086) A[1]:(0.656147122383) A[2]:(-0.000557303370442) A[3]:(0.530155658722)\n",
      " state (5)  A[0]:(0.166009277105) A[1]:(0.926861405373) A[2]:(-0.164728924632) A[3]:(0.508307218552)\n",
      " state (6)  A[0]:(-0.00138798262924) A[1]:(0.81001418829) A[2]:(-0.000757694127969) A[3]:(0.654690682888)\n",
      " state (7)  A[0]:(0.637231945992) A[1]:(-0.248473316431) A[2]:(0.223813131452) A[3]:(0.900424182415)\n",
      " state (8)  A[0]:(0.655399143696) A[1]:(-0.00047148016165) A[2]:(0.728929281235) A[3]:(0.588070392609)\n",
      " state (9)  A[0]:(0.655227303505) A[1]:(0.809888005257) A[2]:(0.809918761253) A[3]:(-0.0047349575907)\n",
      " state (10)  A[0]:(0.728354334831) A[1]:(0.899938225746) A[2]:(-0.000926732725929) A[3]:(0.726804494858)\n",
      " state (11)  A[0]:(0.513683140278) A[1]:(0.876770794392) A[2]:(-0.573079526424) A[3]:(0.839693307877)\n",
      " state (12)  A[0]:(0.0683489963412) A[1]:(0.824133694172) A[2]:(-0.502451896667) A[3]:(0.789018750191)\n",
      " state (13)  A[0]:(-0.00207489426248) A[1]:(0.808918774128) A[2]:(0.899616658688) A[3]:(0.726913452148)\n",
      " state (14)  A[0]:(0.809312343597) A[1]:(0.900265514851) A[2]:(0.999999761581) A[3]:(0.808454036713)\n",
      " state (15)  A[0]:(0.989234149456) A[1]:(0.961916148663) A[2]:(1.0) A[3]:(0.892247438431)\n",
      "Episode 384000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6079. Times reached goal: 969.               Steps done: 3457705. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0283518068924.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5905,  0.5904,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.9097e-01,  6.5602e-01,  8.7023e-06,  5.3155e-01]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6562, -0.0001,  0.7289,  0.5903]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.8100,  0.8102, -0.0011]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7287,  0.8999, -0.0005,  0.7285]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9003,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53156888485) A[1]:(0.590557098389) A[2]:(0.590506136417) A[3]:(0.531686782837)\n",
      " state (1)  A[0]:(0.531732916832) A[1]:(0.000205796211958) A[2]:(0.656018137932) A[3]:(0.590708971024)\n",
      " state (2)  A[0]:(0.590492010117) A[1]:(0.728995800018) A[2]:(0.590508580208) A[3]:(0.65594804287)\n",
      " state (3)  A[0]:(0.655743360519) A[1]:(-0.365202635527) A[2]:(0.544153869152) A[3]:(0.508641242981)\n",
      " state (4)  A[0]:(0.59102833271) A[1]:(0.656188428402) A[2]:(9.51290130615e-05) A[3]:(0.53173750639)\n",
      " state (5)  A[0]:(0.166607603431) A[1]:(0.926906347275) A[2]:(-0.164439335465) A[3]:(0.510129213333)\n",
      " state (6)  A[0]:(7.44760036469e-05) A[1]:(0.810066699982) A[2]:(-0.000191211700439) A[3]:(0.656007826328)\n",
      " state (7)  A[0]:(0.638205051422) A[1]:(-0.247864589095) A[2]:(0.225018426776) A[3]:(0.900768339634)\n",
      " state (8)  A[0]:(0.656361460686) A[1]:(0.000163923949003) A[2]:(0.728985548019) A[3]:(0.590567350388)\n",
      " state (9)  A[0]:(0.656136870384) A[1]:(0.810032606125) A[2]:(0.810028791428) A[3]:(-0.000177055597305)\n",
      " state (10)  A[0]:(0.729092121124) A[1]:(0.900055289268) A[2]:(4.56571578979e-05) A[3]:(0.728789806366)\n",
      " state (11)  A[0]:(0.515006780624) A[1]:(0.876942098141) A[2]:(-0.572451591492) A[3]:(0.840923905373)\n",
      " state (12)  A[0]:(0.0703535228968) A[1]:(0.824375987053) A[2]:(-0.501895844936) A[3]:(0.790593087673)\n",
      " state (13)  A[0]:(8.81850719452e-05) A[1]:(0.809139311314) A[2]:(0.899988949299) A[3]:(0.728887081146)\n",
      " state (14)  A[0]:(0.810080647469) A[1]:(0.900331914425) A[2]:(0.999999761581) A[3]:(0.8099360466)\n",
      " state (15)  A[0]:(0.989248931408) A[1]:(0.961877942085) A[2]:(1.0) A[3]:(0.893042087555)\n",
      "Episode 385000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6070. Times reached goal: 977.               Steps done: 3463775. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0281802326791.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531536698341) A[1]:(0.590808153152) A[2]:(0.590795516968) A[3]:(0.531650424004)\n",
      " state (1)  A[0]:(0.531725525856) A[1]:(0.000705942395143) A[2]:(0.656118512154) A[3]:(0.590783596039)\n",
      " state (2)  A[0]:(0.590450167656) A[1]:(0.729013562202) A[2]:(0.590668082237) A[3]:(0.656126856804)\n",
      " state (3)  A[0]:(0.655751228333) A[1]:(-0.383276134729) A[2]:(0.547412872314) A[3]:(0.506490647793)\n",
      " state (4)  A[0]:(0.590930044651) A[1]:(0.656143188477) A[2]:(0.000291347503662) A[3]:(0.531983494759)\n",
      " state (5)  A[0]:(0.166327923536) A[1]:(0.926909208298) A[2]:(-0.164843514562) A[3]:(0.510896980762)\n",
      " state (6)  A[0]:(5.18262386322e-05) A[1]:(0.809963285923) A[2]:(-0.000718474271707) A[3]:(0.656525492668)\n",
      " state (7)  A[0]:(0.63809132576) A[1]:(-0.248150497675) A[2]:(0.225016504526) A[3]:(0.900748312473)\n",
      " state (8)  A[0]:(0.656070411205) A[1]:(-6.3344836235e-05) A[2]:(0.728866755962) A[3]:(0.59052079916)\n",
      " state (9)  A[0]:(0.655900239944) A[1]:(0.809957027435) A[2]:(0.80994361639) A[3]:(-5.67585229874e-05)\n",
      " state (10)  A[0]:(0.728889167309) A[1]:(0.899964809418) A[2]:(-0.000580072344746) A[3]:(0.729071140289)\n",
      " state (11)  A[0]:(0.514612972736) A[1]:(0.876780986786) A[2]:(-0.573298513889) A[3]:(0.841175615788)\n",
      " state (12)  A[0]:(0.0697495490313) A[1]:(0.824127197266) A[2]:(-0.503086686134) A[3]:(0.790917098522)\n",
      " state (13)  A[0]:(-0.000343203544617) A[1]:(0.808914422989) A[2]:(0.90005594492) A[3]:(0.729275226593)\n",
      " state (14)  A[0]:(0.810073316097) A[1]:(0.900272130966) A[2]:(0.999999761581) A[3]:(0.810208439827)\n",
      " state (15)  A[0]:(0.989212930202) A[1]:(0.961838841438) A[2]:(1.0) A[3]:(0.893059074879)\n",
      "Episode 386000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6054. Times reached goal: 977.               Steps done: 3469829. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0280101449256.\n",
      " state (0)  A[0]:(0.531230807304) A[1]:(0.590334773064) A[2]:(0.590417921543) A[3]:(0.531713724136)\n",
      " state (1)  A[0]:(0.53137075901) A[1]:(-7.09481537342e-05) A[2]:(0.656047463417) A[3]:(0.590784311295)\n",
      " state (2)  A[0]:(0.590118825436) A[1]:(0.728965342045) A[2]:(0.590309739113) A[3]:(0.656291306019)\n",
      " state (3)  A[0]:(0.65550506115) A[1]:(-0.393695294857) A[2]:(0.548722028732) A[3]:(0.504784822464)\n",
      " state (4)  A[0]:(0.590387225151) A[1]:(0.655994415283) A[2]:(8.26120376587e-05) A[3]:(0.53164601326)\n",
      " state (5)  A[0]:(0.165513277054) A[1]:(0.926874518394) A[2]:(-0.164731130004) A[3]:(0.51088643074)\n",
      " state (6)  A[0]:(-0.000500440539327) A[1]:(0.809988737106) A[2]:(-0.000109791755676) A[3]:(0.656300187111)\n",
      " state (7)  A[0]:(0.637837767601) A[1]:(-0.247691050172) A[2]:(0.226129725575) A[3]:(0.900524675846)\n",
      " state (8)  A[0]:(0.656188130379) A[1]:(0.000170361250639) A[2]:(0.728775620461) A[3]:(0.590994715691)\n",
      " state (9)  A[0]:(0.656079173088) A[1]:(0.810015916824) A[2]:(0.80987226963) A[3]:(0.000820293847937)\n",
      " state (10)  A[0]:(0.728983283043) A[1]:(0.899980425835) A[2]:(-0.000381350488169) A[3]:(0.729154229164)\n",
      " state (11)  A[0]:(0.514791190624) A[1]:(0.876756906509) A[2]:(-0.573326349258) A[3]:(0.841134607792)\n",
      " state (12)  A[0]:(0.0700536221266) A[1]:(0.82402163744) A[2]:(-0.503448009491) A[3]:(0.790777802467)\n",
      " state (13)  A[0]:(7.09295272827e-06) A[1]:(0.808705210686) A[2]:(0.90012383461) A[3]:(0.729043483734)\n",
      " state (14)  A[0]:(0.810234725475) A[1]:(0.900102972984) A[2]:(0.999999761581) A[3]:(0.81008708477)\n",
      " state (15)  A[0]:(0.989196002483) A[1]:(0.961716532707) A[2]:(1.0) A[3]:(0.892949342728)\n",
      "Episode 387000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6107. Times reached goal: 975.               Steps done: 3475936. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0278396082343.\n",
      " state (0)  A[0]:(0.531643688679) A[1]:(0.590731263161) A[2]:(0.590605974197) A[3]:(0.531920433044)\n",
      " state (1)  A[0]:(0.531693279743) A[1]:(3.97637486458e-05) A[2]:(0.656272530556) A[3]:(0.590798974037)\n",
      " state (2)  A[0]:(0.590462446213) A[1]:(0.729158401489) A[2]:(0.590349495411) A[3]:(0.656392872334)\n",
      " state (3)  A[0]:(0.656025886536) A[1]:(-0.392176300287) A[2]:(0.548453092575) A[3]:(0.505281805992)\n",
      " state (4)  A[0]:(0.59085714817) A[1]:(0.65621805191) A[2]:(3.18288803101e-05) A[3]:(0.532059431076)\n",
      " state (5)  A[0]:(0.166042804718) A[1]:(0.9269053936) A[2]:(-0.164747595787) A[3]:(0.511227428913)\n",
      " state (6)  A[0]:(-0.000287145376205) A[1]:(0.810045301914) A[2]:(5.88893890381e-05) A[3]:(0.656378984451)\n",
      " state (7)  A[0]:(0.637777864933) A[1]:(-0.248009636998) A[2]:(0.226912662387) A[3]:(0.90048956871)\n",
      " state (8)  A[0]:(0.656206130981) A[1]:(-0.000327162444592) A[2]:(0.729068040848) A[3]:(0.590811729431)\n",
      " state (9)  A[0]:(0.656117320061) A[1]:(0.80996465683) A[2]:(0.810093164444) A[3]:(6.33746385574e-05)\n",
      " state (10)  A[0]:(0.729093551636) A[1]:(0.899989902973) A[2]:(-0.000165700912476) A[3]:(0.729112267494)\n",
      " state (11)  A[0]:(0.515006899834) A[1]:(0.8768273592) A[2]:(-0.57372957468) A[3]:(0.841262757778)\n",
      " state (12)  A[0]:(0.0701779350638) A[1]:(0.824237823486) A[2]:(-0.504459619522) A[3]:(0.790987372398)\n",
      " state (13)  A[0]:(-0.000183582305908) A[1]:(0.809110283852) A[2]:(0.900026917458) A[3]:(0.729229211807)\n",
      " state (14)  A[0]:(0.810061812401) A[1]:(0.900449812412) A[2]:(0.999999761581) A[3]:(0.810073435307)\n",
      " state (15)  A[0]:(0.98914539814) A[1]:(0.961877286434) A[2]:(1.0) A[3]:(0.892732262611)\n",
      "Episode 388000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6072. Times reached goal: 984.               Steps done: 3482008. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0276710783078.\n",
      " state (0)  A[0]:(0.531212985516) A[1]:(0.590483009815) A[2]:(0.590233325958) A[3]:(0.531438291073)\n",
      " state (1)  A[0]:(0.531343460083) A[1]:(0.000137772411108) A[2]:(0.655927658081) A[3]:(0.590443909168)\n",
      " state (2)  A[0]:(0.590129852295) A[1]:(0.729018092155) A[2]:(0.590380072594) A[3]:(0.656018972397)\n",
      " state (3)  A[0]:(0.655797243118) A[1]:(-0.393913477659) A[2]:(0.548682570457) A[3]:(0.504393219948)\n",
      " state (4)  A[0]:(0.590581774712) A[1]:(0.656077802181) A[2]:(-4.44650650024e-05) A[3]:(0.531296908855)\n",
      " state (5)  A[0]:(0.165702983737) A[1]:(0.926962554455) A[2]:(-0.165057703853) A[3]:(0.510515213013)\n",
      " state (6)  A[0]:(-0.000187873840332) A[1]:(0.810096621513) A[2]:(-1.41859054565e-05) A[3]:(0.655847847462)\n",
      " state (7)  A[0]:(0.637644648552) A[1]:(-0.247927755117) A[2]:(0.227407172322) A[3]:(0.900216281414)\n",
      " state (8)  A[0]:(0.655867695808) A[1]:(9.88207757473e-05) A[2]:(0.729051291943) A[3]:(0.590279757977)\n",
      " state (9)  A[0]:(0.655737280846) A[1]:(0.810046553612) A[2]:(0.81007361412) A[3]:(-0.000307783484459)\n",
      " state (10)  A[0]:(0.728785812855) A[1]:(0.900020062923) A[2]:(-3.09944152832e-05) A[3]:(0.728831529617)\n",
      " state (11)  A[0]:(0.514631688595) A[1]:(0.876853942871) A[2]:(-0.57387650013) A[3]:(0.841052532196)\n",
      " state (12)  A[0]:(0.0697697773576) A[1]:(0.824247956276) A[2]:(-0.50502383709) A[3]:(0.79070264101)\n",
      " state (13)  A[0]:(-0.000583678425755) A[1]:(0.809060335159) A[2]:(0.900024473667) A[3]:(0.7289031744)\n",
      " state (14)  A[0]:(0.80992847681) A[1]:(0.900365829468) A[2]:(0.999999821186) A[3]:(0.809944868088)\n",
      " state (15)  A[0]:(0.989108681679) A[1]:(0.961788773537) A[2]:(1.0) A[3]:(0.892654240131)\n",
      "Episode 389000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6040. Times reached goal: 976.               Steps done: 3488048. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0275044487227.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5904,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.0001,  0.6561,  0.5902]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.7290,  0.5902,  0.6559]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  0.8099, -0.0000,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0002,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9003,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53179448843) A[1]:(0.590388834476) A[2]:(0.590445518494) A[3]:(0.531413733959)\n",
      " state (1)  A[0]:(0.532015442848) A[1]:(-5.8501958847e-05) A[2]:(0.656078279018) A[3]:(0.590416908264)\n",
      " state (2)  A[0]:(0.590775132179) A[1]:(0.728943109512) A[2]:(0.59027159214) A[3]:(0.656083226204)\n",
      " state (3)  A[0]:(0.656151890755) A[1]:(-0.393008083105) A[2]:(0.548443436623) A[3]:(0.504572749138)\n",
      " state (4)  A[0]:(0.590702772141) A[1]:(0.656057357788) A[2]:(-9.84668731689e-05) A[3]:(0.531419754028)\n",
      " state (5)  A[0]:(0.16564437747) A[1]:(0.926941037178) A[2]:(-0.165105134249) A[3]:(0.510670900345)\n",
      " state (6)  A[0]:(-0.000195413827896) A[1]:(0.81000316143) A[2]:(1.02519989014e-05) A[3]:(0.655850172043)\n",
      " state (7)  A[0]:(0.637712061405) A[1]:(-0.248250111938) A[2]:(0.22764377296) A[3]:(0.900094687939)\n",
      " state (8)  A[0]:(0.65609228611) A[1]:(-0.000174462795258) A[2]:(0.728994190693) A[3]:(0.589972615242)\n",
      " state (9)  A[0]:(0.656100690365) A[1]:(0.809952259064) A[2]:(0.809987902641) A[3]:(-0.000550240220036)\n",
      " state (10)  A[0]:(0.72912466526) A[1]:(0.899996697903) A[2]:(-0.00029444694519) A[3]:(0.728760063648)\n",
      " state (11)  A[0]:(0.515209019184) A[1]:(0.876865446568) A[2]:(-0.574280619621) A[3]:(0.841018021107)\n",
      " state (12)  A[0]:(0.0705115273595) A[1]:(0.824305534363) A[2]:(-0.505759298801) A[3]:(0.790639519691)\n",
      " state (13)  A[0]:(3.66866588593e-05) A[1]:(0.809139788151) A[2]:(0.90000718832) A[3]:(0.728783726692)\n",
      " state (14)  A[0]:(0.810088932514) A[1]:(0.900393605232) A[2]:(0.999999821186) A[3]:(0.809831023216)\n",
      " state (15)  A[0]:(0.989087164402) A[1]:(0.9617613554) A[2]:(1.0) A[3]:(0.892489433289)\n",
      "Episode 390000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6023. Times reached goal: 961.               Steps done: 3494071. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0273392873109.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531086921692) A[1]:(0.590698182583) A[2]:(0.590521216393) A[3]:(0.531615972519)\n",
      " state (1)  A[0]:(0.530891180038) A[1]:(0.000279542058706) A[2]:(0.65624666214) A[3]:(0.590615391731)\n",
      " state (2)  A[0]:(0.589751720428) A[1]:(0.72920280695) A[2]:(0.590656518936) A[3]:(0.656244456768)\n",
      " state (3)  A[0]:(0.655448973179) A[1]:(-0.392825216055) A[2]:(0.548714637756) A[3]:(0.504964470863)\n",
      " state (4)  A[0]:(0.590020358562) A[1]:(0.656443357468) A[2]:(4.23192977905e-05) A[3]:(0.531867027283)\n",
      " state (5)  A[0]:(0.164688423276) A[1]:(0.927048802376) A[2]:(-0.165127977729) A[3]:(0.511159241199)\n",
      " state (6)  A[0]:(-0.000977873452939) A[1]:(0.810216844082) A[2]:(7.70092010498e-05) A[3]:(0.656350374222)\n",
      " state (7)  A[0]:(0.637197136879) A[1]:(-0.24761864543) A[2]:(0.228169634938) A[3]:(0.900280952454)\n",
      " state (8)  A[0]:(0.655502915382) A[1]:(0.000744577380829) A[2]:(0.729297041893) A[3]:(0.590365588665)\n",
      " state (9)  A[0]:(0.655546426773) A[1]:(0.810257971287) A[2]:(0.810213029385) A[3]:(-0.00020070374012)\n",
      " state (10)  A[0]:(0.72875648737) A[1]:(0.900166630745) A[2]:(0.000113964080811) A[3]:(0.729051709175)\n",
      " state (11)  A[0]:(0.514783143997) A[1]:(0.877083539963) A[2]:(-0.574359178543) A[3]:(0.841248691082)\n",
      " state (12)  A[0]:(0.0700326561928) A[1]:(0.824619174004) A[2]:(-0.506246328354) A[3]:(0.790931522846)\n",
      " state (13)  A[0]:(-0.000451058120234) A[1]:(0.809475064278) A[2]:(0.900008678436) A[3]:(0.72909617424)\n",
      " state (14)  A[0]:(0.809916675091) A[1]:(0.900563001633) A[2]:(0.999999821186) A[3]:(0.810001134872)\n",
      " state (15)  A[0]:(0.989049553871) A[1]:(0.961795091629) A[2]:(1.0) A[3]:(0.892474651337)\n",
      "Episode 391000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6051. Times reached goal: 967.               Steps done: 3500122. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.027174356784.\n",
      " state (0)  A[0]:(0.531603932381) A[1]:(0.590341866016) A[2]:(0.590449333191) A[3]:(0.531398713589)\n",
      " state (1)  A[0]:(0.531728029251) A[1]:(-6.13406300545e-05) A[2]:(0.656026959419) A[3]:(0.590370297432)\n",
      " state (2)  A[0]:(0.590581834316) A[1]:(0.728890240192) A[2]:(0.590349018574) A[3]:(0.656020402908)\n",
      " state (3)  A[0]:(0.656059920788) A[1]:(-0.393455773592) A[2]:(0.548578500748) A[3]:(0.504446268082)\n",
      " state (4)  A[0]:(0.590595960617) A[1]:(0.656081914902) A[2]:(-5.61475753784e-05) A[3]:(0.53135573864)\n",
      " state (5)  A[0]:(0.165532290936) A[1]:(0.926980137825) A[2]:(-0.165288224816) A[3]:(0.51071703434)\n",
      " state (6)  A[0]:(0.000149756669998) A[1]:(0.8099796772) A[2]:(4.17232513428e-06) A[3]:(0.656008660793)\n",
      " state (7)  A[0]:(0.637926697731) A[1]:(-0.248414546251) A[2]:(0.228224217892) A[3]:(0.900129139423)\n",
      " state (8)  A[0]:(0.656288743019) A[1]:(5.55440783501e-05) A[2]:(0.72893512249) A[3]:(0.590494573116)\n",
      " state (9)  A[0]:(0.656217634678) A[1]:(0.810013234615) A[2]:(0.809981942177) A[3]:(0.000148579478264)\n",
      " state (10)  A[0]:(0.729150652885) A[1]:(0.89999550581) A[2]:(-0.000128269195557) A[3]:(0.729004383087)\n",
      " state (11)  A[0]:(0.515338480473) A[1]:(0.876831948757) A[2]:(-0.5746306777) A[3]:(0.841169834137)\n",
      " state (12)  A[0]:(0.0707652717829) A[1]:(0.824210703373) A[2]:(-0.506809294224) A[3]:(0.790826380253)\n",
      " state (13)  A[0]:(0.000282138586044) A[1]:(0.808958172798) A[2]:(0.90005004406) A[3]:(0.729019165039)\n",
      " state (14)  A[0]:(0.810199141502) A[1]:(0.900222659111) A[2]:(0.999999821186) A[3]:(0.810068070889)\n",
      " state (15)  A[0]:(0.989039957523) A[1]:(0.961604773998) A[2]:(1.0) A[3]:(0.892512619495)\n",
      "Episode 392000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6065. Times reached goal: 974.               Steps done: 3506187. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.027010043095.\n",
      " state (0)  A[0]:(0.531844377518) A[1]:(0.590644598007) A[2]:(0.590568065643) A[3]:(0.531563401222)\n",
      " state (1)  A[0]:(0.531973540783) A[1]:(3.47122550011e-05) A[2]:(0.656181573868) A[3]:(0.590477347374)\n",
      " state (2)  A[0]:(0.590915262699) A[1]:(0.729046940804) A[2]:(0.590429604053) A[3]:(0.656136631966)\n",
      " state (3)  A[0]:(0.656500220299) A[1]:(-0.392559945583) A[2]:(0.54847574234) A[3]:(0.504503428936)\n",
      " state (4)  A[0]:(0.591084480286) A[1]:(0.656181275845) A[2]:(-7.12871551514e-05) A[3]:(0.531298518181)\n",
      " state (5)  A[0]:(0.166132465005) A[1]:(0.926977038383) A[2]:(-0.165351882577) A[3]:(0.510689675808)\n",
      " state (6)  A[0]:(0.000596284808125) A[1]:(0.809945583344) A[2]:(-3.77893447876e-05) A[3]:(0.656007647514)\n",
      " state (7)  A[0]:(0.638078451157) A[1]:(-0.248533427715) A[2]:(0.228462517262) A[3]:(0.900122642517)\n",
      " state (8)  A[0]:(0.656470000744) A[1]:(0.000153891742229) A[2]:(0.729011416435) A[3]:(0.590414881706)\n",
      " state (9)  A[0]:(0.656395554543) A[1]:(0.810078561306) A[2]:(0.809992909431) A[3]:(6.16908073425e-06)\n",
      " state (10)  A[0]:(0.729184389114) A[1]:(0.900014877319) A[2]:(-0.000200986862183) A[3]:(0.728978872299)\n",
      " state (11)  A[0]:(0.515258610249) A[1]:(0.876856863499) A[2]:(-0.574959754944) A[3]:(0.841159105301)\n",
      " state (12)  A[0]:(0.0704961940646) A[1]:(0.824270963669) A[2]:(-0.507515788078) A[3]:(0.790799856186)\n",
      " state (13)  A[0]:(-8.54730606079e-05) A[1]:(0.809062600136) A[2]:(0.900058865547) A[3]:(0.728946328163)\n",
      " state (14)  A[0]:(0.810105860233) A[1]:(0.900300562382) A[2]:(0.999999821186) A[3]:(0.809958934784)\n",
      " state (15)  A[0]:(0.989006102085) A[1]:(0.961613893509) A[2]:(1.0) A[3]:(0.892317354679)\n",
      "Episode 393000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6093. Times reached goal: 979.               Steps done: 3512280. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0268459712549.\n",
      " state (0)  A[0]:(0.53137075901) A[1]:(0.590532898903) A[2]:(0.590550780296) A[3]:(0.531570672989)\n",
      " state (1)  A[0]:(0.531533241272) A[1]:(5.38304448128e-05) A[2]:(0.656205058098) A[3]:(0.590623736382)\n",
      " state (2)  A[0]:(0.590553939342) A[1]:(0.729013860226) A[2]:(0.590515255928) A[3]:(0.656304776669)\n",
      " state (3)  A[0]:(0.656092166901) A[1]:(-0.392654329538) A[2]:(0.548571228981) A[3]:(0.504851937294)\n",
      " state (4)  A[0]:(0.590675115585) A[1]:(0.656098365784) A[2]:(-2.92062759399e-05) A[3]:(0.531674981117)\n",
      " state (5)  A[0]:(0.165686517954) A[1]:(0.926988005638) A[2]:(-0.165496587753) A[3]:(0.511134028435)\n",
      " state (6)  A[0]:(0.000205963850021) A[1]:(0.810005784035) A[2]:(-9.78708267212e-05) A[3]:(0.656262159348)\n",
      " state (7)  A[0]:(0.637815833092) A[1]:(-0.248509272933) A[2]:(0.228861436248) A[3]:(0.900119721889)\n",
      " state (8)  A[0]:(0.656321525574) A[1]:(-0.000210992991924) A[2]:(0.729092240334) A[3]:(0.590495467186)\n",
      " state (9)  A[0]:(0.656306266785) A[1]:(0.809906840324) A[2]:(0.810073256493) A[3]:(-0.000142499804497)\n",
      " state (10)  A[0]:(0.729244232178) A[1]:(0.899947762489) A[2]:(-0.000232458114624) A[3]:(0.729076385498)\n",
      " state (11)  A[0]:(0.515553951263) A[1]:(0.876804232597) A[2]:(-0.575356721878) A[3]:(0.841333985329)\n",
      " state (12)  A[0]:(0.0710013359785) A[1]:(0.824223995209) A[2]:(-0.508278369904) A[3]:(0.79107862711)\n",
      " state (13)  A[0]:(0.000425875157816) A[1]:(0.809027194977) A[2]:(0.900041162968) A[3]:(0.729315757751)\n",
      " state (14)  A[0]:(0.81026661396) A[1]:(0.900277674198) A[2]:(0.999999821186) A[3]:(0.810249626637)\n",
      " state (15)  A[0]:(0.98898601532) A[1]:(0.961573004723) A[2]:(1.0) A[3]:(0.89241361618)\n",
      "Episode 394000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6074. Times reached goal: 970.               Steps done: 3518354. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.026683403045.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5905,  0.5907,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.0001,  0.6563,  0.5907]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7291,  0.5905,  0.6564]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0000,  0.8101,  0.0002,  0.6563]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0002,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9001,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531612753868) A[1]:(0.590513885021) A[2]:(0.590753853321) A[3]:(0.531495273113)\n",
      " state (1)  A[0]:(0.531794548035) A[1]:(0.000147756189108) A[2]:(0.656345188618) A[3]:(0.590616762638)\n",
      " state (2)  A[0]:(0.590721845627) A[1]:(0.729158520699) A[2]:(0.590552687645) A[3]:(0.656357765198)\n",
      " state (3)  A[0]:(0.656311750412) A[1]:(-0.391457259655) A[2]:(0.548473775387) A[3]:(0.504711568356)\n",
      " state (4)  A[0]:(0.590707659721) A[1]:(0.656343698502) A[2]:(8.30888748169e-05) A[3]:(0.531456947327)\n",
      " state (5)  A[0]:(0.165435910225) A[1]:(0.927052497864) A[2]:(-0.165348291397) A[3]:(0.511015355587)\n",
      " state (6)  A[0]:(-2.33352184296e-05) A[1]:(0.810140371323) A[2]:(0.000212669372559) A[3]:(0.656146705151)\n",
      " state (7)  A[0]:(0.637668132782) A[1]:(-0.248012602329) A[2]:(0.229413092136) A[3]:(0.899987399578)\n",
      " state (8)  A[0]:(0.656223833561) A[1]:(0.000522311718669) A[2]:(0.729232549667) A[3]:(0.590185761452)\n",
      " state (9)  A[0]:(0.656272888184) A[1]:(0.810124456882) A[2]:(0.810134291649) A[3]:(-0.00026535987854)\n",
      " state (10)  A[0]:(0.729206204414) A[1]:(0.90006428957) A[2]:(-6.91413879395e-06) A[3]:(0.728996813297)\n",
      " state (11)  A[0]:(0.515494167805) A[1]:(0.87694132328) A[2]:(-0.57539254427) A[3]:(0.841247320175)\n",
      " state (12)  A[0]:(0.0708533674479) A[1]:(0.824383020401) A[2]:(-0.508633434772) A[3]:(0.790910959244)\n",
      " state (13)  A[0]:(0.000109314918518) A[1]:(0.809123635292) A[2]:(0.900070607662) A[3]:(0.72903752327)\n",
      " state (14)  A[0]:(0.810096740723) A[1]:(0.90026140213) A[2]:(0.999999821186) A[3]:(0.810020446777)\n",
      " state (15)  A[0]:(0.988946795464) A[1]:(0.961509108543) A[2]:(1.0) A[3]:(0.892199873924)\n",
      "Episode 395000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6070. Times reached goal: 970.               Steps done: 3524424. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.026521925369.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531390845776) A[1]:(0.590424478054) A[2]:(0.590407013893) A[3]:(0.531458079815)\n",
      " state (1)  A[0]:(0.531472921371) A[1]:(-0.000183697789907) A[2]:(0.656006336212) A[3]:(0.59048384428)\n",
      " state (2)  A[0]:(0.5904494524) A[1]:(0.728937923908) A[2]:(0.590428829193) A[3]:(0.656129360199)\n",
      " state (3)  A[0]:(0.656026959419) A[1]:(-0.392718791962) A[2]:(0.548557043076) A[3]:(0.504676401615)\n",
      " state (4)  A[0]:(0.590536653996) A[1]:(0.655986428261) A[2]:(-0.000104784965515) A[3]:(0.531522274017)\n",
      " state (5)  A[0]:(0.165420293808) A[1]:(0.926989972591) A[2]:(-0.165814951062) A[3]:(0.511082530022)\n",
      " state (6)  A[0]:(8.13007354736e-05) A[1]:(0.809948444366) A[2]:(-0.000276923179626) A[3]:(0.656203150749)\n",
      " state (7)  A[0]:(0.637609839439) A[1]:(-0.248672321439) A[2]:(0.229125648737) A[3]:(0.900029957294)\n",
      " state (8)  A[0]:(0.656249761581) A[1]:(-0.000201430171728) A[2]:(0.72865831852) A[3]:(0.591150283813)\n",
      " state (9)  A[0]:(0.656160235405) A[1]:(0.809898614883) A[2]:(0.809806406498) A[3]:(0.00103913212661)\n",
      " state (10)  A[0]:(0.729046285152) A[1]:(0.899932742119) A[2]:(-0.000517845095601) A[3]:(0.729384064674)\n",
      " state (11)  A[0]:(0.515335202217) A[1]:(0.876777052879) A[2]:(-0.575914919376) A[3]:(0.841466426849)\n",
      " state (12)  A[0]:(0.0707388222218) A[1]:(0.824166953564) A[2]:(-0.509574890137) A[3]:(0.79118347168)\n",
      " state (13)  A[0]:(-2.28881835938e-05) A[1]:(0.808910727501) A[2]:(0.899935483932) A[3]:(0.729348778725)\n",
      " state (14)  A[0]:(0.810037136078) A[1]:(0.900158584118) A[2]:(0.999999821186) A[3]:(0.810226678848)\n",
      " state (15)  A[0]:(0.98891711235) A[1]:(0.961451411247) A[2]:(1.0) A[3]:(0.8922316432)\n",
      "Episode 396000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6059. Times reached goal: 977.               Steps done: 3530483. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.026361714871.\n",
      " state (0)  A[0]:(0.531477928162) A[1]:(0.590966939926) A[2]:(0.590730309486) A[3]:(0.531643271446)\n",
      " state (1)  A[0]:(0.53120046854) A[1]:(-0.00363514828496) A[2]:(0.655873656273) A[3]:(0.589498400688)\n",
      " state (2)  A[0]:(0.589573860168) A[1]:(0.729386150837) A[2]:(0.584401965141) A[3]:(0.656414031982)\n",
      " state (3)  A[0]:(0.655858755112) A[1]:(-0.309265673161) A[2]:(0.533560395241) A[3]:(0.512490868568)\n",
      " state (4)  A[0]:(0.588604927063) A[1]:(0.654846906662) A[2]:(-0.00136697210837) A[3]:(0.529485881329)\n",
      " state (5)  A[0]:(0.161580935121) A[1]:(0.926564872265) A[2]:(-0.167390272021) A[3]:(0.50800460577)\n",
      " state (6)  A[0]:(-0.00604014191777) A[1]:(0.809627652168) A[2]:(-0.00191271072254) A[3]:(0.652947306633)\n",
      " state (7)  A[0]:(0.633767724037) A[1]:(-0.249855861068) A[2]:(0.228260144591) A[3]:(0.89876127243)\n",
      " state (8)  A[0]:(0.653805077076) A[1]:(-0.00306363566779) A[2]:(0.728537321091) A[3]:(0.586697638035)\n",
      " state (9)  A[0]:(0.654273867607) A[1]:(0.80884629488) A[2]:(0.809744596481) A[3]:(-0.00622119940817)\n",
      " state (10)  A[0]:(0.727964401245) A[1]:(0.899397969246) A[2]:(-0.00114715052769) A[3]:(0.726326704025)\n",
      " state (11)  A[0]:(0.514151394367) A[1]:(0.87616610527) A[2]:(-0.576414585114) A[3]:(0.839715003967)\n",
      " state (12)  A[0]:(0.0695658773184) A[1]:(0.82333701849) A[2]:(-0.509568095207) A[3]:(0.789027631283)\n",
      " state (13)  A[0]:(-0.000344425439835) A[1]:(0.808045685291) A[2]:(0.900668680668) A[3]:(0.726843833923)\n",
      " state (14)  A[0]:(0.810454726219) A[1]:(0.899693191051) A[2]:(0.999999821186) A[3]:(0.808681607246)\n",
      " state (15)  A[0]:(0.988922297955) A[1]:(0.961209714413) A[2]:(1.0) A[3]:(0.891342401505)\n",
      "Episode 397000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6059. Times reached goal: 973.               Steps done: 3536542. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0262024721536.\n",
      " state (0)  A[0]:(0.530102729797) A[1]:(0.591262459755) A[2]:(0.590378880501) A[3]:(0.531380712986)\n",
      " state (1)  A[0]:(0.530890345573) A[1]:(-0.00276536913589) A[2]:(0.656601548195) A[3]:(0.589608669281)\n",
      " state (2)  A[0]:(0.588478803635) A[1]:(0.729224920273) A[2]:(0.581609249115) A[3]:(0.657978415489)\n",
      " state (3)  A[0]:(0.657796621323) A[1]:(-0.136320129037) A[2]:(0.510930538177) A[3]:(0.53201842308)\n",
      " state (4)  A[0]:(0.589829683304) A[1]:(0.656447291374) A[2]:(0.00163996068295) A[3]:(0.531954348087)\n",
      " state (5)  A[0]:(0.163363561034) A[1]:(0.927249670029) A[2]:(-0.167112857103) A[3]:(0.50967168808)\n",
      " state (6)  A[0]:(-4.63128089905e-05) A[1]:(0.81000995636) A[2]:(-0.000250697135925) A[3]:(0.656165838242)\n",
      " state (7)  A[0]:(0.638022184372) A[1]:(-0.251349210739) A[2]:(0.232226744294) A[3]:(0.900198578835)\n",
      " state (8)  A[0]:(0.655885338783) A[1]:(-0.000216323882341) A[2]:(0.729328274727) A[3]:(0.591734647751)\n",
      " state (9)  A[0]:(0.655782699585) A[1]:(0.809975147247) A[2]:(0.810167551041) A[3]:(0.000933363742661)\n",
      " state (10)  A[0]:(0.728710889816) A[1]:(0.899994254112) A[2]:(0.000147700309753) A[3]:(0.729205667973)\n",
      " state (11)  A[0]:(0.514807701111) A[1]:(0.876845538616) A[2]:(-0.575889468193) A[3]:(0.841361165047)\n",
      " state (12)  A[0]:(0.0696044042706) A[1]:(0.824218511581) A[2]:(-0.509727954865) A[3]:(0.790976524353)\n",
      " state (13)  A[0]:(-0.00155925622676) A[1]:(0.808890342712) A[2]:(0.900126099586) A[3]:(0.729070484638)\n",
      " state (14)  A[0]:(0.809728145599) A[1]:(0.900069236755) A[2]:(0.999999821186) A[3]:(0.810220181942)\n",
      " state (15)  A[0]:(0.988902211189) A[1]:(0.961381435394) A[2]:(1.0) A[3]:(0.892345011234)\n",
      "Episode 398000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6094. Times reached goal: 977.               Steps done: 3542636. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.02604327984.\n",
      " state (0)  A[0]:(0.530469357967) A[1]:(0.590216755867) A[2]:(0.59045702219) A[3]:(0.531376838684)\n",
      " state (1)  A[0]:(0.532383322716) A[1]:(-0.00168932811357) A[2]:(0.656170845032) A[3]:(0.58978497982)\n",
      " state (2)  A[0]:(0.588708877563) A[1]:(0.728914380074) A[2]:(0.588280916214) A[3]:(0.656904459)\n",
      " state (3)  A[0]:(0.657525539398) A[1]:(-0.0179106369615) A[2]:(0.498864680529) A[3]:(0.542576432228)\n",
      " state (4)  A[0]:(0.588855862617) A[1]:(0.655788421631) A[2]:(0.00101566279773) A[3]:(0.53161406517)\n",
      " state (5)  A[0]:(0.160815298557) A[1]:(0.927862524986) A[2]:(-0.170681625605) A[3]:(0.508499264717)\n",
      " state (6)  A[0]:(0.000265598297119) A[1]:(0.810048937798) A[2]:(-0.000965833372902) A[3]:(0.655814230442)\n",
      " state (7)  A[0]:(0.638751983643) A[1]:(-0.254233568907) A[2]:(0.235296562314) A[3]:(0.899457633495)\n",
      " state (8)  A[0]:(0.655788302422) A[1]:(0.000318840146065) A[2]:(0.728826582432) A[3]:(0.590922534466)\n",
      " state (9)  A[0]:(0.656350135803) A[1]:(0.810140609741) A[2]:(0.809899926186) A[3]:(0.000717729213648)\n",
      " state (10)  A[0]:(0.729330778122) A[1]:(0.900030612946) A[2]:(-0.000161647796631) A[3]:(0.729033231735)\n",
      " state (11)  A[0]:(0.515941500664) A[1]:(0.876771330833) A[2]:(-0.576723814011) A[3]:(0.841331541538)\n",
      " state (12)  A[0]:(0.0710355266929) A[1]:(0.823953688145) A[2]:(-0.511160492897) A[3]:(0.790937006474)\n",
      " state (13)  A[0]:(-0.000201851129532) A[1]:(0.808498680592) A[2]:(0.899928748608) A[3]:(0.728961110115)\n",
      " state (14)  A[0]:(0.810271441936) A[1]:(0.899856686592) A[2]:(0.999999821186) A[3]:(0.810005366802)\n",
      " state (15)  A[0]:(0.988924562931) A[1]:(0.961297035217) A[2]:(1.0) A[3]:(0.892084956169)\n",
      "Episode 399000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6062. Times reached goal: 979.               Steps done: 3548698. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0258858830293.\n",
      "q_values \n",
      "tensor([[ 0.5310,  0.5901,  0.5903,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.0001,  0.6560,  0.5897]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5895,  0.7290,  0.5912,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  0.8098, -0.0000,  0.6554]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7298,  0.9002,  0.0010,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9002,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531627297401) A[1]:(0.590085029602) A[2]:(0.590361833572) A[3]:(0.530661284924)\n",
      " state (1)  A[0]:(0.532313704491) A[1]:(-0.000559102685656) A[2]:(0.655844569206) A[3]:(0.589196264744)\n",
      " state (2)  A[0]:(0.590248525143) A[1]:(0.728900790215) A[2]:(0.591046214104) A[3]:(0.655797064304)\n",
      " state (3)  A[0]:(0.657553672791) A[1]:(-0.000863384222612) A[2]:(0.497310757637) A[3]:(0.542918682098)\n",
      " state (4)  A[0]:(0.590493917465) A[1]:(0.655955672264) A[2]:(-0.000280141830444) A[3]:(0.530807852745)\n",
      " state (5)  A[0]:(0.164482325315) A[1]:(0.927799880505) A[2]:(-0.171099632978) A[3]:(0.508167862892)\n",
      " state (6)  A[0]:(0.00206252629869) A[1]:(0.809838533401) A[2]:(-0.000463128060801) A[3]:(0.65539431572)\n",
      " state (7)  A[0]:(0.639206111431) A[1]:(-0.254506856203) A[2]:(0.235826998949) A[3]:(0.899028837681)\n",
      " state (8)  A[0]:(0.656600475311) A[1]:(-0.000554334314074) A[2]:(0.72859275341) A[3]:(0.590397238731)\n",
      " state (9)  A[0]:(0.656735956669) A[1]:(0.809849500656) A[2]:(0.809819161892) A[3]:(5.02169132233e-06)\n",
      " state (10)  A[0]:(0.729518949986) A[1]:(0.899916052818) A[2]:(-0.000417947740061) A[3]:(0.728937029839)\n",
      " state (11)  A[0]:(0.516450107098) A[1]:(0.876703500748) A[2]:(-0.577366411686) A[3]:(0.841444730759)\n",
      " state (12)  A[0]:(0.0719098746777) A[1]:(0.823984742165) A[2]:(-0.51240336895) A[3]:(0.791186034679)\n",
      " state (13)  A[0]:(0.000433236331446) A[1]:(0.808697640896) A[2]:(0.899666309357) A[3]:(0.729259610176)\n",
      " state (14)  A[0]:(0.810164690018) A[1]:(0.900071561337) A[2]:(0.999999821186) A[3]:(0.810086667538)\n",
      " state (15)  A[0]:(0.988873839378) A[1]:(0.961415290833) A[2]:(1.0) A[3]:(0.891968846321)\n",
      "Episode 400000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6074. Times reached goal: 979.               Steps done: 3554772. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0257291287206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531847715378) A[1]:(0.590389847755) A[2]:(0.590402305126) A[3]:(0.531554698944)\n",
      " state (1)  A[0]:(0.531611025333) A[1]:(-7.95386731625e-05) A[2]:(0.656024634838) A[3]:(0.590285181999)\n",
      " state (2)  A[0]:(0.590512514114) A[1]:(0.728927731514) A[2]:(0.590908169746) A[3]:(0.656559824944)\n",
      " state (3)  A[0]:(0.656606435776) A[1]:(-0.000562038214412) A[2]:(0.497202187777) A[3]:(0.543793082237)\n",
      " state (4)  A[0]:(0.589941680431) A[1]:(0.655997991562) A[2]:(-4.86373901367e-05) A[3]:(0.531589925289)\n",
      " state (5)  A[0]:(0.164390787482) A[1]:(0.927634179592) A[2]:(-0.170336008072) A[3]:(0.509215474129)\n",
      " state (6)  A[0]:(-0.00014129281044) A[1]:(0.809957742691) A[2]:(-7.39097595215e-05) A[3]:(0.65603017807)\n",
      " state (7)  A[0]:(0.637499690056) A[1]:(-0.253254115582) A[2]:(0.23569662869) A[3]:(0.899189591408)\n",
      " state (8)  A[0]:(0.655591249466) A[1]:(-0.000281777232885) A[2]:(0.72893434763) A[3]:(0.590503931046)\n",
      " state (9)  A[0]:(0.655673325062) A[1]:(0.809882223606) A[2]:(0.809951066971) A[3]:(-8.44746828079e-05)\n",
      " state (10)  A[0]:(0.728633403778) A[1]:(0.899945318699) A[2]:(-0.000316023826599) A[3]:(0.728910446167)\n",
      " state (11)  A[0]:(0.515188574791) A[1]:(0.876789271832) A[2]:(-0.577474653721) A[3]:(0.841390430927)\n",
      " state (12)  A[0]:(0.0703503564) A[1]:(0.824174642563) A[2]:(-0.512696266174) A[3]:(0.791060984135)\n",
      " state (13)  A[0]:(-0.00108224106953) A[1]:(0.808963179588) A[2]:(0.899852991104) A[3]:(0.729029715061)\n",
      " state (14)  A[0]:(0.80960470438) A[1]:(0.90022611618) A[2]:(0.999999821186) A[3]:(0.809899806976)\n",
      " state (15)  A[0]:(0.988796591759) A[1]:(0.961440742016) A[2]:(1.0) A[3]:(0.891736388206)\n",
      "Episode 401000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6078. Times reached goal: 977.               Steps done: 3560850. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0255732213586.\n",
      " state (0)  A[0]:(0.531817793846) A[1]:(0.590961933136) A[2]:(0.590739369392) A[3]:(0.532116413116)\n",
      " state (1)  A[0]:(0.531785130501) A[1]:(0.000186581164598) A[2]:(0.65637242794) A[3]:(0.590908885002)\n",
      " state (2)  A[0]:(0.590815901756) A[1]:(0.728982567787) A[2]:(0.591497063637) A[3]:(0.656778454781)\n",
      " state (3)  A[0]:(0.657064020634) A[1]:(-0.0061069377698) A[2]:(0.499325931072) A[3]:(0.543579936028)\n",
      " state (4)  A[0]:(0.590942263603) A[1]:(0.656329035759) A[2]:(0.000410795182688) A[3]:(0.531809449196)\n",
      " state (5)  A[0]:(0.165500372648) A[1]:(0.927698194981) A[2]:(-0.171200424433) A[3]:(0.509527742863)\n",
      " state (6)  A[0]:(0.000570356787648) A[1]:(0.809860229492) A[2]:(-0.00085973716341) A[3]:(0.6564463377)\n",
      " state (7)  A[0]:(0.638322949409) A[1]:(-0.253807276487) A[2]:(0.235987186432) A[3]:(0.899413526058)\n",
      " state (8)  A[0]:(0.65671813488) A[1]:(-0.000779073510785) A[2]:(0.728942632675) A[3]:(0.591434955597)\n",
      " state (9)  A[0]:(0.657029271126) A[1]:(0.80985224247) A[2]:(0.809928894043) A[3]:(0.000589251460042)\n",
      " state (10)  A[0]:(0.729677915573) A[1]:(0.899944841862) A[2]:(-0.000283241271973) A[3]:(0.729158401489)\n",
      " state (11)  A[0]:(0.516683697701) A[1]:(0.876808404922) A[2]:(-0.577605307102) A[3]:(0.84158885479)\n",
      " state (12)  A[0]:(0.0721668601036) A[1]:(0.824268698692) A[2]:(-0.513078153133) A[3]:(0.791339159012)\n",
      " state (13)  A[0]:(0.000499933899846) A[1]:(0.809170603752) A[2]:(0.899851918221) A[3]:(0.729334712029)\n",
      " state (14)  A[0]:(0.810109138489) A[1]:(0.900413930416) A[2]:(0.999999821186) A[3]:(0.810045421124)\n",
      " state (15)  A[0]:(0.988806545734) A[1]:(0.961530029774) A[2]:(1.0) A[3]:(0.89169460535)\n",
      "Episode 402000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6077. Times reached goal: 978.               Steps done: 3566927. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.025418284146.\n",
      " state (0)  A[0]:(0.531199216843) A[1]:(0.590732812881) A[2]:(0.590481162071) A[3]:(0.531703710556)\n",
      " state (1)  A[0]:(0.531292200089) A[1]:(0.0006712077884) A[2]:(0.656157016754) A[3]:(0.590704858303)\n",
      " state (2)  A[0]:(0.590426981449) A[1]:(0.729076087475) A[2]:(0.591738939285) A[3]:(0.656442761421)\n",
      " state (3)  A[0]:(0.656424045563) A[1]:(-0.0153976129368) A[2]:(0.50106471777) A[3]:(0.542477488518)\n",
      " state (4)  A[0]:(0.590618133545) A[1]:(0.656295537949) A[2]:(0.000271916389465) A[3]:(0.531651437283)\n",
      " state (5)  A[0]:(0.16511118412) A[1]:(0.92781829834) A[2]:(-0.171838521957) A[3]:(0.509611487389)\n",
      " state (6)  A[0]:(0.000197291374207) A[1]:(0.810148119926) A[2]:(-0.000852465396747) A[3]:(0.656211078167)\n",
      " state (7)  A[0]:(0.638026535511) A[1]:(-0.252484977245) A[2]:(0.237052276731) A[3]:(0.898995399475)\n",
      " state (8)  A[0]:(0.65630710125) A[1]:(0.000332832336426) A[2]:(0.729224801064) A[3]:(0.590605556965)\n",
      " state (9)  A[0]:(0.656652927399) A[1]:(0.810043394566) A[2]:(0.810109853745) A[3]:(8.04960727692e-05)\n",
      " state (10)  A[0]:(0.729480564594) A[1]:(0.900011599064) A[2]:(9.17911529541e-06) A[3]:(0.729086995125)\n",
      " state (11)  A[0]:(0.51650583744) A[1]:(0.876834332943) A[2]:(-0.577670812607) A[3]:(0.841583073139)\n",
      " state (12)  A[0]:(0.0719898641109) A[1]:(0.824199974537) A[2]:(-0.513299703598) A[3]:(0.791284322739)\n",
      " state (13)  A[0]:(0.000405311555369) A[1]:(0.808954596519) A[2]:(0.899991750717) A[3]:(0.729244053364)\n",
      " state (14)  A[0]:(0.810134470463) A[1]:(0.900195837021) A[2]:(0.999999821186) A[3]:(0.810100018978)\n",
      " state (15)  A[0]:(0.988785684109) A[1]:(0.961371243) A[2]:(1.0) A[3]:(0.891750872135)\n",
      "Episode 403000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6051. Times reached goal: 974.               Steps done: 3572978. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0252649425116.\n",
      " state (0)  A[0]:(0.530544757843) A[1]:(0.59084379673) A[2]:(0.590982317924) A[3]:(0.530875742435)\n",
      " state (1)  A[0]:(0.531006217003) A[1]:(-0.000100687146187) A[2]:(0.655877172947) A[3]:(0.590651810169)\n",
      " state (2)  A[0]:(0.590183496475) A[1]:(0.729158520699) A[2]:(0.592268943787) A[3]:(0.656324505806)\n",
      " state (3)  A[0]:(0.656024932861) A[1]:(-0.02062080428) A[2]:(0.503552556038) A[3]:(0.541384339333)\n",
      " state (4)  A[0]:(0.590429067612) A[1]:(0.65603530407) A[2]:(0.00188958423678) A[3]:(0.530934691429)\n",
      " state (5)  A[0]:(0.164819180965) A[1]:(0.927874147892) A[2]:(-0.171506360173) A[3]:(0.509298920631)\n",
      " state (6)  A[0]:(0.000800788227934) A[1]:(0.809706270695) A[2]:(-0.000137567520142) A[3]:(0.656109571457)\n",
      " state (7)  A[0]:(0.638112902641) A[1]:(-0.253963440657) A[2]:(0.238249242306) A[3]:(0.898667931557)\n",
      " state (8)  A[0]:(0.655542194843) A[1]:(-0.000634595693555) A[2]:(0.728830575943) A[3]:(0.590387284756)\n",
      " state (9)  A[0]:(0.65561234951) A[1]:(0.809681117535) A[2]:(0.809798777103) A[3]:(-0.000148609280586)\n",
      " state (10)  A[0]:(0.728678166866) A[1]:(0.899786233902) A[2]:(-0.000964522070717) A[3]:(0.729028344154)\n",
      " state (11)  A[0]:(0.515491306782) A[1]:(0.876514434814) A[2]:(-0.578645586967) A[3]:(0.841641545296)\n",
      " state (12)  A[0]:(0.0709723383188) A[1]:(0.823725402355) A[2]:(-0.514426529408) A[3]:(0.791377425194)\n",
      " state (13)  A[0]:(8.29994678497e-05) A[1]:(0.808480679989) A[2]:(0.900101602077) A[3]:(0.729382634163)\n",
      " state (14)  A[0]:(0.810424983501) A[1]:(0.900003254414) A[2]:(0.999999821186) A[3]:(0.810274124146)\n",
      " state (15)  A[0]:(0.98878800869) A[1]:(0.961283326149) A[2]:(1.0) A[3]:(0.8917760849)\n",
      "Episode 404000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6048. Times reached goal: 963.               Steps done: 3579026. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0251126012835.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5903,  0.5911,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.0001,  0.6562,  0.5909]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5910,  0.7291,  0.5913,  0.6567]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0007,  0.8099, -0.0008,  0.6565]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7295,  0.9000, -0.0003,  0.7294]], device='cuda:0')\n",
      "On state=10, selected action=3 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0010,  0.8100, -0.0007,  0.6565]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7294,  0.9000, -0.0003,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9003,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531789243221) A[1]:(0.590353786945) A[2]:(0.591038107872) A[3]:(0.53139179945)\n",
      " state (1)  A[0]:(0.531996846199) A[1]:(0.000137783586979) A[2]:(0.656227946281) A[3]:(0.590701818466)\n",
      " state (2)  A[0]:(0.591008841991) A[1]:(0.729114294052) A[2]:(0.591339230537) A[3]:(0.656560957432)\n",
      " state (3)  A[0]:(0.656246423721) A[1]:(-0.00714873243123) A[2]:(0.500973820686) A[3]:(0.54280012846)\n",
      " state (4)  A[0]:(0.590226531029) A[1]:(0.65628015995) A[2]:(0.000300765037537) A[3]:(0.531470298767)\n",
      " state (5)  A[0]:(0.164376750588) A[1]:(0.927935540676) A[2]:(-0.173234879971) A[3]:(0.509954810143)\n",
      " state (6)  A[0]:(0.000121414661407) A[1]:(0.810048997402) A[2]:(-0.000944137282204) A[3]:(0.656147003174)\n",
      " state (7)  A[0]:(0.638249218464) A[1]:(-0.252751678228) A[2]:(0.239050120115) A[3]:(0.898399949074)\n",
      " state (8)  A[0]:(0.656071126461) A[1]:(0.000240065157413) A[2]:(0.729085206985) A[3]:(0.590168893337)\n",
      " state (9)  A[0]:(0.656176686287) A[1]:(0.810011267662) A[2]:(0.810011029243) A[3]:(-0.000298485159874)\n",
      " state (10)  A[0]:(0.729056119919) A[1]:(0.899979889393) A[2]:(-0.000492930354085) A[3]:(0.729046821594)\n",
      " state (11)  A[0]:(0.515843153) A[1]:(0.876766741276) A[2]:(-0.578567862511) A[3]:(0.841655611992)\n",
      " state (12)  A[0]:(0.0709866583347) A[1]:(0.82411390543) A[2]:(-0.514663338661) A[3]:(0.791274189949)\n",
      " state (13)  A[0]:(-0.000528842152562) A[1]:(0.808946788311) A[2]:(0.899993300438) A[3]:(0.729043245316)\n",
      " state (14)  A[0]:(0.809994935989) A[1]:(0.900301277637) A[2]:(0.999999821186) A[3]:(0.809888005257)\n",
      " state (15)  A[0]:(0.988740622997) A[1]:(0.961416184902) A[2]:(1.0) A[3]:(0.891458213329)\n",
      "Episode 405000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6062. Times reached goal: 972.               Steps done: 3585088. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0249608291806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531589925289) A[1]:(0.590328216553) A[2]:(0.590531349182) A[3]:(0.531556665897)\n",
      " state (1)  A[0]:(0.53163588047) A[1]:(0.000179938971996) A[2]:(0.655958175659) A[3]:(0.590415120125)\n",
      " state (2)  A[0]:(0.590575814247) A[1]:(0.728944540024) A[2]:(0.591057658195) A[3]:(0.656134605408)\n",
      " state (3)  A[0]:(0.656306564808) A[1]:(-0.00859183538705) A[2]:(0.501541733742) A[3]:(0.542519390583)\n",
      " state (4)  A[0]:(0.590512037277) A[1]:(0.656236469746) A[2]:(0.000308156013489) A[3]:(0.531508982182)\n",
      " state (5)  A[0]:(0.164331898093) A[1]:(0.927962958813) A[2]:(-0.17363679409) A[3]:(0.510033845901)\n",
      " state (6)  A[0]:(-6.32405281067e-05) A[1]:(0.809966385365) A[2]:(-0.000515818537679) A[3]:(0.655842661858)\n",
      " state (7)  A[0]:(0.638293623924) A[1]:(-0.253038436174) A[2]:(0.240569591522) A[3]:(0.898039579391)\n",
      " state (8)  A[0]:(0.656245470047) A[1]:(-0.000168092548847) A[2]:(0.729100584984) A[3]:(0.590269207954)\n",
      " state (9)  A[0]:(0.656230866909) A[1]:(0.809943318367) A[2]:(0.810003399849) A[3]:(-1.7985701561e-05)\n",
      " state (10)  A[0]:(0.729085743427) A[1]:(0.899987101555) A[2]:(-0.000112056732178) A[3]:(0.728937625885)\n",
      " state (11)  A[0]:(0.516035079956) A[1]:(0.876800239086) A[2]:(-0.578392803669) A[3]:(0.841588139534)\n",
      " state (12)  A[0]:(0.0713785588741) A[1]:(0.824185431004) A[2]:(-0.514652371407) A[3]:(0.791196405888)\n",
      " state (13)  A[0]:(-0.00017112493515) A[1]:(0.809038519859) A[2]:(0.900072515011) A[3]:(0.728994369507)\n",
      " state (14)  A[0]:(0.810052514076) A[1]:(0.900357842445) A[2]:(0.999999821186) A[3]:(0.810002803802)\n",
      " state (15)  A[0]:(0.988722145557) A[1]:(0.961423754692) A[2]:(1.0) A[3]:(0.891582906246)\n",
      "Episode 406000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6069. Times reached goal: 972.               Steps done: 3591157. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0248098006678.\n",
      " state (0)  A[0]:(0.531192421913) A[1]:(0.590218901634) A[2]:(0.590391159058) A[3]:(0.531396925449)\n",
      " state (1)  A[0]:(0.531377673149) A[1]:(0.000116493552923) A[2]:(0.655958890915) A[3]:(0.590445816517)\n",
      " state (2)  A[0]:(0.590245842934) A[1]:(0.728842020035) A[2]:(0.591325581074) A[3]:(0.656155288219)\n",
      " state (3)  A[0]:(0.655961751938) A[1]:(-0.00863843876868) A[2]:(0.50239521265) A[3]:(0.542487502098)\n",
      " state (4)  A[0]:(0.590327858925) A[1]:(0.655923008919) A[2]:(0.000207304954529) A[3]:(0.531626582146)\n",
      " state (5)  A[0]:(0.164053365588) A[1]:(0.92805147171) A[2]:(-0.175035730004) A[3]:(0.510448515415)\n",
      " state (6)  A[0]:(-0.000160753726959) A[1]:(0.810015201569) A[2]:(-0.00130891730078) A[3]:(0.655993700027)\n",
      " state (7)  A[0]:(0.637944698334) A[1]:(-0.252717196941) A[2]:(0.241180017591) A[3]:(0.897786915302)\n",
      " state (8)  A[0]:(0.655710816383) A[1]:(0.000480260670884) A[2]:(0.728936553001) A[3]:(0.59024810791)\n",
      " state (9)  A[0]:(0.655983805656) A[1]:(0.8101323843) A[2]:(0.809987068176) A[3]:(0.000206544995308)\n",
      " state (10)  A[0]:(0.729034304619) A[1]:(0.900047361851) A[2]:(3.77893447876e-05) A[3]:(0.729028224945)\n",
      " state (11)  A[0]:(0.516100525856) A[1]:(0.876802802086) A[2]:(-0.578491568565) A[3]:(0.841671824455)\n",
      " state (12)  A[0]:(0.0715582072735) A[1]:(0.824095308781) A[2]:(-0.515048623085) A[3]:(0.79126560688)\n",
      " state (13)  A[0]:(1.27553939819e-05) A[1]:(0.808834552765) A[2]:(0.899992465973) A[3]:(0.729009985924)\n",
      " state (14)  A[0]:(0.810133576393) A[1]:(0.900187432766) A[2]:(0.999999821186) A[3]:(0.810001373291)\n",
      " state (15)  A[0]:(0.988716721535) A[1]:(0.961320161819) A[2]:(1.0) A[3]:(0.891551494598)\n",
      "Episode 407000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6058. Times reached goal: 969.               Steps done: 3597215. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0246599572294.\n",
      " state (0)  A[0]:(0.531449079514) A[1]:(0.59075152874) A[2]:(0.590508759022) A[3]:(0.531371235847)\n",
      " state (1)  A[0]:(0.531405687332) A[1]:(4.20957803726e-05) A[2]:(0.656282067299) A[3]:(0.590450167656)\n",
      " state (2)  A[0]:(0.590347588062) A[1]:(0.729068398476) A[2]:(0.591270446777) A[3]:(0.656332910061)\n",
      " state (3)  A[0]:(0.656265437603) A[1]:(-0.00674498267472) A[2]:(0.502291381359) A[3]:(0.542325377464)\n",
      " state (4)  A[0]:(0.590568780899) A[1]:(0.656474411488) A[2]:(0.00014340877533) A[3]:(0.531469166279)\n",
      " state (5)  A[0]:(0.16379481554) A[1]:(0.928085207939) A[2]:(-0.174770176411) A[3]:(0.510688066483)\n",
      " state (6)  A[0]:(-0.000409752101405) A[1]:(0.809950530529) A[2]:(-0.000207662582397) A[3]:(0.656226873398)\n",
      " state (7)  A[0]:(0.638352930546) A[1]:(-0.252629578114) A[2]:(0.242865890265) A[3]:(0.897765338421)\n",
      " state (8)  A[0]:(0.656348288059) A[1]:(0.000340029597282) A[2]:(0.729064464569) A[3]:(0.590938448906)\n",
      " state (9)  A[0]:(0.656375229359) A[1]:(0.810057163239) A[2]:(0.809985756874) A[3]:(0.000396549672587)\n",
      " state (10)  A[0]:(0.729123353958) A[1]:(0.900005221367) A[2]:(-1.60932540894e-05) A[3]:(0.728942871094)\n",
      " state (11)  A[0]:(0.516079187393) A[1]:(0.876736581326) A[2]:(-0.578775346279) A[3]:(0.84166175127)\n",
      " state (12)  A[0]:(0.0713684782386) A[1]:(0.823981881142) A[2]:(-0.515568733215) A[3]:(0.791272163391)\n",
      " state (13)  A[0]:(-0.000277310609818) A[1]:(0.80869191885) A[2]:(0.900014638901) A[3]:(0.729029536247)\n",
      " state (14)  A[0]:(0.81003767252) A[1]:(0.900100708008) A[2]:(0.999999821186) A[3]:(0.810044825077)\n",
      " state (15)  A[0]:(0.988687932491) A[1]:(0.961258590221) A[2]:(1.0) A[3]:(0.891532063484)\n",
      "Episode 408000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6058. Times reached goal: 974.               Steps done: 3603273. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0245110187985.\n",
      " state (0)  A[0]:(0.531633079052) A[1]:(0.590585231781) A[2]:(0.590358257294) A[3]:(0.531473994255)\n",
      " state (1)  A[0]:(0.531776070595) A[1]:(0.000260673463345) A[2]:(0.656703710556) A[3]:(0.590330123901)\n",
      " state (2)  A[0]:(0.590475976467) A[1]:(0.728891551495) A[2]:(0.592737793922) A[3]:(0.65602183342)\n",
      " state (3)  A[0]:(0.65595805645) A[1]:(-0.0056347027421) A[2]:(0.506108283997) A[3]:(0.542080521584)\n",
      " state (4)  A[0]:(0.590442240238) A[1]:(0.655815005302) A[2]:(0.00514788832515) A[3]:(0.531180858612)\n",
      " state (5)  A[0]:(0.164209321141) A[1]:(0.928258538246) A[2]:(-0.171830296516) A[3]:(0.51066505909)\n",
      " state (6)  A[0]:(0.000538498105016) A[1]:(0.810542464256) A[2]:(0.00262474408373) A[3]:(0.65608304739)\n",
      " state (7)  A[0]:(0.63900667429) A[1]:(-0.250835210085) A[2]:(0.245424196124) A[3]:(0.897678673267)\n",
      " state (8)  A[0]:(0.657379627228) A[1]:(0.00186847674195) A[2]:(0.728448271751) A[3]:(0.593834042549)\n",
      " state (9)  A[0]:(0.657118976116) A[1]:(0.810576975346) A[2]:(0.809697270393) A[3]:(0.0043101045303)\n",
      " state (10)  A[0]:(0.729333758354) A[1]:(0.900280117989) A[2]:(0.000750303152017) A[3]:(0.729748010635)\n",
      " state (11)  A[0]:(0.516236960888) A[1]:(0.877018690109) A[2]:(-0.578183293343) A[3]:(0.841938138008)\n",
      " state (12)  A[0]:(0.0714892446995) A[1]:(0.824312090874) A[2]:(-0.515361726284) A[3]:(0.791459679604)\n",
      " state (13)  A[0]:(-0.000538706721272) A[1]:(0.808960437775) A[2]:(0.899844408035) A[3]:(0.729049444199)\n",
      " state (14)  A[0]:(0.809725999832) A[1]:(0.900190114975) A[2]:(0.999999821186) A[3]:(0.809892475605)\n",
      " state (15)  A[0]:(0.988658905029) A[1]:(0.961278557777) A[2]:(1.0) A[3]:(0.891368031502)\n",
      "Episode 409000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6069. Times reached goal: 977.               Steps done: 3609342. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0243627119179.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5908,  0.5904,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.6565,  0.0012,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558, -0.0003,  0.7290,  0.5906]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.8099,  0.8100, -0.0004]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0000,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9005,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53122150898) A[1]:(0.590779006481) A[2]:(0.590631246567) A[3]:(0.531781792641)\n",
      " state (1)  A[0]:(0.531633853912) A[1]:(0.0007737686974) A[2]:(0.656245112419) A[3]:(0.59063076973)\n",
      " state (2)  A[0]:(0.590460896492) A[1]:(0.729022741318) A[2]:(0.592119812965) A[3]:(0.656063735485)\n",
      " state (3)  A[0]:(0.656077265739) A[1]:(-0.0200662985444) A[2]:(0.506300210953) A[3]:(0.541139245033)\n",
      " state (4)  A[0]:(0.590744376183) A[1]:(0.656284928322) A[2]:(0.00101280177478) A[3]:(0.531578958035)\n",
      " state (5)  A[0]:(0.163600161672) A[1]:(0.928315341473) A[2]:(-0.176180317998) A[3]:(0.510993719101)\n",
      " state (6)  A[0]:(-0.000141799449921) A[1]:(0.810017466545) A[2]:(-0.000703096273355) A[3]:(0.656107068062)\n",
      " state (7)  A[0]:(0.638785004616) A[1]:(-0.253119051456) A[2]:(0.244599372149) A[3]:(0.897314429283)\n",
      " state (8)  A[0]:(0.656640172005) A[1]:(-8.69520008564e-05) A[2]:(0.728914499283) A[3]:(0.590965986252)\n",
      " state (9)  A[0]:(0.656898856163) A[1]:(0.81003844738) A[2]:(0.809975385666) A[3]:(0.000458821625216)\n",
      " state (10)  A[0]:(0.729612767696) A[1]:(0.900036215782) A[2]:(-0.000336289405823) A[3]:(0.729172348976)\n",
      " state (11)  A[0]:(0.516790747643) A[1]:(0.876793444157) A[2]:(-0.579706788063) A[3]:(0.841930806637)\n",
      " state (12)  A[0]:(0.0720620602369) A[1]:(0.824146568775) A[2]:(-0.517140030861) A[3]:(0.79151558876)\n",
      " state (13)  A[0]:(0.000148713588715) A[1]:(0.809055626392) A[2]:(0.899894595146) A[3]:(0.729087471962)\n",
      " state (14)  A[0]:(0.810109496117) A[1]:(0.900464653969) A[2]:(0.999999821186) A[3]:(0.809893250465)\n",
      " state (15)  A[0]:(0.98864197731) A[1]:(0.961436927319) A[2]:(1.0) A[3]:(0.891194283962)\n",
      "Episode 410000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6046. Times reached goal: 974.               Steps done: 3615388. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0242158593443.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531444430351) A[1]:(0.590770542622) A[2]:(0.589112520218) A[3]:(0.531342804432)\n",
      " state (1)  A[0]:(0.531625986099) A[1]:(0.000795177940745) A[2]:(0.656081318855) A[3]:(0.590218484402)\n",
      " state (2)  A[0]:(0.590322971344) A[1]:(0.729280114174) A[2]:(0.592309296131) A[3]:(0.65595895052)\n",
      " state (3)  A[0]:(0.655797243118) A[1]:(-0.0029337098822) A[2]:(0.506704330444) A[3]:(0.541779279709)\n",
      " state (4)  A[0]:(0.590298771858) A[1]:(0.656524002552) A[2]:(0.00530333304778) A[3]:(0.530955553055)\n",
      " state (5)  A[0]:(0.16356742382) A[1]:(0.928546071053) A[2]:(-0.172519147396) A[3]:(0.510917186737)\n",
      " state (6)  A[0]:(0.000595778168645) A[1]:(0.810738325119) A[2]:(0.00363264861517) A[3]:(0.655943155289)\n",
      " state (7)  A[0]:(0.639145851135) A[1]:(-0.250552862883) A[2]:(0.248708963394) A[3]:(0.897012412548)\n",
      " state (8)  A[0]:(0.657001614571) A[1]:(0.0023067179136) A[2]:(0.72912967205) A[3]:(0.592694938183)\n",
      " state (9)  A[0]:(0.656679749489) A[1]:(0.810661673546) A[2]:(0.810194611549) A[3]:(0.00253761769272)\n",
      " state (10)  A[0]:(0.729106426239) A[1]:(0.900321185589) A[2]:(0.00177287869155) A[3]:(0.729087710381)\n",
      " state (11)  A[0]:(0.516054213047) A[1]:(0.877048373222) A[2]:(-0.578125178814) A[3]:(0.841672480106)\n",
      " state (12)  A[0]:(0.0713120251894) A[1]:(0.824354171753) A[2]:(-0.515482068062) A[3]:(0.791104495525)\n",
      " state (13)  A[0]:(-0.000418603391154) A[1]:(0.809077858925) A[2]:(0.900453388691) A[3]:(0.728616952896)\n",
      " state (14)  A[0]:(0.8099219203) A[1]:(0.900340080261) A[2]:(0.999999821186) A[3]:(0.809750497341)\n",
      " state (15)  A[0]:(0.988607168198) A[1]:(0.961303532124) A[2]:(1.0) A[3]:(0.891202449799)\n",
      "Episode 411000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6072. Times reached goal: 979.               Steps done: 3621460. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0240692661537.\n",
      " state (0)  A[0]:(0.533427536488) A[1]:(0.590293645859) A[2]:(0.590317964554) A[3]:(0.531196177006)\n",
      " state (1)  A[0]:(0.53345990181) A[1]:(0.00012718513608) A[2]:(0.656133592129) A[3]:(0.590373754501)\n",
      " state (2)  A[0]:(0.591859459877) A[1]:(0.728999853134) A[2]:(0.592248260975) A[3]:(0.65593880415)\n",
      " state (3)  A[0]:(0.657097697258) A[1]:(-0.020638525486) A[2]:(0.507835030556) A[3]:(0.540406227112)\n",
      " state (4)  A[0]:(0.591938853264) A[1]:(0.6556468606) A[2]:(0.00163698045071) A[3]:(0.531070649624)\n",
      " state (5)  A[0]:(0.164964064956) A[1]:(0.928388416767) A[2]:(-0.177312657237) A[3]:(0.511137366295)\n",
      " state (6)  A[0]:(0.00104039872531) A[1]:(0.809970319271) A[2]:(-0.000712633016519) A[3]:(0.655675649643)\n",
      " state (7)  A[0]:(0.638562560081) A[1]:(-0.253026604652) A[2]:(0.24693185091) A[3]:(0.896352112293)\n",
      " state (8)  A[0]:(0.655137598515) A[1]:(-2.52239406109e-05) A[2]:(0.729446649551) A[3]:(0.588401675224)\n",
      " state (9)  A[0]:(0.654933094978) A[1]:(0.809950947762) A[2]:(0.810301780701) A[3]:(-0.00266035762616)\n",
      " state (10)  A[0]:(0.72820276022) A[1]:(0.899986445904) A[2]:(-0.000339984893799) A[3]:(0.728363513947)\n",
      " state (11)  A[0]:(0.514907300472) A[1]:(0.876701056957) A[2]:(-0.580505490303) A[3]:(0.841710567474)\n",
      " state (12)  A[0]:(0.0697109103203) A[1]:(0.823969602585) A[2]:(-0.518420100212) A[3]:(0.791288137436)\n",
      " state (13)  A[0]:(-0.0019094622694) A[1]:(0.808839023113) A[2]:(0.900015354156) A[3]:(0.728837907314)\n",
      " state (14)  A[0]:(0.809570789337) A[1]:(0.900353312492) A[2]:(0.999999821186) A[3]:(0.809881806374)\n",
      " state (15)  A[0]:(0.988558113575) A[1]:(0.961337208748) A[2]:(1.0) A[3]:(0.89114677906)\n",
      "Episode 412000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6060. Times reached goal: 973.               Steps done: 3627520. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0239238474645.\n",
      " state (0)  A[0]:(0.531089067459) A[1]:(0.590215682983) A[2]:(0.590556561947) A[3]:(0.530585408211)\n",
      " state (1)  A[0]:(0.531675219536) A[1]:(0.000212747603655) A[2]:(0.655848443508) A[3]:(0.589919984341)\n",
      " state (2)  A[0]:(0.590725779533) A[1]:(0.728893041611) A[2]:(0.591539144516) A[3]:(0.655560135841)\n",
      " state (3)  A[0]:(0.656252980232) A[1]:(-0.0261412709951) A[2]:(0.508297324181) A[3]:(0.539220929146)\n",
      " state (4)  A[0]:(0.591591835022) A[1]:(0.656042993069) A[2]:(0.00084912753664) A[3]:(0.530455291271)\n",
      " state (5)  A[0]:(0.165226906538) A[1]:(0.928548038006) A[2]:(-0.178326785564) A[3]:(0.510888576508)\n",
      " state (6)  A[0]:(0.00310938549228) A[1]:(0.809960782528) A[2]:(-0.000889181857929) A[3]:(0.65560400486)\n",
      " state (7)  A[0]:(0.640545964241) A[1]:(-0.253244966269) A[2]:(0.247486442327) A[3]:(0.896162509918)\n",
      " state (8)  A[0]:(0.657127141953) A[1]:(0.00012256577611) A[2]:(0.72876816988) A[3]:(0.589363098145)\n",
      " state (9)  A[0]:(0.656962990761) A[1]:(0.81001162529) A[2]:(0.809824109077) A[3]:(-0.000714495661668)\n",
      " state (10)  A[0]:(0.729830861092) A[1]:(0.90002310276) A[2]:(-0.00125813414343) A[3]:(0.729087710381)\n",
      " state (11)  A[0]:(0.517514765263) A[1]:(0.876756608486) A[2]:(-0.58121740818) A[3]:(0.842173814774)\n",
      " state (12)  A[0]:(0.0732844695449) A[1]:(0.824093818665) A[2]:(-0.519459247589) A[3]:(0.791879594326)\n",
      " state (13)  A[0]:(0.00152474525385) A[1]:(0.809040427208) A[2]:(0.899748563766) A[3]:(0.729548811913)\n",
      " state (14)  A[0]:(0.810627639294) A[1]:(0.900513827801) A[2]:(0.999999821186) A[3]:(0.810404539108)\n",
      " state (15)  A[0]:(0.988607823849) A[1]:(0.961414217949) A[2]:(1.0) A[3]:(0.891429483891)\n",
      "Episode 413000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6062. Times reached goal: 977.               Steps done: 3633582. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0237792597891.\n",
      " state (0)  A[0]:(0.531402826309) A[1]:(0.590440392494) A[2]:(0.59047973156) A[3]:(0.531218647957)\n",
      " state (1)  A[0]:(0.531487703323) A[1]:(0.000392951042159) A[2]:(0.656082868576) A[3]:(0.59049654007)\n",
      " state (2)  A[0]:(0.59072303772) A[1]:(0.728935837746) A[2]:(0.592219591141) A[3]:(0.655731678009)\n",
      " state (3)  A[0]:(0.655614495277) A[1]:(-0.1034938097) A[2]:(0.518324136734) A[3]:(0.533068418503)\n",
      " state (4)  A[0]:(0.591408371925) A[1]:(0.656123578548) A[2]:(-1.56164169312e-05) A[3]:(0.531248211861)\n",
      " state (5)  A[0]:(0.163619741797) A[1]:(0.928476691246) A[2]:(-0.178206413984) A[3]:(0.511928796768)\n",
      " state (6)  A[0]:(-0.000178605318069) A[1]:(0.809915006161) A[2]:(-0.000398755044444) A[3]:(0.655833363533)\n",
      " state (7)  A[0]:(0.638920664787) A[1]:(-0.252384334803) A[2]:(0.248323202133) A[3]:(0.896224975586)\n",
      " state (8)  A[0]:(0.6565554142) A[1]:(-8.51973891258e-05) A[2]:(0.728915452957) A[3]:(0.590524077415)\n",
      " state (9)  A[0]:(0.656381487846) A[1]:(0.809965372086) A[2]:(0.809928238392) A[3]:(0.00031241774559)\n",
      " state (10)  A[0]:(0.729123353958) A[1]:(0.899978339672) A[2]:(-0.000102758407593) A[3]:(0.728962481022)\n",
      " state (11)  A[0]:(0.516331911087) A[1]:(0.876661539078) A[2]:(-0.58030217886) A[3]:(0.841945409775)\n",
      " state (12)  A[0]:(0.0718175470829) A[1]:(0.823908030987) A[2]:(-0.518624663353) A[3]:(0.791534483433)\n",
      " state (13)  A[0]:(0.000117868185043) A[1]:(0.808752715588) A[2]:(0.900022029877) A[3]:(0.729075193405)\n",
      " state (14)  A[0]:(0.81008887291) A[1]:(0.900277674198) A[2]:(0.999999821186) A[3]:(0.810018360615)\n",
      " state (15)  A[0]:(0.988547563553) A[1]:(0.961262583733) A[2]:(1.0) A[3]:(0.891114473343)\n",
      "Episode 414000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6024. Times reached goal: 969.               Steps done: 3639606. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0236364441209.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5903,  0.5904,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.0002,  0.6560,  0.5907]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5911,  0.7287,  0.5907,  0.6558]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  0.8098,  0.0004,  0.6556]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9001,  0.0011,  0.7286]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8093,  0.9002,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532143652439) A[1]:(0.590256631374) A[2]:(0.590386033058) A[3]:(0.531450390816)\n",
      " state (1)  A[0]:(0.532248735428) A[1]:(-0.000124841928482) A[2]:(0.65584897995) A[3]:(0.590443134308)\n",
      " state (2)  A[0]:(0.59140765667) A[1]:(0.728796958923) A[2]:(0.590552508831) A[3]:(0.65575170517)\n",
      " state (3)  A[0]:(0.655809760094) A[1]:(-0.148006305099) A[2]:(0.523456811905) A[3]:(0.529187083244)\n",
      " state (4)  A[0]:(0.591618001461) A[1]:(0.655967891216) A[2]:(-0.000271439552307) A[3]:(0.53152513504)\n",
      " state (5)  A[0]:(0.163966447115) A[1]:(0.928438186646) A[2]:(-0.178137034178) A[3]:(0.512488603592)\n",
      " state (6)  A[0]:(0.000354409188731) A[1]:(0.809914767742) A[2]:(-0.000252604484558) A[3]:(0.655760169029)\n",
      " state (7)  A[0]:(0.638791799545) A[1]:(-0.251679569483) A[2]:(0.248525768518) A[3]:(0.895924568176)\n",
      " state (8)  A[0]:(0.656232833862) A[1]:(-0.000236660242081) A[2]:(0.728462994099) A[3]:(0.590707421303)\n",
      " state (9)  A[0]:(0.655788958073) A[1]:(0.809879362583) A[2]:(0.809810400009) A[3]:(0.000437334150774)\n",
      " state (10)  A[0]:(0.728706359863) A[1]:(0.899989664555) A[2]:(-2.95639038086e-05) A[3]:(0.728953003883)\n",
      " state (11)  A[0]:(0.515839934349) A[1]:(0.876703560352) A[2]:(-0.580436527729) A[3]:(0.84199243784)\n",
      " state (12)  A[0]:(0.0711176916957) A[1]:(0.823960363865) A[2]:(-0.519368767738) A[3]:(0.791548073292)\n",
      " state (13)  A[0]:(-0.00127175380476) A[1]:(0.808705985546) A[2]:(0.899449408054) A[3]:(0.728912234306)\n",
      " state (14)  A[0]:(0.809162259102) A[1]:(0.90013563633) A[2]:(0.999999821186) A[3]:(0.809756278992)\n",
      " state (15)  A[0]:(0.988472163677) A[1]:(0.961168527603) A[2]:(1.0) A[3]:(0.890927195549)\n",
      "Episode 415000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6053. Times reached goal: 975.               Steps done: 3645659. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0234938048578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531918168068) A[1]:(0.590081214905) A[2]:(0.590439736843) A[3]:(0.531752586365)\n",
      " state (1)  A[0]:(0.532055675983) A[1]:(-7.97584652901e-06) A[2]:(0.655686676502) A[3]:(0.590617060661)\n",
      " state (2)  A[0]:(0.591062009335) A[1]:(0.72861135006) A[2]:(0.590759515762) A[3]:(0.655631422997)\n",
      " state (3)  A[0]:(0.655273079872) A[1]:(-0.20022675395) A[2]:(0.531014084816) A[3]:(0.524164319038)\n",
      " state (4)  A[0]:(0.59133708477) A[1]:(0.655478954315) A[2]:(-9.44137573242e-05) A[3]:(0.531368553638)\n",
      " state (5)  A[0]:(0.163531914353) A[1]:(0.928447306156) A[2]:(-0.178557500243) A[3]:(0.512746453285)\n",
      " state (6)  A[0]:(0.000397235126002) A[1]:(0.809911549091) A[2]:(-0.0008325575036) A[3]:(0.655751347542)\n",
      " state (7)  A[0]:(0.638815164566) A[1]:(-0.251427739859) A[2]:(0.248642981052) A[3]:(0.895718753338)\n",
      " state (8)  A[0]:(0.655950665474) A[1]:(-0.000129569321871) A[2]:(0.728713750839) A[3]:(0.589645802975)\n",
      " state (9)  A[0]:(0.655555903912) A[1]:(0.809864938259) A[2]:(0.809922099113) A[3]:(-0.000914558535442)\n",
      " state (10)  A[0]:(0.728594720364) A[1]:(0.899922966957) A[2]:(-0.000151515007019) A[3]:(0.728555083275)\n",
      " state (11)  A[0]:(0.515706062317) A[1]:(0.876569330692) A[2]:(-0.580766797066) A[3]:(0.841794371605)\n",
      " state (12)  A[0]:(0.0711060091853) A[1]:(0.823746740818) A[2]:(-0.519723653793) A[3]:(0.791291236877)\n",
      " state (13)  A[0]:(-0.000653326394968) A[1]:(0.808511972427) A[2]:(0.899797439575) A[3]:(0.728643655777)\n",
      " state (14)  A[0]:(0.809776127338) A[1]:(0.900081753731) A[2]:(0.999999821186) A[3]:(0.80970877409)\n",
      " state (15)  A[0]:(0.98849105835) A[1]:(0.961120188236) A[2]:(1.0) A[3]:(0.890856206417)\n",
      "Episode 416000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6055. Times reached goal: 979.               Steps done: 3651714. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0233519796785.\n",
      " state (0)  A[0]:(0.530864953995) A[1]:(0.590432405472) A[2]:(0.590592503548) A[3]:(0.531251549721)\n",
      " state (1)  A[0]:(0.530945837498) A[1]:(0.0001330524683) A[2]:(0.65625077486) A[3]:(0.590558409691)\n",
      " state (2)  A[0]:(0.589919924736) A[1]:(0.728839635849) A[2]:(0.590922474861) A[3]:(0.655754089355)\n",
      " state (3)  A[0]:(0.655097126961) A[1]:(-0.22851999104) A[2]:(0.535219192505) A[3]:(0.521056652069)\n",
      " state (4)  A[0]:(0.591056704521) A[1]:(0.655789852142) A[2]:(0.000131845474243) A[3]:(0.530976474285)\n",
      " state (5)  A[0]:(0.162487924099) A[1]:(0.928487420082) A[2]:(-0.178251773119) A[3]:(0.512556910515)\n",
      " state (6)  A[0]:(-0.000332802534103) A[1]:(0.809960246086) A[2]:(-0.000320672988892) A[3]:(0.655547738075)\n",
      " state (7)  A[0]:(0.638652682304) A[1]:(-0.251027345657) A[2]:(0.249663129449) A[3]:(0.89561009407)\n",
      " state (8)  A[0]:(0.655927419662) A[1]:(2.16215848923e-05) A[2]:(0.729003548622) A[3]:(0.589553356171)\n",
      " state (9)  A[0]:(0.655716180801) A[1]:(0.809923052788) A[2]:(0.810081124306) A[3]:(-0.00141766574234)\n",
      " state (10)  A[0]:(0.728733181953) A[1]:(0.900018453598) A[2]:(7.76052474976e-05) A[3]:(0.728248000145)\n",
      " state (11)  A[0]:(0.515790939331) A[1]:(0.876764297485) A[2]:(-0.580847799778) A[3]:(0.841577291489)\n",
      " state (12)  A[0]:(0.0710545033216) A[1]:(0.824139595032) A[2]:(-0.519751310349) A[3]:(0.790925860405)\n",
      " state (13)  A[0]:(-0.000568777264562) A[1]:(0.809100985527) A[2]:(0.900261223316) A[3]:(0.728120207787)\n",
      " state (14)  A[0]:(0.809899508953) A[1]:(0.900498270988) A[2]:(0.999999821186) A[3]:(0.809341549873)\n",
      " state (15)  A[0]:(0.988454699516) A[1]:(0.961273133755) A[2]:(1.0) A[3]:(0.890511751175)\n",
      "Episode 417000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6051. Times reached goal: 976.               Steps done: 3657765. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0232111035001.\n",
      " state (0)  A[0]:(0.531586289406) A[1]:(0.590905368328) A[2]:(0.590834856033) A[3]:(0.531551718712)\n",
      " state (1)  A[0]:(0.531941831112) A[1]:(3.01748514175e-07) A[2]:(0.656435012817) A[3]:(0.590857625008)\n",
      " state (2)  A[0]:(0.590686678886) A[1]:(0.729227244854) A[2]:(0.590113341808) A[3]:(0.656350195408)\n",
      " state (3)  A[0]:(0.656025052071) A[1]:(-0.234154626727) A[2]:(0.535589337349) A[3]:(0.520761728287)\n",
      " state (4)  A[0]:(0.591357767582) A[1]:(0.656492114067) A[2]:(-0.000149011611938) A[3]:(0.531396150589)\n",
      " state (5)  A[0]:(0.162223026156) A[1]:(0.928577542305) A[2]:(-0.178193703294) A[3]:(0.513206720352)\n",
      " state (6)  A[0]:(-0.000117629766464) A[1]:(0.810008823872) A[2]:(4.61339950562e-05) A[3]:(0.656263113022)\n",
      " state (7)  A[0]:(0.639154791832) A[1]:(-0.251230865717) A[2]:(0.250494807959) A[3]:(0.895909368992)\n",
      " state (8)  A[0]:(0.656317710876) A[1]:(-5.092471838e-06) A[2]:(0.729294240475) A[3]:(0.590199887753)\n",
      " state (9)  A[0]:(0.656136035919) A[1]:(0.809940218925) A[2]:(0.810056209564) A[3]:(-0.000847086135764)\n",
      " state (10)  A[0]:(0.729026675224) A[1]:(0.899993360043) A[2]:(-0.000584125460591) A[3]:(0.728644430637)\n",
      " state (11)  A[0]:(0.516040980816) A[1]:(0.876690983772) A[2]:(-0.581625699997) A[3]:(0.841887772083)\n",
      " state (12)  A[0]:(0.0710592791438) A[1]:(0.823984622955) A[2]:(-0.520986199379) A[3]:(0.791359722614)\n",
      " state (13)  A[0]:(-0.000886916881427) A[1]:(0.808862149715) A[2]:(0.899965941906) A[3]:(0.728694438934)\n",
      " state (14)  A[0]:(0.809746563435) A[1]:(0.900312423706) A[2]:(0.999999821186) A[3]:(0.809803009033)\n",
      " state (15)  A[0]:(0.988429009914) A[1]:(0.961161196232) A[2]:(1.0) A[3]:(0.890777349472)\n",
      "Episode 418000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6059. Times reached goal: 981.               Steps done: 3663824. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0230708926218.\n",
      " state (0)  A[0]:(0.531087040901) A[1]:(0.590556502342) A[2]:(0.590428948402) A[3]:(0.531289279461)\n",
      " state (1)  A[0]:(0.531511247158) A[1]:(-7.00689852238e-05) A[2]:(0.65610575676) A[3]:(0.590447425842)\n",
      " state (2)  A[0]:(0.590161323547) A[1]:(0.728990495205) A[2]:(0.589966535568) A[3]:(0.656016170979)\n",
      " state (3)  A[0]:(0.655725061893) A[1]:(-0.233638554811) A[2]:(0.535529613495) A[3]:(0.520692169666)\n",
      " state (4)  A[0]:(0.590885996819) A[1]:(0.656000196934) A[2]:(8.74996185303e-05) A[3]:(0.531290411949)\n",
      " state (5)  A[0]:(0.161729767919) A[1]:(0.928604066372) A[2]:(-0.178335562348) A[3]:(0.513075768948)\n",
      " state (6)  A[0]:(8.88407230377e-05) A[1]:(0.809992074966) A[2]:(5.65052032471e-05) A[3]:(0.656075119972)\n",
      " state (7)  A[0]:(0.639054179192) A[1]:(-0.251553744078) A[2]:(0.250820577145) A[3]:(0.895747005939)\n",
      " state (8)  A[0]:(0.65610563755) A[1]:(0.000120159238577) A[2]:(0.729028761387) A[3]:(0.590507864952)\n",
      " state (9)  A[0]:(0.656010627747) A[1]:(0.810019314289) A[2]:(0.810043036938) A[3]:(4.82797622681e-05)\n",
      " state (10)  A[0]:(0.729054927826) A[1]:(0.900018692017) A[2]:(-8.8095664978e-05) A[3]:(0.728973925114)\n",
      " state (11)  A[0]:(0.516404271126) A[1]:(0.87670135498) A[2]:(-0.58145904541) A[3]:(0.842092990875)\n",
      " state (12)  A[0]:(0.0719163119793) A[1]:(0.823981881142) A[2]:(-0.521158695221) A[3]:(0.791624069214)\n",
      " state (13)  A[0]:(0.000184535980225) A[1]:(0.808839261532) A[2]:(0.900007784367) A[3]:(0.729019165039)\n",
      " state (14)  A[0]:(0.81014585495) A[1]:(0.900287628174) A[2]:(0.999999821186) A[3]:(0.810049057007)\n",
      " state (15)  A[0]:(0.988433659077) A[1]:(0.961124718189) A[2]:(1.0) A[3]:(0.890866100788)\n",
      "Episode 419000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6039. Times reached goal: 980.               Steps done: 3669863. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0229319873479.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5902,  0.5903,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.0000,  0.6560,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.7289,  0.5899,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8100,  0.0001,  0.6558]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 7.2873e-01,  9.0000e-01, -6.7949e-06,  7.2877e-01]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9002,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531328320503) A[1]:(0.590267062187) A[2]:(0.590312957764) A[3]:(0.531352043152)\n",
      " state (1)  A[0]:(0.5315361619) A[1]:(3.21194529533e-05) A[2]:(0.656012415886) A[3]:(0.590335309505)\n",
      " state (2)  A[0]:(0.590161323547) A[1]:(0.728902816772) A[2]:(0.589991807938) A[3]:(0.655906498432)\n",
      " state (3)  A[0]:(0.655637085438) A[1]:(-0.234126776457) A[2]:(0.535528600216) A[3]:(0.520647466183)\n",
      " state (4)  A[0]:(0.590714156628) A[1]:(0.655815720558) A[2]:(1.31130218506e-06) A[3]:(0.53129196167)\n",
      " state (5)  A[0]:(0.161531686783) A[1]:(0.928626716137) A[2]:(-0.178575739264) A[3]:(0.513000607491)\n",
      " state (6)  A[0]:(0.000130414962769) A[1]:(0.809990286827) A[2]:(4.56571578979e-05) A[3]:(0.655787527561)\n",
      " state (7)  A[0]:(0.63884139061) A[1]:(-0.251683235168) A[2]:(0.251266807318) A[3]:(0.895477354527)\n",
      " state (8)  A[0]:(0.655763268471) A[1]:(3.39671969414e-05) A[2]:(0.728907942772) A[3]:(0.590111613274)\n",
      " state (9)  A[0]:(0.655599415302) A[1]:(0.80998390913) A[2]:(0.810021519661) A[3]:(-0.000393182010157)\n",
      " state (10)  A[0]:(0.728724658489) A[1]:(0.900017857552) A[2]:(-4.24385070801e-05) A[3]:(0.728777527809)\n",
      " state (11)  A[0]:(0.515967607498) A[1]:(0.876704454422) A[2]:(-0.581716418266) A[3]:(0.84199988842)\n",
      " state (12)  A[0]:(0.0713547766209) A[1]:(0.823974251747) A[2]:(-0.521777391434) A[3]:(0.791497290134)\n",
      " state (13)  A[0]:(-0.000425249309046) A[1]:(0.80880355835) A[2]:(0.899997293949) A[3]:(0.728833377361)\n",
      " state (14)  A[0]:(0.809895575047) A[1]:(0.900243461132) A[2]:(0.999999821186) A[3]:(0.809891581535)\n",
      " state (15)  A[0]:(0.988386213779) A[1]:(0.961068749428) A[2]:(1.0) A[3]:(0.890685677528)\n",
      "Episode 420000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6073. Times reached goal: 975.               Steps done: 3675936. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0227931434151.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531870007515) A[1]:(0.590428709984) A[2]:(0.590441703796) A[3]:(0.531088232994)\n",
      " state (1)  A[0]:(0.53225004673) A[1]:(-7.04117119312e-05) A[2]:(0.655985832214) A[3]:(0.590131282806)\n",
      " state (2)  A[0]:(0.590933382511) A[1]:(0.728982448578) A[2]:(0.589919805527) A[3]:(0.655820846558)\n",
      " state (3)  A[0]:(0.656210303307) A[1]:(-0.233357921243) A[2]:(0.535255551338) A[3]:(0.520656108856)\n",
      " state (4)  A[0]:(0.591175556183) A[1]:(0.656148433685) A[2]:(-0.000419974297984) A[3]:(0.531378626823)\n",
      " state (5)  A[0]:(0.162105709314) A[1]:(0.92869746685) A[2]:(-0.179128363729) A[3]:(0.513242006302)\n",
      " state (6)  A[0]:(0.00126561452635) A[1]:(0.80996131897) A[2]:(-0.000409722299082) A[3]:(0.656202375889)\n",
      " state (7)  A[0]:(0.639715790749) A[1]:(-0.251946926117) A[2]:(0.251259207726) A[3]:(0.895632505417)\n",
      " state (8)  A[0]:(0.656448483467) A[1]:(0.000396888674004) A[2]:(0.728923201561) A[3]:(0.590366780758)\n",
      " state (9)  A[0]:(0.65630531311) A[1]:(0.810091435909) A[2]:(0.809947192669) A[3]:(0.000379398436053)\n",
      " state (10)  A[0]:(0.729203939438) A[1]:(0.900014281273) A[2]:(-0.000353455514414) A[3]:(0.729279398918)\n",
      " state (11)  A[0]:(0.516586661339) A[1]:(0.876650035381) A[2]:(-0.582140922546) A[3]:(0.842328071594)\n",
      " state (12)  A[0]:(0.072083465755) A[1]:(0.823855519295) A[2]:(-0.522499322891) A[3]:(0.791935622692)\n",
      " state (13)  A[0]:(0.000324070453644) A[1]:(0.808650076389) A[2]:(0.900057435036) A[3]:(0.729421019554)\n",
      " state (14)  A[0]:(0.810221850872) A[1]:(0.900152504444) A[2]:(0.999999821186) A[3]:(0.810363650322)\n",
      " state (15)  A[0]:(0.988377034664) A[1]:(0.960995316505) A[2]:(1.0) A[3]:(0.890891194344)\n",
      "Episode 421000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6048. Times reached goal: 984.               Steps done: 3681984. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0226557065118.\n",
      " state (0)  A[0]:(0.530637383461) A[1]:(0.590579032898) A[2]:(0.590319216251) A[3]:(0.5317081213)\n",
      " state (1)  A[0]:(0.530982613564) A[1]:(-0.000137902796268) A[2]:(0.656037330627) A[3]:(0.590668141842)\n",
      " state (2)  A[0]:(0.589826524258) A[1]:(0.729032099247) A[2]:(0.590270519257) A[3]:(0.656251728535)\n",
      " state (3)  A[0]:(0.655461668968) A[1]:(-0.233444303274) A[2]:(0.535684943199) A[3]:(0.520760536194)\n",
      " state (4)  A[0]:(0.59030354023) A[1]:(0.656162023544) A[2]:(0.000201463699341) A[3]:(0.531311273575)\n",
      " state (5)  A[0]:(0.160597875714) A[1]:(0.92871940136) A[2]:(-0.178582429886) A[3]:(0.513152599335)\n",
      " state (6)  A[0]:(-0.00062230223557) A[1]:(0.810119330883) A[2]:(0.000245213508606) A[3]:(0.656059205532)\n",
      " state (7)  A[0]:(0.638778090477) A[1]:(-0.25178322196) A[2]:(0.252291887999) A[3]:(0.895625293255)\n",
      " state (8)  A[0]:(0.656024336815) A[1]:(0.000222139060497) A[2]:(0.729192376137) A[3]:(0.590696513653)\n",
      " state (9)  A[0]:(0.656001925468) A[1]:(0.810063898563) A[2]:(0.810100317001) A[3]:(0.00027984380722)\n",
      " state (10)  A[0]:(0.728945076466) A[1]:(0.900013267994) A[2]:(7.66515731812e-05) A[3]:(0.729044616222)\n",
      " state (11)  A[0]:(0.516175746918) A[1]:(0.876678228378) A[2]:(-0.582174897194) A[3]:(0.842169106007)\n",
      " state (12)  A[0]:(0.0714297369123) A[1]:(0.823952972889) A[2]:(-0.523015856743) A[3]:(0.791729867458)\n",
      " state (13)  A[0]:(-0.00057712191483) A[1]:(0.808820962906) A[2]:(0.899986505508) A[3]:(0.72912478447)\n",
      " state (14)  A[0]:(0.809814095497) A[1]:(0.900281846523) A[2]:(0.999999821186) A[3]:(0.81007951498)\n",
      " state (15)  A[0]:(0.988319516182) A[1]:(0.961041510105) A[2]:(1.0) A[3]:(0.890606701374)\n",
      "Episode 422000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6047. Times reached goal: 976.               Steps done: 3688031. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0225191208376.\n",
      " state (0)  A[0]:(0.531844973564) A[1]:(0.590684056282) A[2]:(0.590721130371) A[3]:(0.532087624073)\n",
      " state (1)  A[0]:(0.532292485237) A[1]:(-3.45706939697e-05) A[2]:(0.656136751175) A[3]:(0.590920686722)\n",
      " state (2)  A[0]:(0.591045975685) A[1]:(0.729011654854) A[2]:(0.590386152267) A[3]:(0.656407654285)\n",
      " state (3)  A[0]:(0.656306505203) A[1]:(-0.234704762697) A[2]:(0.53582239151) A[3]:(0.520940303802)\n",
      " state (4)  A[0]:(0.590932488441) A[1]:(0.656280755997) A[2]:(-9.53674316406e-07) A[3]:(0.531591296196)\n",
      " state (5)  A[0]:(0.161060720682) A[1]:(0.928725123405) A[2]:(-0.178847163916) A[3]:(0.513334274292)\n",
      " state (6)  A[0]:(-0.000263452529907) A[1]:(0.810010671616) A[2]:(2.26497650146e-05) A[3]:(0.656074047089)\n",
      " state (7)  A[0]:(0.63895791769) A[1]:(-0.252208381891) A[2]:(0.252262860537) A[3]:(0.895552396774)\n",
      " state (8)  A[0]:(0.656229376793) A[1]:(6.16908073425e-05) A[2]:(0.72896873951) A[3]:(0.590607821941)\n",
      " state (9)  A[0]:(0.656154870987) A[1]:(0.810061812401) A[2]:(0.80997800827) A[3]:(2.09659337997e-05)\n",
      " state (10)  A[0]:(0.728997945786) A[1]:(0.899985313416) A[2]:(-0.000199198722839) A[3]:(0.728853583336)\n",
      " state (11)  A[0]:(0.516262233257) A[1]:(0.876599669456) A[2]:(-0.582609653473) A[3]:(0.842027425766)\n",
      " state (12)  A[0]:(0.0716215968132) A[1]:(0.823783755302) A[2]:(-0.523736476898) A[3]:(0.791519463062)\n",
      " state (13)  A[0]:(-7.81118869781e-05) A[1]:(0.808583259583) A[2]:(0.900101423264) A[3]:(0.728841006756)\n",
      " state (14)  A[0]:(0.81022721529) A[1]:(0.900122404099) A[2]:(0.999999821186) A[3]:(0.809860944748)\n",
      " state (15)  A[0]:(0.988323271275) A[1]:(0.960924446583) A[2]:(1.0) A[3]:(0.890353560448)\n",
      "Episode 423000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6044. Times reached goal: 976.               Steps done: 3694075. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0223834257548.\n",
      " state (0)  A[0]:(0.531067848206) A[1]:(0.59042942524) A[2]:(0.590567350388) A[3]:(0.531510233879)\n",
      " state (1)  A[0]:(0.531279921532) A[1]:(-0.00016863271594) A[2]:(0.65614759922) A[3]:(0.590520739555)\n",
      " state (2)  A[0]:(0.590186774731) A[1]:(0.728946685791) A[2]:(0.590313315392) A[3]:(0.656183302402)\n",
      " state (3)  A[0]:(0.655804276466) A[1]:(-0.233243390918) A[2]:(0.535477757454) A[3]:(0.520926952362)\n",
      " state (4)  A[0]:(0.590562939644) A[1]:(0.655989766121) A[2]:(-0.000203132629395) A[3]:(0.531517505646)\n",
      " state (5)  A[0]:(0.160839647055) A[1]:(0.92870247364) A[2]:(-0.17927582562) A[3]:(0.513371050358)\n",
      " state (6)  A[0]:(-0.00016975402832) A[1]:(0.809965014458) A[2]:(-0.000349998474121) A[3]:(0.656076669693)\n",
      " state (7)  A[0]:(0.638898670673) A[1]:(-0.252318829298) A[2]:(0.252163618803) A[3]:(0.895475745201)\n",
      " state (8)  A[0]:(0.656167566776) A[1]:(-0.000133823603392) A[2]:(0.728559315205) A[3]:(0.590837001801)\n",
      " state (9)  A[0]:(0.656091570854) A[1]:(0.809964597225) A[2]:(0.809684157372) A[3]:(0.000356078118784)\n",
      " state (10)  A[0]:(0.728975892067) A[1]:(0.899975836277) A[2]:(-0.000957488722634) A[3]:(0.728937387466)\n",
      " state (11)  A[0]:(0.516297459602) A[1]:(0.87664681673) A[2]:(-0.583359599113) A[3]:(0.842076063156)\n",
      " state (12)  A[0]:(0.071638174355) A[1]:(0.823923110962) A[2]:(-0.524873614311) A[3]:(0.791547477245)\n",
      " state (13)  A[0]:(-0.000253349542618) A[1]:(0.80879586935) A[2]:(0.899947881699) A[3]:(0.728833317757)\n",
      " state (14)  A[0]:(0.810037016869) A[1]:(0.900254607201) A[2]:(0.999999821186) A[3]:(0.80986893177)\n",
      " state (15)  A[0]:(0.988273441792) A[1]:(0.960956215858) A[2]:(1.0) A[3]:(0.890297174454)\n",
      "Episode 424000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6062. Times reached goal: 976.               Steps done: 3700137. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0222481478694.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5902,  0.5905,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.0001,  0.6561,  0.5902]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.7290,  0.5901,  0.6559]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8100,  0.0001,  0.6558]], device='cuda:0')\n",
      "On state=6, selected action=3 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7290,  0.5902,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  0.8100,  0.0000,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0002,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531352043152) A[1]:(0.590221762657) A[2]:(0.590468049049) A[3]:(0.53161585331)\n",
      " state (1)  A[0]:(0.531507134438) A[1]:(4.59365546703e-05) A[2]:(0.6561191082) A[3]:(0.590715944767)\n",
      " state (2)  A[0]:(0.590351104736) A[1]:(0.728976249695) A[2]:(0.590135931969) A[3]:(0.656410872936)\n",
      " state (3)  A[0]:(0.655777812004) A[1]:(-0.232148751616) A[2]:(0.535275220871) A[3]:(0.521185278893)\n",
      " state (4)  A[0]:(0.59032022953) A[1]:(0.655961275101) A[2]:(-5.14984130859e-05) A[3]:(0.531630218029)\n",
      " state (5)  A[0]:(0.16042996943) A[1]:(0.928722023964) A[2]:(-0.179070681334) A[3]:(0.513566374779)\n",
      " state (6)  A[0]:(-0.000353783339961) A[1]:(0.809974074364) A[2]:(0.000125169754028) A[3]:(0.656149864197)\n",
      " state (7)  A[0]:(0.638760149479) A[1]:(-0.25238853693) A[2]:(0.253083437681) A[3]:(0.895342946053)\n",
      " state (8)  A[0]:(0.655976951122) A[1]:(-4.69572842121e-05) A[2]:(0.72898876667) A[3]:(0.590457737446)\n",
      " state (9)  A[0]:(0.65599155426) A[1]:(0.809983432293) A[2]:(0.810002207756) A[3]:(0.00023677945137)\n",
      " state (10)  A[0]:(0.72904419899) A[1]:(0.899985909462) A[2]:(-0.000164389610291) A[3]:(0.729066312313)\n",
      " state (11)  A[0]:(0.516580462456) A[1]:(0.876659870148) A[2]:(-0.583109140396) A[3]:(0.84221303463)\n",
      " state (12)  A[0]:(0.0720760524273) A[1]:(0.823930203915) A[2]:(-0.524968028069) A[3]:(0.791733086109)\n",
      " state (13)  A[0]:(1.8298625946e-05) A[1]:(0.808769524097) A[2]:(0.899993956089) A[3]:(0.72905087471)\n",
      " state (14)  A[0]:(0.810015439987) A[1]:(0.900212168694) A[2]:(0.999999821186) A[3]:(0.810044407845)\n",
      " state (15)  A[0]:(0.988241732121) A[1]:(0.960905849934) A[2]:(1.0) A[3]:(0.890355288982)\n",
      "Episode 425000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6055. Times reached goal: 974.               Steps done: 3706192. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0221138423544.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531421661377) A[1]:(0.590294003487) A[2]:(0.59043431282) A[3]:(0.531237602234)\n",
      " state (1)  A[0]:(0.531542956829) A[1]:(3.44850122929e-05) A[2]:(0.655930876732) A[3]:(0.590158939362)\n",
      " state (2)  A[0]:(0.590395927429) A[1]:(0.728911221027) A[2]:(0.590283632278) A[3]:(0.655772566795)\n",
      " state (3)  A[0]:(0.655874490738) A[1]:(-0.234117850661) A[2]:(0.535781383514) A[3]:(0.520575165749)\n",
      " state (4)  A[0]:(0.590499997139) A[1]:(0.655919075012) A[2]:(0.000140905380249) A[3]:(0.531279802322)\n",
      " state (5)  A[0]:(0.16074681282) A[1]:(0.928755998611) A[2]:(-0.179165989161) A[3]:(0.513202369213)\n",
      " state (6)  A[0]:(0.000227004289627) A[1]:(0.809988021851) A[2]:(0.0001220703125) A[3]:(0.655829906464)\n",
      " state (7)  A[0]:(0.6390671134) A[1]:(-0.252439260483) A[2]:(0.253558456898) A[3]:(0.895154118538)\n",
      " state (8)  A[0]:(0.656113266945) A[1]:(0.00019321218133) A[2]:(0.729149460793) A[3]:(0.589910745621)\n",
      " state (9)  A[0]:(0.656007766724) A[1]:(0.810060441494) A[2]:(0.810067117214) A[3]:(-0.000205785036087)\n",
      " state (10)  A[0]:(0.729038357735) A[1]:(0.900010824203) A[2]:(-4.89950180054e-05) A[3]:(0.728933334351)\n",
      " state (11)  A[0]:(0.516649007797) A[1]:(0.876678466797) A[2]:(-0.58329641819) A[3]:(0.842154562473)\n",
      " state (12)  A[0]:(0.0722533091903) A[1]:(0.823942661285) A[2]:(-0.525510191917) A[3]:(0.791677057743)\n",
      " state (13)  A[0]:(0.000277370214462) A[1]:(0.808767080307) A[2]:(0.900026023388) A[3]:(0.729002773762)\n",
      " state (14)  A[0]:(0.810177147388) A[1]:(0.900200426579) A[2]:(0.999999821186) A[3]:(0.810043632984)\n",
      " state (15)  A[0]:(0.98822581768) A[1]:(0.960866093636) A[2]:(1.0) A[3]:(0.890285730362)\n",
      "Episode 426000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6054. Times reached goal: 981.               Steps done: 3712246. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0219803695825.\n",
      " state (0)  A[0]:(0.531078517437) A[1]:(0.590434908867) A[2]:(0.590441465378) A[3]:(0.531349956989)\n",
      " state (1)  A[0]:(0.531139373779) A[1]:(0.000280898064375) A[2]:(0.656101346016) A[3]:(0.590339660645)\n",
      " state (2)  A[0]:(0.59014415741) A[1]:(0.729149878025) A[2]:(0.590421259403) A[3]:(0.655997037888)\n",
      " state (3)  A[0]:(0.655730128288) A[1]:(-0.232100248337) A[2]:(0.535752773285) A[3]:(0.520880222321)\n",
      " state (4)  A[0]:(0.590456843376) A[1]:(0.65602093935) A[2]:(0.00050485128304) A[3]:(0.531405985355)\n",
      " state (5)  A[0]:(0.160894021392) A[1]:(0.928799450397) A[2]:(-0.178860202432) A[3]:(0.513371169567)\n",
      " state (6)  A[0]:(0.000214844942093) A[1]:(0.810158371925) A[2]:(0.000611543597188) A[3]:(0.655823588371)\n",
      " state (7)  A[0]:(0.63868367672) A[1]:(-0.251958817244) A[2]:(0.254408121109) A[3]:(0.895049273968)\n",
      " state (8)  A[0]:(0.65589261055) A[1]:(0.000371765316231) A[2]:(0.729313850403) A[3]:(0.589990973473)\n",
      " state (9)  A[0]:(0.655950665474) A[1]:(0.810142815113) A[2]:(0.810321331024) A[3]:(-0.000536948384251)\n",
      " state (10)  A[0]:(0.729140162468) A[1]:(0.90014141798) A[2]:(0.000727057340555) A[3]:(0.72882193327)\n",
      " state (11)  A[0]:(0.516997158527) A[1]:(0.876926958561) A[2]:(-0.58311265707) A[3]:(0.84215927124)\n",
      " state (12)  A[0]:(0.0727640166879) A[1]:(0.824388980865) A[2]:(-0.525653600693) A[3]:(0.791669368744)\n",
      " state (13)  A[0]:(0.000693589332514) A[1]:(0.809334635735) A[2]:(0.900137662888) A[3]:(0.728890061378)\n",
      " state (14)  A[0]:(0.810292065144) A[1]:(0.900544524193) A[2]:(0.999999821186) A[3]:(0.809845089912)\n",
      " state (15)  A[0]:(0.98820233345) A[1]:(0.960988759995) A[2]:(1.0) A[3]:(0.890006661415)\n",
      "Episode 427000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6043. Times reached goal: 971.               Steps done: 3718289. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0218479427398.\n",
      " state (0)  A[0]:(0.531350851059) A[1]:(0.590373873711) A[2]:(0.590537309647) A[3]:(0.531425118446)\n",
      " state (1)  A[0]:(0.531404733658) A[1]:(-1.4640390873e-06) A[2]:(0.656120061874) A[3]:(0.590444326401)\n",
      " state (2)  A[0]:(0.590363264084) A[1]:(0.728958845139) A[2]:(0.590358614922) A[3]:(0.656099677086)\n",
      " state (3)  A[0]:(0.65585899353) A[1]:(-0.233288273215) A[2]:(0.5356143713) A[3]:(0.52073097229)\n",
      " state (4)  A[0]:(0.590375125408) A[1]:(0.65639770031) A[2]:(-0.000125885009766) A[3]:(0.531368374825)\n",
      " state (5)  A[0]:(0.160343289375) A[1]:(0.928816020489) A[2]:(-0.179353013635) A[3]:(0.513378798962)\n",
      " state (6)  A[0]:(-0.000141263008118) A[1]:(0.810004234314) A[2]:(0.000408768624766) A[3]:(0.655964851379)\n",
      " state (7)  A[0]:(0.638932466507) A[1]:(-0.252649396658) A[2]:(0.254765987396) A[3]:(0.895151436329)\n",
      " state (8)  A[0]:(0.656304955482) A[1]:(-0.000214014202356) A[2]:(0.729289889336) A[3]:(0.590346932411)\n",
      " state (9)  A[0]:(0.656293153763) A[1]:(0.8099193573) A[2]:(0.810063838959) A[3]:(-1.52140855789e-05)\n",
      " state (10)  A[0]:(0.729163885117) A[1]:(0.899990200996) A[2]:(-0.000136494636536) A[3]:(0.728950560093)\n",
      " state (11)  A[0]:(0.516732335091) A[1]:(0.876720964909) A[2]:(-0.583894491196) A[3]:(0.842193722725)\n",
      " state (12)  A[0]:(0.0720808282495) A[1]:(0.824081897736) A[2]:(-0.52680760622) A[3]:(0.79170358181)\n",
      " state (13)  A[0]:(-0.000296086072922) A[1]:(0.808983445168) A[2]:(0.900028884411) A[3]:(0.728962242603)\n",
      " state (14)  A[0]:(0.80985057354) A[1]:(0.900337338448) A[2]:(0.999999821186) A[3]:(0.80996119976)\n",
      " state (15)  A[0]:(0.988133192062) A[1]:(0.960868060589) A[2]:(1.0) A[3]:(0.890030324459)\n",
      "Episode 428000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6079. Times reached goal: 978.               Steps done: 3724368. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0217155319662.\n",
      " state (0)  A[0]:(0.531634926796) A[1]:(0.590172588825) A[2]:(0.590510725975) A[3]:(0.531476974487)\n",
      " state (1)  A[0]:(0.531814455986) A[1]:(-0.00013567879796) A[2]:(0.656153142452) A[3]:(0.590517401695)\n",
      " state (2)  A[0]:(0.590710520744) A[1]:(0.728893518448) A[2]:(0.590692520142) A[3]:(0.656175494194)\n",
      " state (3)  A[0]:(0.656212329865) A[1]:(-0.233699172735) A[2]:(0.536021709442) A[3]:(0.520869851112)\n",
      " state (4)  A[0]:(0.590797662735) A[1]:(0.656133174896) A[2]:(0.000179648399353) A[3]:(0.531654119492)\n",
      " state (5)  A[0]:(0.161062434316) A[1]:(0.928816556931) A[2]:(-0.179489731789) A[3]:(0.513881206512)\n",
      " state (6)  A[0]:(0.000921755796298) A[1]:(0.809925675392) A[2]:(0.000174641609192) A[3]:(0.656338691711)\n",
      " state (7)  A[0]:(0.639299213886) A[1]:(-0.252747535706) A[2]:(0.254491090775) A[3]:(0.895147264004)\n",
      " state (8)  A[0]:(0.656493186951) A[1]:(-8.48658382893e-05) A[2]:(0.728754043579) A[3]:(0.591052472591)\n",
      " state (9)  A[0]:(0.656315088272) A[1]:(0.809927940369) A[2]:(0.80990344286) A[3]:(0.00111541105434)\n",
      " state (10)  A[0]:(0.729103565216) A[1]:(0.899961411953) A[2]:(-2.43186950684e-05) A[3]:(0.729244470596)\n",
      " state (11)  A[0]:(0.516724169254) A[1]:(0.876628100872) A[2]:(-0.583990931511) A[3]:(0.842335343361)\n",
      " state (12)  A[0]:(0.0721729099751) A[1]:(0.823859095573) A[2]:(-0.527348458767) A[3]:(0.791859149933)\n",
      " state (13)  A[0]:(-0.000294357538223) A[1]:(0.808618247509) A[2]:(0.899880707264) A[3]:(0.729131340981)\n",
      " state (14)  A[0]:(0.809779405594) A[1]:(0.900061547756) A[2]:(0.999999821186) A[3]:(0.810111105442)\n",
      " state (15)  A[0]:(0.988107025623) A[1]:(0.960709631443) A[2]:(1.0) A[3]:(0.890101730824)\n",
      "Episode 429000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6053. Times reached goal: 973.               Steps done: 3730421. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0215844848654.\n",
      "q_values \n",
      "tensor([[ 0.5298,  0.5906,  0.5905,  0.5299]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5897,  0.6559, -0.0002,  0.5295]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6553,  0.0003,  0.7289,  0.5892]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6550,  0.8103,  0.8103, -0.0025]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0021,  0.8085,  0.9002,  0.7283]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9004,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531248390675) A[1]:(0.590494275093) A[2]:(0.590274870396) A[3]:(0.532109975815)\n",
      " state (1)  A[0]:(0.53119802475) A[1]:(0.000153008848429) A[2]:(0.655990660191) A[3]:(0.591010808945)\n",
      " state (2)  A[0]:(0.590116500854) A[1]:(0.729210317135) A[2]:(0.590383112431) A[3]:(0.656484663486)\n",
      " state (3)  A[0]:(0.655737757683) A[1]:(-0.231362298131) A[2]:(0.535587847233) A[3]:(0.520431756973)\n",
      " state (4)  A[0]:(0.590064704418) A[1]:(0.656328499317) A[2]:(0.000155210494995) A[3]:(0.530553877354)\n",
      " state (5)  A[0]:(0.159680962563) A[1]:(0.928835690022) A[2]:(-0.179437458515) A[3]:(0.51263743639)\n",
      " state (6)  A[0]:(-0.000608384551015) A[1]:(0.810021877289) A[2]:(0.000356674165232) A[3]:(0.655404686928)\n",
      " state (7)  A[0]:(0.638459384441) A[1]:(-0.252204447985) A[2]:(0.255121320486) A[3]:(0.894836068153)\n",
      " state (8)  A[0]:(0.65575003624) A[1]:(0.000788185570855) A[2]:(0.729203820229) A[3]:(0.589786708355)\n",
      " state (9)  A[0]:(0.655716538429) A[1]:(0.81021296978) A[2]:(0.810026884079) A[3]:(-0.000198811292648)\n",
      " state (10)  A[0]:(0.728683829308) A[1]:(0.900115549564) A[2]:(-0.000197172164917) A[3]:(0.728868305683)\n",
      " state (11)  A[0]:(0.516112446785) A[1]:(0.876836121082) A[2]:(-0.584429860115) A[3]:(0.842101573944)\n",
      " state (12)  A[0]:(0.0713344365358) A[1]:(0.824179589748) A[2]:(-0.528157174587) A[3]:(0.791494250298)\n",
      " state (13)  A[0]:(-0.00113880587742) A[1]:(0.808994233608) A[2]:(0.899811923504) A[3]:(0.728558182716)\n",
      " state (14)  A[0]:(0.809540867805) A[1]:(0.900279581547) A[2]:(0.999999821186) A[3]:(0.809583425522)\n",
      " state (15)  A[0]:(0.988067865372) A[1]:(0.960777401924) A[2]:(1.0) A[3]:(0.889642596245)\n",
      "Episode 430000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6073. Times reached goal: 978.               Steps done: 3736494. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0214537995165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531435132027) A[1]:(0.590428829193) A[2]:(0.590471386909) A[3]:(0.531364262104)\n",
      " state (1)  A[0]:(0.53151512146) A[1]:(7.86036252975e-06) A[2]:(0.656108736992) A[3]:(0.590481758118)\n",
      " state (2)  A[0]:(0.590482473373) A[1]:(0.728960573673) A[2]:(0.59047305584) A[3]:(0.65617364645)\n",
      " state (3)  A[0]:(0.655997753143) A[1]:(-0.232282713056) A[2]:(0.535693764687) A[3]:(0.520855307579)\n",
      " state (4)  A[0]:(0.590444684029) A[1]:(0.656060874462) A[2]:(6.61611557007e-05) A[3]:(0.531433582306)\n",
      " state (5)  A[0]:(0.160388037562) A[1]:(0.928801774979) A[2]:(-0.179726570845) A[3]:(0.513701021671)\n",
      " state (6)  A[0]:(-0.000170201063156) A[1]:(0.810000479221) A[2]:(-1.27553939819e-05) A[3]:(0.656170368195)\n",
      " state (7)  A[0]:(0.638512372971) A[1]:(-0.252468883991) A[2]:(0.254907548428) A[3]:(0.895062983036)\n",
      " state (8)  A[0]:(0.655957341194) A[1]:(0.000254295766354) A[2]:(0.728990912437) A[3]:(0.590577363968)\n",
      " state (9)  A[0]:(0.65594792366) A[1]:(0.810053467751) A[2]:(0.810004353523) A[3]:(0.000230610370636)\n",
      " state (10)  A[0]:(0.72887635231) A[1]:(0.900001704693) A[2]:(-5.40018081665e-05) A[3]:(0.729056358337)\n",
      " state (11)  A[0]:(0.516489386559) A[1]:(0.876659750938) A[2]:(-0.584458529949) A[3]:(0.842303931713)\n",
      " state (12)  A[0]:(0.0719706267118) A[1]:(0.823897004128) A[2]:(-0.528240561485) A[3]:(0.791852116585)\n",
      " state (13)  A[0]:(-0.00022166967392) A[1]:(0.808675169945) A[2]:(0.900085151196) A[3]:(0.729124426842)\n",
      " state (14)  A[0]:(0.810009837151) A[1]:(0.900107324123) A[2]:(0.999999880791) A[3]:(0.810075759888)\n",
      " state (15)  A[0]:(0.988074660301) A[1]:(0.960675060749) A[2]:(1.0) A[3]:(0.889877200127)\n",
      "Episode 431000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6087. Times reached goal: 985.               Steps done: 3742581. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0213236068821.\n",
      " state (0)  A[0]:(0.531417131424) A[1]:(0.590426385403) A[2]:(0.590499103069) A[3]:(0.531336545944)\n",
      " state (1)  A[0]:(0.531504750252) A[1]:(-7.45058059692e-07) A[2]:(0.65616184473) A[3]:(0.590388178825)\n",
      " state (2)  A[0]:(0.590496659279) A[1]:(0.728970229626) A[2]:(0.590596437454) A[3]:(0.656055808067)\n",
      " state (3)  A[0]:(0.656099617481) A[1]:(-0.232471644878) A[2]:(0.535792708397) A[3]:(0.520744085312)\n",
      " state (4)  A[0]:(0.590562582016) A[1]:(0.656134784222) A[2]:(0.000100135803223) A[3]:(0.531362295151)\n",
      " state (5)  A[0]:(0.160547465086) A[1]:(0.928819179535) A[2]:(-0.179723456502) A[3]:(0.513633728027)\n",
      " state (6)  A[0]:(0.000116497278214) A[1]:(0.809981584549) A[2]:(0.00013530254364) A[3]:(0.656025767326)\n",
      " state (7)  A[0]:(0.638672351837) A[1]:(-0.252680242062) A[2]:(0.255380988121) A[3]:(0.894910991192)\n",
      " state (8)  A[0]:(0.656082391739) A[1]:(5.51342964172e-05) A[2]:(0.729135096073) A[3]:(0.590176224709)\n",
      " state (9)  A[0]:(0.656057238579) A[1]:(0.810000777245) A[2]:(0.810085892677) A[3]:(-0.000216364860535)\n",
      " state (10)  A[0]:(0.729013860226) A[1]:(0.900005340576) A[2]:(1.34706497192e-05) A[3]:(0.728911399841)\n",
      " state (11)  A[0]:(0.516764044762) A[1]:(0.876719295979) A[2]:(-0.584662675858) A[3]:(0.842227876186)\n",
      " state (12)  A[0]:(0.0723053738475) A[1]:(0.824067294598) A[2]:(-0.528734445572) A[3]:(0.791727781296)\n",
      " state (13)  A[0]:(-3.48687171936e-05) A[1]:(0.808960795403) A[2]:(0.900050878525) A[3]:(0.728930950165)\n",
      " state (14)  A[0]:(0.810002088547) A[1]:(0.900324165821) A[2]:(0.999999880791) A[3]:(0.809958577156)\n",
      " state (15)  A[0]:(0.988048195839) A[1]:(0.960770189762) A[2]:(1.0) A[3]:(0.889768242836)\n",
      "Episode 432000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6071. Times reached goal: 983.               Steps done: 3748652. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0211945434332.\n",
      " state (0)  A[0]:(0.531574130058) A[1]:(0.590538740158) A[2]:(0.590462684631) A[3]:(0.531511545181)\n",
      " state (1)  A[0]:(0.531589925289) A[1]:(-3.04467976093e-05) A[2]:(0.656136631966) A[3]:(0.590495705605)\n",
      " state (2)  A[0]:(0.590551733971) A[1]:(0.729005098343) A[2]:(0.590536952019) A[3]:(0.6561460495)\n",
      " state (3)  A[0]:(0.65617275238) A[1]:(-0.23155990243) A[2]:(0.535577476025) A[3]:(0.520981729031)\n",
      " state (4)  A[0]:(0.59062743187) A[1]:(0.655992627144) A[2]:(-6.09159469604e-05) A[3]:(0.531524181366)\n",
      " state (5)  A[0]:(0.160643830895) A[1]:(0.92880064249) A[2]:(-0.180004909635) A[3]:(0.513829350471)\n",
      " state (6)  A[0]:(6.18696212769e-05) A[1]:(0.810003042221) A[2]:(-0.000126719474792) A[3]:(0.656098127365)\n",
      " state (7)  A[0]:(0.638547897339) A[1]:(-0.252644777298) A[2]:(0.255325824022) A[3]:(0.894903481007)\n",
      " state (8)  A[0]:(0.65606379509) A[1]:(-0.000186879187822) A[2]:(0.728881597519) A[3]:(0.590520381927)\n",
      " state (9)  A[0]:(0.655968785286) A[1]:(0.809932589531) A[2]:(0.809955537319) A[3]:(-7.02142715454e-05)\n",
      " state (10)  A[0]:(0.728962063789) A[1]:(0.899989426136) A[2]:(-0.00031840801239) A[3]:(0.728915333748)\n",
      " state (11)  A[0]:(0.516758859158) A[1]:(0.876698851585) A[2]:(-0.585075616837) A[3]:(0.842253804207)\n",
      " state (12)  A[0]:(0.0723104998469) A[1]:(0.824011147022) A[2]:(-0.529378890991) A[3]:(0.791760742664)\n",
      " state (13)  A[0]:(-7.06315040588e-05) A[1]:(0.808845639229) A[2]:(0.90000975132) A[3]:(0.728958845139)\n",
      " state (14)  A[0]:(0.809970617294) A[1]:(0.900212526321) A[2]:(0.999999880791) A[3]:(0.809981405735)\n",
      " state (15)  A[0]:(0.988023221493) A[1]:(0.960681855679) A[2]:(1.0) A[3]:(0.889727532864)\n",
      "Episode 433000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6034. Times reached goal: 974.               Steps done: 3754686. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.021067040621.\n",
      " state (0)  A[0]:(0.531319737434) A[1]:(0.590293526649) A[2]:(0.590471625328) A[3]:(0.531368196011)\n",
      " state (1)  A[0]:(0.53132045269) A[1]:(-5.39757311344e-05) A[2]:(0.656022965908) A[3]:(0.590438127518)\n",
      " state (2)  A[0]:(0.590349555016) A[1]:(0.728911757469) A[2]:(0.590340852737) A[3]:(0.656077802181)\n",
      " state (3)  A[0]:(0.655835270882) A[1]:(-0.231861621141) A[2]:(0.535584449768) A[3]:(0.520850241184)\n",
      " state (4)  A[0]:(0.59025323391) A[1]:(0.655908942223) A[2]:(2.43186950684e-05) A[3]:(0.531353592873)\n",
      " state (5)  A[0]:(0.160260647535) A[1]:(0.928794384003) A[2]:(-0.179978147149) A[3]:(0.513669252396)\n",
      " state (6)  A[0]:(-0.000127881765366) A[1]:(0.80991756916) A[2]:(3.69548797607e-06) A[3]:(0.655972480774)\n",
      " state (7)  A[0]:(0.638296365738) A[1]:(-0.252912372351) A[2]:(0.255626142025) A[3]:(0.894809305668)\n",
      " state (8)  A[0]:(0.655825972557) A[1]:(-0.000149581581354) A[2]:(0.728849291801) A[3]:(0.59041428566)\n",
      " state (9)  A[0]:(0.655814647675) A[1]:(0.809982955456) A[2]:(0.809919416904) A[3]:(-8.64267349243e-05)\n",
      " state (10)  A[0]:(0.728833436966) A[1]:(0.900002598763) A[2]:(-0.000262141227722) A[3]:(0.728879153728)\n",
      " state (11)  A[0]:(0.516584694386) A[1]:(0.876705884933) A[2]:(-0.585139870644) A[3]:(0.842225015163)\n",
      " state (12)  A[0]:(0.0721162259579) A[1]:(0.824024021626) A[2]:(-0.529638528824) A[3]:(0.791715919971)\n",
      " state (13)  A[0]:(-0.000139355659485) A[1]:(0.808874964714) A[2]:(0.900056779385) A[3]:(0.728875041008)\n",
      " state (14)  A[0]:(0.810092568398) A[1]:(0.900245130062) A[2]:(0.999999880791) A[3]:(0.809879362583)\n",
      " state (15)  A[0]:(0.988021671772) A[1]:(0.960683226585) A[2]:(1.0) A[3]:(0.889576554298)\n",
      "Episode 434000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6083. Times reached goal: 977.               Steps done: 3760769. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0209392787945.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5903,  0.5904,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5312, -0.0000,  0.6560,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.7289,  0.5903,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8099, -0.0000,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7288,  0.9000,  0.0001,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8097,  0.9001,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53142529726) A[1]:(0.590336084366) A[2]:(0.590402245522) A[3]:(0.531426787376)\n",
      " state (1)  A[0]:(0.531482696533) A[1]:(3.23168933392e-05) A[2]:(0.655965149403) A[3]:(0.590400636196)\n",
      " state (2)  A[0]:(0.590502262115) A[1]:(0.728964805603) A[2]:(0.590287685394) A[3]:(0.656022071838)\n",
      " state (3)  A[0]:(0.656021177769) A[1]:(-0.231468826532) A[2]:(0.535569310188) A[3]:(0.520864009857)\n",
      " state (4)  A[0]:(0.590471029282) A[1]:(0.655969381332) A[2]:(3.40938568115e-05) A[3]:(0.531389117241)\n",
      " state (5)  A[0]:(0.160588026047) A[1]:(0.928816974163) A[2]:(-0.180057510734) A[3]:(0.513765513897)\n",
      " state (6)  A[0]:(0.00028321146965) A[1]:(0.809980034828) A[2]:(2.02655792236e-05) A[3]:(0.65602004528)\n",
      " state (7)  A[0]:(0.638504385948) A[1]:(-0.252620309591) A[2]:(0.255940407515) A[3]:(0.89476621151)\n",
      " state (8)  A[0]:(0.655969142914) A[1]:(0.000113379210234) A[2]:(0.728947401047) A[3]:(0.59042096138)\n",
      " state (9)  A[0]:(0.655864417553) A[1]:(0.809997916222) A[2]:(0.80998313427) A[3]:(0.000181615352631)\n",
      " state (10)  A[0]:(0.728843688965) A[1]:(0.900004148483) A[2]:(-7.00950622559e-05) A[3]:(0.729058265686)\n",
      " state (11)  A[0]:(0.516622066498) A[1]:(0.876708269119) A[2]:(-0.585180819035) A[3]:(0.842347145081)\n",
      " state (12)  A[0]:(0.0721093788743) A[1]:(0.824014306068) A[2]:(-0.530033588409) A[3]:(0.791855990887)\n",
      " state (13)  A[0]:(-0.00048887723824) A[1]:(0.808807730675) A[2]:(0.899863004684) A[3]:(0.729002535343)\n",
      " state (14)  A[0]:(0.809758484364) A[1]:(0.900147497654) A[2]:(0.999999880791) A[3]:(0.809952616692)\n",
      " state (15)  A[0]:(0.987980127335) A[1]:(0.960613787174) A[2]:(1.0) A[3]:(0.88960558176)\n",
      "Episode 435000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6017. Times reached goal: 974.               Steps done: 3766786. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0208136654407.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531494915485) A[1]:(0.59043109417) A[2]:(0.59057199955) A[3]:(0.531354725361)\n",
      " state (1)  A[0]:(0.531560182571) A[1]:(4.62494790554e-05) A[2]:(0.656244039536) A[3]:(0.590419948101)\n",
      " state (2)  A[0]:(0.590571284294) A[1]:(0.729076206684) A[2]:(0.590613365173) A[3]:(0.656072556973)\n",
      " state (3)  A[0]:(0.656112790108) A[1]:(-0.23069806397) A[2]:(0.53574436903) A[3]:(0.520738482475)\n",
      " state (4)  A[0]:(0.590472102165) A[1]:(0.656180977821) A[2]:(0.000317454338074) A[3]:(0.53118044138)\n",
      " state (5)  A[0]:(0.160418406129) A[1]:(0.928841769695) A[2]:(-0.179768443108) A[3]:(0.513628482819)\n",
      " state (6)  A[0]:(7.76350498199e-05) A[1]:(0.810033559799) A[2]:(0.000390529603465) A[3]:(0.655935525894)\n",
      " state (7)  A[0]:(0.638529241085) A[1]:(-0.252473026514) A[2]:(0.256530284882) A[3]:(0.8947224617)\n",
      " state (8)  A[0]:(0.656196594238) A[1]:(0.000323589891195) A[2]:(0.729194402695) A[3]:(0.590284347534)\n",
      " state (9)  A[0]:(0.656193733215) A[1]:(0.810101747513) A[2]:(0.810136675835) A[3]:(-0.000153109431267)\n",
      " state (10)  A[0]:(0.729119300842) A[1]:(0.900055170059) A[2]:(0.000221967697144) A[3]:(0.728967428207)\n",
      " state (11)  A[0]:(0.517050623894) A[1]:(0.876759111881) A[2]:(-0.585207462311) A[3]:(0.842337071896)\n",
      " state (12)  A[0]:(0.0726700425148) A[1]:(0.824070692062) A[2]:(-0.530223965645) A[3]:(0.79187899828)\n",
      " state (13)  A[0]:(0.000156313180923) A[1]:(0.80886387825) A[2]:(0.900031328201) A[3]:(0.729071497917)\n",
      " state (14)  A[0]:(0.810073018074) A[1]:(0.900179624557) A[2]:(0.999999880791) A[3]:(0.810037016869)\n",
      " state (15)  A[0]:(0.987978518009) A[1]:(0.960598766804) A[2]:(1.0) A[3]:(0.88958966732)\n",
      "Episode 436000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6057. Times reached goal: 977.               Steps done: 3772843. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0206879780975.\n",
      " state (0)  A[0]:(0.531339406967) A[1]:(0.590276241302) A[2]:(0.590296924114) A[3]:(0.531064450741)\n",
      " state (1)  A[0]:(0.531283974648) A[1]:(0.000213112682104) A[2]:(0.655961513519) A[3]:(0.590155243874)\n",
      " state (2)  A[0]:(0.590237975121) A[1]:(0.728978872299) A[2]:(0.59035807848) A[3]:(0.655854225159)\n",
      " state (3)  A[0]:(0.655781507492) A[1]:(-0.232133448124) A[2]:(0.535714864731) A[3]:(0.520465672016)\n",
      " state (4)  A[0]:(0.590035021305) A[1]:(0.656139969826) A[2]:(4.57763671875e-05) A[3]:(0.531111598015)\n",
      " state (5)  A[0]:(0.159611344337) A[1]:(0.928836226463) A[2]:(-0.180021986365) A[3]:(0.513558626175)\n",
      " state (6)  A[0]:(-0.000902354484424) A[1]:(0.810006141663) A[2]:(0.000285029411316) A[3]:(0.655647516251)\n",
      " state (7)  A[0]:(0.637952744961) A[1]:(-0.252754747868) A[2]:(0.256763577461) A[3]:(0.894468426704)\n",
      " state (8)  A[0]:(0.655881047249) A[1]:(-0.000225398689508) A[2]:(0.729101061821) A[3]:(0.589651584625)\n",
      " state (9)  A[0]:(0.656058073044) A[1]:(0.809927165508) A[2]:(0.809994697571) A[3]:(-0.00100646878127)\n",
      " state (10)  A[0]:(0.729091227055) A[1]:(0.900002479553) A[2]:(-0.000357866258128) A[3]:(0.728675842285)\n",
      " state (11)  A[0]:(0.517035126686) A[1]:(0.876751482487) A[2]:(-0.585831880569) A[3]:(0.842217803001)\n",
      " state (12)  A[0]:(0.0725346207619) A[1]:(0.824145555496) A[2]:(-0.531128168106) A[3]:(0.791749536991)\n",
      " state (13)  A[0]:(-0.000164151191711) A[1]:(0.809052944183) A[2]:(0.899933278561) A[3]:(0.728922665119)\n",
      " state (14)  A[0]:(0.809897899628) A[1]:(0.900355815887) A[2]:(0.999999880791) A[3]:(0.80996966362)\n",
      " state (15)  A[0]:(0.987937986851) A[1]:(0.960680484772) A[2]:(1.0) A[3]:(0.889508008957)\n",
      "Episode 437000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6063. Times reached goal: 989.               Steps done: 3778906. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0205629263637.\n",
      " state (0)  A[0]:(0.531570911407) A[1]:(0.590433239937) A[2]:(0.590474307537) A[3]:(0.532075881958)\n",
      " state (1)  A[0]:(0.531434834003) A[1]:(2.20462679863e-05) A[2]:(0.655918598175) A[3]:(0.591082394123)\n",
      " state (2)  A[0]:(0.590413689613) A[1]:(0.729066193104) A[2]:(0.590273618698) A[3]:(0.656681537628)\n",
      " state (3)  A[0]:(0.655944466591) A[1]:(-0.230712473392) A[2]:(0.535544097424) A[3]:(0.521853327751)\n",
      " state (4)  A[0]:(0.590336561203) A[1]:(0.656071305275) A[2]:(2.51531600952e-05) A[3]:(0.532417178154)\n",
      " state (5)  A[0]:(0.160415276885) A[1]:(0.928849220276) A[2]:(-0.180296838284) A[3]:(0.514993429184)\n",
      " state (6)  A[0]:(0.000246286392212) A[1]:(0.810092628002) A[2]:(-0.000147700309753) A[3]:(0.657015562057)\n",
      " state (7)  A[0]:(0.638576924801) A[1]:(-0.251864343882) A[2]:(0.25607162714) A[3]:(0.895136415958)\n",
      " state (8)  A[0]:(0.656752586365) A[1]:(0.00148340209853) A[2]:(0.728395104408) A[3]:(0.592808008194)\n",
      " state (9)  A[0]:(0.656972467899) A[1]:(0.810469329357) A[2]:(0.809622585773) A[3]:(0.00399016728625)\n",
      " state (10)  A[0]:(0.729567229748) A[1]:(0.900142192841) A[2]:(-7.91549682617e-05) A[3]:(0.730278491974)\n",
      " state (11)  A[0]:(0.517719626427) A[1]:(0.876737713814) A[2]:(-0.585309684277) A[3]:(0.842942655087)\n",
      " state (12)  A[0]:(0.0737157762051) A[1]:(0.823883473873) A[2]:(-0.530706524849) A[3]:(0.792575716972)\n",
      " state (13)  A[0]:(0.00131398369558) A[1]:(0.808480858803) A[2]:(0.900032281876) A[3]:(0.729911744595)\n",
      " state (14)  A[0]:(0.810557425022) A[1]:(0.899867296219) A[2]:(0.999999880791) A[3]:(0.810662150383)\n",
      " state (15)  A[0]:(0.987985014915) A[1]:(0.960396945477) A[2]:(1.0) A[3]:(0.889886140823)\n",
      "Episode 438000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6074. Times reached goal: 987.               Steps done: 3784980. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0204384057011.\n",
      " state (0)  A[0]:(0.531335115433) A[1]:(0.590514779091) A[2]:(0.590506970882) A[3]:(0.531492590904)\n",
      " state (1)  A[0]:(0.531446576118) A[1]:(-7.2319060564e-05) A[2]:(0.656073331833) A[3]:(0.590519011021)\n",
      " state (2)  A[0]:(0.590465307236) A[1]:(0.729037165642) A[2]:(0.590663731098) A[3]:(0.656124651432)\n",
      " state (3)  A[0]:(0.656077504158) A[1]:(-0.231572329998) A[2]:(0.535861253738) A[3]:(0.520695328712)\n",
      " state (4)  A[0]:(0.590493202209) A[1]:(0.65610897541) A[2]:(2.83718109131e-05) A[3]:(0.531294286251)\n",
      " state (5)  A[0]:(0.160463735461) A[1]:(0.928854346275) A[2]:(-0.180388182402) A[3]:(0.513872742653)\n",
      " state (6)  A[0]:(9.06884670258e-05) A[1]:(0.810051321983) A[2]:(4.31537628174e-05) A[3]:(0.655949354172)\n",
      " state (7)  A[0]:(0.638358235359) A[1]:(-0.252543181181) A[2]:(0.257114648819) A[3]:(0.894567787647)\n",
      " state (8)  A[0]:(0.656111359596) A[1]:(-0.000229943543673) A[2]:(0.729031920433) A[3]:(0.590467095375)\n",
      " state (9)  A[0]:(0.655976235867) A[1]:(0.809888541698) A[2]:(0.810067772865) A[3]:(-0.000268861651421)\n",
      " state (10)  A[0]:(0.728964447975) A[1]:(0.899988412857) A[2]:(-0.000165700912476) A[3]:(0.728904366493)\n",
      " state (11)  A[0]:(0.516928315163) A[1]:(0.876705527306) A[2]:(-0.586124658585) A[3]:(0.842358887196)\n",
      " state (12)  A[0]:(0.0724256187677) A[1]:(0.82400560379) A[2]:(-0.531889796257) A[3]:(0.791872441769)\n",
      " state (13)  A[0]:(-0.000329107046127) A[1]:(0.808802008629) A[2]:(0.89996111393) A[3]:(0.728990674019)\n",
      " state (14)  A[0]:(0.809809207916) A[1]:(0.900161743164) A[2]:(0.999999880791) A[3]:(0.809997498989)\n",
      " state (15)  A[0]:(0.987887263298) A[1]:(0.960537016392) A[2]:(1.0) A[3]:(0.889407098293)\n",
      "Episode 439000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6074. Times reached goal: 986.               Steps done: 3791054. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0203146390846.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5905,  0.5906,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315, -0.0001,  0.6562,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7291,  0.5905,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8100, -0.0001,  0.6562]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000,  0.0001,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9001,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531655430794) A[1]:(0.590672969818) A[2]:(0.590666294098) A[3]:(0.531406760216)\n",
      " state (1)  A[0]:(0.531669020653) A[1]:(7.59027898312e-05) A[2]:(0.656260371208) A[3]:(0.590405583382)\n",
      " state (2)  A[0]:(0.590714633465) A[1]:(0.729129195213) A[2]:(0.590618908405) A[3]:(0.656068921089)\n",
      " state (3)  A[0]:(0.656380653381) A[1]:(-0.2302313447) A[2]:(0.535702109337) A[3]:(0.520914554596)\n",
      " state (4)  A[0]:(0.590824484825) A[1]:(0.656293690205) A[2]:(6.43730163574e-05) A[3]:(0.531488001347)\n",
      " state (5)  A[0]:(0.160899579525) A[1]:(0.928853750229) A[2]:(-0.180322095752) A[3]:(0.514097571373)\n",
      " state (6)  A[0]:(0.00040644404362) A[1]:(0.81003934145) A[2]:(0.000116229057312) A[3]:(0.656131625175)\n",
      " state (7)  A[0]:(0.638544559479) A[1]:(-0.252497404814) A[2]:(0.257289856672) A[3]:(0.894637286663)\n",
      " state (8)  A[0]:(0.656428098679) A[1]:(5.36702573299e-05) A[2]:(0.729067921638) A[3]:(0.590662300587)\n",
      " state (9)  A[0]:(0.656327307224) A[1]:(0.810002446175) A[2]:(0.810021877289) A[3]:(7.67558813095e-05)\n",
      " state (10)  A[0]:(0.729141592979) A[1]:(0.899999678135) A[2]:(-0.000159502029419) A[3]:(0.72902983427)\n",
      " state (11)  A[0]:(0.517121195793) A[1]:(0.876671135426) A[2]:(-0.586174249649) A[3]:(0.842408835888)\n",
      " state (12)  A[0]:(0.0726778432727) A[1]:(0.823913455009) A[2]:(-0.532089173794) A[3]:(0.791929721832)\n",
      " state (13)  A[0]:(5.04851341248e-05) A[1]:(0.808676838875) A[2]:(0.900070428848) A[3]:(0.729068040848)\n",
      " state (14)  A[0]:(0.810087561607) A[1]:(0.900098502636) A[2]:(0.999999880791) A[3]:(0.810064256191)\n",
      " state (15)  A[0]:(0.987892270088) A[1]:(0.960491001606) A[2]:(1.0) A[3]:(0.889380574226)\n",
      "Episode 440000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6073. Times reached goal: 974.               Steps done: 3797127. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0201916421397.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531244754791) A[1]:(0.590369701385) A[2]:(0.590530157089) A[3]:(0.531392455101)\n",
      " state (1)  A[0]:(0.531513094902) A[1]:(-3.607198596e-05) A[2]:(0.656201422215) A[3]:(0.590445518494)\n",
      " state (2)  A[0]:(0.590673923492) A[1]:(0.729114890099) A[2]:(0.590742349625) A[3]:(0.656102180481)\n",
      " state (3)  A[0]:(0.656200885773) A[1]:(-0.231475993991) A[2]:(0.53593057394) A[3]:(0.5206797719)\n",
      " state (4)  A[0]:(0.590633273125) A[1]:(0.656426906586) A[2]:(0.000101327896118) A[3]:(0.531359553337)\n",
      " state (5)  A[0]:(0.160688981414) A[1]:(0.928874254227) A[2]:(-0.180244132876) A[3]:(0.514007568359)\n",
      " state (6)  A[0]:(0.000204086303711) A[1]:(0.810035467148) A[2]:(0.00028383731842) A[3]:(0.656020879745)\n",
      " state (7)  A[0]:(0.638505935669) A[1]:(-0.252788066864) A[2]:(0.257720053196) A[3]:(0.894570827484)\n",
      " state (8)  A[0]:(0.656504333019) A[1]:(-0.000100713223219) A[2]:(0.729286849499) A[3]:(0.590424597263)\n",
      " state (9)  A[0]:(0.656488895416) A[1]:(0.81002676487) A[2]:(0.810102164745) A[3]:(1.30832195282e-05)\n",
      " state (10)  A[0]:(0.729301452637) A[1]:(0.899979710579) A[2]:(-0.000150918960571) A[3]:(0.729177951813)\n",
      " state (11)  A[0]:(0.517362117767) A[1]:(0.876622617245) A[2]:(-0.586427986622) A[3]:(0.842546343803)\n",
      " state (12)  A[0]:(0.0728758648038) A[1]:(0.823851466179) A[2]:(-0.532845497131) A[3]:(0.792113423347)\n",
      " state (13)  A[0]:(-0.000162422657013) A[1]:(0.808648109436) A[2]:(0.899731814861) A[3]:(0.729217886925)\n",
      " state (14)  A[0]:(0.809818208218) A[1]:(0.900138616562) A[2]:(0.999999880791) A[3]:(0.810044169426)\n",
      " state (15)  A[0]:(0.987857818604) A[1]:(0.960537314415) A[2]:(1.0) A[3]:(0.889270663261)\n",
      "Episode 441000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6071. Times reached goal: 978.               Steps done: 3803198. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0200694300305.\n",
      " state (0)  A[0]:(0.531578779221) A[1]:(0.590507209301) A[2]:(0.590555071831) A[3]:(0.531589448452)\n",
      " state (1)  A[0]:(0.531601965427) A[1]:(-2.75410711765e-05) A[2]:(0.656142413616) A[3]:(0.590592026711)\n",
      " state (2)  A[0]:(0.590600132942) A[1]:(0.729007720947) A[2]:(0.590601921082) A[3]:(0.656221389771)\n",
      " state (3)  A[0]:(0.65617954731) A[1]:(-0.23108625412) A[2]:(0.53580391407) A[3]:(0.520990729332)\n",
      " state (4)  A[0]:(0.590563178062) A[1]:(0.656126976013) A[2]:(2.13384628296e-05) A[3]:(0.531660795212)\n",
      " state (5)  A[0]:(0.160558387637) A[1]:(0.92884349823) A[2]:(-0.18053303659) A[3]:(0.514398217201)\n",
      " state (6)  A[0]:(9.56654548645e-06) A[1]:(0.810001194477) A[2]:(3.26633453369e-05) A[3]:(0.656206250191)\n",
      " state (7)  A[0]:(0.638136386871) A[1]:(-0.252750456333) A[2]:(0.257676541805) A[3]:(0.894500672817)\n",
      " state (8)  A[0]:(0.656131923199) A[1]:(-9.7069889307e-05) A[2]:(0.729054808617) A[3]:(0.590505123138)\n",
      " state (9)  A[0]:(0.65610396862) A[1]:(0.810022950172) A[2]:(0.810037016869) A[3]:(-0.000123247504234)\n",
      " state (10)  A[0]:(0.72904086113) A[1]:(0.900004267693) A[2]:(-0.000106573104858) A[3]:(0.728943705559)\n",
      " state (11)  A[0]:(0.517088532448) A[1]:(0.876677513123) A[2]:(-0.586484670639) A[3]:(0.842380046844)\n",
      " state (12)  A[0]:(0.0726535916328) A[1]:(0.823953926563) A[2]:(-0.532883644104) A[3]:(0.791882634163)\n",
      " state (13)  A[0]:(-4.93228435516e-05) A[1]:(0.808798193932) A[2]:(0.900061428547) A[3]:(0.728957056999)\n",
      " state (14)  A[0]:(0.810068190098) A[1]:(0.90025049448) A[2]:(0.999999880791) A[3]:(0.809951841831)\n",
      " state (15)  A[0]:(0.987850666046) A[1]:(0.960556149483) A[2]:(1.0) A[3]:(0.889171779156)\n",
      "Episode 442000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6045. Times reached goal: 980.               Steps done: 3809243. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.019948476277.\n",
      " state (0)  A[0]:(0.530891895294) A[1]:(0.590363740921) A[2]:(0.590380430222) A[3]:(0.53123652935)\n",
      " state (1)  A[0]:(0.530968546867) A[1]:(-0.00011533126235) A[2]:(0.655955076218) A[3]:(0.59022551775)\n",
      " state (2)  A[0]:(0.589993000031) A[1]:(0.728843927383) A[2]:(0.590220451355) A[3]:(0.655882894993)\n",
      " state (3)  A[0]:(0.655658900738) A[1]:(-0.230537250638) A[2]:(0.535392999649) A[3]:(0.520391821861)\n",
      " state (4)  A[0]:(0.589883267879) A[1]:(0.655854582787) A[2]:(-0.000338912010193) A[3]:(0.530925035477)\n",
      " state (5)  A[0]:(0.159500017762) A[1]:(0.928794026375) A[2]:(-0.180974811316) A[3]:(0.513716340065)\n",
      " state (6)  A[0]:(-0.000884681707248) A[1]:(0.809883892536) A[2]:(-0.000311493873596) A[3]:(0.655598163605)\n",
      " state (7)  A[0]:(0.637633025646) A[1]:(-0.252916872501) A[2]:(0.257612228394) A[3]:(0.894160985947)\n",
      " state (8)  A[0]:(0.655684232712) A[1]:(-0.000402655423386) A[2]:(0.728878557682) A[3]:(0.589703321457)\n",
      " state (9)  A[0]:(0.655692338943) A[1]:(0.809827804565) A[2]:(0.809936583042) A[3]:(-0.00108824623749)\n",
      " state (10)  A[0]:(0.728806376457) A[1]:(0.89990645647) A[2]:(-0.000379204720957) A[3]:(0.728483617306)\n",
      " state (11)  A[0]:(0.516898155212) A[1]:(0.876566231251) A[2]:(-0.586843132973) A[3]:(0.842111051083)\n",
      " state (12)  A[0]:(0.0725187659264) A[1]:(0.823782145977) A[2]:(-0.533501982689) A[3]:(0.79154419899)\n",
      " state (13)  A[0]:(-0.000191986560822) A[1]:(0.808556854725) A[2]:(0.899976491928) A[3]:(0.728568196297)\n",
      " state (14)  A[0]:(0.809983730316) A[1]:(0.900060594082) A[2]:(0.999999880791) A[3]:(0.809777200222)\n",
      " state (15)  A[0]:(0.98782402277) A[1]:(0.960434317589) A[2]:(1.0) A[3]:(0.88909804821)\n",
      "Episode 443000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6053. Times reached goal: 983.               Steps done: 3815296. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.019828092858.\n",
      " state (0)  A[0]:(0.531058490276) A[1]:(0.590514659882) A[2]:(0.590321242809) A[3]:(0.531045734882)\n",
      " state (1)  A[0]:(0.531153440475) A[1]:(-1.18836760521e-05) A[2]:(0.655940711498) A[3]:(0.590268373489)\n",
      " state (2)  A[0]:(0.590250372887) A[1]:(0.72897541523) A[2]:(0.590427458286) A[3]:(0.65596306324)\n",
      " state (3)  A[0]:(0.655999302864) A[1]:(-0.230949908495) A[2]:(0.535768270493) A[3]:(0.520436763763)\n",
      " state (4)  A[0]:(0.590422987938) A[1]:(0.65599501133) A[2]:(2.7060508728e-05) A[3]:(0.531070351601)\n",
      " state (5)  A[0]:(0.160393372178) A[1]:(0.928840398788) A[2]:(-0.180714786053) A[3]:(0.513985276222)\n",
      " state (6)  A[0]:(-3.79383563995e-05) A[1]:(0.810017585754) A[2]:(-1.43051147461e-06) A[3]:(0.655902862549)\n",
      " state (7)  A[0]:(0.638022661209) A[1]:(-0.252534091473) A[2]:(0.258105635643) A[3]:(0.89430809021)\n",
      " state (8)  A[0]:(0.656062602997) A[1]:(-3.11061739922e-05) A[2]:(0.72898453474) A[3]:(0.590482115746)\n",
      " state (9)  A[0]:(0.655980288982) A[1]:(0.809960603714) A[2]:(0.81001085043) A[3]:(2.41845846176e-05)\n",
      " state (10)  A[0]:(0.72897785902) A[1]:(0.899993956089) A[2]:(-0.000164031982422) A[3]:(0.728949904442)\n",
      " state (11)  A[0]:(0.517133831978) A[1]:(0.876701116562) A[2]:(-0.586895346642) A[3]:(0.842403590679)\n",
      " state (12)  A[0]:(0.0727633088827) A[1]:(0.824019134045) A[2]:(-0.533800959587) A[3]:(0.79191327095)\n",
      " state (13)  A[0]:(-4.10974025726e-05) A[1]:(0.808871150017) A[2]:(0.900009453297) A[3]:(0.728991627693)\n",
      " state (14)  A[0]:(0.810035228729) A[1]:(0.900266945362) A[2]:(0.999999880791) A[3]:(0.810031235218)\n",
      " state (15)  A[0]:(0.987805426121) A[1]:(0.960512757301) A[2]:(1.0) A[3]:(0.88914847374)\n",
      "Episode 444000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6057. Times reached goal: 980.               Steps done: 3821353. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0197083570854.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5905,  0.5905,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.0001,  0.6560,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.7291,  0.5899,  0.6563]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 3.7664e-04,  8.1000e-01, -8.9407e-06,  6.5633e-01]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.8999, -0.0007,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9003,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531213521957) A[1]:(0.590459346771) A[2]:(0.590510487556) A[3]:(0.531489849091)\n",
      " state (1)  A[0]:(0.53123652935) A[1]:(0.000406097591622) A[2]:(0.656177163124) A[3]:(0.590567529202)\n",
      " state (2)  A[0]:(0.590425908566) A[1]:(0.729026198387) A[2]:(0.589970231056) A[3]:(0.656351804733)\n",
      " state (3)  A[0]:(0.656170010567) A[1]:(-0.227805331349) A[2]:(0.53505885601) A[3]:(0.521306872368)\n",
      " state (4)  A[0]:(0.590611875057) A[1]:(0.655934333801) A[2]:(-3.00407409668e-05) A[3]:(0.531692802906)\n",
      " state (5)  A[0]:(0.160834014416) A[1]:(0.928799629211) A[2]:(-0.180613070726) A[3]:(0.514622747898)\n",
      " state (6)  A[0]:(0.000247895717621) A[1]:(0.810035347939) A[2]:(0.000186324119568) A[3]:(0.656209588051)\n",
      " state (7)  A[0]:(0.638032436371) A[1]:(-0.252255111933) A[2]:(0.258458763361) A[3]:(0.894296824932)\n",
      " state (8)  A[0]:(0.656139612198) A[1]:(0.000182520598173) A[2]:(0.72915828228) A[3]:(0.5904058218)\n",
      " state (9)  A[0]:(0.65604698658) A[1]:(0.810016751289) A[2]:(0.810132265091) A[3]:(-5.66095113754e-05)\n",
      " state (10)  A[0]:(0.729063510895) A[1]:(0.900027930737) A[2]:(0.000206828117371) A[3]:(0.728992819786)\n",
      " state (11)  A[0]:(0.517364621162) A[1]:(0.876761317253) A[2]:(-0.586757659912) A[3]:(0.842456579208)\n",
      " state (12)  A[0]:(0.0731546953321) A[1]:(0.824128866196) A[2]:(-0.5338139534) A[3]:(0.791975617409)\n",
      " state (13)  A[0]:(0.000396043033106) A[1]:(0.809005558491) A[2]:(0.900123596191) A[3]:(0.729025602341)\n",
      " state (14)  A[0]:(0.810237646103) A[1]:(0.900340020657) A[2]:(0.999999880791) A[3]:(0.810007810593)\n",
      " state (15)  A[0]:(0.987801790237) A[1]:(0.960520625114) A[2]:(1.0) A[3]:(0.889040291309)\n",
      "Episode 445000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6049. Times reached goal: 979.               Steps done: 3827402. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0195895010758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53216791153) A[1]:(0.590460479259) A[2]:(0.590556740761) A[3]:(0.53119790554)\n",
      " state (1)  A[0]:(0.532309651375) A[1]:(-1.8123537302e-05) A[2]:(0.656096279621) A[3]:(0.590139389038)\n",
      " state (2)  A[0]:(0.591370284557) A[1]:(0.729032635689) A[2]:(0.590603172779) A[3]:(0.655781030655)\n",
      " state (3)  A[0]:(0.656896591187) A[1]:(-0.230753421783) A[2]:(0.535911440849) A[3]:(0.520253360271)\n",
      " state (4)  A[0]:(0.591494321823) A[1]:(0.656127095222) A[2]:(0.000179886817932) A[3]:(0.530961096287)\n",
      " state (5)  A[0]:(0.162148803473) A[1]:(0.928880035877) A[2]:(-0.180586203933) A[3]:(0.513942062855)\n",
      " state (6)  A[0]:(0.00187948124949) A[1]:(0.81003510952) A[2]:(0.000462293595774) A[3]:(0.655690491199)\n",
      " state (7)  A[0]:(0.638847231865) A[1]:(-0.252632796764) A[2]:(0.259146481752) A[3]:(0.894015908241)\n",
      " state (8)  A[0]:(0.656697511673) A[1]:(-8.14236700535e-05) A[2]:(0.729278564453) A[3]:(0.589586019516)\n",
      " state (9)  A[0]:(0.656565845013) A[1]:(0.810037732124) A[2]:(0.810234189034) A[3]:(-0.00155566504691)\n",
      " state (10)  A[0]:(0.729575872421) A[1]:(0.900122642517) A[2]:(0.000255465507507) A[3]:(0.728442072868)\n",
      " state (11)  A[0]:(0.518251299858) A[1]:(0.876937031746) A[2]:(-0.587041199207) A[3]:(0.842220008373)\n",
      " state (12)  A[0]:(0.0742767006159) A[1]:(0.824425041676) A[2]:(-0.534362435341) A[3]:(0.791708230972)\n",
      " state (13)  A[0]:(0.00129437376745) A[1]:(0.809362888336) A[2]:(0.900118649006) A[3]:(0.728704929352)\n",
      " state (14)  A[0]:(0.810411095619) A[1]:(0.9005458951) A[2]:(0.999999880791) A[3]:(0.809818387032)\n",
      " state (15)  A[0]:(0.987779378891) A[1]:(0.960584759712) A[2]:(1.0) A[3]:(0.888888776302)\n",
      "Episode 446000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6068. Times reached goal: 982.               Steps done: 3833470. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0194709919038.\n",
      " state (0)  A[0]:(0.531387090683) A[1]:(0.590439796448) A[2]:(0.590552210808) A[3]:(0.531198620796)\n",
      " state (1)  A[0]:(0.53154695034) A[1]:(2.26125121117e-05) A[2]:(0.656123280525) A[3]:(0.590416431427)\n",
      " state (2)  A[0]:(0.59070289135) A[1]:(0.728993177414) A[2]:(0.590693473816) A[3]:(0.656098008156)\n",
      " state (3)  A[0]:(0.656350910664) A[1]:(-0.231539204717) A[2]:(0.536069214344) A[3]:(0.520596504211)\n",
      " state (4)  A[0]:(0.590887963772) A[1]:(0.656200706959) A[2]:(8.30888748169e-05) A[3]:(0.531364917755)\n",
      " state (5)  A[0]:(0.161188557744) A[1]:(0.92887878418) A[2]:(-0.180851653218) A[3]:(0.514380991459)\n",
      " state (6)  A[0]:(0.00103512371425) A[1]:(0.809997081757) A[2]:(3.34978103638e-05) A[3]:(0.656154572964)\n",
      " state (7)  A[0]:(0.638656497002) A[1]:(-0.252613455057) A[2]:(0.258722186089) A[3]:(0.894278526306)\n",
      " state (8)  A[0]:(0.656695723534) A[1]:(0.000289522111416) A[2]:(0.729056298733) A[3]:(0.590549945831)\n",
      " state (9)  A[0]:(0.656500518322) A[1]:(0.81006449461) A[2]:(0.810021281242) A[3]:(0.000239297747612)\n",
      " state (10)  A[0]:(0.729203701019) A[1]:(0.899996101856) A[2]:(-2.63452529907e-05) A[3]:(0.729042887688)\n",
      " state (11)  A[0]:(0.517367005348) A[1]:(0.876675724983) A[2]:(-0.587227940559) A[3]:(0.842477321625)\n",
      " state (12)  A[0]:(0.0729447305202) A[1]:(0.823978424072) A[2]:(-0.534800291061) A[3]:(0.792022645473)\n",
      " state (13)  A[0]:(3.90708446503e-05) A[1]:(0.808820009232) A[2]:(0.900072753429) A[3]:(0.729130446911)\n",
      " state (14)  A[0]:(0.810151100159) A[1]:(0.900220155716) A[2]:(0.999999880791) A[3]:(0.810152947903)\n",
      " state (15)  A[0]:(0.987757146358) A[1]:(0.960426270962) A[2]:(1.0) A[3]:(0.889053940773)\n",
      "Episode 447000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6065. Times reached goal: 981.               Steps done: 3839535. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0193532577277.\n",
      " state (0)  A[0]:(0.531567811966) A[1]:(0.590666949749) A[2]:(0.590499520302) A[3]:(0.53157222271)\n",
      " state (1)  A[0]:(0.531513750553) A[1]:(2.0619481802e-05) A[2]:(0.656143307686) A[3]:(0.590540349483)\n",
      " state (2)  A[0]:(0.590568482876) A[1]:(0.729053139687) A[2]:(0.59047305584) A[3]:(0.656203269958)\n",
      " state (3)  A[0]:(0.656243979931) A[1]:(-0.229847222567) A[2]:(0.535503745079) A[3]:(0.520859479904)\n",
      " state (4)  A[0]:(0.590611100197) A[1]:(0.656199634075) A[2]:(-0.000409483880503) A[3]:(0.531513690948)\n",
      " state (5)  A[0]:(0.160549268126) A[1]:(0.92884272337) A[2]:(-0.181284382939) A[3]:(0.514576673508)\n",
      " state (6)  A[0]:(-2.52425670624e-05) A[1]:(0.809951424599) A[2]:(-0.000357627839549) A[3]:(0.656253099442)\n",
      " state (7)  A[0]:(0.637897253036) A[1]:(-0.252804607153) A[2]:(0.258481025696) A[3]:(0.894315421581)\n",
      " state (8)  A[0]:(0.656255960464) A[1]:(-0.000235382467508) A[2]:(0.728672862053) A[3]:(0.591091871262)\n",
      " state (9)  A[0]:(0.656168341637) A[1]:(0.809906840324) A[2]:(0.809732854366) A[3]:(0.000695556285791)\n",
      " state (10)  A[0]:(0.728957772255) A[1]:(0.89995765686) A[2]:(-0.00080072862329) A[3]:(0.729099035263)\n",
      " state (11)  A[0]:(0.517029881477) A[1]:(0.876687169075) A[2]:(-0.587897598743) A[3]:(0.842488110065)\n",
      " state (12)  A[0]:(0.0724320188165) A[1]:(0.824066400528) A[2]:(-0.535775661469) A[3]:(0.791993260384)\n",
      " state (13)  A[0]:(-0.000722437980585) A[1]:(0.808963418007) A[2]:(0.899856328964) A[3]:(0.729015409946)\n",
      " state (14)  A[0]:(0.809748709202) A[1]:(0.900295555592) A[2]:(0.999999880791) A[3]:(0.810019373894)\n",
      " state (15)  A[0]:(0.98770314455) A[1]:(0.960440456867) A[2]:(1.0) A[3]:(0.888902306557)\n",
      "Episode 448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6056. Times reached goal: 989.               Steps done: 3845591. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0192364085752.\n",
      " state (0)  A[0]:(0.531242847443) A[1]:(0.59073060751) A[2]:(0.59047305584) A[3]:(0.531381607056)\n",
      " state (1)  A[0]:(0.531267285347) A[1]:(3.61874699593e-05) A[2]:(0.656103551388) A[3]:(0.590530872345)\n",
      " state (2)  A[0]:(0.590355157852) A[1]:(0.728974103928) A[2]:(0.590541243553) A[3]:(0.656205534935)\n",
      " state (3)  A[0]:(0.656073093414) A[1]:(-0.230422064662) A[2]:(0.535804510117) A[3]:(0.520668923855)\n",
      " state (4)  A[0]:(0.590474367142) A[1]:(0.656165480614) A[2]:(-1.91926956177e-05) A[3]:(0.531334936619)\n",
      " state (5)  A[0]:(0.160456880927) A[1]:(0.928857862949) A[2]:(-0.180947244167) A[3]:(0.514440894127)\n",
      " state (6)  A[0]:(-4.72068786621e-05) A[1]:(0.809976577759) A[2]:(9.46521759033e-05) A[3]:(0.656098127365)\n",
      " state (7)  A[0]:(0.637779355049) A[1]:(-0.252870291471) A[2]:(0.259159266949) A[3]:(0.894193649292)\n",
      " state (8)  A[0]:(0.65614926815) A[1]:(-0.000336192548275) A[2]:(0.728962004185) A[3]:(0.590805768967)\n",
      " state (9)  A[0]:(0.656107664108) A[1]:(0.809917092323) A[2]:(0.810012340546) A[3]:(0.000173881649971)\n",
      " state (10)  A[0]:(0.729062795639) A[1]:(0.899991929531) A[2]:(-0.000143766403198) A[3]:(0.72899055481)\n",
      " state (11)  A[0]:(0.517416954041) A[1]:(0.876756906509) A[2]:(-0.587718307972) A[3]:(0.842499613762)\n",
      " state (12)  A[0]:(0.0731063112617) A[1]:(0.824194431305) A[2]:(-0.535783648491) A[3]:(0.792034029961)\n",
      " state (13)  A[0]:(9.80496406555e-06) A[1]:(0.809125542641) A[2]:(0.900000572205) A[3]:(0.729046821594)\n",
      " state (14)  A[0]:(0.80999994278) A[1]:(0.900385856628) A[2]:(0.999999880791) A[3]:(0.810006320477)\n",
      " state (15)  A[0]:(0.987695336342) A[1]:(0.960453927517) A[2]:(1.0) A[3]:(0.888797223568)\n",
      "Episode 449000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6048. Times reached goal: 982.               Steps done: 3851639. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0191204178856.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5906,  0.5904,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6561,  0.0002,  0.5317]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6562,  0.0001,  0.7289,  0.5907]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.8100,  0.8100,  0.0001]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8090,  0.9002,  0.7291]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531477928162) A[1]:(0.590601086617) A[2]:(0.590500473976) A[3]:(0.531602919102)\n",
      " state (1)  A[0]:(0.531518161297) A[1]:(4.21069562435e-05) A[2]:(0.656106650829) A[3]:(0.590466976166)\n",
      " state (2)  A[0]:(0.590568959713) A[1]:(0.729071855545) A[2]:(0.590503692627) A[3]:(0.656055927277)\n",
      " state (3)  A[0]:(0.656198382378) A[1]:(-0.229391425848) A[2]:(0.535775184631) A[3]:(0.520835042)\n",
      " state (4)  A[0]:(0.590536236763) A[1]:(0.656181454659) A[2]:(0.00013256072998) A[3]:(0.531446754932)\n",
      " state (5)  A[0]:(0.160501793027) A[1]:(0.928866922855) A[2]:(-0.180938601494) A[3]:(0.514521002769)\n",
      " state (6)  A[0]:(-7.95722007751e-06) A[1]:(0.810054838657) A[2]:(5.2809715271e-05) A[3]:(0.656079232693)\n",
      " state (7)  A[0]:(0.637800812721) A[1]:(-0.252409785986) A[2]:(0.259303152561) A[3]:(0.894106924534)\n",
      " state (8)  A[0]:(0.656120598316) A[1]:(0.000299360603094) A[2]:(0.72912478447) A[3]:(0.590287387371)\n",
      " state (9)  A[0]:(0.656028568745) A[1]:(0.810053467751) A[2]:(0.810046374798) A[3]:(-0.000239044427872)\n",
      " state (10)  A[0]:(0.728957295418) A[1]:(0.900025367737) A[2]:(-0.00017261505127) A[3]:(0.728887915611)\n",
      " state (11)  A[0]:(0.517253398895) A[1]:(0.876773118973) A[2]:(-0.587889194489) A[3]:(0.842434763908)\n",
      " state (12)  A[0]:(0.0728901252151) A[1]:(0.824181079865) A[2]:(-0.536223173141) A[3]:(0.791944921017)\n",
      " state (13)  A[0]:(-0.000226944684982) A[1]:(0.809041261673) A[2]:(0.899946689606) A[3]:(0.728933393955)\n",
      " state (14)  A[0]:(0.809955537319) A[1]:(0.90026974678) A[2]:(0.999999880791) A[3]:(0.809949815273)\n",
      " state (15)  A[0]:(0.987677633762) A[1]:(0.960357904434) A[2]:(1.0) A[3]:(0.888734519482)\n",
      "Episode 450000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6062. Times reached goal: 979.               Steps done: 3857701. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0190048605206.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531072616577) A[1]:(0.590450048447) A[2]:(0.590436697006) A[3]:(0.531296372414)\n",
      " state (1)  A[0]:(0.531620264053) A[1]:(0.000261429697275) A[2]:(0.656148195267) A[3]:(0.590682387352)\n",
      " state (2)  A[0]:(0.59097379446) A[1]:(0.728949904442) A[2]:(0.590488553047) A[3]:(0.656461238861)\n",
      " state (3)  A[0]:(0.656895279884) A[1]:(-0.229734420776) A[2]:(0.535740017891) A[3]:(0.521252393723)\n",
      " state (4)  A[0]:(0.591761410236) A[1]:(0.655984163284) A[2]:(-3.93390655518e-06) A[3]:(0.531987011433)\n",
      " state (5)  A[0]:(0.162926837802) A[1]:(0.92885774374) A[2]:(-0.181164011359) A[3]:(0.515316605568)\n",
      " state (6)  A[0]:(0.00298809120432) A[1]:(0.810044407845) A[2]:(-6.3419342041e-05) A[3]:(0.656766414642)\n",
      " state (7)  A[0]:(0.639704823494) A[1]:(-0.25240150094) A[2]:(0.259330183268) A[3]:(0.894349992275)\n",
      " state (8)  A[0]:(0.658365666866) A[1]:(0.000306252390146) A[2]:(0.728845119476) A[3]:(0.592044353485)\n",
      " state (9)  A[0]:(0.658573746681) A[1]:(0.810052514076) A[2]:(0.809956789017) A[3]:(0.00303128920496)\n",
      " state (10)  A[0]:(0.731115281582) A[1]:(0.900026082993) A[2]:(0.000118970870972) A[3]:(0.730313539505)\n",
      " state (11)  A[0]:(0.520743191242) A[1]:(0.876769125462) A[2]:(-0.587752938271) A[3]:(0.843300521374)\n",
      " state (12)  A[0]:(0.0776055306196) A[1]:(0.824144482613) A[2]:(-0.536620616913) A[3]:(0.793034791946)\n",
      " state (13)  A[0]:(0.00385857699439) A[1]:(0.808886706829) A[2]:(0.899488091469) A[3]:(0.730196356773)\n",
      " state (14)  A[0]:(0.810952305794) A[1]:(0.90007430315) A[2]:(0.999999880791) A[3]:(0.810766458511)\n",
      " state (15)  A[0]:(0.987735331059) A[1]:(0.960244894028) A[2]:(1.0) A[3]:(0.889209866524)\n",
      "Episode 451000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6042. Times reached goal: 976.               Steps done: 3863743. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0188903793492.\n",
      " state (0)  A[0]:(0.531500160694) A[1]:(0.590330839157) A[2]:(0.590498328209) A[3]:(0.531603455544)\n",
      " state (1)  A[0]:(0.531494259834) A[1]:(-5.53950667381e-06) A[2]:(0.656102061272) A[3]:(0.59063410759)\n",
      " state (2)  A[0]:(0.590557813644) A[1]:(0.728926837444) A[2]:(0.590555846691) A[3]:(0.656256079674)\n",
      " state (3)  A[0]:(0.656009078026) A[1]:(-0.230508267879) A[2]:(0.535861253738) A[3]:(0.520941615105)\n",
      " state (4)  A[0]:(0.590343415737) A[1]:(0.655917882919) A[2]:(9.41753387451e-06) A[3]:(0.531658351421)\n",
      " state (5)  A[0]:(0.160330399871) A[1]:(0.928820073605) A[2]:(-0.181142792106) A[3]:(0.514825820923)\n",
      " state (6)  A[0]:(-0.000126421451569) A[1]:(0.809915423393) A[2]:(9.05990600586e-06) A[3]:(0.656194448471)\n",
      " state (7)  A[0]:(0.637595295906) A[1]:(-0.252794444561) A[2]:(0.259617030621) A[3]:(0.894031107426)\n",
      " state (8)  A[0]:(0.656119704247) A[1]:(-0.000129517167807) A[2]:(0.728983163834) A[3]:(0.590550899506)\n",
      " state (9)  A[0]:(0.656134366989) A[1]:(0.809913158417) A[2]:(0.810003459454) A[3]:(0.000221714377403)\n",
      " state (10)  A[0]:(0.72903418541) A[1]:(0.899963796139) A[2]:(-0.000129461288452) A[3]:(0.729066848755)\n",
      " state (11)  A[0]:(0.517421066761) A[1]:(0.876717507839) A[2]:(-0.588161766529) A[3]:(0.842552959919)\n",
      " state (12)  A[0]:(0.073100797832) A[1]:(0.824124217033) A[2]:(-0.536917448044) A[3]:(0.79206764698)\n",
      " state (13)  A[0]:(-8.88109207153e-05) A[1]:(0.808977484703) A[2]:(0.899995625019) A[3]:(0.729029417038)\n",
      " state (14)  A[0]:(0.81000828743) A[1]:(0.900207459927) A[2]:(0.999999880791) A[3]:(0.80999815464)\n",
      " state (15)  A[0]:(0.987638413906) A[1]:(0.960278093815) A[2]:(1.0) A[3]:(0.888633847237)\n",
      "Episode 452000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6069. Times reached goal: 980.               Steps done: 3869812. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0187760808266.\n",
      " state (0)  A[0]:(0.531710386276) A[1]:(0.590660095215) A[2]:(0.590532302856) A[3]:(0.531527578831)\n",
      " state (1)  A[0]:(0.531581640244) A[1]:(-7.25202262402e-05) A[2]:(0.656111121178) A[3]:(0.590484380722)\n",
      " state (2)  A[0]:(0.590574860573) A[1]:(0.729001998901) A[2]:(0.590471506119) A[3]:(0.656099081039)\n",
      " state (3)  A[0]:(0.656139850616) A[1]:(-0.229589864612) A[2]:(0.535703480244) A[3]:(0.520776867867)\n",
      " state (4)  A[0]:(0.590435564518) A[1]:(0.656169176102) A[2]:(-9.10758972168e-05) A[3]:(0.531426310539)\n",
      " state (5)  A[0]:(0.160287752748) A[1]:(0.928839921951) A[2]:(-0.181226044893) A[3]:(0.51458388567)\n",
      " state (6)  A[0]:(-0.000486552686198) A[1]:(0.809947907925) A[2]:(-7.35521316528e-05) A[3]:(0.656024098396)\n",
      " state (7)  A[0]:(0.637303829193) A[1]:(-0.252893745899) A[2]:(0.25964948535) A[3]:(0.894018650055)\n",
      " state (8)  A[0]:(0.655899941921) A[1]:(-0.000236075371504) A[2]:(0.728862285614) A[3]:(0.590515017509)\n",
      " state (9)  A[0]:(0.655792832375) A[1]:(0.809946477413) A[2]:(0.809874653816) A[3]:(-0.000435680121882)\n",
      " state (10)  A[0]:(0.728682160378) A[1]:(0.899973511696) A[2]:(-0.000484228105051) A[3]:(0.728665113449)\n",
      " state (11)  A[0]:(0.516841769218) A[1]:(0.876729786396) A[2]:(-0.588539540768) A[3]:(0.842314898968)\n",
      " state (12)  A[0]:(0.0722687616944) A[1]:(0.824164509773) A[2]:(-0.537517786026) A[3]:(0.791783094406)\n",
      " state (13)  A[0]:(-0.000949382490944) A[1]:(0.809056818485) A[2]:(0.89998036623) A[3]:(0.728681206703)\n",
      " state (14)  A[0]:(0.80975317955) A[1]:(0.900268018246) A[2]:(0.999999880791) A[3]:(0.809739589691)\n",
      " state (15)  A[0]:(0.987598419189) A[1]:(0.960288465023) A[2]:(1.0) A[3]:(0.888397097588)\n",
      "Episode 453000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6048. Times reached goal: 978.               Steps done: 3875860. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0186628657972.\n",
      " state (0)  A[0]:(0.531492114067) A[1]:(0.59055352211) A[2]:(0.590488314629) A[3]:(0.531554222107)\n",
      " state (1)  A[0]:(0.531482219696) A[1]:(0.000138159841299) A[2]:(0.656238973141) A[3]:(0.590629935265)\n",
      " state (2)  A[0]:(0.59054517746) A[1]:(0.729204058647) A[2]:(0.590638399124) A[3]:(0.656302452087)\n",
      " state (3)  A[0]:(0.656224727631) A[1]:(-0.22807058692) A[2]:(0.53559076786) A[3]:(0.521118760109)\n",
      " state (4)  A[0]:(0.590636610985) A[1]:(0.656093597412) A[2]:(-4.61339950562e-05) A[3]:(0.531675219536)\n",
      " state (5)  A[0]:(0.160803049803) A[1]:(0.928860127926) A[2]:(-0.181318745017) A[3]:(0.514956772327)\n",
      " state (6)  A[0]:(0.000100076198578) A[1]:(0.810099422932) A[2]:(-0.000173568725586) A[3]:(0.656279802322)\n",
      " state (7)  A[0]:(0.637530982494) A[1]:(-0.252288579941) A[2]:(0.259793460369) A[3]:(0.894032001495)\n",
      " state (8)  A[0]:(0.656110167503) A[1]:(0.000275488942862) A[2]:(0.72905921936) A[3]:(0.590619802475)\n",
      " state (9)  A[0]:(0.656108140945) A[1]:(0.810039937496) A[2]:(0.810100317001) A[3]:(0.000139251351357)\n",
      " state (10)  A[0]:(0.729106009007) A[1]:(0.900008022785) A[2]:(-7.28368759155e-05) A[3]:(0.729160904884)\n",
      " state (11)  A[0]:(0.517672419548) A[1]:(0.876758873463) A[2]:(-0.588541507721) A[3]:(0.842680811882)\n",
      " state (12)  A[0]:(0.0734550580382) A[1]:(0.824174880981) A[2]:(-0.537774860859) A[3]:(0.792244911194)\n",
      " state (13)  A[0]:(0.00020232796669) A[1]:(0.809011340141) A[2]:(0.900025367737) A[3]:(0.729233980179)\n",
      " state (14)  A[0]:(0.810129880905) A[1]:(0.900190412998) A[2]:(0.999999880791) A[3]:(0.810165822506)\n",
      " state (15)  A[0]:(0.987601161003) A[1]:(0.960211455822) A[2]:(1.0) A[3]:(0.888614833355)\n",
      "Episode 454000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6049. Times reached goal: 978.               Steps done: 3881909. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0185503148754.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5905,  0.5904,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5909,  0.6562,  0.0001,  0.5313]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6567,  0.0006,  0.7290,  0.5904]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565,  0.8101,  0.8099,  0.0002]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8091,  0.9001,  0.7288]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9002,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531724274158) A[1]:(0.590497851372) A[2]:(0.590424835682) A[3]:(0.531593501568)\n",
      " state (1)  A[0]:(0.531621336937) A[1]:(7.66664743423e-05) A[2]:(0.656087756157) A[3]:(0.590533256531)\n",
      " state (2)  A[0]:(0.590591549873) A[1]:(0.72899222374) A[2]:(0.590529322624) A[3]:(0.656132638454)\n",
      " state (3)  A[0]:(0.656262695789) A[1]:(-0.229827821255) A[2]:(0.535876631737) A[3]:(0.520773768425)\n",
      " state (4)  A[0]:(0.590595602989) A[1]:(0.656298875809) A[2]:(3.46899032593e-05) A[3]:(0.531496286392)\n",
      " state (5)  A[0]:(0.160564273596) A[1]:(0.928898334503) A[2]:(-0.181244835258) A[3]:(0.51477766037)\n",
      " state (6)  A[0]:(0.000101208686829) A[1]:(0.81003844738) A[2]:(-6.91413879395e-06) A[3]:(0.656146109104)\n",
      " state (7)  A[0]:(0.637602329254) A[1]:(-0.252512276173) A[2]:(0.260013639927) A[3]:(0.893927872181)\n",
      " state (8)  A[0]:(0.656140089035) A[1]:(0.000572547258344) A[2]:(0.728992402554) A[3]:(0.590336024761)\n",
      " state (9)  A[0]:(0.656073212624) A[1]:(0.810165345669) A[2]:(0.809999287128) A[3]:(-0.000188902020454)\n",
      " state (10)  A[0]:(0.728897929192) A[1]:(0.900048136711) A[2]:(-6.29425048828e-05) A[3]:(0.728772640228)\n",
      " state (11)  A[0]:(0.517189264297) A[1]:(0.876804232597) A[2]:(-0.588561713696) A[3]:(0.842348277569)\n",
      " state (12)  A[0]:(0.0726807191968) A[1]:(0.824261307716) A[2]:(-0.53803396225) A[3]:(0.791787624359)\n",
      " state (13)  A[0]:(-0.000694811227731) A[1]:(0.809130311012) A[2]:(0.899989962578) A[3]:(0.728670477867)\n",
      " state (14)  A[0]:(0.809829235077) A[1]:(0.900262355804) A[2]:(0.999999880791) A[3]:(0.809817612171)\n",
      " state (15)  A[0]:(0.987565815449) A[1]:(0.960230767727) A[2]:(1.0) A[3]:(0.888405203819)\n",
      "Episode 455000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6065. Times reached goal: 988.               Steps done: 3887974. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0184381477064.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53101170063) A[1]:(0.590614676476) A[2]:(0.590473890305) A[3]:(0.531492292881)\n",
      " state (1)  A[0]:(0.531236052513) A[1]:(0.000262644141912) A[2]:(0.656123816967) A[3]:(0.590624213219)\n",
      " state (2)  A[0]:(0.590475082397) A[1]:(0.729122757912) A[2]:(0.590504407883) A[3]:(0.656347215176)\n",
      " state (3)  A[0]:(0.656391561031) A[1]:(-0.228633373976) A[2]:(0.535750865936) A[3]:(0.521077036858)\n",
      " state (4)  A[0]:(0.591118216515) A[1]:(0.65632724762) A[2]:(9.77516174316e-06) A[3]:(0.531857311726)\n",
      " state (5)  A[0]:(0.1619258672) A[1]:(0.928940474987) A[2]:(-0.181398972869) A[3]:(0.515412807465)\n",
      " state (6)  A[0]:(0.00168114737608) A[1]:(0.810159444809) A[2]:(3.03983688354e-05) A[3]:(0.656667947769)\n",
      " state (7)  A[0]:(0.638235092163) A[1]:(-0.252351909876) A[2]:(0.260497659445) A[3]:(0.894036710262)\n",
      " state (8)  A[0]:(0.656652033329) A[1]:(0.000388052285416) A[2]:(0.729077219963) A[3]:(0.590986013412)\n",
      " state (9)  A[0]:(0.656567215919) A[1]:(0.810103952885) A[2]:(0.810144662857) A[3]:(0.000500574649777)\n",
      " state (10)  A[0]:(0.729464650154) A[1]:(0.90007174015) A[2]:(0.00027596950531) A[3]:(0.729208946228)\n",
      " state (11)  A[0]:(0.518304228783) A[1]:(0.876882016659) A[2]:(-0.588590145111) A[3]:(0.842724561691)\n",
      " state (12)  A[0]:(0.0742878392339) A[1]:(0.824407756329) A[2]:(-0.538323760033) A[3]:(0.792306184769)\n",
      " state (13)  A[0]:(0.000812083308119) A[1]:(0.809290349483) A[2]:(0.899969398975) A[3]:(0.729276180267)\n",
      " state (14)  A[0]:(0.810268580914) A[1]:(0.900321006775) A[2]:(0.999999880791) A[3]:(0.810178458691)\n",
      " state (15)  A[0]:(0.987573862076) A[1]:(0.960223913193) A[2]:(1.0) A[3]:(0.888519346714)\n",
      "Episode 456000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6082. Times reached goal: 988.               Steps done: 3894056. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.018326347222.\n",
      " state (0)  A[0]:(0.531290829182) A[1]:(0.590673983097) A[2]:(0.590491890907) A[3]:(0.531610369682)\n",
      " state (1)  A[0]:(0.53128528595) A[1]:(0.000147052109241) A[2]:(0.6561409235) A[3]:(0.590707421303)\n",
      " state (2)  A[0]:(0.590370953083) A[1]:(0.729055047035) A[2]:(0.590284585953) A[3]:(0.65639257431)\n",
      " state (3)  A[0]:(0.656099081039) A[1]:(-0.228028073907) A[2]:(0.535465717316) A[3]:(0.521119475365)\n",
      " state (4)  A[0]:(0.590388178825) A[1]:(0.656314253807) A[2]:(-0.000121831893921) A[3]:(0.531684041023)\n",
      " state (5)  A[0]:(0.160290673375) A[1]:(0.928863823414) A[2]:(-0.181371420622) A[3]:(0.515065312386)\n",
      " state (6)  A[0]:(-0.000355303258402) A[1]:(0.809997975826) A[2]:(4.79221343994e-05) A[3]:(0.656300663948)\n",
      " state (7)  A[0]:(0.637314915657) A[1]:(-0.25271692872) A[2]:(0.260563194752) A[3]:(0.893947064877)\n",
      " state (8)  A[0]:(0.656224250793) A[1]:(1.94683670998e-05) A[2]:(0.728999614716) A[3]:(0.59088820219)\n",
      " state (9)  A[0]:(0.656274080276) A[1]:(0.809999585152) A[2]:(0.809982180595) A[3]:(0.00042167303036)\n",
      " state (10)  A[0]:(0.729139447212) A[1]:(0.899985671043) A[2]:(-0.000216484069824) A[3]:(0.729061245918)\n",
      " state (11)  A[0]:(0.517701148987) A[1]:(0.876757502556) A[2]:(-0.589042425156) A[3]:(0.842583060265)\n",
      " state (12)  A[0]:(0.07339823246) A[1]:(0.824224531651) A[2]:(-0.538987994194) A[3]:(0.792108595371)\n",
      " state (13)  A[0]:(-5.49256801605e-05) A[1]:(0.809090077877) A[2]:(0.899958789349) A[3]:(0.729057967663)\n",
      " state (14)  A[0]:(0.810044646263) A[1]:(0.900206327438) A[2]:(0.999999880791) A[3]:(0.810095787048)\n",
      " state (15)  A[0]:(0.987537205219) A[1]:(0.960150659084) A[2]:(1.0) A[3]:(0.888449192047)\n",
      "Episode 457000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6066. Times reached goal: 989.               Steps done: 3900122. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0182155160904.\n",
      " state (0)  A[0]:(0.531550705433) A[1]:(0.590658307076) A[2]:(0.590472340584) A[3]:(0.531417071819)\n",
      " state (1)  A[0]:(0.531412124634) A[1]:(0.000121135264635) A[2]:(0.656057357788) A[3]:(0.590411663055)\n",
      " state (2)  A[0]:(0.590488553047) A[1]:(0.72902905941) A[2]:(0.590416789055) A[3]:(0.65603518486)\n",
      " state (3)  A[0]:(0.656185805798) A[1]:(-0.229515329003) A[2]:(0.535672545433) A[3]:(0.520672976971)\n",
      " state (4)  A[0]:(0.590647161007) A[1]:(0.656001210213) A[2]:(-0.000222563743591) A[3]:(0.531389832497)\n",
      " state (5)  A[0]:(0.160820901394) A[1]:(0.928809762001) A[2]:(-0.181608214974) A[3]:(0.51477265358)\n",
      " state (6)  A[0]:(1.20103359222e-05) A[1]:(0.809890031815) A[2]:(-0.000175476074219) A[3]:(0.65597897768)\n",
      " state (7)  A[0]:(0.637196063995) A[1]:(-0.252953052521) A[2]:(0.260540097952) A[3]:(0.893768429756)\n",
      " state (8)  A[0]:(0.655945897102) A[1]:(-0.000249605625868) A[2]:(0.728902816772) A[3]:(0.590445756912)\n",
      " state (9)  A[0]:(0.655815660954) A[1]:(0.809920608997) A[2]:(0.809932470322) A[3]:(-0.000155627727509)\n",
      " state (10)  A[0]:(0.72876650095) A[1]:(0.899970054626) A[2]:(-0.000359058351023) A[3]:(0.728913545609)\n",
      " state (11)  A[0]:(0.517243206501) A[1]:(0.876783907413) A[2]:(-0.589295506477) A[3]:(0.842537879944)\n",
      " state (12)  A[0]:(0.0728953108191) A[1]:(0.824326694012) A[2]:(-0.539415001869) A[3]:(0.792046546936)\n",
      " state (13)  A[0]:(-0.000478416652186) A[1]:(0.809262156487) A[2]:(0.900015711784) A[3]:(0.728933691978)\n",
      " state (14)  A[0]:(0.809930860996) A[1]:(0.900318741798) A[2]:(0.999999880791) A[3]:(0.809955954552)\n",
      " state (15)  A[0]:(0.9875035882) A[1]:(0.960178136826) A[2]:(1.0) A[3]:(0.888254106045)\n",
      "Episode 458000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6048. Times reached goal: 985.               Steps done: 3906170. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0181056811248.\n",
      " state (0)  A[0]:(0.531663298607) A[1]:(0.590485453606) A[2]:(0.590360462666) A[3]:(0.531691193581)\n",
      " state (1)  A[0]:(0.531606793404) A[1]:(-0.000128455460072) A[2]:(0.656035721302) A[3]:(0.590729236603)\n",
      " state (2)  A[0]:(0.590589523315) A[1]:(0.729080736637) A[2]:(0.590373337269) A[3]:(0.656386077404)\n",
      " state (3)  A[0]:(0.656145632267) A[1]:(-0.227883800864) A[2]:(0.535532832146) A[3]:(0.521056413651)\n",
      " state (4)  A[0]:(0.590428590775) A[1]:(0.65625) A[2]:(-0.000134468078613) A[3]:(0.531667709351)\n",
      " state (5)  A[0]:(0.160427495837) A[1]:(0.928866147995) A[2]:(-0.18156683445) A[3]:(0.515134811401)\n",
      " state (6)  A[0]:(-0.000312238931656) A[1]:(0.809912085533) A[2]:(1.04904174805e-05) A[3]:(0.656150996685)\n",
      " state (7)  A[0]:(0.636697649956) A[1]:(-0.252981603146) A[2]:(0.260943502188) A[3]:(0.893652677536)\n",
      " state (8)  A[0]:(0.655320703983) A[1]:(-9.52854752541e-05) A[2]:(0.728992938995) A[3]:(0.590136826038)\n",
      " state (9)  A[0]:(0.655283093452) A[1]:(0.80991756916) A[2]:(0.809947967529) A[3]:(-0.000157535076141)\n",
      " state (10)  A[0]:(0.728321790695) A[1]:(0.899945557117) A[2]:(-0.000301837921143) A[3]:(0.728851854801)\n",
      " state (11)  A[0]:(0.516496062279) A[1]:(0.876732647419) A[2]:(-0.589432835579) A[3]:(0.842454135418)\n",
      " state (12)  A[0]:(0.0716799199581) A[1]:(0.824210166931) A[2]:(-0.540074706078) A[3]:(0.791891753674)\n",
      " state (13)  A[0]:(-0.00231533823535) A[1]:(0.809026002884) A[2]:(0.899585902691) A[3]:(0.728651762009)\n",
      " state (14)  A[0]:(0.808980643749) A[1]:(0.900089561939) A[2]:(0.999999880791) A[3]:(0.809693336487)\n",
      " state (15)  A[0]:(0.987421274185) A[1]:(0.960048794746) A[2]:(1.0) A[3]:(0.888091623783)\n",
      "Episode 459000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6051. Times reached goal: 983.               Steps done: 3912221. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0179964544469.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5905,  0.5903,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.9065e-01,  6.5586e-01,  9.5367e-06,  5.3124e-01]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560, -0.0002,  0.7289,  0.5906]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558,  0.8099,  0.8100, -0.0002]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7288,  0.9000, -0.0002,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9004,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531159639359) A[1]:(0.590509951115) A[2]:(0.5903236866) A[3]:(0.53126335144)\n",
      " state (1)  A[0]:(0.531185805798) A[1]:(1.89132988453e-05) A[2]:(0.656101167202) A[3]:(0.590406060219)\n",
      " state (2)  A[0]:(0.590258896351) A[1]:(0.728935420513) A[2]:(0.590524792671) A[3]:(0.656128168106)\n",
      " state (3)  A[0]:(0.655986666679) A[1]:(-0.228655844927) A[2]:(0.535773873329) A[3]:(0.520622372627)\n",
      " state (4)  A[0]:(0.590415835381) A[1]:(0.655856430531) A[2]:(5.43594360352e-05) A[3]:(0.531317770481)\n",
      " state (5)  A[0]:(0.160584434867) A[1]:(0.928824722767) A[2]:(-0.181591033936) A[3]:(0.514913439751)\n",
      " state (6)  A[0]:(-0.000228524208069) A[1]:(0.809998571873) A[2]:(-5.5193901062e-05) A[3]:(0.65597474575)\n",
      " state (7)  A[0]:(0.63687479496) A[1]:(-0.252654403448) A[2]:(0.261059612036) A[3]:(0.893634915352)\n",
      " state (8)  A[0]:(0.655797123909) A[1]:(-0.000336956232786) A[2]:(0.728797256947) A[3]:(0.59071624279)\n",
      " state (9)  A[0]:(0.655678987503) A[1]:(0.809864997864) A[2]:(0.809992432594) A[3]:(-5.26607036591e-05)\n",
      " state (10)  A[0]:(0.7287504673) A[1]:(0.899981796741) A[2]:(-0.000180959701538) A[3]:(0.7289083004)\n",
      " state (11)  A[0]:(0.517406105995) A[1]:(0.876831173897) A[2]:(-0.589622676373) A[3]:(0.842589795589)\n",
      " state (12)  A[0]:(0.0731389522552) A[1]:(0.82441085577) A[2]:(-0.540243506432) A[3]:(0.792108893394)\n",
      " state (13)  A[0]:(-0.00044602152775) A[1]:(0.809336185455) A[2]:(0.90002399683) A[3]:(0.728965401649)\n",
      " state (14)  A[0]:(0.809796929359) A[1]:(0.90031850338) A[2]:(0.999999880791) A[3]:(0.809972941875)\n",
      " state (15)  A[0]:(0.987436652184) A[1]:(0.960116267204) A[2]:(1.0) A[3]:(0.888140380383)\n",
      "Episode 460000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6037. Times reached goal: 979.               Steps done: 3918258. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0178881371362.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531270742416) A[1]:(0.590223550797) A[2]:(0.590484619141) A[3]:(0.531603336334)\n",
      " state (1)  A[0]:(0.531008243561) A[1]:(-0.000286646187305) A[2]:(0.656141042709) A[3]:(0.590702176094)\n",
      " state (2)  A[0]:(0.589787483215) A[1]:(0.728854894638) A[2]:(0.590544462204) A[3]:(0.656260848045)\n",
      " state (3)  A[0]:(0.654615521431) A[1]:(-0.229339599609) A[2]:(0.535851120949) A[3]:(0.520549714565)\n",
      " state (4)  A[0]:(0.588127493858) A[1]:(0.655947685242) A[2]:(4.73260879517e-05) A[3]:(0.531051754951)\n",
      " state (5)  A[0]:(0.156489178538) A[1]:(0.928816378117) A[2]:(-0.181554153562) A[3]:(0.514364719391)\n",
      " state (6)  A[0]:(-0.00514455093071) A[1]:(0.810077548027) A[2]:(-6.06775283813e-05) A[3]:(0.655190646648)\n",
      " state (7)  A[0]:(0.634056329727) A[1]:(-0.252529710531) A[2]:(0.261417716742) A[3]:(0.893222808838)\n",
      " state (8)  A[0]:(0.653181135654) A[1]:(-0.000159360468388) A[2]:(0.729302048683) A[3]:(0.588818192482)\n",
      " state (9)  A[0]:(0.65297961235) A[1]:(0.809940636158) A[2]:(0.810145378113) A[3]:(-0.00232339976355)\n",
      " state (10)  A[0]:(0.726419627666) A[1]:(0.899973750114) A[2]:(-0.000269889831543) A[3]:(0.727956414223)\n",
      " state (11)  A[0]:(0.513621807098) A[1]:(0.876808166504) A[2]:(-0.589910507202) A[3]:(0.841928243637)\n",
      " state (12)  A[0]:(0.0679668039083) A[1]:(0.824413061142) A[2]:(-0.540686488152) A[3]:(0.791179537773)\n",
      " state (13)  A[0]:(-0.00525988265872) A[1]:(0.809431552887) A[2]:(0.900190889835) A[3]:(0.727739691734)\n",
      " state (14)  A[0]:(0.808510482311) A[1]:(0.900448560715) A[2]:(0.999999880791) A[3]:(0.809050321579)\n",
      " state (15)  A[0]:(0.987335562706) A[1]:(0.960172235966) A[2]:(1.0) A[3]:(0.88746035099)\n",
      "Episode 461000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6048. Times reached goal: 981.               Steps done: 3924306. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0177802761831.\n",
      " state (0)  A[0]:(0.531128168106) A[1]:(0.590539455414) A[2]:(0.590553402901) A[3]:(0.531587958336)\n",
      " state (1)  A[0]:(0.531177282333) A[1]:(-4.22559678555e-05) A[2]:(0.656137406826) A[3]:(0.590644240379)\n",
      " state (2)  A[0]:(0.590290546417) A[1]:(0.729033231735) A[2]:(0.590553104877) A[3]:(0.656311690807)\n",
      " state (3)  A[0]:(0.656064748764) A[1]:(-0.228686645627) A[2]:(0.535865902901) A[3]:(0.521028280258)\n",
      " state (4)  A[0]:(0.59049487114) A[1]:(0.656109154224) A[2]:(8.26120376587e-05) A[3]:(0.531843423843)\n",
      " state (5)  A[0]:(0.16065852344) A[1]:(0.928854107857) A[2]:(-0.181589543819) A[3]:(0.515549957752)\n",
      " state (6)  A[0]:(-8.31484794617e-06) A[1]:(0.809965848923) A[2]:(0.000132918357849) A[3]:(0.656480193138)\n",
      " state (7)  A[0]:(0.637050032616) A[1]:(-0.252763777971) A[2]:(0.261685669422) A[3]:(0.89374256134)\n",
      " state (8)  A[0]:(0.656102895737) A[1]:(2.99550592899e-05) A[2]:(0.728968262672) A[3]:(0.591163218021)\n",
      " state (9)  A[0]:(0.656090021133) A[1]:(0.810005605221) A[2]:(0.809988558292) A[3]:(0.00102210009936)\n",
      " state (10)  A[0]:(0.728994369507) A[1]:(0.900002717972) A[2]:(-8.28504562378e-05) A[3]:(0.729335308075)\n",
      " state (11)  A[0]:(0.517726302147) A[1]:(0.876816749573) A[2]:(-0.58976650238) A[3]:(0.842816233635)\n",
      " state (12)  A[0]:(0.0735411345959) A[1]:(0.824357450008) A[2]:(-0.540790677071) A[3]:(0.79239577055)\n",
      " state (13)  A[0]:(7.15255737305e-07) A[1]:(0.809232771397) A[2]:(0.900087296963) A[3]:(0.729322195053)\n",
      " state (14)  A[0]:(0.810083568096) A[1]:(0.900216400623) A[2]:(0.999999880791) A[3]:(0.810226559639)\n",
      " state (15)  A[0]:(0.987425804138) A[1]:(0.960015356541) A[2]:(1.0) A[3]:(0.888170480728)\n",
      "Episode 462000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6007. Times reached goal: 977.               Steps done: 3930313. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0176737902149.\n",
      " state (0)  A[0]:(0.531004369259) A[1]:(0.59031522274) A[2]:(0.590423464775) A[3]:(0.531329154968)\n",
      " state (1)  A[0]:(0.531045079231) A[1]:(1.95913016796e-05) A[2]:(0.656012654305) A[3]:(0.590403914452)\n",
      " state (2)  A[0]:(0.590180039406) A[1]:(0.728981494904) A[2]:(0.590454220772) A[3]:(0.656044363976)\n",
      " state (3)  A[0]:(0.656319618225) A[1]:(-0.228591695428) A[2]:(0.535933256149) A[3]:(0.520326435566)\n",
      " state (4)  A[0]:(0.59102332592) A[1]:(0.656252384186) A[2]:(0.000109553337097) A[3]:(0.531031966209)\n",
      " state (5)  A[0]:(0.161768659949) A[1]:(0.928936719894) A[2]:(-0.181798994541) A[3]:(0.514770746231)\n",
      " state (6)  A[0]:(0.00185820250772) A[1]:(0.810011327267) A[2]:(2.96831130981e-05) A[3]:(0.655790328979)\n",
      " state (7)  A[0]:(0.638024330139) A[1]:(-0.252647429705) A[2]:(0.261863797903) A[3]:(0.893263459206)\n",
      " state (8)  A[0]:(0.656451165676) A[1]:(0.000637423188891) A[2]:(0.729060411453) A[3]:(0.589391469955)\n",
      " state (9)  A[0]:(0.65612745285) A[1]:(0.810173511505) A[2]:(0.810089945793) A[3]:(-0.00122316123452)\n",
      " state (10)  A[0]:(0.728965997696) A[1]:(0.900075733662) A[2]:(0.000104308128357) A[3]:(0.728403091431)\n",
      " state (11)  A[0]:(0.517670512199) A[1]:(0.876894056797) A[2]:(-0.589857816696) A[3]:(0.842259824276)\n",
      " state (12)  A[0]:(0.0734228044748) A[1]:(0.824449598789) A[2]:(-0.541135966778) A[3]:(0.791679620743)\n",
      " state (13)  A[0]:(-0.000150203704834) A[1]:(0.809310436249) A[2]:(0.900114357471) A[3]:(0.728458762169)\n",
      " state (14)  A[0]:(0.810066461563) A[1]:(0.900240957737) A[2]:(0.999999880791) A[3]:(0.809722006321)\n",
      " state (15)  A[0]:(0.987403869629) A[1]:(0.959995448589) A[2]:(1.0) A[3]:(0.887888729572)\n",
      "Episode 463000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6055. Times reached goal: 986.               Steps done: 3936368. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0175670987495.\n",
      " state (0)  A[0]:(0.531392097473) A[1]:(0.590505838394) A[2]:(0.590412318707) A[3]:(0.531309485435)\n",
      " state (1)  A[0]:(0.531423330307) A[1]:(7.42897391319e-05) A[2]:(0.65600502491) A[3]:(0.590330421925)\n",
      " state (2)  A[0]:(0.590506315231) A[1]:(0.729012131691) A[2]:(0.590433716774) A[3]:(0.655977964401)\n",
      " state (3)  A[0]:(0.656194627285) A[1]:(-0.228969573975) A[2]:(0.535759985447) A[3]:(0.520414710045)\n",
      " state (4)  A[0]:(0.590682029724) A[1]:(0.656093478203) A[2]:(-0.000236868858337) A[3]:(0.53124076128)\n",
      " state (5)  A[0]:(0.161071494222) A[1]:(0.928880810738) A[2]:(-0.182092100382) A[3]:(0.515047192574)\n",
      " state (6)  A[0]:(0.000449091166956) A[1]:(0.810000419617) A[2]:(-0.000255465507507) A[3]:(0.655988097191)\n",
      " state (7)  A[0]:(0.637079834938) A[1]:(-0.252817749977) A[2]:(0.261764734983) A[3]:(0.893419086933)\n",
      " state (8)  A[0]:(0.65606033802) A[1]:(3.22684645653e-05) A[2]:(0.728800654411) A[3]:(0.590298771858)\n",
      " state (9)  A[0]:(0.655946731567) A[1]:(0.810029864311) A[2]:(0.809909582138) A[3]:(-0.000523731054273)\n",
      " state (10)  A[0]:(0.728830039501) A[1]:(0.900012552738) A[2]:(-0.000304818153381) A[3]:(0.728577375412)\n",
      " state (11)  A[0]:(0.517452478409) A[1]:(0.876834154129) A[2]:(-0.590293288231) A[3]:(0.842370569706)\n",
      " state (12)  A[0]:(0.073037661612) A[1]:(0.824399650097) A[2]:(-0.541912317276) A[3]:(0.791821956635)\n",
      " state (13)  A[0]:(-0.000736683490686) A[1]:(0.809290289879) A[2]:(0.899923443794) A[3]:(0.728578567505)\n",
      " state (14)  A[0]:(0.809815764427) A[1]:(0.900245666504) A[2]:(0.999999880791) A[3]:(0.809706151485)\n",
      " state (15)  A[0]:(0.987367808819) A[1]:(0.959992706776) A[2]:(1.0) A[3]:(0.887764632702)\n",
      "Episode 464000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6046. Times reached goal: 985.               Steps done: 3942414. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0174612084993.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5905,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6562, -0.0001,  0.5317]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6562, -0.0004,  0.7291,  0.5907]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.8099,  0.8100,  0.0000]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0004,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9004,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53158134222) A[1]:(0.590575039387) A[2]:(0.590538680553) A[3]:(0.531738638878)\n",
      " state (1)  A[0]:(0.53151845932) A[1]:(-2.41063535213e-05) A[2]:(0.656204342842) A[3]:(0.59072136879)\n",
      " state (2)  A[0]:(0.590611338615) A[1]:(0.729025006294) A[2]:(0.590536355972) A[3]:(0.656337916851)\n",
      " state (3)  A[0]:(0.656129539013) A[1]:(-0.22822701931) A[2]:(0.535780191422) A[3]:(0.521021783352)\n",
      " state (4)  A[0]:(0.590542316437) A[1]:(0.656203687191) A[2]:(-2.15768814087e-05) A[3]:(0.531790971756)\n",
      " state (5)  A[0]:(0.160861805081) A[1]:(0.928866922855) A[2]:(-0.18179127574) A[3]:(0.515594244003)\n",
      " state (6)  A[0]:(6.06179237366e-05) A[1]:(0.809998512268) A[2]:(9.1552734375e-05) A[3]:(0.656336545944)\n",
      " state (7)  A[0]:(0.636997938156) A[1]:(-0.253078341484) A[2]:(0.262303084135) A[3]:(0.893539071083)\n",
      " state (8)  A[0]:(0.656210780144) A[1]:(-0.000524189264979) A[2]:(0.729021668434) A[3]:(0.590767502785)\n",
      " state (9)  A[0]:(0.656115472317) A[1]:(0.809909045696) A[2]:(0.810011863708) A[3]:(0.000107705593109)\n",
      " state (10)  A[0]:(0.729055643082) A[1]:(0.899978756905) A[2]:(-0.000299572944641) A[3]:(0.729034304619)\n",
      " state (11)  A[0]:(0.517911314964) A[1]:(0.876828968525) A[2]:(-0.590540528297) A[3]:(0.842722415924)\n",
      " state (12)  A[0]:(0.0736679956317) A[1]:(0.824449717999) A[2]:(-0.542335152626) A[3]:(0.79228323698)\n",
      " state (13)  A[0]:(-0.000161021947861) A[1]:(0.809427320957) A[2]:(0.900047600269) A[3]:(0.729114770889)\n",
      " state (14)  A[0]:(0.809960603714) A[1]:(0.900381207466) A[2]:(0.999999880791) A[3]:(0.81003010273)\n",
      " state (15)  A[0]:(0.987338364124) A[1]:(0.960044145584) A[2]:(1.0) A[3]:(0.887815713882)\n",
      "Episode 465000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6035. Times reached goal: 982.               Steps done: 3948449. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0173561474466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53125500679) A[1]:(0.590419769287) A[2]:(0.590461373329) A[3]:(0.531488537788)\n",
      " state (1)  A[0]:(0.5311383605) A[1]:(-4.20734286308e-05) A[2]:(0.656091928482) A[3]:(0.590604066849)\n",
      " state (2)  A[0]:(0.590177536011) A[1]:(0.728939712048) A[2]:(0.590616464615) A[3]:(0.65622150898)\n",
      " state (3)  A[0]:(0.655917048454) A[1]:(-0.229027241468) A[2]:(0.536123514175) A[3]:(0.520750403404)\n",
      " state (4)  A[0]:(0.5902967453) A[1]:(0.656082987785) A[2]:(0.000239849090576) A[3]:(0.531563401222)\n",
      " state (5)  A[0]:(0.160402357578) A[1]:(0.928871452808) A[2]:(-0.18178653717) A[3]:(0.515417575836)\n",
      " state (6)  A[0]:(-3.57627868652e-05) A[1]:(0.809962928295) A[2]:(0.000127792358398) A[3]:(0.656293690205)\n",
      " state (7)  A[0]:(0.637025296688) A[1]:(-0.252789050341) A[2]:(0.262512117624) A[3]:(0.893505275249)\n",
      " state (8)  A[0]:(0.656287193298) A[1]:(0.000242091715336) A[2]:(0.729041934013) A[3]:(0.590756773949)\n",
      " state (9)  A[0]:(0.656363606453) A[1]:(0.810079574585) A[2]:(0.810002207756) A[3]:(0.000467598409159)\n",
      " state (10)  A[0]:(0.72926056385) A[1]:(0.900023341179) A[2]:(-0.000157713890076) A[3]:(0.729116141796)\n",
      " state (11)  A[0]:(0.518270730972) A[1]:(0.876839458942) A[2]:(-0.590534687042) A[3]:(0.842725634575)\n",
      " state (12)  A[0]:(0.0742387026548) A[1]:(0.82439494133) A[2]:(-0.542583703995) A[3]:(0.792272627354)\n",
      " state (13)  A[0]:(0.000509232224431) A[1]:(0.809263110161) A[2]:(0.900024652481) A[3]:(0.729119896889)\n",
      " state (14)  A[0]:(0.81030356884) A[1]:(0.900213479996) A[2]:(0.999999880791) A[3]:(0.810096502304)\n",
      " state (15)  A[0]:(0.987355947495) A[1]:(0.95992398262) A[2]:(1.0) A[3]:(0.887859880924)\n",
      "Episode 466000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6055. Times reached goal: 983.               Steps done: 3954504. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.017251373497.\n",
      " state (0)  A[0]:(0.53170633316) A[1]:(0.590489327908) A[2]:(0.59055685997) A[3]:(0.530872941017)\n",
      " state (1)  A[0]:(0.531670570374) A[1]:(-0.000107776373625) A[2]:(0.656042098999) A[3]:(0.590147018433)\n",
      " state (2)  A[0]:(0.590581595898) A[1]:(0.728958666325) A[2]:(0.590474009514) A[3]:(0.655881404877)\n",
      " state (3)  A[0]:(0.655924081802) A[1]:(-0.228778511286) A[2]:(0.5358928442) A[3]:(0.520481228828)\n",
      " state (4)  A[0]:(0.590002477169) A[1]:(0.656130313873) A[2]:(-2.77757644653e-05) A[3]:(0.531386256218)\n",
      " state (5)  A[0]:(0.159676730633) A[1]:(0.92886865139) A[2]:(-0.181973963976) A[3]:(0.515248537064)\n",
      " state (6)  A[0]:(-0.00101476872806) A[1]:(0.809944093227) A[2]:(8.5711479187e-05) A[3]:(0.655934691429)\n",
      " state (7)  A[0]:(0.636296987534) A[1]:(-0.252934575081) A[2]:(0.262735635042) A[3]:(0.893192231655)\n",
      " state (8)  A[0]:(0.655550479889) A[1]:(-0.000192981213331) A[2]:(0.729022264481) A[3]:(0.589690566063)\n",
      " state (9)  A[0]:(0.655537724495) A[1]:(0.80993616581) A[2]:(0.810019254684) A[3]:(-0.0016024694778)\n",
      " state (10)  A[0]:(0.72863304615) A[1]:(0.899992525578) A[2]:(-0.000352263421519) A[3]:(0.728227734566)\n",
      " state (11)  A[0]:(0.517364621162) A[1]:(0.876845240593) A[2]:(-0.590950131416) A[3]:(0.842251300812)\n",
      " state (12)  A[0]:(0.0729667544365) A[1]:(0.824445068836) A[2]:(-0.543210148811) A[3]:(0.791687548161)\n",
      " state (13)  A[0]:(-0.00084429961862) A[1]:(0.809355676174) A[2]:(0.900066137314) A[3]:(0.728410840034)\n",
      " state (14)  A[0]:(0.809804201126) A[1]:(0.900280296803) A[2]:(0.999999880791) A[3]:(0.809652149677)\n",
      " state (15)  A[0]:(0.987285375595) A[1]:(0.959928810596) A[2]:(1.0) A[3]:(0.88755518198)\n",
      "Episode 467000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6030. Times reached goal: 978.               Steps done: 3960534. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0171476607231.\n",
      " state (0)  A[0]:(0.531428873539) A[1]:(0.590418577194) A[2]:(0.590472340584) A[3]:(0.531503915787)\n",
      " state (1)  A[0]:(0.531423568726) A[1]:(-5.17815351486e-06) A[2]:(0.656076073647) A[3]:(0.590565085411)\n",
      " state (2)  A[0]:(0.590480744839) A[1]:(0.728960156441) A[2]:(0.590594112873) A[3]:(0.656202554703)\n",
      " state (3)  A[0]:(0.656082749367) A[1]:(-0.228900462389) A[2]:(0.536015033722) A[3]:(0.520873427391)\n",
      " state (4)  A[0]:(0.590506076813) A[1]:(0.656008839607) A[2]:(1.9907951355e-05) A[3]:(0.531790733337)\n",
      " state (5)  A[0]:(0.160815268755) A[1]:(0.928863406181) A[2]:(-0.182096719742) A[3]:(0.515753269196)\n",
      " state (6)  A[0]:(0.000227332115173) A[1]:(0.809972643852) A[2]:(1.01327896118e-05) A[3]:(0.65639936924)\n",
      " state (7)  A[0]:(0.636937975883) A[1]:(-0.252841472626) A[2]:(0.262905865908) A[3]:(0.893432676792)\n",
      " state (8)  A[0]:(0.656347692013) A[1]:(-0.000127427279949) A[2]:(0.72889816761) A[3]:(0.591224551201)\n",
      " state (9)  A[0]:(0.65637665987) A[1]:(0.809950709343) A[2]:(0.809940576553) A[3]:(0.00125752319582)\n",
      " state (10)  A[0]:(0.729278802872) A[1]:(0.899982213974) A[2]:(-0.000253438949585) A[3]:(0.729506075382)\n",
      " state (11)  A[0]:(0.518399178982) A[1]:(0.876820087433) A[2]:(-0.590975522995) A[3]:(0.843016445637)\n",
      " state (12)  A[0]:(0.0743654966354) A[1]:(0.824396371841) A[2]:(-0.543614387512) A[3]:(0.792636811733)\n",
      " state (13)  A[0]:(0.00032576918602) A[1]:(0.809266388416) A[2]:(0.899868428707) A[3]:(0.729499101639)\n",
      " state (14)  A[0]:(0.810107707977) A[1]:(0.900200068951) A[2]:(0.999999880791) A[3]:(0.810315728188)\n",
      " state (15)  A[0]:(0.987293958664) A[1]:(0.959877192974) A[2]:(1.0) A[3]:(0.887854874134)\n",
      "Episode 468000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6046. Times reached goal: 983.               Steps done: 3966580. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0170442987445.\n",
      " state (0)  A[0]:(0.531254470348) A[1]:(0.59010130167) A[2]:(0.590340018272) A[3]:(0.531371831894)\n",
      " state (1)  A[0]:(0.53123652935) A[1]:(-3.31848859787e-05) A[2]:(0.655830621719) A[3]:(0.590345859528)\n",
      " state (2)  A[0]:(0.590291380882) A[1]:(0.728805303574) A[2]:(0.590050935745) A[3]:(0.655980348587)\n",
      " state (3)  A[0]:(0.655805587769) A[1]:(-0.22800797224) A[2]:(0.535694360733) A[3]:(0.520613372326)\n",
      " state (4)  A[0]:(0.590140223503) A[1]:(0.655785739422) A[2]:(5.59091567993e-05) A[3]:(0.531354725361)\n",
      " state (5)  A[0]:(0.160404145718) A[1]:(0.928843796253) A[2]:(-0.182153314352) A[3]:(0.515297651291)\n",
      " state (6)  A[0]:(-3.57031822205e-05) A[1]:(0.809908330441) A[2]:(3.55243682861e-05) A[3]:(0.655893027782)\n",
      " state (7)  A[0]:(0.636528849602) A[1]:(-0.252944856882) A[2]:(0.263077437878) A[3]:(0.89308577776)\n",
      " state (8)  A[0]:(0.655797481537) A[1]:(-5.57750463486e-05) A[2]:(0.728791058064) A[3]:(0.590286195278)\n",
      " state (9)  A[0]:(0.655747056007) A[1]:(0.809973120689) A[2]:(0.809887886047) A[3]:(-4.24534082413e-05)\n",
      " state (10)  A[0]:(0.728760063648) A[1]:(0.900003254414) A[2]:(-0.000242352485657) A[3]:(0.728834748268)\n",
      " state (11)  A[0]:(0.517695963383) A[1]:(0.876858651638) A[2]:(-0.591073572636) A[3]:(0.842588186264)\n",
      " state (12)  A[0]:(0.0735603123903) A[1]:(0.824467122555) A[2]:(-0.543830156326) A[3]:(0.792073607445)\n",
      " state (13)  A[0]:(-0.000285059213638) A[1]:(0.809361457825) A[2]:(0.900021791458) A[3]:(0.728810071945)\n",
      " state (14)  A[0]:(0.80999237299) A[1]:(0.90026062727) A[2]:(0.999999880791) A[3]:(0.809892773628)\n",
      " state (15)  A[0]:(0.987261176109) A[1]:(0.959876418114) A[2]:(1.0) A[3]:(0.887569308281)\n",
      "Episode 469000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6050. Times reached goal: 984.               Steps done: 3972630. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.016941492041.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5906,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6563,  0.0002,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.0003,  0.7290,  0.5909]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.8100,  0.8100,  0.0007]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8091,  0.9000,  0.7291]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9000,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53153949976) A[1]:(0.590683877468) A[2]:(0.590582549572) A[3]:(0.53147149086)\n",
      " state (1)  A[0]:(0.531547546387) A[1]:(-3.81097197533e-05) A[2]:(0.656146943569) A[3]:(0.590412735939)\n",
      " state (2)  A[0]:(0.590605974197) A[1]:(0.729116499424) A[2]:(0.590475738049) A[3]:(0.656050205231)\n",
      " state (3)  A[0]:(0.656322002411) A[1]:(-0.226474717259) A[2]:(0.535779833794) A[3]:(0.520693421364)\n",
      " state (4)  A[0]:(0.590629637241) A[1]:(0.656361281872) A[2]:(0.000106930732727) A[3]:(0.531391978264)\n",
      " state (5)  A[0]:(0.160727903247) A[1]:(0.928918719292) A[2]:(-0.182114809752) A[3]:(0.515397071838)\n",
      " state (6)  A[0]:(0.000103622674942) A[1]:(0.810061633587) A[2]:(7.23600387573e-05) A[3]:(0.656081676483)\n",
      " state (7)  A[0]:(0.636807560921) A[1]:(-0.252458333969) A[2]:(0.263355016708) A[3]:(0.89320743084)\n",
      " state (8)  A[0]:(0.656188607216) A[1]:(0.000538147927728) A[2]:(0.72906255722) A[3]:(0.590416073799)\n",
      " state (9)  A[0]:(0.656172931194) A[1]:(0.81016600132) A[2]:(0.810074687004) A[3]:(-0.000119388103485)\n",
      " state (10)  A[0]:(0.729060947895) A[1]:(0.900068759918) A[2]:(1.82390213013e-05) A[3]:(0.728844285011)\n",
      " state (11)  A[0]:(0.518060803413) A[1]:(0.876883387566) A[2]:(-0.591174066067) A[3]:(0.842616021633)\n",
      " state (12)  A[0]:(0.0738880485296) A[1]:(0.824416637421) A[2]:(-0.544238567352) A[3]:(0.792126476765)\n",
      " state (13)  A[0]:(-0.000107854604721) A[1]:(0.809198617935) A[2]:(0.899998903275) A[3]:(0.72889226675)\n",
      " state (14)  A[0]:(0.810048162937) A[1]:(0.900096178055) A[2]:(0.999999880791) A[3]:(0.80996710062)\n",
      " state (15)  A[0]:(0.987245082855) A[1]:(0.959756910801) A[2]:(1.0) A[3]:(0.887570679188)\n",
      "Episode 470000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6039. Times reached goal: 982.               Steps done: 3978669. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0168394906737.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531774640083) A[1]:(0.590619206429) A[2]:(0.590412259102) A[3]:(0.532244324684)\n",
      " state (1)  A[0]:(0.531736969948) A[1]:(4.60781157017e-05) A[2]:(0.656141102314) A[3]:(0.591712415218)\n",
      " state (2)  A[0]:(0.590772390366) A[1]:(0.728957772255) A[2]:(0.59057867527) A[3]:(0.657498896122)\n",
      " state (3)  A[0]:(0.656389892101) A[1]:(-0.228251159191) A[2]:(0.535954773426) A[3]:(0.523092269897)\n",
      " state (4)  A[0]:(0.59072637558) A[1]:(0.656140804291) A[2]:(6.79492950439e-06) A[3]:(0.534289240837)\n",
      " state (5)  A[0]:(0.160862118006) A[1]:(0.92885774374) A[2]:(-0.182188913226) A[3]:(0.518556952477)\n",
      " state (6)  A[0]:(1.63912773132e-06) A[1]:(0.809945642948) A[2]:(2.56299972534e-05) A[3]:(0.658511936665)\n",
      " state (7)  A[0]:(0.636855602264) A[1]:(-0.253038078547) A[2]:(0.263330161572) A[3]:(0.894112288952)\n",
      " state (8)  A[0]:(0.656654834747) A[1]:(-0.0005419216468) A[2]:(0.728599727154) A[3]:(0.594075679779)\n",
      " state (9)  A[0]:(0.656557559967) A[1]:(0.809796094894) A[2]:(0.809755146503) A[3]:(0.00493355561048)\n",
      " state (10)  A[0]:(0.729203224182) A[1]:(0.899859249592) A[2]:(-0.000472903222544) A[3]:(0.730845868587)\n",
      " state (11)  A[0]:(0.518169760704) A[1]:(0.876635015011) A[2]:(-0.591618359089) A[3]:(0.843774616718)\n",
      " state (12)  A[0]:(0.0738143920898) A[1]:(0.824101746082) A[2]:(-0.545295119286) A[3]:(0.793540835381)\n",
      " state (13)  A[0]:(-0.000904112821445) A[1]:(0.80887311697) A[2]:(0.899400055408) A[3]:(0.730474233627)\n",
      " state (14)  A[0]:(0.809386074543) A[1]:(0.89992249012) A[2]:(0.999999880791) A[3]:(0.810875236988)\n",
      " state (15)  A[0]:(0.987180292606) A[1]:(0.959703743458) A[2]:(1.0) A[3]:(0.887997329235)\n",
      "Episode 471000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6054. Times reached goal: 986.               Steps done: 3984723. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0167378523667.\n",
      " state (0)  A[0]:(0.531117796898) A[1]:(0.590538918972) A[2]:(0.590489029884) A[3]:(0.531141519547)\n",
      " state (1)  A[0]:(0.531374633312) A[1]:(6.52745366096e-05) A[2]:(0.656104028225) A[3]:(0.590235829353)\n",
      " state (2)  A[0]:(0.590502202511) A[1]:(0.729057610035) A[2]:(0.590482354164) A[3]:(0.655892252922)\n",
      " state (3)  A[0]:(0.656197011471) A[1]:(-0.227469429374) A[2]:(0.535726428032) A[3]:(0.520225048065)\n",
      " state (4)  A[0]:(0.590466380119) A[1]:(0.656294822693) A[2]:(-0.000274658203125) A[3]:(0.530974268913)\n",
      " state (5)  A[0]:(0.160407438874) A[1]:(0.928916156292) A[2]:(-0.182518988848) A[3]:(0.515024781227)\n",
      " state (6)  A[0]:(-0.000609993876424) A[1]:(0.810105383396) A[2]:(-0.000123262405396) A[3]:(0.655599355698)\n",
      " state (7)  A[0]:(0.636101424694) A[1]:(-0.252464652061) A[2]:(0.263717204332) A[3]:(0.892913401127)\n",
      " state (8)  A[0]:(0.655676960945) A[1]:(0.000306285917759) A[2]:(0.72898042202) A[3]:(0.589923858643)\n",
      " state (9)  A[0]:(0.655668854713) A[1]:(0.810132443905) A[2]:(0.809976100922) A[3]:(-0.000843763176817)\n",
      " state (10)  A[0]:(0.728670835495) A[1]:(0.900097131729) A[2]:(-0.000295877456665) A[3]:(0.728487372398)\n",
      " state (11)  A[0]:(0.517546415329) A[1]:(0.876981496811) A[2]:(-0.59173643589) A[3]:(0.842407107353)\n",
      " state (12)  A[0]:(0.0731621906161) A[1]:(0.824645340443) A[2]:(-0.54536396265) A[3]:(0.791824221611)\n",
      " state (13)  A[0]:(-0.00109577132389) A[1]:(0.809539318085) A[2]:(0.899830818176) A[3]:(0.728428065777)\n",
      " state (14)  A[0]:(0.809586822987) A[1]:(0.900335133076) A[2]:(0.999999880791) A[3]:(0.809577703476)\n",
      " state (15)  A[0]:(0.987163186073) A[1]:(0.959844887257) A[2]:(1.0) A[3]:(0.887197136879)\n",
      "Episode 472000 finished after 0 timesteps with r=0.0. Running score: 0.99. Times trained:               6038. Times reached goal: 985.               Steps done: 3990761. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0166370937106.\n",
      " state (0)  A[0]:(0.531388401985) A[1]:(0.590458333492) A[2]:(0.590428829193) A[3]:(0.531465530396)\n",
      " state (1)  A[0]:(0.531320095062) A[1]:(2.53468751907e-05) A[2]:(0.656066656113) A[3]:(0.590475976467)\n",
      " state (2)  A[0]:(0.590390145779) A[1]:(0.729035198689) A[2]:(0.590460300446) A[3]:(0.656122922897)\n",
      " state (3)  A[0]:(0.655973672867) A[1]:(-0.226766526699) A[2]:(0.535843074322) A[3]:(0.520832300186)\n",
      " state (4)  A[0]:(0.590355813503) A[1]:(0.655934512615) A[2]:(0.000142812728882) A[3]:(0.531567454338)\n",
      " state (5)  A[0]:(0.160659775138) A[1]:(0.928868293762) A[2]:(-0.182375848293) A[3]:(0.515703618526)\n",
      " state (6)  A[0]:(-0.000250816345215) A[1]:(0.810051262379) A[2]:(-7.15255737305e-05) A[3]:(0.656085550785)\n",
      " state (7)  A[0]:(0.63614988327) A[1]:(-0.252590656281) A[2]:(0.263854175806) A[3]:(0.893050014973)\n",
      " state (8)  A[0]:(0.655761122704) A[1]:(-6.46263360977e-05) A[2]:(0.728916049004) A[3]:(0.59064656496)\n",
      " state (9)  A[0]:(0.655749797821) A[1]:(0.809953987598) A[2]:(0.810035467148) A[3]:(0.000117212533951)\n",
      " state (10)  A[0]:(0.728759646416) A[1]:(0.899980247021) A[2]:(-8.76188278198e-05) A[3]:(0.72903740406)\n",
      " state (11)  A[0]:(0.51773673296) A[1]:(0.876826763153) A[2]:(-0.591818094254) A[3]:(0.842827022076)\n",
      " state (12)  A[0]:(0.0734249129891) A[1]:(0.824427783489) A[2]:(-0.545648694038) A[3]:(0.792399942875)\n",
      " state (13)  A[0]:(-0.000754743698053) A[1]:(0.809320449829) A[2]:(0.899941921234) A[3]:(0.729137420654)\n",
      " state (14)  A[0]:(0.80982375145) A[1]:(0.900234818459) A[2]:(0.999999880791) A[3]:(0.810042500496)\n",
      " state (15)  A[0]:(0.987159729004) A[1]:(0.959786057472) A[2]:(1.0) A[3]:(0.887356638908)\n",
      "Episode 473000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6001. Times reached goal: 978.               Steps done: 3996762. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0165375534804.\n",
      " state (0)  A[0]:(0.531345486641) A[1]:(0.590206623077) A[2]:(0.590402364731) A[3]:(0.531542181969)\n",
      " state (1)  A[0]:(0.531370818615) A[1]:(-6.39632344246e-06) A[2]:(0.656026482582) A[3]:(0.59057533741)\n",
      " state (2)  A[0]:(0.590455532074) A[1]:(0.728919744492) A[2]:(0.59044277668) A[3]:(0.656228542328)\n",
      " state (3)  A[0]:(0.656058192253) A[1]:(-0.227733671665) A[2]:(0.535935163498) A[3]:(0.52079975605)\n",
      " state (4)  A[0]:(0.590439081192) A[1]:(0.656033873558) A[2]:(5.50746917725e-05) A[3]:(0.531676411629)\n",
      " state (5)  A[0]:(0.160784527659) A[1]:(0.928887009621) A[2]:(-0.182436361909) A[3]:(0.515898942947)\n",
      " state (6)  A[0]:(0.000231385231018) A[1]:(0.81000739336) A[2]:(2.92062759399e-05) A[3]:(0.656158685684)\n",
      " state (7)  A[0]:(0.636618733406) A[1]:(-0.252748250961) A[2]:(0.264208287001) A[3]:(0.892947673798)\n",
      " state (8)  A[0]:(0.656187295914) A[1]:(7.189437747e-05) A[2]:(0.728990852833) A[3]:(0.59038579464)\n",
      " state (9)  A[0]:(0.656089365482) A[1]:(0.810018539429) A[2]:(0.810022652149) A[3]:(0.000140383839607)\n",
      " state (10)  A[0]:(0.729022979736) A[1]:(0.900009870529) A[2]:(-9.03606414795e-05) A[3]:(0.72903907299)\n",
      " state (11)  A[0]:(0.518231630325) A[1]:(0.876866936684) A[2]:(-0.591939926147) A[3]:(0.842804074287)\n",
      " state (12)  A[0]:(0.0742007941008) A[1]:(0.824494242668) A[2]:(-0.545967519283) A[3]:(0.792348384857)\n",
      " state (13)  A[0]:(5.13195991516e-05) A[1]:(0.809398055077) A[2]:(0.89999806881) A[3]:(0.729055166245)\n",
      " state (14)  A[0]:(0.810076713562) A[1]:(0.900272846222) A[2]:(0.999999880791) A[3]:(0.809995234013)\n",
      " state (15)  A[0]:(0.98714953661) A[1]:(0.959775805473) A[2]:(1.0) A[3]:(0.887266874313)\n",
      "Episode 474000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6013. Times reached goal: 982.               Steps done: 4002775. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0164384115403.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5904,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315, -0.0001,  0.6562,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7291,  0.5907,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8100, -0.0002,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 7.2913e-01,  9.0002e-01, -2.3842e-07,  7.2885e-01]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531711041927) A[1]:(0.590474784374) A[2]:(0.590547382832) A[3]:(0.53143119812)\n",
      " state (1)  A[0]:(0.531796813011) A[1]:(-1.98557972908e-06) A[2]:(0.656177043915) A[3]:(0.590532422066)\n",
      " state (2)  A[0]:(0.59084546566) A[1]:(0.729037880898) A[2]:(0.59066927433) A[3]:(0.656191468239)\n",
      " state (3)  A[0]:(0.656412899494) A[1]:(-0.227400124073) A[2]:(0.535984694958) A[3]:(0.52071750164)\n",
      " state (4)  A[0]:(0.590775132179) A[1]:(0.656192302704) A[2]:(-3.51667404175e-05) A[3]:(0.531594514847)\n",
      " state (5)  A[0]:(0.161119177938) A[1]:(0.928900420666) A[2]:(-0.182588353753) A[3]:(0.51587677002)\n",
      " state (6)  A[0]:(0.000430554122431) A[1]:(0.810029625893) A[2]:(-9.19103622437e-05) A[3]:(0.656225085258)\n",
      " state (7)  A[0]:(0.636745095253) A[1]:(-0.252684175968) A[2]:(0.264300316572) A[3]:(0.893021821976)\n",
      " state (8)  A[0]:(0.656428098679) A[1]:(0.000125925987959) A[2]:(0.728937804699) A[3]:(0.590700149536)\n",
      " state (9)  A[0]:(0.656331777573) A[1]:(0.810036659241) A[2]:(0.809967458248) A[3]:(0.000243365764618)\n",
      " state (10)  A[0]:(0.729189991951) A[1]:(0.900005757809) A[2]:(-0.000275492668152) A[3]:(0.729013562202)\n",
      " state (11)  A[0]:(0.518478155136) A[1]:(0.876847028732) A[2]:(-0.592239022255) A[3]:(0.842788279057)\n",
      " state (12)  A[0]:(0.0744803398848) A[1]:(0.824447631836) A[2]:(-0.546500444412) A[3]:(0.79233288765)\n",
      " state (13)  A[0]:(0.000222206115723) A[1]:(0.809323728085) A[2]:(0.900004684925) A[3]:(0.729058265686)\n",
      " state (14)  A[0]:(0.810076355934) A[1]:(0.900212168694) A[2]:(0.999999880791) A[3]:(0.81005692482)\n",
      " state (15)  A[0]:(0.987117767334) A[1]:(0.959717750549) A[2]:(1.0) A[3]:(0.887273728848)\n",
      "Episode 475000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6064. Times reached goal: 989.               Steps done: 4008839. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0163390306402.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.529953241348) A[1]:(0.590702056885) A[2]:(0.590693533421) A[3]:(0.531693160534)\n",
      " state (1)  A[0]:(0.529938697815) A[1]:(0.000158842653036) A[2]:(0.656152427197) A[3]:(0.59083199501)\n",
      " state (2)  A[0]:(0.589162707329) A[1]:(0.729220747948) A[2]:(0.590318322182) A[3]:(0.656530082226)\n",
      " state (3)  A[0]:(0.654407858849) A[1]:(-0.225840702653) A[2]:(0.535499095917) A[3]:(0.521262049675)\n",
      " state (4)  A[0]:(0.588386595249) A[1]:(0.656772375107) A[2]:(-0.000558137835469) A[3]:(0.532076239586)\n",
      " state (5)  A[0]:(0.157895445824) A[1]:(0.928994953632) A[2]:(-0.183078408241) A[3]:(0.516353845596)\n",
      " state (6)  A[0]:(-0.00224428996444) A[1]:(0.810181379318) A[2]:(-0.000474214524729) A[3]:(0.656426072121)\n",
      " state (7)  A[0]:(0.635737776756) A[1]:(-0.252597093582) A[2]:(0.264520078897) A[3]:(0.892935216427)\n",
      " state (8)  A[0]:(0.655534863472) A[1]:(3.24212014675e-05) A[2]:(0.729509592056) A[3]:(0.590152800083)\n",
      " state (9)  A[0]:(0.655510902405) A[1]:(0.809917330742) A[2]:(0.810394704342) A[3]:(0.000807732169051)\n",
      " state (10)  A[0]:(0.728591263294) A[1]:(0.899901509285) A[2]:(0.000405669183237) A[3]:(0.729908525944)\n",
      " state (11)  A[0]:(0.517574310303) A[1]:(0.876694560051) A[2]:(-0.592113494873) A[3]:(0.84348154068)\n",
      " state (12)  A[0]:(0.0732501372695) A[1]:(0.824208498001) A[2]:(-0.546573519707) A[3]:(0.793264687061)\n",
      " state (13)  A[0]:(-0.000787883822341) A[1]:(0.809046506882) A[2]:(0.900197267532) A[3]:(0.730238080025)\n",
      " state (14)  A[0]:(0.810001790524) A[1]:(0.900053203106) A[2]:(0.999999880791) A[3]:(0.810929059982)\n",
      " state (15)  A[0]:(0.987101614475) A[1]:(0.959619402885) A[2]:(1.0) A[3]:(0.887727558613)\n",
      "Episode 476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6031. Times reached goal: 982.               Steps done: 4014870. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0162407864994.\n",
      " state (0)  A[0]:(0.531700849533) A[1]:(0.590539515018) A[2]:(0.590593755245) A[3]:(0.531268239021)\n",
      " state (1)  A[0]:(0.531757891178) A[1]:(3.80724668503e-05) A[2]:(0.656220436096) A[3]:(0.590343594551)\n",
      " state (2)  A[0]:(0.590779542923) A[1]:(0.729225993156) A[2]:(0.590475678444) A[3]:(0.656101405621)\n",
      " state (3)  A[0]:(0.656370639801) A[1]:(-0.223930880427) A[2]:(0.535581052303) A[3]:(0.520672798157)\n",
      " state (4)  A[0]:(0.590681910515) A[1]:(0.656165361404) A[2]:(0.000195384025574) A[3]:(0.531290531158)\n",
      " state (5)  A[0]:(0.161129310727) A[1]:(0.928924798965) A[2]:(-0.182493746281) A[3]:(0.515767455101)\n",
      " state (6)  A[0]:(0.00040957328747) A[1]:(0.810168802738) A[2]:(0.000165104866028) A[3]:(0.656063437462)\n",
      " state (7)  A[0]:(0.636288285255) A[1]:(-0.252006232738) A[2]:(0.264975786209) A[3]:(0.892800211906)\n",
      " state (8)  A[0]:(0.655945181847) A[1]:(0.000813856546301) A[2]:(0.729099035263) A[3]:(0.590368032455)\n",
      " state (9)  A[0]:(0.655928373337) A[1]:(0.810207784176) A[2]:(0.810138523579) A[3]:(5.02169132233e-06)\n",
      " state (10)  A[0]:(0.728962361813) A[1]:(0.900114059448) A[2]:(0.00018048286438) A[3]:(0.728981614113)\n",
      " state (11)  A[0]:(0.518280804157) A[1]:(0.876998543739) A[2]:(-0.592290580273) A[3]:(0.842811405659)\n",
      " state (12)  A[0]:(0.0742711275816) A[1]:(0.824661970139) A[2]:(-0.546896576881) A[3]:(0.792348861694)\n",
      " state (13)  A[0]:(0.000115513801575) A[1]:(0.809529781342) A[2]:(0.900248229504) A[3]:(0.729049026966)\n",
      " state (14)  A[0]:(0.810187101364) A[1]:(0.900287806988) A[2]:(0.999999880791) A[3]:(0.810076951981)\n",
      " state (15)  A[0]:(0.987079501152) A[1]:(0.959676682949) A[2]:(1.0) A[3]:(0.88715326786)\n",
      "Episode 477000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6060. Times reached goal: 984.               Steps done: 4020930. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0161426649418.\n",
      " state (0)  A[0]:(0.531473577023) A[1]:(0.590701937675) A[2]:(0.590616285801) A[3]:(0.531508803368)\n",
      " state (1)  A[0]:(0.531443536282) A[1]:(6.12661242485e-05) A[2]:(0.656122744083) A[3]:(0.590560972691)\n",
      " state (2)  A[0]:(0.59054672718) A[1]:(0.729078769684) A[2]:(0.590469419956) A[3]:(0.656204223633)\n",
      " state (3)  A[0]:(0.656118094921) A[1]:(-0.226441442966) A[2]:(0.535891294479) A[3]:(0.520677685738)\n",
      " state (4)  A[0]:(0.590477466583) A[1]:(0.656221151352) A[2]:(6.50882720947e-05) A[3]:(0.531481742859)\n",
      " state (5)  A[0]:(0.160776972771) A[1]:(0.928896069527) A[2]:(-0.182623386383) A[3]:(0.515904963017)\n",
      " state (6)  A[0]:(-2.65538692474e-05) A[1]:(0.8100284338) A[2]:(3.12328338623e-05) A[3]:(0.656175374985)\n",
      " state (7)  A[0]:(0.636261820793) A[1]:(-0.252601891756) A[2]:(0.264978796244) A[3]:(0.892896950245)\n",
      " state (8)  A[0]:(0.656151235104) A[1]:(0.000344447791576) A[2]:(0.729007363319) A[3]:(0.590643167496)\n",
      " state (9)  A[0]:(0.656092643738) A[1]:(0.810114264488) A[2]:(0.809999346733) A[3]:(0.000118911266327)\n",
      " state (10)  A[0]:(0.728952646255) A[1]:(0.900013923645) A[2]:(-9.62018966675e-05) A[3]:(0.728958547115)\n",
      " state (11)  A[0]:(0.518142998219) A[1]:(0.876831293106) A[2]:(-0.592581510544) A[3]:(0.842790007591)\n",
      " state (12)  A[0]:(0.0739730522037) A[1]:(0.824395239353) A[2]:(-0.547571837902) A[3]:(0.792346596718)\n",
      " state (13)  A[0]:(-0.000363200873835) A[1]:(0.809209048748) A[2]:(0.900011479855) A[3]:(0.729033589363)\n",
      " state (14)  A[0]:(0.810018002987) A[1]:(0.900095880032) A[2]:(0.999999880791) A[3]:(0.809993505478)\n",
      " state (15)  A[0]:(0.98705881834) A[1]:(0.959585607052) A[2]:(1.0) A[3]:(0.8870216012)\n",
      "Episode 478000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6054. Times reached goal: 986.               Steps done: 4026984. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0160452324739.\n",
      " state (0)  A[0]:(0.531054615974) A[1]:(0.59061384201) A[2]:(0.590502500534) A[3]:(0.531447410583)\n",
      " state (1)  A[0]:(0.530643701553) A[1]:(3.26931476593e-05) A[2]:(0.656159639359) A[3]:(0.591002702713)\n",
      " state (2)  A[0]:(0.589691281319) A[1]:(0.72910296917) A[2]:(0.590461432934) A[3]:(0.65679693222)\n",
      " state (3)  A[0]:(0.655550420284) A[1]:(-0.225974097848) A[2]:(0.535583972931) A[3]:(0.521698534489)\n",
      " state (4)  A[0]:(0.589836061001) A[1]:(0.656181931496) A[2]:(-0.00042629239033) A[3]:(0.53255045414)\n",
      " state (5)  A[0]:(0.15972417593) A[1]:(0.928867340088) A[2]:(-0.183126792312) A[3]:(0.517005801201)\n",
      " state (6)  A[0]:(-0.00125360419042) A[1]:(0.80997300148) A[2]:(-0.000394582719309) A[3]:(0.656926393509)\n",
      " state (7)  A[0]:(0.63554251194) A[1]:(-0.252806931734) A[2]:(0.264931797981) A[3]:(0.893102586269)\n",
      " state (8)  A[0]:(0.65570306778) A[1]:(3.5434961319e-05) A[2]:(0.728971481323) A[3]:(0.59126585722)\n",
      " state (9)  A[0]:(0.655838608742) A[1]:(0.810033679008) A[2]:(0.809964597225) A[3]:(0.000863745575771)\n",
      " state (10)  A[0]:(0.728836238384) A[1]:(0.900002479553) A[2]:(-0.00030517578125) A[3]:(0.729286909103)\n",
      " state (11)  A[0]:(0.518041670322) A[1]:(0.876871585846) A[2]:(-0.592929542065) A[3]:(0.842988491058)\n",
      " state (12)  A[0]:(0.0738119632006) A[1]:(0.824535965919) A[2]:(-0.548211991787) A[3]:(0.79256683588)\n",
      " state (13)  A[0]:(-0.000688522937708) A[1]:(0.809454441071) A[2]:(0.899927735329) A[3]:(0.729254066944)\n",
      " state (14)  A[0]:(0.809826016426) A[1]:(0.900282979012) A[2]:(0.999999880791) A[3]:(0.810146808624)\n",
      " state (15)  A[0]:(0.987015783787) A[1]:(0.959667503834) A[2]:(1.0) A[3]:(0.887052536011)\n",
      "Episode 479000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6021. Times reached goal: 984.               Steps done: 4033005. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0159489143857.\n",
      "q_values \n",
      "tensor([[ 0.5307,  0.5906,  0.5906,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5307, -0.0000,  0.6560,  0.5907]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.7290,  0.5904,  0.6563]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0017,  0.8101,  0.0003,  0.6562]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000, -0.0002,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.9003,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532450795174) A[1]:(0.59054505825) A[2]:(0.590570688248) A[3]:(0.531545758247)\n",
      " state (1)  A[0]:(0.532421112061) A[1]:(4.86150383949e-05) A[2]:(0.656075000763) A[3]:(0.590431571007)\n",
      " state (2)  A[0]:(0.591299712658) A[1]:(0.729016542435) A[2]:(0.590317249298) A[3]:(0.656038999557)\n",
      " state (3)  A[0]:(0.656211793423) A[1]:(-0.226511508226) A[2]:(0.535720467567) A[3]:(0.520670413971)\n",
      " state (4)  A[0]:(0.590176522732) A[1]:(0.656093239784) A[2]:(-0.000105142593384) A[3]:(0.531502842903)\n",
      " state (5)  A[0]:(0.160074964166) A[1]:(0.92885106802) A[2]:(-0.182750940323) A[3]:(0.515873908997)\n",
      " state (6)  A[0]:(-0.0010522302473) A[1]:(0.810023128986) A[2]:(9.17911529541e-06) A[3]:(0.655856609344)\n",
      " state (7)  A[0]:(0.635817587376) A[1]:(-0.252673983574) A[2]:(0.265383303165) A[3]:(0.892657339573)\n",
      " state (8)  A[0]:(0.656281232834) A[1]:(1.09151005745e-05) A[2]:(0.729054689407) A[3]:(0.590351462364)\n",
      " state (9)  A[0]:(0.656441271305) A[1]:(0.810031950474) A[2]:(0.810009121895) A[3]:(3.40640544891e-05)\n",
      " state (10)  A[0]:(0.729312717915) A[1]:(0.899990558624) A[2]:(-0.000109672546387) A[3]:(0.728944063187)\n",
      " state (11)  A[0]:(0.518822312355) A[1]:(0.876846134663) A[2]:(-0.592937350273) A[3]:(0.842783510685)\n",
      " state (12)  A[0]:(0.0748947113752) A[1]:(0.824486732483) A[2]:(-0.548432826996) A[3]:(0.792301476002)\n",
      " state (13)  A[0]:(0.000393569440348) A[1]:(0.809381902218) A[2]:(0.899983465672) A[3]:(0.728910386562)\n",
      " state (14)  A[0]:(0.810229897499) A[1]:(0.90023034811) A[2]:(0.999999880791) A[3]:(0.80990254879)\n",
      " state (15)  A[0]:(0.987024366856) A[1]:(0.959618449211) A[2]:(1.0) A[3]:(0.88684129715)\n",
      "Episode 480000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6048. Times reached goal: 981.               Steps done: 4039053. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0158527464565.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53203189373) A[1]:(0.590453624725) A[2]:(0.590634942055) A[3]:(0.53217124939)\n",
      " state (1)  A[0]:(0.531516432762) A[1]:(-0.000197444111109) A[2]:(0.656114578247) A[3]:(0.590693116188)\n",
      " state (2)  A[0]:(0.590337872505) A[1]:(0.728950381279) A[2]:(0.590704083443) A[3]:(0.656061053276)\n",
      " state (3)  A[0]:(0.656039357185) A[1]:(-0.227203771472) A[2]:(0.536271810532) A[3]:(0.519916474819)\n",
      " state (4)  A[0]:(0.590419411659) A[1]:(0.656185746193) A[2]:(0.000185966491699) A[3]:(0.530617952347)\n",
      " state (5)  A[0]:(0.160682156682) A[1]:(0.928934395313) A[2]:(-0.182903498411) A[3]:(0.515094339848)\n",
      " state (6)  A[0]:(7.18235969543e-05) A[1]:(0.810051858425) A[2]:(-7.66515731812e-05) A[3]:(0.655389606953)\n",
      " state (7)  A[0]:(0.636082172394) A[1]:(-0.252524346113) A[2]:(0.265562146902) A[3]:(0.892412483692)\n",
      " state (8)  A[0]:(0.655743002892) A[1]:(0.00056638935348) A[2]:(0.729005634785) A[3]:(0.589615404606)\n",
      " state (9)  A[0]:(0.655377268791) A[1]:(0.810122847557) A[2]:(0.80999737978) A[3]:(-0.000846028153319)\n",
      " state (10)  A[0]:(0.728167057037) A[1]:(0.900012671947) A[2]:(-9.36985015869e-05) A[3]:(0.728488922119)\n",
      " state (11)  A[0]:(0.516795873642) A[1]:(0.876847624779) A[2]:(-0.593095242977) A[3]:(0.842460513115)\n",
      " state (12)  A[0]:(0.0719001218677) A[1]:(0.824447989464) A[2]:(-0.548811733723) A[3]:(0.79182523489)\n",
      " state (13)  A[0]:(-0.00280051701702) A[1]:(0.809283614159) A[2]:(0.900006532669) A[3]:(0.728250741959)\n",
      " state (14)  A[0]:(0.809086561203) A[1]:(0.900131344795) A[2]:(0.999999880791) A[3]:(0.809396326542)\n",
      " state (15)  A[0]:(0.986913025379) A[1]:(0.959535598755) A[2]:(1.0) A[3]:(0.88646364212)\n",
      "Episode 481000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6044. Times reached goal: 987.               Steps done: 4045097. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0157572214243.\n",
      " state (0)  A[0]:(0.531498253345) A[1]:(0.590526819229) A[2]:(0.590622603893) A[3]:(0.531583249569)\n",
      " state (1)  A[0]:(0.531448602676) A[1]:(7.24978744984e-05) A[2]:(0.656210303307) A[3]:(0.59053838253)\n",
      " state (2)  A[0]:(0.590514540672) A[1]:(0.729061126709) A[2]:(0.59059214592) A[3]:(0.656202912331)\n",
      " state (3)  A[0]:(0.65615773201) A[1]:(-0.225885257125) A[2]:(0.535927951336) A[3]:(0.520666122437)\n",
      " state (4)  A[0]:(0.590575456619) A[1]:(0.656114518642) A[2]:(0.000105023384094) A[3]:(0.531531095505)\n",
      " state (5)  A[0]:(0.161077067256) A[1]:(0.92889469862) A[2]:(-0.182799682021) A[3]:(0.516152143478)\n",
      " state (6)  A[0]:(0.000211894512177) A[1]:(0.810061514378) A[2]:(0.000195145606995) A[3]:(0.656080245972)\n",
      " state (7)  A[0]:(0.636053264141) A[1]:(-0.252561748028) A[2]:(0.266129463911) A[3]:(0.892591238022)\n",
      " state (8)  A[0]:(0.656077325344) A[1]:(0.000147420912981) A[2]:(0.729161381721) A[3]:(0.590380311012)\n",
      " state (9)  A[0]:(0.655967175961) A[1]:(0.810049831867) A[2]:(0.810109436512) A[3]:(-5.05447387695e-05)\n",
      " state (10)  A[0]:(0.728949189186) A[1]:(0.900031745434) A[2]:(2.53915786743e-05) A[3]:(0.728991985321)\n",
      " state (11)  A[0]:(0.518378734589) A[1]:(0.876930475235) A[2]:(-0.593276262283) A[3]:(0.842882096767)\n",
      " state (12)  A[0]:(0.0743050351739) A[1]:(0.824637711048) A[2]:(-0.54919141531) A[3]:(0.792426407337)\n",
      " state (13)  A[0]:(-0.000264912843704) A[1]:(0.809569716454) A[2]:(0.900071024895) A[3]:(0.729025423527)\n",
      " state (14)  A[0]:(0.809969902039) A[1]:(0.900334835052) A[2]:(0.999999880791) A[3]:(0.809964358807)\n",
      " state (15)  A[0]:(0.98695063591) A[1]:(0.959615528584) A[2]:(1.0) A[3]:(0.886731386185)\n",
      "Episode 482000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6027. Times reached goal: 987.               Steps done: 4051124. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0156625382651.\n",
      " state (0)  A[0]:(0.531660437584) A[1]:(0.590643048286) A[2]:(0.590603411198) A[3]:(0.531091213226)\n",
      " state (1)  A[0]:(0.531515538692) A[1]:(3.1054019928e-05) A[2]:(0.656168103218) A[3]:(0.590018451214)\n",
      " state (2)  A[0]:(0.59051823616) A[1]:(0.72901058197) A[2]:(0.590520620346) A[3]:(0.655743479729)\n",
      " state (3)  A[0]:(0.656134128571) A[1]:(-0.226411581039) A[2]:(0.535873174667) A[3]:(0.520403206348)\n",
      " state (4)  A[0]:(0.59047293663) A[1]:(0.656190276146) A[2]:(-0.000183463096619) A[3]:(0.531488001347)\n",
      " state (5)  A[0]:(0.160804241896) A[1]:(0.928887188435) A[2]:(-0.183133929968) A[3]:(0.516169786453)\n",
      " state (6)  A[0]:(-6.49392604828e-05) A[1]:(0.809975624084) A[2]:(-0.000115275382996) A[3]:(0.656104445457)\n",
      " state (7)  A[0]:(0.635923981667) A[1]:(-0.25286385417) A[2]:(0.265997767448) A[3]:(0.892576217651)\n",
      " state (8)  A[0]:(0.656045496464) A[1]:(-3.64780426025e-05) A[2]:(0.72900223732) A[3]:(0.590419530869)\n",
      " state (9)  A[0]:(0.655960559845) A[1]:(0.810014903545) A[2]:(0.809992015362) A[3]:(-3.35574150085e-05)\n",
      " state (10)  A[0]:(0.72889482975) A[1]:(0.899997472763) A[2]:(-0.000244975090027) A[3]:(0.728998661041)\n",
      " state (11)  A[0]:(0.518266677856) A[1]:(0.876870393753) A[2]:(-0.593589663506) A[3]:(0.842912316322)\n",
      " state (12)  A[0]:(0.0740990489721) A[1]:(0.824532032013) A[2]:(-0.549798846245) A[3]:(0.792503118515)\n",
      " state (13)  A[0]:(-0.000567644776311) A[1]:(0.809422373772) A[2]:(0.899938941002) A[3]:(0.729174137115)\n",
      " state (14)  A[0]:(0.809884309769) A[1]:(0.900228619576) A[2]:(0.999999880791) A[3]:(0.810157954693)\n",
      " state (15)  A[0]:(0.986930191517) A[1]:(0.959546685219) A[2]:(1.0) A[3]:(0.88686221838)\n",
      "Episode 483000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6034. Times reached goal: 980.               Steps done: 4057158. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0155683150665.\n",
      " state (0)  A[0]:(0.531867265701) A[1]:(0.590510964394) A[2]:(0.590497493744) A[3]:(0.530804514885)\n",
      " state (1)  A[0]:(0.531434297562) A[1]:(-0.000285476446152) A[2]:(0.655981659889) A[3]:(0.5899310112)\n",
      " state (2)  A[0]:(0.590018749237) A[1]:(0.728861570358) A[2]:(0.590695500374) A[3]:(0.655547082424)\n",
      " state (3)  A[0]:(0.654858112335) A[1]:(-0.228580132127) A[2]:(0.536538124084) A[3]:(0.519630670547)\n",
      " state (4)  A[0]:(0.588333547115) A[1]:(0.65625244379) A[2]:(0.000225186347961) A[3]:(0.53070628643)\n",
      " state (5)  A[0]:(0.156813830137) A[1]:(0.928894519806) A[2]:(-0.18296353519) A[3]:(0.515264391899)\n",
      " state (6)  A[0]:(-0.00465631112456) A[1]:(0.809833884239) A[2]:(-2.05039978027e-05) A[3]:(0.655424118042)\n",
      " state (7)  A[0]:(0.632664203644) A[1]:(-0.253223508596) A[2]:(0.26615473628) A[3]:(0.892289817333)\n",
      " state (8)  A[0]:(0.652526438236) A[1]:(6.94990158081e-05) A[2]:(0.729037284851) A[3]:(0.588883876801)\n",
      " state (9)  A[0]:(0.652327120304) A[1]:(0.810037910938) A[2]:(0.809891700745) A[3]:(-0.00286763114855)\n",
      " state (10)  A[0]:(0.725748181343) A[1]:(0.899953484535) A[2]:(-0.000896215206012) A[3]:(0.727584004402)\n",
      " state (11)  A[0]:(0.513168215752) A[1]:(0.87676024437) A[2]:(-0.594294428825) A[3]:(0.841980814934)\n",
      " state (12)  A[0]:(0.0670626908541) A[1]:(0.82432192564) A[2]:(-0.550783753395) A[3]:(0.791258275509)\n",
      " state (13)  A[0]:(-0.00746492575854) A[1]:(0.809159338474) A[2]:(0.89988642931) A[3]:(0.727598190308)\n",
      " state (14)  A[0]:(0.807715654373) A[1]:(0.900071382523) A[2]:(0.999999880791) A[3]:(0.809030234814)\n",
      " state (15)  A[0]:(0.986752510071) A[1]:(0.959448993206) A[2]:(1.0) A[3]:(0.886105895042)\n",
      "Episode 484000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6024. Times reached goal: 982.               Steps done: 4063182. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0154748134462.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5903,  0.5904,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5320, -0.0000,  0.6560,  0.5901]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5914,  0.7290,  0.5902,  0.6557]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 1.9015e-03,  8.0997e-01, -8.9407e-06,  6.5583e-01]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7294,  0.9000, -0.0000,  0.7296]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9001,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532293915749) A[1]:(0.590315699577) A[2]:(0.590342998505) A[3]:(0.531262516975)\n",
      " state (1)  A[0]:(0.532344937325) A[1]:(8.26716423035e-05) A[2]:(0.65595215559) A[3]:(0.589851140976)\n",
      " state (2)  A[0]:(0.591420054436) A[1]:(0.72891664505) A[2]:(0.590135395527) A[3]:(0.655412912369)\n",
      " state (3)  A[0]:(0.656898617744) A[1]:(-0.225779667497) A[2]:(0.535628914833) A[3]:(0.51943230629)\n",
      " state (4)  A[0]:(0.591567754745) A[1]:(0.655897974968) A[2]:(-0.000319361686707) A[3]:(0.530196726322)\n",
      " state (5)  A[0]:(0.162848994136) A[1]:(0.928855955601) A[2]:(-0.18353253603) A[3]:(0.514871001244)\n",
      " state (6)  A[0]:(0.00178429298103) A[1]:(0.809975504875) A[2]:(-0.000433325738413) A[3]:(0.654928982258)\n",
      " state (7)  A[0]:(0.636370420456) A[1]:(-0.252796024084) A[2]:(0.26619836688) A[3]:(0.892031431198)\n",
      " state (8)  A[0]:(0.65632468462) A[1]:(-0.000250995159149) A[2]:(0.728893995285) A[3]:(0.589155912399)\n",
      " state (9)  A[0]:(0.656098008156) A[1]:(0.809896349907) A[2]:(0.809983253479) A[3]:(-0.00191889470443)\n",
      " state (10)  A[0]:(0.729033470154) A[1]:(0.899938046932) A[2]:(-0.000248908996582) A[3]:(0.728161752224)\n",
      " state (11)  A[0]:(0.51866042614) A[1]:(0.876799702644) A[2]:(-0.593952775002) A[3]:(0.842434883118)\n",
      " state (12)  A[0]:(0.0747437179089) A[1]:(0.824421226978) A[2]:(-0.550626575947) A[3]:(0.79186040163)\n",
      " state (13)  A[0]:(8.34465026855e-07) A[1]:(0.809264779091) A[2]:(0.899927318096) A[3]:(0.728266358376)\n",
      " state (14)  A[0]:(0.810006558895) A[1]:(0.900101661682) A[2]:(0.999999880791) A[3]:(0.809402942657)\n",
      " state (15)  A[0]:(0.986887454987) A[1]:(0.959434807301) A[2]:(1.0) A[3]:(0.886215627193)\n",
      "Episode 485000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6040. Times reached goal: 985.               Steps done: 4069222. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0153816272785.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532816052437) A[1]:(0.59087061882) A[2]:(0.590686321259) A[3]:(0.53121638298)\n",
      " state (1)  A[0]:(0.532800078392) A[1]:(0.00013704225421) A[2]:(0.656234383583) A[3]:(0.590225279331)\n",
      " state (2)  A[0]:(0.591724038124) A[1]:(0.729185223579) A[2]:(0.590658187866) A[3]:(0.655895352364)\n",
      " state (3)  A[0]:(0.656855940819) A[1]:(-0.226121589541) A[2]:(0.536013484001) A[3]:(0.520277500153)\n",
      " state (4)  A[0]:(0.591144919395) A[1]:(0.656446874142) A[2]:(-0.000138998031616) A[3]:(0.531314313412)\n",
      " state (5)  A[0]:(0.161719143391) A[1]:(0.928934216499) A[2]:(-0.183238774538) A[3]:(0.516072273254)\n",
      " state (6)  A[0]:(0.000386297673685) A[1]:(0.810222744942) A[2]:(-0.000139713287354) A[3]:(0.655855536461)\n",
      " state (7)  A[0]:(0.636016666889) A[1]:(-0.252095609903) A[2]:(0.266570836306) A[3]:(0.892405390739)\n",
      " state (8)  A[0]:(0.656439721584) A[1]:(0.000402610719902) A[2]:(0.729004979134) A[3]:(0.590354323387)\n",
      " state (9)  A[0]:(0.656254172325) A[1]:(0.810126543045) A[2]:(0.80994451046) A[3]:(-0.000349193811417)\n",
      " state (10)  A[0]:(0.729109525681) A[1]:(0.900048494339) A[2]:(-0.000464081735117) A[3]:(0.728771805763)\n",
      " state (11)  A[0]:(0.518788933754) A[1]:(0.876935482025) A[2]:(-0.594239234924) A[3]:(0.842771828175)\n",
      " state (12)  A[0]:(0.0750000327826) A[1]:(0.824630022049) A[2]:(-0.551046133041) A[3]:(0.792284727097)\n",
      " state (13)  A[0]:(0.000521302164998) A[1]:(0.809527933598) A[2]:(0.900068879128) A[3]:(0.728839099407)\n",
      " state (14)  A[0]:(0.81037402153) A[1]:(0.900267183781) A[2]:(0.999999880791) A[3]:(0.809907257557)\n",
      " state (15)  A[0]:(0.986893236637) A[1]:(0.959480583668) A[2]:(1.0) A[3]:(0.886485695839)\n",
      "Episode 486000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6024. Times reached goal: 982.               Steps done: 4075246. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0152892468849.\n",
      " state (0)  A[0]:(0.531407833099) A[1]:(0.590459227562) A[2]:(0.590433001518) A[3]:(0.531409502029)\n",
      " state (1)  A[0]:(0.531304001808) A[1]:(-4.01139259338e-05) A[2]:(0.656036019325) A[3]:(0.590413928032)\n",
      " state (2)  A[0]:(0.590351462364) A[1]:(0.728941082954) A[2]:(0.590442895889) A[3]:(0.656059920788)\n",
      " state (3)  A[0]:(0.65597474575) A[1]:(-0.226363092661) A[2]:(0.536085009575) A[3]:(0.520494937897)\n",
      " state (4)  A[0]:(0.590390563011) A[1]:(0.656006336212) A[2]:(0.000128149986267) A[3]:(0.531470656395)\n",
      " state (5)  A[0]:(0.160878032446) A[1]:(0.92887532711) A[2]:(-0.183182671666) A[3]:(0.516285657883)\n",
      " state (6)  A[0]:(-0.00010934472084) A[1]:(0.810005784035) A[2]:(-9.65595245361e-06) A[3]:(0.656033337116)\n",
      " state (7)  A[0]:(0.635533094406) A[1]:(-0.252609491348) A[2]:(0.266855299473) A[3]:(0.892384648323)\n",
      " state (8)  A[0]:(0.655930638313) A[1]:(0.000238865613937) A[2]:(0.729015350342) A[3]:(0.590385437012)\n",
      " state (9)  A[0]:(0.655948758125) A[1]:(0.810072779655) A[2]:(0.809999704361) A[3]:(-0.000161051750183)\n",
      " state (10)  A[0]:(0.728882074356) A[1]:(0.900003373623) A[2]:(-0.00012731552124) A[3]:(0.728876948357)\n",
      " state (11)  A[0]:(0.51840531826) A[1]:(0.876864492893) A[2]:(-0.594128608704) A[3]:(0.842851638794)\n",
      " state (12)  A[0]:(0.074388846755) A[1]:(0.824512720108) A[2]:(-0.551167011261) A[3]:(0.792395234108)\n",
      " state (13)  A[0]:(-0.000244408845901) A[1]:(0.809369981289) A[2]:(0.900058507919) A[3]:(0.728971898556)\n",
      " state (14)  A[0]:(0.810077607632) A[1]:(0.900154948235) A[2]:(0.999999880791) A[3]:(0.810000777245)\n",
      " state (15)  A[0]:(0.98685580492) A[1]:(0.959411144257) A[2]:(1.0) A[3]:(0.88650739193)\n",
      "Episode 487000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6002. Times reached goal: 977.               Steps done: 4081248. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0151977556649.\n",
      " state (0)  A[0]:(0.532044053078) A[1]:(0.590777873993) A[2]:(0.590634584427) A[3]:(0.531897544861)\n",
      " state (1)  A[0]:(0.532045841217) A[1]:(-7.98515975475e-05) A[2]:(0.656246602535) A[3]:(0.590907335281)\n",
      " state (2)  A[0]:(0.591092586517) A[1]:(0.729130983353) A[2]:(0.590692400932) A[3]:(0.656552493572)\n",
      " state (3)  A[0]:(0.656705141068) A[1]:(-0.225462347269) A[2]:(0.535982489586) A[3]:(0.521213591099)\n",
      " state (4)  A[0]:(0.591228723526) A[1]:(0.656148731709) A[2]:(-6.91413879395e-05) A[3]:(0.532220721245)\n",
      " state (5)  A[0]:(0.162092760205) A[1]:(0.928868293762) A[2]:(-0.183350175619) A[3]:(0.517133712769)\n",
      " state (6)  A[0]:(0.00115185929462) A[1]:(0.809952259064) A[2]:(-5.92470169067e-05) A[3]:(0.65666359663)\n",
      " state (7)  A[0]:(0.636426329613) A[1]:(-0.252986520529) A[2]:(0.267146676779) A[3]:(0.892574131489)\n",
      " state (8)  A[0]:(0.656803071499) A[1]:(-0.000419478834374) A[2]:(0.729082524776) A[3]:(0.590990543365)\n",
      " state (9)  A[0]:(0.656664729118) A[1]:(0.809875011444) A[2]:(0.80999559164) A[3]:(0.000504091323819)\n",
      " state (10)  A[0]:(0.729459166527) A[1]:(0.899943590164) A[2]:(-0.000417947740061) A[3]:(0.729253768921)\n",
      " state (11)  A[0]:(0.519306540489) A[1]:(0.876845836639) A[2]:(-0.594581484795) A[3]:(0.84311401844)\n",
      " state (12)  A[0]:(0.0754809230566) A[1]:(0.824550390244) A[2]:(-0.551888465881) A[3]:(0.792699217796)\n",
      " state (13)  A[0]:(0.000554352940526) A[1]:(0.809468448162) A[2]:(0.89997446537) A[3]:(0.729272007942)\n",
      " state (14)  A[0]:(0.810173749924) A[1]:(0.900229632854) A[2]:(0.999999880791) A[3]:(0.810157954693)\n",
      " state (15)  A[0]:(0.986825525761) A[1]:(0.959430396557) A[2]:(1.0) A[3]:(0.886504113674)\n",
      "Episode 488000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6050. Times reached goal: 991.               Steps done: 4087298. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.015106086821.\n",
      " state (0)  A[0]:(0.531382262707) A[1]:(0.590441584587) A[2]:(0.590399205685) A[3]:(0.531419992447)\n",
      " state (1)  A[0]:(0.531266570091) A[1]:(3.37846577168e-05) A[2]:(0.65606212616) A[3]:(0.590418219566)\n",
      " state (2)  A[0]:(0.590340614319) A[1]:(0.729012966156) A[2]:(0.590425670147) A[3]:(0.65609151125)\n",
      " state (3)  A[0]:(0.655971050262) A[1]:(-0.225694060326) A[2]:(0.535880923271) A[3]:(0.520452558994)\n",
      " state (4)  A[0]:(0.590413987637) A[1]:(0.656041443348) A[2]:(-0.000146389007568) A[3]:(0.531424999237)\n",
      " state (5)  A[0]:(0.16100628674) A[1]:(0.928873360157) A[2]:(-0.183491066098) A[3]:(0.516339063644)\n",
      " state (6)  A[0]:(7.00056552887e-05) A[1]:(0.809978663921) A[2]:(-0.000101566314697) A[3]:(0.655986428261)\n",
      " state (7)  A[0]:(0.635491847992) A[1]:(-0.252732217312) A[2]:(0.267265886068) A[3]:(0.892272830009)\n",
      " state (8)  A[0]:(0.655959486961) A[1]:(-0.000112276524305) A[2]:(0.728932857513) A[3]:(0.590552687645)\n",
      " state (9)  A[0]:(0.655941367149) A[1]:(0.80996286869) A[2]:(0.809978842735) A[3]:(-2.0369887352e-05)\n",
      " state (10)  A[0]:(0.728968501091) A[1]:(0.899998307228) A[2]:(-0.000258803367615) A[3]:(0.728898286819)\n",
      " state (11)  A[0]:(0.518729627132) A[1]:(0.87691193819) A[2]:(-0.594610691071) A[3]:(0.842878460884)\n",
      " state (12)  A[0]:(0.0748889297247) A[1]:(0.824629187584) A[2]:(-0.552102565765) A[3]:(0.792379558086)\n",
      " state (13)  A[0]:(0.000139653682709) A[1]:(0.809521794319) A[2]:(0.90002810955) A[3]:(0.728861689568)\n",
      " state (14)  A[0]:(0.810120642185) A[1]:(0.900228857994) A[2]:(0.999999880791) A[3]:(0.809885263443)\n",
      " state (15)  A[0]:(0.986806333065) A[1]:(0.959395885468) A[2]:(1.0) A[3]:(0.886300325394)\n",
      "Episode 489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6058. Times reached goal: 990.               Steps done: 4093356. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.01501485078.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5904,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.3141e-01, -3.5390e-07,  6.5603e-01,  5.9037e-01]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7290,  0.5905,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-3.3188e-04,  8.0990e-01, -3.0994e-06,  6.5596e-01]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0001,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531371712685) A[1]:(0.590364575386) A[2]:(0.590440630913) A[3]:(0.531416356564)\n",
      " state (1)  A[0]:(0.531351327896) A[1]:(-3.08938324451e-05) A[2]:(0.656035661697) A[3]:(0.590373039246)\n",
      " state (2)  A[0]:(0.590367257595) A[1]:(0.728906035423) A[2]:(0.590500593185) A[3]:(0.656017720699)\n",
      " state (3)  A[0]:(0.655901074409) A[1]:(-0.226745381951) A[2]:(0.536154985428) A[3]:(0.520335674286)\n",
      " state (4)  A[0]:(0.590225338936) A[1]:(0.655933380127) A[2]:(4.66108322144e-05) A[3]:(0.531421542168)\n",
      " state (5)  A[0]:(0.160576239228) A[1]:(0.92886286974) A[2]:(-0.183392345905) A[3]:(0.516378879547)\n",
      " state (6)  A[0]:(-0.000388205022318) A[1]:(0.809964001179) A[2]:(3.60012054443e-05) A[3]:(0.655940175056)\n",
      " state (7)  A[0]:(0.63528585434) A[1]:(-0.252734929323) A[2]:(0.267565965652) A[3]:(0.89218133688)\n",
      " state (8)  A[0]:(0.655901670456) A[1]:(-0.000110525637865) A[2]:(0.728973448277) A[3]:(0.590417027473)\n",
      " state (9)  A[0]:(0.655939757824) A[1]:(0.809949040413) A[2]:(0.80999815464) A[3]:(-2.62856483459e-05)\n",
      " state (10)  A[0]:(0.72894859314) A[1]:(0.899987399578) A[2]:(-0.000186085700989) A[3]:(0.728995144367)\n",
      " state (11)  A[0]:(0.518678486347) A[1]:(0.876904547215) A[2]:(-0.594725012779) A[3]:(0.842986166477)\n",
      " state (12)  A[0]:(0.0747616440058) A[1]:(0.824631333351) A[2]:(-0.552434444427) A[3]:(0.792548656464)\n",
      " state (13)  A[0]:(-2.70307064056e-05) A[1]:(0.809536933899) A[2]:(0.900046050549) A[3]:(0.729082643986)\n",
      " state (14)  A[0]:(0.81010633707) A[1]:(0.900239646435) A[2]:(0.999999880791) A[3]:(0.810036122799)\n",
      " state (15)  A[0]:(0.986786842346) A[1]:(0.959382236004) A[2]:(1.0) A[3]:(0.886322975159)\n",
      "Episode 490000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6046. Times reached goal: 984.               Steps done: 4099402. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0149243448673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531366109848) A[1]:(0.590626358986) A[2]:(0.590579032898) A[3]:(0.531569123268)\n",
      " state (1)  A[0]:(0.531279563904) A[1]:(0.000114377588034) A[2]:(0.656143546104) A[3]:(0.590465784073)\n",
      " state (2)  A[0]:(0.590279579163) A[1]:(0.729072809219) A[2]:(0.590688705444) A[3]:(0.65605866909)\n",
      " state (3)  A[0]:(0.655924260616) A[1]:(-0.226315557957) A[2]:(0.536361694336) A[3]:(0.520451903343)\n",
      " state (4)  A[0]:(0.590264976025) A[1]:(0.656192421913) A[2]:(0.000198721885681) A[3]:(0.531533837318)\n",
      " state (5)  A[0]:(0.160535216331) A[1]:(0.928917229176) A[2]:(-0.183430582285) A[3]:(0.516483783722)\n",
      " state (6)  A[0]:(-0.000591903866734) A[1]:(0.810080587864) A[2]:(-5.87701797485e-05) A[3]:(0.656017303467)\n",
      " state (7)  A[0]:(0.635020971298) A[1]:(-0.252398252487) A[2]:(0.267627835274) A[3]:(0.892202973366)\n",
      " state (8)  A[0]:(0.655531525612) A[1]:(0.000374313414795) A[2]:(0.728960573673) A[3]:(0.590395212173)\n",
      " state (9)  A[0]:(0.655388593674) A[1]:(0.810115396976) A[2]:(0.809980928898) A[3]:(-0.000324323773384)\n",
      " state (10)  A[0]:(0.728344321251) A[1]:(0.90005004406) A[2]:(-0.000201463699341) A[3]:(0.728760480881)\n",
      " state (11)  A[0]:(0.517627179623) A[1]:(0.876953721046) A[2]:(-0.594886064529) A[3]:(0.842802703381)\n",
      " state (12)  A[0]:(0.0732548236847) A[1]:(0.824674427509) A[2]:(-0.552862524986) A[3]:(0.79228079319)\n",
      " state (13)  A[0]:(-0.00160917500034) A[1]:(0.809557020664) A[2]:(0.899986624718) A[3]:(0.728707909584)\n",
      " state (14)  A[0]:(0.809563159943) A[1]:(0.900233149529) A[2]:(0.999999880791) A[3]:(0.809734463692)\n",
      " state (15)  A[0]:(0.986728549004) A[1]:(0.9593577981) A[2]:(1.0) A[3]:(0.886075258255)\n",
      "Episode 491000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6054. Times reached goal: 982.               Steps done: 4105456. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0148342658278.\n",
      " state (0)  A[0]:(0.531546831131) A[1]:(0.590454101562) A[2]:(0.5904302001) A[3]:(0.531157970428)\n",
      " state (1)  A[0]:(0.531542956829) A[1]:(9.6146017313e-05) A[2]:(0.65606033802) A[3]:(0.590150356293)\n",
      " state (2)  A[0]:(0.590644836426) A[1]:(0.729035258293) A[2]:(0.590446233749) A[3]:(0.65587335825)\n",
      " state (3)  A[0]:(0.656300187111) A[1]:(-0.22596552968) A[2]:(0.5361109972) A[3]:(0.520344257355)\n",
      " state (4)  A[0]:(0.590764343739) A[1]:(0.656209588051) A[2]:(0.000113248825073) A[3]:(0.531510353088)\n",
      " state (5)  A[0]:(0.161437317729) A[1]:(0.928898751736) A[2]:(-0.183306753635) A[3]:(0.516540527344)\n",
      " state (6)  A[0]:(0.00050148362061) A[1]:(0.810050606728) A[2]:(0.000276565551758) A[3]:(0.655917823315)\n",
      " state (7)  A[0]:(0.635858416557) A[1]:(-0.252535134554) A[2]:(0.268187403679) A[3]:(0.892061710358)\n",
      " state (8)  A[0]:(0.656497955322) A[1]:(4.83430922031e-05) A[2]:(0.729041337967) A[3]:(0.590428113937)\n",
      " state (9)  A[0]:(0.656247258186) A[1]:(0.810025155544) A[2]:(0.810038864613) A[3]:(-2.27987766266e-05)\n",
      " state (10)  A[0]:(0.729071259499) A[1]:(0.900030195713) A[2]:(4.55379486084e-05) A[3]:(0.72888481617)\n",
      " state (11)  A[0]:(0.518895804882) A[1]:(0.876963019371) A[2]:(-0.594872832298) A[3]:(0.842905700207)\n",
      " state (12)  A[0]:(0.0750852003694) A[1]:(0.824728250504) A[2]:(-0.553028941154) A[3]:(0.792434334755)\n",
      " state (13)  A[0]:(0.000226646661758) A[1]:(0.809652686119) A[2]:(0.900061488152) A[3]:(0.728919148445)\n",
      " state (14)  A[0]:(0.810151517391) A[1]:(0.900301218033) A[2]:(0.999999880791) A[3]:(0.809919297695)\n",
      " state (15)  A[0]:(0.98674595356) A[1]:(0.959371864796) A[2]:(1.0) A[3]:(0.886145591736)\n",
      "Episode 492000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6033. Times reached goal: 985.               Steps done: 4111489. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.014745040122.\n",
      " state (0)  A[0]:(0.531505942345) A[1]:(0.590746998787) A[2]:(0.590408980846) A[3]:(0.531462073326)\n",
      " state (1)  A[0]:(0.531397104263) A[1]:(-0.000166580080986) A[2]:(0.656178772449) A[3]:(0.59044444561)\n",
      " state (2)  A[0]:(0.590451478958) A[1]:(0.729489922523) A[2]:(0.591092228889) A[3]:(0.656102180481)\n",
      " state (3)  A[0]:(0.656224012375) A[1]:(-0.224878445268) A[2]:(0.536161899567) A[3]:(0.520346045494)\n",
      " state (4)  A[0]:(0.590717196465) A[1]:(0.656018435955) A[2]:(-3.75509262085e-05) A[3]:(0.531411230564)\n",
      " state (5)  A[0]:(0.161316171288) A[1]:(0.928866744041) A[2]:(-0.183672264218) A[3]:(0.516663551331)\n",
      " state (6)  A[0]:(-9.40561294556e-05) A[1]:(0.809999346733) A[2]:(-0.000127911567688) A[3]:(0.656148314476)\n",
      " state (7)  A[0]:(0.635113358498) A[1]:(-0.252899676561) A[2]:(0.268110722303) A[3]:(0.892170190811)\n",
      " state (8)  A[0]:(0.655869841576) A[1]:(-0.000481381983263) A[2]:(0.729031443596) A[3]:(0.590569972992)\n",
      " state (9)  A[0]:(0.655933320522) A[1]:(0.809887707233) A[2]:(0.80996799469) A[3]:(-0.000112935900688)\n",
      " state (10)  A[0]:(0.728967666626) A[1]:(0.899977862835) A[2]:(-0.000576019228902) A[3]:(0.729029297829)\n",
      " state (11)  A[0]:(0.518748402596) A[1]:(0.8769287467) A[2]:(-0.595565021038) A[3]:(0.843064427376)\n",
      " state (12)  A[0]:(0.0747416093946) A[1]:(0.824731171131) A[2]:(-0.553949594498) A[3]:(0.792639493942)\n",
      " state (13)  A[0]:(-0.000240325927734) A[1]:(0.809731543064) A[2]:(0.900006651878) A[3]:(0.729127287865)\n",
      " state (14)  A[0]:(0.809972524643) A[1]:(0.900395154953) A[2]:(0.999999880791) A[3]:(0.810000598431)\n",
      " state (15)  A[0]:(0.986701071262) A[1]:(0.959409058094) A[2]:(1.0) A[3]:(0.886066555977)\n",
      "Episode 493000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6044. Times reached goal: 983.               Steps done: 4117533. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0146561898755.\n",
      " state (0)  A[0]:(0.53149998188) A[1]:(0.590577602386) A[2]:(0.590547561646) A[3]:(0.532180786133)\n",
      " state (1)  A[0]:(0.531601667404) A[1]:(-4.38988208771e-05) A[2]:(0.656126141548) A[3]:(0.590937077999)\n",
      " state (2)  A[0]:(0.590651512146) A[1]:(0.72900891304) A[2]:(0.590563356876) A[3]:(0.656417429447)\n",
      " state (3)  A[0]:(0.655959188938) A[1]:(-0.225768014789) A[2]:(0.536049962044) A[3]:(0.520552158356)\n",
      " state (4)  A[0]:(0.590245008469) A[1]:(0.656146287918) A[2]:(-0.000199794769287) A[3]:(0.531447291374)\n",
      " state (5)  A[0]:(0.160696983337) A[1]:(0.928904950619) A[2]:(-0.183899983764) A[3]:(0.516472935677)\n",
      " state (6)  A[0]:(-0.00015714764595) A[1]:(0.809999227524) A[2]:(-0.000240921974182) A[3]:(0.65586078167)\n",
      " state (7)  A[0]:(0.635334968567) A[1]:(-0.252801090479) A[2]:(0.268192499876) A[3]:(0.891969442368)\n",
      " state (8)  A[0]:(0.655903577805) A[1]:(-2.38418579102e-05) A[2]:(0.728951871395) A[3]:(0.589726448059)\n",
      " state (9)  A[0]:(0.655636310577) A[1]:(0.810016214848) A[2]:(0.809947669506) A[3]:(-0.0019919697661)\n",
      " state (10)  A[0]:(0.728465735912) A[1]:(0.900002419949) A[2]:(-0.000408053369028) A[3]:(0.727840781212)\n",
      " state (11)  A[0]:(0.517760634422) A[1]:(0.876913607121) A[2]:(-0.595539808273) A[3]:(0.842225968838)\n",
      " state (12)  A[0]:(0.0732576698065) A[1]:(0.824660003185) A[2]:(-0.554133832455) A[3]:(0.791496634483)\n",
      " state (13)  A[0]:(-0.00178584270179) A[1]:(0.809601783752) A[2]:(0.900023877621) A[3]:(0.727665662766)\n",
      " state (14)  A[0]:(0.809484839439) A[1]:(0.900294423103) A[2]:(0.999999880791) A[3]:(0.808975756168)\n",
      " state (15)  A[0]:(0.986650884151) A[1]:(0.95933920145) A[2]:(1.0) A[3]:(0.885424613953)\n",
      "Episode 494000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6048. Times reached goal: 988.               Steps done: 4123581. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0145678167488.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5905,  0.5905,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6561, -0.0000,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6562,  0.0003,  0.7291,  0.5906]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6562,  0.8100,  0.8100,  0.0003]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  0.8095,  0.9001,  0.7291]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.531505942345) A[1]:(0.59054338932) A[2]:(0.590467691422) A[3]:(0.531578660011)\n",
      " state (1)  A[0]:(0.531390309334) A[1]:(7.49193131924e-05) A[2]:(0.656117081642) A[3]:(0.590463459492)\n",
      " state (2)  A[0]:(0.590424060822) A[1]:(0.729037582874) A[2]:(0.590593218803) A[3]:(0.656105518341)\n",
      " state (3)  A[0]:(0.656091272831) A[1]:(-0.225767835975) A[2]:(0.536148786545) A[3]:(0.520442306995)\n",
      " state (4)  A[0]:(0.590480446815) A[1]:(0.656141459942) A[2]:(-4.85181808472e-05) A[3]:(0.531577944756)\n",
      " state (5)  A[0]:(0.16096059978) A[1]:(0.928899526596) A[2]:(-0.183768332005) A[3]:(0.516804277897)\n",
      " state (6)  A[0]:(-1.10566616058e-05) A[1]:(0.810019671917) A[2]:(-6.5803527832e-05) A[3]:(0.656136512756)\n",
      " state (7)  A[0]:(0.635387539864) A[1]:(-0.252613544464) A[2]:(0.268512099981) A[3]:(0.892040550709)\n",
      " state (8)  A[0]:(0.656236410141) A[1]:(0.000168915838003) A[2]:(0.729030609131) A[3]:(0.590525746346)\n",
      " state (9)  A[0]:(0.65624666214) A[1]:(0.810054600239) A[2]:(0.809998095036) A[3]:(0.000188007950783)\n",
      " state (10)  A[0]:(0.729143738747) A[1]:(0.900012791157) A[2]:(-0.000180959701538) A[3]:(0.729017972946)\n",
      " state (11)  A[0]:(0.519068002701) A[1]:(0.876914083958) A[2]:(-0.595511555672) A[3]:(0.843010604382)\n",
      " state (12)  A[0]:(0.0752689614892) A[1]:(0.824630975723) A[2]:(-0.554313540459) A[3]:(0.792564690113)\n",
      " state (13)  A[0]:(0.000327080488205) A[1]:(0.809514284134) A[2]:(0.900043725967) A[3]:(0.729073524475)\n",
      " state (14)  A[0]:(0.810206055641) A[1]:(0.900197088718) A[2]:(0.999999880791) A[3]:(0.810069799423)\n",
      " state (15)  A[0]:(0.986686170101) A[1]:(0.959258913994) A[2]:(1.0) A[3]:(0.886093616486)\n",
      "Episode 495000 finished after 0 timesteps with r=0.0. Running score: 0.96. Times trained:               6041. Times reached goal: 981.               Steps done: 4129622. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.01448007785.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531528055668) A[1]:(0.590540111065) A[2]:(0.590396642685) A[3]:(0.531455993652)\n",
      " state (1)  A[0]:(0.531376302242) A[1]:(-0.000130042433739) A[2]:(0.656070590019) A[3]:(0.590346217155)\n",
      " state (2)  A[0]:(0.590356111526) A[1]:(0.728890180588) A[2]:(0.590577125549) A[3]:(0.655978739262)\n",
      " state (3)  A[0]:(0.656017541885) A[1]:(-0.226268157363) A[2]:(0.53611767292) A[3]:(0.520167708397)\n",
      " state (4)  A[0]:(0.590313136578) A[1]:(0.655910491943) A[2]:(-0.000199437141418) A[3]:(0.53129607439)\n",
      " state (5)  A[0]:(0.160536140203) A[1]:(0.928852975368) A[2]:(-0.184005603194) A[3]:(0.516569316387)\n",
      " state (6)  A[0]:(-0.000585019530263) A[1]:(0.809899270535) A[2]:(-0.000277757644653) A[3]:(0.656024873257)\n",
      " state (7)  A[0]:(0.634939968586) A[1]:(-0.252946972847) A[2]:(0.268423497677) A[3]:(0.892036497593)\n",
      " state (8)  A[0]:(0.655825257301) A[1]:(-0.000267252326012) A[2]:(0.7287722826) A[3]:(0.590700089931)\n",
      " state (9)  A[0]:(0.655787289143) A[1]:(0.80992347002) A[2]:(0.809859871864) A[3]:(-0.000157848000526)\n",
      " state (10)  A[0]:(0.728729844093) A[1]:(0.899963200092) A[2]:(-0.000517487467732) A[3]:(0.728764593601)\n",
      " state (11)  A[0]:(0.518374085426) A[1]:(0.876877427101) A[2]:(-0.595913112164) A[3]:(0.842860221863)\n",
      " state (12)  A[0]:(0.0742170661688) A[1]:(0.824614942074) A[2]:(-0.554951310158) A[3]:(0.792354464531)\n",
      " state (13)  A[0]:(-0.000801950518508) A[1]:(0.80953669548) A[2]:(0.899958133698) A[3]:(0.728776156902)\n",
      " state (14)  A[0]:(0.809844493866) A[1]:(0.900229454041) A[2]:(0.999999880791) A[3]:(0.80983299017)\n",
      " state (15)  A[0]:(0.986642956734) A[1]:(0.959265768528) A[2]:(1.0) A[3]:(0.885888874531)\n",
      "Episode 496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6040. Times reached goal: 988.               Steps done: 4135662. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.014392881777.\n",
      " state (0)  A[0]:(0.531419456005) A[1]:(0.590589523315) A[2]:(0.590552926064) A[3]:(0.53144299984)\n",
      " state (1)  A[0]:(0.531410813332) A[1]:(9.07219946384e-05) A[2]:(0.656242251396) A[3]:(0.590569376945)\n",
      " state (2)  A[0]:(0.590533018112) A[1]:(0.729026913643) A[2]:(0.590371251106) A[3]:(0.656325995922)\n",
      " state (3)  A[0]:(0.65618622303) A[1]:(-0.223648443818) A[2]:(0.535716533661) A[3]:(0.520804047585)\n",
      " state (4)  A[0]:(0.590592861176) A[1]:(0.656057775021) A[2]:(-9.78708267212e-05) A[3]:(0.531720936298)\n",
      " state (5)  A[0]:(0.16123509407) A[1]:(0.928880214691) A[2]:(-0.183773055673) A[3]:(0.516975462437)\n",
      " state (6)  A[0]:(0.000138431787491) A[1]:(0.810026705265) A[2]:(0.000223278999329) A[3]:(0.656072854996)\n",
      " state (7)  A[0]:(0.635229468346) A[1]:(-0.252708256245) A[2]:(0.269297480583) A[3]:(0.891889929771)\n",
      " state (8)  A[0]:(0.656118810177) A[1]:(-0.000629451067653) A[2]:(0.728988409042) A[3]:(0.590752601624)\n",
      " state (9)  A[0]:(0.656005144119) A[1]:(0.809806406498) A[2]:(0.810044109821) A[3]:(8.53538513184e-05)\n",
      " state (10)  A[0]:(0.729082763195) A[1]:(0.899994492531) A[2]:(-0.000216841697693) A[3]:(0.729070842266)\n",
      " state (11)  A[0]:(0.519166588783) A[1]:(0.876989722252) A[2]:(-0.596009612083) A[3]:(0.843134224415)\n",
      " state (12)  A[0]:(0.0753653272986) A[1]:(0.824823617935) A[2]:(-0.555224061012) A[3]:(0.792673230171)\n",
      " state (13)  A[0]:(0.000198274850845) A[1]:(0.80978500843) A[2]:(0.900041997433) A[3]:(0.72908115387)\n",
      " state (14)  A[0]:(0.810016274452) A[1]:(0.900358855724) A[2]:(0.999999880791) A[3]:(0.810004115105)\n",
      " state (15)  A[0]:(0.986614882946) A[1]:(0.959290742874) A[2]:(1.0) A[3]:(0.885899484158)\n",
      "Episode 497000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6041. Times reached goal: 986.               Steps done: 4141703. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0143061964748.\n",
      " state (0)  A[0]:(0.53219127655) A[1]:(0.590702414513) A[2]:(0.590669035912) A[3]:(0.531782150269)\n",
      " state (1)  A[0]:(0.531921863556) A[1]:(-0.00030467286706) A[2]:(0.656070411205) A[3]:(0.590541303158)\n",
      " state (2)  A[0]:(0.590914607048) A[1]:(0.728938341141) A[2]:(0.590302228928) A[3]:(0.656123042107)\n",
      " state (3)  A[0]:(0.656406283379) A[1]:(-0.225215911865) A[2]:(0.535910010338) A[3]:(0.520762562752)\n",
      " state (4)  A[0]:(0.590680837631) A[1]:(0.656038045883) A[2]:(-0.000242352485657) A[3]:(0.531800746918)\n",
      " state (5)  A[0]:(0.16100358963) A[1]:(0.928833961487) A[2]:(-0.184145867825) A[3]:(0.516967773438)\n",
      " state (6)  A[0]:(-0.000344067811966) A[1]:(0.809895515442) A[2]:(-0.000483632058604) A[3]:(0.656212747097)\n",
      " state (7)  A[0]:(0.635195970535) A[1]:(-0.252973943949) A[2]:(0.268505454063) A[3]:(0.892114162445)\n",
      " state (8)  A[0]:(0.656319618225) A[1]:(-0.000471297622425) A[2]:(0.728623390198) A[3]:(0.591354250908)\n",
      " state (9)  A[0]:(0.656163573265) A[1]:(0.8098706007) A[2]:(0.809707283974) A[3]:(0.000881910091266)\n",
      " state (10)  A[0]:(0.728887557983) A[1]:(0.89990824461) A[2]:(-0.00112390471622) A[3]:(0.729255199432)\n",
      " state (11)  A[0]:(0.518491029739) A[1]:(0.876789152622) A[2]:(-0.596710145473) A[3]:(0.843168139458)\n",
      " state (12)  A[0]:(0.0742204189301) A[1]:(0.824491500854) A[2]:(-0.556229233742) A[3]:(0.792730987072)\n",
      " state (13)  A[0]:(-0.000838577572722) A[1]:(0.809438169003) A[2]:(0.899886906147) A[3]:(0.729222655296)\n",
      " state (14)  A[0]:(0.809909820557) A[1]:(0.900215387344) A[2]:(0.999999880791) A[3]:(0.810160696507)\n",
      " state (15)  A[0]:(0.986604332924) A[1]:(0.959237098694) A[2]:(1.0) A[3]:(0.885961413383)\n",
      "Episode 498000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6023. Times reached goal: 988.               Steps done: 4147726. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0142202892228.\n",
      " state (0)  A[0]:(0.531505823135) A[1]:(0.590874433517) A[2]:(0.590458869934) A[3]:(0.531531691551)\n",
      " state (1)  A[0]:(0.531459569931) A[1]:(0.000135574489832) A[2]:(0.656298696995) A[3]:(0.590594053268)\n",
      " state (2)  A[0]:(0.590553343296) A[1]:(0.729060173035) A[2]:(0.590946614742) A[3]:(0.656244158745)\n",
      " state (3)  A[0]:(0.656391739845) A[1]:(-0.226637512445) A[2]:(0.536476314068) A[3]:(0.5203551054)\n",
      " state (4)  A[0]:(0.590853214264) A[1]:(0.656308472157) A[2]:(-6.29425048828e-05) A[3]:(0.531639635563)\n",
      " state (5)  A[0]:(0.161321997643) A[1]:(0.928914427757) A[2]:(-0.183946281672) A[3]:(0.517080724239)\n",
      " state (6)  A[0]:(8.70227813721e-05) A[1]:(0.810027062893) A[2]:(-5.84125518799e-06) A[3]:(0.656280636787)\n",
      " state (7)  A[0]:(0.635353684425) A[1]:(-0.252741962671) A[2]:(0.269362837076) A[3]:(0.891990423203)\n",
      " state (8)  A[0]:(0.656409561634) A[1]:(-0.000111427158117) A[2]:(0.72897875309) A[3]:(0.590963840485)\n",
      " state (9)  A[0]:(0.656253933907) A[1]:(0.810022413731) A[2]:(0.809952974319) A[3]:(0.000358030170901)\n",
      " state (10)  A[0]:(0.729056596756) A[1]:(0.900006651878) A[2]:(-0.000309586524963) A[3]:(0.729057073593)\n",
      " state (11)  A[0]:(0.518929898739) A[1]:(0.876917898655) A[2]:(-0.596277475357) A[3]:(0.843084812164)\n",
      " state (12)  A[0]:(0.0749422758818) A[1]:(0.824666023254) A[2]:(-0.555958032608) A[3]:(0.792647719383)\n",
      " state (13)  A[0]:(-0.000175446271896) A[1]:(0.809598326683) A[2]:(0.900013566017) A[3]:(0.729103446007)\n",
      " state (14)  A[0]:(0.810057401657) A[1]:(0.900274157524) A[2]:(0.999999880791) A[3]:(0.81003254652)\n",
      " state (15)  A[0]:(0.986591219902) A[1]:(0.959232568741) A[2]:(1.0) A[3]:(0.885804116726)\n",
      "Episode 499000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6041. Times reached goal: 987.               Steps done: 4153767. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0141346434092.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5905,  0.5907,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5317, -0.0001,  0.6561,  0.5906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.7290,  0.5906,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8100,  0.0001,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0004,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9003,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531513690948) A[1]:(0.59045624733) A[2]:(0.590687513351) A[3]:(0.531587958336)\n",
      " state (1)  A[0]:(0.531536221504) A[1]:(-9.3836337328e-05) A[2]:(0.656159162521) A[3]:(0.590604901314)\n",
      " state (2)  A[0]:(0.590560913086) A[1]:(0.728994250298) A[2]:(0.590606808662) A[3]:(0.656217336655)\n",
      " state (3)  A[0]:(0.656148791313) A[1]:(-0.225569009781) A[2]:(0.536338627338) A[3]:(0.520482599735)\n",
      " state (4)  A[0]:(0.59047806263) A[1]:(0.655963778496) A[2]:(0.00019907951355) A[3]:(0.531557500362)\n",
      " state (5)  A[0]:(0.160959050059) A[1]:(0.928876757622) A[2]:(-0.18390032649) A[3]:(0.51693969965)\n",
      " state (6)  A[0]:(-4.8965215683e-05) A[1]:(0.809947609901) A[2]:(6.00814819336e-05) A[3]:(0.656076431274)\n",
      " state (7)  A[0]:(0.635016798973) A[1]:(-0.252751678228) A[2]:(0.269600301981) A[3]:(0.891801834106)\n",
      " state (8)  A[0]:(0.655943274498) A[1]:(2.75187194347e-05) A[2]:(0.72904086113) A[3]:(0.590406119823)\n",
      " state (9)  A[0]:(0.655884861946) A[1]:(0.81002342701) A[2]:(0.810027360916) A[3]:(-0.000235795974731)\n",
      " state (10)  A[0]:(0.728894710541) A[1]:(0.900003254414) A[2]:(-0.000194907188416) A[3]:(0.728891015053)\n",
      " state (11)  A[0]:(0.518840491772) A[1]:(0.876916110516) A[2]:(-0.596397399902) A[3]:(0.843012690544)\n",
      " state (12)  A[0]:(0.0749193653464) A[1]:(0.824666142464) A[2]:(-0.556274533272) A[3]:(0.792543053627)\n",
      " state (13)  A[0]:(-0.000180721282959) A[1]:(0.809601068497) A[2]:(0.900027990341) A[3]:(0.728951573372)\n",
      " state (14)  A[0]:(0.81003767252) A[1]:(0.900279700756) A[2]:(0.999999880791) A[3]:(0.809944808483)\n",
      " state (15)  A[0]:(0.986567437649) A[1]:(0.959219157696) A[2]:(1.0) A[3]:(0.885715842247)\n",
      "Episode 500000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6051. Times reached goal: 991.               Steps done: 4159818. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.014049372928.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531526684761) A[1]:(0.590578317642) A[2]:(0.590574085712) A[3]:(0.531533122063)\n",
      " state (1)  A[0]:(0.531463384628) A[1]:(-7.53998756409e-06) A[2]:(0.656170606613) A[3]:(0.59049397707)\n",
      " state (2)  A[0]:(0.590511918068) A[1]:(0.729075670242) A[2]:(0.590519070625) A[3]:(0.6561845541)\n",
      " state (3)  A[0]:(0.656201601028) A[1]:(-0.22468726337) A[2]:(0.536126375198) A[3]:(0.520363688469)\n",
      " state (4)  A[0]:(0.590516209602) A[1]:(0.656219065189) A[2]:(2.14576721191e-05) A[3]:(0.531496047974)\n",
      " state (5)  A[0]:(0.160918042064) A[1]:(0.928909003735) A[2]:(-0.184004679322) A[3]:(0.517047524452)\n",
      " state (6)  A[0]:(-7.95125961304e-05) A[1]:(0.810007214546) A[2]:(7.65323638916e-05) A[3]:(0.656147003174)\n",
      " state (7)  A[0]:(0.635098814964) A[1]:(-0.252557903528) A[2]:(0.269819051027) A[3]:(0.891763567924)\n",
      " state (8)  A[0]:(0.656221270561) A[1]:(0.000130951404572) A[2]:(0.728990912437) A[3]:(0.590583086014)\n",
      " state (9)  A[0]:(0.656245589256) A[1]:(0.810025811195) A[2]:(0.809973239899) A[3]:(0.000188589096069)\n",
      " state (10)  A[0]:(0.72916674614) A[1]:(0.900007665157) A[2]:(-0.000293731689453) A[3]:(0.729040980339)\n",
      " state (11)  A[0]:(0.519239425659) A[1]:(0.876925885677) A[2]:(-0.596603751183) A[3]:(0.843090772629)\n",
      " state (12)  A[0]:(0.0753889754415) A[1]:(0.824679076672) A[2]:(-0.556692421436) A[3]:(0.792621672153)\n",
      " state (13)  A[0]:(0.000140100717545) A[1]:(0.809604287148) A[2]:(0.90001553297) A[3]:(0.729028701782)\n",
      " state (14)  A[0]:(0.810056626797) A[1]:(0.900266706944) A[2]:(0.999999880791) A[3]:(0.810008943081)\n",
      " state (15)  A[0]:(0.98654037714) A[1]:(0.95918828249) A[2]:(1.0) A[3]:(0.885710597038)\n",
      "Episode 501000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6033. Times reached goal: 992.               Steps done: 4165851. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0139648682259.\n",
      " state (0)  A[0]:(0.530977487564) A[1]:(0.590255618095) A[2]:(0.590237438679) A[3]:(0.531096458435)\n",
      " state (1)  A[0]:(0.530939459801) A[1]:(0.000310059636831) A[2]:(0.655829787254) A[3]:(0.590060353279)\n",
      " state (2)  A[0]:(0.590081751347) A[1]:(0.729014635086) A[2]:(0.589981079102) A[3]:(0.655828356743)\n",
      " state (3)  A[0]:(0.655692100525) A[1]:(-0.22457511723) A[2]:(0.535752773285) A[3]:(0.519977331161)\n",
      " state (4)  A[0]:(0.589994430542) A[1]:(0.656018972397) A[2]:(-0.000293493270874) A[3]:(0.531112194061)\n",
      " state (5)  A[0]:(0.160407200456) A[1]:(0.928896188736) A[2]:(-0.184409245849) A[3]:(0.516733169556)\n",
      " state (6)  A[0]:(-0.000378787488444) A[1]:(0.809995412827) A[2]:(-0.000285387039185) A[3]:(0.655923724174)\n",
      " state (7)  A[0]:(0.634754300117) A[1]:(-0.252159237862) A[2]:(0.269702345133) A[3]:(0.891620755196)\n",
      " state (8)  A[0]:(0.65582627058) A[1]:(0.00129863177426) A[2]:(0.729088544846) A[3]:(0.589985251427)\n",
      " state (9)  A[0]:(0.65597563982) A[1]:(0.810330450535) A[2]:(0.809979736805) A[3]:(0.00024189054966)\n",
      " state (10)  A[0]:(0.728869318962) A[1]:(0.900011897087) A[2]:(7.56978988647e-05) A[3]:(0.729051947594)\n",
      " state (11)  A[0]:(0.518772006035) A[1]:(0.876760959625) A[2]:(-0.596299886703) A[3]:(0.843034625053)\n",
      " state (12)  A[0]:(0.0749025940895) A[1]:(0.824228286743) A[2]:(-0.556684017181) A[3]:(0.792574167252)\n",
      " state (13)  A[0]:(-0.00029468536377) A[1]:(0.808851361275) A[2]:(0.899877607822) A[3]:(0.729031324387)\n",
      " state (14)  A[0]:(0.809947669506) A[1]:(0.899695336819) A[2]:(0.999999880791) A[3]:(0.810057640076)\n",
      " state (15)  A[0]:(0.986537277699) A[1]:(0.958880782127) A[2]:(1.0) A[3]:(0.885779857635)\n",
      "Episode 502000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6028. Times reached goal: 982.               Steps done: 4171879. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0138809412104.\n",
      " state (0)  A[0]:(0.531551837921) A[1]:(0.590344190598) A[2]:(0.590437531471) A[3]:(0.531257390976)\n",
      " state (1)  A[0]:(0.53158390522) A[1]:(3.61762940884e-05) A[2]:(0.656055748463) A[3]:(0.590318918228)\n",
      " state (2)  A[0]:(0.590699195862) A[1]:(0.729014098644) A[2]:(0.590442955494) A[3]:(0.656075358391)\n",
      " state (3)  A[0]:(0.656233549118) A[1]:(-0.22528770566) A[2]:(0.536198079586) A[3]:(0.520143687725)\n",
      " state (4)  A[0]:(0.59063565731) A[1]:(0.656121790409) A[2]:(3.37362289429e-05) A[3]:(0.531385660172)\n",
      " state (5)  A[0]:(0.161337032914) A[1]:(0.928910732269) A[2]:(-0.184105455875) A[3]:(0.517065644264)\n",
      " state (6)  A[0]:(0.000341922044754) A[1]:(0.810021519661) A[2]:(0.000135779380798) A[3]:(0.656029701233)\n",
      " state (7)  A[0]:(0.635138869286) A[1]:(-0.252630501986) A[2]:(0.270360350609) A[3]:(0.89159488678)\n",
      " state (8)  A[0]:(0.656304478645) A[1]:(4.39845025539e-05) A[2]:(0.729045152664) A[3]:(0.590537786484)\n",
      " state (9)  A[0]:(0.656275331974) A[1]:(0.81003433466) A[2]:(0.810024917126) A[3]:(0.000425457925303)\n",
      " state (10)  A[0]:(0.729186892509) A[1]:(0.900002241135) A[2]:(-0.000167727470398) A[3]:(0.729196071625)\n",
      " state (11)  A[0]:(0.519336521626) A[1]:(0.876907110214) A[2]:(-0.596886634827) A[3]:(0.843217968941)\n",
      " state (12)  A[0]:(0.0755310952663) A[1]:(0.824652373791) A[2]:(-0.557452380657) A[3]:(0.792787730694)\n",
      " state (13)  A[0]:(0.000231266021729) A[1]:(0.809598088264) A[2]:(0.89998883009) A[3]:(0.729217767715)\n",
      " state (14)  A[0]:(0.810096919537) A[1]:(0.900298714638) A[2]:(0.999999880791) A[3]:(0.810147881508)\n",
      " state (15)  A[0]:(0.986502885818) A[1]:(0.95918482542) A[2]:(1.0) A[3]:(0.885689139366)\n",
      "Episode 503000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6057. Times reached goal: 993.               Steps done: 4177936. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0137971184629.\n",
      " state (0)  A[0]:(0.531239390373) A[1]:(0.590405523777) A[2]:(0.590384960175) A[3]:(0.531379580498)\n",
      " state (1)  A[0]:(0.531301021576) A[1]:(-9.28044319153e-05) A[2]:(0.655964493752) A[3]:(0.590386152267)\n",
      " state (2)  A[0]:(0.590529322624) A[1]:(0.729015827179) A[2]:(0.59041082859) A[3]:(0.656073212624)\n",
      " state (3)  A[0]:(0.65637075901) A[1]:(-0.225821629167) A[2]:(0.536005735397) A[3]:(0.519977092743)\n",
      " state (4)  A[0]:(0.591003298759) A[1]:(0.656145274639) A[2]:(-0.000643134000711) A[3]:(0.531262516975)\n",
      " state (5)  A[0]:(0.161996617913) A[1]:(0.928910017014) A[2]:(-0.184921979904) A[3]:(0.516999959946)\n",
      " state (6)  A[0]:(0.00117999257054) A[1]:(0.810001909733) A[2]:(-0.000687003019266) A[3]:(0.655969381332)\n",
      " state (7)  A[0]:(0.635843873024) A[1]:(-0.252676159143) A[2]:(0.269678771496) A[3]:(0.891576528549)\n",
      " state (8)  A[0]:(0.657264471054) A[1]:(-0.000235676765442) A[2]:(0.72832942009) A[3]:(0.591063857079)\n",
      " state (9)  A[0]:(0.657214641571) A[1]:(0.809898257256) A[2]:(0.809549152851) A[3]:(0.000830724660773)\n",
      " state (10)  A[0]:(0.729817867279) A[1]:(0.899919390678) A[2]:(-0.00111901713535) A[3]:(0.729086279869)\n",
      " state (11)  A[0]:(0.520214676857) A[1]:(0.876785695553) A[2]:(-0.597543716431) A[3]:(0.843092143536)\n",
      " state (12)  A[0]:(0.0766324326396) A[1]:(0.824447095394) A[2]:(-0.558324038982) A[3]:(0.792598128319)\n",
      " state (13)  A[0]:(0.00125554134138) A[1]:(0.80932277441) A[2]:(0.899831295013) A[3]:(0.728954911232)\n",
      " state (14)  A[0]:(0.810446977615) A[1]:(0.900107145309) A[2]:(0.999999880791) A[3]:(0.809940397739)\n",
      " state (15)  A[0]:(0.986512303352) A[1]:(0.959069788456) A[2]:(1.0) A[3]:(0.885505437851)\n",
      "Episode 504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6025. Times reached goal: 983.               Steps done: 4183961. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.013714240744.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5905,  0.5903,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.6558, -0.0000,  0.5318]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.0001,  0.7289,  0.5911]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6562,  0.8101,  0.8101,  0.0010]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0005,  0.8093,  0.9000,  0.7294]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9001,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531474947929) A[1]:(0.590509176254) A[2]:(0.590272426605) A[3]:(0.531361579895)\n",
      " state (1)  A[0]:(0.531559824944) A[1]:(0.000141896307468) A[2]:(0.655948877335) A[3]:(0.590247869492)\n",
      " state (2)  A[0]:(0.590588927269) A[1]:(0.728942513466) A[2]:(0.590554893017) A[3]:(0.655899524689)\n",
      " state (3)  A[0]:(0.656111419201) A[1]:(-0.226117700338) A[2]:(0.536489963531) A[3]:(0.520135164261)\n",
      " state (4)  A[0]:(0.590489268303) A[1]:(0.655930280685) A[2]:(-1.25169754028e-05) A[3]:(0.531565546989)\n",
      " state (5)  A[0]:(0.16108879447) A[1]:(0.928913831711) A[2]:(-0.184587731957) A[3]:(0.517354130745)\n",
      " state (6)  A[0]:(3.06665897369e-05) A[1]:(0.810061514378) A[2]:(-0.000167369842529) A[3]:(0.656129002571)\n",
      " state (7)  A[0]:(0.634688019753) A[1]:(-0.252298772335) A[2]:(0.270808577538) A[3]:(0.891471505165)\n",
      " state (8)  A[0]:(0.65591275692) A[1]:(0.000158466398716) A[2]:(0.728982031345) A[3]:(0.590526938438)\n",
      " state (9)  A[0]:(0.655940055847) A[1]:(0.810003459454) A[2]:(0.810020267963) A[3]:(8.20755958557e-05)\n",
      " state (10)  A[0]:(0.729020237923) A[1]:(0.900008440018) A[2]:(-0.000117778778076) A[3]:(0.728947579861)\n",
      " state (11)  A[0]:(0.519260704517) A[1]:(0.876912474632) A[2]:(-0.59719234705) A[3]:(0.843091607094)\n",
      " state (12)  A[0]:(0.0754987373948) A[1]:(0.824612081051) A[2]:(-0.558191299438) A[3]:(0.79261559248)\n",
      " state (13)  A[0]:(0.000111430883408) A[1]:(0.809461474419) A[2]:(0.899954378605) A[3]:(0.728977620602)\n",
      " state (14)  A[0]:(0.810006737709) A[1]:(0.900149345398) A[2]:(0.999999880791) A[3]:(0.810002803802)\n",
      " state (15)  A[0]:(0.98645311594) A[1]:(0.959053337574) A[2]:(1.0) A[3]:(0.885527729988)\n",
      "Episode 505000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6058. Times reached goal: 989.               Steps done: 4190019. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0136314110181.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53185403347) A[1]:(0.590805113316) A[2]:(0.590658783913) A[3]:(0.531018733978)\n",
      " state (1)  A[0]:(0.53167206049) A[1]:(-0.000142686069012) A[2]:(0.65630710125) A[3]:(0.590481162071)\n",
      " state (2)  A[0]:(0.590690255165) A[1]:(0.729133844376) A[2]:(0.590897619724) A[3]:(0.656286120415)\n",
      " state (3)  A[0]:(0.656366229057) A[1]:(-0.224517151713) A[2]:(0.536477446556) A[3]:(0.520671784878)\n",
      " state (4)  A[0]:(0.590758562088) A[1]:(0.656281471252) A[2]:(6.87837600708e-05) A[3]:(0.531960010529)\n",
      " state (5)  A[0]:(0.16139857471) A[1]:(0.92893153429) A[2]:(-0.184474423528) A[3]:(0.517661690712)\n",
      " state (6)  A[0]:(0.000514179409947) A[1]:(0.809979081154) A[2]:(-1.71661376953e-05) A[3]:(0.656175851822)\n",
      " state (7)  A[0]:(0.635130226612) A[1]:(-0.25261503458) A[2]:(0.271085381508) A[3]:(0.89134311676)\n",
      " state (8)  A[0]:(0.656300842762) A[1]:(9.07555222511e-05) A[2]:(0.729106783867) A[3]:(0.590020418167)\n",
      " state (9)  A[0]:(0.656242251396) A[1]:(0.810023546219) A[2]:(0.81003433466) A[3]:(-0.00016950070858)\n",
      " state (10)  A[0]:(0.729115128517) A[1]:(0.900004267693) A[2]:(-0.000356912583811) A[3]:(0.728949308395)\n",
      " state (11)  A[0]:(0.51922416687) A[1]:(0.876896440983) A[2]:(-0.597594261169) A[3]:(0.843072772026)\n",
      " state (12)  A[0]:(0.0752820000052) A[1]:(0.824595332146) A[2]:(-0.558856487274) A[3]:(0.792527079582)\n",
      " state (13)  A[0]:(-5.87999820709e-05) A[1]:(0.809478998184) A[2]:(0.899915874004) A[3]:(0.728760898113)\n",
      " state (14)  A[0]:(0.810116052628) A[1]:(0.900200307369) A[2]:(0.999999880791) A[3]:(0.809732496738)\n",
      " state (15)  A[0]:(0.986450314522) A[1]:(0.959072768688) A[2]:(1.0) A[3]:(0.885218560696)\n",
      "Episode 506000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6039. Times reached goal: 992.               Steps done: 4196058. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0135493389929.\n",
      " state (0)  A[0]:(0.531319260597) A[1]:(0.59052836895) A[2]:(0.590494036674) A[3]:(0.531058311462)\n",
      " state (1)  A[0]:(0.531496703625) A[1]:(3.51630151272e-05) A[2]:(0.656108677387) A[3]:(0.590087294579)\n",
      " state (2)  A[0]:(0.590669512749) A[1]:(0.729087948799) A[2]:(0.590526461601) A[3]:(0.655809819698)\n",
      " state (3)  A[0]:(0.656426429749) A[1]:(-0.224041551352) A[2]:(0.536337077618) A[3]:(0.519751727581)\n",
      " state (4)  A[0]:(0.590954601765) A[1]:(0.656200110912) A[2]:(7.08103179932e-05) A[3]:(0.530938029289)\n",
      " state (5)  A[0]:(0.161888927221) A[1]:(0.928944587708) A[2]:(-0.184633791447) A[3]:(0.516814827919)\n",
      " state (6)  A[0]:(0.00115930987522) A[1]:(0.810055613518) A[2]:(-7.78436660767e-05) A[3]:(0.655731141567)\n",
      " state (7)  A[0]:(0.635429084301) A[1]:(-0.252158343792) A[2]:(0.271305501461) A[3]:(0.891253590584)\n",
      " state (8)  A[0]:(0.656735181808) A[1]:(0.000521983893123) A[2]:(0.729041576385) A[3]:(0.590011954308)\n",
      " state (9)  A[0]:(0.656837463379) A[1]:(0.810106515884) A[2]:(0.810050368309) A[3]:(-0.00064836430829)\n",
      " state (10)  A[0]:(0.729671478271) A[1]:(0.900041162968) A[2]:(-8.70227813721e-06) A[3]:(0.72865742445)\n",
      " state (11)  A[0]:(0.520211219788) A[1]:(0.876920700073) A[2]:(-0.59742975235) A[3]:(0.842944741249)\n",
      " state (12)  A[0]:(0.0767257288098) A[1]:(0.824581205845) A[2]:(-0.558780431747) A[3]:(0.792434334755)\n",
      " state (13)  A[0]:(0.00141620542854) A[1]:(0.809399187565) A[2]:(0.900098621845) A[3]:(0.72875636816)\n",
      " state (14)  A[0]:(0.810580849648) A[1]:(0.900116562843) A[2]:(0.999999880791) A[3]:(0.80986982584)\n",
      " state (15)  A[0]:(0.986456215382) A[1]:(0.95899528265) A[2]:(1.0) A[3]:(0.885328233242)\n",
      "Episode 507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6023. Times reached goal: 987.               Steps done: 4202081. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.013467976593.\n",
      " state (0)  A[0]:(0.531537413597) A[1]:(0.590726971626) A[2]:(0.590303003788) A[3]:(0.531377315521)\n",
      " state (1)  A[0]:(0.531359314919) A[1]:(-0.000546630413737) A[2]:(0.656061887741) A[3]:(0.590307831764)\n",
      " state (2)  A[0]:(0.590339541435) A[1]:(0.728925704956) A[2]:(0.591752052307) A[3]:(0.655773460865)\n",
      " state (3)  A[0]:(0.656160950661) A[1]:(-0.230750799179) A[2]:(0.538012325764) A[3]:(0.519566297531)\n",
      " state (4)  A[0]:(0.590740740299) A[1]:(0.656175017357) A[2]:(0.000497460307088) A[3]:(0.531522810459)\n",
      " state (5)  A[0]:(0.161235064268) A[1]:(0.928950428963) A[2]:(-0.184466019273) A[3]:(0.517419159412)\n",
      " state (6)  A[0]:(0.000131338834763) A[1]:(0.809961557388) A[2]:(8.79764556885e-05) A[3]:(0.656150221825)\n",
      " state (7)  A[0]:(0.634837090969) A[1]:(-0.253004133701) A[2]:(0.271744310856) A[3]:(0.891421437263)\n",
      " state (8)  A[0]:(0.656283378601) A[1]:(-0.000600244791713) A[2]:(0.729030549526) A[3]:(0.590731263161)\n",
      " state (9)  A[0]:(0.656311511993) A[1]:(0.809890568256) A[2]:(0.809954285622) A[3]:(8.34912061691e-05)\n",
      " state (10)  A[0]:(0.72921872139) A[1]:(0.899994134903) A[2]:(-0.000842571083922) A[3]:(0.729118406773)\n",
      " state (11)  A[0]:(0.519441366196) A[1]:(0.876924753189) A[2]:(-0.598405003548) A[3]:(0.843285799026)\n",
      " state (12)  A[0]:(0.0755215808749) A[1]:(0.824690461159) A[2]:(-0.560033798218) A[3]:(0.792845904827)\n",
      " state (13)  A[0]:(0.000261425971985) A[1]:(0.809692144394) A[2]:(0.900048136711) A[3]:(0.729187786579)\n",
      " state (14)  A[0]:(0.810313344002) A[1]:(0.900416314602) A[2]:(0.999999880791) A[3]:(0.81007361412)\n",
      " state (15)  A[0]:(0.98640525341) A[1]:(0.959152519703) A[2]:(1.0) A[3]:(0.885271310806)\n",
      "Episode 508000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6021. Times reached goal: 993.               Steps done: 4208102. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0133871295402.\n",
      " state (0)  A[0]:(0.531823992729) A[1]:(0.590413093567) A[2]:(0.590498924255) A[3]:(0.531486988068)\n",
      " state (1)  A[0]:(0.531638503075) A[1]:(-0.00028495118022) A[2]:(0.656015634537) A[3]:(0.590709447861)\n",
      " state (2)  A[0]:(0.590551018715) A[1]:(0.728959083557) A[2]:(0.590733289719) A[3]:(0.656437456608)\n",
      " state (3)  A[0]:(0.656006574631) A[1]:(-0.226158067584) A[2]:(0.536745131016) A[3]:(0.520510435104)\n",
      " state (4)  A[0]:(0.590258359909) A[1]:(0.656094133854) A[2]:(0.000168919563293) A[3]:(0.532060086727)\n",
      " state (5)  A[0]:(0.160626173019) A[1]:(0.928920030594) A[2]:(-0.184577018023) A[3]:(0.518200993538)\n",
      " state (6)  A[0]:(-0.000138461589813) A[1]:(0.809933781624) A[2]:(0.000208735466003) A[3]:(0.656800150871)\n",
      " state (7)  A[0]:(0.634768128395) A[1]:(-0.25273412466) A[2]:(0.272035747766) A[3]:(0.891572535038)\n",
      " state (8)  A[0]:(0.656248271465) A[1]:(-0.000182639807463) A[2]:(0.729013562202) A[3]:(0.591621696949)\n",
      " state (9)  A[0]:(0.656217932701) A[1]:(0.809941351414) A[2]:(0.809971988201) A[3]:(0.00182047288399)\n",
      " state (10)  A[0]:(0.729125499725) A[1]:(0.899971723557) A[2]:(-0.000358223885996) A[3]:(0.729826688766)\n",
      " state (11)  A[0]:(0.519384622574) A[1]:(0.876840233803) A[2]:(-0.598068654537) A[3]:(0.843702733517)\n",
      " state (12)  A[0]:(0.0755173116922) A[1]:(0.824477791786) A[2]:(-0.559921383858) A[3]:(0.793395698071)\n",
      " state (13)  A[0]:(-4.32133674622e-06) A[1]:(0.809322118759) A[2]:(0.89999628067) A[3]:(0.729899704456)\n",
      " state (14)  A[0]:(0.809990286827) A[1]:(0.900123775005) A[2]:(0.999999880791) A[3]:(0.810632705688)\n",
      " state (15)  A[0]:(0.986356675625) A[1]:(0.958985209465) A[2]:(1.0) A[3]:(0.885641276836)\n",
      "Episode 509000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               5997. Times reached goal: 974.               Steps done: 4214099. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0133070871713.\n",
      "q_values \n",
      "tensor([[ 0.5331,  0.5912,  0.5912,  0.5336]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5911,  0.6567, -0.0000,  0.5329]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6572, -0.0001,  0.7291,  0.5917]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6570,  0.8101,  0.8101,  0.0005]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  0.8092,  0.9000,  0.7293]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8105,  0.9002,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533277869225) A[1]:(0.591180801392) A[2]:(0.591147899628) A[3]:(0.533464431763)\n",
      " state (1)  A[0]:(0.532817602158) A[1]:(-0.000288650393486) A[2]:(0.656487524509) A[3]:(0.591340661049)\n",
      " state (2)  A[0]:(0.5917558074) A[1]:(0.729132652283) A[2]:(0.591248810291) A[3]:(0.656654834747)\n",
      " state (3)  A[0]:(0.657032847404) A[1]:(-0.226856872439) A[2]:(0.536918878555) A[3]:(0.521169483662)\n",
      " state (4)  A[0]:(0.59126830101) A[1]:(0.656668305397) A[2]:(-3.7670135498e-05) A[3]:(0.532890319824)\n",
      " state (5)  A[0]:(0.16155359149) A[1]:(0.928916811943) A[2]:(-0.184532463551) A[3]:(0.518811583519)\n",
      " state (6)  A[0]:(-6.63697719574e-05) A[1]:(0.810050666332) A[2]:(-0.000190615653992) A[3]:(0.657163977623)\n",
      " state (7)  A[0]:(0.635422468185) A[1]:(-0.252570539713) A[2]:(0.271360814571) A[3]:(0.891864061356)\n",
      " state (8)  A[0]:(0.657359421253) A[1]:(-7.08848237991e-05) A[2]:(0.729085803032) A[3]:(0.591653108597)\n",
      " state (9)  A[0]:(0.657040059566) A[1]:(0.810153603554) A[2]:(0.810048699379) A[3]:(0.000502496899571)\n",
      " state (10)  A[0]:(0.729525864124) A[1]:(0.900002062321) A[2]:(-0.000629305781331) A[3]:(0.72911220789)\n",
      " state (11)  A[0]:(0.519709229469) A[1]:(0.876789212227) A[2]:(-0.59856659174) A[3]:(0.843231499195)\n",
      " state (12)  A[0]:(0.0757896304131) A[1]:(0.824355483055) A[2]:(-0.56066763401) A[3]:(0.792810499668)\n",
      " state (13)  A[0]:(0.00053742522141) A[1]:(0.809234023094) A[2]:(0.900012433529) A[3]:(0.729249715805)\n",
      " state (14)  A[0]:(0.810460150242) A[1]:(0.900165557861) A[2]:(0.999999940395) A[3]:(0.810232996941)\n",
      " state (15)  A[0]:(0.986386835575) A[1]:(0.959023892879) A[2]:(1.0) A[3]:(0.885359585285)\n",
      "Episode 510000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6048. Times reached goal: 990.               Steps done: 4220147. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0132268487935.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531507134438) A[1]:(0.590532362461) A[2]:(0.590558409691) A[3]:(0.53158146143)\n",
      " state (1)  A[0]:(0.531531751156) A[1]:(-3.86610627174e-05) A[2]:(0.656114339828) A[3]:(0.590584635735)\n",
      " state (2)  A[0]:(0.590690374374) A[1]:(0.728986144066) A[2]:(0.590506851673) A[3]:(0.656251966953)\n",
      " state (3)  A[0]:(0.656212687492) A[1]:(-0.224404200912) A[2]:(0.536303281784) A[3]:(0.520287394524)\n",
      " state (4)  A[0]:(0.590611636639) A[1]:(0.656255483627) A[2]:(-5.14984130859e-05) A[3]:(0.531559705734)\n",
      " state (5)  A[0]:(0.161374688148) A[1]:(0.928912758827) A[2]:(-0.184701725841) A[3]:(0.517614722252)\n",
      " state (6)  A[0]:(0.000400602788432) A[1]:(0.8099604249) A[2]:(-1.26361846924e-05) A[3]:(0.656199753284)\n",
      " state (7)  A[0]:(0.634896159172) A[1]:(-0.252643615007) A[2]:(0.271803021431) A[3]:(0.891294002533)\n",
      " state (8)  A[0]:(0.656514763832) A[1]:(9.69097018242e-05) A[2]:(0.728945851326) A[3]:(0.590577661991)\n",
      " state (9)  A[0]:(0.656434595585) A[1]:(0.81007707119) A[2]:(0.809958994389) A[3]:(0.000111699104309)\n",
      " state (10)  A[0]:(0.729216873646) A[1]:(0.899993538857) A[2]:(-0.000166773796082) A[3]:(0.728985488415)\n",
      " state (11)  A[0]:(0.519505262375) A[1]:(0.876830756664) A[2]:(-0.598045945168) A[3]:(0.843158721924)\n",
      " state (12)  A[0]:(0.0757176280022) A[1]:(0.824444174767) A[2]:(-0.56019371748) A[3]:(0.792703449726)\n",
      " state (13)  A[0]:(0.0002201795578) A[1]:(0.809281229973) A[2]:(0.899982511997) A[3]:(0.729045510292)\n",
      " state (14)  A[0]:(0.810098409653) A[1]:(0.900114357471) A[2]:(0.999999940395) A[3]:(0.810013651848)\n",
      " state (15)  A[0]:(0.986346244812) A[1]:(0.958970367908) A[2]:(1.0) A[3]:(0.885202229023)\n",
      "Episode 511000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6031. Times reached goal: 984.               Steps done: 4226178. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0131473177354.\n",
      " state (0)  A[0]:(0.53132468462) A[1]:(0.590481638908) A[2]:(0.590487003326) A[3]:(0.531516194344)\n",
      " state (1)  A[0]:(0.531286358833) A[1]:(3.67201864719e-05) A[2]:(0.656019926071) A[3]:(0.590357840061)\n",
      " state (2)  A[0]:(0.590387940407) A[1]:(0.729075193405) A[2]:(0.590725064278) A[3]:(0.655937552452)\n",
      " state (3)  A[0]:(0.656076848507) A[1]:(-0.225230365992) A[2]:(0.536749958992) A[3]:(0.520196676254)\n",
      " state (4)  A[0]:(0.590539216995) A[1]:(0.656316876411) A[2]:(0.000221490859985) A[3]:(0.531656861305)\n",
      " state (5)  A[0]:(0.161232486367) A[1]:(0.928960740566) A[2]:(-0.184653252363) A[3]:(0.517646372318)\n",
      " state (6)  A[0]:(0.000185340642929) A[1]:(0.810119509697) A[2]:(0.000101685523987) A[3]:(0.656113386154)\n",
      " state (7)  A[0]:(0.63472032547) A[1]:(-0.252248376608) A[2]:(0.272236764431) A[3]:(0.891220629215)\n",
      " state (8)  A[0]:(0.656368732452) A[1]:(7.57910311222e-05) A[2]:(0.729049801826) A[3]:(0.590561509132)\n",
      " state (9)  A[0]:(0.656239151955) A[1]:(0.810035705566) A[2]:(0.810044646263) A[3]:(-2.35885381699e-05)\n",
      " state (10)  A[0]:(0.72907435894) A[1]:(0.900045633316) A[2]:(-8.76188278198e-05) A[3]:(0.729010879993)\n",
      " state (11)  A[0]:(0.519284009933) A[1]:(0.876962840557) A[2]:(-0.598176002502) A[3]:(0.843214392662)\n",
      " state (12)  A[0]:(0.0753144770861) A[1]:(0.824685335159) A[2]:(-0.560419857502) A[3]:(0.792741537094)\n",
      " state (13)  A[0]:(-0.000255137681961) A[1]:(0.809574782848) A[2]:(0.900020241737) A[3]:(0.729024887085)\n",
      " state (14)  A[0]:(0.809958279133) A[1]:(0.900271296501) A[2]:(0.999999940395) A[3]:(0.809958338737)\n",
      " state (15)  A[0]:(0.986322462559) A[1]:(0.959018230438) A[2]:(1.0) A[3]:(0.885108292103)\n",
      "Episode 512000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6031. Times reached goal: 989.               Steps done: 4232209. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0130682648856.\n",
      " state (0)  A[0]:(0.531654715538) A[1]:(0.590342760086) A[2]:(0.590371251106) A[3]:(0.531743049622)\n",
      " state (1)  A[0]:(0.531623005867) A[1]:(-0.000177156180143) A[2]:(0.656085014343) A[3]:(0.5909075737)\n",
      " state (2)  A[0]:(0.590678751469) A[1]:(0.728868365288) A[2]:(0.590731024742) A[3]:(0.656646132469)\n",
      " state (3)  A[0]:(0.656345844269) A[1]:(-0.224790975451) A[2]:(0.53664457798) A[3]:(0.521288633347)\n",
      " state (4)  A[0]:(0.590761125088) A[1]:(0.655776262283) A[2]:(0.000447154016001) A[3]:(0.532832920551)\n",
      " state (5)  A[0]:(0.161567211151) A[1]:(0.928829312325) A[2]:(-0.184323683381) A[3]:(0.519122481346)\n",
      " state (6)  A[0]:(0.00063079589745) A[1]:(0.809818089008) A[2]:(0.000508427561726) A[3]:(0.657425045967)\n",
      " state (7)  A[0]:(0.635037660599) A[1]:(-0.252888649702) A[2]:(0.272454738617) A[3]:(0.891761720181)\n",
      " state (8)  A[0]:(0.65704035759) A[1]:(-0.000596206577029) A[2]:(0.728813529015) A[3]:(0.593144416809)\n",
      " state (9)  A[0]:(0.657126307487) A[1]:(0.809800505638) A[2]:(0.80997979641) A[3]:(0.00397976627573)\n",
      " state (10)  A[0]:(0.729952335358) A[1]:(0.899885177612) A[2]:(0.000238060951233) A[3]:(0.730664551258)\n",
      " state (11)  A[0]:(0.520953536034) A[1]:(0.876706004143) A[2]:(-0.597947955132) A[3]:(0.844220638275)\n",
      " state (12)  A[0]:(0.0778938382864) A[1]:(0.82421541214) A[2]:(-0.560458183289) A[3]:(0.794059932232)\n",
      " state (13)  A[0]:(0.00228267512284) A[1]:(0.808889746666) A[2]:(0.899791955948) A[3]:(0.730676531792)\n",
      " state (14)  A[0]:(0.810715556145) A[1]:(0.899769544601) A[2]:(0.999999940395) A[3]:(0.811127901077)\n",
      " state (15)  A[0]:(0.986384868622) A[1]:(0.958760678768) A[2]:(1.0) A[3]:(0.885846912861)\n",
      "Episode 513000 finished after 0 timesteps with r=0.0. Running score: 0.97. Times trained:               6046. Times reached goal: 984.               Steps done: 4238255. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0129894925249.\n",
      " state (0)  A[0]:(0.531750261784) A[1]:(0.590496182442) A[2]:(0.590520143509) A[3]:(0.531452000141)\n",
      " state (1)  A[0]:(0.531795620918) A[1]:(-8.69929790497e-05) A[2]:(0.656211018562) A[3]:(0.590570926666)\n",
      " state (2)  A[0]:(0.590806126595) A[1]:(0.72915160656) A[2]:(0.590729892254) A[3]:(0.656280636787)\n",
      " state (3)  A[0]:(0.656448602676) A[1]:(-0.222450047731) A[2]:(0.536259531975) A[3]:(0.520357728004)\n",
      " state (4)  A[0]:(0.590761840343) A[1]:(0.656137764454) A[2]:(6.90221786499e-05) A[3]:(0.531500697136)\n",
      " state (5)  A[0]:(0.161426261067) A[1]:(0.928915739059) A[2]:(-0.184854403138) A[3]:(0.517741680145)\n",
      " state (6)  A[0]:(0.000326722860336) A[1]:(0.810045778751) A[2]:(-5.69820404053e-05) A[3]:(0.65630787611)\n",
      " state (7)  A[0]:(0.634659349918) A[1]:(-0.252141356468) A[2]:(0.272267758846) A[3]:(0.891251385212)\n",
      " state (8)  A[0]:(0.656412363052) A[1]:(0.000341296195984) A[2]:(0.729082465172) A[3]:(0.590775609016)\n",
      " state (9)  A[0]:(0.656509637833) A[1]:(0.810020685196) A[2]:(0.810075223446) A[3]:(0.000652059796266)\n",
      " state (10)  A[0]:(0.729358315468) A[1]:(0.899966359138) A[2]:(7.164478302e-05) A[3]:(0.729264974594)\n",
      " state (11)  A[0]:(0.519796490669) A[1]:(0.87679207325) A[2]:(-0.598195910454) A[3]:(0.843340992928)\n",
      " state (12)  A[0]:(0.0760569274426) A[1]:(0.824335336685) A[2]:(-0.560740470886) A[3]:(0.79290497303)\n",
      " state (13)  A[0]:(0.000347942113876) A[1]:(0.809038937092) A[2]:(0.899908363819) A[3]:(0.729263305664)\n",
      " state (14)  A[0]:(0.81005358696) A[1]:(0.899866104126) A[2]:(0.999999940395) A[3]:(0.810217261314)\n",
      " state (15)  A[0]:(0.986309885979) A[1]:(0.958787381649) A[2]:(1.0) A[3]:(0.885307192802)\n",
      "Episode 514000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6027. Times reached goal: 982.               Steps done: 4244282. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0129114403001.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5907,  0.5906,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5900,  0.6561,  0.0000,  0.5317]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556,  0.0004,  0.7294,  0.5904]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556,  0.8100,  0.8098,  0.0005]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0015,  0.8096,  0.9000,  0.7290]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8094,  0.9000,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531921267509) A[1]:(0.590727925301) A[2]:(0.590622425079) A[3]:(0.531390190125)\n",
      " state (1)  A[0]:(0.531851172447) A[1]:(6.3207000494e-05) A[2]:(0.656140804291) A[3]:(0.590499341488)\n",
      " state (2)  A[0]:(0.590894043446) A[1]:(0.729124009609) A[2]:(0.590601921082) A[3]:(0.656184434891)\n",
      " state (3)  A[0]:(0.656457602978) A[1]:(-0.223437085748) A[2]:(0.536460995674) A[3]:(0.52036690712)\n",
      " state (4)  A[0]:(0.590865969658) A[1]:(0.656466007233) A[2]:(0.000172972679138) A[3]:(0.53165781498)\n",
      " state (5)  A[0]:(0.161681383848) A[1]:(0.92898774147) A[2]:(-0.184724286199) A[3]:(0.517846107483)\n",
      " state (6)  A[0]:(0.000567346753087) A[1]:(0.810178101063) A[2]:(0.000172972679138) A[3]:(0.656378209591)\n",
      " state (7)  A[0]:(0.634671926498) A[1]:(-0.251903235912) A[2]:(0.272661298513) A[3]:(0.891275048256)\n",
      " state (8)  A[0]:(0.656184613705) A[1]:(0.000721093150787) A[2]:(0.72923386097) A[3]:(0.590712785721)\n",
      " state (9)  A[0]:(0.655992150307) A[1]:(0.810213446617) A[2]:(0.810144543648) A[3]:(0.000258773565292)\n",
      " state (10)  A[0]:(0.728792309761) A[1]:(0.900098323822) A[2]:(0.0002361536026) A[3]:(0.729092657566)\n",
      " state (11)  A[0]:(0.518804013729) A[1]:(0.877000153065) A[2]:(-0.598155260086) A[3]:(0.843249857426)\n",
      " state (12)  A[0]:(0.0746281370521) A[1]:(0.824716925621) A[2]:(-0.560684800148) A[3]:(0.792792201042)\n",
      " state (13)  A[0]:(-0.000966787047219) A[1]:(0.809586644173) A[2]:(0.900126874447) A[3]:(0.729120373726)\n",
      " state (14)  A[0]:(0.809739470482) A[1]:(0.9002597332) A[2]:(0.999999940395) A[3]:(0.810096502304)\n",
      " state (15)  A[0]:(0.986270904541) A[1]:(0.95897424221) A[2]:(1.0) A[3]:(0.885147631168)\n",
      "Episode 515000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6034. Times reached goal: 986.               Steps done: 4250316. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0128337672446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.528517842293) A[1]:(0.590371608734) A[2]:(0.590403556824) A[3]:(0.532490253448)\n",
      " state (1)  A[0]:(0.529094576836) A[1]:(-4.24198806286e-05) A[2]:(0.655929744244) A[3]:(0.590190410614)\n",
      " state (2)  A[0]:(0.588753402233) A[1]:(0.72906935215) A[2]:(0.590380191803) A[3]:(0.655103325844)\n",
      " state (3)  A[0]:(0.654944896698) A[1]:(-0.223327100277) A[2]:(0.536185383797) A[3]:(0.517130851746)\n",
      " state (4)  A[0]:(0.589555025101) A[1]:(0.655816316605) A[2]:(-5.13792037964e-05) A[3]:(0.527256906033)\n",
      " state (5)  A[0]:(0.160223945975) A[1]:(0.928867280483) A[2]:(-0.18510825932) A[3]:(0.51281696558)\n",
      " state (6)  A[0]:(-0.00103828276042) A[1]:(0.810109376907) A[2]:(-0.000369071931345) A[3]:(0.652040183544)\n",
      " state (7)  A[0]:(0.633937120438) A[1]:(-0.252185314894) A[2]:(0.272295355797) A[3]:(0.889652132988)\n",
      " state (8)  A[0]:(0.65624165535) A[1]:(-0.000265877693892) A[2]:(0.728991389275) A[3]:(0.585732221603)\n",
      " state (9)  A[0]:(0.656625390053) A[1]:(0.80988740921) A[2]:(0.809972286224) A[3]:(-0.0077325203456)\n",
      " state (10)  A[0]:(0.729712367058) A[1]:(0.899942040443) A[2]:(-0.000535607279744) A[3]:(0.725414276123)\n",
      " state (11)  A[0]:(0.520682930946) A[1]:(0.876825630665) A[2]:(-0.598948121071) A[3]:(0.841037034988)\n",
      " state (12)  A[0]:(0.0774699449539) A[1]:(0.824482262135) A[2]:(-0.562154769897) A[3]:(0.789931058884)\n",
      " state (13)  A[0]:(0.00150760891847) A[1]:(0.80929094553) A[2]:(0.89920771122) A[3]:(0.725348353386)\n",
      " state (14)  A[0]:(0.810291826725) A[1]:(0.900056123734) A[2]:(0.999999940395) A[3]:(0.807116687298)\n",
      " state (15)  A[0]:(0.986329555511) A[1]:(0.958907604218) A[2]:(1.0) A[3]:(0.883234858513)\n",
      "Episode 516000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6032. Times reached goal: 986.               Steps done: 4256348. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0127565869703.\n",
      " state (0)  A[0]:(0.532418251038) A[1]:(0.590534448624) A[2]:(0.590505838394) A[3]:(0.530804932117)\n",
      " state (1)  A[0]:(0.532153904438) A[1]:(0.000134371221066) A[2]:(0.656093478203) A[3]:(0.589989542961)\n",
      " state (2)  A[0]:(0.590963602066) A[1]:(0.729073882103) A[2]:(0.59047806263) A[3]:(0.655768752098)\n",
      " state (3)  A[0]:(0.656092405319) A[1]:(-0.222698524594) A[2]:(0.536397755146) A[3]:(0.51938700676)\n",
      " state (4)  A[0]:(0.590190410614) A[1]:(0.656220912933) A[2]:(0.000446557969553) A[3]:(0.530554711819)\n",
      " state (5)  A[0]:(0.16070073843) A[1]:(0.928944945335) A[2]:(-0.184455305338) A[3]:(0.517057299614)\n",
      " state (6)  A[0]:(-0.000543147267308) A[1]:(0.810110270977) A[2]:(0.000574707926717) A[3]:(0.656133234501)\n",
      " state (7)  A[0]:(0.633802592754) A[1]:(-0.252196669579) A[2]:(0.273199826479) A[3]:(0.891372740269)\n",
      " state (8)  A[0]:(0.655394673347) A[1]:(0.000142507255077) A[2]:(0.729145526886) A[3]:(0.591484665871)\n",
      " state (9)  A[0]:(0.655059218407) A[1]:(0.810005605221) A[2]:(0.810073971748) A[3]:(0.00048284229706)\n",
      " state (10)  A[0]:(0.728129863739) A[1]:(0.899969637394) A[2]:(0.000169157981873) A[3]:(0.729102611542)\n",
      " state (11)  A[0]:(0.51811337471) A[1]:(0.876813769341) A[2]:(-0.598361074924) A[3]:(0.843368649483)\n",
      " state (12)  A[0]:(0.0740203559399) A[1]:(0.824402928352) A[2]:(-0.561240315437) A[3]:(0.793082416058)\n",
      " state (13)  A[0]:(-0.00157365074847) A[1]:(0.809156000614) A[2]:(0.899900972843) A[3]:(0.729613661766)\n",
      " state (14)  A[0]:(0.809448301792) A[1]:(0.899956762791) A[2]:(0.999999940395) A[3]:(0.810562133789)\n",
      " state (15)  A[0]:(0.986232817173) A[1]:(0.958811163902) A[2]:(1.0) A[3]:(0.885479211807)\n",
      "Episode 517000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6036. Times reached goal: 988.               Steps done: 4262384. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0126798201268.\n",
      " state (0)  A[0]:(0.530672848225) A[1]:(0.590456724167) A[2]:(0.590466380119) A[3]:(0.531649112701)\n",
      " state (1)  A[0]:(0.53086656332) A[1]:(-7.64802098274e-06) A[2]:(0.65607637167) A[3]:(0.590608119965)\n",
      " state (2)  A[0]:(0.590108394623) A[1]:(0.729016363621) A[2]:(0.590450942516) A[3]:(0.656273603439)\n",
      " state (3)  A[0]:(0.656071782112) A[1]:(-0.222901955247) A[2]:(0.536186099052) A[3]:(0.520900845528)\n",
      " state (4)  A[0]:(0.590653538704) A[1]:(0.655948638916) A[2]:(-5.18560409546e-05) A[3]:(0.532206118107)\n",
      " state (5)  A[0]:(0.161658897996) A[1]:(0.928885340691) A[2]:(-0.185138076544) A[3]:(0.518417954445)\n",
      " state (6)  A[0]:(0.000474572152598) A[1]:(0.809922933578) A[2]:(-0.000259518623352) A[3]:(0.656527400017)\n",
      " state (7)  A[0]:(0.634155988693) A[1]:(-0.252667248249) A[2]:(0.272430121899) A[3]:(0.891091108322)\n",
      " state (8)  A[0]:(0.655564069748) A[1]:(-2.42777168751e-05) A[2]:(0.728941321373) A[3]:(0.590390622616)\n",
      " state (9)  A[0]:(0.655314147472) A[1]:(0.81000238657) A[2]:(0.809976458549) A[3]:(0.000109761953354)\n",
      " state (10)  A[0]:(0.728320240974) A[1]:(0.899983763695) A[2]:(-0.000183820724487) A[3]:(0.729059934616)\n",
      " state (11)  A[0]:(0.518285751343) A[1]:(0.876862585545) A[2]:(-0.598674893379) A[3]:(0.843236863613)\n",
      " state (12)  A[0]:(0.0740994289517) A[1]:(0.824531376362) A[2]:(-0.561642885208) A[3]:(0.79273545742)\n",
      " state (13)  A[0]:(-0.00152164581232) A[1]:(0.8093765378) A[2]:(0.899904131889) A[3]:(0.728954792023)\n",
      " state (14)  A[0]:(0.809523940086) A[1]:(0.900128126144) A[2]:(0.999999940395) A[3]:(0.809908866882)\n",
      " state (15)  A[0]:(0.98622751236) A[1]:(0.958892047405) A[2]:(1.0) A[3]:(0.88492667675)\n",
      "Episode 518000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6006. Times reached goal: 989.               Steps done: 4268390. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0126038933634.\n",
      " state (0)  A[0]:(0.531866192818) A[1]:(0.590499162674) A[2]:(0.590481102467) A[3]:(0.531270205975)\n",
      " state (1)  A[0]:(0.531588554382) A[1]:(5.03584742546e-05) A[2]:(0.656115531921) A[3]:(0.590017914772)\n",
      " state (2)  A[0]:(0.590506851673) A[1]:(0.729085803032) A[2]:(0.590480089188) A[3]:(0.655648589134)\n",
      " state (3)  A[0]:(0.655872702599) A[1]:(-0.223329946399) A[2]:(0.53618311882) A[3]:(0.519504845142)\n",
      " state (4)  A[0]:(0.589919686317) A[1]:(0.65627348423) A[2]:(-0.000134468078613) A[3]:(0.53071564436)\n",
      " state (5)  A[0]:(0.160019695759) A[1]:(0.92889726162) A[2]:(-0.184920832515) A[3]:(0.516879081726)\n",
      " state (6)  A[0]:(-0.00150108220987) A[1]:(0.81001418829) A[2]:(9.3936920166e-05) A[3]:(0.655369579792)\n",
      " state (7)  A[0]:(0.633540987968) A[1]:(-0.252528011799) A[2]:(0.27294510603) A[3]:(0.890811443329)\n",
      " state (8)  A[0]:(0.65572309494) A[1]:(-0.000338323414326) A[2]:(0.729048490524) A[3]:(0.590045809746)\n",
      " state (9)  A[0]:(0.65579354763) A[1]:(0.809952616692) A[2]:(0.810020923615) A[3]:(-0.000591024698224)\n",
      " state (10)  A[0]:(0.72884118557) A[1]:(0.900008559227) A[2]:(-0.000236034393311) A[3]:(0.728781223297)\n",
      " state (11)  A[0]:(0.519189715385) A[1]:(0.876944959164) A[2]:(-0.598889052868) A[3]:(0.843142211437)\n",
      " state (12)  A[0]:(0.0753356963396) A[1]:(0.824714303017) A[2]:(-0.561945676804) A[3]:(0.792686223984)\n",
      " state (13)  A[0]:(-0.00028795003891) A[1]:(0.809653878212) A[2]:(0.899972140789) A[3]:(0.728991925716)\n",
      " state (14)  A[0]:(0.809930443764) A[1]:(0.900322198868) A[2]:(0.999999940395) A[3]:(0.810054183006)\n",
      " state (15)  A[0]:(0.986236214638) A[1]:(0.958974182606) A[2]:(1.0) A[3]:(0.885035037994)\n",
      "Episode 519000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6034. Times reached goal: 987.               Steps done: 4274424. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0125280704586.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5904,  0.5906,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314, -0.0001,  0.6561,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7290,  0.5904,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 1.0312e-05,  8.1002e-01, -4.5300e-06,  6.5609e-01]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0002,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531430363655) A[1]:(0.590414404869) A[2]:(0.590524554253) A[3]:(0.531467556953)\n",
      " state (1)  A[0]:(0.531460702419) A[1]:(-7.41556286812e-05) A[2]:(0.656129062176) A[3]:(0.590466856956)\n",
      " state (2)  A[0]:(0.590498328209) A[1]:(0.729015827179) A[2]:(0.590500354767) A[3]:(0.656138300896)\n",
      " state (3)  A[0]:(0.656107485294) A[1]:(-0.222682774067) A[2]:(0.536280870438) A[3]:(0.520232796669)\n",
      " state (4)  A[0]:(0.590408444405) A[1]:(0.656079530716) A[2]:(0.000140190124512) A[3]:(0.531412363052)\n",
      " state (5)  A[0]:(0.161104112864) A[1]:(0.928903281689) A[2]:(-0.184909313917) A[3]:(0.517729580402)\n",
      " state (6)  A[0]:(3.73423099518e-05) A[1]:(0.810006022453) A[2]:(9.76324081421e-05) A[3]:(0.656108736992)\n",
      " state (7)  A[0]:(0.634313464165) A[1]:(-0.252401649952) A[2]:(0.273046940565) A[3]:(0.89100497961)\n",
      " state (8)  A[0]:(0.656133651733) A[1]:(6.54384493828e-05) A[2]:(0.729119777679) A[3]:(0.590398192406)\n",
      " state (9)  A[0]:(0.656076073647) A[1]:(0.810012161732) A[2]:(0.81008541584) A[3]:(-7.57873058319e-05)\n",
      " state (10)  A[0]:(0.729033827782) A[1]:(0.900005698204) A[2]:(6.25848770142e-05) A[3]:(0.72900813818)\n",
      " state (11)  A[0]:(0.519494354725) A[1]:(0.876917541027) A[2]:(-0.598706841469) A[3]:(0.843265175819)\n",
      " state (12)  A[0]:(0.0757717043161) A[1]:(0.824645638466) A[2]:(-0.561841011047) A[3]:(0.79282450676)\n",
      " state (13)  A[0]:(0.000125914812088) A[1]:(0.809535205364) A[2]:(0.900033652782) A[3]:(0.729134082794)\n",
      " state (14)  A[0]:(0.810051739216) A[1]:(0.900223016739) A[2]:(0.999999940395) A[3]:(0.810127317905)\n",
      " state (15)  A[0]:(0.986234366894) A[1]:(0.95890802145) A[2]:(1.0) A[3]:(0.885037899017)\n",
      "Episode 520000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6015. Times reached goal: 981.               Steps done: 4280439. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0124529402953.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531953990459) A[1]:(0.590591907501) A[2]:(0.590475440025) A[3]:(0.531329154968)\n",
      " state (1)  A[0]:(0.531803250313) A[1]:(2.41920351982e-05) A[2]:(0.655988156796) A[3]:(0.590211749077)\n",
      " state (2)  A[0]:(0.590863764286) A[1]:(0.729153215885) A[2]:(0.590119242668) A[3]:(0.655911684036)\n",
      " state (3)  A[0]:(0.656402051449) A[1]:(-0.221015200019) A[2]:(0.535889387131) A[3]:(0.520071208477)\n",
      " state (4)  A[0]:(0.590814709663) A[1]:(0.656211256981) A[2]:(7.86781311035e-06) A[3]:(0.531124591827)\n",
      " state (5)  A[0]:(0.161890789866) A[1]:(0.92892998457) A[2]:(-0.18508374691) A[3]:(0.517456769943)\n",
      " state (6)  A[0]:(0.000879108672962) A[1]:(0.810103535652) A[2]:(-0.000149726867676) A[3]:(0.655931949615)\n",
      " state (7)  A[0]:(0.634842634201) A[1]:(-0.252179950476) A[2]:(0.272813916206) A[3]:(0.890973269939)\n",
      " state (8)  A[0]:(0.656634628773) A[1]:(0.000108979642391) A[2]:(0.728929162025) A[3]:(0.590343236923)\n",
      " state (9)  A[0]:(0.656485795975) A[1]:(0.810003638268) A[2]:(0.809952616692) A[3]:(-0.000506207288709)\n",
      " state (10)  A[0]:(0.729303717613) A[1]:(0.899994969368) A[2]:(-0.000461578340037) A[3]:(0.728766918182)\n",
      " state (11)  A[0]:(0.519812226295) A[1]:(0.876906454563) A[2]:(-0.599198818207) A[3]:(0.843109965324)\n",
      " state (12)  A[0]:(0.076058678329) A[1]:(0.824647009373) A[2]:(-0.562490224838) A[3]:(0.792594134808)\n",
      " state (13)  A[0]:(0.000343441963196) A[1]:(0.809567451477) A[2]:(0.899926781654) A[3]:(0.728793025017)\n",
      " state (14)  A[0]:(0.81016087532) A[1]:(0.900261700153) A[2]:(0.999999940395) A[3]:(0.809836745262)\n",
      " state (15)  A[0]:(0.986233472824) A[1]:(0.958924055099) A[2]:(1.0) A[3]:(0.884796619415)\n",
      "Episode 521000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6034. Times reached goal: 987.               Steps done: 4286473. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0123780254988.\n",
      " state (0)  A[0]:(0.531273007393) A[1]:(0.590486586094) A[2]:(0.590461730957) A[3]:(0.531190872192)\n",
      " state (1)  A[0]:(0.531400024891) A[1]:(1.81049108505e-06) A[2]:(0.656098484993) A[3]:(0.590290427208)\n",
      " state (2)  A[0]:(0.590517520905) A[1]:(0.728956580162) A[2]:(0.590439915657) A[3]:(0.655998051167)\n",
      " state (3)  A[0]:(0.656169116497) A[1]:(-0.223072588444) A[2]:(0.536261796951) A[3]:(0.520155787468)\n",
      " state (4)  A[0]:(0.590569198132) A[1]:(0.656082034111) A[2]:(-2.20537185669e-05) A[3]:(0.531423926353)\n",
      " state (5)  A[0]:(0.161437675357) A[1]:(0.928912103176) A[2]:(-0.185142800212) A[3]:(0.517780900002)\n",
      " state (6)  A[0]:(0.000459194154246) A[1]:(0.809972703457) A[2]:(7.15255737305e-07) A[3]:(0.656102776527)\n",
      " state (7)  A[0]:(0.63444685936) A[1]:(-0.252622932196) A[2]:(0.27323359251) A[3]:(0.890937149525)\n",
      " state (8)  A[0]:(0.656122565269) A[1]:(-0.000202544033527) A[2]:(0.728991866112) A[3]:(0.590484976768)\n",
      " state (9)  A[0]:(0.655856132507) A[1]:(0.809937596321) A[2]:(0.810002267361) A[3]:(-8.68886709213e-05)\n",
      " state (10)  A[0]:(0.72875636816) A[1]:(0.89998626709) A[2]:(-0.000143766403198) A[3]:(0.728900194168)\n",
      " state (11)  A[0]:(0.519018828869) A[1]:(0.876920104027) A[2]:(-0.599027991295) A[3]:(0.843184232712)\n",
      " state (12)  A[0]:(0.075042411685) A[1]:(0.824686944485) A[2]:(-0.562436103821) A[3]:(0.792689681053)\n",
      " state (13)  A[0]:(-0.000725835445337) A[1]:(0.809612154961) A[2]:(0.899942338467) A[3]:(0.728913426399)\n",
      " state (14)  A[0]:(0.809743225574) A[1]:(0.900272905827) A[2]:(0.999999940395) A[3]:(0.809942603111)\n",
      " state (15)  A[0]:(0.986188709736) A[1]:(0.958915412426) A[2]:(1.0) A[3]:(0.884856879711)\n",
      "Episode 522000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6012. Times reached goal: 980.               Steps done: 4292485. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0123038320584.\n",
      " state (0)  A[0]:(0.531350433826) A[1]:(0.590462327003) A[2]:(0.590468883514) A[3]:(0.531463980675)\n",
      " state (1)  A[0]:(0.531362652779) A[1]:(-3.44961881638e-05) A[2]:(0.656147241592) A[3]:(0.590482473373)\n",
      " state (2)  A[0]:(0.590444624424) A[1]:(0.728999257088) A[2]:(0.590493917465) A[3]:(0.656152129173)\n",
      " state (3)  A[0]:(0.656028568745) A[1]:(-0.222626984119) A[2]:(0.536192655563) A[3]:(0.520334124565)\n",
      " state (4)  A[0]:(0.590301632881) A[1]:(0.656055212021) A[2]:(-2.56299972534e-05) A[3]:(0.531521677971)\n",
      " state (5)  A[0]:(0.160914376378) A[1]:(0.928898751736) A[2]:(-0.185106307268) A[3]:(0.51784658432)\n",
      " state (6)  A[0]:(-0.000188261270523) A[1]:(0.809966623783) A[2]:(8.47578048706e-05) A[3]:(0.656039476395)\n",
      " state (7)  A[0]:(0.634075880051) A[1]:(-0.252529174089) A[2]:(0.273386359215) A[3]:(0.890847802162)\n",
      " state (8)  A[0]:(0.656031489372) A[1]:(-9.4547867775e-05) A[2]:(0.729003787041) A[3]:(0.590314686298)\n",
      " state (9)  A[0]:(0.656043887138) A[1]:(0.809959888458) A[2]:(0.809990763664) A[3]:(-0.000122860074043)\n",
      " state (10)  A[0]:(0.728981971741) A[1]:(0.899981379509) A[2]:(-0.000191807746887) A[3]:(0.728930711746)\n",
      " state (11)  A[0]:(0.519383907318) A[1]:(0.876892447472) A[2]:(-0.599139451981) A[3]:(0.843204975128)\n",
      " state (12)  A[0]:(0.0755321606994) A[1]:(0.824617862701) A[2]:(-0.562620401382) A[3]:(0.792704105377)\n",
      " state (13)  A[0]:(-0.000170320272446) A[1]:(0.809503555298) A[2]:(0.900002837181) A[3]:(0.728912472725)\n",
      " state (14)  A[0]:(0.809996247292) A[1]:(0.900188863277) A[2]:(0.999999940395) A[3]:(0.8099142313)\n",
      " state (15)  A[0]:(0.986197650433) A[1]:(0.958854496479) A[2]:(1.0) A[3]:(0.884781181812)\n",
      "Episode 523000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6017. Times reached goal: 983.               Steps done: 4298502. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0122300221807.\n",
      " state (0)  A[0]:(0.530671954155) A[1]:(0.59049654007) A[2]:(0.590440630913) A[3]:(0.533323526382)\n",
      " state (1)  A[0]:(0.531010270119) A[1]:(4.0091574192e-05) A[2]:(0.65604019165) A[3]:(0.592071294785)\n",
      " state (2)  A[0]:(0.590256035328) A[1]:(0.729049921036) A[2]:(0.59054505825) A[3]:(0.657465338707)\n",
      " state (3)  A[0]:(0.656061410904) A[1]:(-0.223212227225) A[2]:(0.536455512047) A[3]:(0.521590590477)\n",
      " state (4)  A[0]:(0.590473175049) A[1]:(0.655950307846) A[2]:(0.000173807144165) A[3]:(0.532689094543)\n",
      " state (5)  A[0]:(0.161178782582) A[1]:(0.928897559643) A[2]:(-0.185121268034) A[3]:(0.519078016281)\n",
      " state (6)  A[0]:(-3.64780426025e-05) A[1]:(0.810008347034) A[2]:(-3.93390655518e-06) A[3]:(0.656954169273)\n",
      " state (7)  A[0]:(0.634113311768) A[1]:(-0.252357363701) A[2]:(0.27342826128) A[3]:(0.891113162041)\n",
      " state (8)  A[0]:(0.656078100204) A[1]:(9.11131501198e-05) A[2]:(0.728991508484) A[3]:(0.590597808361)\n",
      " state (9)  A[0]:(0.656024336815) A[1]:(0.810008168221) A[2]:(0.809969902039) A[3]:(-0.000979780801572)\n",
      " state (10)  A[0]:(0.728889167309) A[1]:(0.899985790253) A[2]:(-0.000212788581848) A[3]:(0.728191196918)\n",
      " state (11)  A[0]:(0.519199430943) A[1]:(0.876874923706) A[2]:(-0.599234104156) A[3]:(0.842672467232)\n",
      " state (12)  A[0]:(0.0752870962024) A[1]:(0.8245652318) A[2]:(-0.562856554985) A[3]:(0.791985034943)\n",
      " state (13)  A[0]:(-0.000347554683685) A[1]:(0.809412658215) A[2]:(0.899982869625) A[3]:(0.727999985218)\n",
      " state (14)  A[0]:(0.810010373592) A[1]:(0.900113999844) A[2]:(0.999999940395) A[3]:(0.809270679951)\n",
      " state (15)  A[0]:(0.986194252968) A[1]:(0.95880228281) A[2]:(1.0) A[3]:(0.884369015694)\n",
      "Episode 524000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6030. Times reached goal: 988.               Steps done: 4304532. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.012156497048.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5906,  0.5904,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6559,  0.0001,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557, -0.0001,  0.7289,  0.5908]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556,  0.8100,  0.8100, -0.0001]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7286,  0.9000, -0.0001,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9003,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531466007233) A[1]:(0.590544700623) A[2]:(0.590448498726) A[3]:(0.531277835369)\n",
      " state (1)  A[0]:(0.531435132027) A[1]:(-0.000104233622551) A[2]:(0.656008720398) A[3]:(0.590358912945)\n",
      " state (2)  A[0]:(0.590534210205) A[1]:(0.728891015053) A[2]:(0.590587973595) A[3]:(0.655969619751)\n",
      " state (3)  A[0]:(0.656052172184) A[1]:(-0.224760472775) A[2]:(0.536626696587) A[3]:(0.519896030426)\n",
      " state (4)  A[0]:(0.590505897999) A[1]:(0.655939340591) A[2]:(8.8095664978e-05) A[3]:(0.53126180172)\n",
      " state (5)  A[0]:(0.161422744393) A[1]:(0.928888618946) A[2]:(-0.185164093971) A[3]:(0.517656564713)\n",
      " state (6)  A[0]:(4.46438789368e-05) A[1]:(0.809979259968) A[2]:(1.39474868774e-05) A[3]:(0.655937850475)\n",
      " state (7)  A[0]:(0.633867383003) A[1]:(-0.252550333738) A[2]:(0.273527979851) A[3]:(0.890863060951)\n",
      " state (8)  A[0]:(0.655838310719) A[1]:(-0.00017087161541) A[2]:(0.728942215443) A[3]:(0.590579271317)\n",
      " state (9)  A[0]:(0.655739247799) A[1]:(0.809999346733) A[2]:(0.810013532639) A[3]:(-0.000268563628197)\n",
      " state (10)  A[0]:(0.728732824326) A[1]:(0.90000218153) A[2]:(2.4676322937e-05) A[3]:(0.728843331337)\n",
      " state (11)  A[0]:(0.519114971161) A[1]:(0.876915335655) A[2]:(-0.59915292263) A[3]:(0.843211233616)\n",
      " state (12)  A[0]:(0.0752665624022) A[1]:(0.824657261372) A[2]:(-0.562879860401) A[3]:(0.792764782906)\n",
      " state (13)  A[0]:(-0.000487059325678) A[1]:(0.809556603432) A[2]:(0.899999558926) A[3]:(0.729003310204)\n",
      " state (14)  A[0]:(0.809828460217) A[1]:(0.900222420692) A[2]:(0.999999940395) A[3]:(0.809959053993)\n",
      " state (15)  A[0]:(0.986158013344) A[1]:(0.958855032921) A[2]:(1.0) A[3]:(0.884731411934)\n",
      "Episode 525000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6047. Times reached goal: 986.               Steps done: 4310579. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0120832085215.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531495094299) A[1]:(0.590517163277) A[2]:(0.590512812138) A[3]:(0.531687498093)\n",
      " state (1)  A[0]:(0.531588554382) A[1]:(-2.15023756027e-05) A[2]:(0.656199455261) A[3]:(0.590756654739)\n",
      " state (2)  A[0]:(0.590679526329) A[1]:(0.729056477547) A[2]:(0.590495407581) A[3]:(0.656451225281)\n",
      " state (3)  A[0]:(0.656229376793) A[1]:(-0.221613839269) A[2]:(0.53610932827) A[3]:(0.520693302155)\n",
      " state (4)  A[0]:(0.590515255928) A[1]:(0.655902922153) A[2]:(-4.97102737427e-05) A[3]:(0.531827926636)\n",
      " state (5)  A[0]:(0.161230504513) A[1]:(0.928857028484) A[2]:(-0.185367390513) A[3]:(0.518298149109)\n",
      " state (6)  A[0]:(-0.0002101957798) A[1]:(0.810011267662) A[2]:(-0.00029993057251) A[3]:(0.656335711479)\n",
      " state (7)  A[0]:(0.633908629417) A[1]:(-0.252075046301) A[2]:(0.27327683568) A[3]:(0.890918493271)\n",
      " state (8)  A[0]:(0.656027793884) A[1]:(0.00018136203289) A[2]:(0.728931963444) A[3]:(0.590719521046)\n",
      " state (9)  A[0]:(0.655980825424) A[1]:(0.809981107712) A[2]:(0.809990823269) A[3]:(0.000473335356219)\n",
      " state (10)  A[0]:(0.728919744492) A[1]:(0.899954319) A[2]:(-0.000110626220703) A[3]:(0.729244112968)\n",
      " state (11)  A[0]:(0.519398748875) A[1]:(0.876822888851) A[2]:(-0.59931230545) A[3]:(0.843428134918)\n",
      " state (12)  A[0]:(0.0756586268544) A[1]:(0.824465274811) A[2]:(-0.56315600872) A[3]:(0.793002307415)\n",
      " state (13)  A[0]:(-9.79602336884e-05) A[1]:(0.809250950813) A[2]:(0.899955868721) A[3]:(0.729246735573)\n",
      " state (14)  A[0]:(0.80995619297) A[1]:(0.899979293346) A[2]:(0.999999940395) A[3]:(0.810074567795)\n",
      " state (15)  A[0]:(0.986159443855) A[1]:(0.958708763123) A[2]:(1.0) A[3]:(0.884749412537)\n",
      "Episode 526000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6031. Times reached goal: 987.               Steps done: 4316610. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0120105540008.\n",
      " state (0)  A[0]:(0.53141617775) A[1]:(0.590484380722) A[2]:(0.590507626534) A[3]:(0.531502366066)\n",
      " state (1)  A[0]:(0.531465709209) A[1]:(-0.000146672129631) A[2]:(0.656059503555) A[3]:(0.59052747488)\n",
      " state (2)  A[0]:(0.5906265378) A[1]:(0.729091584682) A[2]:(0.590620875359) A[3]:(0.656175553799)\n",
      " state (3)  A[0]:(0.656179189682) A[1]:(-0.222981899977) A[2]:(0.536546587944) A[3]:(0.520464062691)\n",
      " state (4)  A[0]:(0.590662240982) A[1]:(0.656039476395) A[2]:(0.000234842300415) A[3]:(0.531815052032)\n",
      " state (5)  A[0]:(0.16174146533) A[1]:(0.928894281387) A[2]:(-0.185154080391) A[3]:(0.518295645714)\n",
      " state (6)  A[0]:(0.000555872858968) A[1]:(0.809998810291) A[2]:(7.2717666626e-06) A[3]:(0.656340718269)\n",
      " state (7)  A[0]:(0.634326219559) A[1]:(-0.252411931753) A[2]:(0.273699134588) A[3]:(0.890919446945)\n",
      " state (8)  A[0]:(0.656363964081) A[1]:(-0.000110909342766) A[2]:(0.728929638863) A[3]:(0.591093301773)\n",
      " state (9)  A[0]:(0.656179308891) A[1]:(0.809976696968) A[2]:(0.809993982315) A[3]:(0.00101008976344)\n",
      " state (10)  A[0]:(0.729026913643) A[1]:(0.899987995625) A[2]:(-7.00950622559e-05) A[3]:(0.729413747787)\n",
      " state (11)  A[0]:(0.519535183907) A[1]:(0.87689936161) A[2]:(-0.59940892458) A[3]:(0.843523263931)\n",
      " state (12)  A[0]:(0.0757721215487) A[1]:(0.824630320072) A[2]:(-0.563416421413) A[3]:(0.793109536171)\n",
      " state (13)  A[0]:(-8.0019235611e-05) A[1]:(0.809510171413) A[2]:(0.89992249012) A[3]:(0.729378700256)\n",
      " state (14)  A[0]:(0.80995029211) A[1]:(0.900180160999) A[2]:(0.999999940395) A[3]:(0.810224413872)\n",
      " state (15)  A[0]:(0.986146509647) A[1]:(0.958811938763) A[2]:(1.0) A[3]:(0.884849667549)\n",
      "Episode 527000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6043. Times reached goal: 991.               Steps done: 4322653. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0119381930817.\n",
      " state (0)  A[0]:(0.530805528164) A[1]:(0.590336561203) A[2]:(0.590644240379) A[3]:(0.531601965427)\n",
      " state (1)  A[0]:(0.530815601349) A[1]:(-6.23539090157e-05) A[2]:(0.656144857407) A[3]:(0.590568900108)\n",
      " state (2)  A[0]:(0.589937269688) A[1]:(0.72904253006) A[2]:(0.590477049351) A[3]:(0.656193494797)\n",
      " state (3)  A[0]:(0.655629813671) A[1]:(-0.22211098671) A[2]:(0.536278486252) A[3]:(0.52017980814)\n",
      " state (4)  A[0]:(0.589927017689) A[1]:(0.656054019928) A[2]:(4.68492507935e-05) A[3]:(0.531344294548)\n",
      " state (5)  A[0]:(0.160553589463) A[1]:(0.928911328316) A[2]:(-0.185361295938) A[3]:(0.517892479897)\n",
      " state (6)  A[0]:(-0.000426441401942) A[1]:(0.809985280037) A[2]:(-0.000102043151855) A[3]:(0.655996203423)\n",
      " state (7)  A[0]:(0.633735656738) A[1]:(-0.252354562283) A[2]:(0.27374368906) A[3]:(0.890644848347)\n",
      " state (8)  A[0]:(0.655700922012) A[1]:(0.000226490199566) A[2]:(0.728982329369) A[3]:(0.58985042572)\n",
      " state (9)  A[0]:(0.655642688274) A[1]:(0.81004345417) A[2]:(0.80999058485) A[3]:(-0.000851168995723)\n",
      " state (10)  A[0]:(0.728615581989) A[1]:(0.899993658066) A[2]:(-0.000206351280212) A[3]:(0.728597342968)\n",
      " state (11)  A[0]:(0.518856406212) A[1]:(0.876878380775) A[2]:(-0.599585294724) A[3]:(0.84302520752)\n",
      " state (12)  A[0]:(0.0747988075018) A[1]:(0.824564278126) A[2]:(-0.563671350479) A[3]:(0.792471289635)\n",
      " state (13)  A[0]:(-0.000943541235756) A[1]:(0.809401512146) A[2]:(0.899967074394) A[3]:(0.728600919247)\n",
      " state (14)  A[0]:(0.809813559055) A[1]:(0.900101363659) A[2]:(0.999999940395) A[3]:(0.80970877409)\n",
      " state (15)  A[0]:(0.986132323742) A[1]:(0.958754658699) A[2]:(1.0) A[3]:(0.884520828724)\n",
      "Episode 528000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6052. Times reached goal: 990.               Steps done: 4328705. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0118661613252.\n",
      " state (0)  A[0]:(0.530910372734) A[1]:(0.590110599995) A[2]:(0.590412259102) A[3]:(0.531491994858)\n",
      " state (1)  A[0]:(0.530935883522) A[1]:(-0.000170685350895) A[2]:(0.656048536301) A[3]:(0.590525627136)\n",
      " state (2)  A[0]:(0.590076208115) A[1]:(0.728883385658) A[2]:(0.59035551548) A[3]:(0.656179845333)\n",
      " state (3)  A[0]:(0.655759572983) A[1]:(-0.222778275609) A[2]:(0.536277413368) A[3]:(0.520343065262)\n",
      " state (4)  A[0]:(0.5901222229) A[1]:(0.6559278965) A[2]:(5.94854354858e-05) A[3]:(0.531591176987)\n",
      " state (5)  A[0]:(0.160861834884) A[1]:(0.92887455225) A[2]:(-0.185251936316) A[3]:(0.518120467663)\n",
      " state (6)  A[0]:(-0.00036177036236) A[1]:(0.809860944748) A[2]:(0.000184297561646) A[3]:(0.65614771843)\n",
      " state (7)  A[0]:(0.633400261402) A[1]:(-0.252842545509) A[2]:(0.274158507586) A[3]:(0.89071983099)\n",
      " state (8)  A[0]:(0.655245363712) A[1]:(-0.000336416065693) A[2]:(0.728888630867) A[3]:(0.590609908104)\n",
      " state (9)  A[0]:(0.654942452908) A[1]:(0.809916198254) A[2]:(0.809893012047) A[3]:(0.000280797481537)\n",
      " state (10)  A[0]:(0.727910399437) A[1]:(0.899971485138) A[2]:(-0.000205397605896) A[3]:(0.728996515274)\n",
      " state (11)  A[0]:(0.517714858055) A[1]:(0.876909732819) A[2]:(-0.599575161934) A[3]:(0.843245923519)\n",
      " state (12)  A[0]:(0.0732152834535) A[1]:(0.824696660042) A[2]:(-0.563718914986) A[3]:(0.792729258537)\n",
      " state (13)  A[0]:(-0.00259503140114) A[1]:(0.809648215771) A[2]:(0.900021910667) A[3]:(0.728877604008)\n",
      " state (14)  A[0]:(0.809220314026) A[1]:(0.900299549103) A[2]:(0.999999940395) A[3]:(0.809862971306)\n",
      " state (15)  A[0]:(0.986069440842) A[1]:(0.958855807781) A[2]:(1.0) A[3]:(0.884548604488)\n",
      "Episode 529000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6027. Times reached goal: 986.               Steps done: 4334732. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0117948590561.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5906,  0.5905,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.6560,  0.0001,  0.5311]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561, -0.0001,  0.7291,  0.5900]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.8100,  0.8100, -0.0007]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0001,  0.7285]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8096]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531392753124) A[1]:(0.590565562248) A[2]:(0.590456843376) A[3]:(0.531490802765)\n",
      " state (1)  A[0]:(0.531367719173) A[1]:(2.44081020355e-05) A[2]:(0.656134486198) A[3]:(0.590389609337)\n",
      " state (2)  A[0]:(0.590462625027) A[1]:(0.729033350945) A[2]:(0.590602815151) A[3]:(0.655975341797)\n",
      " state (3)  A[0]:(0.656142055988) A[1]:(-0.222820341587) A[2]:(0.536395311356) A[3]:(0.519844412804)\n",
      " state (4)  A[0]:(0.590511977673) A[1]:(0.656039297581) A[2]:(-6.52074813843e-05) A[3]:(0.531063079834)\n",
      " state (5)  A[0]:(0.161250889301) A[1]:(0.928895354271) A[2]:(-0.185506910086) A[3]:(0.517562389374)\n",
      " state (6)  A[0]:(-0.00024801492691) A[1]:(0.810031890869) A[2]:(-0.000169992446899) A[3]:(0.655593395233)\n",
      " state (7)  A[0]:(0.633625149727) A[1]:(-0.252221375704) A[2]:(0.274037778378) A[3]:(0.890470981598)\n",
      " state (8)  A[0]:(0.655812621117) A[1]:(-1.01625919342e-05) A[2]:(0.728990316391) A[3]:(0.589644134045)\n",
      " state (9)  A[0]:(0.655800640583) A[1]:(0.809990108013) A[2]:(0.810009002686) A[3]:(-0.00142252352089)\n",
      " state (10)  A[0]:(0.728822469711) A[1]:(0.900019407272) A[2]:(-0.000200748443604) A[3]:(0.728306531906)\n",
      " state (11)  A[0]:(0.519313275814) A[1]:(0.876968502998) A[2]:(-0.599800109863) A[3]:(0.842876911163)\n",
      " state (12)  A[0]:(0.0754811316729) A[1]:(0.824761688709) A[2]:(-0.564106941223) A[3]:(0.792281866074)\n",
      " state (13)  A[0]:(-0.000327467918396) A[1]:(0.809684514999) A[2]:(0.899981856346) A[3]:(0.728336572647)\n",
      " state (14)  A[0]:(0.80993783474) A[1]:(0.900286436081) A[2]:(0.999999940395) A[3]:(0.809494316578)\n",
      " state (15)  A[0]:(0.986108124256) A[1]:(0.958823919296) A[2]:(1.0) A[3]:(0.884303689003)\n",
      "Episode 530000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6036. Times reached goal: 989.               Steps done: 4340768. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.011723879718.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531253814697) A[1]:(0.589932203293) A[2]:(0.590276956558) A[3]:(0.531324267387)\n",
      " state (1)  A[0]:(0.531147241592) A[1]:(-0.000216074287891) A[2]:(0.655777096748) A[3]:(0.590230286121)\n",
      " state (2)  A[0]:(0.590302288532) A[1]:(0.728762090206) A[2]:(0.589961290359) A[3]:(0.655864477158)\n",
      " state (3)  A[0]:(0.655825257301) A[1]:(-0.223221570253) A[2]:(0.536171197891) A[3]:(0.520137071609)\n",
      " state (4)  A[0]:(0.590205073357) A[1]:(0.655462622643) A[2]:(0.000151872634888) A[3]:(0.531398653984)\n",
      " state (5)  A[0]:(0.16108264029) A[1]:(0.928766965866) A[2]:(-0.185189425945) A[3]:(0.517849683762)\n",
      " state (6)  A[0]:(-0.000351250171661) A[1]:(0.809797048569) A[2]:(0.000119209289551) A[3]:(0.655669808388)\n",
      " state (7)  A[0]:(0.633618712425) A[1]:(-0.25291788578) A[2]:(0.274047255516) A[3]:(0.890493571758)\n",
      " state (8)  A[0]:(0.655935406685) A[1]:(-0.00108656985685) A[2]:(0.728543162346) A[3]:(0.590594649315)\n",
      " state (9)  A[0]:(0.655594825745) A[1]:(0.809728085995) A[2]:(0.809828519821) A[3]:(-0.000296413898468)\n",
      " state (10)  A[0]:(0.728623390198) A[1]:(0.89993751049) A[2]:(-0.000381827325327) A[3]:(0.728806018829)\n",
      " state (11)  A[0]:(0.519199252129) A[1]:(0.876916944981) A[2]:(-0.599985718727) A[3]:(0.843250930309)\n",
      " state (12)  A[0]:(0.0755304396152) A[1]:(0.824753582478) A[2]:(-0.564417362213) A[3]:(0.792779445648)\n",
      " state (13)  A[0]:(-0.000276654958725) A[1]:(0.80975061655) A[2]:(0.89991247654) A[3]:(0.728892564774)\n",
      " state (14)  A[0]:(0.809855222702) A[1]:(0.900374650955) A[2]:(0.999999940395) A[3]:(0.809768557549)\n",
      " state (15)  A[0]:(0.986083686352) A[1]:(0.958878934383) A[2]:(1.0) A[3]:(0.884358525276)\n",
      "Episode 531000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6033. Times reached goal: 985.               Steps done: 4346801. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0116533624807.\n",
      " state (0)  A[0]:(0.531610846519) A[1]:(0.590497255325) A[2]:(0.590526938438) A[3]:(0.5315502882)\n",
      " state (1)  A[0]:(0.531485795975) A[1]:(3.38032841682e-05) A[2]:(0.656102240086) A[3]:(0.590526282787)\n",
      " state (2)  A[0]:(0.590462386608) A[1]:(0.729025483131) A[2]:(0.590523838997) A[3]:(0.656115889549)\n",
      " state (3)  A[0]:(0.655997157097) A[1]:(-0.222955271602) A[2]:(0.536442279816) A[3]:(0.520222842693)\n",
      " state (4)  A[0]:(0.590197503567) A[1]:(0.656129062176) A[2]:(1.38282775879e-05) A[3]:(0.531528711319)\n",
      " state (5)  A[0]:(0.160700902343) A[1]:(0.928896546364) A[2]:(-0.1854069978) A[3]:(0.518089771271)\n",
      " state (6)  A[0]:(-0.000630021037068) A[1]:(0.809997618198) A[2]:(5.48362731934e-06) A[3]:(0.656051695347)\n",
      " state (7)  A[0]:(0.633662581444) A[1]:(-0.252365797758) A[2]:(0.274333119392) A[3]:(0.890664041042)\n",
      " state (8)  A[0]:(0.656056523323) A[1]:(-7.6912343502e-05) A[2]:(0.728972196579) A[3]:(0.590587735176)\n",
      " state (9)  A[0]:(0.656021952629) A[1]:(0.809986770153) A[2]:(0.809981286526) A[3]:(0.000137120485306)\n",
      " state (10)  A[0]:(0.728941977024) A[1]:(0.899994194508) A[2]:(-0.000103712081909) A[3]:(0.729013621807)\n",
      " state (11)  A[0]:(0.51951956749) A[1]:(0.876908361912) A[2]:(-0.599815607071) A[3]:(0.843323230743)\n",
      " state (12)  A[0]:(0.0758016631007) A[1]:(0.824640214443) A[2]:(-0.564288020134) A[3]:(0.792873501778)\n",
      " state (13)  A[0]:(-3.43918800354e-05) A[1]:(0.809509038925) A[2]:(0.900047898293) A[3]:(0.729078650475)\n",
      " state (14)  A[0]:(0.809997856617) A[1]:(0.900161743164) A[2]:(0.999999940395) A[3]:(0.809999406338)\n",
      " state (15)  A[0]:(0.986085116863) A[1]:(0.958738386631) A[2]:(1.0) A[3]:(0.884530603886)\n",
      "Episode 532000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6049. Times reached goal: 994.               Steps done: 4352850. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0115830840625.\n",
      " state (0)  A[0]:(0.531672716141) A[1]:(0.590449333191) A[2]:(0.590527534485) A[3]:(0.531546294689)\n",
      " state (1)  A[0]:(0.53159558773) A[1]:(4.28482890129e-05) A[2]:(0.656068086624) A[3]:(0.590461671352)\n",
      " state (2)  A[0]:(0.590585947037) A[1]:(0.728984594345) A[2]:(0.590573430061) A[3]:(0.656033873558)\n",
      " state (3)  A[0]:(0.656031191349) A[1]:(-0.223509654403) A[2]:(0.536638259888) A[3]:(0.520110487938)\n",
      " state (4)  A[0]:(0.590308129787) A[1]:(0.656092762947) A[2]:(0.000197291374207) A[3]:(0.531484484673)\n",
      " state (5)  A[0]:(0.161032617092) A[1]:(0.928919196129) A[2]:(-0.185299828649) A[3]:(0.518064856529)\n",
      " state (6)  A[0]:(-0.000278830528259) A[1]:(0.81004345417) A[2]:(0.00017523765564) A[3]:(0.656016111374)\n",
      " state (7)  A[0]:(0.633579730988) A[1]:(-0.252185761929) A[2]:(0.274607807398) A[3]:(0.890606641769)\n",
      " state (8)  A[0]:(0.655859708786) A[1]:(0.000294230878353) A[2]:(0.729087471962) A[3]:(0.590317249298)\n",
      " state (9)  A[0]:(0.655893921852) A[1]:(0.810119867325) A[2]:(0.810057163239) A[3]:(-0.000249519944191)\n",
      " state (10)  A[0]:(0.728847503662) A[1]:(0.900054991245) A[2]:(7.64131546021e-05) A[3]:(0.728907227516)\n",
      " state (11)  A[0]:(0.519360721111) A[1]:(0.876970827579) A[2]:(-0.599795222282) A[3]:(0.843272089958)\n",
      " state (12)  A[0]:(0.07555963099) A[1]:(0.824716150761) A[2]:(-0.564398646355) A[3]:(0.792797267437)\n",
      " state (13)  A[0]:(-0.000252962112427) A[1]:(0.80958199501) A[2]:(0.900046408176) A[3]:(0.72895860672)\n",
      " state (14)  A[0]:(0.810004115105) A[1]:(0.900200724602) A[2]:(0.999999940395) A[3]:(0.809902012348)\n",
      " state (15)  A[0]:(0.986083507538) A[1]:(0.958747029305) A[2]:(1.0) A[3]:(0.884440779686)\n",
      "Episode 533000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6011. Times reached goal: 988.               Steps done: 4358861. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0115136669862.\n",
      " state (0)  A[0]:(0.531668186188) A[1]:(0.59055685997) A[2]:(0.590534329414) A[3]:(0.531642019749)\n",
      " state (1)  A[0]:(0.531624674797) A[1]:(-2.01612710953e-05) A[2]:(0.65610563755) A[3]:(0.590523123741)\n",
      " state (2)  A[0]:(0.590638637543) A[1]:(0.728999853134) A[2]:(0.590568423271) A[3]:(0.656088232994)\n",
      " state (3)  A[0]:(0.656143784523) A[1]:(-0.222982630134) A[2]:(0.536521553993) A[3]:(0.520103573799)\n",
      " state (4)  A[0]:(0.590449571609) A[1]:(0.656125545502) A[2]:(5.57899475098e-05) A[3]:(0.531419157982)\n",
      " state (5)  A[0]:(0.161241918802) A[1]:(0.92890805006) A[2]:(-0.185487344861) A[3]:(0.518028378487)\n",
      " state (6)  A[0]:(1.08778476715e-05) A[1]:(0.809997618198) A[2]:(-2.78949737549e-05) A[3]:(0.655963897705)\n",
      " state (7)  A[0]:(0.633908629417) A[1]:(-0.252380341291) A[2]:(0.274514347315) A[3]:(0.890566468239)\n",
      " state (8)  A[0]:(0.65617609024) A[1]:(7.33286142349e-05) A[2]:(0.729000926018) A[3]:(0.590263664722)\n",
      " state (9)  A[0]:(0.656060695648) A[1]:(0.810057461262) A[2]:(0.809992909431) A[3]:(-0.000353440613253)\n",
      " state (10)  A[0]:(0.728925704956) A[1]:(0.900006055832) A[2]:(-0.000110030174255) A[3]:(0.728818774223)\n",
      " state (11)  A[0]:(0.519451677799) A[1]:(0.876890420914) A[2]:(-0.599996089935) A[3]:(0.84320127964)\n",
      " state (12)  A[0]:(0.0756427198648) A[1]:(0.824578583241) A[2]:(-0.564727604389) A[3]:(0.792692422867)\n",
      " state (13)  A[0]:(-0.000216096639633) A[1]:(0.809409081936) A[2]:(0.900008022785) A[3]:(0.728837609291)\n",
      " state (14)  A[0]:(0.810013711452) A[1]:(0.900095880032) A[2]:(0.999999940395) A[3]:(0.809880256653)\n",
      " state (15)  A[0]:(0.986072063446) A[1]:(0.958688616753) A[2]:(1.0) A[3]:(0.884444594383)\n",
      "Episode 534000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6021. Times reached goal: 990.               Steps done: 4364882. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0114445514783.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5905,  0.5903,  0.5308]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.6561,  0.0002,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.531282246113) A[1]:(0.590478658676) A[2]:(0.590303122997) A[3]:(0.530977010727)\n",
      " state (1)  A[0]:(0.531339645386) A[1]:(0.000103741884232) A[2]:(0.656174719334) A[3]:(0.590373396873)\n",
      " state (2)  A[0]:(0.590451478958) A[1]:(0.728975296021) A[2]:(0.590568721294) A[3]:(0.656169056892)\n",
      " state (3)  A[0]:(0.65615773201) A[1]:(-0.221939265728) A[2]:(0.536415696144) A[3]:(0.520195245743)\n",
      " state (4)  A[0]:(0.5905803442) A[1]:(0.655966877937) A[2]:(0.000157594680786) A[3]:(0.531479418278)\n",
      " state (5)  A[0]:(0.16153138876) A[1]:(0.928883016109) A[2]:(-0.185452803969) A[3]:(0.518247961998)\n",
      " state (6)  A[0]:(-1.75833702087e-06) A[1]:(0.810017585754) A[2]:(1.51395797729e-05) A[3]:(0.656125724316)\n",
      " state (7)  A[0]:(0.633558690548) A[1]:(-0.252279639244) A[2]:(0.274704366922) A[3]:(0.890592515469)\n",
      " state (8)  A[0]:(0.655863404274) A[1]:(-1.17719173431e-05) A[2]:(0.729039907455) A[3]:(0.59057199955)\n",
      " state (9)  A[0]:(0.655802726746) A[1]:(0.810030341148) A[2]:(0.810041248798) A[3]:(0.000154227018356)\n",
      " state (10)  A[0]:(0.728782057762) A[1]:(0.900018393993) A[2]:(1.54972076416e-06) A[3]:(0.729051470757)\n",
      " state (11)  A[0]:(0.519303917885) A[1]:(0.876940310001) A[2]:(-0.600036025047) A[3]:(0.843346834183)\n",
      " state (12)  A[0]:(0.0754525065422) A[1]:(0.824696779251) A[2]:(-0.564886629581) A[3]:(0.792853593826)\n",
      " state (13)  A[0]:(-0.000489771307912) A[1]:(0.809593081474) A[2]:(0.900008141994) A[3]:(0.728990495205)\n",
      " state (14)  A[0]:(0.809859514236) A[1]:(0.900232851505) A[2]:(0.999999940395) A[3]:(0.80996209383)\n",
      " state (15)  A[0]:(0.986043512821) A[1]:(0.958754181862) A[2]:(1.0) A[3]:(0.884449720383)\n",
      "Episode 535000 finished after 0 timesteps with r=0.0. Running score: 0.98. Times trained:               6037. Times reached goal: 991.               Steps done: 4370919. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0113756688525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531501412392) A[1]:(0.590388536453) A[2]:(0.590391635895) A[3]:(0.531695127487)\n",
      " state (1)  A[0]:(0.531367242336) A[1]:(3.41981649399e-06) A[2]:(0.656071543694) A[3]:(0.590525627136)\n",
      " state (2)  A[0]:(0.590384483337) A[1]:(0.728924274445) A[2]:(0.590454101562) A[3]:(0.656094908714)\n",
      " state (3)  A[0]:(0.656066417694) A[1]:(-0.222491711378) A[2]:(0.53640627861) A[3]:(0.520241618156)\n",
      " state (4)  A[0]:(0.590443193913) A[1]:(0.655877947807) A[2]:(5.3882598877e-05) A[3]:(0.531547546387)\n",
      " state (5)  A[0]:(0.161282002926) A[1]:(0.928862333298) A[2]:(-0.185590237379) A[3]:(0.518213033676)\n",
      " state (6)  A[0]:(-0.000240832567215) A[1]:(0.809956789017) A[2]:(-5.71012496948e-05) A[3]:(0.655925214291)\n",
      " state (7)  A[0]:(0.633374929428) A[1]:(-0.252403646708) A[2]:(0.27475297451) A[3]:(0.890415072441)\n",
      " state (8)  A[0]:(0.655768573284) A[1]:(-0.000277139246464) A[2]:(0.728899121284) A[3]:(0.590241611004)\n",
      " state (9)  A[0]:(0.655749320984) A[1]:(0.809921562672) A[2]:(0.809953212738) A[3]:(-0.000338837504387)\n",
      " state (10)  A[0]:(0.728783845901) A[1]:(0.899982750416) A[2]:(-0.000244379043579) A[3]:(0.728811502457)\n",
      " state (11)  A[0]:(0.519387304783) A[1]:(0.876912713051) A[2]:(-0.600302815437) A[3]:(0.843216180801)\n",
      " state (12)  A[0]:(0.0755970552564) A[1]:(0.824660718441) A[2]:(-0.565266191959) A[3]:(0.792681276798)\n",
      " state (13)  A[0]:(-0.000376850337489) A[1]:(0.809540092945) A[2]:(0.899977087975) A[3]:(0.728767752647)\n",
      " state (14)  A[0]:(0.809869587421) A[1]:(0.900185406208) A[2]:(0.999999940395) A[3]:(0.809825778008)\n",
      " state (15)  A[0]:(0.986027598381) A[1]:(0.95871257782) A[2]:(1.0) A[3]:(0.884347915649)\n",
      "Episode 536000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6030. Times reached goal: 988.               Steps done: 4376949. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.011307279969.\n",
      " state (0)  A[0]:(0.5308547616) A[1]:(0.590455532074) A[2]:(0.590431451797) A[3]:(0.530704975128)\n",
      " state (1)  A[0]:(0.530673980713) A[1]:(1.16750597954e-05) A[2]:(0.65606200695) A[3]:(0.589725673199)\n",
      " state (2)  A[0]:(0.589868664742) A[1]:(0.728989124298) A[2]:(0.590531885624) A[3]:(0.655466496944)\n",
      " state (3)  A[0]:(0.655913710594) A[1]:(-0.222581848502) A[2]:(0.536464989185) A[3]:(0.519568860531)\n",
      " state (4)  A[0]:(0.59058701992) A[1]:(0.656016230583) A[2]:(-6.06775283813e-05) A[3]:(0.53115093708)\n",
      " state (5)  A[0]:(0.161851018667) A[1]:(0.928910315037) A[2]:(-0.185825735331) A[3]:(0.518081068993)\n",
      " state (6)  A[0]:(0.000761836592574) A[1]:(0.809973120689) A[2]:(-0.000227808952332) A[3]:(0.656045079231)\n",
      " state (7)  A[0]:(0.633921802044) A[1]:(-0.252378851175) A[2]:(0.274722993374) A[3]:(0.890459835529)\n",
      " state (8)  A[0]:(0.656166791916) A[1]:(-2.87666916847e-05) A[2]:(0.728871941566) A[3]:(0.590061485767)\n",
      " state (9)  A[0]:(0.656202614307) A[1]:(0.809938907623) A[2]:(0.809942364693) A[3]:(-0.00115030957386)\n",
      " state (10)  A[0]:(0.729139328003) A[1]:(0.899947226048) A[2]:(-0.000142097473145) A[3]:(0.728243649006)\n",
      " state (11)  A[0]:(0.519944429398) A[1]:(0.876827955246) A[2]:(-0.600256204605) A[3]:(0.842801451683)\n",
      " state (12)  A[0]:(0.0764117389917) A[1]:(0.824491858482) A[2]:(-0.565330028534) A[3]:(0.792109370232)\n",
      " state (13)  A[0]:(0.000493794621434) A[1]:(0.809298872948) A[2]:(0.899973869324) A[3]:(0.728038012981)\n",
      " state (14)  A[0]:(0.810184597969) A[1]:(0.900017619133) A[2]:(0.999999940395) A[3]:(0.809329986572)\n",
      " state (15)  A[0]:(0.986047208309) A[1]:(0.958619654179) A[2]:(1.0) A[3]:(0.884051322937)\n",
      "Episode 537000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6044. Times reached goal: 986.               Steps done: 4382993. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0112391448805.\n",
      " state (0)  A[0]:(0.53197145462) A[1]:(0.590844750404) A[2]:(0.590469181538) A[3]:(0.531485319138)\n",
      " state (1)  A[0]:(0.5316644907) A[1]:(0.000561207474675) A[2]:(0.65628683567) A[3]:(0.59052336216)\n",
      " state (2)  A[0]:(0.590574622154) A[1]:(0.729219198227) A[2]:(0.590646147728) A[3]:(0.656200528145)\n",
      " state (3)  A[0]:(0.656142771244) A[1]:(-0.221224725246) A[2]:(0.536469697952) A[3]:(0.520313382149)\n",
      " state (4)  A[0]:(0.59037142992) A[1]:(0.65625089407) A[2]:(0.000303864479065) A[3]:(0.531597673893)\n",
      " state (5)  A[0]:(0.160953223705) A[1]:(0.928944528103) A[2]:(-0.185255035758) A[3]:(0.518379688263)\n",
      " state (6)  A[0]:(-0.00102478230838) A[1]:(0.810289740562) A[2]:(0.000397562951548) A[3]:(0.655990898609)\n",
      " state (7)  A[0]:(0.632854223251) A[1]:(-0.251462221146) A[2]:(0.27546530962) A[3]:(0.890408813953)\n",
      " state (8)  A[0]:(0.6555108428) A[1]:(0.000340849161148) A[2]:(0.729209721088) A[3]:(0.590465784073)\n",
      " state (9)  A[0]:(0.655500769615) A[1]:(0.810139417648) A[2]:(0.810232520103) A[3]:(-0.000207304954529)\n",
      " state (10)  A[0]:(0.728605031967) A[1]:(0.900136888027) A[2]:(0.000587940157857) A[3]:(0.728907704353)\n",
      " state (11)  A[0]:(0.519178450108) A[1]:(0.877135396004) A[2]:(-0.599958240986) A[3]:(0.843316316605)\n",
      " state (12)  A[0]:(0.075307957828) A[1]:(0.8250041008) A[2]:(-0.56517624855) A[3]:(0.792817115784)\n",
      " state (13)  A[0]:(-0.000836521212477) A[1]:(0.809929430485) A[2]:(0.9000248909) A[3]:(0.728918671608)\n",
      " state (14)  A[0]:(0.809609770775) A[1]:(0.900403439999) A[2]:(0.999999940395) A[3]:(0.809932291508)\n",
      " state (15)  A[0]:(0.985982358456) A[1]:(0.958793401718) A[2]:(1.0) A[3]:(0.884375035763)\n",
      "Episode 538000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6046. Times reached goal: 990.               Steps done: 4389039. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0111713980157.\n",
      " state (0)  A[0]:(0.531207561493) A[1]:(0.590571284294) A[2]:(0.590495586395) A[3]:(0.531329393387)\n",
      " state (1)  A[0]:(0.531243085861) A[1]:(7.9557299614e-05) A[2]:(0.656095445156) A[3]:(0.590297400951)\n",
      " state (2)  A[0]:(0.590361952782) A[1]:(0.72902661562) A[2]:(0.590494632721) A[3]:(0.655980110168)\n",
      " state (3)  A[0]:(0.656051278114) A[1]:(-0.22236725688) A[2]:(0.536418437958) A[3]:(0.520094275475)\n",
      " state (4)  A[0]:(0.590501308441) A[1]:(0.656177222729) A[2]:(-6.59227371216e-05) A[3]:(0.531600594521)\n",
      " state (5)  A[0]:(0.161554083228) A[1]:(0.928928613663) A[2]:(-0.185737118125) A[3]:(0.518570184708)\n",
      " state (6)  A[0]:(0.000547736824956) A[1]:(0.81007027626) A[2]:(-1.71661376953e-05) A[3]:(0.656428456306)\n",
      " state (7)  A[0]:(0.634315371513) A[1]:(-0.252218723297) A[2]:(0.275196403265) A[3]:(0.89064759016)\n",
      " state (8)  A[0]:(0.656777977943) A[1]:(-6.56098127365e-05) A[2]:(0.728969931602) A[3]:(0.591071844101)\n",
      " state (9)  A[0]:(0.656575322151) A[1]:(0.81000238657) A[2]:(0.810002088547) A[3]:(0.000461682648165)\n",
      " state (10)  A[0]:(0.729339122772) A[1]:(0.900008141994) A[2]:(-5.94854354858e-05) A[3]:(0.729243278503)\n",
      " state (11)  A[0]:(0.520201086998) A[1]:(0.87692117691) A[2]:(-0.600409030914) A[3]:(0.843561589718)\n",
      " state (12)  A[0]:(0.0766353979707) A[1]:(0.82464838028) A[2]:(-0.565690755844) A[3]:(0.793187916279)\n",
      " state (13)  A[0]:(0.00053656095406) A[1]:(0.809508323669) A[2]:(0.899996161461) A[3]:(0.729428708553)\n",
      " state (14)  A[0]:(0.810134410858) A[1]:(0.900167047977) A[2]:(0.999999940395) A[3]:(0.81027841568)\n",
      " state (15)  A[0]:(0.986012995243) A[1]:(0.9586789608) A[2]:(1.0) A[3]:(0.88452476263)\n",
      "Episode 539000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6021. Times reached goal: 992.               Steps done: 4395060. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0111043371177.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5906,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.9015e-01,  6.5599e-01, -5.2452e-06,  5.3125e-01]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5899,  0.6560,  0.0000,  0.5312]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6553,  0.0003,  0.7291,  0.5901]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556,  0.8099,  0.8100, -0.0003]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0001,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9001,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530850172043) A[1]:(0.590513408184) A[2]:(0.590471506119) A[3]:(0.531500458717)\n",
      " state (1)  A[0]:(0.530812203884) A[1]:(-2.52351164818e-05) A[2]:(0.656099259853) A[3]:(0.590466380119)\n",
      " state (2)  A[0]:(0.589835762978) A[1]:(0.729054570198) A[2]:(0.590672373772) A[3]:(0.656071662903)\n",
      " state (3)  A[0]:(0.655707240105) A[1]:(-0.222033113241) A[2]:(0.536479592323) A[3]:(0.519887447357)\n",
      " state (4)  A[0]:(0.589945793152) A[1]:(0.655920505524) A[2]:(-2.4676322937e-05) A[3]:(0.531193375587)\n",
      " state (5)  A[0]:(0.160390362144) A[1]:(0.928896129131) A[2]:(-0.185876950622) A[3]:(0.518110752106)\n",
      " state (6)  A[0]:(-0.000813185994048) A[1]:(0.81002175808) A[2]:(-9.69171524048e-05) A[3]:(0.655931472778)\n",
      " state (7)  A[0]:(0.633232474327) A[1]:(-0.251943737268) A[2]:(0.275326728821) A[3]:(0.890336036682)\n",
      " state (8)  A[0]:(0.655900120735) A[1]:(0.000135280191898) A[2]:(0.728977739811) A[3]:(0.590373277664)\n",
      " state (9)  A[0]:(0.656177282333) A[1]:(0.809950351715) A[2]:(0.810056269169) A[3]:(-7.18235969543e-05)\n",
      " state (10)  A[0]:(0.729284763336) A[1]:(0.899997413158) A[2]:(-3.93390655518e-05) A[3]:(0.729028344154)\n",
      " state (11)  A[0]:(0.520342707634) A[1]:(0.876921653748) A[2]:(-0.600578308105) A[3]:(0.843396306038)\n",
      " state (12)  A[0]:(0.0769783854485) A[1]:(0.824632167816) A[2]:(-0.566068768501) A[3]:(0.792891025543)\n",
      " state (13)  A[0]:(0.000926375098061) A[1]:(0.809430241585) A[2]:(0.899856984615) A[3]:(0.728977203369)\n",
      " state (14)  A[0]:(0.810267031193) A[1]:(0.900061845779) A[2]:(0.999999940395) A[3]:(0.809975206852)\n",
      " state (15)  A[0]:(0.986021637917) A[1]:(0.958602070808) A[2]:(1.0) A[3]:(0.884366989136)\n",
      "Episode 540000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6027. Times reached goal: 988.               Steps done: 4401087. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0110376125543.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531467378139) A[1]:(0.590573251247) A[2]:(0.590507626534) A[3]:(0.531444251537)\n",
      " state (1)  A[0]:(0.531292319298) A[1]:(-0.000150799751282) A[2]:(0.656097054482) A[3]:(0.590343117714)\n",
      " state (2)  A[0]:(0.590260744095) A[1]:(0.728893041611) A[2]:(0.590431451797) A[3]:(0.655983924866)\n",
      " state (3)  A[0]:(0.655822575092) A[1]:(-0.222340464592) A[2]:(0.536276936531) A[3]:(0.519782721996)\n",
      " state (4)  A[0]:(0.589935958385) A[1]:(0.656022906303) A[2]:(-0.000318884849548) A[3]:(0.531128168106)\n",
      " state (5)  A[0]:(0.160236060619) A[1]:(0.928861498833) A[2]:(-0.186064198613) A[3]:(0.518044233322)\n",
      " state (6)  A[0]:(-0.00111094070598) A[1]:(0.809839129448) A[2]:(-0.000251173973083) A[3]:(0.655919313431)\n",
      " state (7)  A[0]:(0.633093953133) A[1]:(-0.252626150846) A[2]:(0.27522328496) A[3]:(0.890377700329)\n",
      " state (8)  A[0]:(0.655797600746) A[1]:(-0.000320613384247) A[2]:(0.728801071644) A[3]:(0.59055274725)\n",
      " state (9)  A[0]:(0.655921220779) A[1]:(0.80986225605) A[2]:(0.809828162193) A[3]:(5.84870576859e-05)\n",
      " state (10)  A[0]:(0.728806436062) A[1]:(0.899902880192) A[2]:(-0.000465750665171) A[3]:(0.728865742683)\n",
      " state (11)  A[0]:(0.519319832325) A[1]:(0.876763939857) A[2]:(-0.600785732269) A[3]:(0.843236982822)\n",
      " state (12)  A[0]:(0.0754156187177) A[1]:(0.824391007423) A[2]:(-0.566269874573) A[3]:(0.792720913887)\n",
      " state (13)  A[0]:(-0.000576078833546) A[1]:(0.80918687582) A[2]:(0.89997190237) A[3]:(0.728883624077)\n",
      " state (14)  A[0]:(0.80986571312) A[1]:(0.899960458279) A[2]:(0.999999940395) A[3]:(0.810039877892)\n",
      " state (15)  A[0]:(0.985977351665) A[1]:(0.958557069302) A[2]:(1.0) A[3]:(0.884436190128)\n",
      "Episode 541000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6043. Times reached goal: 992.               Steps done: 4407130. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0109711133912.\n",
      " state (0)  A[0]:(0.531388044357) A[1]:(0.590467572212) A[2]:(0.590431571007) A[3]:(0.531190931797)\n",
      " state (1)  A[0]:(0.531435489655) A[1]:(-5.54919242859e-05) A[2]:(0.656067013741) A[3]:(0.590308308601)\n",
      " state (2)  A[0]:(0.590564489365) A[1]:(0.728994131088) A[2]:(0.5905585289) A[3]:(0.656007766724)\n",
      " state (3)  A[0]:(0.656320869923) A[1]:(-0.222710892558) A[2]:(0.536653935909) A[3]:(0.519884824753)\n",
      " state (4)  A[0]:(0.590768098831) A[1]:(0.65632134676) A[2]:(0.000110864639282) A[3]:(0.531372368336)\n",
      " state (5)  A[0]:(0.161769211292) A[1]:(0.928944647312) A[2]:(-0.185697287321) A[3]:(0.518378734589)\n",
      " state (6)  A[0]:(0.000689774635248) A[1]:(0.81003177166) A[2]:(0.000152587890625) A[3]:(0.656276941299)\n",
      " state (7)  A[0]:(0.634194552898) A[1]:(-0.252103209496) A[2]:(0.275805354118) A[3]:(0.890527546406)\n",
      " state (8)  A[0]:(0.656661629677) A[1]:(0.00040307638119) A[2]:(0.729252576828) A[3]:(0.590805768967)\n",
      " state (9)  A[0]:(0.656637430191) A[1]:(0.810084998608) A[2]:(0.810128331184) A[3]:(0.000642090919428)\n",
      " state (10)  A[0]:(0.72937142849) A[1]:(0.899974048138) A[2]:(0.000180244445801) A[3]:(0.729277133942)\n",
      " state (11)  A[0]:(0.520218074322) A[1]:(0.87680709362) A[2]:(-0.600524187088) A[3]:(0.843527138233)\n",
      " state (12)  A[0]:(0.0766763389111) A[1]:(0.824414670467) A[2]:(-0.566129505634) A[3]:(0.793121039867)\n",
      " state (13)  A[0]:(0.000728964689188) A[1]:(0.809196829796) A[2]:(0.900091409683) A[3]:(0.729390382767)\n",
      " state (14)  A[0]:(0.810323119164) A[1]:(0.899973988533) A[2]:(0.999999940395) A[3]:(0.810370147228)\n",
      " state (15)  A[0]:(0.985998868942) A[1]:(0.958555698395) A[2]:(1.0) A[3]:(0.884571075439)\n",
      "Episode 542000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6058. Times reached goal: 992.               Steps done: 4413188. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0109048512968.\n",
      " state (0)  A[0]:(0.531388521194) A[1]:(0.59089744091) A[2]:(0.590668559074) A[3]:(0.531528294086)\n",
      " state (1)  A[0]:(0.531461000443) A[1]:(0.000105291604996) A[2]:(0.65626180172) A[3]:(0.590695023537)\n",
      " state (2)  A[0]:(0.590607523918) A[1]:(0.729152321815) A[2]:(0.590723514557) A[3]:(0.656377196312)\n",
      " state (3)  A[0]:(0.656432509422) A[1]:(-0.222090303898) A[2]:(0.536526858807) A[3]:(0.520220994949)\n",
      " state (4)  A[0]:(0.590874910355) A[1]:(0.656314015388) A[2]:(-0.000112414360046) A[3]:(0.531612634659)\n",
      " state (5)  A[0]:(0.161758929491) A[1]:(0.928921818733) A[2]:(-0.185905724764) A[3]:(0.51861166954)\n",
      " state (6)  A[0]:(0.000220388174057) A[1]:(0.810034871101) A[2]:(-1.39474868774e-05) A[3]:(0.656331896782)\n",
      " state (7)  A[0]:(0.633721113205) A[1]:(-0.252094745636) A[2]:(0.275820463896) A[3]:(0.890495955944)\n",
      " state (8)  A[0]:(0.656365513802) A[1]:(0.00016462802887) A[2]:(0.729137539864) A[3]:(0.590775847435)\n",
      " state (9)  A[0]:(0.656415343285) A[1]:(0.81005603075) A[2]:(0.810041487217) A[3]:(0.000132128596306)\n",
      " state (10)  A[0]:(0.729216337204) A[1]:(0.900006890297) A[2]:(-0.000134587287903) A[3]:(0.729032576084)\n",
      " state (11)  A[0]:(0.519958972931) A[1]:(0.876892328262) A[2]:(-0.600857138634) A[3]:(0.843405783176)\n",
      " state (12)  A[0]:(0.0762082189322) A[1]:(0.824585795403) A[2]:(-0.566609978676) A[3]:(0.792958259583)\n",
      " state (13)  A[0]:(0.000141084194183) A[1]:(0.809434056282) A[2]:(0.900015711784) A[3]:(0.729136347771)\n",
      " state (14)  A[0]:(0.81012737751) A[1]:(0.900131344795) A[2]:(0.999999940395) A[3]:(0.810121893883)\n",
      " state (15)  A[0]:(0.985971331596) A[1]:(0.958623826504) A[2]:(1.0) A[3]:(0.884338736534)\n",
      "Episode 543000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6017. Times reached goal: 992.               Steps done: 4419205. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0108394338125.\n",
      " state (0)  A[0]:(0.531435847282) A[1]:(0.590461730957) A[2]:(0.590500473976) A[3]:(0.531608879566)\n",
      " state (1)  A[0]:(0.5313975811) A[1]:(6.7874789238e-06) A[2]:(0.656109631062) A[3]:(0.590545713902)\n",
      " state (2)  A[0]:(0.590505897999) A[1]:(0.729066848755) A[2]:(0.590406119823) A[3]:(0.656213521957)\n",
      " state (3)  A[0]:(0.656055748463) A[1]:(-0.22156278789) A[2]:(0.536352992058) A[3]:(0.520116209984)\n",
      " state (4)  A[0]:(0.590392231941) A[1]:(0.656215429306) A[2]:(-2.87294387817e-05) A[3]:(0.531490325928)\n",
      " state (5)  A[0]:(0.161238223314) A[1]:(0.928915321827) A[2]:(-0.185819402337) A[3]:(0.518495440483)\n",
      " state (6)  A[0]:(-0.000253796577454) A[1]:(0.810080111027) A[2]:(0.000111818313599) A[3]:(0.656017124653)\n",
      " state (7)  A[0]:(0.633480429649) A[1]:(-0.252120554447) A[2]:(0.276016831398) A[3]:(0.890249729156)\n",
      " state (8)  A[0]:(0.656220197678) A[1]:(-0.000263132154942) A[2]:(0.729032218456) A[3]:(0.590460896492)\n",
      " state (9)  A[0]:(0.65614771843) A[1]:(0.809942662716) A[2]:(0.810025334358) A[3]:(-0.000253349542618)\n",
      " state (10)  A[0]:(0.729018449783) A[1]:(0.900006115437) A[2]:(-0.000189661979675) A[3]:(0.728911578655)\n",
      " state (11)  A[0]:(0.519718110561) A[1]:(0.876954972744) A[2]:(-0.601032733917) A[3]:(0.843367874622)\n",
      " state (12)  A[0]:(0.0758738741279) A[1]:(0.824758052826) A[2]:(-0.56687951088) A[3]:(0.792880237103)\n",
      " state (13)  A[0]:(-0.000238180160522) A[1]:(0.809719800949) A[2]:(0.900037825108) A[3]:(0.728945732117)\n",
      " state (14)  A[0]:(0.809980869293) A[1]:(0.900350511074) A[2]:(0.999999940395) A[3]:(0.809881210327)\n",
      " state (15)  A[0]:(0.985941827297) A[1]:(0.958731591702) A[2]:(1.0) A[3]:(0.884076356888)\n",
      "Episode 544000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6033. Times reached goal: 988.               Steps done: 4425238. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0107742363741.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5900,  0.5903,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5319, -0.0000,  0.6559,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5910,  0.7287,  0.5901,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  0.8099,  0.0001,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0002,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9001,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53210568428) A[1]:(0.590032577515) A[2]:(0.590354323387) A[3]:(0.531209647655)\n",
      " state (1)  A[0]:(0.531888604164) A[1]:(2.51829624176e-05) A[2]:(0.655866682529) A[3]:(0.59025156498)\n",
      " state (2)  A[0]:(0.590801119804) A[1]:(0.72871440649) A[2]:(0.590203166008) A[3]:(0.655904591084)\n",
      " state (3)  A[0]:(0.656132698059) A[1]:(-0.223919168115) A[2]:(0.536677956581) A[3]:(0.519673705101)\n",
      " state (4)  A[0]:(0.590412378311) A[1]:(0.656063318253) A[2]:(6.59227371216e-05) A[3]:(0.531217813492)\n",
      " state (5)  A[0]:(0.161278113723) A[1]:(0.928924798965) A[2]:(-0.18591503799) A[3]:(0.518178582191)\n",
      " state (6)  A[0]:(2.63750553131e-05) A[1]:(0.810001313686) A[2]:(-4.19616699219e-05) A[3]:(0.655689835548)\n",
      " state (7)  A[0]:(0.633386969566) A[1]:(-0.252165287733) A[2]:(0.275762856007) A[3]:(0.889995276928)\n",
      " state (8)  A[0]:(0.655873298645) A[1]:(0.000352382630808) A[2]:(0.728882193565) A[3]:(0.589586257935)\n",
      " state (9)  A[0]:(0.655801653862) A[1]:(0.810131728649) A[2]:(0.809945821762) A[3]:(-0.0010437812889)\n",
      " state (10)  A[0]:(0.728691697121) A[1]:(0.900036513805) A[2]:(-0.000124335289001) A[3]:(0.728473782539)\n",
      " state (11)  A[0]:(0.519220471382) A[1]:(0.876919209957) A[2]:(-0.600949168205) A[3]:(0.843047082424)\n",
      " state (12)  A[0]:(0.0752978250384) A[1]:(0.82462221384) A[2]:(-0.566907763481) A[3]:(0.792471706867)\n",
      " state (13)  A[0]:(-0.000684976461343) A[1]:(0.80948138237) A[2]:(0.900030255318) A[3]:(0.728499472141)\n",
      " state (14)  A[0]:(0.8099219203) A[1]:(0.900170505047) A[2]:(0.999999940395) A[3]:(0.809680342674)\n",
      " state (15)  A[0]:(0.985938310623) A[1]:(0.958628237247) A[2]:(1.0) A[3]:(0.884020209312)\n",
      "Episode 545000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6013. Times reached goal: 987.               Steps done: 4431251. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0107096452785.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531445026398) A[1]:(0.590273082256) A[2]:(0.590611994267) A[3]:(0.531300485134)\n",
      " state (1)  A[0]:(0.531552791595) A[1]:(-0.000129230320454) A[2]:(0.656164884567) A[3]:(0.590312957764)\n",
      " state (2)  A[0]:(0.590616345406) A[1]:(0.728931546211) A[2]:(0.590487480164) A[3]:(0.656004905701)\n",
      " state (3)  A[0]:(0.656128287315) A[1]:(-0.221955671906) A[2]:(0.536479949951) A[3]:(0.519848048687)\n",
      " state (4)  A[0]:(0.590393126011) A[1]:(0.656182527542) A[2]:(7.7486038208e-06) A[3]:(0.531283736229)\n",
      " state (5)  A[0]:(0.161232277751) A[1]:(0.928915441036) A[2]:(-0.185892596841) A[3]:(0.518372952938)\n",
      " state (6)  A[0]:(0.000163823366165) A[1]:(0.809952020645) A[2]:(0.000126957893372) A[3]:(0.655985116959)\n",
      " state (7)  A[0]:(0.633625626564) A[1]:(-0.252223670483) A[2]:(0.276123076677) A[3]:(0.890155017376)\n",
      " state (8)  A[0]:(0.656212329865) A[1]:(0.000170640647411) A[2]:(0.728983521461) A[3]:(0.590289592743)\n",
      " state (9)  A[0]:(0.656219065189) A[1]:(0.810020029545) A[2]:(0.809984564781) A[3]:(7.95125961304e-05)\n",
      " state (10)  A[0]:(0.7290995121) A[1]:(0.899991035461) A[2]:(-0.000107765197754) A[3]:(0.729024171829)\n",
      " state (11)  A[0]:(0.51990288496) A[1]:(0.876879453659) A[2]:(-0.601051568985) A[3]:(0.843396306038)\n",
      " state (12)  A[0]:(0.076179921627) A[1]:(0.824568510056) A[2]:(-0.567135572433) A[3]:(0.792901754379)\n",
      " state (13)  A[0]:(2.62260437012e-05) A[1]:(0.809400439262) A[2]:(0.900004267693) A[3]:(0.72900313139)\n",
      " state (14)  A[0]:(0.810052573681) A[1]:(0.900097072124) A[2]:(0.999999940395) A[3]:(0.810031294823)\n",
      " state (15)  A[0]:(0.985928595066) A[1]:(0.958573162556) A[2]:(1.0) A[3]:(0.884207427502)\n",
      "Episode 546000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6027. Times reached goal: 983.               Steps done: 4437278. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0106452923687.\n",
      " state (0)  A[0]:(0.531518220901) A[1]:(0.590559720993) A[2]:(0.590542018414) A[3]:(0.531574606895)\n",
      " state (1)  A[0]:(0.531548261642) A[1]:(2.53841280937e-05) A[2]:(0.656112492085) A[3]:(0.590474128723)\n",
      " state (2)  A[0]:(0.590640664101) A[1]:(0.729059517384) A[2]:(0.590422749519) A[3]:(0.656119346619)\n",
      " state (3)  A[0]:(0.656140089035) A[1]:(-0.22149759531) A[2]:(0.536390542984) A[3]:(0.520265460014)\n",
      " state (4)  A[0]:(0.59047716856) A[1]:(0.656108021736) A[2]:(-3.2901763916e-05) A[3]:(0.531713783741)\n",
      " state (5)  A[0]:(0.161457553506) A[1]:(0.928895711899) A[2]:(-0.185984671116) A[3]:(0.518760979176)\n",
      " state (6)  A[0]:(2.97725200653e-05) A[1]:(0.810020565987) A[2]:(-1.89542770386e-05) A[3]:(0.656090438366)\n",
      " state (7)  A[0]:(0.633415937424) A[1]:(-0.252075731754) A[2]:(0.276131898165) A[3]:(0.890132009983)\n",
      " state (8)  A[0]:(0.656216621399) A[1]:(2.05859541893e-05) A[2]:(0.728986442089) A[3]:(0.590451359749)\n",
      " state (9)  A[0]:(0.656266987324) A[1]:(0.80999571085) A[2]:(0.810019493103) A[3]:(0.00043515858124)\n",
      " state (10)  A[0]:(0.729153394699) A[1]:(0.900001525879) A[2]:(6.16312026978e-05) A[3]:(0.729153215885)\n",
      " state (11)  A[0]:(0.520033419132) A[1]:(0.876920998096) A[2]:(-0.601016223431) A[3]:(0.843449473381)\n",
      " state (12)  A[0]:(0.0763788744807) A[1]:(0.824667334557) A[2]:(-0.567226350307) A[3]:(0.792920053005)\n",
      " state (13)  A[0]:(0.000166296958923) A[1]:(0.809548139572) A[2]:(0.900005996227) A[3]:(0.728945434093)\n",
      " state (14)  A[0]:(0.810063481331) A[1]:(0.900199353695) A[2]:(0.999999940395) A[3]:(0.809925496578)\n",
      " state (15)  A[0]:(0.985916078091) A[1]:(0.958618104458) A[2]:(1.0) A[3]:(0.884074091911)\n",
      "Episode 547000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6018. Times reached goal: 986.               Steps done: 4443296. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0105814213798.\n",
      " state (0)  A[0]:(0.5315939188) A[1]:(0.590344190598) A[2]:(0.59051835537) A[3]:(0.531698405743)\n",
      " state (1)  A[0]:(0.531633496284) A[1]:(-0.000129021704197) A[2]:(0.656134128571) A[3]:(0.590695500374)\n",
      " state (2)  A[0]:(0.590719103813) A[1]:(0.728966355324) A[2]:(0.590438365936) A[3]:(0.656319856644)\n",
      " state (3)  A[0]:(0.656301856041) A[1]:(-0.22192555666) A[2]:(0.536288261414) A[3]:(0.520188808441)\n",
      " state (4)  A[0]:(0.590658843517) A[1]:(0.655944705009) A[2]:(-0.000307559967041) A[3]:(0.531541347504)\n",
      " state (5)  A[0]:(0.161654248834) A[1]:(0.928847432137) A[2]:(-0.186278477311) A[3]:(0.518663465977)\n",
      " state (6)  A[0]:(0.000233799219131) A[1]:(0.809882700443) A[2]:(-0.000311255455017) A[3]:(0.656199455261)\n",
      " state (7)  A[0]:(0.63353562355) A[1]:(-0.252363860607) A[2]:(0.275892913342) A[3]:(0.890268743038)\n",
      " state (8)  A[0]:(0.656282901764) A[1]:(-0.000185646116734) A[2]:(0.728739500046) A[3]:(0.59082531929)\n",
      " state (9)  A[0]:(0.656220972538) A[1]:(0.809894442558) A[2]:(0.809769511223) A[3]:(0.000537469924893)\n",
      " state (10)  A[0]:(0.72904419899) A[1]:(0.899907469749) A[2]:(-0.000684499624185) A[3]:(0.729138314724)\n",
      " state (11)  A[0]:(0.519823849201) A[1]:(0.876779139042) A[2]:(-0.601578593254) A[3]:(0.843460321426)\n",
      " state (12)  A[0]:(0.0760592669249) A[1]:(0.824453473091) A[2]:(-0.56798517704) A[3]:(0.792978346348)\n",
      " state (13)  A[0]:(-0.000234037637711) A[1]:(0.80931186676) A[2]:(0.899795532227) A[3]:(0.729053199291)\n",
      " state (14)  A[0]:(0.809895157814) A[1]:(0.900074720383) A[2]:(0.999999940395) A[3]:(0.809993684292)\n",
      " state (15)  A[0]:(0.985892593861) A[1]:(0.958563506603) A[2]:(1.0) A[3]:(0.884085118771)\n",
      "Episode 548000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6038. Times reached goal: 994.               Steps done: 4449334. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0105177232557.\n",
      " state (0)  A[0]:(0.530967116356) A[1]:(0.590682983398) A[2]:(0.590542674065) A[3]:(0.531615018845)\n",
      " state (1)  A[0]:(0.530824899673) A[1]:(7.82236456871e-05) A[2]:(0.656078696251) A[3]:(0.590664029121)\n",
      " state (2)  A[0]:(0.589965224266) A[1]:(0.729093134403) A[2]:(0.590419173241) A[3]:(0.656302452087)\n",
      " state (3)  A[0]:(0.655707597733) A[1]:(-0.22204965353) A[2]:(0.536448895931) A[3]:(0.520117104053)\n",
      " state (4)  A[0]:(0.590022921562) A[1]:(0.656210064888) A[2]:(-8.82148742676e-05) A[3]:(0.53153026104)\n",
      " state (5)  A[0]:(0.160663247108) A[1]:(0.928899407387) A[2]:(-0.186015054584) A[3]:(0.518676936626)\n",
      " state (6)  A[0]:(-0.000960468954872) A[1]:(0.810012996197) A[2]:(4.43458557129e-05) A[3]:(0.656172037125)\n",
      " state (7)  A[0]:(0.632763206959) A[1]:(-0.252125710249) A[2]:(0.276438772678) A[3]:(0.890232980251)\n",
      " state (8)  A[0]:(0.655717253685) A[1]:(7.50049948692e-05) A[2]:(0.729013085365) A[3]:(0.590744376183)\n",
      " state (9)  A[0]:(0.655845463276) A[1]:(0.81005692482) A[2]:(0.809941887856) A[3]:(0.000386625499232)\n",
      " state (10)  A[0]:(0.728809595108) A[1]:(0.9000248909) A[2]:(-0.000349521636963) A[3]:(0.729071378708)\n",
      " state (11)  A[0]:(0.519475281239) A[1]:(0.876950621605) A[2]:(-0.601508021355) A[3]:(0.8434202075)\n",
      " state (12)  A[0]:(0.0755545645952) A[1]:(0.824736356735) A[2]:(-0.56797593832) A[3]:(0.792920231819)\n",
      " state (13)  A[0]:(-0.000671386602335) A[1]:(0.809680819511) A[2]:(0.899962961674) A[3]:(0.729005515575)\n",
      " state (14)  A[0]:(0.809826314449) A[1]:(0.900318801403) A[2]:(0.999999940395) A[3]:(0.810038805008)\n",
      " state (15)  A[0]:(0.985870361328) A[1]:(0.958666622639) A[2]:(1.0) A[3]:(0.884109318256)\n",
      "Episode 549000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6055. Times reached goal: 995.               Steps done: 4455389. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0104542308586.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5906,  0.5906,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6561,  0.0000,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559, -0.0004,  0.7290,  0.5908]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.8099,  0.8101, -0.0000]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0002,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9004,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53154361248) A[1]:(0.590617179871) A[2]:(0.590582847595) A[3]:(0.531528413296)\n",
      " state (1)  A[0]:(0.531542420387) A[1]:(-1.07660889626e-05) A[2]:(0.656106352806) A[3]:(0.590517401695)\n",
      " state (2)  A[0]:(0.590632081032) A[1]:(0.728982686996) A[2]:(0.59056699276) A[3]:(0.656108081341)\n",
      " state (3)  A[0]:(0.656336188316) A[1]:(-0.222958952188) A[2]:(0.536749839783) A[3]:(0.51991134882)\n",
      " state (4)  A[0]:(0.590782642365) A[1]:(0.656180202961) A[2]:(6.56843185425e-05) A[3]:(0.531438469887)\n",
      " state (5)  A[0]:(0.161813810468) A[1]:(0.928907334805) A[2]:(-0.185992389917) A[3]:(0.518593668938)\n",
      " state (6)  A[0]:(0.000254988670349) A[1]:(0.810034573078) A[2]:(9.6321105957e-05) A[3]:(0.656043410301)\n",
      " state (7)  A[0]:(0.633469641209) A[1]:(-0.252116054296) A[2]:(0.276656419039) A[3]:(0.890133976936)\n",
      " state (8)  A[0]:(0.65632724762) A[1]:(-0.000138439238071) A[2]:(0.729039490223) A[3]:(0.590606093407)\n",
      " state (9)  A[0]:(0.656292557716) A[1]:(0.809994399548) A[2]:(0.810073137283) A[3]:(-9.31024551392e-05)\n",
      " state (10)  A[0]:(0.729204297066) A[1]:(0.900034666061) A[2]:(3.57627868652e-06) A[3]:(0.728951871395)\n",
      " state (11)  A[0]:(0.520175457001) A[1]:(0.8769993186) A[2]:(-0.601447224617) A[3]:(0.843432605267)\n",
      " state (12)  A[0]:(0.0765149593353) A[1]:(0.824836730957) A[2]:(-0.568032264709) A[3]:(0.792970836163)\n",
      " state (13)  A[0]:(0.000215530395508) A[1]:(0.809812664986) A[2]:(0.900029957294) A[3]:(0.729040384293)\n",
      " state (14)  A[0]:(0.810076653957) A[1]:(0.900397181511) A[2]:(0.999999940395) A[3]:(0.809982776642)\n",
      " state (15)  A[0]:(0.985869944096) A[1]:(0.958688676357) A[2]:(1.0) A[3]:(0.883973360062)\n",
      "Episode 550000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6026. Times reached goal: 989.               Steps done: 4461415. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0103914230933.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531507015228) A[1]:(0.590272307396) A[2]:(0.590281963348) A[3]:(0.531175017357)\n",
      " state (1)  A[0]:(0.531471848488) A[1]:(-0.000147573649883) A[2]:(0.65606713295) A[3]:(0.590222120285)\n",
      " state (2)  A[0]:(0.590532422066) A[1]:(0.728812992573) A[2]:(0.590618729591) A[3]:(0.655902147293)\n",
      " state (3)  A[0]:(0.656178474426) A[1]:(-0.22308242321) A[2]:(0.536889672279) A[3]:(0.519675254822)\n",
      " state (4)  A[0]:(0.590639591217) A[1]:(0.655837714672) A[2]:(0.000363111466868) A[3]:(0.531269788742)\n",
      " state (5)  A[0]:(0.161791265011) A[1]:(0.928853154182) A[2]:(-0.185778200626) A[3]:(0.518555521965)\n",
      " state (6)  A[0]:(0.00051346415421) A[1]:(0.809854209423) A[2]:(0.000447154016001) A[3]:(0.656023025513)\n",
      " state (7)  A[0]:(0.633461833) A[1]:(-0.252508401871) A[2]:(0.277204990387) A[3]:(0.890023589134)\n",
      " state (8)  A[0]:(0.656117737293) A[1]:(-0.000365816027625) A[2]:(0.729328453541) A[3]:(0.590179383755)\n",
      " state (9)  A[0]:(0.65611076355) A[1]:(0.809878647327) A[2]:(0.810251414776) A[3]:(-0.000349372625351)\n",
      " state (10)  A[0]:(0.729135751724) A[1]:(0.899961054325) A[2]:(0.000379204720957) A[3]:(0.728985130787)\n",
      " state (11)  A[0]:(0.520173370838) A[1]:(0.876893520355) A[2]:(-0.601315379143) A[3]:(0.843494951725)\n",
      " state (12)  A[0]:(0.0766003206372) A[1]:(0.824652314186) A[2]:(-0.567977309227) A[3]:(0.793067455292)\n",
      " state (13)  A[0]:(0.000403732032282) A[1]:(0.809560120106) A[2]:(0.900138914585) A[3]:(0.72918510437)\n",
      " state (14)  A[0]:(0.810208559036) A[1]:(0.9002212286) A[2]:(0.999999940395) A[3]:(0.810135304928)\n",
      " state (15)  A[0]:(0.985869765282) A[1]:(0.958581566811) A[2]:(1.0) A[3]:(0.88406252861)\n",
      "Episode 551000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6035. Times reached goal: 992.               Steps done: 4467450. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.010328899709.\n",
      " state (0)  A[0]:(0.531779408455) A[1]:(0.590532541275) A[2]:(0.590450048447) A[3]:(0.531511068344)\n",
      " state (1)  A[0]:(0.531772851944) A[1]:(0.000134825706482) A[2]:(0.65601682663) A[3]:(0.590273439884)\n",
      " state (2)  A[0]:(0.590846061707) A[1]:(0.729085445404) A[2]:(0.590340614319) A[3]:(0.655874967575)\n",
      " state (3)  A[0]:(0.656434774399) A[1]:(-0.221936538815) A[2]:(0.536422133446) A[3]:(0.519707202911)\n",
      " state (4)  A[0]:(0.590892553329) A[1]:(0.656203210354) A[2]:(-0.000221133232117) A[3]:(0.531206190586)\n",
      " state (5)  A[0]:(0.162065863609) A[1]:(0.928914427757) A[2]:(-0.186349928379) A[3]:(0.518446207047)\n",
      " state (6)  A[0]:(0.000348091125488) A[1]:(0.810072422028) A[2]:(-0.000226259231567) A[3]:(0.655958414078)\n",
      " state (7)  A[0]:(0.633305191994) A[1]:(-0.251995801926) A[2]:(0.276489078999) A[3]:(0.890140891075)\n",
      " state (8)  A[0]:(0.656242370605) A[1]:(-2.11298465729e-05) A[2]:(0.728690743446) A[3]:(0.591245651245)\n",
      " state (9)  A[0]:(0.656099736691) A[1]:(0.809992969036) A[2]:(0.809802591801) A[3]:(0.00105619395617)\n",
      " state (10)  A[0]:(0.728797256947) A[1]:(0.899979233742) A[2]:(-0.000321984291077) A[3]:(0.729180634022)\n",
      " state (11)  A[0]:(0.519330978394) A[1]:(0.876880228519) A[2]:(-0.601646482944) A[3]:(0.843459069729)\n",
      " state (12)  A[0]:(0.075234644115) A[1]:(0.824618160725) A[2]:(-0.568411409855) A[3]:(0.792949080467)\n",
      " state (13)  A[0]:(-0.00107693625614) A[1]:(0.809523463249) A[2]:(0.900049209595) A[3]:(0.729005694389)\n",
      " state (14)  A[0]:(0.809725046158) A[1]:(0.900208473206) A[2]:(0.999999940395) A[3]:(0.810012996197)\n",
      " state (15)  A[0]:(0.985823273659) A[1]:(0.958572924137) A[2]:(1.0) A[3]:(0.88396948576)\n",
      "Episode 552000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6016. Times reached goal: 989.               Steps done: 4473466. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0102669475872.\n",
      " state (0)  A[0]:(0.527923107147) A[1]:(0.59049642086) A[2]:(0.590647041798) A[3]:(0.531571626663)\n",
      " state (1)  A[0]:(0.52813065052) A[1]:(0.000291384756565) A[2]:(0.656279444695) A[3]:(0.590672135353)\n",
      " state (2)  A[0]:(0.587814629078) A[1]:(0.729153037071) A[2]:(0.590652823448) A[3]:(0.656353116035)\n",
      " state (3)  A[0]:(0.654605507851) A[1]:(-0.221117809415) A[2]:(0.536568403244) A[3]:(0.520255684853)\n",
      " state (4)  A[0]:(0.589564502239) A[1]:(0.656230092049) A[2]:(-0.000165224075317) A[3]:(0.531738638878)\n",
      " state (5)  A[0]:(0.161003708839) A[1]:(0.928973972797) A[2]:(-0.186639651656) A[3]:(0.519169092178)\n",
      " state (6)  A[0]:(0.000465631455882) A[1]:(0.810091614723) A[2]:(-0.000466585130198) A[3]:(0.656567513943)\n",
      " state (7)  A[0]:(0.633529782295) A[1]:(-0.251953542233) A[2]:(0.276548981667) A[3]:(0.890166044235)\n",
      " state (8)  A[0]:(0.655962109566) A[1]:(0.000285364687443) A[2]:(0.728856563568) A[3]:(0.590564012527)\n",
      " state (9)  A[0]:(0.655733644962) A[1]:(0.81007963419) A[2]:(0.809948921204) A[3]:(-0.000257804989815)\n",
      " state (10)  A[0]:(0.728791713715) A[1]:(0.900054931641) A[2]:(-0.000505328120198) A[3]:(0.728971004486)\n",
      " state (11)  A[0]:(0.519698023796) A[1]:(0.876989364624) A[2]:(-0.602101206779) A[3]:(0.84349167347)\n",
      " state (12)  A[0]:(0.0759995952249) A[1]:(0.824763298035) A[2]:(-0.569193005562) A[3]:(0.793038249016)\n",
      " state (13)  A[0]:(-0.000260651111603) A[1]:(0.809642791748) A[2]:(0.899739682674) A[3]:(0.72908270359)\n",
      " state (14)  A[0]:(0.810004591942) A[1]:(0.900240182877) A[2]:(0.999999940395) A[3]:(0.810023069382)\n",
      " state (15)  A[0]:(0.985845983028) A[1]:(0.958571434021) A[2]:(1.0) A[3]:(0.88394588232)\n",
      "Episode 553000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6013. Times reached goal: 987.               Steps done: 4479479. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0102053976666.\n",
      " state (0)  A[0]:(0.531461834908) A[1]:(0.590251326561) A[2]:(0.590306818485) A[3]:(0.531248033047)\n",
      " state (1)  A[0]:(0.531403958797) A[1]:(0.000273004174232) A[2]:(0.65591609478) A[3]:(0.590106844902)\n",
      " state (2)  A[0]:(0.590520203114) A[1]:(0.729003667831) A[2]:(0.59021961689) A[3]:(0.655692219734)\n",
      " state (3)  A[0]:(0.656191706657) A[1]:(-0.222507819533) A[2]:(0.536369204521) A[3]:(0.519216656685)\n",
      " state (4)  A[0]:(0.590691864491) A[1]:(0.656091690063) A[2]:(-0.000508427561726) A[3]:(0.530644774437)\n",
      " state (5)  A[0]:(0.161945462227) A[1]:(0.928909361362) A[2]:(-0.186847671866) A[3]:(0.517835855484)\n",
      " state (6)  A[0]:(0.000787854020018) A[1]:(0.80996888876) A[2]:(-0.000697493436746) A[3]:(0.655248999596)\n",
      " state (7)  A[0]:(0.633711695671) A[1]:(-0.252236306667) A[2]:(0.276434600353) A[3]:(0.889581918716)\n",
      " state (8)  A[0]:(0.656345129013) A[1]:(8.10176134109e-05) A[2]:(0.728910207748) A[3]:(0.58879160881)\n",
      " state (9)  A[0]:(0.656219005585) A[1]:(0.809993207455) A[2]:(0.809890627861) A[3]:(-0.00201008934528)\n",
      " state (10)  A[0]:(0.729053854942) A[1]:(0.899962246418) A[2]:(-0.000670313718729) A[3]:(0.728166103363)\n",
      " state (11)  A[0]:(0.519889354706) A[1]:(0.87684994936) A[2]:(-0.602198839188) A[3]:(0.842929601669)\n",
      " state (12)  A[0]:(0.0760935172439) A[1]:(0.824561059475) A[2]:(-0.569243729115) A[3]:(0.792295098305)\n",
      " state (13)  A[0]:(-0.000100672245026) A[1]:(0.80944955349) A[2]:(0.899974107742) A[3]:(0.728226661682)\n",
      " state (14)  A[0]:(0.810147285461) A[1]:(0.900164008141) A[2]:(0.999999940395) A[3]:(0.809560716152)\n",
      " state (15)  A[0]:(0.985835075378) A[1]:(0.958527624607) A[2]:(1.0) A[3]:(0.883691370487)\n",
      "Episode 554000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6039. Times reached goal: 992.               Steps done: 4485518. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0101439529891.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5903,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314, -0.0002,  0.6558,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7288,  0.5903,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  0.8100,  0.0001,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=2 , Random? True\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.531360268593) A[1]:(0.590306401253) A[2]:(0.590427458286) A[3]:(0.531548500061)\n",
      " state (1)  A[0]:(0.531417250633) A[1]:(-7.38352537155e-06) A[2]:(0.655859470367) A[3]:(0.590597033501)\n",
      " state (2)  A[0]:(0.590502381325) A[1]:(0.728874504566) A[2]:(0.590148508549) A[3]:(0.656196832657)\n",
      " state (3)  A[0]:(0.656104683876) A[1]:(-0.223017737269) A[2]:(0.536302387714) A[3]:(0.51996421814)\n",
      " state (4)  A[0]:(0.590488314629) A[1]:(0.655924320221) A[2]:(-0.000615119875874) A[3]:(0.531470775604)\n",
      " state (5)  A[0]:(0.161541312933) A[1]:(0.928863048553) A[2]:(-0.186836749315) A[3]:(0.51873844862)\n",
      " state (6)  A[0]:(0.000476926536066) A[1]:(0.809813261032) A[2]:(-0.00046598908375) A[3]:(0.656084656715)\n",
      " state (7)  A[0]:(0.633544564247) A[1]:(-0.252592861652) A[2]:(0.276630997658) A[3]:(0.890002369881)\n",
      " state (8)  A[0]:(0.656404554844) A[1]:(-0.000521384121384) A[2]:(0.728484392166) A[3]:(0.591027081013)\n",
      " state (9)  A[0]:(0.656275868416) A[1]:(0.809787273407) A[2]:(0.809743285179) A[3]:(0.000705450656824)\n",
      " state (10)  A[0]:(0.729091286659) A[1]:(0.899885296822) A[2]:(-0.000432491273386) A[3]:(0.729237794876)\n",
      " state (11)  A[0]:(0.520023345947) A[1]:(0.876772761345) A[2]:(-0.602004647255) A[3]:(0.843627095222)\n",
      " state (12)  A[0]:(0.0763011947274) A[1]:(0.824450016022) A[2]:(-0.569185137749) A[3]:(0.793214797974)\n",
      " state (13)  A[0]:(-4.44948673248e-05) A[1]:(0.809285700321) A[2]:(0.899889469147) A[3]:(0.729296147823)\n",
      " state (14)  A[0]:(0.810097455978) A[1]:(0.90002822876) A[2]:(0.999999940395) A[3]:(0.810123443604)\n",
      " state (15)  A[0]:(0.985829591751) A[1]:(0.958451032639) A[2]:(1.0) A[3]:(0.883903622627)\n",
      "Episode 555000 finished after 0 timesteps with r=0.0. Running score: 0.98. Times trained:               6016. Times reached goal: 987.               Steps done: 4491534. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0100831101666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531457781792) A[1]:(0.590545892715) A[2]:(0.590626776218) A[3]:(0.53170466423)\n",
      " state (1)  A[0]:(0.531522333622) A[1]:(-8.69259238243e-05) A[2]:(0.656095385551) A[3]:(0.590680956841)\n",
      " state (2)  A[0]:(0.590540587902) A[1]:(0.728943824768) A[2]:(0.590681910515) A[3]:(0.6562281847)\n",
      " state (3)  A[0]:(0.65617954731) A[1]:(-0.223074316978) A[2]:(0.536970555782) A[3]:(0.519974768162)\n",
      " state (4)  A[0]:(0.590505540371) A[1]:(0.656165838242) A[2]:(0.000122547149658) A[3]:(0.531552374363)\n",
      " state (5)  A[0]:(0.161397323012) A[1]:(0.928933560848) A[2]:(-0.186313331127) A[3]:(0.518883705139)\n",
      " state (6)  A[0]:(0.000244975090027) A[1]:(0.810001134872) A[2]:(-6.55651092529e-05) A[3]:(0.656243681908)\n",
      " state (7)  A[0]:(0.633422076702) A[1]:(-0.251923650503) A[2]:(0.277064681053) A[3]:(0.890037477016)\n",
      " state (8)  A[0]:(0.656355023384) A[1]:(0.000655270996504) A[2]:(0.728955745697) A[3]:(0.590577840805)\n",
      " state (9)  A[0]:(0.656473457813) A[1]:(0.810176968575) A[2]:(0.809994995594) A[3]:(0.000217333436012)\n",
      " state (10)  A[0]:(0.729315757751) A[1]:(0.90002399683) A[2]:(0.000156998634338) A[3]:(0.728963911533)\n",
      " state (11)  A[0]:(0.520429491997) A[1]:(0.87686675787) A[2]:(-0.601673007011) A[3]:(0.843390226364)\n",
      " state (12)  A[0]:(0.0769727528095) A[1]:(0.824488162994) A[2]:(-0.568941533566) A[3]:(0.792894423008)\n",
      " state (13)  A[0]:(0.000690877321176) A[1]:(0.809225201607) A[2]:(0.900019049644) A[3]:(0.72894525528)\n",
      " state (14)  A[0]:(0.810301542282) A[1]:(0.899933815002) A[2]:(0.999999940395) A[3]:(0.809981644154)\n",
      " state (15)  A[0]:(0.985827803612) A[1]:(0.958372414112) A[2]:(1.0) A[3]:(0.883862733841)\n",
      "Episode 556000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               5997. Times reached goal: 982.               Steps done: 4497531. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0100228227075.\n",
      " state (0)  A[0]:(0.531335890293) A[1]:(0.590230703354) A[2]:(0.590400993824) A[3]:(0.531263291836)\n",
      " state (1)  A[0]:(0.53126013279) A[1]:(-4.24683094025e-07) A[2]:(0.656071722507) A[3]:(0.590433716774)\n",
      " state (2)  A[0]:(0.590364336967) A[1]:(0.728908777237) A[2]:(0.590549528599) A[3]:(0.656079888344)\n",
      " state (3)  A[0]:(0.655800104141) A[1]:(-0.223098680377) A[2]:(0.536749184132) A[3]:(0.519820928574)\n",
      " state (4)  A[0]:(0.59015327692) A[1]:(0.655881285667) A[2]:(-1.87158584595e-05) A[3]:(0.531314373016)\n",
      " state (5)  A[0]:(0.161111116409) A[1]:(0.92886453867) A[2]:(-0.186320349574) A[3]:(0.51851350069)\n",
      " state (6)  A[0]:(-0.000432968110545) A[1]:(0.809949874878) A[2]:(1.2993812561e-05) A[3]:(0.655686855316)\n",
      " state (7)  A[0]:(0.632701039314) A[1]:(-0.252167224884) A[2]:(0.277340561152) A[3]:(0.889749526978)\n",
      " state (8)  A[0]:(0.655656278133) A[1]:(-0.00012568384409) A[2]:(0.728997588158) A[3]:(0.589983701706)\n",
      " state (9)  A[0]:(0.655656576157) A[1]:(0.809985399246) A[2]:(0.810052394867) A[3]:(-0.000677645090036)\n",
      " state (10)  A[0]:(0.728781461716) A[1]:(0.900009691715) A[2]:(-8.82148742676e-06) A[3]:(0.728834867477)\n",
      " state (11)  A[0]:(0.519791841507) A[1]:(0.876950502396) A[2]:(-0.602034211159) A[3]:(0.843437552452)\n",
      " state (12)  A[0]:(0.0761921852827) A[1]:(0.824747085571) A[2]:(-0.569421350956) A[3]:(0.792978346348)\n",
      " state (13)  A[0]:(-7.11679458618e-05) A[1]:(0.809684514999) A[2]:(0.900025725365) A[3]:(0.729018211365)\n",
      " state (14)  A[0]:(0.810024023056) A[1]:(0.900305509567) A[2]:(0.999999940395) A[3]:(0.810007810593)\n",
      " state (15)  A[0]:(0.985781550407) A[1]:(0.958568394184) A[2]:(1.0) A[3]:(0.883802175522)\n",
      "Episode 557000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6011. Times reached goal: 987.               Steps done: 4503542. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0099627562309.\n",
      " state (0)  A[0]:(0.531444907188) A[1]:(0.590428352356) A[2]:(0.590411961079) A[3]:(0.531551361084)\n",
      " state (1)  A[0]:(0.531448602676) A[1]:(-5.01200556755e-05) A[2]:(0.6560754776) A[3]:(0.590547561646)\n",
      " state (2)  A[0]:(0.590444684029) A[1]:(0.728909671307) A[2]:(0.590661883354) A[3]:(0.656157672405)\n",
      " state (3)  A[0]:(0.656085371971) A[1]:(-0.222997769713) A[2]:(0.536859631538) A[3]:(0.519864857197)\n",
      " state (4)  A[0]:(0.590401649475) A[1]:(0.655962586403) A[2]:(5.48362731934e-06) A[3]:(0.531459569931)\n",
      " state (5)  A[0]:(0.1612855196) A[1]:(0.928882837296) A[2]:(-0.186423808336) A[3]:(0.518848121166)\n",
      " state (6)  A[0]:(1.99377536774e-05) A[1]:(0.80994361639) A[2]:(-4.30345535278e-05) A[3]:(0.65611988306)\n",
      " state (7)  A[0]:(0.633363366127) A[1]:(-0.252305060625) A[2]:(0.277511775494) A[3]:(0.889960944653)\n",
      " state (8)  A[0]:(0.656303882599) A[1]:(-0.000213138759136) A[2]:(0.729050993919) A[3]:(0.5906265378)\n",
      " state (9)  A[0]:(0.656125426292) A[1]:(0.809941768646) A[2]:(0.810045540333) A[3]:(9.81688499451e-05)\n",
      " state (10)  A[0]:(0.729021787643) A[1]:(0.899969518185) A[2]:(-3.77893447876e-05) A[3]:(0.729066073895)\n",
      " state (11)  A[0]:(0.520024955273) A[1]:(0.876886546612) A[2]:(-0.602133393288) A[3]:(0.843533992767)\n",
      " state (12)  A[0]:(0.0763563290238) A[1]:(0.824638962746) A[2]:(-0.569693446159) A[3]:(0.793071389198)\n",
      " state (13)  A[0]:(-9.50694084167e-05) A[1]:(0.809538424015) A[2]:(0.899940133095) A[3]:(0.72908949852)\n",
      " state (14)  A[0]:(0.809953927994) A[1]:(0.900201320648) A[2]:(0.999999940395) A[3]:(0.810002088547)\n",
      " state (15)  A[0]:(0.985764861107) A[1]:(0.958508372307) A[2]:(1.0) A[3]:(0.883746504784)\n",
      "Episode 558000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6020. Times reached goal: 986.               Steps done: 4509562. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00990296060382.\n",
      " state (0)  A[0]:(0.531594693661) A[1]:(0.590578556061) A[2]:(0.590452134609) A[3]:(0.531358003616)\n",
      " state (1)  A[0]:(0.531539320946) A[1]:(-3.29539179802e-05) A[2]:(0.656059741974) A[3]:(0.590423464775)\n",
      " state (2)  A[0]:(0.590610325336) A[1]:(0.728997707367) A[2]:(0.590509295464) A[3]:(0.656116425991)\n",
      " state (3)  A[0]:(0.656328558922) A[1]:(-0.221974506974) A[2]:(0.536655247211) A[3]:(0.519801080227)\n",
      " state (4)  A[0]:(0.590770483017) A[1]:(0.65611410141) A[2]:(-5.24520874023e-05) A[3]:(0.531359910965)\n",
      " state (5)  A[0]:(0.161899164319) A[1]:(0.92889893055) A[2]:(-0.186427593231) A[3]:(0.518833220005)\n",
      " state (6)  A[0]:(0.000481128663523) A[1]:(0.809958040714) A[2]:(3.09944152832e-05) A[3]:(0.656136989594)\n",
      " state (7)  A[0]:(0.633327662945) A[1]:(-0.252188056707) A[2]:(0.277611136436) A[3]:(0.889958262444)\n",
      " state (8)  A[0]:(0.656390309334) A[1]:(-9.0204179287e-05) A[2]:(0.728920102119) A[3]:(0.590960502625)\n",
      " state (9)  A[0]:(0.65642118454) A[1]:(0.809982776642) A[2]:(0.809993267059) A[3]:(0.000481069058878)\n",
      " state (10)  A[0]:(0.729306519032) A[1]:(0.89998793602) A[2]:(-5.36441802979e-06) A[3]:(0.729109525681)\n",
      " state (11)  A[0]:(0.520494103432) A[1]:(0.876888692379) A[2]:(-0.602154374123) A[3]:(0.843557536602)\n",
      " state (12)  A[0]:(0.0770237371325) A[1]:(0.82460039854) A[2]:(-0.569761753082) A[3]:(0.793132483959)\n",
      " state (13)  A[0]:(0.000713795307092) A[1]:(0.809439659119) A[2]:(0.900042176247) A[3]:(0.729241132736)\n",
      " state (14)  A[0]:(0.810356914997) A[1]:(0.900104701519) A[2]:(0.999999940395) A[3]:(0.810211956501)\n",
      " state (15)  A[0]:(0.985790491104) A[1]:(0.958430945873) A[2]:(1.0) A[3]:(0.883901059628)\n",
      "Episode 559000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6037. Times reached goal: 992.               Steps done: 4515599. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00984335652658.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5902,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0000,  0.6561,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.7289,  0.5905,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8099, -0.0003,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7285,  0.9001,  0.0002,  0.7284]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8095,  0.9002,  1.0000,  0.8095]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531206905842) A[1]:(0.590212583542) A[2]:(0.590322732925) A[3]:(0.532431006432)\n",
      " state (1)  A[0]:(0.531110703945) A[1]:(-0.000103801488876) A[2]:(0.655997276306) A[3]:(0.591233551502)\n",
      " state (2)  A[0]:(0.590173482895) A[1]:(0.728941440582) A[2]:(0.590495467186) A[3]:(0.656771421432)\n",
      " state (3)  A[0]:(0.65578186512) A[1]:(-0.221759021282) A[2]:(0.536576032639) A[3]:(0.520809412003)\n",
      " state (4)  A[0]:(0.590211093426) A[1]:(0.655688285828) A[2]:(-0.000112652778625) A[3]:(0.532323598862)\n",
      " state (5)  A[0]:(0.161397233605) A[1]:(0.928855717182) A[2]:(-0.186715587974) A[3]:(0.519810378551)\n",
      " state (6)  A[0]:(0.000126272439957) A[1]:(0.809918105602) A[2]:(-0.000339031219482) A[3]:(0.656652331352)\n",
      " state (7)  A[0]:(0.632913827896) A[1]:(-0.252257555723) A[2]:(0.277279049158) A[3]:(0.889938294888)\n",
      " state (8)  A[0]:(0.65576338768) A[1]:(-0.000374615163309) A[2]:(0.728615999222) A[3]:(0.590819835663)\n",
      " state (9)  A[0]:(0.655534863472) A[1]:(0.809865117073) A[2]:(0.80986571312) A[3]:(-0.000195100903511)\n",
      " state (10)  A[0]:(0.728600740433) A[1]:(0.899960517883) A[2]:(-0.000399112672312) A[3]:(0.728779554367)\n",
      " state (11)  A[0]:(0.519501924515) A[1]:(0.876900136471) A[2]:(-0.602565467358) A[3]:(0.84336233139)\n",
      " state (12)  A[0]:(0.0757005885243) A[1]:(0.824675440788) A[2]:(-0.570371985435) A[3]:(0.792817890644)\n",
      " state (13)  A[0]:(-0.000794887368102) A[1]:(0.809582829475) A[2]:(0.899864792824) A[3]:(0.728692412376)\n",
      " state (14)  A[0]:(0.809674799442) A[1]:(0.900219619274) A[2]:(0.999999940395) A[3]:(0.809639275074)\n",
      " state (15)  A[0]:(0.985713720322) A[1]:(0.958492040634) A[2]:(1.0) A[3]:(0.883410513401)\n",
      "Episode 560000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6017. Times reached goal: 992.               Steps done: 4521616. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00978430687938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532087445259) A[1]:(0.590429067612) A[2]:(0.59040749073) A[3]:(0.532324552536)\n",
      " state (1)  A[0]:(0.532071828842) A[1]:(-2.35065817833e-05) A[2]:(0.656128883362) A[3]:(0.591437518597)\n",
      " state (2)  A[0]:(0.591064929962) A[1]:(0.728955626488) A[2]:(0.590503096581) A[3]:(0.656980574131)\n",
      " state (3)  A[0]:(0.656607151031) A[1]:(-0.220741719007) A[2]:(0.536486268044) A[3]:(0.520677685738)\n",
      " state (4)  A[0]:(0.591040432453) A[1]:(0.655651152134) A[2]:(-1.1682510376e-05) A[3]:(0.531841039658)\n",
      " state (5)  A[0]:(0.162423714995) A[1]:(0.928832769394) A[2]:(-0.186622053385) A[3]:(0.519254684448)\n",
      " state (6)  A[0]:(0.000827312294859) A[1]:(0.809937953949) A[2]:(-0.000225067138672) A[3]:(0.656197667122)\n",
      " state (7)  A[0]:(0.633191347122) A[1]:(-0.252065569162) A[2]:(0.277666151524) A[3]:(0.889775514603)\n",
      " state (8)  A[0]:(0.655992627144) A[1]:(-0.000213675200939) A[2]:(0.729059457779) A[3]:(0.590228915215)\n",
      " state (9)  A[0]:(0.655822455883) A[1]:(0.809858858585) A[2]:(0.810081541538) A[3]:(-0.000147104263306)\n",
      " state (10)  A[0]:(0.728902935982) A[1]:(0.899937689304) A[2]:(-8.6784362793e-05) A[3]:(0.72910130024)\n",
      " state (11)  A[0]:(0.52004379034) A[1]:(0.876862823963) A[2]:(-0.602457940578) A[3]:(0.843595027924)\n",
      " state (12)  A[0]:(0.0765221565962) A[1]:(0.824604451656) A[2]:(-0.570272088051) A[3]:(0.793116390705)\n",
      " state (13)  A[0]:(0.000175416469574) A[1]:(0.809479117393) A[2]:(0.900050580502) A[3]:(0.729076385498)\n",
      " state (14)  A[0]:(0.810077786446) A[1]:(0.900140285492) A[2]:(0.999999940395) A[3]:(0.809949755669)\n",
      " state (15)  A[0]:(0.985730588436) A[1]:(0.958428144455) A[2]:(1.0) A[3]:(0.883575320244)\n",
      "Episode 561000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6011. Times reached goal: 991.               Steps done: 4527627. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00972566982096.\n",
      " state (0)  A[0]:(0.531514048576) A[1]:(0.590416550636) A[2]:(0.590501725674) A[3]:(0.531749010086)\n",
      " state (1)  A[0]:(0.531485378742) A[1]:(3.72156500816e-05) A[2]:(0.656039714813) A[3]:(0.590743899345)\n",
      " state (2)  A[0]:(0.590570688248) A[1]:(0.728964686394) A[2]:(0.590495467186) A[3]:(0.656343638897)\n",
      " state (3)  A[0]:(0.656023621559) A[1]:(-0.222452878952) A[2]:(0.53679138422) A[3]:(0.520079255104)\n",
      " state (4)  A[0]:(0.590361356735) A[1]:(0.656108736992) A[2]:(1.19209289551e-06) A[3]:(0.53166282177)\n",
      " state (5)  A[0]:(0.161379620433) A[1]:(0.928917825222) A[2]:(-0.186545878649) A[3]:(0.519128918648)\n",
      " state (6)  A[0]:(-3.71932983398e-05) A[1]:(0.810016036034) A[2]:(-7.20024108887e-05) A[3]:(0.656115174294)\n",
      " state (7)  A[0]:(0.632885932922) A[1]:(-0.25201806426) A[2]:(0.277814656496) A[3]:(0.889733016491)\n",
      " state (8)  A[0]:(0.655923187733) A[1]:(0.000167660415173) A[2]:(0.728964328766) A[3]:(0.590207874775)\n",
      " state (9)  A[0]:(0.655877053738) A[1]:(0.810054838657) A[2]:(0.809995293617) A[3]:(-0.00038573142956)\n",
      " state (10)  A[0]:(0.72880512476) A[1]:(0.900012433529) A[2]:(-9.83476638794e-05) A[3]:(0.728729188442)\n",
      " state (11)  A[0]:(0.519703149796) A[1]:(0.876915454865) A[2]:(-0.602478265762) A[3]:(0.843296170235)\n",
      " state (12)  A[0]:(0.0759297832847) A[1]:(0.824647068977) A[2]:(-0.570426702499) A[3]:(0.792734801769)\n",
      " state (13)  A[0]:(-0.000374287337763) A[1]:(0.809513032436) A[2]:(0.900035977364) A[3]:(0.728644013405)\n",
      " state (14)  A[0]:(0.81003767252) A[1]:(0.900166511536) A[2]:(0.999999940395) A[3]:(0.809691309929)\n",
      " state (15)  A[0]:(0.985731303692) A[1]:(0.958436906338) A[2]:(1.0) A[3]:(0.88342487812)\n",
      "Episode 562000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6035. Times reached goal: 993.               Steps done: 4533662. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00966715215825.\n",
      " state (0)  A[0]:(0.532041072845) A[1]:(0.590464055538) A[2]:(0.590454280376) A[3]:(0.531582772732)\n",
      " state (1)  A[0]:(0.532087445259) A[1]:(-4.60743904114e-05) A[2]:(0.656089305878) A[3]:(0.590584158897)\n",
      " state (2)  A[0]:(0.591139018536) A[1]:(0.729005098343) A[2]:(0.590556561947) A[3]:(0.656207203865)\n",
      " state (3)  A[0]:(0.656551241875) A[1]:(-0.221602350473) A[2]:(0.536742687225) A[3]:(0.520106911659)\n",
      " state (4)  A[0]:(0.590984046459) A[1]:(0.656110882759) A[2]:(5.90085983276e-05) A[3]:(0.531664729118)\n",
      " state (5)  A[0]:(0.16238720715) A[1]:(0.928915679455) A[2]:(-0.1865324229) A[3]:(0.519202232361)\n",
      " state (6)  A[0]:(0.000987648614682) A[1]:(0.810027062893) A[2]:(-2.09808349609e-05) A[3]:(0.656291007996)\n",
      " state (7)  A[0]:(0.633435606956) A[1]:(-0.251900792122) A[2]:(0.278019607067) A[3]:(0.889864206314)\n",
      " state (8)  A[0]:(0.656523704529) A[1]:(0.000396758288844) A[2]:(0.729086875916) A[3]:(0.590746700764)\n",
      " state (9)  A[0]:(0.65659070015) A[1]:(0.810106277466) A[2]:(0.810057640076) A[3]:(0.000726193073206)\n",
      " state (10)  A[0]:(0.729414641857) A[1]:(0.89999628067) A[2]:(0.000159978866577) A[3]:(0.729318618774)\n",
      " state (11)  A[0]:(0.520675420761) A[1]:(0.876845240593) A[2]:(-0.602341473103) A[3]:(0.843683242798)\n",
      " state (12)  A[0]:(0.0772393494844) A[1]:(0.824475944042) A[2]:(-0.570453286171) A[3]:(0.79326570034)\n",
      " state (13)  A[0]:(0.000743925455026) A[1]:(0.809226453304) A[2]:(0.899967908859) A[3]:(0.729327917099)\n",
      " state (14)  A[0]:(0.81029266119) A[1]:(0.899940550327) A[2]:(0.999999940395) A[3]:(0.81018102169)\n",
      " state (15)  A[0]:(0.985739707947) A[1]:(0.958308398724) A[2]:(1.0) A[3]:(0.88372117281)\n",
      "Episode 563000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6027. Times reached goal: 994.               Steps done: 4539689. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00960906345833.\n",
      " state (0)  A[0]:(0.531671226025) A[1]:(0.590472579002) A[2]:(0.590453386307) A[3]:(0.531176328659)\n",
      " state (1)  A[0]:(0.531895518303) A[1]:(0.000119879841805) A[2]:(0.656076848507) A[3]:(0.590309858322)\n",
      " state (2)  A[0]:(0.590977668762) A[1]:(0.728991389275) A[2]:(0.590477347374) A[3]:(0.656047821045)\n",
      " state (3)  A[0]:(0.656388759613) A[1]:(-0.222324758768) A[2]:(0.536718845367) A[3]:(0.519852221012)\n",
      " state (4)  A[0]:(0.590569376945) A[1]:(0.65618044138) A[2]:(1.82390213013e-05) A[3]:(0.531585812569)\n",
      " state (5)  A[0]:(0.1613381356) A[1]:(0.928891539574) A[2]:(-0.186298266053) A[3]:(0.519201755524)\n",
      " state (6)  A[0]:(-0.000396072835429) A[1]:(0.810045182705) A[2]:(0.000355720490916) A[3]:(0.656227231026)\n",
      " state (7)  A[0]:(0.632953286171) A[1]:(-0.251882463694) A[2]:(0.278470635414) A[3]:(0.889851272106)\n",
      " state (8)  A[0]:(0.656772613525) A[1]:(-3.60459089279e-05) A[2]:(0.729053318501) A[3]:(0.591258168221)\n",
      " state (9)  A[0]:(0.657227396965) A[1]:(0.810017049313) A[2]:(0.810032129288) A[3]:(0.00116793753114)\n",
      " state (10)  A[0]:(0.730143666267) A[1]:(0.900026082993) A[2]:(7.73668289185e-05) A[3]:(0.729415714741)\n",
      " state (11)  A[0]:(0.522013664246) A[1]:(0.876957058907) A[2]:(-0.602538347244) A[3]:(0.843766748905)\n",
      " state (12)  A[0]:(0.0791500210762) A[1]:(0.824717104435) A[2]:(-0.57079064846) A[3]:(0.793390631676)\n",
      " state (13)  A[0]:(0.00255306996405) A[1]:(0.809570372105) A[2]:(0.899933755398) A[3]:(0.729505896568)\n",
      " state (14)  A[0]:(0.810778796673) A[1]:(0.900172412395) A[2]:(0.999999940395) A[3]:(0.810361027718)\n",
      " state (15)  A[0]:(0.985753834248) A[1]:(0.958414912224) A[2]:(1.0) A[3]:(0.883834004402)\n",
      "Episode 564000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6034. Times reached goal: 993.               Steps done: 4545723. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00955125694705.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5903,  0.5906,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315, -0.0001,  0.6562,  0.5906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7290,  0.5908,  0.6563]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  0.8100, -0.0001,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 7.2920e-01,  9.0001e-01,  2.3842e-07,  7.2902e-01]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9000,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531349420547) A[1]:(0.590330004692) A[2]:(0.59068274498) A[3]:(0.531593561172)\n",
      " state (1)  A[0]:(0.531415939331) A[1]:(-3.94657254219e-05) A[2]:(0.656243741512) A[3]:(0.590700387955)\n",
      " state (2)  A[0]:(0.590534746647) A[1]:(0.729030072689) A[2]:(0.590700984001) A[3]:(0.656318306923)\n",
      " state (3)  A[0]:(0.656167566776) A[1]:(-0.221650257707) A[2]:(0.53680741787) A[3]:(0.520042419434)\n",
      " state (4)  A[0]:(0.590573430061) A[1]:(0.656065165997) A[2]:(1.53779983521e-05) A[3]:(0.531528115273)\n",
      " state (5)  A[0]:(0.161690235138) A[1]:(0.928901433945) A[2]:(-0.186676591635) A[3]:(0.519063651562)\n",
      " state (6)  A[0]:(0.000180184841156) A[1]:(0.810038089752) A[2]:(-0.000146508216858) A[3]:(0.656030893326)\n",
      " state (7)  A[0]:(0.632983207703) A[1]:(-0.251773327589) A[2]:(0.278108268976) A[3]:(0.889667212963)\n",
      " state (8)  A[0]:(0.656266689301) A[1]:(0.000290028750896) A[2]:(0.728944420815) A[3]:(0.590437173843)\n",
      " state (9)  A[0]:(0.656338691711) A[1]:(0.8100669384) A[2]:(0.809984385967) A[3]:(0.000120848417282)\n",
      " state (10)  A[0]:(0.729216217995) A[1]:(0.900008559227) A[2]:(-0.000124096870422) A[3]:(0.729028761387)\n",
      " state (11)  A[0]:(0.520399332047) A[1]:(0.876888751984) A[2]:(-0.602761983871) A[3]:(0.843540966511)\n",
      " state (12)  A[0]:(0.0768321454525) A[1]:(0.824564933777) A[2]:(-0.571091651917) A[3]:(0.793095231056)\n",
      " state (13)  A[0]:(0.000276207923889) A[1]:(0.809357047081) A[2]:(0.899977207184) A[3]:(0.729125022888)\n",
      " state (14)  A[0]:(0.810087680817) A[1]:(0.900035083294) A[2]:(0.999999940395) A[3]:(0.810061275959)\n",
      " state (15)  A[0]:(0.98568624258) A[1]:(0.95833170414) A[2]:(1.0) A[3]:(0.883584797382)\n",
      "Episode 565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6027. Times reached goal: 986.               Steps done: 4551750. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00949386464686.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531585812569) A[1]:(0.590582966805) A[2]:(0.590558648109) A[3]:(0.531788945198)\n",
      " state (1)  A[0]:(0.531434714794) A[1]:(1.58846378326e-05) A[2]:(0.656214356422) A[3]:(0.591005563736)\n",
      " state (2)  A[0]:(0.590432822704) A[1]:(0.729083061218) A[2]:(0.590786516666) A[3]:(0.656679391861)\n",
      " state (3)  A[0]:(0.655865550041) A[1]:(-0.221591860056) A[2]:(0.536973416805) A[3]:(0.520681738853)\n",
      " state (4)  A[0]:(0.590171337128) A[1]:(0.656093001366) A[2]:(0.000246644020081) A[3]:(0.532346904278)\n",
      " state (5)  A[0]:(0.161206766963) A[1]:(0.928897976875) A[2]:(-0.186469137669) A[3]:(0.520076751709)\n",
      " state (6)  A[0]:(0.000190645456314) A[1]:(0.8098295331) A[2]:(0.000319242477417) A[3]:(0.657009780407)\n",
      " state (7)  A[0]:(0.63291567564) A[1]:(-0.252536416054) A[2]:(0.278875470161) A[3]:(0.890010893345)\n",
      " state (8)  A[0]:(0.655536651611) A[1]:(-0.000207334756851) A[2]:(0.729290008545) A[3]:(0.591282844543)\n",
      " state (9)  A[0]:(0.655024170876) A[1]:(0.80993616581) A[2]:(0.810207664967) A[3]:(0.00113999797031)\n",
      " state (10)  A[0]:(0.727975130081) A[1]:(0.899973452091) A[2]:(0.000300288200378) A[3]:(0.729519605637)\n",
      " state (11)  A[0]:(0.518382668495) A[1]:(0.876891195774) A[2]:(-0.602660894394) A[3]:(0.843839049339)\n",
      " state (12)  A[0]:(0.0739635974169) A[1]:(0.824633836746) A[2]:(-0.571106076241) A[3]:(0.793422520161)\n",
      " state (13)  A[0]:(-0.00275468127802) A[1]:(0.809511065483) A[2]:(0.900047302246) A[3]:(0.72943675518)\n",
      " state (14)  A[0]:(0.808981359005) A[1]:(0.900170564651) A[2]:(0.999999940395) A[3]:(0.810199558735)\n",
      " state (15)  A[0]:(0.985574841499) A[1]:(0.958401143551) A[2]:(1.0) A[3]:(0.883578598499)\n",
      "Episode 566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6039. Times reached goal: 992.               Steps done: 4557789. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00943670396864.\n",
      " state (0)  A[0]:(0.531449019909) A[1]:(0.590445160866) A[2]:(0.590519487858) A[3]:(0.531250953674)\n",
      " state (1)  A[0]:(0.531376183033) A[1]:(-4.20957803726e-06) A[2]:(0.656128108501) A[3]:(0.590078294277)\n",
      " state (2)  A[0]:(0.590300738811) A[1]:(0.729019522667) A[2]:(0.590559303761) A[3]:(0.655750334263)\n",
      " state (3)  A[0]:(0.655727505684) A[1]:(-0.221228376031) A[2]:(0.536673545837) A[3]:(0.519420981407)\n",
      " state (4)  A[0]:(0.58975738287) A[1]:(0.656101226807) A[2]:(-8.48770141602e-05) A[3]:(0.531079053879)\n",
      " state (5)  A[0]:(0.160187542439) A[1]:(0.928910911083) A[2]:(-0.186810389161) A[3]:(0.518830537796)\n",
      " state (6)  A[0]:(-0.00139355566353) A[1]:(0.810023963451) A[2]:(-0.000141263008118) A[3]:(0.655893862247)\n",
      " state (7)  A[0]:(0.631941318512) A[1]:(-0.251789182425) A[2]:(0.278407633305) A[3]:(0.889535307884)\n",
      " state (8)  A[0]:(0.655269503593) A[1]:(0.000289551913738) A[2]:(0.72899222374) A[3]:(0.590080022812)\n",
      " state (9)  A[0]:(0.655400514603) A[1]:(0.810026526451) A[2]:(0.809995234013) A[3]:(-0.000474020809634)\n",
      " state (10)  A[0]:(0.72848379612) A[1]:(0.899995684624) A[2]:(-0.000194549560547) A[3]:(0.728691875935)\n",
      " state (11)  A[0]:(0.519297897816) A[1]:(0.876891493797) A[2]:(-0.603001117706) A[3]:(0.843310713768)\n",
      " state (12)  A[0]:(0.0753345713019) A[1]:(0.8245921731) A[2]:(-0.571491241455) A[3]:(0.792763292789)\n",
      " state (13)  A[0]:(-0.0011164243333) A[1]:(0.809416294098) A[2]:(0.900082051754) A[3]:(0.728724658489)\n",
      " state (14)  A[0]:(0.809730648994) A[1]:(0.900085687637) A[2]:(0.999999940395) A[3]:(0.809903085232)\n",
      " state (15)  A[0]:(0.985632121563) A[1]:(0.958331644535) A[2]:(1.0) A[3]:(0.883495569229)\n",
      "Episode 567000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6043. Times reached goal: 993.               Steps done: 4563832. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00937984992407.\n",
      " state (0)  A[0]:(0.531442523003) A[1]:(0.590508341789) A[2]:(0.590526223183) A[3]:(0.531167387962)\n",
      " state (1)  A[0]:(0.531406283379) A[1]:(-8.8058412075e-05) A[2]:(0.656089186668) A[3]:(0.590187311172)\n",
      " state (2)  A[0]:(0.590417265892) A[1]:(0.728991150856) A[2]:(0.590370595455) A[3]:(0.655913829803)\n",
      " state (3)  A[0]:(0.656053423882) A[1]:(-0.220105528831) A[2]:(0.536608278751) A[3]:(0.51981985569)\n",
      " state (4)  A[0]:(0.590248882771) A[1]:(0.655973792076) A[2]:(0.000177383422852) A[3]:(0.531355679035)\n",
      " state (5)  A[0]:(0.161015108228) A[1]:(0.928876042366) A[2]:(-0.18664598465) A[3]:(0.51908415556)\n",
      " state (6)  A[0]:(-0.000437438458903) A[1]:(0.809969186783) A[2]:(2.32458114624e-05) A[3]:(0.656049966812)\n",
      " state (7)  A[0]:(0.632605433464) A[1]:(-0.251817286015) A[2]:(0.278630614281) A[3]:(0.889579236507)\n",
      " state (8)  A[0]:(0.655891895294) A[1]:(0.000143088400364) A[2]:(0.728976368904) A[3]:(0.590461373329)\n",
      " state (9)  A[0]:(0.655875325203) A[1]:(0.809979856014) A[2]:(0.809998750687) A[3]:(9.38177108765e-05)\n",
      " state (10)  A[0]:(0.728852748871) A[1]:(0.900000572205) A[2]:(-0.000139236450195) A[3]:(0.728958725929)\n",
      " state (11)  A[0]:(0.519930720329) A[1]:(0.876915931702) A[2]:(-0.603044986725) A[3]:(0.843480229378)\n",
      " state (12)  A[0]:(0.0762447193265) A[1]:(0.824626803398) A[2]:(-0.571672439575) A[3]:(0.792954564095)\n",
      " state (13)  A[0]:(-0.000264465808868) A[1]:(0.809424161911) A[2]:(0.900003671646) A[3]:(0.728907465935)\n",
      " state (14)  A[0]:(0.809968173504) A[1]:(0.900055110455) A[2]:(0.999999940395) A[3]:(0.809989690781)\n",
      " state (15)  A[0]:(0.985644519329) A[1]:(0.958298921585) A[2]:(1.0) A[3]:(0.88351893425)\n",
      "Episode 568000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6039. Times reached goal: 992.               Steps done: 4569871. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00932337570591.\n",
      " state (0)  A[0]:(0.531277775764) A[1]:(0.590418875217) A[2]:(0.59050142765) A[3]:(0.531603634357)\n",
      " state (1)  A[0]:(0.531207680702) A[1]:(-0.000122807919979) A[2]:(0.656102001667) A[3]:(0.590647339821)\n",
      " state (2)  A[0]:(0.590239644051) A[1]:(0.729011654854) A[2]:(0.590541541576) A[3]:(0.656272828579)\n",
      " state (3)  A[0]:(0.655784964561) A[1]:(-0.221200153232) A[2]:(0.536824584007) A[3]:(0.519990563393)\n",
      " state (4)  A[0]:(0.589975476265) A[1]:(0.656089782715) A[2]:(0.000174164772034) A[3]:(0.53154695034)\n",
      " state (5)  A[0]:(0.160664826632) A[1]:(0.928900241852) A[2]:(-0.186611130834) A[3]:(0.519288480282)\n",
      " state (6)  A[0]:(-0.000842809502501) A[1]:(0.809998095036) A[2]:(0.000141620635986) A[3]:(0.656205654144)\n",
      " state (7)  A[0]:(0.632305264473) A[1]:(-0.25187420845) A[2]:(0.278913497925) A[3]:(0.88962739706)\n",
      " state (8)  A[0]:(0.655624568462) A[1]:(0.000170059502125) A[2]:(0.729130744934) A[3]:(0.590650200844)\n",
      " state (9)  A[0]:(0.655624747276) A[1]:(0.810034871101) A[2]:(0.810084044933) A[3]:(0.000617116631474)\n",
      " state (10)  A[0]:(0.728634357452) A[1]:(0.900021791458) A[2]:(5.66244125366e-05) A[3]:(0.729329228401)\n",
      " state (11)  A[0]:(0.519590139389) A[1]:(0.876930713654) A[2]:(-0.603004813194) A[3]:(0.84374332428)\n",
      " state (12)  A[0]:(0.075755879283) A[1]:(0.824642062187) A[2]:(-0.57175552845) A[3]:(0.793303370476)\n",
      " state (13)  A[0]:(-0.000849336152896) A[1]:(0.809446871281) A[2]:(0.900014817715) A[3]:(0.729319274426)\n",
      " state (14)  A[0]:(0.809716999531) A[1]:(0.900082588196) A[2]:(0.999999940395) A[3]:(0.810234963894)\n",
      " state (15)  A[0]:(0.985608279705) A[1]:(0.958310544491) A[2]:(1.0) A[3]:(0.883603870869)\n",
      "Episode 569000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6010. Times reached goal: 984.               Steps done: 4575881. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00926751026174.\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.5904,  0.5905,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5318, -0.0000,  0.6562,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5910,  0.7289,  0.5905,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0005,  0.8099, -0.0000,  0.6563]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0003,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9002,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53179359436) A[1]:(0.590354561806) A[2]:(0.590486884117) A[3]:(0.531383097172)\n",
      " state (1)  A[0]:(0.531717002392) A[1]:(-4.53144311905e-05) A[2]:(0.656233429909) A[3]:(0.590574085712)\n",
      " state (2)  A[0]:(0.59084546566) A[1]:(0.728893697262) A[2]:(0.590564966202) A[3]:(0.656323671341)\n",
      " state (3)  A[0]:(0.656500518322) A[1]:(-0.22121411562) A[2]:(0.536738872528) A[3]:(0.520356476307)\n",
      " state (4)  A[0]:(0.591025590897) A[1]:(0.655710339546) A[2]:(0.000171422958374) A[3]:(0.532017827034)\n",
      " state (5)  A[0]:(0.16248704493) A[1]:(0.928822338581) A[2]:(-0.186615496874) A[3]:(0.519833743572)\n",
      " state (6)  A[0]:(0.000647902372293) A[1]:(0.80992102623) A[2]:(9.32216644287e-05) A[3]:(0.656441509724)\n",
      " state (7)  A[0]:(0.632917165756) A[1]:(-0.252293050289) A[2]:(0.278809189796) A[3]:(0.88964253664)\n",
      " state (8)  A[0]:(0.656284809113) A[1]:(-0.000819817010779) A[2]:(0.728733658791) A[3]:(0.59125906229)\n",
      " state (9)  A[0]:(0.656046569347) A[1]:(0.809824705124) A[2]:(0.809952318668) A[3]:(0.000620886625256)\n",
      " state (10)  A[0]:(0.729025363922) A[1]:(0.899976968765) A[2]:(-0.000225186347961) A[3]:(0.729210615158)\n",
      " state (11)  A[0]:(0.520371317863) A[1]:(0.87692630291) A[2]:(-0.60337293148) A[3]:(0.843740582466)\n",
      " state (12)  A[0]:(0.0769426524639) A[1]:(0.824704110622) A[2]:(-0.572288691998) A[3]:(0.793339192867)\n",
      " state (13)  A[0]:(0.000404268474085) A[1]:(0.809610307217) A[2]:(0.899930775166) A[3]:(0.72935885191)\n",
      " state (14)  A[0]:(0.810170412064) A[1]:(0.900249540806) A[2]:(0.999999940395) A[3]:(0.810232996941)\n",
      " state (15)  A[0]:(0.985632896423) A[1]:(0.958407223225) A[2]:(1.0) A[3]:(0.883538126945)\n",
      "Episode 570000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6038. Times reached goal: 991.               Steps done: 4581919. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00921172163015.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531255424023) A[1]:(0.590533256531) A[2]:(0.590369224548) A[3]:(0.531316041946)\n",
      " state (1)  A[0]:(0.531344294548) A[1]:(0.000228241086006) A[2]:(0.656185984612) A[3]:(0.590353310108)\n",
      " state (2)  A[0]:(0.590547204018) A[1]:(0.728965818882) A[2]:(0.59031867981) A[3]:(0.656113505363)\n",
      " state (3)  A[0]:(0.656389176846) A[1]:(-0.219202309847) A[2]:(0.536459922791) A[3]:(0.52013862133)\n",
      " state (4)  A[0]:(0.591012954712) A[1]:(0.655890583992) A[2]:(6.16312026978e-05) A[3]:(0.531633257866)\n",
      " state (5)  A[0]:(0.162703931332) A[1]:(0.928890287876) A[2]:(-0.186986297369) A[3]:(0.519556522369)\n",
      " state (6)  A[0]:(0.00119328440633) A[1]:(0.810131430626) A[2]:(-0.000305533409119) A[3]:(0.656378269196)\n",
      " state (7)  A[0]:(0.633331775665) A[1]:(-0.251358479261) A[2]:(0.278817087412) A[3]:(0.889621734619)\n",
      " state (8)  A[0]:(0.65661251545) A[1]:(0.000480309099657) A[2]:(0.729160428047) A[3]:(0.590615272522)\n",
      " state (9)  A[0]:(0.656546115875) A[1]:(0.810093224049) A[2]:(0.810219228268) A[3]:(0.000224411487579)\n",
      " state (10)  A[0]:(0.729408860207) A[1]:(0.900000989437) A[2]:(0.000488758028951) A[3]:(0.729233562946)\n",
      " state (11)  A[0]:(0.520857751369) A[1]:(0.876834392548) A[2]:(-0.602909743786) A[3]:(0.843794107437)\n",
      " state (12)  A[0]:(0.0774934068322) A[1]:(0.824405372143) A[2]:(-0.571960091591) A[3]:(0.793485999107)\n",
      " state (13)  A[0]:(0.000820219342131) A[1]:(0.809068083763) A[2]:(0.899935245514) A[3]:(0.729651987553)\n",
      " state (14)  A[0]:(0.810316979885) A[1]:(0.899807810783) A[2]:(0.999999940395) A[3]:(0.810538053513)\n",
      " state (15)  A[0]:(0.985649168491) A[1]:(0.958154559135) A[2]:(1.0) A[3]:(0.883797943592)\n",
      "Episode 571000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6021. Times reached goal: 991.               Steps done: 4587940. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0091564244933.\n",
      " state (0)  A[0]:(0.531256079674) A[1]:(0.590455710888) A[2]:(0.590528786182) A[3]:(0.531347453594)\n",
      " state (1)  A[0]:(0.53144967556) A[1]:(9.19476151466e-05) A[2]:(0.65616440773) A[3]:(0.590328335762)\n",
      " state (2)  A[0]:(0.590592384338) A[1]:(0.729073822498) A[2]:(0.590566635132) A[3]:(0.655970215797)\n",
      " state (3)  A[0]:(0.656273961067) A[1]:(-0.220589160919) A[2]:(0.536727547646) A[3]:(0.519506931305)\n",
      " state (4)  A[0]:(0.590722203255) A[1]:(0.656150817871) A[2]:(-8.70227813721e-06) A[3]:(0.531061768532)\n",
      " state (5)  A[0]:(0.162071689963) A[1]:(0.928932845592) A[2]:(-0.186952471733) A[3]:(0.518915176392)\n",
      " state (6)  A[0]:(0.000762402836699) A[1]:(0.810053944588) A[2]:(-6.18696212769e-05) A[3]:(0.655757904053)\n",
      " state (7)  A[0]:(0.633022785187) A[1]:(-0.251728862524) A[2]:(0.279102146626) A[3]:(0.889268577099)\n",
      " state (8)  A[0]:(0.656279325485) A[1]:(0.000250317156315) A[2]:(0.728957951069) A[3]:(0.58979010582)\n",
      " state (9)  A[0]:(0.656239569187) A[1]:(0.810049831867) A[2]:(0.809994220734) A[3]:(-0.000967874831986)\n",
      " state (10)  A[0]:(0.729161918163) A[1]:(0.900034368038) A[2]:(-0.000187039375305) A[3]:(0.72849804163)\n",
      " state (11)  A[0]:(0.520487010479) A[1]:(0.876944303513) A[2]:(-0.603437900543) A[3]:(0.843251466751)\n",
      " state (12)  A[0]:(0.0769665911794) A[1]:(0.824657917023) A[2]:(-0.572498559952) A[3]:(0.792671263218)\n",
      " state (13)  A[0]:(0.000272512435913) A[1]:(0.80947047472) A[2]:(0.900011122227) A[3]:(0.728516101837)\n",
      " state (14)  A[0]:(0.810049712658) A[1]:(0.900109171867) A[2]:(0.999999940395) A[3]:(0.809676527977)\n",
      " state (15)  A[0]:(0.985590696335) A[1]:(0.958296060562) A[2]:(1.0) A[3]:(0.883167147636)\n",
      "Episode 572000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6008. Times reached goal: 990.               Steps done: 4593948. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00910157761993.\n",
      " state (0)  A[0]:(0.531786441803) A[1]:(0.590582370758) A[2]:(0.59062987566) A[3]:(0.531317591667)\n",
      " state (1)  A[0]:(0.531562566757) A[1]:(0.000305652618408) A[2]:(0.656284213066) A[3]:(0.590569496155)\n",
      " state (2)  A[0]:(0.590505123138) A[1]:(0.729206502438) A[2]:(0.590679526329) A[3]:(0.656279146671)\n",
      " state (3)  A[0]:(0.656033039093) A[1]:(-0.220416381955) A[2]:(0.536921679974) A[3]:(0.519913911819)\n",
      " state (4)  A[0]:(0.590183496475) A[1]:(0.656460165977) A[2]:(0.000301241874695) A[3]:(0.531485319138)\n",
      " state (5)  A[0]:(0.160828128457) A[1]:(0.928969502449) A[2]:(-0.186527013779) A[3]:(0.51933836937)\n",
      " state (6)  A[0]:(-0.000883817439899) A[1]:(0.810194015503) A[2]:(0.000459074944956) A[3]:(0.656145393848)\n",
      " state (7)  A[0]:(0.632115125656) A[1]:(-0.251242935658) A[2]:(0.279875069857) A[3]:(0.889461219311)\n",
      " state (8)  A[0]:(0.655480265617) A[1]:(0.000649660709314) A[2]:(0.729491651058) A[3]:(0.589908957481)\n",
      " state (9)  A[0]:(0.655453205109) A[1]:(0.810174047947) A[2]:(0.810272574425) A[3]:(-0.00117886008229)\n",
      " state (10)  A[0]:(0.72854077816) A[1]:(0.900110304356) A[2]:(0.000158548355103) A[3]:(0.72863638401)\n",
      " state (11)  A[0]:(0.519485533237) A[1]:(0.877050697803) A[2]:(-0.603431165218) A[3]:(0.843431651592)\n",
      " state (12)  A[0]:(0.0754886865616) A[1]:(0.824823498726) A[2]:(-0.572626292706) A[3]:(0.792954087257)\n",
      " state (13)  A[0]:(-0.00126177002676) A[1]:(0.809676587582) A[2]:(0.90007352829) A[3]:(0.728891670704)\n",
      " state (14)  A[0]:(0.809571266174) A[1]:(0.900246739388) A[2]:(0.999999940395) A[3]:(0.809930324554)\n",
      " state (15)  A[0]:(0.985538244247) A[1]:(0.958352267742) A[2]:(1.0) A[3]:(0.883266687393)\n",
      "Episode 573000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6028. Times reached goal: 992.               Steps done: 4599976. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0090468783393.\n",
      " state (0)  A[0]:(0.531988024712) A[1]:(0.590304136276) A[2]:(0.590344667435) A[3]:(0.529225349426)\n",
      " state (1)  A[0]:(0.531798183918) A[1]:(-0.000456914276583) A[2]:(0.655832946301) A[3]:(0.588796138763)\n",
      " state (2)  A[0]:(0.590760231018) A[1]:(0.728690743446) A[2]:(0.590031981468) A[3]:(0.654858350754)\n",
      " state (3)  A[0]:(0.656081259251) A[1]:(-0.222130075097) A[2]:(0.536334276199) A[3]:(0.518970012665)\n",
      " state (4)  A[0]:(0.590184271336) A[1]:(0.655776262283) A[2]:(-0.000578045786824) A[3]:(0.53102517128)\n",
      " state (5)  A[0]:(0.16083727777) A[1]:(0.928781092167) A[2]:(-0.187308624387) A[3]:(0.518891692162)\n",
      " state (6)  A[0]:(-0.000965028710198) A[1]:(0.80969786644) A[2]:(-0.000330328941345) A[3]:(0.655656576157)\n",
      " state (7)  A[0]:(0.632123351097) A[1]:(-0.252739369869) A[2]:(0.278976291418) A[3]:(0.889267981052)\n",
      " state (8)  A[0]:(0.655900359154) A[1]:(-0.000957593030762) A[2]:(0.728540182114) A[3]:(0.590209722519)\n",
      " state (9)  A[0]:(0.655933141708) A[1]:(0.809725284576) A[2]:(0.809621214867) A[3]:(-0.000923350162338)\n",
      " state (10)  A[0]:(0.72881257534) A[1]:(0.899869918823) A[2]:(-0.00106704200152) A[3]:(0.728261590004)\n",
      " state (11)  A[0]:(0.519861400127) A[1]:(0.876758873463) A[2]:(-0.604108214378) A[3]:(0.84306961298)\n",
      " state (12)  A[0]:(0.0760567486286) A[1]:(0.824443697929) A[2]:(-0.573391914368) A[3]:(0.792442679405)\n",
      " state (13)  A[0]:(-0.000579059065785) A[1]:(0.809321880341) A[2]:(0.89991658926) A[3]:(0.728257358074)\n",
      " state (14)  A[0]:(0.809878468513) A[1]:(0.900102734566) A[2]:(0.999999940395) A[3]:(0.8095266819)\n",
      " state (15)  A[0]:(0.985558211803) A[1]:(0.958306014538) A[2]:(1.0) A[3]:(0.883024215698)\n",
      "Episode 574000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6016. Times reached goal: 993.               Steps done: 4605992. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00899261570487.\n",
      "q_values \n",
      "tensor([[ 0.5325,  0.5907,  0.5907,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5326,  0.0000,  0.6561,  0.5907]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5915,  0.7288,  0.5906,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0000,  0.8101, -0.0000,  0.6557]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7286,  0.9000,  0.0001,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8094,  0.8999,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532526314259) A[1]:(0.59051668644) A[2]:(0.590628862381) A[3]:(0.530921816826)\n",
      " state (1)  A[0]:(0.532547950745) A[1]:(-8.81478190422e-05) A[2]:(0.656082630157) A[3]:(0.589782595634)\n",
      " state (2)  A[0]:(0.591418623924) A[1]:(0.728978395462) A[2]:(0.590542078018) A[3]:(0.655312478542)\n",
      " state (3)  A[0]:(0.656577169895) A[1]:(-0.222184374928) A[2]:(0.536780536175) A[3]:(0.518378615379)\n",
      " state (4)  A[0]:(0.59077835083) A[1]:(0.655830621719) A[2]:(-0.00025749206543) A[3]:(0.52986395359)\n",
      " state (5)  A[0]:(0.161746487021) A[1]:(0.928842067719) A[2]:(-0.187251806259) A[3]:(0.517544746399)\n",
      " state (6)  A[0]:(-0.000437349051936) A[1]:(0.80995619297) A[2]:(-0.000389695138438) A[3]:(0.654544711113)\n",
      " state (7)  A[0]:(0.631974577904) A[1]:(-0.25187420845) A[2]:(0.279026627541) A[3]:(0.888883113861)\n",
      " state (8)  A[0]:(0.655466914177) A[1]:(-6.23986124992e-05) A[2]:(0.72872364521) A[3]:(0.58934879303)\n",
      " state (9)  A[0]:(0.655305683613) A[1]:(0.809993624687) A[2]:(0.809817254543) A[3]:(-0.00117506028619)\n",
      " state (10)  A[0]:(0.728258013725) A[1]:(0.899977684021) A[2]:(-0.000516891421285) A[3]:(0.728473126888)\n",
      " state (11)  A[0]:(0.518990814686) A[1]:(0.876842200756) A[2]:(-0.603832840919) A[3]:(0.843260228634)\n",
      " state (12)  A[0]:(0.0747964382172) A[1]:(0.824486255646) A[2]:(-0.573254406452) A[3]:(0.79267680645)\n",
      " state (13)  A[0]:(-0.0020408006385) A[1]:(0.809272766113) A[2]:(0.899929881096) A[3]:(0.728445529938)\n",
      " state (14)  A[0]:(0.809305906296) A[1]:(0.900018632412) A[2]:(0.999999940395) A[3]:(0.809501767159)\n",
      " state (15)  A[0]:(0.985498666763) A[1]:(0.958239674568) A[2]:(1.0) A[3]:(0.882877528667)\n",
      "Episode 575000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6036. Times reached goal: 994.               Steps done: 4612028. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00893849976269.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531259357929) A[1]:(0.59014737606) A[2]:(0.590295910835) A[3]:(0.531412601471)\n",
      " state (1)  A[0]:(0.531156897545) A[1]:(-0.000170305371284) A[2]:(0.655865550041) A[3]:(0.59029084444)\n",
      " state (2)  A[0]:(0.590166032314) A[1]:(0.728844046593) A[2]:(0.59059637785) A[3]:(0.655839562416)\n",
      " state (3)  A[0]:(0.655651092529) A[1]:(-0.223562389612) A[2]:(0.537335991859) A[3]:(0.5194221735)\n",
      " state (4)  A[0]:(0.589986145496) A[1]:(0.655953884125) A[2]:(0.000283002853394) A[3]:(0.531360149384)\n",
      " state (5)  A[0]:(0.160998418927) A[1]:(0.928898692131) A[2]:(-0.18688288331) A[3]:(0.519288599491)\n",
      " state (6)  A[0]:(-0.000180214643478) A[1]:(0.809899449348) A[2]:(1.41859054565e-05) A[3]:(0.655954957008)\n",
      " state (7)  A[0]:(0.6324390769) A[1]:(-0.252044528723) A[2]:(0.27932715416) A[3]:(0.889244318008)\n",
      " state (8)  A[0]:(0.655906438828) A[1]:(0.000255301594734) A[2]:(0.728752851486) A[3]:(0.590268373489)\n",
      " state (9)  A[0]:(0.655907690525) A[1]:(0.810080647469) A[2]:(0.809912443161) A[3]:(5.52088022232e-05)\n",
      " state (10)  A[0]:(0.728848457336) A[1]:(0.899995565414) A[2]:(-4.9352645874e-05) A[3]:(0.728943586349)\n",
      " state (11)  A[0]:(0.520073652267) A[1]:(0.876832008362) A[2]:(-0.603582561016) A[3]:(0.843529164791)\n",
      " state (12)  A[0]:(0.0764633491635) A[1]:(0.824429929256) A[2]:(-0.573125243187) A[3]:(0.79301661253)\n",
      " state (13)  A[0]:(-0.000228613615036) A[1]:(0.809163987637) A[2]:(0.899982213974) A[3]:(0.728881120682)\n",
      " state (14)  A[0]:(0.809988498688) A[1]:(0.899933993816) A[2]:(0.999999940395) A[3]:(0.809868752956)\n",
      " state (15)  A[0]:(0.985550522804) A[1]:(0.958185076714) A[2]:(1.0) A[3]:(0.88311958313)\n",
      "Episode 576000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 988.               Steps done: 4618033. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00888498491077.\n",
      " state (0)  A[0]:(0.531580626965) A[1]:(0.590384781361) A[2]:(0.590391993523) A[3]:(0.531404018402)\n",
      " state (1)  A[0]:(0.531539082527) A[1]:(-8.09282064438e-05) A[2]:(0.656028151512) A[3]:(0.590439021587)\n",
      " state (2)  A[0]:(0.590599060059) A[1]:(0.728932499886) A[2]:(0.590456962585) A[3]:(0.65611743927)\n",
      " state (3)  A[0]:(0.656190872192) A[1]:(-0.22159935534) A[2]:(0.536847472191) A[3]:(0.519713640213)\n",
      " state (4)  A[0]:(0.59059035778) A[1]:(0.656002283096) A[2]:(-2.02655792236e-06) A[3]:(0.531495451927)\n",
      " state (5)  A[0]:(0.161801740527) A[1]:(0.92887711525) A[2]:(-0.187052682042) A[3]:(0.519555807114)\n",
      " state (6)  A[0]:(0.000255614519119) A[1]:(0.809956014156) A[2]:(-6.79492950439e-05) A[3]:(0.656203746796)\n",
      " state (7)  A[0]:(0.632721781731) A[1]:(-0.25194427371) A[2]:(0.279566735029) A[3]:(0.889381527901)\n",
      " state (8)  A[0]:(0.656333863735) A[1]:(-2.42441892624e-05) A[2]:(0.728885173798) A[3]:(0.590820372105)\n",
      " state (9)  A[0]:(0.656289696693) A[1]:(0.809999704361) A[2]:(0.809933304787) A[3]:(0.000734314206056)\n",
      " state (10)  A[0]:(0.729150950909) A[1]:(0.899980306625) A[2]:(-0.000192284584045) A[3]:(0.729241013527)\n",
      " state (11)  A[0]:(0.520523309708) A[1]:(0.876846551895) A[2]:(-0.603795170784) A[3]:(0.843728363514)\n",
      " state (12)  A[0]:(0.0770100802183) A[1]:(0.82449555397) A[2]:(-0.573424100876) A[3]:(0.793290019035)\n",
      " state (13)  A[0]:(0.000260949134827) A[1]:(0.809290289879) A[2]:(0.899992823601) A[3]:(0.729254364967)\n",
      " state (14)  A[0]:(0.810140550137) A[1]:(0.900037407875) A[2]:(0.999999940395) A[3]:(0.810183882713)\n",
      " state (15)  A[0]:(0.985546350479) A[1]:(0.958232104778) A[2]:(1.0) A[3]:(0.883308291435)\n",
      "Episode 577000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6038. Times reached goal: 996.               Steps done: 4624071. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00883149900832.\n",
      " state (0)  A[0]:(0.531452775002) A[1]:(0.590430796146) A[2]:(0.590438127518) A[3]:(0.531431078911)\n",
      " state (1)  A[0]:(0.531337857246) A[1]:(1.48862600327e-05) A[2]:(0.656076192856) A[3]:(0.590433835983)\n",
      " state (2)  A[0]:(0.590409755707) A[1]:(0.728961884975) A[2]:(0.590503096581) A[3]:(0.65605700016)\n",
      " state (3)  A[0]:(0.656052649021) A[1]:(-0.221644610167) A[2]:(0.536902964115) A[3]:(0.519629478455)\n",
      " state (4)  A[0]:(0.590431332588) A[1]:(0.656000494957) A[2]:(2.121925354e-05) A[3]:(0.531320929527)\n",
      " state (5)  A[0]:(0.161481499672) A[1]:(0.928875267506) A[2]:(-0.18707408011) A[3]:(0.519247710705)\n",
      " state (6)  A[0]:(-0.000186622142792) A[1]:(0.809953153133) A[2]:(-1.4066696167e-05) A[3]:(0.655846297741)\n",
      " state (7)  A[0]:(0.632369041443) A[1]:(-0.251968890429) A[2]:(0.279832988977) A[3]:(0.889201104641)\n",
      " state (8)  A[0]:(0.655947804451) A[1]:(-0.000253453850746) A[2]:(0.728925228119) A[3]:(0.590267002583)\n",
      " state (9)  A[0]:(0.655858874321) A[1]:(0.809936225414) A[2]:(0.809970617294) A[3]:(-0.000474497646792)\n",
      " state (10)  A[0]:(0.728853046894) A[1]:(0.89999204874) A[2]:(-0.000330924987793) A[3]:(0.728743076324)\n",
      " state (11)  A[0]:(0.520100712776) A[1]:(0.876904129982) A[2]:(-0.604096412659) A[3]:(0.843470394611)\n",
      " state (12)  A[0]:(0.0763912349939) A[1]:(0.824627041817) A[2]:(-0.573837637901) A[3]:(0.792952120304)\n",
      " state (13)  A[0]:(-0.000374078721507) A[1]:(0.809494376183) A[2]:(0.899998664856) A[3]:(0.728803515434)\n",
      " state (14)  A[0]:(0.809933960438) A[1]:(0.900186896324) A[2]:(0.999999940395) A[3]:(0.80984890461)\n",
      " state (15)  A[0]:(0.985512316227) A[1]:(0.958298504353) A[2]:(1.0) A[3]:(0.883049547672)\n",
      "Episode 578000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6017. Times reached goal: 989.               Steps done: 4630088. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0087785194277.\n",
      " state (0)  A[0]:(0.531530022621) A[1]:(0.590452313423) A[2]:(0.590318381786) A[3]:(0.533745288849)\n",
      " state (1)  A[0]:(0.532286822796) A[1]:(-9.14186239243e-05) A[2]:(0.655991196632) A[3]:(0.592615783215)\n",
      " state (2)  A[0]:(0.591631531715) A[1]:(0.728802204132) A[2]:(0.590323984623) A[3]:(0.657952427864)\n",
      " state (3)  A[0]:(0.657248497009) A[1]:(-0.222524672747) A[2]:(0.536581873894) A[3]:(0.521907031536)\n",
      " state (4)  A[0]:(0.591694712639) A[1]:(0.655568003654) A[2]:(-0.000288844108582) A[3]:(0.533474683762)\n",
      " state (5)  A[0]:(0.163302704692) A[1]:(0.928696870804) A[2]:(-0.186935335398) A[3]:(0.521325349808)\n",
      " state (6)  A[0]:(0.00205218512565) A[1]:(0.809575676918) A[2]:(0.000253558158875) A[3]:(0.657281637192)\n",
      " state (7)  A[0]:(0.634882867336) A[1]:(-0.252923101187) A[2]:(0.279808372259) A[3]:(0.889765024185)\n",
      " state (8)  A[0]:(0.660452008247) A[1]:(-0.00148881867062) A[2]:(0.728370904922) A[3]:(0.593108534813)\n",
      " state (9)  A[0]:(0.66188877821) A[1]:(0.809577941895) A[2]:(0.809780955315) A[3]:(0.00287047727033)\n",
      " state (10)  A[0]:(0.734322071075) A[1]:(0.899759471416) A[2]:(0.000442385644419) A[3]:(0.72955083847)\n",
      " state (11)  A[0]:(0.529230833054) A[1]:(0.876537382603) A[2]:(-0.603323817253) A[3]:(0.843835234642)\n",
      " state (12)  A[0]:(0.0894816741347) A[1]:(0.823964178562) A[2]:(-0.573275566101) A[3]:(0.793424844742)\n",
      " state (13)  A[0]:(0.0127025861293) A[1]:(0.808534741402) A[2]:(0.899839520454) A[3]:(0.729383707047)\n",
      " state (14)  A[0]:(0.814113795757) A[1]:(0.899490654469) A[2]:(0.999999940395) A[3]:(0.810225903988)\n",
      " state (15)  A[0]:(0.985856533051) A[1]:(0.957939863205) A[2]:(1.0) A[3]:(0.883304476738)\n",
      "Episode 579000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5995. Times reached goal: 982.               Steps done: 4636083. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00872604963906.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5904,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315, -0.0000,  0.6561,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7290,  0.5905,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  0.8100, -0.0001,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000, -0.0002,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9001,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531362295151) A[1]:(0.590399563313) A[2]:(0.590449035168) A[3]:(0.531396627426)\n",
      " state (1)  A[0]:(0.531380534172) A[1]:(-7.67409801483e-06) A[2]:(0.656059920788) A[3]:(0.590483307838)\n",
      " state (2)  A[0]:(0.590498447418) A[1]:(0.72895181179) A[2]:(0.590507507324) A[3]:(0.65611577034)\n",
      " state (3)  A[0]:(0.656187653542) A[1]:(-0.222102671862) A[2]:(0.53696334362) A[3]:(0.519647777081)\n",
      " state (4)  A[0]:(0.590604782104) A[1]:(0.656183063984) A[2]:(-7.56978988647e-05) A[3]:(0.53139936924)\n",
      " state (5)  A[0]:(0.161724224687) A[1]:(0.92891061306) A[2]:(-0.187191411853) A[3]:(0.519367575645)\n",
      " state (6)  A[0]:(9.61720943451e-05) A[1]:(0.809974074364) A[2]:(-2.18152999878e-05) A[3]:(0.655986785889)\n",
      " state (7)  A[0]:(0.632531940937) A[1]:(-0.251996338367) A[2]:(0.280100286007) A[3]:(0.889256060123)\n",
      " state (8)  A[0]:(0.656251192093) A[1]:(-5.37931919098e-06) A[2]:(0.729040145874) A[3]:(0.590393364429)\n",
      " state (9)  A[0]:(0.656359434128) A[1]:(0.810030937195) A[2]:(0.809992313385) A[3]:(-0.000154748558998)\n",
      " state (10)  A[0]:(0.729191422462) A[1]:(0.899993777275) A[2]:(-0.00019645690918) A[3]:(0.728903591633)\n",
      " state (11)  A[0]:(0.520513296127) A[1]:(0.876861453056) A[2]:(-0.604104936123) A[3]:(0.843564510345)\n",
      " state (12)  A[0]:(0.0768497735262) A[1]:(0.824527740479) A[2]:(-0.574082493782) A[3]:(0.793086230755)\n",
      " state (13)  A[0]:(9.38773155212e-06) A[1]:(0.809359490871) A[2]:(0.89998281002) A[3]:(0.728984177113)\n",
      " state (14)  A[0]:(0.81010723114) A[1]:(0.900114119053) A[2]:(0.999999940395) A[3]:(0.8099848032)\n",
      " state (15)  A[0]:(0.985512018204) A[1]:(0.958254396915) A[2]:(1.0) A[3]:(0.883088767529)\n",
      "Episode 580000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 983.               Steps done: 4642090. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00867378937972.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531646609306) A[1]:(0.590424418449) A[2]:(0.59049808979) A[3]:(0.531441450119)\n",
      " state (1)  A[0]:(0.531691491604) A[1]:(-2.34767794609e-05) A[2]:(0.656158387661) A[3]:(0.590550899506)\n",
      " state (2)  A[0]:(0.590733885765) A[1]:(0.729079723358) A[2]:(0.59057033062) A[3]:(0.656221926212)\n",
      " state (3)  A[0]:(0.656192719936) A[1]:(-0.220258980989) A[2]:(0.536903381348) A[3]:(0.519938588142)\n",
      " state (4)  A[0]:(0.590486645699) A[1]:(0.656230270863) A[2]:(0.000154137611389) A[3]:(0.5315797925)\n",
      " state (5)  A[0]:(0.16162340343) A[1]:(0.928933918476) A[2]:(-0.187116980553) A[3]:(0.519632935524)\n",
      " state (6)  A[0]:(6.43134117126e-05) A[1]:(0.810068547726) A[2]:(-4.41074371338e-06) A[3]:(0.656203746796)\n",
      " state (7)  A[0]:(0.632464408875) A[1]:(-0.251620531082) A[2]:(0.280151367188) A[3]:(0.88931208849)\n",
      " state (8)  A[0]:(0.656212329865) A[1]:(0.000330872833729) A[2]:(0.729029893875) A[3]:(0.590740501881)\n",
      " state (9)  A[0]:(0.656351685524) A[1]:(0.810097813606) A[2]:(0.810059309006) A[3]:(0.000455111236079)\n",
      " state (10)  A[0]:(0.729264497757) A[1]:(0.90001231432) A[2]:(0.000122308731079) A[3]:(0.729209661484)\n",
      " state (11)  A[0]:(0.520782232285) A[1]:(0.876860678196) A[2]:(-0.60396528244) A[3]:(0.843765258789)\n",
      " state (12)  A[0]:(0.0773611068726) A[1]:(0.824486076832) A[2]:(-0.57407271862) A[3]:(0.7933396101)\n",
      " state (13)  A[0]:(0.000493019761052) A[1]:(0.80925577879) A[2]:(0.899980008602) A[3]:(0.72927904129)\n",
      " state (14)  A[0]:(0.81017768383) A[1]:(0.900016784668) A[2]:(0.999999940395) A[3]:(0.810182154179)\n",
      " state (15)  A[0]:(0.985502123833) A[1]:(0.958188951015) A[2]:(1.0) A[3]:(0.883187174797)\n",
      "Episode 581000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6040. Times reached goal: 994.               Steps done: 4648130. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00862155759066.\n",
      " state (0)  A[0]:(0.53097820282) A[1]:(0.590157985687) A[2]:(0.590107798576) A[3]:(0.531486034393)\n",
      " state (1)  A[0]:(0.531029164791) A[1]:(0.000228226184845) A[2]:(0.6559060812) A[3]:(0.590369999409)\n",
      " state (2)  A[0]:(0.590205729008) A[1]:(0.72901237011) A[2]:(0.590158939362) A[3]:(0.655958533287)\n",
      " state (3)  A[0]:(0.655900299549) A[1]:(-0.220102667809) A[2]:(0.536566972733) A[3]:(0.519336521626)\n",
      " state (4)  A[0]:(0.590342283249) A[1]:(0.656051695347) A[2]:(-0.000124454498291) A[3]:(0.530791282654)\n",
      " state (5)  A[0]:(0.161661326885) A[1]:(0.92891985178) A[2]:(-0.187365561724) A[3]:(0.518730044365)\n",
      " state (6)  A[0]:(0.000226408243179) A[1]:(0.810090899467) A[2]:(-0.0001060962677) A[3]:(0.655213236809)\n",
      " state (7)  A[0]:(0.632401585579) A[1]:(-0.251415342093) A[2]:(0.280256390572) A[3]:(0.888746201992)\n",
      " state (8)  A[0]:(0.655965447426) A[1]:(0.000356852979166) A[2]:(0.728922247887) A[3]:(0.589195966721)\n",
      " state (9)  A[0]:(0.655872941017) A[1]:(0.810071885586) A[2]:(0.809962034225) A[3]:(-0.00164972094353)\n",
      " state (10)  A[0]:(0.728889584541) A[1]:(0.900057137012) A[2]:(-0.000141859054565) A[3]:(0.728218436241)\n",
      " state (11)  A[0]:(0.520319700241) A[1]:(0.876981735229) A[2]:(-0.604200720787) A[3]:(0.843147456646)\n",
      " state (12)  A[0]:(0.0768383070827) A[1]:(0.824725449085) A[2]:(-0.574362158775) A[3]:(0.792503476143)\n",
      " state (13)  A[0]:(2.84314155579e-05) A[1]:(0.809572994709) A[2]:(0.899996161461) A[3]:(0.728175401688)\n",
      " state (14)  A[0]:(0.810016334057) A[1]:(0.900212705135) A[2]:(0.999999940395) A[3]:(0.809369802475)\n",
      " state (15)  A[0]:(0.985472738743) A[1]:(0.958267748356) A[2]:(1.0) A[3]:(0.882634997368)\n",
      "Episode 582000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6034. Times reached goal: 988.               Steps done: 4654164. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00856969174877.\n",
      " state (0)  A[0]:(0.531385540962) A[1]:(0.590254187584) A[2]:(0.590652346611) A[3]:(0.531790614128)\n",
      " state (1)  A[0]:(0.531447649002) A[1]:(0.000266902148724) A[2]:(0.655989646912) A[3]:(0.590712726116)\n",
      " state (2)  A[0]:(0.590505540371) A[1]:(0.729001283646) A[2]:(0.590584874153) A[3]:(0.656248152256)\n",
      " state (3)  A[0]:(0.655966103077) A[1]:(-0.223151743412) A[2]:(0.537308454514) A[3]:(0.519640684128)\n",
      " state (4)  A[0]:(0.590272843838) A[1]:(0.656406164169) A[2]:(4.64916229248e-05) A[3]:(0.531576991081)\n",
      " state (5)  A[0]:(0.161292567849) A[1]:(0.929011404514) A[2]:(-0.187242597342) A[3]:(0.519671559334)\n",
      " state (6)  A[0]:(8.15689563751e-05) A[1]:(0.810198545456) A[2]:(0.000110507011414) A[3]:(0.656058430672)\n",
      " state (7)  A[0]:(0.632642745972) A[1]:(-0.251198977232) A[2]:(0.280608952045) A[3]:(0.889063417912)\n",
      " state (8)  A[0]:(0.656383752823) A[1]:(0.00108810467646) A[2]:(0.729112923145) A[3]:(0.590133726597)\n",
      " state (9)  A[0]:(0.656461775303) A[1]:(0.810333549976) A[2]:(0.810037612915) A[3]:(0.000124961137772)\n",
      " state (10)  A[0]:(0.729280352592) A[1]:(0.90013229847) A[2]:(0.000245094299316) A[3]:(0.728909790516)\n",
      " state (11)  A[0]:(0.520812153816) A[1]:(0.877001523972) A[2]:(-0.603953957558) A[3]:(0.843490302563)\n",
      " state (12)  A[0]:(0.0774668604136) A[1]:(0.824669420719) A[2]:(-0.574285745621) A[3]:(0.792928934097)\n",
      " state (13)  A[0]:(0.000607311667409) A[1]:(0.809425652027) A[2]:(0.899974226952) A[3]:(0.728743553162)\n",
      " state (14)  A[0]:(0.810196995735) A[1]:(0.900089085102) A[2]:(0.999999940395) A[3]:(0.809834957123)\n",
      " state (15)  A[0]:(0.985483586788) A[1]:(0.958194494247) A[2]:(1.0) A[3]:(0.882962763309)\n",
      "Episode 583000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               5994. Times reached goal: 987.               Steps done: 4660158. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0085184786554.\n",
      " state (0)  A[0]:(0.531785011292) A[1]:(0.590680956841) A[2]:(0.590541362762) A[3]:(0.531668782234)\n",
      " state (1)  A[0]:(0.531545162201) A[1]:(2.51904129982e-05) A[2]:(0.656107664108) A[3]:(0.590600132942)\n",
      " state (2)  A[0]:(0.590514540672) A[1]:(0.729034900665) A[2]:(0.590636968613) A[3]:(0.656238913536)\n",
      " state (3)  A[0]:(0.656320393085) A[1]:(-0.22165235877) A[2]:(0.537001729012) A[3]:(0.519979596138)\n",
      " state (4)  A[0]:(0.590608596802) A[1]:(0.656170666218) A[2]:(-2.25305557251e-05) A[3]:(0.531888067722)\n",
      " state (5)  A[0]:(0.161397993565) A[1]:(0.928902387619) A[2]:(-0.187198653817) A[3]:(0.520061850548)\n",
      " state (6)  A[0]:(-0.000390946835978) A[1]:(0.810043513775) A[2]:(0.00010621547699) A[3]:(0.656381309032)\n",
      " state (7)  A[0]:(0.632455468178) A[1]:(-0.251661717892) A[2]:(0.280615866184) A[3]:(0.889265835285)\n",
      " state (8)  A[0]:(0.656455039978) A[1]:(4.08962368965e-05) A[2]:(0.728954076767) A[3]:(0.591055035591)\n",
      " state (9)  A[0]:(0.656398653984) A[1]:(0.810040831566) A[2]:(0.809971809387) A[3]:(0.000687867286615)\n",
      " state (10)  A[0]:(0.729225039482) A[1]:(0.900030493736) A[2]:(-0.000127196311951) A[3]:(0.729168653488)\n",
      " state (11)  A[0]:(0.520723342896) A[1]:(0.876930952072) A[2]:(-0.604398131371) A[3]:(0.843733310699)\n",
      " state (12)  A[0]:(0.0772562325001) A[1]:(0.824646472931) A[2]:(-0.574787259102) A[3]:(0.793290197849)\n",
      " state (13)  A[0]:(0.000428318948252) A[1]:(0.809511244297) A[2]:(0.900058686733) A[3]:(0.729210972786)\n",
      " state (14)  A[0]:(0.810211241245) A[1]:(0.900215446949) A[2]:(0.999999940395) A[3]:(0.810135066509)\n",
      " state (15)  A[0]:(0.985461831093) A[1]:(0.958261549473) A[2]:(1.0) A[3]:(0.883043050766)\n",
      "Episode 584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 991.               Steps done: 4666172. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00846740226542.\n",
      "q_values \n",
      "tensor([[ 0.5302,  0.5902,  0.5904,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5302, -0.0001,  0.6561,  0.5908]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5898,  0.7292,  0.5905,  0.6567]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  0.8100,  0.0000,  0.6571]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.8999,  0.0001,  0.7301]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8097,  0.9001,  1.0000,  0.8107]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532082974911) A[1]:(0.590278029442) A[2]:(0.590460062027) A[3]:(0.531133890152)\n",
      " state (1)  A[0]:(0.531593501568) A[1]:(-7.73444771767e-05) A[2]:(0.656116724014) A[3]:(0.590380072594)\n",
      " state (2)  A[0]:(0.590517282486) A[1]:(0.729024410248) A[2]:(0.590442895889) A[3]:(0.656252682209)\n",
      " state (3)  A[0]:(0.656113505363) A[1]:(-0.220159441233) A[2]:(0.536677002907) A[3]:(0.519892692566)\n",
      " state (4)  A[0]:(0.590459823608) A[1]:(0.656218886375) A[2]:(-0.000182032585144) A[3]:(0.531779885292)\n",
      " state (5)  A[0]:(0.161557838321) A[1]:(0.928944945335) A[2]:(-0.187466666102) A[3]:(0.520218729973)\n",
      " state (6)  A[0]:(-0.000175982713699) A[1]:(0.810088813305) A[2]:(-0.000132083892822) A[3]:(0.656537175179)\n",
      " state (7)  A[0]:(0.632027983665) A[1]:(-0.251596182585) A[2]:(0.28047144413) A[3]:(0.889207303524)\n",
      " state (8)  A[0]:(0.655668079853) A[1]:(0.000395052105887) A[2]:(0.728898406029) A[3]:(0.591086745262)\n",
      " state (9)  A[0]:(0.655557870865) A[1]:(0.810123503208) A[2]:(0.809926748276) A[3]:(0.00182568829041)\n",
      " state (10)  A[0]:(0.728404939175) A[1]:(0.900032937527) A[2]:(-4.27961349487e-05) A[3]:(0.729783654213)\n",
      " state (11)  A[0]:(0.519256412983) A[1]:(0.876900076866) A[2]:(-0.604293823242) A[3]:(0.844082355499)\n",
      " state (12)  A[0]:(0.0750625580549) A[1]:(0.824572563171) A[2]:(-0.574750900269) A[3]:(0.793718874454)\n",
      " state (13)  A[0]:(-0.00197800737806) A[1]:(0.809403777122) A[2]:(0.900079667568) A[3]:(0.729745149612)\n",
      " state (14)  A[0]:(0.809318304062) A[1]:(0.900147199631) A[2]:(0.999999940395) A[3]:(0.810544252396)\n",
      " state (15)  A[0]:(0.985374867916) A[1]:(0.958222985268) A[2]:(1.0) A[3]:(0.883302927017)\n",
      "Episode 585000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6015. Times reached goal: 993.               Steps done: 4672187. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00841662371039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53679561615) A[1]:(0.59047472477) A[2]:(0.590365231037) A[3]:(0.532189905643)\n",
      " state (1)  A[0]:(0.537021756172) A[1]:(-0.000201307237148) A[2]:(0.656023979187) A[3]:(0.59074985981)\n",
      " state (2)  A[0]:(0.595794856548) A[1]:(0.729011774063) A[2]:(0.590285122395) A[3]:(0.656280338764)\n",
      " state (3)  A[0]:(0.660757422447) A[1]:(-0.219285473228) A[2]:(0.536444664001) A[3]:(0.519934058189)\n",
      " state (4)  A[0]:(0.595948278904) A[1]:(0.655851960182) A[2]:(-0.000378131837351) A[3]:(0.531461119652)\n",
      " state (5)  A[0]:(0.170049771667) A[1]:(0.928881943226) A[2]:(-0.187926366925) A[3]:(0.519706904888)\n",
      " state (6)  A[0]:(0.00827797222883) A[1]:(0.809914946556) A[2]:(-0.000538706721272) A[3]:(0.656089186668)\n",
      " state (7)  A[0]:(0.636213064194) A[1]:(-0.252115249634) A[2]:(0.280521512032) A[3]:(0.888954699039)\n",
      " state (8)  A[0]:(0.658549368382) A[1]:(-0.000341854989529) A[2]:(0.729038000107) A[3]:(0.589131057262)\n",
      " state (9)  A[0]:(0.657616496086) A[1]:(0.809919834137) A[2]:(0.81007963419) A[3]:(-0.00324885896407)\n",
      " state (10)  A[0]:(0.730018079281) A[1]:(0.899987697601) A[2]:(-0.000450491876109) A[3]:(0.727713704109)\n",
      " state (11)  A[0]:(0.521706163883) A[1]:(0.876891732216) A[2]:(-0.605001091957) A[3]:(0.843041837215)\n",
      " state (12)  A[0]:(0.0781109035015) A[1]:(0.824602603912) A[2]:(-0.575672388077) A[3]:(0.792492628098)\n",
      " state (13)  A[0]:(0.000721573713236) A[1]:(0.809486687183) A[2]:(0.899936079979) A[3]:(0.728252530098)\n",
      " state (14)  A[0]:(0.810097575188) A[1]:(0.900228619576) A[2]:(0.999999940395) A[3]:(0.809492766857)\n",
      " state (15)  A[0]:(0.985408723354) A[1]:(0.95825946331) A[2]:(1.0) A[3]:(0.88260436058)\n",
      "Episode 586000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6029. Times reached goal: 992.               Steps done: 4678216. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00836603254635.\n",
      " state (0)  A[0]:(0.53137922287) A[1]:(0.59007871151) A[2]:(0.590495586395) A[3]:(0.531208157539)\n",
      " state (1)  A[0]:(0.531345248222) A[1]:(-9.86307859421e-05) A[2]:(0.656042695045) A[3]:(0.590257406235)\n",
      " state (2)  A[0]:(0.590471982956) A[1]:(0.729004144669) A[2]:(0.590270638466) A[3]:(0.655970871449)\n",
      " state (3)  A[0]:(0.656015515327) A[1]:(-0.220335587859) A[2]:(0.536691784859) A[3]:(0.51956641674)\n",
      " state (4)  A[0]:(0.590360283852) A[1]:(0.656221032143) A[2]:(-0.000102281570435) A[3]:(0.531286358833)\n",
      " state (5)  A[0]:(0.16153678298) A[1]:(0.928931415081) A[2]:(-0.187372922897) A[3]:(0.519474983215)\n",
      " state (6)  A[0]:(0.000132948160172) A[1]:(0.810025393963) A[2]:(0.000173330307007) A[3]:(0.655869364738)\n",
      " state (7)  A[0]:(0.632461547852) A[1]:(-0.251821160316) A[2]:(0.281214296818) A[3]:(0.888942122459)\n",
      " state (8)  A[0]:(0.656229257584) A[1]:(-5.23775815964e-05) A[2]:(0.729194819927) A[3]:(0.590025305748)\n",
      " state (9)  A[0]:(0.656315684319) A[1]:(0.809972465038) A[2]:(0.810040652752) A[3]:(-0.000319480895996)\n",
      " state (10)  A[0]:(0.729297459126) A[1]:(0.899996101856) A[2]:(-0.000342130661011) A[3]:(0.729020237923)\n",
      " state (11)  A[0]:(0.520946145058) A[1]:(0.876895427704) A[2]:(-0.60486972332) A[3]:(0.843716979027)\n",
      " state (12)  A[0]:(0.0775262266397) A[1]:(0.82459962368) A[2]:(-0.575596630573) A[3]:(0.793241500854)\n",
      " state (13)  A[0]:(0.000509828270879) A[1]:(0.809465706348) A[2]:(0.900004029274) A[3]:(0.729068756104)\n",
      " state (14)  A[0]:(0.810125946999) A[1]:(0.900204360485) A[2]:(0.999999940395) A[3]:(0.809994876385)\n",
      " state (15)  A[0]:(0.985406160355) A[1]:(0.95823186636) A[2]:(1.0) A[3]:(0.882839441299)\n",
      "Episode 587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 990.               Steps done: 4684221. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0083159450591.\n",
      " state (0)  A[0]:(0.531626105309) A[1]:(0.590475082397) A[2]:(0.590528488159) A[3]:(0.531378746033)\n",
      " state (1)  A[0]:(0.531662106514) A[1]:(-1.46701931953e-05) A[2]:(0.656162023544) A[3]:(0.590445995331)\n",
      " state (2)  A[0]:(0.590684890747) A[1]:(0.729072093964) A[2]:(0.59046792984) A[3]:(0.656196832657)\n",
      " state (3)  A[0]:(0.656298041344) A[1]:(-0.219406560063) A[2]:(0.536794960499) A[3]:(0.519739627838)\n",
      " state (4)  A[0]:(0.590558648109) A[1]:(0.656207680702) A[2]:(7.55786895752e-05) A[3]:(0.5314463377)\n",
      " state (5)  A[0]:(0.161607578397) A[1]:(0.92893832922) A[2]:(-0.187394201756) A[3]:(0.519820988178)\n",
      " state (6)  A[0]:(0.000216841697693) A[1]:(0.810012459755) A[2]:(0.000126838684082) A[3]:(0.656162381172)\n",
      " state (7)  A[0]:(0.632411003113) A[1]:(-0.251685142517) A[2]:(0.281216800213) A[3]:(0.888957023621)\n",
      " state (8)  A[0]:(0.655973553658) A[1]:(0.0002725943923) A[2]:(0.729134082794) A[3]:(0.590076863766)\n",
      " state (9)  A[0]:(0.65588670969) A[1]:(0.810052633286) A[2]:(0.810057520866) A[3]:(-0.000399813026888)\n",
      " state (10)  A[0]:(0.728893756866) A[1]:(0.900028347969) A[2]:(-0.000160336494446) A[3]:(0.728791356087)\n",
      " state (11)  A[0]:(0.520330190659) A[1]:(0.876919984818) A[2]:(-0.604816794395) A[3]:(0.843520760536)\n",
      " state (12)  A[0]:(0.0767266750336) A[1]:(0.82460796833) A[2]:(-0.575673222542) A[3]:(0.792969405651)\n",
      " state (13)  A[0]:(-0.000302076339722) A[1]:(0.809434890747) A[2]:(0.900004327297) A[3]:(0.728763639927)\n",
      " state (14)  A[0]:(0.809810936451) A[1]:(0.900156378746) A[2]:(0.999999940395) A[3]:(0.80988496542)\n",
      " state (15)  A[0]:(0.985367655754) A[1]:(0.958189845085) A[2]:(1.0) A[3]:(0.882833480835)\n",
      "Episode 588000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 988.               Steps done: 4690227. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00826614917979.\n",
      " state (0)  A[0]:(0.531317591667) A[1]:(0.590455114841) A[2]:(0.590542018414) A[3]:(0.531417489052)\n",
      " state (1)  A[0]:(0.531517505646) A[1]:(6.32554292679e-06) A[2]:(0.656050562859) A[3]:(0.59047794342)\n",
      " state (2)  A[0]:(0.590633630753) A[1]:(0.728883504868) A[2]:(0.590510725975) A[3]:(0.656131744385)\n",
      " state (3)  A[0]:(0.656355500221) A[1]:(-0.222279131413) A[2]:(0.537247061729) A[3]:(0.519800662994)\n",
      " state (4)  A[0]:(0.590720295906) A[1]:(0.656206309795) A[2]:(0.000127077102661) A[3]:(0.531756997108)\n",
      " state (5)  A[0]:(0.161666810513) A[1]:(0.928930401802) A[2]:(-0.187351420522) A[3]:(0.51994562149)\n",
      " state (6)  A[0]:(-0.000219762325287) A[1]:(0.809987068176) A[2]:(0.000159740447998) A[3]:(0.656042098999)\n",
      " state (7)  A[0]:(0.631944775581) A[1]:(-0.251878350973) A[2]:(0.281281471252) A[3]:(0.8888656497)\n",
      " state (8)  A[0]:(0.656020224094) A[1]:(-8.87736678123e-05) A[2]:(0.729081988335) A[3]:(0.590120613575)\n",
      " state (9)  A[0]:(0.656473994255) A[1]:(0.809948325157) A[2]:(0.810059666634) A[3]:(-1.01029872894e-05)\n",
      " state (10)  A[0]:(0.729419112206) A[1]:(0.899971306324) A[2]:(2.15768814087e-05) A[3]:(0.729033231735)\n",
      " state (11)  A[0]:(0.52103471756) A[1]:(0.876849293709) A[2]:(-0.604751288891) A[3]:(0.843690395355)\n",
      " state (12)  A[0]:(0.0774955376983) A[1]:(0.824514687061) A[2]:(-0.575749278069) A[3]:(0.793200135231)\n",
      " state (13)  A[0]:(0.000294268131256) A[1]:(0.809344470501) A[2]:(0.899982452393) A[3]:(0.729029774666)\n",
      " state (14)  A[0]:(0.810011506081) A[1]:(0.900118887424) A[2]:(0.999999940395) A[3]:(0.810003042221)\n",
      " state (15)  A[0]:(0.985378623009) A[1]:(0.958174943924) A[2]:(1.0) A[3]:(0.882837176323)\n",
      "Episode 589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6020. Times reached goal: 995.               Steps done: 4696247. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00821653644589.\n",
      "q_values \n",
      "tensor([[ 0.5324,  0.5908,  0.5907,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5915,  0.6566,  0.0002,  0.5312]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6568, -0.0004,  0.7291,  0.5904]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6568,  0.8098,  0.8099, -0.0001]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7295,  0.9001, -0.0002,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532335877419) A[1]:(0.590688228607) A[2]:(0.590410590172) A[3]:(0.531571269035)\n",
      " state (1)  A[0]:(0.532123804092) A[1]:(-6.98044896126e-05) A[2]:(0.656124711037) A[3]:(0.590531110764)\n",
      " state (2)  A[0]:(0.59099817276) A[1]:(0.728884339333) A[2]:(0.590534210205) A[3]:(0.656191408634)\n",
      " state (3)  A[0]:(0.656587362289) A[1]:(-0.22088176012) A[2]:(0.53690135479) A[3]:(0.519639074802)\n",
      " state (4)  A[0]:(0.590836286545) A[1]:(0.655864477158) A[2]:(-0.000121474266052) A[3]:(0.531386971474)\n",
      " state (5)  A[0]:(0.161801487207) A[1]:(0.928839325905) A[2]:(-0.187671855092) A[3]:(0.5197660923)\n",
      " state (6)  A[0]:(0.000199288129807) A[1]:(0.809740304947) A[2]:(-0.000102996826172) A[3]:(0.656204223633)\n",
      " state (7)  A[0]:(0.632393240929) A[1]:(-0.252491682768) A[2]:(0.281283438206) A[3]:(0.889042675495)\n",
      " state (8)  A[0]:(0.656111717224) A[1]:(-0.000634200812783) A[2]:(0.72908949852) A[3]:(0.59033882618)\n",
      " state (9)  A[0]:(0.65610653162) A[1]:(0.809741795063) A[2]:(0.809971213341) A[3]:(-0.000362947554095)\n",
      " state (10)  A[0]:(0.728986501694) A[1]:(0.899834275246) A[2]:(-0.000614643038716) A[3]:(0.72885209322)\n",
      " state (11)  A[0]:(0.520301103592) A[1]:(0.876659393311) A[2]:(-0.605343699455) A[3]:(0.843601107597)\n",
      " state (12)  A[0]:(0.0764648616314) A[1]:(0.824226856232) A[2]:(-0.576458632946) A[3]:(0.793107628822)\n",
      " state (13)  A[0]:(-0.000630944909062) A[1]:(0.809025645256) A[2]:(0.899919748306) A[3]:(0.728969395161)\n",
      " state (14)  A[0]:(0.809806108475) A[1]:(0.899949550629) A[2]:(0.999999940395) A[3]:(0.810054242611)\n",
      " state (15)  A[0]:(0.985351204872) A[1]:(0.958089232445) A[2]:(1.0) A[3]:(0.882886171341)\n",
      "Episode 590000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6026. Times reached goal: 992.               Steps done: 4702273. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00816717248027.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53220975399) A[1]:(0.590464115143) A[2]:(0.590375185013) A[3]:(0.531696081161)\n",
      " state (1)  A[0]:(0.532208681107) A[1]:(0.000108063220978) A[2]:(0.656048893929) A[3]:(0.590561270714)\n",
      " state (2)  A[0]:(0.591196060181) A[1]:(0.728935301304) A[2]:(0.590400099754) A[3]:(0.656230270863)\n",
      " state (3)  A[0]:(0.656785368919) A[1]:(-0.220659896731) A[2]:(0.536942481995) A[3]:(0.519986629486)\n",
      " state (4)  A[0]:(0.591206252575) A[1]:(0.655978322029) A[2]:(4.19616699219e-05) A[3]:(0.531912922859)\n",
      " state (5)  A[0]:(0.162573084235) A[1]:(0.928903639317) A[2]:(-0.187548786402) A[3]:(0.520422458649)\n",
      " state (6)  A[0]:(0.000768869940657) A[1]:(0.810037612915) A[2]:(8.54730606079e-05) A[3]:(0.656648993492)\n",
      " state (7)  A[0]:(0.632519364357) A[1]:(-0.251611083746) A[2]:(0.281641185284) A[3]:(0.889160752296)\n",
      " state (8)  A[0]:(0.656383872032) A[1]:(-7.93933868408e-05) A[2]:(0.729164779186) A[3]:(0.590968132019)\n",
      " state (9)  A[0]:(0.656428217888) A[1]:(0.809945344925) A[2]:(0.810189723969) A[3]:(-0.000106066465378)\n",
      " state (10)  A[0]:(0.729438900948) A[1]:(0.89998203516) A[2]:(0.000292897224426) A[3]:(0.728865385056)\n",
      " state (11)  A[0]:(0.521335959435) A[1]:(0.876856088638) A[2]:(-0.604818284512) A[3]:(0.843648970127)\n",
      " state (12)  A[0]:(0.0781743526459) A[1]:(0.824497103691) A[2]:(-0.575969278812) A[3]:(0.793188333511)\n",
      " state (13)  A[0]:(0.00117295922246) A[1]:(0.80929517746) A[2]:(0.900128722191) A[3]:(0.729050219059)\n",
      " state (14)  A[0]:(0.810338973999) A[1]:(0.900079131126) A[2]:(0.999999940395) A[3]:(0.810066401958)\n",
      " state (15)  A[0]:(0.985372245312) A[1]:(0.958123147488) A[2]:(1.0) A[3]:(0.882821798325)\n",
      "Episode 591000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6032. Times reached goal: 993.               Steps done: 4708305. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00811805637896.\n",
      " state (0)  A[0]:(0.531734704971) A[1]:(0.59063243866) A[2]:(0.590603113174) A[3]:(0.531460702419)\n",
      " state (1)  A[0]:(0.531653225422) A[1]:(-0.000124223530293) A[2]:(0.656185388565) A[3]:(0.590584218502)\n",
      " state (2)  A[0]:(0.590686917305) A[1]:(0.729048967361) A[2]:(0.590741038322) A[3]:(0.656265854836)\n",
      " state (3)  A[0]:(0.656480789185) A[1]:(-0.221062794328) A[2]:(0.537082374096) A[3]:(0.519614338875)\n",
      " state (4)  A[0]:(0.590896248817) A[1]:(0.656105935574) A[2]:(-0.000102639198303) A[3]:(0.531469106674)\n",
      " state (5)  A[0]:(0.162008106709) A[1]:(0.928903102875) A[2]:(-0.187728092074) A[3]:(0.519953370094)\n",
      " state (6)  A[0]:(0.000344544649124) A[1]:(0.809893012047) A[2]:(-0.000131726264954) A[3]:(0.656246781349)\n",
      " state (7)  A[0]:(0.632318735123) A[1]:(-0.252087116241) A[2]:(0.281305730343) A[3]:(0.888966560364)\n",
      " state (8)  A[0]:(0.656209111214) A[1]:(-0.000246711075306) A[2]:(0.728797197342) A[3]:(0.590742707253)\n",
      " state (9)  A[0]:(0.656263530254) A[1]:(0.809897482395) A[2]:(0.809819579124) A[3]:(0.000492513121571)\n",
      " state (10)  A[0]:(0.729049921036) A[1]:(0.89994096756) A[2]:(-0.000666618230753) A[3]:(0.729092597961)\n",
      " state (11)  A[0]:(0.520339012146) A[1]:(0.876813352108) A[2]:(-0.605445742607) A[3]:(0.843689203262)\n",
      " state (12)  A[0]:(0.0764167457819) A[1]:(0.824480354786) A[2]:(-0.576739907265) A[3]:(0.793147683144)\n",
      " state (13)  A[0]:(-0.00080135447206) A[1]:(0.809348702431) A[2]:(0.899950146675) A[3]:(0.728907525539)\n",
      " state (14)  A[0]:(0.80971622467) A[1]:(0.900160431862) A[2]:(0.999999940395) A[3]:(0.809897899628)\n",
      " state (15)  A[0]:(0.985316693783) A[1]:(0.958176016808) A[2]:(1.0) A[3]:(0.882653474808)\n",
      "Episode 592000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6013. Times reached goal: 991.               Steps done: 4714318. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00806938897115.\n",
      " state (0)  A[0]:(0.531704545021) A[1]:(0.59057533741) A[2]:(0.590443432331) A[3]:(0.532014489174)\n",
      " state (1)  A[0]:(0.531830668449) A[1]:(-0.000114426016808) A[2]:(0.656090855598) A[3]:(0.591138482094)\n",
      " state (2)  A[0]:(0.590903759003) A[1]:(0.729063272476) A[2]:(0.590614795685) A[3]:(0.65678691864)\n",
      " state (3)  A[0]:(0.65651845932) A[1]:(-0.220048546791) A[2]:(0.536997199059) A[3]:(0.5206990242)\n",
      " state (4)  A[0]:(0.590927481651) A[1]:(0.656002759933) A[2]:(-8.94069671631e-05) A[3]:(0.53253018856)\n",
      " state (5)  A[0]:(0.162193745375) A[1]:(0.928899765015) A[2]:(-0.188006281853) A[3]:(0.520988106728)\n",
      " state (6)  A[0]:(0.00047397610615) A[1]:(0.809945106506) A[2]:(-0.000572681368794) A[3]:(0.656884431839)\n",
      " state (7)  A[0]:(0.632208585739) A[1]:(-0.251803934574) A[2]:(0.280977576971) A[3]:(0.889072179794)\n",
      " state (8)  A[0]:(0.655935704708) A[1]:(1.37239694595e-05) A[2]:(0.728682994843) A[3]:(0.590702772141)\n",
      " state (9)  A[0]:(0.655812978745) A[1]:(0.809926271439) A[2]:(0.809762716293) A[3]:(1.20997428894e-05)\n",
      " state (10)  A[0]:(0.728655576706) A[1]:(0.899924337864) A[2]:(-0.000785827462096) A[3]:(0.728773593903)\n",
      " state (11)  A[0]:(0.51980304718) A[1]:(0.876759529114) A[2]:(-0.605580568314) A[3]:(0.843464493752)\n",
      " state (12)  A[0]:(0.0758197382092) A[1]:(0.824355304241) A[2]:(-0.57701832056) A[3]:(0.792838037014)\n",
      " state (13)  A[0]:(-0.00130593706854) A[1]:(0.809148073196) A[2]:(0.899867713451) A[3]:(0.728493988514)\n",
      " state (14)  A[0]:(0.809585094452) A[1]:(0.900007367134) A[2]:(0.999999940395) A[3]:(0.809587955475)\n",
      " state (15)  A[0]:(0.985303163528) A[1]:(0.95808583498) A[2]:(1.0) A[3]:(0.882439434528)\n",
      "Episode 593000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 988.               Steps done: 4720332. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0080210053014.\n",
      " state (0)  A[0]:(0.531377911568) A[1]:(0.590438365936) A[2]:(0.590460062027) A[3]:(0.531431674957)\n",
      " state (1)  A[0]:(0.531419038773) A[1]:(-4.36305999756e-05) A[2]:(0.6560972929) A[3]:(0.590515255928)\n",
      " state (2)  A[0]:(0.59053170681) A[1]:(0.728994369507) A[2]:(0.590603590012) A[3]:(0.656195044518)\n",
      " state (3)  A[0]:(0.656149983406) A[1]:(-0.221323728561) A[2]:(0.537197589874) A[3]:(0.519521653652)\n",
      " state (4)  A[0]:(0.590462327003) A[1]:(0.656430482864) A[2]:(3.32593917847e-05) A[3]:(0.531443834305)\n",
      " state (5)  A[0]:(0.161447137594) A[1]:(0.928971409798) A[2]:(-0.187597095966) A[3]:(0.519934058189)\n",
      " state (6)  A[0]:(5.79059123993e-05) A[1]:(0.810008883476) A[2]:(9.76324081421e-05) A[3]:(0.656152963638)\n",
      " state (7)  A[0]:(0.63234937191) A[1]:(-0.251815915108) A[2]:(0.28174829483) A[3]:(0.888845205307)\n",
      " state (8)  A[0]:(0.656327843666) A[1]:(0.000206351280212) A[2]:(0.729001045227) A[3]:(0.590363383293)\n",
      " state (9)  A[0]:(0.656378149986) A[1]:(0.810066640377) A[2]:(0.80999559164) A[3]:(6.27338886261e-06)\n",
      " state (10)  A[0]:(0.729151725769) A[1]:(0.899999082088) A[2]:(-8.20159912109e-05) A[3]:(0.72892332077)\n",
      " state (11)  A[0]:(0.520595550537) A[1]:(0.876834750175) A[2]:(-0.605215549469) A[3]:(0.843621492386)\n",
      " state (12)  A[0]:(0.0769018903375) A[1]:(0.82443857193) A[2]:(-0.576758503914) A[3]:(0.79310542345)\n",
      " state (13)  A[0]:(-0.000228941440582) A[1]:(0.809222638607) A[2]:(0.899994730949) A[3]:(0.728911876678)\n",
      " state (14)  A[0]:(0.809964537621) A[1]:(0.900051653385) A[2]:(0.999999940395) A[3]:(0.80997043848)\n",
      " state (15)  A[0]:(0.985322654247) A[1]:(0.958096563816) A[2]:(1.0) A[3]:(0.882698774338)\n",
      "Episode 594000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6016. Times reached goal: 993.               Steps done: 4726348. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00797289579201.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5906,  0.5907,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.3149e-01, -3.3155e-06,  6.5634e-01,  5.8968e-01]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7292,  0.5904,  0.6556]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0015,  0.8101,  0.0004,  0.6556]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7300,  0.9000,  0.0007,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9001,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530416786671) A[1]:(0.590774297714) A[2]:(0.590601980686) A[3]:(0.532294750214)\n",
      " state (1)  A[0]:(0.530398845673) A[1]:(0.000388964981539) A[2]:(0.656538188457) A[3]:(0.591321706772)\n",
      " state (2)  A[0]:(0.589743435383) A[1]:(0.729388654232) A[2]:(0.590875267982) A[3]:(0.657050013542)\n",
      " state (3)  A[0]:(0.655850291252) A[1]:(-0.217767804861) A[2]:(0.537042200565) A[3]:(0.52115637064)\n",
      " state (4)  A[0]:(0.590346217155) A[1]:(0.656454741955) A[2]:(0.000657319906168) A[3]:(0.532895207405)\n",
      " state (5)  A[0]:(0.161553859711) A[1]:(0.928949773312) A[2]:(-0.186796471477) A[3]:(0.521496534348)\n",
      " state (6)  A[0]:(-3.35574150085e-05) A[1]:(0.810157597065) A[2]:(0.00093460053904) A[3]:(0.657515108585)\n",
      " state (7)  A[0]:(0.632479667664) A[1]:(-0.251300185919) A[2]:(0.282456994057) A[3]:(0.889589071274)\n",
      " state (8)  A[0]:(0.657103657722) A[1]:(0.00041405853699) A[2]:(0.729037761688) A[3]:(0.593646705151)\n",
      " state (9)  A[0]:(0.657290935516) A[1]:(0.810200631618) A[2]:(0.810060679913) A[3]:(0.00459925550967)\n",
      " state (10)  A[0]:(0.729858994484) A[1]:(0.900098085403) A[2]:(0.00069904315751) A[3]:(0.730593442917)\n",
      " state (11)  A[0]:(0.521709442139) A[1]:(0.876987993717) A[2]:(-0.604630947113) A[3]:(0.844512104988)\n",
      " state (12)  A[0]:(0.0783806219697) A[1]:(0.824700951576) A[2]:(-0.576368927956) A[3]:(0.794149339199)\n",
      " state (13)  A[0]:(0.00089880800806) A[1]:(0.80953669548) A[2]:(0.899909973145) A[3]:(0.730048537254)\n",
      " state (14)  A[0]:(0.810154259205) A[1]:(0.900236666203) A[2]:(0.999999940395) A[3]:(0.810598134995)\n",
      " state (15)  A[0]:(0.985332012177) A[1]:(0.958191156387) A[2]:(1.0) A[3]:(0.882985413074)\n",
      "Episode 595000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6018. Times reached goal: 994.               Steps done: 4732366. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00792505899044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531595885754) A[1]:(0.590395987034) A[2]:(0.590355992317) A[3]:(0.531396150589)\n",
      " state (1)  A[0]:(0.531536102295) A[1]:(-3.83704900742e-06) A[2]:(0.655958294868) A[3]:(0.590457201004)\n",
      " state (2)  A[0]:(0.590574920177) A[1]:(0.728995680809) A[2]:(0.590368747711) A[3]:(0.656103730202)\n",
      " state (3)  A[0]:(0.656048774719) A[1]:(-0.220539972186) A[2]:(0.536972403526) A[3]:(0.519571602345)\n",
      " state (4)  A[0]:(0.590334892273) A[1]:(0.656106233597) A[2]:(9.22679901123e-05) A[3]:(0.531335115433)\n",
      " state (5)  A[0]:(0.161452904344) A[1]:(0.928916811943) A[2]:(-0.187515094876) A[3]:(0.519765794277)\n",
      " state (6)  A[0]:(-0.000148326158524) A[1]:(0.81002831459) A[2]:(0.000332593917847) A[3]:(0.655925154686)\n",
      " state (7)  A[0]:(0.631992042065) A[1]:(-0.251634031534) A[2]:(0.282296150923) A[3]:(0.888772130013)\n",
      " state (8)  A[0]:(0.656157255173) A[1]:(0.000142224133015) A[2]:(0.72907269001) A[3]:(0.590710520744)\n",
      " state (9)  A[0]:(0.656170845032) A[1]:(0.810070991516) A[2]:(0.810047745705) A[3]:(0.00048950308701)\n",
      " state (10)  A[0]:(0.729045808315) A[1]:(0.900045454502) A[2]:(0.000238060951233) A[3]:(0.72907102108)\n",
      " state (11)  A[0]:(0.52059262991) A[1]:(0.876940011978) A[2]:(-0.605121254921) A[3]:(0.843724250793)\n",
      " state (12)  A[0]:(0.0769697055221) A[1]:(0.824638843536) A[2]:(-0.576893091202) A[3]:(0.793232083321)\n",
      " state (13)  A[0]:(-0.000300139188766) A[1]:(0.809476613998) A[2]:(0.899977087975) A[3]:(0.729012489319)\n",
      " state (14)  A[0]:(0.809868574142) A[1]:(0.900207638741) A[2]:(0.999999940395) A[3]:(0.809975385666)\n",
      " state (15)  A[0]:(0.985291063786) A[1]:(0.958156585693) A[2]:(1.0) A[3]:(0.882607936859)\n",
      "Episode 596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6030. Times reached goal: 996.               Steps done: 4738396. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0078774146767.\n",
      " state (0)  A[0]:(0.531295120716) A[1]:(0.59055685997) A[2]:(0.590459227562) A[3]:(0.531453669071)\n",
      " state (1)  A[0]:(0.531295061111) A[1]:(0.000137224793434) A[2]:(0.656111598015) A[3]:(0.590564131737)\n",
      " state (2)  A[0]:(0.590282440186) A[1]:(0.728959441185) A[2]:(0.591035842896) A[3]:(0.656189441681)\n",
      " state (3)  A[0]:(0.655962705612) A[1]:(-0.222602337599) A[2]:(0.53774356842) A[3]:(0.519573569298)\n",
      " state (4)  A[0]:(0.590318322182) A[1]:(0.655695080757) A[2]:(0.00042617318104) A[3]:(0.531637907028)\n",
      " state (5)  A[0]:(0.161332160234) A[1]:(0.928887367249) A[2]:(-0.187640681863) A[3]:(0.52028208971)\n",
      " state (6)  A[0]:(-0.000496208609547) A[1]:(0.81000995636) A[2]:(6.27040863037e-05) A[3]:(0.656320214272)\n",
      " state (7)  A[0]:(0.631407141685) A[1]:(-0.251464813948) A[2]:(0.282301098108) A[3]:(0.888781547546)\n",
      " state (8)  A[0]:(0.655379474163) A[1]:(0.000438436836703) A[2]:(0.729382097721) A[3]:(0.590152323246)\n",
      " state (9)  A[0]:(0.655556440353) A[1]:(0.810109376907) A[2]:(0.810315966606) A[3]:(-0.000210985541344)\n",
      " state (10)  A[0]:(0.728804349899) A[1]:(0.9000441432) A[2]:(0.000603437365498) A[3]:(0.729044437408)\n",
      " state (11)  A[0]:(0.520508289337) A[1]:(0.876918196678) A[2]:(-0.605102062225) A[3]:(0.843803286552)\n",
      " state (12)  A[0]:(0.0771543830633) A[1]:(0.824577093124) A[2]:(-0.576889395714) A[3]:(0.793387234211)\n",
      " state (13)  A[0]:(0.000314027070999) A[1]:(0.809388458729) A[2]:(0.900232732296) A[3]:(0.72927826643)\n",
      " state (14)  A[0]:(0.810265183449) A[1]:(0.900151431561) A[2]:(0.999999940395) A[3]:(0.810241937637)\n",
      " state (15)  A[0]:(0.985306978226) A[1]:(0.958101511002) A[2]:(1.0) A[3]:(0.88274538517)\n",
      "Episode 597000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6025. Times reached goal: 994.               Steps done: 4744421. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0078300959441.\n",
      " state (0)  A[0]:(0.530274987221) A[1]:(0.590581893921) A[2]:(0.590519726276) A[3]:(0.529109477997)\n",
      " state (1)  A[0]:(0.530391871929) A[1]:(4.97624278069e-05) A[2]:(0.656202554703) A[3]:(0.589035511017)\n",
      " state (2)  A[0]:(0.589774608612) A[1]:(0.729101657867) A[2]:(0.591047048569) A[3]:(0.655220508575)\n",
      " state (3)  A[0]:(0.656148731709) A[1]:(-0.221539691091) A[2]:(0.537684261799) A[3]:(0.518929064274)\n",
      " state (4)  A[0]:(0.591068744659) A[1]:(0.656423926353) A[2]:(0.000346660614014) A[3]:(0.531408905983)\n",
      " state (5)  A[0]:(0.163044393063) A[1]:(0.929028570652) A[2]:(-0.187709122896) A[3]:(0.520372271538)\n",
      " state (6)  A[0]:(0.00242608319968) A[1]:(0.810101747513) A[2]:(9.64403152466e-05) A[3]:(0.656672954559)\n",
      " state (7)  A[0]:(0.633651018143) A[1]:(-0.251543074846) A[2]:(0.282349377871) A[3]:(0.888896167278)\n",
      " state (8)  A[0]:(0.656977653503) A[1]:(0.000591449381318) A[2]:(0.729210495949) A[3]:(0.59009206295)\n",
      " state (9)  A[0]:(0.656554102898) A[1]:(0.810212731361) A[2]:(0.810102641582) A[3]:(-0.000989079126157)\n",
      " state (10)  A[0]:(0.729421496391) A[1]:(0.90011125803) A[2]:(-0.000588416995015) A[3]:(0.729024231434)\n",
      " state (11)  A[0]:(0.521294474602) A[1]:(0.87699753046) A[2]:(-0.606176614761) A[3]:(0.844023108482)\n",
      " state (12)  A[0]:(0.0780182778835) A[1]:(0.824693739414) A[2]:(-0.578144788742) A[3]:(0.793878495693)\n",
      " state (13)  A[0]:(0.0011244113557) A[1]:(0.80955350399) A[2]:(0.900045514107) A[3]:(0.730155229568)\n",
      " state (14)  A[0]:(0.810579299927) A[1]:(0.900280177593) A[2]:(0.999999940395) A[3]:(0.811122059822)\n",
      " state (15)  A[0]:(0.985313415527) A[1]:(0.958157420158) A[2]:(1.0) A[3]:(0.883399307728)\n",
      "Episode 598000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6012. Times reached goal: 989.               Steps done: 4750433. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00778316263019.\n",
      " state (0)  A[0]:(0.531356930733) A[1]:(0.590556025505) A[2]:(0.590537667274) A[3]:(0.531090378761)\n",
      " state (1)  A[0]:(0.531485676765) A[1]:(7.90953636169e-05) A[2]:(0.656134128571) A[3]:(0.590124726295)\n",
      " state (2)  A[0]:(0.590656459332) A[1]:(0.729029655457) A[2]:(0.59080773592) A[3]:(0.655799269676)\n",
      " state (3)  A[0]:(0.656199455261) A[1]:(-0.222606942058) A[2]:(0.537326455116) A[3]:(0.519094586372)\n",
      " state (4)  A[0]:(0.590562462807) A[1]:(0.656353831291) A[2]:(-0.000219821929932) A[3]:(0.53125)\n",
      " state (5)  A[0]:(0.161647632718) A[1]:(0.92894589901) A[2]:(-0.187887147069) A[3]:(0.519834399223)\n",
      " state (6)  A[0]:(-5.9187412262e-05) A[1]:(0.810018360615) A[2]:(-3.00407409668e-05) A[3]:(0.655954480171)\n",
      " state (7)  A[0]:(0.632059633732) A[1]:(-0.251833826303) A[2]:(0.282213330269) A[3]:(0.888710260391)\n",
      " state (8)  A[0]:(0.656239509583) A[1]:(-5.01573085785e-05) A[2]:(0.728965818882) A[3]:(0.590460896492)\n",
      " state (9)  A[0]:(0.656165480614) A[1]:(0.810013771057) A[2]:(0.809931635857) A[3]:(0.000116750597954)\n",
      " state (10)  A[0]:(0.729020357132) A[1]:(0.899995148182) A[2]:(-0.000336289405823) A[3]:(0.729043364525)\n",
      " state (11)  A[0]:(0.520587563515) A[1]:(0.876880228519) A[2]:(-0.605810284615) A[3]:(0.843751966953)\n",
      " state (12)  A[0]:(0.0769806355238) A[1]:(0.824593186378) A[2]:(-0.577908992767) A[3]:(0.793271660805)\n",
      " state (13)  A[0]:(-0.000285685062408) A[1]:(0.809511780739) A[2]:(0.899957358837) A[3]:(0.72904843092)\n",
      " state (14)  A[0]:(0.809841811657) A[1]:(0.900298893452) A[2]:(0.999999940395) A[3]:(0.809989392757)\n",
      " state (15)  A[0]:(0.985236823559) A[1]:(0.958193063736) A[2]:(1.0) A[3]:(0.882487893105)\n",
      "Episode 599000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6021. Times reached goal: 992.               Steps done: 4756454. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00773644100459.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5907,  0.5905,  0.5305]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5909,  0.6558, -0.0000,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.0013,  0.7292,  0.5901]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.8103,  0.8101, -0.0004]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0009,  0.8088,  0.9000,  0.7285]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.8997,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531319141388) A[1]:(0.590655565262) A[2]:(0.590559720993) A[3]:(0.531830191612)\n",
      " state (1)  A[0]:(0.531499505043) A[1]:(0.000700577977113) A[2]:(0.656139492989) A[3]:(0.590686261654)\n",
      " state (2)  A[0]:(0.590653300285) A[1]:(0.729100942612) A[2]:(0.590374946594) A[3]:(0.656404733658)\n",
      " state (3)  A[0]:(0.656308293343) A[1]:(-0.21877373755) A[2]:(0.536786794662) A[3]:(0.520543456078)\n",
      " state (4)  A[0]:(0.59080862999) A[1]:(0.655772209167) A[2]:(-9.81092453003e-05) A[3]:(0.532476067543)\n",
      " state (5)  A[0]:(0.16242736578) A[1]:(0.928915441036) A[2]:(-0.188292264938) A[3]:(0.521284222603)\n",
      " state (6)  A[0]:(0.000725954654627) A[1]:(0.810160160065) A[2]:(-0.000528097094502) A[3]:(0.657131314278)\n",
      " state (7)  A[0]:(0.632016658783) A[1]:(-0.250772416592) A[2]:(0.282119393349) A[3]:(0.889026761055)\n",
      " state (8)  A[0]:(0.656212210655) A[1]:(0.00137708999682) A[2]:(0.729234576225) A[3]:(0.590606927872)\n",
      " state (9)  A[0]:(0.656644582748) A[1]:(0.810301482677) A[2]:(0.810120522976) A[3]:(-9.60379838943e-05)\n",
      " state (10)  A[0]:(0.72950220108) A[1]:(0.900040745735) A[2]:(0.00045263764332) A[3]:(0.728666603565)\n",
      " state (11)  A[0]:(0.521342217922) A[1]:(0.876802861691) A[2]:(-0.605192363262) A[3]:(0.843403398991)\n",
      " state (12)  A[0]:(0.0780824646354) A[1]:(0.824256718159) A[2]:(-0.577375411987) A[3]:(0.792799413204)\n",
      " state (13)  A[0]:(0.000956386036705) A[1]:(0.808815121651) A[2]:(0.900014042854) A[3]:(0.728544473648)\n",
      " state (14)  A[0]:(0.810434937477) A[1]:(0.899673819542) A[2]:(0.999999940395) A[3]:(0.809830546379)\n",
      " state (15)  A[0]:(0.985307455063) A[1]:(0.957810878754) A[2]:(1.0) A[3]:(0.882558703423)\n",
      "Episode 600000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6034. Times reached goal: 994.               Steps done: 4762488. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00768989987537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531373739243) A[1]:(0.590436816216) A[2]:(0.590502977371) A[3]:(0.531466245651)\n",
      " state (1)  A[0]:(0.5313154459) A[1]:(-2.77161598206e-06) A[2]:(0.656084537506) A[3]:(0.59041172266)\n",
      " state (2)  A[0]:(0.590492844582) A[1]:(0.729014277458) A[2]:(0.590242743492) A[3]:(0.656098008156)\n",
      " state (3)  A[0]:(0.656072258949) A[1]:(-0.218583032489) A[2]:(0.536764502525) A[3]:(0.519863605499)\n",
      " state (4)  A[0]:(0.590453386307) A[1]:(0.656096100807) A[2]:(0.000121355056763) A[3]:(0.531524062157)\n",
      " state (5)  A[0]:(0.161803483963) A[1]:(0.928904473782) A[2]:(-0.187766730785) A[3]:(0.520039796829)\n",
      " state (6)  A[0]:(7.49826431274e-05) A[1]:(0.810027241707) A[2]:(5.69820404053e-05) A[3]:(0.655996263027)\n",
      " state (7)  A[0]:(0.631794810295) A[1]:(-0.251556426287) A[2]:(0.282503277063) A[3]:(0.888642907143)\n",
      " state (8)  A[0]:(0.656034111977) A[1]:(0.00011819601059) A[2]:(0.729054331779) A[3]:(0.590405523777)\n",
      " state (9)  A[0]:(0.65612912178) A[1]:(0.810025453568) A[2]:(0.81004846096) A[3]:(-2.3677945137e-05)\n",
      " state (10)  A[0]:(0.729080915451) A[1]:(0.900020480156) A[2]:(-4.81605529785e-05) A[3]:(0.728951454163)\n",
      " state (11)  A[0]:(0.520753145218) A[1]:(0.876930892467) A[2]:(-0.605810284615) A[3]:(0.843696296215)\n",
      " state (12)  A[0]:(0.0772130712867) A[1]:(0.824671685696) A[2]:(-0.578087806702) A[3]:(0.793176829815)\n",
      " state (13)  A[0]:(2.78949737549e-05) A[1]:(0.809579014778) A[2]:(0.90003234148) A[3]:(0.72893768549)\n",
      " state (14)  A[0]:(0.810091257095) A[1]:(0.900305271149) A[2]:(0.999999940395) A[3]:(0.810022234917)\n",
      " state (15)  A[0]:(0.985245406628) A[1]:(0.958158671856) A[2]:(1.0) A[3]:(0.882530093193)\n",
      "Episode 601000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6028. Times reached goal: 991.               Steps done: 4768516. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00764368459172.\n",
      " state (0)  A[0]:(0.53128361702) A[1]:(0.590131282806) A[2]:(0.590314388275) A[3]:(0.531713366508)\n",
      " state (1)  A[0]:(0.531182408333) A[1]:(0.000149130821228) A[2]:(0.655924916267) A[3]:(0.590702295303)\n",
      " state (2)  A[0]:(0.590277194977) A[1]:(0.728848278522) A[2]:(0.590065121651) A[3]:(0.656413316727)\n",
      " state (3)  A[0]:(0.655751645565) A[1]:(-0.219688683748) A[2]:(0.536688566208) A[3]:(0.520108342171)\n",
      " state (4)  A[0]:(0.589995861053) A[1]:(0.655746936798) A[2]:(-0.000102281570435) A[3]:(0.531897544861)\n",
      " state (5)  A[0]:(0.161077469587) A[1]:(0.928860783577) A[2]:(-0.188069418073) A[3]:(0.520600318909)\n",
      " state (6)  A[0]:(-0.000609636248555) A[1]:(0.809960007668) A[2]:(-0.000303506851196) A[3]:(0.656366407871)\n",
      " state (7)  A[0]:(0.63137370348) A[1]:(-0.251488298178) A[2]:(0.28214353323) A[3]:(0.888625919819)\n",
      " state (8)  A[0]:(0.655585289001) A[1]:(0.000622242630925) A[2]:(0.728935480118) A[3]:(0.590089917183)\n",
      " state (9)  A[0]:(0.655630350113) A[1]:(0.810142934322) A[2]:(0.809934973717) A[3]:(-0.000203818082809)\n",
      " state (10)  A[0]:(0.728552818298) A[1]:(0.899992585182) A[2]:(-8.95261764526e-05) A[3]:(0.728761553764)\n",
      " state (11)  A[0]:(0.519867897034) A[1]:(0.876814007759) A[2]:(-0.605753660202) A[3]:(0.84352761507)\n",
      " state (12)  A[0]:(0.0760503485799) A[1]:(0.824415445328) A[2]:(-0.578163862228) A[3]:(0.792988300323)\n",
      " state (13)  A[0]:(-0.00113061023876) A[1]:(0.809195756912) A[2]:(0.899949610233) A[3]:(0.728769719601)\n",
      " state (14)  A[0]:(0.809703886509) A[1]:(0.900031745434) A[2]:(0.999999940395) A[3]:(0.809958994389)\n",
      " state (15)  A[0]:(0.985213458538) A[1]:(0.958013296127) A[2]:(1.0) A[3]:(0.882528603077)\n",
      "Episode 602000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6025. Times reached goal: 992.               Steps done: 4774541. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00759776984912.\n",
      " state (0)  A[0]:(0.531327724457) A[1]:(0.590311288834) A[2]:(0.590372383595) A[3]:(0.531461000443)\n",
      " state (1)  A[0]:(0.531275749207) A[1]:(2.79396772385e-05) A[2]:(0.656029224396) A[3]:(0.590461850166)\n",
      " state (2)  A[0]:(0.590386509895) A[1]:(0.728938579559) A[2]:(0.590435564518) A[3]:(0.656113803387)\n",
      " state (3)  A[0]:(0.656018972397) A[1]:(-0.220382422209) A[2]:(0.536962389946) A[3]:(0.519514799118)\n",
      " state (4)  A[0]:(0.590409874916) A[1]:(0.655946493149) A[2]:(-8.48770141602e-05) A[3]:(0.531338810921)\n",
      " state (5)  A[0]:(0.161729827523) A[1]:(0.92889547348) A[2]:(-0.18800444901) A[3]:(0.520008802414)\n",
      " state (6)  A[0]:(7.6949596405e-05) A[1]:(0.809985816479) A[2]:(-2.78949737549e-05) A[3]:(0.655917823315)\n",
      " state (7)  A[0]:(0.631810426712) A[1]:(-0.251867175102) A[2]:(0.282699286938) A[3]:(0.888515293598)\n",
      " state (8)  A[0]:(0.655999720097) A[1]:(-0.000374540657504) A[2]:(0.728929340839) A[3]:(0.590323448181)\n",
      " state (9)  A[0]:(0.655880212784) A[1]:(0.809898018837) A[2]:(0.809978842735) A[3]:(-0.000298678874969)\n",
      " state (10)  A[0]:(0.72888147831) A[1]:(0.899988055229) A[2]:(-0.000263571739197) A[3]:(0.728866815567)\n",
      " state (11)  A[0]:(0.520545840263) A[1]:(0.876925230026) A[2]:(-0.606159031391) A[3]:(0.843710780144)\n",
      " state (12)  A[0]:(0.0769682526588) A[1]:(0.824704647064) A[2]:(-0.578677415848) A[3]:(0.793221831322)\n",
      " state (13)  A[0]:(-0.000316023826599) A[1]:(0.809652209282) A[2]:(0.899965822697) A[3]:(0.728944063187)\n",
      " state (14)  A[0]:(0.809889435768) A[1]:(0.90035879612) A[2]:(0.999999940395) A[3]:(0.809901893139)\n",
      " state (15)  A[0]:(0.98519641161) A[1]:(0.958169043064) A[2]:(1.0) A[3]:(0.88230329752)\n",
      "Episode 603000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 990.               Steps done: 4780552. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00755223664171.\n",
      " state (0)  A[0]:(0.531101107597) A[1]:(0.590472102165) A[2]:(0.590516090393) A[3]:(0.531460881233)\n",
      " state (1)  A[0]:(0.531053900719) A[1]:(0.000192448496819) A[2]:(0.656120538712) A[3]:(0.59049808979)\n",
      " state (2)  A[0]:(0.590206980705) A[1]:(0.728963375092) A[2]:(0.590192675591) A[3]:(0.656222224236)\n",
      " state (3)  A[0]:(0.65582638979) A[1]:(-0.219147250056) A[2]:(0.536602735519) A[3]:(0.51988196373)\n",
      " state (4)  A[0]:(0.590029418468) A[1]:(0.6560972929) A[2]:(-0.000288486480713) A[3]:(0.531649768353)\n",
      " state (5)  A[0]:(0.160997316241) A[1]:(0.928895235062) A[2]:(-0.188111394644) A[3]:(0.520329475403)\n",
      " state (6)  A[0]:(-0.000648051383905) A[1]:(0.809977948666) A[2]:(-0.000104188919067) A[3]:(0.656181812286)\n",
      " state (7)  A[0]:(0.63149869442) A[1]:(-0.25164821744) A[2]:(0.282673299313) A[3]:(0.888599157333)\n",
      " state (8)  A[0]:(0.655848741531) A[1]:(0.000159308314323) A[2]:(0.728969573975) A[3]:(0.590421259403)\n",
      " state (9)  A[0]:(0.65588247776) A[1]:(0.810039401054) A[2]:(0.809957206249) A[3]:(-1.43051147461e-05)\n",
      " state (10)  A[0]:(0.728845119476) A[1]:(0.900009214878) A[2]:(-0.000200152397156) A[3]:(0.728884339333)\n",
      " state (11)  A[0]:(0.520445942879) A[1]:(0.876902461052) A[2]:(-0.606084823608) A[3]:(0.843654572964)\n",
      " state (12)  A[0]:(0.0768874287605) A[1]:(0.824616849422) A[2]:(-0.578668355942) A[3]:(0.793135046959)\n",
      " state (13)  A[0]:(-0.000215023756027) A[1]:(0.809486567974) A[2]:(0.899992287159) A[3]:(0.728866457939)\n",
      " state (14)  A[0]:(0.810067474842) A[1]:(0.900212883949) A[2]:(0.999999940395) A[3]:(0.80989664793)\n",
      " state (15)  A[0]:(0.9852181077) A[1]:(0.958075225353) A[2]:(1.0) A[3]:(0.882323861122)\n",
      "Episode 604000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6017. Times reached goal: 995.               Steps done: 4786569. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00750693127173.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5904,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.0002,  0.6562,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7289,  0.5904,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0000,  0.8100,  0.0000,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9001,  0.0004,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9001,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531450510025) A[1]:(0.590465247631) A[2]:(0.590523481369) A[3]:(0.531419873238)\n",
      " state (1)  A[0]:(0.531383633614) A[1]:(9.61348414421e-05) A[2]:(0.656186580658) A[3]:(0.590463280678)\n",
      " state (2)  A[0]:(0.590505599976) A[1]:(0.72904086113) A[2]:(0.590565562248) A[3]:(0.656146168709)\n",
      " state (3)  A[0]:(0.656181097031) A[1]:(-0.219708561897) A[2]:(0.537219882011) A[3]:(0.519740998745)\n",
      " state (4)  A[0]:(0.590503692627) A[1]:(0.656175196171) A[2]:(0.000414848298533) A[3]:(0.53158891201)\n",
      " state (5)  A[0]:(0.16169500351) A[1]:(0.928925991058) A[2]:(-0.187542229891) A[3]:(0.520310401917)\n",
      " state (6)  A[0]:(1.88946723938e-05) A[1]:(0.810068249702) A[2]:(0.000464677781565) A[3]:(0.656166195869)\n",
      " state (7)  A[0]:(0.631896615028) A[1]:(-0.251461625099) A[2]:(0.28339189291) A[3]:(0.888570487499)\n",
      " state (8)  A[0]:(0.656113386154) A[1]:(0.000264875590801) A[2]:(0.729428291321) A[3]:(0.590261101723)\n",
      " state (9)  A[0]:(0.65602850914) A[1]:(0.8100720644) A[2]:(0.810277998447) A[3]:(-2.88933515549e-05)\n",
      " state (10)  A[0]:(0.728980541229) A[1]:(0.900039255619) A[2]:(0.000422954530222) A[3]:(0.729100227356)\n",
      " state (11)  A[0]:(0.520686984062) A[1]:(0.876957714558) A[2]:(-0.605885028839) A[3]:(0.843863070011)\n",
      " state (12)  A[0]:(0.0771953612566) A[1]:(0.8247179389) A[2]:(-0.578584551811) A[3]:(0.793432593346)\n",
      " state (13)  A[0]:(7.2181224823e-05) A[1]:(0.809623062611) A[2]:(0.900104284286) A[3]:(0.729237556458)\n",
      " state (14)  A[0]:(0.810173392296) A[1]:(0.90030002594) A[2]:(0.999999940395) A[3]:(0.810141324997)\n",
      " state (15)  A[0]:(0.985210955143) A[1]:(0.958105027676) A[2]:(1.0) A[3]:(0.882416844368)\n",
      "Episode 605000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 990.               Steps done: 4792576. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00746197230503.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531562447548) A[1]:(0.590490281582) A[2]:(0.59042930603) A[3]:(0.53106969595)\n",
      " state (1)  A[0]:(0.531191885471) A[1]:(0.000129505991936) A[2]:(0.65608382225) A[3]:(0.590198397636)\n",
      " state (2)  A[0]:(0.5903236866) A[1]:(0.729011654854) A[2]:(0.590479671955) A[3]:(0.655928313732)\n",
      " state (3)  A[0]:(0.656122565269) A[1]:(-0.220606535673) A[2]:(0.537115573883) A[3]:(0.519627928734)\n",
      " state (4)  A[0]:(0.590627789497) A[1]:(0.656127750874) A[2]:(-5.14984130859e-05) A[3]:(0.531638562679)\n",
      " state (5)  A[0]:(0.162031009793) A[1]:(0.928915262222) A[2]:(-0.188094139099) A[3]:(0.520369291306)\n",
      " state (6)  A[0]:(0.00038373467396) A[1]:(0.810003042221) A[2]:(-8.95261764526e-05) A[3]:(0.656187176704)\n",
      " state (7)  A[0]:(0.632020235062) A[1]:(-0.251745253801) A[2]:(0.282909870148) A[3]:(0.888575911522)\n",
      " state (8)  A[0]:(0.656238436699) A[1]:(-0.00028495490551) A[2]:(0.728916704655) A[3]:(0.590727746487)\n",
      " state (9)  A[0]:(0.656087696552) A[1]:(0.809909820557) A[2]:(0.809986293316) A[3]:(0.000366270513041)\n",
      " state (10)  A[0]:(0.729002416134) A[1]:(0.899987578392) A[2]:(-0.000422358483775) A[3]:(0.729277133942)\n",
      " state (11)  A[0]:(0.520691156387) A[1]:(0.876923084259) A[2]:(-0.606585800648) A[3]:(0.844017922878)\n",
      " state (12)  A[0]:(0.077110864222) A[1]:(0.82470959425) A[2]:(-0.579390048981) A[3]:(0.793641924858)\n",
      " state (13)  A[0]:(-5.90085983276e-05) A[1]:(0.809669852257) A[2]:(0.900024354458) A[3]:(0.729512691498)\n",
      " state (14)  A[0]:(0.810112833977) A[1]:(0.900359094143) A[2]:(0.999999940395) A[3]:(0.810383796692)\n",
      " state (15)  A[0]:(0.985183000565) A[1]:(0.958129167557) A[2]:(1.0) A[3]:(0.882548570633)\n",
      "Episode 606000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6021. Times reached goal: 993.               Steps done: 4798597. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00741717875608.\n",
      " state (0)  A[0]:(0.531701564789) A[1]:(0.590498447418) A[2]:(0.590666413307) A[3]:(0.531846880913)\n",
      " state (1)  A[0]:(0.531538367271) A[1]:(0.00010784715414) A[2]:(0.656331419945) A[3]:(0.59084713459)\n",
      " state (2)  A[0]:(0.590641498566) A[1]:(0.729096293449) A[2]:(0.590776979923) A[3]:(0.656436443329)\n",
      " state (3)  A[0]:(0.656293451786) A[1]:(-0.22017338872) A[2]:(0.537262141705) A[3]:(0.519840121269)\n",
      " state (4)  A[0]:(0.590639948845) A[1]:(0.656164348125) A[2]:(0.000201821327209) A[3]:(0.531623840332)\n",
      " state (5)  A[0]:(0.161834970117) A[1]:(0.928914010525) A[2]:(-0.187806993723) A[3]:(0.520328760147)\n",
      " state (6)  A[0]:(-0.000106394290924) A[1]:(0.810068428516) A[2]:(0.000191330909729) A[3]:(0.656112790108)\n",
      " state (7)  A[0]:(0.631652414799) A[1]:(-0.251436322927) A[2]:(0.283348917961) A[3]:(0.888504326344)\n",
      " state (8)  A[0]:(0.655871212482) A[1]:(0.000243358314037) A[2]:(0.729402303696) A[3]:(0.589850783348)\n",
      " state (9)  A[0]:(0.655795931816) A[1]:(0.810087740421) A[2]:(0.810216426849) A[3]:(-0.00102142954711)\n",
      " state (10)  A[0]:(0.728853821754) A[1]:(0.900050222874) A[2]:(-0.00013279914856) A[3]:(0.728706002235)\n",
      " state (11)  A[0]:(0.520545661449) A[1]:(0.876985669136) A[2]:(-0.606526911259) A[3]:(0.843659162521)\n",
      " state (12)  A[0]:(0.0769938752055) A[1]:(0.824798226357) A[2]:(-0.579428911209) A[3]:(0.793173909187)\n",
      " state (13)  A[0]:(-0.000125914812088) A[1]:(0.8097833395) A[2]:(0.900102138519) A[3]:(0.728898644447)\n",
      " state (14)  A[0]:(0.810081958771) A[1]:(0.900439143181) A[2]:(0.999999940395) A[3]:(0.80988073349)\n",
      " state (15)  A[0]:(0.98516201973) A[1]:(0.958159267902) A[2]:(1.0) A[3]:(0.882144093513)\n",
      "Episode 607000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6001. Times reached goal: 988.               Steps done: 4804598. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00737280155334.\n",
      " state (0)  A[0]:(0.53182888031) A[1]:(0.590386152267) A[2]:(0.590350151062) A[3]:(0.531273841858)\n",
      " state (1)  A[0]:(0.531671226025) A[1]:(1.45956873894e-05) A[2]:(0.656037926674) A[3]:(0.590445876122)\n",
      " state (2)  A[0]:(0.590696752071) A[1]:(0.72893512249) A[2]:(0.590601980686) A[3]:(0.65610909462)\n",
      " state (3)  A[0]:(0.656092882156) A[1]:(-0.22108002007) A[2]:(0.537227869034) A[3]:(0.519662797451)\n",
      " state (4)  A[0]:(0.590397357941) A[1]:(0.655944883823) A[2]:(-3.38554382324e-05) A[3]:(0.531627297401)\n",
      " state (5)  A[0]:(0.161643624306) A[1]:(0.928905844688) A[2]:(-0.188175097108) A[3]:(0.520404577255)\n",
      " state (6)  A[0]:(-9.96589660645e-05) A[1]:(0.809946358204) A[2]:(-4.02927398682e-05) A[3]:(0.656179368496)\n",
      " state (7)  A[0]:(0.631399989128) A[1]:(-0.251904904842) A[2]:(0.283161431551) A[3]:(0.888503611088)\n",
      " state (8)  A[0]:(0.655764043331) A[1]:(-0.000192709267139) A[2]:(0.728861689568) A[3]:(0.590745091438)\n",
      " state (9)  A[0]:(0.655887424946) A[1]:(0.809925496578) A[2]:(0.809922873974) A[3]:(0.000544980110135)\n",
      " state (10)  A[0]:(0.728892982006) A[1]:(0.899950802326) A[2]:(-0.000292539596558) A[3]:(0.729178547859)\n",
      " state (11)  A[0]:(0.520576775074) A[1]:(0.876835584641) A[2]:(-0.606536269188) A[3]:(0.843897879124)\n",
      " state (12)  A[0]:(0.0770043954253) A[1]:(0.824536442757) A[2]:(-0.579582571983) A[3]:(0.793470561504)\n",
      " state (13)  A[0]:(-0.000242531299591) A[1]:(0.809409379959) A[2]:(0.899962425232) A[3]:(0.729275345802)\n",
      " state (14)  A[0]:(0.810005962849) A[1]:(0.900161981583) A[2]:(0.999999940395) A[3]:(0.8101760149)\n",
      " state (15)  A[0]:(0.985156953335) A[1]:(0.958009123802) A[2]:(1.0) A[3]:(0.882362246513)\n",
      "Episode 608000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6017. Times reached goal: 995.               Steps done: 4810615. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00732857260263.\n",
      " state (0)  A[0]:(0.531593203545) A[1]:(0.5907356143) A[2]:(0.590673208237) A[3]:(0.531419336796)\n",
      " state (1)  A[0]:(0.5315797925) A[1]:(-0.000145949423313) A[2]:(0.656131386757) A[3]:(0.590564846992)\n",
      " state (2)  A[0]:(0.590661287308) A[1]:(0.729052066803) A[2]:(0.590619802475) A[3]:(0.656219124794)\n",
      " state (3)  A[0]:(0.65636920929) A[1]:(-0.220468044281) A[2]:(0.537095069885) A[3]:(0.519684433937)\n",
      " state (4)  A[0]:(0.590668857098) A[1]:(0.656391382217) A[2]:(-0.000290989875793) A[3]:(0.531651556492)\n",
      " state (5)  A[0]:(0.161798864603) A[1]:(0.9289509058) A[2]:(-0.188344344497) A[3]:(0.520455956459)\n",
      " state (6)  A[0]:(0.000356554955943) A[1]:(0.809963107109) A[2]:(-0.000133395195007) A[3]:(0.656228780746)\n",
      " state (7)  A[0]:(0.632219016552) A[1]:(-0.25196325779) A[2]:(0.283215582371) A[3]:(0.888510286808)\n",
      " state (8)  A[0]:(0.656671047211) A[1]:(-0.000219084322453) A[2]:(0.728793740273) A[3]:(0.590860664845)\n",
      " state (9)  A[0]:(0.656643986702) A[1]:(0.809919416904) A[2]:(0.809781253338) A[3]:(0.000768706027884)\n",
      " state (10)  A[0]:(0.729365944862) A[1]:(0.89996445179) A[2]:(-0.000748038175516) A[3]:(0.729080379009)\n",
      " state (11)  A[0]:(0.521155953407) A[1]:(0.876883506775) A[2]:(-0.606900393963) A[3]:(0.843744874001)\n",
      " state (12)  A[0]:(0.0776095837355) A[1]:(0.824651479721) A[2]:(-0.580079078674) A[3]:(0.793205201626)\n",
      " state (13)  A[0]:(0.000131517648697) A[1]:(0.809581577778) A[2]:(0.899863421917) A[3]:(0.728911101818)\n",
      " state (14)  A[0]:(0.81001573801) A[1]:(0.900273680687) A[2]:(0.999999940395) A[3]:(0.8099655509)\n",
      " state (15)  A[0]:(0.985136568546) A[1]:(0.958056271076) A[2]:(1.0) A[3]:(0.882245481014)\n",
      "Episode 609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6018. Times reached goal: 994.               Steps done: 4816633. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00728460169388.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5906,  0.5906,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.6560,  0.0001,  0.5318]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559, -0.0001,  0.7289,  0.5909]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558,  0.8099,  0.8100,  0.0005]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7287,  0.9000, -0.0001,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9003,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531520605087) A[1]:(0.590589165688) A[2]:(0.590572118759) A[3]:(0.531272888184)\n",
      " state (1)  A[0]:(0.53161239624) A[1]:(3.70368361473e-05) A[2]:(0.656167984009) A[3]:(0.590395689011)\n",
      " state (2)  A[0]:(0.590758800507) A[1]:(0.728950738907) A[2]:(0.590614557266) A[3]:(0.656076550484)\n",
      " state (3)  A[0]:(0.656347870827) A[1]:(-0.220415443182) A[2]:(0.537229418755) A[3]:(0.519661664963)\n",
      " state (4)  A[0]:(0.590763986111) A[1]:(0.655951499939) A[2]:(7.9870223999e-05) A[3]:(0.531599640846)\n",
      " state (5)  A[0]:(0.162173181772) A[1]:(0.928900420666) A[2]:(-0.188153594732) A[3]:(0.520425379276)\n",
      " state (6)  A[0]:(0.000306397676468) A[1]:(0.81001663208) A[2]:(-3.54051589966e-05) A[3]:(0.656141757965)\n",
      " state (7)  A[0]:(0.631700158119) A[1]:(-0.25164988637) A[2]:(0.283367007971) A[3]:(0.888476014137)\n",
      " state (8)  A[0]:(0.656042575836) A[1]:(-0.000157959759235) A[2]:(0.728870987892) A[3]:(0.590947389603)\n",
      " state (9)  A[0]:(0.655845999718) A[1]:(0.809961080551) A[2]:(0.80999648571) A[3]:(0.000565662921872)\n",
      " state (10)  A[0]:(0.728778243065) A[1]:(0.899993598461) A[2]:(-0.000130653381348) A[3]:(0.729145050049)\n",
      " state (11)  A[0]:(0.520445346832) A[1]:(0.876912355423) A[2]:(-0.6066583395) A[3]:(0.843898892403)\n",
      " state (12)  A[0]:(0.0768501907587) A[1]:(0.824677705765) A[2]:(-0.579926848412) A[3]:(0.793471038342)\n",
      " state (13)  A[0]:(-0.000496357621159) A[1]:(0.809596180916) A[2]:(0.899983763695) A[3]:(0.729273438454)\n",
      " state (14)  A[0]:(0.809806883335) A[1]:(0.900275528431) A[2]:(0.999999940395) A[3]:(0.81021720171)\n",
      " state (15)  A[0]:(0.985103428364) A[1]:(0.958042860031) A[2]:(1.0) A[3]:(0.882354259491)\n",
      "Episode 610000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6004. Times reached goal: 987.               Steps done: 4822637. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00724099598065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531379759312) A[1]:(0.590527892113) A[2]:(0.590518116951) A[3]:(0.531464219093)\n",
      " state (1)  A[0]:(0.531351208687) A[1]:(-2.77161598206e-05) A[2]:(0.656143128872) A[3]:(0.590443611145)\n",
      " state (2)  A[0]:(0.590521633625) A[1]:(0.729046523571) A[2]:(0.590598940849) A[3]:(0.656099677086)\n",
      " state (3)  A[0]:(0.656155109406) A[1]:(-0.219882711768) A[2]:(0.537197113037) A[3]:(0.519552230835)\n",
      " state (4)  A[0]:(0.590544581413) A[1]:(0.65610563755) A[2]:(0.000117897987366) A[3]:(0.53144967556)\n",
      " state (5)  A[0]:(0.161873430014) A[1]:(0.928922176361) A[2]:(-0.188096553087) A[3]:(0.520308434963)\n",
      " state (6)  A[0]:(0.000197052955627) A[1]:(0.810017228127) A[2]:(0.00010621547699) A[3]:(0.656040072441)\n",
      " state (7)  A[0]:(0.631770849228) A[1]:(-0.251633703709) A[2]:(0.283680856228) A[3]:(0.888381838799)\n",
      " state (8)  A[0]:(0.656129419804) A[1]:(0.000108897686005) A[2]:(0.7291046381) A[3]:(0.590398550034)\n",
      " state (9)  A[0]:(0.656063556671) A[1]:(0.810040593147) A[2]:(0.810068666935) A[3]:(5.87850809097e-05)\n",
      " state (10)  A[0]:(0.728970885277) A[1]:(0.900014519691) A[2]:(-6.69956207275e-05) A[3]:(0.728990316391)\n",
      " state (11)  A[0]:(0.520691514015) A[1]:(0.876933515072) A[2]:(-0.606682777405) A[3]:(0.843782186508)\n",
      " state (12)  A[0]:(0.0771285817027) A[1]:(0.824719071388) A[2]:(-0.580009818077) A[3]:(0.793286681175)\n",
      " state (13)  A[0]:(-0.000183045864105) A[1]:(0.809667825699) A[2]:(0.900051653385) A[3]:(0.729002952576)\n",
      " state (14)  A[0]:(0.809971988201) A[1]:(0.90033352375) A[2]:(0.999999940395) A[3]:(0.809985756874)\n",
      " state (15)  A[0]:(0.985108077526) A[1]:(0.95806491375) A[2]:(1.0) A[3]:(0.882149338722)\n",
      "Episode 611000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 992.               Steps done: 4828645. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0071976225012.\n",
      " state (0)  A[0]:(0.531060039997) A[1]:(0.590346574783) A[2]:(0.590471982956) A[3]:(0.531337618828)\n",
      " state (1)  A[0]:(0.531069993973) A[1]:(0.00010808557272) A[2]:(0.656098008156) A[3]:(0.590114474297)\n",
      " state (2)  A[0]:(0.590203821659) A[1]:(0.728914499283) A[2]:(0.590364694595) A[3]:(0.655787825584)\n",
      " state (3)  A[0]:(0.655798077583) A[1]:(-0.219677954912) A[2]:(0.536911547184) A[3]:(0.519101321697)\n",
      " state (4)  A[0]:(0.590041637421) A[1]:(0.656012296677) A[2]:(-0.00025475025177) A[3]:(0.53102517128)\n",
      " state (5)  A[0]:(0.161103010178) A[1]:(0.928913116455) A[2]:(-0.18851095438) A[3]:(0.519939482212)\n",
      " state (6)  A[0]:(-0.000327914953232) A[1]:(0.809965670109) A[2]:(-0.000190854072571) A[3]:(0.655592918396)\n",
      " state (7)  A[0]:(0.631467759609) A[1]:(-0.251670509577) A[2]:(0.283521145582) A[3]:(0.888035535812)\n",
      " state (8)  A[0]:(0.65569716692) A[1]:(-9.06363129616e-05) A[2]:(0.728768408298) A[3]:(0.58953166008)\n",
      " state (9)  A[0]:(0.655392408371) A[1]:(0.809964478016) A[2]:(0.809935569763) A[3]:(-0.00173169199843)\n",
      " state (10)  A[0]:(0.728471457958) A[1]:(0.900026082993) A[2]:(-0.000355958909495) A[3]:(0.72811961174)\n",
      " state (11)  A[0]:(0.520090341568) A[1]:(0.876975595951) A[2]:(-0.606986999512) A[3]:(0.843307137489)\n",
      " state (12)  A[0]:(0.0764440596104) A[1]:(0.824775218964) A[2]:(-0.580383360386) A[3]:(0.792705535889)\n",
      " state (13)  A[0]:(-0.000843107525725) A[1]:(0.809695124626) A[2]:(0.900049746037) A[3]:(0.72829914093)\n",
      " state (14)  A[0]:(0.809673666954) A[1]:(0.900309145451) A[2]:(0.999999940395) A[3]:(0.809515595436)\n",
      " state (15)  A[0]:(0.985059857368) A[1]:(0.958021044731) A[2]:(1.0) A[3]:(0.881844699383)\n",
      "Episode 612000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6020. Times reached goal: 992.               Steps done: 4834665. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00715442297478.\n",
      " state (0)  A[0]:(0.530750393867) A[1]:(0.590592384338) A[2]:(0.590635895729) A[3]:(0.531684875488)\n",
      " state (1)  A[0]:(0.530478417873) A[1]:(-5.01051545143e-05) A[2]:(0.656186342239) A[3]:(0.590385377407)\n",
      " state (2)  A[0]:(0.589648962021) A[1]:(0.729046523571) A[2]:(0.590539097786) A[3]:(0.655994772911)\n",
      " state (3)  A[0]:(0.655550003052) A[1]:(-0.219531163573) A[2]:(0.537132143974) A[3]:(0.519157111645)\n",
      " state (4)  A[0]:(0.589715361595) A[1]:(0.656368255615) A[2]:(7.80820846558e-05) A[3]:(0.531027555466)\n",
      " state (5)  A[0]:(0.160343587399) A[1]:(0.928927063942) A[2]:(-0.188018247485) A[3]:(0.519940197468)\n",
      " state (6)  A[0]:(-0.0012056225678) A[1]:(0.80999314785) A[2]:(0.000303387641907) A[3]:(0.655760407448)\n",
      " state (7)  A[0]:(0.631387352943) A[1]:(-0.251590371132) A[2]:(0.284167826176) A[3]:(0.888231754303)\n",
      " state (8)  A[0]:(0.656266093254) A[1]:(0.000154070556164) A[2]:(0.72942340374) A[3]:(0.589622437954)\n",
      " state (9)  A[0]:(0.656710028648) A[1]:(0.809998512268) A[2]:(0.810153126717) A[3]:(-0.00100314582232)\n",
      " state (10)  A[0]:(0.729735612869) A[1]:(0.899988770485) A[2]:(-0.000231862068176) A[3]:(0.728629171848)\n",
      " state (11)  A[0]:(0.522055268288) A[1]:(0.876908659935) A[2]:(-0.607001185417) A[3]:(0.843600034714)\n",
      " state (12)  A[0]:(0.0791455805302) A[1]:(0.824685037136) A[2]:(-0.580399513245) A[3]:(0.793088912964)\n",
      " state (13)  A[0]:(0.00218018540181) A[1]:(0.809634447098) A[2]:(0.900261461735) A[3]:(0.728846788406)\n",
      " state (14)  A[0]:(0.810972988605) A[1]:(0.900312364101) A[2]:(0.999999940395) A[3]:(0.80999994278)\n",
      " state (15)  A[0]:(0.985166013241) A[1]:(0.958016455173) A[2]:(1.0) A[3]:(0.882139086723)\n",
      "Episode 613000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 994.               Steps done: 4840668. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00711160362453.\n",
      " state (0)  A[0]:(0.531515240669) A[1]:(0.590803027153) A[2]:(0.590534925461) A[3]:(0.531416654587)\n",
      " state (1)  A[0]:(0.531453251839) A[1]:(0.000265471637249) A[2]:(0.656212747097) A[3]:(0.590270340443)\n",
      " state (2)  A[0]:(0.590650200844) A[1]:(0.729274988174) A[2]:(0.590541124344) A[3]:(0.655997991562)\n",
      " state (3)  A[0]:(0.65631043911) A[1]:(-0.218430519104) A[2]:(0.536996245384) A[3]:(0.519629061222)\n",
      " state (4)  A[0]:(0.590652227402) A[1]:(0.656574487686) A[2]:(-7.4028968811e-05) A[3]:(0.531610369682)\n",
      " state (5)  A[0]:(0.161859437823) A[1]:(0.928977966309) A[2]:(-0.188282266259) A[3]:(0.520582199097)\n",
      " state (6)  A[0]:(-4.57167625427e-05) A[1]:(0.810181260109) A[2]:(-3.55243682861e-05) A[3]:(0.656225442886)\n",
      " state (7)  A[0]:(0.631781041622) A[1]:(-0.251172870398) A[2]:(0.283955872059) A[3]:(0.888428211212)\n",
      " state (8)  A[0]:(0.656446456909) A[1]:(0.000449709565146) A[2]:(0.729309201241) A[3]:(0.590546607971)\n",
      " state (9)  A[0]:(0.656471908092) A[1]:(0.81011068821) A[2]:(0.810163497925) A[3]:(0.000446915597422)\n",
      " state (10)  A[0]:(0.729258239269) A[1]:(0.900015771389) A[2]:(0.000143766403198) A[3]:(0.729197323322)\n",
      " state (11)  A[0]:(0.521092534065) A[1]:(0.876896739006) A[2]:(-0.606768846512) A[3]:(0.843911767006)\n",
      " state (12)  A[0]:(0.0776067748666) A[1]:(0.824611067772) A[2]:(-0.580457091331) A[3]:(0.793465793133)\n",
      " state (13)  A[0]:(0.000168025493622) A[1]:(0.809471130371) A[2]:(0.899995446205) A[3]:(0.72924888134)\n",
      " state (14)  A[0]:(0.81006860733) A[1]:(0.900169670582) A[2]:(0.999999940395) A[3]:(0.810196995735)\n",
      " state (15)  A[0]:(0.985088050365) A[1]:(0.957947313786) A[2]:(1.0) A[3]:(0.882244884968)\n",
      "Episode 614000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6005. Times reached goal: 992.               Steps done: 4846673. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00706902641079.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5903,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5318, -0.0000,  0.6561,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.7290,  0.5905,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8100,  0.0002,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0001,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9003,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531641125679) A[1]:(0.590317904949) A[2]:(0.590510487556) A[3]:(0.531353056431)\n",
      " state (1)  A[0]:(0.531667709351) A[1]:(-2.76640057564e-05) A[2]:(0.656114935875) A[3]:(0.590393841267)\n",
      " state (2)  A[0]:(0.590758442879) A[1]:(0.729014873505) A[2]:(0.590485095978) A[3]:(0.656104922295)\n",
      " state (3)  A[0]:(0.656175374985) A[1]:(-0.219461560249) A[2]:(0.537061572075) A[3]:(0.519524514675)\n",
      " state (4)  A[0]:(0.590379476547) A[1]:(0.656089663506) A[2]:(-2.86102294922e-05) A[3]:(0.531446874142)\n",
      " state (5)  A[0]:(0.161481559277) A[1]:(0.928911089897) A[2]:(-0.188325494528) A[3]:(0.520424842834)\n",
      " state (6)  A[0]:(-0.000226736068726) A[1]:(0.80997300148) A[2]:(2.09808349609e-05) A[3]:(0.655997693539)\n",
      " state (7)  A[0]:(0.631482243538) A[1]:(-0.251692146063) A[2]:(0.28398501873) A[3]:(0.888217687607)\n",
      " state (8)  A[0]:(0.656113266945) A[1]:(-4.03597950935e-05) A[2]:(0.728953957558) A[3]:(0.590358972549)\n",
      " state (9)  A[0]:(0.656205415726) A[1]:(0.809955954552) A[2]:(0.809934675694) A[3]:(0.000196069478989)\n",
      " state (10)  A[0]:(0.729139208794) A[1]:(0.899969696999) A[2]:(-0.000427961320383) A[3]:(0.729036569595)\n",
      " state (11)  A[0]:(0.521025419235) A[1]:(0.876880705357) A[2]:(-0.607232093811) A[3]:(0.843823671341)\n",
      " state (12)  A[0]:(0.0775608569384) A[1]:(0.824642419815) A[2]:(-0.580992043018) A[3]:(0.793325066566)\n",
      " state (13)  A[0]:(0.000119626522064) A[1]:(0.809573948383) A[2]:(0.899963974953) A[3]:(0.729005098343)\n",
      " state (14)  A[0]:(0.810018062592) A[1]:(0.900273680687) A[2]:(0.999999940395) A[3]:(0.809946775436)\n",
      " state (15)  A[0]:(0.985061764717) A[1]:(0.957998991013) A[2]:(1.0) A[3]:(0.881990909576)\n",
      "Episode 615000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6011. Times reached goal: 994.               Steps done: 4852684. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00702666194699.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531271338463) A[1]:(0.590609312057) A[2]:(0.590521454811) A[3]:(0.53149920702)\n",
      " state (1)  A[0]:(0.53139513731) A[1]:(5.88074326515e-05) A[2]:(0.656126320362) A[3]:(0.590523004532)\n",
      " state (2)  A[0]:(0.590616762638) A[1]:(0.729006588459) A[2]:(0.590527534485) A[3]:(0.65623998642)\n",
      " state (3)  A[0]:(0.656354248524) A[1]:(-0.219515115023) A[2]:(0.537032663822) A[3]:(0.519629836082)\n",
      " state (4)  A[0]:(0.590845346451) A[1]:(0.656048297882) A[2]:(-0.000248432159424) A[3]:(0.531624495983)\n",
      " state (5)  A[0]:(0.162404939532) A[1]:(0.928913414478) A[2]:(-0.188692942262) A[3]:(0.520765781403)\n",
      " state (6)  A[0]:(0.000830828968901) A[1]:(0.809965729713) A[2]:(-0.000325083732605) A[3]:(0.656369328499)\n",
      " state (7)  A[0]:(0.632069349289) A[1]:(-0.251718848944) A[2]:(0.283780843019) A[3]:(0.888388931751)\n",
      " state (8)  A[0]:(0.656793534756) A[1]:(-4.97624278069e-05) A[2]:(0.728687822819) A[3]:(0.591199994087)\n",
      " state (9)  A[0]:(0.656945347786) A[1]:(0.809953212738) A[2]:(0.809793591499) A[3]:(0.00121031643357)\n",
      " state (10)  A[0]:(0.729658484459) A[1]:(0.899952113628) A[2]:(-0.000524878443684) A[3]:(0.729315161705)\n",
      " state (11)  A[0]:(0.521739780903) A[1]:(0.876832485199) A[2]:(-0.607280731201) A[3]:(0.843970119953)\n",
      " state (12)  A[0]:(0.0784540474415) A[1]:(0.824537038803) A[2]:(-0.581127762794) A[3]:(0.793529033661)\n",
      " state (13)  A[0]:(0.000950336165261) A[1]:(0.809412777424) A[2]:(0.899955451488) A[3]:(0.729291200638)\n",
      " state (14)  A[0]:(0.810302197933) A[1]:(0.900155603886) A[2]:(0.999999940395) A[3]:(0.810170948505)\n",
      " state (15)  A[0]:(0.985077619553) A[1]:(0.957927286625) A[2]:(1.0) A[3]:(0.882123053074)\n",
      "Episode 616000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6011. Times reached goal: 994.               Steps done: 4858695. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00698455137216.\n",
      " state (0)  A[0]:(0.531782448292) A[1]:(0.590165376663) A[2]:(0.590512394905) A[3]:(0.532023429871)\n",
      " state (1)  A[0]:(0.531145215034) A[1]:(-0.000264182686806) A[2]:(0.656021654606) A[3]:(0.590924143791)\n",
      " state (2)  A[0]:(0.589569509029) A[1]:(0.728836894035) A[2]:(0.590204000473) A[3]:(0.656481921673)\n",
      " state (3)  A[0]:(0.654111981392) A[1]:(-0.220797389746) A[2]:(0.536605358124) A[3]:(0.519877195358)\n",
      " state (4)  A[0]:(0.586433649063) A[1]:(0.655788362026) A[2]:(-0.000551938952412) A[3]:(0.53167951107)\n",
      " state (5)  A[0]:(0.153258636594) A[1]:(0.928704738617) A[2]:(-0.188172444701) A[3]:(0.520270586014)\n",
      " state (6)  A[0]:(-0.0115351928398) A[1]:(0.809923052788) A[2]:(9.69171524048e-05) A[3]:(0.655256569386)\n",
      " state (7)  A[0]:(0.624458909035) A[1]:(-0.251533776522) A[2]:(0.284004539251) A[3]:(0.887928724289)\n",
      " state (8)  A[0]:(0.651497244835) A[1]:(-0.001183732762) A[2]:(0.728574097157) A[3]:(0.590359568596)\n",
      " state (9)  A[0]:(0.652962863445) A[1]:(0.80971288681) A[2]:(0.809660971165) A[3]:(-0.000870674615726)\n",
      " state (10)  A[0]:(0.727141022682) A[1]:(0.899947166443) A[2]:(-0.00123381556477) A[3]:(0.728366196156)\n",
      " state (11)  A[0]:(0.518598079681) A[1]:(0.876923263073) A[2]:(-0.60802936554) A[3]:(0.843484401703)\n",
      " state (12)  A[0]:(0.074778303504) A[1]:(0.824741542339) A[2]:(-0.582219719887) A[3]:(0.79295027256)\n",
      " state (13)  A[0]:(-0.00247668707743) A[1]:(0.809673130512) A[2]:(0.899538576603) A[3]:(0.72852408886)\n",
      " state (14)  A[0]:(0.809179484844) A[1]:(0.90030580759) A[2]:(0.999999940395) A[3]:(0.809526324272)\n",
      " state (15)  A[0]:(0.984985888004) A[1]:(0.957995712757) A[2]:(1.0) A[3]:(0.881651103497)\n",
      "Episode 617000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6001. Times reached goal: 990.               Steps done: 4864696. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00694276259201.\n",
      " state (0)  A[0]:(0.53097730875) A[1]:(0.590469479561) A[2]:(0.590514063835) A[3]:(0.531467795372)\n",
      " state (1)  A[0]:(0.531042933464) A[1]:(-2.58088111877e-05) A[2]:(0.656102061272) A[3]:(0.590446829796)\n",
      " state (2)  A[0]:(0.590261816978) A[1]:(0.729012072086) A[2]:(0.590693831444) A[3]:(0.656072437763)\n",
      " state (3)  A[0]:(0.656019926071) A[1]:(-0.220317780972) A[2]:(0.537330508232) A[3]:(0.519336938858)\n",
      " state (4)  A[0]:(0.590448558331) A[1]:(0.656071424484) A[2]:(-1.6450881958e-05) A[3]:(0.531343221664)\n",
      " state (5)  A[0]:(0.161805316806) A[1]:(0.92891484499) A[2]:(-0.18846783042) A[3]:(0.520413815975)\n",
      " state (6)  A[0]:(0.000279635190964) A[1]:(0.810017585754) A[2]:(-1.46627426147e-05) A[3]:(0.656011343002)\n",
      " state (7)  A[0]:(0.631971478462) A[1]:(-0.25159060955) A[2]:(0.284392654896) A[3]:(0.888227403164)\n",
      " state (8)  A[0]:(0.656869590282) A[1]:(-6.90221786499e-05) A[2]:(0.729009747505) A[3]:(0.590575218201)\n",
      " state (9)  A[0]:(0.657019734383) A[1]:(0.809985280037) A[2]:(0.810007214546) A[3]:(-3.70740890503e-05)\n",
      " state (10)  A[0]:(0.729924201965) A[1]:(0.900000631809) A[2]:(-0.000196695327759) A[3]:(0.728953719139)\n",
      " state (11)  A[0]:(0.522494852543) A[1]:(0.876918375492) A[2]:(-0.607327461243) A[3]:(0.843867361546)\n",
      " state (12)  A[0]:(0.0797893926501) A[1]:(0.824683129787) A[2]:(-0.581349134445) A[3]:(0.79345947504)\n",
      " state (13)  A[0]:(0.00251218141057) A[1]:(0.80960470438) A[2]:(0.900062799454) A[3]:(0.729223847389)\n",
      " state (14)  A[0]:(0.810901224613) A[1]:(0.900289773941) A[2]:(0.999999940395) A[3]:(0.810117602348)\n",
      " state (15)  A[0]:(0.985102057457) A[1]:(0.957973480225) A[2]:(1.0) A[3]:(0.88199955225)\n",
      "Episode 618000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6020. Times reached goal: 993.               Steps done: 4870716. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00690109271339.\n",
      " state (0)  A[0]:(0.530902445316) A[1]:(0.590543985367) A[2]:(0.590384721756) A[3]:(0.531154751778)\n",
      " state (1)  A[0]:(0.530933082104) A[1]:(4.5970082283e-05) A[2]:(0.65588414669) A[3]:(0.590232372284)\n",
      " state (2)  A[0]:(0.590151369572) A[1]:(0.728922009468) A[2]:(0.590207338333) A[3]:(0.655924797058)\n",
      " state (3)  A[0]:(0.655708432198) A[1]:(-0.219715476036) A[2]:(0.536969423294) A[3]:(0.519324541092)\n",
      " state (4)  A[0]:(0.590066432953) A[1]:(0.656169176102) A[2]:(-0.000213503837585) A[3]:(0.531246423721)\n",
      " state (5)  A[0]:(0.161371812224) A[1]:(0.928933799267) A[2]:(-0.188560038805) A[3]:(0.52017223835)\n",
      " state (6)  A[0]:(-0.000141590833664) A[1]:(0.809985280037) A[2]:(8.42809677124e-05) A[3]:(0.655649960041)\n",
      " state (7)  A[0]:(0.631331086159) A[1]:(-0.251742333174) A[2]:(0.284593164921) A[3]:(0.887997806072)\n",
      " state (8)  A[0]:(0.655988097191) A[1]:(-0.000209793448448) A[2]:(0.728898227215) A[3]:(0.59028750658)\n",
      " state (9)  A[0]:(0.655919849873) A[1]:(0.809936642647) A[2]:(0.809951901436) A[3]:(-4.07695770264e-05)\n",
      " state (10)  A[0]:(0.728850960732) A[1]:(0.899985790253) A[2]:(-0.000172972679138) A[3]:(0.728993415833)\n",
      " state (11)  A[0]:(0.520625829697) A[1]:(0.876901686192) A[2]:(-0.60734641552) A[3]:(0.843889474869)\n",
      " state (12)  A[0]:(0.0769935771823) A[1]:(0.82464993) A[2]:(-0.581482231617) A[3]:(0.793455541134)\n",
      " state (13)  A[0]:(-0.000483661860926) A[1]:(0.80954605341) A[2]:(0.900014519691) A[3]:(0.729177355766)\n",
      " state (14)  A[0]:(0.809894919395) A[1]:(0.90024125576) A[2]:(0.999999940395) A[3]:(0.810073792934)\n",
      " state (15)  A[0]:(0.985014736652) A[1]:(0.957941353321) A[2]:(1.0) A[3]:(0.881964147091)\n",
      "Episode 619000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6016. Times reached goal: 991.               Steps done: 4876732. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00685970037263.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5905,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6560,  0.0001,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.0001,  0.7290,  0.5903]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560,  0.8100,  0.8099,  0.0001]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8095,  0.9000,  0.7290]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9001,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531500816345) A[1]:(0.590505719185) A[2]:(0.590452373028) A[3]:(0.53149741888)\n",
      " state (1)  A[0]:(0.531398057938) A[1]:(-0.000208109617233) A[2]:(0.65602850914) A[3]:(0.590445399284)\n",
      " state (2)  A[0]:(0.590470433235) A[1]:(0.729007840157) A[2]:(0.590583443642) A[3]:(0.656114339828)\n",
      " state (3)  A[0]:(0.656024158001) A[1]:(-0.218887031078) A[2]:(0.537186264992) A[3]:(0.519451081753)\n",
      " state (4)  A[0]:(0.590328097343) A[1]:(0.656010389328) A[2]:(8.54730606079e-05) A[3]:(0.531369745731)\n",
      " state (5)  A[0]:(0.16158105433) A[1]:(0.92891150713) A[2]:(-0.188508421183) A[3]:(0.520495891571)\n",
      " state (6)  A[0]:(-5.47766685486e-05) A[1]:(0.809986948967) A[2]:(-3.46899032593e-05) A[3]:(0.6559497118)\n",
      " state (7)  A[0]:(0.631396651268) A[1]:(-0.251610606909) A[2]:(0.284515172243) A[3]:(0.888060450554)\n",
      " state (8)  A[0]:(0.656034111977) A[1]:(-3.86536121368e-05) A[2]:(0.72899723053) A[3]:(0.590203404427)\n",
      " state (9)  A[0]:(0.655994355679) A[1]:(0.809969067574) A[2]:(0.810013413429) A[3]:(-0.000149250030518)\n",
      " state (10)  A[0]:(0.728981137276) A[1]:(0.899988532066) A[2]:(-0.000210762023926) A[3]:(0.728953480721)\n",
      " state (11)  A[0]:(0.520917534828) A[1]:(0.876883029938) A[2]:(-0.607503056526) A[3]:(0.843842566013)\n",
      " state (12)  A[0]:(0.0774773210287) A[1]:(0.824582517147) A[2]:(-0.581743001938) A[3]:(0.793364048004)\n",
      " state (13)  A[0]:(3.09646129608e-05) A[1]:(0.809423804283) A[2]:(0.900025725365) A[3]:(0.729035377502)\n",
      " state (14)  A[0]:(0.810015618801) A[1]:(0.900146842003) A[2]:(0.999999940395) A[3]:(0.809959709644)\n",
      " state (15)  A[0]:(0.985004901886) A[1]:(0.957875370979) A[2]:(1.0) A[3]:(0.881855547428)\n",
      "Episode 620000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6012. Times reached goal: 990.               Steps done: 4882744. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00681858357493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531503915787) A[1]:(0.590449929237) A[2]:(0.590479135513) A[3]:(0.531284809113)\n",
      " state (1)  A[0]:(0.531446099281) A[1]:(-1.53779983521e-05) A[2]:(0.656056880951) A[3]:(0.590240478516)\n",
      " state (2)  A[0]:(0.590586662292) A[1]:(0.728996753693) A[2]:(0.590249061584) A[3]:(0.656011581421)\n",
      " state (3)  A[0]:(0.656100153923) A[1]:(-0.217632055283) A[2]:(0.536818802357) A[3]:(0.519470453262)\n",
      " state (4)  A[0]:(0.59036564827) A[1]:(0.656047224998) A[2]:(2.08616256714e-05) A[3]:(0.531306624413)\n",
      " state (5)  A[0]:(0.161633819342) A[1]:(0.928897380829) A[2]:(-0.188440814614) A[3]:(0.520446777344)\n",
      " state (6)  A[0]:(-0.000107139348984) A[1]:(0.810007691383) A[2]:(0.000107645988464) A[3]:(0.655895590782)\n",
      " state (7)  A[0]:(0.631398916245) A[1]:(-0.251516550779) A[2]:(0.284744113684) A[3]:(0.888065516949)\n",
      " state (8)  A[0]:(0.656152963638) A[1]:(-0.000161319971085) A[2]:(0.728981077671) A[3]:(0.590529203415)\n",
      " state (9)  A[0]:(0.656018733978) A[1]:(0.80994695425) A[2]:(0.810031473637) A[3]:(9.27597284317e-05)\n",
      " state (10)  A[0]:(0.72896271944) A[1]:(0.899999141693) A[2]:(-0.000154137611389) A[3]:(0.729017972946)\n",
      " state (11)  A[0]:(0.520893335342) A[1]:(0.876916646957) A[2]:(-0.60758793354) A[3]:(0.843900442123)\n",
      " state (12)  A[0]:(0.0774361416698) A[1]:(0.824657738209) A[2]:(-0.581952989101) A[3]:(0.793445587158)\n",
      " state (13)  A[0]:(2.27093696594e-05) A[1]:(0.809544324875) A[2]:(0.900020837784) A[3]:(0.729140877724)\n",
      " state (14)  A[0]:(0.810084462166) A[1]:(0.900246143341) A[2]:(0.999999940395) A[3]:(0.810055375099)\n",
      " state (15)  A[0]:(0.985004603863) A[1]:(0.957923769951) A[2]:(1.0) A[3]:(0.881897926331)\n",
      "Episode 621000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6030. Times reached goal: 995.               Steps done: 4888774. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00677759123209.\n",
      " state (0)  A[0]:(0.531524121761) A[1]:(0.59052246809) A[2]:(0.590530157089) A[3]:(0.531513571739)\n",
      " state (1)  A[0]:(0.531429052353) A[1]:(-0.000189021229744) A[2]:(0.656106591225) A[3]:(0.590577363968)\n",
      " state (2)  A[0]:(0.590500175953) A[1]:(0.729023814201) A[2]:(0.590660572052) A[3]:(0.656269669533)\n",
      " state (3)  A[0]:(0.656106829643) A[1]:(-0.21881274879) A[2]:(0.537243366241) A[3]:(0.519749164581)\n",
      " state (4)  A[0]:(0.590413570404) A[1]:(0.656167089939) A[2]:(0.00010359287262) A[3]:(0.531702160835)\n",
      " state (5)  A[0]:(0.161670297384) A[1]:(0.928934633732) A[2]:(-0.188510611653) A[3]:(0.520833313465)\n",
      " state (6)  A[0]:(-5.86807727814e-05) A[1]:(0.809987485409) A[2]:(-3.09944152832e-06) A[3]:(0.65612590313)\n",
      " state (7)  A[0]:(0.631138324738) A[1]:(-0.251545608044) A[2]:(0.284645646811) A[3]:(0.888028800488)\n",
      " state (8)  A[0]:(0.655847370625) A[1]:(0.000342860817909) A[2]:(0.729029893875) A[3]:(0.590139269829)\n",
      " state (9)  A[0]:(0.655980467796) A[1]:(0.810067713261) A[2]:(0.810001552105) A[3]:(0.000189378857613)\n",
      " state (10)  A[0]:(0.728908538818) A[1]:(0.899992585182) A[2]:(-4.8041343689e-05) A[3]:(0.728996872902)\n",
      " state (11)  A[0]:(0.520697116852) A[1]:(0.876859068871) A[2]:(-0.607452988625) A[3]:(0.843793511391)\n",
      " state (12)  A[0]:(0.0770732164383) A[1]:(0.824536800385) A[2]:(-0.581916809082) A[3]:(0.79326581955)\n",
      " state (13)  A[0]:(-0.000413000554545) A[1]:(0.809378683567) A[2]:(0.900019943714) A[3]:(0.728920400143)\n",
      " state (14)  A[0]:(0.809963524342) A[1]:(0.900143623352) A[2]:(0.999999940395) A[3]:(0.809958457947)\n",
      " state (15)  A[0]:(0.984991490841) A[1]:(0.957870960236) A[2]:(1.0) A[3]:(0.881867825985)\n",
      "Episode 622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6021. Times reached goal: 993.               Steps done: 4894795. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0067369059612.\n",
      " state (0)  A[0]:(0.531328082085) A[1]:(0.590184569359) A[2]:(0.59038567543) A[3]:(0.531533539295)\n",
      " state (1)  A[0]:(0.531414031982) A[1]:(-3.15010547638e-05) A[2]:(0.655975461006) A[3]:(0.590447306633)\n",
      " state (2)  A[0]:(0.590561330318) A[1]:(0.728929877281) A[2]:(0.59037142992) A[3]:(0.656112074852)\n",
      " state (3)  A[0]:(0.656039297581) A[1]:(-0.218582183123) A[2]:(0.537087082863) A[3]:(0.519518971443)\n",
      " state (4)  A[0]:(0.590411186218) A[1]:(0.655921757221) A[2]:(3.48091125488e-05) A[3]:(0.531402349472)\n",
      " state (5)  A[0]:(0.161928743124) A[1]:(0.928911805153) A[2]:(-0.188733637333) A[3]:(0.520586967468)\n",
      " state (6)  A[0]:(0.000393629044993) A[1]:(0.809992551804) A[2]:(-0.000211000442505) A[3]:(0.656015515327)\n",
      " state (7)  A[0]:(0.631454229355) A[1]:(-0.251449853182) A[2]:(0.284581452608) A[3]:(0.888045251369)\n",
      " state (8)  A[0]:(0.656107068062) A[1]:(0.000266194343567) A[2]:(0.728817641735) A[3]:(0.590371847153)\n",
      " state (9)  A[0]:(0.65603095293) A[1]:(0.810064136982) A[2]:(0.809960365295) A[3]:(-0.000367313594325)\n",
      " state (10)  A[0]:(0.728936076164) A[1]:(0.899994134903) A[2]:(-6.60419464111e-05) A[3]:(0.728676319122)\n",
      " state (11)  A[0]:(0.520796775818) A[1]:(0.876823544502) A[2]:(-0.607579827309) A[3]:(0.843655467033)\n",
      " state (12)  A[0]:(0.0772435888648) A[1]:(0.824409008026) A[2]:(-0.582152247429) A[3]:(0.793126642704)\n",
      " state (13)  A[0]:(-0.000196874141693) A[1]:(0.809142827988) A[2]:(0.900009214878) A[3]:(0.72876727581)\n",
      " state (14)  A[0]:(0.810082733631) A[1]:(0.899956941605) A[2]:(0.999999940395) A[3]:(0.809853434563)\n",
      " state (15)  A[0]:(0.984994769096) A[1]:(0.957754254341) A[2]:(1.0) A[3]:(0.881779491901)\n",
      "Episode 623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6023. Times reached goal: 996.               Steps done: 4900818. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00669645152742.\n",
      " state (0)  A[0]:(0.532283782959) A[1]:(0.590728342533) A[2]:(0.590689837933) A[3]:(0.531501471996)\n",
      " state (1)  A[0]:(0.532193303108) A[1]:(5.97313046455e-05) A[2]:(0.65634059906) A[3]:(0.590653181076)\n",
      " state (2)  A[0]:(0.591129183769) A[1]:(0.729074239731) A[2]:(0.590830206871) A[3]:(0.656396269798)\n",
      " state (3)  A[0]:(0.656480908394) A[1]:(-0.218381732702) A[2]:(0.537279129028) A[3]:(0.519712984562)\n",
      " state (4)  A[0]:(0.590638995171) A[1]:(0.656247377396) A[2]:(0.000116586685181) A[3]:(0.531668066978)\n",
      " state (5)  A[0]:(0.161805570126) A[1]:(0.928924798965) A[2]:(-0.188479557633) A[3]:(0.520974516869)\n",
      " state (6)  A[0]:(-2.23517417908e-05) A[1]:(0.80998146534) A[2]:(0.000186681747437) A[3]:(0.656419873238)\n",
      " state (7)  A[0]:(0.631263613701) A[1]:(-0.251470685005) A[2]:(0.285155504942) A[3]:(0.888237059116)\n",
      " state (8)  A[0]:(0.656243681908) A[1]:(0.000201620161533) A[2]:(0.729141056538) A[3]:(0.590954184532)\n",
      " state (9)  A[0]:(0.656541347504) A[1]:(0.81002420187) A[2]:(0.810096263885) A[3]:(0.00076790136518)\n",
      " state (10)  A[0]:(0.729488015175) A[1]:(0.899980962276) A[2]:(0.000142931938171) A[3]:(0.729343950748)\n",
      " state (11)  A[0]:(0.521700441837) A[1]:(0.876827836037) A[2]:(-0.607545018196) A[3]:(0.844104290009)\n",
      " state (12)  A[0]:(0.0784097984433) A[1]:(0.824439704418) A[2]:(-0.582223057747) A[3]:(0.793721616268)\n",
      " state (13)  A[0]:(0.000758617941756) A[1]:(0.809195518494) A[2]:(0.900029599667) A[3]:(0.729497671127)\n",
      " state (14)  A[0]:(0.810262560844) A[1]:(0.899993538857) A[2]:(0.999999940395) A[3]:(0.81032627821)\n",
      " state (15)  A[0]:(0.984985589981) A[1]:(0.95776450634) A[2]:(1.0) A[3]:(0.882010221481)\n",
      "Episode 624000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6021. Times reached goal: 992.               Steps done: 4906839. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00665625333089.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5904,  0.5904,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.6561,  0.0001,  0.5313]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 6.5610e-01, -8.2105e-06,  7.2886e-01,  5.9065e-01]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.8100,  0.8099,  0.0002]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0000,  0.8095,  0.9000,  0.7290]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531411886215) A[1]:(0.590461015701) A[2]:(0.590394318104) A[3]:(0.531421542168)\n",
      " state (1)  A[0]:(0.531359910965) A[1]:(0.000165335834026) A[2]:(0.656076073647) A[3]:(0.590424537659)\n",
      " state (2)  A[0]:(0.590460181236) A[1]:(0.728978633881) A[2]:(0.590692043304) A[3]:(0.656073868275)\n",
      " state (3)  A[0]:(0.656067788601) A[1]:(-0.22014118731) A[2]:(0.537431776524) A[3]:(0.519363164902)\n",
      " state (4)  A[0]:(0.59043854475) A[1]:(0.656116485596) A[2]:(-1.51395797729e-05) A[3]:(0.531438827515)\n",
      " state (5)  A[0]:(0.161760479212) A[1]:(0.928922235966) A[2]:(-0.188723877072) A[3]:(0.520615935326)\n",
      " state (6)  A[0]:(7.2717666626e-06) A[1]:(0.809974312782) A[2]:(-9.77516174316e-06) A[3]:(0.655979335308)\n",
      " state (7)  A[0]:(0.631204366684) A[1]:(-0.251611828804) A[2]:(0.285153001547) A[3]:(0.888020396233)\n",
      " state (8)  A[0]:(0.656090497971) A[1]:(-4.23863530159e-05) A[2]:(0.728952527046) A[3]:(0.590609192848)\n",
      " state (9)  A[0]:(0.656136989594) A[1]:(0.809986591339) A[2]:(0.809963285923) A[3]:(0.000222012400627)\n",
      " state (10)  A[0]:(0.729054927826) A[1]:(0.899991452694) A[2]:(-0.000211834907532) A[3]:(0.729024350643)\n",
      " state (11)  A[0]:(0.521024465561) A[1]:(0.876877486706) A[2]:(-0.607878684998) A[3]:(0.843888759613)\n",
      " state (12)  A[0]:(0.0775280371308) A[1]:(0.824575483799) A[2]:(-0.58267390728) A[3]:(0.793403863907)\n",
      " state (13)  A[0]:(-6.20484352112e-05) A[1]:(0.809442341328) A[2]:(0.899983525276) A[3]:(0.72904586792)\n",
      " state (14)  A[0]:(0.810008168221) A[1]:(0.900204539299) A[2]:(0.999999940395) A[3]:(0.809973716736)\n",
      " state (15)  A[0]:(0.98495054245) A[1]:(0.957880437374) A[2]:(1.0) A[3]:(0.88173365593)\n",
      "Episode 625000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6019. Times reached goal: 994.               Steps done: 4912858. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00661630967312.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531866490841) A[1]:(0.590527236462) A[2]:(0.590526342392) A[3]:(0.531170129776)\n",
      " state (1)  A[0]:(0.531893968582) A[1]:(0.000277057290077) A[2]:(0.656186699867) A[3]:(0.590130448341)\n",
      " state (2)  A[0]:(0.591072380543) A[1]:(0.729066610336) A[2]:(0.59048974514) A[3]:(0.655912637711)\n",
      " state (3)  A[0]:(0.657003283501) A[1]:(-0.217808350921) A[2]:(0.536933422089) A[3]:(0.519298195839)\n",
      " state (4)  A[0]:(0.591726243496) A[1]:(0.656036973) A[2]:(-0.000300884246826) A[3]:(0.531286358833)\n",
      " state (5)  A[0]:(0.163868561387) A[1]:(0.928902566433) A[2]:(-0.189165979624) A[3]:(0.520662307739)\n",
      " state (6)  A[0]:(0.00208651716821) A[1]:(0.80993616581) A[2]:(-0.000603675784077) A[3]:(0.656019210815)\n",
      " state (7)  A[0]:(0.632158875465) A[1]:(-0.251742452383) A[2]:(0.284507721663) A[3]:(0.887958109379)\n",
      " state (8)  A[0]:(0.656669139862) A[1]:(-0.000230744481087) A[2]:(0.728514075279) A[3]:(0.590499103069)\n",
      " state (9)  A[0]:(0.656326115131) A[1]:(0.809901118279) A[2]:(0.809648811817) A[3]:(7.37905502319e-05)\n",
      " state (10)  A[0]:(0.729045629501) A[1]:(0.899928033352) A[2]:(-0.00102364982013) A[3]:(0.728951215744)\n",
      " state (11)  A[0]:(0.520920455456) A[1]:(0.876782476902) A[2]:(-0.608430445194) A[3]:(0.843857645988)\n",
      " state (12)  A[0]:(0.0773232132196) A[1]:(0.824424386024) A[2]:(-0.583317399025) A[3]:(0.79337978363)\n",
      " state (13)  A[0]:(-0.00031989812851) A[1]:(0.809264779091) A[2]:(0.899869382381) A[3]:(0.729027628899)\n",
      " state (14)  A[0]:(0.809901535511) A[1]:(0.900103449821) A[2]:(0.999999940395) A[3]:(0.809962809086)\n",
      " state (15)  A[0]:(0.984926640987) A[1]:(0.957825183868) A[2]:(1.0) A[3]:(0.881694972515)\n",
      "Episode 626000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6026. Times reached goal: 995.               Steps done: 4918884. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00657655967805.\n",
      " state (0)  A[0]:(0.531552553177) A[1]:(0.590801477432) A[2]:(0.590517044067) A[3]:(0.531351089478)\n",
      " state (1)  A[0]:(0.531433165073) A[1]:(0.000157117843628) A[2]:(0.655997693539) A[3]:(0.590676903725)\n",
      " state (2)  A[0]:(0.59048384428) A[1]:(0.728906869888) A[2]:(0.59070789814) A[3]:(0.656355321407)\n",
      " state (3)  A[0]:(0.656026721001) A[1]:(-0.221270442009) A[2]:(0.537683486938) A[3]:(0.51954972744)\n",
      " state (4)  A[0]:(0.590374469757) A[1]:(0.656009793282) A[2]:(8.01086425781e-05) A[3]:(0.531713664532)\n",
      " state (5)  A[0]:(0.161559864879) A[1]:(0.928899288177) A[2]:(-0.188771575689) A[3]:(0.520974755287)\n",
      " state (6)  A[0]:(-0.000407338113291) A[1]:(0.809943199158) A[2]:(-8.17775726318e-05) A[3]:(0.656254351139)\n",
      " state (7)  A[0]:(0.630885064602) A[1]:(-0.251706629992) A[2]:(0.28521925211) A[3]:(0.888119578362)\n",
      " state (8)  A[0]:(0.655862212181) A[1]:(-0.000291347503662) A[2]:(0.728809297085) A[3]:(0.591214179993)\n",
      " state (9)  A[0]:(0.655774831772) A[1]:(0.809940099716) A[2]:(0.809944391251) A[3]:(0.000645428779535)\n",
      " state (10)  A[0]:(0.728710055351) A[1]:(0.899978458881) A[2]:(-0.000195860862732) A[3]:(0.729121565819)\n",
      " state (11)  A[0]:(0.5204859972) A[1]:(0.876860558987) A[2]:(-0.608078122139) A[3]:(0.843971133232)\n",
      " state (12)  A[0]:(0.0767481848598) A[1]:(0.824547290802) A[2]:(-0.583114504814) A[3]:(0.793528437614)\n",
      " state (13)  A[0]:(-0.000847071234602) A[1]:(0.809415280819) A[2]:(0.899988293648) A[3]:(0.729226410389)\n",
      " state (14)  A[0]:(0.80981194973) A[1]:(0.900202810764) A[2]:(0.999999940395) A[3]:(0.810149669647)\n",
      " state (15)  A[0]:(0.984912335873) A[1]:(0.957864224911) A[2]:(1.0) A[3]:(0.881808400154)\n",
      "Episode 627000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6011. Times reached goal: 991.               Steps done: 4924895. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00653714655264.\n",
      " state (0)  A[0]:(0.531801462173) A[1]:(0.590808749199) A[2]:(0.590663015842) A[3]:(0.531751275063)\n",
      " state (1)  A[0]:(0.531715273857) A[1]:(-0.000160381197929) A[2]:(0.656189620495) A[3]:(0.590640425682)\n",
      " state (2)  A[0]:(0.590781986713) A[1]:(0.729004025459) A[2]:(0.59081697464) A[3]:(0.656278610229)\n",
      " state (3)  A[0]:(0.65630120039) A[1]:(-0.219866275787) A[2]:(0.53747344017) A[3]:(0.519619762897)\n",
      " state (4)  A[0]:(0.590585231781) A[1]:(0.65619468689) A[2]:(1.27553939819e-05) A[3]:(0.531780958176)\n",
      " state (5)  A[0]:(0.161729454994) A[1]:(0.928889214993) A[2]:(-0.188692376018) A[3]:(0.52112364769)\n",
      " state (6)  A[0]:(-0.000184834003448) A[1]:(0.809931099415) A[2]:(1.27553939819e-05) A[3]:(0.656387805939)\n",
      " state (7)  A[0]:(0.631389915943) A[1]:(-0.251700818539) A[2]:(0.285447567701) A[3]:(0.888144850731)\n",
      " state (8)  A[0]:(0.656464457512) A[1]:(-0.000305488705635) A[2]:(0.729094028473) A[3]:(0.590844869614)\n",
      " state (9)  A[0]:(0.656411528587) A[1]:(0.809906601906) A[2]:(0.810005486012) A[3]:(0.00028221309185)\n",
      " state (10)  A[0]:(0.729220330715) A[1]:(0.899939894676) A[2]:(-0.00043165680836) A[3]:(0.729200184345)\n",
      " state (11)  A[0]:(0.52120256424) A[1]:(0.876802861691) A[2]:(-0.608359336853) A[3]:(0.84407389164)\n",
      " state (12)  A[0]:(0.0776086971164) A[1]:(0.824457883835) A[2]:(-0.583459079266) A[3]:(0.793681323528)\n",
      " state (13)  A[0]:(-1.81198120117e-05) A[1]:(0.809318304062) A[2]:(0.900024652481) A[3]:(0.729423046112)\n",
      " state (14)  A[0]:(0.810119211674) A[1]:(0.900153577328) A[2]:(0.999999940395) A[3]:(0.810273766518)\n",
      " state (15)  A[0]:(0.984921216965) A[1]:(0.957828938961) A[2]:(1.0) A[3]:(0.881823718548)\n",
      "Episode 628000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6010. Times reached goal: 992.               Steps done: 4930905. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00649797612689.\n",
      " state (0)  A[0]:(0.532376348972) A[1]:(0.590440273285) A[2]:(0.590535998344) A[3]:(0.531454801559)\n",
      " state (1)  A[0]:(0.532508850098) A[1]:(-0.000288233160973) A[2]:(0.656086623669) A[3]:(0.590472579002)\n",
      " state (2)  A[0]:(0.591671228409) A[1]:(0.728945732117) A[2]:(0.590681433678) A[3]:(0.656177282333)\n",
      " state (3)  A[0]:(0.657321572304) A[1]:(-0.218847200274) A[2]:(0.537526965141) A[3]:(0.519671440125)\n",
      " state (4)  A[0]:(0.592121124268) A[1]:(0.656120181084) A[2]:(0.000357747048838) A[3]:(0.531777262688)\n",
      " state (5)  A[0]:(0.164579719305) A[1]:(0.928947687149) A[2]:(-0.188646033406) A[3]:(0.521173834801)\n",
      " state (6)  A[0]:(0.0029083408881) A[1]:(0.810097813606) A[2]:(4.4584274292e-05) A[3]:(0.656277000904)\n",
      " state (7)  A[0]:(0.6326867342) A[1]:(-0.251117914915) A[2]:(0.285598874092) A[3]:(0.887902140617)\n",
      " state (8)  A[0]:(0.657153844833) A[1]:(0.000469520658953) A[2]:(0.729203820229) A[3]:(0.589978277683)\n",
      " state (9)  A[0]:(0.656782925129) A[1]:(0.810113668442) A[2]:(0.810202121735) A[3]:(-0.000950008339714)\n",
      " state (10)  A[0]:(0.729410946369) A[1]:(0.900025784969) A[2]:(0.000310063362122) A[3]:(0.728604197502)\n",
      " state (11)  A[0]:(0.521437048912) A[1]:(0.876884698868) A[2]:(-0.607948482037) A[3]:(0.843700647354)\n",
      " state (12)  A[0]:(0.0778522193432) A[1]:(0.824544906616) A[2]:(-0.583218634129) A[3]:(0.793190479279)\n",
      " state (13)  A[0]:(-1.28149986267e-05) A[1]:(0.809376120567) A[2]:(0.900018155575) A[3]:(0.728781104088)\n",
      " state (14)  A[0]:(0.809971272945) A[1]:(0.900166451931) A[2]:(0.999999940395) A[3]:(0.809799194336)\n",
      " state (15)  A[0]:(0.984895348549) A[1]:(0.957826912403) A[2]:(1.0) A[3]:(0.881518542767)\n",
      "Episode 629000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 993.               Steps done: 4936914. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00645904686844.\n",
      "q_values \n",
      "tensor([[ 0.5322,  0.5905,  0.5907,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5321, -0.0000,  0.6561,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5911,  0.7290,  0.5906,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0011,  0.8100, -0.0002,  0.6553]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0005,  0.7277]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9003,  1.0000,  0.8091]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531213641167) A[1]:(0.590531945229) A[2]:(0.590666472912) A[3]:(0.532239735126)\n",
      " state (1)  A[0]:(0.531349778175) A[1]:(7.55041837692e-05) A[2]:(0.65617275238) A[3]:(0.590975880623)\n",
      " state (2)  A[0]:(0.590702414513) A[1]:(0.729056358337) A[2]:(0.590613603592) A[3]:(0.656469404697)\n",
      " state (3)  A[0]:(0.656598210335) A[1]:(-0.218977153301) A[2]:(0.537247538567) A[3]:(0.519949316978)\n",
      " state (4)  A[0]:(0.591346025467) A[1]:(0.656070947647) A[2]:(-9.89437103271e-05) A[3]:(0.531858205795)\n",
      " state (5)  A[0]:(0.163405925035) A[1]:(0.928897738457) A[2]:(-0.188959300518) A[3]:(0.521008729935)\n",
      " state (6)  A[0]:(0.00160422781482) A[1]:(0.810057878494) A[2]:(-0.000236749649048) A[3]:(0.655993163586)\n",
      " state (7)  A[0]:(0.632204353809) A[1]:(-0.251264005899) A[2]:(0.285487741232) A[3]:(0.887827634811)\n",
      " state (8)  A[0]:(0.657115936279) A[1]:(-6.18547201157e-05) A[2]:(0.729031324387) A[3]:(0.589812636375)\n",
      " state (9)  A[0]:(0.65687328577) A[1]:(0.810003399849) A[2]:(0.810065507889) A[3]:(-0.00200469512492)\n",
      " state (10)  A[0]:(0.729677200317) A[1]:(0.900023818016) A[2]:(-0.000298380851746) A[3]:(0.728044450283)\n",
      " state (11)  A[0]:(0.522119164467) A[1]:(0.876937150955) A[2]:(-0.608516573906) A[3]:(0.843372583389)\n",
      " state (12)  A[0]:(0.0790195986629) A[1]:(0.824687063694) A[2]:(-0.583901166916) A[3]:(0.792739152908)\n",
      " state (13)  A[0]:(0.00139900948852) A[1]:(0.809614956379) A[2]:(0.899939477444) A[3]:(0.728136539459)\n",
      " state (14)  A[0]:(0.810568869114) A[1]:(0.900353193283) A[2]:(0.999999940395) A[3]:(0.809249520302)\n",
      " state (15)  A[0]:(0.984937429428) A[1]:(0.957918345928) A[2]:(1.0) A[3]:(0.88106238842)\n",
      "Episode 630000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6020. Times reached goal: 994.               Steps done: 4942934. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00642028021101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531369030476) A[1]:(0.590408444405) A[2]:(0.590429902077) A[3]:(0.531512260437)\n",
      " state (1)  A[0]:(0.531432688236) A[1]:(-5.53950667381e-05) A[2]:(0.656017541885) A[3]:(0.59050321579)\n",
      " state (2)  A[0]:(0.590523481369) A[1]:(0.728906869888) A[2]:(0.590388715267) A[3]:(0.656215667725)\n",
      " state (3)  A[0]:(0.656096220016) A[1]:(-0.218604430556) A[2]:(0.537086367607) A[3]:(0.519495666027)\n",
      " state (4)  A[0]:(0.590330541134) A[1]:(0.655935049057) A[2]:(-6.56843185425e-05) A[3]:(0.531492948532)\n",
      " state (5)  A[0]:(0.161488324404) A[1]:(0.928864240646) A[2]:(-0.188874125481) A[3]:(0.520945549011)\n",
      " state (6)  A[0]:(-0.000377774209483) A[1]:(0.809895157814) A[2]:(-5.79357147217e-05) A[3]:(0.656174778938)\n",
      " state (7)  A[0]:(0.630941212177) A[1]:(-0.251710683107) A[2]:(0.285542488098) A[3]:(0.887959063053)\n",
      " state (8)  A[0]:(0.656009316444) A[1]:(-0.000356309086783) A[2]:(0.728724598885) A[3]:(0.590943932533)\n",
      " state (9)  A[0]:(0.655830621719) A[1]:(0.809898555279) A[2]:(0.809844017029) A[3]:(0.000172227621078)\n",
      " state (10)  A[0]:(0.728743195534) A[1]:(0.899958312511) A[2]:(-0.000511646212544) A[3]:(0.728952050209)\n",
      " state (11)  A[0]:(0.520597457886) A[1]:(0.876844584942) A[2]:(-0.608582139015) A[3]:(0.843916237354)\n",
      " state (12)  A[0]:(0.0768972337246) A[1]:(0.824537456036) A[2]:(-0.58399617672) A[3]:(0.79346883297)\n",
      " state (13)  A[0]:(-0.000716328504495) A[1]:(0.809423089027) A[2]:(0.899982690811) A[3]:(0.729121446609)\n",
      " state (14)  A[0]:(0.809898614883) A[1]:(0.90022444725) A[2]:(0.999999940395) A[3]:(0.810034096241)\n",
      " state (15)  A[0]:(0.984871864319) A[1]:(0.957840919495) A[2]:(1.0) A[3]:(0.881580471992)\n",
      "Episode 631000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6015. Times reached goal: 993.               Steps done: 4948949. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00638177813661.\n",
      " state (0)  A[0]:(0.531533122063) A[1]:(0.59019947052) A[2]:(0.590470552444) A[3]:(0.531240344048)\n",
      " state (1)  A[0]:(0.531513869762) A[1]:(-6.73979520798e-05) A[2]:(0.656005978584) A[3]:(0.590233325958)\n",
      " state (2)  A[0]:(0.590562641621) A[1]:(0.728947877884) A[2]:(0.590400576591) A[3]:(0.655941128731)\n",
      " state (3)  A[0]:(0.656071186066) A[1]:(-0.218105375767) A[2]:(0.537079334259) A[3]:(0.518993735313)\n",
      " state (4)  A[0]:(0.590354919434) A[1]:(0.655989170074) A[2]:(-5.11407852173e-05) A[3]:(0.530904173851)\n",
      " state (5)  A[0]:(0.161724925041) A[1]:(0.928913533688) A[2]:(-0.188964828849) A[3]:(0.520342707634)\n",
      " state (6)  A[0]:(1.82390213013e-05) A[1]:(0.809978485107) A[2]:(1.09672546387e-05) A[3]:(0.655532896519)\n",
      " state (7)  A[0]:(0.630924224854) A[1]:(-0.251515686512) A[2]:(0.285947024822) A[3]:(0.887534976006)\n",
      " state (8)  A[0]:(0.655855298042) A[1]:(-2.40951776505e-05) A[2]:(0.728979349136) A[3]:(0.589684724808)\n",
      " state (9)  A[0]:(0.655849099159) A[1]:(0.809990167618) A[2]:(0.809999346733) A[3]:(-0.000888228183612)\n",
      " state (10)  A[0]:(0.728901863098) A[1]:(0.900013506413) A[2]:(-0.000192642211914) A[3]:(0.728621244431)\n",
      " state (11)  A[0]:(0.520969927311) A[1]:(0.876913309097) A[2]:(-0.608473300934) A[3]:(0.843713641167)\n",
      " state (12)  A[0]:(0.0774937048554) A[1]:(0.824621915817) A[2]:(-0.58392906189) A[3]:(0.793166816235)\n",
      " state (13)  A[0]:(-6.03497028351e-05) A[1]:(0.809493541718) A[2]:(0.900113761425) A[3]:(0.728712558746)\n",
      " state (14)  A[0]:(0.81010890007) A[1]:(0.900245189667) A[2]:(0.999999940395) A[3]:(0.809774637222)\n",
      " state (15)  A[0]:(0.984870612621) A[1]:(0.957826137543) A[2]:(1.0) A[3]:(0.881405591965)\n",
      "Episode 632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6019. Times reached goal: 992.               Steps done: 4954968. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00634348158311.\n",
      " state (0)  A[0]:(0.532733917236) A[1]:(0.590583324432) A[2]:(0.590735554695) A[3]:(0.531578958035)\n",
      " state (1)  A[0]:(0.533108830452) A[1]:(0.000111036002636) A[2]:(0.656344413757) A[3]:(0.590698897839)\n",
      " state (2)  A[0]:(0.592170357704) A[1]:(0.729200541973) A[2]:(0.590735077858) A[3]:(0.656479120255)\n",
      " state (3)  A[0]:(0.657424926758) A[1]:(-0.216784283519) A[2]:(0.537170052528) A[3]:(0.5200111866)\n",
      " state (4)  A[0]:(0.591802477837) A[1]:(0.656359434128) A[2]:(0.000100493431091) A[3]:(0.531984329224)\n",
      " state (5)  A[0]:(0.163753315806) A[1]:(0.92895758152) A[2]:(-0.18884113431) A[3]:(0.521515011787)\n",
      " state (6)  A[0]:(0.00202792603523) A[1]:(0.810051202774) A[2]:(6.80685043335e-05) A[3]:(0.656578421593)\n",
      " state (7)  A[0]:(0.632187128067) A[1]:(-0.251296073198) A[2]:(0.285962879658) A[3]:(0.888003468513)\n",
      " state (8)  A[0]:(0.657029986382) A[1]:(0.000275366008282) A[2]:(0.728931665421) A[3]:(0.591041862965)\n",
      " state (9)  A[0]:(0.656864464283) A[1]:(0.810106635094) A[2]:(0.809975981712) A[3]:(0.000561207474675)\n",
      " state (10)  A[0]:(0.729574203491) A[1]:(0.900055587292) A[2]:(-0.000251650810242) A[3]:(0.729121088982)\n",
      " state (11)  A[0]:(0.521816730499) A[1]:(0.876940965652) A[2]:(-0.608616232872) A[3]:(0.843985378742)\n",
      " state (12)  A[0]:(0.0784056484699) A[1]:(0.824640154839) A[2]:(-0.584251224995) A[3]:(0.793506264687)\n",
      " state (13)  A[0]:(0.000575661601033) A[1]:(0.80949819088) A[2]:(0.900017559528) A[3]:(0.729134082794)\n",
      " state (14)  A[0]:(0.810219705105) A[1]:(0.9002430439) A[2]:(0.999999940395) A[3]:(0.810077428818)\n",
      " state (15)  A[0]:(0.984864473343) A[1]:(0.957819581032) A[2]:(1.0) A[3]:(0.881581187248)\n",
      "Episode 633000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6023. Times reached goal: 994.               Steps done: 4960991. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00630538962262.\n",
      " state (0)  A[0]:(0.53169965744) A[1]:(0.590410351753) A[2]:(0.59046792984) A[3]:(0.531227588654)\n",
      " state (1)  A[0]:(0.531726360321) A[1]:(-6.47827982903e-05) A[2]:(0.656151533127) A[3]:(0.590350866318)\n",
      " state (2)  A[0]:(0.590795874596) A[1]:(0.728935241699) A[2]:(0.590462565422) A[3]:(0.656160116196)\n",
      " state (3)  A[0]:(0.656298995018) A[1]:(-0.217295974493) A[2]:(0.537041068077) A[3]:(0.519387125969)\n",
      " state (4)  A[0]:(0.590539336205) A[1]:(0.655984818935) A[2]:(3.36170196533e-05) A[3]:(0.531303822994)\n",
      " state (5)  A[0]:(0.161892384291) A[1]:(0.928880214691) A[2]:(-0.188931256533) A[3]:(0.520885586739)\n",
      " state (6)  A[0]:(0.000122040510178) A[1]:(0.809914708138) A[2]:(-1.12056732178e-05) A[3]:(0.656095981598)\n",
      " state (7)  A[0]:(0.631071686745) A[1]:(-0.251483559608) A[2]:(0.285997360945) A[3]:(0.887810349464)\n",
      " state (8)  A[0]:(0.656168103218) A[1]:(0.000188000500202) A[2]:(0.728979349136) A[3]:(0.590504169464)\n",
      " state (9)  A[0]:(0.656210899353) A[1]:(0.810037851334) A[2]:(0.809977769852) A[3]:(0.000101178884506)\n",
      " state (10)  A[0]:(0.729100286961) A[1]:(0.899978637695) A[2]:(-0.000186443328857) A[3]:(0.728946864605)\n",
      " state (11)  A[0]:(0.521180152893) A[1]:(0.876813828945) A[2]:(-0.608592748642) A[3]:(0.843882799149)\n",
      " state (12)  A[0]:(0.077704615891) A[1]:(0.824424564838) A[2]:(-0.584320783615) A[3]:(0.793402969837)\n",
      " state (13)  A[0]:(3.99053096771e-05) A[1]:(0.809222221375) A[2]:(0.900020837784) A[3]:(0.729058027267)\n",
      " state (14)  A[0]:(0.810111761093) A[1]:(0.900067567825) A[2]:(0.999999940395) A[3]:(0.810084462166)\n",
      " state (15)  A[0]:(0.984852969646) A[1]:(0.95772600174) A[2]:(1.0) A[3]:(0.881607174873)\n",
      "Episode 634000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6025. Times reached goal: 995.               Steps done: 4967016. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00626751386544.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5904,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5311, -0.0000,  0.6561,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.7290,  0.5904,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8101,  0.0000,  0.6563]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7295,  0.9000, -0.0001,  0.7295]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8112,  0.9001,  1.0000,  0.8105]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531008124352) A[1]:(0.590376257896) A[2]:(0.590402960777) A[3]:(0.531509578228)\n",
      " state (1)  A[0]:(0.531332015991) A[1]:(4.00841236115e-06) A[2]:(0.656093001366) A[3]:(0.590499520302)\n",
      " state (2)  A[0]:(0.590824186802) A[1]:(0.729050517082) A[2]:(0.590416550636) A[3]:(0.65630376339)\n",
      " state (3)  A[0]:(0.657061636448) A[1]:(-0.216816142201) A[2]:(0.536982178688) A[3]:(0.519735872746)\n",
      " state (4)  A[0]:(0.592032432556) A[1]:(0.656033277512) A[2]:(1.33514404297e-05) A[3]:(0.531730890274)\n",
      " state (5)  A[0]:(0.164722084999) A[1]:(0.928911685944) A[2]:(-0.188997477293) A[3]:(0.52140891552)\n",
      " state (6)  A[0]:(0.00343464454636) A[1]:(0.810010552406) A[2]:(6.65187835693e-05) A[3]:(0.656450331211)\n",
      " state (7)  A[0]:(0.633211731911) A[1]:(-0.251348614693) A[2]:(0.286394447088) A[3]:(0.887880921364)\n",
      " state (8)  A[0]:(0.658353567123) A[1]:(3.29166650772e-05) A[2]:(0.729118943214) A[3]:(0.591080307961)\n",
      " state (9)  A[0]:(0.658564686775) A[1]:(0.810011208057) A[2]:(0.810150682926) A[3]:(0.0011800074717)\n",
      " state (10)  A[0]:(0.731281161308) A[1]:(0.90001052618) A[2]:(0.000113487243652) A[3]:(0.729661107063)\n",
      " state (11)  A[0]:(0.524863779545) A[1]:(0.876879990101) A[2]:(-0.608637571335) A[3]:(0.844419360161)\n",
      " state (12)  A[0]:(0.0829066559672) A[1]:(0.824527561665) A[2]:(-0.584635853767) A[3]:(0.794108688831)\n",
      " state (13)  A[0]:(0.00516684213653) A[1]:(0.809320151806) A[2]:(0.899822533131) A[3]:(0.729882121086)\n",
      " state (14)  A[0]:(0.81176674366) A[1]:(0.900107622147) A[2]:(0.999999940395) A[3]:(0.810604035854)\n",
      " state (15)  A[0]:(0.984990656376) A[1]:(0.957739472389) A[2]:(1.0) A[3]:(0.881891727448)\n",
      "Episode 635000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5997. Times reached goal: 988.               Steps done: 4973013. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0062300400623.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531447827816) A[1]:(0.590412080288) A[2]:(0.590454816818) A[3]:(0.531242847443)\n",
      " state (1)  A[0]:(0.531348764896) A[1]:(4.92483377457e-06) A[2]:(0.656031847) A[3]:(0.590331792831)\n",
      " state (2)  A[0]:(0.590451598167) A[1]:(0.728981435299) A[2]:(0.590625464916) A[3]:(0.656079173088)\n",
      " state (3)  A[0]:(0.656060814857) A[1]:(-0.218897804618) A[2]:(0.537340164185) A[3]:(0.519059658051)\n",
      " state (4)  A[0]:(0.590374708176) A[1]:(0.656062483788) A[2]:(-0.00011134147644) A[3]:(0.531133532524)\n",
      " state (5)  A[0]:(0.161607950926) A[1]:(0.928926110268) A[2]:(-0.189247354865) A[3]:(0.520755052567)\n",
      " state (6)  A[0]:(-6.4879655838e-05) A[1]:(0.809943854809) A[2]:(-0.000157713890076) A[3]:(0.655982136726)\n",
      " state (7)  A[0]:(0.630917131901) A[1]:(-0.251557528973) A[2]:(0.286287069321) A[3]:(0.887710571289)\n",
      " state (8)  A[0]:(0.655983567238) A[1]:(-6.46933913231e-05) A[2]:(0.728966534138) A[3]:(0.590381443501)\n",
      " state (9)  A[0]:(0.656114518642) A[1]:(0.809942722321) A[2]:(0.809976458549) A[3]:(-0.000187158584595)\n",
      " state (10)  A[0]:(0.729074597359) A[1]:(0.899957835674) A[2]:(-0.000607728899922) A[3]:(0.72894090414)\n",
      " state (11)  A[0]:(0.521067976952) A[1]:(0.876807510853) A[2]:(-0.609228372574) A[3]:(0.843927681446)\n",
      " state (12)  A[0]:(0.0773635953665) A[1]:(0.824438989162) A[2]:(-0.585137069225) A[3]:(0.793428897858)\n",
      " state (13)  A[0]:(-0.000283718109131) A[1]:(0.809293270111) A[2]:(0.900072157383) A[3]:(0.729036927223)\n",
      " state (14)  A[0]:(0.81007963419) A[1]:(0.900153160095) A[2]:(0.999999940395) A[3]:(0.810018181801)\n",
      " state (15)  A[0]:(0.984818041325) A[1]:(0.957753539085) A[2]:(1.0) A[3]:(0.881444811821)\n",
      "Episode 636000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6034. Times reached goal: 1000.               Steps done: 4979047. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00619256118804.\n",
      " state (0)  A[0]:(0.531802773476) A[1]:(0.590482354164) A[2]:(0.590520918369) A[3]:(0.531733155251)\n",
      " state (1)  A[0]:(0.531867980957) A[1]:(-0.000412277848227) A[2]:(0.65603941679) A[3]:(0.590617239475)\n",
      " state (2)  A[0]:(0.590954124928) A[1]:(0.729038953781) A[2]:(0.59078347683) A[3]:(0.656272292137)\n",
      " state (3)  A[0]:(0.656787276268) A[1]:(-0.218363910913) A[2]:(0.537732601166) A[3]:(0.519296646118)\n",
      " state (4)  A[0]:(0.591151714325) A[1]:(0.656151771545) A[2]:(0.000756621244363) A[3]:(0.53130197525)\n",
      " state (5)  A[0]:(0.162427157164) A[1]:(0.92892229557) A[2]:(-0.188305139542) A[3]:(0.520903468132)\n",
      " state (6)  A[0]:(0.000253200531006) A[1]:(0.809957742691) A[2]:(0.000782847229857) A[3]:(0.655943393707)\n",
      " state (7)  A[0]:(0.630897402763) A[1]:(-0.251525402069) A[2]:(0.286939650774) A[3]:(0.887651562691)\n",
      " state (8)  A[0]:(0.656122803688) A[1]:(-5.08055090904e-05) A[2]:(0.728935539722) A[3]:(0.590902626514)\n",
      " state (9)  A[0]:(0.656091928482) A[1]:(0.80997043848) A[2]:(0.809967577457) A[3]:(0.000718518975191)\n",
      " state (10)  A[0]:(0.728803396225) A[1]:(0.899949967861) A[2]:(7.54594802856e-05) A[3]:(0.728978991508)\n",
      " state (11)  A[0]:(0.520488202572) A[1]:(0.876770555973) A[2]:(-0.608658730984) A[3]:(0.843847990036)\n",
      " state (12)  A[0]:(0.0764990225434) A[1]:(0.824354588985) A[2]:(-0.584730863571) A[3]:(0.793319165707)\n",
      " state (13)  A[0]:(-0.00136065401603) A[1]:(0.809143662453) A[2]:(0.900027155876) A[3]:(0.72891420126)\n",
      " state (14)  A[0]:(0.809615910053) A[1]:(0.90002810955) A[2]:(0.999999940395) A[3]:(0.809939265251)\n",
      " state (15)  A[0]:(0.984779238701) A[1]:(0.957687497139) A[2]:(1.0) A[3]:(0.881424069405)\n",
      "Episode 637000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6022. Times reached goal: 994.               Steps done: 4985069. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00615538164453.\n",
      " state (0)  A[0]:(0.531590759754) A[1]:(0.590605676174) A[2]:(0.590527057648) A[3]:(0.531756877899)\n",
      " state (1)  A[0]:(0.53152680397) A[1]:(-0.000179380178452) A[2]:(0.656068325043) A[3]:(0.590727448463)\n",
      " state (2)  A[0]:(0.590644478798) A[1]:(0.728950858116) A[2]:(0.590693593025) A[3]:(0.656362652779)\n",
      " state (3)  A[0]:(0.656320214272) A[1]:(-0.218698352575) A[2]:(0.53747856617) A[3]:(0.519342899323)\n",
      " state (4)  A[0]:(0.590710043907) A[1]:(0.656106650829) A[2]:(5.74588775635e-05) A[3]:(0.531312942505)\n",
      " state (5)  A[0]:(0.162089198828) A[1]:(0.928927540779) A[2]:(-0.189216449857) A[3]:(0.520914196968)\n",
      " state (6)  A[0]:(0.000268489122391) A[1]:(0.80996710062) A[2]:(-0.000166654586792) A[3]:(0.656075298786)\n",
      " state (7)  A[0]:(0.631067872047) A[1]:(-0.251499533653) A[2]:(0.286414682865) A[3]:(0.887741982937)\n",
      " state (8)  A[0]:(0.65626680851) A[1]:(-2.61664390564e-05) A[2]:(0.728915452957) A[3]:(0.590495467186)\n",
      " state (9)  A[0]:(0.656384825706) A[1]:(0.809952437878) A[2]:(0.809935569763) A[3]:(-0.000530347169843)\n",
      " state (10)  A[0]:(0.729181587696) A[1]:(0.899942457676) A[2]:(-0.000511407793965) A[3]:(0.728554368019)\n",
      " state (11)  A[0]:(0.521148145199) A[1]:(0.876770913601) A[2]:(-0.609249591827) A[3]:(0.843644559383)\n",
      " state (12)  A[0]:(0.0774013921618) A[1]:(0.824372112751) A[2]:(-0.585405945778) A[3]:(0.793064951897)\n",
      " state (13)  A[0]:(-0.000389903754694) A[1]:(0.809199333191) A[2]:(0.900006473064) A[3]:(0.728601634502)\n",
      " state (14)  A[0]:(0.809983730316) A[1]:(0.900088906288) A[2]:(0.999999940395) A[3]:(0.809746146202)\n",
      " state (15)  A[0]:(0.984790146351) A[1]:(0.957709252834) A[2]:(1.0) A[3]:(0.881267309189)\n",
      "Episode 638000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6011. Times reached goal: 992.               Steps done: 4991080. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00611849262648.\n",
      " state (0)  A[0]:(0.531382918358) A[1]:(0.590569317341) A[2]:(0.5904712677) A[3]:(0.53176176548)\n",
      " state (1)  A[0]:(0.531421840191) A[1]:(0.000100001692772) A[2]:(0.656061410904) A[3]:(0.590819120407)\n",
      " state (2)  A[0]:(0.590600252151) A[1]:(0.728965997696) A[2]:(0.59059214592) A[3]:(0.656477153301)\n",
      " state (3)  A[0]:(0.656248271465) A[1]:(-0.218963474035) A[2]:(0.537330269814) A[3]:(0.51978623867)\n",
      " state (4)  A[0]:(0.59066426754) A[1]:(0.656025052071) A[2]:(-0.000162720680237) A[3]:(0.531852722168)\n",
      " state (5)  A[0]:(0.162129417062) A[1]:(0.928910076618) A[2]:(-0.189356088638) A[3]:(0.521401643753)\n",
      " state (6)  A[0]:(0.00023552775383) A[1]:(0.809967517853) A[2]:(-0.000247597694397) A[3]:(0.656249701977)\n",
      " state (7)  A[0]:(0.630931258202) A[1]:(-0.251533329487) A[2]:(0.286385118961) A[3]:(0.887704491615)\n",
      " state (8)  A[0]:(0.656088590622) A[1]:(-0.000223621726036) A[2]:(0.728719115257) A[3]:(0.590732336044)\n",
      " state (9)  A[0]:(0.655983448029) A[1]:(0.809944808483) A[2]:(0.809830307961) A[3]:(-0.000139504671097)\n",
      " state (10)  A[0]:(0.728932380676) A[1]:(0.899984240532) A[2]:(-0.000723957899027) A[3]:(0.728752732277)\n",
      " state (11)  A[0]:(0.521052479744) A[1]:(0.876865983009) A[2]:(-0.60944545269) A[3]:(0.843791246414)\n",
      " state (12)  A[0]:(0.0775848254561) A[1]:(0.824559152126) A[2]:(-0.585645854473) A[3]:(0.79323643446)\n",
      " state (13)  A[0]:(-9.45031642914e-05) A[1]:(0.809461355209) A[2]:(0.900032997131) A[3]:(0.728743195534)\n",
      " state (14)  A[0]:(0.809984564781) A[1]:(0.900267958641) A[2]:(0.999999940395) A[3]:(0.809753417969)\n",
      " state (15)  A[0]:(0.984762310982) A[1]:(0.957789778709) A[2]:(1.0) A[3]:(0.881161034107)\n",
      "Episode 639000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 993.               Steps done: 4997091. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00608182468322.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5901,  0.5904,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0001,  0.6559,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7289,  0.5903,  0.6563]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0000,  0.8099,  0.0000,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0005,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9004,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531380772591) A[1]:(0.590146303177) A[2]:(0.590411663055) A[3]:(0.531446576118)\n",
      " state (1)  A[0]:(0.531324028969) A[1]:(-7.74934887886e-05) A[2]:(0.655906260014) A[3]:(0.590472340584)\n",
      " state (2)  A[0]:(0.590431451797) A[1]:(0.728896915913) A[2]:(0.590259075165) A[3]:(0.65620470047)\n",
      " state (3)  A[0]:(0.655953526497) A[1]:(-0.217881083488) A[2]:(0.537058472633) A[3]:(0.519580423832)\n",
      " state (4)  A[0]:(0.590196967125) A[1]:(0.655918776989) A[2]:(-6.47306442261e-05) A[3]:(0.53158390522)\n",
      " state (5)  A[0]:(0.161483824253) A[1]:(0.928901433945) A[2]:(-0.189211726189) A[3]:(0.521196842194)\n",
      " state (6)  A[0]:(-0.000111192464828) A[1]:(0.809922456741) A[2]:(4.16040420532e-05) A[3]:(0.656064867973)\n",
      " state (7)  A[0]:(0.630793094635) A[1]:(-0.251646012068) A[2]:(0.286785721779) A[3]:(0.887566566467)\n",
      " state (8)  A[0]:(0.65592610836) A[1]:(-0.000300385057926) A[2]:(0.728804826736) A[3]:(0.590705037117)\n",
      " state (9)  A[0]:(0.655817449093) A[1]:(0.809908509254) A[2]:(0.809888422489) A[3]:(0.000539436878171)\n",
      " state (10)  A[0]:(0.728848934174) A[1]:(0.89997780323) A[2]:(-0.000672459485941) A[3]:(0.729236960411)\n",
      " state (11)  A[0]:(0.520984768867) A[1]:(0.87687432766) A[2]:(-0.609558701515) A[3]:(0.84411251545)\n",
      " state (12)  A[0]:(0.0775102004409) A[1]:(0.824592471123) A[2]:(-0.585903048515) A[3]:(0.793616473675)\n",
      " state (13)  A[0]:(-0.000175327062607) A[1]:(0.809524178505) A[2]:(0.899982452393) A[3]:(0.729155063629)\n",
      " state (14)  A[0]:(0.80996710062) A[1]:(0.900321364403) A[2]:(0.999999940395) A[3]:(0.810002446175)\n",
      " state (15)  A[0]:(0.98475342989) A[1]:(0.95781570673) A[2]:(1.0) A[3]:(0.881271779537)\n",
      "Episode 640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5998. Times reached goal: 993.               Steps done: 5003089. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00604545508024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531381726265) A[1]:(0.590283513069) A[2]:(0.590422391891) A[3]:(0.531408309937)\n",
      " state (1)  A[0]:(0.53127849102) A[1]:(-2.85357236862e-05) A[2]:(0.656030416489) A[3]:(0.590538144112)\n",
      " state (2)  A[0]:(0.590398073196) A[1]:(0.728970170021) A[2]:(0.590531229973) A[3]:(0.656306862831)\n",
      " state (3)  A[0]:(0.65604698658) A[1]:(-0.217748239636) A[2]:(0.537301540375) A[3]:(0.51983332634)\n",
      " state (4)  A[0]:(0.590410709381) A[1]:(0.655937671661) A[2]:(0.00015127658844) A[3]:(0.531930923462)\n",
      " state (5)  A[0]:(0.161857962608) A[1]:(0.928915262222) A[2]:(-0.189120471478) A[3]:(0.5216370821)\n",
      " state (6)  A[0]:(0.000111937522888) A[1]:(0.810012996197) A[2]:(0.000153303146362) A[3]:(0.656356334686)\n",
      " state (7)  A[0]:(0.630867481232) A[1]:(-0.251372843981) A[2]:(0.287157535553) A[3]:(0.887614667416)\n",
      " state (8)  A[0]:(0.656076550484) A[1]:(-0.000111147761345) A[2]:(0.729068696499) A[3]:(0.590752482414)\n",
      " state (9)  A[0]:(0.656039953232) A[1]:(0.809964179993) A[2]:(0.810071468353) A[3]:(0.000613838376012)\n",
      " state (10)  A[0]:(0.72905254364) A[1]:(0.899995625019) A[2]:(-0.00018310546875) A[3]:(0.729324221611)\n",
      " state (11)  A[0]:(0.521300554276) A[1]:(0.876873254776) A[2]:(-0.609333992004) A[3]:(0.844189584255)\n",
      " state (12)  A[0]:(0.0779055729508) A[1]:(0.824548959732) A[2]:(-0.585804283619) A[3]:(0.793734908104)\n",
      " state (13)  A[0]:(0.000152260065079) A[1]:(0.809414744377) A[2]:(0.900009930134) A[3]:(0.72932100296)\n",
      " state (14)  A[0]:(0.810071587563) A[1]:(0.900215804577) A[2]:(0.999999940395) A[3]:(0.810139000416)\n",
      " state (15)  A[0]:(0.984755694866) A[1]:(0.957743525505) A[2]:(1.0) A[3]:(0.881355345249)\n",
      "Episode 641000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6018. Times reached goal: 997.               Steps done: 5009107. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00600918278438.\n",
      " state (0)  A[0]:(0.531284570694) A[1]:(0.590365827084) A[2]:(0.590394854546) A[3]:(0.529535710812)\n",
      " state (1)  A[0]:(0.531110942364) A[1]:(0.000115245580673) A[2]:(0.656120181084) A[3]:(0.588476657867)\n",
      " state (2)  A[0]:(0.590145230293) A[1]:(0.728832066059) A[2]:(0.590486884117) A[3]:(0.654298782349)\n",
      " state (3)  A[0]:(0.655601501465) A[1]:(-0.218600198627) A[2]:(0.53705739975) A[3]:(0.516650438309)\n",
      " state (4)  A[0]:(0.589621305466) A[1]:(0.655915021896) A[2]:(-0.000555872858968) A[3]:(0.528616309166)\n",
      " state (5)  A[0]:(0.160227462649) A[1]:(0.928865134716) A[2]:(-0.189820036292) A[3]:(0.518171668053)\n",
      " state (6)  A[0]:(-0.00199287873693) A[1]:(0.809878408909) A[2]:(-0.000603198946919) A[3]:(0.653435349464)\n",
      " state (7)  A[0]:(0.629495084286) A[1]:(-0.251723617315) A[2]:(0.286748945713) A[3]:(0.886341333389)\n",
      " state (8)  A[0]:(0.654623866081) A[1]:(-0.000498384179082) A[2]:(0.729244709015) A[3]:(0.586061239243)\n",
      " state (9)  A[0]:(0.654590010643) A[1]:(0.809785246849) A[2]:(0.810085594654) A[3]:(-0.0060455808416)\n",
      " state (10)  A[0]:(0.727902710438) A[1]:(0.899885773659) A[2]:(-0.000775217835326) A[3]:(0.726632714272)\n",
      " state (11)  A[0]:(0.519502282143) A[1]:(0.876748204231) A[2]:(-0.609930515289) A[3]:(0.842666685581)\n",
      " state (12)  A[0]:(0.0754294842482) A[1]:(0.82439917326) A[2]:(-0.586541414261) A[3]:(0.791889250278)\n",
      " state (13)  A[0]:(-0.00226109824143) A[1]:(0.809297144413) A[2]:(0.899896264076) A[3]:(0.727168321609)\n",
      " state (14)  A[0]:(0.809321522713) A[1]:(0.900185167789) A[2]:(0.999999940395) A[3]:(0.808776676655)\n",
      " state (15)  A[0]:(0.984681606293) A[1]:(0.957734644413) A[2]:(1.0) A[3]:(0.880577921867)\n",
      "Episode 642000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6033. Times reached goal: 993.               Steps done: 5015140. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00597303852343.\n",
      " state (0)  A[0]:(0.531003892422) A[1]:(0.59043854475) A[2]:(0.590488910675) A[3]:(0.53143286705)\n",
      " state (1)  A[0]:(0.53121972084) A[1]:(0.000833086494822) A[2]:(0.65607213974) A[3]:(0.590377926826)\n",
      " state (2)  A[0]:(0.590487658978) A[1]:(0.729056477547) A[2]:(0.589810073376) A[3]:(0.656217992306)\n",
      " state (3)  A[0]:(0.6559445858) A[1]:(-0.215509161353) A[2]:(0.536440074444) A[3]:(0.519641160965)\n",
      " state (4)  A[0]:(0.590155243874) A[1]:(0.65620225668) A[2]:(-0.000479578942759) A[3]:(0.531534671783)\n",
      " state (5)  A[0]:(0.161520943046) A[1]:(0.928940176964) A[2]:(-0.189619272947) A[3]:(0.521354198456)\n",
      " state (6)  A[0]:(-0.000204145908356) A[1]:(0.810116767883) A[2]:(-0.00029444694519) A[3]:(0.656186640263)\n",
      " state (7)  A[0]:(0.630630671978) A[1]:(-0.250583648682) A[2]:(0.286858916283) A[3]:(0.887514829636)\n",
      " state (8)  A[0]:(0.655899405479) A[1]:(0.00111230416223) A[2]:(0.729025304317) A[3]:(0.59029841423)\n",
      " state (9)  A[0]:(0.655847549438) A[1]:(0.8102414608) A[2]:(0.810038566589) A[3]:(0.000158131122589)\n",
      " state (10)  A[0]:(0.728681087494) A[1]:(0.900009453297) A[2]:(0.000269293785095) A[3]:(0.72889983654)\n",
      " state (11)  A[0]:(0.520577192307) A[1]:(0.876756250858) A[2]:(-0.608914494514) A[3]:(0.843840718269)\n",
      " state (12)  A[0]:(0.0769323706627) A[1]:(0.824215054512) A[2]:(-0.585588037968) A[3]:(0.793310046196)\n",
      " state (13)  A[0]:(-0.000871866708621) A[1]:(0.808846175671) A[2]:(0.899975895882) A[3]:(0.728860020638)\n",
      " state (14)  A[0]:(0.809706687927) A[1]:(0.899777472019) A[2]:(0.999999940395) A[3]:(0.809861958027)\n",
      " state (15)  A[0]:(0.984721899033) A[1]:(0.957494795322) A[2]:(1.0) A[3]:(0.881218135357)\n",
      "Episode 643000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 992.               Steps done: 5021149. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00593725415675.\n",
      " state (0)  A[0]:(0.532250642776) A[1]:(0.590511083603) A[2]:(0.590468883514) A[3]:(0.530532956123)\n",
      " state (1)  A[0]:(0.53233397007) A[1]:(9.88766551018e-05) A[2]:(0.656091272831) A[3]:(0.589796304703)\n",
      " state (2)  A[0]:(0.591316580772) A[1]:(0.729020774364) A[2]:(0.590504765511) A[3]:(0.655634760857)\n",
      " state (3)  A[0]:(0.656513571739) A[1]:(-0.217261359096) A[2]:(0.537289381027) A[3]:(0.518342733383)\n",
      " state (4)  A[0]:(0.590736269951) A[1]:(0.656061530113) A[2]:(7.7486038208e-06) A[3]:(0.530263960361)\n",
      " state (5)  A[0]:(0.16213478148) A[1]:(0.928930282593) A[2]:(-0.189529180527) A[3]:(0.520122289658)\n",
      " state (6)  A[0]:(-5.23328781128e-05) A[1]:(0.80992347002) A[2]:(-0.000200986862183) A[3]:(0.65530192852)\n",
      " state (7)  A[0]:(0.62988191843) A[1]:(-0.251590639353) A[2]:(0.287167370319) A[3]:(0.887163937092)\n",
      " state (8)  A[0]:(0.654159843922) A[1]:(-8.6821615696e-05) A[2]:(0.728921055794) A[3]:(0.589110195637)\n",
      " state (9)  A[0]:(0.653207302094) A[1]:(0.809967517853) A[2]:(0.809922814369) A[3]:(-0.00262347748503)\n",
      " state (10)  A[0]:(0.726227343082) A[1]:(0.899969279766) A[2]:(-0.000718355062418) A[3]:(0.72767162323)\n",
      " state (11)  A[0]:(0.516497731209) A[1]:(0.876811265945) A[2]:(-0.60993885994) A[3]:(0.843179702759)\n",
      " state (12)  A[0]:(0.0710323229432) A[1]:(0.824447512627) A[2]:(-0.586675703526) A[3]:(0.792478680611)\n",
      " state (13)  A[0]:(-0.00688717793673) A[1]:(0.809333205223) A[2]:(0.900017082691) A[3]:(0.727862596512)\n",
      " state (14)  A[0]:(0.807631969452) A[1]:(0.900213479996) A[2]:(0.999999940395) A[3]:(0.809251725674)\n",
      " state (15)  A[0]:(0.984497368336) A[1]:(0.957730710506) A[2]:(1.0) A[3]:(0.880804538727)\n",
      "Episode 644000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6015. Times reached goal: 993.               Steps done: 5027164. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00590164876357.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5903,  0.5904,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0000,  0.6560,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7289,  0.5905,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8100,  0.0000,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0004,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531433165073) A[1]:(0.590314924717) A[2]:(0.59038412571) A[3]:(0.531471669674)\n",
      " state (1)  A[0]:(0.531358480453) A[1]:(-9.07480716705e-06) A[2]:(0.65603941679) A[3]:(0.590399265289)\n",
      " state (2)  A[0]:(0.590442299843) A[1]:(0.728893637657) A[2]:(0.590579152107) A[3]:(0.656105399132)\n",
      " state (3)  A[0]:(0.656060099602) A[1]:(-0.218464046717) A[2]:(0.537433981895) A[3]:(0.519233167171)\n",
      " state (4)  A[0]:(0.590377926826) A[1]:(0.655997753143) A[2]:(2.36034393311e-05) A[3]:(0.531427919865)\n",
      " state (5)  A[0]:(0.161755248904) A[1]:(0.92891818285) A[2]:(-0.189363911748) A[3]:(0.521333575249)\n",
      " state (6)  A[0]:(6.88433647156e-06) A[1]:(0.809954047203) A[2]:(0.000169157981873) A[3]:(0.656035363674)\n",
      " state (7)  A[0]:(0.630631387234) A[1]:(-0.251477599144) A[2]:(0.28771713376) A[3]:(0.887326955795)\n",
      " state (8)  A[0]:(0.655984640121) A[1]:(-0.000192627310753) A[2]:(0.728996753693) A[3]:(0.590396523476)\n",
      " state (9)  A[0]:(0.655997931957) A[1]:(0.809968173504) A[2]:(0.81002676487) A[3]:(1.94907188416e-05)\n",
      " state (10)  A[0]:(0.729017794132) A[1]:(0.90001732111) A[2]:(-0.000128865242004) A[3]:(0.728922605515)\n",
      " state (11)  A[0]:(0.521327912807) A[1]:(0.876900017262) A[2]:(-0.609579324722) A[3]:(0.843951106071)\n",
      " state (12)  A[0]:(0.0779997929931) A[1]:(0.824575483799) A[2]:(-0.586431384087) A[3]:(0.79343944788)\n",
      " state (13)  A[0]:(0.000232040882111) A[1]:(0.809436559677) A[2]:(0.90004491806) A[3]:(0.72898375988)\n",
      " state (14)  A[0]:(0.81007194519) A[1]:(0.900236129761) A[2]:(0.999999940395) A[3]:(0.809980094433)\n",
      " state (15)  A[0]:(0.984705269337) A[1]:(0.957719326019) A[2]:(1.0) A[3]:(0.881201982498)\n",
      "Episode 645000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 991.               Steps done: 5033175. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00586628035908.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532969892025) A[1]:(0.590117692947) A[2]:(0.590564966202) A[3]:(0.531167387962)\n",
      " state (1)  A[0]:(0.5326384902) A[1]:(3.35276126862e-07) A[2]:(0.656172156334) A[3]:(0.590272605419)\n",
      " state (2)  A[0]:(0.591317296028) A[1]:(0.729027271271) A[2]:(0.590662479401) A[3]:(0.656061649323)\n",
      " state (3)  A[0]:(0.65609228611) A[1]:(-0.217155694962) A[2]:(0.537338495255) A[3]:(0.519643425941)\n",
      " state (4)  A[0]:(0.589874625206) A[1]:(0.655879199505) A[2]:(6.22272491455e-05) A[3]:(0.531780481339)\n",
      " state (5)  A[0]:(0.160644650459) A[1]:(0.928899526596) A[2]:(-0.18950240314) A[3]:(0.521634280682)\n",
      " state (6)  A[0]:(-0.00146907463204) A[1]:(0.80999314785) A[2]:(-9.21487808228e-05) A[3]:(0.656169116497)\n",
      " state (7)  A[0]:(0.629630446434) A[1]:(-0.25114646554) A[2]:(0.287500619888) A[3]:(0.887345314026)\n",
      " state (8)  A[0]:(0.655067324638) A[1]:(9.84147191048e-05) A[2]:(0.728849291801) A[3]:(0.590670704842)\n",
      " state (9)  A[0]:(0.655082583427) A[1]:(0.810023546219) A[2]:(0.809927344322) A[3]:(0.000906482106075)\n",
      " state (10)  A[0]:(0.728228449821) A[1]:(0.900004804134) A[2]:(-0.000430941552622) A[3]:(0.729520916939)\n",
      " state (11)  A[0]:(0.520034730434) A[1]:(0.8768222332) A[2]:(-0.609863758087) A[3]:(0.844363212585)\n",
      " state (12)  A[0]:(0.0761732831597) A[1]:(0.824375092983) A[2]:(-0.586834847927) A[3]:(0.79398214817)\n",
      " state (13)  A[0]:(-0.00152248027734) A[1]:(0.809120893478) A[2]:(0.89999461174) A[3]:(0.729647815228)\n",
      " state (14)  A[0]:(0.80960804224) A[1]:(0.900011897087) A[2]:(0.999999940395) A[3]:(0.810396552086)\n",
      " state (15)  A[0]:(0.984664857388) A[1]:(0.95758831501) A[2]:(1.0) A[3]:(0.881396114826)\n",
      "Episode 646000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5997. Times reached goal: 990.               Steps done: 5039172. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0058312055527.\n",
      " state (0)  A[0]:(0.530751883984) A[1]:(0.590178251266) A[2]:(0.590518951416) A[3]:(0.53155207634)\n",
      " state (1)  A[0]:(0.530943095684) A[1]:(-0.000237785279751) A[2]:(0.655899763107) A[3]:(0.590387523174)\n",
      " state (2)  A[0]:(0.59007537365) A[1]:(0.728870153427) A[2]:(0.590362668037) A[3]:(0.656031012535)\n",
      " state (3)  A[0]:(0.655569374561) A[1]:(-0.21760378778) A[2]:(0.537356615067) A[3]:(0.519353747368)\n",
      " state (4)  A[0]:(0.589764773846) A[1]:(0.655940175056) A[2]:(-4.38690185547e-05) A[3]:(0.531433045864)\n",
      " state (5)  A[0]:(0.160912513733) A[1]:(0.928939938545) A[2]:(-0.189842790365) A[3]:(0.521289646626)\n",
      " state (6)  A[0]:(-0.000818192784209) A[1]:(0.810017585754) A[2]:(-0.000358462304575) A[3]:(0.65590518713)\n",
      " state (7)  A[0]:(0.630012154579) A[1]:(-0.251198381186) A[2]:(0.287529051304) A[3]:(0.887207627296)\n",
      " state (8)  A[0]:(0.655478954315) A[1]:(6.33373856544e-05) A[2]:(0.728754043579) A[3]:(0.590294003487)\n",
      " state (9)  A[0]:(0.655631542206) A[1]:(0.80999648571) A[2]:(0.809910655022) A[3]:(-0.000139743089676)\n",
      " state (10)  A[0]:(0.728655099869) A[1]:(0.899988234043) A[2]:(-0.000307679176331) A[3]:(0.728807330132)\n",
      " state (11)  A[0]:(0.520613193512) A[1]:(0.876795589924) A[2]:(-0.609841227531) A[3]:(0.843881428242)\n",
      " state (12)  A[0]:(0.0768536254764) A[1]:(0.82433116436) A[2]:(-0.586932420731) A[3]:(0.793350100517)\n",
      " state (13)  A[0]:(-0.000921189552173) A[1]:(0.809070944786) A[2]:(0.899988532066) A[3]:(0.728865504265)\n",
      " state (14)  A[0]:(0.80981618166) A[1]:(0.899989545345) A[2]:(0.999999940395) A[3]:(0.809869408607)\n",
      " state (15)  A[0]:(0.984675645828) A[1]:(0.95757496357) A[2]:(1.0) A[3]:(0.881070315838)\n",
      "Episode 647000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6020. Times reached goal: 993.               Steps done: 5045192. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00579620714617.\n",
      " state (0)  A[0]:(0.531809806824) A[1]:(0.590593755245) A[2]:(0.590657234192) A[3]:(0.531558275223)\n",
      " state (1)  A[0]:(0.531783223152) A[1]:(-0.000131413340569) A[2]:(0.656143784523) A[3]:(0.590625107288)\n",
      " state (2)  A[0]:(0.590834498405) A[1]:(0.728984713554) A[2]:(0.590548336506) A[3]:(0.656363129616)\n",
      " state (3)  A[0]:(0.65639936924) A[1]:(-0.217438235879) A[2]:(0.537308216095) A[3]:(0.519482254982)\n",
      " state (4)  A[0]:(0.590672969818) A[1]:(0.656128644943) A[2]:(-0.000130295753479) A[3]:(0.531579494476)\n",
      " state (5)  A[0]:(0.162043750286) A[1]:(0.928924620152) A[2]:(-0.1896828264) A[3]:(0.521592617035)\n",
      " state (6)  A[0]:(0.000251233577728) A[1]:(0.809953570366) A[2]:(-7.29560852051e-05) A[3]:(0.656271576881)\n",
      " state (7)  A[0]:(0.630838394165) A[1]:(-0.251380324364) A[2]:(0.287895143032) A[3]:(0.88739079237)\n",
      " state (8)  A[0]:(0.656119585037) A[1]:(-0.0001490265131) A[2]:(0.728913128376) A[3]:(0.590704739094)\n",
      " state (9)  A[0]:(0.655923366547) A[1]:(0.809943675995) A[2]:(0.809940516949) A[3]:(0.000101283192635)\n",
      " state (10)  A[0]:(0.728815674782) A[1]:(0.899966180325) A[2]:(-0.000420451135142) A[3]:(0.728918254375)\n",
      " state (11)  A[0]:(0.520853459835) A[1]:(0.87679451704) A[2]:(-0.610039114952) A[3]:(0.84397149086)\n",
      " state (12)  A[0]:(0.0771474540234) A[1]:(0.824385941029) A[2]:(-0.587265312672) A[3]:(0.793482780457)\n",
      " state (13)  A[0]:(-0.000745445373468) A[1]:(0.809217214584) A[2]:(0.899939358234) A[3]:(0.729030191898)\n",
      " state (14)  A[0]:(0.809790968895) A[1]:(0.900140821934) A[2]:(0.999999940395) A[3]:(0.80997210741)\n",
      " state (15)  A[0]:(0.98465192318) A[1]:(0.957664966583) A[2]:(1.0) A[3]:(0.881086528301)\n",
      "Episode 648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6023. Times reached goal: 996.               Steps done: 5051215. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00576140151291.\n",
      " state (0)  A[0]:(0.531656563282) A[1]:(0.590522587299) A[2]:(0.590479016304) A[3]:(0.531884253025)\n",
      " state (1)  A[0]:(0.531574726105) A[1]:(-0.000400043994887) A[2]:(0.656046509743) A[3]:(0.590558409691)\n",
      " state (2)  A[0]:(0.590539216995) A[1]:(0.729104936123) A[2]:(0.590879321098) A[3]:(0.656029939651)\n",
      " state (3)  A[0]:(0.656084299088) A[1]:(-0.218056708574) A[2]:(0.537812948227) A[3]:(0.518940031528)\n",
      " state (4)  A[0]:(0.59018433094) A[1]:(0.656319022179) A[2]:(0.000289678573608) A[3]:(0.530962109566)\n",
      " state (5)  A[0]:(0.161108210683) A[1]:(0.928968667984) A[2]:(-0.189387589693) A[3]:(0.520801961422)\n",
      " state (6)  A[0]:(-0.000580161751714) A[1]:(0.810046553612) A[2]:(0.000199556350708) A[3]:(0.655565202236)\n",
      " state (7)  A[0]:(0.630585372448) A[1]:(-0.250849843025) A[2]:(0.28820297122) A[3]:(0.887067139149)\n",
      " state (8)  A[0]:(0.656305074692) A[1]:(0.000758938316721) A[2]:(0.729086220264) A[3]:(0.589365422726)\n",
      " state (9)  A[0]:(0.656646490097) A[1]:(0.810225009918) A[2]:(0.810031175613) A[3]:(-0.00215956242755)\n",
      " state (10)  A[0]:(0.729543685913) A[1]:(0.900068223476) A[2]:(-3.91006469727e-05) A[3]:(0.727795600891)\n",
      " state (11)  A[0]:(0.522032439709) A[1]:(0.876857280731) A[2]:(-0.609799742699) A[3]:(0.843270540237)\n",
      " state (12)  A[0]:(0.0788006410003) A[1]:(0.824385046959) A[2]:(-0.587074041367) A[3]:(0.792615890503)\n",
      " state (13)  A[0]:(0.00101044739131) A[1]:(0.809116065502) A[2]:(0.900088191032) A[3]:(0.728021025658)\n",
      " state (14)  A[0]:(0.810462832451) A[1]:(0.900028645992) A[2]:(0.999999940395) A[3]:(0.809328794479)\n",
      " state (15)  A[0]:(0.984700739384) A[1]:(0.957576274872) A[2]:(1.0) A[3]:(0.880693793297)\n",
      "Episode 649000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6016. Times reached goal: 992.               Steps done: 5057231. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00572684497171.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5902,  0.5903,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0000,  0.6559,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7289,  0.5903,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8099,  0.0000,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0003,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? True\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53149586916) A[1]:(0.590175747871) A[2]:(0.590337991714) A[3]:(0.531382203102)\n",
      " state (1)  A[0]:(0.531503796577) A[1]:(-4.60967421532e-05) A[2]:(0.655918419361) A[3]:(0.59040158987)\n",
      " state (2)  A[0]:(0.59067261219) A[1]:(0.728859901428) A[2]:(0.590254426003) A[3]:(0.656122267246)\n",
      " state (3)  A[0]:(0.656276464462) A[1]:(-0.218311712146) A[2]:(0.537299990654) A[3]:(0.519363939762)\n",
      " state (4)  A[0]:(0.590653538704) A[1]:(0.655972480774) A[2]:(-0.000149846076965) A[3]:(0.531531751156)\n",
      " state (5)  A[0]:(0.162153333426) A[1]:(0.928889989853) A[2]:(-0.189709842205) A[3]:(0.521440029144)\n",
      " state (6)  A[0]:(0.000265538692474) A[1]:(0.809904813766) A[2]:(-1.8835067749e-05) A[3]:(0.655959665775)\n",
      " state (7)  A[0]:(0.630728006363) A[1]:(-0.251409471035) A[2]:(0.288143932819) A[3]:(0.887199878693)\n",
      " state (8)  A[0]:(0.656239628792) A[1]:(-0.000161997973919) A[2]:(0.728844046593) A[3]:(0.590577840805)\n",
      " state (9)  A[0]:(0.65621227026) A[1]:(0.809979856014) A[2]:(0.809881746769) A[3]:(0.000329151749611)\n",
      " state (10)  A[0]:(0.729126572609) A[1]:(0.899993419647) A[2]:(-0.000398278207285) A[3]:(0.729072988033)\n",
      " state (11)  A[0]:(0.521430492401) A[1]:(0.876830041409) A[2]:(-0.610115408897) A[3]:(0.844068050385)\n",
      " state (12)  A[0]:(0.0780068188906) A[1]:(0.824442386627) A[2]:(-0.587497532368) A[3]:(0.793578267097)\n",
      " state (13)  A[0]:(0.00022280216217) A[1]:(0.809299588203) A[2]:(0.900019109249) A[3]:(0.729104220867)\n",
      " state (14)  A[0]:(0.810222327709) A[1]:(0.900215089321) A[2]:(0.999999940395) A[3]:(0.810000240803)\n",
      " state (15)  A[0]:(0.984669625759) A[1]:(0.957688391209) A[2]:(1.0) A[3]:(0.881017804146)\n",
      "Episode 650000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6005. Times reached goal: 994.               Steps done: 5063236. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00569255831637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531444907188) A[1]:(0.590415716171) A[2]:(0.590549707413) A[3]:(0.531566560268)\n",
      " state (1)  A[0]:(0.531078338623) A[1]:(-0.000378817290766) A[2]:(0.656128048897) A[3]:(0.590431690216)\n",
      " state (2)  A[0]:(0.590140342712) A[1]:(0.728779554367) A[2]:(0.590673685074) A[3]:(0.656138181686)\n",
      " state (3)  A[0]:(0.655834615231) A[1]:(-0.219526693225) A[2]:(0.53767824173) A[3]:(0.51935249567)\n",
      " state (4)  A[0]:(0.590108394623) A[1]:(0.655932188034) A[2]:(7.48634338379e-05) A[3]:(0.531771600246)\n",
      " state (5)  A[0]:(0.161137148738) A[1]:(0.928845345974) A[2]:(-0.189353570342) A[3]:(0.521810650826)\n",
      " state (6)  A[0]:(-0.000973641581368) A[1]:(0.809819936752) A[2]:(0.000501155795064) A[3]:(0.656217813492)\n",
      " state (7)  A[0]:(0.630264401436) A[1]:(-0.251988202333) A[2]:(0.288932442665) A[3]:(0.887296199799)\n",
      " state (8)  A[0]:(0.65587747097) A[1]:(-0.00137156166602) A[2]:(0.729075551033) A[3]:(0.591078221798)\n",
      " state (9)  A[0]:(0.655511677265) A[1]:(0.809670269489) A[2]:(0.810008764267) A[3]:(0.00046698746155)\n",
      " state (10)  A[0]:(0.728549778461) A[1]:(0.899932980537) A[2]:(-0.000456213922007) A[3]:(0.729259669781)\n",
      " state (11)  A[0]:(0.520601332188) A[1]:(0.87686663866) A[2]:(-0.610443711281) A[3]:(0.844298303127)\n",
      " state (12)  A[0]:(0.0768152922392) A[1]:(0.824639201164) A[2]:(-0.587974190712) A[3]:(0.793911576271)\n",
      " state (13)  A[0]:(-0.00114467693493) A[1]:(0.80968940258) A[2]:(0.899994969368) A[3]:(0.729469060898)\n",
      " state (14)  A[0]:(0.809632360935) A[1]:(0.900542497635) A[2]:(0.999999940395) A[3]:(0.810138225555)\n",
      " state (15)  A[0]:(0.984586119652) A[1]:(0.957868695259) A[2]:(1.0) A[3]:(0.880962431431)\n",
      "Episode 651000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               5997. Times reached goal: 991.               Steps done: 5069233. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00565852220343.\n",
      " state (0)  A[0]:(0.528933763504) A[1]:(0.590020775795) A[2]:(0.590450048447) A[3]:(0.530811071396)\n",
      " state (1)  A[0]:(0.529268801212) A[1]:(-0.000421225995524) A[2]:(0.655895829201) A[3]:(0.589725375175)\n",
      " state (2)  A[0]:(0.588841676712) A[1]:(0.728796184063) A[2]:(0.590232372284) A[3]:(0.655550420284)\n",
      " state (3)  A[0]:(0.654761791229) A[1]:(-0.217276886106) A[2]:(0.537408828735) A[3]:(0.518488287926)\n",
      " state (4)  A[0]:(0.589315474033) A[1]:(0.655932426453) A[2]:(0.000280141830444) A[3]:(0.530747175217)\n",
      " state (5)  A[0]:(0.161185219884) A[1]:(0.928903102875) A[2]:(-0.189417347312) A[3]:(0.521152019501)\n",
      " state (6)  A[0]:(0.00119534076657) A[1]:(0.809746682644) A[2]:(0.000571012438741) A[3]:(0.656225204468)\n",
      " state (7)  A[0]:(0.632348954678) A[1]:(-0.251735746861) A[2]:(0.28893199563) A[3]:(0.887409985065)\n",
      " state (8)  A[0]:(0.658501386642) A[1]:(0.000224173069) A[2]:(0.729016661644) A[3]:(0.592026352882)\n",
      " state (9)  A[0]:(0.659183382988) A[1]:(0.810034632683) A[2]:(0.810070157051) A[3]:(0.00343065103516)\n",
      " state (10)  A[0]:(0.73163831234) A[1]:(0.899912416935) A[2]:(0.00108277751133) A[3]:(0.730302095413)\n",
      " state (11)  A[0]:(0.525402069092) A[1]:(0.87660574913) A[2]:(-0.609032928944) A[3]:(0.844765663147)\n",
      " state (12)  A[0]:(0.0834333822131) A[1]:(0.823940515518) A[2]:(-0.586969137192) A[3]:(0.794516324997)\n",
      " state (13)  A[0]:(0.00493894983083) A[1]:(0.808462560177) A[2]:(0.899574160576) A[3]:(0.730287432671)\n",
      " state (14)  A[0]:(0.811406612396) A[1]:(0.89955830574) A[2]:(0.999999940395) A[3]:(0.810836493969)\n",
      " state (15)  A[0]:(0.984780430794) A[1]:(0.957348227501) A[2]:(1.0) A[3]:(0.88164305687)\n",
      "Episode 652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6030. Times reached goal: 997.               Steps done: 5075263. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00562450398256.\n",
      " state (0)  A[0]:(0.531607866287) A[1]:(0.590294599533) A[2]:(0.590358674526) A[3]:(0.530986070633)\n",
      " state (1)  A[0]:(0.531639933586) A[1]:(-0.000272989273071) A[2]:(0.655894100666) A[3]:(0.590245723724)\n",
      " state (2)  A[0]:(0.590680956841) A[1]:(0.72876739502) A[2]:(0.590208292007) A[3]:(0.656065821648)\n",
      " state (3)  A[0]:(0.656170725822) A[1]:(-0.217799529433) A[2]:(0.537122488022) A[3]:(0.519333839417)\n",
      " state (4)  A[0]:(0.59041839838) A[1]:(0.655642032623) A[2]:(-0.000452995271189) A[3]:(0.53154528141)\n",
      " state (5)  A[0]:(0.161774411798) A[1]:(0.928821921349) A[2]:(-0.190322622657) A[3]:(0.52169251442)\n",
      " state (6)  A[0]:(-0.000271618366241) A[1]:(0.809757113457) A[2]:(-0.000696897390299) A[3]:(0.656180679798)\n",
      " state (7)  A[0]:(0.630133450031) A[1]:(-0.251717209816) A[2]:(0.287698447704) A[3]:(0.887215733528)\n",
      " state (8)  A[0]:(0.655777871609) A[1]:(-0.000620782317128) A[2]:(0.728292405605) A[3]:(0.591069817543)\n",
      " state (9)  A[0]:(0.655726909637) A[1]:(0.809776186943) A[2]:(0.809510886669) A[3]:(0.000647872569971)\n",
      " state (10)  A[0]:(0.728645920753) A[1]:(0.899862289429) A[2]:(-0.00119340361562) A[3]:(0.728905677795)\n",
      " state (11)  A[0]:(0.520664691925) A[1]:(0.876639246941) A[2]:(-0.610799908638) A[3]:(0.843917489052)\n",
      " state (12)  A[0]:(0.0768985673785) A[1]:(0.824124157429) A[2]:(-0.588594079018) A[3]:(0.793372511864)\n",
      " state (13)  A[0]:(-0.00108188344166) A[1]:(0.808882117271) A[2]:(0.899737417698) A[3]:(0.72883105278)\n",
      " state (14)  A[0]:(0.80973315239) A[1]:(0.899943828583) A[2]:(0.999999940395) A[3]:(0.809818446636)\n",
      " state (15)  A[0]:(0.984598040581) A[1]:(0.957531809807) A[2]:(1.0) A[3]:(0.880854070187)\n",
      "Episode 653000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6017. Times reached goal: 994.               Steps done: 5081280. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00559076295378.\n",
      " state (0)  A[0]:(0.530770421028) A[1]:(0.590638399124) A[2]:(0.590495824814) A[3]:(0.532814800739)\n",
      " state (1)  A[0]:(0.530847668648) A[1]:(0.000101178884506) A[2]:(0.656150460243) A[3]:(0.591401934624)\n",
      " state (2)  A[0]:(0.590057253838) A[1]:(0.729040026665) A[2]:(0.590534448624) A[3]:(0.656797528267)\n",
      " state (3)  A[0]:(0.655863046646) A[1]:(-0.217460602522) A[2]:(0.537535190582) A[3]:(0.519596457481)\n",
      " state (4)  A[0]:(0.59020870924) A[1]:(0.656118035316) A[2]:(0.000126361846924) A[3]:(0.531413197517)\n",
      " state (5)  A[0]:(0.161489769816) A[1]:(0.928908526897) A[2]:(-0.189648926258) A[3]:(0.521309614182)\n",
      " state (6)  A[0]:(-0.000509232224431) A[1]:(0.810020089149) A[2]:(4.19616699219e-05) A[3]:(0.655665159225)\n",
      " state (7)  A[0]:(0.630301594734) A[1]:(-0.250916659832) A[2]:(0.288570731878) A[3]:(0.886944890022)\n",
      " state (8)  A[0]:(0.656090259552) A[1]:(0.000237822532654) A[2]:(0.728885412216) A[3]:(0.590108156204)\n",
      " state (9)  A[0]:(0.656054496765) A[1]:(0.810115218163) A[2]:(0.80989921093) A[3]:(-0.000417590112193)\n",
      " state (10)  A[0]:(0.728886365891) A[1]:(0.900074124336) A[2]:(-0.000357508630259) A[3]:(0.728588402271)\n",
      " state (11)  A[0]:(0.520973086357) A[1]:(0.876935184002) A[2]:(-0.610416054726) A[3]:(0.843736410141)\n",
      " state (12)  A[0]:(0.0771956220269) A[1]:(0.824594914913) A[2]:(-0.588221848011) A[3]:(0.793097615242)\n",
      " state (13)  A[0]:(-0.000877827173099) A[1]:(0.809477210045) A[2]:(0.900036811829) A[3]:(0.728417873383)\n",
      " state (14)  A[0]:(0.809748649597) A[1]:(0.900334596634) A[2]:(0.999999940395) A[3]:(0.809473335743)\n",
      " state (15)  A[0]:(0.984565615654) A[1]:(0.957709431648) A[2]:(1.0) A[3]:(0.880531609058)\n",
      "Episode 654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 992.               Steps done: 5087289. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00555726879318.\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.5902,  0.5904,  0.5301]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5318, -0.0001,  0.6558,  0.5887]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7291,  0.5903,  0.6540]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0008,  0.8102, -0.0008,  0.6514]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7273,  0.9000, -0.0021,  0.7226]], device='cuda:0')\n",
      "On state=10, selected action=0 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6548,  0.8099,  0.8100, -0.0144]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7288,  0.9001, -0.0010,  0.7225]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.9005,  1.0000,  0.8058]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530538439751) A[1]:(0.590266346931) A[2]:(0.590170860291) A[3]:(0.529519319534)\n",
      " state (1)  A[0]:(0.531212806702) A[1]:(0.000577725411858) A[2]:(0.655935049057) A[3]:(0.587728738785)\n",
      " state (2)  A[0]:(0.590730428696) A[1]:(0.72894769907) A[2]:(0.590133965015) A[3]:(0.653168439865)\n",
      " state (3)  A[0]:(0.655999302864) A[1]:(-0.217636823654) A[2]:(0.537007927895) A[3]:(0.514253854752)\n",
      " state (4)  A[0]:(0.590737223625) A[1]:(0.655927062035) A[2]:(-0.00100719893817) A[3]:(0.525795698166)\n",
      " state (5)  A[0]:(0.163533627987) A[1]:(0.928950190544) A[2]:(-0.191262036562) A[3]:(0.51549744606)\n",
      " state (6)  A[0]:(0.00306694745086) A[1]:(0.809984624386) A[2]:(-0.00153183820657) A[3]:(0.650865316391)\n",
      " state (7)  A[0]:(0.632645726204) A[1]:(-0.251406490803) A[2]:(0.287701487541) A[3]:(0.884779751301)\n",
      " state (8)  A[0]:(0.657511472702) A[1]:(-0.000584326626267) A[2]:(0.728700518608) A[3]:(0.582245349884)\n",
      " state (9)  A[0]:(0.657070577145) A[1]:(0.809825062752) A[2]:(0.809925377369) A[3]:(-0.013587532565)\n",
      " state (10)  A[0]:(0.730134367943) A[1]:(0.900006771088) A[2]:(-0.00141477491707) A[3]:(0.723081231117)\n",
      " state (11)  A[0]:(0.523446917534) A[1]:(0.876930236816) A[2]:(-0.61168718338) A[3]:(0.840727984905)\n",
      " state (12)  A[0]:(0.0808851718903) A[1]:(0.82466429472) A[2]:(-0.58983540535) A[3]:(0.789411783218)\n",
      " state (13)  A[0]:(0.00290893693455) A[1]:(0.809640526772) A[2]:(0.899580538273) A[3]:(0.723869025707)\n",
      " state (14)  A[0]:(0.811016857624) A[1]:(0.900488674641) A[2]:(0.999999940395) A[3]:(0.80624204874)\n",
      " state (15)  A[0]:(0.984665572643) A[1]:(0.957801222801) A[2]:(1.0) A[3]:(0.878471791744)\n",
      "Episode 655000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6020. Times reached goal: 994.               Steps done: 5093309. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0055239145321.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531237959862) A[1]:(0.590489923954) A[2]:(0.590475022793) A[3]:(0.531450033188)\n",
      " state (1)  A[0]:(0.531209647655) A[1]:(0.000138364732265) A[2]:(0.656099915504) A[3]:(0.59047973156)\n",
      " state (2)  A[0]:(0.590266704559) A[1]:(0.728990852833) A[2]:(0.590766429901) A[3]:(0.656172633171)\n",
      " state (3)  A[0]:(0.655952811241) A[1]:(-0.21929192543) A[2]:(0.537827134132) A[3]:(0.519032359123)\n",
      " state (4)  A[0]:(0.59025144577) A[1]:(0.656085133553) A[2]:(-4.91142272949e-05) A[3]:(0.531442284584)\n",
      " state (5)  A[0]:(0.161507502198) A[1]:(0.928929805756) A[2]:(-0.18991599977) A[3]:(0.521717905998)\n",
      " state (6)  A[0]:(-0.000316172838211) A[1]:(0.809995770454) A[2]:(1.28746032715e-05) A[3]:(0.65612757206)\n",
      " state (7)  A[0]:(0.630287587643) A[1]:(-0.250977009535) A[2]:(0.288961052895) A[3]:(0.887048244476)\n",
      " state (8)  A[0]:(0.655917882919) A[1]:(0.000168733298779) A[2]:(0.728995919228) A[3]:(0.590435564518)\n",
      " state (9)  A[0]:(0.655842065811) A[1]:(0.810012102127) A[2]:(0.809991240501) A[3]:(1.51544809341e-05)\n",
      " state (10)  A[0]:(0.728793978691) A[1]:(0.900001227856) A[2]:(-0.000110030174255) A[3]:(0.728933215141)\n",
      " state (11)  A[0]:(0.521015942097) A[1]:(0.876820206642) A[2]:(-0.610426068306) A[3]:(0.844043850899)\n",
      " state (12)  A[0]:(0.077462092042) A[1]:(0.824382007122) A[2]:(-0.588421344757) A[3]:(0.79356867075)\n",
      " state (13)  A[0]:(-0.000419825286372) A[1]:(0.809179782867) A[2]:(0.900073170662) A[3]:(0.729070901871)\n",
      " state (14)  A[0]:(0.809990286827) A[1]:(0.900135815144) A[2]:(0.999999940395) A[3]:(0.809963881969)\n",
      " state (15)  A[0]:(0.984572410583) A[1]:(0.95758831501) A[2]:(1.0) A[3]:(0.880806088448)\n",
      "Episode 656000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6016. Times reached goal: 991.               Steps done: 5099325. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00549078242359.\n",
      " state (0)  A[0]:(0.531066536903) A[1]:(0.590452313423) A[2]:(0.590394556522) A[3]:(0.531223356724)\n",
      " state (1)  A[0]:(0.531173706055) A[1]:(2.45347619057e-05) A[2]:(0.655941963196) A[3]:(0.590322375298)\n",
      " state (2)  A[0]:(0.590328812599) A[1]:(0.728904008865) A[2]:(0.590419530869) A[3]:(0.656038880348)\n",
      " state (3)  A[0]:(0.6559009552) A[1]:(-0.218894541264) A[2]:(0.537510871887) A[3]:(0.519067168236)\n",
      " state (4)  A[0]:(0.590271353722) A[1]:(0.656128883362) A[2]:(-0.000390887231333) A[3]:(0.531417608261)\n",
      " state (5)  A[0]:(0.161837875843) A[1]:(0.928935587406) A[2]:(-0.190281838179) A[3]:(0.52160012722)\n",
      " state (6)  A[0]:(0.000336557626724) A[1]:(0.809979081154) A[2]:(-0.000306844711304) A[3]:(0.65599912405)\n",
      " state (7)  A[0]:(0.630836367607) A[1]:(-0.251141220331) A[2]:(0.288608670235) A[3]:(0.88706356287)\n",
      " state (8)  A[0]:(0.656693100929) A[1]:(-0.000300817191601) A[2]:(0.728366971016) A[3]:(0.591501355171)\n",
      " state (9)  A[0]:(0.656568169594) A[1]:(0.809893846512) A[2]:(0.809735059738) A[3]:(0.00123545469251)\n",
      " state (10)  A[0]:(0.729387998581) A[1]:(0.899972975254) A[2]:(-0.000243544578552) A[3]:(0.729259729385)\n",
      " state (11)  A[0]:(0.522051990032) A[1]:(0.876800119877) A[2]:(-0.610509753227) A[3]:(0.844246089458)\n",
      " state (12)  A[0]:(0.0789386853576) A[1]:(0.824360370636) A[2]:(-0.588682174683) A[3]:(0.793836295605)\n",
      " state (13)  A[0]:(0.000785440031905) A[1]:(0.80914491415) A[2]:(0.89988809824) A[3]:(0.729390859604)\n",
      " state (14)  A[0]:(0.810141026974) A[1]:(0.90010458231) A[2]:(0.999999940395) A[3]:(0.810190081596)\n",
      " state (15)  A[0]:(0.984565913677) A[1]:(0.957573831081) A[2]:(1.0) A[3]:(0.88095843792)\n",
      "Episode 657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5994. Times reached goal: 991.               Steps done: 5105319. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00545796911347.\n",
      " state (0)  A[0]:(0.532612204552) A[1]:(0.590639829636) A[2]:(0.590491175652) A[3]:(0.531524300575)\n",
      " state (1)  A[0]:(0.53204035759) A[1]:(-4.75570559502e-05) A[2]:(0.656082034111) A[3]:(0.59062063694)\n",
      " state (2)  A[0]:(0.590693533421) A[1]:(0.728975057602) A[2]:(0.590616464615) A[3]:(0.656391203403)\n",
      " state (3)  A[0]:(0.656010329723) A[1]:(-0.218334138393) A[2]:(0.537750601768) A[3]:(0.519593417645)\n",
      " state (4)  A[0]:(0.58987224102) A[1]:(0.656128406525) A[2]:(9.14335250854e-05) A[3]:(0.531997561455)\n",
      " state (5)  A[0]:(0.160292416811) A[1]:(0.928912937641) A[2]:(-0.18986980617) A[3]:(0.522284030914)\n",
      " state (6)  A[0]:(-0.00256242789328) A[1]:(0.809990108013) A[2]:(-3.67164611816e-05) A[3]:(0.656530976295)\n",
      " state (7)  A[0]:(0.628269672394) A[1]:(-0.250984609127) A[2]:(0.289012521505) A[3]:(0.887214899063)\n",
      " state (8)  A[0]:(0.653619408607) A[1]:(0.000101514160633) A[2]:(0.728975653648) A[3]:(0.591081738472)\n",
      " state (9)  A[0]:(0.653061568737) A[1]:(0.810009181499) A[2]:(0.809964478016) A[3]:(0.000942244834732)\n",
      " state (10)  A[0]:(0.726195573807) A[1]:(0.899982869625) A[2]:(-0.00024402141571) A[3]:(0.729241490364)\n",
      " state (11)  A[0]:(0.516693115234) A[1]:(0.87678706646) A[2]:(-0.610691189766) A[3]:(0.844158470631)\n",
      " state (12)  A[0]:(0.0713506862521) A[1]:(0.824348270893) A[2]:(-0.588868021965) A[3]:(0.793648540974)\n",
      " state (13)  A[0]:(-0.0065435427241) A[1]:(0.809197962284) A[2]:(0.900117754936) A[3]:(0.729177117348)\n",
      " state (14)  A[0]:(0.808009207249) A[1]:(0.900202035904) A[2]:(0.999999940395) A[3]:(0.810179531574)\n",
      " state (15)  A[0]:(0.98437923193) A[1]:(0.957622528076) A[2]:(1.0) A[3]:(0.880980074406)\n",
      "Episode 658000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6002. Times reached goal: 992.               Steps done: 5111321. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00542530849542.\n",
      " state (0)  A[0]:(0.531042098999) A[1]:(0.590121150017) A[2]:(0.590442061424) A[3]:(0.531294882298)\n",
      " state (1)  A[0]:(0.531123757362) A[1]:(3.03387641907e-05) A[2]:(0.655891895294) A[3]:(0.590215265751)\n",
      " state (2)  A[0]:(0.590407192707) A[1]:(0.728855609894) A[2]:(0.590028822422) A[3]:(0.655973851681)\n",
      " state (3)  A[0]:(0.655971288681) A[1]:(-0.218157112598) A[2]:(0.537201046944) A[3]:(0.518852233887)\n",
      " state (4)  A[0]:(0.590451955795) A[1]:(0.656038045883) A[2]:(-0.000436305970652) A[3]:(0.531130194664)\n",
      " state (5)  A[0]:(0.16218817234) A[1]:(0.928913533688) A[2]:(-0.190303787589) A[3]:(0.521420240402)\n",
      " state (6)  A[0]:(8.55922698975e-05) A[1]:(0.809975504875) A[2]:(-0.000327467918396) A[3]:(0.655858933926)\n",
      " state (7)  A[0]:(0.630087137222) A[1]:(-0.251198261976) A[2]:(0.288757711649) A[3]:(0.886982858181)\n",
      " state (8)  A[0]:(0.656140804291) A[1]:(-0.000326246023178) A[2]:(0.72845184803) A[3]:(0.591091752052)\n",
      " state (9)  A[0]:(0.656462907791) A[1]:(0.809890210629) A[2]:(0.809651374817) A[3]:(0.000728934886865)\n",
      " state (10)  A[0]:(0.729273557663) A[1]:(0.899950742722) A[2]:(-0.000737428548746) A[3]:(0.729018330574)\n",
      " state (11)  A[0]:(0.521613955498) A[1]:(0.876785218716) A[2]:(-0.610995292664) A[3]:(0.844041287899)\n",
      " state (12)  A[0]:(0.0779962390661) A[1]:(0.824403941631) A[2]:(-0.58935379982) A[3]:(0.793498337269)\n",
      " state (13)  A[0]:(-0.000284969806671) A[1]:(0.809316933155) A[2]:(0.899860262871) A[3]:(0.728909134865)\n",
      " state (14)  A[0]:(0.809888780117) A[1]:(0.90030580759) A[2]:(0.999999940395) A[3]:(0.80986392498)\n",
      " state (15)  A[0]:(0.984525263309) A[1]:(0.95769071579) A[2]:(1.0) A[3]:(0.880695939064)\n",
      "Episode 659000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6022. Times reached goal: 998.               Steps done: 5117343. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00539273546349.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5904,  0.5906,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314, -0.0002,  0.6561,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7289,  0.5904,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  0.8100,  0.0002,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7294,  0.9000, -0.0001,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9002,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531237363815) A[1]:(0.590398311615) A[2]:(0.590569138527) A[3]:(0.531584978104)\n",
      " state (1)  A[0]:(0.531385540962) A[1]:(-0.00017386674881) A[2]:(0.656079292297) A[3]:(0.590491533279)\n",
      " state (2)  A[0]:(0.590665459633) A[1]:(0.728927969933) A[2]:(0.590327382088) A[3]:(0.656227827072)\n",
      " state (3)  A[0]:(0.656504809856) A[1]:(-0.217356652021) A[2]:(0.537441134453) A[3]:(0.519247710705)\n",
      " state (4)  A[0]:(0.591041505337) A[1]:(0.656225085258) A[2]:(-6.55651092529e-06) A[3]:(0.531506180763)\n",
      " state (5)  A[0]:(0.162941426039) A[1]:(0.928919672966) A[2]:(-0.189861178398) A[3]:(0.521883964539)\n",
      " state (6)  A[0]:(0.00117853225674) A[1]:(0.809961497784) A[2]:(0.000154733657837) A[3]:(0.656354188919)\n",
      " state (7)  A[0]:(0.631135821342) A[1]:(-0.251018106937) A[2]:(0.289377599955) A[3]:(0.887186527252)\n",
      " state (8)  A[0]:(0.657175600529) A[1]:(0.000235885381699) A[2]:(0.729017078876) A[3]:(0.591228723526)\n",
      " state (9)  A[0]:(0.657531499863) A[1]:(0.810067892075) A[2]:(0.810034930706) A[3]:(0.0010150965536)\n",
      " state (10)  A[0]:(0.730280280113) A[1]:(0.900001525879) A[2]:(0.000221729278564) A[3]:(0.729342222214)\n",
      " state (11)  A[0]:(0.523424506187) A[1]:(0.876797318459) A[2]:(-0.610470294952) A[3]:(0.84431707859)\n",
      " state (12)  A[0]:(0.0807512551546) A[1]:(0.824343502522) A[2]:(-0.588974416256) A[3]:(0.793943524361)\n",
      " state (13)  A[0]:(0.00254895724356) A[1]:(0.809142529964) A[2]:(0.899912893772) A[3]:(0.729553937912)\n",
      " state (14)  A[0]:(0.810788750648) A[1]:(0.900134921074) A[2]:(0.999999940395) A[3]:(0.810370743275)\n",
      " state (15)  A[0]:(0.984596014023) A[1]:(0.957580387592) A[2]:(1.0) A[3]:(0.881036698818)\n",
      "Episode 660000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6024. Times reached goal: 997.               Steps done: 5123367. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00536034727622.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531623244286) A[1]:(0.590402364731) A[2]:(0.590465188026) A[3]:(0.531250953674)\n",
      " state (1)  A[0]:(0.531601130962) A[1]:(-4.26843762398e-05) A[2]:(0.656060695648) A[3]:(0.590218901634)\n",
      " state (2)  A[0]:(0.590676188469) A[1]:(0.728957414627) A[2]:(0.590693831444) A[3]:(0.655925273895)\n",
      " state (3)  A[0]:(0.656255364418) A[1]:(-0.219197645783) A[2]:(0.537902474403) A[3]:(0.518713235855)\n",
      " state (4)  A[0]:(0.590616703033) A[1]:(0.65604865551) A[2]:(6.74724578857e-05) A[3]:(0.531115889549)\n",
      " state (5)  A[0]:(0.162138149142) A[1]:(0.928916990757) A[2]:(-0.189992636442) A[3]:(0.521423697472)\n",
      " state (6)  A[0]:(0.000163555145264) A[1]:(0.809965312481) A[2]:(-1.91926956177e-05) A[3]:(0.655809938908)\n",
      " state (7)  A[0]:(0.630324482918) A[1]:(-0.251097470522) A[2]:(0.289343178272) A[3]:(0.886876940727)\n",
      " state (8)  A[0]:(0.65594959259) A[1]:(7.2680413723e-05) A[2]:(0.728922069073) A[3]:(0.590389072895)\n",
      " state (9)  A[0]:(0.655736684799) A[1]:(0.810019373894) A[2]:(0.809932112694) A[3]:(-2.27987766266e-06)\n",
      " state (10)  A[0]:(0.72862637043) A[1]:(0.899986624718) A[2]:(-0.000265717506409) A[3]:(0.728865385056)\n",
      " state (11)  A[0]:(0.520771026611) A[1]:(0.876795351505) A[2]:(-0.610929369926) A[3]:(0.843975186348)\n",
      " state (12)  A[0]:(0.0771273970604) A[1]:(0.824378728867) A[2]:(-0.589448928833) A[3]:(0.79342186451)\n",
      " state (13)  A[0]:(-0.000827222887892) A[1]:(0.809260010719) A[2]:(0.900025725365) A[3]:(0.728865027428)\n",
      " state (14)  A[0]:(0.809804141521) A[1]:(0.900261998177) A[2]:(0.999999940395) A[3]:(0.809946119785)\n",
      " state (15)  A[0]:(0.984492719173) A[1]:(0.957640767097) A[2]:(1.0) A[3]:(0.880753278732)\n",
      "Episode 661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 997.               Steps done: 5129377. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00532822820359.\n",
      " state (0)  A[0]:(0.531284570694) A[1]:(0.590435624123) A[2]:(0.590508103371) A[3]:(0.531420350075)\n",
      " state (1)  A[0]:(0.531385838985) A[1]:(-0.000135406851768) A[2]:(0.656069815159) A[3]:(0.590467691422)\n",
      " state (2)  A[0]:(0.590523123741) A[1]:(0.728941202164) A[2]:(0.590603291988) A[3]:(0.656216740608)\n",
      " state (3)  A[0]:(0.656186580658) A[1]:(-0.21835950017) A[2]:(0.537723779678) A[3]:(0.519211053848)\n",
      " state (4)  A[0]:(0.59051823616) A[1]:(0.656006455421) A[2]:(1.71661376953e-05) A[3]:(0.531584143639)\n",
      " state (5)  A[0]:(0.161955177784) A[1]:(0.928901433945) A[2]:(-0.190028369427) A[3]:(0.521933794022)\n",
      " state (6)  A[0]:(2.74777412415e-05) A[1]:(0.80997210741) A[2]:(2.37226486206e-05) A[3]:(0.656123042107)\n",
      " state (7)  A[0]:(0.630323171616) A[1]:(-0.250919729471) A[2]:(0.289582282305) A[3]:(0.886904537678)\n",
      " state (8)  A[0]:(0.656031489372) A[1]:(1.10417604446e-05) A[2]:(0.729056060314) A[3]:(0.590505480766)\n",
      " state (9)  A[0]:(0.655907869339) A[1]:(0.809940099716) A[2]:(0.810070753098) A[3]:(0.000199228525162)\n",
      " state (10)  A[0]:(0.728918194771) A[1]:(0.899978458881) A[2]:(-5.82933425903e-05) A[3]:(0.72907024622)\n",
      " state (11)  A[0]:(0.521343588829) A[1]:(0.876824855804) A[2]:(-0.610972046852) A[3]:(0.844144940376)\n",
      " state (12)  A[0]:(0.0778985768557) A[1]:(0.8244535923) A[2]:(-0.589615106583) A[3]:(0.793631672859)\n",
      " state (13)  A[0]:(-0.000148773193359) A[1]:(0.80935806036) A[2]:(0.900025129318) A[3]:(0.729097247124)\n",
      " state (14)  A[0]:(0.809990167618) A[1]:(0.900314509869) A[2]:(0.999999940395) A[3]:(0.810108184814)\n",
      " state (15)  A[0]:(0.984493434429) A[1]:(0.957652509212) A[2]:(1.0) A[3]:(0.88082921505)\n",
      "Episode 662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6022. Times reached goal: 993.               Steps done: 5135399. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00529623803243.\n",
      " state (0)  A[0]:(0.53268456459) A[1]:(0.590977787971) A[2]:(0.590655326843) A[3]:(0.533717572689)\n",
      " state (1)  A[0]:(0.532177269459) A[1]:(0.000422343582613) A[2]:(0.656139969826) A[3]:(0.592220544815)\n",
      " state (2)  A[0]:(0.590939164162) A[1]:(0.729256391525) A[2]:(0.590422511101) A[3]:(0.657651901245)\n",
      " state (3)  A[0]:(0.655551075935) A[1]:(-0.217038288713) A[2]:(0.537301301956) A[3]:(0.521074056625)\n",
      " state (4)  A[0]:(0.589301586151) A[1]:(0.656496584415) A[2]:(-0.000450253457529) A[3]:(0.533268928528)\n",
      " state (5)  A[0]:(0.160132780671) A[1]:(0.92895424366) A[2]:(-0.190331816673) A[3]:(0.523484706879)\n",
      " state (6)  A[0]:(-0.00150450947694) A[1]:(0.810030400753) A[2]:(-0.000300526618958) A[3]:(0.657103419304)\n",
      " state (7)  A[0]:(0.629866719246) A[1]:(-0.250877708197) A[2]:(0.289113014936) A[3]:(0.887168169022)\n",
      " state (8)  A[0]:(0.655931949615) A[1]:(0.000193014740944) A[2]:(0.728611707687) A[3]:(0.591627120972)\n",
      " state (9)  A[0]:(0.65576428175) A[1]:(0.810005486012) A[2]:(0.809709072113) A[3]:(0.00221910700202)\n",
      " state (10)  A[0]:(0.72857606411) A[1]:(0.899976432323) A[2]:(-0.000594139040913) A[3]:(0.72966170311)\n",
      " state (11)  A[0]:(0.520613074303) A[1]:(0.876795232296) A[2]:(-0.611179471016) A[3]:(0.844344139099)\n",
      " state (12)  A[0]:(0.0766896754503) A[1]:(0.824385523796) A[2]:(-0.59004676342) A[3]:(0.793775081635)\n",
      " state (13)  A[0]:(-0.00190159445629) A[1]:(0.809227287769) A[2]:(0.899682581425) A[3]:(0.729114294052)\n",
      " state (14)  A[0]:(0.809080839157) A[1]:(0.900196433067) A[2]:(0.999999940395) A[3]:(0.809943795204)\n",
      " state (15)  A[0]:(0.984399616718) A[1]:(0.957592666149) A[2]:(1.0) A[3]:(0.880640506744)\n",
      "Episode 663000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6019. Times reached goal: 996.               Steps done: 5141418. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00526445572053.\n",
      " state (0)  A[0]:(0.53057706356) A[1]:(0.590573906898) A[2]:(0.590531349182) A[3]:(0.531761884689)\n",
      " state (1)  A[0]:(0.530095338821) A[1]:(0.000349506735802) A[2]:(0.656160831451) A[3]:(0.590411424637)\n",
      " state (2)  A[0]:(0.589060783386) A[1]:(0.729109883308) A[2]:(0.590449810028) A[3]:(0.656004846096)\n",
      " state (3)  A[0]:(0.65460062027) A[1]:(-0.217516690493) A[2]:(0.53746265173) A[3]:(0.518543362617)\n",
      " state (4)  A[0]:(0.588339567184) A[1]:(0.656327962875) A[2]:(-0.000229001045227) A[3]:(0.530671536922)\n",
      " state (5)  A[0]:(0.158208921552) A[1]:(0.928928017616) A[2]:(-0.190187975764) A[3]:(0.520894765854)\n",
      " state (6)  A[0]:(-0.00446444749832) A[1]:(0.810098111629) A[2]:(-0.000237226486206) A[3]:(0.655044674873)\n",
      " state (7)  A[0]:(0.627435922623) A[1]:(-0.250578045845) A[2]:(0.289432197809) A[3]:(0.886321663857)\n",
      " state (8)  A[0]:(0.653422415257) A[1]:(0.000261262059212) A[2]:(0.729075670242) A[3]:(0.588154196739)\n",
      " state (9)  A[0]:(0.653357386589) A[1]:(0.810058355331) A[2]:(0.810018777847) A[3]:(-0.00390900159255)\n",
      " state (10)  A[0]:(0.726853787899) A[1]:(0.900055885315) A[2]:(-0.000580668391194) A[3]:(0.727158665657)\n",
      " state (11)  A[0]:(0.518158793449) A[1]:(0.876940310001) A[2]:(-0.611553490162) A[3]:(0.842992722988)\n",
      " state (12)  A[0]:(0.073532268405) A[1]:(0.824645757675) A[2]:(-0.590433597565) A[3]:(0.792161643505)\n",
      " state (13)  A[0]:(-0.00455736927688) A[1]:(0.809603631496) A[2]:(0.899912238121) A[3]:(0.727244615555)\n",
      " state (14)  A[0]:(0.808489441872) A[1]:(0.900470197201) A[2]:(0.999999940395) A[3]:(0.808750331402)\n",
      " state (15)  A[0]:(0.984335005283) A[1]:(0.95771074295) A[2]:(1.0) A[3]:(0.879882216454)\n",
      "Episode 664000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6013. Times reached goal: 993.               Steps done: 5147431. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00523289552909.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5904,  0.5905,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0001,  0.6560,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7290,  0.5903,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8100, -0.0000,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0003,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9003,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531460165977) A[1]:(0.590359568596) A[2]:(0.590456068516) A[3]:(0.531123757362)\n",
      " state (1)  A[0]:(0.531332731247) A[1]:(-0.000117786228657) A[2]:(0.656011044979) A[3]:(0.590144395828)\n",
      " state (2)  A[0]:(0.590486943722) A[1]:(0.729036688805) A[2]:(0.590373337269) A[3]:(0.655902981758)\n",
      " state (3)  A[0]:(0.655951917171) A[1]:(-0.217150300741) A[2]:(0.537514925003) A[3]:(0.518802344799)\n",
      " state (4)  A[0]:(0.590281844139) A[1]:(0.656005144119) A[2]:(8.32080841064e-05) A[3]:(0.531029462814)\n",
      " state (5)  A[0]:(0.161869451404) A[1]:(0.928888976574) A[2]:(-0.190006539226) A[3]:(0.521437168121)\n",
      " state (6)  A[0]:(-0.000172078609467) A[1]:(0.809980154037) A[2]:(3.06367874146e-05) A[3]:(0.655728876591)\n",
      " state (7)  A[0]:(0.62996250391) A[1]:(-0.250915259123) A[2]:(0.289782017469) A[3]:(0.886731207371)\n",
      " state (8)  A[0]:(0.655829906464) A[1]:(0.000147275626659) A[2]:(0.729090213776) A[3]:(0.590198993683)\n",
      " state (9)  A[0]:(0.655829429626) A[1]:(0.810029506683) A[2]:(0.810015499592) A[3]:(0.000149473547935)\n",
      " state (10)  A[0]:(0.728875279427) A[1]:(0.900015890598) A[2]:(-0.000240921974182) A[3]:(0.729071855545)\n",
      " state (11)  A[0]:(0.521350502968) A[1]:(0.876871228218) A[2]:(-0.611256837845) A[3]:(0.844153761864)\n",
      " state (12)  A[0]:(0.0779802203178) A[1]:(0.824533462524) A[2]:(-0.590133607388) A[3]:(0.793654978275)\n",
      " state (13)  A[0]:(-2.54213809967e-05) A[1]:(0.809468865395) A[2]:(0.90007096529) A[3]:(0.729096889496)\n",
      " state (14)  A[0]:(0.810053765774) A[1]:(0.900390207767) A[2]:(0.999999940395) A[3]:(0.810017347336)\n",
      " state (15)  A[0]:(0.984460234642) A[1]:(0.957661926746) A[2]:(1.0) A[3]:(0.880603909492)\n",
      "Episode 665000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6029. Times reached goal: 996.               Steps done: 5153460. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00520144131594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531452417374) A[1]:(0.590774655342) A[2]:(0.590625166893) A[3]:(0.531393170357)\n",
      " state (1)  A[0]:(0.53140604496) A[1]:(4.41819429398e-05) A[2]:(0.656229436398) A[3]:(0.590427339077)\n",
      " state (2)  A[0]:(0.590579390526) A[1]:(0.729141414165) A[2]:(0.590554833412) A[3]:(0.656202554703)\n",
      " state (3)  A[0]:(0.656301856041) A[1]:(-0.216241508722) A[2]:(0.537322580814) A[3]:(0.519302964211)\n",
      " state (4)  A[0]:(0.590616285801) A[1]:(0.656398653984) A[2]:(-0.000356197328074) A[3]:(0.531510829926)\n",
      " state (5)  A[0]:(0.162056431174) A[1]:(0.928949594498) A[2]:(-0.190443485975) A[3]:(0.521896004677)\n",
      " state (6)  A[0]:(0.000137805938721) A[1]:(0.810041964054) A[2]:(-0.0002760887146) A[3]:(0.656021595001)\n",
      " state (7)  A[0]:(0.630418896675) A[1]:(-0.250818908215) A[2]:(0.289817601442) A[3]:(0.88675057888)\n",
      " state (8)  A[0]:(0.656293392181) A[1]:(0.000145807862282) A[2]:(0.729143977165) A[3]:(0.589872002602)\n",
      " state (9)  A[0]:(0.656318426132) A[1]:(0.809971749783) A[2]:(0.810030698776) A[3]:(-0.000746399047785)\n",
      " state (10)  A[0]:(0.729231357574) A[1]:(0.899981081486) A[2]:(-0.000459670991404) A[3]:(0.728724002838)\n",
      " state (11)  A[0]:(0.521744012833) A[1]:(0.876827418804) A[2]:(-0.611573219299) A[3]:(0.843983530998)\n",
      " state (12)  A[0]:(0.0782989636064) A[1]:(0.824473977089) A[2]:(-0.590599060059) A[3]:(0.793468356133)\n",
      " state (13)  A[0]:(0.000179708003998) A[1]:(0.809411764145) A[2]:(0.899976730347) A[3]:(0.72892832756)\n",
      " state (14)  A[0]:(0.81013995409) A[1]:(0.900358080864) A[2]:(0.999999940395) A[3]:(0.809991836548)\n",
      " state (15)  A[0]:(0.984462916851) A[1]:(0.957643270493) A[2]:(1.0) A[3]:(0.880641460419)\n",
      "Episode 666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6017. Times reached goal: 994.               Steps done: 5159477. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00517023821222.\n",
      " state (0)  A[0]:(0.531685709953) A[1]:(0.590640544891) A[2]:(0.590656280518) A[3]:(0.53144466877)\n",
      " state (1)  A[0]:(0.531681537628) A[1]:(-0.000259712338448) A[2]:(0.656074881554) A[3]:(0.590281784534)\n",
      " state (2)  A[0]:(0.590819478035) A[1]:(0.729036927223) A[2]:(0.59057867527) A[3]:(0.656029820442)\n",
      " state (3)  A[0]:(0.656435489655) A[1]:(-0.217110648751) A[2]:(0.537607014179) A[3]:(0.519126772881)\n",
      " state (4)  A[0]:(0.590770959854) A[1]:(0.656125664711) A[2]:(2.41994857788e-05) A[3]:(0.531502366066)\n",
      " state (5)  A[0]:(0.162345215678) A[1]:(0.928923487663) A[2]:(-0.190151900053) A[3]:(0.522021174431)\n",
      " state (6)  A[0]:(0.000434100598795) A[1]:(0.809989333153) A[2]:(1.02519989014e-05) A[3]:(0.656127691269)\n",
      " state (7)  A[0]:(0.630488753319) A[1]:(-0.251105695963) A[2]:(0.289994716644) A[3]:(0.886780142784)\n",
      " state (8)  A[0]:(0.656252384186) A[1]:(-0.000225111842155) A[2]:(0.728957414627) A[3]:(0.590430855751)\n",
      " state (9)  A[0]:(0.656008660793) A[1]:(0.809911906719) A[2]:(0.809947729111) A[3]:(-7.42077827454e-06)\n",
      " state (10)  A[0]:(0.728920459747) A[1]:(0.899975836277) A[2]:(-0.000516891421285) A[3]:(0.728998064995)\n",
      " state (11)  A[0]:(0.521307110786) A[1]:(0.876830875874) A[2]:(-0.611703872681) A[3]:(0.844150841236)\n",
      " state (12)  A[0]:(0.0777387693524) A[1]:(0.824481666088) A[2]:(-0.590979337692) A[3]:(0.793663322926)\n",
      " state (13)  A[0]:(-0.000542163790669) A[1]:(0.809405326843) A[2]:(0.899724304676) A[3]:(0.72910130024)\n",
      " state (14)  A[0]:(0.809776425362) A[1]:(0.900340795517) A[2]:(0.999999940395) A[3]:(0.810019314289)\n",
      " state (15)  A[0]:(0.98442864418) A[1]:(0.957637369633) A[2]:(1.0) A[3]:(0.880617976189)\n",
      "Episode 667000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6009. Times reached goal: 997.               Steps done: 5165486. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00513926340781.\n",
      " state (0)  A[0]:(0.531594514847) A[1]:(0.590392351151) A[2]:(0.590444207191) A[3]:(0.531338572502)\n",
      " state (1)  A[0]:(0.5314027071) A[1]:(0.00012119114399) A[2]:(0.656058371067) A[3]:(0.59033614397)\n",
      " state (2)  A[0]:(0.590469241142) A[1]:(0.728835940361) A[2]:(0.59041082859) A[3]:(0.656078100204)\n",
      " state (3)  A[0]:(0.655983090401) A[1]:(-0.217443063855) A[2]:(0.537491559982) A[3]:(0.519060015678)\n",
      " state (4)  A[0]:(0.590248882771) A[1]:(0.655738055706) A[2]:(-0.000100612640381) A[3]:(0.531301736832)\n",
      " state (5)  A[0]:(0.161734059453) A[1]:(0.928868949413) A[2]:(-0.190361917019) A[3]:(0.521799266338)\n",
      " state (6)  A[0]:(-4.73856925964e-05) A[1]:(0.809793353081) A[2]:(-3.74317169189e-05) A[3]:(0.656014382839)\n",
      " state (7)  A[0]:(0.629745006561) A[1]:(-0.251440137625) A[2]:(0.290156394243) A[3]:(0.886708140373)\n",
      " state (8)  A[0]:(0.655451893806) A[1]:(-0.000215902924538) A[2]:(0.729056477547) A[3]:(0.59020447731)\n",
      " state (9)  A[0]:(0.655571937561) A[1]:(0.809871196747) A[2]:(0.810063242912) A[3]:(-0.000119835138321)\n",
      " state (10)  A[0]:(0.728758096695) A[1]:(0.899937272072) A[2]:(-5.14984130859e-05) A[3]:(0.728911399841)\n",
      " state (11)  A[0]:(0.521245956421) A[1]:(0.876768708229) A[2]:(-0.611395597458) A[3]:(0.844050765038)\n",
      " state (12)  A[0]:(0.0778733715415) A[1]:(0.824367344379) A[2]:(-0.590571403503) A[3]:(0.793477654457)\n",
      " state (13)  A[0]:(-9.84668731689e-05) A[1]:(0.809250473976) A[2]:(0.900066196918) A[3]:(0.728834867477)\n",
      " state (14)  A[0]:(0.810037255287) A[1]:(0.900234878063) A[2]:(0.999999940395) A[3]:(0.809837758541)\n",
      " state (15)  A[0]:(0.984432697296) A[1]:(0.957556664944) A[2]:(1.0) A[3]:(0.880449771881)\n",
      "Episode 668000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6012. Times reached goal: 992.               Steps done: 5171498. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00510845884749.\n",
      " state (0)  A[0]:(0.530750751495) A[1]:(0.590174138546) A[2]:(0.590434551239) A[3]:(0.531434714794)\n",
      " state (1)  A[0]:(0.530358672142) A[1]:(-0.00101294333581) A[2]:(0.655926823616) A[3]:(0.590249538422)\n",
      " state (2)  A[0]:(0.589794874191) A[1]:(0.728567779064) A[2]:(0.590412378311) A[3]:(0.655987620354)\n",
      " state (3)  A[0]:(0.656282782555) A[1]:(-0.220443993807) A[2]:(0.537775456905) A[3]:(0.518990159035)\n",
      " state (4)  A[0]:(0.591121852398) A[1]:(0.656368374825) A[2]:(-0.000184774398804) A[3]:(0.531729757786)\n",
      " state (5)  A[0]:(0.162927538157) A[1]:(0.928906857967) A[2]:(-0.189854294062) A[3]:(0.522178053856)\n",
      " state (6)  A[0]:(0.000913440948352) A[1]:(0.809868097305) A[2]:(0.00076878053369) A[3]:(0.656165957451)\n",
      " state (7)  A[0]:(0.63100361824) A[1]:(-0.252075642347) A[2]:(0.291233122349) A[3]:(0.886824965477)\n",
      " state (8)  A[0]:(0.656797111034) A[1]:(-0.00195838999934) A[2]:(0.729200124741) A[3]:(0.590953230858)\n",
      " state (9)  A[0]:(0.656265735626) A[1]:(0.809539973736) A[2]:(0.809872567654) A[3]:(0.000289812684059)\n",
      " state (10)  A[0]:(0.72912466526) A[1]:(0.899998545647) A[2]:(-0.00170409516431) A[3]:(0.729327559471)\n",
      " state (11)  A[0]:(0.521555542946) A[1]:(0.877098560333) A[2]:(-0.6129322052) A[3]:(0.844430863857)\n",
      " state (12)  A[0]:(0.0777925029397) A[1]:(0.825185060501) A[2]:(-0.592189192772) A[3]:(0.79393774271)\n",
      " state (13)  A[0]:(-0.000616669596639) A[1]:(0.810592412949) A[2]:(0.899973213673) A[3]:(0.72931933403)\n",
      " state (14)  A[0]:(0.80962407589) A[1]:(0.901262640953) A[2]:(0.999999940395) A[3]:(0.810117840767)\n",
      " state (15)  A[0]:(0.984327435493) A[1]:(0.958106040955) A[2]:(1.0) A[3]:(0.880467534065)\n",
      "Episode 669000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6033. Times reached goal: 997.               Steps done: 5177531. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00507773229511.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5903,  0.5904,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.0001,  0.6561,  0.5906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.7291,  0.5906,  0.6563]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8099, -0.0004,  0.6562]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0003,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9004,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531433045864) A[1]:(0.590285360813) A[2]:(0.590425848961) A[3]:(0.531651616096)\n",
      " state (1)  A[0]:(0.531300425529) A[1]:(-0.000253707170486) A[2]:(0.655978322029) A[3]:(0.59056609869)\n",
      " state (2)  A[0]:(0.590425908566) A[1]:(0.728796601295) A[2]:(0.590450644493) A[3]:(0.656237304211)\n",
      " state (3)  A[0]:(0.656056940556) A[1]:(-0.218228012323) A[2]:(0.537627220154) A[3]:(0.519200801849)\n",
      " state (4)  A[0]:(0.590365409851) A[1]:(0.655735492706) A[2]:(-0.000154852867126) A[3]:(0.531516075134)\n",
      " state (5)  A[0]:(0.161784946918) A[1]:(0.928848624229) A[2]:(-0.190521031618) A[3]:(0.522041082382)\n",
      " state (6)  A[0]:(-0.000162810087204) A[1]:(0.809867441654) A[2]:(-0.00038123127888) A[3]:(0.656123816967)\n",
      " state (7)  A[0]:(0.630126953125) A[1]:(-0.251223117113) A[2]:(0.289812356234) A[3]:(0.886782944202)\n",
      " state (8)  A[0]:(0.656150460243) A[1]:(-0.000446587771876) A[2]:(0.728620290756) A[3]:(0.590910077095)\n",
      " state (9)  A[0]:(0.655956506729) A[1]:(0.809803724289) A[2]:(0.80975496769) A[3]:(0.000479549140437)\n",
      " state (10)  A[0]:(0.728829026222) A[1]:(0.899894893169) A[2]:(-0.000853895908222) A[3]:(0.729006409645)\n",
      " state (11)  A[0]:(0.521203398705) A[1]:(0.876696884632) A[2]:(-0.612082600594) A[3]:(0.844093620777)\n",
      " state (12)  A[0]:(0.0777135640383) A[1]:(0.824242293835) A[2]:(-0.591523706913) A[3]:(0.793539047241)\n",
      " state (13)  A[0]:(-0.00032839179039) A[1]:(0.809095680714) A[2]:(0.899881064892) A[3]:(0.728938639164)\n",
      " state (14)  A[0]:(0.809942126274) A[1]:(0.900141656399) A[2]:(0.999999940395) A[3]:(0.809954941273)\n",
      " state (15)  A[0]:(0.984400749207) A[1]:(0.957496345043) A[2]:(1.0) A[3]:(0.880506277084)\n",
      "Episode 670000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6018. Times reached goal: 999.               Steps done: 5183549. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00504726626638.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531293809414) A[1]:(0.590421795845) A[2]:(0.590570628643) A[3]:(0.53146982193)\n",
      " state (1)  A[0]:(0.531265497208) A[1]:(-0.000273488461971) A[2]:(0.656123578548) A[3]:(0.590368270874)\n",
      " state (2)  A[0]:(0.590440273285) A[1]:(0.729010939598) A[2]:(0.590547680855) A[3]:(0.656119346619)\n",
      " state (3)  A[0]:(0.656112909317) A[1]:(-0.216543123126) A[2]:(0.537618756294) A[3]:(0.519164323807)\n",
      " state (4)  A[0]:(0.590363264084) A[1]:(0.656134963036) A[2]:(0.000158667564392) A[3]:(0.531440258026)\n",
      " state (5)  A[0]:(0.161699786782) A[1]:(0.928917050362) A[2]:(-0.190101101995) A[3]:(0.522036254406)\n",
      " state (6)  A[0]:(-0.000261068344116) A[1]:(0.809980511665) A[2]:(0.000192761421204) A[3]:(0.656152248383)\n",
      " state (7)  A[0]:(0.629944086075) A[1]:(-0.250908017159) A[2]:(0.290545672178) A[3]:(0.886749565601)\n",
      " state (8)  A[0]:(0.655956447124) A[1]:(-1.62944197655e-05) A[2]:(0.729076623917) A[3]:(0.590694248676)\n",
      " state (9)  A[0]:(0.655961751938) A[1]:(0.80993193388) A[2]:(0.810048818588) A[3]:(0.00052744144341)\n",
      " state (10)  A[0]:(0.728913664818) A[1]:(0.899968981743) A[2]:(-0.000277638435364) A[3]:(0.729213953018)\n",
      " state (11)  A[0]:(0.52131485939) A[1]:(0.876799941063) A[2]:(-0.611885011196) A[3]:(0.844255268574)\n",
      " state (12)  A[0]:(0.0777483358979) A[1]:(0.824404597282) A[2]:(-0.591441035271) A[3]:(0.793743014336)\n",
      " state (13)  A[0]:(-0.0004448890395) A[1]:(0.809295356274) A[2]:(0.899987220764) A[3]:(0.729197144508)\n",
      " state (14)  A[0]:(0.809828937054) A[1]:(0.900267481804) A[2]:(0.999999940395) A[3]:(0.810182392597)\n",
      " state (15)  A[0]:(0.984367311001) A[1]:(0.957545995712) A[2]:(1.0) A[3]:(0.880644202232)\n",
      "Episode 671000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6029. Times reached goal: 999.               Steps done: 5189578. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00501692784513.\n",
      " state (0)  A[0]:(0.531312346458) A[1]:(0.590512156487) A[2]:(0.590585708618) A[3]:(0.531417250633)\n",
      " state (1)  A[0]:(0.531279563904) A[1]:(-0.000243082642555) A[2]:(0.656108677387) A[3]:(0.590424239635)\n",
      " state (2)  A[0]:(0.590459942818) A[1]:(0.729028403759) A[2]:(0.590729653835) A[3]:(0.656112313271)\n",
      " state (3)  A[0]:(0.65617030859) A[1]:(-0.217748865485) A[2]:(0.537821471691) A[3]:(0.519137024879)\n",
      " state (4)  A[0]:(0.590510487556) A[1]:(0.656091094017) A[2]:(1.8835067749e-05) A[3]:(0.531473398209)\n",
      " state (5)  A[0]:(0.161972209811) A[1]:(0.928928971291) A[2]:(-0.190367773175) A[3]:(0.522002220154)\n",
      " state (6)  A[0]:(-4.59849834442e-05) A[1]:(0.81001496315) A[2]:(-7.55786895752e-05) A[3]:(0.656066834927)\n",
      " state (7)  A[0]:(0.629945397377) A[1]:(-0.2508071661) A[2]:(0.290393292904) A[3]:(0.886705577374)\n",
      " state (8)  A[0]:(0.655979752541) A[1]:(0.000152245163918) A[2]:(0.728895783424) A[3]:(0.590793371201)\n",
      " state (9)  A[0]:(0.655996978283) A[1]:(0.81001162529) A[2]:(0.809943795204) A[3]:(0.000662758830003)\n",
      " state (10)  A[0]:(0.72895270586) A[1]:(0.900005102158) A[2]:(-0.000465273828013) A[3]:(0.729154527187)\n",
      " state (11)  A[0]:(0.521455168724) A[1]:(0.876835942268) A[2]:(-0.612068295479) A[3]:(0.844175040722)\n",
      " state (12)  A[0]:(0.0780576169491) A[1]:(0.824452519417) A[2]:(-0.591774582863) A[3]:(0.793603301048)\n",
      " state (13)  A[0]:(-6.38067722321e-05) A[1]:(0.809349656105) A[2]:(0.89989733696) A[3]:(0.728989720345)\n",
      " state (14)  A[0]:(0.809977591038) A[1]:(0.900303244591) A[2]:(0.999999940395) A[3]:(0.810036659241)\n",
      " state (15)  A[0]:(0.98437333107) A[1]:(0.95755892992) A[2]:(1.0) A[3]:(0.880536496639)\n",
      "Episode 672000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6036. Times reached goal: 999.               Steps done: 5195614. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00498673687667.\n",
      " state (0)  A[0]:(0.531273961067) A[1]:(0.59072893858) A[2]:(0.590558052063) A[3]:(0.530801892281)\n",
      " state (1)  A[0]:(0.531224489212) A[1]:(0.000151239335537) A[2]:(0.656125426292) A[3]:(0.59000825882)\n",
      " state (2)  A[0]:(0.590348601341) A[1]:(0.729032516479) A[2]:(0.590695738792) A[3]:(0.655814051628)\n",
      " state (3)  A[0]:(0.655801892281) A[1]:(-0.218042775989) A[2]:(0.537912547588) A[3]:(0.518671393394)\n",
      " state (4)  A[0]:(0.589981436729) A[1]:(0.65623497963) A[2]:(4.00543212891e-05) A[3]:(0.531117677689)\n",
      " state (5)  A[0]:(0.161224037409) A[1]:(0.928963720798) A[2]:(-0.190396264195) A[3]:(0.521764397621)\n",
      " state (6)  A[0]:(-0.000496625842061) A[1]:(0.810061275959) A[2]:(-9.89437103271e-05) A[3]:(0.655970335007)\n",
      " state (7)  A[0]:(0.629916667938) A[1]:(-0.250725924969) A[2]:(0.290395468473) A[3]:(0.886676490307)\n",
      " state (8)  A[0]:(0.655962109566) A[1]:(0.000258229672909) A[2]:(0.728822171688) A[3]:(0.590766072273)\n",
      " state (9)  A[0]:(0.655896008015) A[1]:(0.810063004494) A[2]:(0.809931457043) A[3]:(0.000408902735217)\n",
      " state (10)  A[0]:(0.728902816772) A[1]:(0.900036811829) A[2]:(-0.000553250254598) A[3]:(0.729104161263)\n",
      " state (11)  A[0]:(0.521461844444) A[1]:(0.876867175102) A[2]:(-0.612234473228) A[3]:(0.844215393066)\n",
      " state (12)  A[0]:(0.0781219527125) A[1]:(0.82447719574) A[2]:(-0.592000246048) A[3]:(0.79371970892)\n",
      " state (13)  A[0]:(1.29342079163e-05) A[1]:(0.809351801872) A[2]:(0.899955153465) A[3]:(0.729205667973)\n",
      " state (14)  A[0]:(0.809963583946) A[1]:(0.900285363197) A[2]:(0.999999940395) A[3]:(0.810246288776)\n",
      " state (15)  A[0]:(0.984348118305) A[1]:(0.957526743412) A[2]:(1.0) A[3]:(0.880657613277)\n",
      "Episode 673000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5998. Times reached goal: 993.               Steps done: 5201612. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00495691595124.\n",
      " state (0)  A[0]:(0.531600177288) A[1]:(0.590459346771) A[2]:(0.590476632118) A[3]:(0.531608879566)\n",
      " state (1)  A[0]:(0.531483829021) A[1]:(-4.08217310905e-05) A[2]:(0.656019449234) A[3]:(0.590508580208)\n",
      " state (2)  A[0]:(0.590559601784) A[1]:(0.728874921799) A[2]:(0.59045624733) A[3]:(0.656162142754)\n",
      " state (3)  A[0]:(0.656131982803) A[1]:(-0.217999607325) A[2]:(0.537684082985) A[3]:(0.51925444603)\n",
      " state (4)  A[0]:(0.590351104736) A[1]:(0.655744791031) A[2]:(-7.53402709961e-05) A[3]:(0.531529963017)\n",
      " state (5)  A[0]:(0.161690443754) A[1]:(0.928841590881) A[2]:(-0.190541833639) A[3]:(0.522037863731)\n",
      " state (6)  A[0]:(-0.000198096036911) A[1]:(0.809825658798) A[2]:(-0.000247597694397) A[3]:(0.656042814255)\n",
      " state (7)  A[0]:(0.63010686636) A[1]:(-0.251241624355) A[2]:(0.290420681238) A[3]:(0.886666595936)\n",
      " state (8)  A[0]:(0.6562281847) A[1]:(-0.000262193381786) A[2]:(0.728839755058) A[3]:(0.590667128563)\n",
      " state (9)  A[0]:(0.656104624271) A[1]:(0.80987149477) A[2]:(0.809872627258) A[3]:(0.000284790992737)\n",
      " state (10)  A[0]:(0.728924632072) A[1]:(0.899893581867) A[2]:(-0.000740408780985) A[3]:(0.728990972042)\n",
      " state (11)  A[0]:(0.521349191666) A[1]:(0.876656293869) A[2]:(-0.612421452999) A[3]:(0.844108104706)\n",
      " state (12)  A[0]:(0.0778636857867) A[1]:(0.824145436287) A[2]:(-0.59240436554) A[3]:(0.793560624123)\n",
      " state (13)  A[0]:(-0.000367373198969) A[1]:(0.808950781822) A[2]:(0.899753034115) A[3]:(0.72897541523)\n",
      " state (14)  A[0]:(0.809821009636) A[1]:(0.900043964386) A[2]:(0.999999940395) A[3]:(0.810043156147)\n",
      " state (15)  A[0]:(0.984337151051) A[1]:(0.957414627075) A[2]:(1.0) A[3]:(0.88051122427)\n",
      "Episode 674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 993.               Steps done: 5207613. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00492725857457.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5905,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.6561, -0.0001,  0.5312]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.0002,  0.7291,  0.5900]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.8100,  0.8100, -0.0007]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8093,  0.9000,  0.7286]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531602263451) A[1]:(0.590518832207) A[2]:(0.590484023094) A[3]:(0.531469106674)\n",
      " state (1)  A[0]:(0.531554877758) A[1]:(0.000164896249771) A[2]:(0.656095027924) A[3]:(0.590418279171)\n",
      " state (2)  A[0]:(0.590697526932) A[1]:(0.728986442089) A[2]:(0.590699195862) A[3]:(0.656087517738)\n",
      " state (3)  A[0]:(0.65615427494) A[1]:(-0.218565776944) A[2]:(0.537930250168) A[3]:(0.518790006638)\n",
      " state (4)  A[0]:(0.590457499027) A[1]:(0.656099915504) A[2]:(-7.10487365723e-05) A[3]:(0.531155884266)\n",
      " state (5)  A[0]:(0.161992445588) A[1]:(0.928940594196) A[2]:(-0.190567910671) A[3]:(0.521753072739)\n",
      " state (6)  A[0]:(0.000195771455765) A[1]:(0.809985637665) A[2]:(-0.000110149383545) A[3]:(0.655789732933)\n",
      " state (7)  A[0]:(0.63008582592) A[1]:(-0.250916868448) A[2]:(0.290805965662) A[3]:(0.886458992958)\n",
      " state (8)  A[0]:(0.656062662601) A[1]:(0.000136435031891) A[2]:(0.729029178619) A[3]:(0.589906275272)\n",
      " state (9)  A[0]:(0.6560562253) A[1]:(0.810021221638) A[2]:(0.810023486614) A[3]:(-0.000905588036403)\n",
      " state (10)  A[0]:(0.728958308697) A[1]:(0.899999499321) A[2]:(-0.000517606677022) A[3]:(0.728525400162)\n",
      " state (11)  A[0]:(0.521400570869) A[1]:(0.876802802086) A[2]:(-0.612439990044) A[3]:(0.843857884407)\n",
      " state (12)  A[0]:(0.0779013633728) A[1]:(0.824368834496) A[2]:(-0.592397689819) A[3]:(0.793240368366)\n",
      " state (13)  A[0]:(-0.000120311975479) A[1]:(0.809233427048) A[2]:(0.900025606155) A[3]:(0.728583872318)\n",
      " state (14)  A[0]:(0.810067653656) A[1]:(0.900229334831) A[2]:(0.999999940395) A[3]:(0.809769034386)\n",
      " state (15)  A[0]:(0.984340429306) A[1]:(0.957482516766) A[2]:(1.0) A[3]:(0.880263924599)\n",
      "Episode 675000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6010. Times reached goal: 995.               Steps done: 5213623. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00489773455908.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531350016594) A[1]:(0.59051835537) A[2]:(0.590426921844) A[3]:(0.531288862228)\n",
      " state (1)  A[0]:(0.531220555305) A[1]:(0.00029818713665) A[2]:(0.6561409235) A[3]:(0.590292036533)\n",
      " state (2)  A[0]:(0.590445160866) A[1]:(0.729005098343) A[2]:(0.590715289116) A[3]:(0.656033694744)\n",
      " state (3)  A[0]:(0.656165480614) A[1]:(-0.217844799161) A[2]:(0.538046479225) A[3]:(0.518830001354)\n",
      " state (4)  A[0]:(0.590694308281) A[1]:(0.656213402748) A[2]:(0.000176072120667) A[3]:(0.531215190887)\n",
      " state (5)  A[0]:(0.162674054503) A[1]:(0.928986668587) A[2]:(-0.190531715751) A[3]:(0.521972537041)\n",
      " state (6)  A[0]:(0.00125542213209) A[1]:(0.810006737709) A[2]:(-0.00010085105896) A[3]:(0.65619122982)\n",
      " state (7)  A[0]:(0.630631804466) A[1]:(-0.250746011734) A[2]:(0.290819168091) A[3]:(0.886677742004)\n",
      " state (8)  A[0]:(0.656533718109) A[1]:(0.00105784798507) A[2]:(0.72904753685) A[3]:(0.590523600578)\n",
      " state (9)  A[0]:(0.656685829163) A[1]:(0.810315966606) A[2]:(0.810022711754) A[3]:(0.000286102294922)\n",
      " state (10)  A[0]:(0.729380726814) A[1]:(0.900035440922) A[2]:(-5.00679016113e-06) A[3]:(0.728867888451)\n",
      " state (11)  A[0]:(0.521985054016) A[1]:(0.876719117165) A[2]:(-0.611957907677) A[3]:(0.843988835812)\n",
      " state (12)  A[0]:(0.0787839069963) A[1]:(0.824107885361) A[2]:(-0.5920394063) A[3]:(0.793449759483)\n",
      " state (13)  A[0]:(0.000878184800968) A[1]:(0.808799028397) A[2]:(0.900029361248) A[3]:(0.728940963745)\n",
      " state (14)  A[0]:(0.81051170826) A[1]:(0.899912655354) A[2]:(0.999999940395) A[3]:(0.810090899467)\n",
      " state (15)  A[0]:(0.984393596649) A[1]:(0.957312703133) A[2]:(1.0) A[3]:(0.880519211292)\n",
      "Episode 676000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 996.               Steps done: 5219632. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00486839231916.\n",
      " state (0)  A[0]:(0.531087934971) A[1]:(0.590197741985) A[2]:(0.590548276901) A[3]:(0.531344175339)\n",
      " state (1)  A[0]:(0.530986428261) A[1]:(-7.92741775513e-06) A[2]:(0.656156003475) A[3]:(0.590386509895)\n",
      " state (2)  A[0]:(0.59024566412) A[1]:(0.729014277458) A[2]:(0.590660154819) A[3]:(0.656178236008)\n",
      " state (3)  A[0]:(0.655812263489) A[1]:(-0.217404529452) A[2]:(0.537841200829) A[3]:(0.519114375114)\n",
      " state (4)  A[0]:(0.590202331543) A[1]:(0.656035065651) A[2]:(5.19752502441e-05) A[3]:(0.531492829323)\n",
      " state (5)  A[0]:(0.161885157228) A[1]:(0.928941965103) A[2]:(-0.190578013659) A[3]:(0.522234916687)\n",
      " state (6)  A[0]:(0.000185012817383) A[1]:(0.809999644756) A[2]:(-4.1127204895e-05) A[3]:(0.656109929085)\n",
      " state (7)  A[0]:(0.630003631115) A[1]:(-0.250903815031) A[2]:(0.291128069162) A[3]:(0.886497557163)\n",
      " state (8)  A[0]:(0.656131148338) A[1]:(0.000123113393784) A[2]:(0.729006946087) A[3]:(0.59051156044)\n",
      " state (9)  A[0]:(0.656303405762) A[1]:(0.810031354427) A[2]:(0.810003995895) A[3]:(0.000507160963025)\n",
      " state (10)  A[0]:(0.729174017906) A[1]:(0.900002598763) A[2]:(-0.000513672770467) A[3]:(0.729258060455)\n",
      " state (11)  A[0]:(0.521666049957) A[1]:(0.876797020435) A[2]:(-0.612606167793) A[3]:(0.844337463379)\n",
      " state (12)  A[0]:(0.078127399087) A[1]:(0.824353337288) A[2]:(-0.592830359936) A[3]:(0.793866634369)\n",
      " state (13)  A[0]:(7.39097595215e-05) A[1]:(0.809217572212) A[2]:(0.899947583675) A[3]:(0.729341387749)\n",
      " state (14)  A[0]:(0.810272693634) A[1]:(0.900234460831) A[2]:(0.999999940395) A[3]:(0.810248017311)\n",
      " state (15)  A[0]:(0.984353661537) A[1]:(0.957477450371) A[2]:(1.0) A[3]:(0.880479156971)\n",
      "Episode 677000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6006. Times reached goal: 995.               Steps done: 5225638. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00483924038578.\n",
      " state (0)  A[0]:(0.531757354736) A[1]:(0.590318620205) A[2]:(0.590427815914) A[3]:(0.531611859798)\n",
      " state (1)  A[0]:(0.53164768219) A[1]:(2.63825058937e-05) A[2]:(0.656020820141) A[3]:(0.590455055237)\n",
      " state (2)  A[0]:(0.590752065182) A[1]:(0.728910565376) A[2]:(0.590594887733) A[3]:(0.656124949455)\n",
      " state (3)  A[0]:(0.656346082687) A[1]:(-0.218936339021) A[2]:(0.537890195847) A[3]:(0.518819093704)\n",
      " state (4)  A[0]:(0.590694189072) A[1]:(0.655971646309) A[2]:(-0.000189304351807) A[3]:(0.531288504601)\n",
      " state (5)  A[0]:(0.16227479279) A[1]:(0.928916633129) A[2]:(-0.190754815936) A[3]:(0.522047281265)\n",
      " state (6)  A[0]:(0.000298768281937) A[1]:(0.809954047203) A[2]:(-0.000141739845276) A[3]:(0.65592366457)\n",
      " state (7)  A[0]:(0.630136966705) A[1]:(-0.25113505125) A[2]:(0.291183918715) A[3]:(0.886435687542)\n",
      " state (8)  A[0]:(0.656333327293) A[1]:(-0.000284634530544) A[2]:(0.728895187378) A[3]:(0.590716123581)\n",
      " state (9)  A[0]:(0.656303882599) A[1]:(0.809933960438) A[2]:(0.809949636459) A[3]:(0.000734463217668)\n",
      " state (10)  A[0]:(0.729134202003) A[1]:(0.899960517883) A[2]:(-0.000501871050801) A[3]:(0.729265630245)\n",
      " state (11)  A[0]:(0.521684467793) A[1]:(0.876749277115) A[2]:(-0.612637400627) A[3]:(0.844336330891)\n",
      " state (12)  A[0]:(0.0782430693507) A[1]:(0.824292063713) A[2]:(-0.592929244041) A[3]:(0.793870449066)\n",
      " state (13)  A[0]:(0.000112652778625) A[1]:(0.809164702892) A[2]:(0.900006592274) A[3]:(0.729359984398)\n",
      " state (14)  A[0]:(0.810123801231) A[1]:(0.900221765041) A[2]:(0.999999940395) A[3]:(0.810292601585)\n",
      " state (15)  A[0]:(0.9843069911) A[1]:(0.957466781139) A[2]:(1.0) A[3]:(0.880487442017)\n",
      "Episode 678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5991. Times reached goal: 992.               Steps done: 5231629. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00481033516866.\n",
      " state (0)  A[0]:(0.53130364418) A[1]:(0.590545535088) A[2]:(0.590700984001) A[3]:(0.531659901142)\n",
      " state (1)  A[0]:(0.531403541565) A[1]:(5.77121973038e-05) A[2]:(0.65630531311) A[3]:(0.590697646141)\n",
      " state (2)  A[0]:(0.590656340122) A[1]:(0.729114174843) A[2]:(0.59085470438) A[3]:(0.65644133091)\n",
      " state (3)  A[0]:(0.656432211399) A[1]:(-0.217203587294) A[2]:(0.537913382053) A[3]:(0.519358754158)\n",
      " state (4)  A[0]:(0.590848684311) A[1]:(0.656315207481) A[2]:(5.61475753784e-05) A[3]:(0.531748056412)\n",
      " state (5)  A[0]:(0.162632614374) A[1]:(0.928982496262) A[2]:(-0.190470263362) A[3]:(0.522582411766)\n",
      " state (6)  A[0]:(0.000953256792855) A[1]:(0.810100615025) A[2]:(0.000271677970886) A[3]:(0.656434178352)\n",
      " state (7)  A[0]:(0.630672931671) A[1]:(-0.250526100397) A[2]:(0.291697621346) A[3]:(0.886636793613)\n",
      " state (8)  A[0]:(0.657200336456) A[1]:(0.00056026870152) A[2]:(0.729115605354) A[3]:(0.591237902641)\n",
      " state (9)  A[0]:(0.657713294029) A[1]:(0.810179829597) A[2]:(0.810045957565) A[3]:(0.00152592244558)\n",
      " state (10)  A[0]:(0.730495452881) A[1]:(0.900061190128) A[2]:(-0.000222086906433) A[3]:(0.729664325714)\n",
      " state (11)  A[0]:(0.523930072784) A[1]:(0.876829504967) A[2]:(-0.612520217896) A[3]:(0.844595551491)\n",
      " state (12)  A[0]:(0.0813756510615) A[1]:(0.824328780174) A[2]:(-0.593019366264) A[3]:(0.794224798679)\n",
      " state (13)  A[0]:(0.00317080738023) A[1]:(0.809090018272) A[2]:(0.899869024754) A[3]:(0.729822516441)\n",
      " state (14)  A[0]:(0.81113910675) A[1]:(0.900103151798) A[2]:(0.999999940395) A[3]:(0.810649991035)\n",
      " state (15)  A[0]:(0.984399497509) A[1]:(0.957381129265) A[2]:(1.0) A[3]:(0.880743741989)\n",
      "Episode 679000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 997.               Steps done: 5237643. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0047814926294.\n",
      "q_values \n",
      "tensor([[ 0.5309,  0.5905,  0.5904,  0.5310]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.6563, -0.0001,  0.5317]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565,  0.0003,  0.7291,  0.5914]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.8101,  0.8101,  0.0025]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0008,  0.8090,  0.9001,  0.7301]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9001,  1.0000,  0.8107]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531124591827) A[1]:(0.590484261513) A[2]:(0.59042519331) A[3]:(0.530867934227)\n",
      " state (1)  A[0]:(0.531227469444) A[1]:(0.000121407210827) A[2]:(0.656045258045) A[3]:(0.590281069279)\n",
      " state (2)  A[0]:(0.590436816216) A[1]:(0.729122281075) A[2]:(0.590523838997) A[3]:(0.6562063694)\n",
      " state (3)  A[0]:(0.655962109566) A[1]:(-0.216876968741) A[2]:(0.537758231163) A[3]:(0.519060969353)\n",
      " state (4)  A[0]:(0.590197443962) A[1]:(0.656293570995) A[2]:(5.96046447754e-06) A[3]:(0.531522154808)\n",
      " state (5)  A[0]:(0.161695465446) A[1]:(0.928980708122) A[2]:(-0.190569981933) A[3]:(0.522490620613)\n",
      " state (6)  A[0]:(7.28070735931e-05) A[1]:(0.810081362724) A[2]:(0.000231146812439) A[3]:(0.656346797943)\n",
      " state (7)  A[0]:(0.630150794983) A[1]:(-0.250656098127) A[2]:(0.291845053434) A[3]:(0.886555790901)\n",
      " state (8)  A[0]:(0.656461417675) A[1]:(0.000397466094) A[2]:(0.729146003723) A[3]:(0.591356873512)\n",
      " state (9)  A[0]:(0.656509160995) A[1]:(0.810096204281) A[2]:(0.810082376003) A[3]:(0.00235242699273)\n",
      " state (10)  A[0]:(0.729275941849) A[1]:(0.900006234646) A[2]:(0.000152230262756) A[3]:(0.7300542593)\n",
      " state (11)  A[0]:(0.521956205368) A[1]:(0.876762509346) A[2]:(-0.612254500389) A[3]:(0.844809114933)\n",
      " state (12)  A[0]:(0.0787007063627) A[1]:(0.824243545532) A[2]:(-0.592803478241) A[3]:(0.794472813606)\n",
      " state (13)  A[0]:(0.000459015340311) A[1]:(0.809012293816) A[2]:(0.899987518787) A[3]:(0.730046153069)\n",
      " state (14)  A[0]:(0.810144364834) A[1]:(0.900073826313) A[2]:(0.999999940395) A[3]:(0.810631155968)\n",
      " state (15)  A[0]:(0.984290421009) A[1]:(0.957364916801) A[2]:(1.0) A[3]:(0.880576252937)\n",
      "Episode 680000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6022. Times reached goal: 995.               Steps done: 5243665. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0047527850062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531305491924) A[1]:(0.590408444405) A[2]:(0.590454101562) A[3]:(0.531295061111)\n",
      " state (1)  A[0]:(0.531536698341) A[1]:(4.44352626801e-05) A[2]:(0.656054139137) A[3]:(0.590420246124)\n",
      " state (2)  A[0]:(0.590767383575) A[1]:(0.728929638863) A[2]:(0.590604662895) A[3]:(0.656163096428)\n",
      " state (3)  A[0]:(0.656431555748) A[1]:(-0.218408584595) A[2]:(0.537975370884) A[3]:(0.519156336784)\n",
      " state (4)  A[0]:(0.590883672237) A[1]:(0.65621483326) A[2]:(-0.000143766403198) A[3]:(0.531737685204)\n",
      " state (5)  A[0]:(0.162770986557) A[1]:(0.928969740868) A[2]:(-0.190872088075) A[3]:(0.522587776184)\n",
      " state (6)  A[0]:(0.00119960249867) A[1]:(0.809975147247) A[2]:(-6.3419342041e-05) A[3]:(0.65616774559)\n",
      " state (7)  A[0]:(0.630537748337) A[1]:(-0.250892311335) A[2]:(0.291547209024) A[3]:(0.886295199394)\n",
      " state (8)  A[0]:(0.656751394272) A[1]:(0.000219255685806) A[2]:(0.728742182255) A[3]:(0.590835690498)\n",
      " state (9)  A[0]:(0.656758129597) A[1]:(0.810067951679) A[2]:(0.809967875481) A[3]:(0.00119809748139)\n",
      " state (10)  A[0]:(0.729524970055) A[1]:(0.900008022785) A[2]:(0.000258207321167) A[3]:(0.729401111603)\n",
      " state (11)  A[0]:(0.522497534752) A[1]:(0.876756072044) A[2]:(-0.612197697163) A[3]:(0.844439029694)\n",
      " state (12)  A[0]:(0.079592615366) A[1]:(0.824202358723) A[2]:(-0.592787265778) A[3]:(0.794036149979)\n",
      " state (13)  A[0]:(0.00144535198342) A[1]:(0.808922946453) A[2]:(0.900093138218) A[3]:(0.72958022356)\n",
      " state (14)  A[0]:(0.810487687588) A[1]:(0.899997532368) A[2]:(0.999999940395) A[3]:(0.810448765755)\n",
      " state (15)  A[0]:(0.984302699566) A[1]:(0.957303583622) A[2]:(1.0) A[3]:(0.880519747734)\n",
      "Episode 681000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6023. Times reached goal: 999.               Steps done: 5249688. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00472424501656.\n",
      " state (0)  A[0]:(0.532147049904) A[1]:(0.590741991997) A[2]:(0.590650558472) A[3]:(0.531785011292)\n",
      " state (1)  A[0]:(0.531905174255) A[1]:(4.59179282188e-05) A[2]:(0.656154036522) A[3]:(0.59065413475)\n",
      " state (2)  A[0]:(0.590941369534) A[1]:(0.729091405869) A[2]:(0.590652227402) A[3]:(0.656295895576)\n",
      " state (3)  A[0]:(0.656578421593) A[1]:(-0.218097224832) A[2]:(0.537892401218) A[3]:(0.519246101379)\n",
      " state (4)  A[0]:(0.590842068195) A[1]:(0.656531095505) A[2]:(-0.000240206718445) A[3]:(0.531774997711)\n",
      " state (5)  A[0]:(0.162212878466) A[1]:(0.928987562656) A[2]:(-0.190747693181) A[3]:(0.522522628307)\n",
      " state (6)  A[0]:(0.000151604413986) A[1]:(0.810085654259) A[2]:(0.000217914581299) A[3]:(0.656125545502)\n",
      " state (7)  A[0]:(0.63007748127) A[1]:(-0.250789612532) A[2]:(0.292251557112) A[3]:(0.886368095875)\n",
      " state (8)  A[0]:(0.656169056892) A[1]:(-0.000444151432021) A[2]:(0.729099392891) A[3]:(0.590660154819)\n",
      " state (9)  A[0]:(0.655760526657) A[1]:(0.809852778912) A[2]:(0.810024380684) A[3]:(-6.14076852798e-05)\n",
      " state (10)  A[0]:(0.728694200516) A[1]:(0.900002598763) A[2]:(-0.000744461896829) A[3]:(0.729051709175)\n",
      " state (11)  A[0]:(0.521116375923) A[1]:(0.876883208752) A[2]:(-0.613327920437) A[3]:(0.844315111637)\n",
      " state (12)  A[0]:(0.0773933976889) A[1]:(0.824566721916) A[2]:(-0.594056606293) A[3]:(0.793813884258)\n",
      " state (13)  A[0]:(-0.000983595498838) A[1]:(0.809564828873) A[2]:(0.900011956692) A[3]:(0.729151189327)\n",
      " state (14)  A[0]:(0.809585869312) A[1]:(0.900515973568) A[2]:(0.999999940395) A[3]:(0.809996604919)\n",
      " state (15)  A[0]:(0.984176456928) A[1]:(0.957579255104) A[2]:(1.0) A[3]:(0.880043327808)\n",
      "Episode 682000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 993.               Steps done: 5255702. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00469591866973.\n",
      " state (0)  A[0]:(0.531414628029) A[1]:(0.590521812439) A[2]:(0.590555727482) A[3]:(0.531351089478)\n",
      " state (1)  A[0]:(0.53136408329) A[1]:(9.58144664764e-06) A[2]:(0.656180977821) A[3]:(0.59034883976)\n",
      " state (2)  A[0]:(0.590514481068) A[1]:(0.729063391685) A[2]:(0.590691685677) A[3]:(0.656093716621)\n",
      " state (3)  A[0]:(0.656215846539) A[1]:(-0.216954514384) A[2]:(0.537945628166) A[3]:(0.519072055817)\n",
      " state (4)  A[0]:(0.590526103973) A[1]:(0.656126320362) A[2]:(9.35792922974e-05) A[3]:(0.531504452229)\n",
      " state (5)  A[0]:(0.16201557219) A[1]:(0.928942501545) A[2]:(-0.190739989281) A[3]:(0.522400259972)\n",
      " state (6)  A[0]:(9.36388969421e-05) A[1]:(0.810025036335) A[2]:(0.000122904777527) A[3]:(0.656074523926)\n",
      " state (7)  A[0]:(0.629975378513) A[1]:(-0.250763326883) A[2]:(0.292189747095) A[3]:(0.886323094368)\n",
      " state (8)  A[0]:(0.656287908554) A[1]:(-0.000146672129631) A[2]:(0.729027867317) A[3]:(0.590759038925)\n",
      " state (9)  A[0]:(0.656226873398) A[1]:(0.80992680788) A[2]:(0.810030460358) A[3]:(0.000485017866595)\n",
      " state (10)  A[0]:(0.729112744331) A[1]:(0.899972975254) A[2]:(-0.000294089317322) A[3]:(0.729256391525)\n",
      " state (11)  A[0]:(0.521787524223) A[1]:(0.87676101923) A[2]:(-0.612971246243) A[3]:(0.844421625137)\n",
      " state (12)  A[0]:(0.0784006118774) A[1]:(0.824278473854) A[2]:(-0.593851983547) A[3]:(0.793980538845)\n",
      " state (13)  A[0]:(9.00030136108e-05) A[1]:(0.809110879898) A[2]:(0.899953484535) A[3]:(0.729391932487)\n",
      " state (14)  A[0]:(0.810020208359) A[1]:(0.900184750557) A[2]:(0.999999940395) A[3]:(0.810159802437)\n",
      " state (15)  A[0]:(0.98422896862) A[1]:(0.957401812077) A[2]:(1.0) A[3]:(0.880157768726)\n",
      "Episode 683000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6017. Times reached goal: 993.               Steps done: 5261719. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00466774816306.\n",
      " state (0)  A[0]:(0.531592547894) A[1]:(0.590701043606) A[2]:(0.590505361557) A[3]:(0.531299829483)\n",
      " state (1)  A[0]:(0.531334996223) A[1]:(2.571195364e-05) A[2]:(0.656216800213) A[3]:(0.59031444788)\n",
      " state (2)  A[0]:(0.590422987938) A[1]:(0.729124188423) A[2]:(0.590974628925) A[3]:(0.656002163887)\n",
      " state (3)  A[0]:(0.656252741814) A[1]:(-0.216902583838) A[2]:(0.538204312325) A[3]:(0.518953084946)\n",
      " state (4)  A[0]:(0.59063076973) A[1]:(0.655683875084) A[2]:(0.000380158395274) A[3]:(0.531230688095)\n",
      " state (5)  A[0]:(0.162122189999) A[1]:(0.928879857063) A[2]:(-0.190765038133) A[3]:(0.521962285042)\n",
      " state (6)  A[0]:(-0.000312566757202) A[1]:(0.810060977936) A[2]:(-0.000170826911926) A[3]:(0.655358910561)\n",
      " state (7)  A[0]:(0.629581809044) A[1]:(-0.250670671463) A[2]:(0.29183447361) A[3]:(0.885923206806)\n",
      " state (8)  A[0]:(0.656172335148) A[1]:(-0.00058273220202) A[2]:(0.728713989258) A[3]:(0.590126514435)\n",
      " state (9)  A[0]:(0.656118273735) A[1]:(0.809866547585) A[2]:(0.810034632683) A[3]:(-0.000603347958531)\n",
      " state (10)  A[0]:(0.729063391685) A[1]:(0.900015056133) A[2]:(-0.000159859657288) A[3]:(0.728757381439)\n",
      " state (11)  A[0]:(0.521722793579) A[1]:(0.876865983009) A[2]:(-0.613076865673) A[3]:(0.844150602818)\n",
      " state (12)  A[0]:(0.0782449916005) A[1]:(0.824484825134) A[2]:(-0.594101846218) A[3]:(0.793611884117)\n",
      " state (13)  A[0]:(-3.88622283936e-05) A[1]:(0.809408903122) A[2]:(0.899969875813) A[3]:(0.728875756264)\n",
      " state (14)  A[0]:(0.810067594051) A[1]:(0.900403499603) A[2]:(0.999999940395) A[3]:(0.809732198715)\n",
      " state (15)  A[0]:(0.984225988388) A[1]:(0.957510411739) A[2]:(1.0) A[3]:(0.879812598228)\n",
      "Episode 684000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 994.               Steps done: 5267730. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00463977448796.\n",
      "q_values \n",
      "tensor([[ 0.5295,  0.5904,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5298, -0.0001,  0.6560,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5894,  0.7290,  0.5902,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0030,  0.8100,  0.0003,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7299,  0.9000, -0.0001,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8109,  0.9003,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530921459198) A[1]:(0.590495884418) A[2]:(0.590522885323) A[3]:(0.531555354595)\n",
      " state (1)  A[0]:(0.53092110157) A[1]:(0.000118568539619) A[2]:(0.656124591827) A[3]:(0.590399682522)\n",
      " state (2)  A[0]:(0.59002327919) A[1]:(0.728978633881) A[2]:(0.590259790421) A[3]:(0.656130671501)\n",
      " state (3)  A[0]:(0.655078172684) A[1]:(-0.21612252295) A[2]:(0.537504732609) A[3]:(0.518941581249)\n",
      " state (4)  A[0]:(0.588669836521) A[1]:(0.656140327454) A[2]:(-9.67979431152e-05) A[3]:(0.531179666519)\n",
      " state (5)  A[0]:(0.158869206905) A[1]:(0.928903758526) A[2]:(-0.190696001053) A[3]:(0.522025585175)\n",
      " state (6)  A[0]:(-0.00349755655043) A[1]:(0.809999167919) A[2]:(0.000285029411316) A[3]:(0.655654191971)\n",
      " state (7)  A[0]:(0.627931118011) A[1]:(-0.250751495361) A[2]:(0.292487472296) A[3]:(0.886129021645)\n",
      " state (8)  A[0]:(0.655255734921) A[1]:(-0.000271067023277) A[2]:(0.728999018669) A[3]:(0.590461671352)\n",
      " state (9)  A[0]:(0.656087636948) A[1]:(0.809919059277) A[2]:(0.809966266155) A[3]:(-0.00013755261898)\n",
      " state (10)  A[0]:(0.729310750961) A[1]:(0.899983763695) A[2]:(-0.00034761428833) A[3]:(0.728795051575)\n",
      " state (11)  A[0]:(0.522300601006) A[1]:(0.876782894135) A[2]:(-0.613098978996) A[3]:(0.844098329544)\n",
      " state (12)  A[0]:(0.0792673304677) A[1]:(0.824315845966) A[2]:(-0.594120919704) A[3]:(0.793554842472)\n",
      " state (13)  A[0]:(0.00120198668446) A[1]:(0.809167206287) A[2]:(0.900054633617) A[3]:(0.7289083004)\n",
      " state (14)  A[0]:(0.810563921928) A[1]:(0.900238871574) A[2]:(0.999999940395) A[3]:(0.809935748577)\n",
      " state (15)  A[0]:(0.984260261059) A[1]:(0.957409918308) A[2]:(1.0) A[3]:(0.88002872467)\n",
      "Episode 685000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6024. Times reached goal: 994.               Steps done: 5273754. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00461190850306.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531333148479) A[1]:(0.590565800667) A[2]:(0.590632796288) A[3]:(0.529521465302)\n",
      " state (1)  A[0]:(0.531324028969) A[1]:(-2.47657299042e-05) A[2]:(0.656229496002) A[3]:(0.58941245079)\n",
      " state (2)  A[0]:(0.590548634529) A[1]:(0.729095876217) A[2]:(0.590700387955) A[3]:(0.655441641808)\n",
      " state (3)  A[0]:(0.656476199627) A[1]:(-0.216834366322) A[2]:(0.537858068943) A[3]:(0.517824530602)\n",
      " state (4)  A[0]:(0.590907812119) A[1]:(0.656171500683) A[2]:(-0.000137448310852) A[3]:(0.530110239983)\n",
      " state (5)  A[0]:(0.162440940738) A[1]:(0.928943753242) A[2]:(-0.191040024161) A[3]:(0.520945966244)\n",
      " state (6)  A[0]:(-9.00030136108e-06) A[1]:(0.810033023357) A[2]:(-4.57763671875e-05) A[3]:(0.654848217964)\n",
      " state (7)  A[0]:(0.62938451767) A[1]:(-0.250778645277) A[2]:(0.292491942644) A[3]:(0.88582110405)\n",
      " state (8)  A[0]:(0.655513346195) A[1]:(-0.000157207250595) A[2]:(0.729045271873) A[3]:(0.589349746704)\n",
      " state (9)  A[0]:(0.655227541924) A[1]:(0.809968173504) A[2]:(0.809949994087) A[3]:(-0.00185687630437)\n",
      " state (10)  A[0]:(0.728025615215) A[1]:(0.900031626225) A[2]:(-0.000625371874776) A[3]:(0.727870583534)\n",
      " state (11)  A[0]:(0.519699454308) A[1]:(0.876888692379) A[2]:(-0.613447904587) A[3]:(0.843409478664)\n",
      " state (12)  A[0]:(0.0750913321972) A[1]:(0.824558138847) A[2]:(-0.594640374184) A[3]:(0.792510569096)\n",
      " state (13)  A[0]:(-0.00338871963322) A[1]:(0.809572160244) A[2]:(0.899967193604) A[3]:(0.727443814278)\n",
      " state (14)  A[0]:(0.808975815773) A[1]:(0.900569200516) A[2]:(0.999999940395) A[3]:(0.808835923672)\n",
      " state (15)  A[0]:(0.984103441238) A[1]:(0.957596063614) A[2]:(1.0) A[3]:(0.879275798798)\n",
      "Episode 686000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 997.               Steps done: 5279764. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00458427405764.\n",
      " state (0)  A[0]:(0.533559203148) A[1]:(0.590339779854) A[2]:(0.59063243866) A[3]:(0.529857873917)\n",
      " state (1)  A[0]:(0.533416569233) A[1]:(-0.000462032825453) A[2]:(0.656176924706) A[3]:(0.589177966118)\n",
      " state (2)  A[0]:(0.592275738716) A[1]:(0.728732943535) A[2]:(0.590890884399) A[3]:(0.655287265778)\n",
      " state (3)  A[0]:(0.657475948334) A[1]:(-0.218563422561) A[2]:(0.538459479809) A[3]:(0.518428146839)\n",
      " state (4)  A[0]:(0.59186565876) A[1]:(0.655815720558) A[2]:(0.000591278017964) A[3]:(0.531522452831)\n",
      " state (5)  A[0]:(0.164081051946) A[1]:(0.928891420364) A[2]:(-0.190336182714) A[3]:(0.523021697998)\n",
      " state (6)  A[0]:(0.00238516484387) A[1]:(0.809788525105) A[2]:(0.000930428213906) A[3]:(0.657056391239)\n",
      " state (7)  A[0]:(0.630926787853) A[1]:(-0.251315176487) A[2]:(0.293507814407) A[3]:(0.886854052544)\n",
      " state (8)  A[0]:(0.656875491142) A[1]:(-0.000333644449711) A[2]:(0.72938477993) A[3]:(0.593411803246)\n",
      " state (9)  A[0]:(0.65652179718) A[1]:(0.809896349907) A[2]:(0.810331583023) A[3]:(0.00526810763404)\n",
      " state (10)  A[0]:(0.729245424271) A[1]:(0.899945437908) A[2]:(0.000952243513893) A[3]:(0.731484949589)\n",
      " state (11)  A[0]:(0.521998524666) A[1]:(0.876690208912) A[2]:(-0.612412929535) A[3]:(0.845804452896)\n",
      " state (12)  A[0]:(0.0786652565002) A[1]:(0.824109494686) A[2]:(-0.593698263168) A[3]:(0.795777142048)\n",
      " state (13)  A[0]:(0.000306814908981) A[1]:(0.808848619461) A[2]:(0.900178849697) A[3]:(0.731680512428)\n",
      " state (14)  A[0]:(0.810155034065) A[1]:(0.900021135807) A[2]:(0.999999940395) A[3]:(0.811868190765)\n",
      " state (15)  A[0]:(0.984197556973) A[1]:(0.957281708717) A[2]:(1.0) A[3]:(0.881186664104)\n",
      "Episode 687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 993.               Steps done: 5285775. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00455680064036.\n",
      " state (0)  A[0]:(0.531128406525) A[1]:(0.590089678764) A[2]:(0.590328216553) A[3]:(0.531232893467)\n",
      " state (1)  A[0]:(0.531211853027) A[1]:(-0.000345170497894) A[2]:(0.655915379524) A[3]:(0.590234041214)\n",
      " state (2)  A[0]:(0.590416431427) A[1]:(0.728887557983) A[2]:(0.590344309807) A[3]:(0.655969619751)\n",
      " state (3)  A[0]:(0.656186819077) A[1]:(-0.217241987586) A[2]:(0.537802696228) A[3]:(0.518875479698)\n",
      " state (4)  A[0]:(0.59056520462) A[1]:(0.655887365341) A[2]:(-7.95125961304e-05) A[3]:(0.531281471252)\n",
      " state (5)  A[0]:(0.162194132805) A[1]:(0.92889893055) A[2]:(-0.191091239452) A[3]:(0.522147238255)\n",
      " state (6)  A[0]:(0.000238418579102) A[1]:(0.809931695461) A[2]:(-8.6784362793e-05) A[3]:(0.655492424965)\n",
      " state (7)  A[0]:(0.629859805107) A[1]:(-0.251009881496) A[2]:(0.292529463768) A[3]:(0.885817110538)\n",
      " state (8)  A[0]:(0.656256735325) A[1]:(-0.000500649155583) A[2]:(0.728892445564) A[3]:(0.589742183685)\n",
      " state (9)  A[0]:(0.656193196774) A[1]:(0.809865117073) A[2]:(0.809947550297) A[3]:(-0.000616744102444)\n",
      " state (10)  A[0]:(0.729068040848) A[1]:(0.899978578091) A[2]:(-0.000536322535481) A[3]:(0.728854179382)\n",
      " state (11)  A[0]:(0.521653056145) A[1]:(0.876792252064) A[2]:(-0.6135597229) A[3]:(0.844193398952)\n",
      " state (12)  A[0]:(0.0779822319746) A[1]:(0.824350237846) A[2]:(-0.594952821732) A[3]:(0.793613553047)\n",
      " state (13)  A[0]:(-0.000457912654383) A[1]:(0.809255242348) A[2]:(0.899988234043) A[3]:(0.728850364685)\n",
      " state (14)  A[0]:(0.809960663319) A[1]:(0.900354206562) A[2]:(0.999999940395) A[3]:(0.809820532799)\n",
      " state (15)  A[0]:(0.984160900116) A[1]:(0.957460522652) A[2]:(1.0) A[3]:(0.879809379578)\n",
      "Episode 688000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6011. Times reached goal: 994.               Steps done: 5291786. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00452949187045.\n",
      " state (0)  A[0]:(0.531429052353) A[1]:(0.590695977211) A[2]:(0.590764224529) A[3]:(0.531760692596)\n",
      " state (1)  A[0]:(0.531452178955) A[1]:(0.000264182686806) A[2]:(0.656310796738) A[3]:(0.59080350399)\n",
      " state (2)  A[0]:(0.590609967709) A[1]:(0.729094326496) A[2]:(0.59060049057) A[3]:(0.656498432159)\n",
      " state (3)  A[0]:(0.656291007996) A[1]:(-0.21659244597) A[2]:(0.537975668907) A[3]:(0.519087076187)\n",
      " state (4)  A[0]:(0.590520143509) A[1]:(0.656273365021) A[2]:(0.00019907951355) A[3]:(0.531396925449)\n",
      " state (5)  A[0]:(0.161859735847) A[1]:(0.928945958614) A[2]:(-0.190676242113) A[3]:(0.522440671921)\n",
      " state (6)  A[0]:(-0.000451683969004) A[1]:(0.810122191906) A[2]:(0.000436425179942) A[3]:(0.656057238579)\n",
      " state (7)  A[0]:(0.629348158836) A[1]:(-0.250168293715) A[2]:(0.29333665967) A[3]:(0.886186182499)\n",
      " state (8)  A[0]:(0.6556224823) A[1]:(0.000570669712033) A[2]:(0.729565560818) A[3]:(0.589779138565)\n",
      " state (9)  A[0]:(0.655450940132) A[1]:(0.810186088085) A[2]:(0.810230731964) A[3]:(-0.00189213233534)\n",
      " state (10)  A[0]:(0.728604257107) A[1]:(0.900098800659) A[2]:(-0.000180721282959) A[3]:(0.728158354759)\n",
      " state (11)  A[0]:(0.521200180054) A[1]:(0.876883208752) A[2]:(-0.613449931145) A[3]:(0.843778610229)\n",
      " state (12)  A[0]:(0.0776665508747) A[1]:(0.824393451214) A[2]:(-0.594930171967) A[3]:(0.793141543865)\n",
      " state (13)  A[0]:(-0.000539332570042) A[1]:(0.809201538563) A[2]:(0.900069773197) A[3]:(0.728370189667)\n",
      " state (14)  A[0]:(0.810001552105) A[1]:(0.900266826153) A[2]:(0.999999940395) A[3]:(0.809619188309)\n",
      " state (15)  A[0]:(0.984153270721) A[1]:(0.957382440567) A[2]:(1.0) A[3]:(0.879735589027)\n",
      "Episode 689000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6025. Times reached goal: 996.               Steps done: 5297811. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00450228372876.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5907,  0.5905,  0.5325]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.6563,  0.0001,  0.5330]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566, -0.0002,  0.7291,  0.5923]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6568,  0.8099,  0.8100,  0.0026]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7296,  0.9000,  0.0001,  0.7300]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9002,  1.0000,  0.8107]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531417369843) A[1]:(0.590738415718) A[2]:(0.590558886528) A[3]:(0.531615495682)\n",
      " state (1)  A[0]:(0.531369805336) A[1]:(-3.60235571861e-05) A[2]:(0.656204521656) A[3]:(0.591172575951)\n",
      " state (2)  A[0]:(0.590517997742) A[1]:(0.729095578194) A[2]:(0.590698122978) A[3]:(0.657064318657)\n",
      " state (3)  A[0]:(0.65621483326) A[1]:(-0.216934755445) A[2]:(0.53802138567) A[3]:(0.520388484001)\n",
      " state (4)  A[0]:(0.590471029282) A[1]:(0.656207740307) A[2]:(0.000100016593933) A[3]:(0.532987356186)\n",
      " state (5)  A[0]:(0.161842525005) A[1]:(0.928914546967) A[2]:(-0.190820634365) A[3]:(0.524095892906)\n",
      " state (6)  A[0]:(-0.000394165486796) A[1]:(0.809959948063) A[2]:(0.000307083129883) A[3]:(0.657284021378)\n",
      " state (7)  A[0]:(0.629471480846) A[1]:(-0.250794202089) A[2]:(0.293101638556) A[3]:(0.886667728424)\n",
      " state (8)  A[0]:(0.656314730644) A[1]:(-0.000130884349346) A[2]:(0.729013383389) A[3]:(0.592497944832)\n",
      " state (9)  A[0]:(0.65653437376) A[1]:(0.809993684292) A[2]:(0.809968352318) A[3]:(0.00286624534056)\n",
      " state (10)  A[0]:(0.72938644886) A[1]:(0.899988472462) A[2]:(-0.000168681144714) A[3]:(0.730080008507)\n",
      " state (11)  A[0]:(0.522231936455) A[1]:(0.876733720303) A[2]:(-0.613363265991) A[3]:(0.844849050045)\n",
      " state (12)  A[0]:(0.0788535624743) A[1]:(0.824177801609) A[2]:(-0.595045268536) A[3]:(0.794447660446)\n",
      " state (13)  A[0]:(0.000261187553406) A[1]:(0.80896371603) A[2]:(0.899905025959) A[3]:(0.729932785034)\n",
      " state (14)  A[0]:(0.810093581676) A[1]:(0.900145471096) A[2]:(0.999999940395) A[3]:(0.810691177845)\n",
      " state (15)  A[0]:(0.98415094614) A[1]:(0.95734000206) A[2]:(1.0) A[3]:(0.880405366421)\n",
      "Episode 690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 995.               Steps done: 5303819. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00447531510299.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531000614166) A[1]:(0.591096639633) A[2]:(0.590539813042) A[3]:(0.532624721527)\n",
      " state (1)  A[0]:(0.531455457211) A[1]:(0.0002451390028) A[2]:(0.65626680851) A[3]:(0.591228485107)\n",
      " state (2)  A[0]:(0.590710878372) A[1]:(0.729176282883) A[2]:(0.590897381306) A[3]:(0.656757056713)\n",
      " state (3)  A[0]:(0.65663677454) A[1]:(-0.218002915382) A[2]:(0.53810274601) A[3]:(0.519331336021)\n",
      " state (4)  A[0]:(0.590942323208) A[1]:(0.656374514103) A[2]:(-0.00031566619873) A[3]:(0.531865298748)\n",
      " state (5)  A[0]:(0.162219375372) A[1]:(0.928931713104) A[2]:(-0.191295221448) A[3]:(0.522976219654)\n",
      " state (6)  A[0]:(-9.71257686615e-05) A[1]:(0.80998146534) A[2]:(-0.000185132026672) A[3]:(0.656482577324)\n",
      " state (7)  A[0]:(0.629862546921) A[1]:(-0.250832796097) A[2]:(0.29289239645) A[3]:(0.886388838291)\n",
      " state (8)  A[0]:(0.656357288361) A[1]:(-0.000283613801003) A[2]:(0.728991150856) A[3]:(0.591000676155)\n",
      " state (9)  A[0]:(0.655989289284) A[1]:(0.809962570667) A[2]:(0.809908688068) A[3]:(-0.000639468373265)\n",
      " state (10)  A[0]:(0.728716850281) A[1]:(0.899985074997) A[2]:(-0.000844478432555) A[3]:(0.728419303894)\n",
      " state (11)  A[0]:(0.520988047123) A[1]:(0.876753032207) A[2]:(-0.614046692848) A[3]:(0.843870162964)\n",
      " state (12)  A[0]:(0.0769494995475) A[1]:(0.824258744717) A[2]:(-0.59579706192) A[3]:(0.793205976486)\n",
      " state (13)  A[0]:(-0.00157379976008) A[1]:(0.809160113335) A[2]:(0.899936974049) A[3]:(0.72837293148)\n",
      " state (14)  A[0]:(0.809607863426) A[1]:(0.900342941284) A[2]:(0.999999940395) A[3]:(0.809522688389)\n",
      " state (15)  A[0]:(0.984087347984) A[1]:(0.957444787025) A[2]:(1.0) A[3]:(0.879541218281)\n",
      "Episode 691000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5999. Times reached goal: 995.               Steps done: 5309818. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00444854805572.\n",
      " state (0)  A[0]:(0.530872941017) A[1]:(0.59076821804) A[2]:(0.590562403202) A[3]:(0.531363487244)\n",
      " state (1)  A[0]:(0.530814409256) A[1]:(0.000143676996231) A[2]:(0.656244754791) A[3]:(0.59014827013)\n",
      " state (2)  A[0]:(0.58989071846) A[1]:(0.72903496027) A[2]:(0.590686440468) A[3]:(0.655819654465)\n",
      " state (3)  A[0]:(0.655599057674) A[1]:(-0.217380732298) A[2]:(0.537997663021) A[3]:(0.518246531487)\n",
      " state (4)  A[0]:(0.589605927467) A[1]:(0.656232178211) A[2]:(-0.000136256217957) A[3]:(0.530690789223)\n",
      " state (5)  A[0]:(0.160319343209) A[1]:(0.928921997547) A[2]:(-0.191132932901) A[3]:(0.521752119064)\n",
      " state (6)  A[0]:(-0.00211310060695) A[1]:(0.810011446476) A[2]:(1.4066696167e-05) A[3]:(0.65532296896)\n",
      " state (7)  A[0]:(0.628461241722) A[1]:(-0.250649333) A[2]:(0.293130844831) A[3]:(0.885804712772)\n",
      " state (8)  A[0]:(0.655458688736) A[1]:(-7.72774219513e-05) A[2]:(0.729068040848) A[3]:(0.589616656303)\n",
      " state (9)  A[0]:(0.655727207661) A[1]:(0.810014009476) A[2]:(0.809991419315) A[3]:(-0.0018175522564)\n",
      " state (10)  A[0]:(0.728706717491) A[1]:(0.900014281273) A[2]:(-0.000385284394724) A[3]:(0.72799372673)\n",
      " state (11)  A[0]:(0.521099209785) A[1]:(0.876789271832) A[2]:(-0.613745570183) A[3]:(0.84363090992)\n",
      " state (12)  A[0]:(0.0771858170629) A[1]:(0.824299812317) A[2]:(-0.595574617386) A[3]:(0.792918264866)\n",
      " state (13)  A[0]:(-0.00132650055457) A[1]:(0.809181034565) A[2]:(0.90001052618) A[3]:(0.728036999702)\n",
      " state (14)  A[0]:(0.809694766998) A[1]:(0.900340437889) A[2]:(0.999999940395) A[3]:(0.809316754341)\n",
      " state (15)  A[0]:(0.984086990356) A[1]:(0.957429409027) A[2]:(1.0) A[3]:(0.879415154457)\n",
      "Episode 692000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6018. Times reached goal: 997.               Steps done: 5315836. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0044218570872.\n",
      " state (0)  A[0]:(0.531490147114) A[1]:(0.59054172039) A[2]:(0.590542197227) A[3]:(0.531358599663)\n",
      " state (1)  A[0]:(0.531487107277) A[1]:(2.57790088654e-05) A[2]:(0.656148433685) A[3]:(0.590376615524)\n",
      " state (2)  A[0]:(0.590558409691) A[1]:(0.729076862335) A[2]:(0.590740442276) A[3]:(0.656079530716)\n",
      " state (3)  A[0]:(0.656270384789) A[1]:(-0.217095911503) A[2]:(0.538166284561) A[3]:(0.518947958946)\n",
      " state (4)  A[0]:(0.590629339218) A[1]:(0.656181097031) A[2]:(5.19752502441e-05) A[3]:(0.531480669975)\n",
      " state (5)  A[0]:(0.16231867671) A[1]:(0.92895925045) A[2]:(-0.191228955984) A[3]:(0.522606372833)\n",
      " state (6)  A[0]:(0.000280171632767) A[1]:(0.810063302517) A[2]:(-5.34057617188e-05) A[3]:(0.656008124352)\n",
      " state (7)  A[0]:(0.629566192627) A[1]:(-0.250467956066) A[2]:(0.293227374554) A[3]:(0.886015892029)\n",
      " state (8)  A[0]:(0.656074285507) A[1]:(0.000184386968613) A[2]:(0.729057431221) A[3]:(0.590533852577)\n",
      " state (9)  A[0]:(0.656079530716) A[1]:(0.810050547123) A[2]:(0.810054779053) A[3]:(6.58631324768e-05)\n",
      " state (10)  A[0]:(0.72908616066) A[1]:(0.900014340878) A[2]:(-0.000105500221252) A[3]:(0.729030549526)\n",
      " state (11)  A[0]:(0.521973848343) A[1]:(0.876753509045) A[2]:(-0.61363363266) A[3]:(0.844327151775)\n",
      " state (12)  A[0]:(0.0786861106753) A[1]:(0.824183166027) A[2]:(-0.595560073853) A[3]:(0.793838500977)\n",
      " state (13)  A[0]:(0.000216215848923) A[1]:(0.808964848518) A[2]:(0.900030136108) A[3]:(0.729165256023)\n",
      " state (14)  A[0]:(0.810068190098) A[1]:(0.900165200233) A[2]:(0.999999940395) A[3]:(0.810051202774)\n",
      " state (15)  A[0]:(0.984095871449) A[1]:(0.957319259644) A[2]:(1.0) A[3]:(0.879809975624)\n",
      "Episode 693000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6018. Times reached goal: 998.               Steps done: 5321854. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00439532626257.\n",
      " state (0)  A[0]:(0.532760858536) A[1]:(0.59062743187) A[2]:(0.590552508831) A[3]:(0.53188252449)\n",
      " state (1)  A[0]:(0.532972931862) A[1]:(7.78883695602e-05) A[2]:(0.656107008457) A[3]:(0.59022796154)\n",
      " state (2)  A[0]:(0.591952979565) A[1]:(0.729026079178) A[2]:(0.590498924255) A[3]:(0.655616998672)\n",
      " state (3)  A[0]:(0.657297849655) A[1]:(-0.216496169567) A[2]:(0.537811934948) A[3]:(0.517558574677)\n",
      " state (4)  A[0]:(0.591743946075) A[1]:(0.656132280827) A[2]:(-0.000464200944407) A[3]:(0.529618263245)\n",
      " state (5)  A[0]:(0.164073422551) A[1]:(0.928952097893) A[2]:(-0.191932201385) A[3]:(0.520517647266)\n",
      " state (6)  A[0]:(0.00240343343467) A[1]:(0.809999465942) A[2]:(-0.000784277741332) A[3]:(0.654005885124)\n",
      " state (7)  A[0]:(0.63103222847) A[1]:(-0.250717133284) A[2]:(0.292779058218) A[3]:(0.884960114956)\n",
      " state (8)  A[0]:(0.657172679901) A[1]:(-9.52631235123e-05) A[2]:(0.728880703449) A[3]:(0.586778640747)\n",
      " state (9)  A[0]:(0.65682387352) A[1]:(0.809949755669) A[2]:(0.809902787209) A[3]:(-0.00590606639162)\n",
      " state (10)  A[0]:(0.729470789433) A[1]:(0.89997959137) A[2]:(-0.000857114559039) A[3]:(0.726292610168)\n",
      " state (11)  A[0]:(0.522255778313) A[1]:(0.876734673977) A[2]:(-0.614279925823) A[3]:(0.842653274536)\n",
      " state (12)  A[0]:(0.0786710307002) A[1]:(0.824192523956) A[2]:(-0.596286714077) A[3]:(0.7916482687)\n",
      " state (13)  A[0]:(-1.27553939819e-05) A[1]:(0.809032857418) A[2]:(0.89999204874) A[3]:(0.72641992569)\n",
      " state (14)  A[0]:(0.810011029243) A[1]:(0.900246679783) A[2]:(0.999999940395) A[3]:(0.808203577995)\n",
      " state (15)  A[0]:(0.984068274498) A[1]:(0.957356274128) A[2]:(1.0) A[3]:(0.878658056259)\n",
      "Episode 694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6013. Times reached goal: 997.               Steps done: 5327867. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00436897646581.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5905,  0.5905,  0.5308]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.0001,  0.6561,  0.5902]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.7290,  0.5905,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0013,  0.8100, -0.0004,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7282,  0.9000,  0.0001,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8090,  0.9001,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531616330147) A[1]:(0.590568482876) A[2]:(0.590535402298) A[3]:(0.531211614609)\n",
      " state (1)  A[0]:(0.531465888023) A[1]:(8.06078314781e-05) A[2]:(0.656130194664) A[3]:(0.590567827225)\n",
      " state (2)  A[0]:(0.590379834175) A[1]:(0.729036927223) A[2]:(0.590551733971) A[3]:(0.656349360943)\n",
      " state (3)  A[0]:(0.655684947968) A[1]:(-0.217132270336) A[2]:(0.537926197052) A[3]:(0.519179582596)\n",
      " state (4)  A[0]:(0.589700937271) A[1]:(0.656285762787) A[2]:(-0.000364899606211) A[3]:(0.531703352928)\n",
      " state (5)  A[0]:(0.160774365067) A[1]:(0.928962469101) A[2]:(-0.191611945629) A[3]:(0.522805690765)\n",
      " state (6)  A[0]:(-0.00137039937545) A[1]:(0.810085654259) A[2]:(-0.000280737876892) A[3]:(0.655924081802)\n",
      " state (7)  A[0]:(0.628722429276) A[1]:(-0.250362664461) A[2]:(0.293376654387) A[3]:(0.88583868742)\n",
      " state (8)  A[0]:(0.655491232872) A[1]:(0.000195525586605) A[2]:(0.728964745998) A[3]:(0.590422272682)\n",
      " state (9)  A[0]:(0.655378103256) A[1]:(0.810087442398) A[2]:(0.810050964355) A[3]:(-0.00013741850853)\n",
      " state (10)  A[0]:(0.728274047375) A[1]:(0.90004992485) A[2]:(0.000168323516846) A[3]:(0.728717803955)\n",
      " state (11)  A[0]:(0.520449519157) A[1]:(0.876804172993) A[2]:(-0.613561809063) A[3]:(0.844067454338)\n",
      " state (12)  A[0]:(0.0763007476926) A[1]:(0.824262082577) A[2]:(-0.59567630291) A[3]:(0.793445646763)\n",
      " state (13)  A[0]:(-0.0023640349973) A[1]:(0.80906355381) A[2]:(0.900125205517) A[3]:(0.728641152382)\n",
      " state (14)  A[0]:(0.809246480465) A[1]:(0.900234341621) A[2]:(0.999999940395) A[3]:(0.809737145901)\n",
      " state (15)  A[0]:(0.983998000622) A[1]:(0.957334160805) A[2]:(1.0) A[3]:(0.879586160183)\n",
      "Episode 695000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6017. Times reached goal: 999.               Steps done: 5333884. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00434276726387.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531060576439) A[1]:(0.590518712997) A[2]:(0.590519189835) A[3]:(0.531516671181)\n",
      " state (1)  A[0]:(0.531389474869) A[1]:(0.000136934220791) A[2]:(0.656106710434) A[3]:(0.590152740479)\n",
      " state (2)  A[0]:(0.590719997883) A[1]:(0.728986740112) A[2]:(0.590536832809) A[3]:(0.655730009079)\n",
      " state (3)  A[0]:(0.656756699085) A[1]:(-0.217466592789) A[2]:(0.538043439388) A[3]:(0.517756700516)\n",
      " state (4)  A[0]:(0.591442942619) A[1]:(0.656199157238) A[2]:(-0.000124931335449) A[3]:(0.53022223711)\n",
      " state (5)  A[0]:(0.163787394762) A[1]:(0.928932905197) A[2]:(-0.191319227219) A[3]:(0.521484732628)\n",
      " state (6)  A[0]:(0.00218513258733) A[1]:(0.809977889061) A[2]:(5.13792037964e-05) A[3]:(0.654909968376)\n",
      " state (7)  A[0]:(0.630965530872) A[1]:(-0.25055924058) A[2]:(0.293694972992) A[3]:(0.885325193405)\n",
      " state (8)  A[0]:(0.657691597939) A[1]:(0.00016101449728) A[2]:(0.729154109955) A[3]:(0.588486433029)\n",
      " state (9)  A[0]:(0.657842636108) A[1]:(0.81004858017) A[2]:(0.810081958771) A[3]:(-0.00291553814895)\n",
      " state (10)  A[0]:(0.730626583099) A[1]:(0.900040149689) A[2]:(-5.69820404053e-05) A[3]:(0.727647662163)\n",
      " state (11)  A[0]:(0.524513542652) A[1]:(0.876824319363) A[2]:(-0.61382162571) A[3]:(0.843501567841)\n",
      " state (12)  A[0]:(0.0822365656495) A[1]:(0.824333369732) A[2]:(-0.595973968506) A[3]:(0.792773425579)\n",
      " state (13)  A[0]:(0.00393898226321) A[1]:(0.809191524982) A[2]:(0.900198221207) A[3]:(0.727809906006)\n",
      " state (14)  A[0]:(0.811527371407) A[1]:(0.900338053703) A[2]:(0.999999940395) A[3]:(0.809102773666)\n",
      " state (15)  A[0]:(0.984190583229) A[1]:(0.957374215126) A[2]:(1.0) A[3]:(0.879077911377)\n",
      "Episode 696000 finished after 0 timesteps with r=0.0. Running score: 0.99. Times trained:               6011. Times reached goal: 998.               Steps done: 5339895. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00431674118958.\n",
      " state (0)  A[0]:(0.52876663208) A[1]:(0.590670228004) A[2]:(0.590616703033) A[3]:(0.5309227705)\n",
      " state (1)  A[0]:(0.528552234173) A[1]:(7.96020030975e-05) A[2]:(0.656200289726) A[3]:(0.589686274529)\n",
      " state (2)  A[0]:(0.58771276474) A[1]:(0.72899544239) A[2]:(0.590889692307) A[3]:(0.655444383621)\n",
      " state (3)  A[0]:(0.653687238693) A[1]:(-0.218389227986) A[2]:(0.538656651974) A[3]:(0.518210291862)\n",
      " state (4)  A[0]:(0.58743429184) A[1]:(0.656128108501) A[2]:(0.000622749270406) A[3]:(0.531152129173)\n",
      " state (5)  A[0]:(0.157077565789) A[1]:(0.928912103176) A[2]:(-0.190649822354) A[3]:(0.522505879402)\n",
      " state (6)  A[0]:(-0.00535035971552) A[1]:(0.81007194519) A[2]:(0.000473022431834) A[3]:(0.655632972717)\n",
      " state (7)  A[0]:(0.626747012138) A[1]:(-0.25022855401) A[2]:(0.293796271086) A[3]:(0.885630130768)\n",
      " state (8)  A[0]:(0.654653906822) A[1]:(0.000119835138321) A[2]:(0.728982806206) A[3]:(0.589789807796)\n",
      " state (9)  A[0]:(0.655462801456) A[1]:(0.810008168221) A[2]:(0.809984207153) A[3]:(-0.00134333886672)\n",
      " state (10)  A[0]:(0.728667378426) A[1]:(0.899990618229) A[2]:(-0.000173330307007) A[3]:(0.72815322876)\n",
      " state (11)  A[0]:(0.521249234676) A[1]:(0.876738667488) A[2]:(-0.614001572132) A[3]:(0.843761324883)\n",
      " state (12)  A[0]:(0.0774937868118) A[1]:(0.824198961258) A[2]:(-0.596379458904) A[3]:(0.793101131916)\n",
      " state (13)  A[0]:(-0.000976472801995) A[1]:(0.809044897556) A[2]:(0.900041818619) A[3]:(0.728242635727)\n",
      " state (14)  A[0]:(0.809929013252) A[1]:(0.900267004967) A[2]:(0.999999940395) A[3]:(0.809445738792)\n",
      " state (15)  A[0]:(0.984050512314) A[1]:(0.957347273827) A[2]:(1.0) A[3]:(0.879318237305)\n",
      "Episode 697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6017. Times reached goal: 996.               Steps done: 5345912. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00429084534363.\n",
      " state (0)  A[0]:(0.531430840492) A[1]:(0.590414941311) A[2]:(0.590554237366) A[3]:(0.53106802702)\n",
      " state (1)  A[0]:(0.531292438507) A[1]:(-8.74996185303e-05) A[2]:(0.656076431274) A[3]:(0.59032702446)\n",
      " state (2)  A[0]:(0.590487122536) A[1]:(0.728991150856) A[2]:(0.590486288071) A[3]:(0.656152963638)\n",
      " state (3)  A[0]:(0.656115114689) A[1]:(-0.21695664525) A[2]:(0.538050234318) A[3]:(0.518731236458)\n",
      " state (4)  A[0]:(0.590600669384) A[1]:(0.656049609184) A[2]:(0.000115156173706) A[3]:(0.531310319901)\n",
      " state (5)  A[0]:(0.1625764817) A[1]:(0.928903162479) A[2]:(-0.191148102283) A[3]:(0.522733032703)\n",
      " state (6)  A[0]:(0.000406801671488) A[1]:(0.809987187386) A[2]:(0.000161170959473) A[3]:(0.656068921089)\n",
      " state (7)  A[0]:(0.629364728928) A[1]:(-0.250543087721) A[2]:(0.293871641159) A[3]:(0.885922014713)\n",
      " state (8)  A[0]:(0.656050443649) A[1]:(-2.4288892746e-05) A[2]:(0.729122877121) A[3]:(0.590744376183)\n",
      " state (9)  A[0]:(0.655996322632) A[1]:(0.810002148151) A[2]:(0.810112774372) A[3]:(0.000287994742393)\n",
      " state (10)  A[0]:(0.729063153267) A[1]:(0.900003790855) A[2]:(-4.42266464233e-05) A[3]:(0.729111194611)\n",
      " state (11)  A[0]:(0.52209854126) A[1]:(0.876772463322) A[2]:(-0.614063680172) A[3]:(0.84441190958)\n",
      " state (12)  A[0]:(0.078904889524) A[1]:(0.824270367622) A[2]:(-0.596512794495) A[3]:(0.793946325779)\n",
      " state (13)  A[0]:(0.000446736783488) A[1]:(0.809157133102) A[2]:(0.900107979774) A[3]:(0.729315340519)\n",
      " state (14)  A[0]:(0.810234844685) A[1]:(0.900356292725) A[2]:(0.999999940395) A[3]:(0.810298979282)\n",
      " state (15)  A[0]:(0.984039127827) A[1]:(0.957384467125) A[2]:(1.0) A[3]:(0.879873812199)\n",
      "Episode 698000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6022. Times reached goal: 994.               Steps done: 5351934. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00426508351967.\n",
      " state (0)  A[0]:(0.531496882439) A[1]:(0.590468645096) A[2]:(0.590447425842) A[3]:(0.531801581383)\n",
      " state (1)  A[0]:(0.531503558159) A[1]:(5.11854887009e-05) A[2]:(0.65605789423) A[3]:(0.590570926666)\n",
      " state (2)  A[0]:(0.590697646141) A[1]:(0.728985905647) A[2]:(0.590382635593) A[3]:(0.656229913235)\n",
      " state (3)  A[0]:(0.656327486038) A[1]:(-0.217038735747) A[2]:(0.537826895714) A[3]:(0.518850445747)\n",
      " state (4)  A[0]:(0.590759634972) A[1]:(0.656063020229) A[2]:(-0.000253081321716) A[3]:(0.53141772747)\n",
      " state (5)  A[0]:(0.162611752748) A[1]:(0.928891122341) A[2]:(-0.191447734833) A[3]:(0.522744238377)\n",
      " state (6)  A[0]:(0.000167548656464) A[1]:(0.809980094433) A[2]:(-0.000149965286255) A[3]:(0.65583384037)\n",
      " state (7)  A[0]:(0.629160165787) A[1]:(-0.250555962324) A[2]:(0.293604016304) A[3]:(0.885694086552)\n",
      " state (8)  A[0]:(0.656061291695) A[1]:(-5.57750463486e-05) A[2]:(0.72895026207) A[3]:(0.590093612671)\n",
      " state (9)  A[0]:(0.65618288517) A[1]:(0.809995174408) A[2]:(0.809949994087) A[3]:(-0.000422790617449)\n",
      " state (10)  A[0]:(0.729172050953) A[1]:(0.900003790855) A[2]:(-0.000429630250437) A[3]:(0.728678762913)\n",
      " state (11)  A[0]:(0.522131383419) A[1]:(0.876785755157) A[2]:(-0.614326238632) A[3]:(0.844064891338)\n",
      " state (12)  A[0]:(0.0787379667163) A[1]:(0.82430934906) A[2]:(-0.596936583519) A[3]:(0.793412804604)\n",
      " state (13)  A[0]:(-2.33054161072e-05) A[1]:(0.809209942818) A[2]:(0.899925112724) A[3]:(0.728524804115)\n",
      " state (14)  A[0]:(0.809968352318) A[1]:(0.900387227535) A[2]:(0.999999940395) A[3]:(0.809626758099)\n",
      " state (15)  A[0]:(0.984004795551) A[1]:(0.957398593426) A[2]:(1.0) A[3]:(0.87937361002)\n",
      "Episode 699000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6018. Times reached goal: 996.               Steps done: 5357952. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00423949332518.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5905,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.6560, -0.0001,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.0002,  0.7291,  0.5905]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560,  0.8100,  0.8099,  0.0005]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  0.8091,  0.9000,  0.7292]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9003,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53166949749) A[1]:(0.590481162071) A[2]:(0.590534925461) A[3]:(0.531352877617)\n",
      " state (1)  A[0]:(0.531572759151) A[1]:(-0.00011882185936) A[2]:(0.656090855598) A[3]:(0.59044611454)\n",
      " state (2)  A[0]:(0.590638816357) A[1]:(0.728958725929) A[2]:(0.590591013432) A[3]:(0.656225204468)\n",
      " state (3)  A[0]:(0.656243801117) A[1]:(-0.217544585466) A[2]:(0.538117408752) A[3]:(0.519022703171)\n",
      " state (4)  A[0]:(0.590647816658) A[1]:(0.656088352203) A[2]:(-8.07046890259e-05) A[3]:(0.531798481941)\n",
      " state (5)  A[0]:(0.162497848272) A[1]:(0.928917825222) A[2]:(-0.191439926624) A[3]:(0.523298501968)\n",
      " state (6)  A[0]:(0.000299394130707) A[1]:(0.809985637665) A[2]:(-9.52482223511e-05) A[3]:(0.656432271004)\n",
      " state (7)  A[0]:(0.629256129265) A[1]:(-0.250586807728) A[2]:(0.293785601854) A[3]:(0.88595867157)\n",
      " state (8)  A[0]:(0.656026601791) A[1]:(-2.32458114624e-05) A[2]:(0.728972196579) A[3]:(0.590987563133)\n",
      " state (9)  A[0]:(0.656049489975) A[1]:(0.809969961643) A[2]:(0.809985995293) A[3]:(0.00085924542509)\n",
      " state (10)  A[0]:(0.729044318199) A[1]:(0.899963378906) A[2]:(-0.000310897827148) A[3]:(0.729316949844)\n",
      " state (11)  A[0]:(0.521967411041) A[1]:(0.876705050468) A[2]:(-0.614337980747) A[3]:(0.844506978989)\n",
      " state (12)  A[0]:(0.0785569995642) A[1]:(0.824155211449) A[2]:(-0.597013115883) A[3]:(0.794039368629)\n",
      " state (13)  A[0]:(-0.000129878520966) A[1]:(0.809003412724) A[2]:(0.900027990341) A[3]:(0.729390144348)\n",
      " state (14)  A[0]:(0.809970140457) A[1]:(0.900255680084) A[2]:(0.999999940395) A[3]:(0.810325026512)\n",
      " state (15)  A[0]:(0.98398655653) A[1]:(0.95731574297) A[2]:(1.0) A[3]:(0.879820406437)\n",
      "Episode 700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6025. Times reached goal: 996.               Steps done: 5363977. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00421402717172.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531414091587) A[1]:(0.590710282326) A[2]:(0.590533077717) A[3]:(0.53082048893)\n",
      " state (1)  A[0]:(0.531330645084) A[1]:(0.000518031360116) A[2]:(0.656182646751) A[3]:(0.589983165264)\n",
      " state (2)  A[0]:(0.590496361256) A[1]:(0.729102730751) A[2]:(0.590541243553) A[3]:(0.655858516693)\n",
      " state (3)  A[0]:(0.656213283539) A[1]:(-0.217129826546) A[2]:(0.538161814213) A[3]:(0.518647372723)\n",
      " state (4)  A[0]:(0.590718626976) A[1]:(0.656283318996) A[2]:(9.69171524048e-05) A[3]:(0.531440258026)\n",
      " state (5)  A[0]:(0.162723168731) A[1]:(0.928956151009) A[2]:(-0.191225513816) A[3]:(0.522935986519)\n",
      " state (6)  A[0]:(0.000540614069905) A[1]:(0.810094475746) A[2]:(0.000225424766541) A[3]:(0.656157970428)\n",
      " state (7)  A[0]:(0.629390239716) A[1]:(-0.250393152237) A[2]:(0.294270217419) A[3]:(0.885883152485)\n",
      " state (8)  A[0]:(0.656188189983) A[1]:(-0.000159405171871) A[2]:(0.729093194008) A[3]:(0.591005563736)\n",
      " state (9)  A[0]:(0.656163215637) A[1]:(0.80995541811) A[2]:(0.810121715069) A[3]:(0.000530183257069)\n",
      " state (10)  A[0]:(0.72920191288) A[1]:(0.900011122227) A[2]:(-0.000169396400452) A[3]:(0.729320168495)\n",
      " state (11)  A[0]:(0.522260189056) A[1]:(0.876811265945) A[2]:(-0.614493012428) A[3]:(0.844620943069)\n",
      " state (12)  A[0]:(0.0788897275925) A[1]:(0.824357748032) A[2]:(-0.597325801849) A[3]:(0.794234037399)\n",
      " state (13)  A[0]:(0.000150501728058) A[1]:(0.80928260088) A[2]:(0.900011003017) A[3]:(0.729644298553)\n",
      " state (14)  A[0]:(0.810102581978) A[1]:(0.900443851948) A[2]:(0.999999940395) A[3]:(0.810503840446)\n",
      " state (15)  A[0]:(0.983986318111) A[1]:(0.95740288496) A[2]:(1.0) A[3]:(0.879891872406)\n",
      "Episode 701000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6024. Times reached goal: 998.               Steps done: 5370001. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00418871817925.\n",
      " state (0)  A[0]:(0.531338453293) A[1]:(0.590457201004) A[2]:(0.59050321579) A[3]:(0.531452953815)\n",
      " state (1)  A[0]:(0.531424939632) A[1]:(-6.93276524544e-05) A[2]:(0.65612745285) A[3]:(0.590398669243)\n",
      " state (2)  A[0]:(0.590562105179) A[1]:(0.728967130184) A[2]:(0.590675473213) A[3]:(0.656140983105)\n",
      " state (3)  A[0]:(0.656214356422) A[1]:(-0.217771396041) A[2]:(0.538247942924) A[3]:(0.518846154213)\n",
      " state (4)  A[0]:(0.590582728386) A[1]:(0.656111955643) A[2]:(5.48362731934e-06) A[3]:(0.531645774841)\n",
      " state (5)  A[0]:(0.162393972278) A[1]:(0.928912878036) A[2]:(-0.191413402557) A[3]:(0.523174583912)\n",
      " state (6)  A[0]:(0.000543475092854) A[1]:(0.809971094131) A[2]:(-2.53915786743e-05) A[3]:(0.656305074692)\n",
      " state (7)  A[0]:(0.629801630974) A[1]:(-0.250598728657) A[2]:(0.294091194868) A[3]:(0.885862410069)\n",
      " state (8)  A[0]:(0.656614303589) A[1]:(-0.000163413584232) A[2]:(0.729063868523) A[3]:(0.590733408928)\n",
      " state (9)  A[0]:(0.656544029713) A[1]:(0.809939324856) A[2]:(0.810049235821) A[3]:(0.000418797106249)\n",
      " state (10)  A[0]:(0.729536831379) A[1]:(0.899988472462) A[2]:(-0.000533461512532) A[3]:(0.729350805283)\n",
      " state (11)  A[0]:(0.522854447365) A[1]:(0.876780390739) A[2]:(-0.614816069603) A[3]:(0.844626784325)\n",
      " state (12)  A[0]:(0.0798221752048) A[1]:(0.824317336082) A[2]:(-0.597708582878) A[3]:(0.794213473797)\n",
      " state (13)  A[0]:(0.00130903651007) A[1]:(0.809250593185) A[2]:(0.900048971176) A[3]:(0.729615986347)\n",
      " state (14)  A[0]:(0.810625374317) A[1]:(0.900433063507) A[2]:(0.999999940395) A[3]:(0.810534894466)\n",
      " state (15)  A[0]:(0.984022080898) A[1]:(0.957383036613) A[2]:(1.0) A[3]:(0.879895329475)\n",
      "Episode 702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6025. Times reached goal: 1000.               Steps done: 5376026. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00416355702631.\n",
      " state (0)  A[0]:(0.5317029953) A[1]:(0.590557217598) A[2]:(0.590343117714) A[3]:(0.530884385109)\n",
      " state (1)  A[0]:(0.531849980354) A[1]:(0.000494033040013) A[2]:(0.656056165695) A[3]:(0.589897871017)\n",
      " state (2)  A[0]:(0.591088891029) A[1]:(0.729080915451) A[2]:(0.590238571167) A[3]:(0.655714690685)\n",
      " state (3)  A[0]:(0.657000422478) A[1]:(-0.216665223241) A[2]:(0.53779566288) A[3]:(0.518017053604)\n",
      " state (4)  A[0]:(0.591553151608) A[1]:(0.656166553497) A[2]:(-9.59634780884e-05) A[3]:(0.530573606491)\n",
      " state (5)  A[0]:(0.16372255981) A[1]:(0.928886473179) A[2]:(-0.191198974848) A[3]:(0.522013664246)\n",
      " state (6)  A[0]:(0.00152528169565) A[1]:(0.810047984123) A[2]:(0.000292301177979) A[3]:(0.655270040035)\n",
      " state (7)  A[0]:(0.630561292171) A[1]:(-0.250423461199) A[2]:(0.294448554516) A[3]:(0.885520160198)\n",
      " state (8)  A[0]:(0.657698154449) A[1]:(-0.000539720000234) A[2]:(0.729002833366) A[3]:(0.590447068214)\n",
      " state (9)  A[0]:(0.657476484776) A[1]:(0.809851527214) A[2]:(0.810056567192) A[3]:(-0.000276356935501)\n",
      " state (10)  A[0]:(0.730240941048) A[1]:(0.899993002415) A[2]:(-0.000336527824402) A[3]:(0.728836655617)\n",
      " state (11)  A[0]:(0.523949146271) A[1]:(0.876841306686) A[2]:(-0.614776968956) A[3]:(0.844289779663)\n",
      " state (12)  A[0]:(0.0812371894717) A[1]:(0.824473619461) A[2]:(-0.597828984261) A[3]:(0.793751597404)\n",
      " state (13)  A[0]:(0.00240349303931) A[1]:(0.809485852718) A[2]:(0.899982750416) A[3]:(0.728956103325)\n",
      " state (14)  A[0]:(0.810759365559) A[1]:(0.900592982769) A[2]:(0.999999940395) A[3]:(0.809954822063)\n",
      " state (15)  A[0]:(0.984006822109) A[1]:(0.957464516163) A[2]:(1.0) A[3]:(0.879436969757)\n",
      "Episode 703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 994.               Steps done: 5382033. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00413862150807.\n",
      " state (0)  A[0]:(0.531028389931) A[1]:(0.590581655502) A[2]:(0.59053260088) A[3]:(0.531455159187)\n",
      " state (1)  A[0]:(0.530883252621) A[1]:(6.33671879768e-05) A[2]:(0.656078338623) A[3]:(0.590333938599)\n",
      " state (2)  A[0]:(0.59006780386) A[1]:(0.728939414024) A[2]:(0.590196490288) A[3]:(0.65612745285)\n",
      " state (3)  A[0]:(0.655964195728) A[1]:(-0.216515928507) A[2]:(0.537725627422) A[3]:(0.518803954124)\n",
      " state (4)  A[0]:(0.590195894241) A[1]:(0.656043887138) A[2]:(-0.000122666358948) A[3]:(0.531421542168)\n",
      " state (5)  A[0]:(0.161486029625) A[1]:(0.928846955299) A[2]:(-0.191256865859) A[3]:(0.522828996181)\n",
      " state (6)  A[0]:(-0.00102862680797) A[1]:(0.809960484505) A[2]:(0.00021755695343) A[3]:(0.65560400486)\n",
      " state (7)  A[0]:(0.628871142864) A[1]:(-0.250682324171) A[2]:(0.29447132349) A[3]:(0.885415315628)\n",
      " state (8)  A[0]:(0.656084597111) A[1]:(-0.000856272643432) A[2]:(0.728966653347) A[3]:(0.589985609055)\n",
      " state (9)  A[0]:(0.65589928627) A[1]:(0.809777736664) A[2]:(0.809931635857) A[3]:(-0.000716596725397)\n",
      " state (10)  A[0]:(0.728985786438) A[1]:(0.899984300137) A[2]:(-0.0010399814928) A[3]:(0.728693246841)\n",
      " state (11)  A[0]:(0.52199780941) A[1]:(0.876863360405) A[2]:(-0.615386664867) A[3]:(0.84419375658)\n",
      " state (12)  A[0]:(0.0785774961114) A[1]:(0.824549496174) A[2]:(-0.598474264145) A[3]:(0.793595194817)\n",
      " state (13)  A[0]:(5.06043434143e-05) A[1]:(0.809634208679) A[2]:(0.900015413761) A[3]:(0.728758096695)\n",
      " state (14)  A[0]:(0.810235738754) A[1]:(0.90071439743) A[2]:(0.999999940395) A[3]:(0.809862852097)\n",
      " state (15)  A[0]:(0.983955979347) A[1]:(0.957513511181) A[2]:(1.0) A[3]:(0.879350244999)\n",
      "Episode 704000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6002. Times reached goal: 994.               Steps done: 5388035. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00411385589772.\n",
      "q_values \n",
      "tensor([[ 0.5308,  0.5904,  0.5905,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5308, -0.0002,  0.6561,  0.5902]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5900,  0.7290,  0.5904,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0006,  0.8100,  0.0001,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7287,  0.9000, -0.0007,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8096,  0.9004,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530894577503) A[1]:(0.590467691422) A[2]:(0.590503334999) A[3]:(0.531468331814)\n",
      " state (1)  A[0]:(0.531002879143) A[1]:(-0.000238582491875) A[2]:(0.65605276823) A[3]:(0.590449512005)\n",
      " state (2)  A[0]:(0.590221345425) A[1]:(0.728981137276) A[2]:(0.590485334396) A[3]:(0.656233131886)\n",
      " state (3)  A[0]:(0.656147539616) A[1]:(-0.216764137149) A[2]:(0.538132667542) A[3]:(0.518831968307)\n",
      " state (4)  A[0]:(0.590466976166) A[1]:(0.656098008156) A[2]:(0.000216484069824) A[3]:(0.531508088112)\n",
      " state (5)  A[0]:(0.161940991879) A[1]:(0.928895771503) A[2]:(-0.191224470735) A[3]:(0.523095667362)\n",
      " state (6)  A[0]:(-0.000257909297943) A[1]:(0.810010194778) A[2]:(0.000123262405396) A[3]:(0.656105875969)\n",
      " state (7)  A[0]:(0.629213213921) A[1]:(-0.250171035528) A[2]:(0.294324338436) A[3]:(0.885663568974)\n",
      " state (8)  A[0]:(0.656084418297) A[1]:(0.000392004818423) A[2]:(0.729090094566) A[3]:(0.590219497681)\n",
      " state (9)  A[0]:(0.655853748322) A[1]:(0.810077428818) A[2]:(0.810047149658) A[3]:(-0.00035415586899)\n",
      " state (10)  A[0]:(0.728749454021) A[1]:(0.900016546249) A[2]:(-0.000570297183003) A[3]:(0.728744387627)\n",
      " state (11)  A[0]:(0.521376490593) A[1]:(0.876789867878) A[2]:(-0.615087509155) A[3]:(0.844147264957)\n",
      " state (12)  A[0]:(0.0775458067656) A[1]:(0.8243278265) A[2]:(-0.598309278488) A[3]:(0.793528437614)\n",
      " state (13)  A[0]:(-0.00116723717656) A[1]:(0.809277176857) A[2]:(0.900053858757) A[3]:(0.728715121746)\n",
      " state (14)  A[0]:(0.80972391367) A[1]:(0.900458335876) A[2]:(0.999999940395) A[3]:(0.809874117374)\n",
      " state (15)  A[0]:(0.983894169331) A[1]:(0.957370340824) A[2]:(1.0) A[3]:(0.879377424717)\n",
      "Episode 705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 996.               Steps done: 5394049. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00408918941481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530536949635) A[1]:(0.590578436852) A[2]:(0.590554893017) A[3]:(0.531770467758)\n",
      " state (1)  A[0]:(0.530254840851) A[1]:(-0.000377416581614) A[2]:(0.656088113785) A[3]:(0.590582787991)\n",
      " state (2)  A[0]:(0.58925563097) A[1]:(0.729173183441) A[2]:(0.59077000618) A[3]:(0.656213402748)\n",
      " state (3)  A[0]:(0.654797077179) A[1]:(-0.216245889664) A[2]:(0.538323163986) A[3]:(0.518857836723)\n",
      " state (4)  A[0]:(0.588583827019) A[1]:(0.656243145466) A[2]:(0.000438451737864) A[3]:(0.531429588795)\n",
      " state (5)  A[0]:(0.158991158009) A[1]:(0.928939461708) A[2]:(-0.191059783101) A[3]:(0.522936999798)\n",
      " state (6)  A[0]:(-0.00326960114762) A[1]:(0.81010645628) A[2]:(0.00041031834553) A[3]:(0.655969619751)\n",
      " state (7)  A[0]:(0.627377688885) A[1]:(-0.249981284142) A[2]:(0.29484307766) A[3]:(0.885601997375)\n",
      " state (8)  A[0]:(0.654645323753) A[1]:(0.000441551179392) A[2]:(0.729263782501) A[3]:(0.589992105961)\n",
      " state (9)  A[0]:(0.654901027679) A[1]:(0.810151457787) A[2]:(0.810217022896) A[3]:(-0.00136886455584)\n",
      " state (10)  A[0]:(0.728337883949) A[1]:(0.900110423565) A[2]:(-0.000456452340586) A[3]:(0.728406190872)\n",
      " state (11)  A[0]:(0.521060585976) A[1]:(0.876929044724) A[2]:(-0.615310668945) A[3]:(0.844050168991)\n",
      " state (12)  A[0]:(0.0772946551442) A[1]:(0.824522197247) A[2]:(-0.598726868629) A[3]:(0.793436765671)\n",
      " state (13)  A[0]:(-0.00127690960653) A[1]:(0.80946791172) A[2]:(0.899986743927) A[3]:(0.728547096252)\n",
      " state (14)  A[0]:(0.809787929058) A[1]:(0.900541126728) A[2]:(0.999999940395) A[3]:(0.809631466866)\n",
      " state (15)  A[0]:(0.983894765377) A[1]:(0.957384943962) A[2]:(1.0) A[3]:(0.879099428654)\n",
      "Episode 706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 997.               Steps done: 5400063. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00406467083095.\n",
      " state (0)  A[0]:(0.531684398651) A[1]:(0.590545535088) A[2]:(0.590545356274) A[3]:(0.531488418579)\n",
      " state (1)  A[0]:(0.531410098076) A[1]:(-0.000130906701088) A[2]:(0.656098365784) A[3]:(0.590464293957)\n",
      " state (2)  A[0]:(0.590425133705) A[1]:(0.729017794132) A[2]:(0.590637862682) A[3]:(0.656180381775)\n",
      " state (3)  A[0]:(0.656183362007) A[1]:(-0.216979727149) A[2]:(0.538296580315) A[3]:(0.518782198429)\n",
      " state (4)  A[0]:(0.5904545784) A[1]:(0.656116783619) A[2]:(0.000201344490051) A[3]:(0.531448662281)\n",
      " state (5)  A[0]:(0.161939188838) A[1]:(0.928911089897) A[2]:(-0.191455662251) A[3]:(0.523020625114)\n",
      " state (6)  A[0]:(-0.000184655189514) A[1]:(0.810007750988) A[2]:(-0.000100016593933) A[3]:(0.656049370766)\n",
      " state (7)  A[0]:(0.629219174385) A[1]:(-0.25031927228) A[2]:(0.294371366501) A[3]:(0.885639429092)\n",
      " state (8)  A[0]:(0.656101822853) A[1]:(0.000146344304085) A[2]:(0.72899055481) A[3]:(0.590429604053)\n",
      " state (9)  A[0]:(0.655900299549) A[1]:(0.810019910336) A[2]:(0.809975802898) A[3]:(8.41170549393e-05)\n",
      " state (10)  A[0]:(0.728854358196) A[1]:(0.90000218153) A[2]:(-0.00106596911792) A[3]:(0.729083538055)\n",
      " state (11)  A[0]:(0.521598339081) A[1]:(0.876773715019) A[2]:(-0.61569750309) A[3]:(0.84441691637)\n",
      " state (12)  A[0]:(0.077831543982) A[1]:(0.824296534061) A[2]:(-0.599149465561) A[3]:(0.793877780437)\n",
      " state (13)  A[0]:(-0.000822305504698) A[1]:(0.80923473835) A[2]:(0.90000128746) A[3]:(0.72908103466)\n",
      " state (14)  A[0]:(0.809926986694) A[1]:(0.900426387787) A[2]:(0.999999940395) A[3]:(0.809984743595)\n",
      " state (15)  A[0]:(0.983884811401) A[1]:(0.957325160503) A[2]:(1.0) A[3]:(0.879257678986)\n",
      "Episode 707000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5994. Times reached goal: 995.               Steps done: 5406057. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00404038006614.\n",
      " state (0)  A[0]:(0.531274080276) A[1]:(0.590773105621) A[2]:(0.590504765511) A[3]:(0.532514214516)\n",
      " state (1)  A[0]:(0.531654596329) A[1]:(0.00011084228754) A[2]:(0.656135797501) A[3]:(0.590607762337)\n",
      " state (2)  A[0]:(0.59103679657) A[1]:(0.728980183601) A[2]:(0.590757727623) A[3]:(0.655913889408)\n",
      " state (3)  A[0]:(0.657075643539) A[1]:(-0.217902377248) A[2]:(0.538303911686) A[3]:(0.517857849598)\n",
      " state (4)  A[0]:(0.591925382614) A[1]:(0.655897140503) A[2]:(-0.000123023986816) A[3]:(0.530313432217)\n",
      " state (5)  A[0]:(0.164668619633) A[1]:(0.928878426552) A[2]:(-0.191845968366) A[3]:(0.521759986877)\n",
      " state (6)  A[0]:(0.00282417982817) A[1]:(0.809945583344) A[2]:(-0.000408530206187) A[3]:(0.654828190804)\n",
      " state (7)  A[0]:(0.630896568298) A[1]:(-0.250405788422) A[2]:(0.294279694557) A[3]:(0.884985268116)\n",
      " state (8)  A[0]:(0.657894611359) A[1]:(0.000110603868961) A[2]:(0.728913068771) A[3]:(0.588075041771)\n",
      " state (9)  A[0]:(0.658022165298) A[1]:(0.809998631477) A[2]:(0.809965133667) A[3]:(-0.00430804817006)\n",
      " state (10)  A[0]:(0.730859875679) A[1]:(0.899984776974) A[2]:(-0.000836252991576) A[3]:(0.726841807365)\n",
      " state (11)  A[0]:(0.525116503239) A[1]:(0.876748383045) A[2]:(-0.615534067154) A[3]:(0.843047380447)\n",
      " state (12)  A[0]:(0.0830455869436) A[1]:(0.824247479439) A[2]:(-0.599109888077) A[3]:(0.792159080505)\n",
      " state (13)  A[0]:(0.00435212487355) A[1]:(0.809140086174) A[2]:(0.899944543839) A[3]:(0.726965785027)\n",
      " state (14)  A[0]:(0.811403632164) A[1]:(0.900336742401) A[2]:(0.999999940395) A[3]:(0.808509945869)\n",
      " state (15)  A[0]:(0.983990550041) A[1]:(0.957267343998) A[2]:(1.0) A[3]:(0.878331124783)\n",
      "Episode 708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 995.               Steps done: 5412065. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00401617823779.\n",
      " state (0)  A[0]:(0.532004952431) A[1]:(0.590513110161) A[2]:(0.590463757515) A[3]:(0.531242012978)\n",
      " state (1)  A[0]:(0.531822323799) A[1]:(7.6062977314e-05) A[2]:(0.656096458435) A[3]:(0.590263605118)\n",
      " state (2)  A[0]:(0.590936183929) A[1]:(0.728983640671) A[2]:(0.590535402298) A[3]:(0.656061649323)\n",
      " state (3)  A[0]:(0.65655040741) A[1]:(-0.217626750469) A[2]:(0.538145601749) A[3]:(0.518591701984)\n",
      " state (4)  A[0]:(0.590949892998) A[1]:(0.656084358692) A[2]:(-0.000110983848572) A[3]:(0.531397223473)\n",
      " state (5)  A[0]:(0.162805050611) A[1]:(0.928883612156) A[2]:(-0.19162389636) A[3]:(0.523089587688)\n",
      " state (6)  A[0]:(0.000556826533284) A[1]:(0.809988677502) A[2]:(-0.000133752822876) A[3]:(0.656063318253)\n",
      " state (7)  A[0]:(0.629586935043) A[1]:(-0.250281274319) A[2]:(0.294601291418) A[3]:(0.885597884655)\n",
      " state (8)  A[0]:(0.656627655029) A[1]:(0.000147469341755) A[2]:(0.729072928429) A[3]:(0.590486288071)\n",
      " state (9)  A[0]:(0.656472086906) A[1]:(0.810008704662) A[2]:(0.810024797916) A[3]:(0.000323012471199)\n",
      " state (10)  A[0]:(0.729350030422) A[1]:(0.899981021881) A[2]:(-0.000763654534239) A[3]:(0.729190349579)\n",
      " state (11)  A[0]:(0.522512614727) A[1]:(0.876744031906) A[2]:(-0.615593194962) A[3]:(0.844503581524)\n",
      " state (12)  A[0]:(0.0792377740145) A[1]:(0.824254631996) A[2]:(-0.599309921265) A[3]:(0.794036448002)\n",
      " state (13)  A[0]:(0.000455260247691) A[1]:(0.809172689915) A[2]:(0.899928629398) A[3]:(0.72933369875)\n",
      " state (14)  A[0]:(0.810183703899) A[1]:(0.900375366211) A[2]:(0.999999940395) A[3]:(0.810226202011)\n",
      " state (15)  A[0]:(0.983877062798) A[1]:(0.957285404205) A[2]:(1.0) A[3]:(0.879420638084)\n",
      "Episode 709000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 998.               Steps done: 5418071. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00399212936236.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5905,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6561,  0.0000,  0.5310]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.0002,  0.7289,  0.5902]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.8101,  0.8101, -0.0005]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  0.8091,  0.9000,  0.7290]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9003,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531391263008) A[1]:(0.590560793877) A[2]:(0.590351939201) A[3]:(0.531200647354)\n",
      " state (1)  A[0]:(0.531326413155) A[1]:(0.000549189688172) A[2]:(0.656049251556) A[3]:(0.590180516243)\n",
      " state (2)  A[0]:(0.590518951416) A[1]:(0.729103922844) A[2]:(0.59050899744) A[3]:(0.655929327011)\n",
      " state (3)  A[0]:(0.655951142311) A[1]:(-0.217554718256) A[2]:(0.53826379776) A[3]:(0.518429160118)\n",
      " state (4)  A[0]:(0.590300917625) A[1]:(0.656212210655) A[2]:(5.49554824829e-05) A[3]:(0.531193196774)\n",
      " state (5)  A[0]:(0.162065193057) A[1]:(0.928948938847) A[2]:(-0.191529273987) A[3]:(0.522835969925)\n",
      " state (6)  A[0]:(-0.000212430953979) A[1]:(0.810147047043) A[2]:(6.79492950439e-05) A[3]:(0.655810594559)\n",
      " state (7)  A[0]:(0.628800868988) A[1]:(-0.24989195168) A[2]:(0.29494830966) A[3]:(0.88546192646)\n",
      " state (8)  A[0]:(0.655912816525) A[1]:(0.00056888902327) A[2]:(0.729155123234) A[3]:(0.590255618095)\n",
      " state (9)  A[0]:(0.655957579613) A[1]:(0.810156345367) A[2]:(0.810125172138) A[3]:(6.83963298798e-06)\n",
      " state (10)  A[0]:(0.729006052017) A[1]:(0.900070786476) A[2]:(-0.000328421592712) A[3]:(0.729060947895)\n",
      " state (11)  A[0]:(0.5220246315) A[1]:(0.876857042313) A[2]:(-0.615369915962) A[3]:(0.844431638718)\n",
      " state (12)  A[0]:(0.0785796865821) A[1]:(0.824408650398) A[2]:(-0.599162638187) A[3]:(0.793924808502)\n",
      " state (13)  A[0]:(-0.00016325712204) A[1]:(0.809326291084) A[2]:(0.900040388107) A[3]:(0.729154706001)\n",
      " state (14)  A[0]:(0.810046136379) A[1]:(0.90044683218) A[2]:(0.999999940395) A[3]:(0.810063660145)\n",
      " state (15)  A[0]:(0.9838565588) A[1]:(0.95729893446) A[2]:(1.0) A[3]:(0.879260122776)\n",
      "Episode 710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6028. Times reached goal: 997.               Steps done: 5424099. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00396813719161.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531654238701) A[1]:(0.590471863747) A[2]:(0.590446591377) A[3]:(0.530992090702)\n",
      " state (1)  A[0]:(0.531609892845) A[1]:(0.000115036964417) A[2]:(0.656193256378) A[3]:(0.590327739716)\n",
      " state (2)  A[0]:(0.590832650661) A[1]:(0.729059100151) A[2]:(0.590711116791) A[3]:(0.656190872192)\n",
      " state (3)  A[0]:(0.656675457954) A[1]:(-0.217494949698) A[2]:(0.538449048996) A[3]:(0.518904209137)\n",
      " state (4)  A[0]:(0.591182649136) A[1]:(0.65630531311) A[2]:(0.000318288803101) A[3]:(0.531728863716)\n",
      " state (5)  A[0]:(0.163097247481) A[1]:(0.92892575264) A[2]:(-0.191238954663) A[3]:(0.523327827454)\n",
      " state (6)  A[0]:(0.000579833926167) A[1]:(0.810124278069) A[2]:(0.000209808349609) A[3]:(0.655971169472)\n",
      " state (7)  A[0]:(0.629433989525) A[1]:(-0.249952450395) A[2]:(0.295145958662) A[3]:(0.885359346867)\n",
      " state (8)  A[0]:(0.656594872475) A[1]:(0.000460803479655) A[2]:(0.729553103447) A[3]:(0.589439630508)\n",
      " state (9)  A[0]:(0.656799793243) A[1]:(0.810086607933) A[2]:(0.810192346573) A[3]:(-6.36130571365e-05)\n",
      " state (10)  A[0]:(0.729761123657) A[1]:(0.900045216084) A[2]:(-0.000884651904926) A[3]:(0.729466319084)\n",
      " state (11)  A[0]:(0.523121774197) A[1]:(0.876891076565) A[2]:(-0.615935146809) A[3]:(0.844709813595)\n",
      " state (12)  A[0]:(0.0798864886165) A[1]:(0.824580311775) A[2]:(-0.599811851978) A[3]:(0.794231176376)\n",
      " state (13)  A[0]:(0.00095719069941) A[1]:(0.809685468674) A[2]:(0.900021255016) A[3]:(0.729458034039)\n",
      " state (14)  A[0]:(0.810328006744) A[1]:(0.900757908821) A[2]:(0.999999940395) A[3]:(0.810229301453)\n",
      " state (15)  A[0]:(0.983846366405) A[1]:(0.957470953465) A[2]:(1.0) A[3]:(0.879267096519)\n",
      "Episode 711000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 996.               Steps done: 5430108. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00394436415286.\n",
      " state (0)  A[0]:(0.531378507614) A[1]:(0.590405523777) A[2]:(0.59051823616) A[3]:(0.531494617462)\n",
      " state (1)  A[0]:(0.531293570995) A[1]:(8.4213912487e-05) A[2]:(0.656181335449) A[3]:(0.590469419956)\n",
      " state (2)  A[0]:(0.590477347374) A[1]:(0.729042172432) A[2]:(0.590693235397) A[3]:(0.65619468689)\n",
      " state (3)  A[0]:(0.656010627747) A[1]:(-0.217526271939) A[2]:(0.538269877434) A[3]:(0.518628954887)\n",
      " state (4)  A[0]:(0.5903429389) A[1]:(0.656106829643) A[2]:(1.52587890625e-05) A[3]:(0.531393408775)\n",
      " state (5)  A[0]:(0.162029966712) A[1]:(0.928896784782) A[2]:(-0.191544428468) A[3]:(0.523147821426)\n",
      " state (6)  A[0]:(-0.000351846218109) A[1]:(0.810054540634) A[2]:(5.72204589844e-05) A[3]:(0.656079649925)\n",
      " state (7)  A[0]:(0.628779768944) A[1]:(-0.250078260899) A[2]:(0.295069783926) A[3]:(0.885556161404)\n",
      " state (8)  A[0]:(0.655956864357) A[1]:(0.000267647206783) A[2]:(0.729135632515) A[3]:(0.59049987793)\n",
      " state (9)  A[0]:(0.655839920044) A[1]:(0.810057461262) A[2]:(0.810088276863) A[3]:(-0.000164404511452)\n",
      " state (10)  A[0]:(0.728882789612) A[1]:(0.900016307831) A[2]:(-0.000501394213643) A[3]:(0.728855967522)\n",
      " state (11)  A[0]:(0.521911859512) A[1]:(0.876787483692) A[2]:(-0.615638971329) A[3]:(0.844299852848)\n",
      " state (12)  A[0]:(0.0785064101219) A[1]:(0.824301302433) A[2]:(-0.599622666836) A[3]:(0.793761610985)\n",
      " state (13)  A[0]:(-0.000198423862457) A[1]:(0.809189915657) A[2]:(0.900019705296) A[3]:(0.728981614113)\n",
      " state (14)  A[0]:(0.810047984123) A[1]:(0.900352776051) A[2]:(0.999999940395) A[3]:(0.81002342701)\n",
      " state (15)  A[0]:(0.983830571175) A[1]:(0.957227945328) A[2]:(1.0) A[3]:(0.87923002243)\n",
      "Episode 712000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6012. Times reached goal: 994.               Steps done: 5436120. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00392072177577.\n",
      " state (0)  A[0]:(0.535146534443) A[1]:(0.590483367443) A[2]:(0.590425252914) A[3]:(0.531957089901)\n",
      " state (1)  A[0]:(0.535736680031) A[1]:(-0.000395156414015) A[2]:(0.656014204025) A[3]:(0.590532839298)\n",
      " state (2)  A[0]:(0.594725012779) A[1]:(0.728948235512) A[2]:(0.59046459198) A[3]:(0.656267285347)\n",
      " state (3)  A[0]:(0.65942722559) A[1]:(-0.216958522797) A[2]:(0.538034558296) A[3]:(0.519590377808)\n",
      " state (4)  A[0]:(0.594065129757) A[1]:(0.655958414078) A[2]:(-6.69956207275e-05) A[3]:(0.53265941143)\n",
      " state (5)  A[0]:(0.167638853192) A[1]:(0.928843557835) A[2]:(-0.191606089473) A[3]:(0.524565041065)\n",
      " state (6)  A[0]:(0.00604359898716) A[1]:(0.809943974018) A[2]:(-2.50339508057e-06) A[3]:(0.657158911228)\n",
      " state (7)  A[0]:(0.633662462234) A[1]:(-0.250475227833) A[2]:(0.295093297958) A[3]:(0.885890603065)\n",
      " state (8)  A[0]:(0.66112613678) A[1]:(-0.000404097110732) A[2]:(0.729052186012) A[3]:(0.591098964214)\n",
      " state (9)  A[0]:(0.661188364029) A[1]:(0.809832811356) A[2]:(0.810013413429) A[3]:(-0.000450238556368)\n",
      " state (10)  A[0]:(0.733150064945) A[1]:(0.899906098843) A[2]:(-0.000890373950824) A[3]:(0.728683590889)\n",
      " state (11)  A[0]:(0.528136372566) A[1]:(0.876667320728) A[2]:(-0.616028428078) A[3]:(0.844272375107)\n",
      " state (12)  A[0]:(0.0863367244601) A[1]:(0.824160337448) A[2]:(-0.600062608719) A[3]:(0.793800532818)\n",
      " state (13)  A[0]:(0.00692755822092) A[1]:(0.809086084366) A[2]:(0.900071680546) A[3]:(0.729084789753)\n",
      " state (14)  A[0]:(0.812157034874) A[1]:(0.900334358215) A[2]:(0.999999940395) A[3]:(0.810086727142)\n",
      " state (15)  A[0]:(0.983970761299) A[1]:(0.957218110561) A[2]:(1.0) A[3]:(0.87919151783)\n",
      "Episode 713000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 998.               Steps done: 5442134. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0038972133158.\n",
      " state (0)  A[0]:(0.532093644142) A[1]:(0.590491175652) A[2]:(0.590575218201) A[3]:(0.531670689583)\n",
      " state (1)  A[0]:(0.531848430634) A[1]:(-0.000323176383972) A[2]:(0.656081438065) A[3]:(0.590879678726)\n",
      " state (2)  A[0]:(0.590953350067) A[1]:(0.728898644447) A[2]:(0.5904494524) A[3]:(0.656707644463)\n",
      " state (3)  A[0]:(0.656456708908) A[1]:(-0.217603296041) A[2]:(0.538011312485) A[3]:(0.519680261612)\n",
      " state (4)  A[0]:(0.590802907944) A[1]:(0.656148016453) A[2]:(-0.000326633453369) A[3]:(0.532611429691)\n",
      " state (5)  A[0]:(0.162576422095) A[1]:(0.928865551949) A[2]:(-0.191804975271) A[3]:(0.524415254593)\n",
      " state (6)  A[0]:(0.0001460313797) A[1]:(0.809871554375) A[2]:(-6.69956207275e-05) A[3]:(0.657055139542)\n",
      " state (7)  A[0]:(0.628959536552) A[1]:(-0.250728517771) A[2]:(0.295047044754) A[3]:(0.885924637318)\n",
      " state (8)  A[0]:(0.655981659889) A[1]:(-0.000481545896037) A[2]:(0.728699564934) A[3]:(0.592255353928)\n",
      " state (9)  A[0]:(0.655520558357) A[1]:(0.809862971306) A[2]:(0.809744119644) A[3]:(0.00242152344435)\n",
      " state (10)  A[0]:(0.728424847126) A[1]:(0.899944663048) A[2]:(-0.00143730535638) A[3]:(0.729919075966)\n",
      " state (11)  A[0]:(0.521007180214) A[1]:(0.87673664093) A[2]:(-0.616415798664) A[3]:(0.844918370247)\n",
      " state (12)  A[0]:(0.0769911184907) A[1]:(0.824295580387) A[2]:(-0.600650668144) A[3]:(0.794494986534)\n",
      " state (13)  A[0]:(-0.00206145341508) A[1]:(0.809275090694) A[2]:(0.899818837643) A[3]:(0.729780554771)\n",
      " state (14)  A[0]:(0.80925732851) A[1]:(0.900462865829) A[2]:(0.999999940395) A[3]:(0.810443878174)\n",
      " state (15)  A[0]:(0.983718454838) A[1]:(0.95728969574) A[2]:(1.0) A[3]:(0.879340946674)\n",
      "Episode 714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 997.               Steps done: 5448138. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00387388455009.\n",
      "q_values \n",
      "tensor([[ 0.5330,  0.5904,  0.5905,  0.5340]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5337,  0.0001,  0.6560,  0.5922]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5931,  0.7290,  0.5905,  0.6572]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0054,  0.8099, -0.0009,  0.6571]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000, -0.0004,  0.7294]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9001,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532616198063) A[1]:(0.590345144272) A[2]:(0.590479373932) A[3]:(0.530135273933)\n",
      " state (1)  A[0]:(0.532859802246) A[1]:(7.53700733185e-05) A[2]:(0.656060218811) A[3]:(0.589330911636)\n",
      " state (2)  A[0]:(0.591987013817) A[1]:(0.728855013847) A[2]:(0.59048974514) A[3]:(0.655276477337)\n",
      " state (3)  A[0]:(0.657307386398) A[1]:(-0.217234298587) A[2]:(0.53831589222) A[3]:(0.517772614956)\n",
      " state (4)  A[0]:(0.591946780682) A[1]:(0.656146526337) A[2]:(-0.000126361846924) A[3]:(0.530780673027)\n",
      " state (5)  A[0]:(0.164725109935) A[1]:(0.928958654404) A[2]:(-0.192295566201) A[3]:(0.522832751274)\n",
      " state (6)  A[0]:(0.00262671103701) A[1]:(0.809978842735) A[2]:(-0.000753641012125) A[3]:(0.656001210213)\n",
      " state (7)  A[0]:(0.62964963913) A[1]:(-0.250261157751) A[2]:(0.294575721025) A[3]:(0.885405302048)\n",
      " state (8)  A[0]:(0.655950546265) A[1]:(0.000826120201964) A[2]:(0.728872895241) A[3]:(0.590214371681)\n",
      " state (9)  A[0]:(0.655589580536) A[1]:(0.810139894485) A[2]:(0.809923350811) A[3]:(0.000625431479421)\n",
      " state (10)  A[0]:(0.728448271751) A[1]:(0.899949848652) A[2]:(-0.000772595230956) A[3]:(0.729200601578)\n",
      " state (11)  A[0]:(0.520980417728) A[1]:(0.876607179642) A[2]:(-0.615955471992) A[3]:(0.844426870346)\n",
      " state (12)  A[0]:(0.0770250707865) A[1]:(0.82394695282) A[2]:(-0.600300788879) A[3]:(0.793872833252)\n",
      " state (13)  A[0]:(-0.00179925363045) A[1]:(0.808710336685) A[2]:(0.899913966656) A[3]:(0.729074001312)\n",
      " state (14)  A[0]:(0.809552192688) A[1]:(0.900044739246) A[2]:(0.999999940395) A[3]:(0.810036122799)\n",
      " state (15)  A[0]:(0.98375916481) A[1]:(0.957050144672) A[2]:(1.0) A[3]:(0.879137516022)\n",
      "Episode 715000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 998.               Steps done: 5454148. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00385067232654.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.529996573925) A[1]:(0.590611100197) A[2]:(0.590479969978) A[3]:(0.531337618828)\n",
      " state (1)  A[0]:(0.530087351799) A[1]:(0.000143550336361) A[2]:(0.656205058098) A[3]:(0.590460300446)\n",
      " state (2)  A[0]:(0.589539170265) A[1]:(0.729068994522) A[2]:(0.590516090393) A[3]:(0.656297683716)\n",
      " state (3)  A[0]:(0.655693292618) A[1]:(-0.216068163514) A[2]:(0.53808516264) A[3]:(0.518794298172)\n",
      " state (4)  A[0]:(0.590179800987) A[1]:(0.656326532364) A[2]:(1.04904174805e-05) A[3]:(0.531533837318)\n",
      " state (5)  A[0]:(0.161877080798) A[1]:(0.928926229477) A[2]:(-0.191661670804) A[3]:(0.52349460125)\n",
      " state (6)  A[0]:(-0.000334620475769) A[1]:(0.81004357338) A[2]:(8.74996185303e-05) A[3]:(0.656334638596)\n",
      " state (7)  A[0]:(0.628777086735) A[1]:(-0.250242054462) A[2]:(0.295531362295) A[3]:(0.885527253151)\n",
      " state (8)  A[0]:(0.656087458134) A[1]:(0.000114373862743) A[2]:(0.729110836983) A[3]:(0.590842545033)\n",
      " state (9)  A[0]:(0.656136751175) A[1]:(0.810033202171) A[2]:(0.810015320778) A[3]:(0.000673547270708)\n",
      " state (10)  A[0]:(0.729193508625) A[1]:(0.900015413761) A[2]:(-0.000962376303505) A[3]:(0.729268312454)\n",
      " state (11)  A[0]:(0.522394776344) A[1]:(0.876798093319) A[2]:(-0.616361439228) A[3]:(0.844567835331)\n",
      " state (12)  A[0]:(0.0790307000279) A[1]:(0.824342906475) A[2]:(-0.600820422173) A[3]:(0.794081032276)\n",
      " state (13)  A[0]:(0.000221371650696) A[1]:(0.809284806252) A[2]:(0.899925172329) A[3]:(0.729317724705)\n",
      " state (14)  A[0]:(0.81023645401) A[1]:(0.900447666645) A[2]:(0.999999940395) A[3]:(0.810196578503)\n",
      " state (15)  A[0]:(0.983794271946) A[1]:(0.957248091698) A[2]:(1.0) A[3]:(0.879161715508)\n",
      "Episode 716000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 997.               Steps done: 5460162. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00382758387972.\n",
      " state (0)  A[0]:(0.531171441078) A[1]:(0.590619683266) A[2]:(0.590617060661) A[3]:(0.531202614307)\n",
      " state (1)  A[0]:(0.531149804592) A[1]:(-4.49270009995e-06) A[2]:(0.655822873116) A[3]:(0.590418219566)\n",
      " state (2)  A[0]:(0.590348899364) A[1]:(0.728908002377) A[2]:(0.590114712715) A[3]:(0.65619790554)\n",
      " state (3)  A[0]:(0.655817806721) A[1]:(-0.218678712845) A[2]:(0.538107514381) A[3]:(0.518639802933)\n",
      " state (4)  A[0]:(0.590103030205) A[1]:(0.656593680382) A[2]:(-0.000433683366282) A[3]:(0.531699895859)\n",
      " state (5)  A[0]:(0.161760419607) A[1]:(0.928982138634) A[2]:(-0.191946670413) A[3]:(0.523702979088)\n",
      " state (6)  A[0]:(4.25577163696e-05) A[1]:(0.81000316143) A[2]:(9.83476638794e-05) A[3]:(0.656648159027)\n",
      " state (7)  A[0]:(0.629260480404) A[1]:(-0.250596404076) A[2]:(0.295654207468) A[3]:(0.885730743408)\n",
      " state (8)  A[0]:(0.656560063362) A[1]:(-0.000196568667889) A[2]:(0.728762626648) A[3]:(0.591997504234)\n",
      " state (9)  A[0]:(0.656393051147) A[1]:(0.809944808483) A[2]:(0.809791564941) A[3]:(0.00195568543859)\n",
      " state (10)  A[0]:(0.729214668274) A[1]:(0.899979352951) A[2]:(-0.00112700415775) A[3]:(0.729630529881)\n",
      " state (11)  A[0]:(0.522295534611) A[1]:(0.876760840416) A[2]:(-0.616402506828) A[3]:(0.844768702984)\n",
      " state (12)  A[0]:(0.0787392109632) A[1]:(0.824297308922) A[2]:(-0.600892364979) A[3]:(0.794335365295)\n",
      " state (13)  A[0]:(-0.000327229499817) A[1]:(0.809237122536) A[2]:(0.899968981743) A[3]:(0.729598760605)\n",
      " state (14)  A[0]:(0.809895813465) A[1]:(0.900419771671) A[2]:(0.999999940395) A[3]:(0.810314297676)\n",
      " state (15)  A[0]:(0.983733832836) A[1]:(0.957224011421) A[2]:(1.0) A[3]:(0.879142165184)\n",
      "Episode 717000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6005. Times reached goal: 994.               Steps done: 5466167. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00380466811198.\n",
      " state (0)  A[0]:(0.531528115273) A[1]:(0.590544819832) A[2]:(0.590600788593) A[3]:(0.531504273415)\n",
      " state (1)  A[0]:(0.531232714653) A[1]:(-0.000251233577728) A[2]:(0.656121611595) A[3]:(0.590419530869)\n",
      " state (2)  A[0]:(0.590262651443) A[1]:(0.728954911232) A[2]:(0.590829610825) A[3]:(0.656139612198)\n",
      " state (3)  A[0]:(0.656153023243) A[1]:(-0.217543333769) A[2]:(0.538583397865) A[3]:(0.518927395344)\n",
      " state (4)  A[0]:(0.590633034706) A[1]:(0.656006515026) A[2]:(0.000203132629395) A[3]:(0.531988382339)\n",
      " state (5)  A[0]:(0.162585124373) A[1]:(0.928911566734) A[2]:(-0.191862612963) A[3]:(0.524139046669)\n",
      " state (6)  A[0]:(0.000656902673654) A[1]:(0.809990644455) A[2]:(-7.79628753662e-05) A[3]:(0.656847000122)\n",
      " state (7)  A[0]:(0.629373788834) A[1]:(-0.250353127718) A[2]:(0.295647442341) A[3]:(0.885610461235)\n",
      " state (8)  A[0]:(0.656623125076) A[1]:(2.24709510803e-05) A[2]:(0.728940844536) A[3]:(0.591114878654)\n",
      " state (9)  A[0]:(0.65656799078) A[1]:(0.810012757778) A[2]:(0.809997916222) A[3]:(0.000187531113625)\n",
      " state (10)  A[0]:(0.729521632195) A[1]:(0.90000885725) A[2]:(-0.000835776154418) A[3]:(0.728968501091)\n",
      " state (11)  A[0]:(0.522947430611) A[1]:(0.876760959625) A[2]:(-0.616440832615) A[3]:(0.844473838806)\n",
      " state (12)  A[0]:(0.0797472521663) A[1]:(0.824220478535) A[2]:(-0.60107666254) A[3]:(0.794054627419)\n",
      " state (13)  A[0]:(0.000785291020293) A[1]:(0.809053778648) A[2]:(0.899999380112) A[3]:(0.72939902544)\n",
      " state (14)  A[0]:(0.810334205627) A[1]:(0.900258243084) A[2]:(0.999999940395) A[3]:(0.81038826704)\n",
      " state (15)  A[0]:(0.983761370182) A[1]:(0.957109391689) A[2]:(1.0) A[3]:(0.879296779633)\n",
      "Episode 718000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6013. Times reached goal: 995.               Steps done: 5472180. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00378185928608.\n",
      " state (0)  A[0]:(0.531612873077) A[1]:(0.590536832809) A[2]:(0.590494632721) A[3]:(0.531705379486)\n",
      " state (1)  A[0]:(0.531598508358) A[1]:(-0.000256471335888) A[2]:(0.656200528145) A[3]:(0.590503096581)\n",
      " state (2)  A[0]:(0.59071457386) A[1]:(0.7291187644) A[2]:(0.590795993805) A[3]:(0.656134724617)\n",
      " state (3)  A[0]:(0.656358122826) A[1]:(-0.216009035707) A[2]:(0.538289546967) A[3]:(0.518954217434)\n",
      " state (4)  A[0]:(0.590801239014) A[1]:(0.656127870083) A[2]:(9.2625617981e-05) A[3]:(0.531746804714)\n",
      " state (5)  A[0]:(0.16292463243) A[1]:(0.928911328316) A[2]:(-0.19181497395) A[3]:(0.523703277111)\n",
      " state (6)  A[0]:(0.000569224299397) A[1]:(0.810061573982) A[2]:(0.000162839889526) A[3]:(0.656178236008)\n",
      " state (7)  A[0]:(0.628973782063) A[1]:(-0.250238239765) A[2]:(0.29621142149) A[3]:(0.88522875309)\n",
      " state (8)  A[0]:(0.656330823898) A[1]:(-0.000185690820217) A[2]:(0.729129314423) A[3]:(0.590489983559)\n",
      " state (9)  A[0]:(0.656241297722) A[1]:(0.809935331345) A[2]:(0.810085177422) A[3]:(-1.96695327759e-06)\n",
      " state (10)  A[0]:(0.729294121265) A[1]:(0.900006353855) A[2]:(-0.000275492668152) A[3]:(0.728916049004)\n",
      " state (11)  A[0]:(0.522775530815) A[1]:(0.8768247962) A[2]:(-0.616032004356) A[3]:(0.84443295002)\n",
      " state (12)  A[0]:(0.0797000527382) A[1]:(0.824404418468) A[2]:(-0.600766897202) A[3]:(0.793968439102)\n",
      " state (13)  A[0]:(0.000692188623361) A[1]:(0.809341907501) A[2]:(0.900022625923) A[3]:(0.729171395302)\n",
      " state (14)  A[0]:(0.810228943825) A[1]:(0.900462150574) A[2]:(0.999999940395) A[3]:(0.810063481331)\n",
      " state (15)  A[0]:(0.983741700649) A[1]:(0.957219362259) A[2]:(1.0) A[3]:(0.878960847855)\n",
      "Episode 719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 989.               Steps done: 5478181. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00375923230865.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5905,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5887,  0.6564, -0.0000,  0.5302]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560,  0.0004,  0.7291,  0.5882]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.8101,  0.8100, -0.0020]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  0.8091,  0.8999,  0.7277]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9003,  1.0000,  0.8091]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530403256416) A[1]:(0.590444803238) A[2]:(0.590413331985) A[3]:(0.53135061264)\n",
      " state (1)  A[0]:(0.530619740486) A[1]:(0.000135041773319) A[2]:(0.656120657921) A[3]:(0.590665459633)\n",
      " state (2)  A[0]:(0.589734315872) A[1]:(0.72897207737) A[2]:(0.590672492981) A[3]:(0.65625846386)\n",
      " state (3)  A[0]:(0.654702305794) A[1]:(-0.217378914356) A[2]:(0.538506448269) A[3]:(0.518143236637)\n",
      " state (4)  A[0]:(0.588516592979) A[1]:(0.656310856342) A[2]:(0.000178098678589) A[3]:(0.53064507246)\n",
      " state (5)  A[0]:(0.159599378705) A[1]:(0.928957462311) A[2]:(-0.191719323397) A[3]:(0.522417426109)\n",
      " state (6)  A[0]:(-0.00165432540234) A[1]:(0.809999346733) A[2]:(0.000383138627512) A[3]:(0.655022025108)\n",
      " state (7)  A[0]:(0.62856554985) A[1]:(-0.250302433968) A[2]:(0.296360522509) A[3]:(0.884651660919)\n",
      " state (8)  A[0]:(0.656485855579) A[1]:(0.000497080327477) A[2]:(0.729127526283) A[3]:(0.589067459106)\n",
      " state (9)  A[0]:(0.656894147396) A[1]:(0.810094296932) A[2]:(0.810051500797) A[3]:(-0.000791191880126)\n",
      " state (10)  A[0]:(0.729779362679) A[1]:(0.899999082088) A[2]:(0.000251531600952) A[3]:(0.728313207626)\n",
      " state (11)  A[0]:(0.523449063301) A[1]:(0.876740157604) A[2]:(-0.615500926971) A[3]:(0.843863010406)\n",
      " state (12)  A[0]:(0.0806564763188) A[1]:(0.824204325676) A[2]:(-0.600333213806) A[3]:(0.793110251427)\n",
      " state (13)  A[0]:(0.00160336354747) A[1]:(0.809029996395) A[2]:(0.900067567825) A[3]:(0.728018224239)\n",
      " state (14)  A[0]:(0.81049233675) A[1]:(0.900238156319) A[2]:(0.999999940395) A[3]:(0.80923885107)\n",
      " state (15)  A[0]:(0.983763217926) A[1]:(0.957097232342) A[2]:(1.0) A[3]:(0.878447830677)\n",
      "Episode 720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6016. Times reached goal: 996.               Steps done: 5484197. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00373668465842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531612157822) A[1]:(0.590222358704) A[2]:(0.590580701828) A[3]:(0.530748784542)\n",
      " state (1)  A[0]:(0.531611204147) A[1]:(-0.000182084739208) A[2]:(0.656045496464) A[3]:(0.590212106705)\n",
      " state (2)  A[0]:(0.590669333935) A[1]:(0.72895026207) A[2]:(0.590360224247) A[3]:(0.656046271324)\n",
      " state (3)  A[0]:(0.656134366989) A[1]:(-0.216150835156) A[2]:(0.538130402565) A[3]:(0.518402814865)\n",
      " state (4)  A[0]:(0.590437054634) A[1]:(0.656094431877) A[2]:(3.60012054443e-05) A[3]:(0.531090378761)\n",
      " state (5)  A[0]:(0.162472486496) A[1]:(0.928925991058) A[2]:(-0.191900506616) A[3]:(0.523193717003)\n",
      " state (6)  A[0]:(0.000569939555135) A[1]:(0.810024619102) A[2]:(0.000269770622253) A[3]:(0.655979514122)\n",
      " state (7)  A[0]:(0.629041314125) A[1]:(-0.250198870897) A[2]:(0.296390295029) A[3]:(0.885213434696)\n",
      " state (8)  A[0]:(0.656451165676) A[1]:(0.00014091283083) A[2]:(0.728808760643) A[3]:(0.590936422348)\n",
      " state (9)  A[0]:(0.65630364418) A[1]:(0.810087203979) A[2]:(0.809957921505) A[3]:(-6.07967376709e-06)\n",
      " state (10)  A[0]:(0.729258537292) A[1]:(0.900092720985) A[2]:(-0.000107049942017) A[3]:(0.728715538979)\n",
      " state (11)  A[0]:(0.522761464119) A[1]:(0.876909255981) A[2]:(-0.615949034691) A[3]:(0.844328045845)\n",
      " state (12)  A[0]:(0.0797971785069) A[1]:(0.824480295181) A[2]:(-0.600795149803) A[3]:(0.793879628181)\n",
      " state (13)  A[0]:(0.000988840707578) A[1]:(0.8093791008) A[2]:(0.900196909904) A[3]:(0.729162335396)\n",
      " state (14)  A[0]:(0.810468912125) A[1]:(0.900465726852) A[2]:(0.999999940395) A[3]:(0.810219883919)\n",
      " state (15)  A[0]:(0.983741879463) A[1]:(0.957185447216) A[2]:(1.0) A[3]:(0.879095315933)\n",
      "Episode 721000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6005. Times reached goal: 998.               Steps done: 5490202. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00371431310486.\n",
      " state (0)  A[0]:(0.531569778919) A[1]:(0.589865386486) A[2]:(0.590594112873) A[3]:(0.531710684299)\n",
      " state (1)  A[0]:(0.531279444695) A[1]:(-0.000507809163537) A[2]:(0.656028270721) A[3]:(0.590726494789)\n",
      " state (2)  A[0]:(0.590320527554) A[1]:(0.72892665863) A[2]:(0.590214490891) A[3]:(0.656437039375)\n",
      " state (3)  A[0]:(0.65621060133) A[1]:(-0.214525654912) A[2]:(0.537816107273) A[3]:(0.519375622272)\n",
      " state (4)  A[0]:(0.59065079689) A[1]:(0.655638694763) A[2]:(-5.24520874023e-06) A[3]:(0.531914234161)\n",
      " state (5)  A[0]:(0.162750095129) A[1]:(0.928833246231) A[2]:(-0.192191556096) A[3]:(0.523956298828)\n",
      " state (6)  A[0]:(0.000353574723704) A[1]:(0.809971690178) A[2]:(-0.000353097886546) A[3]:(0.656344175339)\n",
      " state (7)  A[0]:(0.628597378731) A[1]:(-0.250031799078) A[2]:(0.295782238245) A[3]:(0.885224878788)\n",
      " state (8)  A[0]:(0.655970692635) A[1]:(0.000225655734539) A[2]:(0.728770494461) A[3]:(0.590844035149)\n",
      " state (9)  A[0]:(0.655824780464) A[1]:(0.810011148453) A[2]:(0.809910476208) A[3]:(0.000851854449138)\n",
      " state (10)  A[0]:(0.728832006454) A[1]:(0.89999717474) A[2]:(-0.00045275685261) A[3]:(0.729248166084)\n",
      " state (11)  A[0]:(0.522019684315) A[1]:(0.876762449741) A[2]:(-0.61627048254) A[3]:(0.844592571259)\n",
      " state (12)  A[0]:(0.0786968022585) A[1]:(0.824254930019) A[2]:(-0.601266205311) A[3]:(0.794134378433)\n",
      " state (13)  A[0]:(-0.000180900096893) A[1]:(0.809127569199) A[2]:(0.900085330009) A[3]:(0.729396224022)\n",
      " state (14)  A[0]:(0.810071349144) A[1]:(0.900336682796) A[2]:(0.999999940395) A[3]:(0.810352921486)\n",
      " state (15)  A[0]:(0.983695030212) A[1]:(0.957126140594) A[2]:(1.0) A[3]:(0.879142045975)\n",
      "Episode 722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5995. Times reached goal: 996.               Steps done: 5496197. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00369211241087.\n",
      " state (0)  A[0]:(0.531652569771) A[1]:(0.590688228607) A[2]:(0.590422272682) A[3]:(0.531741380692)\n",
      " state (1)  A[0]:(0.531676411629) A[1]:(0.000144883990288) A[2]:(0.656106710434) A[3]:(0.590959310532)\n",
      " state (2)  A[0]:(0.590669751167) A[1]:(0.728948831558) A[2]:(0.59045279026) A[3]:(0.65660405159)\n",
      " state (3)  A[0]:(0.656183123589) A[1]:(-0.216625645757) A[2]:(0.538116931915) A[3]:(0.518924832344)\n",
      " state (4)  A[0]:(0.590261816978) A[1]:(0.656046748161) A[2]:(-0.000239133834839) A[3]:(0.531509876251)\n",
      " state (5)  A[0]:(0.161646708846) A[1]:(0.92887365818) A[2]:(-0.19225114584) A[3]:(0.52347433567)\n",
      " state (6)  A[0]:(-0.00079137069406) A[1]:(0.809964895248) A[2]:(-0.000284790992737) A[3]:(0.65596395731)\n",
      " state (7)  A[0]:(0.628288447857) A[1]:(-0.250214964151) A[2]:(0.296000003815) A[3]:(0.885089576244)\n",
      " state (8)  A[0]:(0.655905365944) A[1]:(-0.000105395913124) A[2]:(0.728854954243) A[3]:(0.590257227421)\n",
      " state (9)  A[0]:(0.655877351761) A[1]:(0.809906482697) A[2]:(0.809952199459) A[3]:(-0.000491172017064)\n",
      " state (10)  A[0]:(0.728959679604) A[1]:(0.899974644184) A[2]:(-0.000620007456746) A[3]:(0.728711366653)\n",
      " state (11)  A[0]:(0.522260189056) A[1]:(0.876787424088) A[2]:(-0.616583108902) A[3]:(0.844304680824)\n",
      " state (12)  A[0]:(0.0789532214403) A[1]:(0.824371576309) A[2]:(-0.601741671562) A[3]:(0.793747186661)\n",
      " state (13)  A[0]:(-0.000134885311127) A[1]:(0.809358298779) A[2]:(0.900002837181) A[3]:(0.728835403919)\n",
      " state (14)  A[0]:(0.809930562973) A[1]:(0.900531291962) A[2]:(0.999999940395) A[3]:(0.809854567051)\n",
      " state (15)  A[0]:(0.983652353287) A[1]:(0.957235217094) A[2]:(1.0) A[3]:(0.878720164299)\n",
      "Episode 723000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5997. Times reached goal: 995.               Steps done: 5502194. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0036700370718.\n",
      " state (0)  A[0]:(0.53146481514) A[1]:(0.590325653553) A[2]:(0.590316295624) A[3]:(0.531313180923)\n",
      " state (1)  A[0]:(0.53143966198) A[1]:(-0.000208757817745) A[2]:(0.655983448029) A[3]:(0.590336859226)\n",
      " state (2)  A[0]:(0.59056520462) A[1]:(0.728989899158) A[2]:(0.590465664864) A[3]:(0.656061470509)\n",
      " state (3)  A[0]:(0.656154870987) A[1]:(-0.216434463859) A[2]:(0.538237571716) A[3]:(0.518629431725)\n",
      " state (4)  A[0]:(0.590493500233) A[1]:(0.656057178974) A[2]:(-2.99215316772e-05) A[3]:(0.531465411186)\n",
      " state (5)  A[0]:(0.162442773581) A[1]:(0.928896188736) A[2]:(-0.192161127925) A[3]:(0.523603141308)\n",
      " state (6)  A[0]:(0.00034761428833) A[1]:(0.809951007366) A[2]:(-0.000159382820129) A[3]:(0.656055808067)\n",
      " state (7)  A[0]:(0.628855526447) A[1]:(-0.250312060118) A[2]:(0.296139866114) A[3]:(0.885031163692)\n",
      " state (8)  A[0]:(0.656371235847) A[1]:(5.7227909565e-05) A[2]:(0.728765726089) A[3]:(0.590335845947)\n",
      " state (9)  A[0]:(0.656287193298) A[1]:(0.809946656227) A[2]:(0.809853553772) A[3]:(8.82893800735e-05)\n",
      " state (10)  A[0]:(0.72908103466) A[1]:(0.899944841862) A[2]:(-0.000589966715779) A[3]:(0.728856980801)\n",
      " state (11)  A[0]:(0.522242248058) A[1]:(0.876692652702) A[2]:(-0.616553425789) A[3]:(0.844349980354)\n",
      " state (12)  A[0]:(0.0787685960531) A[1]:(0.824165761471) A[2]:(-0.602010369301) A[3]:(0.793822288513)\n",
      " state (13)  A[0]:(-0.000540018023457) A[1]:(0.809037923813) A[2]:(0.899685263634) A[3]:(0.72898542881)\n",
      " state (14)  A[0]:(0.809815645218) A[1]:(0.900302231312) A[2]:(0.999999940395) A[3]:(0.81007373333)\n",
      " state (15)  A[0]:(0.983661770821) A[1]:(0.957123219967) A[2]:(1.0) A[3]:(0.878978908062)\n",
      "Episode 724000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6008. Times reached goal: 997.               Steps done: 5508202. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00364805359357.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5905,  0.5905,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.0001,  0.6561,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7290,  0.5907,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0007,  0.8101,  0.0002,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7287,  0.9000, -0.0005,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9004,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531557500362) A[1]:(0.590456783772) A[2]:(0.590427517891) A[3]:(0.531045377254)\n",
      " state (1)  A[0]:(0.531456947327) A[1]:(6.7412853241e-05) A[2]:(0.656059086323) A[3]:(0.590251982212)\n",
      " state (2)  A[0]:(0.590516269207) A[1]:(0.72906690836) A[2]:(0.590730309486) A[3]:(0.655923843384)\n",
      " state (3)  A[0]:(0.655929803848) A[1]:(-0.217895254493) A[2]:(0.538539707661) A[3]:(0.518125116825)\n",
      " state (4)  A[0]:(0.590172171593) A[1]:(0.656179308891) A[2]:(-5.07831573486e-05) A[3]:(0.530990719795)\n",
      " state (5)  A[0]:(0.16187441349) A[1]:(0.928932785988) A[2]:(-0.192141160369) A[3]:(0.523056864738)\n",
      " state (6)  A[0]:(-0.000451147527201) A[1]:(0.810028076172) A[2]:(5.00679016113e-05) A[3]:(0.655646204948)\n",
      " state (7)  A[0]:(0.628101110458) A[1]:(-0.250151753426) A[2]:(0.296707034111) A[3]:(0.884898722172)\n",
      " state (8)  A[0]:(0.655608892441) A[1]:(0.000198900699615) A[2]:(0.729130625725) A[3]:(0.589806675911)\n",
      " state (9)  A[0]:(0.655696034431) A[1]:(0.810021340847) A[2]:(0.810067772865) A[3]:(-0.000664770486765)\n",
      " state (10)  A[0]:(0.728759348392) A[1]:(0.90001052618) A[2]:(-0.000367522210581) A[3]:(0.728761553764)\n",
      " state (11)  A[0]:(0.521837413311) A[1]:(0.876789450645) A[2]:(-0.616587042809) A[3]:(0.84436160326)\n",
      " state (12)  A[0]:(0.0782532319427) A[1]:(0.82431602478) A[2]:(-0.601952195168) A[3]:(0.793826818466)\n",
      " state (13)  A[0]:(-0.000715374830179) A[1]:(0.809243261814) A[2]:(0.900081932545) A[3]:(0.728930473328)\n",
      " state (14)  A[0]:(0.810006558895) A[1]:(0.900452375412) A[2]:(0.999999940395) A[3]:(0.809914290905)\n",
      " state (15)  A[0]:(0.983655154705) A[1]:(0.957169473171) A[2]:(1.0) A[3]:(0.878679871559)\n",
      "Episode 725000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6004. Times reached goal: 994.               Steps done: 5514206. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00362621630095.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531808495522) A[1]:(0.590601086617) A[2]:(0.590564727783) A[3]:(0.531392931938)\n",
      " state (1)  A[0]:(0.531640946865) A[1]:(0.000112131237984) A[2]:(0.656220674515) A[3]:(0.590349435806)\n",
      " state (2)  A[0]:(0.590654492378) A[1]:(0.729124188423) A[2]:(0.590849041939) A[3]:(0.655983626842)\n",
      " state (3)  A[0]:(0.656305670738) A[1]:(-0.217137023807) A[2]:(0.538637518883) A[3]:(0.518508851528)\n",
      " state (4)  A[0]:(0.590566039085) A[1]:(0.656382918358) A[2]:(0.000218391418457) A[3]:(0.531439781189)\n",
      " state (5)  A[0]:(0.162253290415) A[1]:(0.928952693939) A[2]:(-0.191835179925) A[3]:(0.523535847664)\n",
      " state (6)  A[0]:(-9.22083854675e-05) A[1]:(0.810121953487) A[2]:(0.00038111206959) A[3]:(0.656023025513)\n",
      " state (7)  A[0]:(0.628608644009) A[1]:(-0.249791219831) A[2]:(0.297046720982) A[3]:(0.885079920292)\n",
      " state (8)  A[0]:(0.656211853027) A[1]:(0.000542029680219) A[2]:(0.7290995121) A[3]:(0.590584099293)\n",
      " state (9)  A[0]:(0.656000316143) A[1]:(0.810166478157) A[2]:(0.810029923916) A[3]:(3.26633453369e-05)\n",
      " state (10)  A[0]:(0.728893756866) A[1]:(0.900086224079) A[2]:(-0.000208973884583) A[3]:(0.728896021843)\n",
      " state (11)  A[0]:(0.522115826607) A[1]:(0.876869022846) A[2]:(-0.616488575935) A[3]:(0.844434022903)\n",
      " state (12)  A[0]:(0.0787607133389) A[1]:(0.824403226376) A[2]:(-0.602002024651) A[3]:(0.793948590755)\n",
      " state (13)  A[0]:(-0.000327050685883) A[1]:(0.80929672718) A[2]:(0.900014698505) A[3]:(0.729085803032)\n",
      " state (14)  A[0]:(0.809960722923) A[1]:(0.900453090668) A[2]:(0.999999940395) A[3]:(0.809975028038)\n",
      " state (15)  A[0]:(0.983629763126) A[1]:(0.957154870033) A[2]:(1.0) A[3]:(0.8786714077)\n",
      "Episode 726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 997.               Steps done: 5520217. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00360448449514.\n",
      " state (0)  A[0]:(0.531301140785) A[1]:(0.590361118317) A[2]:(0.590414106846) A[3]:(0.531275033951)\n",
      " state (1)  A[0]:(0.530855476856) A[1]:(-0.00046574321459) A[2]:(0.655989766121) A[3]:(0.590360283852)\n",
      " state (2)  A[0]:(0.589619398117) A[1]:(0.729038476944) A[2]:(0.591099262238) A[3]:(0.655825376511)\n",
      " state (3)  A[0]:(0.655019700527) A[1]:(-0.21864695847) A[2]:(0.539143383503) A[3]:(0.517939984798)\n",
      " state (4)  A[0]:(0.588933169842) A[1]:(0.656304240227) A[2]:(0.000278949737549) A[3]:(0.530754685402)\n",
      " state (5)  A[0]:(0.15956273675) A[1]:(0.928990960121) A[2]:(-0.192326337099) A[3]:(0.522569060326)\n",
      " state (6)  A[0]:(-0.00348858605139) A[1]:(0.810074687004) A[2]:(-0.00031578540802) A[3]:(0.654935836792)\n",
      " state (7)  A[0]:(0.625240087509) A[1]:(-0.250039488077) A[2]:(0.296534925699) A[3]:(0.884362101555)\n",
      " state (8)  A[0]:(0.65175640583) A[1]:(0.000557817460503) A[2]:(0.729057073593) A[3]:(0.587310433388)\n",
      " state (9)  A[0]:(0.650922775269) A[1]:(0.81012493372) A[2]:(0.80994707346) A[3]:(-0.00517301121727)\n",
      " state (10)  A[0]:(0.724469184875) A[1]:(0.900049269199) A[2]:(-0.00117051543202) A[3]:(0.726547241211)\n",
      " state (11)  A[0]:(0.514911532402) A[1]:(0.876820147038) A[2]:(-0.617412090302) A[3]:(0.842922329903)\n",
      " state (12)  A[0]:(0.0685847848654) A[1]:(0.824353933334) A[2]:(-0.603052437305) A[3]:(0.791845679283)\n",
      " state (13)  A[0]:(-0.0106320548803) A[1]:(0.809311747551) A[2]:(0.899910509586) A[3]:(0.726273238659)\n",
      " state (14)  A[0]:(0.806448221207) A[1]:(0.900523662567) A[2]:(0.999999940395) A[3]:(0.807851970196)\n",
      " state (15)  A[0]:(0.983277440071) A[1]:(0.957197189331) A[2]:(1.0) A[3]:(0.877185821533)\n",
      "Episode 727000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 995.               Steps done: 5526227. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00358288651028.\n",
      " state (0)  A[0]:(0.53158891201) A[1]:(0.590513885021) A[2]:(0.590502977371) A[3]:(0.531433880329)\n",
      " state (1)  A[0]:(0.531478524208) A[1]:(1.48266553879e-06) A[2]:(0.656142354012) A[3]:(0.590475440025)\n",
      " state (2)  A[0]:(0.590620398521) A[1]:(0.729016542435) A[2]:(0.590631604195) A[3]:(0.656163692474)\n",
      " state (3)  A[0]:(0.656146764755) A[1]:(-0.217262744904) A[2]:(0.538444459438) A[3]:(0.518609166145)\n",
      " state (4)  A[0]:(0.590510249138) A[1]:(0.656125545502) A[2]:(4.88758087158e-06) A[3]:(0.531536340714)\n",
      " state (5)  A[0]:(0.16247805953) A[1]:(0.928884506226) A[2]:(-0.192102238536) A[3]:(0.523748755455)\n",
      " state (6)  A[0]:(4.20808792114e-05) A[1]:(0.809996604919) A[2]:(8.08238983154e-05) A[3]:(0.656246542931)\n",
      " state (7)  A[0]:(0.628505289555) A[1]:(-0.250138789415) A[2]:(0.296922683716) A[3]:(0.885194599628)\n",
      " state (8)  A[0]:(0.656315684319) A[1]:(-1.08033418655e-06) A[2]:(0.729015648365) A[3]:(0.591061115265)\n",
      " state (9)  A[0]:(0.656331598759) A[1]:(0.81000161171) A[2]:(0.810011923313) A[3]:(0.000544771493878)\n",
      " state (10)  A[0]:(0.729233026505) A[1]:(0.899999380112) A[2]:(-0.000365495652659) A[3]:(0.729188203812)\n",
      " state (11)  A[0]:(0.522646427155) A[1]:(0.876752078533) A[2]:(-0.616784691811) A[3]:(0.844661593437)\n",
      " state (12)  A[0]:(0.0794058665633) A[1]:(0.824220359325) A[2]:(-0.602476716042) A[3]:(0.794265151024)\n",
      " state (13)  A[0]:(0.000320553779602) A[1]:(0.809087157249) A[2]:(0.900033295155) A[3]:(0.729499101639)\n",
      " state (14)  A[0]:(0.810274720192) A[1]:(0.900344729424) A[2]:(0.999999940395) A[3]:(0.810289561749)\n",
      " state (15)  A[0]:(0.983636498451) A[1]:(0.95708668232) A[2]:(1.0) A[3]:(0.878811597824)\n",
      "Episode 728000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5987. Times reached goal: 992.               Steps done: 5532214. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00356149985358.\n",
      " state (0)  A[0]:(0.531527042389) A[1]:(0.590763986111) A[2]:(0.590717673302) A[3]:(0.531658530235)\n",
      " state (1)  A[0]:(0.531355082989) A[1]:(0.000330068171024) A[2]:(0.65638333559) A[3]:(0.590571045876)\n",
      " state (2)  A[0]:(0.590515494347) A[1]:(0.729193091393) A[2]:(0.590591013432) A[3]:(0.65625834465)\n",
      " state (3)  A[0]:(0.656605899334) A[1]:(-0.216025978327) A[2]:(0.538014888763) A[3]:(0.518595397472)\n",
      " state (4)  A[0]:(0.590982079506) A[1]:(0.656410634518) A[2]:(-0.000464200944407) A[3]:(0.531371831894)\n",
      " state (5)  A[0]:(0.162598639727) A[1]:(0.928887486458) A[2]:(-0.192369163036) A[3]:(0.523512482643)\n",
      " state (6)  A[0]:(-0.000504434050526) A[1]:(0.81006538868) A[2]:(-0.000202775001526) A[3]:(0.655902504921)\n",
      " state (7)  A[0]:(0.628142356873) A[1]:(-0.249819919467) A[2]:(0.296811699867) A[3]:(0.884991466999)\n",
      " state (8)  A[0]:(0.656161189079) A[1]:(0.000331737101078) A[2]:(0.729208350182) A[3]:(0.590176939964)\n",
      " state (9)  A[0]:(0.656300544739) A[1]:(0.810068666935) A[2]:(0.81008374691) A[3]:(-0.000272035598755)\n",
      " state (10)  A[0]:(0.729074716568) A[1]:(0.900008440018) A[2]:(-0.00021231174469) A[3]:(0.728855013847)\n",
      " state (11)  A[0]:(0.522127747536) A[1]:(0.876770079136) A[2]:(-0.616716146469) A[3]:(0.844398319721)\n",
      " state (12)  A[0]:(0.0783741921186) A[1]:(0.824278414249) A[2]:(-0.602550566196) A[3]:(0.793874621391)\n",
      " state (13)  A[0]:(-0.00102257693652) A[1]:(0.809195280075) A[2]:(0.900000274181) A[3]:(0.728948712349)\n",
      " state (14)  A[0]:(0.809756159782) A[1]:(0.900438845158) A[2]:(0.999999940395) A[3]:(0.809846043587)\n",
      " state (15)  A[0]:(0.98357629776) A[1]:(0.957140266895) A[2]:(1.0) A[3]:(0.878477752209)\n",
      "Episode 729000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 995.               Steps done: 5538220. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00354017359233.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5904,  0.5905,  0.5319]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.0002,  0.6561,  0.5908]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7291,  0.5905,  0.6565]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0009,  0.8100, -0.0002,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7296,  0.9000, -0.0000,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8109,  0.9004,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531300783157) A[1]:(0.590333223343) A[2]:(0.590496063232) A[3]:(0.531583070755)\n",
      " state (1)  A[0]:(0.531147480011) A[1]:(0.000119619071484) A[2]:(0.656101644039) A[3]:(0.590449213982)\n",
      " state (2)  A[0]:(0.590371668339) A[1]:(0.728994488716) A[2]:(0.590459465981) A[3]:(0.656074285507)\n",
      " state (3)  A[0]:(0.656048536301) A[1]:(-0.217177450657) A[2]:(0.538275361061) A[3]:(0.518265366554)\n",
      " state (4)  A[0]:(0.590510964394) A[1]:(0.65597486496) A[2]:(-0.000181555747986) A[3]:(0.531085252762)\n",
      " state (5)  A[0]:(0.162628009915) A[1]:(0.928853034973) A[2]:(-0.192363649607) A[3]:(0.523291230202)\n",
      " state (6)  A[0]:(0.000207304954529) A[1]:(0.809954285622) A[2]:(-0.000212073326111) A[3]:(0.655723810196)\n",
      " state (7)  A[0]:(0.628531455994) A[1]:(-0.250225245953) A[2]:(0.296796917915) A[3]:(0.884864449501)\n",
      " state (8)  A[0]:(0.656338274479) A[1]:(-0.000114388763905) A[2]:(0.728942036629) A[3]:(0.590085864067)\n",
      " state (9)  A[0]:(0.656262934208) A[1]:(0.809958815575) A[2]:(0.810001671314) A[3]:(-0.000777780835051)\n",
      " state (10)  A[0]:(0.729165554047) A[1]:(0.899969577789) A[2]:(-0.000326991081238) A[3]:(0.728706777096)\n",
      " state (11)  A[0]:(0.522577047348) A[1]:(0.876711964607) A[2]:(-0.616896569729) A[3]:(0.844426929951)\n",
      " state (12)  A[0]:(0.0793060064316) A[1]:(0.824158251286) A[2]:(-0.602861940861) A[3]:(0.793993353844)\n",
      " state (13)  A[0]:(0.000205159187317) A[1]:(0.809008836746) A[2]:(0.899910271168) A[3]:(0.72911131382)\n",
      " state (14)  A[0]:(0.810360014439) A[1]:(0.900304675102) A[2]:(0.999999940395) A[3]:(0.809894919395)\n",
      " state (15)  A[0]:(0.983641982079) A[1]:(0.957059979439) A[2]:(1.0) A[3]:(0.878432691097)\n",
      "Episode 730000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6024. Times reached goal: 998.               Steps done: 5544244. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00351891169175.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.529552400112) A[1]:(0.590210199356) A[2]:(0.5905854702) A[3]:(0.530418395996)\n",
      " state (1)  A[0]:(0.528595685959) A[1]:(-0.000630341412034) A[2]:(0.655957579613) A[3]:(0.58978497982)\n",
      " state (2)  A[0]:(0.587590098381) A[1]:(0.729024171829) A[2]:(0.59059804678) A[3]:(0.655728578568)\n",
      " state (3)  A[0]:(0.653156518936) A[1]:(-0.216737419367) A[2]:(0.538781046867) A[3]:(0.518610298634)\n",
      " state (4)  A[0]:(0.586832225323) A[1]:(0.656008481979) A[2]:(0.000851869350299) A[3]:(0.531829953194)\n",
      " state (5)  A[0]:(0.157012462616) A[1]:(0.928844094276) A[2]:(-0.191371366382) A[3]:(0.524285852909)\n",
      " state (6)  A[0]:(-0.00494207907468) A[1]:(0.809744238853) A[2]:(0.000822663132567) A[3]:(0.656767785549)\n",
      " state (7)  A[0]:(0.625651061535) A[1]:(-0.250725001097) A[2]:(0.297384053469) A[3]:(0.885372817516)\n",
      " state (8)  A[0]:(0.653844475746) A[1]:(1.45509839058e-05) A[2]:(0.72869682312) A[3]:(0.592729687691)\n",
      " state (9)  A[0]:(0.653933048248) A[1]:(0.810052633286) A[2]:(0.809731721878) A[3]:(0.00430189399049)\n",
      " state (10)  A[0]:(0.727136731148) A[1]:(0.900016963482) A[2]:(-0.000635385455098) A[3]:(0.73071461916)\n",
      " state (11)  A[0]:(0.519357800484) A[1]:(0.876780331135) A[2]:(-0.617005109787) A[3]:(0.845479667187)\n",
      " state (12)  A[0]:(0.074974603951) A[1]:(0.824297845364) A[2]:(-0.602960824966) A[3]:(0.795242071152)\n",
      " state (13)  A[0]:(-0.00390432286076) A[1]:(0.809239208698) A[2]:(0.900063276291) A[3]:(0.730631113052)\n",
      " state (14)  A[0]:(0.80908626318) A[1]:(0.90048789978) A[2]:(0.999999940395) A[3]:(0.810991048813)\n",
      " state (15)  A[0]:(0.983507394791) A[1]:(0.957148134708) A[2]:(1.0) A[3]:(0.879089295864)\n",
      "Episode 731000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6012. Times reached goal: 997.               Steps done: 5550256. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00349781946143.\n",
      " state (0)  A[0]:(0.531391143799) A[1]:(0.590558290482) A[2]:(0.590248823166) A[3]:(0.531301379204)\n",
      " state (1)  A[0]:(0.531380414963) A[1]:(0.000417396397097) A[2]:(0.655951142311) A[3]:(0.590371131897)\n",
      " state (2)  A[0]:(0.590572595596) A[1]:(0.729041934013) A[2]:(0.590435922146) A[3]:(0.656091034412)\n",
      " state (3)  A[0]:(0.656310796738) A[1]:(-0.21764922142) A[2]:(0.538452744484) A[3]:(0.518367171288)\n",
      " state (4)  A[0]:(0.590790390968) A[1]:(0.656100273132) A[2]:(-5.7578086853e-05) A[3]:(0.531391382217)\n",
      " state (5)  A[0]:(0.162975639105) A[1]:(0.92888957262) A[2]:(-0.192286729813) A[3]:(0.523724198341)\n",
      " state (6)  A[0]:(0.000643253210001) A[1]:(0.809987545013) A[2]:(5.32865524292e-05) A[3]:(0.656038880348)\n",
      " state (7)  A[0]:(0.628705501556) A[1]:(-0.25013679266) A[2]:(0.297349512577) A[3]:(0.884902119637)\n",
      " state (8)  A[0]:(0.656545996666) A[1]:(9.69916582108e-05) A[2]:(0.729046702385) A[3]:(0.590507507324)\n",
      " state (9)  A[0]:(0.656554460526) A[1]:(0.810004472733) A[2]:(0.809985458851) A[3]:(0.000365048617823)\n",
      " state (10)  A[0]:(0.729360580444) A[1]:(0.900009632111) A[2]:(-0.000302076339722) A[3]:(0.729134321213)\n",
      " state (11)  A[0]:(0.522789776325) A[1]:(0.876799464226) A[2]:(-0.616971552372) A[3]:(0.844614505768)\n",
      " state (12)  A[0]:(0.0794499367476) A[1]:(0.8243470788) A[2]:(-0.603055238724) A[3]:(0.794162154198)\n",
      " state (13)  A[0]:(0.000186145305634) A[1]:(0.809298276901) A[2]:(0.900039792061) A[3]:(0.729261517525)\n",
      " state (14)  A[0]:(0.810263037682) A[1]:(0.900515198708) A[2]:(0.999999940395) A[3]:(0.80998647213)\n",
      " state (15)  A[0]:(0.983589887619) A[1]:(0.957151949406) A[2]:(1.0) A[3]:(0.878407716751)\n",
      "Episode 732000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 996.               Steps done: 5556263. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0034768710415.\n",
      " state (0)  A[0]:(0.530981600285) A[1]:(0.590508580208) A[2]:(0.590494573116) A[3]:(0.52998650074)\n",
      " state (1)  A[0]:(0.531520664692) A[1]:(-1.07660889626e-05) A[2]:(0.656035780907) A[3]:(0.588958323002)\n",
      " state (2)  A[0]:(0.590816140175) A[1]:(0.72900390625) A[2]:(0.590498924255) A[3]:(0.654881715775)\n",
      " state (3)  A[0]:(0.656246006489) A[1]:(-0.216985955834) A[2]:(0.538444638252) A[3]:(0.517303943634)\n",
      " state (4)  A[0]:(0.590438961983) A[1]:(0.655970036983) A[2]:(0.000117540359497) A[3]:(0.53051507473)\n",
      " state (5)  A[0]:(0.162171632051) A[1]:(0.928842008114) A[2]:(-0.192144602537) A[3]:(0.522923111916)\n",
      " state (6)  A[0]:(-0.00043952462147) A[1]:(0.809936761856) A[2]:(8.23736190796e-05) A[3]:(0.655388712883)\n",
      " state (7)  A[0]:(0.628082811832) A[1]:(-0.250148445368) A[2]:(0.297308951616) A[3]:(0.884636223316)\n",
      " state (8)  A[0]:(0.656031012535) A[1]:(-9.2901289463e-05) A[2]:(0.728991687298) A[3]:(0.589640378952)\n",
      " state (9)  A[0]:(0.656008899212) A[1]:(0.809952259064) A[2]:(0.810009479523) A[3]:(-0.00148427381646)\n",
      " state (10)  A[0]:(0.729019522667) A[1]:(0.899983108044) A[2]:(-0.000368952722056) A[3]:(0.728337049484)\n",
      " state (11)  A[0]:(0.522407054901) A[1]:(0.876750349998) A[2]:(-0.617187738419) A[3]:(0.844212114811)\n",
      " state (12)  A[0]:(0.079057238996) A[1]:(0.824245274067) A[2]:(-0.60338562727) A[3]:(0.79373395443)\n",
      " state (13)  A[0]:(-9.98973846436e-05) A[1]:(0.809154629707) A[2]:(0.900041937828) A[3]:(0.728876948357)\n",
      " state (14)  A[0]:(0.810162842274) A[1]:(0.900420665741) A[2]:(0.999999940395) A[3]:(0.809908866882)\n",
      " state (15)  A[0]:(0.983562588692) A[1]:(0.957089841366) A[2]:(1.0) A[3]:(0.87846082449)\n",
      "Episode 733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 994.               Steps done: 5562265. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00345606536181.\n",
      " state (0)  A[0]:(0.534712076187) A[1]:(0.590200901031) A[2]:(0.590474486351) A[3]:(0.526649832726)\n",
      " state (1)  A[0]:(0.534962415695) A[1]:(4.93302941322e-05) A[2]:(0.656039595604) A[3]:(0.585035204887)\n",
      " state (2)  A[0]:(0.593513607979) A[1]:(0.728872776031) A[2]:(0.59028750658) A[3]:(0.650875449181)\n",
      " state (3)  A[0]:(0.657494306564) A[1]:(-0.217725604773) A[2]:(0.538124024868) A[3]:(0.510970234871)\n",
      " state (4)  A[0]:(0.590673089027) A[1]:(0.656167864799) A[2]:(-0.000374793977244) A[3]:(0.523633956909)\n",
      " state (5)  A[0]:(0.161211952567) A[1]:(0.928817510605) A[2]:(-0.192250907421) A[3]:(0.515479922295)\n",
      " state (6)  A[0]:(-0.00220623263158) A[1]:(0.809940040112) A[2]:(-4.06503677368e-05) A[3]:(0.649125397205)\n",
      " state (7)  A[0]:(0.627290964127) A[1]:(-0.250089228153) A[2]:(0.296951293945) A[3]:(0.882150292397)\n",
      " state (8)  A[0]:(0.656210660934) A[1]:(-0.000379316479666) A[2]:(0.728386461735) A[3]:(0.582876563072)\n",
      " state (9)  A[0]:(0.656799852848) A[1]:(0.81004524231) A[2]:(0.809659004211) A[3]:(-0.0123369693756)\n",
      " state (10)  A[0]:(0.729988574982) A[1]:(0.900175988674) A[2]:(-0.00143313314766) A[3]:(0.723232507706)\n",
      " state (11)  A[0]:(0.524150848389) A[1]:(0.877103328705) A[2]:(-0.618035018444) A[3]:(0.841237187386)\n",
      " state (12)  A[0]:(0.0815297439694) A[1]:(0.824872255325) A[2]:(-0.604100823402) A[3]:(0.790064811707)\n",
      " state (13)  A[0]:(0.00261294236407) A[1]:(0.810008049011) A[2]:(0.900326669216) A[3]:(0.724523901939)\n",
      " state (14)  A[0]:(0.811137080193) A[1]:(0.900998353958) A[2]:(0.999999940395) A[3]:(0.807083070278)\n",
      " state (15)  A[0]:(0.983596622944) A[1]:(0.957348942757) A[2]:(1.0) A[3]:(0.876688241959)\n",
      "Episode 734000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 995.               Steps done: 5568267. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00343538418381.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5904,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.0001,  0.6562,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7290,  0.5906,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0002,  0.8100, -0.0000,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0001,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9004,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531469106674) A[1]:(0.590428233147) A[2]:(0.59053838253) A[3]:(0.531587600708)\n",
      " state (1)  A[0]:(0.531380534172) A[1]:(0.000100463628769) A[2]:(0.656143724918) A[3]:(0.590442061424)\n",
      " state (2)  A[0]:(0.590539515018) A[1]:(0.72902071476) A[2]:(0.590579152107) A[3]:(0.656168937683)\n",
      " state (3)  A[0]:(0.656089961529) A[1]:(-0.216404274106) A[2]:(0.538417816162) A[3]:(0.51856982708)\n",
      " state (4)  A[0]:(0.590419650078) A[1]:(0.656123518944) A[2]:(4.24385070801e-05) A[3]:(0.531551361084)\n",
      " state (5)  A[0]:(0.162406504154) A[1]:(0.928895354271) A[2]:(-0.192320138216) A[3]:(0.523954749107)\n",
      " state (6)  A[0]:(-0.000198364257812) A[1]:(0.810041904449) A[2]:(1.68085098267e-05) A[3]:(0.656064629555)\n",
      " state (7)  A[0]:(0.627892434597) A[1]:(-0.249960079789) A[2]:(0.297543674707) A[3]:(0.884779930115)\n",
      " state (8)  A[0]:(0.655858397484) A[1]:(0.000196605920792) A[2]:(0.729035258293) A[3]:(0.590318024158)\n",
      " state (9)  A[0]:(0.655951917171) A[1]:(0.810032010078) A[2]:(0.810016274452) A[3]:(4.52101230621e-05)\n",
      " state (10)  A[0]:(0.728976428509) A[1]:(0.900014638901) A[2]:(-0.000139594078064) A[3]:(0.728905916214)\n",
      " state (11)  A[0]:(0.522366285324) A[1]:(0.876787483692) A[2]:(-0.617096543312) A[3]:(0.844462871552)\n",
      " state (12)  A[0]:(0.0789978280663) A[1]:(0.824299752712) A[2]:(-0.603515863419) A[3]:(0.793965339661)\n",
      " state (13)  A[0]:(-0.000379383534892) A[1]:(0.809208869934) A[2]:(0.900015473366) A[3]:(0.729051589966)\n",
      " state (14)  A[0]:(0.809890568256) A[1]:(0.90044683218) A[2]:(0.999999940395) A[3]:(0.809941768646)\n",
      " state (15)  A[0]:(0.983502864838) A[1]:(0.957088053226) A[2]:(1.0) A[3]:(0.878385841846)\n",
      "Episode 735000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6021. Times reached goal: 997.               Steps done: 5574288. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00341476188138.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531450390816) A[1]:(0.590497732162) A[2]:(0.590502619743) A[3]:(0.531602978706)\n",
      " state (1)  A[0]:(0.531369566917) A[1]:(-3.45781445503e-05) A[2]:(0.656135797501) A[3]:(0.590589761734)\n",
      " state (2)  A[0]:(0.590586543083) A[1]:(0.729017853737) A[2]:(0.59072726965) A[3]:(0.656295776367)\n",
      " state (3)  A[0]:(0.656358480453) A[1]:(-0.216715693474) A[2]:(0.538597464561) A[3]:(0.518680930138)\n",
      " state (4)  A[0]:(0.590815246105) A[1]:(0.656169116497) A[2]:(0.000139594078064) A[3]:(0.53159570694)\n",
      " state (5)  A[0]:(0.162917256355) A[1]:(0.928902328014) A[2]:(-0.19229568541) A[3]:(0.523858368397)\n",
      " state (6)  A[0]:(0.000154137611389) A[1]:(0.810011327267) A[2]:(9.48905944824e-05) A[3]:(0.655886411667)\n",
      " state (7)  A[0]:(0.627833664417) A[1]:(-0.250046521425) A[2]:(0.297773033381) A[3]:(0.884686112404)\n",
      " state (8)  A[0]:(0.655726313591) A[1]:(0.000177167356014) A[2]:(0.729117333889) A[3]:(0.590343654156)\n",
      " state (9)  A[0]:(0.655875921249) A[1]:(0.810051500797) A[2]:(0.810088038445) A[3]:(0.000707834842615)\n",
      " state (10)  A[0]:(0.728902697563) A[1]:(0.900016129017) A[2]:(-0.000115036964417) A[3]:(0.729355692863)\n",
      " state (11)  A[0]:(0.522186994553) A[1]:(0.876752257347) A[2]:(-0.617267131805) A[3]:(0.844741761684)\n",
      " state (12)  A[0]:(0.078697450459) A[1]:(0.824186623096) A[2]:(-0.603850603104) A[3]:(0.79428756237)\n",
      " state (13)  A[0]:(-0.000471651525004) A[1]:(0.809015572071) A[2]:(0.899973094463) A[3]:(0.729445695877)\n",
      " state (14)  A[0]:(0.810146450996) A[1]:(0.900303065777) A[2]:(0.999999940395) A[3]:(0.810288906097)\n",
      " state (15)  A[0]:(0.983543097973) A[1]:(0.956997156143) A[2]:(1.0) A[3]:(0.87863868475)\n",
      "Episode 736000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 996.               Steps done: 5580299. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00339429731558.\n",
      " state (0)  A[0]:(0.53148496151) A[1]:(0.590095937252) A[2]:(0.590682029724) A[3]:(0.531547546387)\n",
      " state (1)  A[0]:(0.531482696533) A[1]:(-4.66927886009e-05) A[2]:(0.656167089939) A[3]:(0.590615093708)\n",
      " state (2)  A[0]:(0.590736269951) A[1]:(0.729078114033) A[2]:(0.590269088745) A[3]:(0.65641450882)\n",
      " state (3)  A[0]:(0.656266450882) A[1]:(-0.214102551341) A[2]:(0.537907063961) A[3]:(0.519100189209)\n",
      " state (4)  A[0]:(0.590546488762) A[1]:(0.65599912405) A[2]:(5.47170639038e-05) A[3]:(0.531716704369)\n",
      " state (5)  A[0]:(0.16263282299) A[1]:(0.928850889206) A[2]:(-0.19216285646) A[3]:(0.523957192898)\n",
      " state (6)  A[0]:(-6.02602958679e-05) A[1]:(0.810052335262) A[2]:(0.000377178163035) A[3]:(0.656024932861)\n",
      " state (7)  A[0]:(0.628004074097) A[1]:(-0.249845102429) A[2]:(0.298258692026) A[3]:(0.884844779968)\n",
      " state (8)  A[0]:(0.656042814255) A[1]:(-0.000126712024212) A[2]:(0.729170739651) A[3]:(0.590760409832)\n",
      " state (9)  A[0]:(0.655955374241) A[1]:(0.809937059879) A[2]:(0.810047864914) A[3]:(6.15417957306e-06)\n",
      " state (10)  A[0]:(0.729009509087) A[1]:(0.899991691113) A[2]:(-0.000480174989207) A[3]:(0.728953182697)\n",
      " state (11)  A[0]:(0.522510707378) A[1]:(0.876755833626) A[2]:(-0.617654085159) A[3]:(0.844561398029)\n",
      " state (12)  A[0]:(0.0792831480503) A[1]:(0.824226200581) A[2]:(-0.604270458221) A[3]:(0.794094026089)\n",
      " state (13)  A[0]:(0.0002481341362) A[1]:(0.809103012085) A[2]:(0.900015413761) A[3]:(0.729206800461)\n",
      " state (14)  A[0]:(0.810398638248) A[1]:(0.900383532047) A[2]:(0.999999940395) A[3]:(0.810068905354)\n",
      " state (15)  A[0]:(0.983542740345) A[1]:(0.957029223442) A[2]:(1.0) A[3]:(0.878405511379)\n",
      "Episode 737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 997.               Steps done: 5586306. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00337396888917.\n",
      " state (0)  A[0]:(0.531436681747) A[1]:(0.590568184853) A[2]:(0.590485811234) A[3]:(0.531310260296)\n",
      " state (1)  A[0]:(0.531353771687) A[1]:(0.000122629106045) A[2]:(0.656168103218) A[3]:(0.59009796381)\n",
      " state (2)  A[0]:(0.590547680855) A[1]:(0.729028880596) A[2]:(0.590684652328) A[3]:(0.655813217163)\n",
      " state (3)  A[0]:(0.656083583832) A[1]:(-0.2161270082) A[2]:(0.538484930992) A[3]:(0.518275260925)\n",
      " state (4)  A[0]:(0.590447068214) A[1]:(0.656175911427) A[2]:(4.4584274292e-05) A[3]:(0.531264185905)\n",
      " state (5)  A[0]:(0.162593185902) A[1]:(0.928909540176) A[2]:(-0.192445725203) A[3]:(0.523665964603)\n",
      " state (6)  A[0]:(0.000341176986694) A[1]:(0.810027301311) A[2]:(9.05990600586e-05) A[3]:(0.655896186829)\n",
      " state (7)  A[0]:(0.628367185593) A[1]:(-0.24999153614) A[2]:(0.298006385565) A[3]:(0.884773612022)\n",
      " state (8)  A[0]:(0.656465768814) A[1]:(0.000103436410427) A[2]:(0.728953003883) A[3]:(0.590970635414)\n",
      " state (9)  A[0]:(0.656550884247) A[1]:(0.809999167919) A[2]:(0.810005784035) A[3]:(0.000941753096413)\n",
      " state (10)  A[0]:(0.729412913322) A[1]:(0.899987399578) A[2]:(-1.69277191162e-05) A[3]:(0.729235112667)\n",
      " state (11)  A[0]:(0.523011922836) A[1]:(0.876708865166) A[2]:(-0.617263555527) A[3]:(0.844653725624)\n",
      " state (12)  A[0]:(0.0798215791583) A[1]:(0.824104726315) A[2]:(-0.604044795036) A[3]:(0.794167637825)\n",
      " state (13)  A[0]:(0.000414371461375) A[1]:(0.808885097504) A[2]:(0.899925947189) A[3]:(0.729221105576)\n",
      " state (14)  A[0]:(0.810237288475) A[1]:(0.900200307369) A[2]:(0.999999940395) A[3]:(0.809968590736)\n",
      " state (15)  A[0]:(0.983517169952) A[1]:(0.956926703453) A[2]:(1.0) A[3]:(0.878289103508)\n",
      "Episode 738000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6009. Times reached goal: 997.               Steps done: 5592315. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00335375550205.\n",
      " state (0)  A[0]:(0.533131480217) A[1]:(0.590397357941) A[2]:(0.590302705765) A[3]:(0.535125851631)\n",
      " state (1)  A[0]:(0.533892035484) A[1]:(0.000540576817002) A[2]:(0.655834436417) A[3]:(0.592910647392)\n",
      " state (2)  A[0]:(0.593550682068) A[1]:(0.729021549225) A[2]:(0.590406537056) A[3]:(0.657718360424)\n",
      " state (3)  A[0]:(0.659992933273) A[1]:(-0.216406404972) A[2]:(0.537848711014) A[3]:(0.519229471684)\n",
      " state (4)  A[0]:(0.59634411335) A[1]:(0.655961990356) A[2]:(-0.00199448806234) A[3]:(0.531460225582)\n",
      " state (5)  A[0]:(0.173288136721) A[1]:(0.92901724577) A[2]:(-0.195702716708) A[3]:(0.523634314537)\n",
      " state (6)  A[0]:(0.0138221522793) A[1]:(0.809859037399) A[2]:(-0.00340078957379) A[3]:(0.655215620995)\n",
      " state (7)  A[0]:(0.636135339737) A[1]:(-0.250629037619) A[2]:(0.295616656542) A[3]:(0.883300423622)\n",
      " state (8)  A[0]:(0.661468565464) A[1]:(0.000174693763256) A[2]:(0.728893041611) A[3]:(0.580145835876)\n",
      " state (9)  A[0]:(0.661101222038) A[1]:(0.809782981873) A[2]:(0.809770703316) A[3]:(-0.0198571048677)\n",
      " state (10)  A[0]:(0.733936071396) A[1]:(0.899844825268) A[2]:(-0.00376592297107) A[3]:(0.720476508141)\n",
      " state (11)  A[0]:(0.530789256096) A[1]:(0.876511871815) A[2]:(-0.620527148247) A[3]:(0.839818775654)\n",
      " state (12)  A[0]:(0.091022759676) A[1]:(0.823774933815) A[2]:(-0.60714161396) A[3]:(0.788250923157)\n",
      " state (13)  A[0]:(0.0126092899591) A[1]:(0.808541893959) A[2]:(0.899902641773) A[3]:(0.722150444984)\n",
      " state (14)  A[0]:(0.814675450325) A[1]:(0.900046348572) A[2]:(0.999999940395) A[3]:(0.805288672447)\n",
      " state (15)  A[0]:(0.983864843845) A[1]:(0.956797659397) A[2]:(1.0) A[3]:(0.875289976597)\n",
      "Episode 739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6015. Times reached goal: 999.               Steps done: 5598330. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00333364321106.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5906,  0.5906,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315, -0.0001,  0.6561,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7289,  0.5907,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8100, -0.0001,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0000,  0.7286]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9002,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531771600246) A[1]:(0.590552031994) A[2]:(0.590553164482) A[3]:(0.531500399113)\n",
      " state (1)  A[0]:(0.53141784668) A[1]:(-0.00019209831953) A[2]:(0.656065225601) A[3]:(0.590365171432)\n",
      " state (2)  A[0]:(0.590442419052) A[1]:(0.728919029236) A[2]:(0.59073138237) A[3]:(0.656091868877)\n",
      " state (3)  A[0]:(0.656357526779) A[1]:(-0.217371165752) A[2]:(0.53853726387) A[3]:(0.51818549633)\n",
      " state (4)  A[0]:(0.590709805489) A[1]:(0.655831217766) A[2]:(-0.000125885009766) A[3]:(0.531274914742)\n",
      " state (5)  A[0]:(0.162540957332) A[1]:(0.928824424744) A[2]:(-0.192652121186) A[3]:(0.523905992508)\n",
      " state (6)  A[0]:(-0.000100493431091) A[1]:(0.809959173203) A[2]:(-0.000203371047974) A[3]:(0.656036198139)\n",
      " state (7)  A[0]:(0.628215909004) A[1]:(-0.24990567565) A[2]:(0.297887742519) A[3]:(0.884718596935)\n",
      " state (8)  A[0]:(0.656241893768) A[1]:(0.000140674412251) A[2]:(0.728921890259) A[3]:(0.59046626091)\n",
      " state (9)  A[0]:(0.655875086784) A[1]:(0.810005068779) A[2]:(0.809938251972) A[3]:(-0.000371038884623)\n",
      " state (10)  A[0]:(0.728775501251) A[1]:(0.899977564812) A[2]:(-0.000320434570312) A[3]:(0.728613257408)\n",
      " state (11)  A[0]:(0.522122144699) A[1]:(0.876686573029) A[2]:(-0.617596030235) A[3]:(0.844346284866)\n",
      " state (12)  A[0]:(0.0787621960044) A[1]:(0.82405936718) A[2]:(-0.604494571686) A[3]:(0.79390245676)\n",
      " state (13)  A[0]:(-0.000528037489858) A[1]:(0.808828175068) A[2]:(0.899992227554) A[3]:(0.729119420052)\n",
      " state (14)  A[0]:(0.809893131256) A[1]:(0.900169014931) A[2]:(0.999999940395) A[3]:(0.8101785779)\n",
      " state (15)  A[0]:(0.983446598053) A[1]:(0.956886947155) A[2]:(1.0) A[3]:(0.878537297249)\n",
      "Episode 740000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5998. Times reached goal: 993.               Steps done: 5604328. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00331370786495.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530821561813) A[1]:(0.590691566467) A[2]:(0.590308845043) A[3]:(0.530836939812)\n",
      " state (1)  A[0]:(0.53070127964) A[1]:(0.000528663338628) A[2]:(0.656042039394) A[3]:(0.590000271797)\n",
      " state (2)  A[0]:(0.58991932869) A[1]:(0.728922128677) A[2]:(0.590671479702) A[3]:(0.655810475349)\n",
      " state (3)  A[0]:(0.655928730965) A[1]:(-0.218352407217) A[2]:(0.538862943649) A[3]:(0.517996490002)\n",
      " state (4)  A[0]:(0.590340554714) A[1]:(0.655967831612) A[2]:(0.000209927558899) A[3]:(0.531230807304)\n",
      " state (5)  A[0]:(0.162121891975) A[1]:(0.928866803646) A[2]:(-0.192382365465) A[3]:(0.523757696152)\n",
      " state (6)  A[0]:(-0.000605761946645) A[1]:(0.810047626495) A[2]:(5.2809715271e-05) A[3]:(0.655823528767)\n",
      " state (7)  A[0]:(0.627650618553) A[1]:(-0.249798774719) A[2]:(0.298277258873) A[3]:(0.884594678879)\n",
      " state (8)  A[0]:(0.655448913574) A[1]:(0.000243127346039) A[2]:(0.729297280312) A[3]:(0.590092241764)\n",
      " state (9)  A[0]:(0.655036449432) A[1]:(0.81007373333) A[2]:(0.810182094574) A[3]:(-8.16136598587e-05)\n",
      " state (10)  A[0]:(0.728286981583) A[1]:(0.900055110455) A[2]:(-0.00015914440155) A[3]:(0.729044556618)\n",
      " state (11)  A[0]:(0.521624326706) A[1]:(0.876841902733) A[2]:(-0.617739677429) A[3]:(0.844608008862)\n",
      " state (12)  A[0]:(0.0783018022776) A[1]:(0.824375271797) A[2]:(-0.604700684547) A[3]:(0.794111609459)\n",
      " state (13)  A[0]:(-0.000773429695982) A[1]:(0.809310317039) A[2]:(0.90010869503) A[3]:(0.729158043861)\n",
      " state (14)  A[0]:(0.809865117073) A[1]:(0.900529682636) A[2]:(0.999999940395) A[3]:(0.809992074966)\n",
      " state (15)  A[0]:(0.983420848846) A[1]:(0.957073688507) A[2]:(1.0) A[3]:(0.878206372261)\n",
      "Episode 741000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6032. Times reached goal: 1000.               Steps done: 5610360. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00329377974275.\n",
      " state (0)  A[0]:(0.531697392464) A[1]:(0.590539336205) A[2]:(0.590349733829) A[3]:(0.531870365143)\n",
      " state (1)  A[0]:(0.53143966198) A[1]:(3.02568078041e-05) A[2]:(0.656028926373) A[3]:(0.590570151806)\n",
      " state (2)  A[0]:(0.590557456017) A[1]:(0.729055643082) A[2]:(0.590599536896) A[3]:(0.656255841255)\n",
      " state (3)  A[0]:(0.655980706215) A[1]:(-0.215975195169) A[2]:(0.538437366486) A[3]:(0.518748760223)\n",
      " state (4)  A[0]:(0.590237736702) A[1]:(0.655969798565) A[2]:(2.76565551758e-05) A[3]:(0.531780064106)\n",
      " state (5)  A[0]:(0.162178084254) A[1]:(0.928864181042) A[2]:(-0.192614004016) A[3]:(0.524375557899)\n",
      " state (6)  A[0]:(-0.000350058078766) A[1]:(0.810058236122) A[2]:(-0.000158905982971) A[3]:(0.656265497208)\n",
      " state (7)  A[0]:(0.627935528755) A[1]:(-0.249686822295) A[2]:(0.298127710819) A[3]:(0.884690403938)\n",
      " state (8)  A[0]:(0.656159520149) A[1]:(0.000315614044666) A[2]:(0.729067087173) A[3]:(0.590383827686)\n",
      " state (9)  A[0]:(0.656156122684) A[1]:(0.81006705761) A[2]:(0.810084104538) A[3]:(-0.000331565737724)\n",
      " state (10)  A[0]:(0.729144513607) A[1]:(0.90001899004) A[2]:(-8.95261764526e-05) A[3]:(0.728715360165)\n",
      " state (11)  A[0]:(0.522716760635) A[1]:(0.876739382744) A[2]:(-0.61768245697) A[3]:(0.84441024065)\n",
      " state (12)  A[0]:(0.079478956759) A[1]:(0.824132859707) A[2]:(-0.604782700539) A[3]:(0.793928265572)\n",
      " state (13)  A[0]:(0.000161945819855) A[1]:(0.808911919594) A[2]:(0.900029718876) A[3]:(0.729069709778)\n",
      " state (14)  A[0]:(0.810212850571) A[1]:(0.900221347809) A[2]:(0.999999940395) A[3]:(0.810108006001)\n",
      " state (15)  A[0]:(0.983458161354) A[1]:(0.956893384457) A[2]:(1.0) A[3]:(0.878409385681)\n",
      "Episode 742000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6006. Times reached goal: 993.               Steps done: 5616366. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00327405658953.\n",
      " state (0)  A[0]:(0.528383910656) A[1]:(0.59040927887) A[2]:(0.590314328671) A[3]:(0.532052278519)\n",
      " state (1)  A[0]:(0.529274106026) A[1]:(0.000237785279751) A[2]:(0.655879616737) A[3]:(0.590556144714)\n",
      " state (2)  A[0]:(0.589320540428) A[1]:(0.728861510754) A[2]:(0.59031188488) A[3]:(0.656082034111)\n",
      " state (3)  A[0]:(0.656192660332) A[1]:(-0.217132404447) A[2]:(0.538497686386) A[3]:(0.517756938934)\n",
      " state (4)  A[0]:(0.591629087925) A[1]:(0.655874252319) A[2]:(-0.000184297561646) A[3]:(0.530571341515)\n",
      " state (5)  A[0]:(0.16515122354) A[1]:(0.928907215595) A[2]:(-0.193189144135) A[3]:(0.523124814034)\n",
      " state (6)  A[0]:(0.00311957788654) A[1]:(0.810045897961) A[2]:(-0.000774502579588) A[3]:(0.655299782753)\n",
      " state (7)  A[0]:(0.629501700401) A[1]:(-0.249891251326) A[2]:(0.297614961863) A[3]:(0.884254693985)\n",
      " state (8)  A[0]:(0.656749606133) A[1]:(0.000346951186657) A[2]:(0.728687405586) A[3]:(0.58883702755)\n",
      " state (9)  A[0]:(0.655976712704) A[1]:(0.810083985329) A[2]:(0.809836387634) A[3]:(-0.00328681175597)\n",
      " state (10)  A[0]:(0.728464841843) A[1]:(0.900038838387) A[2]:(-0.000745773199014) A[3]:(0.727079391479)\n",
      " state (11)  A[0]:(0.521065413952) A[1]:(0.876801848412) A[2]:(-0.618177890778) A[3]:(0.843306005001)\n",
      " state (12)  A[0]:(0.0766125544906) A[1]:(0.824312746525) A[2]:(-0.605445027351) A[3]:(0.792399168015)\n",
      " state (13)  A[0]:(-0.00321723893285) A[1]:(0.809246063232) A[2]:(0.899812579155) A[3]:(0.727004230022)\n",
      " state (14)  A[0]:(0.808917760849) A[1]:(0.900506317616) A[2]:(0.999999940395) A[3]:(0.808479905128)\n",
      " state (15)  A[0]:(0.983321368694) A[1]:(0.957068502903) A[2]:(1.0) A[3]:(0.877254843712)\n",
      "Episode 743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 1000.               Steps done: 5622375. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00325444177522.\n",
      " state (0)  A[0]:(0.531215429306) A[1]:(0.591046690941) A[2]:(0.590835213661) A[3]:(0.531809568405)\n",
      " state (1)  A[0]:(0.531298279762) A[1]:(-0.000118032097816) A[2]:(0.656390786171) A[3]:(0.590807318687)\n",
      " state (2)  A[0]:(0.59063321352) A[1]:(0.729124426842) A[2]:(0.590762853622) A[3]:(0.656570672989)\n",
      " state (3)  A[0]:(0.656287908554) A[1]:(-0.215743049979) A[2]:(0.538350701332) A[3]:(0.518949627876)\n",
      " state (4)  A[0]:(0.590502202511) A[1]:(0.65659558773) A[2]:(-0.00010085105896) A[3]:(0.531977772713)\n",
      " state (5)  A[0]:(0.162216901779) A[1]:(0.928879022598) A[2]:(-0.192267790437) A[3]:(0.524588227272)\n",
      " state (6)  A[0]:(-0.000581979693379) A[1]:(0.809968948364) A[2]:(0.000385761231882) A[3]:(0.656571388245)\n",
      " state (7)  A[0]:(0.627888560295) A[1]:(-0.250051349401) A[2]:(0.298679560423) A[3]:(0.884936094284)\n",
      " state (8)  A[0]:(0.656598567963) A[1]:(2.69562005997e-05) A[2]:(0.729132652283) A[3]:(0.591353297234)\n",
      " state (9)  A[0]:(0.656938433647) A[1]:(0.809983849525) A[2]:(0.809902846813) A[3]:(0.00115717900917)\n",
      " state (10)  A[0]:(0.729698419571) A[1]:(0.899947524071) A[2]:(-0.000602364481892) A[3]:(0.729149341583)\n",
      " state (11)  A[0]:(0.523387670517) A[1]:(0.876649975777) A[2]:(-0.618069648743) A[3]:(0.844581186771)\n",
      " state (12)  A[0]:(0.080161049962) A[1]:(0.824032902718) A[2]:(-0.605417013168) A[3]:(0.794114768505)\n",
      " state (13)  A[0]:(0.000528395117726) A[1]:(0.80884206295) A[2]:(0.899838805199) A[3]:(0.729235649109)\n",
      " state (14)  A[0]:(0.810224890709) A[1]:(0.900214254856) A[2]:(0.999999940395) A[3]:(0.810086429119)\n",
      " state (15)  A[0]:(0.983434259892) A[1]:(0.956896185875) A[2]:(1.0) A[3]:(0.878258168697)\n",
      "Episode 744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6012. Times reached goal: 999.               Steps done: 5628387. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00323493476808.\n",
      "q_values \n",
      "tensor([[ 0.5308,  0.5905,  0.5905,  0.5310]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5909,  0.6562, -0.0001,  0.5310]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561, -0.0002,  0.7291,  0.5910]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.8099,  0.8100,  0.0014]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0004,  0.7296]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9004,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530550122261) A[1]:(0.590463340282) A[2]:(0.590489983559) A[3]:(0.530983567238)\n",
      " state (1)  A[0]:(0.530967235565) A[1]:(-0.000250019133091) A[2]:(0.656050205231) A[3]:(0.58987981081)\n",
      " state (2)  A[0]:(0.590478181839) A[1]:(0.728931546211) A[2]:(0.590249657631) A[3]:(0.655627250671)\n",
      " state (3)  A[0]:(0.656151294708) A[1]:(-0.215653806925) A[2]:(0.53806245327) A[3]:(0.517587780952)\n",
      " state (4)  A[0]:(0.590516030788) A[1]:(0.656060576439) A[2]:(-0.00023877620697) A[3]:(0.53041189909)\n",
      " state (5)  A[0]:(0.162488207221) A[1]:(0.928806602955) A[2]:(-0.19263535738) A[3]:(0.522939264774)\n",
      " state (6)  A[0]:(-0.00080597383203) A[1]:(0.809957921505) A[2]:(-0.000166177749634) A[3]:(0.65505361557)\n",
      " state (7)  A[0]:(0.627122282982) A[1]:(-0.250021487474) A[2]:(0.298243790865) A[3]:(0.884252846241)\n",
      " state (8)  A[0]:(0.655589222908) A[1]:(-0.000311352312565) A[2]:(0.728968560696) A[3]:(0.589856028557)\n",
      " state (9)  A[0]:(0.655667066574) A[1]:(0.809908032417) A[2]:(0.809930443764) A[3]:(-0.000258237123489)\n",
      " state (10)  A[0]:(0.728743910789) A[1]:(0.899959802628) A[2]:(-0.000656008603983) A[3]:(0.728822231293)\n",
      " state (11)  A[0]:(0.522131502628) A[1]:(0.876708209515) A[2]:(-0.618275225163) A[3]:(0.844476819038)\n",
      " state (12)  A[0]:(0.078663982451) A[1]:(0.824161112309) A[2]:(-0.605661571026) A[3]:(0.79397559166)\n",
      " state (13)  A[0]:(-0.000671625020914) A[1]:(0.809043467045) A[2]:(0.899949610233) A[3]:(0.729001045227)\n",
      " state (14)  A[0]:(0.809989213943) A[1]:(0.900369465351) A[2]:(0.999999940395) A[3]:(0.809867322445)\n",
      " state (15)  A[0]:(0.983399629593) A[1]:(0.956964671612) A[2]:(1.0) A[3]:(0.878011584282)\n",
      "Episode 745000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6016. Times reached goal: 999.               Steps done: 5634403. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0032155318231.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530794620514) A[1]:(0.590415716171) A[2]:(0.590298771858) A[3]:(0.53112745285)\n",
      " state (1)  A[0]:(0.531017005444) A[1]:(0.000660136225633) A[2]:(0.656085252762) A[3]:(0.59026825428)\n",
      " state (2)  A[0]:(0.590422868729) A[1]:(0.728935837746) A[2]:(0.590553581715) A[3]:(0.656037509441)\n",
      " state (3)  A[0]:(0.656094253063) A[1]:(-0.217874035239) A[2]:(0.538622260094) A[3]:(0.518026351929)\n",
      " state (4)  A[0]:(0.590715289116) A[1]:(0.655967473984) A[2]:(-0.000120162963867) A[3]:(0.531210303307)\n",
      " state (5)  A[0]:(0.163281217217) A[1]:(0.928862571716) A[2]:(-0.192763000727) A[3]:(0.523952782154)\n",
      " state (6)  A[0]:(0.000763594929595) A[1]:(0.809973239899) A[2]:(-7.10487365723e-05) A[3]:(0.655839085579)\n",
      " state (7)  A[0]:(0.628021359444) A[1]:(-0.25005158782) A[2]:(0.298649281263) A[3]:(0.884348750114)\n",
      " state (8)  A[0]:(0.656191706657) A[1]:(-2.5287270546e-05) A[2]:(0.72910797596) A[3]:(0.589745342731)\n",
      " state (9)  A[0]:(0.65628683567) A[1]:(0.809983968735) A[2]:(0.810074627399) A[3]:(-0.000932052440476)\n",
      " state (10)  A[0]:(0.729357004166) A[1]:(0.900010824203) A[2]:(-9.90629196167e-05) A[3]:(0.728454709053)\n",
      " state (11)  A[0]:(0.523220777512) A[1]:(0.876775026321) A[2]:(-0.617950320244) A[3]:(0.844274580479)\n",
      " state (12)  A[0]:(0.0801965221763) A[1]:(0.824239373207) A[2]:(-0.605388820171) A[3]:(0.793732762337)\n",
      " state (13)  A[0]:(0.000695586088113) A[1]:(0.809091687202) A[2]:(0.900079548359) A[3]:(0.728720188141)\n",
      " state (14)  A[0]:(0.810288786888) A[1]:(0.900369882584) A[2]:(0.999999940395) A[3]:(0.809731781483)\n",
      " state (15)  A[0]:(0.983397364616) A[1]:(0.956941068172) A[2]:(1.0) A[3]:(0.877936422825)\n",
      "Episode 746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 994.               Steps done: 5640406. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00319628680727.\n",
      " state (0)  A[0]:(0.531504869461) A[1]:(0.590722560883) A[2]:(0.590536177158) A[3]:(0.531745433807)\n",
      " state (1)  A[0]:(0.531571686268) A[1]:(0.000275313854218) A[2]:(0.656231939793) A[3]:(0.590647339821)\n",
      " state (2)  A[0]:(0.590802788734) A[1]:(0.729109883308) A[2]:(0.590885818005) A[3]:(0.656316995621)\n",
      " state (3)  A[0]:(0.65642952919) A[1]:(-0.216548740864) A[2]:(0.538773655891) A[3]:(0.518554508686)\n",
      " state (4)  A[0]:(0.590895652771) A[1]:(0.656255960464) A[2]:(0.000182151794434) A[3]:(0.531672418118)\n",
      " state (5)  A[0]:(0.163258597255) A[1]:(0.928910732269) A[2]:(-0.192517235875) A[3]:(0.524407327175)\n",
      " state (6)  A[0]:(0.000812530342955) A[1]:(0.810091078281) A[2]:(0.000161290168762) A[3]:(0.656204342842)\n",
      " state (7)  A[0]:(0.628318071365) A[1]:(-0.2496073246) A[2]:(0.298923969269) A[3]:(0.884499788284)\n",
      " state (8)  A[0]:(0.656526505947) A[1]:(0.000454276771052) A[2]:(0.729235470295) A[3]:(0.590260505676)\n",
      " state (9)  A[0]:(0.656509339809) A[1]:(0.810096442699) A[2]:(0.810180604458) A[3]:(-0.000127285718918)\n",
      " state (10)  A[0]:(0.729448497295) A[1]:(0.900029301643) A[2]:(0.000121355056763) A[3]:(0.728811979294)\n",
      " state (11)  A[0]:(0.523288965225) A[1]:(0.876739084721) A[2]:(-0.617946386337) A[3]:(0.844478666782)\n",
      " state (12)  A[0]:(0.0802267864347) A[1]:(0.824100494385) A[2]:(-0.605552196503) A[3]:(0.793983161449)\n",
      " state (13)  A[0]:(0.000704645994119) A[1]:(0.80883538723) A[2]:(0.90003234148) A[3]:(0.72905421257)\n",
      " state (14)  A[0]:(0.810336709023) A[1]:(0.900168180466) A[2]:(0.999999940395) A[3]:(0.810047268867)\n",
      " state (15)  A[0]:(0.983399748802) A[1]:(0.956817746162) A[2]:(1.0) A[3]:(0.878185808659)\n",
      "Episode 747000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               5997. Times reached goal: 995.               Steps done: 5646403. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00317717603621.\n",
      " state (0)  A[0]:(0.531074285507) A[1]:(0.590797305107) A[2]:(0.590690672398) A[3]:(0.531636893749)\n",
      " state (1)  A[0]:(0.530934095383) A[1]:(3.04281711578e-05) A[2]:(0.65625756979) A[3]:(0.590297222137)\n",
      " state (2)  A[0]:(0.59027826786) A[1]:(0.729317307472) A[2]:(0.590988337994) A[3]:(0.65596395731)\n",
      " state (3)  A[0]:(0.656348109245) A[1]:(-0.215801432729) A[2]:(0.538797616959) A[3]:(0.518142461777)\n",
      " state (4)  A[0]:(0.591022014618) A[1]:(0.656444787979) A[2]:(0.000316858291626) A[3]:(0.531292557716)\n",
      " state (5)  A[0]:(0.16342125833) A[1]:(0.928944349289) A[2]:(-0.192355722189) A[3]:(0.524032235146)\n",
      " state (6)  A[0]:(0.000361084908945) A[1]:(0.810339808464) A[2]:(0.000277996063232) A[3]:(0.655711829662)\n",
      " state (7)  A[0]:(0.627840280533) A[1]:(-0.248957052827) A[2]:(0.299262195826) A[3]:(0.884234845638)\n",
      " state (8)  A[0]:(0.655969977379) A[1]:(0.000717401388101) A[2]:(0.729471862316) A[3]:(0.589420557022)\n",
      " state (9)  A[0]:(0.655601978302) A[1]:(0.810268342495) A[2]:(0.810317158699) A[3]:(-0.00157417228911)\n",
      " state (10)  A[0]:(0.728660106659) A[1]:(0.900185286999) A[2]:(9.82284545898e-05) A[3]:(0.728274047375)\n",
      " state (11)  A[0]:(0.522069215775) A[1]:(0.876990199089) A[2]:(-0.618195056915) A[3]:(0.844198405743)\n",
      " state (12)  A[0]:(0.0785280913115) A[1]:(0.824536979198) A[2]:(-0.605854570866) A[3]:(0.793612658978)\n",
      " state (13)  A[0]:(-0.000951528258156) A[1]:(0.809434115887) A[2]:(0.900158703327) A[3]:(0.728562474251)\n",
      " state (14)  A[0]:(0.809778213501) A[1]:(0.900588572025) A[2]:(0.999999940395) A[3]:(0.809679389)\n",
      " state (15)  A[0]:(0.983315229416) A[1]:(0.957024872303) A[2]:(1.0) A[3]:(0.877865433693)\n",
      "Episode 748000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5998. Times reached goal: 995.               Steps done: 5652401. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0031581763713.\n",
      " state (0)  A[0]:(0.531666278839) A[1]:(0.590567469597) A[2]:(0.590588629246) A[3]:(0.531681418419)\n",
      " state (1)  A[0]:(0.531589686871) A[1]:(-6.09904527664e-05) A[2]:(0.656153559685) A[3]:(0.590620398521)\n",
      " state (2)  A[0]:(0.590789914131) A[1]:(0.729000151157) A[2]:(0.590610444546) A[3]:(0.656353473663)\n",
      " state (3)  A[0]:(0.65641272068) A[1]:(-0.216255858541) A[2]:(0.538460433483) A[3]:(0.518686652184)\n",
      " state (4)  A[0]:(0.590818524361) A[1]:(0.656102776527) A[2]:(-7.95125961304e-05) A[3]:(0.531819343567)\n",
      " state (5)  A[0]:(0.163040444255) A[1]:(0.928838610649) A[2]:(-0.192721217871) A[3]:(0.524586677551)\n",
      " state (6)  A[0]:(0.000321209430695) A[1]:(0.809949696064) A[2]:(-3.33786010742e-05) A[3]:(0.656241297722)\n",
      " state (7)  A[0]:(0.628004789352) A[1]:(-0.250008225441) A[2]:(0.298869341612) A[3]:(0.88448882103)\n",
      " state (8)  A[0]:(0.656369447708) A[1]:(-8.27461481094e-05) A[2]:(0.728983044624) A[3]:(0.590700268745)\n",
      " state (9)  A[0]:(0.656202077866) A[1]:(0.809994518757) A[2]:(0.809965372086) A[3]:(0.000624328793492)\n",
      " state (10)  A[0]:(0.729133784771) A[1]:(0.899980306625) A[2]:(-0.000456809968455) A[3]:(0.729133963585)\n",
      " state (11)  A[0]:(0.522836744785) A[1]:(0.876664996147) A[2]:(-0.618451178074) A[3]:(0.844684362411)\n",
      " state (12)  A[0]:(0.0796667635441) A[1]:(0.823978364468) A[2]:(-0.606228232384) A[3]:(0.794248342514)\n",
      " state (13)  A[0]:(0.000153660774231) A[1]:(0.808702945709) A[2]:(0.899980008602) A[3]:(0.729366600513)\n",
      " state (14)  A[0]:(0.810104966164) A[1]:(0.900115847588) A[2]:(0.999999940395) A[3]:(0.810226261616)\n",
      " state (15)  A[0]:(0.983342289925) A[1]:(0.956782639027) A[2]:(1.0) A[3]:(0.878200888634)\n",
      "Episode 749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 997.               Steps done: 5658405. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00313927148961.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5903,  0.5905,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5312, -0.0008,  0.6559,  0.5899]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.7288,  0.5905,  0.6556]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0023,  0.8099,  0.0000,  0.6550]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7284,  0.9000, -0.0005,  0.7280]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9004,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531429290771) A[1]:(0.590341866016) A[2]:(0.590509057045) A[3]:(0.531411409378)\n",
      " state (1)  A[0]:(0.53138422966) A[1]:(-0.00075133133214) A[2]:(0.655945420265) A[3]:(0.590265512466)\n",
      " state (2)  A[0]:(0.590574026108) A[1]:(0.72883939743) A[2]:(0.590511441231) A[3]:(0.655891776085)\n",
      " state (3)  A[0]:(0.656229376793) A[1]:(-0.216748282313) A[2]:(0.53845846653) A[3]:(0.51792216301)\n",
      " state (4)  A[0]:(0.590569972992) A[1]:(0.656093895435) A[2]:(-0.000136494636536) A[3]:(0.530950248241)\n",
      " state (5)  A[0]:(0.162636652589) A[1]:(0.928837776184) A[2]:(-0.192771956325) A[3]:(0.52361702919)\n",
      " state (6)  A[0]:(6.80088996887e-05) A[1]:(0.809898614883) A[2]:(2.05039978027e-05) A[3]:(0.655405282974)\n",
      " state (7)  A[0]:(0.627868711948) A[1]:(-0.250119686127) A[2]:(0.298986375332) A[3]:(0.884106636047)\n",
      " state (8)  A[0]:(0.656455278397) A[1]:(-0.000183299183846) A[2]:(0.728822827339) A[3]:(0.589755296707)\n",
      " state (9)  A[0]:(0.656549334526) A[1]:(0.809989690781) A[2]:(0.809909403324) A[3]:(-0.00114539219067)\n",
      " state (10)  A[0]:(0.729585409164) A[1]:(0.900022029877) A[2]:(-0.000504255236592) A[3]:(0.728365063667)\n",
      " state (11)  A[0]:(0.523739635944) A[1]:(0.876765847206) A[2]:(-0.618577957153) A[3]:(0.844307363033)\n",
      " state (12)  A[0]:(0.0810211822391) A[1]:(0.824185729027) A[2]:(-0.606496572495) A[3]:(0.793827533722)\n",
      " state (13)  A[0]:(0.00143331196159) A[1]:(0.809001505375) A[2]:(0.899892985821) A[3]:(0.728842616081)\n",
      " state (14)  A[0]:(0.810445785522) A[1]:(0.900330662727) A[2]:(0.999999940395) A[3]:(0.809799134731)\n",
      " state (15)  A[0]:(0.983356952667) A[1]:(0.956896901131) A[2]:(1.0) A[3]:(0.877857685089)\n",
      "Episode 750000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6007. Times reached goal: 994.               Steps done: 5664412. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00312047041135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531664729118) A[1]:(0.590622603893) A[2]:(0.590526163578) A[3]:(0.531463742256)\n",
      " state (1)  A[0]:(0.531812250614) A[1]:(0.000254966318607) A[2]:(0.656026721001) A[3]:(0.590241134167)\n",
      " state (2)  A[0]:(0.591043949127) A[1]:(0.729071855545) A[2]:(0.590284526348) A[3]:(0.655948638916)\n",
      " state (3)  A[0]:(0.656370937824) A[1]:(-0.215094417334) A[2]:(0.538304686546) A[3]:(0.518053412437)\n",
      " state (4)  A[0]:(0.590692162514) A[1]:(0.656213700771) A[2]:(-6.91413879395e-06) A[3]:(0.531005561352)\n",
      " state (5)  A[0]:(0.163020193577) A[1]:(0.928905129433) A[2]:(-0.192803636193) A[3]:(0.5238327384)\n",
      " state (6)  A[0]:(0.000634670199361) A[1]:(0.810080289841) A[2]:(9.76324081421e-05) A[3]:(0.655777454376)\n",
      " state (7)  A[0]:(0.628183484077) A[1]:(-0.249581694603) A[2]:(0.299327641726) A[3]:(0.884337544441)\n",
      " state (8)  A[0]:(0.656597793102) A[1]:(0.000429756910307) A[2]:(0.728925585747) A[3]:(0.590483665466)\n",
      " state (9)  A[0]:(0.656556606293) A[1]:(0.810119152069) A[2]:(0.809917390347) A[3]:(-0.000196844339371)\n",
      " state (10)  A[0]:(0.729335308075) A[1]:(0.900052189827) A[2]:(-0.00037586686085) A[3]:(0.728641748428)\n",
      " state (11)  A[0]:(0.523010373116) A[1]:(0.876776099205) A[2]:(-0.618503689766) A[3]:(0.844395160675)\n",
      " state (12)  A[0]:(0.0797487348318) A[1]:(0.824186503887) A[2]:(-0.606454312801) A[3]:(0.793882489204)\n",
      " state (13)  A[0]:(0.000182390213013) A[1]:(0.809012770653) A[2]:(0.900009691715) A[3]:(0.728880286217)\n",
      " state (14)  A[0]:(0.810176253319) A[1]:(0.90035623312) A[2]:(0.999999940395) A[3]:(0.809808135033)\n",
      " state (15)  A[0]:(0.983332812786) A[1]:(0.956906616688) A[2]:(1.0) A[3]:(0.877827346325)\n",
      "Episode 751000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6000. Times reached goal: 997.               Steps done: 5670412. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00310180364518.\n",
      " state (0)  A[0]:(0.53122150898) A[1]:(0.590535402298) A[2]:(0.590498209) A[3]:(0.531313359737)\n",
      " state (1)  A[0]:(0.531165599823) A[1]:(0.000178843736649) A[2]:(0.656153082848) A[3]:(0.590360224247)\n",
      " state (2)  A[0]:(0.590407133102) A[1]:(0.729088187218) A[2]:(0.59056520462) A[3]:(0.656110167503)\n",
      " state (3)  A[0]:(0.655974030495) A[1]:(-0.215291842818) A[2]:(0.538595676422) A[3]:(0.5183583498)\n",
      " state (4)  A[0]:(0.590296983719) A[1]:(0.656224489212) A[2]:(0.000279545783997) A[3]:(0.531330108643)\n",
      " state (5)  A[0]:(0.162375971675) A[1]:(0.928903400898) A[2]:(-0.192591741681) A[3]:(0.524098575115)\n",
      " state (6)  A[0]:(-0.000282526016235) A[1]:(0.810132980347) A[2]:(0.000269412994385) A[3]:(0.655871868134)\n",
      " state (7)  A[0]:(0.627518415451) A[1]:(-0.249435707927) A[2]:(0.299677491188) A[3]:(0.88432097435)\n",
      " state (8)  A[0]:(0.65597641468) A[1]:(0.000435054273112) A[2]:(0.729261398315) A[3]:(0.590267539024)\n",
      " state (9)  A[0]:(0.655979096889) A[1]:(0.810143053532) A[2]:(0.810193836689) A[3]:(-0.000472217769129)\n",
      " state (10)  A[0]:(0.729068636894) A[1]:(0.900083422661) A[2]:(0.000168442726135) A[3]:(0.728710412979)\n",
      " state (11)  A[0]:(0.522887825966) A[1]:(0.876830458641) A[2]:(-0.618342995644) A[3]:(0.844498932362)\n",
      " state (12)  A[0]:(0.0798399969935) A[1]:(0.824282646179) A[2]:(-0.606406390667) A[3]:(0.79402589798)\n",
      " state (13)  A[0]:(0.000344514846802) A[1]:(0.809148907661) A[2]:(0.900095939636) A[3]:(0.72904419899)\n",
      " state (14)  A[0]:(0.810129642487) A[1]:(0.900460481644) A[2]:(0.999999940395) A[3]:(0.809915542603)\n",
      " state (15)  A[0]:(0.983300924301) A[1]:(0.95695579052) A[2]:(1.0) A[3]:(0.877852380276)\n",
      "Episode 752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 995.               Steps done: 5676423. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00308321462873.\n",
      " state (0)  A[0]:(0.532651185989) A[1]:(0.590417683125) A[2]:(0.590379714966) A[3]:(0.53076416254)\n",
      " state (1)  A[0]:(0.53215277195) A[1]:(1.35749578476e-05) A[2]:(0.656005859375) A[3]:(0.590312838554)\n",
      " state (2)  A[0]:(0.59130191803) A[1]:(0.728916287422) A[2]:(0.590547919273) A[3]:(0.656030893326)\n",
      " state (3)  A[0]:(0.65734565258) A[1]:(-0.21763369441) A[2]:(0.538708508015) A[3]:(0.518149018288)\n",
      " state (4)  A[0]:(0.592288136482) A[1]:(0.656172633171) A[2]:(-0.000203490257263) A[3]:(0.531132936478)\n",
      " state (5)  A[0]:(0.165406554937) A[1]:(0.928899586201) A[2]:(-0.193159073591) A[3]:(0.523509383202)\n",
      " state (6)  A[0]:(0.00247907126322) A[1]:(0.809953093529) A[2]:(-0.000154972076416) A[3]:(0.654940962791)\n",
      " state (7)  A[0]:(0.62827193737) A[1]:(-0.250105828047) A[2]:(0.299442350864) A[3]:(0.883676052094)\n",
      " state (8)  A[0]:(0.655640125275) A[1]:(-1.61081552505e-05) A[2]:(0.728938400745) A[3]:(0.58806848526)\n",
      " state (9)  A[0]:(0.654644906521) A[1]:(0.810011148453) A[2]:(0.809949398041) A[3]:(-0.00436011189595)\n",
      " state (10)  A[0]:(0.72743666172) A[1]:(0.900003552437) A[2]:(-0.000308632850647) A[3]:(0.726436018944)\n",
      " state (11)  A[0]:(0.519932985306) A[1]:(0.876734793186) A[2]:(-0.618650257587) A[3]:(0.842895627022)\n",
      " state (12)  A[0]:(0.0755175501108) A[1]:(0.824179470539) A[2]:(-0.606808304787) A[3]:(0.791767299175)\n",
      " state (13)  A[0]:(-0.00398833444342) A[1]:(0.809101700783) A[2]:(0.900028467178) A[3]:(0.726048946381)\n",
      " state (14)  A[0]:(0.808824419975) A[1]:(0.900492012501) A[2]:(0.999999940395) A[3]:(0.807754874229)\n",
      " state (15)  A[0]:(0.983183503151) A[1]:(0.956989824772) A[2]:(1.0) A[3]:(0.8764321208)\n",
      "Episode 753000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5994. Times reached goal: 994.               Steps done: 5682417. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00306478911667.\n",
      " state (0)  A[0]:(0.531574726105) A[1]:(0.590601325035) A[2]:(0.590605139732) A[3]:(0.53199416399)\n",
      " state (1)  A[0]:(0.531322360039) A[1]:(-3.83257865906e-05) A[2]:(0.656143188477) A[3]:(0.590754806995)\n",
      " state (2)  A[0]:(0.590480864048) A[1]:(0.729090452194) A[2]:(0.590748429298) A[3]:(0.656260490417)\n",
      " state (3)  A[0]:(0.656055986881) A[1]:(-0.216775625944) A[2]:(0.538796365261) A[3]:(0.518355846405)\n",
      " state (4)  A[0]:(0.590374588966) A[1]:(0.656274557114) A[2]:(0.000118732452393) A[3]:(0.531338751316)\n",
      " state (5)  A[0]:(0.162340641022) A[1]:(0.92890048027) A[2]:(-0.192737057805) A[3]:(0.523942470551)\n",
      " state (6)  A[0]:(-0.000580668391194) A[1]:(0.810084342957) A[2]:(0.000253558158875) A[3]:(0.655563175678)\n",
      " state (7)  A[0]:(0.627172827721) A[1]:(-0.249607875943) A[2]:(0.299870282412) A[3]:(0.884146690369)\n",
      " state (8)  A[0]:(0.655978143215) A[1]:(0.000326246023178) A[2]:(0.729125082493) A[3]:(0.590213298798)\n",
      " state (9)  A[0]:(0.656325340271) A[1]:(0.810131371021) A[2]:(0.810040473938) A[3]:(-0.000235095620155)\n",
      " state (10)  A[0]:(0.729373812675) A[1]:(0.900049865246) A[2]:(-5.61475753784e-05) A[3]:(0.728628218174)\n",
      " state (11)  A[0]:(0.523334860802) A[1]:(0.87674677372) A[2]:(-0.618548989296) A[3]:(0.844338297844)\n",
      " state (12)  A[0]:(0.0804033204913) A[1]:(0.824108839035) A[2]:(-0.60681438446) A[3]:(0.793705403805)\n",
      " state (13)  A[0]:(0.000871538883075) A[1]:(0.80890327692) A[2]:(0.900031685829) A[3]:(0.728509902954)\n",
      " state (14)  A[0]:(0.810357391834) A[1]:(0.900305390358) A[2]:(0.999999940395) A[3]:(0.809451878071)\n",
      " state (15)  A[0]:(0.983307123184) A[1]:(0.956863164902) A[2]:(1.0) A[3]:(0.877455413342)\n",
      "Episode 754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 998.               Steps done: 5688425. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00304643106646.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5906,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5913,  0.6558,  0.0004,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565, -0.0005,  0.7289,  0.5912]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560,  0.8099,  0.8099,  0.0007]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7285,  0.9000, -0.0003,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8089,  0.9006,  1.0000,  0.8095]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531648516655) A[1]:(0.590567648411) A[2]:(0.590462446213) A[3]:(0.531249165535)\n",
      " state (1)  A[0]:(0.531556665897) A[1]:(-4.70653176308e-05) A[2]:(0.656098842621) A[3]:(0.589969038963)\n",
      " state (2)  A[0]:(0.590698003769) A[1]:(0.729014456272) A[2]:(0.590644538403) A[3]:(0.655649602413)\n",
      " state (3)  A[0]:(0.656358778477) A[1]:(-0.21641716361) A[2]:(0.538718163967) A[3]:(0.51786839962)\n",
      " state (4)  A[0]:(0.590717017651) A[1]:(0.655859589577) A[2]:(0.000208973884583) A[3]:(0.530999839306)\n",
      " state (5)  A[0]:(0.162684872746) A[1]:(0.928823709488) A[2]:(-0.192774251103) A[3]:(0.523744761944)\n",
      " state (6)  A[0]:(-0.000641524675302) A[1]:(0.810001552105) A[2]:(0.000168800354004) A[3]:(0.655306577682)\n",
      " state (7)  A[0]:(0.626891732216) A[1]:(-0.249868705869) A[2]:(0.299908578396) A[3]:(0.883967638016)\n",
      " state (8)  A[0]:(0.655273199081) A[1]:(-0.000496305467095) A[2]:(0.72895771265) A[3]:(0.589987158775)\n",
      " state (9)  A[0]:(0.654883861542) A[1]:(0.809860944748) A[2]:(0.809947133064) A[3]:(-0.000805362884421)\n",
      " state (10)  A[0]:(0.727920651436) A[1]:(0.899985194206) A[2]:(-0.000609993876424) A[3]:(0.72850227356)\n",
      " state (11)  A[0]:(0.52082824707) A[1]:(0.876766979694) A[2]:(-0.619150519371) A[3]:(0.844340384007)\n",
      " state (12)  A[0]:(0.0765865370631) A[1]:(0.824281811714) A[2]:(-0.607563972473) A[3]:(0.793697237968)\n",
      " state (13)  A[0]:(-0.00340931280516) A[1]:(0.809278249741) A[2]:(0.899892091751) A[3]:(0.728424787521)\n",
      " state (14)  A[0]:(0.808668673038) A[1]:(0.900640010834) A[2]:(0.999999940395) A[3]:(0.809302687645)\n",
      " state (15)  A[0]:(0.983108699322) A[1]:(0.957062363625) A[2]:(1.0) A[3]:(0.877262949944)\n",
      "Episode 755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 995.               Steps done: 5694435. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00302817692439.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531620681286) A[1]:(0.590697169304) A[2]:(0.590527176857) A[3]:(0.531014084816)\n",
      " state (1)  A[0]:(0.531411409378) A[1]:(0.000176265835762) A[2]:(0.65613424778) A[3]:(0.590259850025)\n",
      " state (2)  A[0]:(0.590487241745) A[1]:(0.729112744331) A[2]:(0.590722978115) A[3]:(0.65615105629)\n",
      " state (3)  A[0]:(0.655859470367) A[1]:(-0.217024996877) A[2]:(0.538821458817) A[3]:(0.518527269363)\n",
      " state (4)  A[0]:(0.589933037758) A[1]:(0.656340718269) A[2]:(2.2292137146e-05) A[3]:(0.531973004341)\n",
      " state (5)  A[0]:(0.161405682564) A[1]:(0.928909242153) A[2]:(-0.192920818925) A[3]:(0.524981975555)\n",
      " state (6)  A[0]:(-0.00181239645462) A[1]:(0.810177147388) A[2]:(2.51531600952e-05) A[3]:(0.656503677368)\n",
      " state (7)  A[0]:(0.626517057419) A[1]:(-0.249367624521) A[2]:(0.299896419048) A[3]:(0.884528696537)\n",
      " state (8)  A[0]:(0.655452489853) A[1]:(0.000287309288979) A[2]:(0.729056835175) A[3]:(0.591875851154)\n",
      " state (9)  A[0]:(0.65565097332) A[1]:(0.810122311115) A[2]:(0.809961616993) A[3]:(0.00288229389116)\n",
      " state (10)  A[0]:(0.728778600693) A[1]:(0.900069773197) A[2]:(-0.000505685748067) A[3]:(0.730287373066)\n",
      " state (11)  A[0]:(0.522382676601) A[1]:(0.876807808876) A[2]:(-0.619070529938) A[3]:(0.8454413414)\n",
      " state (12)  A[0]:(0.0790273249149) A[1]:(0.824261665344) A[2]:(-0.607466578484) A[3]:(0.795188188553)\n",
      " state (13)  A[0]:(-0.000424146623118) A[1]:(0.809188008308) A[2]:(0.900110423565) A[3]:(0.730480313301)\n",
      " state (14)  A[0]:(0.810026228428) A[1]:(0.900560975075) A[2]:(0.999999940395) A[3]:(0.811021447182)\n",
      " state (15)  A[0]:(0.983241558075) A[1]:(0.956991970539) A[2]:(1.0) A[3]:(0.878477692604)\n",
      "Episode 756000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 1000.               Steps done: 5700441. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00301004420086.\n",
      " state (0)  A[0]:(0.532089352608) A[1]:(0.591069102287) A[2]:(0.590682923794) A[3]:(0.531848430634)\n",
      " state (1)  A[0]:(0.532287478447) A[1]:(0.000278279185295) A[2]:(0.656377792358) A[3]:(0.590166211128)\n",
      " state (2)  A[0]:(0.59142100811) A[1]:(0.729285001755) A[2]:(0.591083526611) A[3]:(0.655609846115)\n",
      " state (3)  A[0]:(0.656933426857) A[1]:(-0.216212496161) A[2]:(0.538965344429) A[3]:(0.517002344131)\n",
      " state (4)  A[0]:(0.591222822666) A[1]:(0.656483888626) A[2]:(0.000144004821777) A[3]:(0.529911279678)\n",
      " state (5)  A[0]:(0.163202315569) A[1]:(0.928932189941) A[2]:(-0.192900151014) A[3]:(0.522614836693)\n",
      " state (6)  A[0]:(-1.54972076416e-06) A[1]:(0.810136318207) A[2]:(0.000153541564941) A[3]:(0.654308199883)\n",
      " state (7)  A[0]:(0.62741458416) A[1]:(-0.249510660768) A[2]:(0.300269693136) A[3]:(0.883394539356)\n",
      " state (8)  A[0]:(0.656020522118) A[1]:(0.000282451510429) A[2]:(0.72930264473) A[3]:(0.587473332882)\n",
      " state (9)  A[0]:(0.656160831451) A[1]:(0.810128808022) A[2]:(0.810149788857) A[3]:(-0.00512090325356)\n",
      " state (10)  A[0]:(0.729089319706) A[1]:(0.900063216686) A[2]:(-0.00027871131897) A[3]:(0.726472258568)\n",
      " state (11)  A[0]:(0.522573947906) A[1]:(0.876761972904) A[2]:(-0.619137644768) A[3]:(0.843130171299)\n",
      " state (12)  A[0]:(0.0788733810186) A[1]:(0.824131309986) A[2]:(-0.607618391514) A[3]:(0.7922321558)\n",
      " state (13)  A[0]:(-0.000851452117786) A[1]:(0.808983445168) A[2]:(0.900227427483) A[3]:(0.726828336716)\n",
      " state (14)  A[0]:(0.809815168381) A[1]:(0.900420367718) A[2]:(0.999999940395) A[3]:(0.808507919312)\n",
      " state (15)  A[0]:(0.98319286108) A[1]:(0.956898272038) A[2]:(1.0) A[3]:(0.876873791218)\n",
      "Episode 757000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 1000.               Steps done: 5706452. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00299200509601.\n",
      " state (0)  A[0]:(0.531151354313) A[1]:(0.590394318104) A[2]:(0.5905854702) A[3]:(0.531116247177)\n",
      " state (1)  A[0]:(0.530998826027) A[1]:(-7.63237476349e-05) A[2]:(0.656121492386) A[3]:(0.590291142464)\n",
      " state (2)  A[0]:(0.59016919136) A[1]:(0.729016304016) A[2]:(0.590565562248) A[3]:(0.656071662903)\n",
      " state (3)  A[0]:(0.656039118767) A[1]:(-0.215838164091) A[2]:(0.538487434387) A[3]:(0.517901957035)\n",
      " state (4)  A[0]:(0.590421915054) A[1]:(0.656200647354) A[2]:(-0.000221133232117) A[3]:(0.530943274498)\n",
      " state (5)  A[0]:(0.162503704429) A[1]:(0.9289072752) A[2]:(-0.193236529827) A[3]:(0.523915529251)\n",
      " state (6)  A[0]:(0.000113070011139) A[1]:(0.810011982918) A[2]:(0.000120282173157) A[3]:(0.655576229095)\n",
      " state (7)  A[0]:(0.627515554428) A[1]:(-0.249612241983) A[2]:(0.300411880016) A[3]:(0.883943796158)\n",
      " state (8)  A[0]:(0.65594381094) A[1]:(0.000237196683884) A[2]:(0.729058146477) A[3]:(0.5899477005)\n",
      " state (9)  A[0]:(0.655982255936) A[1]:(0.810025930405) A[2]:(0.81007951498) A[3]:(-0.000884875422344)\n",
      " state (10)  A[0]:(0.729103803635) A[1]:(0.90001386404) A[2]:(-8.38041305542e-05) A[3]:(0.728573679924)\n",
      " state (11)  A[0]:(0.522946476936) A[1]:(0.876698374748) A[2]:(-0.619029641151) A[3]:(0.844491362572)\n",
      " state (12)  A[0]:(0.079699575901) A[1]:(0.824006676674) A[2]:(-0.607746839523) A[3]:(0.794002652168)\n",
      " state (13)  A[0]:(-7.50422477722e-05) A[1]:(0.808762729168) A[2]:(0.899953007698) A[3]:(0.728957295418)\n",
      " state (14)  A[0]:(0.809984982014) A[1]:(0.900239527225) A[2]:(0.999999940395) A[3]:(0.809860467911)\n",
      " state (15)  A[0]:(0.983217537403) A[1]:(0.956802308559) A[2]:(1.0) A[3]:(0.877670824528)\n",
      "Episode 758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 996.               Steps done: 5712466. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00297406517678.\n",
      " state (0)  A[0]:(0.531655192375) A[1]:(0.590440750122) A[2]:(0.590347707272) A[3]:(0.531514883041)\n",
      " state (1)  A[0]:(0.531722247601) A[1]:(-3.3512711525e-05) A[2]:(0.656075716019) A[3]:(0.590397715569)\n",
      " state (2)  A[0]:(0.590765953064) A[1]:(0.728791475296) A[2]:(0.59078669548) A[3]:(0.656071901321)\n",
      " state (3)  A[0]:(0.656255722046) A[1]:(-0.217763721943) A[2]:(0.538966417313) A[3]:(0.518609642982)\n",
      " state (4)  A[0]:(0.590428113937) A[1]:(0.655709266663) A[2]:(4.87565994263e-05) A[3]:(0.531952619553)\n",
      " state (5)  A[0]:(0.162192821503) A[1]:(0.928806304932) A[2]:(-0.193238824606) A[3]:(0.524809956551)\n",
      " state (6)  A[0]:(-0.000603794993367) A[1]:(0.809841930866) A[2]:(-6.94990158081e-05) A[3]:(0.656194329262)\n",
      " state (7)  A[0]:(0.62705719471) A[1]:(-0.249977871776) A[2]:(0.300273686647) A[3]:(0.884229660034)\n",
      " state (8)  A[0]:(0.655732750893) A[1]:(-0.000182285904884) A[2]:(0.728906035423) A[3]:(0.591019928455)\n",
      " state (9)  A[0]:(0.655875682831) A[1]:(0.809889853001) A[2]:(0.809873461723) A[3]:(0.000969633169007)\n",
      " state (10)  A[0]:(0.728952765465) A[1]:(0.899894297123) A[2]:(-0.000818967644591) A[3]:(0.729317188263)\n",
      " state (11)  A[0]:(0.522589445114) A[1]:(0.876480817795) A[2]:(-0.619600772858) A[3]:(0.844857215881)\n",
      " state (12)  A[0]:(0.0791248232126) A[1]:(0.823605239391) A[2]:(-0.608403742313) A[3]:(0.794403254986)\n",
      " state (13)  A[0]:(-0.000528991164174) A[1]:(0.808240294456) A[2]:(0.89990645647) A[3]:(0.729445338249)\n",
      " state (14)  A[0]:(0.809963226318) A[1]:(0.899917900562) A[2]:(0.999999940395) A[3]:(0.810263156891)\n",
      " state (15)  A[0]:(0.983206391335) A[1]:(0.956626594067) A[2]:(1.0) A[3]:(0.877931892872)\n",
      "Episode 759000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 997.               Steps done: 5718472. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00295625647439.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5905,  0.5906,  0.5293]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.3125e-01, -3.2336e-06,  6.5599e-01,  5.8850e-01]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7291,  0.5908,  0.6543]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  0.8099, -0.0001,  0.6533]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7294,  0.9000, -0.0009,  0.7276]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9002,  1.0000,  0.8092]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531700134277) A[1]:(0.590527653694) A[2]:(0.590652167797) A[3]:(0.528718233109)\n",
      " state (1)  A[0]:(0.531502723694) A[1]:(1.92299485207e-05) A[2]:(0.656087875366) A[3]:(0.587915301323)\n",
      " state (2)  A[0]:(0.590568184853) A[1]:(0.728981614113) A[2]:(0.590690016747) A[3]:(0.653764605522)\n",
      " state (3)  A[0]:(0.655980169773) A[1]:(-0.217797741294) A[2]:(0.538778245449) A[3]:(0.514473974705)\n",
      " state (4)  A[0]:(0.590126931667) A[1]:(0.656386375427) A[2]:(-0.000463008851511) A[3]:(0.527552604675)\n",
      " state (5)  A[0]:(0.161847710609) A[1]:(0.928932368755) A[2]:(-0.193603843451) A[3]:(0.520330071449)\n",
      " state (6)  A[0]:(-0.000649034860544) A[1]:(0.810029864311) A[2]:(-0.0002521276474) A[3]:(0.652660012245)\n",
      " state (7)  A[0]:(0.627118051052) A[1]:(-0.249656289816) A[2]:(0.300404727459) A[3]:(0.88281083107)\n",
      " state (8)  A[0]:(0.65554523468) A[1]:(0.000179089605808) A[2]:(0.729076325893) A[3]:(0.586763679981)\n",
      " state (9)  A[0]:(0.655545175076) A[1]:(0.810053467751) A[2]:(0.809992730618) A[3]:(-0.00493333209306)\n",
      " state (10)  A[0]:(0.728821635246) A[1]:(0.900025963783) A[2]:(-0.000873088603839) A[3]:(0.727002680302)\n",
      " state (11)  A[0]:(0.52260184288) A[1]:(0.876676797867) A[2]:(-0.619863033295) A[3]:(0.84361243248)\n",
      " state (12)  A[0]:(0.079308077693) A[1]:(0.823913276196) A[2]:(-0.608819246292) A[3]:(0.792909622192)\n",
      " state (13)  A[0]:(-0.000282466411591) A[1]:(0.808613955975) A[2]:(0.899830281734) A[3]:(0.727677464485)\n",
      " state (14)  A[0]:(0.810019910336) A[1]:(0.90015411377) A[2]:(0.999999940395) A[3]:(0.809117674828)\n",
      " state (15)  A[0]:(0.983195185661) A[1]:(0.956736981869) A[2]:(1.0) A[3]:(0.877238810062)\n",
      "Episode 760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5997. Times reached goal: 995.               Steps done: 5724469. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00293858085762.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530957937241) A[1]:(0.590387284756) A[2]:(0.590489923954) A[3]:(0.532313406467)\n",
      " state (1)  A[0]:(0.531615614891) A[1]:(5.69224357605e-06) A[2]:(0.656089186668) A[3]:(0.591183543205)\n",
      " state (2)  A[0]:(0.591040968895) A[1]:(0.729044437408) A[2]:(0.590875506401) A[3]:(0.656718552113)\n",
      " state (3)  A[0]:(0.656714856625) A[1]:(-0.217718929052) A[2]:(0.539159595966) A[3]:(0.518218874931)\n",
      " state (4)  A[0]:(0.591079235077) A[1]:(0.65621227026) A[2]:(0.000291347503662) A[3]:(0.53122651577)\n",
      " state (5)  A[0]:(0.163147374988) A[1]:(0.928906917572) A[2]:(-0.192864224315) A[3]:(0.524042069912)\n",
      " state (6)  A[0]:(-3.11732292175e-05) A[1]:(0.81020283699) A[2]:(0.000406622857554) A[3]:(0.655302166939)\n",
      " state (7)  A[0]:(0.627329468727) A[1]:(-0.249022215605) A[2]:(0.301067858934) A[3]:(0.883737742901)\n",
      " state (8)  A[0]:(0.656110823154) A[1]:(0.000502437294926) A[2]:(0.72934114933) A[3]:(0.589951515198)\n",
      " state (9)  A[0]:(0.656147003174) A[1]:(0.810177028179) A[2]:(0.810201525688) A[3]:(0.000143975019455)\n",
      " state (10)  A[0]:(0.729205489159) A[1]:(0.900094926357) A[2]:(2.89678573608e-05) A[3]:(0.7291431427)\n",
      " state (11)  A[0]:(0.523109197617) A[1]:(0.876762449741) A[2]:(-0.61928832531) A[3]:(0.844780087471)\n",
      " state (12)  A[0]:(0.0800520777702) A[1]:(0.824045419693) A[2]:(-0.60821223259) A[3]:(0.794282913208)\n",
      " state (13)  A[0]:(0.000906884437427) A[1]:(0.808800339699) A[2]:(0.900221765041) A[3]:(0.729301214218)\n",
      " state (14)  A[0]:(0.810782074928) A[1]:(0.900301039219) A[2]:(0.999999940395) A[3]:(0.810225486755)\n",
      " state (15)  A[0]:(0.983274579048) A[1]:(0.956800878048) A[2]:(1.0) A[3]:(0.877865731716)\n",
      "Episode 761000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5996. Times reached goal: 994.               Steps done: 5730465. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00292101384534.\n",
      " state (0)  A[0]:(0.531674742699) A[1]:(0.590177059174) A[2]:(0.590288996696) A[3]:(0.530884563923)\n",
      " state (1)  A[0]:(0.532019257545) A[1]:(-1.12652778625e-05) A[2]:(0.655993878841) A[3]:(0.589862465858)\n",
      " state (2)  A[0]:(0.591382443905) A[1]:(0.728861689568) A[2]:(0.590745091438) A[3]:(0.655642271042)\n",
      " state (3)  A[0]:(0.657302379608) A[1]:(-0.218058586121) A[2]:(0.53907585144) A[3]:(0.517606556416)\n",
      " state (4)  A[0]:(0.592046737671) A[1]:(0.655687510967) A[2]:(0.000130295753479) A[3]:(0.530966639519)\n",
      " state (5)  A[0]:(0.164924219251) A[1]:(0.928807199001) A[2]:(-0.193280607462) A[3]:(0.523910164833)\n",
      " state (6)  A[0]:(0.00225478038192) A[1]:(0.809885323048) A[2]:(-4.24385070801e-05) A[3]:(0.655205130577)\n",
      " state (7)  A[0]:(0.628762304783) A[1]:(-0.249950885773) A[2]:(0.300662577152) A[3]:(0.8836363554)\n",
      " state (8)  A[0]:(0.657114326954) A[1]:(-0.000431776017649) A[2]:(0.728847503662) A[3]:(0.589826226234)\n",
      " state (9)  A[0]:(0.656635582447) A[1]:(0.809920251369) A[2]:(0.809951782227) A[3]:(-0.00073336053174)\n",
      " state (10)  A[0]:(0.729389071465) A[1]:(0.90000140667) A[2]:(-0.00053679937264) A[3]:(0.728646218777)\n",
      " state (11)  A[0]:(0.523203015327) A[1]:(0.876692295074) A[2]:(-0.61975812912) A[3]:(0.844503760338)\n",
      " state (12)  A[0]:(0.0798684880137) A[1]:(0.824016213417) A[2]:(-0.60886490345) A[3]:(0.793916761875)\n",
      " state (13)  A[0]:(0.000154912471771) A[1]:(0.808861136436) A[2]:(0.899987697601) A[3]:(0.728748440742)\n",
      " state (14)  A[0]:(0.810166358948) A[1]:(0.900405943394) A[2]:(0.999999940395) A[3]:(0.809675335884)\n",
      " state (15)  A[0]:(0.983178079128) A[1]:(0.956881761551) A[2]:(1.0) A[3]:(0.877393603325)\n",
      "Episode 762000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6002. Times reached goal: 996.               Steps done: 5736467. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00290353442844.\n",
      " state (0)  A[0]:(0.532365918159) A[1]:(0.590537786484) A[2]:(0.590201497078) A[3]:(0.53114014864)\n",
      " state (1)  A[0]:(0.532014727592) A[1]:(0.000446028978331) A[2]:(0.656080126762) A[3]:(0.590261638165)\n",
      " state (2)  A[0]:(0.590849161148) A[1]:(0.728997349739) A[2]:(0.590962648392) A[3]:(0.656096875668)\n",
      " state (3)  A[0]:(0.655713796616) A[1]:(-0.218142211437) A[2]:(0.539336562157) A[3]:(0.518554925919)\n",
      " state (4)  A[0]:(0.589598417282) A[1]:(0.656083583832) A[2]:(0.000152826309204) A[3]:(0.532212734222)\n",
      " state (5)  A[0]:(0.16105723381) A[1]:(0.92890971899) A[2]:(-0.193459838629) A[3]:(0.525346398354)\n",
      " state (6)  A[0]:(-0.00149154546671) A[1]:(0.810024261475) A[2]:(-0.000151515007019) A[3]:(0.65645802021)\n",
      " state (7)  A[0]:(0.626399993896) A[1]:(-0.249509707093) A[2]:(0.300848662853) A[3]:(0.884011745453)\n",
      " state (8)  A[0]:(0.654404640198) A[1]:(0.000354357034666) A[2]:(0.729087471962) A[3]:(0.589644670486)\n",
      " state (9)  A[0]:(0.653639674187) A[1]:(0.810082614422) A[2]:(0.810035586357) A[3]:(-0.00255242921412)\n",
      " state (10)  A[0]:(0.726822674274) A[1]:(0.900025606155) A[2]:(-0.000493287981953) A[3]:(0.727554857731)\n",
      " state (11)  A[0]:(0.519156932831) A[1]:(0.876670300961) A[2]:(-0.619803905487) A[3]:(0.843806743622)\n",
      " state (12)  A[0]:(0.0743578150868) A[1]:(0.823928356171) A[2]:(-0.608952879906) A[3]:(0.793060183525)\n",
      " state (13)  A[0]:(-0.00513528240845) A[1]:(0.808719038963) A[2]:(0.900084614754) A[3]:(0.727837443352)\n",
      " state (14)  A[0]:(0.808581054211) A[1]:(0.900309145451) A[2]:(0.999999940395) A[3]:(0.80930942297)\n",
      " state (15)  A[0]:(0.983028173447) A[1]:(0.956815719604) A[2]:(1.0) A[3]:(0.877312600613)\n",
      "Episode 763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 997.               Steps done: 5742476. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00288613940575.\n",
      " state (0)  A[0]:(0.532916426659) A[1]:(0.590349733829) A[2]:(0.590585231781) A[3]:(0.531741678715)\n",
      " state (1)  A[0]:(0.532274723053) A[1]:(-1.37984752655e-05) A[2]:(0.656025290489) A[3]:(0.590738117695)\n",
      " state (2)  A[0]:(0.591114163399) A[1]:(0.728955984116) A[2]:(0.590577483177) A[3]:(0.656435370445)\n",
      " state (3)  A[0]:(0.656229972839) A[1]:(-0.217734858394) A[2]:(0.538824677467) A[3]:(0.518372654915)\n",
      " state (4)  A[0]:(0.590185523033) A[1]:(0.656054139137) A[2]:(-0.000131607055664) A[3]:(0.53170800209)\n",
      " state (5)  A[0]:(0.161725535989) A[1]:(0.928839325905) A[2]:(-0.193263620138) A[3]:(0.524768471718)\n",
      " state (6)  A[0]:(-0.00125819374807) A[1]:(0.809948205948) A[2]:(0.000191211700439) A[3]:(0.655900776386)\n",
      " state (7)  A[0]:(0.626627087593) A[1]:(-0.249654918909) A[2]:(0.301119565964) A[3]:(0.883855760098)\n",
      " state (8)  A[0]:(0.655540108681) A[1]:(-9.9778175354e-05) A[2]:(0.729006290436) A[3]:(0.590352892876)\n",
      " state (9)  A[0]:(0.65554523468) A[1]:(0.809981107712) A[2]:(0.810001075268) A[3]:(-0.000323042273521)\n",
      " state (10)  A[0]:(0.728707015514) A[1]:(0.900007545948) A[2]:(-0.000230669975281) A[3]:(0.728685259819)\n",
      " state (11)  A[0]:(0.522348761559) A[1]:(0.876681208611) A[2]:(-0.619605541229) A[3]:(0.844494223595)\n",
      " state (12)  A[0]:(0.0788367688656) A[1]:(0.823972821236) A[2]:(-0.608891487122) A[3]:(0.79389333725)\n",
      " state (13)  A[0]:(-0.000895797973499) A[1]:(0.808774590492) A[2]:(0.900002539158) A[3]:(0.728713750839)\n",
      " state (14)  A[0]:(0.809807181358) A[1]:(0.90034288168) A[2]:(0.999999940395) A[3]:(0.809693276882)\n",
      " state (15)  A[0]:(0.983125030994) A[1]:(0.956833124161) A[2]:(1.0) A[3]:(0.877390801907)\n",
      "Episode 764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 998.               Steps done: 5748483. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00286885433402.\n",
      "q_values \n",
      "tensor([[ 0.5293,  0.5903,  0.5904,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5292, -0.0002,  0.6560,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5886,  0.7288,  0.5906,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0037,  0.8099,  0.0004,  0.6552]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7277,  0.9000, -0.0005,  0.7280]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8092,  0.9006,  1.0000,  0.8093]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530287027359) A[1]:(0.590245962143) A[2]:(0.590358018875) A[3]:(0.531457126141)\n",
      " state (1)  A[0]:(0.530322790146) A[1]:(-0.000206395983696) A[2]:(0.655943214893) A[3]:(0.590421617031)\n",
      " state (2)  A[0]:(0.589686632156) A[1]:(0.728831708431) A[2]:(0.590607523918) A[3]:(0.656132221222)\n",
      " state (3)  A[0]:(0.655519366264) A[1]:(-0.218209519982) A[2]:(0.538949847221) A[3]:(0.518011450768)\n",
      " state (4)  A[0]:(0.589801192284) A[1]:(0.655804872513) A[2]:(2.63452529907e-05) A[3]:(0.531383872032)\n",
      " state (5)  A[0]:(0.161500886083) A[1]:(0.928795456886) A[2]:(-0.193166643381) A[3]:(0.524469077587)\n",
      " state (6)  A[0]:(-0.00125765730627) A[1]:(0.809886097908) A[2]:(0.000329613685608) A[3]:(0.65553355217)\n",
      " state (7)  A[0]:(0.626853942871) A[1]:(-0.249951168895) A[2]:(0.301331460476) A[3]:(0.883645713329)\n",
      " state (8)  A[0]:(0.655964016914) A[1]:(-0.000763341609854) A[2]:(0.728820085526) A[3]:(0.590280532837)\n",
      " state (9)  A[0]:(0.655826032162) A[1]:(0.809822022915) A[2]:(0.809976935387) A[3]:(-0.000850856071338)\n",
      " state (10)  A[0]:(0.728888034821) A[1]:(0.899983108044) A[2]:(-8.64267349243e-05) A[3]:(0.728354513645)\n",
      " state (11)  A[0]:(0.522655904293) A[1]:(0.876703083515) A[2]:(-0.619636058807) A[3]:(0.844336569309)\n",
      " state (12)  A[0]:(0.0792160332203) A[1]:(0.824066996574) A[2]:(-0.609122216702) A[3]:(0.793714404106)\n",
      " state (13)  A[0]:(-0.000824272457976) A[1]:(0.808943986893) A[2]:(0.899858117104) A[3]:(0.728477954865)\n",
      " state (14)  A[0]:(0.809582054615) A[1]:(0.900481939316) A[2]:(0.999999940395) A[3]:(0.809500873089)\n",
      " state (15)  A[0]:(0.983079612255) A[1]:(0.956918776035) A[2]:(1.0) A[3]:(0.877250671387)\n",
      "Episode 765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 997.               Steps done: 5754492. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00285166707916.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530421376228) A[1]:(0.590830564499) A[2]:(0.5906894207) A[3]:(0.532911002636)\n",
      " state (1)  A[0]:(0.530447483063) A[1]:(-0.000104397535324) A[2]:(0.656353771687) A[3]:(0.592220902443)\n",
      " state (2)  A[0]:(0.58980768919) A[1]:(0.729171276093) A[2]:(0.591261863708) A[3]:(0.657946825027)\n",
      " state (3)  A[0]:(0.655659914017) A[1]:(-0.216929242015) A[2]:(0.539291977882) A[3]:(0.5204834342)\n",
      " state (4)  A[0]:(0.58993601799) A[1]:(0.656255960464) A[2]:(0.000311732292175) A[3]:(0.533967137337)\n",
      " state (5)  A[0]:(0.161707878113) A[1]:(0.928872764111) A[2]:(-0.193050965667) A[3]:(0.527393996716)\n",
      " state (6)  A[0]:(-0.000781953160185) A[1]:(0.810055732727) A[2]:(0.00032103061676) A[3]:(0.658241510391)\n",
      " state (7)  A[0]:(0.627596795559) A[1]:(-0.249339833856) A[2]:(0.301460117102) A[3]:(0.884878218174)\n",
      " state (8)  A[0]:(0.656991958618) A[1]:(0.000353634328349) A[2]:(0.729259133339) A[3]:(0.593599379063)\n",
      " state (9)  A[0]:(0.65726596117) A[1]:(0.810075879097) A[2]:(0.810128808022) A[3]:(0.0051609268412)\n",
      " state (10)  A[0]:(0.730051517487) A[1]:(0.899954020977) A[2]:(0.000363349885447) A[3]:(0.731177568436)\n",
      " state (11)  A[0]:(0.524379014969) A[1]:(0.87650167942) A[2]:(-0.619252681732) A[3]:(0.84597080946)\n",
      " state (12)  A[0]:(0.0816437825561) A[1]:(0.823578596115) A[2]:(-0.60874068737) A[3]:(0.795809984207)\n",
      " state (13)  A[0]:(0.0018636562163) A[1]:(0.808196723461) A[2]:(0.900081157684) A[3]:(0.731148898602)\n",
      " state (14)  A[0]:(0.810668766499) A[1]:(0.89995187521) A[2]:(0.999999940395) A[3]:(0.811408996582)\n",
      " state (15)  A[0]:(0.983181476593) A[1]:(0.956611990929) A[2]:(1.0) A[3]:(0.878445088863)\n",
      "Episode 766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 994.               Steps done: 5760492. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00283460830418.\n",
      " state (0)  A[0]:(0.531472086906) A[1]:(0.590482354164) A[2]:(0.590497493744) A[3]:(0.531631231308)\n",
      " state (1)  A[0]:(0.531330525875) A[1]:(-2.06604599953e-05) A[2]:(0.656106114388) A[3]:(0.590491294861)\n",
      " state (2)  A[0]:(0.590570092201) A[1]:(0.728951096535) A[2]:(0.590809702873) A[3]:(0.656192421913)\n",
      " state (3)  A[0]:(0.656137347221) A[1]:(-0.217820450664) A[2]:(0.539150118828) A[3]:(0.51815032959)\n",
      " state (4)  A[0]:(0.590516507626) A[1]:(0.656078338623) A[2]:(6.42538070679e-05) A[3]:(0.531653225422)\n",
      " state (5)  A[0]:(0.162690967321) A[1]:(0.928869307041) A[2]:(-0.193450659513) A[3]:(0.524965286255)\n",
      " state (6)  A[0]:(-7.80820846558e-05) A[1]:(0.809981524944) A[2]:(-1.23977661133e-05) A[3]:(0.656085371971)\n",
      " state (7)  A[0]:(0.627102971077) A[1]:(-0.249593570828) A[2]:(0.301289737225) A[3]:(0.883813977242)\n",
      " state (8)  A[0]:(0.655894219875) A[1]:(1.75461173058e-05) A[2]:(0.728994727135) A[3]:(0.590405225754)\n",
      " state (9)  A[0]:(0.655869603157) A[1]:(0.809994339943) A[2]:(0.810001134872) A[3]:(-1.42604112625e-05)\n",
      " state (10)  A[0]:(0.728910565376) A[1]:(0.899990797043) A[2]:(-0.000227689743042) A[3]:(0.728882193565)\n",
      " state (11)  A[0]:(0.522668421268) A[1]:(0.876646757126) A[2]:(-0.619843840599) A[3]:(0.844654321671)\n",
      " state (12)  A[0]:(0.0793003216386) A[1]:(0.8239377141) A[2]:(-0.609433114529) A[3]:(0.79413151741)\n",
      " state (13)  A[0]:(-0.000477731198771) A[1]:(0.808796107769) A[2]:(0.899998486042) A[3]:(0.729021906853)\n",
      " state (14)  A[0]:(0.809875547886) A[1]:(0.900422871113) A[2]:(0.999999940395) A[3]:(0.809891164303)\n",
      " state (15)  A[0]:(0.983084440231) A[1]:(0.956876397133) A[2]:(1.0) A[3]:(0.877408027649)\n",
      "Episode 767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6012. Times reached goal: 998.               Steps done: 5766504. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0028176177638.\n",
      " state (0)  A[0]:(0.531558275223) A[1]:(0.590638995171) A[2]:(0.590555131435) A[3]:(0.531853914261)\n",
      " state (1)  A[0]:(0.53145211935) A[1]:(4.85554337502e-05) A[2]:(0.656178116798) A[3]:(0.590762913227)\n",
      " state (2)  A[0]:(0.590698242188) A[1]:(0.729050338268) A[2]:(0.590815603733) A[3]:(0.656463980675)\n",
      " state (3)  A[0]:(0.656240284443) A[1]:(-0.216312259436) A[2]:(0.538912653923) A[3]:(0.518421530724)\n",
      " state (4)  A[0]:(0.590540289879) A[1]:(0.656191885471) A[2]:(7.86781311035e-06) A[3]:(0.531727254391)\n",
      " state (5)  A[0]:(0.16268453002) A[1]:(0.9288803339) A[2]:(-0.193445965648) A[3]:(0.525011658669)\n",
      " state (6)  A[0]:(7.18235969543e-05) A[1]:(0.810035467148) A[2]:(0.000108599662781) A[3]:(0.656046688557)\n",
      " state (7)  A[0]:(0.627427458763) A[1]:(-0.249395743012) A[2]:(0.301628857851) A[3]:(0.883764445782)\n",
      " state (8)  A[0]:(0.656322538853) A[1]:(0.000140614807606) A[2]:(0.729061245918) A[3]:(0.590498447418)\n",
      " state (9)  A[0]:(0.656193554401) A[1]:(0.810030460358) A[2]:(0.810011029243) A[3]:(0.000173017382622)\n",
      " state (10)  A[0]:(0.729121506214) A[1]:(0.900004923344) A[2]:(-0.000259637832642) A[3]:(0.728813648224)\n",
      " state (11)  A[0]:(0.522983670235) A[1]:(0.876636683941) A[2]:(-0.619973957539) A[3]:(0.844547986984)\n",
      " state (12)  A[0]:(0.0797457695007) A[1]:(0.823872864246) A[2]:(-0.609673678875) A[3]:(0.793942928314)\n",
      " state (13)  A[0]:(1.5914440155e-05) A[1]:(0.808662950993) A[2]:(0.899978220463) A[3]:(0.728772819042)\n",
      " state (14)  A[0]:(0.810072422028) A[1]:(0.900307774544) A[2]:(0.999999940395) A[3]:(0.809772491455)\n",
      " state (15)  A[0]:(0.983095467091) A[1]:(0.956798315048) A[2]:(1.0) A[3]:(0.877358078957)\n",
      "Episode 768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6016. Times reached goal: 999.               Steps done: 5772520. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00280071786121.\n",
      " state (0)  A[0]:(0.531388640404) A[1]:(0.590211749077) A[2]:(0.590396225452) A[3]:(0.531042575836)\n",
      " state (1)  A[0]:(0.531250476837) A[1]:(-0.000215724110603) A[2]:(0.655920386314) A[3]:(0.590106844902)\n",
      " state (2)  A[0]:(0.59047305584) A[1]:(0.728815317154) A[2]:(0.590388417244) A[3]:(0.65595895052)\n",
      " state (3)  A[0]:(0.655924201012) A[1]:(-0.216473594308) A[2]:(0.538653373718) A[3]:(0.51810580492)\n",
      " state (4)  A[0]:(0.590218186378) A[1]:(0.655787825584) A[2]:(-0.000177979469299) A[3]:(0.531500518322)\n",
      " state (5)  A[0]:(0.162432149053) A[1]:(0.928816437721) A[2]:(-0.193730399013) A[3]:(0.5248259902)\n",
      " state (6)  A[0]:(-6.62207603455e-05) A[1]:(0.809887468815) A[2]:(-0.000109434127808) A[3]:(0.655854344368)\n",
      " state (7)  A[0]:(0.627151966095) A[1]:(-0.249799221754) A[2]:(0.30149513483) A[3]:(0.883657932281)\n",
      " state (8)  A[0]:(0.656019091606) A[1]:(-0.00046299395035) A[2]:(0.728666543961) A[3]:(0.59083044529)\n",
      " state (9)  A[0]:(0.655791580677) A[1]:(0.809884548187) A[2]:(0.809833407402) A[3]:(0.000584512890782)\n",
      " state (10)  A[0]:(0.728849172592) A[1]:(0.899984478951) A[2]:(-0.000583648623433) A[3]:(0.729100942612)\n",
      " state (11)  A[0]:(0.5227445364) A[1]:(0.876644313335) A[2]:(-0.620260417461) A[3]:(0.844831049442)\n",
      " state (12)  A[0]:(0.0795686244965) A[1]:(0.823903799057) A[2]:(-0.610010445118) A[3]:(0.79436981678)\n",
      " state (13)  A[0]:(-0.00018972158432) A[1]:(0.808712780476) A[2]:(0.899962365627) A[3]:(0.729359507561)\n",
      " state (14)  A[0]:(0.809862434864) A[1]:(0.900344908237) A[2]:(0.999999940395) A[3]:(0.810250759125)\n",
      " state (15)  A[0]:(0.983044505119) A[1]:(0.956807136536) A[2]:(1.0) A[3]:(0.877672255039)\n",
      "Episode 769000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6012. Times reached goal: 998.               Steps done: 5778532. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00278393045892.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5905,  0.5906,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315, -0.0004,  0.6561,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7289,  0.5907,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-5.4950e-04,  8.0998e-01,  2.3842e-06,  6.5611e-01]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7288,  0.9000, -0.0004,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9004,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531549692154) A[1]:(0.590509474277) A[2]:(0.590624094009) A[3]:(0.531284809113)\n",
      " state (1)  A[0]:(0.531416535378) A[1]:(-0.00036027279566) A[2]:(0.656093120575) A[3]:(0.590286254883)\n",
      " state (2)  A[0]:(0.590539038181) A[1]:(0.728943943977) A[2]:(0.59067094326) A[3]:(0.656131386757)\n",
      " state (3)  A[0]:(0.656121253967) A[1]:(-0.216182738543) A[2]:(0.538807749748) A[3]:(0.518030405045)\n",
      " state (4)  A[0]:(0.590254545212) A[1]:(0.656118273735) A[2]:(-0.000106692314148) A[3]:(0.531494855881)\n",
      " state (5)  A[0]:(0.162040814757) A[1]:(0.92885518074) A[2]:(-0.19363643229) A[3]:(0.52499204874)\n",
      " state (6)  A[0]:(-0.000680029275827) A[1]:(0.809964537621) A[2]:(-1.22785568237e-05) A[3]:(0.656025648117)\n",
      " state (7)  A[0]:(0.626919984818) A[1]:(-0.249577373266) A[2]:(0.301755964756) A[3]:(0.88366830349)\n",
      " state (8)  A[0]:(0.655883133411) A[1]:(-0.000188134610653) A[2]:(0.728960573673) A[3]:(0.590389728546)\n",
      " state (9)  A[0]:(0.655717372894) A[1]:(0.809953987598) A[2]:(0.809977412224) A[3]:(-0.000265806913376)\n",
      " state (10)  A[0]:(0.728781223297) A[1]:(0.900002360344) A[2]:(-0.000428557366831) A[3]:(0.728731632233)\n",
      " state (11)  A[0]:(0.522599637508) A[1]:(0.876654684544) A[2]:(-0.620293021202) A[3]:(0.844603836536)\n",
      " state (12)  A[0]:(0.0793146565557) A[1]:(0.823907852173) A[2]:(-0.61016023159) A[3]:(0.794080138206)\n",
      " state (13)  A[0]:(-0.000477015943034) A[1]:(0.80871295929) A[2]:(0.89999371767) A[3]:(0.729000926018)\n",
      " state (14)  A[0]:(0.809766054153) A[1]:(0.900349736214) A[2]:(0.999999940395) A[3]:(0.809996604919)\n",
      " state (15)  A[0]:(0.983018279076) A[1]:(0.95679962635) A[2]:(1.0) A[3]:(0.877469778061)\n",
      "Episode 770000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 997.               Steps done: 5784535. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00276726858503.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531416416168) A[1]:(0.590781211853) A[2]:(0.590551137924) A[3]:(0.531617164612)\n",
      " state (1)  A[0]:(0.531571030617) A[1]:(0.000372409791453) A[2]:(0.656210422516) A[3]:(0.590308189392)\n",
      " state (2)  A[0]:(0.590728759766) A[1]:(0.729093909264) A[2]:(0.590881228447) A[3]:(0.656076908112)\n",
      " state (3)  A[0]:(0.655877351761) A[1]:(-0.215121909976) A[2]:(0.53914129734) A[3]:(0.517835021019)\n",
      " state (4)  A[0]:(0.590084791183) A[1]:(0.65582716465) A[2]:(0.000389814347727) A[3]:(0.531187415123)\n",
      " state (5)  A[0]:(0.162416085601) A[1]:(0.928883314133) A[2]:(-0.193722605705) A[3]:(0.524952173233)\n",
      " state (6)  A[0]:(0.000112652778625) A[1]:(0.80999815464) A[2]:(-9.00030136108e-05) A[3]:(0.656279206276)\n",
      " state (7)  A[0]:(0.626951098442) A[1]:(-0.249433606863) A[2]:(0.301951408386) A[3]:(0.883799493313)\n",
      " state (8)  A[0]:(0.655533850193) A[1]:(0.000297576189041) A[2]:(0.728969037533) A[3]:(0.590558767319)\n",
      " state (9)  A[0]:(0.655355215073) A[1]:(0.810064673424) A[2]:(0.809947967529) A[3]:(-0.000549241842236)\n",
      " state (10)  A[0]:(0.728454053402) A[1]:(0.900007367134) A[2]:(-0.000408291787608) A[3]:(0.728459239006)\n",
      " state (11)  A[0]:(0.522107481956) A[1]:(0.876600801945) A[2]:(-0.620296418667) A[3]:(0.844430446625)\n",
      " state (12)  A[0]:(0.0787540823221) A[1]:(0.823760390282) A[2]:(-0.610242962837) A[3]:(0.79389244318)\n",
      " state (13)  A[0]:(-0.000795125786681) A[1]:(0.808484256268) A[2]:(0.900011360645) A[3]:(0.728834569454)\n",
      " state (14)  A[0]:(0.809844970703) A[1]:(0.90019351244) A[2]:(0.999999940395) A[3]:(0.809953987598)\n",
      " state (15)  A[0]:(0.983033239841) A[1]:(0.956707775593) A[2]:(1.0) A[3]:(0.877469599247)\n",
      "Episode 771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 997.               Steps done: 5790541. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00275069818064.\n",
      " state (0)  A[0]:(0.531390547752) A[1]:(0.59049987793) A[2]:(0.590522527695) A[3]:(0.531239151955)\n",
      " state (1)  A[0]:(0.531299233437) A[1]:(0.000432699889643) A[2]:(0.656211256981) A[3]:(0.59040749073)\n",
      " state (2)  A[0]:(0.590531229973) A[1]:(0.729178965092) A[2]:(0.590749979019) A[3]:(0.656294584274)\n",
      " state (3)  A[0]:(0.656045854092) A[1]:(-0.215707883239) A[2]:(0.539040982723) A[3]:(0.518072843552)\n",
      " state (4)  A[0]:(0.590274214745) A[1]:(0.656344056129) A[2]:(0.000283002853394) A[3]:(0.531428039074)\n",
      " state (5)  A[0]:(0.162306040525) A[1]:(0.928933680058) A[2]:(-0.193328917027) A[3]:(0.524898648262)\n",
      " state (6)  A[0]:(-0.00030243396759) A[1]:(0.810185790062) A[2]:(0.000511050166097) A[3]:(0.655824065208)\n",
      " state (7)  A[0]:(0.627078056335) A[1]:(-0.248963266611) A[2]:(0.302708208561) A[3]:(0.883488237858)\n",
      " state (8)  A[0]:(0.655837833881) A[1]:(0.000325977802277) A[2]:(0.729334294796) A[3]:(0.589976429939)\n",
      " state (9)  A[0]:(0.655441999435) A[1]:(0.810132622719) A[2]:(0.810175955296) A[3]:(-0.000778853718657)\n",
      " state (10)  A[0]:(0.728510260582) A[1]:(0.900108277798) A[2]:(-0.00020694732666) A[3]:(0.728660345078)\n",
      " state (11)  A[0]:(0.522180974483) A[1]:(0.876776576042) A[2]:(-0.620449662209) A[3]:(0.844622254372)\n",
      " state (12)  A[0]:(0.0787645056844) A[1]:(0.824064970016) A[2]:(-0.610494554043) A[3]:(0.794105172157)\n",
      " state (13)  A[0]:(-0.000803053204436) A[1]:(0.808895111084) A[2]:(0.900121152401) A[3]:(0.728984594345)\n",
      " state (14)  A[0]:(0.809834957123) A[1]:(0.9004778862) A[2]:(0.999999940395) A[3]:(0.809876859188)\n",
      " state (15)  A[0]:(0.983003616333) A[1]:(0.95684170723) A[2]:(1.0) A[3]:(0.877233445644)\n",
      "Episode 772000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 998.               Steps done: 5796548. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00273422426561.\n",
      " state (0)  A[0]:(0.531345009804) A[1]:(0.590594530106) A[2]:(0.590438783169) A[3]:(0.531288504601)\n",
      " state (1)  A[0]:(0.531393408775) A[1]:(0.000130154192448) A[2]:(0.656153380871) A[3]:(0.590466499329)\n",
      " state (2)  A[0]:(0.590617775917) A[1]:(0.729090690613) A[2]:(0.590747833252) A[3]:(0.656413316727)\n",
      " state (3)  A[0]:(0.656403899193) A[1]:(-0.215946048498) A[2]:(0.538864254951) A[3]:(0.518681168556)\n",
      " state (4)  A[0]:(0.590577661991) A[1]:(0.656215071678) A[2]:(-8.04662704468e-05) A[3]:(0.532246291637)\n",
      " state (5)  A[0]:(0.162357285619) A[1]:(0.928876459599) A[2]:(-0.19366247952) A[3]:(0.525704264641)\n",
      " state (6)  A[0]:(-0.000424444646342) A[1]:(0.810083091259) A[2]:(0.000118374824524) A[3]:(0.656184613705)\n",
      " state (7)  A[0]:(0.62720477581) A[1]:(-0.249014422297) A[2]:(0.302359253168) A[3]:(0.883441030979)\n",
      " state (8)  A[0]:(0.656310975552) A[1]:(0.0002656057477) A[2]:(0.729181885719) A[3]:(0.589992463589)\n",
      " state (9)  A[0]:(0.656164169312) A[1]:(0.810067117214) A[2]:(0.810130417347) A[3]:(-0.000170916318893)\n",
      " state (10)  A[0]:(0.729121685028) A[1]:(0.90005248785) A[2]:(-0.000167489051819) A[3]:(0.728909909725)\n",
      " state (11)  A[0]:(0.523085772991) A[1]:(0.87669557333) A[2]:(-0.620468497276) A[3]:(0.844712555408)\n",
      " state (12)  A[0]:(0.0799251049757) A[1]:(0.823937594891) A[2]:(-0.610662341118) A[3]:(0.794163227081)\n",
      " state (13)  A[0]:(0.000175058841705) A[1]:(0.808739066124) A[2]:(0.900058865547) A[3]:(0.729008376598)\n",
      " state (14)  A[0]:(0.810035467148) A[1]:(0.900384306908) A[2]:(0.999999940395) A[3]:(0.809877455235)\n",
      " state (15)  A[0]:(0.983004629612) A[1]:(0.956792354584) A[2]:(1.0) A[3]:(0.877221226692)\n",
      "Episode 773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 998.               Steps done: 5802559. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00271783814138.\n",
      " state (0)  A[0]:(0.531484484673) A[1]:(0.590498447418) A[2]:(0.590509772301) A[3]:(0.531890690327)\n",
      " state (1)  A[0]:(0.531027972698) A[1]:(0.000260069966316) A[2]:(0.656119227409) A[3]:(0.591153562069)\n",
      " state (2)  A[0]:(0.590191364288) A[1]:(0.729074239731) A[2]:(0.59060382843) A[3]:(0.65702188015)\n",
      " state (3)  A[0]:(0.656242489815) A[1]:(-0.215165570378) A[2]:(0.538836240768) A[3]:(0.518998503685)\n",
      " state (4)  A[0]:(0.590646803379) A[1]:(0.655908465385) A[2]:(0.000104188919067) A[3]:(0.532262146473)\n",
      " state (5)  A[0]:(0.162835508585) A[1]:(0.928842306137) A[2]:(-0.193746000528) A[3]:(0.525850653648)\n",
      " state (6)  A[0]:(5.4657459259e-05) A[1]:(0.810024857521) A[2]:(-4.8041343689e-05) A[3]:(0.656497478485)\n",
      " state (7)  A[0]:(0.627091407776) A[1]:(-0.24912545085) A[2]:(0.302262961864) A[3]:(0.883650779724)\n",
      " state (8)  A[0]:(0.65610897541) A[1]:(0.000358715624316) A[2]:(0.729116082191) A[3]:(0.590694904327)\n",
      " state (9)  A[0]:(0.656052172184) A[1]:(0.810072243214) A[2]:(0.810048818588) A[3]:(0.00104100967292)\n",
      " state (10)  A[0]:(0.729088187218) A[1]:(0.900014579296) A[2]:(-0.000128149986267) A[3]:(0.729375422001)\n",
      " state (11)  A[0]:(0.523181557655) A[1]:(0.876610577106) A[2]:(-0.620370745659) A[3]:(0.844951510429)\n",
      " state (12)  A[0]:(0.0802750512958) A[1]:(0.823767662048) A[2]:(-0.61065030098) A[3]:(0.794463574886)\n",
      " state (13)  A[0]:(0.000616908015218) A[1]:(0.808489143848) A[2]:(0.900030791759) A[3]:(0.729399979115)\n",
      " state (14)  A[0]:(0.810169875622) A[1]:(0.900210857391) A[2]:(0.999999940395) A[3]:(0.810195863247)\n",
      " state (15)  A[0]:(0.983011722565) A[1]:(0.956694960594) A[2]:(1.0) A[3]:(0.877448022366)\n",
      "Episode 774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6019. Times reached goal: 999.               Steps done: 5808578. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0027015286064.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5905,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314, -0.0002,  0.6561,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7290,  0.5906,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  0.8100, -0.0000,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0003,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9004,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531656503677) A[1]:(0.590472698212) A[2]:(0.590528130531) A[3]:(0.531516551971)\n",
      " state (1)  A[0]:(0.531381547451) A[1]:(-0.000215157866478) A[2]:(0.656043589115) A[3]:(0.590400338173)\n",
      " state (2)  A[0]:(0.590519726276) A[1]:(0.728947341442) A[2]:(0.590631365776) A[3]:(0.65621471405)\n",
      " state (3)  A[0]:(0.656150341034) A[1]:(-0.216506943107) A[2]:(0.538944602013) A[3]:(0.517895579338)\n",
      " state (4)  A[0]:(0.590284109116) A[1]:(0.656235694885) A[2]:(-0.000128388404846) A[3]:(0.531353354454)\n",
      " state (5)  A[0]:(0.162062928081) A[1]:(0.928881347179) A[2]:(-0.193883910775) A[3]:(0.524956941605)\n",
      " state (6)  A[0]:(-0.000449001759989) A[1]:(0.809950530529) A[2]:(-9.33408737183e-05) A[3]:(0.655987262726)\n",
      " state (7)  A[0]:(0.627086102962) A[1]:(-0.24949875474) A[2]:(0.302185624838) A[3]:(0.883580088615)\n",
      " state (8)  A[0]:(0.656305909157) A[1]:(0.000114396214485) A[2]:(0.728840589523) A[3]:(0.590732574463)\n",
      " state (9)  A[0]:(0.656358957291) A[1]:(0.810018479824) A[2]:(0.809910833836) A[3]:(0.000477209658129)\n",
      " state (10)  A[0]:(0.729170918465) A[1]:(0.899994671345) A[2]:(-0.000379800767405) A[3]:(0.728900194168)\n",
      " state (11)  A[0]:(0.522989451885) A[1]:(0.876612305641) A[2]:(-0.620623707771) A[3]:(0.844636142254)\n",
      " state (12)  A[0]:(0.079646922648) A[1]:(0.823836922646) A[2]:(-0.611029267311) A[3]:(0.79406774044)\n",
      " state (13)  A[0]:(-0.000186264514923) A[1]:(0.808671951294) A[2]:(0.899963021278) A[3]:(0.728945612907)\n",
      " state (14)  A[0]:(0.809954762459) A[1]:(0.900392949581) A[2]:(0.999999940395) A[3]:(0.809923648834)\n",
      " state (15)  A[0]:(0.982988476753) A[1]:(0.956809163094) A[2]:(1.0) A[3]:(0.877287507057)\n",
      "Episode 775000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6013. Times reached goal: 999.               Steps done: 5814591. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00268533305561.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531500697136) A[1]:(0.590634703636) A[2]:(0.590547084808) A[3]:(0.531024694443)\n",
      " state (1)  A[0]:(0.531394064426) A[1]:(-1.02818012238e-05) A[2]:(0.656092166901) A[3]:(0.589857578278)\n",
      " state (2)  A[0]:(0.590636849403) A[1]:(0.728931009769) A[2]:(0.590778231621) A[3]:(0.655746817589)\n",
      " state (3)  A[0]:(0.656297683716) A[1]:(-0.217129990458) A[2]:(0.539238929749) A[3]:(0.517461240292)\n",
      " state (4)  A[0]:(0.590574383736) A[1]:(0.656170368195) A[2]:(8.97645950317e-05) A[3]:(0.531145572662)\n",
      " state (5)  A[0]:(0.16260252893) A[1]:(0.928888261318) A[2]:(-0.19384123385) A[3]:(0.524903893471)\n",
      " state (6)  A[0]:(0.000104129314423) A[1]:(0.809960842133) A[2]:(-5.48362731934e-05) A[3]:(0.656061053276)\n",
      " state (7)  A[0]:(0.627298593521) A[1]:(-0.249444231391) A[2]:(0.302413970232) A[3]:(0.88360697031)\n",
      " state (8)  A[0]:(0.656294763088) A[1]:(0.000337213277817) A[2]:(0.729050040245) A[3]:(0.590313315392)\n",
      " state (9)  A[0]:(0.656268060207) A[1]:(0.810084998608) A[2]:(0.809986770153) A[3]:(-0.000547379197087)\n",
      " state (10)  A[0]:(0.72905421257) A[1]:(0.899991095066) A[2]:(-0.00043427941273) A[3]:(0.728495240211)\n",
      " state (11)  A[0]:(0.522723555565) A[1]:(0.876561641693) A[2]:(-0.620783090591) A[3]:(0.844425082207)\n",
      " state (12)  A[0]:(0.0792343392968) A[1]:(0.823713004589) A[2]:(-0.611244738102) A[3]:(0.793841719627)\n",
      " state (13)  A[0]:(-0.000431358785136) A[1]:(0.808500409126) A[2]:(0.900035262108) A[3]:(0.728729248047)\n",
      " state (14)  A[0]:(0.810026884079) A[1]:(0.900286197662) A[2]:(0.999999940395) A[3]:(0.809803366661)\n",
      " state (15)  A[0]:(0.982991576195) A[1]:(0.956741929054) A[2]:(1.0) A[3]:(0.877191603184)\n",
      "Episode 776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 997.               Steps done: 5820601. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00266924260418.\n",
      " state (0)  A[0]:(0.531537532806) A[1]:(0.590460181236) A[2]:(0.590540111065) A[3]:(0.530907154083)\n",
      " state (1)  A[0]:(0.531163930893) A[1]:(-9.61124897003e-07) A[2]:(0.656119585037) A[3]:(0.590123176575)\n",
      " state (2)  A[0]:(0.590267896652) A[1]:(0.728949368) A[2]:(0.590753793716) A[3]:(0.656074285507)\n",
      " state (3)  A[0]:(0.65575170517) A[1]:(-0.216296598315) A[2]:(0.539097011089) A[3]:(0.51778370142)\n",
      " state (4)  A[0]:(0.589849293232) A[1]:(0.656033813953) A[2]:(5.90085983276e-05) A[3]:(0.53126502037)\n",
      " state (5)  A[0]:(0.161593645811) A[1]:(0.928877174854) A[2]:(-0.193946778774) A[3]:(0.524945497513)\n",
      " state (6)  A[0]:(-0.000903486972675) A[1]:(0.809964537621) A[2]:(-9.51290130615e-05) A[3]:(0.655867338181)\n",
      " state (7)  A[0]:(0.626611828804) A[1]:(-0.249527737498) A[2]:(0.30252918601) A[3]:(0.883412599564)\n",
      " state (8)  A[0]:(0.655576109886) A[1]:(-6.11692667007e-06) A[2]:(0.72891408205) A[3]:(0.590411424637)\n",
      " state (9)  A[0]:(0.655367255211) A[1]:(0.80998903513) A[2]:(0.809911131859) A[3]:(0.000480309099657)\n",
      " state (10)  A[0]:(0.72832262516) A[1]:(0.899984240532) A[2]:(-0.000672459485941) A[3]:(0.729237437248)\n",
      " state (11)  A[0]:(0.521655499935) A[1]:(0.876590669155) A[2]:(-0.62106013298) A[3]:(0.844947218895)\n",
      " state (12)  A[0]:(0.0777789950371) A[1]:(0.823790252209) A[2]:(-0.611645102501) A[3]:(0.794488072395)\n",
      " state (13)  A[0]:(-0.00191718107089) A[1]:(0.808613419533) A[2]:(0.899946033955) A[3]:(0.729422330856)\n",
      " state (14)  A[0]:(0.809548199177) A[1]:(0.900360524654) A[2]:(0.999999940395) A[3]:(0.810141563416)\n",
      " state (15)  A[0]:(0.98293697834) A[1]:(0.956772744656) A[2]:(1.0) A[3]:(0.87727534771)\n",
      "Episode 777000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 998.               Steps done: 5826605. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00265326448589.\n",
      " state (0)  A[0]:(0.531438291073) A[1]:(0.590887546539) A[2]:(0.590529084206) A[3]:(0.531389176846)\n",
      " state (1)  A[0]:(0.531836271286) A[1]:(0.000499129237141) A[2]:(0.656212091446) A[3]:(0.590165257454)\n",
      " state (2)  A[0]:(0.591241538525) A[1]:(0.729275941849) A[2]:(0.590883374214) A[3]:(0.656023979187)\n",
      " state (3)  A[0]:(0.656602621078) A[1]:(-0.215743198991) A[2]:(0.539224803448) A[3]:(0.517649829388)\n",
      " state (4)  A[0]:(0.590840816498) A[1]:(0.656552135944) A[2]:(0.000293016433716) A[3]:(0.531226277351)\n",
      " state (5)  A[0]:(0.163010567427) A[1]:(0.928974509239) A[2]:(-0.193522036076) A[3]:(0.524997711182)\n",
      " state (6)  A[0]:(-9.22083854675e-05) A[1]:(0.810317099094) A[2]:(0.00047123429249) A[3]:(0.655882716179)\n",
      " state (7)  A[0]:(0.626795768738) A[1]:(-0.248596101999) A[2]:(0.303525000811) A[3]:(0.883369684219)\n",
      " state (8)  A[0]:(0.656280517578) A[1]:(0.000696748378687) A[2]:(0.729758262634) A[3]:(0.589603066444)\n",
      " state (9)  A[0]:(0.657034516335) A[1]:(0.810262680054) A[2]:(0.810381829739) A[3]:(-0.000945552892517)\n",
      " state (10)  A[0]:(0.730275154114) A[1]:(0.900179207325) A[2]:(-0.000161290168762) A[3]:(0.728916049004)\n",
      " state (11)  A[0]:(0.525119185448) A[1]:(0.876876115799) A[2]:(-0.621037185192) A[3]:(0.844864606857)\n",
      " state (12)  A[0]:(0.0827647671103) A[1]:(0.824239373207) A[2]:(-0.611676216125) A[3]:(0.794415235519)\n",
      " state (13)  A[0]:(0.00320379831828) A[1]:(0.8091609478) A[2]:(0.900157213211) A[3]:(0.729331254959)\n",
      " state (14)  A[0]:(0.811244010925) A[1]:(0.9006986022) A[2]:(0.999999940395) A[3]:(0.810078501701)\n",
      " state (15)  A[0]:(0.983060717583) A[1]:(0.956915974617) A[2]:(1.0) A[3]:(0.877149939537)\n",
      "Episode 778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 995.               Steps done: 5832605. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00263739256237.\n",
      " state (0)  A[0]:(0.531335353851) A[1]:(0.590638637543) A[2]:(0.590508580208) A[3]:(0.531460762024)\n",
      " state (1)  A[0]:(0.531161427498) A[1]:(0.000185087323189) A[2]:(0.656138062477) A[3]:(0.59026157856)\n",
      " state (2)  A[0]:(0.590395092964) A[1]:(0.729031264782) A[2]:(0.590774416924) A[3]:(0.656062960625)\n",
      " state (3)  A[0]:(0.656135678291) A[1]:(-0.216630727053) A[2]:(0.53913128376) A[3]:(0.517757952213)\n",
      " state (4)  A[0]:(0.59038567543) A[1]:(0.656278610229) A[2]:(-6.4492225647e-05) A[3]:(0.531293511391)\n",
      " state (5)  A[0]:(0.1622916013) A[1]:(0.928903460503) A[2]:(-0.193980172276) A[3]:(0.524981200695)\n",
      " state (6)  A[0]:(-0.000340163707733) A[1]:(0.810000002384) A[2]:(0.000109076499939) A[3]:(0.655930280685)\n",
      " state (7)  A[0]:(0.626841723919) A[1]:(-0.249325305223) A[2]:(0.303104162216) A[3]:(0.883453607559)\n",
      " state (8)  A[0]:(0.656046271324) A[1]:(9.38326120377e-05) A[2]:(0.729065895081) A[3]:(0.590678632259)\n",
      " state (9)  A[0]:(0.656123518944) A[1]:(0.810017108917) A[2]:(0.81003010273) A[3]:(0.000252142548561)\n",
      " state (10)  A[0]:(0.729039847851) A[1]:(0.900002837181) A[2]:(-0.000427246064646) A[3]:(0.728985130787)\n",
      " state (11)  A[0]:(0.5228317976) A[1]:(0.876590192318) A[2]:(-0.62112891674) A[3]:(0.84480202198)\n",
      " state (12)  A[0]:(0.0793718099594) A[1]:(0.823734343052) A[2]:(-0.611930429935) A[3]:(0.794314265251)\n",
      " state (13)  A[0]:(-0.000402808160288) A[1]:(0.808478951454) A[2]:(0.899975001812) A[3]:(0.729213178158)\n",
      " state (14)  A[0]:(0.810009121895) A[1]:(0.900239348412) A[2]:(0.999999940395) A[3]:(0.809997797012)\n",
      " state (15)  A[0]:(0.982948362827) A[1]:(0.956675946712) A[2]:(1.0) A[3]:(0.877126097679)\n",
      "Episode 779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5996. Times reached goal: 997.               Steps done: 5838601. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00262162607174.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5906,  0.5905,  0.5309]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6560,  0.0003,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.0003,  0.7290,  0.5911]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6571,  0.8101,  0.8100,  0.0017]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0021,  0.8082,  0.9000,  0.7300]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8108,  0.9000,  1.0000,  0.8106]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531089425087) A[1]:(0.590597987175) A[2]:(0.590485215187) A[3]:(0.530550837517)\n",
      " state (1)  A[0]:(0.53092944622) A[1]:(5.92321157455e-05) A[2]:(0.656167566776) A[3]:(0.589669585228)\n",
      " state (2)  A[0]:(0.590241670609) A[1]:(0.728977799416) A[2]:(0.590890049934) A[3]:(0.655675411224)\n",
      " state (3)  A[0]:(0.656089663506) A[1]:(-0.216570705175) A[2]:(0.539344131947) A[3]:(0.517595350742)\n",
      " state (4)  A[0]:(0.590422034264) A[1]:(0.656024634838) A[2]:(0.000287771224976) A[3]:(0.531306266785)\n",
      " state (5)  A[0]:(0.162439063191) A[1]:(0.928855240345) A[2]:(-0.193814277649) A[3]:(0.525128483772)\n",
      " state (6)  A[0]:(-0.000295996665955) A[1]:(0.809963464737) A[2]:(7.41481781006e-05) A[3]:(0.656017243862)\n",
      " state (7)  A[0]:(0.62688434124) A[1]:(-0.249252796173) A[2]:(0.302961349487) A[3]:(0.883453011513)\n",
      " state (8)  A[0]:(0.656516432762) A[1]:(0.000297233462334) A[2]:(0.729007959366) A[3]:(0.590981006622)\n",
      " state (9)  A[0]:(0.657080590725) A[1]:(0.81008887291) A[2]:(0.809964418411) A[3]:(0.00159507850185)\n",
      " state (10)  A[0]:(0.729986131191) A[1]:(0.899995803833) A[2]:(-0.000522017420735) A[3]:(0.729676365852)\n",
      " state (11)  A[0]:(0.524455189705) A[1]:(0.876522302628) A[2]:(-0.621193647385) A[3]:(0.845211565495)\n",
      " state (12)  A[0]:(0.0817908495665) A[1]:(0.823558688164) A[2]:(-0.612074553967) A[3]:(0.794856190681)\n",
      " state (13)  A[0]:(0.00213753851131) A[1]:(0.8082010746) A[2]:(0.899993121624) A[3]:(0.729933381081)\n",
      " state (14)  A[0]:(0.810846686363) A[1]:(0.900041759014) A[2]:(0.999999940395) A[3]:(0.810548663139)\n",
      " state (15)  A[0]:(0.983011424541) A[1]:(0.956555783749) A[2]:(1.0) A[3]:(0.87747836113)\n",
      "Episode 780000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 998.               Steps done: 5844610. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00260591995695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532214283943) A[1]:(0.590471386909) A[2]:(0.590510964394) A[3]:(0.531628847122)\n",
      " state (1)  A[0]:(0.532352566719) A[1]:(6.26593828201e-06) A[2]:(0.6561191082) A[3]:(0.590447425842)\n",
      " state (2)  A[0]:(0.591476559639) A[1]:(0.729033470154) A[2]:(0.590836286545) A[3]:(0.656229913235)\n",
      " state (3)  A[0]:(0.656763672829) A[1]:(-0.216503664851) A[2]:(0.53916144371) A[3]:(0.517834365368)\n",
      " state (4)  A[0]:(0.590885102749) A[1]:(0.656227588654) A[2]:(-0.000122547149658) A[3]:(0.531348466873)\n",
      " state (5)  A[0]:(0.162898689508) A[1]:(0.928902983665) A[2]:(-0.194241955876) A[3]:(0.525123476982)\n",
      " state (6)  A[0]:(3.83257865906e-05) A[1]:(0.810028254986) A[2]:(-0.000231981277466) A[3]:(0.655954897404)\n",
      " state (7)  A[0]:(0.626837611198) A[1]:(-0.249233067036) A[2]:(0.303019702435) A[3]:(0.88333594799)\n",
      " state (8)  A[0]:(0.656092047691) A[1]:(9.62093472481e-05) A[2]:(0.72905933857) A[3]:(0.590189874172)\n",
      " state (9)  A[0]:(0.656334757805) A[1]:(0.810003042221) A[2]:(0.810007035732) A[3]:(-0.000460177630885)\n",
      " state (10)  A[0]:(0.729239702225) A[1]:(0.899989962578) A[2]:(-0.000892281299457) A[3]:(0.72875225544)\n",
      " state (11)  A[0]:(0.523037910461) A[1]:(0.876564562321) A[2]:(-0.621720433235) A[3]:(0.844694852829)\n",
      " state (12)  A[0]:(0.0794334709644) A[1]:(0.82368940115) A[2]:(-0.612705469131) A[3]:(0.794180870056)\n",
      " state (13)  A[0]:(-0.000429034203989) A[1]:(0.808453083038) A[2]:(0.899995088577) A[3]:(0.729050517082)\n",
      " state (14)  A[0]:(0.810028791428) A[1]:(0.90026396513) A[2]:(0.999999940395) A[3]:(0.809888899326)\n",
      " state (15)  A[0]:(0.982909679413) A[1]:(0.956673979759) A[2]:(1.0) A[3]:(0.876964747906)\n",
      "Episode 781000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5998. Times reached goal: 997.               Steps done: 5850608. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00259033643077.\n",
      " state (0)  A[0]:(0.531515598297) A[1]:(0.590529859066) A[2]:(0.590560913086) A[3]:(0.531428575516)\n",
      " state (1)  A[0]:(0.531533360481) A[1]:(6.96107745171e-05) A[2]:(0.656079232693) A[3]:(0.590535700321)\n",
      " state (2)  A[0]:(0.590673923492) A[1]:(0.728961229324) A[2]:(0.59099560976) A[3]:(0.656311392784)\n",
      " state (3)  A[0]:(0.656195282936) A[1]:(-0.217878758907) A[2]:(0.539434194565) A[3]:(0.517670154572)\n",
      " state (4)  A[0]:(0.590424180031) A[1]:(0.656153023243) A[2]:(-0.00015664100647) A[3]:(0.531265258789)\n",
      " state (5)  A[0]:(0.162547454238) A[1]:(0.928929448128) A[2]:(-0.194433286786) A[3]:(0.525082230568)\n",
      " state (6)  A[0]:(0.000367522210581) A[1]:(0.810009837151) A[2]:(-0.000285148620605) A[3]:(0.655913710594)\n",
      " state (7)  A[0]:(0.627182066441) A[1]:(-0.24930845201) A[2]:(0.303169101477) A[3]:(0.883246243)\n",
      " state (8)  A[0]:(0.656157612801) A[1]:(0.00019033998251) A[2]:(0.729037106037) A[3]:(0.590137958527)\n",
      " state (9)  A[0]:(0.656153678894) A[1]:(0.810023665428) A[2]:(0.810040593147) A[3]:(-0.000234767794609)\n",
      " state (10)  A[0]:(0.729145884514) A[1]:(0.900010883808) A[2]:(-0.000786900345702) A[3]:(0.728894591331)\n",
      " state (11)  A[0]:(0.523096859455) A[1]:(0.876597464085) A[2]:(-0.621760964394) A[3]:(0.844770491123)\n",
      " state (12)  A[0]:(0.0797431096435) A[1]:(0.823739647865) A[2]:(-0.612841904163) A[3]:(0.794213593006)\n",
      " state (13)  A[0]:(3.01599502563e-05) A[1]:(0.808512091637) A[2]:(0.900024414062) A[3]:(0.72896450758)\n",
      " state (14)  A[0]:(0.810188233852) A[1]:(0.900303304195) A[2]:(0.999999940395) A[3]:(0.809680938721)\n",
      " state (15)  A[0]:(0.982909083366) A[1]:(0.956683337688) A[2]:(1.0) A[3]:(0.876695930958)\n",
      "Episode 782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 999.               Steps done: 5856616. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00257482034648.\n",
      " state (0)  A[0]:(0.531591176987) A[1]:(0.589826703072) A[2]:(0.590431034565) A[3]:(0.531464457512)\n",
      " state (1)  A[0]:(0.531340837479) A[1]:(-0.000427953869803) A[2]:(0.655698060989) A[3]:(0.589838385582)\n",
      " state (2)  A[0]:(0.590466737747) A[1]:(0.728514790535) A[2]:(0.590159535408) A[3]:(0.655561923981)\n",
      " state (3)  A[0]:(0.655929684639) A[1]:(-0.217416584492) A[2]:(0.538888812065) A[3]:(0.517077922821)\n",
      " state (4)  A[0]:(0.590248942375) A[1]:(0.655536413193) A[2]:(-0.000308871269226) A[3]:(0.530616819859)\n",
      " state (5)  A[0]:(0.162701696157) A[1]:(0.928827345371) A[2]:(-0.194644555449) A[3]:(0.524461507797)\n",
      " state (6)  A[0]:(0.000672101858072) A[1]:(0.809868454933) A[2]:(-0.000430703134043) A[3]:(0.655390977859)\n",
      " state (7)  A[0]:(0.627163767815) A[1]:(-0.249480590224) A[2]:(0.303070902824) A[3]:(0.883042633533)\n",
      " state (8)  A[0]:(0.656026422977) A[1]:(-0.00020944327116) A[2]:(0.728565573692) A[3]:(0.590258181095)\n",
      " state (9)  A[0]:(0.655607581139) A[1]:(0.809945225716) A[2]:(0.809868276119) A[3]:(-0.00092549592955)\n",
      " state (10)  A[0]:(0.728630185127) A[1]:(0.900009155273) A[2]:(-0.000927090353798) A[3]:(0.728374958038)\n",
      " state (11)  A[0]:(0.522435545921) A[1]:(0.876600384712) A[2]:(-0.621930360794) A[3]:(0.844547390938)\n",
      " state (12)  A[0]:(0.0790073648095) A[1]:(0.823726177216) A[2]:(-0.613101422787) A[3]:(0.794071435928)\n",
      " state (13)  A[0]:(-0.000602185667958) A[1]:(0.808473169804) A[2]:(0.900017499924) A[3]:(0.729055702686)\n",
      " state (14)  A[0]:(0.809975028038) A[1]:(0.90027230978) A[2]:(0.999999940395) A[3]:(0.81013572216)\n",
      " state (15)  A[0]:(0.982872605324) A[1]:(0.956653118134) A[2]:(1.0) A[3]:(0.877228796482)\n",
      "Episode 783000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 996.               Steps done: 5862624. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00255939720335.\n",
      " state (0)  A[0]:(0.532064914703) A[1]:(0.590545892715) A[2]:(0.59052926302) A[3]:(0.531838297844)\n",
      " state (1)  A[0]:(0.531864523888) A[1]:(0.000235810875893) A[2]:(0.656108617783) A[3]:(0.590751051903)\n",
      " state (2)  A[0]:(0.590874314308) A[1]:(0.729011237621) A[2]:(0.590418100357) A[3]:(0.656577646732)\n",
      " state (3)  A[0]:(0.65582549572) A[1]:(-0.213671460748) A[2]:(0.538704872131) A[3]:(0.518728852272)\n",
      " state (4)  A[0]:(0.589909434319) A[1]:(0.656116485596) A[2]:(-4.82797622681e-05) A[3]:(0.532046556473)\n",
      " state (5)  A[0]:(0.162435695529) A[1]:(0.928929805756) A[2]:(-0.194355860353) A[3]:(0.525900483131)\n",
      " state (6)  A[0]:(0.000509381236043) A[1]:(0.810009777546) A[2]:(-2.26497650146e-06) A[3]:(0.656403064728)\n",
      " state (7)  A[0]:(0.626682877541) A[1]:(-0.249269515276) A[2]:(0.303690463305) A[3]:(0.88325470686)\n",
      " state (8)  A[0]:(0.655439376831) A[1]:(0.000227883458138) A[2]:(0.729027032852) A[3]:(0.590420603752)\n",
      " state (9)  A[0]:(0.6553170681) A[1]:(0.810016214848) A[2]:(0.810013711452) A[3]:(7.23451375961e-05)\n",
      " state (10)  A[0]:(0.728581666946) A[1]:(0.900015175343) A[2]:(-0.000649332883768) A[3]:(0.728938937187)\n",
      " state (11)  A[0]:(0.522583961487) A[1]:(0.876609444618) A[2]:(-0.62172973156) A[3]:(0.84481048584)\n",
      " state (12)  A[0]:(0.0793889313936) A[1]:(0.823751807213) A[2]:(-0.612980127335) A[3]:(0.794270634651)\n",
      " state (13)  A[0]:(-0.000364422769053) A[1]:(0.808499276638) A[2]:(0.900007367134) A[3]:(0.729065895081)\n",
      " state (14)  A[0]:(0.80987071991) A[1]:(0.900278866291) A[2]:(0.999999940395) A[3]:(0.809911429882)\n",
      " state (15)  A[0]:(0.982837975025) A[1]:(0.956649780273) A[2]:(1.0) A[3]:(0.876906037331)\n",
      "Episode 784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 998.               Steps done: 5868625. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0025440842532.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5902,  0.5904,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.0000,  0.6561,  0.5901]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7292,  0.5905,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8099, -0.0000,  0.6555]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7288,  0.9000, -0.0006,  0.7285]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9004,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531555056572) A[1]:(0.590350389481) A[2]:(0.590402841568) A[3]:(0.531446099281)\n",
      " state (1)  A[0]:(0.53141450882) A[1]:(4.16412949562e-05) A[2]:(0.656051516533) A[3]:(0.590249657631)\n",
      " state (2)  A[0]:(0.590533018112) A[1]:(0.729062974453) A[2]:(0.590271830559) A[3]:(0.656095266342)\n",
      " state (3)  A[0]:(0.655989646912) A[1]:(-0.212824866176) A[2]:(0.538388133049) A[3]:(0.517903506756)\n",
      " state (4)  A[0]:(0.590088248253) A[1]:(0.655993998051) A[2]:(-0.000336050987244) A[3]:(0.531010508537)\n",
      " state (5)  A[0]:(0.162263333797) A[1]:(0.928869962692) A[2]:(-0.194718196988) A[3]:(0.524849653244)\n",
      " state (6)  A[0]:(-2.46167182922e-05) A[1]:(0.810000777245) A[2]:(-0.000707387807779) A[3]:(0.65558719635)\n",
      " state (7)  A[0]:(0.626866817474) A[1]:(-0.249130517244) A[2]:(0.302733868361) A[3]:(0.883061528206)\n",
      " state (8)  A[0]:(0.656120538712) A[1]:(9.22456383705e-05) A[2]:(0.728321433067) A[3]:(0.590495288372)\n",
      " state (9)  A[0]:(0.655898571014) A[1]:(0.809963345528) A[2]:(0.809568405151) A[3]:(5.73396682739e-05)\n",
      " state (10)  A[0]:(0.728718578815) A[1]:(0.899950921535) A[2]:(-0.00158059468959) A[3]:(0.728690385818)\n",
      " state (11)  A[0]:(0.522407531738) A[1]:(0.876493096352) A[2]:(-0.622317314148) A[3]:(0.844602704048)\n",
      " state (12)  A[0]:(0.078851878643) A[1]:(0.823565721512) A[2]:(-0.613649249077) A[3]:(0.794018745422)\n",
      " state (13)  A[0]:(-0.000906526809558) A[1]:(0.808309674263) A[2]:(0.899926662445) A[3]:(0.728849291801)\n",
      " state (14)  A[0]:(0.809816241264) A[1]:(0.90020275116) A[2]:(0.999999940395) A[3]:(0.809896349907)\n",
      " state (15)  A[0]:(0.982827425003) A[1]:(0.956616640091) A[2]:(1.0) A[3]:(0.876955330372)\n",
      "Episode 785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6015. Times reached goal: 996.               Steps done: 5874640. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00252882751705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530614435673) A[1]:(0.590140640736) A[2]:(0.590319991112) A[3]:(0.531168758869)\n",
      " state (1)  A[0]:(0.53069460392) A[1]:(2.87517905235e-05) A[2]:(0.656089127064) A[3]:(0.590106606483)\n",
      " state (2)  A[0]:(0.59013402462) A[1]:(0.729007124901) A[2]:(0.590549826622) A[3]:(0.655997037888)\n",
      " state (3)  A[0]:(0.656493067741) A[1]:(-0.214466363192) A[2]:(0.538864612579) A[3]:(0.517859816551)\n",
      " state (4)  A[0]:(0.591180980206) A[1]:(0.656188011169) A[2]:(-4.49419021606e-05) A[3]:(0.531295061111)\n",
      " state (5)  A[0]:(0.164044305682) A[1]:(0.928906440735) A[2]:(-0.194324091077) A[3]:(0.525249242783)\n",
      " state (6)  A[0]:(0.00186896102969) A[1]:(0.809998273849) A[2]:(-1.70469284058e-05) A[3]:(0.656021475792)\n",
      " state (7)  A[0]:(0.627807736397) A[1]:(-0.24927982688) A[2]:(0.303851276636) A[3]:(0.883228182793)\n",
      " state (8)  A[0]:(0.656809091568) A[1]:(8.62702727318e-05) A[2]:(0.729074776173) A[3]:(0.590635418892)\n",
      " state (9)  A[0]:(0.656672120094) A[1]:(0.81000328064) A[2]:(0.810039639473) A[3]:(0.000381752819521)\n",
      " state (10)  A[0]:(0.729628741741) A[1]:(0.899999260902) A[2]:(-0.000735640409403) A[3]:(0.729138493538)\n",
      " state (11)  A[0]:(0.524142980576) A[1]:(0.876571953297) A[2]:(-0.622025489807) A[3]:(0.844975948334)\n",
      " state (12)  A[0]:(0.0814186111093) A[1]:(0.823679983616) A[2]:(-0.613546013832) A[3]:(0.794531285763)\n",
      " state (13)  A[0]:(0.00156605115626) A[1]:(0.80842000246) A[2]:(0.899927675724) A[3]:(0.729474544525)\n",
      " state (14)  A[0]:(0.810515284538) A[1]:(0.900258123875) A[2]:(0.999999940395) A[3]:(0.810324430466)\n",
      " state (15)  A[0]:(0.98287332058) A[1]:(0.956633388996) A[2]:(1.0) A[3]:(0.877199709415)\n",
      "Episode 786000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6007. Times reached goal: 997.               Steps done: 5880647. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0025136823841.\n",
      " state (0)  A[0]:(0.531821489334) A[1]:(0.590324044228) A[2]:(0.590454936028) A[3]:(0.53112578392)\n",
      " state (1)  A[0]:(0.531080365181) A[1]:(-6.3918530941e-05) A[2]:(0.656053900719) A[3]:(0.590283930302)\n",
      " state (2)  A[0]:(0.589869022369) A[1]:(0.728914439678) A[2]:(0.590671062469) A[3]:(0.656125426292)\n",
      " state (3)  A[0]:(0.655062913895) A[1]:(-0.215825632215) A[2]:(0.539190471172) A[3]:(0.517635703087)\n",
      " state (4)  A[0]:(0.588709592819) A[1]:(0.656018555164) A[2]:(0.000163316726685) A[3]:(0.531007826328)\n",
      " state (5)  A[0]:(0.159799486399) A[1]:(0.928870201111) A[2]:(-0.194211557508) A[3]:(0.524878680706)\n",
      " state (6)  A[0]:(-0.00246792519465) A[1]:(0.809957027435) A[2]:(-2.58684158325e-05) A[3]:(0.65562081337)\n",
      " state (7)  A[0]:(0.625733017921) A[1]:(-0.249408811331) A[2]:(0.303768604994) A[3]:(0.883055865765)\n",
      " state (8)  A[0]:(0.655295491219) A[1]:(-0.000255636870861) A[2]:(0.728875756264) A[3]:(0.590440869331)\n",
      " state (9)  A[0]:(0.655261039734) A[1]:(0.809923887253) A[2]:(0.809940516949) A[3]:(-4.9352645874e-05)\n",
      " state (10)  A[0]:(0.728399693966) A[1]:(0.899999916553) A[2]:(-0.00114166690037) A[3]:(0.728874802589)\n",
      " state (11)  A[0]:(0.522053003311) A[1]:(0.876627504826) A[2]:(-0.622466683388) A[3]:(0.844789266586)\n",
      " state (12)  A[0]:(0.0783301219344) A[1]:(0.823850393295) A[2]:(-0.614042043686) A[3]:(0.794233083725)\n",
      " state (13)  A[0]:(-0.00139862205833) A[1]:(0.808749675751) A[2]:(0.900026082993) A[3]:(0.729034304619)\n",
      " state (14)  A[0]:(0.809757232666) A[1]:(0.900544643402) A[2]:(0.999999940395) A[3]:(0.809956252575)\n",
      " state (15)  A[0]:(0.982789516449) A[1]:(0.95678627491) A[2]:(1.0) A[3]:(0.876847505569)\n",
      "Episode 787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 998.               Steps done: 5886651. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00249863545116.\n",
      " state (0)  A[0]:(0.534121513367) A[1]:(0.590463638306) A[2]:(0.590477466583) A[3]:(0.531510412693)\n",
      " state (1)  A[0]:(0.532893776894) A[1]:(-0.000291123986244) A[2]:(0.656053721905) A[3]:(0.59061217308)\n",
      " state (2)  A[0]:(0.591376304626) A[1]:(0.728956580162) A[2]:(0.590827703476) A[3]:(0.656529128551)\n",
      " state (3)  A[0]:(0.656323313713) A[1]:(-0.215808853507) A[2]:(0.539435684681) A[3]:(0.518603026867)\n",
      " state (4)  A[0]:(0.590100049973) A[1]:(0.655996620655) A[2]:(0.000611782015767) A[3]:(0.532308161259)\n",
      " state (5)  A[0]:(0.161623373628) A[1]:(0.928865969181) A[2]:(-0.193713530898) A[3]:(0.526438176632)\n",
      " state (6)  A[0]:(-0.0014730085386) A[1]:(0.809975862503) A[2]:(0.000559091509786) A[3]:(0.656900763512)\n",
      " state (7)  A[0]:(0.625438570976) A[1]:(-0.249245032668) A[2]:(0.304360151291) A[3]:(0.883505702019)\n",
      " state (8)  A[0]:(0.654789209366) A[1]:(-1.81645154953e-05) A[2]:(0.729092240334) A[3]:(0.591618180275)\n",
      " state (9)  A[0]:(0.654722213745) A[1]:(0.809980213642) A[2]:(0.810060143471) A[3]:(0.00124201120343)\n",
      " state (10)  A[0]:(0.727849125862) A[1]:(0.899978637695) A[2]:(-0.00065934646409) A[3]:(0.729334115982)\n",
      " state (11)  A[0]:(0.521077394485) A[1]:(0.876530468464) A[2]:(-0.622195601463) A[3]:(0.845062434673)\n",
      " state (12)  A[0]:(0.0768607705832) A[1]:(0.82360881567) A[2]:(-0.613949418068) A[3]:(0.794626653194)\n",
      " state (13)  A[0]:(-0.00310020637698) A[1]:(0.808354556561) A[2]:(0.899977087975) A[3]:(0.729612350464)\n",
      " state (14)  A[0]:(0.809097766876) A[1]:(0.900252223015) A[2]:(0.999999940395) A[3]:(0.810480415821)\n",
      " state (15)  A[0]:(0.982717216015) A[1]:(0.956619620323) A[2]:(1.0) A[3]:(0.877268671989)\n",
      "Episode 788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 998.               Steps done: 5892660. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00248366617098.\n",
      " state (0)  A[0]:(0.531853020191) A[1]:(0.590246319771) A[2]:(0.590461730957) A[3]:(0.531694412231)\n",
      " state (1)  A[0]:(0.531830132008) A[1]:(-5.95301389694e-05) A[2]:(0.655948877335) A[3]:(0.59034204483)\n",
      " state (2)  A[0]:(0.591065168381) A[1]:(0.72886133194) A[2]:(0.59049642086) A[3]:(0.656136274338)\n",
      " state (3)  A[0]:(0.656669557095) A[1]:(-0.216748341918) A[2]:(0.539152979851) A[3]:(0.517692446709)\n",
      " state (4)  A[0]:(0.590864539146) A[1]:(0.655906319618) A[2]:(8.84532928467e-05) A[3]:(0.531306266785)\n",
      " state (5)  A[0]:(0.162843868136) A[1]:(0.928836226463) A[2]:(-0.194194465876) A[3]:(0.52532595396)\n",
      " state (6)  A[0]:(4.34517860413e-05) A[1]:(0.809935808182) A[2]:(5.10215759277e-05) A[3]:(0.655927479267)\n",
      " state (7)  A[0]:(0.626804888248) A[1]:(-0.249260857701) A[2]:(0.303963780403) A[3]:(0.883094251156)\n",
      " state (8)  A[0]:(0.656076073647) A[1]:(-0.000129118561745) A[2]:(0.72883194685) A[3]:(0.590302705765)\n",
      " state (9)  A[0]:(0.655717968941) A[1]:(0.809996843338) A[2]:(0.8099219203) A[3]:(-0.00142049696296)\n",
      " state (10)  A[0]:(0.728537619114) A[1]:(0.900038182735) A[2]:(-0.00136744894553) A[3]:(0.72799706459)\n",
      " state (11)  A[0]:(0.521988630295) A[1]:(0.876664698124) A[2]:(-0.622883498669) A[3]:(0.84421312809)\n",
      " state (12)  A[0]:(0.0780043005943) A[1]:(0.823902547359) A[2]:(-0.614678621292) A[3]:(0.793470025063)\n",
      " state (13)  A[0]:(-0.0015269506257) A[1]:(0.80884718895) A[2]:(0.900047719479) A[3]:(0.728147268295)\n",
      " state (14)  A[0]:(0.810049474239) A[1]:(0.900652468204) A[2]:(0.999999940395) A[3]:(0.809507966042)\n",
      " state (15)  A[0]:(0.982815861702) A[1]:(0.956833600998) A[2]:(1.0) A[3]:(0.876616537571)\n",
      "Episode 789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 997.               Steps done: 5898661. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00246880632186.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5902,  0.5903,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5309, -0.0001,  0.6560,  0.5901]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.7289,  0.5903,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0011,  0.8099, -0.0004,  0.6558]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7287,  0.9000, -0.0009,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9007,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531453609467) A[1]:(0.590135335922) A[2]:(0.590230882168) A[3]:(0.531380534172)\n",
      " state (1)  A[0]:(0.531164824963) A[1]:(-0.000305019319057) A[2]:(0.655846595764) A[3]:(0.590211987495)\n",
      " state (2)  A[0]:(0.590409755707) A[1]:(0.728738307953) A[2]:(0.590180397034) A[3]:(0.656144678593)\n",
      " state (3)  A[0]:(0.656065881252) A[1]:(-0.215535625815) A[2]:(0.53860604763) A[3]:(0.517919063568)\n",
      " state (4)  A[0]:(0.590242266655) A[1]:(0.655430674553) A[2]:(-0.000233888626099) A[3]:(0.53143876791)\n",
      " state (5)  A[0]:(0.162143841386) A[1]:(0.928725659847) A[2]:(-0.194477796555) A[3]:(0.525525569916)\n",
      " state (6)  A[0]:(-0.000851034885272) A[1]:(0.809773623943) A[2]:(-0.000281453132629) A[3]:(0.655899047852)\n",
      " state (7)  A[0]:(0.626139998436) A[1]:(-0.249623730779) A[2]:(0.303610175848) A[3]:(0.882983922958)\n",
      " state (8)  A[0]:(0.655848622322) A[1]:(-0.000707618775778) A[2]:(0.728501617908) A[3]:(0.590804219246)\n",
      " state (9)  A[0]:(0.655796587467) A[1]:(0.809793949127) A[2]:(0.809714436531) A[3]:(0.000536575855222)\n",
      " state (10)  A[0]:(0.728623211384) A[1]:(0.899929881096) A[2]:(-0.00156581273768) A[3]:(0.729000806808)\n",
      " state (11)  A[0]:(0.522072970867) A[1]:(0.876539707184) A[2]:(-0.622936725616) A[3]:(0.844818949699)\n",
      " state (12)  A[0]:(0.077984392643) A[1]:(0.823738574982) A[2]:(-0.614838838577) A[3]:(0.794224619865)\n",
      " state (13)  A[0]:(-0.00186770933215) A[1]:(0.808666884899) A[2]:(0.899938642979) A[3]:(0.729010820389)\n",
      " state (14)  A[0]:(0.809766173363) A[1]:(0.900548756123) A[2]:(0.999999940395) A[3]:(0.809998333454)\n",
      " state (15)  A[0]:(0.9827709198) A[1]:(0.956784427166) A[2]:(1.0) A[3]:(0.876844763756)\n",
      "Episode 790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 999.               Steps done: 5904671. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00245401329344.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530962944031) A[1]:(0.590378046036) A[2]:(0.590463340282) A[3]:(0.531446576118)\n",
      " state (1)  A[0]:(0.531074404716) A[1]:(-1.85519456863e-05) A[2]:(0.656091690063) A[3]:(0.590308129787)\n",
      " state (2)  A[0]:(0.590453743935) A[1]:(0.728933215141) A[2]:(0.590719759464) A[3]:(0.656105160713)\n",
      " state (3)  A[0]:(0.656055748463) A[1]:(-0.216440752149) A[2]:(0.539169251919) A[3]:(0.517773091793)\n",
      " state (4)  A[0]:(0.590359568596) A[1]:(0.656054496765) A[2]:(-8.225440979e-05) A[3]:(0.53137421608)\n",
      " state (5)  A[0]:(0.162574678659) A[1]:(0.9288803339) A[2]:(-0.194433748722) A[3]:(0.525390625)\n",
      " state (6)  A[0]:(-2.52723693848e-05) A[1]:(0.809997260571) A[2]:(1.83582305908e-05) A[3]:(0.655859827995)\n",
      " state (7)  A[0]:(0.626559972763) A[1]:(-0.249177888036) A[2]:(0.304384171963) A[3]:(0.882957339287)\n",
      " state (8)  A[0]:(0.656070351601) A[1]:(0.000161424279213) A[2]:(0.729076981544) A[3]:(0.590329945087)\n",
      " state (9)  A[0]:(0.656178474426) A[1]:(0.810056567192) A[2]:(0.810047030449) A[3]:(5.94407320023e-05)\n",
      " state (10)  A[0]:(0.729030549526) A[1]:(0.900022029877) A[2]:(-0.000675797346048) A[3]:(0.728791236877)\n",
      " state (11)  A[0]:(0.522774815559) A[1]:(0.87659996748) A[2]:(-0.622425436974) A[3]:(0.844650745392)\n",
      " state (12)  A[0]:(0.0790064781904) A[1]:(0.823748767376) A[2]:(-0.614460587502) A[3]:(0.793980836868)\n",
      " state (13)  A[0]:(-0.000780284230132) A[1]:(0.80858361721) A[2]:(0.900039792061) A[3]:(0.72870194912)\n",
      " state (14)  A[0]:(0.810233712196) A[1]:(0.900447249413) A[2]:(0.999999940395) A[3]:(0.80983453989)\n",
      " state (15)  A[0]:(0.982817769051) A[1]:(0.956706106663) A[2]:(1.0) A[3]:(0.876760423183)\n",
      "Episode 791000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6009. Times reached goal: 999.               Steps done: 5910680. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00243931134381.\n",
      " state (0)  A[0]:(0.531695902348) A[1]:(0.590222358704) A[2]:(0.590216219425) A[3]:(0.530608296394)\n",
      " state (1)  A[0]:(0.531586408615) A[1]:(-2.07722187042e-05) A[2]:(0.655884981155) A[3]:(0.58972299099)\n",
      " state (2)  A[0]:(0.590921163559) A[1]:(0.728718400002) A[2]:(0.590157568455) A[3]:(0.655789017677)\n",
      " state (3)  A[0]:(0.65657967329) A[1]:(-0.215757235885) A[2]:(0.538787126541) A[3]:(0.517539978027)\n",
      " state (4)  A[0]:(0.591010570526) A[1]:(0.655772805214) A[2]:(-0.00012195110321) A[3]:(0.531221568584)\n",
      " state (5)  A[0]:(0.16360655427) A[1]:(0.928811311722) A[2]:(-0.19443821907) A[3]:(0.525437176228)\n",
      " state (6)  A[0]:(0.0010073777521) A[1]:(0.809895575047) A[2]:(7.15255737305e-06) A[3]:(0.655911207199)\n",
      " state (7)  A[0]:(0.627232432365) A[1]:(-0.249453678727) A[2]:(0.304374665022) A[3]:(0.882962405682)\n",
      " state (8)  A[0]:(0.656600773335) A[1]:(-0.000432997912867) A[2]:(0.72883027792) A[3]:(0.590892851353)\n",
      " state (9)  A[0]:(0.656309008598) A[1]:(0.809905648232) A[2]:(0.809881687164) A[3]:(0.00101640785579)\n",
      " state (10)  A[0]:(0.729018330574) A[1]:(0.899996936321) A[2]:(-0.001249312656) A[3]:(0.729354560375)\n",
      " state (11)  A[0]:(0.522674381733) A[1]:(0.876616597176) A[2]:(-0.622941076756) A[3]:(0.845074832439)\n",
      " state (12)  A[0]:(0.0787324607372) A[1]:(0.823829472065) A[2]:(-0.615050315857) A[3]:(0.794561684132)\n",
      " state (13)  A[0]:(-0.00108617497608) A[1]:(0.808747708797) A[2]:(0.900013267994) A[3]:(0.729449272156)\n",
      " state (14)  A[0]:(0.810181498528) A[1]:(0.900590538979) A[2]:(0.999999940395) A[3]:(0.810374379158)\n",
      " state (15)  A[0]:(0.982794046402) A[1]:(0.95677793026) A[2]:(1.0) A[3]:(0.8770622015)\n",
      "Episode 792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 999.               Steps done: 5916688. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00242469989799.\n",
      " state (0)  A[0]:(0.531762719154) A[1]:(0.590489327908) A[2]:(0.590592503548) A[3]:(0.53164434433)\n",
      " state (1)  A[0]:(0.53167206049) A[1]:(-6.63325190544e-05) A[2]:(0.656178712845) A[3]:(0.59063577652)\n",
      " state (2)  A[0]:(0.590910553932) A[1]:(0.728975236416) A[2]:(0.590561509132) A[3]:(0.656504690647)\n",
      " state (3)  A[0]:(0.656219244003) A[1]:(-0.214449450374) A[2]:(0.538951098919) A[3]:(0.518202662468)\n",
      " state (4)  A[0]:(0.59035718441) A[1]:(0.656157135963) A[2]:(6.97374343872e-05) A[3]:(0.531596839428)\n",
      " state (5)  A[0]:(0.162522912025) A[1]:(0.928874969482) A[2]:(-0.194327190518) A[3]:(0.525738418102)\n",
      " state (6)  A[0]:(-1.89542770386e-05) A[1]:(0.809962809086) A[2]:(0.000180244445801) A[3]:(0.656185984612)\n",
      " state (7)  A[0]:(0.626644015312) A[1]:(-0.249275267124) A[2]:(0.304706037045) A[3]:(0.883066296577)\n",
      " state (8)  A[0]:(0.656274795532) A[1]:(4.47779893875e-06) A[2]:(0.729012966156) A[3]:(0.590819358826)\n",
      " state (9)  A[0]:(0.656382322311) A[1]:(0.810018181801) A[2]:(0.809958040714) A[3]:(0.00040832158993)\n",
      " state (10)  A[0]:(0.729097783566) A[1]:(0.89999204874) A[2]:(-0.000989317544736) A[3]:(0.728895425797)\n",
      " state (11)  A[0]:(0.522675156593) A[1]:(0.876528918743) A[2]:(-0.622817277908) A[3]:(0.844753801823)\n",
      " state (12)  A[0]:(0.078586563468) A[1]:(0.823595523834) A[2]:(-0.615032851696) A[3]:(0.794166326523)\n",
      " state (13)  A[0]:(-0.00131004978903) A[1]:(0.808372616768) A[2]:(0.90006840229) A[3]:(0.728991746902)\n",
      " state (14)  A[0]:(0.810128092766) A[1]:(0.900320887566) A[2]:(0.999999940395) A[3]:(0.810046315193)\n",
      " state (15)  A[0]:(0.98277848959) A[1]:(0.956617236137) A[2]:(1.0) A[3]:(0.87682056427)\n",
      "Episode 793000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5998. Times reached goal: 995.               Steps done: 5922686. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00241020007644.\n",
      " state (0)  A[0]:(0.531932234764) A[1]:(0.590394496918) A[2]:(0.590480446815) A[3]:(0.531580150127)\n",
      " state (1)  A[0]:(0.530698776245) A[1]:(-0.000477999419672) A[2]:(0.656053483486) A[3]:(0.590793073177)\n",
      " state (2)  A[0]:(0.589433312416) A[1]:(0.728960573673) A[2]:(0.591096818447) A[3]:(0.656688630581)\n",
      " state (3)  A[0]:(0.65441262722) A[1]:(-0.215972155333) A[2]:(0.539916038513) A[3]:(0.518647432327)\n",
      " state (4)  A[0]:(0.588326454163) A[1]:(0.656025648117) A[2]:(0.000812291924376) A[3]:(0.532450675964)\n",
      " state (5)  A[0]:(0.159942999482) A[1]:(0.928947985172) A[2]:(-0.194250896573) A[3]:(0.52699637413)\n",
      " state (6)  A[0]:(-0.00217663892545) A[1]:(0.810015976429) A[2]:(0.000133991241455) A[3]:(0.657493829727)\n",
      " state (7)  A[0]:(0.62485986948) A[1]:(-0.249313980341) A[2]:(0.304724216461) A[3]:(0.883584141731)\n",
      " state (8)  A[0]:(0.653857350349) A[1]:(-5.96344470978e-05) A[2]:(0.728882849216) A[3]:(0.592741966248)\n",
      " state (9)  A[0]:(0.653613448143) A[1]:(0.809919416904) A[2]:(0.809871017933) A[3]:(0.00428115203977)\n",
      " state (10)  A[0]:(0.726777136326) A[1]:(0.899949669838) A[2]:(-0.0017417651834) A[3]:(0.731270074844)\n",
      " state (11)  A[0]:(0.518983960152) A[1]:(0.876487970352) A[2]:(-0.6235922575) A[3]:(0.846414804459)\n",
      " state (12)  A[0]:(0.0732907801867) A[1]:(0.823554873466) A[2]:(-0.61593413353) A[3]:(0.796391844749)\n",
      " state (13)  A[0]:(-0.00681172218174) A[1]:(0.808374047279) A[2]:(0.899975597858) A[3]:(0.731774389744)\n",
      " state (14)  A[0]:(0.808229386806) A[1]:(0.900364935398) A[2]:(0.999999940395) A[3]:(0.811918258667)\n",
      " state (15)  A[0]:(0.982559919357) A[1]:(0.956637680531) A[2]:(1.0) A[3]:(0.877883195877)\n",
      "Episode 794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 1000.               Steps done: 5928697. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00239575581948.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5903,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5319, -0.0001,  0.6561,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5913,  0.7290,  0.5904,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0012,  0.8101, -0.0000,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0012,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.9004,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531767487526) A[1]:(0.590365827084) A[2]:(0.590536594391) A[3]:(0.531197667122)\n",
      " state (1)  A[0]:(0.531969130039) A[1]:(-3.56882810593e-06) A[2]:(0.656137347221) A[3]:(0.590202093124)\n",
      " state (2)  A[0]:(0.591288864613) A[1]:(0.729069054127) A[2]:(0.590458393097) A[3]:(0.656068503857)\n",
      " state (3)  A[0]:(0.656823158264) A[1]:(-0.213730663061) A[2]:(0.53884524107) A[3]:(0.517772614956)\n",
      " state (4)  A[0]:(0.591129541397) A[1]:(0.656031131744) A[2]:(0.000103712081909) A[3]:(0.531120240688)\n",
      " state (5)  A[0]:(0.163737297058) A[1]:(0.928859233856) A[2]:(-0.194413095713) A[3]:(0.525347471237)\n",
      " state (6)  A[0]:(0.00128555228002) A[1]:(0.810039997101) A[2]:(7.06911087036e-05) A[3]:(0.655895352364)\n",
      " state (7)  A[0]:(0.627515435219) A[1]:(-0.248890995979) A[2]:(0.304846733809) A[3]:(0.88294249773)\n",
      " state (8)  A[0]:(0.65685904026) A[1]:(3.0554831028e-05) A[2]:(0.729025304317) A[3]:(0.590770900249)\n",
      " state (9)  A[0]:(0.656417012215) A[1]:(0.810021162033) A[2]:(0.810072779655) A[3]:(0.000156089663506)\n",
      " state (10)  A[0]:(0.729057192802) A[1]:(0.90003401041) A[2]:(-0.00101017917041) A[3]:(0.728979766369)\n",
      " state (11)  A[0]:(0.522628903389) A[1]:(0.876604139805) A[2]:(-0.623211860657) A[3]:(0.844907045364)\n",
      " state (12)  A[0]:(0.0784303545952) A[1]:(0.823714017868) A[2]:(-0.615702271461) A[3]:(0.794352531433)\n",
      " state (13)  A[0]:(-0.00139957573265) A[1]:(0.808525502682) A[2]:(0.900036931038) A[3]:(0.729097366333)\n",
      " state (14)  A[0]:(0.81031948328) A[1]:(0.90043669939) A[2]:(0.999999940395) A[3]:(0.809931755066)\n",
      " state (15)  A[0]:(0.982782363892) A[1]:(0.956657409668) A[2]:(1.0) A[3]:(0.876524686813)\n",
      "Episode 795000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6002. Times reached goal: 996.               Steps done: 5934699. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00238141955921.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532206773758) A[1]:(0.59068775177) A[2]:(0.590593457222) A[3]:(0.53173917532)\n",
      " state (1)  A[0]:(0.532066345215) A[1]:(-3.72529029846e-08) A[2]:(0.65623909235) A[3]:(0.590584278107)\n",
      " state (2)  A[0]:(0.591288864613) A[1]:(0.729042887688) A[2]:(0.590651452541) A[3]:(0.656276106834)\n",
      " state (3)  A[0]:(0.657194018364) A[1]:(-0.214462384582) A[2]:(0.539100944996) A[3]:(0.517788350582)\n",
      " state (4)  A[0]:(0.591533899307) A[1]:(0.655920505524) A[2]:(0.000250339508057) A[3]:(0.530947685242)\n",
      " state (5)  A[0]:(0.163664445281) A[1]:(0.928809463978) A[2]:(-0.194357693195) A[3]:(0.524923622608)\n",
      " state (6)  A[0]:(-0.000133991241455) A[1]:(0.810020625591) A[2]:(-8.15391540527e-05) A[3]:(0.655070245266)\n",
      " state (7)  A[0]:(0.625993251801) A[1]:(-0.249032393098) A[2]:(0.304680854082) A[3]:(0.882352769375)\n",
      " state (8)  A[0]:(0.655499219894) A[1]:(-0.000608183385339) A[2]:(0.728864848614) A[3]:(0.588791251183)\n",
      " state (9)  A[0]:(0.65513497591) A[1]:(0.809855341911) A[2]:(0.809958457947) A[3]:(-0.00335471564904)\n",
      " state (10)  A[0]:(0.727944374084) A[1]:(0.900010943413) A[2]:(-0.00161754945293) A[3]:(0.727452993393)\n",
      " state (11)  A[0]:(0.520661592484) A[1]:(0.876660943031) A[2]:(-0.623820900917) A[3]:(0.84405207634)\n",
      " state (12)  A[0]:(0.0753677859902) A[1]:(0.823930442333) A[2]:(-0.616430461407) A[3]:(0.793294250965)\n",
      " state (13)  A[0]:(-0.00466027483344) A[1]:(0.808954536915) A[2]:(0.899984657764) A[3]:(0.727848887444)\n",
      " state (14)  A[0]:(0.809239089489) A[1]:(0.900808811188) A[2]:(0.999999940395) A[3]:(0.809182703495)\n",
      " state (15)  A[0]:(0.982653558254) A[1]:(0.956871092319) A[2]:(1.0) A[3]:(0.876076996326)\n",
      "Episode 796000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6003. Times reached goal: 999.               Steps done: 5940702. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00236716672029.\n",
      " state (0)  A[0]:(0.530682325363) A[1]:(0.5905585289) A[2]:(0.590575098991) A[3]:(0.531324267387)\n",
      " state (1)  A[0]:(0.531157910824) A[1]:(1.27926468849e-05) A[2]:(0.656119942665) A[3]:(0.590737700462)\n",
      " state (2)  A[0]:(0.59071457386) A[1]:(0.72906088829) A[2]:(0.590490579605) A[3]:(0.656615376472)\n",
      " state (3)  A[0]:(0.656863331795) A[1]:(-0.215711012483) A[2]:(0.538963139057) A[3]:(0.518649339676)\n",
      " state (4)  A[0]:(0.591079354286) A[1]:(0.656052589417) A[2]:(-1.07288360596e-06) A[3]:(0.532114028931)\n",
      " state (5)  A[0]:(0.16264373064) A[1]:(0.928778767586) A[2]:(-0.194143414497) A[3]:(0.526016533375)\n",
      " state (6)  A[0]:(-0.00141256954521) A[1]:(0.810050785542) A[2]:(0.00030505657196) A[3]:(0.65584307909)\n",
      " state (7)  A[0]:(0.625840485096) A[1]:(-0.248817563057) A[2]:(0.30514395237) A[3]:(0.882809400558)\n",
      " state (8)  A[0]:(0.656379103661) A[1]:(-0.000488392950501) A[2]:(0.72901982069) A[3]:(0.591629862785)\n",
      " state (9)  A[0]:(0.656556487083) A[1]:(0.809901297092) A[2]:(0.810068666935) A[3]:(0.00314602698199)\n",
      " state (10)  A[0]:(0.729102134705) A[1]:(0.900000572205) A[2]:(-0.00064086902421) A[3]:(0.730432450771)\n",
      " state (11)  A[0]:(0.52239716053) A[1]:(0.876616179943) A[2]:(-0.623064517975) A[3]:(0.845693826675)\n",
      " state (12)  A[0]:(0.0777022764087) A[1]:(0.823828816414) A[2]:(-0.61586278677) A[3]:(0.795219063759)\n",
      " state (13)  A[0]:(-0.00248568737879) A[1]:(0.808780193329) A[2]:(0.899972081184) A[3]:(0.729950726032)\n",
      " state (14)  A[0]:(0.809957325459) A[1]:(0.900677204132) A[2]:(0.999999940395) A[3]:(0.810276627541)\n",
      " state (15)  A[0]:(0.982734024525) A[1]:(0.956803560257) A[2]:(1.0) A[3]:(0.876540362835)\n",
      "Episode 797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 999.               Steps done: 5946711. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00235298506692.\n",
      " state (0)  A[0]:(0.531003594398) A[1]:(0.590284347534) A[2]:(0.590327501297) A[3]:(0.532518386841)\n",
      " state (1)  A[0]:(0.531403541565) A[1]:(-0.000487282843096) A[2]:(0.655924797058) A[3]:(0.591170072556)\n",
      " state (2)  A[0]:(0.590865850449) A[1]:(0.729014754295) A[2]:(0.590686500072) A[3]:(0.656763672829)\n",
      " state (3)  A[0]:(0.656215310097) A[1]:(-0.215261220932) A[2]:(0.539327859879) A[3]:(0.518710017204)\n",
      " state (4)  A[0]:(0.590507388115) A[1]:(0.656136393547) A[2]:(0.000251650810242) A[3]:(0.532178044319)\n",
      " state (5)  A[0]:(0.162891030312) A[1]:(0.928905844688) A[2]:(-0.194499462843) A[3]:(0.526381731033)\n",
      " state (6)  A[0]:(0.000154972076416) A[1]:(0.81003266573) A[2]:(0.000174045562744) A[3]:(0.656476259232)\n",
      " state (7)  A[0]:(0.626166820526) A[1]:(-0.248890697956) A[2]:(0.305371165276) A[3]:(0.882920563221)\n",
      " state (8)  A[0]:(0.655874252319) A[1]:(0.000455424160464) A[2]:(0.729124903679) A[3]:(0.59078001976)\n",
      " state (9)  A[0]:(0.656195282936) A[1]:(0.810101747513) A[2]:(0.810103774071) A[3]:(0.000640436890535)\n",
      " state (10)  A[0]:(0.728872776031) A[1]:(0.900011241436) A[2]:(-0.000453591317637) A[3]:(0.729020237923)\n",
      " state (11)  A[0]:(0.522055864334) A[1]:(0.876534879208) A[2]:(-0.622936606407) A[3]:(0.844826042652)\n",
      " state (12)  A[0]:(0.077278368175) A[1]:(0.823599398136) A[2]:(-0.61579567194) A[3]:(0.794204056263)\n",
      " state (13)  A[0]:(-0.00276374118403) A[1]:(0.808414459229) A[2]:(0.900067806244) A[3]:(0.728946805)\n",
      " state (14)  A[0]:(0.810001790524) A[1]:(0.900421261787) A[2]:(0.999999940395) A[3]:(0.809972822666)\n",
      " state (15)  A[0]:(0.982735514641) A[1]:(0.95665037632) A[2]:(1.0) A[3]:(0.876595199108)\n",
      "Episode 798000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6013. Times reached goal: 1000.               Steps done: 5952724. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00233887902004.\n",
      " state (0)  A[0]:(0.532378315926) A[1]:(0.590466499329) A[2]:(0.59044110775) A[3]:(0.533721625805)\n",
      " state (1)  A[0]:(0.532176017761) A[1]:(-4.03821468353e-05) A[2]:(0.656031012535) A[3]:(0.591424942017)\n",
      " state (2)  A[0]:(0.591389477253) A[1]:(0.728988647461) A[2]:(0.590652048588) A[3]:(0.656679868698)\n",
      " state (3)  A[0]:(0.656864881516) A[1]:(-0.21611623466) A[2]:(0.539307594299) A[3]:(0.518628835678)\n",
      " state (4)  A[0]:(0.591172337532) A[1]:(0.655856251717) A[2]:(0.00022280216217) A[3]:(0.532119035721)\n",
      " state (5)  A[0]:(0.163552835584) A[1]:(0.928805291653) A[2]:(-0.194319501519) A[3]:(0.526205837727)\n",
      " state (6)  A[0]:(0.000296115875244) A[1]:(0.809978067875) A[2]:(0.000405192346079) A[3]:(0.656235337257)\n",
      " state (7)  A[0]:(0.626547396183) A[1]:(-0.249128639698) A[2]:(0.305680394173) A[3]:(0.882950365543)\n",
      " state (8)  A[0]:(0.656673073769) A[1]:(-0.000584900320973) A[2]:(0.728836417198) A[3]:(0.591336727142)\n",
      " state (9)  A[0]:(0.656585752964) A[1]:(0.809890508652) A[2]:(0.809971928596) A[3]:(-0.000747009995393)\n",
      " state (10)  A[0]:(0.729059815407) A[1]:(0.900003612041) A[2]:(-0.000589966715779) A[3]:(0.727761209011)\n",
      " state (11)  A[0]:(0.522362470627) A[1]:(0.876603364944) A[2]:(-0.623153805733) A[3]:(0.843964457512)\n",
      " state (12)  A[0]:(0.0776753202081) A[1]:(0.823773980141) A[2]:(-0.616169095039) A[3]:(0.792987167835)\n",
      " state (13)  A[0]:(-0.00238668476231) A[1]:(0.80868023634) A[2]:(0.899996221066) A[3]:(0.727203726768)\n",
      " state (14)  A[0]:(0.810196280479) A[1]:(0.900613069534) A[2]:(0.999999940395) A[3]:(0.808455944061)\n",
      " state (15)  A[0]:(0.982746899128) A[1]:(0.956746816635) A[2]:(1.0) A[3]:(0.875381588936)\n",
      "Episode 799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6012. Times reached goal: 998.               Steps done: 5958736. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00232485986319.\n",
      "q_values \n",
      "tensor([[ 0.5366,  0.5902,  0.5903,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5367, -0.0003,  0.6559,  0.5906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5953,  0.7288,  0.5906,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0037,  0.8098, -0.0001,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7278,  0.9000, -0.0014,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8087,  0.9005,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.536305427551) A[1]:(0.590154409409) A[2]:(0.590265154839) A[3]:(0.531234741211)\n",
      " state (1)  A[0]:(0.536200523376) A[1]:(-0.000401034922106) A[2]:(0.65590596199) A[3]:(0.590376377106)\n",
      " state (2)  A[0]:(0.594722151756) A[1]:(0.728746831417) A[2]:(0.590612411499) A[3]:(0.656169652939)\n",
      " state (3)  A[0]:(0.658945083618) A[1]:(-0.216205373406) A[2]:(0.539499998093) A[3]:(0.517889261246)\n",
      " state (4)  A[0]:(0.593003809452) A[1]:(0.655723333359) A[2]:(0.000340223312378) A[3]:(0.531477689743)\n",
      " state (5)  A[0]:(0.166004836559) A[1]:(0.928833127022) A[2]:(-0.194693759084) A[3]:(0.525845527649)\n",
      " state (6)  A[0]:(0.00290953298099) A[1]:(0.809832513332) A[2]:(-6.18696212769e-05) A[3]:(0.656142234802)\n",
      " state (7)  A[0]:(0.627332627773) A[1]:(-0.249389141798) A[2]:(0.305240929127) A[3]:(0.882790744305)\n",
      " state (8)  A[0]:(0.656119346619) A[1]:(-5.08278608322e-05) A[2]:(0.728745341301) A[3]:(0.590847253799)\n",
      " state (9)  A[0]:(0.655331015587) A[1]:(0.810004413128) A[2]:(0.809886515141) A[3]:(0.000448062986834)\n",
      " state (10)  A[0]:(0.727719128132) A[1]:(0.899995684624) A[2]:(-0.00112223578617) A[3]:(0.728953897953)\n",
      " state (11)  A[0]:(0.519911527634) A[1]:(0.876523435116) A[2]:(-0.623609900475) A[3]:(0.844841659069)\n",
      " state (12)  A[0]:(0.0739115178585) A[1]:(0.823577642441) A[2]:(-0.616691946983) A[3]:(0.794222712517)\n",
      " state (13)  A[0]:(-0.0064497590065) A[1]:(0.808397591114) A[2]:(0.899991929531) A[3]:(0.728898406029)\n",
      " state (14)  A[0]:(0.808738470078) A[1]:(0.900434017181) A[2]:(0.999999940395) A[3]:(0.809810459614)\n",
      " state (15)  A[0]:(0.982571721077) A[1]:(0.956637978554) A[2]:(1.0) A[3]:(0.876312196255)\n",
      "Episode 800000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6005. Times reached goal: 995.               Steps done: 5964741. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00231094091318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531426787376) A[1]:(0.590341925621) A[2]:(0.59040749073) A[3]:(0.530794084072)\n",
      " state (1)  A[0]:(0.531142771244) A[1]:(-5.84870576859e-06) A[2]:(0.655964851379) A[3]:(0.590055465698)\n",
      " state (2)  A[0]:(0.590527832508) A[1]:(0.728982746601) A[2]:(0.590372920036) A[3]:(0.655848562717)\n",
      " state (3)  A[0]:(0.656730413437) A[1]:(-0.2148591429) A[2]:(0.539012491703) A[3]:(0.517477035522)\n",
      " state (4)  A[0]:(0.591484129429) A[1]:(0.656007349491) A[2]:(-9.75131988525e-05) A[3]:(0.530747294426)\n",
      " state (5)  A[0]:(0.164511173964) A[1]:(0.928874492645) A[2]:(-0.194924741983) A[3]:(0.524749517441)\n",
      " state (6)  A[0]:(0.00180208485108) A[1]:(0.81001663208) A[2]:(-0.000106811523438) A[3]:(0.654735207558)\n",
      " state (7)  A[0]:(0.627136230469) A[1]:(-0.2490157336) A[2]:(0.305603444576) A[3]:(0.881969213486)\n",
      " state (8)  A[0]:(0.656593084335) A[1]:(-0.000108756124973) A[2]:(0.728954434395) A[3]:(0.588432848454)\n",
      " state (9)  A[0]:(0.65639936924) A[1]:(0.810000896454) A[2]:(0.809982657433) A[3]:(-0.00287770433351)\n",
      " state (10)  A[0]:(0.728982806206) A[1]:(0.900048553944) A[2]:(-0.00107061816379) A[3]:(0.727577269077)\n",
      " state (11)  A[0]:(0.522217333317) A[1]:(0.876649022102) A[2]:(-0.623714327812) A[3]:(0.84405374527)\n",
      " state (12)  A[0]:(0.0773476287723) A[1]:(0.823829829693) A[2]:(-0.616865158081) A[3]:(0.793211817741)\n",
      " state (13)  A[0]:(-0.00263195624575) A[1]:(0.808762133121) A[2]:(0.90006506443) A[3]:(0.727648019791)\n",
      " state (14)  A[0]:(0.810280561447) A[1]:(0.900693655014) A[2]:(0.999999940395) A[3]:(0.809009552002)\n",
      " state (15)  A[0]:(0.982724487782) A[1]:(0.956765294075) A[2]:(1.0) A[3]:(0.87580370903)\n",
      "Episode 801000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5997. Times reached goal: 996.               Steps done: 5970738. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00229712367293.\n",
      " state (0)  A[0]:(0.531995296478) A[1]:(0.590388715267) A[2]:(0.59051322937) A[3]:(0.53160405159)\n",
      " state (1)  A[0]:(0.531836628914) A[1]:(-0.000250391662121) A[2]:(0.656071186066) A[3]:(0.590457558632)\n",
      " state (2)  A[0]:(0.590946853161) A[1]:(0.728992700577) A[2]:(0.590135395527) A[3]:(0.65635240078)\n",
      " state (3)  A[0]:(0.656391561031) A[1]:(-0.211765527725) A[2]:(0.538500607014) A[3]:(0.518135905266)\n",
      " state (4)  A[0]:(0.59041249752) A[1]:(0.655922293663) A[2]:(-1.45435333252e-05) A[3]:(0.531348705292)\n",
      " state (5)  A[0]:(0.162512764335) A[1]:(0.928842067719) A[2]:(-0.194829329848) A[3]:(0.525865793228)\n",
      " state (6)  A[0]:(-0.000203192234039) A[1]:(0.809973478317) A[2]:(-4.60147857666e-05) A[3]:(0.65602850914)\n",
      " state (7)  A[0]:(0.626171112061) A[1]:(-0.248939499259) A[2]:(0.305587232113) A[3]:(0.88260859251)\n",
      " state (8)  A[0]:(0.655790090561) A[1]:(2.76640057564e-05) A[2]:(0.728908836842) A[3]:(0.590384840965)\n",
      " state (9)  A[0]:(0.655610561371) A[1]:(0.809979498386) A[2]:(0.809959769249) A[3]:(-0.000125780701637)\n",
      " state (10)  A[0]:(0.728255271912) A[1]:(0.899985969067) A[2]:(-0.00105941260699) A[3]:(0.728829085827)\n",
      " state (11)  A[0]:(0.520967960358) A[1]:(0.876522600651) A[2]:(-0.623755097389) A[3]:(0.844828546047)\n",
      " state (12)  A[0]:(0.0755220577121) A[1]:(0.823592722416) A[2]:(-0.617033779621) A[3]:(0.794244706631)\n",
      " state (13)  A[0]:(-0.00450912024826) A[1]:(0.808443188667) A[2]:(0.900033473969) A[3]:(0.729009032249)\n",
      " state (14)  A[0]:(0.809704124928) A[1]:(0.900493264198) A[2]:(0.999999940395) A[3]:(0.810054123402)\n",
      " state (15)  A[0]:(0.982664525509) A[1]:(0.956656336784) A[2]:(1.0) A[3]:(0.876517474651)\n",
      "Episode 802000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 999.               Steps done: 5976743. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00228337077966.\n",
      " state (0)  A[0]:(0.531653225422) A[1]:(0.590201377869) A[2]:(0.590433239937) A[3]:(0.531145095825)\n",
      " state (1)  A[0]:(0.531762361526) A[1]:(-0.000106692314148) A[2]:(0.656064152718) A[3]:(0.590308964252)\n",
      " state (2)  A[0]:(0.591114759445) A[1]:(0.728943347931) A[2]:(0.590263962746) A[3]:(0.656226336956)\n",
      " state (3)  A[0]:(0.656686425209) A[1]:(-0.213242202997) A[2]:(0.538740754128) A[3]:(0.517695426941)\n",
      " state (4)  A[0]:(0.591136455536) A[1]:(0.655986368656) A[2]:(-0.000161170959473) A[3]:(0.530989050865)\n",
      " state (5)  A[0]:(0.1640868783) A[1]:(0.928874075413) A[2]:(-0.195086196065) A[3]:(0.525506615639)\n",
      " state (6)  A[0]:(0.0015621172497) A[1]:(0.809968173504) A[2]:(-0.000142693519592) A[3]:(0.655789136887)\n",
      " state (7)  A[0]:(0.626889467239) A[1]:(-0.249152451754) A[2]:(0.305816680193) A[3]:(0.882535457611)\n",
      " state (8)  A[0]:(0.656508564949) A[1]:(-0.00029468536377) A[2]:(0.728876113892) A[3]:(0.590502023697)\n",
      " state (9)  A[0]:(0.656643390656) A[1]:(0.809913873672) A[2]:(0.809961378574) A[3]:(-3.65972518921e-05)\n",
      " state (10)  A[0]:(0.72937721014) A[1]:(0.900000393391) A[2]:(-0.00116169406101) A[3]:(0.728871822357)\n",
      " state (11)  A[0]:(0.522957921028) A[1]:(0.876583814621) A[2]:(-0.623987436295) A[3]:(0.844869017601)\n",
      " state (12)  A[0]:(0.0783355683088) A[1]:(0.82373124361) A[2]:(-0.617390751839) A[3]:(0.794251799583)\n",
      " state (13)  A[0]:(-0.00180208485108) A[1]:(0.808659791946) A[2]:(0.900002360344) A[3]:(0.728932797909)\n",
      " state (14)  A[0]:(0.810484290123) A[1]:(0.900661528111) A[2]:(0.999999940395) A[3]:(0.809956073761)\n",
      " state (15)  A[0]:(0.982707440853) A[1]:(0.956742525101) A[2]:(1.0) A[3]:(0.876379847527)\n",
      "Episode 803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5992. Times reached goal: 995.               Steps done: 5982735. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00226972973135.\n",
      " state (0)  A[0]:(0.532425642014) A[1]:(0.590366780758) A[2]:(0.59036719799) A[3]:(0.53178435564)\n",
      " state (1)  A[0]:(0.532077908516) A[1]:(-0.000107079744339) A[2]:(0.655975997448) A[3]:(0.590407133102)\n",
      " state (2)  A[0]:(0.591133594513) A[1]:(0.728997588158) A[2]:(0.590513467789) A[3]:(0.656145751476)\n",
      " state (3)  A[0]:(0.656706094742) A[1]:(-0.213970154524) A[2]:(0.539276123047) A[3]:(0.518093943596)\n",
      " state (4)  A[0]:(0.591003417969) A[1]:(0.655943036079) A[2]:(0.000452518434031) A[3]:(0.531618714333)\n",
      " state (5)  A[0]:(0.163596510887) A[1]:(0.92889046669) A[2]:(-0.194642961025) A[3]:(0.52609539032)\n",
      " state (6)  A[0]:(0.000940680212807) A[1]:(0.810069918633) A[2]:(0.000185012817383) A[3]:(0.656059682369)\n",
      " state (7)  A[0]:(0.626519441605) A[1]:(-0.24867503345) A[2]:(0.306125462055) A[3]:(0.882482588291)\n",
      " state (8)  A[0]:(0.656130433083) A[1]:(0.000241756439209) A[2]:(0.729152083397) A[3]:(0.590136408806)\n",
      " state (9)  A[0]:(0.656302809715) A[1]:(0.810030579567) A[2]:(0.810156524181) A[3]:(-6.76661729813e-05)\n",
      " state (10)  A[0]:(0.729273915291) A[1]:(0.900045514107) A[2]:(-0.000836849038024) A[3]:(0.729101002216)\n",
      " state (11)  A[0]:(0.523041963577) A[1]:(0.876638412476) A[2]:(-0.623934030533) A[3]:(0.845058560371)\n",
      " state (12)  A[0]:(0.0786532014608) A[1]:(0.823817610741) A[2]:(-0.617413461208) A[3]:(0.794505238533)\n",
      " state (13)  A[0]:(-0.0015241492074) A[1]:(0.808781564236) A[2]:(0.900125265121) A[3]:(0.729266524315)\n",
      " state (14)  A[0]:(0.810355842113) A[1]:(0.900757074356) A[2]:(0.999999940395) A[3]:(0.810243368149)\n",
      " state (15)  A[0]:(0.982647120953) A[1]:(0.956783235073) A[2]:(1.0) A[3]:(0.87655043602)\n",
      "Episode 804000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6009. Times reached goal: 999.               Steps done: 5988744. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00225613182123.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5908,  0.5905,  0.5322]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5895,  0.6562,  0.0003,  0.5324]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6550, -0.0004,  0.7288,  0.5915]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6552,  0.8101,  0.8101,  0.0018]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7283,  0.9000, -0.0008,  0.7297]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8092,  0.9006,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531700849533) A[1]:(0.590748548508) A[2]:(0.590449154377) A[3]:(0.53126347065)\n",
      " state (1)  A[0]:(0.531587958336) A[1]:(0.000259935855865) A[2]:(0.656064689159) A[3]:(0.590122878551)\n",
      " state (2)  A[0]:(0.590621709824) A[1]:(0.729037582874) A[2]:(0.59059214592) A[3]:(0.655960261822)\n",
      " state (3)  A[0]:(0.655892491341) A[1]:(-0.214849308133) A[2]:(0.539352834225) A[3]:(0.517966687679)\n",
      " state (4)  A[0]:(0.58974313736) A[1]:(0.656166434288) A[2]:(0.000282406806946) A[3]:(0.531723499298)\n",
      " state (5)  A[0]:(0.16136701405) A[1]:(0.92890304327) A[2]:(-0.194665893912) A[3]:(0.526247501373)\n",
      " state (6)  A[0]:(-0.00148695602547) A[1]:(0.810067951679) A[2]:(0.000312209129333) A[3]:(0.6562281847)\n",
      " state (7)  A[0]:(0.625205039978) A[1]:(-0.248714253306) A[2]:(0.306418329477) A[3]:(0.882627308369)\n",
      " state (8)  A[0]:(0.655109465122) A[1]:(0.000294536352158) A[2]:(0.729175925255) A[3]:(0.59110891819)\n",
      " state (9)  A[0]:(0.655222475529) A[1]:(0.810083985329) A[2]:(0.81016933918) A[3]:(0.00171859387774)\n",
      " state (10)  A[0]:(0.728300571442) A[1]:(0.900063216686) A[2]:(-0.000393629044993) A[3]:(0.729678809643)\n",
      " state (11)  A[0]:(0.521550059319) A[1]:(0.876648724079) A[2]:(-0.6236089468) A[3]:(0.845302343369)\n",
      " state (12)  A[0]:(0.0766563415527) A[1]:(0.823820412159) A[2]:(-0.617270052433) A[3]:(0.794747173786)\n",
      " state (13)  A[0]:(-0.003816049546) A[1]:(0.808758974075) A[2]:(0.90003746748) A[3]:(0.729455947876)\n",
      " state (14)  A[0]:(0.809304952621) A[1]:(0.900730013847) A[2]:(0.999999940395) A[3]:(0.810235142708)\n",
      " state (15)  A[0]:(0.982520401478) A[1]:(0.956769585609) A[2]:(1.0) A[3]:(0.876458406448)\n",
      "Episode 805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 1000.               Steps done: 5994755. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00224261089072.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.533638000488) A[1]:(0.590676784515) A[2]:(0.590767502785) A[3]:(0.53156208992)\n",
      " state (1)  A[0]:(0.535303354263) A[1]:(1.76504254341e-05) A[2]:(0.656356155872) A[3]:(0.590769350529)\n",
      " state (2)  A[0]:(0.595312952995) A[1]:(0.729089319706) A[2]:(0.591442704201) A[3]:(0.65667450428)\n",
      " state (3)  A[0]:(0.662044763565) A[1]:(-0.216267570853) A[2]:(0.540101826191) A[3]:(0.518934011459)\n",
      " state (4)  A[0]:(0.598881602287) A[1]:(0.656507074833) A[2]:(0.00012481212616) A[3]:(0.533205509186)\n",
      " state (5)  A[0]:(0.177336096764) A[1]:(0.929093658924) A[2]:(-0.195511370897) A[3]:(0.528291225433)\n",
      " state (6)  A[0]:(0.0165964029729) A[1]:(0.810128748417) A[2]:(-0.000219106674194) A[3]:(0.658334493637)\n",
      " state (7)  A[0]:(0.635252118111) A[1]:(-0.248969003558) A[2]:(0.306546539068) A[3]:(0.883392989635)\n",
      " state (8)  A[0]:(0.662978231907) A[1]:(0.000823713664431) A[2]:(0.729334831238) A[3]:(0.59271132946)\n",
      " state (9)  A[0]:(0.66244584322) A[1]:(0.810106754303) A[2]:(0.810092270374) A[3]:(0.00494695128873)\n",
      " state (10)  A[0]:(0.734198212624) A[1]:(0.899933040142) A[2]:(-0.000676393392496) A[3]:(0.731592297554)\n",
      " state (11)  A[0]:(0.530882835388) A[1]:(0.876324117184) A[2]:(-0.623696923256) A[3]:(0.84661012888)\n",
      " state (12)  A[0]:(0.0897284373641) A[1]:(0.823134064674) A[2]:(-0.617323756218) A[3]:(0.796582877636)\n",
      " state (13)  A[0]:(0.00931236054748) A[1]:(0.807751476765) A[2]:(0.900104403496) A[3]:(0.731953382492)\n",
      " state (14)  A[0]:(0.813526272774) A[1]:(0.900029420853) A[2]:(0.999999940395) A[3]:(0.812178432941)\n",
      " state (15)  A[0]:(0.982899188995) A[1]:(0.956372380257) A[2]:(1.0) A[3]:(0.877800107002)\n",
      "Episode 806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 997.               Steps done: 6000759. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00222918659499.\n",
      " state (0)  A[0]:(0.532627403736) A[1]:(0.590464115143) A[2]:(0.590492784977) A[3]:(0.531687855721)\n",
      " state (1)  A[0]:(0.531744360924) A[1]:(0.000234663486481) A[2]:(0.656090974808) A[3]:(0.591351866722)\n",
      " state (2)  A[0]:(0.590150475502) A[1]:(0.728975713253) A[2]:(0.590636253357) A[3]:(0.657559990883)\n",
      " state (3)  A[0]:(0.6547549963) A[1]:(-0.217281684279) A[2]:(0.539247274399) A[3]:(0.521550238132)\n",
      " state (4)  A[0]:(0.587208628654) A[1]:(0.656124949455) A[2]:(-0.000107407569885) A[3]:(0.536376476288)\n",
      " state (5)  A[0]:(0.155790045857) A[1]:(0.928776144981) A[2]:(-0.194300338626) A[3]:(0.531073093414)\n",
      " state (6)  A[0]:(-0.00878809113055) A[1]:(0.80999571085) A[2]:(0.000879764324054) A[3]:(0.659574151039)\n",
      " state (7)  A[0]:(0.621228814125) A[1]:(-0.248568296432) A[2]:(0.306954115629) A[3]:(0.883651137352)\n",
      " state (8)  A[0]:(0.653476953506) A[1]:(-9.92119312286e-05) A[2]:(0.728926062584) A[3]:(0.592837750912)\n",
      " state (9)  A[0]:(0.655178427696) A[1]:(0.810014903545) A[2]:(0.809920907021) A[3]:(-0.000957593030762)\n",
      " state (10)  A[0]:(0.728944301605) A[1]:(0.899984538555) A[2]:(-0.000417470902903) A[3]:(0.726940631866)\n",
      " state (11)  A[0]:(0.523282170296) A[1]:(0.876444399357) A[2]:(-0.623556494713) A[3]:(0.84340262413)\n",
      " state (12)  A[0]:(0.0797306671739) A[1]:(0.823344290257) A[2]:(-0.617397010326) A[3]:(0.792324781418)\n",
      " state (13)  A[0]:(-0.000771760765929) A[1]:(0.807999968529) A[2]:(0.900008380413) A[3]:(0.726572155952)\n",
      " state (14)  A[0]:(0.809813141823) A[1]:(0.900176584721) A[2]:(0.999999940395) A[3]:(0.808382034302)\n",
      " state (15)  A[0]:(0.982497930527) A[1]:(0.956443786621) A[2]:(1.0) A[3]:(0.875366568565)\n",
      "Episode 807000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 1000.               Steps done: 6006765. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00221583822558.\n",
      " state (0)  A[0]:(0.529533624649) A[1]:(0.590438187122) A[2]:(0.590628504753) A[3]:(0.526745021343)\n",
      " state (1)  A[0]:(0.528507709503) A[1]:(-0.000688977423124) A[2]:(0.656053900719) A[3]:(0.587714672089)\n",
      " state (2)  A[0]:(0.587147116661) A[1]:(0.728929519653) A[2]:(0.590909838676) A[3]:(0.654821038246)\n",
      " state (3)  A[0]:(0.651452064514) A[1]:(-0.21648286283) A[2]:(0.539923429489) A[3]:(0.517909646034)\n",
      " state (4)  A[0]:(0.583414196968) A[1]:(0.655948340893) A[2]:(0.00144016637933) A[3]:(0.533075094223)\n",
      " state (5)  A[0]:(0.151016041636) A[1]:(0.928729534149) A[2]:(-0.192678630352) A[3]:(0.52863407135)\n",
      " state (6)  A[0]:(-0.0122402757406) A[1]:(0.809826135635) A[2]:(0.0026837522164) A[3]:(0.659057497978)\n",
      " state (7)  A[0]:(0.620222091675) A[1]:(-0.249047234654) A[2]:(0.308252066374) A[3]:(0.884585499763)\n",
      " state (8)  A[0]:(0.653576850891) A[1]:(-0.000322587788105) A[2]:(0.728822588921) A[3]:(0.601519405842)\n",
      " state (9)  A[0]:(0.655518054962) A[1]:(0.809980630875) A[2]:(0.809976875782) A[3]:(0.0201021768153)\n",
      " state (10)  A[0]:(0.729134082794) A[1]:(0.899927854538) A[2]:(0.00160384038463) A[3]:(0.737381458282)\n",
      " state (11)  A[0]:(0.52387034893) A[1]:(0.8763256073) A[2]:(-0.62180274725) A[3]:(0.8498467803)\n",
      " state (12)  A[0]:(0.0811184719205) A[1]:(0.823104798794) A[2]:(-0.615768790245) A[3]:(0.80071246624)\n",
      " state (13)  A[0]:(0.00097030372126) A[1]:(0.807606935501) A[2]:(0.900179684162) A[3]:(0.737129449844)\n",
      " state (14)  A[0]:(0.81061321497) A[1]:(0.899872422218) A[2]:(0.999999940395) A[3]:(0.815804064274)\n",
      " state (15)  A[0]:(0.982621073723) A[1]:(0.956281781197) A[2]:(1.0) A[3]:(0.880176067352)\n",
      "Episode 808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 997.               Steps done: 6012774. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00220256317851.\n",
      " state (0)  A[0]:(0.531482458115) A[1]:(0.590628862381) A[2]:(0.590442121029) A[3]:(0.531431376934)\n",
      " state (1)  A[0]:(0.531456112862) A[1]:(0.000183135271072) A[2]:(0.656054854393) A[3]:(0.590447902679)\n",
      " state (2)  A[0]:(0.590661346912) A[1]:(0.729077458382) A[2]:(0.590662360191) A[3]:(0.656299233437)\n",
      " state (3)  A[0]:(0.656273186207) A[1]:(-0.214896216989) A[2]:(0.539391040802) A[3]:(0.517884790897)\n",
      " state (4)  A[0]:(0.590445876122) A[1]:(0.656205534935) A[2]:(0.000166296958923) A[3]:(0.531670928001)\n",
      " state (5)  A[0]:(0.162823498249) A[1]:(0.92889291048) A[2]:(-0.194869577885) A[3]:(0.526555299759)\n",
      " state (6)  A[0]:(0.000694751623087) A[1]:(0.810049951077) A[2]:(0.00031054019928) A[3]:(0.656719982624)\n",
      " state (7)  A[0]:(0.627053976059) A[1]:(-0.248566403985) A[2]:(0.306954860687) A[3]:(0.882875084877)\n",
      " state (8)  A[0]:(0.656934857368) A[1]:(0.000513642968144) A[2]:(0.72917509079) A[3]:(0.592044591904)\n",
      " state (9)  A[0]:(0.656715393066) A[1]:(0.810118317604) A[2]:(0.810071349144) A[3]:(0.00257118977606)\n",
      " state (10)  A[0]:(0.729787230492) A[1]:(0.900003731251) A[2]:(-0.00034499168396) A[3]:(0.729974746704)\n",
      " state (11)  A[0]:(0.524661660194) A[1]:(0.876450061798) A[2]:(-0.623705863953) A[3]:(0.845559060574)\n",
      " state (12)  A[0]:(0.0818583443761) A[1]:(0.823339343071) A[2]:(-0.617731571198) A[3]:(0.795208573341)\n",
      " state (13)  A[0]:(0.0015979395248) A[1]:(0.807996988297) A[2]:(0.899999856949) A[3]:(0.730192601681)\n",
      " state (14)  A[0]:(0.810754835606) A[1]:(0.900197386742) A[2]:(0.999999940395) A[3]:(0.810914635658)\n",
      " state (15)  A[0]:(0.982576489449) A[1]:(0.956445395947) A[2]:(1.0) A[3]:(0.876910507679)\n",
      "Episode 809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5998. Times reached goal: 996.               Steps done: 6018772. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00218939174518.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5905,  0.5905,  0.5319]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.0002,  0.6560,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.7290,  0.5905,  0.6559]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  0.8101, -0.0001,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000, -0.0004,  0.7283]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9006,  1.0000,  0.8096]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531440973282) A[1]:(0.590486586094) A[2]:(0.590524077415) A[3]:(0.531747400761)\n",
      " state (1)  A[0]:(0.531563997269) A[1]:(0.000290989875793) A[2]:(0.656052052975) A[3]:(0.590102732182)\n",
      " state (2)  A[0]:(0.590859889984) A[1]:(0.72900891304) A[2]:(0.590463876724) A[3]:(0.655804276466)\n",
      " state (3)  A[0]:(0.656201839447) A[1]:(-0.216243162751) A[2]:(0.539078116417) A[3]:(0.517306089401)\n",
      " state (4)  A[0]:(0.590243816376) A[1]:(0.656347692013) A[2]:(-0.000739216688089) A[3]:(0.531189322472)\n",
      " state (5)  A[0]:(0.162244305015) A[1]:(0.928910076618) A[2]:(-0.195698931813) A[3]:(0.525819480419)\n",
      " state (6)  A[0]:(-0.000428378552897) A[1]:(0.810029089451) A[2]:(-0.00028657913208) A[3]:(0.655717372894)\n",
      " state (7)  A[0]:(0.626040935516) A[1]:(-0.248857200146) A[2]:(0.306877553463) A[3]:(0.882238507271)\n",
      " state (8)  A[0]:(0.656055450439) A[1]:(-7.33733177185e-05) A[2]:(0.729063630104) A[3]:(0.589999556541)\n",
      " state (9)  A[0]:(0.656085014343) A[1]:(0.81000161171) A[2]:(0.810079813004) A[3]:(-0.00129695166834)\n",
      " state (10)  A[0]:(0.729220271111) A[1]:(0.900020182133) A[2]:(-0.000458240479929) A[3]:(0.728134274483)\n",
      " state (11)  A[0]:(0.523471593857) A[1]:(0.876552700996) A[2]:(-0.62399494648) A[3]:(0.844459891319)\n",
      " state (12)  A[0]:(0.079787530005) A[1]:(0.823613941669) A[2]:(-0.618125081062) A[3]:(0.79377245903)\n",
      " state (13)  A[0]:(-0.000687420251779) A[1]:(0.808487057686) A[2]:(0.900011599064) A[3]:(0.728345632553)\n",
      " state (14)  A[0]:(0.80999571085) A[1]:(0.900604724884) A[2]:(0.999999940395) A[3]:(0.809532105923)\n",
      " state (15)  A[0]:(0.982486009598) A[1]:(0.956682384014) A[2]:(1.0) A[3]:(0.875928103924)\n",
      "Episode 810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 999.               Steps done: 6024782. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00217627296222.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.5316375494) A[1]:(0.590683937073) A[2]:(0.590725064278) A[3]:(0.531388819218)\n",
      " state (1)  A[0]:(0.531520545483) A[1]:(5.15580177307e-05) A[2]:(0.65619301796) A[3]:(0.590341567993)\n",
      " state (2)  A[0]:(0.590845108032) A[1]:(0.729120016098) A[2]:(0.590641438961) A[3]:(0.656193852425)\n",
      " state (3)  A[0]:(0.656366944313) A[1]:(-0.215728119016) A[2]:(0.539242863655) A[3]:(0.517733454704)\n",
      " state (4)  A[0]:(0.590626657009) A[1]:(0.656831502914) A[2]:(-0.000453591317637) A[3]:(0.531654059887)\n",
      " state (5)  A[0]:(0.163045436144) A[1]:(0.928978562355) A[2]:(-0.195225298405) A[3]:(0.526359200478)\n",
      " state (6)  A[0]:(0.000607967318501) A[1]:(0.810147583485) A[2]:(0.000353574723704) A[3]:(0.656160414219)\n",
      " state (7)  A[0]:(0.626840114594) A[1]:(-0.248555853963) A[2]:(0.307554006577) A[3]:(0.882417082787)\n",
      " state (8)  A[0]:(0.656840801239) A[1]:(0.000334322452545) A[2]:(0.729318678379) A[3]:(0.59083122015)\n",
      " state (9)  A[0]:(0.656675875187) A[1]:(0.810096979141) A[2]:(0.810154080391) A[3]:(0.000674113514833)\n",
      " state (10)  A[0]:(0.729545235634) A[1]:(0.899998903275) A[2]:(8.94069671631e-05) A[3]:(0.7291046381)\n",
      " state (11)  A[0]:(0.52394425869) A[1]:(0.876431941986) A[2]:(-0.623532533646) A[3]:(0.845067739487)\n",
      " state (12)  A[0]:(0.0804647877812) A[1]:(0.823296308517) A[2]:(-0.617784678936) A[3]:(0.794614851475)\n",
      " state (13)  A[0]:(-0.000178635120392) A[1]:(0.807938992977) A[2]:(0.899981439114) A[3]:(0.72945535183)\n",
      " state (14)  A[0]:(0.810072183609) A[1]:(0.900175154209) A[2]:(0.999999940395) A[3]:(0.810337424278)\n",
      " state (15)  A[0]:(0.9824898839) A[1]:(0.956431984901) A[2]:(1.0) A[3]:(0.876471579075)\n",
      "Episode 811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5998. Times reached goal: 996.               Steps done: 6030780. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00216325874565.\n",
      " state (0)  A[0]:(0.532399177551) A[1]:(0.590871930122) A[2]:(0.59080696106) A[3]:(0.532350420952)\n",
      " state (1)  A[0]:(0.533064007759) A[1]:(-2.82377004623e-06) A[2]:(0.656293392181) A[3]:(0.591469407082)\n",
      " state (2)  A[0]:(0.592325925827) A[1]:(0.729148030281) A[2]:(0.590969800949) A[3]:(0.657205224037)\n",
      " state (3)  A[0]:(0.657397866249) A[1]:(-0.215642526746) A[2]:(0.539642333984) A[3]:(0.519206523895)\n",
      " state (4)  A[0]:(0.591458618641) A[1]:(0.656534731388) A[2]:(0.000116348266602) A[3]:(0.533062636852)\n",
      " state (5)  A[0]:(0.163909047842) A[1]:(0.928924739361) A[2]:(-0.194874048233) A[3]:(0.527690052986)\n",
      " state (6)  A[0]:(0.00127112795599) A[1]:(0.810084104538) A[2]:(0.000579595507588) A[3]:(0.657148361206)\n",
      " state (7)  A[0]:(0.627361178398) A[1]:(-0.24857661128) A[2]:(0.307720541954) A[3]:(0.882807016373)\n",
      " state (8)  A[0]:(0.657851338387) A[1]:(0.000242240726948) A[2]:(0.72929161787) A[3]:(0.591838359833)\n",
      " state (9)  A[0]:(0.658250570297) A[1]:(0.810041069984) A[2]:(0.81012237072) A[3]:(0.00141771044582)\n",
      " state (10)  A[0]:(0.730996608734) A[1]:(0.899947524071) A[2]:(-9.53674316406e-06) A[3]:(0.729138076305)\n",
      " state (11)  A[0]:(0.52623295784) A[1]:(0.876337647438) A[2]:(-0.623685002327) A[3]:(0.844951987267)\n",
      " state (12)  A[0]:(0.083545871079) A[1]:(0.823117554188) A[2]:(-0.618032813072) A[3]:(0.794312059879)\n",
      " state (13)  A[0]:(0.00272189895622) A[1]:(0.807698130608) A[2]:(0.899971306324) A[3]:(0.728884994984)\n",
      " state (14)  A[0]:(0.810878396034) A[1]:(0.900022923946) A[2]:(0.999999940395) A[3]:(0.809771001339)\n",
      " state (15)  A[0]:(0.98253762722) A[1]:(0.956344127655) A[2]:(1.0) A[3]:(0.875970244408)\n",
      "Episode 812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 998.               Steps done: 6036781. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00215031590375.\n",
      " state (0)  A[0]:(0.530335426331) A[1]:(0.590543448925) A[2]:(0.590555548668) A[3]:(0.532314777374)\n",
      " state (1)  A[0]:(0.530839681625) A[1]:(0.000640720012598) A[2]:(0.656184434891) A[3]:(0.590570628643)\n",
      " state (2)  A[0]:(0.590209960938) A[1]:(0.729035496712) A[2]:(0.590353190899) A[3]:(0.656070232391)\n",
      " state (3)  A[0]:(0.655343294144) A[1]:(-0.21467359364) A[2]:(0.538696944714) A[3]:(0.517115831375)\n",
      " state (4)  A[0]:(0.588880360126) A[1]:(0.656014800072) A[2]:(-0.000888108974323) A[3]:(0.530507802963)\n",
      " state (5)  A[0]:(0.160096555948) A[1]:(0.928813993931) A[2]:(-0.195866540074) A[3]:(0.525035738945)\n",
      " state (6)  A[0]:(-0.00271510402672) A[1]:(0.810107707977) A[2]:(-0.000669479253702) A[3]:(0.654713869095)\n",
      " state (7)  A[0]:(0.625349640846) A[1]:(-0.248143270612) A[2]:(0.306685447693) A[3]:(0.881588697433)\n",
      " state (8)  A[0]:(0.657409906387) A[1]:(0.000237286090851) A[2]:(0.729105353355) A[3]:(0.587509274483)\n",
      " state (9)  A[0]:(0.6596763134) A[1]:(0.81007951498) A[2]:(0.810073137283) A[3]:(-0.0055587911047)\n",
      " state (10)  A[0]:(0.733569145203) A[1]:(0.900050282478) A[2]:(-0.000822782341857) A[3]:(0.726619362831)\n",
      " state (11)  A[0]:(0.531684994698) A[1]:(0.876522839069) A[2]:(-0.624451816082) A[3]:(0.843810319901)\n",
      " state (12)  A[0]:(0.0923522934318) A[1]:(0.823400199413) A[2]:(-0.618743062019) A[3]:(0.793133497238)\n",
      " state (13)  A[0]:(0.0125285983086) A[1]:(0.808008909225) A[2]:(0.900064766407) A[3]:(0.72765225172)\n",
      " state (14)  A[0]:(0.814347803593) A[1]:(0.900189995766) A[2]:(0.999999940395) A[3]:(0.809045135975)\n",
      " state (15)  A[0]:(0.982852160931) A[1]:(0.956382095814) A[2]:(1.0) A[3]:(0.875474333763)\n",
      "Episode 813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5997. Times reached goal: 997.               Steps done: 6042778. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00213745904909.\n",
      " state (0)  A[0]:(0.531416296959) A[1]:(0.590490937233) A[2]:(0.590442538261) A[3]:(0.531577169895)\n",
      " state (1)  A[0]:(0.531592845917) A[1]:(0.000215075910091) A[2]:(0.656163573265) A[3]:(0.590354859829)\n",
      " state (2)  A[0]:(0.59094619751) A[1]:(0.729012608528) A[2]:(0.590469360352) A[3]:(0.656188368797)\n",
      " state (3)  A[0]:(0.656671643257) A[1]:(-0.214055463672) A[2]:(0.539093911648) A[3]:(0.518028557301)\n",
      " state (4)  A[0]:(0.591100275517) A[1]:(0.655935168266) A[2]:(-0.000200510025024) A[3]:(0.531783103943)\n",
      " state (5)  A[0]:(0.163961201906) A[1]:(0.928839921951) A[2]:(-0.195506319404) A[3]:(0.526573240757)\n",
      " state (6)  A[0]:(0.00111836148426) A[1]:(0.810005903244) A[2]:(-0.000123500823975) A[3]:(0.656123280525)\n",
      " state (7)  A[0]:(0.626506090164) A[1]:(-0.248771712184) A[2]:(0.307307481766) A[3]:(0.882261872292)\n",
      " state (8)  A[0]:(0.65630531311) A[1]:(-0.000376708776457) A[2]:(0.728903532028) A[3]:(0.590983331203)\n",
      " state (9)  A[0]:(0.655838131905) A[1]:(0.809908509254) A[2]:(0.81003177166) A[3]:(0.000455230445368)\n",
      " state (10)  A[0]:(0.729093015194) A[1]:(0.899974286556) A[2]:(-0.000195026397705) A[3]:(0.729009628296)\n",
      " state (11)  A[0]:(0.523798108101) A[1]:(0.876451134682) A[2]:(-0.624005079269) A[3]:(0.845065653324)\n",
      " state (12)  A[0]:(0.0807588621974) A[1]:(0.823358654976) A[2]:(-0.618499040604) A[3]:(0.79454523325)\n",
      " state (13)  A[0]:(0.000197887420654) A[1]:(0.808044672012) A[2]:(0.899957299232) A[3]:(0.729177594185)\n",
      " state (14)  A[0]:(0.809977412224) A[1]:(0.900269687176) A[2]:(0.999999940395) A[3]:(0.809951484203)\n",
      " state (15)  A[0]:(0.98241007328) A[1]:(0.956459820271) A[2]:(1.0) A[3]:(0.875985741615)\n",
      "Episode 814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 1000.               Steps done: 6048782. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00212466419355.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5905,  0.5904,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5919,  0.6559,  0.0012,  0.5347]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6627,  0.0003,  0.7287,  0.5990]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6638,  0.8102,  0.8101,  0.0145]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0155,  0.8071,  0.9005,  0.7354]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8148,  0.8994,  1.0000,  0.8145]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530024528503) A[1]:(0.590582311153) A[2]:(0.590501964092) A[3]:(0.530992925167)\n",
      " state (1)  A[0]:(0.530125677586) A[1]:(-0.000456385285361) A[2]:(0.656036734581) A[3]:(0.590534806252)\n",
      " state (2)  A[0]:(0.589792370796) A[1]:(0.729096531868) A[2]:(0.590946435928) A[3]:(0.65682888031)\n",
      " state (3)  A[0]:(0.656204819679) A[1]:(-0.214817017317) A[2]:(0.540140807629) A[3]:(0.519791960716)\n",
      " state (4)  A[0]:(0.591057896614) A[1]:(0.656134843826) A[2]:(0.00142824556679) A[3]:(0.534509301186)\n",
      " state (5)  A[0]:(0.164726898074) A[1]:(0.928876757622) A[2]:(-0.193720653653) A[3]:(0.530164122581)\n",
      " state (6)  A[0]:(0.00394991971552) A[1]:(0.810073435307) A[2]:(0.00178909115493) A[3]:(0.659771323204)\n",
      " state (7)  A[0]:(0.630455493927) A[1]:(-0.248357236385) A[2]:(0.308645248413) A[3]:(0.884166896343)\n",
      " state (8)  A[0]:(0.662336826324) A[1]:(0.000491835118737) A[2]:(0.729042708874) A[3]:(0.59874099493)\n",
      " state (9)  A[0]:(0.663561224937) A[1]:(0.81014662981) A[2]:(0.810166060925) A[3]:(0.0140879964456)\n",
      " state (10)  A[0]:(0.735686182976) A[1]:(0.899973034859) A[2]:(0.0017451030435) A[3]:(0.734944581985)\n",
      " state (11)  A[0]:(0.534217834473) A[1]:(0.876285970211) A[2]:(-0.62237906456) A[3]:(0.84860098362)\n",
      " state (12)  A[0]:(0.0952554345131) A[1]:(0.822871685028) A[2]:(-0.617290496826) A[3]:(0.799207687378)\n",
      " state (13)  A[0]:(0.0141788162291) A[1]:(0.807120680809) A[2]:(0.899592638016) A[3]:(0.735132098198)\n",
      " state (14)  A[0]:(0.814392626286) A[1]:(0.899492740631) A[2]:(0.999999940395) A[3]:(0.814183294773)\n",
      " state (15)  A[0]:(0.982892155647) A[1]:(0.956034719944) A[2]:(1.0) A[3]:(0.878884971142)\n",
      "Episode 815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 999.               Steps done: 6054782. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00211195437597.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532799124718) A[1]:(0.590479671955) A[2]:(0.590490102768) A[3]:(0.531378626823)\n",
      " state (1)  A[0]:(0.533059000969) A[1]:(-7.24494457245e-05) A[2]:(0.656078934669) A[3]:(0.590383291245)\n",
      " state (2)  A[0]:(0.592337429523) A[1]:(0.729000329971) A[2]:(0.590627908707) A[3]:(0.656220555305)\n",
      " state (3)  A[0]:(0.658203363419) A[1]:(-0.215148448944) A[2]:(0.539419531822) A[3]:(0.517499268055)\n",
      " state (4)  A[0]:(0.592978775501) A[1]:(0.656153559685) A[2]:(-0.000112056732178) A[3]:(0.531292796135)\n",
      " state (5)  A[0]:(0.166618943214) A[1]:(0.928891718388) A[2]:(-0.195501744747) A[3]:(0.526195287704)\n",
      " state (6)  A[0]:(0.00376473087817) A[1]:(0.809986174107) A[2]:(2.45571136475e-05) A[3]:(0.655932068825)\n",
      " state (7)  A[0]:(0.627616524696) A[1]:(-0.24881529808) A[2]:(0.307711064816) A[3]:(0.882112026215)\n",
      " state (8)  A[0]:(0.656810641289) A[1]:(-0.000189751386642) A[2]:(0.729114294052) A[3]:(0.590018510818)\n",
      " state (9)  A[0]:(0.656192541122) A[1]:(0.809916973114) A[2]:(0.810003936291) A[3]:(-0.000947400636505)\n",
      " state (10)  A[0]:(0.729238092899) A[1]:(0.899973809719) A[2]:(-0.000933050818276) A[3]:(0.728603363037)\n",
      " state (11)  A[0]:(0.52368915081) A[1]:(0.876450061798) A[2]:(-0.624752640724) A[3]:(0.84487259388)\n",
      " state (12)  A[0]:(0.080146536231) A[1]:(0.823359370232) A[2]:(-0.619341850281) A[3]:(0.794291913509)\n",
      " state (13)  A[0]:(-0.000635862292256) A[1]:(0.808078110218) A[2]:(0.899978578091) A[3]:(0.728866279125)\n",
      " state (14)  A[0]:(0.809695959091) A[1]:(0.900322496891) A[2]:(0.999999940395) A[3]:(0.809756457806)\n",
      " state (15)  A[0]:(0.982342362404) A[1]:(0.956470608711) A[2]:(1.0) A[3]:(0.875794827938)\n",
      "Episode 816000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6015. Times reached goal: 1000.               Steps done: 6060797. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00209928909941.\n",
      " state (0)  A[0]:(0.531832695007) A[1]:(0.591094732285) A[2]:(0.590613007545) A[3]:(0.531790614128)\n",
      " state (1)  A[0]:(0.531832695007) A[1]:(0.000631451548543) A[2]:(0.656251370907) A[3]:(0.590464949608)\n",
      " state (2)  A[0]:(0.5910333395) A[1]:(0.729150295258) A[2]:(0.59107863903) A[3]:(0.656034469604)\n",
      " state (3)  A[0]:(0.656778216362) A[1]:(-0.217335700989) A[2]:(0.540090680122) A[3]:(0.516830742359)\n",
      " state (4)  A[0]:(0.590914845467) A[1]:(0.656473278999) A[2]:(0.00025749206543) A[3]:(0.530618309975)\n",
      " state (5)  A[0]:(0.162808582187) A[1]:(0.928890645504) A[2]:(-0.195018321276) A[3]:(0.525171160698)\n",
      " state (6)  A[0]:(-0.000279247760773) A[1]:(0.810026347637) A[2]:(0.000224232673645) A[3]:(0.654794573784)\n",
      " state (7)  A[0]:(0.625997006893) A[1]:(-0.248543858528) A[2]:(0.307630002499) A[3]:(0.881550312042)\n",
      " state (8)  A[0]:(0.656145215034) A[1]:(0.000105947256088) A[2]:(0.729231715202) A[3]:(0.587664723396)\n",
      " state (9)  A[0]:(0.656062483788) A[1]:(0.810018360615) A[2]:(0.810015261173) A[3]:(-0.00500458758324)\n",
      " state (10)  A[0]:(0.729200363159) A[1]:(0.900009453297) A[2]:(-0.00127839972265) A[3]:(0.726584553719)\n",
      " state (11)  A[0]:(0.523539423943) A[1]:(0.876490473747) A[2]:(-0.625134944916) A[3]:(0.843564212322)\n",
      " state (12)  A[0]:(0.0798452124) A[1]:(0.823430776596) A[2]:(-0.619808793068) A[3]:(0.792560219765)\n",
      " state (13)  A[0]:(-0.000738143804483) A[1]:(0.808203577995) A[2]:(0.899988770485) A[3]:(0.72668838501)\n",
      " state (14)  A[0]:(0.809906244278) A[1]:(0.900437891483) A[2]:(0.999999940395) A[3]:(0.808235228062)\n",
      " state (15)  A[0]:(0.982365965843) A[1]:(0.956528663635) A[2]:(1.0) A[3]:(0.874775707722)\n",
      "Episode 817000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 1000.               Steps done: 6066801. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00208672272964.\n",
      " state (0)  A[0]:(0.530653357506) A[1]:(0.59042930603) A[2]:(0.590450704098) A[3]:(0.531304597855)\n",
      " state (1)  A[0]:(0.53098243475) A[1]:(0.000405482918723) A[2]:(0.656056523323) A[3]:(0.590278148651)\n",
      " state (2)  A[0]:(0.590445816517) A[1]:(0.72897785902) A[2]:(0.590489268303) A[3]:(0.656091034412)\n",
      " state (3)  A[0]:(0.656308174133) A[1]:(-0.216134041548) A[2]:(0.539306879044) A[3]:(0.517451405525)\n",
      " state (4)  A[0]:(0.590582251549) A[1]:(0.656012415886) A[2]:(-0.000299215316772) A[3]:(0.531291604042)\n",
      " state (5)  A[0]:(0.16276152432) A[1]:(0.928817749023) A[2]:(-0.195416435599) A[3]:(0.525975108147)\n",
      " state (6)  A[0]:(-0.000349640846252) A[1]:(0.810025930405) A[2]:(0.000134229660034) A[3]:(0.655413508415)\n",
      " state (7)  A[0]:(0.625813007355) A[1]:(-0.248338222504) A[2]:(0.307883918285) A[3]:(0.881879746914)\n",
      " state (8)  A[0]:(0.656028032303) A[1]:(-9.70140099525e-05) A[2]:(0.729017019272) A[3]:(0.590291023254)\n",
      " state (9)  A[0]:(0.655545175076) A[1]:(0.809962570667) A[2]:(0.810041308403) A[3]:(-0.000234231352806)\n",
      " state (10)  A[0]:(0.728719830513) A[1]:(0.900011003017) A[2]:(-0.000323295593262) A[3]:(0.728637754917)\n",
      " state (11)  A[0]:(0.52308511734) A[1]:(0.876488447189) A[2]:(-0.624387741089) A[3]:(0.844797313213)\n",
      " state (12)  A[0]:(0.0796315819025) A[1]:(0.823375880718) A[2]:(-0.619140386581) A[3]:(0.794138133526)\n",
      " state (13)  A[0]:(-0.000851273303851) A[1]:(0.8080316782) A[2]:(0.900056481361) A[3]:(0.728624820709)\n",
      " state (14)  A[0]:(0.809785187244) A[1]:(0.900261342525) A[2]:(0.999999940395) A[3]:(0.809559881687)\n",
      " state (15)  A[0]:(0.982352674007) A[1]:(0.956413209438) A[2]:(1.0) A[3]:(0.875624716282)\n",
      "Episode 818000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 998.               Steps done: 6072807. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00207422743386.\n",
      " state (0)  A[0]:(0.532186031342) A[1]:(0.590465784073) A[2]:(0.590495884418) A[3]:(0.531837582588)\n",
      " state (1)  A[0]:(0.531930088997) A[1]:(-0.000181593000889) A[2]:(0.656068205833) A[3]:(0.590734362602)\n",
      " state (2)  A[0]:(0.590923786163) A[1]:(0.728932976723) A[2]:(0.590651750565) A[3]:(0.656515240669)\n",
      " state (3)  A[0]:(0.656053364277) A[1]:(-0.216850489378) A[2]:(0.5397772789) A[3]:(0.517753601074)\n",
      " state (4)  A[0]:(0.589475393295) A[1]:(0.656074762344) A[2]:(0.000737786176614) A[3]:(0.531633496284)\n",
      " state (5)  A[0]:(0.160158708692) A[1]:(0.928728938103) A[2]:(-0.193821042776) A[3]:(0.526325464249)\n",
      " state (6)  A[0]:(-0.00323920301162) A[1]:(0.809930562973) A[2]:(0.00188600795809) A[3]:(0.655780017376)\n",
      " state (7)  A[0]:(0.625494122505) A[1]:(-0.248498782516) A[2]:(0.309229582548) A[3]:(0.882364809513)\n",
      " state (8)  A[0]:(0.657530784607) A[1]:(-0.000481940776808) A[2]:(0.729041159153) A[3]:(0.593765377998)\n",
      " state (9)  A[0]:(0.657720386982) A[1]:(0.809930026531) A[2]:(0.809994101524) A[3]:(0.00570690399036)\n",
      " state (10)  A[0]:(0.730194747448) A[1]:(0.899957537651) A[2]:(0.000680923345499) A[3]:(0.73061555624)\n",
      " state (11)  A[0]:(0.524916052818) A[1]:(0.876353859901) A[2]:(-0.623520612717) A[3]:(0.845722675323)\n",
      " state (12)  A[0]:(0.081665687263) A[1]:(0.823088526726) A[2]:(-0.618553996086) A[3]:(0.795208215714)\n",
      " state (13)  A[0]:(0.000411629647715) A[1]:(0.807573080063) A[2]:(0.899864196777) A[3]:(0.729879558086)\n",
      " state (14)  A[0]:(0.809860467911) A[1]:(0.899918615818) A[2]:(0.999999940395) A[3]:(0.810393810272)\n",
      " state (15)  A[0]:(0.982360422611) A[1]:(0.956241846085) A[2]:(1.0) A[3]:(0.876214921474)\n",
      "Episode 819000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5998. Times reached goal: 998.               Steps done: 6078805. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00206182345444.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5904,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314, -0.0001,  0.6561,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.7291,  0.5908,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  0.8100,  0.0000,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.8999, -0.0008,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9000,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531384110451) A[1]:(0.590453743935) A[2]:(0.590591430664) A[3]:(0.531468689442)\n",
      " state (1)  A[0]:(0.531458795071) A[1]:(9.27895307541e-05) A[2]:(0.656141161919) A[3]:(0.590449094772)\n",
      " state (2)  A[0]:(0.590789139271) A[1]:(0.728987693787) A[2]:(0.590705394745) A[3]:(0.656289517879)\n",
      " state (3)  A[0]:(0.656248450279) A[1]:(-0.21554492414) A[2]:(0.539632380009) A[3]:(0.51773416996)\n",
      " state (4)  A[0]:(0.590493559837) A[1]:(0.656086087227) A[2]:(5.42402267456e-05) A[3]:(0.531654894352)\n",
      " state (5)  A[0]:(0.162932112813) A[1]:(0.928863048553) A[2]:(-0.195478126407) A[3]:(0.526559174061)\n",
      " state (6)  A[0]:(0.000324308872223) A[1]:(0.810037493706) A[2]:(5.30481338501e-05) A[3]:(0.656057238579)\n",
      " state (7)  A[0]:(0.626327753067) A[1]:(-0.248302847147) A[2]:(0.30797252059) A[3]:(0.882160067558)\n",
      " state (8)  A[0]:(0.656581163406) A[1]:(0.000135734677315) A[2]:(0.728814899921) A[3]:(0.591449081898)\n",
      " state (9)  A[0]:(0.656313657761) A[1]:(0.810033321381) A[2]:(0.809841632843) A[3]:(0.00163192907348)\n",
      " state (10)  A[0]:(0.729356169701) A[1]:(0.899997770786) A[2]:(-0.00102007354144) A[3]:(0.729468524456)\n",
      " state (11)  A[0]:(0.524018168449) A[1]:(0.876374125481) A[2]:(-0.6250218153) A[3]:(0.845315694809)\n",
      " state (12)  A[0]:(0.0808347165585) A[1]:(0.823071241379) A[2]:(-0.619954168797) A[3]:(0.794817984104)\n",
      " state (13)  A[0]:(0.000400125951273) A[1]:(0.80756187439) A[2]:(0.899960517883) A[3]:(0.729488968849)\n",
      " state (14)  A[0]:(0.810291409492) A[1]:(0.899946987629) A[2]:(0.999999940395) A[3]:(0.810164213181)\n",
      " state (15)  A[0]:(0.982381463051) A[1]:(0.956224024296) A[2]:(1.0) A[3]:(0.875947237015)\n",
      "Episode 820000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5996. Times reached goal: 996.               Steps done: 6084801. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00204949775039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531406164169) A[1]:(0.590157747269) A[2]:(0.590459108353) A[3]:(0.531011402607)\n",
      " state (1)  A[0]:(0.530998289585) A[1]:(-0.000346980988979) A[2]:(0.656102538109) A[3]:(0.589605569839)\n",
      " state (2)  A[0]:(0.590148210526) A[1]:(0.729009151459) A[2]:(0.590696811676) A[3]:(0.655392587185)\n",
      " state (3)  A[0]:(0.655626952648) A[1]:(-0.215044558048) A[2]:(0.539397954941) A[3]:(0.516445279121)\n",
      " state (4)  A[0]:(0.58952319622) A[1]:(0.656073689461) A[2]:(-0.000301957130432) A[3]:(0.530196726322)\n",
      " state (5)  A[0]:(0.160971015692) A[1]:(0.928834557533) A[2]:(-0.195875018835) A[3]:(0.524964094162)\n",
      " state (6)  A[0]:(-0.00223946198821) A[1]:(0.809982538223) A[2]:(-0.000366091699107) A[3]:(0.654695510864)\n",
      " state (7)  A[0]:(0.624552965164) A[1]:(-0.248461902142) A[2]:(0.30791208148) A[3]:(0.881544470787)\n",
      " state (8)  A[0]:(0.654750585556) A[1]:(-0.000209502875805) A[2]:(0.728988468647) A[3]:(0.588756084442)\n",
      " state (9)  A[0]:(0.654507875443) A[1]:(0.809977412224) A[2]:(0.809939682484) A[3]:(-0.00346598122269)\n",
      " state (10)  A[0]:(0.727898240089) A[1]:(0.900022029877) A[2]:(-0.00156605115626) A[3]:(0.727426171303)\n",
      " state (11)  A[0]:(0.521563649178) A[1]:(0.87644970417) A[2]:(-0.625746250153) A[3]:(0.844244897366)\n",
      " state (12)  A[0]:(0.077146358788) A[1]:(0.823242485523) A[2]:(-0.620776176453) A[3]:(0.793546319008)\n",
      " state (13)  A[0]:(-0.00321452692151) A[1]:(0.807867884636) A[2]:(0.899937152863) A[3]:(0.728032231331)\n",
      " state (14)  A[0]:(0.809326708317) A[1]:(0.900215685368) A[2]:(0.999999940395) A[3]:(0.809287071228)\n",
      " state (15)  A[0]:(0.982283353806) A[1]:(0.956372261047) A[2]:(1.0) A[3]:(0.875421345234)\n",
      "Episode 821000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 1000.               Steps done: 6090812. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00203721517169.\n",
      " state (0)  A[0]:(0.532014429569) A[1]:(0.590676426888) A[2]:(0.590596556664) A[3]:(0.531936824322)\n",
      " state (1)  A[0]:(0.531594395638) A[1]:(0.000261574983597) A[2]:(0.656201183796) A[3]:(0.590628385544)\n",
      " state (2)  A[0]:(0.590671300888) A[1]:(0.729140877724) A[2]:(0.591067671776) A[3]:(0.656286537647)\n",
      " state (3)  A[0]:(0.656462967396) A[1]:(-0.215179339051) A[2]:(0.540225684643) A[3]:(0.517071485519)\n",
      " state (4)  A[0]:(0.590735077858) A[1]:(0.656150460243) A[2]:(0.00076878053369) A[3]:(0.530778884888)\n",
      " state (5)  A[0]:(0.163117259741) A[1]:(0.92890393734) A[2]:(-0.195185050368) A[3]:(0.525876045227)\n",
      " state (6)  A[0]:(0.000274479389191) A[1]:(0.810022711754) A[2]:(0.000235915184021) A[3]:(0.655847847462)\n",
      " state (7)  A[0]:(0.625427484512) A[1]:(-0.248109206557) A[2]:(0.308255404234) A[3]:(0.882077276707)\n",
      " state (8)  A[0]:(0.6555082798) A[1]:(0.000774204556365) A[2]:(0.729208469391) A[3]:(0.590163350105)\n",
      " state (9)  A[0]:(0.656061172485) A[1]:(0.81020116806) A[2]:(0.810071229935) A[3]:(-0.000669926288538)\n",
      " state (10)  A[0]:(0.729835987091) A[1]:(0.900012969971) A[2]:(-0.00127649237402) A[3]:(0.729155421257)\n",
      " state (11)  A[0]:(0.5254316926) A[1]:(0.876248419285) A[2]:(-0.625585436821) A[3]:(0.845505654812)\n",
      " state (12)  A[0]:(0.0833943486214) A[1]:(0.822644472122) A[2]:(-0.620669364929) A[3]:(0.795392155647)\n",
      " state (13)  A[0]:(0.00373552483507) A[1]:(0.806817293167) A[2]:(0.89999037981) A[3]:(0.730571687222)\n",
      " state (14)  A[0]:(0.811826467514) A[1]:(0.899392127991) A[2]:(0.999999940395) A[3]:(0.81121134758)\n",
      " state (15)  A[0]:(0.982534348965) A[1]:(0.955872774124) A[2]:(1.0) A[3]:(0.876721978188)\n",
      "Episode 822000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 995.               Steps done: 6096812. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0020250284773.\n",
      " state (0)  A[0]:(0.532070398331) A[1]:(0.590494692326) A[2]:(0.590581595898) A[3]:(0.531825661659)\n",
      " state (1)  A[0]:(0.531618595123) A[1]:(-0.00220393040217) A[2]:(0.656133651733) A[3]:(0.590866029263)\n",
      " state (2)  A[0]:(0.590764641762) A[1]:(0.729056239128) A[2]:(0.59200668335) A[3]:(0.656487464905)\n",
      " state (3)  A[0]:(0.65579110384) A[1]:(-0.216445147991) A[2]:(0.541306495667) A[3]:(0.517686963081)\n",
      " state (4)  A[0]:(0.59008538723) A[1]:(0.656501948833) A[2]:(0.00179397861939) A[3]:(0.531680226326)\n",
      " state (5)  A[0]:(0.162830814719) A[1]:(0.92896604538) A[2]:(-0.19422647357) A[3]:(0.526623487473)\n",
      " state (6)  A[0]:(0.000881910091266) A[1]:(0.810011744499) A[2]:(0.00139832403511) A[3]:(0.656127691269)\n",
      " state (7)  A[0]:(0.626526474953) A[1]:(-0.248966053128) A[2]:(0.309594362974) A[3]:(0.882046043873)\n",
      " state (8)  A[0]:(0.656287431717) A[1]:(-0.000588290335145) A[2]:(0.72943353653) A[3]:(0.590558290482)\n",
      " state (9)  A[0]:(0.65594625473) A[1]:(0.809866905212) A[2]:(0.810030996799) A[3]:(-6.56843185425e-05)\n",
      " state (10)  A[0]:(0.729231119156) A[1]:(0.899999260902) A[2]:(-0.00160527089611) A[3]:(0.729214251041)\n",
      " state (11)  A[0]:(0.52399379015) A[1]:(0.876464307308) A[2]:(-0.62596321106) A[3]:(0.845402002335)\n",
      " state (12)  A[0]:(0.0807826071978) A[1]:(0.823321938515) A[2]:(-0.62111890316) A[3]:(0.795018732548)\n",
      " state (13)  A[0]:(0.000337958335876) A[1]:(0.808042764664) A[2]:(0.899997115135) A[3]:(0.729754328728)\n",
      " state (14)  A[0]:(0.810230731964) A[1]:(0.900389790535) A[2]:(0.999999940395) A[3]:(0.810341000557)\n",
      " state (15)  A[0]:(0.982303261757) A[1]:(0.956462800503) A[2]:(1.0) A[3]:(0.875892221928)\n",
      "Episode 823000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6012. Times reached goal: 1000.               Steps done: 6102824. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00201289052933.\n",
      " state (0)  A[0]:(0.531281709671) A[1]:(0.59047460556) A[2]:(0.59046626091) A[3]:(0.531255602837)\n",
      " state (1)  A[0]:(0.531079649925) A[1]:(5.01349568367e-05) A[2]:(0.656099915504) A[3]:(0.590238451958)\n",
      " state (2)  A[0]:(0.590376615524) A[1]:(0.728943586349) A[2]:(0.590879917145) A[3]:(0.656066358089)\n",
      " state (3)  A[0]:(0.656059265137) A[1]:(-0.217050284147) A[2]:(0.540098130703) A[3]:(0.517518520355)\n",
      " state (4)  A[0]:(0.590422511101) A[1]:(0.656022548676) A[2]:(0.000240087509155) A[3]:(0.531643152237)\n",
      " state (5)  A[0]:(0.16285738349) A[1]:(0.928849935532) A[2]:(-0.195585429668) A[3]:(0.526544868946)\n",
      " state (6)  A[0]:(2.90274620056e-05) A[1]:(0.809960186481) A[2]:(-2.7060508728e-05) A[3]:(0.655923962593)\n",
      " state (7)  A[0]:(0.62570142746) A[1]:(-0.248509049416) A[2]:(0.30838984251) A[3]:(0.881966531277)\n",
      " state (8)  A[0]:(0.655897974968) A[1]:(-0.000108890235424) A[2]:(0.729062795639) A[3]:(0.590639948845)\n",
      " state (9)  A[0]:(0.655784606934) A[1]:(0.809968650341) A[2]:(0.810021519661) A[3]:(0.000249639153481)\n",
      " state (10)  A[0]:(0.729023218155) A[1]:(0.899975597858) A[2]:(-0.000913738971576) A[3]:(0.729044437408)\n",
      " state (11)  A[0]:(0.5236068964) A[1]:(0.876339435577) A[2]:(-0.625378251076) A[3]:(0.845170259476)\n",
      " state (12)  A[0]:(0.0802781283855) A[1]:(0.823001623154) A[2]:(-0.620654225349) A[3]:(0.794688165188)\n",
      " state (13)  A[0]:(-0.000213623046875) A[1]:(0.807505846024) A[2]:(0.899992465973) A[3]:(0.729389190674)\n",
      " state (14)  A[0]:(0.810033261776) A[1]:(0.89998626709) A[2]:(0.999999940395) A[3]:(0.81020450592)\n",
      " state (15)  A[0]:(0.982295572758) A[1]:(0.956236362457) A[2]:(1.0) A[3]:(0.875931859016)\n",
      "Episode 824000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6000. Times reached goal: 998.               Steps done: 6108824. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00200084934582.\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.5908,  0.5902,  0.5295]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.6563,  0.0005,  0.5289]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6587, -0.0002,  0.7290,  0.5895]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6589,  0.8100,  0.8100, -0.0006]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7314,  0.9000, -0.0008,  0.7286]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8113,  0.9003,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533166408539) A[1]:(0.590782403946) A[2]:(0.590198516846) A[3]:(0.529772281647)\n",
      " state (1)  A[0]:(0.533537149429) A[1]:(0.000805318180937) A[2]:(0.655940175056) A[3]:(0.588840842247)\n",
      " state (2)  A[0]:(0.592785060406) A[1]:(0.728980302811) A[2]:(0.590517520905) A[3]:(0.654785513878)\n",
      " state (3)  A[0]:(0.657530069351) A[1]:(-0.21775662899) A[2]:(0.540111899376) A[3]:(0.515076994896)\n",
      " state (4)  A[0]:(0.591686725616) A[1]:(0.656201601028) A[2]:(0.000387310952647) A[3]:(0.529049158096)\n",
      " state (5)  A[0]:(0.164542093873) A[1]:(0.928841531277) A[2]:(-0.195250868797) A[3]:(0.523887515068)\n",
      " state (6)  A[0]:(0.00233619986102) A[1]:(0.809941053391) A[2]:(0.00025749206543) A[3]:(0.653870046139)\n",
      " state (7)  A[0]:(0.628285527229) A[1]:(-0.248565331101) A[2]:(0.308488219976) A[3]:(0.881295144558)\n",
      " state (8)  A[0]:(0.659031152725) A[1]:(-0.00024226307869) A[2]:(0.728936672211) A[3]:(0.589555263519)\n",
      " state (9)  A[0]:(0.658972978592) A[1]:(0.809938430786) A[2]:(0.809941411018) A[3]:(-0.000614166201558)\n",
      " state (10)  A[0]:(0.731354594231) A[1]:(0.899962663651) A[2]:(-0.00100266898517) A[3]:(0.728546857834)\n",
      " state (11)  A[0]:(0.526804208755) A[1]:(0.876352131367) A[2]:(-0.625505924225) A[3]:(0.844802975655)\n",
      " state (12)  A[0]:(0.0841969698668) A[1]:(0.823094367981) A[2]:(-0.620897650719) A[3]:(0.794179975986)\n",
      " state (13)  A[0]:(0.00343637308106) A[1]:(0.807728946209) A[2]:(0.899996101856) A[3]:(0.728768229485)\n",
      " state (14)  A[0]:(0.81129527092) A[1]:(0.900204062462) A[2]:(0.999999940395) A[3]:(0.809857606888)\n",
      " state (15)  A[0]:(0.982409238815) A[1]:(0.956369519234) A[2]:(1.0) A[3]:(0.875734210014)\n",
      "Episode 825000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 998.               Steps done: 6114829. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00198887024874.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532156586647) A[1]:(0.590535759926) A[2]:(0.590517759323) A[3]:(0.531231641769)\n",
      " state (1)  A[0]:(0.531774282455) A[1]:(-0.000724322977476) A[2]:(0.656165480614) A[3]:(0.590106368065)\n",
      " state (2)  A[0]:(0.590639770031) A[1]:(0.729061365128) A[2]:(0.591660320759) A[3]:(0.65581703186)\n",
      " state (3)  A[0]:(0.655375242233) A[1]:(-0.217669844627) A[2]:(0.541201114655) A[3]:(0.517233371735)\n",
      " state (4)  A[0]:(0.588928461075) A[1]:(0.656213521957) A[2]:(0.00148761167657) A[3]:(0.531436622143)\n",
      " state (5)  A[0]:(0.160101786256) A[1]:(0.928891599178) A[2]:(-0.194690898061) A[3]:(0.526226758957)\n",
      " state (6)  A[0]:(-0.00264566508122) A[1]:(0.809973478317) A[2]:(0.000676870229654) A[3]:(0.655610561371)\n",
      " state (7)  A[0]:(0.624166846275) A[1]:(-0.248413309455) A[2]:(0.308934479952) A[3]:(0.881761610508)\n",
      " state (8)  A[0]:(0.653969764709) A[1]:(-2.29552388191e-05) A[2]:(0.729120969772) A[3]:(0.589483201504)\n",
      " state (9)  A[0]:(0.653331160545) A[1]:(0.809999465942) A[2]:(0.809934854507) A[3]:(-0.00264137377962)\n",
      " state (10)  A[0]:(0.727023243904) A[1]:(0.90001398325) A[2]:(-0.002303119516) A[3]:(0.727709650993)\n",
      " state (11)  A[0]:(0.520619869232) A[1]:(0.876402258873) A[2]:(-0.626842141151) A[3]:(0.844384431839)\n",
      " state (12)  A[0]:(0.0762912631035) A[1]:(0.823134362698) A[2]:(-0.622262477875) A[3]:(0.793620586395)\n",
      " state (13)  A[0]:(-0.00383840105496) A[1]:(0.807789623737) A[2]:(0.900002062321) A[3]:(0.728029429913)\n",
      " state (14)  A[0]:(0.808927178383) A[1]:(0.900273680687) A[2]:(0.999999940395) A[3]:(0.809314966202)\n",
      " state (15)  A[0]:(0.982123672962) A[1]:(0.956380188465) A[2]:(1.0) A[3]:(0.875257074833)\n",
      "Episode 826000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6003. Times reached goal: 997.               Steps done: 6120832. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00197696682451.\n",
      " state (0)  A[0]:(0.531404614449) A[1]:(0.590394794941) A[2]:(0.590469479561) A[3]:(0.531716942787)\n",
      " state (1)  A[0]:(0.531752884388) A[1]:(1.61305069923e-05) A[2]:(0.656074643135) A[3]:(0.590400993824)\n",
      " state (2)  A[0]:(0.591254532337) A[1]:(0.728976786137) A[2]:(0.59054851532) A[3]:(0.656256914139)\n",
      " state (3)  A[0]:(0.657069325447) A[1]:(-0.216276913881) A[2]:(0.539622843266) A[3]:(0.517966032028)\n",
      " state (4)  A[0]:(0.591539382935) A[1]:(0.656036496162) A[2]:(-0.000110387802124) A[3]:(0.532143890858)\n",
      " state (5)  A[0]:(0.16436521709) A[1]:(0.928806126118) A[2]:(-0.195730119944) A[3]:(0.527090191841)\n",
      " state (6)  A[0]:(0.00158202520106) A[1]:(0.809990167618) A[2]:(-0.000104069709778) A[3]:(0.656200826168)\n",
      " state (7)  A[0]:(0.627102732658) A[1]:(-0.248072683811) A[2]:(0.308534681797) A[3]:(0.882006287575)\n",
      " state (8)  A[0]:(0.657409191132) A[1]:(7.41556286812e-05) A[2]:(0.728950917721) A[3]:(0.591050982475)\n",
      " state (9)  A[0]:(0.65685236454) A[1]:(0.810012996197) A[2]:(0.809982895851) A[3]:(0.000191554427147)\n",
      " state (10)  A[0]:(0.729702115059) A[1]:(0.900003910065) A[2]:(-0.00102782214526) A[3]:(0.728693783283)\n",
      " state (11)  A[0]:(0.524627447128) A[1]:(0.876390397549) A[2]:(-0.62572479248) A[3]:(0.844893217087)\n",
      " state (12)  A[0]:(0.0816491693258) A[1]:(0.823119044304) A[2]:(-0.621292829514) A[3]:(0.794292926788)\n",
      " state (13)  A[0]:(0.000957131094765) A[1]:(0.807730674744) A[2]:(0.899975836277) A[3]:(0.72890740633)\n",
      " state (14)  A[0]:(0.810199916363) A[1]:(0.900207877159) A[2]:(0.999999940395) A[3]:(0.80998313427)\n",
      " state (15)  A[0]:(0.982244670391) A[1]:(0.956354856491) A[2]:(1.0) A[3]:(0.8757827878)\n",
      "Episode 827000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 997.               Steps done: 6126832. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0019651405379.\n",
      " state (0)  A[0]:(0.533041536808) A[1]:(0.590590834618) A[2]:(0.590638995171) A[3]:(0.534379065037)\n",
      " state (1)  A[0]:(0.532460391521) A[1]:(0.00019945204258) A[2]:(0.656218111515) A[3]:(0.592879772186)\n",
      " state (2)  A[0]:(0.591584086418) A[1]:(0.729151248932) A[2]:(0.590857744217) A[3]:(0.658473134041)\n",
      " state (3)  A[0]:(0.65623819828) A[1]:(-0.214408814907) A[2]:(0.539944171906) A[3]:(0.521274447441)\n",
      " state (4)  A[0]:(0.59068274498) A[1]:(0.656267225742) A[2]:(0.000284194946289) A[3]:(0.535416841507)\n",
      " state (5)  A[0]:(0.164619103074) A[1]:(0.928941845894) A[2]:(-0.196008905768) A[3]:(0.530448973179)\n",
      " state (6)  A[0]:(0.00414977082983) A[1]:(0.810035705566) A[2]:(-0.000457763642771) A[3]:(0.658640384674)\n",
      " state (7)  A[0]:(0.629232227802) A[1]:(-0.248382046819) A[2]:(0.308451652527) A[3]:(0.882475554943)\n",
      " state (8)  A[0]:(0.658796310425) A[1]:(0.000282451510429) A[2]:(0.729228854179) A[3]:(0.590588569641)\n",
      " state (9)  A[0]:(0.658253729343) A[1]:(0.810049057007) A[2]:(0.810122847557) A[3]:(-0.00103887880687)\n",
      " state (10)  A[0]:(0.731199026108) A[1]:(0.900032639503) A[2]:(-0.00164246407803) A[3]:(0.728559672832)\n",
      " state (11)  A[0]:(0.527258396149) A[1]:(0.876447141171) A[2]:(-0.626487374306) A[3]:(0.844917356968)\n",
      " state (12)  A[0]:(0.0853710249066) A[1]:(0.823229670525) A[2]:(-0.622248768806) A[3]:(0.794272184372)\n",
      " state (13)  A[0]:(0.00458678370342) A[1]:(0.8079007864) A[2]:(0.899730682373) A[3]:(0.728691220284)\n",
      " state (14)  A[0]:(0.811302185059) A[1]:(0.900343894958) A[2]:(0.999999940395) A[3]:(0.80960381031)\n",
      " state (15)  A[0]:(0.982327461243) A[1]:(0.956428587437) A[2]:(1.0) A[3]:(0.875340104103)\n",
      "Episode 828000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6007. Times reached goal: 998.               Steps done: 6132839. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00195337132292.\n",
      " state (0)  A[0]:(0.531451225281) A[1]:(0.590558588505) A[2]:(0.590406596661) A[3]:(0.531515717506)\n",
      " state (1)  A[0]:(0.531272411346) A[1]:(0.000170730054379) A[2]:(0.656073331833) A[3]:(0.590529859066)\n",
      " state (2)  A[0]:(0.590573012829) A[1]:(0.728997588158) A[2]:(0.59065759182) A[3]:(0.656443715096)\n",
      " state (3)  A[0]:(0.65613591671) A[1]:(-0.216064587235) A[2]:(0.539780497551) A[3]:(0.517668664455)\n",
      " state (4)  A[0]:(0.590413451195) A[1]:(0.655973672867) A[2]:(5.10215759277e-05) A[3]:(0.531723558903)\n",
      " state (5)  A[0]:(0.162958338857) A[1]:(0.928821563721) A[2]:(-0.195816785097) A[3]:(0.526887714863)\n",
      " state (6)  A[0]:(0.000292181968689) A[1]:(0.809992611408) A[2]:(-0.000244379043579) A[3]:(0.656191706657)\n",
      " state (7)  A[0]:(0.626022398472) A[1]:(-0.248180031776) A[2]:(0.308505237103) A[3]:(0.882012426853)\n",
      " state (8)  A[0]:(0.656359314919) A[1]:(2.5562942028e-05) A[2]:(0.72884362936) A[3]:(0.591364622116)\n",
      " state (9)  A[0]:(0.655873537064) A[1]:(0.81001663208) A[2]:(0.80998134613) A[3]:(0.00077277404489)\n",
      " state (10)  A[0]:(0.728935003281) A[1]:(0.899992763996) A[2]:(-0.00111103011295) A[3]:(0.729015946388)\n",
      " state (11)  A[0]:(0.523465335369) A[1]:(0.876344144344) A[2]:(-0.626008450985) A[3]:(0.845131993294)\n",
      " state (12)  A[0]:(0.0800708532333) A[1]:(0.823009848595) A[2]:(-0.621782302856) A[3]:(0.794630169868)\n",
      " state (13)  A[0]:(-0.000380158395274) A[1]:(0.807587265968) A[2]:(0.899985611439) A[3]:(0.729356944561)\n",
      " state (14)  A[0]:(0.810029029846) A[1]:(0.900138795376) A[2]:(0.999999940395) A[3]:(0.810317873955)\n",
      " state (15)  A[0]:(0.982223570347) A[1]:(0.956305623055) A[2]:(1.0) A[3]:(0.875939428806)\n",
      "Episode 829000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 997.               Steps done: 6138845. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00194167453537.\n",
      "q_values \n",
      "tensor([[ 0.5326,  0.5908,  0.5905,  0.5323]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.6561, -0.0001,  0.5318]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.0002,  0.7291,  0.5912]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.8099,  0.8099,  0.0002]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8076,  0.9000,  0.7289]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9002,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532688856125) A[1]:(0.590837240219) A[2]:(0.590574502945) A[3]:(0.53221809864)\n",
      " state (1)  A[0]:(0.532094955444) A[1]:(-3.96594405174e-05) A[2]:(0.656011343002) A[3]:(0.590813815594)\n",
      " state (2)  A[0]:(0.591208696365) A[1]:(0.728945553303) A[2]:(0.590295910835) A[3]:(0.656603693962)\n",
      " state (3)  A[0]:(0.656574726105) A[1]:(-0.215094074607) A[2]:(0.539232611656) A[3]:(0.51794052124)\n",
      " state (4)  A[0]:(0.590815663338) A[1]:(0.656012654305) A[2]:(-0.000414252252085) A[3]:(0.531830310822)\n",
      " state (5)  A[0]:(0.163518369198) A[1]:(0.928806364536) A[2]:(-0.196097284555) A[3]:(0.526926398277)\n",
      " state (6)  A[0]:(0.000625312270131) A[1]:(0.8100233078) A[2]:(-0.000373601884348) A[3]:(0.656220853329)\n",
      " state (7)  A[0]:(0.626208126545) A[1]:(-0.248084202409) A[2]:(0.30865162611) A[3]:(0.882082998753)\n",
      " state (8)  A[0]:(0.656781077385) A[1]:(-0.000129632651806) A[2]:(0.72884708643) A[3]:(0.59152585268)\n",
      " state (9)  A[0]:(0.656444191933) A[1]:(0.809964418411) A[2]:(0.809943556786) A[3]:(0.00020557641983)\n",
      " state (10)  A[0]:(0.729364156723) A[1]:(0.89996099472) A[2]:(-0.00133573927451) A[3]:(0.728608548641)\n",
      " state (11)  A[0]:(0.523952841759) A[1]:(0.876298904419) A[2]:(-0.62626093626) A[3]:(0.844875335693)\n",
      " state (12)  A[0]:(0.0804480314255) A[1]:(0.822944104671) A[2]:(-0.622153759003) A[3]:(0.794299066067)\n",
      " state (13)  A[0]:(-0.000269770622253) A[1]:(0.807527303696) A[2]:(0.899906158447) A[3]:(0.728958129883)\n",
      " state (14)  A[0]:(0.810004770756) A[1]:(0.900128424168) A[2]:(0.999999940395) A[3]:(0.810078680515)\n",
      " state (15)  A[0]:(0.982205331326) A[1]:(0.956304788589) A[2]:(1.0) A[3]:(0.875795781612)\n",
      "Episode 830000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6002. Times reached goal: 999.               Steps done: 6144847. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00193005550838.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530611991882) A[1]:(0.590505361557) A[2]:(0.59052324295) A[3]:(0.531555652618)\n",
      " state (1)  A[0]:(0.529930591583) A[1]:(0.000312998890877) A[2]:(0.656224966049) A[3]:(0.590629458427)\n",
      " state (2)  A[0]:(0.589182972908) A[1]:(0.729099810123) A[2]:(0.590482711792) A[3]:(0.656550228596)\n",
      " state (3)  A[0]:(0.654941916466) A[1]:(-0.214430898428) A[2]:(0.539361417294) A[3]:(0.51799249649)\n",
      " state (4)  A[0]:(0.5889955163) A[1]:(0.655860722065) A[2]:(-7.04526901245e-05) A[3]:(0.531756162643)\n",
      " state (5)  A[0]:(0.16079851985) A[1]:(0.928771555424) A[2]:(-0.195807620883) A[3]:(0.526683330536)\n",
      " state (6)  A[0]:(-0.00244956719689) A[1]:(0.81006705761) A[2]:(-0.000204086303711) A[3]:(0.655547261238)\n",
      " state (7)  A[0]:(0.624297499657) A[1]:(-0.248003318906) A[2]:(0.308807998896) A[3]:(0.88155657053)\n",
      " state (8)  A[0]:(0.655182719231) A[1]:(-0.000646754982881) A[2]:(0.728859782219) A[3]:(0.590566396713)\n",
      " state (9)  A[0]:(0.654890656471) A[1]:(0.809858560562) A[2]:(0.810085892677) A[3]:(-0.000358030170901)\n",
      " state (10)  A[0]:(0.728338122368) A[1]:(0.900027096272) A[2]:(-0.00113809062168) A[3]:(0.728851437569)\n",
      " state (11)  A[0]:(0.522676467896) A[1]:(0.876507997513) A[2]:(-0.626377463341) A[3]:(0.845193505287)\n",
      " state (12)  A[0]:(0.078861117363) A[1]:(0.823397934437) A[2]:(-0.622316837311) A[3]:(0.794708609581)\n",
      " state (13)  A[0]:(-0.00169747904874) A[1]:(0.808212757111) A[2]:(0.900069355965) A[3]:(0.72934281826)\n",
      " state (14)  A[0]:(0.809643149376) A[1]:(0.900629401207) A[2]:(0.999999940395) A[3]:(0.810191214085)\n",
      " state (15)  A[0]:(0.98214328289) A[1]:(0.956563830376) A[2]:(1.0) A[3]:(0.875660836697)\n",
      "Episode 831000 finished after 0 timesteps with r=0.0. Running score: 0.99. Times trained:               6006. Times reached goal: 998.               Steps done: 6150853. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00191849833593.\n",
      " state (0)  A[0]:(0.533302426338) A[1]:(0.58992099762) A[2]:(0.590220689774) A[3]:(0.531380236149)\n",
      " state (1)  A[0]:(0.532486200333) A[1]:(-0.00042254474829) A[2]:(0.65586400032) A[3]:(0.590215444565)\n",
      " state (2)  A[0]:(0.591452479362) A[1]:(0.728849887848) A[2]:(0.590672254562) A[3]:(0.656008601189)\n",
      " state (3)  A[0]:(0.656810522079) A[1]:(-0.217276602983) A[2]:(0.53996014595) A[3]:(0.517244100571)\n",
      " state (4)  A[0]:(0.591372847557) A[1]:(0.655781030655) A[2]:(-3.54051589966e-05) A[3]:(0.531405508518)\n",
      " state (5)  A[0]:(0.164868965745) A[1]:(0.92882579565) A[2]:(-0.196133390069) A[3]:(0.526528000832)\n",
      " state (6)  A[0]:(0.00268559996039) A[1]:(0.809983968735) A[2]:(-0.00032365322113) A[3]:(0.655643105507)\n",
      " state (7)  A[0]:(0.627397298813) A[1]:(-0.24822884798) A[2]:(0.308982133865) A[3]:(0.881572365761)\n",
      " state (8)  A[0]:(0.657363772392) A[1]:(-0.000153310596943) A[2]:(0.728820204735) A[3]:(0.590593516827)\n",
      " state (9)  A[0]:(0.656457841396) A[1]:(0.809968829155) A[2]:(0.809980630875) A[3]:(-4.23640012741e-05)\n",
      " state (10)  A[0]:(0.729236006737) A[1]:(0.900004267693) A[2]:(-0.00112891150638) A[3]:(0.728769779205)\n",
      " state (11)  A[0]:(0.523757696152) A[1]:(0.876387894154) A[2]:(-0.626289963722) A[3]:(0.84505623579)\n",
      " state (12)  A[0]:(0.0801247432828) A[1]:(0.823108077049) A[2]:(-0.622348546982) A[3]:(0.794520497322)\n",
      " state (13)  A[0]:(-0.000619888247456) A[1]:(0.807759284973) A[2]:(0.899987816811) A[3]:(0.729094147682)\n",
      " state (14)  A[0]:(0.809993386269) A[1]:(0.900306344032) A[2]:(0.999999940395) A[3]:(0.809953689575)\n",
      " state (15)  A[0]:(0.982179105282) A[1]:(0.956384122372) A[2]:(1.0) A[3]:(0.875464975834)\n",
      "Episode 832000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5996. Times reached goal: 996.               Steps done: 6156849. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00190702943803.\n",
      " state (0)  A[0]:(0.533198475838) A[1]:(0.591060936451) A[2]:(0.590735793114) A[3]:(0.531648993492)\n",
      " state (1)  A[0]:(0.533018231392) A[1]:(0.000960051722359) A[2]:(0.656299710274) A[3]:(0.590479373932)\n",
      " state (2)  A[0]:(0.592194795609) A[1]:(0.729270815849) A[2]:(0.590510845184) A[3]:(0.656355381012)\n",
      " state (3)  A[0]:(0.657445311546) A[1]:(-0.215190947056) A[2]:(0.539303779602) A[3]:(0.517743468285)\n",
      " state (4)  A[0]:(0.591953217983) A[1]:(0.656491279602) A[2]:(-0.000771641556639) A[3]:(0.53183400631)\n",
      " state (5)  A[0]:(0.165565162897) A[1]:(0.928915739059) A[2]:(-0.196512535214) A[3]:(0.526993155479)\n",
      " state (6)  A[0]:(0.00321083166637) A[1]:(0.810118973255) A[2]:(-0.000397443742258) A[3]:(0.655993938446)\n",
      " state (7)  A[0]:(0.627601027489) A[1]:(-0.247889310122) A[2]:(0.309267967939) A[3]:(0.881649076939)\n",
      " state (8)  A[0]:(0.657735645771) A[1]:(0.000452220410807) A[2]:(0.729132056236) A[3]:(0.590492486954)\n",
      " state (9)  A[0]:(0.657158017159) A[1]:(0.810108065605) A[2]:(0.810114681721) A[3]:(1.51097774506e-05)\n",
      " state (10)  A[0]:(0.729799151421) A[1]:(0.90001553297) A[2]:(-0.000434875459177) A[3]:(0.728652596474)\n",
      " state (11)  A[0]:(0.524572372437) A[1]:(0.876360058784) A[2]:(-0.625714659691) A[3]:(0.844907641411)\n",
      " state (12)  A[0]:(0.0812319815159) A[1]:(0.823023915291) A[2]:(-0.621870458126) A[3]:(0.79433298111)\n",
      " state (13)  A[0]:(0.000485539407237) A[1]:(0.807598948479) A[2]:(0.90003401041) A[3]:(0.728931903839)\n",
      " state (14)  A[0]:(0.810470283031) A[1]:(0.900175154209) A[2]:(0.999999940395) A[3]:(0.809982657433)\n",
      " state (15)  A[0]:(0.982241749763) A[1]:(0.956306278706) A[2]:(1.0) A[3]:(0.875592589378)\n",
      "Episode 833000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 1000.               Steps done: 6162855. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00189561014568.\n",
      " state (0)  A[0]:(0.52957367897) A[1]:(0.590470075607) A[2]:(0.590543210506) A[3]:(0.532288610935)\n",
      " state (1)  A[0]:(0.529696404934) A[1]:(-0.000100448727608) A[2]:(0.656148552895) A[3]:(0.591168224812)\n",
      " state (2)  A[0]:(0.589290440083) A[1]:(0.728909671307) A[2]:(0.590440750122) A[3]:(0.656940937042)\n",
      " state (3)  A[0]:(0.655689656734) A[1]:(-0.214899435639) A[2]:(0.5393897295) A[3]:(0.517488896847)\n",
      " state (4)  A[0]:(0.590397119522) A[1]:(0.655990123749) A[2]:(-0.000454068154795) A[3]:(0.531143248081)\n",
      " state (5)  A[0]:(0.163367077708) A[1]:(0.92883259058) A[2]:(-0.196568578482) A[3]:(0.526559114456)\n",
      " state (6)  A[0]:(-0.000155210494995) A[1]:(0.809975862503) A[2]:(-0.000904440646991) A[3]:(0.655912041664)\n",
      " state (7)  A[0]:(0.623887240887) A[1]:(-0.247962325811) A[2]:(0.308519929647) A[3]:(0.881718695164)\n",
      " state (8)  A[0]:(0.65361058712) A[1]:(0.0010152902687) A[2]:(0.729270994663) A[3]:(0.591413259506)\n",
      " state (9)  A[0]:(0.653192281723) A[1]:(0.810179710388) A[2]:(0.810201466084) A[3]:(0.00564457429573)\n",
      " state (10)  A[0]:(0.726691901684) A[1]:(0.899906873703) A[2]:(9.14335250854e-05) A[3]:(0.732229471207)\n",
      " state (11)  A[0]:(0.520091414452) A[1]:(0.876111268997) A[2]:(-0.625211596489) A[3]:(0.847241461277)\n",
      " state (12)  A[0]:(0.075634598732) A[1]:(0.822545289993) A[2]:(-0.621573209763) A[3]:(0.797477006912)\n",
      " state (13)  A[0]:(-0.00480606453493) A[1]:(0.80691665411) A[2]:(0.899845540524) A[3]:(0.732960343361)\n",
      " state (14)  A[0]:(0.808842301369) A[1]:(0.89970934391) A[2]:(0.999999940395) A[3]:(0.812815725803)\n",
      " state (15)  A[0]:(0.98211479187) A[1]:(0.956070005894) A[2]:(1.0) A[3]:(0.87745141983)\n",
      "Episode 834000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6003. Times reached goal: 996.               Steps done: 6168858. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00188426488485.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5904,  0.5905,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5319, -0.0012,  0.6560,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5910,  0.7290,  0.5911,  0.6566]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0010,  0.8099,  0.0010,  0.6577]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7304,  0.9000,  0.0004,  0.7318]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8106,  0.9001,  1.0000,  0.8115]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531797766685) A[1]:(0.590440273285) A[2]:(0.590494334698) A[3]:(0.531207859516)\n",
      " state (1)  A[0]:(0.531855523586) A[1]:(-0.0012907080818) A[2]:(0.655924201012) A[3]:(0.590425848961)\n",
      " state (2)  A[0]:(0.591011762619) A[1]:(0.729040086269) A[2]:(0.591125011444) A[3]:(0.656423807144)\n",
      " state (3)  A[0]:(0.65619713068) A[1]:(-0.215885281563) A[2]:(0.540765881538) A[3]:(0.518172025681)\n",
      " state (4)  A[0]:(0.590360879898) A[1]:(0.656006217003) A[2]:(0.00159728387371) A[3]:(0.532612025738)\n",
      " state (5)  A[0]:(0.163201913238) A[1]:(0.928834378719) A[2]:(-0.194549709558) A[3]:(0.528214454651)\n",
      " state (6)  A[0]:(0.000934779352974) A[1]:(0.80995541811) A[2]:(0.00105321372394) A[3]:(0.657348334789)\n",
      " state (7)  A[0]:(0.626396775246) A[1]:(-0.247952103615) A[2]:(0.309711098671) A[3]:(0.882405936718)\n",
      " state (8)  A[0]:(0.657555043697) A[1]:(0.0013463787036) A[2]:(0.729067325592) A[3]:(0.59390938282)\n",
      " state (9)  A[0]:(0.657897949219) A[1]:(0.810381531715) A[2]:(0.810021400452) A[3]:(0.00726266857237)\n",
      " state (10)  A[0]:(0.730387926102) A[1]:(0.900029301643) A[2]:(0.00057911867043) A[3]:(0.731642603874)\n",
      " state (11)  A[0]:(0.525462865829) A[1]:(0.876309633255) A[2]:(-0.624718308449) A[3]:(0.846491873264)\n",
      " state (12)  A[0]:(0.0825662761927) A[1]:(0.822946667671) A[2]:(-0.621133327484) A[3]:(0.796295523643)\n",
      " state (13)  A[0]:(0.00156175962184) A[1]:(0.807528376579) A[2]:(0.900021195412) A[3]:(0.731307506561)\n",
      " state (14)  A[0]:(0.810636401176) A[1]:(0.900165855885) A[2]:(0.999999940395) A[3]:(0.811529755592)\n",
      " state (15)  A[0]:(0.982244431973) A[1]:(0.956326305866) A[2]:(1.0) A[3]:(0.876506865025)\n",
      "Episode 835000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 999.               Steps done: 6174860. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00187298939859.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.529617488384) A[1]:(0.588894546032) A[2]:(0.590066671371) A[3]:(0.531410694122)\n",
      " state (1)  A[0]:(0.528973698616) A[1]:(-0.00271119992249) A[2]:(0.655524134636) A[3]:(0.589971661568)\n",
      " state (2)  A[0]:(0.588208794594) A[1]:(0.728005409241) A[2]:(0.590632200241) A[3]:(0.655555307865)\n",
      " state (3)  A[0]:(0.654011368752) A[1]:(-0.216952681541) A[2]:(0.540100336075) A[3]:(0.515715241432)\n",
      " state (4)  A[0]:(0.588564991951) A[1]:(0.653990805149) A[2]:(0.000396132440073) A[3]:(0.52926504612)\n",
      " state (5)  A[0]:(0.161583721638) A[1]:(0.928588926792) A[2]:(-0.196797892451) A[3]:(0.524634003639)\n",
      " state (6)  A[0]:(-0.000575363577809) A[1]:(0.809661030769) A[2]:(-0.0015473352978) A[3]:(0.654056787491)\n",
      " state (7)  A[0]:(0.624883294106) A[1]:(-0.248723417521) A[2]:(0.307729512453) A[3]:(0.880787968636)\n",
      " state (8)  A[0]:(0.655633628368) A[1]:(-0.000795572821517) A[2]:(0.728088617325) A[3]:(0.589033603668)\n",
      " state (9)  A[0]:(0.655513882637) A[1]:(0.809748888016) A[2]:(0.810291051865) A[3]:(-0.00367717281915)\n",
      " state (10)  A[0]:(0.72874724865) A[1]:(0.899952769279) A[2]:(0.00137996580452) A[3]:(0.726611852646)\n",
      " state (11)  A[0]:(0.523322343826) A[1]:(0.876478731632) A[2]:(-0.624805688858) A[3]:(0.843724012375)\n",
      " state (12)  A[0]:(0.0796972066164) A[1]:(0.823522150517) A[2]:(-0.621439933777) A[3]:(0.792792618275)\n",
      " state (13)  A[0]:(-0.00154620280955) A[1]:(0.80856347084) A[2]:(0.900031268597) A[3]:(0.726803660393)\n",
      " state (14)  A[0]:(0.809298574924) A[1]:(0.900997281075) A[2]:(0.999999940395) A[3]:(0.80820453167)\n",
      " state (15)  A[0]:(0.982061505318) A[1]:(0.956815183163) A[2]:(1.0) A[3]:(0.874200463295)\n",
      "Episode 836000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 1000.               Steps done: 6180871. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00186176462916.\n",
      " state (0)  A[0]:(0.531786322594) A[1]:(0.590652883053) A[2]:(0.59054350853) A[3]:(0.531347930431)\n",
      " state (1)  A[0]:(0.53159070015) A[1]:(-8.23810696602e-05) A[2]:(0.656120657921) A[3]:(0.590430557728)\n",
      " state (2)  A[0]:(0.590763986111) A[1]:(0.729001879692) A[2]:(0.590929985046) A[3]:(0.656335055828)\n",
      " state (3)  A[0]:(0.656381428242) A[1]:(-0.217393249273) A[2]:(0.540239572525) A[3]:(0.517203211784)\n",
      " state (4)  A[0]:(0.590806722641) A[1]:(0.656178712845) A[2]:(0.000143647193909) A[3]:(0.531542658806)\n",
      " state (5)  A[0]:(0.163739681244) A[1]:(0.928867220879) A[2]:(-0.195959731936) A[3]:(0.527066648006)\n",
      " state (6)  A[0]:(0.000976621755399) A[1]:(0.809967935085) A[2]:(-0.000142335891724) A[3]:(0.656161785126)\n",
      " state (7)  A[0]:(0.625737309456) A[1]:(-0.24839413166) A[2]:(0.309273898602) A[3]:(0.881641507149)\n",
      " state (8)  A[0]:(0.65600925684) A[1]:(6.7226588726e-05) A[2]:(0.728955864906) A[3]:(0.59060716629)\n",
      " state (9)  A[0]:(0.655598640442) A[1]:(0.809985399246) A[2]:(0.810002446175) A[3]:(0.000333219766617)\n",
      " state (10)  A[0]:(0.72861301899) A[1]:(0.899984955788) A[2]:(-0.000634193362202) A[3]:(0.728814542294)\n",
      " state (11)  A[0]:(0.522882342339) A[1]:(0.876442968845) A[2]:(-0.626076459885) A[3]:(0.845019221306)\n",
      " state (12)  A[0]:(0.0789527520537) A[1]:(0.823376178741) A[2]:(-0.622608423233) A[3]:(0.794469058514)\n",
      " state (13)  A[0]:(-0.00226658186875) A[1]:(0.808291137218) A[2]:(0.89995777607) A[3]:(0.729013860226)\n",
      " state (14)  A[0]:(0.80904853344) A[1]:(0.900758087635) A[2]:(0.999999940395) A[3]:(0.809897124767)\n",
      " state (15)  A[0]:(0.981996536255) A[1]:(0.956636607647) A[2]:(1.0) A[3]:(0.875313878059)\n",
      "Episode 837000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 997.               Steps done: 6186876. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00185061823311.\n",
      " state (0)  A[0]:(0.531327605247) A[1]:(0.590593695641) A[2]:(0.59049487114) A[3]:(0.531779944897)\n",
      " state (1)  A[0]:(0.531253814697) A[1]:(7.58096575737e-05) A[2]:(0.656057834625) A[3]:(0.590524792671)\n",
      " state (2)  A[0]:(0.590509653091) A[1]:(0.729036748409) A[2]:(0.590805768967) A[3]:(0.656274914742)\n",
      " state (3)  A[0]:(0.656251549721) A[1]:(-0.217706292868) A[2]:(0.540199518204) A[3]:(0.516904830933)\n",
      " state (4)  A[0]:(0.590443372726) A[1]:(0.656477987766) A[2]:(5.36441802979e-05) A[3]:(0.531114697456)\n",
      " state (5)  A[0]:(0.162753686309) A[1]:(0.928901672363) A[2]:(-0.195939108729) A[3]:(0.526455998421)\n",
      " state (6)  A[0]:(2.44975090027e-05) A[1]:(0.810042738914) A[2]:(-0.000248312950134) A[3]:(0.655647754669)\n",
      " state (7)  A[0]:(0.625606119633) A[1]:(-0.24797642231) A[2]:(0.308999061584) A[3]:(0.881461441517)\n",
      " state (8)  A[0]:(0.65623831749) A[1]:(0.000694088521414) A[2]:(0.728910207748) A[3]:(0.589704930782)\n",
      " state (9)  A[0]:(0.656156897545) A[1]:(0.810136735439) A[2]:(0.809954762459) A[3]:(-0.00121429504361)\n",
      " state (10)  A[0]:(0.729146361351) A[1]:(0.900007843971) A[2]:(-0.0009908672655) A[3]:(0.728082478046)\n",
      " state (11)  A[0]:(0.523703217506) A[1]:(0.87643712759) A[2]:(-0.62643289566) A[3]:(0.844527363777)\n",
      " state (12)  A[0]:(0.080140851438) A[1]:(0.823361635208) A[2]:(-0.623055756092) A[3]:(0.793797969818)\n",
      " state (13)  A[0]:(-0.000734269502573) A[1]:(0.808310627937) A[2]:(0.899945020676) A[3]:(0.728166639805)\n",
      " state (14)  A[0]:(0.809828877449) A[1]:(0.900809526443) A[2]:(0.999999940395) A[3]:(0.809310853481)\n",
      " state (15)  A[0]:(0.982084035873) A[1]:(0.956667721272) A[2]:(1.0) A[3]:(0.874908328056)\n",
      "Episode 838000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 998.               Steps done: 6192886. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00183952937293.\n",
      " state (0)  A[0]:(0.532321929932) A[1]:(0.590666651726) A[2]:(0.590675234795) A[3]:(0.532407522202)\n",
      " state (1)  A[0]:(0.531988203526) A[1]:(-0.000411704153521) A[2]:(0.656375408173) A[3]:(0.59079182148)\n",
      " state (2)  A[0]:(0.59097224474) A[1]:(0.729169726372) A[2]:(0.591237425804) A[3]:(0.656430482864)\n",
      " state (3)  A[0]:(0.656332731247) A[1]:(-0.216948628426) A[2]:(0.540335834026) A[3]:(0.516771435738)\n",
      " state (4)  A[0]:(0.590031385422) A[1]:(0.656257033348) A[2]:(0.00059235090157) A[3]:(0.530710697174)\n",
      " state (5)  A[0]:(0.161116167903) A[1]:(0.928793609142) A[2]:(-0.195073470473) A[3]:(0.525919795036)\n",
      " state (6)  A[0]:(-0.00405088812113) A[1]:(0.810100317001) A[2]:(0.000601291598286) A[3]:(0.654827952385)\n",
      " state (7)  A[0]:(0.622068047523) A[1]:(-0.248313814402) A[2]:(0.309995174408) A[3]:(0.881122410297)\n",
      " state (8)  A[0]:(0.652689397335) A[1]:(-0.00122751237359) A[2]:(0.729132175446) A[3]:(0.589812278748)\n",
      " state (9)  A[0]:(0.651800394058) A[1]:(0.809711158276) A[2]:(0.810007870197) A[3]:(-0.000587627233472)\n",
      " state (10)  A[0]:(0.725394666195) A[1]:(0.900020956993) A[2]:(-0.00166356412228) A[3]:(0.72900724411)\n",
      " state (11)  A[0]:(0.517767429352) A[1]:(0.876700997353) A[2]:(-0.62729203701) A[3]:(0.845343708992)\n",
      " state (12)  A[0]:(0.0717415586114) A[1]:(0.824058771133) A[2]:(-0.623945713043) A[3]:(0.794883191586)\n",
      " state (13)  A[0]:(-0.00913198199123) A[1]:(0.809467554092) A[2]:(0.900027513504) A[3]:(0.729427337646)\n",
      " state (14)  A[0]:(0.807140469551) A[1]:(0.90168774128) A[2]:(0.999999940395) A[3]:(0.810037493706)\n",
      " state (15)  A[0]:(0.981779813766) A[1]:(0.957144677639) A[2]:(1.0) A[3]:(0.875147342682)\n",
      "Episode 839000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 996.               Steps done: 6198888. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00182852158505.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5905,  0.5904,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.9007e-01,  6.5610e-01, -2.2650e-06,  5.3128e-01]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559, -0.0003,  0.7291,  0.5909]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557,  0.8099,  0.8100,  0.0003]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7288,  0.9000, -0.0010,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8096,  0.9011,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531332314014) A[1]:(0.59044778347) A[2]:(0.590382754803) A[3]:(0.531387686729)\n",
      " state (1)  A[0]:(0.530982017517) A[1]:(0.000176414847374) A[2]:(0.65597987175) A[3]:(0.589904904366)\n",
      " state (2)  A[0]:(0.590165257454) A[1]:(0.728957414627) A[2]:(0.590653538704) A[3]:(0.655800461769)\n",
      " state (3)  A[0]:(0.65601348877) A[1]:(-0.218704640865) A[2]:(0.540042996407) A[3]:(0.516576647758)\n",
      " state (4)  A[0]:(0.590095043182) A[1]:(0.656059920788) A[2]:(-6.53266906738e-05) A[3]:(0.531194448471)\n",
      " state (5)  A[0]:(0.161911040545) A[1]:(0.928795099258) A[2]:(-0.195752814412) A[3]:(0.526865005493)\n",
      " state (6)  A[0]:(-0.00150495651178) A[1]:(0.809968471527) A[2]:(0.000171422958374) A[3]:(0.655955076218)\n",
      " state (7)  A[0]:(0.624771595001) A[1]:(-0.248370751739) A[2]:(0.309691160917) A[3]:(0.881594896317)\n",
      " state (8)  A[0]:(0.655936598778) A[1]:(-0.000524803937878) A[2]:(0.728860974312) A[3]:(0.590979337692)\n",
      " state (9)  A[0]:(0.655755639076) A[1]:(0.809850215912) A[2]:(0.809928417206) A[3]:(0.0002720952034)\n",
      " state (10)  A[0]:(0.728785574436) A[1]:(0.899972558022) A[2]:(-0.000975727743935) A[3]:(0.728903353214)\n",
      " state (11)  A[0]:(0.523165822029) A[1]:(0.876499414444) A[2]:(-0.626611053944) A[3]:(0.845189213753)\n",
      " state (12)  A[0]:(0.0793119892478) A[1]:(0.823565363884) A[2]:(-0.623399555683) A[3]:(0.79475069046)\n",
      " state (13)  A[0]:(-0.00166916695889) A[1]:(0.808645009995) A[2]:(0.899968504906) A[3]:(0.729421317577)\n",
      " state (14)  A[0]:(0.809539735317) A[1]:(0.901051700115) A[2]:(0.999999940395) A[3]:(0.810230672359)\n",
      " state (15)  A[0]:(0.982027351856) A[1]:(0.956782162189) A[2]:(1.0) A[3]:(0.875457763672)\n",
      "Episode 840000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 1000.               Steps done: 6204902. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00181755785721.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531944215298) A[1]:(0.590570688248) A[2]:(0.590524554253) A[3]:(0.530953407288)\n",
      " state (1)  A[0]:(0.531699955463) A[1]:(0.000525876821484) A[2]:(0.65612745285) A[3]:(0.589957952499)\n",
      " state (2)  A[0]:(0.590998530388) A[1]:(0.729041576385) A[2]:(0.590481340885) A[3]:(0.655932784081)\n",
      " state (3)  A[0]:(0.656771302223) A[1]:(-0.21636107564) A[2]:(0.539661943913) A[3]:(0.517063260078)\n",
      " state (4)  A[0]:(0.591319322586) A[1]:(0.656144976616) A[2]:(-0.000248074531555) A[3]:(0.531378090382)\n",
      " state (5)  A[0]:(0.164629369974) A[1]:(0.928848087788) A[2]:(-0.196148991585) A[3]:(0.526894986629)\n",
      " state (6)  A[0]:(0.00193208211567) A[1]:(0.81006616354) A[2]:(-0.000181078910828) A[3]:(0.655817389488)\n",
      " state (7)  A[0]:(0.626564860344) A[1]:(-0.247891440988) A[2]:(0.309658169746) A[3]:(0.881369769573)\n",
      " state (8)  A[0]:(0.657103776932) A[1]:(0.000650510075502) A[2]:(0.729228019714) A[3]:(0.589890480042)\n",
      " state (9)  A[0]:(0.656768143177) A[1]:(0.810109138489) A[2]:(0.810081779957) A[3]:(0.000332772731781)\n",
      " state (10)  A[0]:(0.729582250118) A[1]:(0.899982154369) A[2]:(-0.000328183174133) A[3]:(0.729128956795)\n",
      " state (11)  A[0]:(0.524509429932) A[1]:(0.87639349699) A[2]:(-0.62602943182) A[3]:(0.8452231884)\n",
      " state (12)  A[0]:(0.081401027739) A[1]:(0.823255419731) A[2]:(-0.622927129269) A[3]:(0.794688940048)\n",
      " state (13)  A[0]:(0.000514149607625) A[1]:(0.808085620403) A[2]:(0.899950325489) A[3]:(0.729209423065)\n",
      " state (14)  A[0]:(0.810319721699) A[1]:(0.90059441328) A[2]:(0.999999940395) A[3]:(0.809984028339)\n",
      " state (15)  A[0]:(0.982120513916) A[1]:(0.956511199474) A[2]:(1.0) A[3]:(0.875254511833)\n",
      "Episode 841000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 994.               Steps done: 6210903. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00180668335409.\n",
      " state (0)  A[0]:(0.531820058823) A[1]:(0.590636372566) A[2]:(0.590309739113) A[3]:(0.531536459923)\n",
      " state (1)  A[0]:(0.531423091888) A[1]:(0.000427275866969) A[2]:(0.656078457832) A[3]:(0.590253055096)\n",
      " state (2)  A[0]:(0.590575814247) A[1]:(0.729091703892) A[2]:(0.590716481209) A[3]:(0.656100511551)\n",
      " state (3)  A[0]:(0.656568467617) A[1]:(-0.216391935945) A[2]:(0.539791703224) A[3]:(0.517041802406)\n",
      " state (4)  A[0]:(0.590951025486) A[1]:(0.655706167221) A[2]:(-8.65459442139e-05) A[3]:(0.531304776669)\n",
      " state (5)  A[0]:(0.16362221539) A[1]:(0.928749620914) A[2]:(-0.196092709899) A[3]:(0.527025699615)\n",
      " state (6)  A[0]:(8.30292701721e-05) A[1]:(0.81005603075) A[2]:(-0.000280499458313) A[3]:(0.65607637167)\n",
      " state (7)  A[0]:(0.62525343895) A[1]:(-0.247790828347) A[2]:(0.309633612633) A[3]:(0.881596744061)\n",
      " state (8)  A[0]:(0.656338095665) A[1]:(-0.000118464231491) A[2]:(0.728977262974) A[3]:(0.590735435486)\n",
      " state (9)  A[0]:(0.656198740005) A[1]:(0.809911847115) A[2]:(0.810048103333) A[3]:(-0.000381603807909)\n",
      " state (10)  A[0]:(0.729364037514) A[1]:(0.900005996227) A[2]:(-0.000696778181009) A[3]:(0.728743553162)\n",
      " state (11)  A[0]:(0.524364590645) A[1]:(0.876536488533) A[2]:(-0.626573622227) A[3]:(0.845163822174)\n",
      " state (12)  A[0]:(0.0811668485403) A[1]:(0.823579788208) A[2]:(-0.623527169228) A[3]:(0.794747173786)\n",
      " state (13)  A[0]:(0.000257194042206) A[1]:(0.808575987816) A[2]:(0.900007307529) A[3]:(0.729437112808)\n",
      " state (14)  A[0]:(0.810229539871) A[1]:(0.900939643383) A[2]:(0.999999940395) A[3]:(0.810316622257)\n",
      " state (15)  A[0]:(0.982075572014) A[1]:(0.956677436829) A[2]:(1.0) A[3]:(0.875509500504)\n",
      "Episode 842000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 997.               Steps done: 6216903. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00179587570933.\n",
      " state (0)  A[0]:(0.53127515316) A[1]:(0.59059470892) A[2]:(0.590455293655) A[3]:(0.532466888428)\n",
      " state (1)  A[0]:(0.530687570572) A[1]:(0.00035764274071) A[2]:(0.656190872192) A[3]:(0.59077000618)\n",
      " state (2)  A[0]:(0.589836597443) A[1]:(0.729047060013) A[2]:(0.590893983841) A[3]:(0.656135082245)\n",
      " state (3)  A[0]:(0.655664563179) A[1]:(-0.217069268227) A[2]:(0.54002392292) A[3]:(0.516170740128)\n",
      " state (4)  A[0]:(0.589872598648) A[1]:(0.655929923058) A[2]:(-0.000176429748535) A[3]:(0.529788732529)\n",
      " state (5)  A[0]:(0.162098050117) A[1]:(0.928798854351) A[2]:(-0.196417868137) A[3]:(0.52491736412)\n",
      " state (6)  A[0]:(-0.00122177542653) A[1]:(0.810008704662) A[2]:(-0.000685334089212) A[3]:(0.653958916664)\n",
      " state (7)  A[0]:(0.62429702282) A[1]:(-0.248126938939) A[2]:(0.309329837561) A[3]:(0.880529403687)\n",
      " state (8)  A[0]:(0.655061841011) A[1]:(-0.000212863087654) A[2]:(0.72888559103) A[3]:(0.587630748749)\n",
      " state (9)  A[0]:(0.654832541943) A[1]:(0.809909522533) A[2]:(0.809957742691) A[3]:(-0.00366674200632)\n",
      " state (10)  A[0]:(0.728256046772) A[1]:(0.900026857853) A[2]:(-0.00141739752144) A[3]:(0.727650702)\n",
      " state (11)  A[0]:(0.522614240646) A[1]:(0.876611590385) A[2]:(-0.62725263834) A[3]:(0.844508171082)\n",
      " state (12)  A[0]:(0.07874122262) A[1]:(0.823790788651) A[2]:(-0.62426006794) A[3]:(0.793769180775)\n",
      " state (13)  A[0]:(-0.00189322012011) A[1]:(0.808980643749) A[2]:(0.90002477169) A[3]:(0.727960288525)\n",
      " state (14)  A[0]:(0.809746265411) A[1]:(0.901289820671) A[2]:(0.999999940395) A[3]:(0.808974504471)\n",
      " state (15)  A[0]:(0.982016861439) A[1]:(0.956874132156) A[2]:(1.0) A[3]:(0.874348521233)\n",
      "Episode 843000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 998.               Steps done: 6222909. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00178512200551.\n",
      " state (0)  A[0]:(0.530828118324) A[1]:(0.590333104134) A[2]:(0.590271830559) A[3]:(0.531281352043)\n",
      " state (1)  A[0]:(0.531225502491) A[1]:(-0.000207424163818) A[2]:(0.655883967876) A[3]:(0.590323030949)\n",
      " state (2)  A[0]:(0.590655207634) A[1]:(0.7288377285) A[2]:(0.590735435486) A[3]:(0.656181454659)\n",
      " state (3)  A[0]:(0.656414806843) A[1]:(-0.217869058251) A[2]:(0.54020357132) A[3]:(0.51714193821)\n",
      " state (4)  A[0]:(0.590816259384) A[1]:(0.655786216259) A[2]:(0.000145077705383) A[3]:(0.531520485878)\n",
      " state (5)  A[0]:(0.163651570678) A[1]:(0.928786277771) A[2]:(-0.196082621813) A[3]:(0.527154326439)\n",
      " state (6)  A[0]:(0.000639498175588) A[1]:(0.809899449348) A[2]:(-0.000152945518494) A[3]:(0.655986428261)\n",
      " state (7)  A[0]:(0.625361084938) A[1]:(-0.248413220048) A[2]:(0.309797972441) A[3]:(0.881359279156)\n",
      " state (8)  A[0]:(0.655864417553) A[1]:(-0.000226885080338) A[2]:(0.728699922562) A[3]:(0.590289354324)\n",
      " state (9)  A[0]:(0.655212879181) A[1]:(0.809901118279) A[2]:(0.809771358967) A[3]:(-0.000486254662974)\n",
      " state (10)  A[0]:(0.72821688652) A[1]:(0.899958610535) A[2]:(-0.0013233415084) A[3]:(0.728606700897)\n",
      " state (11)  A[0]:(0.522355675697) A[1]:(0.87642544508) A[2]:(-0.627056479454) A[3]:(0.844986915588)\n",
      " state (12)  A[0]:(0.0783938616514) A[1]:(0.823370814323) A[2]:(-0.62419462204) A[3]:(0.794421195984)\n",
      " state (13)  A[0]:(-0.00213658483699) A[1]:(0.808315575123) A[2]:(0.899921417236) A[3]:(0.728911757469)\n",
      " state (14)  A[0]:(0.809798896313) A[1]:(0.900790452957) A[2]:(0.999999940395) A[3]:(0.809806644917)\n",
      " state (15)  A[0]:(0.982047021389) A[1]:(0.956590175629) A[2]:(1.0) A[3]:(0.875038564205)\n",
      "Episode 844000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6010. Times reached goal: 999.               Steps done: 6228919. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00177442559717.\n",
      "q_values \n",
      "tensor([[ 0.5309,  0.5902,  0.5901,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5900,  0.6561,  0.0003,  0.5313]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.0001,  0.7290,  0.5901]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558,  0.8100,  0.8100, -0.0005]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0007,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8105,  0.9010,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531035721302) A[1]:(0.590174555779) A[2]:(0.590118288994) A[3]:(0.531513094902)\n",
      " state (1)  A[0]:(0.530762672424) A[1]:(0.000415779621108) A[2]:(0.655925393105) A[3]:(0.590399742126)\n",
      " state (2)  A[0]:(0.590103387833) A[1]:(0.728981614113) A[2]:(0.590567827225) A[3]:(0.656255483627)\n",
      " state (3)  A[0]:(0.655937790871) A[1]:(-0.216781586409) A[2]:(0.540070533752) A[3]:(0.51697909832)\n",
      " state (4)  A[0]:(0.590253472328) A[1]:(0.656024336815) A[2]:(0.000285387039185) A[3]:(0.531193733215)\n",
      " state (5)  A[0]:(0.162898227572) A[1]:(0.928831875324) A[2]:(-0.19582504034) A[3]:(0.526861548424)\n",
      " state (6)  A[0]:(6.70552253723e-05) A[1]:(0.809989929199) A[2]:(0.0002681016922) A[3]:(0.655763983727)\n",
      " state (7)  A[0]:(0.62512409687) A[1]:(-0.248113721609) A[2]:(0.310346066952) A[3]:(0.881250023842)\n",
      " state (8)  A[0]:(0.656046152115) A[1]:(0.000109829008579) A[2]:(0.728936314583) A[3]:(0.59014737606)\n",
      " state (9)  A[0]:(0.655953645706) A[1]:(0.809981703758) A[2]:(0.809940576553) A[3]:(-0.000387817592127)\n",
      " state (10)  A[0]:(0.729090929031) A[1]:(0.900004804134) A[2]:(-0.000778436486144) A[3]:(0.728712439537)\n",
      " state (11)  A[0]:(0.523931443691) A[1]:(0.876502335072) A[2]:(-0.626768648624) A[3]:(0.845061004162)\n",
      " state (12)  A[0]:(0.0807459503412) A[1]:(0.823521494865) A[2]:(-0.623967051506) A[3]:(0.794517874718)\n",
      " state (13)  A[0]:(0.00025486946106) A[1]:(0.808537960052) A[2]:(0.900035977364) A[3]:(0.72902905941)\n",
      " state (14)  A[0]:(0.81048810482) A[1]:(0.900948941708) A[2]:(0.999999940395) A[3]:(0.809877574444)\n",
      " state (15)  A[0]:(0.982094347477) A[1]:(0.956673979759) A[2]:(1.0) A[3]:(0.87505787611)\n",
      "Episode 845000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 997.               Steps done: 6234919. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00176381091946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531062960625) A[1]:(0.590480685234) A[2]:(0.590461969376) A[3]:(0.531408309937)\n",
      " state (1)  A[0]:(0.530844986439) A[1]:(-1.55195593834e-05) A[2]:(0.656042575836) A[3]:(0.59043097496)\n",
      " state (2)  A[0]:(0.590171456337) A[1]:(0.729046344757) A[2]:(0.590702414513) A[3]:(0.656347513199)\n",
      " state (3)  A[0]:(0.656133234501) A[1]:(-0.216618269682) A[2]:(0.539805054665) A[3]:(0.517013311386)\n",
      " state (4)  A[0]:(0.590335845947) A[1]:(0.65614849329) A[2]:(-0.000243902206421) A[3]:(0.531255245209)\n",
      " state (5)  A[0]:(0.162573635578) A[1]:(0.928812086582) A[2]:(-0.196183025837) A[3]:(0.527012109756)\n",
      " state (6)  A[0]:(-0.000642835977487) A[1]:(0.810034155846) A[2]:(-2.3365020752e-05) A[3]:(0.655922532082)\n",
      " state (7)  A[0]:(0.6250218153) A[1]:(-0.24788184464) A[2]:(0.310385078192) A[3]:(0.88136279583)\n",
      " state (8)  A[0]:(0.656252682209) A[1]:(0.000157155096531) A[2]:(0.729084908962) A[3]:(0.590464115143)\n",
      " state (9)  A[0]:(0.656145572662) A[1]:(0.810001552105) A[2]:(0.810056686401) A[3]:(-1.14738941193e-06)\n",
      " state (10)  A[0]:(0.729098200798) A[1]:(0.900012612343) A[2]:(-0.000444292993052) A[3]:(0.728984832764)\n",
      " state (11)  A[0]:(0.523678958416) A[1]:(0.876508951187) A[2]:(-0.62663424015) A[3]:(0.845279574394)\n",
      " state (12)  A[0]:(0.0800488814712) A[1]:(0.823527991772) A[2]:(-0.623963057995) A[3]:(0.794860780239)\n",
      " state (13)  A[0]:(-0.000858723884448) A[1]:(0.808535158634) A[2]:(0.900023460388) A[3]:(0.729521036148)\n",
      " state (14)  A[0]:(0.809923887253) A[1]:(0.900937795639) A[2]:(0.999999940395) A[3]:(0.810241937637)\n",
      " state (15)  A[0]:(0.98201417923) A[1]:(0.956661880016) A[2]:(1.0) A[3]:(0.875299870968)\n",
      "Episode 846000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 998.               Steps done: 6240919. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00175325973914.\n",
      " state (0)  A[0]:(0.531088769436) A[1]:(0.59056854248) A[2]:(0.590497374535) A[3]:(0.531111836433)\n",
      " state (1)  A[0]:(0.530836105347) A[1]:(0.000155888497829) A[2]:(0.656191825867) A[3]:(0.590001285076)\n",
      " state (2)  A[0]:(0.5900375247) A[1]:(0.729078531265) A[2]:(0.591054201126) A[3]:(0.655768632889)\n",
      " state (3)  A[0]:(0.655870199203) A[1]:(-0.216983571649) A[2]:(0.540393590927) A[3]:(0.516033887863)\n",
      " state (4)  A[0]:(0.589944958687) A[1]:(0.656157016754) A[2]:(0.000418662995799) A[3]:(0.530156970024)\n",
      " state (5)  A[0]:(0.161903783679) A[1]:(0.928834795952) A[2]:(-0.195778608322) A[3]:(0.525764644146)\n",
      " state (6)  A[0]:(-0.00167107430752) A[1]:(0.810025453568) A[2]:(0.000308990478516) A[3]:(0.65472650528)\n",
      " state (7)  A[0]:(0.623799860477) A[1]:(-0.248109906912) A[2]:(0.310662537813) A[3]:(0.880732655525)\n",
      " state (8)  A[0]:(0.654855012894) A[1]:(-4.992634058e-05) A[2]:(0.729042172432) A[3]:(0.588458716869)\n",
      " state (9)  A[0]:(0.654688596725) A[1]:(0.809958338737) A[2]:(0.809971034527) A[3]:(-0.00353406392969)\n",
      " state (10)  A[0]:(0.727945804596) A[1]:(0.90000641346) A[2]:(-0.000770568673033) A[3]:(0.727110862732)\n",
      " state (11)  A[0]:(0.522038221359) A[1]:(0.876531600952) A[2]:(-0.626929402351) A[3]:(0.844059824944)\n",
      " state (12)  A[0]:(0.0779829695821) A[1]:(0.823619782925) A[2]:(-0.624245047569) A[3]:(0.793203949928)\n",
      " state (13)  A[0]:(-0.00279872887768) A[1]:(0.808733463287) A[2]:(0.900181293488) A[3]:(0.727342247963)\n",
      " state (14)  A[0]:(0.809182345867) A[1]:(0.901114583015) A[2]:(0.999999940395) A[3]:(0.808620810509)\n",
      " state (15)  A[0]:(0.981890499592) A[1]:(0.956748962402) A[2]:(1.0) A[3]:(0.874100446701)\n",
      "Episode 847000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 1000.               Steps done: 6246933. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00174274727772.\n",
      " state (0)  A[0]:(0.531399130821) A[1]:(0.590490162373) A[2]:(0.590455591679) A[3]:(0.531284332275)\n",
      " state (1)  A[0]:(0.531665563583) A[1]:(0.00019783526659) A[2]:(0.656054854393) A[3]:(0.590456068516)\n",
      " state (2)  A[0]:(0.591226398945) A[1]:(0.729029238224) A[2]:(0.590791463852) A[3]:(0.656435191631)\n",
      " state (3)  A[0]:(0.657582998276) A[1]:(-0.217557132244) A[2]:(0.540049433708) A[3]:(0.51778024435)\n",
      " state (4)  A[0]:(0.592547416687) A[1]:(0.656409025192) A[2]:(-0.000490665377583) A[3]:(0.532448530197)\n",
      " state (5)  A[0]:(0.166471853852) A[1]:(0.928912699223) A[2]:(-0.196846485138) A[3]:(0.528363883495)\n",
      " state (6)  A[0]:(0.00385383842513) A[1]:(0.809965372086) A[2]:(-0.000618576945271) A[3]:(0.657093584538)\n",
      " state (7)  A[0]:(0.626885533333) A[1]:(-0.248474106193) A[2]:(0.310102373362) A[3]:(0.881663799286)\n",
      " state (8)  A[0]:(0.656654536724) A[1]:(-7.53924250603e-05) A[2]:(0.728922605515) A[3]:(0.590843915939)\n",
      " state (9)  A[0]:(0.655813395977) A[1]:(0.809926390648) A[2]:(0.809877455235) A[3]:(0.000726133468561)\n",
      " state (10)  A[0]:(0.728831589222) A[1]:(0.899994134903) A[2]:(-0.0017133934889) A[3]:(0.729687690735)\n",
      " state (11)  A[0]:(0.523466169834) A[1]:(0.87653028965) A[2]:(-0.627799332142) A[3]:(0.845820486546)\n",
      " state (12)  A[0]:(0.0799664407969) A[1]:(0.823644340038) A[2]:(-0.625244498253) A[3]:(0.795558214188)\n",
      " state (13)  A[0]:(-0.000643014791422) A[1]:(0.80880689621) A[2]:(0.899970769882) A[3]:(0.730382800102)\n",
      " state (14)  A[0]:(0.810101151466) A[1]:(0.901191651821) A[2]:(0.999999940395) A[3]:(0.81091016531)\n",
      " state (15)  A[0]:(0.981984615326) A[1]:(0.956789255142) A[2]:(1.0) A[3]:(0.875648736954)\n",
      "Episode 848000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 1000.               Steps done: 6252939. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00173231170696.\n",
      " state (0)  A[0]:(0.531210482121) A[1]:(0.59011054039) A[2]:(0.590368032455) A[3]:(0.530331969261)\n",
      " state (1)  A[0]:(0.531006336212) A[1]:(-0.000181347131729) A[2]:(0.655975759029) A[3]:(0.588987350464)\n",
      " state (2)  A[0]:(0.590265214443) A[1]:(0.728930830956) A[2]:(0.590824961662) A[3]:(0.65490436554)\n",
      " state (3)  A[0]:(0.656178355217) A[1]:(-0.217203631997) A[2]:(0.540075063705) A[3]:(0.516561985016)\n",
      " state (4)  A[0]:(0.590517282486) A[1]:(0.655672609806) A[2]:(-8.90493392944e-05) A[3]:(0.531301498413)\n",
      " state (5)  A[0]:(0.163197204471) A[1]:(0.928772687912) A[2]:(-0.196511507034) A[3]:(0.527080416679)\n",
      " state (6)  A[0]:(-2.49147415161e-05) A[1]:(0.809952020645) A[2]:(-0.000532150210347) A[3]:(0.655651688576)\n",
      " state (7)  A[0]:(0.624820828438) A[1]:(-0.2481944561) A[2]:(0.309967786074) A[3]:(0.88095676899)\n",
      " state (8)  A[0]:(0.655844330788) A[1]:(-0.0001460313797) A[2]:(0.728632211685) A[3]:(0.589556872845)\n",
      " state (9)  A[0]:(0.655687749386) A[1]:(0.809957325459) A[2]:(0.809791088104) A[3]:(-0.00114162219688)\n",
      " state (10)  A[0]:(0.728993952274) A[1]:(0.900008380413) A[2]:(-0.00123977602925) A[3]:(0.728527724743)\n",
      " state (11)  A[0]:(0.524068951607) A[1]:(0.876509130001) A[2]:(-0.627369761467) A[3]:(0.845044374466)\n",
      " state (12)  A[0]:(0.0811378881335) A[1]:(0.823528409004) A[2]:(-0.624928951263) A[3]:(0.794530034065)\n",
      " state (13)  A[0]:(0.000449776620371) A[1]:(0.808538019657) A[2]:(0.899939894676) A[3]:(0.72900146246)\n",
      " state (14)  A[0]:(0.810234606266) A[1]:(0.900941729546) A[2]:(0.999999940395) A[3]:(0.809766888618)\n",
      " state (15)  A[0]:(0.981979012489) A[1]:(0.956631779671) A[2]:(1.0) A[3]:(0.87479698658)\n",
      "Episode 849000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 999.               Steps done: 6258945. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0017219386244.\n",
      "q_values \n",
      "tensor([[ 0.5324,  0.5906,  0.5905,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6565,  0.0002,  0.5334]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.0004,  0.7289,  0.5946]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6570,  0.8101,  0.8101,  0.0061]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7302,  0.9000,  0.0009,  0.7315]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8107,  0.9002,  1.0000,  0.8121]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532528340816) A[1]:(0.590644836426) A[2]:(0.590577125549) A[3]:(0.531112790108)\n",
      " state (1)  A[0]:(0.532056331635) A[1]:(3.76477837563e-05) A[2]:(0.65611243248) A[3]:(0.591112136841)\n",
      " state (2)  A[0]:(0.591052353382) A[1]:(0.729114234447) A[2]:(0.590980052948) A[3]:(0.657351613045)\n",
      " state (3)  A[0]:(0.656662940979) A[1]:(-0.21764844656) A[2]:(0.540235161781) A[3]:(0.51964533329)\n",
      " state (4)  A[0]:(0.590662479401) A[1]:(0.656391084194) A[2]:(0.000128746032715) A[3]:(0.534683227539)\n",
      " state (5)  A[0]:(0.162817001343) A[1]:(0.928835511208) A[2]:(-0.195670038462) A[3]:(0.530835032463)\n",
      " state (6)  A[0]:(-0.000544905604329) A[1]:(0.810009539127) A[2]:(0.000718235853128) A[3]:(0.659345269203)\n",
      " state (7)  A[0]:(0.624972999096) A[1]:(-0.24792265892) A[2]:(0.311075210571) A[3]:(0.882976531982)\n",
      " state (8)  A[0]:(0.656952679157) A[1]:(0.000487811834319) A[2]:(0.728832542896) A[3]:(0.596386015415)\n",
      " state (9)  A[0]:(0.657334625721) A[1]:(0.810128748417) A[2]:(0.809880316257) A[3]:(0.0085913958028)\n",
      " state (10)  A[0]:(0.730121254921) A[1]:(0.899967193604) A[2]:(0.000253915786743) A[3]:(0.73207950592)\n",
      " state (11)  A[0]:(0.525571227074) A[1]:(0.876299500465) A[2]:(-0.626057505608) A[3]:(0.8469196558)\n",
      " state (12)  A[0]:(0.083050146699) A[1]:(0.823004961014) A[2]:(-0.623832821846) A[3]:(0.796906113625)\n",
      " state (13)  A[0]:(0.00185107975267) A[1]:(0.807642698288) A[2]:(0.899860501289) A[3]:(0.732084035873)\n",
      " state (14)  A[0]:(0.810428857803) A[1]:(0.90023124218) A[2]:(0.999999940395) A[3]:(0.812143564224)\n",
      " state (15)  A[0]:(0.982011914253) A[1]:(0.956234753132) A[2]:(1.0) A[3]:(0.876590073109)\n",
      "Episode 850000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 998.               Steps done: 6264947. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00171163450238.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530892014503) A[1]:(0.590497851372) A[2]:(0.59045290947) A[3]:(0.531515240669)\n",
      " state (1)  A[0]:(0.529811024666) A[1]:(-4.62904572487e-05) A[2]:(0.656029343605) A[3]:(0.59038579464)\n",
      " state (2)  A[0]:(0.588540792465) A[1]:(0.728953003883) A[2]:(0.590816318989) A[3]:(0.656018733978)\n",
      " state (3)  A[0]:(0.653680086136) A[1]:(-0.21802932024) A[2]:(0.540115654469) A[3]:(0.516696810722)\n",
      " state (4)  A[0]:(0.586748957634) A[1]:(0.656204342842) A[2]:(-0.000201940536499) A[3]:(0.530823171139)\n",
      " state (5)  A[0]:(0.156495884061) A[1]:(0.928829908371) A[2]:(-0.196249961853) A[3]:(0.52617585659)\n",
      " state (6)  A[0]:(-0.00775417033583) A[1]:(0.809975862503) A[2]:(0.0001460313797) A[3]:(0.654793381691)\n",
      " state (7)  A[0]:(0.619677662849) A[1]:(-0.24833330512) A[2]:(0.311057001352) A[3]:(0.880611419678)\n",
      " state (8)  A[0]:(0.650735735893) A[1]:(-0.00036463883589) A[2]:(0.72910797596) A[3]:(0.587978243828)\n",
      " state (9)  A[0]:(0.650330305099) A[1]:(0.809944868088) A[2]:(0.810004413128) A[3]:(-0.00494468631223)\n",
      " state (10)  A[0]:(0.724353969097) A[1]:(0.900062680244) A[2]:(-0.00110852671787) A[3]:(0.72651052475)\n",
      " state (11)  A[0]:(0.51649838686) A[1]:(0.876654088497) A[2]:(-0.627539873123) A[3]:(0.843743860722)\n",
      " state (12)  A[0]:(0.0703667849302) A[1]:(0.823858141899) A[2]:(-0.625134766102) A[3]:(0.792771458626)\n",
      " state (13)  A[0]:(-0.0104429498315) A[1]:(0.809071421623) A[2]:(0.900158286095) A[3]:(0.726771056652)\n",
      " state (14)  A[0]:(0.806575655937) A[1]:(0.901339948177) A[2]:(1.0) A[3]:(0.80825227499)\n",
      " state (15)  A[0]:(0.981573224068) A[1]:(0.956835746765) A[2]:(1.0) A[3]:(0.873785912991)\n",
      "Episode 851000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 998.               Steps done: 6270949. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00170139204047.\n",
      " state (0)  A[0]:(0.53145647049) A[1]:(0.59042096138) A[2]:(0.590410709381) A[3]:(0.530966877937)\n",
      " state (1)  A[0]:(0.531262338161) A[1]:(3.95774841309e-05) A[2]:(0.656049013138) A[3]:(0.589984059334)\n",
      " state (2)  A[0]:(0.590480387211) A[1]:(0.728951692581) A[2]:(0.59078335762) A[3]:(0.655690789223)\n",
      " state (3)  A[0]:(0.656402349472) A[1]:(-0.217335551977) A[2]:(0.540047824383) A[3]:(0.516139268875)\n",
      " state (4)  A[0]:(0.590670585632) A[1]:(0.655964136124) A[2]:(-0.000182032585144) A[3]:(0.530205130577)\n",
      " state (5)  A[0]:(0.163240253925) A[1]:(0.9287956357) A[2]:(-0.196429565549) A[3]:(0.525788784027)\n",
      " state (6)  A[0]:(8.0943107605e-05) A[1]:(0.809938073158) A[2]:(-0.000191330909729) A[3]:(0.654663205147)\n",
      " state (7)  A[0]:(0.625128388405) A[1]:(-0.248393788934) A[2]:(0.310649305582) A[3]:(0.880634605885)\n",
      " state (8)  A[0]:(0.656174898148) A[1]:(-0.000349342823029) A[2]:(0.728946685791) A[3]:(0.588607192039)\n",
      " state (9)  A[0]:(0.655811309814) A[1]:(0.809898853302) A[2]:(0.80999737978) A[3]:(-0.0030737570487)\n",
      " state (10)  A[0]:(0.728884339333) A[1]:(0.899983048439) A[2]:(-0.00051021570107) A[3]:(0.727459907532)\n",
      " state (11)  A[0]:(0.523711621761) A[1]:(0.87651014328) A[2]:(-0.626977205276) A[3]:(0.844364404678)\n",
      " state (12)  A[0]:(0.0804935693741) A[1]:(0.823590636253) A[2]:(-0.624679803848) A[3]:(0.793657958508)\n",
      " state (13)  A[0]:(-0.000239133834839) A[1]:(0.808664321899) A[2]:(0.900056362152) A[3]:(0.727935016155)\n",
      " state (14)  A[0]:(0.81010222435) A[1]:(0.901028215885) A[2]:(1.0) A[3]:(0.809070527554)\n",
      " state (15)  A[0]:(0.981960594654) A[1]:(0.956668138504) A[2]:(1.0) A[3]:(0.87434566021)\n",
      "Episode 852000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6013. Times reached goal: 1000.               Steps done: 6276962. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00169119226648.\n",
      " state (0)  A[0]:(0.531420707703) A[1]:(0.590510725975) A[2]:(0.590517044067) A[3]:(0.531535208225)\n",
      " state (1)  A[0]:(0.531267166138) A[1]:(-1.22040510178e-05) A[2]:(0.656091153622) A[3]:(0.590490937233)\n",
      " state (2)  A[0]:(0.590357065201) A[1]:(0.728971898556) A[2]:(0.590944945812) A[3]:(0.656185030937)\n",
      " state (3)  A[0]:(0.655897438526) A[1]:(-0.217709839344) A[2]:(0.540228843689) A[3]:(0.517082929611)\n",
      " state (4)  A[0]:(0.589818835258) A[1]:(0.656097531319) A[2]:(-7.06911087036e-05) A[3]:(0.531418085098)\n",
      " state (5)  A[0]:(0.161769524217) A[1]:(0.928831875324) A[2]:(-0.196295246482) A[3]:(0.52721118927)\n",
      " state (6)  A[0]:(-0.00143766298424) A[1]:(0.8100233078) A[2]:(-2.32458114624e-05) A[3]:(0.6559445858)\n",
      " state (7)  A[0]:(0.624262154102) A[1]:(-0.248106643558) A[2]:(0.310825169086) A[3]:(0.881164491177)\n",
      " state (8)  A[0]:(0.655343651772) A[1]:(0.000134870409966) A[2]:(0.729095160961) A[3]:(0.589920222759)\n",
      " state (9)  A[0]:(0.654966235161) A[1]:(0.81004524231) A[2]:(0.810050189495) A[3]:(-0.00111228181049)\n",
      " state (10)  A[0]:(0.728114187717) A[1]:(0.900035738945) A[2]:(-0.000284433364868) A[3]:(0.728347539902)\n",
      " state (11)  A[0]:(0.522401690483) A[1]:(0.876546680927) A[2]:(-0.626756727695) A[3]:(0.844883680344)\n",
      " state (12)  A[0]:(0.0786372646689) A[1]:(0.823602557182) A[2]:(-0.624314665794) A[3]:(0.794345676899)\n",
      " state (13)  A[0]:(-0.00200557429343) A[1]:(0.808635115623) A[2]:(0.900365173817) A[3]:(0.728877067566)\n",
      " state (14)  A[0]:(0.809527635574) A[1]:(0.900977134705) A[2]:(1.0) A[3]:(0.809809565544)\n",
      " state (15)  A[0]:(0.981882512569) A[1]:(0.956611514091) A[2]:(1.0) A[3]:(0.874839246273)\n",
      "Episode 853000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 1000.               Steps done: 6282970. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.001681062045.\n",
      " state (0)  A[0]:(0.531449079514) A[1]:(0.590300440788) A[2]:(0.590476632118) A[3]:(0.531747341156)\n",
      " state (1)  A[0]:(0.531248390675) A[1]:(-0.000104874372482) A[2]:(0.656075716019) A[3]:(0.590523004532)\n",
      " state (2)  A[0]:(0.590412795544) A[1]:(0.728970468044) A[2]:(0.590885639191) A[3]:(0.656174719334)\n",
      " state (3)  A[0]:(0.656274557114) A[1]:(-0.216610923409) A[2]:(0.540088534355) A[3]:(0.517294704914)\n",
      " state (4)  A[0]:(0.590556621552) A[1]:(0.656208574772) A[2]:(-0.000147819519043) A[3]:(0.531639158726)\n",
      " state (5)  A[0]:(0.163306921721) A[1]:(0.928885817528) A[2]:(-0.196509107947) A[3]:(0.527557253838)\n",
      " state (6)  A[0]:(0.000644087675028) A[1]:(0.810027718544) A[2]:(-0.000171899795532) A[3]:(0.656317472458)\n",
      " state (7)  A[0]:(0.62536239624) A[1]:(-0.248031467199) A[2]:(0.310632288456) A[3]:(0.881270170212)\n",
      " state (8)  A[0]:(0.656301856041) A[1]:(0.000733137014322) A[2]:(0.728980720043) A[3]:(0.590408205986)\n",
      " state (9)  A[0]:(0.656181693077) A[1]:(0.810182392597) A[2]:(0.810032010078) A[3]:(0.000359937519534)\n",
      " state (10)  A[0]:(0.729122698307) A[1]:(0.899996638298) A[2]:(0.00017237663269) A[3]:(0.728986680508)\n",
      " state (11)  A[0]:(0.523963212967) A[1]:(0.876341402531) A[2]:(-0.626319885254) A[3]:(0.845224440098)\n",
      " state (12)  A[0]:(0.0808035135269) A[1]:(0.82306522131) A[2]:(-0.624115109444) A[3]:(0.794775485992)\n",
      " state (13)  A[0]:(-6.95586204529e-05) A[1]:(0.807698011398) A[2]:(0.899997115135) A[3]:(0.729341268539)\n",
      " state (14)  A[0]:(0.810106039047) A[1]:(0.900223731995) A[2]:(1.0) A[3]:(0.809973597527)\n",
      " state (15)  A[0]:(0.981983304024) A[1]:(0.956190764904) A[2]:(1.0) A[3]:(0.874941825867)\n",
      "Episode 854000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 998.               Steps done: 6288979. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00167099083244.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5905,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5308, -0.0001,  0.6560,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5897,  0.7290,  0.5907,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0017,  0.8101, -0.0002,  0.6558]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7277,  0.8999, -0.0007,  0.7282]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8092,  0.9008,  1.0000,  0.8096]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532893538475) A[1]:(0.590479373932) A[2]:(0.59048306942) A[3]:(0.531333327293)\n",
      " state (1)  A[0]:(0.532569527626) A[1]:(1.560151577e-05) A[2]:(0.656087994576) A[3]:(0.590375542641)\n",
      " state (2)  A[0]:(0.591600239277) A[1]:(0.728960156441) A[2]:(0.590757250786) A[3]:(0.656099617481)\n",
      " state (3)  A[0]:(0.657299280167) A[1]:(-0.216551795602) A[2]:(0.539870798588) A[3]:(0.517395079136)\n",
      " state (4)  A[0]:(0.591739118099) A[1]:(0.656141757965) A[2]:(-0.000424146623118) A[3]:(0.531696796417)\n",
      " state (5)  A[0]:(0.165030449629) A[1]:(0.928865134716) A[2]:(-0.196702778339) A[3]:(0.527483284473)\n",
      " state (6)  A[0]:(0.00203990656883) A[1]:(0.81000328064) A[2]:(-0.000261545181274) A[3]:(0.65614759922)\n",
      " state (7)  A[0]:(0.625861167908) A[1]:(-0.248375102878) A[2]:(0.310734152794) A[3]:(0.881188154221)\n",
      " state (8)  A[0]:(0.656335473061) A[1]:(-0.000148847699165) A[2]:(0.72890162468) A[3]:(0.590190052986)\n",
      " state (9)  A[0]:(0.655610918999) A[1]:(0.809999227524) A[2]:(0.810033619404) A[3]:(-0.000942319340538)\n",
      " state (10)  A[0]:(0.728535234928) A[1]:(0.900047123432) A[2]:(-0.000151753425598) A[3]:(0.728475928307)\n",
      " state (11)  A[0]:(0.52297604084) A[1]:(0.876562952995) A[2]:(-0.626727342606) A[3]:(0.845002472401)\n",
      " state (12)  A[0]:(0.0792312026024) A[1]:(0.823584914207) A[2]:(-0.624498009682) A[3]:(0.794466912746)\n",
      " state (13)  A[0]:(-0.00184106617235) A[1]:(0.80850148201) A[2]:(0.9000415802) A[3]:(0.728899359703)\n",
      " state (14)  A[0]:(0.809425354004) A[1]:(0.900796771049) A[2]:(1.0) A[3]:(0.809679150581)\n",
      " state (15)  A[0]:(0.981884300709) A[1]:(0.956498026848) A[2]:(1.0) A[3]:(0.874714076519)\n",
      "Episode 855000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 999.               Steps done: 6294987. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00166098161731.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531342983246) A[1]:(0.590463638306) A[2]:(0.590492129326) A[3]:(0.531515836716)\n",
      " state (1)  A[0]:(0.53131878376) A[1]:(8.18073749542e-06) A[2]:(0.656087815762) A[3]:(0.590407252312)\n",
      " state (2)  A[0]:(0.590491533279) A[1]:(0.72896707058) A[2]:(0.590792894363) A[3]:(0.656078398228)\n",
      " state (3)  A[0]:(0.656162977219) A[1]:(-0.216413423419) A[2]:(0.539940714836) A[3]:(0.517197191715)\n",
      " state (4)  A[0]:(0.59045279026) A[1]:(0.656038284302) A[2]:(-0.000352501840098) A[3]:(0.531530916691)\n",
      " state (5)  A[0]:(0.163340568542) A[1]:(0.928869187832) A[2]:(-0.196767866611) A[3]:(0.527577221394)\n",
      " state (6)  A[0]:(0.00037187334965) A[1]:(0.810001909733) A[2]:(-0.000310063362122) A[3]:(0.656551361084)\n",
      " state (7)  A[0]:(0.624625205994) A[1]:(-0.248318955302) A[2]:(0.310687631369) A[3]:(0.881469607353)\n",
      " state (8)  A[0]:(0.655424356461) A[1]:(3.5360455513e-05) A[2]:(0.728802680969) A[3]:(0.590758085251)\n",
      " state (9)  A[0]:(0.655335843563) A[1]:(0.810025513172) A[2]:(0.809897065163) A[3]:(-0.000807791773695)\n",
      " state (10)  A[0]:(0.728770256042) A[1]:(0.900014996529) A[2]:(-0.000403284997446) A[3]:(0.728525042534)\n",
      " state (11)  A[0]:(0.523923039436) A[1]:(0.876454234123) A[2]:(-0.626762986183) A[3]:(0.845122158527)\n",
      " state (12)  A[0]:(0.081147544086) A[1]:(0.823308706284) A[2]:(-0.624480366707) A[3]:(0.794750571251)\n",
      " state (13)  A[0]:(0.00038123127888) A[1]:(0.808021843433) A[2]:(0.900027453899) A[3]:(0.72937476635)\n",
      " state (14)  A[0]:(0.810114741325) A[1]:(0.900404036045) A[2]:(1.0) A[3]:(0.81006026268)\n",
      " state (15)  A[0]:(0.981945514679) A[1]:(0.956256747246) A[2]:(1.0) A[3]:(0.874980807304)\n",
      "Episode 856000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6008. Times reached goal: 995.               Steps done: 6300995. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00165103235725.\n",
      " state (0)  A[0]:(0.532875239849) A[1]:(0.590302467346) A[2]:(0.590551495552) A[3]:(0.534964799881)\n",
      " state (1)  A[0]:(0.532077968121) A[1]:(-0.000498965324368) A[2]:(0.655973732471) A[3]:(0.593124747276)\n",
      " state (2)  A[0]:(0.590804219246) A[1]:(0.728775978088) A[2]:(0.590709209442) A[3]:(0.658215761185)\n",
      " state (3)  A[0]:(0.656412959099) A[1]:(-0.21785621345) A[2]:(0.540219783783) A[3]:(0.519229888916)\n",
      " state (4)  A[0]:(0.59043943882) A[1]:(0.656002044678) A[2]:(7.70092010498e-05) A[3]:(0.533167123795)\n",
      " state (5)  A[0]:(0.162448346615) A[1]:(0.928817391396) A[2]:(-0.196096137166) A[3]:(0.528815865517)\n",
      " state (6)  A[0]:(-0.00164759007748) A[1]:(0.809884905815) A[2]:(0.000393867463572) A[3]:(0.657172262669)\n",
      " state (7)  A[0]:(0.6229596138) A[1]:(-0.248883262277) A[2]:(0.311049699783) A[3]:(0.881729960442)\n",
      " state (8)  A[0]:(0.653269052505) A[1]:(-0.000990345724858) A[2]:(0.728426933289) A[3]:(0.592802226543)\n",
      " state (9)  A[0]:(0.651686131954) A[1]:(0.809795498848) A[2]:(0.809745430946) A[3]:(0.00206198985688)\n",
      " state (10)  A[0]:(0.724722266197) A[1]:(0.89995765686) A[2]:(-0.000182390213013) A[3]:(0.729154586792)\n",
      " state (11)  A[0]:(0.516653180122) A[1]:(0.876448631287) A[2]:(-0.626565933228) A[3]:(0.845224022865)\n",
      " state (12)  A[0]:(0.070326924324) A[1]:(0.823413431644) A[2]:(-0.624226272106) A[3]:(0.79462826252)\n",
      " state (13)  A[0]:(-0.0109401447698) A[1]:(0.808299303055) A[2]:(0.90025383234) A[3]:(0.728954195976)\n",
      " state (14)  A[0]:(0.806187272072) A[1]:(0.900661408901) A[2]:(1.0) A[3]:(0.809512913227)\n",
      " state (15)  A[0]:(0.98152422905) A[1]:(0.956413209438) A[2]:(1.0) A[3]:(0.874435186386)\n",
      "Episode 857000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 999.               Steps done: 6307004. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00164114105201.\n",
      " state (0)  A[0]:(0.531205892563) A[1]:(0.590149760246) A[2]:(0.59047293663) A[3]:(0.53145635128)\n",
      " state (1)  A[0]:(0.531021475792) A[1]:(-0.000129707157612) A[2]:(0.656153678894) A[3]:(0.590471506119)\n",
      " state (2)  A[0]:(0.590203106403) A[1]:(0.728942334652) A[2]:(0.59098470211) A[3]:(0.65615427494)\n",
      " state (3)  A[0]:(0.655981302261) A[1]:(-0.216303020716) A[2]:(0.540104866028) A[3]:(0.517352104187)\n",
      " state (4)  A[0]:(0.590199947357) A[1]:(0.65558719635) A[2]:(-2.03847885132e-05) A[3]:(0.531558156013)\n",
      " state (5)  A[0]:(0.162810847163) A[1]:(0.928767621517) A[2]:(-0.196580737829) A[3]:(0.527469277382)\n",
      " state (6)  A[0]:(-0.000310719013214) A[1]:(0.809970259666) A[2]:(-0.000448822946055) A[3]:(0.656108498573)\n",
      " state (7)  A[0]:(0.624782502651) A[1]:(-0.248233184218) A[2]:(0.310418248177) A[3]:(0.881177425385)\n",
      " state (8)  A[0]:(0.656131029129) A[1]:(-0.000290431082249) A[2]:(0.72872197628) A[3]:(0.590590536594)\n",
      " state (9)  A[0]:(0.656084299088) A[1]:(0.809920907021) A[2]:(0.809969544411) A[3]:(9.2625617981e-05)\n",
      " state (10)  A[0]:(0.729265093803) A[1]:(0.899991989136) A[2]:(-0.000107407569885) A[3]:(0.729022145271)\n",
      " state (11)  A[0]:(0.524513781071) A[1]:(0.876479744911) A[2]:(-0.626664698124) A[3]:(0.845366954803)\n",
      " state (12)  A[0]:(0.0818006768823) A[1]:(0.823438227177) A[2]:(-0.624471545219) A[3]:(0.794974505901)\n",
      " state (13)  A[0]:(0.000977158197202) A[1]:(0.808276772499) A[2]:(0.900022745132) A[3]:(0.729552686214)\n",
      " state (14)  A[0]:(0.810325980186) A[1]:(0.90060031414) A[2]:(1.0) A[3]:(0.810105860233)\n",
      " state (15)  A[0]:(0.981969833374) A[1]:(0.956375420094) A[2]:(1.0) A[3]:(0.87496727705)\n",
      "Episode 858000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 1000.               Steps done: 6313010. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00163131389933.\n",
      " state (0)  A[0]:(0.531171858311) A[1]:(0.590120732784) A[2]:(0.590342581272) A[3]:(0.531349778175)\n",
      " state (1)  A[0]:(0.531010270119) A[1]:(-1.29714608192e-05) A[2]:(0.656010985374) A[3]:(0.590290665627)\n",
      " state (2)  A[0]:(0.590278029442) A[1]:(0.728972673416) A[2]:(0.590631067753) A[3]:(0.656045079231)\n",
      " state (3)  A[0]:(0.656309604645) A[1]:(-0.216978520155) A[2]:(0.539784669876) A[3]:(0.516862988472)\n",
      " state (4)  A[0]:(0.59046792984) A[1]:(0.656106650829) A[2]:(-0.000431895226939) A[3]:(0.531250178814)\n",
      " state (5)  A[0]:(0.162700250745) A[1]:(0.928800344467) A[2]:(-0.196430489421) A[3]:(0.527323365211)\n",
      " state (6)  A[0]:(-0.000789344136138) A[1]:(0.809989392757) A[2]:(-8.0943107605e-05) A[3]:(0.656193315983)\n",
      " state (7)  A[0]:(0.624551773071) A[1]:(-0.2480930686) A[2]:(0.310750424862) A[3]:(0.881323754787)\n",
      " state (8)  A[0]:(0.656117677689) A[1]:(8.66875052452e-05) A[2]:(0.728950619698) A[3]:(0.591004610062)\n",
      " state (9)  A[0]:(0.656167626381) A[1]:(0.81000739336) A[2]:(0.809999644756) A[3]:(0.00110113574192)\n",
      " state (10)  A[0]:(0.729153513908) A[1]:(0.899992525578) A[2]:(0.000224113464355) A[3]:(0.729349076748)\n",
      " state (11)  A[0]:(0.524136066437) A[1]:(0.876459598541) A[2]:(-0.626290798187) A[3]:(0.845478177071)\n",
      " state (12)  A[0]:(0.0811875090003) A[1]:(0.823396384716) A[2]:(-0.624119699001) A[3]:(0.795110464096)\n",
      " state (13)  A[0]:(0.000357985467417) A[1]:(0.808196365833) A[2]:(0.900012254715) A[3]:(0.729745268822)\n",
      " state (14)  A[0]:(0.810217797756) A[1]:(0.90051984787) A[2]:(1.0) A[3]:(0.810244143009)\n",
      " state (15)  A[0]:(0.981984078884) A[1]:(0.956330895424) A[2]:(1.0) A[3]:(0.875089526176)\n",
      "Episode 859000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5995. Times reached goal: 995.               Steps done: 6319005. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00162156342874.\n",
      "q_values \n",
      "tensor([[ 0.5337,  0.5905,  0.5904,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.6562,  0.0002,  0.5340]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6576,  0.0002,  0.7288,  0.5938]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6582,  0.8100,  0.8099,  0.0051]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0038,  0.8079,  0.9001,  0.7295]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8111,  0.9002,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533308267593) A[1]:(0.590462207794) A[2]:(0.590426325798) A[3]:(0.531170547009)\n",
      " state (1)  A[0]:(0.532816529274) A[1]:(0.00010497123003) A[2]:(0.655969262123) A[3]:(0.590742588043)\n",
      " state (2)  A[0]:(0.591756343842) A[1]:(0.729022562504) A[2]:(0.59065079689) A[3]:(0.656719684601)\n",
      " state (3)  A[0]:(0.656904935837) A[1]:(-0.217591941357) A[2]:(0.54011797905) A[3]:(0.519561290741)\n",
      " state (4)  A[0]:(0.590818405151) A[1]:(0.65625834465) A[2]:(0.000262379646301) A[3]:(0.534637928009)\n",
      " state (5)  A[0]:(0.163256034255) A[1]:(0.928797483444) A[2]:(-0.195353835821) A[3]:(0.530569732189)\n",
      " state (6)  A[0]:(3.30805778503e-05) A[1]:(0.809995412827) A[2]:(0.00119745673146) A[3]:(0.658570289612)\n",
      " state (7)  A[0]:(0.625635325909) A[1]:(-0.248091027141) A[2]:(0.311630696058) A[3]:(0.88232088089)\n",
      " state (8)  A[0]:(0.658198595047) A[1]:(0.000110752880573) A[2]:(0.728828430176) A[3]:(0.594917178154)\n",
      " state (9)  A[0]:(0.658816516399) A[1]:(0.810006856918) A[2]:(0.809808850288) A[3]:(0.00690668262541)\n",
      " state (10)  A[0]:(0.731276631355) A[1]:(0.899940729141) A[2]:(0.000804424111266) A[3]:(0.730967104435)\n",
      " state (11)  A[0]:(0.527402698994) A[1]:(0.8763422966) A[2]:(-0.625518381596) A[3]:(0.846024394035)\n",
      " state (12)  A[0]:(0.0856729596853) A[1]:(0.823151350021) A[2]:(-0.623363316059) A[3]:(0.795500814915)\n",
      " state (13)  A[0]:(0.00436386698857) A[1]:(0.807782292366) A[2]:(0.899998188019) A[3]:(0.729874014854)\n",
      " state (14)  A[0]:(0.81113088131) A[1]:(0.900170207024) A[2]:(0.999999940395) A[3]:(0.809996604919)\n",
      " state (15)  A[0]:(0.982056856155) A[1]:(0.956131696701) A[2]:(1.0) A[3]:(0.874756813049)\n",
      "Episode 860000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 997.               Steps done: 6325006. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00161186156616.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531341195107) A[1]:(0.590496599674) A[2]:(0.590543210506) A[3]:(0.531547963619)\n",
      " state (1)  A[0]:(0.531173586845) A[1]:(-2.52574682236e-06) A[2]:(0.65614002943) A[3]:(0.590520083904)\n",
      " state (2)  A[0]:(0.590380311012) A[1]:(0.729007720947) A[2]:(0.590859651566) A[3]:(0.656163930893)\n",
      " state (3)  A[0]:(0.65610319376) A[1]:(-0.21684384346) A[2]:(0.539974272251) A[3]:(0.51714193821)\n",
      " state (4)  A[0]:(0.590221166611) A[1]:(0.656082630157) A[2]:(-0.000280857086182) A[3]:(0.531408131123)\n",
      " state (5)  A[0]:(0.162612214684) A[1]:(0.928812742233) A[2]:(-0.196437478065) A[3]:(0.527308821678)\n",
      " state (6)  A[0]:(-0.00038522479008) A[1]:(0.810004234314) A[2]:(-0.000107049942017) A[3]:(0.656092107296)\n",
      " state (7)  A[0]:(0.625001311302) A[1]:(-0.248029649258) A[2]:(0.310788661242) A[3]:(0.881224989891)\n",
      " state (8)  A[0]:(0.656292200089) A[1]:(0.000218726694584) A[2]:(0.729031682014) A[3]:(0.59041595459)\n",
      " state (9)  A[0]:(0.65608549118) A[1]:(0.810024380684) A[2]:(0.810023784637) A[3]:(0.000116556882858)\n",
      " state (10)  A[0]:(0.72907191515) A[1]:(0.900006175041) A[2]:(2.40802764893e-05) A[3]:(0.728943705559)\n",
      " state (11)  A[0]:(0.524041891098) A[1]:(0.876488685608) A[2]:(-0.626494765282) A[3]:(0.845236420631)\n",
      " state (12)  A[0]:(0.0810515582561) A[1]:(0.823443591595) A[2]:(-0.624317765236) A[3]:(0.794776320457)\n",
      " state (13)  A[0]:(0.000170886516571) A[1]:(0.808228433132) A[2]:(0.900015830994) A[3]:(0.729326963425)\n",
      " state (14)  A[0]:(0.8100938797) A[1]:(0.90049290657) A[2]:(1.0) A[3]:(0.810029745102)\n",
      " state (15)  A[0]:(0.98195540905) A[1]:(0.956290245056) A[2]:(1.0) A[3]:(0.874989628792)\n",
      "Episode 861000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 1000.               Steps done: 6331013. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00160220813683.\n",
      " state (0)  A[0]:(0.531517982483) A[1]:(0.590377449989) A[2]:(0.590436398983) A[3]:(0.531443297863)\n",
      " state (1)  A[0]:(0.530957639217) A[1]:(7.10412859917e-05) A[2]:(0.656076014042) A[3]:(0.59059882164)\n",
      " state (2)  A[0]:(0.590140104294) A[1]:(0.728943705559) A[2]:(0.590663731098) A[3]:(0.656311273575)\n",
      " state (3)  A[0]:(0.65584218502) A[1]:(-0.217123627663) A[2]:(0.539742946625) A[3]:(0.517896294594)\n",
      " state (4)  A[0]:(0.589957952499) A[1]:(0.655994176865) A[2]:(-0.000560164393391) A[3]:(0.5322701931)\n",
      " state (5)  A[0]:(0.162243247032) A[1]:(0.928765118122) A[2]:(-0.196578100324) A[3]:(0.528006851673)\n",
      " state (6)  A[0]:(-0.00123816670384) A[1]:(0.809944689274) A[2]:(-0.000284075737) A[3]:(0.656534075737)\n",
      " state (7)  A[0]:(0.624157845974) A[1]:(-0.248219817877) A[2]:(0.310600072145) A[3]:(0.881445288658)\n",
      " state (8)  A[0]:(0.656018555164) A[1]:(-0.000324465334415) A[2]:(0.728911161423) A[3]:(0.591320574284)\n",
      " state (9)  A[0]:(0.656500101089) A[1]:(0.80984544754) A[2]:(0.809983372688) A[3]:(0.00151882949285)\n",
      " state (10)  A[0]:(0.729749262333) A[1]:(0.899929344654) A[2]:(-0.000155210494995) A[3]:(0.729675710201)\n",
      " state (11)  A[0]:(0.525344610214) A[1]:(0.876410961151) A[2]:(-0.626673698425) A[3]:(0.84570813179)\n",
      " state (12)  A[0]:(0.0829548463225) A[1]:(0.823345005512) A[2]:(-0.624600529671) A[3]:(0.795360803604)\n",
      " state (13)  A[0]:(0.00185686140321) A[1]:(0.808104038239) A[2]:(0.899807453156) A[3]:(0.729974031448)\n",
      " state (14)  A[0]:(0.810421824455) A[1]:(0.900392055511) A[2]:(0.999999940395) A[3]:(0.810383439064)\n",
      " state (15)  A[0]:(0.981975734234) A[1]:(0.956239104271) A[2]:(1.0) A[3]:(0.875175178051)\n",
      "Episode 862000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 1000.               Steps done: 6337015. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00159262048492.\n",
      " state (0)  A[0]:(0.532250761986) A[1]:(0.590488374233) A[2]:(0.590538859367) A[3]:(0.530628681183)\n",
      " state (1)  A[0]:(0.531201004982) A[1]:(-0.000866696005687) A[2]:(0.656149089336) A[3]:(0.592459082603)\n",
      " state (2)  A[0]:(0.589958310127) A[1]:(0.729016542435) A[2]:(0.591396689415) A[3]:(0.659447014332)\n",
      " state (3)  A[0]:(0.655388236046) A[1]:(-0.217961519957) A[2]:(0.541450977325) A[3]:(0.524265646935)\n",
      " state (4)  A[0]:(0.589136481285) A[1]:(0.656153321266) A[2]:(0.00237905536778) A[3]:(0.540339708328)\n",
      " state (5)  A[0]:(0.160410970449) A[1]:(0.92879396677) A[2]:(-0.193343147635) A[3]:(0.537270069122)\n",
      " state (6)  A[0]:(-0.00360684026964) A[1]:(0.810073375702) A[2]:(0.00323711684905) A[3]:(0.66476392746)\n",
      " state (7)  A[0]:(0.623240232468) A[1]:(-0.248182430863) A[2]:(0.313307076693) A[3]:(0.885415792465)\n",
      " state (8)  A[0]:(0.655307650566) A[1]:(-0.000327661633492) A[2]:(0.729014456272) A[3]:(0.606871247292)\n",
      " state (9)  A[0]:(0.654131293297) A[1]:(0.810032308102) A[2]:(0.809977650642) A[3]:(0.027478704229)\n",
      " state (10)  A[0]:(0.725880384445) A[1]:(0.90006428957) A[2]:(0.00204729754478) A[3]:(0.74037027359)\n",
      " state (11)  A[0]:(0.517370223999) A[1]:(0.876637578011) A[2]:(-0.624674677849) A[3]:(0.851737737656)\n",
      " state (12)  A[0]:(0.0702481642365) A[1]:(0.823809444904) A[2]:(-0.622686564922) A[3]:(0.802890062332)\n",
      " state (13)  A[0]:(-0.012437059544) A[1]:(0.808767199516) A[2]:(0.900017440319) A[3]:(0.739287495613)\n",
      " state (14)  A[0]:(0.805177688599) A[1]:(0.90083783865) A[2]:(0.999999940395) A[3]:(0.816931009293)\n",
      " state (15)  A[0]:(0.981449663639) A[1]:(0.956512093544) A[2]:(1.0) A[3]:(0.879527390003)\n",
      "Episode 863000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5995. Times reached goal: 998.               Steps done: 6343010. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00158310128742.\n",
      " state (0)  A[0]:(0.531686902046) A[1]:(0.590315937996) A[2]:(0.590480566025) A[3]:(0.530325949192)\n",
      " state (1)  A[0]:(0.531124413013) A[1]:(-3.49581241608e-05) A[2]:(0.655963182449) A[3]:(0.589481830597)\n",
      " state (2)  A[0]:(0.590344429016) A[1]:(0.728912830353) A[2]:(0.590561151505) A[3]:(0.655314207077)\n",
      " state (3)  A[0]:(0.655976772308) A[1]:(-0.216621562839) A[2]:(0.53980743885) A[3]:(0.516751348972)\n",
      " state (4)  A[0]:(0.590307056904) A[1]:(0.656040072441) A[2]:(-0.000505804957356) A[3]:(0.531213104725)\n",
      " state (5)  A[0]:(0.163167789578) A[1]:(0.928830206394) A[2]:(-0.196909844875) A[3]:(0.527040600777)\n",
      " state (6)  A[0]:(0.000330746173859) A[1]:(0.809959411621) A[2]:(-0.000753044965677) A[3]:(0.655841827393)\n",
      " state (7)  A[0]:(0.62495470047) A[1]:(-0.248037531972) A[2]:(0.310053557158) A[3]:(0.881106853485)\n",
      " state (8)  A[0]:(0.656039357185) A[1]:(0.000840276305098) A[2]:(0.728881061077) A[3]:(0.590369284153)\n",
      " state (9)  A[0]:(0.656054854393) A[1]:(0.810157895088) A[2]:(0.809936285019) A[3]:(0.00225267931819)\n",
      " state (10)  A[0]:(0.728790819645) A[1]:(0.900000810623) A[2]:(3.99351119995e-05) A[3]:(0.730076789856)\n",
      " state (11)  A[0]:(0.523134112358) A[1]:(0.876441419125) A[2]:(-0.626348316669) A[3]:(0.845742702484)\n",
      " state (12)  A[0]:(0.0794258266687) A[1]:(0.823352575302) A[2]:(-0.624198436737) A[3]:(0.795220792294)\n",
      " state (13)  A[0]:(-0.00147879018914) A[1]:(0.808084666729) A[2]:(0.899995803833) A[3]:(0.729675590992)\n",
      " state (14)  A[0]:(0.809771239758) A[1]:(0.900354385376) A[2]:(1.0) A[3]:(0.810132682323)\n",
      " state (15)  A[0]:(0.98195785284) A[1]:(0.956204712391) A[2]:(1.0) A[3]:(0.874995529652)\n",
      "Episode 864000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 999.               Steps done: 6349011. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00157362954498.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5908,  0.5906,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5957,  0.6563, -0.0003,  0.5317]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6603,  0.0008,  0.7290,  0.5910]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6596,  0.8102,  0.8101,  0.0016]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0010,  0.8082,  0.9001,  0.7287]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9004,  1.0000,  0.8095]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531384944916) A[1]:(0.590781152248) A[2]:(0.590593695641) A[3]:(0.531896233559)\n",
      " state (1)  A[0]:(0.532910585403) A[1]:(3.17618250847e-05) A[2]:(0.656180560589) A[3]:(0.591268122196)\n",
      " state (2)  A[0]:(0.593078136444) A[1]:(0.729151725769) A[2]:(0.590824186802) A[3]:(0.656936943531)\n",
      " state (3)  A[0]:(0.65981221199) A[1]:(-0.214815482497) A[2]:(0.539880037308) A[3]:(0.517694532871)\n",
      " state (4)  A[0]:(0.595785439014) A[1]:(0.656363368034) A[2]:(-0.000256299972534) A[3]:(0.531590044498)\n",
      " state (5)  A[0]:(0.172141954303) A[1]:(0.928927242756) A[2]:(-0.196759968996) A[3]:(0.527529597282)\n",
      " state (6)  A[0]:(0.00986132863909) A[1]:(0.810105085373) A[2]:(-0.000365376443369) A[3]:(0.656457602978)\n",
      " state (7)  A[0]:(0.630461335182) A[1]:(-0.247964859009) A[2]:(0.31077465415) A[3]:(0.881411492825)\n",
      " state (8)  A[0]:(0.660426735878) A[1]:(0.00090392655693) A[2]:(0.72927314043) A[3]:(0.590618550777)\n",
      " state (9)  A[0]:(0.659680366516) A[1]:(0.810156822205) A[2]:(0.810158491135) A[3]:(0.00126278330572)\n",
      " state (10)  A[0]:(0.73123717308) A[1]:(0.900003433228) A[2]:(0.00062417978188) A[3]:(0.729326546192)\n",
      " state (11)  A[0]:(0.526224970818) A[1]:(0.876466691494) A[2]:(-0.626002430916) A[3]:(0.845212936401)\n",
      " state (12)  A[0]:(0.0828254446387) A[1]:(0.823435902596) A[2]:(-0.62392103672) A[3]:(0.794488608837)\n",
      " state (13)  A[0]:(0.00113749457523) A[1]:(0.80822789669) A[2]:(0.899985492229) A[3]:(0.728718757629)\n",
      " state (14)  A[0]:(0.810409069061) A[1]:(0.900455653667) A[2]:(1.0) A[3]:(0.809458494186)\n",
      " state (15)  A[0]:(0.982011497021) A[1]:(0.956270813942) A[2]:(1.0) A[3]:(0.874587059021)\n",
      "Episode 865000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6003. Times reached goal: 998.               Steps done: 6355014. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00156421134384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532531559467) A[1]:(0.590618431568) A[2]:(0.590538680553) A[3]:(0.531729876995)\n",
      " state (1)  A[0]:(0.532090902328) A[1]:(0.000102549791336) A[2]:(0.656190276146) A[3]:(0.590521872044)\n",
      " state (2)  A[0]:(0.591255426407) A[1]:(0.729069709778) A[2]:(0.59084379673) A[3]:(0.656232833862)\n",
      " state (3)  A[0]:(0.656754612923) A[1]:(-0.214925631881) A[2]:(0.540007472038) A[3]:(0.517572522163)\n",
      " state (4)  A[0]:(0.591185688972) A[1]:(0.656302452087) A[2]:(7.28368759155e-05) A[3]:(0.531919598579)\n",
      " state (5)  A[0]:(0.164611965418) A[1]:(0.928882002831) A[2]:(-0.196329280734) A[3]:(0.528069972992)\n",
      " state (6)  A[0]:(0.00219112285413) A[1]:(0.81003344059) A[2]:(-0.000120162963867) A[3]:(0.656851410866)\n",
      " state (7)  A[0]:(0.62646561861) A[1]:(-0.248220354319) A[2]:(0.310630232096) A[3]:(0.881469130516)\n",
      " state (8)  A[0]:(0.657651662827) A[1]:(0.000309228897095) A[2]:(0.729032516479) A[3]:(0.590648651123)\n",
      " state (9)  A[0]:(0.657812595367) A[1]:(0.80998313427) A[2]:(0.809977829456) A[3]:(0.000722333672456)\n",
      " state (10)  A[0]:(0.730412483215) A[1]:(0.899952173233) A[2]:(-0.000108003616333) A[3]:(0.729392170906)\n",
      " state (11)  A[0]:(0.525781571865) A[1]:(0.876425385475) A[2]:(-0.626537561417) A[3]:(0.845523893833)\n",
      " state (12)  A[0]:(0.0830194279552) A[1]:(0.823366761208) A[2]:(-0.624389529228) A[3]:(0.795157253742)\n",
      " state (13)  A[0]:(0.00194632762577) A[1]:(0.808108389378) A[2]:(0.899969220161) A[3]:(0.729845046997)\n",
      " state (14)  A[0]:(0.810819506645) A[1]:(0.900340557098) A[2]:(1.0) A[3]:(0.810470879078)\n",
      " state (15)  A[0]:(0.982047319412) A[1]:(0.956180751324) A[2]:(1.0) A[3]:(0.875340878963)\n",
      "Episode 866000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 998.               Steps done: 6361016. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00155485106565.\n",
      " state (0)  A[0]:(0.531312584877) A[1]:(0.590464532375) A[2]:(0.590403020382) A[3]:(0.531487107277)\n",
      " state (1)  A[0]:(0.531097114086) A[1]:(0.000530831457581) A[2]:(0.656179666519) A[3]:(0.590746879578)\n",
      " state (2)  A[0]:(0.590392589569) A[1]:(0.729026257992) A[2]:(0.590679764748) A[3]:(0.656566917896)\n",
      " state (3)  A[0]:(0.655576229095) A[1]:(-0.214813232422) A[2]:(0.539954185486) A[3]:(0.517974317074)\n",
      " state (4)  A[0]:(0.58964073658) A[1]:(0.656139135361) A[2]:(0.00013792514801) A[3]:(0.532132148743)\n",
      " state (5)  A[0]:(0.162284746766) A[1]:(0.928860545158) A[2]:(-0.196334213018) A[3]:(0.528137326241)\n",
      " state (6)  A[0]:(-0.000384390325053) A[1]:(0.810031652451) A[2]:(-0.000204563140869) A[3]:(0.656955957413)\n",
      " state (7)  A[0]:(0.62469792366) A[1]:(-0.248209670186) A[2]:(0.310541898012) A[3]:(0.881635308266)\n",
      " state (8)  A[0]:(0.655967175961) A[1]:(0.000240497291088) A[2]:(0.729009985924) A[3]:(0.591198325157)\n",
      " state (9)  A[0]:(0.656174838543) A[1]:(0.810007512569) A[2]:(0.810041546822) A[3]:(0.00124841858633)\n",
      " state (10)  A[0]:(0.729125738144) A[1]:(0.900007784367) A[2]:(-5.25712966919e-05) A[3]:(0.729614019394)\n",
      " state (11)  A[0]:(0.523792147636) A[1]:(0.876545190811) A[2]:(-0.626613855362) A[3]:(0.845619559288)\n",
      " state (12)  A[0]:(0.0802992135286) A[1]:(0.823618650436) A[2]:(-0.624473154545) A[3]:(0.795199751854)\n",
      " state (13)  A[0]:(-0.00066459167283) A[1]:(0.808495819569) A[2]:(0.900012135506) A[3]:(0.729833483696)\n",
      " state (14)  A[0]:(0.809976398945) A[1]:(0.900621891022) A[2]:(1.0) A[3]:(0.810455083847)\n",
      " state (15)  A[0]:(0.981960773468) A[1]:(0.9563382864) A[2]:(1.0) A[3]:(0.875330448151)\n",
      "Episode 867000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 999.               Steps done: 6367026. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00154553443526.\n",
      " state (0)  A[0]:(0.53171813488) A[1]:(0.590171873569) A[2]:(0.590402305126) A[3]:(0.53263515234)\n",
      " state (1)  A[0]:(0.531649708748) A[1]:(-0.000125460326672) A[2]:(0.656069874763) A[3]:(0.592634141445)\n",
      " state (2)  A[0]:(0.591020464897) A[1]:(0.728925347328) A[2]:(0.590368628502) A[3]:(0.658644556999)\n",
      " state (3)  A[0]:(0.656589508057) A[1]:(-0.21372063458) A[2]:(0.539348781109) A[3]:(0.521728515625)\n",
      " state (4)  A[0]:(0.590777337551) A[1]:(0.655742883682) A[2]:(-9.6321105957e-05) A[3]:(0.53602463007)\n",
      " state (5)  A[0]:(0.163608402014) A[1]:(0.928723156452) A[2]:(-0.196160674095) A[3]:(0.531914114952)\n",
      " state (6)  A[0]:(8.35657119751e-05) A[1]:(0.809972882271) A[2]:(-6.19888305664e-06) A[3]:(0.659536361694)\n",
      " state (7)  A[0]:(0.624891638756) A[1]:(-0.248242720962) A[2]:(0.310674071312) A[3]:(0.882576346397)\n",
      " state (8)  A[0]:(0.656169354916) A[1]:(-0.000521771551576) A[2]:(0.72879755497) A[3]:(0.594509780407)\n",
      " state (9)  A[0]:(0.655595541) A[1]:(0.809882462025) A[2]:(0.809966683388) A[3]:(0.00541817303747)\n",
      " state (10)  A[0]:(0.728616476059) A[1]:(0.900020062923) A[2]:(-4.44650650024e-05) A[3]:(0.731244683266)\n",
      " state (11)  A[0]:(0.523403227329) A[1]:(0.876588463783) A[2]:(-0.626621961594) A[3]:(0.846558630466)\n",
      " state (12)  A[0]:(0.0803061425686) A[1]:(0.823664784431) A[2]:(-0.624490022659) A[3]:(0.796331584454)\n",
      " state (13)  A[0]:(-0.000432729691966) A[1]:(0.808483064175) A[2]:(0.899984896183) A[3]:(0.731112420559)\n",
      " state (14)  A[0]:(0.809887766838) A[1]:(0.90055000782) A[2]:(1.0) A[3]:(0.81118285656)\n",
      " state (15)  A[0]:(0.981935322285) A[1]:(0.956275045872) A[2]:(1.0) A[3]:(0.8756980896)\n",
      "Episode 868000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 999.               Steps done: 6373027. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00153628745642.\n",
      " state (0)  A[0]:(0.531782209873) A[1]:(0.590678334236) A[2]:(0.590631246567) A[3]:(0.531385302544)\n",
      " state (1)  A[0]:(0.531465828419) A[1]:(6.22645020485e-05) A[2]:(0.656264483929) A[3]:(0.590492725372)\n",
      " state (2)  A[0]:(0.590482294559) A[1]:(0.729086995125) A[2]:(0.590928554535) A[3]:(0.656116247177)\n",
      " state (3)  A[0]:(0.655283987522) A[1]:(-0.213592588902) A[2]:(0.539895236492) A[3]:(0.517028748989)\n",
      " state (4)  A[0]:(0.588927507401) A[1]:(0.656219065189) A[2]:(-4.64916229248e-06) A[3]:(0.530768632889)\n",
      " state (5)  A[0]:(0.161008581519) A[1]:(0.928881585598) A[2]:(-0.196701973677) A[3]:(0.526613116264)\n",
      " state (6)  A[0]:(-0.00163912621792) A[1]:(0.809968829155) A[2]:(-0.000697612646036) A[3]:(0.655892491341)\n",
      " state (7)  A[0]:(0.623459458351) A[1]:(-0.247960418463) A[2]:(0.309980511665) A[3]:(0.881245017052)\n",
      " state (8)  A[0]:(0.654302358627) A[1]:(0.00159639725462) A[2]:(0.729043245316) A[3]:(0.589196145535)\n",
      " state (9)  A[0]:(0.654512166977) A[1]:(0.810317754745) A[2]:(0.809909582138) A[3]:(-0.00124421645887)\n",
      " state (10)  A[0]:(0.727764427662) A[1]:(0.899978280067) A[2]:(-0.000243663787842) A[3]:(0.728276848793)\n",
      " state (11)  A[0]:(0.521891236305) A[1]:(0.876316726208) A[2]:(-0.626477003098) A[3]:(0.844653367996)\n",
      " state (12)  A[0]:(0.0782590955496) A[1]:(0.823052048683) A[2]:(-0.624247014523) A[3]:(0.793949782848)\n",
      " state (13)  A[0]:(-0.00215398939326) A[1]:(0.807586669922) A[2]:(0.900097608566) A[3]:(0.728374242783)\n",
      " state (14)  A[0]:(0.809565186501) A[1]:(0.899926543236) A[2]:(1.0) A[3]:(0.809545397758)\n",
      " state (15)  A[0]:(0.981926620007) A[1]:(0.95592880249) A[2]:(1.0) A[3]:(0.874831616879)\n",
      "Episode 869000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 998.               Steps done: 6379028. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00152709580254.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5909,  0.5905,  0.5309]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.6566,  0.0002,  0.5324]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565, -0.0003,  0.7291,  0.5917]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.8096,  0.8098,  0.0030]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000,  0.0001,  0.7304]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.9007,  1.0000,  0.8111]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53137254715) A[1]:(0.590762138367) A[2]:(0.590459108353) A[3]:(0.530866086483)\n",
      " state (1)  A[0]:(0.531418681145) A[1]:(0.000339232385159) A[2]:(0.656087219715) A[3]:(0.590347766876)\n",
      " state (2)  A[0]:(0.590738177299) A[1]:(0.728970527649) A[2]:(0.590463757515) A[3]:(0.656264901161)\n",
      " state (3)  A[0]:(0.656305611134) A[1]:(-0.214724108577) A[2]:(0.539669215679) A[3]:(0.517997801304)\n",
      " state (4)  A[0]:(0.590542793274) A[1]:(0.656046569347) A[2]:(-8.66651535034e-05) A[3]:(0.532304167747)\n",
      " state (5)  A[0]:(0.163500502706) A[1]:(0.928801834583) A[2]:(-0.196342006326) A[3]:(0.528325676918)\n",
      " state (6)  A[0]:(0.000443220109446) A[1]:(0.810034751892) A[2]:(-0.000254511833191) A[3]:(0.657014787197)\n",
      " state (7)  A[0]:(0.625240027905) A[1]:(-0.248203933239) A[2]:(0.310467809439) A[3]:(0.881697893143)\n",
      " state (8)  A[0]:(0.65647649765) A[1]:(4.62159514427e-05) A[2]:(0.728922784328) A[3]:(0.591919541359)\n",
      " state (9)  A[0]:(0.656124651432) A[1]:(0.809993743896) A[2]:(0.809921205044) A[3]:(0.00292465742677)\n",
      " state (10)  A[0]:(0.729046523571) A[1]:(0.899999380112) A[2]:(-8.11815261841e-05) A[3]:(0.7304225564)\n",
      " state (11)  A[0]:(0.524062335491) A[1]:(0.876532137394) A[2]:(-0.626464247704) A[3]:(0.84615200758)\n",
      " state (12)  A[0]:(0.0812054499984) A[1]:(0.823578178883) A[2]:(-0.624305069447) A[3]:(0.795954942703)\n",
      " state (13)  A[0]:(0.000326871871948) A[1]:(0.808368802071) A[2]:(0.899983227253) A[3]:(0.730786442757)\n",
      " state (14)  A[0]:(0.810097157955) A[1]:(0.900457203388) A[2]:(1.0) A[3]:(0.811041593552)\n",
      " state (15)  A[0]:(0.981951355934) A[1]:(0.956221222878) A[2]:(1.0) A[3]:(0.875636219978)\n",
      "Episode 870000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 1000.               Steps done: 6385038. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00151794548102.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531352043152) A[1]:(0.590419590473) A[2]:(0.590449154377) A[3]:(0.531398415565)\n",
      " state (1)  A[0]:(0.531804144382) A[1]:(-0.000228844583035) A[2]:(0.656150341034) A[3]:(0.590462863445)\n",
      " state (2)  A[0]:(0.591173291206) A[1]:(0.729030847549) A[2]:(0.590829253197) A[3]:(0.656136929989)\n",
      " state (3)  A[0]:(0.656574547291) A[1]:(-0.214171737432) A[2]:(0.540103673935) A[3]:(0.517528653145)\n",
      " state (4)  A[0]:(0.590757250786) A[1]:(0.655988454819) A[2]:(0.00062942499062) A[3]:(0.531517148018)\n",
      " state (5)  A[0]:(0.163709461689) A[1]:(0.928806900978) A[2]:(-0.195777118206) A[3]:(0.527343273163)\n",
      " state (6)  A[0]:(0.000463664502604) A[1]:(0.809981048107) A[2]:(0.000335454940796) A[3]:(0.6561383605)\n",
      " state (7)  A[0]:(0.624851942062) A[1]:(-0.248509645462) A[2]:(0.310976564884) A[3]:(0.881294071674)\n",
      " state (8)  A[0]:(0.655961930752) A[1]:(-0.000475905806525) A[2]:(0.72898042202) A[3]:(0.590564489365)\n",
      " state (9)  A[0]:(0.655767798424) A[1]:(0.809855103493) A[2]:(0.809978365898) A[3]:(-7.69048929214e-05)\n",
      " state (10)  A[0]:(0.728879630566) A[1]:(0.899997055531) A[2]:(-0.000331401824951) A[3]:(0.729022502899)\n",
      " state (11)  A[0]:(0.523799300194) A[1]:(0.876595795155) A[2]:(-0.626857042313) A[3]:(0.845332503319)\n",
      " state (12)  A[0]:(0.0807010084391) A[1]:(0.823754012585) A[2]:(-0.624707698822) A[3]:(0.794854283333)\n",
      " state (13)  A[0]:(-0.000280261039734) A[1]:(0.808673381805) A[2]:(0.900003671646) A[3]:(0.729335427284)\n",
      " state (14)  A[0]:(0.809805095196) A[1]:(0.9006909132) A[2]:(1.0) A[3]:(0.809973835945)\n",
      " state (15)  A[0]:(0.981896698475) A[1]:(0.956348121166) A[2]:(1.0) A[3]:(0.874882876873)\n",
      "Episode 871000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 1000.               Steps done: 6391049. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00150884847917.\n",
      " state (0)  A[0]:(0.530829250813) A[1]:(0.590955734253) A[2]:(0.590624928474) A[3]:(0.5310972929)\n",
      " state (1)  A[0]:(0.530970156193) A[1]:(9.30652022362e-05) A[2]:(0.656014680862) A[3]:(0.590098261833)\n",
      " state (2)  A[0]:(0.590408802032) A[1]:(0.729088544846) A[2]:(0.590722262859) A[3]:(0.655830264091)\n",
      " state (3)  A[0]:(0.656652033329) A[1]:(-0.215907782316) A[2]:(0.540242433548) A[3]:(0.517103433609)\n",
      " state (4)  A[0]:(0.59093528986) A[1]:(0.655999302864) A[2]:(0.000756025197916) A[3]:(0.531429767609)\n",
      " state (5)  A[0]:(0.163439363241) A[1]:(0.928747713566) A[2]:(-0.195240780711) A[3]:(0.527376770973)\n",
      " state (6)  A[0]:(-0.000376880139811) A[1]:(0.809960246086) A[2]:(0.000876068836078) A[3]:(0.656286537647)\n",
      " state (7)  A[0]:(0.624439418316) A[1]:(-0.248095810413) A[2]:(0.311275243759) A[3]:(0.881480038166)\n",
      " state (8)  A[0]:(0.655994296074) A[1]:(0.000136815011501) A[2]:(0.729158520699) A[3]:(0.591060400009)\n",
      " state (9)  A[0]:(0.655966877937) A[1]:(0.810026586056) A[2]:(0.809953808784) A[3]:(0.000746786477976)\n",
      " state (10)  A[0]:(0.729086637497) A[1]:(0.900010228157) A[2]:(-0.000247359275818) A[3]:(0.7292522192)\n",
      " state (11)  A[0]:(0.524301528931) A[1]:(0.876520514488) A[2]:(-0.626660227776) A[3]:(0.845414459705)\n",
      " state (12)  A[0]:(0.0817906111479) A[1]:(0.823510944843) A[2]:(-0.624496877193) A[3]:(0.795005500317)\n",
      " state (13)  A[0]:(0.00134181894828) A[1]:(0.808221817017) A[2]:(0.90002655983) A[3]:(0.72963643074)\n",
      " state (14)  A[0]:(0.810647785664) A[1]:(0.900307953358) A[2]:(1.0) A[3]:(0.81027007103)\n",
      " state (15)  A[0]:(0.982018768787) A[1]:(0.956112384796) A[2]:(1.0) A[3]:(0.875151097775)\n",
      "Episode 872000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 999.               Steps done: 6397057. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0014998104948.\n",
      " state (0)  A[0]:(0.532434225082) A[1]:(0.590663671494) A[2]:(0.59054505825) A[3]:(0.531239748001)\n",
      " state (1)  A[0]:(0.532457113266) A[1]:(0.00013555586338) A[2]:(0.656190574169) A[3]:(0.590612053871)\n",
      " state (2)  A[0]:(0.591663599014) A[1]:(0.729118824005) A[2]:(0.590849518776) A[3]:(0.656346857548)\n",
      " state (3)  A[0]:(0.65710413456) A[1]:(-0.214884623885) A[2]:(0.539885163307) A[3]:(0.517347872257)\n",
      " state (4)  A[0]:(0.591373324394) A[1]:(0.656041026115) A[2]:(5.48362731934e-06) A[3]:(0.531336247921)\n",
      " state (5)  A[0]:(0.164540529251) A[1]:(0.92879229784) A[2]:(-0.196266815066) A[3]:(0.527308821678)\n",
      " state (6)  A[0]:(0.00135904469062) A[1]:(0.810005664825) A[2]:(-0.000232577323914) A[3]:(0.656341910362)\n",
      " state (7)  A[0]:(0.625708341599) A[1]:(-0.247985035181) A[2]:(0.310370087624) A[3]:(0.881496906281)\n",
      " state (8)  A[0]:(0.657177686691) A[1]:(0.000542238296475) A[2]:(0.728995740414) A[3]:(0.59085804224)\n",
      " state (9)  A[0]:(0.657337427139) A[1]:(0.810054540634) A[2]:(0.809991121292) A[3]:(0.000779375259299)\n",
      " state (10)  A[0]:(0.730087816715) A[1]:(0.899953007698) A[2]:(0.000348091125488) A[3]:(0.729249954224)\n",
      " state (11)  A[0]:(0.52558273077) A[1]:(0.876414299011) A[2]:(-0.626103639603) A[3]:(0.845402002335)\n",
      " state (12)  A[0]:(0.0831883624196) A[1]:(0.823343038559) A[2]:(-0.623986721039) A[3]:(0.795045852661)\n",
      " state (13)  A[0]:(0.00198918324895) A[1]:(0.808011412621) A[2]:(0.899981856346) A[3]:(0.729762613773)\n",
      " state (14)  A[0]:(0.810345351696) A[1]:(0.900172233582) A[2]:(1.0) A[3]:(0.810405790806)\n",
      " state (15)  A[0]:(0.981957316399) A[1]:(0.956059396267) A[2]:(1.0) A[3]:(0.875318586826)\n",
      "Episode 873000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 998.               Steps done: 6403064. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00149082813867.\n",
      " state (0)  A[0]:(0.531671404839) A[1]:(0.590522050858) A[2]:(0.590468943119) A[3]:(0.532184362411)\n",
      " state (1)  A[0]:(0.53127849102) A[1]:(0.000103883445263) A[2]:(0.656108438969) A[3]:(0.591027677059)\n",
      " state (2)  A[0]:(0.59054851532) A[1]:(0.729103803635) A[2]:(0.590631783009) A[3]:(0.656554043293)\n",
      " state (3)  A[0]:(0.656067609787) A[1]:(-0.214849621058) A[2]:(0.539750754833) A[3]:(0.518100976944)\n",
      " state (4)  A[0]:(0.590182304382) A[1]:(0.656142950058) A[2]:(2.50339508057e-05) A[3]:(0.532093763351)\n",
      " state (5)  A[0]:(0.162765413523) A[1]:(0.928796410561) A[2]:(-0.196028396487) A[3]:(0.527650654316)\n",
      " state (6)  A[0]:(-0.000589072646108) A[1]:(0.810083687305) A[2]:(5.88893890381e-05) A[3]:(0.655917167664)\n",
      " state (7)  A[0]:(0.624546170235) A[1]:(-0.247924387455) A[2]:(0.310665249825) A[3]:(0.880979180336)\n",
      " state (8)  A[0]:(0.655593693256) A[1]:(0.000189356505871) A[2]:(0.729059875011) A[3]:(0.589326739311)\n",
      " state (9)  A[0]:(0.654829382896) A[1]:(0.810073852539) A[2]:(0.810076415539) A[3]:(-0.00166471151169)\n",
      " state (10)  A[0]:(0.72795188427) A[1]:(0.90007930994) A[2]:(0.00017237663269) A[3]:(0.728173613548)\n",
      " state (11)  A[0]:(0.522525310516) A[1]:(0.876662552357) A[2]:(-0.626430988312) A[3]:(0.844714999199)\n",
      " state (12)  A[0]:(0.079404681921) A[1]:(0.823792934418) A[2]:(-0.624268651009) A[3]:(0.7939889431)\n",
      " state (13)  A[0]:(-0.00120860280003) A[1]:(0.808609843254) A[2]:(0.900091171265) A[3]:(0.72812384367)\n",
      " state (14)  A[0]:(0.809503495693) A[1]:(0.900547981262) A[2]:(1.0) A[3]:(0.808866381645)\n",
      " state (15)  A[0]:(0.981873750687) A[1]:(0.956236004829) A[2]:(1.0) A[3]:(0.874008655548)\n",
      "Episode 874000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 998.               Steps done: 6409069. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00148190254162.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5904,  0.5905,  0.5322]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0001,  0.6562,  0.5928]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5910,  0.7290,  0.5907,  0.6591]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0055,  0.8100,  0.0003,  0.6628]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7298,  0.9000,  0.0006,  0.7368]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8105,  0.9000,  1.0000,  0.8156]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531096816063) A[1]:(0.590416789055) A[2]:(0.590424537659) A[3]:(0.532272458076)\n",
      " state (1)  A[0]:(0.531152963638) A[1]:(-0.000138886272907) A[2]:(0.6561242342) A[3]:(0.592951059341)\n",
      " state (2)  A[0]:(0.590860128403) A[1]:(0.729062914848) A[2]:(0.590858459473) A[3]:(0.659330070019)\n",
      " state (3)  A[0]:(0.656981348991) A[1]:(-0.214389041066) A[2]:(0.540203690529) A[3]:(0.523118972778)\n",
      " state (4)  A[0]:(0.592269539833) A[1]:(0.656133055687) A[2]:(0.000623106898274) A[3]:(0.538221776485)\n",
      " state (5)  A[0]:(0.167386442423) A[1]:(0.928875505924) A[2]:(-0.195837646723) A[3]:(0.535074830055)\n",
      " state (6)  A[0]:(0.00529844593257) A[1]:(0.810063838959) A[2]:(0.000390648812754) A[3]:(0.663140416145)\n",
      " state (7)  A[0]:(0.627641558647) A[1]:(-0.248117297888) A[2]:(0.311008244753) A[3]:(0.884412705898)\n",
      " state (8)  A[0]:(0.657973885536) A[1]:(0.000530920864549) A[2]:(0.729120671749) A[3]:(0.600442051888)\n",
      " state (9)  A[0]:(0.657243072987) A[1]:(0.810071110725) A[2]:(0.810094177723) A[3]:(0.0169741306454)\n",
      " state (10)  A[0]:(0.729889154434) A[1]:(0.899988174438) A[2]:(0.000674486043863) A[3]:(0.73713439703)\n",
      " state (11)  A[0]:(0.525508344173) A[1]:(0.876470029354) A[2]:(-0.625905871391) A[3]:(0.850309967995)\n",
      " state (12)  A[0]:(0.0834498107433) A[1]:(0.823415756226) A[2]:(-0.623779892921) A[3]:(0.801377654076)\n",
      " state (13)  A[0]:(0.00247573340312) A[1]:(0.80804091692) A[2]:(0.900024116039) A[3]:(0.737599015236)\n",
      " state (14)  A[0]:(0.81051325798) A[1]:(0.900116682053) A[2]:(1.0) A[3]:(0.815720319748)\n",
      " state (15)  A[0]:(0.981974303722) A[1]:(0.956000089645) A[2]:(1.0) A[3]:(0.878628134727)\n",
      "Episode 875000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 999.               Steps done: 6415080. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00147302154403.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53130364418) A[1]:(0.59046882391) A[2]:(0.590508759022) A[3]:(0.531551599503)\n",
      " state (1)  A[0]:(0.531164765358) A[1]:(0.000342316925526) A[2]:(0.656154751778) A[3]:(0.590697169304)\n",
      " state (2)  A[0]:(0.590596616268) A[1]:(0.72908616066) A[2]:(0.590317249298) A[3]:(0.65641951561)\n",
      " state (3)  A[0]:(0.656142115593) A[1]:(-0.213824659586) A[2]:(0.539372086525) A[3]:(0.517515897751)\n",
      " state (4)  A[0]:(0.590348482132) A[1]:(0.656194210052) A[2]:(-0.000178813934326) A[3]:(0.531377136707)\n",
      " state (5)  A[0]:(0.16318513453) A[1]:(0.928782105446) A[2]:(-0.196133047342) A[3]:(0.527254164219)\n",
      " state (6)  A[0]:(7.18832015991e-05) A[1]:(0.810030996799) A[2]:(-0.000115156173706) A[3]:(0.65617275238)\n",
      " state (7)  A[0]:(0.625154733658) A[1]:(-0.247976869345) A[2]:(0.310296297073) A[3]:(0.881392359734)\n",
      " state (8)  A[0]:(0.656544208527) A[1]:(0.00023490935564) A[2]:(0.728870987892) A[3]:(0.590464890003)\n",
      " state (9)  A[0]:(0.656251430511) A[1]:(0.810019433498) A[2]:(0.809909164906) A[3]:(-0.000401124329073)\n",
      " state (10)  A[0]:(0.729118049145) A[1]:(0.900002479553) A[2]:(-0.000181198120117) A[3]:(0.728783369064)\n",
      " state (11)  A[0]:(0.524123430252) A[1]:(0.876538276672) A[2]:(-0.626556813717) A[3]:(0.845166563988)\n",
      " state (12)  A[0]:(0.0812757983804) A[1]:(0.823584318161) A[2]:(-0.624396204948) A[3]:(0.794717133045)\n",
      " state (13)  A[0]:(0.000286638736725) A[1]:(0.808315634727) A[2]:(0.899988651276) A[3]:(0.729224681854)\n",
      " state (14)  A[0]:(0.809870958328) A[1]:(0.900314688683) A[2]:(1.0) A[3]:(0.809784531593)\n",
      " state (15)  A[0]:(0.981905341148) A[1]:(0.956101357937) A[2]:(1.0) A[3]:(0.874714493752)\n",
      "Episode 876000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 999.               Steps done: 6421082. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00146420694778.\n",
      " state (0)  A[0]:(0.530467629433) A[1]:(0.590622246265) A[2]:(0.590562582016) A[3]:(0.53364777565)\n",
      " state (1)  A[0]:(0.530773043633) A[1]:(-0.000104367733002) A[2]:(0.656145095825) A[3]:(0.59305524826)\n",
      " state (2)  A[0]:(0.590645372868) A[1]:(0.729259848595) A[2]:(0.590475916862) A[3]:(0.658889293671)\n",
      " state (3)  A[0]:(0.656488895416) A[1]:(-0.21224758029) A[2]:(0.539674639702) A[3]:(0.521768212318)\n",
      " state (4)  A[0]:(0.591243386269) A[1]:(0.656465768814) A[2]:(0.000804066483397) A[3]:(0.536043465137)\n",
      " state (5)  A[0]:(0.165395528078) A[1]:(0.928850710392) A[2]:(-0.194985866547) A[3]:(0.53228533268)\n",
      " state (6)  A[0]:(0.00305478810333) A[1]:(0.810250043869) A[2]:(0.00110471202061) A[3]:(0.660231947899)\n",
      " state (7)  A[0]:(0.627520740032) A[1]:(-0.247186109424) A[2]:(0.310971289873) A[3]:(0.883139491081)\n",
      " state (8)  A[0]:(0.659944176674) A[1]:(0.00154691806529) A[2]:(0.728796243668) A[3]:(0.598397612572)\n",
      " state (9)  A[0]:(0.660458862782) A[1]:(0.810427367687) A[2]:(0.809903860092) A[3]:(0.0159678887576)\n",
      " state (10)  A[0]:(0.732443630695) A[1]:(0.900102317333) A[2]:(0.00174665276427) A[3]:(0.735977113247)\n",
      " state (11)  A[0]:(0.529220223427) A[1]:(0.876540184021) A[2]:(-0.624671697617) A[3]:(0.849235236645)\n",
      " state (12)  A[0]:(0.0884058922529) A[1]:(0.823424577713) A[2]:(-0.622675776482) A[3]:(0.799796462059)\n",
      " state (13)  A[0]:(0.00710099935532) A[1]:(0.807869553566) A[2]:(0.899886488914) A[3]:(0.735402941704)\n",
      " state (14)  A[0]:(0.81201082468) A[1]:(0.899865746498) A[2]:(0.999999940395) A[3]:(0.81393301487)\n",
      " state (15)  A[0]:(0.98217689991) A[1]:(0.955848991871) A[2]:(1.0) A[3]:(0.877417564392)\n",
      "Episode 877000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 998.               Steps done: 6427087. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00145544073197.\n",
      " state (0)  A[0]:(0.531294703484) A[1]:(0.590519487858) A[2]:(0.590521931648) A[3]:(0.531451702118)\n",
      " state (1)  A[0]:(0.531473577023) A[1]:(1.07735395432e-05) A[2]:(0.656205654144) A[3]:(0.590867877007)\n",
      " state (2)  A[0]:(0.590923547745) A[1]:(0.729079723358) A[2]:(0.590793550014) A[3]:(0.656567931175)\n",
      " state (3)  A[0]:(0.656081914902) A[1]:(-0.214158535004) A[2]:(0.539862334728) A[3]:(0.517640352249)\n",
      " state (4)  A[0]:(0.590384483337) A[1]:(0.656179428101) A[2]:(6.55651092529e-05) A[3]:(0.531496763229)\n",
      " state (5)  A[0]:(0.163721457124) A[1]:(0.928847134113) A[2]:(-0.196335464716) A[3]:(0.527421236038)\n",
      " state (6)  A[0]:(0.000833511177916) A[1]:(0.810049235821) A[2]:(-0.00036513802479) A[3]:(0.656333684921)\n",
      " state (7)  A[0]:(0.624802410603) A[1]:(-0.247905030847) A[2]:(0.310048490763) A[3]:(0.881371200085)\n",
      " state (8)  A[0]:(0.655756652355) A[1]:(0.000820562068839) A[2]:(0.728824496269) A[3]:(0.590700566769)\n",
      " state (9)  A[0]:(0.655604600906) A[1]:(0.810151100159) A[2]:(0.810045540333) A[3]:(0.00107997609302)\n",
      " state (10)  A[0]:(0.728679537773) A[1]:(0.900004088879) A[2]:(0.000642061117105) A[3]:(0.72953248024)\n",
      " state (11)  A[0]:(0.523606419563) A[1]:(0.876458883286) A[2]:(-0.625950098038) A[3]:(0.845613121986)\n",
      " state (12)  A[0]:(0.0808174833655) A[1]:(0.823349118233) A[2]:(-0.623881697655) A[3]:(0.795316457748)\n",
      " state (13)  A[0]:(4.50015068054e-05) A[1]:(0.807875990868) A[2]:(0.899977684021) A[3]:(0.730037212372)\n",
      " state (14)  A[0]:(0.809941828251) A[1]:(0.899931490421) A[2]:(1.0) A[3]:(0.810463130474)\n",
      " state (15)  A[0]:(0.98194617033) A[1]:(0.955871701241) A[2]:(1.0) A[3]:(0.875271677971)\n",
      "Episode 878000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 1000.               Steps done: 6433094. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00144672410609.\n",
      " state (0)  A[0]:(0.531686306) A[1]:(0.590754806995) A[2]:(0.590438783169) A[3]:(0.531602144241)\n",
      " state (1)  A[0]:(0.531867980957) A[1]:(0.000376187235815) A[2]:(0.656155943871) A[3]:(0.590520083904)\n",
      " state (2)  A[0]:(0.591210305691) A[1]:(0.729100346565) A[2]:(0.591036081314) A[3]:(0.656118750572)\n",
      " state (3)  A[0]:(0.656398952007) A[1]:(-0.21714822948) A[2]:(0.540454626083) A[3]:(0.517404437065)\n",
      " state (4)  A[0]:(0.590565860271) A[1]:(0.656100511551) A[2]:(0.000364541978342) A[3]:(0.531792879105)\n",
      " state (5)  A[0]:(0.163376361132) A[1]:(0.928800284863) A[2]:(-0.195897713304) A[3]:(0.527697682381)\n",
      " state (6)  A[0]:(2.41994857788e-05) A[1]:(0.81006193161) A[2]:(-7.15255737305e-07) A[3]:(0.656489253044)\n",
      " state (7)  A[0]:(0.624845147133) A[1]:(-0.247879222035) A[2]:(0.310466617346) A[3]:(0.881485462189)\n",
      " state (8)  A[0]:(0.656198084354) A[1]:(0.000553272606339) A[2]:(0.729172348976) A[3]:(0.590853214264)\n",
      " state (9)  A[0]:(0.656129717827) A[1]:(0.810072541237) A[2]:(0.810055375099) A[3]:(0.00168469385244)\n",
      " state (10)  A[0]:(0.729056417942) A[1]:(0.90000808239) A[2]:(9.2625617981e-05) A[3]:(0.729890882969)\n",
      " state (11)  A[0]:(0.523992598057) A[1]:(0.876561820507) A[2]:(-0.626396715641) A[3]:(0.845767617226)\n",
      " state (12)  A[0]:(0.0810825899243) A[1]:(0.823660850525) A[2]:(-0.624247312546) A[3]:(0.79540348053)\n",
      " state (13)  A[0]:(0.000274419784546) A[1]:(0.808430433273) A[2]:(0.900073409081) A[3]:(0.729983508587)\n",
      " state (14)  A[0]:(0.81009376049) A[1]:(0.900361537933) A[2]:(1.0) A[3]:(0.810238182545)\n",
      " state (15)  A[0]:(0.981944918633) A[1]:(0.95611000061) A[2]:(1.0) A[3]:(0.874936103821)\n",
      "Episode 879000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 997.               Steps done: 6439101. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00143805968403.\n",
      "q_values \n",
      "tensor([[ 0.5321,  0.5910,  0.5907,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5913,  0.6565,  0.0001,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6569,  0.0005,  0.7293,  0.5897]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6567,  0.8100,  0.8099,  0.0005]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0014,  0.8086,  0.9001,  0.7298]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9004,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531970143318) A[1]:(0.590956628323) A[2]:(0.59063243866) A[3]:(0.531724214554)\n",
      " state (1)  A[0]:(0.531900644302) A[1]:(0.00036563721369) A[2]:(0.656282424927) A[3]:(0.590526223183)\n",
      " state (2)  A[0]:(0.591208875179) A[1]:(0.72910130024) A[2]:(0.591002345085) A[3]:(0.656060576439)\n",
      " state (3)  A[0]:(0.656791090965) A[1]:(-0.217125400901) A[2]:(0.540226757526) A[3]:(0.517289638519)\n",
      " state (4)  A[0]:(0.591047406197) A[1]:(0.65632879734) A[2]:(7.0333480835e-06) A[3]:(0.531614720821)\n",
      " state (5)  A[0]:(0.163874596357) A[1]:(0.928775668144) A[2]:(-0.196006387472) A[3]:(0.527376174927)\n",
      " state (6)  A[0]:(0.000611305178609) A[1]:(0.809922814369) A[2]:(-0.000120282173157) A[3]:(0.65615773201)\n",
      " state (7)  A[0]:(0.625442266464) A[1]:(-0.248253777623) A[2]:(0.31024479866) A[3]:(0.881322145462)\n",
      " state (8)  A[0]:(0.656746327877) A[1]:(0.000259719789028) A[2]:(0.729104995728) A[3]:(0.590137660503)\n",
      " state (9)  A[0]:(0.656528294086) A[1]:(0.809970855713) A[2]:(0.809941649437) A[3]:(0.000676780822687)\n",
      " state (10)  A[0]:(0.729363679886) A[1]:(0.899948358536) A[2]:(-0.000395655602915) A[3]:(0.729526400566)\n",
      " state (11)  A[0]:(0.524500846863) A[1]:(0.87650346756) A[2]:(-0.62671983242) A[3]:(0.845590233803)\n",
      " state (12)  A[0]:(0.081791266799) A[1]:(0.823605179787) A[2]:(-0.624610304832) A[3]:(0.795224368572)\n",
      " state (13)  A[0]:(0.00092077231966) A[1]:(0.808387517929) A[2]:(0.899926662445) A[3]:(0.729801058769)\n",
      " state (14)  A[0]:(0.810287475586) A[1]:(0.90033519268) A[2]:(1.0) A[3]:(0.810136914253)\n",
      " state (15)  A[0]:(0.98196297884) A[1]:(0.9560983181) A[2]:(1.0) A[3]:(0.87488090992)\n",
      "Episode 880000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 998.               Steps done: 6445109. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00142944572369.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.529918551445) A[1]:(0.590383708477) A[2]:(0.590569496155) A[3]:(0.531323552132)\n",
      " state (1)  A[0]:(0.529524207115) A[1]:(-4.91216778755e-05) A[2]:(0.656158804893) A[3]:(0.590385079384)\n",
      " state (2)  A[0]:(0.588872373104) A[1]:(0.729068875313) A[2]:(0.590749323368) A[3]:(0.656039714813)\n",
      " state (3)  A[0]:(0.65434896946) A[1]:(-0.214575260878) A[2]:(0.539904236794) A[3]:(0.517345190048)\n",
      " state (4)  A[0]:(0.58831101656) A[1]:(0.65607714653) A[2]:(0.00020444393158) A[3]:(0.531376183033)\n",
      " state (5)  A[0]:(0.160446047783) A[1]:(0.928802371025) A[2]:(-0.19606359303) A[3]:(0.527291297913)\n",
      " state (6)  A[0]:(-0.00252907932736) A[1]:(0.810030758381) A[2]:(-0.000136256217957) A[3]:(0.656351923943)\n",
      " state (7)  A[0]:(0.623081028461) A[1]:(-0.247817456722) A[2]:(0.310257405043) A[3]:(0.881495952606)\n",
      " state (8)  A[0]:(0.654265403748) A[1]:(0.000839255575556) A[2]:(0.728991687298) A[3]:(0.590586543083)\n",
      " state (9)  A[0]:(0.654073238373) A[1]:(0.81011903286) A[2]:(0.809940099716) A[3]:(0.000454306573374)\n",
      " state (10)  A[0]:(0.727522552013) A[1]:(0.899999082088) A[2]:(-0.000166296958923) A[3]:(0.729270577431)\n",
      " state (11)  A[0]:(0.521969437599) A[1]:(0.876514554024) A[2]:(-0.626517653465) A[3]:(0.845466136932)\n",
      " state (12)  A[0]:(0.078727722168) A[1]:(0.823533117771) A[2]:(-0.624359071255) A[3]:(0.79512834549)\n",
      " state (13)  A[0]:(-0.00181126396637) A[1]:(0.80819016695) A[2]:(0.900015056133) A[3]:(0.729777812958)\n",
      " state (14)  A[0]:(0.809440791607) A[1]:(0.900142133236) A[2]:(1.0) A[3]:(0.810221552849)\n",
      " state (15)  A[0]:(0.981883049011) A[1]:(0.955968499184) A[2]:(1.0) A[3]:(0.875007212162)\n",
      "Episode 881000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 999.               Steps done: 6451116. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00142088478176.\n",
      " state (0)  A[0]:(0.534598112106) A[1]:(0.589786112309) A[2]:(0.590435028076) A[3]:(0.528328299522)\n",
      " state (1)  A[0]:(0.534510910511) A[1]:(-0.000527515949216) A[2]:(0.655965566635) A[3]:(0.587730765343)\n",
      " state (2)  A[0]:(0.59367954731) A[1]:(0.729087233543) A[2]:(0.590203821659) A[3]:(0.653875112534)\n",
      " state (3)  A[0]:(0.659180998802) A[1]:(-0.213087618351) A[2]:(0.538982450962) A[3]:(0.515200138092)\n",
      " state (4)  A[0]:(0.594287157059) A[1]:(0.655808806419) A[2]:(-0.000721692922525) A[3]:(0.529414951801)\n",
      " state (5)  A[0]:(0.169505089521) A[1]:(0.928754866123) A[2]:(-0.196927607059) A[3]:(0.525511980057)\n",
      " state (6)  A[0]:(0.00562286423519) A[1]:(0.8101952672) A[2]:(-0.00102329219226) A[3]:(0.655058979988)\n",
      " state (7)  A[0]:(0.627380251884) A[1]:(-0.247320696712) A[2]:(0.309837073088) A[3]:(0.881185829639)\n",
      " state (8)  A[0]:(0.657755017281) A[1]:(0.000993474968709) A[2]:(0.729057431221) A[3]:(0.590465188026)\n",
      " state (9)  A[0]:(0.656563043594) A[1]:(0.810153961182) A[2]:(0.809993624687) A[3]:(0.00169078842737)\n",
      " state (10)  A[0]:(0.728704571724) A[1]:(0.899986624718) A[2]:(0.000531792582478) A[3]:(0.729938089848)\n",
      " state (11)  A[0]:(0.522788882256) A[1]:(0.876494586468) A[2]:(-0.625802159309) A[3]:(0.845858335495)\n",
      " state (12)  A[0]:(0.0787344127893) A[1]:(0.82352411747) A[2]:(-0.623575687408) A[3]:(0.795678496361)\n",
      " state (13)  A[0]:(-0.0029640707653) A[1]:(0.808199167252) A[2]:(0.900170981884) A[3]:(0.73054420948)\n",
      " state (14)  A[0]:(0.808605134487) A[1]:(0.90015488863) A[2]:(1.0) A[3]:(0.810841619968)\n",
      " state (15)  A[0]:(0.981762588024) A[1]:(0.955984532833) A[2]:(1.0) A[3]:(0.875476360321)\n",
      "Episode 882000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5997. Times reached goal: 999.               Steps done: 6457113. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00141238923508.\n",
      " state (0)  A[0]:(0.531100273132) A[1]:(0.590341985226) A[2]:(0.590596318245) A[3]:(0.532787501812)\n",
      " state (1)  A[0]:(0.531129360199) A[1]:(-0.000452980370028) A[2]:(0.656137943268) A[3]:(0.591892719269)\n",
      " state (2)  A[0]:(0.590466737747) A[1]:(0.729019284248) A[2]:(0.591027617455) A[3]:(0.657326579094)\n",
      " state (3)  A[0]:(0.656202793121) A[1]:(-0.217219993472) A[2]:(0.540397167206) A[3]:(0.518995046616)\n",
      " state (4)  A[0]:(0.590479135513) A[1]:(0.656332433224) A[2]:(0.000303745269775) A[3]:(0.533248543739)\n",
      " state (5)  A[0]:(0.163190886378) A[1]:(0.928808152676) A[2]:(-0.195770248771) A[3]:(0.528951406479)\n",
      " state (6)  A[0]:(-0.000162184238434) A[1]:(0.810129880905) A[2]:(5.85317611694e-05) A[3]:(0.657325804234)\n",
      " state (7)  A[0]:(0.624942064285) A[1]:(-0.24762134254) A[2]:(0.310446351767) A[3]:(0.881757736206)\n",
      " state (8)  A[0]:(0.656296551228) A[1]:(0.000684723141603) A[2]:(0.72924721241) A[3]:(0.591100096703)\n",
      " state (9)  A[0]:(0.655972480774) A[1]:(0.810161352158) A[2]:(0.810176253319) A[3]:(0.000899299746379)\n",
      " state (10)  A[0]:(0.728958487511) A[1]:(0.900084614754) A[2]:(0.000235080718994) A[3]:(0.729596793652)\n",
      " state (11)  A[0]:(0.523925423622) A[1]:(0.876685500145) A[2]:(-0.626414060593) A[3]:(0.845722734928)\n",
      " state (12)  A[0]:(0.080923832953) A[1]:(0.823858499527) A[2]:(-0.624290823936) A[3]:(0.795457661152)\n",
      " state (13)  A[0]:(-1.01327896118e-05) A[1]:(0.80863404274) A[2]:(0.900096952915) A[3]:(0.73011213541)\n",
      " state (14)  A[0]:(0.810031652451) A[1]:(0.90042579174) A[2]:(1.0) A[3]:(0.810336828232)\n",
      " state (15)  A[0]:(0.981930673122) A[1]:(0.956109046936) A[2]:(1.0) A[3]:(0.874954581261)\n",
      "Episode 883000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6005. Times reached goal: 999.               Steps done: 6463118. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00140393325223.\n",
      " state (0)  A[0]:(0.532908439636) A[1]:(0.590349674225) A[2]:(0.590425372124) A[3]:(0.531815767288)\n",
      " state (1)  A[0]:(0.532335281372) A[1]:(3.31997871399e-05) A[2]:(0.65605121851) A[3]:(0.590743422508)\n",
      " state (2)  A[0]:(0.591225147247) A[1]:(0.72870016098) A[2]:(0.590714991093) A[3]:(0.656160116196)\n",
      " state (3)  A[0]:(0.65626013279) A[1]:(-0.218094915152) A[2]:(0.540397524834) A[3]:(0.517025709152)\n",
      " state (4)  A[0]:(0.590446174145) A[1]:(0.655897319317) A[2]:(0.000335693359375) A[3]:(0.53118366003)\n",
      " state (5)  A[0]:(0.163604691625) A[1]:(0.928755760193) A[2]:(-0.195871472359) A[3]:(0.526974737644)\n",
      " state (6)  A[0]:(0.00122708023991) A[1]:(0.809823513031) A[2]:(0.000169277191162) A[3]:(0.655953228474)\n",
      " state (7)  A[0]:(0.626005887985) A[1]:(-0.248650372028) A[2]:(0.310433447361) A[3]:(0.881288170815)\n",
      " state (8)  A[0]:(0.65706050396) A[1]:(-4.17828559875e-05) A[2]:(0.728711843491) A[3]:(0.590497255325)\n",
      " state (9)  A[0]:(0.656236112118) A[1]:(0.810003399849) A[2]:(0.809963703156) A[3]:(-0.000830143515486)\n",
      " state (10)  A[0]:(0.728818774223) A[1]:(0.900023818016) A[2]:(0.000590205134358) A[3]:(0.72828233242)\n",
      " state (11)  A[0]:(0.523481011391) A[1]:(0.876616895199) A[2]:(-0.625926673412) A[3]:(0.844852387905)\n",
      " state (12)  A[0]:(0.0801602825522) A[1]:(0.823761045933) A[2]:(-0.623626947403) A[3]:(0.794343113899)\n",
      " state (13)  A[0]:(-0.00078338367166) A[1]:(0.80851328373) A[2]:(0.900413513184) A[3]:(0.728738725185)\n",
      " state (14)  A[0]:(0.809849858284) A[1]:(0.900335967541) A[2]:(1.0) A[3]:(0.809383809566)\n",
      " state (15)  A[0]:(0.98190677166) A[1]:(0.95604544878) A[2]:(1.0) A[3]:(0.8743044734)\n",
      "Episode 884000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 6469122. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00139552929092.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5907,  0.5906,  0.5308]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.6560,  0.0004,  0.5310]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6568,  0.0004,  0.7289,  0.5912]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.8102,  0.8103,  0.0005]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.8998, -0.0005,  0.7295]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8105,  0.9004,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531719148159) A[1]:(0.590546607971) A[2]:(0.590386629105) A[3]:(0.530684769154)\n",
      " state (1)  A[0]:(0.531396210194) A[1]:(-0.000346526503563) A[2]:(0.655953466892) A[3]:(0.590191721916)\n",
      " state (2)  A[0]:(0.590541243553) A[1]:(0.728916227818) A[2]:(0.590687274933) A[3]:(0.65586066246)\n",
      " state (3)  A[0]:(0.656395435333) A[1]:(-0.216744229198) A[2]:(0.540077447891) A[3]:(0.516851902008)\n",
      " state (4)  A[0]:(0.590785980225) A[1]:(0.655955672264) A[2]:(6.78300857544e-05) A[3]:(0.531030297279)\n",
      " state (5)  A[0]:(0.163794904947) A[1]:(0.928726911545) A[2]:(-0.196126177907) A[3]:(0.526933193207)\n",
      " state (6)  A[0]:(0.000431954831583) A[1]:(0.809940159321) A[2]:(-0.000449061364634) A[3]:(0.656103253365)\n",
      " state (7)  A[0]:(0.625260591507) A[1]:(-0.248269736767) A[2]:(0.309709370136) A[3]:(0.881567955017)\n",
      " state (8)  A[0]:(0.656865596771) A[1]:(-0.00018022954464) A[2]:(0.728589653969) A[3]:(0.591454386711)\n",
      " state (9)  A[0]:(0.656596302986) A[1]:(0.809826135635) A[2]:(0.809760034084) A[3]:(0.00140313711017)\n",
      " state (10)  A[0]:(0.729336977005) A[1]:(0.899866998196) A[2]:(-0.000328540802002) A[3]:(0.729517817497)\n",
      " state (11)  A[0]:(0.52444010973) A[1]:(0.876388847828) A[2]:(-0.626540839672) A[3]:(0.845616459846)\n",
      " state (12)  A[0]:(0.0815942212939) A[1]:(0.823403060436) A[2]:(-0.62437915802) A[3]:(0.795334339142)\n",
      " state (13)  A[0]:(0.000618159712758) A[1]:(0.808064699173) A[2]:(0.900014281273) A[3]:(0.729996562004)\n",
      " state (14)  A[0]:(0.810314834118) A[1]:(0.900044381618) A[2]:(1.0) A[3]:(0.810330927372)\n",
      " state (15)  A[0]:(0.981970369816) A[1]:(0.955904781818) A[2]:(1.0) A[3]:(0.874998629093)\n",
      "Episode 885000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 6475126. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00138717563582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530860543251) A[1]:(0.590546369553) A[2]:(0.590431809425) A[3]:(0.530853748322)\n",
      " state (1)  A[0]:(0.530721545219) A[1]:(0.000303186476231) A[2]:(0.656116962433) A[3]:(0.590016722679)\n",
      " state (2)  A[0]:(0.589934587479) A[1]:(0.729057848454) A[2]:(0.590619564056) A[3]:(0.655631422997)\n",
      " state (3)  A[0]:(0.655675172806) A[1]:(-0.216132745147) A[2]:(0.539970755577) A[3]:(0.517023742199)\n",
      " state (4)  A[0]:(0.589821815491) A[1]:(0.656186580658) A[2]:(0.000144362449646) A[3]:(0.53124499321)\n",
      " state (5)  A[0]:(0.162315621972) A[1]:(0.928759932518) A[2]:(-0.19581656158) A[3]:(0.527012586594)\n",
      " state (6)  A[0]:(-0.00107926083729) A[1]:(0.810054302216) A[2]:(4.76837158203e-06) A[3]:(0.656011283398)\n",
      " state (7)  A[0]:(0.624378204346) A[1]:(-0.247838661075) A[2]:(0.310285538435) A[3]:(0.881423532963)\n",
      " state (8)  A[0]:(0.656115651131) A[1]:(0.000386930973036) A[2]:(0.729093670845) A[3]:(0.590569972992)\n",
      " state (9)  A[0]:(0.656044006348) A[1]:(0.810029089451) A[2]:(0.810059607029) A[3]:(0.000244051218033)\n",
      " state (10)  A[0]:(0.7291046381) A[1]:(0.899986863136) A[2]:(0.000277996063232) A[3]:(0.729246795177)\n",
      " state (11)  A[0]:(0.524315953255) A[1]:(0.876545131207) A[2]:(-0.626199960709) A[3]:(0.845548152924)\n",
      " state (12)  A[0]:(0.0815524831414) A[1]:(0.82361137867) A[2]:(-0.624096274376) A[3]:(0.795316457748)\n",
      " state (13)  A[0]:(0.000415027112467) A[1]:(0.808238089085) A[2]:(0.899995744228) A[3]:(0.730025529861)\n",
      " state (14)  A[0]:(0.810094892979) A[1]:(0.900087893009) A[2]:(1.0) A[3]:(0.810410618782)\n",
      " state (15)  A[0]:(0.981940090656) A[1]:(0.955905079842) A[2]:(1.0) A[3]:(0.875101268291)\n",
      "Episode 886000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 998.               Steps done: 6481129. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00137887336468.\n",
      " state (0)  A[0]:(0.531559586525) A[1]:(0.591109275818) A[2]:(0.590611457825) A[3]:(0.531989812851)\n",
      " state (1)  A[0]:(0.53112500906) A[1]:(0.000880382722244) A[2]:(0.656312525272) A[3]:(0.590579867363)\n",
      " state (2)  A[0]:(0.590114951134) A[1]:(0.729173660278) A[2]:(0.591109156609) A[3]:(0.655951321125)\n",
      " state (3)  A[0]:(0.655607163906) A[1]:(-0.216867044568) A[2]:(0.540530800819) A[3]:(0.517653048038)\n",
      " state (4)  A[0]:(0.589941680431) A[1]:(0.655769348145) A[2]:(0.000545978487935) A[3]:(0.532081246376)\n",
      " state (5)  A[0]:(0.163017168641) A[1]:(0.928740620613) A[2]:(-0.195833295584) A[3]:(0.528033733368)\n",
      " state (6)  A[0]:(-0.000129342079163) A[1]:(0.810126125813) A[2]:(-8.27312469482e-05) A[3]:(0.656861424446)\n",
      " state (7)  A[0]:(0.62496316433) A[1]:(-0.247677057981) A[2]:(0.310417294502) A[3]:(0.881741642952)\n",
      " state (8)  A[0]:(0.656356930733) A[1]:(0.000511579157319) A[2]:(0.729188680649) A[3]:(0.59129357338)\n",
      " state (9)  A[0]:(0.655716717243) A[1]:(0.810072243214) A[2]:(0.810102343559) A[3]:(0.000837087456603)\n",
      " state (10)  A[0]:(0.728616416454) A[1]:(0.90000474453) A[2]:(0.000692248228006) A[3]:(0.729366540909)\n",
      " state (11)  A[0]:(0.523520112038) A[1]:(0.876557350159) A[2]:(-0.62579202652) A[3]:(0.845592975616)\n",
      " state (12)  A[0]:(0.080454364419) A[1]:(0.823594987392) A[2]:(-0.62368452549) A[3]:(0.795367777348)\n",
      " state (13)  A[0]:(-0.000943898863625) A[1]:(0.808136343956) A[2]:(0.899997889996) A[3]:(0.730047702789)\n",
      " state (14)  A[0]:(0.809439182281) A[1]:(0.89995443821) A[2]:(1.0) A[3]:(0.810364127159)\n",
      " state (15)  A[0]:(0.981864273548) A[1]:(0.955814480782) A[2]:(1.0) A[3]:(0.87504529953)\n",
      "Episode 887000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 1000.               Steps done: 6487133. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00137061941216.\n",
      " state (0)  A[0]:(0.53205704689) A[1]:(0.590643167496) A[2]:(0.590576350689) A[3]:(0.530667304993)\n",
      " state (1)  A[0]:(0.531579256058) A[1]:(0.000136911869049) A[2]:(0.656233072281) A[3]:(0.59047293663)\n",
      " state (2)  A[0]:(0.590572655201) A[1]:(0.729189753532) A[2]:(0.591058552265) A[3]:(0.656197190285)\n",
      " state (3)  A[0]:(0.656163215637) A[1]:(-0.216322079301) A[2]:(0.540392041206) A[3]:(0.517978429794)\n",
      " state (4)  A[0]:(0.590434193611) A[1]:(0.656299948692) A[2]:(0.00040233132313) A[3]:(0.532330930233)\n",
      " state (5)  A[0]:(0.163360238075) A[1]:(0.928813517094) A[2]:(-0.195843502879) A[3]:(0.528081178665)\n",
      " state (6)  A[0]:(6.40153884888e-05) A[1]:(0.810110509396) A[2]:(-0.000208973884583) A[3]:(0.656656742096)\n",
      " state (7)  A[0]:(0.624580860138) A[1]:(-0.247699379921) A[2]:(0.30997556448) A[3]:(0.881405889988)\n",
      " state (8)  A[0]:(0.655603587627) A[1]:(0.000776111904997) A[2]:(0.729108214378) A[3]:(0.589577138424)\n",
      " state (9)  A[0]:(0.655224502087) A[1]:(0.810161650181) A[2]:(0.810016870499) A[3]:(-0.00133053876925)\n",
      " state (10)  A[0]:(0.728492975235) A[1]:(0.90010112524) A[2]:(-0.000469565362437) A[3]:(0.728862762451)\n",
      " state (11)  A[0]:(0.523455500603) A[1]:(0.87676101923) A[2]:(-0.626831650734) A[3]:(0.845407485962)\n",
      " state (12)  A[0]:(0.080410130322) A[1]:(0.824031591415) A[2]:(-0.624628663063) A[3]:(0.795132160187)\n",
      " state (13)  A[0]:(-0.000573277415242) A[1]:(0.808846831322) A[2]:(0.9000852108) A[3]:(0.729716420174)\n",
      " state (14)  A[0]:(0.809826910496) A[1]:(0.900508344173) A[2]:(1.0) A[3]:(0.810049951077)\n",
      " state (15)  A[0]:(0.981887340546) A[1]:(0.95611512661) A[2]:(1.0) A[3]:(0.874686956406)\n",
      "Episode 888000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5993. Times reached goal: 997.               Steps done: 6493126. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00136242985454.\n",
      " state (0)  A[0]:(0.531463027) A[1]:(0.590441465378) A[2]:(0.590464413166) A[3]:(0.531414747238)\n",
      " state (1)  A[0]:(0.531386017799) A[1]:(5.80847263336e-05) A[2]:(0.656057476997) A[3]:(0.590535163879)\n",
      " state (2)  A[0]:(0.590567231178) A[1]:(0.72904086113) A[2]:(0.590816736221) A[3]:(0.656003117561)\n",
      " state (3)  A[0]:(0.656184911728) A[1]:(-0.216896668077) A[2]:(0.540346205235) A[3]:(0.517243087292)\n",
      " state (4)  A[0]:(0.590529024601) A[1]:(0.6561024189) A[2]:(0.000482916802866) A[3]:(0.531424641609)\n",
      " state (5)  A[0]:(0.163588911295) A[1]:(0.928779840469) A[2]:(-0.195647120476) A[3]:(0.527170717716)\n",
      " state (6)  A[0]:(0.000243306159973) A[1]:(0.810029029846) A[2]:(0.000173568725586) A[3]:(0.656164526939)\n",
      " state (7)  A[0]:(0.624588787556) A[1]:(-0.247969731688) A[2]:(0.310415446758) A[3]:(0.88144493103)\n",
      " state (8)  A[0]:(0.655968546867) A[1]:(0.000413268775446) A[2]:(0.729153037071) A[3]:(0.590341091156)\n",
      " state (9)  A[0]:(0.655955672264) A[1]:(0.810024857521) A[2]:(0.81010478735) A[3]:(-0.000313147902489)\n",
      " state (10)  A[0]:(0.729094386101) A[1]:(0.900025367737) A[2]:(0.000166654586792) A[3]:(0.729063868523)\n",
      " state (11)  A[0]:(0.524308443069) A[1]:(0.87667298317) A[2]:(-0.626368463039) A[3]:(0.845437407494)\n",
      " state (12)  A[0]:(0.0814749151468) A[1]:(0.82391679287) A[2]:(-0.624244689941) A[3]:(0.795096337795)\n",
      " state (13)  A[0]:(0.000373840302927) A[1]:(0.808716773987) A[2]:(0.900056183338) A[3]:(0.729559659958)\n",
      " state (14)  A[0]:(0.810172379017) A[1]:(0.900426745415) A[2]:(1.0) A[3]:(0.809816241264)\n",
      " state (15)  A[0]:(0.981942534447) A[1]:(0.956084609032) A[2]:(1.0) A[3]:(0.874484479427)\n",
      "Episode 889000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 997.               Steps done: 6499126. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00135427975018.\n",
      "q_values \n",
      "tensor([[ 0.5321,  0.5906,  0.5904,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5922,  0.6563,  0.0002,  0.5326]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6596,  0.0006,  0.7292,  0.5934]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6605,  0.8098,  0.8100,  0.0045]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7333,  0.9000,  0.0010,  0.7311]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8130,  0.9001,  1.0000,  0.8120]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.5314463377) A[1]:(0.590620398521) A[2]:(0.590495884418) A[3]:(0.530804395676)\n",
      " state (1)  A[0]:(0.532092213631) A[1]:(0.000147998332977) A[2]:(0.656138956547) A[3]:(0.59039747715)\n",
      " state (2)  A[0]:(0.591655373573) A[1]:(0.72892165184) A[2]:(0.5907792449) A[3]:(0.65622651577)\n",
      " state (3)  A[0]:(0.657496452332) A[1]:(-0.217023983598) A[2]:(0.540211081505) A[3]:(0.51769900322)\n",
      " state (4)  A[0]:(0.592379450798) A[1]:(0.655922651291) A[2]:(0.000391244859202) A[3]:(0.532240152359)\n",
      " state (5)  A[0]:(0.16685988009) A[1]:(0.928722798824) A[2]:(-0.19556479156) A[3]:(0.528515219688)\n",
      " state (6)  A[0]:(0.00448706699535) A[1]:(0.809920907021) A[2]:(0.000354409188731) A[3]:(0.657768964767)\n",
      " state (7)  A[0]:(0.628271877766) A[1]:(-0.248184785247) A[2]:(0.310435801744) A[3]:(0.882411599159)\n",
      " state (8)  A[0]:(0.660650968552) A[1]:(0.000194787979126) A[2]:(0.72883939743) A[3]:(0.594223916531)\n",
      " state (9)  A[0]:(0.661479234695) A[1]:(0.809961080551) A[2]:(0.80996286869) A[3]:(0.00547972787172)\n",
      " state (10)  A[0]:(0.733774662018) A[1]:(0.899940609932) A[2]:(0.000662803533487) A[3]:(0.731455922127)\n",
      " state (11)  A[0]:(0.531678199768) A[1]:(0.876484513283) A[2]:(-0.62578368187) A[3]:(0.846922159195)\n",
      " state (12)  A[0]:(0.0917318686843) A[1]:(0.823513448238) A[2]:(-0.623682677746) A[3]:(0.797192037106)\n",
      " state (13)  A[0]:(0.0104984361678) A[1]:(0.808074235916) A[2]:(0.900031089783) A[3]:(0.732492446899)\n",
      " state (14)  A[0]:(0.813413619995) A[1]:(0.899929702282) A[2]:(1.0) A[3]:(0.812202632427)\n",
      " state (15)  A[0]:(0.982279360294) A[1]:(0.955804526806) A[2]:(1.0) A[3]:(0.87630957365)\n",
      "Episode 890000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 999.               Steps done: 6505136. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00134616493831.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530180692673) A[1]:(0.590640068054) A[2]:(0.590548038483) A[3]:(0.531231284142)\n",
      " state (1)  A[0]:(0.529849052429) A[1]:(0.000410996348364) A[2]:(0.656149625778) A[3]:(0.590613245964)\n",
      " state (2)  A[0]:(0.589250385761) A[1]:(0.72912800312) A[2]:(0.590598583221) A[3]:(0.656304955482)\n",
      " state (3)  A[0]:(0.655535340309) A[1]:(-0.216147065163) A[2]:(0.539698839188) A[3]:(0.518040776253)\n",
      " state (4)  A[0]:(0.590015888214) A[1]:(0.656245112419) A[2]:(-0.000401020020945) A[3]:(0.532345950603)\n",
      " state (5)  A[0]:(0.162806898355) A[1]:(0.928772032261) A[2]:(-0.196316897869) A[3]:(0.528153896332)\n",
      " state (6)  A[0]:(-0.000680982950144) A[1]:(0.809948205948) A[2]:(-0.000367999047739) A[3]:(0.657021999359)\n",
      " state (7)  A[0]:(0.62395465374) A[1]:(-0.247990652919) A[2]:(0.310013383627) A[3]:(0.881838142872)\n",
      " state (8)  A[0]:(0.655885696411) A[1]:(0.000935814983677) A[2]:(0.729112863541) A[3]:(0.591706037521)\n",
      " state (9)  A[0]:(0.656677722931) A[1]:(0.810104370117) A[2]:(0.810006260872) A[3]:(0.00305942236446)\n",
      " state (10)  A[0]:(0.72960036993) A[1]:(0.899924814701) A[2]:(0.000486969918711) A[3]:(0.730363488197)\n",
      " state (11)  A[0]:(0.524768233299) A[1]:(0.876412987709) A[2]:(-0.625856757164) A[3]:(0.846038818359)\n",
      " state (12)  A[0]:(0.0819220468402) A[1]:(0.82338821888) A[2]:(-0.623766183853) A[3]:(0.795851230621)\n",
      " state (13)  A[0]:(0.000744163873605) A[1]:(0.807941377163) A[2]:(0.899996876717) A[3]:(0.730657935143)\n",
      " state (14)  A[0]:(0.810343265533) A[1]:(0.899869441986) A[2]:(1.0) A[3]:(0.810844063759)\n",
      " state (15)  A[0]:(0.981994509697) A[1]:(0.955788016319) A[2]:(1.0) A[3]:(0.875416278839)\n",
      "Episode 891000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 1000.               Steps done: 6511141. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00133810544072.\n",
      " state (0)  A[0]:(0.531227648258) A[1]:(0.590479373932) A[2]:(0.590489149094) A[3]:(0.531393170357)\n",
      " state (1)  A[0]:(0.530850172043) A[1]:(5.14090061188e-06) A[2]:(0.656118750572) A[3]:(0.590497970581)\n",
      " state (2)  A[0]:(0.590138792992) A[1]:(0.729022860527) A[2]:(0.590739786625) A[3]:(0.656065106392)\n",
      " state (3)  A[0]:(0.656178951263) A[1]:(-0.216775760055) A[2]:(0.540167808533) A[3]:(0.517214894295)\n",
      " state (4)  A[0]:(0.590622007847) A[1]:(0.65609818697) A[2]:(0.000393390626414) A[3]:(0.531419038773)\n",
      " state (5)  A[0]:(0.163499176502) A[1]:(0.928744375706) A[2]:(-0.195517331362) A[3]:(0.527218699455)\n",
      " state (6)  A[0]:(-0.000473380059702) A[1]:(0.810053348541) A[2]:(0.000207543373108) A[3]:(0.65610742569)\n",
      " state (7)  A[0]:(0.624068021774) A[1]:(-0.247861340642) A[2]:(0.310258179903) A[3]:(0.881411492825)\n",
      " state (8)  A[0]:(0.655862808228) A[1]:(0.000307217240334) A[2]:(0.729038000107) A[3]:(0.590542256832)\n",
      " state (9)  A[0]:(0.656075358391) A[1]:(0.810026168823) A[2]:(0.810054302216) A[3]:(5.93215227127e-05)\n",
      " state (10)  A[0]:(0.729189276695) A[1]:(0.900030195713) A[2]:(0.000238180160522) A[3]:(0.729047298431)\n",
      " state (11)  A[0]:(0.524465799332) A[1]:(0.87668389082) A[2]:(-0.626279830933) A[3]:(0.845372974873)\n",
      " state (12)  A[0]:(0.0818071886897) A[1]:(0.823948919773) A[2]:(-0.624206483364) A[3]:(0.795029520988)\n",
      " state (13)  A[0]:(0.00078076106729) A[1]:(0.808768928051) A[2]:(0.89999961853) A[3]:(0.72956097126)\n",
      " state (14)  A[0]:(0.810260653496) A[1]:(0.900460422039) A[2]:(1.0) A[3]:(0.809923052788)\n",
      " state (15)  A[0]:(0.981955826283) A[1]:(0.956109464169) A[2]:(1.0) A[3]:(0.874660491943)\n",
      "Episode 892000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6001. Times reached goal: 997.               Steps done: 6517142. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00133009951577.\n",
      " state (0)  A[0]:(0.528233885765) A[1]:(0.590571880341) A[2]:(0.59054005146) A[3]:(0.532780826092)\n",
      " state (1)  A[0]:(0.528611660004) A[1]:(0.000304542481899) A[2]:(0.656189918518) A[3]:(0.591549873352)\n",
      " state (2)  A[0]:(0.588367819786) A[1]:(0.729251027107) A[2]:(0.590545892715) A[3]:(0.656950712204)\n",
      " state (3)  A[0]:(0.655086696148) A[1]:(-0.215526387095) A[2]:(0.539575994015) A[3]:(0.518508195877)\n",
      " state (4)  A[0]:(0.589308738708) A[1]:(0.656304836273) A[2]:(-0.000274300575256) A[3]:(0.532488584518)\n",
      " state (5)  A[0]:(0.16094186902) A[1]:(0.928741335869) A[2]:(-0.195985645056) A[3]:(0.527965664864)\n",
      " state (6)  A[0]:(-0.00371221988462) A[1]:(0.809966623783) A[2]:(-0.000198245048523) A[3]:(0.656307578087)\n",
      " state (7)  A[0]:(0.621491670609) A[1]:(-0.247933372855) A[2]:(0.309910356998) A[3]:(0.881196558475)\n",
      " state (8)  A[0]:(0.652766346931) A[1]:(0.000662945094518) A[2]:(0.728948950768) A[3]:(0.588594734669)\n",
      " state (9)  A[0]:(0.652491688728) A[1]:(0.810260772705) A[2]:(0.809960961342) A[3]:(-0.00457349233329)\n",
      " state (10)  A[0]:(0.725980520248) A[1]:(0.900220751762) A[2]:(-0.000439763040049) A[3]:(0.726521730423)\n",
      " state (11)  A[0]:(0.51915538311) A[1]:(0.877004921436) A[2]:(-0.626866817474) A[3]:(0.843672931194)\n",
      " state (12)  A[0]:(0.0742370709777) A[1]:(0.82455009222) A[2]:(-0.624848604202) A[3]:(0.792664051056)\n",
      " state (13)  A[0]:(-0.00687329098582) A[1]:(0.809630453587) A[2]:(0.899852097034) A[3]:(0.726401507854)\n",
      " state (14)  A[0]:(0.807700276375) A[1]:(0.901069641113) A[2]:(1.0) A[3]:(0.807527780533)\n",
      " state (15)  A[0]:(0.981697022915) A[1]:(0.95645147562) A[2]:(1.0) A[3]:(0.872992992401)\n",
      "Episode 893000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6004. Times reached goal: 999.               Steps done: 6523146. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0013221375241.\n",
      " state (0)  A[0]:(0.532536387444) A[1]:(0.590372800827) A[2]:(0.590597093105) A[3]:(0.532128572464)\n",
      " state (1)  A[0]:(0.532939910889) A[1]:(1.77174806595e-05) A[2]:(0.656203210354) A[3]:(0.591136336327)\n",
      " state (2)  A[0]:(0.592251360416) A[1]:(0.728935837746) A[2]:(0.590630829334) A[3]:(0.656593203545)\n",
      " state (3)  A[0]:(0.657384037971) A[1]:(-0.217479839921) A[2]:(0.539890408516) A[3]:(0.518057405949)\n",
      " state (4)  A[0]:(0.591422796249) A[1]:(0.656382620335) A[2]:(-0.000211596488953) A[3]:(0.53212749958)\n",
      " state (5)  A[0]:(0.164050281048) A[1]:(0.928738951683) A[2]:(-0.195887520909) A[3]:(0.527402698994)\n",
      " state (6)  A[0]:(-5.52535057068e-05) A[1]:(0.80992180109) A[2]:(-0.00016176700592) A[3]:(0.655847191811)\n",
      " state (7)  A[0]:(0.624524176121) A[1]:(-0.248173087835) A[2]:(0.309821873903) A[3]:(0.881192743778)\n",
      " state (8)  A[0]:(0.656268835068) A[1]:(0.000312879681587) A[2]:(0.728833198547) A[3]:(0.589954257011)\n",
      " state (9)  A[0]:(0.656330823898) A[1]:(0.810011982918) A[2]:(0.809862434864) A[3]:(-2.23368406296e-05)\n",
      " state (10)  A[0]:(0.7290340662) A[1]:(0.899978935719) A[2]:(-0.000299692153931) A[3]:(0.72875058651)\n",
      " state (11)  A[0]:(0.523724079132) A[1]:(0.876608073711) A[2]:(-0.626622498035) A[3]:(0.844944238663)\n",
      " state (12)  A[0]:(0.0804383158684) A[1]:(0.823879957199) A[2]:(-0.624544620514) A[3]:(0.794266223907)\n",
      " state (13)  A[0]:(-0.000593781413045) A[1]:(0.808792233467) A[2]:(0.900003314018) A[3]:(0.72845685482)\n",
      " state (14)  A[0]:(0.809842824936) A[1]:(0.900551259518) A[2]:(1.0) A[3]:(0.809065341949)\n",
      " state (15)  A[0]:(0.981912076473) A[1]:(0.956184983253) A[2]:(1.0) A[3]:(0.87406027317)\n",
      "Episode 894000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 998.               Steps done: 6529148. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00131422582145.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5907,  0.5908,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315, -0.0003,  0.6562,  0.5906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5909,  0.7289,  0.5909,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0013,  0.8100, -0.0001,  0.6566]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7294,  0.9000,  0.0001,  0.7295]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8105,  0.9003,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531485795975) A[1]:(0.590686321259) A[2]:(0.590796411037) A[3]:(0.531380593777)\n",
      " state (1)  A[0]:(0.531417667866) A[1]:(-0.000324696302414) A[2]:(0.656185150146) A[3]:(0.590406298637)\n",
      " state (2)  A[0]:(0.59090924263) A[1]:(0.728918910027) A[2]:(0.590847849846) A[3]:(0.655937850475)\n",
      " state (3)  A[0]:(0.656422376633) A[1]:(-0.217871919274) A[2]:(0.540104627609) A[3]:(0.517163991928)\n",
      " state (4)  A[0]:(0.590998888016) A[1]:(0.656429409981) A[2]:(-0.000216722488403) A[3]:(0.531491875648)\n",
      " state (5)  A[0]:(0.16454783082) A[1]:(0.928810656071) A[2]:(-0.196072071791) A[3]:(0.52718681097)\n",
      " state (6)  A[0]:(0.00139659550041) A[1]:(0.810012280941) A[2]:(-7.80820846558e-05) A[3]:(0.65628772974)\n",
      " state (7)  A[0]:(0.625372707844) A[1]:(-0.248169675469) A[2]:(0.310269802809) A[3]:(0.881650090218)\n",
      " state (8)  A[0]:(0.656689047813) A[1]:(0.000317513942719) A[2]:(0.728990375996) A[3]:(0.591319680214)\n",
      " state (9)  A[0]:(0.656544625759) A[1]:(0.810038328171) A[2]:(0.809997797012) A[3]:(0.00117662490811)\n",
      " state (10)  A[0]:(0.72948038578) A[1]:(0.900009334087) A[2]:(3.27825546265e-05) A[3]:(0.729434609413)\n",
      " state (11)  A[0]:(0.524975776672) A[1]:(0.876620173454) A[2]:(-0.626444220543) A[3]:(0.845521986485)\n",
      " state (12)  A[0]:(0.0827467143536) A[1]:(0.823815762997) A[2]:(-0.62437582016) A[3]:(0.795144796371)\n",
      " state (13)  A[0]:(0.00191461807117) A[1]:(0.808589339256) A[2]:(0.899998664856) A[3]:(0.729651927948)\n",
      " state (14)  A[0]:(0.810533761978) A[1]:(0.900343120098) A[2]:(1.0) A[3]:(0.809937357903)\n",
      " state (15)  A[0]:(0.981968402863) A[1]:(0.956045925617) A[2]:(1.0) A[3]:(0.874649763107)\n",
      "Episode 895000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 1000.               Steps done: 6535148. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00130636407534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531498551369) A[1]:(0.590549230576) A[2]:(0.590553283691) A[3]:(0.531553208828)\n",
      " state (1)  A[0]:(0.53112912178) A[1]:(-4.4658780098e-05) A[2]:(0.656113624573) A[3]:(0.590466499329)\n",
      " state (2)  A[0]:(0.590523064137) A[1]:(0.729053378105) A[2]:(0.590638637543) A[3]:(0.656120896339)\n",
      " state (3)  A[0]:(0.656136512756) A[1]:(-0.216417551041) A[2]:(0.539841234684) A[3]:(0.517345190048)\n",
      " state (4)  A[0]:(0.590598464012) A[1]:(0.656118988991) A[2]:(-4.92334365845e-05) A[3]:(0.53160405159)\n",
      " state (5)  A[0]:(0.163779348135) A[1]:(0.9287545681) A[2]:(-0.195866316557) A[3]:(0.527468204498)\n",
      " state (6)  A[0]:(0.000128924846649) A[1]:(0.810047090054) A[2]:(7.79628753662e-05) A[3]:(0.656272828579)\n",
      " state (7)  A[0]:(0.624631762505) A[1]:(-0.248071625829) A[2]:(0.31042599678) A[3]:(0.881443917751)\n",
      " state (8)  A[0]:(0.656314611435) A[1]:(0.00014553964138) A[2]:(0.729051113129) A[3]:(0.590670347214)\n",
      " state (9)  A[0]:(0.656302690506) A[1]:(0.810038924217) A[2]:(0.810044944286) A[3]:(2.10106372833e-05)\n",
      " state (10)  A[0]:(0.72916829586) A[1]:(0.900017559528) A[2]:(0.000261902809143) A[3]:(0.728943586349)\n",
      " state (11)  A[0]:(0.524233520031) A[1]:(0.876597225666) A[2]:(-0.626285552979) A[3]:(0.845305323601)\n",
      " state (12)  A[0]:(0.0814312845469) A[1]:(0.823701202869) A[2]:(-0.624254882336) A[3]:(0.795000851154)\n",
      " state (13)  A[0]:(0.000547110976186) A[1]:(0.808342635632) A[2]:(0.899978458881) A[3]:(0.729705452919)\n",
      " state (14)  A[0]:(0.810279786587) A[1]:(0.900125324726) A[2]:(1.0) A[3]:(0.810257136822)\n",
      " state (15)  A[0]:(0.981973946095) A[1]:(0.955910086632) A[2]:(1.0) A[3]:(0.875079870224)\n",
      "Episode 896000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 6541152. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0012985441643.\n",
      " state (0)  A[0]:(0.531382858753) A[1]:(0.590546965599) A[2]:(0.590506076813) A[3]:(0.531638979912)\n",
      " state (1)  A[0]:(0.530765652657) A[1]:(7.02440738678e-05) A[2]:(0.656115591526) A[3]:(0.590345919132)\n",
      " state (2)  A[0]:(0.590152561665) A[1]:(0.729033231735) A[2]:(0.590624809265) A[3]:(0.655915617943)\n",
      " state (3)  A[0]:(0.655820906162) A[1]:(-0.216607987881) A[2]:(0.539965987206) A[3]:(0.516754686832)\n",
      " state (4)  A[0]:(0.590103983879) A[1]:(0.656087219715) A[2]:(0.000218987464905) A[3]:(0.530846714973)\n",
      " state (5)  A[0]:(0.162713542581) A[1]:(0.928714394569) A[2]:(-0.195563539863) A[3]:(0.526549458504)\n",
      " state (6)  A[0]:(-0.00113677931949) A[1]:(0.809947431087) A[2]:(0.000249981880188) A[3]:(0.655387997627)\n",
      " state (7)  A[0]:(0.624020159245) A[1]:(-0.248323410749) A[2]:(0.310378402472) A[3]:(0.881056964397)\n",
      " state (8)  A[0]:(0.655877947807) A[1]:(-0.0001440346241) A[2]:(0.728907823563) A[3]:(0.589723944664)\n",
      " state (9)  A[0]:(0.655800282955) A[1]:(0.809978783131) A[2]:(0.80992770195) A[3]:(-0.00128111173399)\n",
      " state (10)  A[0]:(0.728616774082) A[1]:(0.900007069111) A[2]:(-0.000159740447998) A[3]:(0.728226780891)\n",
      " state (11)  A[0]:(0.523173987865) A[1]:(0.87662011385) A[2]:(-0.626618266106) A[3]:(0.844776332378)\n",
      " state (12)  A[0]:(0.0798434317112) A[1]:(0.823813855648) A[2]:(-0.624525427818) A[3]:(0.794216752052)\n",
      " state (13)  A[0]:(-0.000877141719684) A[1]:(0.808603405952) A[2]:(0.90008699894) A[3]:(0.728613972664)\n",
      " state (14)  A[0]:(0.809918224812) A[1]:(0.900365233421) A[2]:(1.0) A[3]:(0.809353470802)\n",
      " state (15)  A[0]:(0.981931328773) A[1]:(0.95605379343) A[2]:(1.0) A[3]:(0.874366879463)\n",
      "Episode 897000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 999.               Steps done: 6547154. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00129077364488.\n",
      " state (0)  A[0]:(0.531978905201) A[1]:(0.590664148331) A[2]:(0.590549468994) A[3]:(0.5317466259)\n",
      " state (1)  A[0]:(0.531793355942) A[1]:(0.000211901962757) A[2]:(0.656231105328) A[3]:(0.590360283852)\n",
      " state (2)  A[0]:(0.591308534145) A[1]:(0.729117035866) A[2]:(0.590813398361) A[3]:(0.656009316444)\n",
      " state (3)  A[0]:(0.656728625298) A[1]:(-0.216235294938) A[2]:(0.5399492383) A[3]:(0.517372846603)\n",
      " state (4)  A[0]:(0.591352462769) A[1]:(0.656246304512) A[2]:(5.11407852173e-05) A[3]:(0.53182721138)\n",
      " state (5)  A[0]:(0.165176555514) A[1]:(0.928776741028) A[2]:(-0.195765420794) A[3]:(0.527825951576)\n",
      " state (6)  A[0]:(0.00201075989753) A[1]:(0.810023128986) A[2]:(0.000230193138123) A[3]:(0.656582534313)\n",
      " state (7)  A[0]:(0.626003265381) A[1]:(-0.248214393854) A[2]:(0.310587465763) A[3]:(0.881527662277)\n",
      " state (8)  A[0]:(0.657836735249) A[1]:(0.000205434858799) A[2]:(0.729100465775) A[3]:(0.591105699539)\n",
      " state (9)  A[0]:(0.65813344717) A[1]:(0.810045838356) A[2]:(0.810008704662) A[3]:(0.00142052676529)\n",
      " state (10)  A[0]:(0.730663180351) A[1]:(0.899999201298) A[2]:(0.000178337097168) A[3]:(0.72947204113)\n",
      " state (11)  A[0]:(0.526412606239) A[1]:(0.876555144787) A[2]:(-0.626317858696) A[3]:(0.845495045185)\n",
      " state (12)  A[0]:(0.084307461977) A[1]:(0.82362473011) A[2]:(-0.624307572842) A[3]:(0.795121610165)\n",
      " state (13)  A[0]:(0.00352405034937) A[1]:(0.808243453503) A[2]:(0.899957537651) A[3]:(0.729723215103)\n",
      " state (14)  A[0]:(0.81145131588) A[1]:(0.900057971478) A[2]:(1.0) A[3]:(0.81013572216)\n",
      " state (15)  A[0]:(0.982114434242) A[1]:(0.955875337124) A[2]:(1.0) A[3]:(0.874922513962)\n",
      "Episode 898000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 999.               Steps done: 6553154. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00128305219054.\n",
      " state (0)  A[0]:(0.531322479248) A[1]:(0.590410470963) A[2]:(0.590475022793) A[3]:(0.531433999538)\n",
      " state (1)  A[0]:(0.531009912491) A[1]:(0.000103563070297) A[2]:(0.656092882156) A[3]:(0.59054338932)\n",
      " state (2)  A[0]:(0.590514540672) A[1]:(0.728961884975) A[2]:(0.590551495552) A[3]:(0.656300902367)\n",
      " state (3)  A[0]:(0.656127810478) A[1]:(-0.216416373849) A[2]:(0.539802312851) A[3]:(0.517486214638)\n",
      " state (4)  A[0]:(0.590730965137) A[1]:(0.656134128571) A[2]:(-6.89029693604e-05) A[3]:(0.531772375107)\n",
      " state (5)  A[0]:(0.164257749915) A[1]:(0.928769528866) A[2]:(-0.195906892419) A[3]:(0.527673125267)\n",
      " state (6)  A[0]:(0.000976562208962) A[1]:(0.810005784035) A[2]:(8.82148742676e-05) A[3]:(0.656189322472)\n",
      " state (7)  A[0]:(0.625039935112) A[1]:(-0.248340800405) A[2]:(0.310456395149) A[3]:(0.881145119667)\n",
      " state (8)  A[0]:(0.65628683567) A[1]:(-0.000203460454941) A[2]:(0.728997647762) A[3]:(0.589858531952)\n",
      " state (9)  A[0]:(0.655912160873) A[1]:(0.809936761856) A[2]:(0.810035347939) A[3]:(-0.000636726559605)\n",
      " state (10)  A[0]:(0.728833496571) A[1]:(0.900006353855) A[2]:(-9.03606414795e-05) A[3]:(0.728903055191)\n",
      " state (11)  A[0]:(0.523715078831) A[1]:(0.876618921757) A[2]:(-0.626672387123) A[3]:(0.845354735851)\n",
      " state (12)  A[0]:(0.0806727036834) A[1]:(0.823772966862) A[2]:(-0.624616146088) A[3]:(0.795071005821)\n",
      " state (13)  A[0]:(-0.000208973884583) A[1]:(0.808480381966) A[2]:(0.900014817715) A[3]:(0.729815363884)\n",
      " state (14)  A[0]:(0.809969782829) A[1]:(0.900236725807) A[2]:(1.0) A[3]:(0.810391843319)\n",
      " state (15)  A[0]:(0.981921732426) A[1]:(0.955967545509) A[2]:(1.0) A[3]:(0.875185906887)\n",
      "Episode 899000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 999.               Steps done: 6559154. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00127537692621.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5904,  0.5905,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.0001,  0.6562,  0.5906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.7290,  0.5901,  0.6565]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8100,  0.0002,  0.6563]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7287,  0.9000, -0.0001,  0.7285]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8107,  0.9001,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531256735325) A[1]:(0.590456545353) A[2]:(0.590486645699) A[3]:(0.531157612801)\n",
      " state (1)  A[0]:(0.531335949898) A[1]:(0.000138953328133) A[2]:(0.656147181988) A[3]:(0.590596556664)\n",
      " state (2)  A[0]:(0.590909361839) A[1]:(0.728994250298) A[2]:(0.590085685253) A[3]:(0.656476199627)\n",
      " state (3)  A[0]:(0.656184196472) A[1]:(-0.213448956609) A[2]:(0.539029836655) A[3]:(0.517402768135)\n",
      " state (4)  A[0]:(0.590430140495) A[1]:(0.656141281128) A[2]:(-0.000218033790588) A[3]:(0.53130877018)\n",
      " state (5)  A[0]:(0.1636300385) A[1]:(0.928722858429) A[2]:(-0.195735618472) A[3]:(0.527391910553)\n",
      " state (6)  A[0]:(8.05258750916e-05) A[1]:(0.810002803802) A[2]:(0.00023627281189) A[3]:(0.656323075294)\n",
      " state (7)  A[0]:(0.62432795763) A[1]:(-0.247927412391) A[2]:(0.310369879007) A[3]:(0.881402909756)\n",
      " state (8)  A[0]:(0.655703306198) A[1]:(0.000229820609093) A[2]:(0.728982448578) A[3]:(0.590298295021)\n",
      " state (9)  A[0]:(0.655404686928) A[1]:(0.810004651546) A[2]:(0.809995293617) A[3]:(-0.0010033842409)\n",
      " state (10)  A[0]:(0.728725552559) A[1]:(0.900007843971) A[2]:(-0.000116109848022) A[3]:(0.728496670723)\n",
      " state (11)  A[0]:(0.52416664362) A[1]:(0.876596212387) A[2]:(-0.626595377922) A[3]:(0.845079362392)\n",
      " state (12)  A[0]:(0.0820608213544) A[1]:(0.823696672916) A[2]:(-0.624511241913) A[3]:(0.794723629951)\n",
      " state (13)  A[0]:(0.00169849232771) A[1]:(0.808320224285) A[2]:(0.899999141693) A[3]:(0.72932934761)\n",
      " state (14)  A[0]:(0.81068187952) A[1]:(0.900090754032) A[2]:(1.0) A[3]:(0.809914529324)\n",
      " state (15)  A[0]:(0.982002198696) A[1]:(0.955874204636) A[2]:(1.0) A[3]:(0.874775230885)\n",
      "Episode 900000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5997. Times reached goal: 999.               Steps done: 6565151. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00126775137885.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530912399292) A[1]:(0.590258359909) A[2]:(0.590346634388) A[3]:(0.531138300896)\n",
      " state (1)  A[0]:(0.530552387238) A[1]:(0.000134937465191) A[2]:(0.656000673771) A[3]:(0.590249300003)\n",
      " state (2)  A[0]:(0.590026736259) A[1]:(0.729004919529) A[2]:(0.590353965759) A[3]:(0.65604031086)\n",
      " state (3)  A[0]:(0.655655860901) A[1]:(-0.215413421392) A[2]:(0.53975892067) A[3]:(0.517317056656)\n",
      " state (4)  A[0]:(0.5900157094) A[1]:(0.65601682663) A[2]:(0.000243544578552) A[3]:(0.531513750553)\n",
      " state (5)  A[0]:(0.16303306818) A[1]:(0.928738057613) A[2]:(-0.195723474026) A[3]:(0.527465581894)\n",
      " state (6)  A[0]:(-0.000221967697144) A[1]:(0.809992015362) A[2]:(-1.58548355103e-05) A[3]:(0.656157255173)\n",
      " state (7)  A[0]:(0.624467730522) A[1]:(-0.248061671853) A[2]:(0.310103029013) A[3]:(0.881180107594)\n",
      " state (8)  A[0]:(0.6557289958) A[1]:(0.000236861407757) A[2]:(0.729000091553) A[3]:(0.589635968208)\n",
      " state (9)  A[0]:(0.655342698097) A[1]:(0.809980750084) A[2]:(0.809986054897) A[3]:(-0.000717729213648)\n",
      " state (10)  A[0]:(0.728350400925) A[1]:(0.899999380112) A[2]:(-0.000416874856455) A[3]:(0.728832960129)\n",
      " state (11)  A[0]:(0.52298116684) A[1]:(0.876640677452) A[2]:(-0.626891851425) A[3]:(0.845250368118)\n",
      " state (12)  A[0]:(0.0797731354833) A[1]:(0.823891401291) A[2]:(-0.624827206135) A[3]:(0.794898629189)\n",
      " state (13)  A[0]:(-0.000850200420246) A[1]:(0.808741509914) A[2]:(0.900003969669) A[3]:(0.729590177536)\n",
      " state (14)  A[0]:(0.809934198856) A[1]:(0.900472640991) A[2]:(1.0) A[3]:(0.810260295868)\n",
      " state (15)  A[0]:(0.981928825378) A[1]:(0.956114411354) A[2]:(1.0) A[3]:(0.875098526478)\n",
      "Episode 901000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5999. Times reached goal: 998.               Steps done: 6571150. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00126016890469.\n",
      " state (0)  A[0]:(0.533313155174) A[1]:(0.59063154459) A[2]:(0.590576171875) A[3]:(0.531888365746)\n",
      " state (1)  A[0]:(0.533766210079) A[1]:(0.000280678272247) A[2]:(0.656195223331) A[3]:(0.590639829636)\n",
      " state (2)  A[0]:(0.593062341213) A[1]:(0.729013204575) A[2]:(0.590990066528) A[3]:(0.656055033207)\n",
      " state (3)  A[0]:(0.658233046532) A[1]:(-0.217206582427) A[2]:(0.540379524231) A[3]:(0.516488015652)\n",
      " state (4)  A[0]:(0.593092799187) A[1]:(0.656279444695) A[2]:(-1.10864639282e-05) A[3]:(0.530493378639)\n",
      " state (5)  A[0]:(0.167677551508) A[1]:(0.928879261017) A[2]:(-0.196583360434) A[3]:(0.526369094849)\n",
      " state (6)  A[0]:(0.0043980195187) A[1]:(0.809962153435) A[2]:(-0.000748157384805) A[3]:(0.655556082726)\n",
      " state (7)  A[0]:(0.625532746315) A[1]:(-0.248162075877) A[2]:(0.309815406799) A[3]:(0.88085258007)\n",
      " state (8)  A[0]:(0.654531359673) A[1]:(0.00132071890403) A[2]:(0.729262411594) A[3]:(0.586677074432)\n",
      " state (9)  A[0]:(0.653085410595) A[1]:(0.810139417648) A[2]:(0.809927463531) A[3]:(-0.00573183270171)\n",
      " state (10)  A[0]:(0.72585606575) A[1]:(0.899952888489) A[2]:(-0.0010819430463) A[3]:(0.726440906525)\n",
      " state (11)  A[0]:(0.518322825432) A[1]:(0.876522421837) A[2]:(-0.627290606499) A[3]:(0.843673586845)\n",
      " state (12)  A[0]:(0.0726638808846) A[1]:(0.823716521263) A[2]:(-0.625198602676) A[3]:(0.792800605297)\n",
      " state (13)  A[0]:(-0.00850829854608) A[1]:(0.808595716953) A[2]:(0.899940431118) A[3]:(0.726985037327)\n",
      " state (14)  A[0]:(0.807128190994) A[1]:(0.90043926239) A[2]:(1.0) A[3]:(0.808533012867)\n",
      " state (15)  A[0]:(0.981620311737) A[1]:(0.956122934818) A[2]:(1.0) A[3]:(0.874057173729)\n",
      "Episode 902000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 1000.               Steps done: 6577152. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00125262802375.\n",
      " state (0)  A[0]:(0.531073927879) A[1]:(0.590518951416) A[2]:(0.59049397707) A[3]:(0.532165586948)\n",
      " state (1)  A[0]:(0.530888020992) A[1]:(0.000187024474144) A[2]:(0.656087040901) A[3]:(0.590992808342)\n",
      " state (2)  A[0]:(0.590287923813) A[1]:(0.729023933411) A[2]:(0.590611636639) A[3]:(0.656586110592)\n",
      " state (3)  A[0]:(0.655596017838) A[1]:(-0.216616556048) A[2]:(0.539917826653) A[3]:(0.51803958416)\n",
      " state (4)  A[0]:(0.589701592922) A[1]:(0.656253814697) A[2]:(-1.9907951355e-05) A[3]:(0.532375514507)\n",
      " state (5)  A[0]:(0.162310510874) A[1]:(0.928783893585) A[2]:(-0.195929467678) A[3]:(0.528309106827)\n",
      " state (6)  A[0]:(-0.00076490623178) A[1]:(0.809987902641) A[2]:(-6.40153884888e-05) A[3]:(0.657072067261)\n",
      " state (7)  A[0]:(0.624318599701) A[1]:(-0.248077332973) A[2]:(0.310166269541) A[3]:(0.881731152534)\n",
      " state (8)  A[0]:(0.655782341957) A[1]:(0.000400289864046) A[2]:(0.729012191296) A[3]:(0.591272234917)\n",
      " state (9)  A[0]:(0.655734837055) A[1]:(0.809964716434) A[2]:(0.809989213943) A[3]:(0.001546798856)\n",
      " state (10)  A[0]:(0.7286683321) A[1]:(0.899966955185) A[2]:(-0.000298500061035) A[3]:(0.729640424252)\n",
      " state (11)  A[0]:(0.523310184479) A[1]:(0.876610994339) A[2]:(-0.62679028511) A[3]:(0.845591783524)\n",
      " state (12)  A[0]:(0.0800094380975) A[1]:(0.823894262314) A[2]:(-0.624757349491) A[3]:(0.7951836586)\n",
      " state (13)  A[0]:(-0.000795125786681) A[1]:(0.808808028698) A[2]:(0.899985611439) A[3]:(0.729785203934)\n",
      " state (14)  A[0]:(0.809917390347) A[1]:(0.900551140308) A[2]:(1.0) A[3]:(0.810301303864)\n",
      " state (15)  A[0]:(0.981929302216) A[1]:(0.956175148487) A[2]:(1.0) A[3]:(0.875077545643)\n",
      "Episode 903000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 6583156. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00124512977736.\n",
      " state (0)  A[0]:(0.53121483326) A[1]:(0.590403020382) A[2]:(0.590402126312) A[3]:(0.531615495682)\n",
      " state (1)  A[0]:(0.53092443943) A[1]:(4.03299927711e-05) A[2]:(0.656084895134) A[3]:(0.590561270714)\n",
      " state (2)  A[0]:(0.590380549431) A[1]:(0.729005634785) A[2]:(0.59030175209) A[3]:(0.656132221222)\n",
      " state (3)  A[0]:(0.656139492989) A[1]:(-0.214095532894) A[2]:(0.539392888546) A[3]:(0.517545819283)\n",
      " state (4)  A[0]:(0.590636372566) A[1]:(0.656098842621) A[2]:(-2.41994857788e-05) A[3]:(0.531366944313)\n",
      " state (5)  A[0]:(0.164077833295) A[1]:(0.928764104843) A[2]:(-0.195822283626) A[3]:(0.527082562447)\n",
      " state (6)  A[0]:(0.000706791761331) A[1]:(0.810039758682) A[2]:(0.000187993049622) A[3]:(0.65607637167)\n",
      " state (7)  A[0]:(0.624502241611) A[1]:(-0.247774720192) A[2]:(0.310490757227) A[3]:(0.881384730339)\n",
      " state (8)  A[0]:(0.655977964401) A[1]:(0.000693894806318) A[2]:(0.7291457057) A[3]:(0.590268909931)\n",
      " state (9)  A[0]:(0.656286358833) A[1]:(0.810072481632) A[2]:(0.810090839863) A[3]:(-0.000324562191963)\n",
      " state (10)  A[0]:(0.729516088963) A[1]:(0.900023758411) A[2]:(0.000274181365967) A[3]:(0.728879213333)\n",
      " state (11)  A[0]:(0.52519929409) A[1]:(0.876648485661) A[2]:(-0.626279115677) A[3]:(0.845262229443)\n",
      " state (12)  A[0]:(0.0831520184875) A[1]:(0.823850989342) A[2]:(-0.624205470085) A[3]:(0.794909119606)\n",
      " state (13)  A[0]:(0.00240164529532) A[1]:(0.808588504791) A[2]:(0.900037109852) A[3]:(0.729574084282)\n",
      " state (14)  A[0]:(0.810772180557) A[1]:(0.900300681591) A[2]:(1.0) A[3]:(0.810239553452)\n",
      " state (15)  A[0]:(0.982001900673) A[1]:(0.956003785133) A[2]:(1.0) A[3]:(0.875107645988)\n",
      "Episode 904000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 999.               Steps done: 6589158. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00123767889091.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5904,  0.5904,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5309,  0.0001,  0.6561,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.7290,  0.5903,  0.6558]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0021,  0.8099, -0.0002,  0.6543]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7282,  0.9000, -0.0001,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8096,  0.9012,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531484663486) A[1]:(0.590366005898) A[2]:(0.590381085873) A[3]:(0.531510591507)\n",
      " state (1)  A[0]:(0.530959486961) A[1]:(0.000121146440506) A[2]:(0.656113505363) A[3]:(0.590469896793)\n",
      " state (2)  A[0]:(0.590219378471) A[1]:(0.728953361511) A[2]:(0.590299963951) A[3]:(0.655979394913)\n",
      " state (3)  A[0]:(0.656180143356) A[1]:(-0.214496046305) A[2]:(0.539346456528) A[3]:(0.517299413681)\n",
      " state (4)  A[0]:(0.590406537056) A[1]:(0.655094921589) A[2]:(0.000163078308105) A[3]:(0.530797660351)\n",
      " state (5)  A[0]:(0.162950396538) A[1]:(0.928515017033) A[2]:(-0.195674061775) A[3]:(0.526096463203)\n",
      " state (6)  A[0]:(-0.00189232605044) A[1]:(0.809925734997) A[2]:(-0.00020432472229) A[3]:(0.654702067375)\n",
      " state (7)  A[0]:(0.62319535017) A[1]:(-0.248230665922) A[2]:(0.309735447168) A[3]:(0.880915701389)\n",
      " state (8)  A[0]:(0.655396819115) A[1]:(-0.00177469663322) A[2]:(0.728223204613) A[3]:(0.591256856918)\n",
      " state (9)  A[0]:(0.654936254025) A[1]:(0.809496939182) A[2]:(0.809934079647) A[3]:(0.000902741914615)\n",
      " state (10)  A[0]:(0.728291392326) A[1]:(0.900000810623) A[2]:(-0.000112056732178) A[3]:(0.729544520378)\n",
      " state (11)  A[0]:(0.523333191872) A[1]:(0.876895487309) A[2]:(-0.62691962719) A[3]:(0.845790505409)\n",
      " state (12)  A[0]:(0.080388456583) A[1]:(0.824558436871) A[2]:(-0.624920189381) A[3]:(0.795485854149)\n",
      " state (13)  A[0]:(-0.000592291296925) A[1]:(0.809796869755) A[2]:(0.900003433228) A[3]:(0.729993581772)\n",
      " state (14)  A[0]:(0.809649944305) A[1]:(0.901241540909) A[2]:(1.0) A[3]:(0.810182750225)\n",
      " state (15)  A[0]:(0.981850385666) A[1]:(0.956547677517) A[2]:(1.0) A[3]:(0.874759674072)\n",
      "Episode 905000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 999.               Steps done: 6595160. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00123027259075.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531773328781) A[1]:(0.590734243393) A[2]:(0.590480566025) A[3]:(0.530465722084)\n",
      " state (1)  A[0]:(0.532086968422) A[1]:(0.0005747526302) A[2]:(0.656192541122) A[3]:(0.590546071529)\n",
      " state (2)  A[0]:(0.591596364975) A[1]:(0.729060709476) A[2]:(0.590249896049) A[3]:(0.656551599503)\n",
      " state (3)  A[0]:(0.657058537006) A[1]:(-0.21372294426) A[2]:(0.539633154869) A[3]:(0.518564105034)\n",
      " state (4)  A[0]:(0.591412067413) A[1]:(0.656274199486) A[2]:(0.000585198344197) A[3]:(0.532550513744)\n",
      " state (5)  A[0]:(0.164905026555) A[1]:(0.928770363331) A[2]:(-0.195185735822) A[3]:(0.528234899044)\n",
      " state (6)  A[0]:(0.00149249914102) A[1]:(0.810096442699) A[2]:(0.00058698648354) A[3]:(0.656970977783)\n",
      " state (7)  A[0]:(0.62527936697) A[1]:(-0.247388005257) A[2]:(0.310446470976) A[3]:(0.881790757179)\n",
      " state (8)  A[0]:(0.6570135355) A[1]:(0.00106181169394) A[2]:(0.72908616066) A[3]:(0.591508924961)\n",
      " state (9)  A[0]:(0.657379627228) A[1]:(0.810111641884) A[2]:(0.810108244419) A[3]:(0.00128568639047)\n",
      " state (10)  A[0]:(0.730150580406) A[1]:(0.900007963181) A[2]:(0.000450372666819) A[3]:(0.729248702526)\n",
      " state (11)  A[0]:(0.52573287487) A[1]:(0.876631379128) A[2]:(-0.626196444035) A[3]:(0.845293998718)\n",
      " state (12)  A[0]:(0.0834107995033) A[1]:(0.823873400688) A[2]:(-0.624203920364) A[3]:(0.794802308083)\n",
      " state (13)  A[0]:(0.00232028542086) A[1]:(0.808696687222) A[2]:(0.90001231432) A[3]:(0.729360103607)\n",
      " state (14)  A[0]:(0.810660779476) A[1]:(0.900422871113) A[2]:(1.0) A[3]:(0.810123503208)\n",
      " state (15)  A[0]:(0.981987416744) A[1]:(0.956093728542) A[2]:(1.0) A[3]:(0.875084578991)\n",
      "Episode 906000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 1000.               Steps done: 6601165. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00122290694134.\n",
      " state (0)  A[0]:(0.531056523323) A[1]:(0.590539991856) A[2]:(0.59052336216) A[3]:(0.53136408329)\n",
      " state (1)  A[0]:(0.530613303185) A[1]:(-0.000206530094147) A[2]:(0.656121373177) A[3]:(0.590876698494)\n",
      " state (2)  A[0]:(0.590105712414) A[1]:(0.729020893574) A[2]:(0.590113997459) A[3]:(0.656513094902)\n",
      " state (3)  A[0]:(0.655836462975) A[1]:(-0.213375389576) A[2]:(0.539198279381) A[3]:(0.51859074831)\n",
      " state (4)  A[0]:(0.590125918388) A[1]:(0.65619546175) A[2]:(-2.74181365967e-06) A[3]:(0.532201468945)\n",
      " state (5)  A[0]:(0.16305744648) A[1]:(0.928714811802) A[2]:(-0.195645734668) A[3]:(0.527451038361)\n",
      " state (6)  A[0]:(-0.000861346488819) A[1]:(0.809986829758) A[2]:(0.000196695327759) A[3]:(0.656054258347)\n",
      " state (7)  A[0]:(0.623454809189) A[1]:(-0.24776725471) A[2]:(0.310322910547) A[3]:(0.881368041039)\n",
      " state (8)  A[0]:(0.655282855034) A[1]:(0.000559747160878) A[2]:(0.729079723358) A[3]:(0.590378522873)\n",
      " state (9)  A[0]:(0.655760407448) A[1]:(0.810007214546) A[2]:(0.809974551201) A[3]:(0.000297874212265)\n",
      " state (10)  A[0]:(0.728954434395) A[1]:(0.900004327297) A[2]:(-0.000161647796631) A[3]:(0.728976368904)\n",
      " state (11)  A[0]:(0.524061322212) A[1]:(0.876701831818) A[2]:(-0.626578688622) A[3]:(0.845138370991)\n",
      " state (12)  A[0]:(0.0813369601965) A[1]:(0.824086785316) A[2]:(-0.624506354332) A[3]:(0.794559240341)\n",
      " state (13)  A[0]:(0.000557482184377) A[1]:(0.809069931507) A[2]:(0.900028944016) A[3]:(0.728975594044)\n",
      " state (14)  A[0]:(0.810245990753) A[1]:(0.900713741779) A[2]:(1.0) A[3]:(0.809784054756)\n",
      " state (15)  A[0]:(0.981948554516) A[1]:(0.956256747246) A[2]:(1.0) A[3]:(0.874774098396)\n",
      "Episode 907000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6013. Times reached goal: 1000.               Steps done: 6607178. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00121557566547.\n",
      " state (0)  A[0]:(0.531142354012) A[1]:(0.590510189533) A[2]:(0.590518772602) A[3]:(0.531159996986)\n",
      " state (1)  A[0]:(0.530735254288) A[1]:(0.00040732321213) A[2]:(0.656145095825) A[3]:(0.589990139008)\n",
      " state (2)  A[0]:(0.590018451214) A[1]:(0.728975892067) A[2]:(0.589980602264) A[3]:(0.655470728874)\n",
      " state (3)  A[0]:(0.655349433422) A[1]:(-0.214490413666) A[2]:(0.539181172848) A[3]:(0.516765475273)\n",
      " state (4)  A[0]:(0.589058995247) A[1]:(0.656074821949) A[2]:(-0.000120162963867) A[3]:(0.530435502529)\n",
      " state (5)  A[0]:(0.160908192396) A[1]:(0.928667604923) A[2]:(-0.195638403296) A[3]:(0.525790631771)\n",
      " state (6)  A[0]:(-0.00299417087808) A[1]:(0.809997677803) A[2]:(5.31673431396e-05) A[3]:(0.654766559601)\n",
      " state (7)  A[0]:(0.623197197914) A[1]:(-0.247632414103) A[2]:(0.310045152903) A[3]:(0.880887448788)\n",
      " state (8)  A[0]:(0.655953407288) A[1]:(0.000619456113782) A[2]:(0.729045689106) A[3]:(0.588932096958)\n",
      " state (9)  A[0]:(0.656855285168) A[1]:(0.810008645058) A[2]:(0.809987962246) A[3]:(-0.00211509736255)\n",
      " state (10)  A[0]:(0.72989654541) A[1]:(0.899984002113) A[2]:(0.000342965126038) A[3]:(0.72761297226)\n",
      " state (11)  A[0]:(0.525561690331) A[1]:(0.87667620182) A[2]:(-0.626078486443) A[3]:(0.844246506691)\n",
      " state (12)  A[0]:(0.0834143534303) A[1]:(0.824054181576) A[2]:(-0.624070167542) A[3]:(0.793426513672)\n",
      " state (13)  A[0]:(0.00235330616124) A[1]:(0.809007287025) A[2]:(0.899951815605) A[3]:(0.727557063103)\n",
      " state (14)  A[0]:(0.810654401779) A[1]:(0.900649726391) A[2]:(1.0) A[3]:(0.808755040169)\n",
      " state (15)  A[0]:(0.981990695) A[1]:(0.956229031086) A[2]:(1.0) A[3]:(0.874117016792)\n",
      "Episode 908000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5998. Times reached goal: 997.               Steps done: 6613176. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00120830646475.\n",
      " state (0)  A[0]:(0.530800819397) A[1]:(0.590503633022) A[2]:(0.590544700623) A[3]:(0.531606078148)\n",
      " state (1)  A[0]:(0.530736327171) A[1]:(-0.000232137739658) A[2]:(0.656089186668) A[3]:(0.590594410896)\n",
      " state (2)  A[0]:(0.590281188488) A[1]:(0.728981852531) A[2]:(0.590444684029) A[3]:(0.656170845032)\n",
      " state (3)  A[0]:(0.656515419483) A[1]:(-0.215319231153) A[2]:(0.539676785469) A[3]:(0.51798838377)\n",
      " state (4)  A[0]:(0.591183900833) A[1]:(0.656087875366) A[2]:(3.38554382324e-05) A[3]:(0.532217085361)\n",
      " state (5)  A[0]:(0.164749458432) A[1]:(0.928719639778) A[2]:(-0.195863559842) A[3]:(0.528133690357)\n",
      " state (6)  A[0]:(0.00157850852702) A[1]:(0.809992611408) A[2]:(-0.000159025192261) A[3]:(0.657083928585)\n",
      " state (7)  A[0]:(0.625789225101) A[1]:(-0.247903361917) A[2]:(0.309978365898) A[3]:(0.88191062212)\n",
      " state (8)  A[0]:(0.657261371613) A[1]:(0.000438086659415) A[2]:(0.728892087936) A[3]:(0.591754078865)\n",
      " state (9)  A[0]:(0.656849861145) A[1]:(0.80995953083) A[2]:(0.809926867485) A[3]:(0.00116367579903)\n",
      " state (10)  A[0]:(0.729497551918) A[1]:(0.899971008301) A[2]:(0.000192165374756) A[3]:(0.729061901569)\n",
      " state (11)  A[0]:(0.524836659431) A[1]:(0.876670122147) A[2]:(-0.626210808754) A[3]:(0.845194876194)\n",
      " state (12)  A[0]:(0.082391820848) A[1]:(0.824060440063) A[2]:(-0.624208211899) A[3]:(0.79471796751)\n",
      " state (13)  A[0]:(0.00130569864996) A[1]:(0.809035122395) A[2]:(0.899932920933) A[3]:(0.729256808758)\n",
      " state (14)  A[0]:(0.810295343399) A[1]:(0.900676727295) A[2]:(1.0) A[3]:(0.810044527054)\n",
      " state (15)  A[0]:(0.981947779655) A[1]:(0.956244587898) A[2]:(1.0) A[3]:(0.874999642372)\n",
      "Episode 909000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 1000.               Steps done: 6619178. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00120107592989.\n",
      "q_values \n",
      "tensor([[ 0.5333,  0.5908,  0.5905,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.9136e-01,  6.5615e-01,  9.2983e-06,  5.3106e-01]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.0007,  0.7291,  0.5907]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 6.5624e-01,  8.1003e-01,  8.1015e-01,  6.1095e-07]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.8999,  0.0004,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8094,  0.9007,  1.0000,  0.8095]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533186733723) A[1]:(0.590779542923) A[2]:(0.590549468994) A[3]:(0.531813442707)\n",
      " state (1)  A[0]:(0.532137751579) A[1]:(0.000703990343027) A[2]:(0.656203746796) A[3]:(0.590698838234)\n",
      " state (2)  A[0]:(0.590950131416) A[1]:(0.729179918766) A[2]:(0.590547084808) A[3]:(0.656088113785)\n",
      " state (3)  A[0]:(0.657044172287) A[1]:(-0.215934887528) A[2]:(0.539851725101) A[3]:(0.517265796661)\n",
      " state (4)  A[0]:(0.591300725937) A[1]:(0.656298041344) A[2]:(0.000190019607544) A[3]:(0.531239748001)\n",
      " state (5)  A[0]:(0.164061427116) A[1]:(0.928752779961) A[2]:(-0.195512294769) A[3]:(0.526925683022)\n",
      " state (6)  A[0]:(0.000206410884857) A[1]:(0.810113012791) A[2]:(0.00025463104248) A[3]:(0.656008243561)\n",
      " state (7)  A[0]:(0.624845743179) A[1]:(-0.247563168406) A[2]:(0.310389488935) A[3]:(0.881502866745)\n",
      " state (8)  A[0]:(0.656549096107) A[1]:(0.000767737452406) A[2]:(0.729174852371) A[3]:(0.590791344643)\n",
      " state (9)  A[0]:(0.656290590763) A[1]:(0.810078144073) A[2]:(0.810113191605) A[3]:(0.000396072835429)\n",
      " state (10)  A[0]:(0.729029297829) A[1]:(0.900037944317) A[2]:(0.000681757810526) A[3]:(0.728849053383)\n",
      " state (11)  A[0]:(0.524016916752) A[1]:(0.876761853695) A[2]:(-0.625903487206) A[3]:(0.845064103603)\n",
      " state (12)  A[0]:(0.0810737088323) A[1]:(0.824201822281) A[2]:(-0.623861074448) A[3]:(0.794507920742)\n",
      " state (13)  A[0]:(-0.000313282012939) A[1]:(0.809199929237) A[2]:(0.900103449821) A[3]:(0.728870630264)\n",
      " state (14)  A[0]:(0.809556365013) A[1]:(0.900768995285) A[2]:(1.0) A[3]:(0.809569299221)\n",
      " state (15)  A[0]:(0.981842398643) A[1]:(0.956280469894) A[2]:(1.0) A[3]:(0.874519228935)\n",
      "Episode 910000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6013. Times reached goal: 998.               Steps done: 6625191. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00119387553002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531512439251) A[1]:(0.590222358704) A[2]:(0.590413212776) A[3]:(0.531765818596)\n",
      " state (1)  A[0]:(0.531369388103) A[1]:(-7.77915120125e-05) A[2]:(0.656014919281) A[3]:(0.590692400932)\n",
      " state (2)  A[0]:(0.590606570244) A[1]:(0.728893041611) A[2]:(0.590389728546) A[3]:(0.656150996685)\n",
      " state (3)  A[0]:(0.656016528606) A[1]:(-0.215478271246) A[2]:(0.5396951437) A[3]:(0.517657577991)\n",
      " state (4)  A[0]:(0.590250253677) A[1]:(0.655826687813) A[2]:(0.000123262405396) A[3]:(0.531669259071)\n",
      " state (5)  A[0]:(0.163435131311) A[1]:(0.928713560104) A[2]:(-0.195874676108) A[3]:(0.527513027191)\n",
      " state (6)  A[0]:(0.000287294387817) A[1]:(0.809930086136) A[2]:(-9.23871994019e-05) A[3]:(0.656574249268)\n",
      " state (7)  A[0]:(0.624548256397) A[1]:(-0.248194620013) A[2]:(0.310054332018) A[3]:(0.881673455238)\n",
      " state (8)  A[0]:(0.655933618546) A[1]:(0.000303499400616) A[2]:(0.728846788406) A[3]:(0.591230034828)\n",
      " state (9)  A[0]:(0.655699491501) A[1]:(0.809959292412) A[2]:(0.809931457043) A[3]:(0.000662028673105)\n",
      " state (10)  A[0]:(0.728747487068) A[1]:(0.899994254112) A[2]:(0.000153303146362) A[3]:(0.729020535946)\n",
      " state (11)  A[0]:(0.52390474081) A[1]:(0.876711905003) A[2]:(-0.626266598701) A[3]:(0.845238029957)\n",
      " state (12)  A[0]:(0.0813069418073) A[1]:(0.824126183987) A[2]:(-0.624218046665) A[3]:(0.794778466225)\n",
      " state (13)  A[0]:(0.000374853581889) A[1]:(0.809107542038) A[2]:(0.900009870529) A[3]:(0.729259848595)\n",
      " state (14)  A[0]:(0.810025930405) A[1]:(0.900708675385) A[2]:(1.0) A[3]:(0.809907913208)\n",
      " state (15)  A[0]:(0.981912970543) A[1]:(0.956247448921) A[2]:(1.0) A[3]:(0.874779820442)\n",
      "Episode 911000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 1000.               Steps done: 6631198. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00118672541658.\n",
      " state (0)  A[0]:(0.534835159779) A[1]:(0.590477645397) A[2]:(0.5905148983) A[3]:(0.532317698002)\n",
      " state (1)  A[0]:(0.535348296165) A[1]:(0.000150211155415) A[2]:(0.656162142754) A[3]:(0.591388225555)\n",
      " state (2)  A[0]:(0.594399273396) A[1]:(0.728964447975) A[2]:(0.590726971626) A[3]:(0.656684398651)\n",
      " state (3)  A[0]:(0.659330606461) A[1]:(-0.216697886586) A[2]:(0.53977650404) A[3]:(0.517916798592)\n",
      " state (4)  A[0]:(0.594157516956) A[1]:(0.656149625778) A[2]:(-0.000582694949117) A[3]:(0.53182053566)\n",
      " state (5)  A[0]:(0.169314965606) A[1]:(0.928818106651) A[2]:(-0.196752399206) A[3]:(0.527475297451)\n",
      " state (6)  A[0]:(0.00626341253519) A[1]:(0.810001969337) A[2]:(-0.000834941689391) A[3]:(0.656332671642)\n",
      " state (7)  A[0]:(0.627447009087) A[1]:(-0.24817481637) A[2]:(0.309734046459) A[3]:(0.881310462952)\n",
      " state (8)  A[0]:(0.657378256321) A[1]:(0.000698812189512) A[2]:(0.729030370712) A[3]:(0.589061260223)\n",
      " state (9)  A[0]:(0.656348764896) A[1]:(0.810032486916) A[2]:(0.809962928295) A[3]:(-0.00226945779286)\n",
      " state (10)  A[0]:(0.728933393955) A[1]:(0.899992704391) A[2]:(-0.000280976295471) A[3]:(0.728096187115)\n",
      " state (11)  A[0]:(0.523757040501) A[1]:(0.876669764519) A[2]:(-0.62661331892) A[3]:(0.844778418541)\n",
      " state (12)  A[0]:(0.0805813297629) A[1]:(0.824009299278) A[2]:(-0.624545156956) A[3]:(0.794242501259)\n",
      " state (13)  A[0]:(-0.000877976184711) A[1]:(0.80891263485) A[2]:(0.899950861931) A[3]:(0.728657245636)\n",
      " state (14)  A[0]:(0.80934470892) A[1]:(0.90055346489) A[2]:(1.0) A[3]:(0.809560656548)\n",
      " state (15)  A[0]:(0.981814086437) A[1]:(0.956151604652) A[2]:(1.0) A[3]:(0.874605178833)\n",
      "Episode 912000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 998.               Steps done: 6637201. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00117962284361.\n",
      " state (0)  A[0]:(0.53102850914) A[1]:(0.590376019478) A[2]:(0.590445399284) A[3]:(0.531407594681)\n",
      " state (1)  A[0]:(0.53062069416) A[1]:(0.000141389667988) A[2]:(0.656105995178) A[3]:(0.590521216393)\n",
      " state (2)  A[0]:(0.589627742767) A[1]:(0.728955745697) A[2]:(0.590564370155) A[3]:(0.655979275703)\n",
      " state (3)  A[0]:(0.655125021935) A[1]:(-0.216115802526) A[2]:(0.539849042892) A[3]:(0.517279028893)\n",
      " state (4)  A[0]:(0.589015066624) A[1]:(0.655973315239) A[2]:(0.000183701515198) A[3]:(0.531243681908)\n",
      " state (5)  A[0]:(0.161137118936) A[1]:(0.928727805614) A[2]:(-0.195654332638) A[3]:(0.526856541634)\n",
      " state (6)  A[0]:(-0.00270014349371) A[1]:(0.809954881668) A[2]:(9.94205474854e-05) A[3]:(0.655695557594)\n",
      " state (7)  A[0]:(0.622284412384) A[1]:(-0.248225688934) A[2]:(0.310107648373) A[3]:(0.881122529507)\n",
      " state (8)  A[0]:(0.653898715973) A[1]:(0.000263191759586) A[2]:(0.728988349438) A[3]:(0.589549064636)\n",
      " state (9)  A[0]:(0.654026687145) A[1]:(0.809988379478) A[2]:(0.809997797012) A[3]:(-0.000884979730472)\n",
      " state (10)  A[0]:(0.72750222683) A[1]:(0.900020837784) A[2]:(8.53538513184e-05) A[3]:(0.72870516777)\n",
      " state (11)  A[0]:(0.521968245506) A[1]:(0.876733541489) A[2]:(-0.626359820366) A[3]:(0.845135211945)\n",
      " state (12)  A[0]:(0.0785025060177) A[1]:(0.82410889864) A[2]:(-0.624330878258) A[3]:(0.794670760632)\n",
      " state (13)  A[0]:(-0.00273280660622) A[1]:(0.808996617794) A[2]:(0.899947047234) A[3]:(0.729119181633)\n",
      " state (14)  A[0]:(0.808810055256) A[1]:(0.900564670563) A[2]:(1.0) A[3]:(0.809806108475)\n",
      " state (15)  A[0]:(0.981774032116) A[1]:(0.956143081188) A[2]:(1.0) A[3]:(0.874716222286)\n",
      "Episode 913000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 6643205. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0011725616071.\n",
      " state (0)  A[0]:(0.531400024891) A[1]:(0.590047478676) A[2]:(0.590209305286) A[3]:(0.530662536621)\n",
      " state (1)  A[0]:(0.531383574009) A[1]:(-0.000303529202938) A[2]:(0.655843734741) A[3]:(0.589723348618)\n",
      " state (2)  A[0]:(0.590584635735) A[1]:(0.72876739502) A[2]:(0.590041995049) A[3]:(0.655312776566)\n",
      " state (3)  A[0]:(0.656291544437) A[1]:(-0.215921133757) A[2]:(0.539200603962) A[3]:(0.516922235489)\n",
      " state (4)  A[0]:(0.59061473608) A[1]:(0.655605316162) A[2]:(-0.000478029221995) A[3]:(0.531070530415)\n",
      " state (5)  A[0]:(0.163909286261) A[1]:(0.928612709045) A[2]:(-0.196186810732) A[3]:(0.526792824268)\n",
      " state (6)  A[0]:(0.000712454202585) A[1]:(0.809737682343) A[2]:(-0.000533699931111) A[3]:(0.65573990345)\n",
      " state (7)  A[0]:(0.625136971474) A[1]:(-0.248845994473) A[2]:(0.30925783515) A[3]:(0.88128811121)\n",
      " state (8)  A[0]:(0.656691074371) A[1]:(-0.000669143977575) A[2]:(0.728172063828) A[3]:(0.590882956982)\n",
      " state (9)  A[0]:(0.656058251858) A[1]:(0.809736251831) A[2]:(0.809494018555) A[3]:(0.000575289072003)\n",
      " state (10)  A[0]:(0.728866875172) A[1]:(0.899897217751) A[2]:(-0.000740170362405) A[3]:(0.728995084763)\n",
      " state (11)  A[0]:(0.524088382721) A[1]:(0.876576006413) A[2]:(-0.626704335213) A[3]:(0.845255017281)\n",
      " state (12)  A[0]:(0.0815215781331) A[1]:(0.823870301247) A[2]:(-0.624686837196) A[3]:(0.794819533825)\n",
      " state (13)  A[0]:(0.000277519226074) A[1]:(0.808691501617) A[2]:(0.899730563164) A[3]:(0.729302287102)\n",
      " state (14)  A[0]:(0.809820115566) A[1]:(0.900356531143) A[2]:(1.0) A[3]:(0.809969782829)\n",
      " state (15)  A[0]:(0.981885194778) A[1]:(0.956036388874) A[2]:(1.0) A[3]:(0.874856591225)\n",
      "Episode 914000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 999.               Steps done: 6649210. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00116554147369.\n",
      "q_values \n",
      "tensor([[ 0.5281,  0.5905,  0.5905,  0.5331]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5291, -0.0007,  0.6561,  0.5916]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5894,  0.7289,  0.5906,  0.6564]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0019,  0.8100,  0.0021,  0.6541]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7286,  0.9001,  0.0018,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8090,  0.9006,  1.0000,  0.8087]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532002449036) A[1]:(0.590458869934) A[2]:(0.590464353561) A[3]:(0.532589137554)\n",
      " state (1)  A[0]:(0.532744407654) A[1]:(-0.000867649680004) A[2]:(0.655963778496) A[3]:(0.59111726284)\n",
      " state (2)  A[0]:(0.592013716698) A[1]:(0.728889286518) A[2]:(0.59050899744) A[3]:(0.656100213528)\n",
      " state (3)  A[0]:(0.658265352249) A[1]:(-0.216346666217) A[2]:(0.540163099766) A[3]:(0.516303837299)\n",
      " state (4)  A[0]:(0.592572331429) A[1]:(0.655897021294) A[2]:(0.00137817778159) A[3]:(0.529589116573)\n",
      " state (5)  A[0]:(0.165413454175) A[1]:(0.928591787815) A[2]:(-0.193771019578) A[3]:(0.524771392345)\n",
      " state (6)  A[0]:(-0.000156939029694) A[1]:(0.809901416302) A[2]:(0.00186633842532) A[3]:(0.654002070427)\n",
      " state (7)  A[0]:(0.623591780663) A[1]:(-0.248234331608) A[2]:(0.310991317034) A[3]:(0.88093906641)\n",
      " state (8)  A[0]:(0.656049489975) A[1]:(-0.000318929553032) A[2]:(0.728744506836) A[3]:(0.591441512108)\n",
      " state (9)  A[0]:(0.655944228172) A[1]:(0.809848070145) A[2]:(0.80978345871) A[3]:(0.00254302658141)\n",
      " state (10)  A[0]:(0.728485882282) A[1]:(0.899928808212) A[2]:(0.000980734475888) A[3]:(0.72903496027)\n",
      " state (11)  A[0]:(0.523044586182) A[1]:(0.876636624336) A[2]:(-0.625332117081) A[3]:(0.844797611237)\n",
      " state (12)  A[0]:(0.0797025412321) A[1]:(0.824034333229) A[2]:(-0.62337821722) A[3]:(0.793870031834)\n",
      " state (13)  A[0]:(-0.00199842196889) A[1]:(0.808960676193) A[2]:(0.899937570095) A[3]:(0.727785825729)\n",
      " state (14)  A[0]:(0.808858454227) A[1]:(0.900554895401) A[2]:(1.0) A[3]:(0.808709025383)\n",
      " state (15)  A[0]:(0.981789529324) A[1]:(0.956166744232) A[2]:(1.0) A[3]:(0.873965382576)\n",
      "Episode 915000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5994. Times reached goal: 997.               Steps done: 6655204. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00115857611414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531737804413) A[1]:(0.590342879295) A[2]:(0.590389966965) A[3]:(0.531225144863)\n",
      " state (1)  A[0]:(0.530854582787) A[1]:(0.000273063778877) A[2]:(0.656002342701) A[3]:(0.589457392693)\n",
      " state (2)  A[0]:(0.589874148369) A[1]:(0.728997409344) A[2]:(0.590270221233) A[3]:(0.654958724976)\n",
      " state (3)  A[0]:(0.656226694584) A[1]:(-0.216346219182) A[2]:(0.539502263069) A[3]:(0.517250776291)\n",
      " state (4)  A[0]:(0.590678930283) A[1]:(0.656068205833) A[2]:(-0.000239968299866) A[3]:(0.531933426857)\n",
      " state (5)  A[0]:(0.163751572371) A[1]:(0.928702354431) A[2]:(-0.195821598172) A[3]:(0.527787446976)\n",
      " state (6)  A[0]:(-1.37686729431e-05) A[1]:(0.810008227825) A[2]:(-0.000160336494446) A[3]:(0.656618595123)\n",
      " state (7)  A[0]:(0.624360561371) A[1]:(-0.247969090939) A[2]:(0.309791058302) A[3]:(0.881650567055)\n",
      " state (8)  A[0]:(0.65618956089) A[1]:(0.000500574649777) A[2]:(0.728990912437) A[3]:(0.591067671776)\n",
      " state (9)  A[0]:(0.656215429306) A[1]:(0.810058116913) A[2]:(0.810003519058) A[3]:(0.00106477702502)\n",
      " state (10)  A[0]:(0.729266881943) A[1]:(0.899994134903) A[2]:(0.000445365876658) A[3]:(0.729192614555)\n",
      " state (11)  A[0]:(0.524852395058) A[1]:(0.876631259918) A[2]:(-0.625954210758) A[3]:(0.84527361393)\n",
      " state (12)  A[0]:(0.0827215537429) A[1]:(0.823865294456) A[2]:(-0.623902618885) A[3]:(0.794795453548)\n",
      " state (13)  A[0]:(0.0016400202876) A[1]:(0.808583259583) A[2]:(0.900027275085) A[3]:(0.729343175888)\n",
      " state (14)  A[0]:(0.810316562653) A[1]:(0.900220990181) A[2]:(1.0) A[3]:(0.810217320919)\n",
      " state (15)  A[0]:(0.981931388378) A[1]:(0.955932319164) A[2]:(1.0) A[3]:(0.875165820122)\n",
      "Episode 916000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 6661208. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0011516408636.\n",
      " state (0)  A[0]:(0.532522857189) A[1]:(0.590455770493) A[2]:(0.590549588203) A[3]:(0.530733108521)\n",
      " state (1)  A[0]:(0.532122373581) A[1]:(1.011043787e-05) A[2]:(0.65608137846) A[3]:(0.59004932642)\n",
      " state (2)  A[0]:(0.590689301491) A[1]:(0.728957355022) A[2]:(0.590416431427) A[3]:(0.655522167683)\n",
      " state (3)  A[0]:(0.65530705452) A[1]:(-0.21600677073) A[2]:(0.539481878281) A[3]:(0.515777647495)\n",
      " state (4)  A[0]:(0.588402509689) A[1]:(0.656091213226) A[2]:(-0.000473499268992) A[3]:(0.529406070709)\n",
      " state (5)  A[0]:(0.159336701035) A[1]:(0.928708314896) A[2]:(-0.196329846978) A[3]:(0.525006949902)\n",
      " state (6)  A[0]:(-0.00519682280719) A[1]:(0.809972405434) A[2]:(-0.000936150259804) A[3]:(0.654459297657)\n",
      " state (7)  A[0]:(0.620802879333) A[1]:(-0.248081102967) A[2]:(0.309078425169) A[3]:(0.880792140961)\n",
      " state (8)  A[0]:(0.652618050575) A[1]:(0.000321604311466) A[2]:(0.72901725769) A[3]:(0.587723612785)\n",
      " state (9)  A[0]:(0.653009295464) A[1]:(0.809980750084) A[2]:(0.810078144073) A[3]:(-0.0040580406785)\n",
      " state (10)  A[0]:(0.726913571358) A[1]:(0.899976968765) A[2]:(-0.000424265832407) A[3]:(0.727532863617)\n",
      " state (11)  A[0]:(0.521231889725) A[1]:(0.876628816128) A[2]:(-0.626891136169) A[3]:(0.844534993172)\n",
      " state (12)  A[0]:(0.0776406601071) A[1]:(0.823884904385) A[2]:(-0.624749481678) A[3]:(0.794010996819)\n",
      " state (13)  A[0]:(-0.00323002412915) A[1]:(0.808676302433) A[2]:(0.90015488863) A[3]:(0.728580653667)\n",
      " state (14)  A[0]:(0.808799922466) A[1]:(0.90032762289) A[2]:(1.0) A[3]:(0.809897780418)\n",
      " state (15)  A[0]:(0.981747865677) A[1]:(0.955977737904) A[2]:(1.0) A[3]:(0.87502515316)\n",
      "Episode 917000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 999.               Steps done: 6667211. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00114474827231.\n",
      " state (0)  A[0]:(0.531557142735) A[1]:(0.590686261654) A[2]:(0.590377271175) A[3]:(0.531128168106)\n",
      " state (1)  A[0]:(0.531706571579) A[1]:(0.00016862899065) A[2]:(0.655938863754) A[3]:(0.590420782566)\n",
      " state (2)  A[0]:(0.59099727869) A[1]:(0.728912234306) A[2]:(0.590376138687) A[3]:(0.655857563019)\n",
      " state (3)  A[0]:(0.656798362732) A[1]:(-0.21737113595) A[2]:(0.539905071259) A[3]:(0.51658141613)\n",
      " state (4)  A[0]:(0.591187298298) A[1]:(0.656021416187) A[2]:(0.000190377235413) A[3]:(0.530439376831)\n",
      " state (5)  A[0]:(0.164455741644) A[1]:(0.928681313992) A[2]:(-0.195455417037) A[3]:(0.525889515877)\n",
      " state (6)  A[0]:(0.000842511479277) A[1]:(0.809905052185) A[2]:(0.000105381011963) A[3]:(0.654912829399)\n",
      " state (7)  A[0]:(0.625075161457) A[1]:(-0.248475298285) A[2]:(0.309978574514) A[3]:(0.880919873714)\n",
      " state (8)  A[0]:(0.656567275524) A[1]:(-0.00020893663168) A[2]:(0.729162812233) A[3]:(0.588607907295)\n",
      " state (9)  A[0]:(0.6562743783) A[1]:(0.809869527817) A[2]:(0.810073077679) A[3]:(-0.00258042826317)\n",
      " state (10)  A[0]:(0.729241728783) A[1]:(0.899940371513) A[2]:(-0.00013530254364) A[3]:(0.727851629257)\n",
      " state (11)  A[0]:(0.524641335011) A[1]:(0.876602113247) A[2]:(-0.626611590385) A[3]:(0.844541072845)\n",
      " state (12)  A[0]:(0.0822082981467) A[1]:(0.823868870735) A[2]:(-0.624548912048) A[3]:(0.793840110302)\n",
      " state (13)  A[0]:(0.0011936420342) A[1]:(0.808659672737) A[2]:(0.900044560432) A[3]:(0.728133678436)\n",
      " state (14)  A[0]:(0.810258507729) A[1]:(0.90030580759) A[2]:(1.0) A[3]:(0.809363126755)\n",
      " state (15)  A[0]:(0.98191422224) A[1]:(0.955975830555) A[2]:(1.0) A[3]:(0.874576210976)\n",
      "Episode 918000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5999. Times reached goal: 998.               Steps done: 6673210. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0011379014849.\n",
      " state (0)  A[0]:(0.531646490097) A[1]:(0.590522766113) A[2]:(0.590496659279) A[3]:(0.53136330843)\n",
      " state (1)  A[0]:(0.531485319138) A[1]:(0.000116899609566) A[2]:(0.656135439873) A[3]:(0.590569317341)\n",
      " state (2)  A[0]:(0.590699136257) A[1]:(0.729000806808) A[2]:(0.590476036072) A[3]:(0.656078934669)\n",
      " state (3)  A[0]:(0.656149864197) A[1]:(-0.215949952602) A[2]:(0.539666831493) A[3]:(0.517339110374)\n",
      " state (4)  A[0]:(0.590479135513) A[1]:(0.656297564507) A[2]:(-8.29696655273e-05) A[3]:(0.531353116035)\n",
      " state (5)  A[0]:(0.163888052106) A[1]:(0.928763389587) A[2]:(-0.195753276348) A[3]:(0.527027964592)\n",
      " state (6)  A[0]:(0.000623881758656) A[1]:(0.809996604919) A[2]:(-2.03847885132e-05) A[3]:(0.656111061573)\n",
      " state (7)  A[0]:(0.62458896637) A[1]:(-0.248210564256) A[2]:(0.309916943312) A[3]:(0.88149869442)\n",
      " state (8)  A[0]:(0.65613168478) A[1]:(0.000378206343157) A[2]:(0.729009509087) A[3]:(0.590357780457)\n",
      " state (9)  A[0]:(0.656257867813) A[1]:(0.810038685799) A[2]:(0.810006022453) A[3]:(-0.000389665336115)\n",
      " state (10)  A[0]:(0.729386329651) A[1]:(0.899998366833) A[2]:(7.30752944946e-05) A[3]:(0.728645145893)\n",
      " state (11)  A[0]:(0.524987876415) A[1]:(0.876633763313) A[2]:(-0.62633985281) A[3]:(0.844982802868)\n",
      " state (12)  A[0]:(0.0827622786164) A[1]:(0.823855400085) A[2]:(-0.624282121658) A[3]:(0.794410943985)\n",
      " state (13)  A[0]:(0.00156265369151) A[1]:(0.808559119701) A[2]:(0.900013625622) A[3]:(0.728854775429)\n",
      " state (14)  A[0]:(0.810182511806) A[1]:(0.900189936161) A[2]:(1.0) A[3]:(0.809877753258)\n",
      " state (15)  A[0]:(0.981897115707) A[1]:(0.955902695656) A[2]:(1.0) A[3]:(0.874940156937)\n",
      "Episode 919000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5997. Times reached goal: 997.               Steps done: 6679207. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0011310979106.\n",
      "q_values \n",
      "tensor([[ 0.5272,  0.5900,  0.5904,  0.5294]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5282, -0.0003,  0.6561,  0.5888]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5885,  0.7289,  0.5906,  0.6546]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0044,  0.8100, -0.0008,  0.6546]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7296,  0.9000, -0.0001,  0.7271]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9003,  1.0000,  0.8088]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.526355803013) A[1]:(0.590047895908) A[2]:(0.590410113335) A[3]:(0.529512882233)\n",
      " state (1)  A[0]:(0.527430415154) A[1]:(-0.00037998703192) A[2]:(0.656002938747) A[3]:(0.589058041573)\n",
      " state (2)  A[0]:(0.587878584862) A[1]:(0.72891497612) A[2]:(0.590519189835) A[3]:(0.65486741066)\n",
      " state (3)  A[0]:(0.655316829681) A[1]:(-0.21583597362) A[2]:(0.539741158485) A[3]:(0.516280174255)\n",
      " state (4)  A[0]:(0.590951740742) A[1]:(0.655725240707) A[2]:(-0.000101327896118) A[3]:(0.53044462204)\n",
      " state (5)  A[0]:(0.165820747614) A[1]:(0.928731203079) A[2]:(-0.196346819401) A[3]:(0.526165366173)\n",
      " state (6)  A[0]:(0.0036420065444) A[1]:(0.809968173504) A[2]:(-0.000868201022968) A[3]:(0.655273377895)\n",
      " state (7)  A[0]:(0.626968920231) A[1]:(-0.2483959198) A[2]:(0.309066355228) A[3]:(0.881055533886)\n",
      " state (8)  A[0]:(0.657870352268) A[1]:(0.000115744769573) A[2]:(0.728458046913) A[3]:(0.589440047741)\n",
      " state (9)  A[0]:(0.657067358494) A[1]:(0.809998035431) A[2]:(0.809746146202) A[3]:(-0.00153052690439)\n",
      " state (10)  A[0]:(0.729504108429) A[1]:(0.899993777275) A[2]:(-0.000502347887959) A[3]:(0.728074908257)\n",
      " state (11)  A[0]:(0.524666428566) A[1]:(0.876632809639) A[2]:(-0.626718521118) A[3]:(0.844600260258)\n",
      " state (12)  A[0]:(0.0819559693336) A[1]:(0.823875725269) A[2]:(-0.624610960484) A[3]:(0.793876707554)\n",
      " state (13)  A[0]:(0.000730931642465) A[1]:(0.808644235134) A[2]:(0.900043904781) A[3]:(0.728199720383)\n",
      " state (14)  A[0]:(0.809947669506) A[1]:(0.900290548801) A[2]:(1.0) A[3]:(0.809474349022)\n",
      " state (15)  A[0]:(0.981868624687) A[1]:(0.95596653223) A[2]:(1.0) A[3]:(0.874715328217)\n",
      "Episode 920000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 998.               Steps done: 6685214. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00112432377195.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531588196754) A[1]:(0.590955615044) A[2]:(0.590944886208) A[3]:(0.531790018082)\n",
      " state (1)  A[0]:(0.531721293926) A[1]:(1.43349170685e-05) A[2]:(0.656350016594) A[3]:(0.590698897839)\n",
      " state (2)  A[0]:(0.591061115265) A[1]:(0.729132771492) A[2]:(0.590846776962) A[3]:(0.656114459038)\n",
      " state (3)  A[0]:(0.657283902168) A[1]:(-0.216694161296) A[2]:(0.539818525314) A[3]:(0.517019987106)\n",
      " state (4)  A[0]:(0.5919059515) A[1]:(0.656192541122) A[2]:(-0.00024938583374) A[3]:(0.531044542789)\n",
      " state (5)  A[0]:(0.165415301919) A[1]:(0.928721129894) A[2]:(-0.19595938921) A[3]:(0.5267380476)\n",
      " state (6)  A[0]:(0.0017514807405) A[1]:(0.810025334358) A[2]:(-0.0004049539275) A[3]:(0.655854105949)\n",
      " state (7)  A[0]:(0.625908493996) A[1]:(-0.24812887609) A[2]:(0.309631764889) A[3]:(0.881438136101)\n",
      " state (8)  A[0]:(0.65784740448) A[1]:(0.000154584646225) A[2]:(0.729044079781) A[3]:(0.589922606945)\n",
      " state (9)  A[0]:(0.658215939999) A[1]:(0.810046672821) A[2]:(0.80997043848) A[3]:(-0.0012186014792)\n",
      " state (10)  A[0]:(0.73085719347) A[1]:(0.900047898293) A[2]:(-0.000661849859171) A[3]:(0.72845184803)\n",
      " state (11)  A[0]:(0.526760935783) A[1]:(0.876727998257) A[2]:(-0.627007603645) A[3]:(0.844907701015)\n",
      " state (12)  A[0]:(0.0845414027572) A[1]:(0.824031472206) A[2]:(-0.624861836433) A[3]:(0.794308662415)\n",
      " state (13)  A[0]:(0.00318299653009) A[1]:(0.808833539486) A[2]:(0.90014564991) A[3]:(0.728739380836)\n",
      " state (14)  A[0]:(0.810846626759) A[1]:(0.900397658348) A[2]:(1.0) A[3]:(0.809784352779)\n",
      " state (15)  A[0]:(0.981946468353) A[1]:(0.955999612808) A[2]:(1.0) A[3]:(0.87481033802)\n",
      "Episode 921000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 998.               Steps done: 6691216. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00111759579153.\n",
      " state (0)  A[0]:(0.53124755621) A[1]:(0.590336918831) A[2]:(0.590411961079) A[3]:(0.531473875046)\n",
      " state (1)  A[0]:(0.530835747719) A[1]:(-0.000191375613213) A[2]:(0.655983686447) A[3]:(0.59081941843)\n",
      " state (2)  A[0]:(0.589852929115) A[1]:(0.728882670403) A[2]:(0.590606927872) A[3]:(0.656453073025)\n",
      " state (3)  A[0]:(0.655353486538) A[1]:(-0.218010351062) A[2]:(0.540064752102) A[3]:(0.518037259579)\n",
      " state (4)  A[0]:(0.589188218117) A[1]:(0.655870318413) A[2]:(0.000223517417908) A[3]:(0.532420992851)\n",
      " state (5)  A[0]:(0.16108545661) A[1]:(0.92866653204) A[2]:(-0.195453241467) A[3]:(0.528214097023)\n",
      " state (6)  A[0]:(-0.00289433379658) A[1]:(0.809880673885) A[2]:(0.000145077705383) A[3]:(0.657036662102)\n",
      " state (7)  A[0]:(0.622873187065) A[1]:(-0.24856749177) A[2]:(0.309926837683) A[3]:(0.881945967674)\n",
      " state (8)  A[0]:(0.655053138733) A[1]:(-0.000435426802142) A[2]:(0.7287940979) A[3]:(0.592249453068)\n",
      " state (9)  A[0]:(0.655297935009) A[1]:(0.809874355793) A[2]:(0.809892058372) A[3]:(0.00230213603936)\n",
      " state (10)  A[0]:(0.728404045105) A[1]:(0.899956882) A[2]:(-0.000423908204539) A[3]:(0.729952454567)\n",
      " state (11)  A[0]:(0.523034930229) A[1]:(0.876590907574) A[2]:(-0.626827836037) A[3]:(0.845838189125)\n",
      " state (12)  A[0]:(0.0796145871282) A[1]:(0.823790550232) A[2]:(-0.624798715115) A[3]:(0.795551776886)\n",
      " state (13)  A[0]:(-0.00152981164865) A[1]:(0.808498799801) A[2]:(0.899984061718) A[3]:(0.730338573456)\n",
      " state (14)  A[0]:(0.809450030327) A[1]:(0.900166869164) A[2]:(1.0) A[3]:(0.810918688774)\n",
      " state (15)  A[0]:(0.981846392155) A[1]:(0.955887675285) A[2]:(1.0) A[3]:(0.875605583191)\n",
      "Episode 922000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 6697220. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0011109058497.\n",
      " state (0)  A[0]:(0.530809760094) A[1]:(0.590331077576) A[2]:(0.590435862541) A[3]:(0.532704234123)\n",
      " state (1)  A[0]:(0.530865192413) A[1]:(-0.000729031744413) A[2]:(0.656090140343) A[3]:(0.591550707817)\n",
      " state (2)  A[0]:(0.590308368206) A[1]:(0.728939533234) A[2]:(0.590872764587) A[3]:(0.656702518463)\n",
      " state (3)  A[0]:(0.656602621078) A[1]:(-0.218141436577) A[2]:(0.539918661118) A[3]:(0.517115592957)\n",
      " state (4)  A[0]:(0.591102957726) A[1]:(0.656049370766) A[2]:(-0.000290155410767) A[3]:(0.530706167221)\n",
      " state (5)  A[0]:(0.163926512003) A[1]:(0.928678154945) A[2]:(-0.195860922337) A[3]:(0.525864720345)\n",
      " state (6)  A[0]:(-0.000652670743875) A[1]:(0.810000121593) A[2]:(-0.00035452839802) A[3]:(0.654701590538)\n",
      " state (7)  A[0]:(0.624032735825) A[1]:(-0.24839822948) A[2]:(0.309691905975) A[3]:(0.880940914154)\n",
      " state (8)  A[0]:(0.656240284443) A[1]:(-0.000506259442773) A[2]:(0.729124486446) A[3]:(0.589374005795)\n",
      " state (9)  A[0]:(0.656775951385) A[1]:(0.809875071049) A[2]:(0.810054659843) A[3]:(-2.56299972534e-06)\n",
      " state (10)  A[0]:(0.729553222656) A[1]:(0.900007009506) A[2]:(-0.000406622857554) A[3]:(0.729324877262)\n",
      " state (11)  A[0]:(0.524363875389) A[1]:(0.876749396324) A[2]:(-0.626901268959) A[3]:(0.845360815525)\n",
      " state (12)  A[0]:(0.0808219239116) A[1]:(0.824179291725) A[2]:(-0.624862611294) A[3]:(0.794706583023)\n",
      " state (13)  A[0]:(-0.000719249132089) A[1]:(0.809139847755) A[2]:(0.900031089783) A[3]:(0.728962063789)\n",
      " state (14)  A[0]:(0.809653580189) A[1]:(0.900660455227) A[2]:(1.0) A[3]:(0.809643447399)\n",
      " state (15)  A[0]:(0.98185801506) A[1]:(0.956176817417) A[2]:(1.0) A[3]:(0.874570071697)\n",
      "Episode 923000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5997. Times reached goal: 997.               Steps done: 6703217. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00110426368376.\n",
      " state (0)  A[0]:(0.533638596535) A[1]:(0.590334057808) A[2]:(0.590549588203) A[3]:(0.530873954296)\n",
      " state (1)  A[0]:(0.53223657608) A[1]:(0.00013655424118) A[2]:(0.656201422215) A[3]:(0.589697182178)\n",
      " state (2)  A[0]:(0.590748369694) A[1]:(0.729059934616) A[2]:(0.590725302696) A[3]:(0.655190110207)\n",
      " state (3)  A[0]:(0.655478239059) A[1]:(-0.217460766435) A[2]:(0.53954654932) A[3]:(0.515871405602)\n",
      " state (4)  A[0]:(0.588994503021) A[1]:(0.656058371067) A[2]:(-0.000842332665343) A[3]:(0.52986240387)\n",
      " state (5)  A[0]:(0.160608932376) A[1]:(0.928692400455) A[2]:(-0.196500167251) A[3]:(0.525322556496)\n",
      " state (6)  A[0]:(-0.00413773069158) A[1]:(0.809965729713) A[2]:(-0.00103938544635) A[3]:(0.654529511929)\n",
      " state (7)  A[0]:(0.621229887009) A[1]:(-0.248393237591) A[2]:(0.308868914843) A[3]:(0.880949914455)\n",
      " state (8)  A[0]:(0.652552545071) A[1]:(-0.000224947929382) A[2]:(0.728502988815) A[3]:(0.589854955673)\n",
      " state (9)  A[0]:(0.651830911636) A[1]:(0.809906363487) A[2]:(0.809629797935) A[3]:(0.00106666947249)\n",
      " state (10)  A[0]:(0.725190281868) A[1]:(0.899939000607) A[2]:(-0.00113654090092) A[3]:(0.729743778706)\n",
      " state (11)  A[0]:(0.517899930477) A[1]:(0.876542508602) A[2]:(-0.627126693726) A[3]:(0.845661103725)\n",
      " state (12)  A[0]:(0.072763428092) A[1]:(0.823696553707) A[2]:(-0.624827504158) A[3]:(0.795267403126)\n",
      " state (13)  A[0]:(-0.00770717486739) A[1]:(0.808392524719) A[2]:(0.900310099125) A[3]:(0.730063974857)\n",
      " state (14)  A[0]:(0.807723879814) A[1]:(0.900107562542) A[2]:(1.0) A[3]:(0.810931861401)\n",
      " state (15)  A[0]:(0.98167604208) A[1]:(0.955833435059) A[2]:(1.0) A[3]:(0.875714898109)\n",
      "Episode 924000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 6709221. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00109765354809.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5904,  0.5904,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6558, -0.0002,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.0003,  0.7293,  0.5899]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557,  0.8101,  0.8101, -0.0004]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7288,  0.9000, -0.0010,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8105,  0.9000,  1.0000,  0.8109]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531386494637) A[1]:(0.590441226959) A[2]:(0.590440273285) A[3]:(0.53189188242)\n",
      " state (1)  A[0]:(0.530851483345) A[1]:(4.88758087158e-06) A[2]:(0.656087458134) A[3]:(0.590750813484)\n",
      " state (2)  A[0]:(0.590048015118) A[1]:(0.729046702385) A[2]:(0.590870976448) A[3]:(0.656242966652)\n",
      " state (3)  A[0]:(0.65607213974) A[1]:(-0.217357695103) A[2]:(0.54008769989) A[3]:(0.517362654209)\n",
      " state (4)  A[0]:(0.590542376041) A[1]:(0.656080245972) A[2]:(4.05311584473e-05) A[3]:(0.531603932381)\n",
      " state (5)  A[0]:(0.163760736585) A[1]:(0.9287520051) A[2]:(-0.195808067918) A[3]:(0.527425289154)\n",
      " state (6)  A[0]:(0.000329375267029) A[1]:(0.810040712357) A[2]:(-0.000241279602051) A[3]:(0.656370162964)\n",
      " state (7)  A[0]:(0.624578773975) A[1]:(-0.2482355088) A[2]:(0.309767901897) A[3]:(0.881527543068)\n",
      " state (8)  A[0]:(0.655861854553) A[1]:(0.000189185142517) A[2]:(0.729164838791) A[3]:(0.590060353279)\n",
      " state (9)  A[0]:(0.655706524849) A[1]:(0.810047328472) A[2]:(0.810066223145) A[3]:(-0.000277757644653)\n",
      " state (10)  A[0]:(0.728873848915) A[1]:(0.900022089481) A[2]:(-0.000483274430735) A[3]:(0.729286313057)\n",
      " state (11)  A[0]:(0.524061083794) A[1]:(0.876629889011) A[2]:(-0.62694633007) A[3]:(0.845526576042)\n",
      " state (12)  A[0]:(0.0814793035388) A[1]:(0.823763251305) A[2]:(-0.624903798103) A[3]:(0.795192480087)\n",
      " state (13)  A[0]:(0.000901758437976) A[1]:(0.808354794979) A[2]:(0.899984478951) A[3]:(0.73001486063)\n",
      " state (14)  A[0]:(0.810495197773) A[1]:(0.900006592274) A[2]:(1.0) A[3]:(0.810924232006)\n",
      " state (15)  A[0]:(0.981976032257) A[1]:(0.955772936344) A[2]:(1.0) A[3]:(0.875794827938)\n",
      "Episode 925000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 998.               Steps done: 6715224. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00109108407185.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531038999557) A[1]:(0.590329170227) A[2]:(0.590430736542) A[3]:(0.530932784081)\n",
      " state (1)  A[0]:(0.530915379524) A[1]:(-0.000396721035941) A[2]:(0.655911207199) A[3]:(0.59032946825)\n",
      " state (2)  A[0]:(0.590015649796) A[1]:(0.728856921196) A[2]:(0.590543746948) A[3]:(0.655959963799)\n",
      " state (3)  A[0]:(0.65540933609) A[1]:(-0.217676967382) A[2]:(0.540041267872) A[3]:(0.516808509827)\n",
      " state (4)  A[0]:(0.589401364326) A[1]:(0.656011939049) A[2]:(0.000348091125488) A[3]:(0.531022191048)\n",
      " state (5)  A[0]:(0.162035018206) A[1]:(0.928702056408) A[2]:(-0.195182070136) A[3]:(0.526881456375)\n",
      " state (6)  A[0]:(-0.00119513215031) A[1]:(0.809940695763) A[2]:(0.000570058764424) A[3]:(0.656136035919)\n",
      " state (7)  A[0]:(0.624128103256) A[1]:(-0.248540222645) A[2]:(0.310361146927) A[3]:(0.881681919098)\n",
      " state (8)  A[0]:(0.655939817429) A[1]:(-0.000214003026485) A[2]:(0.728991150856) A[3]:(0.591564536095)\n",
      " state (9)  A[0]:(0.655663073063) A[1]:(0.809974551201) A[2]:(0.809929668903) A[3]:(0.00157296529505)\n",
      " state (10)  A[0]:(0.728590011597) A[1]:(0.900005936623) A[2]:(-2.72989273071e-05) A[3]:(0.72954416275)\n",
      " state (11)  A[0]:(0.523432612419) A[1]:(0.876637578011) A[2]:(-0.626392006874) A[3]:(0.845530450344)\n",
      " state (12)  A[0]:(0.0803629308939) A[1]:(0.823808848858) A[2]:(-0.624377608299) A[3]:(0.795119285583)\n",
      " state (13)  A[0]:(-0.000759243790526) A[1]:(0.808412432671) A[2]:(0.89994341135) A[3]:(0.729760527611)\n",
      " state (14)  A[0]:(0.809687376022) A[1]:(0.90003323555) A[2]:(1.0) A[3]:(0.810509622097)\n",
      " state (15)  A[0]:(0.98188662529) A[1]:(0.955795705318) A[2]:(1.0) A[3]:(0.875382184982)\n",
      "Episode 926000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 997.               Steps done: 6721226. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0010845549986.\n",
      " state (0)  A[0]:(0.531295418739) A[1]:(0.590811192989) A[2]:(0.590365052223) A[3]:(0.531639456749)\n",
      " state (1)  A[0]:(0.531211853027) A[1]:(-6.82398676872e-05) A[2]:(0.655894398689) A[3]:(0.591006159782)\n",
      " state (2)  A[0]:(0.590599536896) A[1]:(0.728881120682) A[2]:(0.59044110775) A[3]:(0.656569957733)\n",
      " state (3)  A[0]:(0.656488001347) A[1]:(-0.217062443495) A[2]:(0.539845466614) A[3]:(0.517977416515)\n",
      " state (4)  A[0]:(0.591575145721) A[1]:(0.656010389328) A[2]:(-0.000181078910828) A[3]:(0.532299995422)\n",
      " state (5)  A[0]:(0.166626527905) A[1]:(0.928759217262) A[2]:(-0.196136727929) A[3]:(0.528308153152)\n",
      " state (6)  A[0]:(0.00425830902532) A[1]:(0.809880673885) A[2]:(-0.000347495079041) A[3]:(0.65732884407)\n",
      " state (7)  A[0]:(0.626769244671) A[1]:(-0.248954206705) A[2]:(0.309652894735) A[3]:(0.882035315037)\n",
      " state (8)  A[0]:(0.658025979996) A[1]:(-0.00031691044569) A[2]:(0.728703856468) A[3]:(0.592427194118)\n",
      " state (9)  A[0]:(0.658038675785) A[1]:(0.809880495071) A[2]:(0.809881746769) A[3]:(0.00301182852127)\n",
      " state (10)  A[0]:(0.730707406998) A[1]:(0.899913609028) A[2]:(0.000410079926951) A[3]:(0.730203986168)\n",
      " state (11)  A[0]:(0.526898622513) A[1]:(0.876486361027) A[2]:(-0.625894427299) A[3]:(0.845923841)\n",
      " state (12)  A[0]:(0.0852769464254) A[1]:(0.8235450387) A[2]:(-0.62383556366) A[3]:(0.795598983765)\n",
      " state (13)  A[0]:(0.00414285669103) A[1]:(0.808042168617) A[2]:(0.899994432926) A[3]:(0.730259001255)\n",
      " state (14)  A[0]:(0.81133466959) A[1]:(0.899767637253) A[2]:(1.0) A[3]:(0.810736060143)\n",
      " state (15)  A[0]:(0.982064425945) A[1]:(0.955653488636) A[2]:(1.0) A[3]:(0.875448822975)\n",
      "Episode 927000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 6727230. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00107806283935.\n",
      " state (0)  A[0]:(0.531232535839) A[1]:(0.590605974197) A[2]:(0.590471863747) A[3]:(0.533638954163)\n",
      " state (1)  A[0]:(0.531348407269) A[1]:(0.000193156301975) A[2]:(0.656116127968) A[3]:(0.592342972755)\n",
      " state (2)  A[0]:(0.590524196625) A[1]:(0.729058504105) A[2]:(0.5904687047) A[3]:(0.657543540001)\n",
      " state (3)  A[0]:(0.65610897541) A[1]:(-0.216250240803) A[2]:(0.539613366127) A[3]:(0.518883645535)\n",
      " state (4)  A[0]:(0.590286374092) A[1]:(0.656144320965) A[2]:(-0.000136733055115) A[3]:(0.532802700996)\n",
      " state (5)  A[0]:(0.16328933835) A[1]:(0.928710818291) A[2]:(-0.195647343993) A[3]:(0.528434038162)\n",
      " state (6)  A[0]:(-0.000257730484009) A[1]:(0.810024380684) A[2]:(1.09672546387e-05) A[3]:(0.657009363174)\n",
      " state (7)  A[0]:(0.624575376511) A[1]:(-0.248092353344) A[2]:(0.309884279966) A[3]:(0.881801366806)\n",
      " state (8)  A[0]:(0.6565990448) A[1]:(0.000405646831496) A[2]:(0.729087471962) A[3]:(0.591130375862)\n",
      " state (9)  A[0]:(0.656725406647) A[1]:(0.810103416443) A[2]:(0.810050785542) A[3]:(0.000631079019513)\n",
      " state (10)  A[0]:(0.72941672802) A[1]:(0.899992227554) A[2]:(0.000656485441141) A[3]:(0.728798508644)\n",
      " state (11)  A[0]:(0.524490237236) A[1]:(0.876543223858) A[2]:(-0.625778198242) A[3]:(0.844878435135)\n",
      " state (12)  A[0]:(0.081542417407) A[1]:(0.823573231697) A[2]:(-0.623765647411) A[3]:(0.794101774693)\n",
      " state (13)  A[0]:(0.00022280216217) A[1]:(0.808019161224) A[2]:(0.900019526482) A[3]:(0.728259801865)\n",
      " state (14)  A[0]:(0.810107946396) A[1]:(0.89972692728) A[2]:(1.0) A[3]:(0.809258103371)\n",
      " state (15)  A[0]:(0.981950759888) A[1]:(0.955620586872) A[2]:(1.0) A[3]:(0.874452531338)\n",
      "Episode 928000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 999.               Steps done: 6733230. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0010716138287.\n",
      " state (0)  A[0]:(0.531258225441) A[1]:(0.590416789055) A[2]:(0.590450465679) A[3]:(0.531746506691)\n",
      " state (1)  A[0]:(0.53145980835) A[1]:(0.000381931633456) A[2]:(0.656106293201) A[3]:(0.590720653534)\n",
      " state (2)  A[0]:(0.590699970722) A[1]:(0.729049265385) A[2]:(0.590297341347) A[3]:(0.65620803833)\n",
      " state (3)  A[0]:(0.65622484684) A[1]:(-0.215449362993) A[2]:(0.539520859718) A[3]:(0.516961455345)\n",
      " state (4)  A[0]:(0.590427398682) A[1]:(0.656140089035) A[2]:(1.77621841431e-05) A[3]:(0.530809521675)\n",
      " state (5)  A[0]:(0.163660317659) A[1]:(0.928712666035) A[2]:(-0.195518359542) A[3]:(0.526580810547)\n",
      " state (6)  A[0]:(0.00038909909199) A[1]:(0.810006976128) A[2]:(0.000144004821777) A[3]:(0.655859470367)\n",
      " state (7)  A[0]:(0.624902963638) A[1]:(-0.248114362359) A[2]:(0.309977054596) A[3]:(0.881541788578)\n",
      " state (8)  A[0]:(0.656505227089) A[1]:(0.000261351466179) A[2]:(0.729052662849) A[3]:(0.590851962566)\n",
      " state (9)  A[0]:(0.656230092049) A[1]:(0.810049891472) A[2]:(0.810057997704) A[3]:(0.000588968337979)\n",
      " state (10)  A[0]:(0.729146420956) A[1]:(0.900001227856) A[2]:(0.000350713729858) A[3]:(0.729122400284)\n",
      " state (11)  A[0]:(0.524355232716) A[1]:(0.876587212086) A[2]:(-0.626146018505) A[3]:(0.845249176025)\n",
      " state (12)  A[0]:(0.0816285610199) A[1]:(0.823671877384) A[2]:(-0.624142110348) A[3]:(0.794712543488)\n",
      " state (13)  A[0]:(0.000489175261464) A[1]:(0.808177351952) A[2]:(0.899997591972) A[3]:(0.729163527489)\n",
      " state (14)  A[0]:(0.810144543648) A[1]:(0.899849176407) A[2]:(1.0) A[3]:(0.809986531734)\n",
      " state (15)  A[0]:(0.981937289238) A[1]:(0.955685734749) A[2]:(1.0) A[3]:(0.874967813492)\n",
      "Episode 929000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 998.               Steps done: 6739230. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00106520339625.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5905,  0.5905,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5863,  0.6563,  0.0001,  0.5322]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.0001,  0.7291,  0.5930]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6589,  0.8100,  0.8101,  0.0052]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7323,  0.9000,  0.0014,  0.7311]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8126,  0.8996,  1.0000,  0.8113]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532128572464) A[1]:(0.590459227562) A[2]:(0.590427517891) A[3]:(0.531922698021)\n",
      " state (1)  A[0]:(0.531708478928) A[1]:(-3.83630394936e-05) A[2]:(0.656144499779) A[3]:(0.591149091721)\n",
      " state (2)  A[0]:(0.590270400047) A[1]:(0.728981375694) A[2]:(0.590393781662) A[3]:(0.656744241714)\n",
      " state (3)  A[0]:(0.654164850712) A[1]:(-0.215962648392) A[2]:(0.539478540421) A[3]:(0.518380999565)\n",
      " state (4)  A[0]:(0.586466729641) A[1]:(0.656095266342) A[2]:(0.000247359275818) A[3]:(0.532484054565)\n",
      " state (5)  A[0]:(0.156257838011) A[1]:(0.928592562675) A[2]:(-0.194617494941) A[3]:(0.528105080128)\n",
      " state (6)  A[0]:(-0.0079776737839) A[1]:(0.810031235218) A[2]:(0.00091278529726) A[3]:(0.656723678112)\n",
      " state (7)  A[0]:(0.621458649635) A[1]:(-0.247706100345) A[2]:(0.310204833746) A[3]:(0.881965279579)\n",
      " state (8)  A[0]:(0.656651318073) A[1]:(0.000264577567577) A[2]:(0.728891015053) A[3]:(0.593609571457)\n",
      " state (9)  A[0]:(0.659152388573) A[1]:(0.810105860233) A[2]:(0.809963703156) A[3]:(0.00595397176221)\n",
      " state (10)  A[0]:(0.732357501984) A[1]:(0.900000870228) A[2]:(0.00125682284124) A[3]:(0.731203496456)\n",
      " state (11)  A[0]:(0.52987742424) A[1]:(0.876539409161) A[2]:(-0.625160396099) A[3]:(0.846358954906)\n",
      " state (12)  A[0]:(0.0897135958076) A[1]:(0.823521137238) A[2]:(-0.623119950294) A[3]:(0.796132206917)\n",
      " state (13)  A[0]:(0.00862600840628) A[1]:(0.807870388031) A[2]:(0.90013808012) A[3]:(0.731007814407)\n",
      " state (14)  A[0]:(0.812676072121) A[1]:(0.899574279785) A[2]:(1.0) A[3]:(0.811376452446)\n",
      " state (15)  A[0]:(0.982191264629) A[1]:(0.955520570278) A[2]:(1.0) A[3]:(0.875977277756)\n",
      "Episode 930000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 1000.               Steps done: 6745237. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00105882389945.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530640542507) A[1]:(0.590548336506) A[2]:(0.590555369854) A[3]:(0.53152346611)\n",
      " state (1)  A[0]:(0.53051173687) A[1]:(-1.74418091774e-05) A[2]:(0.656151711941) A[3]:(0.590766191483)\n",
      " state (2)  A[0]:(0.589727342129) A[1]:(0.729026377201) A[2]:(0.590687274933) A[3]:(0.656367897987)\n",
      " state (3)  A[0]:(0.655397057533) A[1]:(-0.215716093779) A[2]:(0.540017366409) A[3]:(0.517824530602)\n",
      " state (4)  A[0]:(0.589628636837) A[1]:(0.656165480614) A[2]:(0.000427842111094) A[3]:(0.531911611557)\n",
      " state (5)  A[0]:(0.162678837776) A[1]:(0.928773880005) A[2]:(-0.195450946689) A[3]:(0.527661561966)\n",
      " state (6)  A[0]:(-0.000235795974731) A[1]:(0.810015916824) A[2]:(0.000185251235962) A[3]:(0.656507253647)\n",
      " state (7)  A[0]:(0.624386131763) A[1]:(-0.248193368316) A[2]:(0.309957444668) A[3]:(0.881540298462)\n",
      " state (8)  A[0]:(0.655859172344) A[1]:(0.000491164566483) A[2]:(0.729018211365) A[3]:(0.590496957302)\n",
      " state (9)  A[0]:(0.655943512917) A[1]:(0.810097098351) A[2]:(0.810022354126) A[3]:(0.000509247125592)\n",
      " state (10)  A[0]:(0.729039430618) A[1]:(0.900004863739) A[2]:(0.000131845474243) A[3]:(0.729360818863)\n",
      " state (11)  A[0]:(0.52418500185) A[1]:(0.876564085484) A[2]:(-0.626313388348) A[3]:(0.845476150513)\n",
      " state (12)  A[0]:(0.0813950449228) A[1]:(0.823606431484) A[2]:(-0.624284982681) A[3]:(0.795076012611)\n",
      " state (13)  A[0]:(0.00034636259079) A[1]:(0.808083295822) A[2]:(0.900004208088) A[3]:(0.729763388634)\n",
      " state (14)  A[0]:(0.810120046139) A[1]:(0.899789094925) A[2]:(1.0) A[3]:(0.81059896946)\n",
      " state (15)  A[0]:(0.981935203075) A[1]:(0.955652594566) A[2]:(1.0) A[3]:(0.875506699085)\n",
      "Episode 931000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 998.               Steps done: 6751246. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00105248050446.\n",
      " state (0)  A[0]:(0.530836105347) A[1]:(0.590007662773) A[2]:(0.590341687202) A[3]:(0.531860351562)\n",
      " state (1)  A[0]:(0.530299782753) A[1]:(-0.000435627967818) A[2]:(0.65584218502) A[3]:(0.589869022369)\n",
      " state (2)  A[0]:(0.589390814304) A[1]:(0.728677928448) A[2]:(0.589981257915) A[3]:(0.655549168587)\n",
      " state (3)  A[0]:(0.655193269253) A[1]:(-0.216991618276) A[2]:(0.539337038994) A[3]:(0.518321990967)\n",
      " state (4)  A[0]:(0.589188814163) A[1]:(0.656070947647) A[2]:(-0.000397801370127) A[3]:(0.533460259438)\n",
      " state (5)  A[0]:(0.161577567458) A[1]:(0.92866641283) A[2]:(-0.195738479495) A[3]:(0.529646992683)\n",
      " state (6)  A[0]:(-0.0017774682492) A[1]:(0.809806585312) A[2]:(5.16176223755e-05) A[3]:(0.658458888531)\n",
      " state (7)  A[0]:(0.623706042767) A[1]:(-0.248875260353) A[2]:(0.309982031584) A[3]:(0.882606863976)\n",
      " state (8)  A[0]:(0.655544936657) A[1]:(-0.000550873519387) A[2]:(0.729004502296) A[3]:(0.594222664833)\n",
      " state (9)  A[0]:(0.655586242676) A[1]:(0.809798836708) A[2]:(0.809849739075) A[3]:(0.00676986668259)\n",
      " state (10)  A[0]:(0.728862762451) A[1]:(0.899908006191) A[2]:(-0.000499129237141) A[3]:(0.732276380062)\n",
      " state (11)  A[0]:(0.524216532707) A[1]:(0.876542210579) A[2]:(-0.626662015915) A[3]:(0.847202539444)\n",
      " state (12)  A[0]:(0.0817996710539) A[1]:(0.82371866703) A[2]:(-0.624535799026) A[3]:(0.797207295895)\n",
      " state (13)  A[0]:(0.000965892977547) A[1]:(0.808376073837) A[2]:(0.900035321712) A[3]:(0.732254266739)\n",
      " state (14)  A[0]:(0.810294866562) A[1]:(0.900055229664) A[2]:(1.0) A[3]:(0.812123537064)\n",
      " state (15)  A[0]:(0.981932222843) A[1]:(0.955813527107) A[2]:(1.0) A[3]:(0.876292467117)\n",
      "Episode 932000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 999.               Steps done: 6757251. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00104617929734.\n",
      " state (0)  A[0]:(0.530623435974) A[1]:(0.590507745743) A[2]:(0.590482294559) A[3]:(0.531416654587)\n",
      " state (1)  A[0]:(0.530187606812) A[1]:(8.51899385452e-05) A[2]:(0.656121432781) A[3]:(0.590567469597)\n",
      " state (2)  A[0]:(0.589190006256) A[1]:(0.729021191597) A[2]:(0.590608358383) A[3]:(0.656167149544)\n",
      " state (3)  A[0]:(0.654829084873) A[1]:(-0.216766357422) A[2]:(0.539735078812) A[3]:(0.517563045025)\n",
      " state (4)  A[0]:(0.588519215584) A[1]:(0.656018614769) A[2]:(-0.000119209289551) A[3]:(0.531777739525)\n",
      " state (5)  A[0]:(0.16020603478) A[1]:(0.928675174713) A[2]:(-0.195667862892) A[3]:(0.527477800846)\n",
      " state (6)  A[0]:(-0.00358615769073) A[1]:(0.809941768646) A[2]:(-4.88758087158e-05) A[3]:(0.656321406364)\n",
      " state (7)  A[0]:(0.622663795948) A[1]:(-0.248364254832) A[2]:(0.309863269329) A[3]:(0.881575882435)\n",
      " state (8)  A[0]:(0.655016720295) A[1]:(-0.000166490674019) A[2]:(0.729020953178) A[3]:(0.590604305267)\n",
      " state (9)  A[0]:(0.655529618263) A[1]:(0.809939026833) A[2]:(0.810001134872) A[3]:(-0.000101178884506)\n",
      " state (10)  A[0]:(0.728914916515) A[1]:(0.899967551231) A[2]:(-1.8835067749e-05) A[3]:(0.728847801685)\n",
      " state (11)  A[0]:(0.524179458618) A[1]:(0.876567184925) A[2]:(-0.62644982338) A[3]:(0.845105648041)\n",
      " state (12)  A[0]:(0.0815224647522) A[1]:(0.823671519756) A[2]:(-0.624453186989) A[3]:(0.794556736946)\n",
      " state (13)  A[0]:(0.000443935365183) A[1]:(0.808218717575) A[2]:(0.899944424629) A[3]:(0.729054570198)\n",
      " state (14)  A[0]:(0.81002920866) A[1]:(0.89990144968) A[2]:(1.0) A[3]:(0.810026168823)\n",
      " state (15)  A[0]:(0.981912851334) A[1]:(0.95572245121) A[2]:(1.0) A[3]:(0.875085175037)\n",
      "Episode 933000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 998.               Steps done: 6763253. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00103991893534.\n",
      " state (0)  A[0]:(0.531303048134) A[1]:(0.590269863605) A[2]:(0.590552568436) A[3]:(0.531134188175)\n",
      " state (1)  A[0]:(0.531289458275) A[1]:(-0.000367432803614) A[2]:(0.656157255173) A[3]:(0.590588748455)\n",
      " state (2)  A[0]:(0.590492606163) A[1]:(0.729043722153) A[2]:(0.590529203415) A[3]:(0.656359910965)\n",
      " state (3)  A[0]:(0.6566016078) A[1]:(-0.215749427676) A[2]:(0.539612114429) A[3]:(0.517839729786)\n",
      " state (4)  A[0]:(0.590887904167) A[1]:(0.656167328358) A[2]:(1.19209289551e-07) A[3]:(0.53202611208)\n",
      " state (5)  A[0]:(0.163781553507) A[1]:(0.928689658642) A[2]:(-0.195445910096) A[3]:(0.527831912041)\n",
      " state (6)  A[0]:(-0.00021231174469) A[1]:(0.809984385967) A[2]:(0.000248074531555) A[3]:(0.656772971153)\n",
      " state (7)  A[0]:(0.624530434608) A[1]:(-0.248566478491) A[2]:(0.310231864452) A[3]:(0.8819206357)\n",
      " state (8)  A[0]:(0.656178712845) A[1]:(-0.000875830417499) A[2]:(0.729042291641) A[3]:(0.592194199562)\n",
      " state (9)  A[0]:(0.655810594559) A[1]:(0.809799969196) A[2]:(0.80999314785) A[3]:(0.00240772496909)\n",
      " state (10)  A[0]:(0.728778123856) A[1]:(0.899993181229) A[2]:(-0.000417470902903) A[3]:(0.730293750763)\n",
      " state (11)  A[0]:(0.523563146591) A[1]:(0.876693427563) A[2]:(-0.626878142357) A[3]:(0.8461176157)\n",
      " state (12)  A[0]:(0.080169223249) A[1]:(0.823976397514) A[2]:(-0.624829173088) A[3]:(0.795897126198)\n",
      " state (13)  A[0]:(-0.00108158541843) A[1]:(0.808722853661) A[2]:(0.90001732111) A[3]:(0.730712413788)\n",
      " state (14)  A[0]:(0.809638261795) A[1]:(0.900294601917) A[2]:(1.0) A[3]:(0.811114728451)\n",
      " state (15)  A[0]:(0.98186558485) A[1]:(0.955941200256) A[2]:(1.0) A[3]:(0.875683426857)\n",
      "Episode 934000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 1000.               Steps done: 6769257. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0010336939681.\n",
      "q_values \n",
      "tensor([[ 0.5322,  0.5907,  0.5907,  0.5321]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.0001,  0.6562,  0.5912]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7290,  0.5905,  0.6567]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8100,  0.0001,  0.6568]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000,  0.0002,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.8997,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531998872757) A[1]:(0.590650081635) A[2]:(0.590681254864) A[3]:(0.532237946987)\n",
      " state (1)  A[0]:(0.531534314156) A[1]:(0.000119112432003) A[2]:(0.656199574471) A[3]:(0.591293811798)\n",
      " state (2)  A[0]:(0.590484440327) A[1]:(0.729037880898) A[2]:(0.590534925461) A[3]:(0.65673494339)\n",
      " state (3)  A[0]:(0.656284213066) A[1]:(-0.216038390994) A[2]:(0.53964561224) A[3]:(0.518134593964)\n",
      " state (4)  A[0]:(0.590510845184) A[1]:(0.656207203865) A[2]:(-0.000163435935974) A[3]:(0.532093524933)\n",
      " state (5)  A[0]:(0.163517683744) A[1]:(0.928745269775) A[2]:(-0.195751443505) A[3]:(0.527736663818)\n",
      " state (6)  A[0]:(-2.3365020752e-05) A[1]:(0.810023903847) A[2]:(9.2625617981e-05) A[3]:(0.656809449196)\n",
      " state (7)  A[0]:(0.624421179295) A[1]:(-0.248177006841) A[2]:(0.310152262449) A[3]:(0.881971478462)\n",
      " state (8)  A[0]:(0.656018197536) A[1]:(0.000187627971172) A[2]:(0.729036331177) A[3]:(0.59181201458)\n",
      " state (9)  A[0]:(0.656018018723) A[1]:(0.810050606728) A[2]:(0.810021936893) A[3]:(0.00102964008693)\n",
      " state (10)  A[0]:(0.729014754295) A[1]:(0.899995088577) A[2]:(0.000200629234314) A[3]:(0.729258775711)\n",
      " state (11)  A[0]:(0.523994743824) A[1]:(0.876535117626) A[2]:(-0.626257479191) A[3]:(0.845368206501)\n",
      " state (12)  A[0]:(0.0809085592628) A[1]:(0.823518753052) A[2]:(-0.624224662781) A[3]:(0.794918298721)\n",
      " state (13)  A[0]:(-0.00034499168396) A[1]:(0.80792081356) A[2]:(0.900021791458) A[3]:(0.729515910149)\n",
      " state (14)  A[0]:(0.809838652611) A[1]:(0.899656832218) A[2]:(1.0) A[3]:(0.810313224792)\n",
      " state (15)  A[0]:(0.981903076172) A[1]:(0.955571174622) A[2]:(1.0) A[3]:(0.875246167183)\n",
      "Episode 935000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5997. Times reached goal: 997.               Steps done: 6775254. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00102751345616.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530912637711) A[1]:(0.590324401855) A[2]:(0.590560078621) A[3]:(0.53192949295)\n",
      " state (1)  A[0]:(0.531419038773) A[1]:(-0.000154696404934) A[2]:(0.656107783318) A[3]:(0.590680003166)\n",
      " state (2)  A[0]:(0.590784072876) A[1]:(0.728989720345) A[2]:(0.590460419655) A[3]:(0.656122922897)\n",
      " state (3)  A[0]:(0.656736552715) A[1]:(-0.215721413493) A[2]:(0.539589285851) A[3]:(0.517497301102)\n",
      " state (4)  A[0]:(0.591191649437) A[1]:(0.65600168705) A[2]:(-0.000138282775879) A[3]:(0.531486451626)\n",
      " state (5)  A[0]:(0.164769992232) A[1]:(0.928713560104) A[2]:(-0.195862874389) A[3]:(0.52717256546)\n",
      " state (6)  A[0]:(0.00161927798763) A[1]:(0.810006082058) A[2]:(-0.000179886817932) A[3]:(0.656335651875)\n",
      " state (7)  A[0]:(0.62598246336) A[1]:(-0.24820497632) A[2]:(0.309779524803) A[3]:(0.881793260574)\n",
      " state (8)  A[0]:(0.658109545708) A[1]:(0.000120587646961) A[2]:(0.728811979294) A[3]:(0.591688513756)\n",
      " state (9)  A[0]:(0.658617258072) A[1]:(0.81002175808) A[2]:(0.809895038605) A[3]:(0.00143356528133)\n",
      " state (10)  A[0]:(0.731160163879) A[1]:(0.899979770184) A[2]:(0.00011157989502) A[3]:(0.729417324066)\n",
      " state (11)  A[0]:(0.527145385742) A[1]:(0.876539111137) A[2]:(-0.626237750053) A[3]:(0.845420479774)\n",
      " state (12)  A[0]:(0.085000783205) A[1]:(0.823578119278) A[2]:(-0.624274134636) A[3]:(0.794948041439)\n",
      " state (13)  A[0]:(0.00363044324331) A[1]:(0.808053195477) A[2]:(0.899884998798) A[3]:(0.729467272758)\n",
      " state (14)  A[0]:(0.811296343803) A[1]:(0.899777472019) A[2]:(1.0) A[3]:(0.81014084816)\n",
      " state (15)  A[0]:(0.982079148293) A[1]:(0.955659747124) A[2]:(1.0) A[3]:(0.875060737133)\n",
      "Episode 936000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5995. Times reached goal: 998.               Steps done: 6781249. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00102137194058.\n",
      " state (0)  A[0]:(0.531463384628) A[1]:(0.590499579906) A[2]:(0.590460300446) A[3]:(0.532390892506)\n",
      " state (1)  A[0]:(0.531684815884) A[1]:(-4.3198466301e-05) A[2]:(0.656074047089) A[3]:(0.591457903385)\n",
      " state (2)  A[0]:(0.590693831444) A[1]:(0.728914141655) A[2]:(0.590636014938) A[3]:(0.65692615509)\n",
      " state (3)  A[0]:(0.656349241734) A[1]:(-0.217591524124) A[2]:(0.539960384369) A[3]:(0.518170416355)\n",
      " state (4)  A[0]:(0.590287327766) A[1]:(0.656024694443) A[2]:(1.7523765564e-05) A[3]:(0.532270133495)\n",
      " state (5)  A[0]:(0.162577167153) A[1]:(0.928706049919) A[2]:(-0.19562086463) A[3]:(0.52788901329)\n",
      " state (6)  A[0]:(-0.00196069222875) A[1]:(0.809999465942) A[2]:(4.20808792114e-05) A[3]:(0.656597137451)\n",
      " state (7)  A[0]:(0.622712433338) A[1]:(-0.248329922557) A[2]:(0.310022860765) A[3]:(0.881713092327)\n",
      " state (8)  A[0]:(0.654522895813) A[1]:(-0.000133380293846) A[2]:(0.729084014893) A[3]:(0.591680765152)\n",
      " state (9)  A[0]:(0.654762983322) A[1]:(0.80997979641) A[2]:(0.810066223145) A[3]:(0.00288956542499)\n",
      " state (10)  A[0]:(0.727982759476) A[1]:(0.900013327599) A[2]:(0.000216960906982) A[3]:(0.730446696281)\n",
      " state (11)  A[0]:(0.522229790688) A[1]:(0.876656115055) A[2]:(-0.626289963722) A[3]:(0.8460688591)\n",
      " state (12)  A[0]:(0.0783318951726) A[1]:(0.82385969162) A[2]:(-0.624142885208) A[3]:(0.795737862587)\n",
      " state (13)  A[0]:(-0.00259112729691) A[1]:(0.808536231518) A[2]:(0.900291800499) A[3]:(0.73046875)\n",
      " state (14)  A[0]:(0.809468150139) A[1]:(0.900162875652) A[2]:(1.0) A[3]:(0.81094956398)\n",
      " state (15)  A[0]:(0.981878757477) A[1]:(0.955861926079) A[2]:(1.0) A[3]:(0.875577270985)\n",
      "Episode 937000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 999.               Steps done: 6787256. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00101525495011.\n",
      " state (0)  A[0]:(0.531445384026) A[1]:(0.590645849705) A[2]:(0.590534090996) A[3]:(0.531531870365)\n",
      " state (1)  A[0]:(0.531802475452) A[1]:(-9.53525304794e-05) A[2]:(0.65611833334) A[3]:(0.590649724007)\n",
      " state (2)  A[0]:(0.590966105461) A[1]:(0.728977441788) A[2]:(0.590867161751) A[3]:(0.656217813492)\n",
      " state (3)  A[0]:(0.65625500679) A[1]:(-0.218252301216) A[2]:(0.540141105652) A[3]:(0.517902731895)\n",
      " state (4)  A[0]:(0.59050321579) A[1]:(0.656086683273) A[2]:(-0.000137329101562) A[3]:(0.532384634018)\n",
      " state (5)  A[0]:(0.163836494088) A[1]:(0.928733170033) A[2]:(-0.195923283696) A[3]:(0.528108119965)\n",
      " state (6)  A[0]:(0.000621199549641) A[1]:(0.80988240242) A[2]:(-0.000157713890076) A[3]:(0.656860172749)\n",
      " state (7)  A[0]:(0.624735236168) A[1]:(-0.248751014471) A[2]:(0.309888809919) A[3]:(0.881737947464)\n",
      " state (8)  A[0]:(0.656279444695) A[1]:(-0.000227823853493) A[2]:(0.728982567787) A[3]:(0.59127676487)\n",
      " state (9)  A[0]:(0.65647149086) A[1]:(0.809939801693) A[2]:(0.810041666031) A[3]:(0.00151425483637)\n",
      " state (10)  A[0]:(0.729441463947) A[1]:(0.899965286255) A[2]:(0.000278830528259) A[3]:(0.729792118073)\n",
      " state (11)  A[0]:(0.524609267712) A[1]:(0.876561462879) A[2]:(-0.626222491264) A[3]:(0.845752835274)\n",
      " state (12)  A[0]:(0.0816322341561) A[1]:(0.823669075966) A[2]:(-0.624214887619) A[3]:(0.795433163643)\n",
      " state (13)  A[0]:(0.000422537297709) A[1]:(0.808235347271) A[2]:(0.900023400784) A[3]:(0.730153620243)\n",
      " state (14)  A[0]:(0.810314178467) A[1]:(0.899934053421) A[2]:(1.0) A[3]:(0.810756266117)\n",
      " state (15)  A[0]:(0.981971561909) A[1]:(0.955747127533) A[2]:(1.0) A[3]:(0.875508666039)\n",
      "Episode 938000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 999.               Steps done: 6793257. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00100918064932.\n",
      " state (0)  A[0]:(0.531586170197) A[1]:(0.589816451073) A[2]:(0.590414404869) A[3]:(0.531472682953)\n",
      " state (1)  A[0]:(0.531674563885) A[1]:(-0.000774599437136) A[2]:(0.656000852585) A[3]:(0.59038233757)\n",
      " state (2)  A[0]:(0.590801239014) A[1]:(0.729000687599) A[2]:(0.590180516243) A[3]:(0.655979275703)\n",
      " state (3)  A[0]:(0.656163930893) A[1]:(-0.215168058872) A[2]:(0.538732767105) A[3]:(0.51761752367)\n",
      " state (4)  A[0]:(0.590175747871) A[1]:(0.6561242342) A[2]:(-0.00131213595159) A[3]:(0.531756877899)\n",
      " state (5)  A[0]:(0.163019031286) A[1]:(0.928689718246) A[2]:(-0.196614310145) A[3]:(0.527414798737)\n",
      " state (6)  A[0]:(-0.00144952430855) A[1]:(0.809990048409) A[2]:(-0.000673413160257) A[3]:(0.656216263771)\n",
      " state (7)  A[0]:(0.622735977173) A[1]:(-0.248315408826) A[2]:(0.309733510017) A[3]:(0.881549417973)\n",
      " state (8)  A[0]:(0.65461397171) A[1]:(-0.000146247446537) A[2]:(0.729006052017) A[3]:(0.590969562531)\n",
      " state (9)  A[0]:(0.655023872852) A[1]:(0.80998134613) A[2]:(0.81000071764) A[3]:(0.00111305667087)\n",
      " state (10)  A[0]:(0.72852396965) A[1]:(0.899995625019) A[2]:(0.000372529000742) A[3]:(0.729411900043)\n",
      " state (11)  A[0]:(0.523666381836) A[1]:(0.87659740448) A[2]:(-0.626013040543) A[3]:(0.845407366753)\n",
      " state (12)  A[0]:(0.0809463337064) A[1]:(0.82369017601) A[2]:(-0.623968839645) A[3]:(0.794872403145)\n",
      " state (13)  A[0]:(0.000206410884857) A[1]:(0.808189213276) A[2]:(0.900032281876) A[3]:(0.729288220406)\n",
      " state (14)  A[0]:(0.810389876366) A[1]:(0.899856209755) A[2]:(1.0) A[3]:(0.810001432896)\n",
      " state (15)  A[0]:(0.981998980045) A[1]:(0.955687105656) A[2]:(1.0) A[3]:(0.874929189682)\n",
      "Episode 939000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               5998. Times reached goal: 998.               Steps done: 6799255. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00100314570069.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5904,  0.5903,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6559,  0.0001,  0.5312]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558, -0.0002,  0.7290,  0.5902]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557,  0.8099,  0.8101,  0.0002]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000,  0.0003,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8105,  0.8999,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531105458736) A[1]:(0.590368747711) A[2]:(0.59027326107) A[3]:(0.531457126141)\n",
      " state (1)  A[0]:(0.530988812447) A[1]:(0.000348031520844) A[2]:(0.656053602695) A[3]:(0.590678989887)\n",
      " state (2)  A[0]:(0.590201973915) A[1]:(0.729080438614) A[2]:(0.590482532978) A[3]:(0.656205415726)\n",
      " state (3)  A[0]:(0.656010627747) A[1]:(-0.216088786721) A[2]:(0.539782643318) A[3]:(0.517363965511)\n",
      " state (4)  A[0]:(0.590517044067) A[1]:(0.655862569809) A[2]:(8.39233398438e-05) A[3]:(0.531266570091)\n",
      " state (5)  A[0]:(0.164159506559) A[1]:(0.92871016264) A[2]:(-0.195738598704) A[3]:(0.526812434196)\n",
      " state (6)  A[0]:(0.000520110072102) A[1]:(0.810075938702) A[2]:(-0.000115394592285) A[3]:(0.655611157417)\n",
      " state (7)  A[0]:(0.624365448952) A[1]:(-0.248149201274) A[2]:(0.309921234846) A[3]:(0.881223022938)\n",
      " state (8)  A[0]:(0.65582036972) A[1]:(-0.000229716300964) A[2]:(0.728985786438) A[3]:(0.590241909027)\n",
      " state (9)  A[0]:(0.655697107315) A[1]:(0.809935986996) A[2]:(0.810077667236) A[3]:(0.000259503722191)\n",
      " state (10)  A[0]:(0.729001402855) A[1]:(0.900001943111) A[2]:(0.000257253646851) A[3]:(0.729313850403)\n",
      " state (11)  A[0]:(0.524377524853) A[1]:(0.876624524593) A[2]:(-0.626299262047) A[3]:(0.845478653908)\n",
      " state (12)  A[0]:(0.081777587533) A[1]:(0.823746204376) A[2]:(-0.624299347401) A[3]:(0.795012295246)\n",
      " state (13)  A[0]:(0.00085425353609) A[1]:(0.808278560638) A[2]:(0.900000929832) A[3]:(0.729488670826)\n",
      " state (14)  A[0]:(0.810503721237) A[1]:(0.89992493391) A[2]:(1.0) A[3]:(0.810186326504)\n",
      " state (15)  A[0]:(0.981990098953) A[1]:(0.955722570419) A[2]:(1.0) A[3]:(0.875056564808)\n",
      "Episode 940000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 1000.               Steps done: 6805255. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000997144847048.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530978441238) A[1]:(0.590500473976) A[2]:(0.590488314629) A[3]:(0.530612707138)\n",
      " state (1)  A[0]:(0.531025886536) A[1]:(0.000155076384544) A[2]:(0.656169772148) A[3]:(0.590090274811)\n",
      " state (2)  A[0]:(0.590235233307) A[1]:(0.729094326496) A[2]:(0.590803444386) A[3]:(0.655741691589)\n",
      " state (3)  A[0]:(0.655809521675) A[1]:(-0.216954737902) A[2]:(0.540138781071) A[3]:(0.517127275467)\n",
      " state (4)  A[0]:(0.590208828449) A[1]:(0.656204402447) A[2]:(0.000137448310852) A[3]:(0.531347990036)\n",
      " state (5)  A[0]:(0.163725286722) A[1]:(0.928775846958) A[2]:(-0.195772990584) A[3]:(0.526957929134)\n",
      " state (6)  A[0]:(0.00065809476655) A[1]:(0.810030698776) A[2]:(-6.96182250977e-05) A[3]:(0.655999779701)\n",
      " state (7)  A[0]:(0.624637067318) A[1]:(-0.248267114162) A[2]:(0.309936434031) A[3]:(0.881489872932)\n",
      " state (8)  A[0]:(0.655837535858) A[1]:(9.90182161331e-05) A[2]:(0.728873193264) A[3]:(0.590718626976)\n",
      " state (9)  A[0]:(0.655539870262) A[1]:(0.810006260872) A[2]:(0.809918344021) A[3]:(0.00024601817131)\n",
      " state (10)  A[0]:(0.728601336479) A[1]:(0.899984836578) A[2]:(-0.00030779838562) A[3]:(0.728993833065)\n",
      " state (11)  A[0]:(0.523391544819) A[1]:(0.876548171043) A[2]:(-0.626690685749) A[3]:(0.845200061798)\n",
      " state (12)  A[0]:(0.080073222518) A[1]:(0.823580265045) A[2]:(-0.624740958214) A[3]:(0.794637084007)\n",
      " state (13)  A[0]:(-0.000953435606789) A[1]:(0.808048844337) A[2]:(0.899865984917) A[3]:(0.729073524475)\n",
      " state (14)  A[0]:(0.810032010078) A[1]:(0.899777829647) A[2]:(1.0) A[3]:(0.810015082359)\n",
      " state (15)  A[0]:(0.981956899166) A[1]:(0.955646336079) A[2]:(1.0) A[3]:(0.875031173229)\n",
      "Episode 941000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 6811259. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000991175926018.\n",
      " state (0)  A[0]:(0.531482994556) A[1]:(0.590520262718) A[2]:(0.590546429157) A[3]:(0.531538844109)\n",
      " state (1)  A[0]:(0.531418502331) A[1]:(-1.45435333252e-05) A[2]:(0.656186580658) A[3]:(0.590512573719)\n",
      " state (2)  A[0]:(0.590431272984) A[1]:(0.729019403458) A[2]:(0.590997457504) A[3]:(0.655935049057)\n",
      " state (3)  A[0]:(0.656183242798) A[1]:(-0.217075780034) A[2]:(0.540260434151) A[3]:(0.516916334629)\n",
      " state (4)  A[0]:(0.59071367979) A[1]:(0.655868291855) A[2]:(0.000176906585693) A[3]:(0.531005501747)\n",
      " state (5)  A[0]:(0.16452434659) A[1]:(0.928748846054) A[2]:(-0.195950329304) A[3]:(0.52680170536)\n",
      " state (6)  A[0]:(0.0015735613415) A[1]:(0.809936881065) A[2]:(-0.000171899795532) A[3]:(0.65609484911)\n",
      " state (7)  A[0]:(0.624871253967) A[1]:(-0.248253688216) A[2]:(0.309954762459) A[3]:(0.881532549858)\n",
      " state (8)  A[0]:(0.655879378319) A[1]:(0.000656306627207) A[2]:(0.728959083557) A[3]:(0.590367913246)\n",
      " state (9)  A[0]:(0.655776619911) A[1]:(0.810140430927) A[2]:(0.809944033623) A[3]:(-0.000620603503194)\n",
      " state (10)  A[0]:(0.72876226902) A[1]:(0.900002002716) A[2]:(2.36034393311e-05) A[3]:(0.728498697281)\n",
      " state (11)  A[0]:(0.523499369621) A[1]:(0.876522779465) A[2]:(-0.626306653023) A[3]:(0.844904303551)\n",
      " state (12)  A[0]:(0.0800304636359) A[1]:(0.823483467102) A[2]:(-0.624280571938) A[3]:(0.794337689877)\n",
      " state (13)  A[0]:(-0.00127333332784) A[1]:(0.807864904404) A[2]:(0.899998605251) A[3]:(0.728811502457)\n",
      " state (14)  A[0]:(0.809770941734) A[1]:(0.899626612663) A[2]:(1.0) A[3]:(0.80989831686)\n",
      " state (15)  A[0]:(0.981915533543) A[1]:(0.955552697182) A[2]:(1.0) A[3]:(0.874997138977)\n",
      "Episode 942000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 6817263. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000985242735021.\n",
      " state (0)  A[0]:(0.532440900803) A[1]:(0.59070110321) A[2]:(0.590561807156) A[3]:(0.531735539436)\n",
      " state (1)  A[0]:(0.533997893333) A[1]:(0.000295713543892) A[2]:(0.656203329563) A[3]:(0.590843200684)\n",
      " state (2)  A[0]:(0.593648314476) A[1]:(0.729104995728) A[2]:(0.590869128704) A[3]:(0.65629529953)\n",
      " state (3)  A[0]:(0.660197675228) A[1]:(-0.217179596424) A[2]:(0.540197730064) A[3]:(0.517404735088)\n",
      " state (4)  A[0]:(0.596086859703) A[1]:(0.656396210194) A[2]:(-4.44650650024e-05) A[3]:(0.531572043896)\n",
      " state (5)  A[0]:(0.172889873385) A[1]:(0.928853392601) A[2]:(-0.196078151464) A[3]:(0.527263522148)\n",
      " state (6)  A[0]:(0.00995495822281) A[1]:(0.810073971748) A[2]:(-0.000192046165466) A[3]:(0.656290411949)\n",
      " state (7)  A[0]:(0.629132807255) A[1]:(-0.248116821051) A[2]:(0.310109049082) A[3]:(0.881453931332)\n",
      " state (8)  A[0]:(0.658442497253) A[1]:(0.000752642634325) A[2]:(0.729289770126) A[3]:(0.589611530304)\n",
      " state (9)  A[0]:(0.657052516937) A[1]:(0.810149490833) A[2]:(0.810165524483) A[3]:(-0.00134362198878)\n",
      " state (10)  A[0]:(0.729523658752) A[1]:(0.900031685829) A[2]:(0.000147223472595) A[3]:(0.728520989418)\n",
      " state (11)  A[0]:(0.524587154388) A[1]:(0.876605212688) A[2]:(-0.626383423805) A[3]:(0.844959557056)\n",
      " state (12)  A[0]:(0.0814314559102) A[1]:(0.823673307896) A[2]:(-0.624334335327) A[3]:(0.794332504272)\n",
      " state (13)  A[0]:(-5.18560409546e-05) A[1]:(0.808175861835) A[2]:(0.90007942915) A[3]:(0.728651881218)\n",
      " state (14)  A[0]:(0.809971034527) A[1]:(0.899870157242) A[2]:(1.0) A[3]:(0.809629261494)\n",
      " state (15)  A[0]:(0.981901466846) A[1]:(0.955691099167) A[2]:(1.0) A[3]:(0.874689340591)\n",
      "Episode 943000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 1000.               Steps done: 6823268. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000979344080832.\n",
      " state (0)  A[0]:(0.531592547894) A[1]:(0.590528786182) A[2]:(0.590517044067) A[3]:(0.531137824059)\n",
      " state (1)  A[0]:(0.531608939171) A[1]:(2.41175293922e-05) A[2]:(0.656108081341) A[3]:(0.590501248837)\n",
      " state (2)  A[0]:(0.590651035309) A[1]:(0.729001760483) A[2]:(0.590605139732) A[3]:(0.656029522419)\n",
      " state (3)  A[0]:(0.656829595566) A[1]:(-0.217662408948) A[2]:(0.539769768715) A[3]:(0.517389297485)\n",
      " state (4)  A[0]:(0.591377496719) A[1]:(0.65605866909) A[2]:(-0.000396370858653) A[3]:(0.531573355198)\n",
      " state (5)  A[0]:(0.164928928018) A[1]:(0.928698956966) A[2]:(-0.195976927876) A[3]:(0.527037501335)\n",
      " state (6)  A[0]:(0.00112831545994) A[1]:(0.809982299805) A[2]:(-0.000255346298218) A[3]:(0.655900120735)\n",
      " state (7)  A[0]:(0.624760985374) A[1]:(-0.248349547386) A[2]:(0.309827387333) A[3]:(0.881446778774)\n",
      " state (8)  A[0]:(0.656081199646) A[1]:(-0.000431768567069) A[2]:(0.72894090414) A[3]:(0.590579509735)\n",
      " state (9)  A[0]:(0.655776381493) A[1]:(0.809869110584) A[2]:(0.809985876083) A[3]:(-3.07112932205e-05)\n",
      " state (10)  A[0]:(0.7289904356) A[1]:(0.899999678135) A[2]:(-0.000353932351572) A[3]:(0.729091286659)\n",
      " state (11)  A[0]:(0.524237036705) A[1]:(0.876680672169) A[2]:(-0.626769781113) A[3]:(0.845319628716)\n",
      " state (12)  A[0]:(0.0812308564782) A[1]:(0.823905825615) A[2]:(-0.624760508537) A[3]:(0.794731676579)\n",
      " state (13)  A[0]:(-0.000264883041382) A[1]:(0.808548748493) A[2]:(0.899937272072) A[3]:(0.729029178619)\n",
      " state (14)  A[0]:(0.809827387333) A[1]:(0.900143384933) A[2]:(1.0) A[3]:(0.809854090214)\n",
      " state (15)  A[0]:(0.981872975826) A[1]:(0.955843746662) A[2]:(1.0) A[3]:(0.87478685379)\n",
      "Episode 944000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 998.               Steps done: 6829275. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000973478794962.\n",
      "q_values \n",
      "tensor([[ 0.5308,  0.5901,  0.5904,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.3105e-01,  1.6391e-07,  6.5608e-01,  5.9013e-01]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.7289,  0.5907,  0.6556]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  0.8100, -0.0001,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000, -0.0001,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.8999,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530986189842) A[1]:(0.590161681175) A[2]:(0.590506374836) A[3]:(0.531092762947)\n",
      " state (1)  A[0]:(0.531258046627) A[1]:(-9.34675335884e-05) A[2]:(0.656082391739) A[3]:(0.590127587318)\n",
      " state (2)  A[0]:(0.590369701385) A[1]:(0.729003071785) A[2]:(0.59078335762) A[3]:(0.655625581741)\n",
      " state (3)  A[0]:(0.655972242355) A[1]:(-0.217558756471) A[2]:(0.540202200413) A[3]:(0.516665637493)\n",
      " state (4)  A[0]:(0.590302765369) A[1]:(0.655997753143) A[2]:(0.000220537185669) A[3]:(0.530899643898)\n",
      " state (5)  A[0]:(0.163731783628) A[1]:(0.928749620914) A[2]:(-0.195673137903) A[3]:(0.526625037193)\n",
      " state (6)  A[0]:(0.000673055532388) A[1]:(0.810013711452) A[2]:(5.84125518799e-06) A[3]:(0.655871987343)\n",
      " state (7)  A[0]:(0.624623775482) A[1]:(-0.248162433505) A[2]:(0.309980928898) A[3]:(0.881461381912)\n",
      " state (8)  A[0]:(0.655849575996) A[1]:(0.00024851411581) A[2]:(0.729083001614) A[3]:(0.590196371078)\n",
      " state (9)  A[0]:(0.655841112137) A[1]:(0.810014486313) A[2]:(0.810066461563) A[3]:(-0.000623315514531)\n",
      " state (10)  A[0]:(0.729172348976) A[1]:(0.90000718832) A[2]:(-0.00012469291687) A[3]:(0.728860676289)\n",
      " state (11)  A[0]:(0.524592280388) A[1]:(0.876616060734) A[2]:(-0.626598238945) A[3]:(0.845199644566)\n",
      " state (12)  A[0]:(0.0818480998278) A[1]:(0.823720991611) A[2]:(-0.624567270279) A[3]:(0.794638872147)\n",
      " state (13)  A[0]:(0.000533223093953) A[1]:(0.808251261711) A[2]:(0.900014519691) A[3]:(0.729024648666)\n",
      " state (14)  A[0]:(0.810147225857) A[1]:(0.899929046631) A[2]:(1.0) A[3]:(0.809941589832)\n",
      " state (15)  A[0]:(0.981911063194) A[1]:(0.95572155714) A[2]:(1.0) A[3]:(0.874914288521)\n",
      "Episode 945000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 1000.               Steps done: 6835283. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000967647668605.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53203946352) A[1]:(0.590386748314) A[2]:(0.590436398983) A[3]:(0.53152346611)\n",
      " state (1)  A[0]:(0.53224915266) A[1]:(0.000227570533752) A[2]:(0.656080365181) A[3]:(0.59052324295)\n",
      " state (2)  A[0]:(0.59127676487) A[1]:(0.729036808014) A[2]:(0.590512812138) A[3]:(0.656026482582)\n",
      " state (3)  A[0]:(0.656910300255) A[1]:(-0.216903403401) A[2]:(0.539750576019) A[3]:(0.517299294472)\n",
      " state (4)  A[0]:(0.591402709484) A[1]:(0.656007766724) A[2]:(-0.000198721885681) A[3]:(0.531466543674)\n",
      " state (5)  A[0]:(0.165390253067) A[1]:(0.928728163242) A[2]:(-0.195871919394) A[3]:(0.527180850506)\n",
      " state (6)  A[0]:(0.00241648685187) A[1]:(0.809971868992) A[2]:(-3.48091125488e-05) A[3]:(0.656276106834)\n",
      " state (7)  A[0]:(0.625808119774) A[1]:(-0.248273208737) A[2]:(0.309996575117) A[3]:(0.881619811058)\n",
      " state (8)  A[0]:(0.656997382641) A[1]:(4.71472740173e-05) A[2]:(0.728955030441) A[3]:(0.590921163559)\n",
      " state (9)  A[0]:(0.656764566898) A[1]:(0.8099578619) A[2]:(0.810003995895) A[3]:(0.000255480408669)\n",
      " state (10)  A[0]:(0.729704260826) A[1]:(0.899978995323) A[2]:(9.82284545898e-05) A[3]:(0.729176998138)\n",
      " state (11)  A[0]:(0.525205016136) A[1]:(0.876577973366) A[2]:(-0.626322209835) A[3]:(0.845429062843)\n",
      " state (12)  A[0]:(0.0824736356735) A[1]:(0.823652505875) A[2]:(-0.624295592308) A[3]:(0.795014381409)\n",
      " state (13)  A[0]:(0.000982463010587) A[1]:(0.808136761189) A[2]:(0.899981975555) A[3]:(0.729571580887)\n",
      " state (14)  A[0]:(0.810351192951) A[1]:(0.89983522892) A[2]:(1.0) A[3]:(0.810357928276)\n",
      " state (15)  A[0]:(0.981951951981) A[1]:(0.955672562122) A[2]:(1.0) A[3]:(0.875226795673)\n",
      "Episode 946000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 1000.               Steps done: 6841287. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000961855318039.\n",
      " state (0)  A[0]:(0.531379103661) A[1]:(0.59055274725) A[2]:(0.590527057648) A[3]:(0.531456708908)\n",
      " state (1)  A[0]:(0.531509459019) A[1]:(-1.81943178177e-05) A[2]:(0.656126379967) A[3]:(0.590560913086)\n",
      " state (2)  A[0]:(0.590597093105) A[1]:(0.729037106037) A[2]:(0.590680122375) A[3]:(0.656007647514)\n",
      " state (3)  A[0]:(0.656386733055) A[1]:(-0.217107370496) A[2]:(0.539922475815) A[3]:(0.517320811749)\n",
      " state (4)  A[0]:(0.590961277485) A[1]:(0.656126141548) A[2]:(-9.28640365601e-05) A[3]:(0.531492829323)\n",
      " state (5)  A[0]:(0.164886310697) A[1]:(0.928749322891) A[2]:(-0.195794895291) A[3]:(0.527114868164)\n",
      " state (6)  A[0]:(0.00174486462492) A[1]:(0.81002920866) A[2]:(-8.14199447632e-05) A[3]:(0.656157135963)\n",
      " state (7)  A[0]:(0.625159680843) A[1]:(-0.248136505485) A[2]:(0.309796661139) A[3]:(0.881563007832)\n",
      " state (8)  A[0]:(0.656395494938) A[1]:(0.000263795256615) A[2]:(0.728948116302) A[3]:(0.590708076954)\n",
      " state (9)  A[0]:(0.656280517578) A[1]:(0.810012578964) A[2]:(0.809978961945) A[3]:(0.000395625800593)\n",
      " state (10)  A[0]:(0.7294267416) A[1]:(0.899993777275) A[2]:(0.000207543373108) A[3]:(0.729228854179)\n",
      " state (11)  A[0]:(0.524993300438) A[1]:(0.876598417759) A[2]:(-0.626128256321) A[3]:(0.845391094685)\n",
      " state (12)  A[0]:(0.0823932960629) A[1]:(0.823688209057) A[2]:(-0.624090671539) A[3]:(0.794879376888)\n",
      " state (13)  A[0]:(0.000648140790872) A[1]:(0.808169364929) A[2]:(0.899964928627) A[3]:(0.729251742363)\n",
      " state (14)  A[0]:(0.809844791889) A[1]:(0.899845302105) A[2]:(1.0) A[3]:(0.809966266155)\n",
      " state (15)  A[0]:(0.981863677502) A[1]:(0.955678462982) A[2]:(1.0) A[3]:(0.874864459038)\n",
      "Episode 947000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 1000.               Steps done: 6847291. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000956097640554.\n",
      " state (0)  A[0]:(0.532196640968) A[1]:(0.590582609177) A[2]:(0.590580821037) A[3]:(0.531880795956)\n",
      " state (1)  A[0]:(0.531462728977) A[1]:(-2.60174274445e-05) A[2]:(0.656147241592) A[3]:(0.590827643871)\n",
      " state (2)  A[0]:(0.590303480625) A[1]:(0.729035496712) A[2]:(0.590733647346) A[3]:(0.656122207642)\n",
      " state (3)  A[0]:(0.656482577324) A[1]:(-0.217329338193) A[2]:(0.540038228035) A[3]:(0.517521262169)\n",
      " state (4)  A[0]:(0.591402769089) A[1]:(0.656126260757) A[2]:(-0.000123262405396) A[3]:(0.531720995903)\n",
      " state (5)  A[0]:(0.16597057879) A[1]:(0.928766191006) A[2]:(-0.196036189795) A[3]:(0.527328848839)\n",
      " state (6)  A[0]:(0.00327281979844) A[1]:(0.810058832169) A[2]:(-0.000380039185984) A[3]:(0.656219363213)\n",
      " state (7)  A[0]:(0.626172661781) A[1]:(-0.248168423772) A[2]:(0.309724152088) A[3]:(0.881459593773)\n",
      " state (8)  A[0]:(0.656877636909) A[1]:(0.000273019075394) A[2]:(0.729133725166) A[3]:(0.590039432049)\n",
      " state (9)  A[0]:(0.656180202961) A[1]:(0.810050070286) A[2]:(0.810134410858) A[3]:(-0.000454664201243)\n",
      " state (10)  A[0]:(0.729272007942) A[1]:(0.900038361549) A[2]:(0.000424623460276) A[3]:(0.72906768322)\n",
      " state (11)  A[0]:(0.524908781052) A[1]:(0.876679778099) A[2]:(-0.626037895679) A[3]:(0.845367372036)\n",
      " state (12)  A[0]:(0.0824410766363) A[1]:(0.823829054832) A[2]:(-0.62397313118) A[3]:(0.794852674007)\n",
      " state (13)  A[0]:(0.000808536831755) A[1]:(0.808345019817) A[2]:(0.90004748106) A[3]:(0.72915160656)\n",
      " state (14)  A[0]:(0.809992015362) A[1]:(0.899953305721) A[2]:(1.0) A[3]:(0.809843480587)\n",
      " state (15)  A[0]:(0.981875121593) A[1]:(0.955725073814) A[2]:(1.0) A[3]:(0.874697625637)\n",
      "Episode 948000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 999.               Steps done: 6853291. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0009503782301.\n",
      " state (0)  A[0]:(0.533250927925) A[1]:(0.5901735425) A[2]:(0.5904763937) A[3]:(0.530901134014)\n",
      " state (1)  A[0]:(0.533293366432) A[1]:(-0.00010696798563) A[2]:(0.656123161316) A[3]:(0.590166330338)\n",
      " state (2)  A[0]:(0.591895580292) A[1]:(0.729031860828) A[2]:(0.590584516525) A[3]:(0.655646443367)\n",
      " state (3)  A[0]:(0.656840980053) A[1]:(-0.216643750668) A[2]:(0.539795637131) A[3]:(0.516691029072)\n",
      " state (4)  A[0]:(0.590882718563) A[1]:(0.655690133572) A[2]:(-0.000128269195557) A[3]:(0.530695915222)\n",
      " state (5)  A[0]:(0.164212271571) A[1]:(0.928647100925) A[2]:(-0.196048691869) A[3]:(0.52632856369)\n",
      " state (6)  A[0]:(6.27636909485e-05) A[1]:(0.809951007366) A[2]:(-0.000661730649881) A[3]:(0.655588328838)\n",
      " state (7)  A[0]:(0.623629212379) A[1]:(-0.248407766223) A[2]:(0.309281885624) A[3]:(0.881452798843)\n",
      " state (8)  A[0]:(0.65463924408) A[1]:(-0.000493593455758) A[2]:(0.728780508041) A[3]:(0.590510249138)\n",
      " state (9)  A[0]:(0.65395796299) A[1]:(0.809861898422) A[2]:(0.80991089344) A[3]:(-0.000207081437111)\n",
      " state (10)  A[0]:(0.727388083935) A[1]:(0.89999961853) A[2]:(-0.000435590714915) A[3]:(0.729154706001)\n",
      " state (11)  A[0]:(0.521858096123) A[1]:(0.876691699028) A[2]:(-0.626718401909) A[3]:(0.845445394516)\n",
      " state (12)  A[0]:(0.0780842155218) A[1]:(0.823932886124) A[2]:(-0.624643743038) A[3]:(0.79494959116)\n",
      " state (13)  A[0]:(-0.00353901111521) A[1]:(0.808586478233) A[2]:(0.899980247021) A[3]:(0.729291677475)\n",
      " state (14)  A[0]:(0.808600246906) A[1]:(0.900178909302) A[2]:(1.0) A[3]:(0.810003340244)\n",
      " state (15)  A[0]:(0.981725692749) A[1]:(0.95586168766) A[2]:(1.0) A[3]:(0.87481379509)\n",
      "Episode 949000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 1000.               Steps done: 6859295. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000944689254601.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5905,  0.5905,  0.5308]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5895,  0.6561,  0.0001,  0.5303]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6567,  0.0000,  0.7291,  0.5899]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6576,  0.8100,  0.8100, -0.0009]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0042,  0.8083,  0.9000,  0.7284]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8113,  0.9000,  1.0000,  0.8093]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532188296318) A[1]:(0.590542793274) A[2]:(0.590469360352) A[3]:(0.531057238579)\n",
      " state (1)  A[0]:(0.532000422478) A[1]:(3.21045517921e-05) A[2]:(0.656155288219) A[3]:(0.59010797739)\n",
      " state (2)  A[0]:(0.590767502785) A[1]:(0.729038953781) A[2]:(0.590667843819) A[3]:(0.655456185341)\n",
      " state (3)  A[0]:(0.656076312065) A[1]:(-0.217819258571) A[2]:(0.539995312691) A[3]:(0.516141891479)\n",
      " state (4)  A[0]:(0.590010881424) A[1]:(0.656116127968) A[2]:(-4.32729721069e-05) A[3]:(0.530175149441)\n",
      " state (5)  A[0]:(0.162806734443) A[1]:(0.928685665131) A[2]:(-0.195615470409) A[3]:(0.525652050972)\n",
      " state (6)  A[0]:(-0.000825345341582) A[1]:(0.810011208057) A[2]:(-0.000112056732178) A[3]:(0.654912889004)\n",
      " state (7)  A[0]:(0.624269008636) A[1]:(-0.248173058033) A[2]:(0.309693425894) A[3]:(0.881147801876)\n",
      " state (8)  A[0]:(0.656704068184) A[1]:(-5.3308904171e-05) A[2]:(0.728984355927) A[3]:(0.589548647404)\n",
      " state (9)  A[0]:(0.657419681549) A[1]:(0.809980511665) A[2]:(0.809965491295) A[3]:(-0.00154745450709)\n",
      " state (10)  A[0]:(0.730567574501) A[1]:(0.899999678135) A[2]:(0.000126719474792) A[3]:(0.728211760521)\n",
      " state (11)  A[0]:(0.526921749115) A[1]:(0.876632332802) A[2]:(-0.626168489456) A[3]:(0.844734966755)\n",
      " state (12)  A[0]:(0.0851971805096) A[1]:(0.823774397373) A[2]:(-0.624141216278) A[3]:(0.794003248215)\n",
      " state (13)  A[0]:(0.00368796102703) A[1]:(0.808307409286) A[2]:(0.899952769279) A[3]:(0.728101551533)\n",
      " state (14)  A[0]:(0.811116278172) A[1]:(0.899952888489) A[2]:(1.0) A[3]:(0.809144079685)\n",
      " state (15)  A[0]:(0.982013106346) A[1]:(0.95573925972) A[2]:(1.0) A[3]:(0.874289870262)\n",
      "Episode 950000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6000. Times reached goal: 998.               Steps done: 6865295. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000939038089522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531984388828) A[1]:(0.590550005436) A[2]:(0.590619683266) A[3]:(0.532198905945)\n",
      " state (1)  A[0]:(0.532201290131) A[1]:(0.000228971242905) A[2]:(0.656267046928) A[3]:(0.591273784637)\n",
      " state (2)  A[0]:(0.591104745865) A[1]:(0.729138791561) A[2]:(0.590779006481) A[3]:(0.65660238266)\n",
      " state (3)  A[0]:(0.656618356705) A[1]:(-0.217601701617) A[2]:(0.540176331997) A[3]:(0.517776012421)\n",
      " state (4)  A[0]:(0.590709030628) A[1]:(0.656197190285) A[2]:(0.000180721282959) A[3]:(0.531884431839)\n",
      " state (5)  A[0]:(0.163721874356) A[1]:(0.928705096245) A[2]:(-0.195545881987) A[3]:(0.527429819107)\n",
      " state (6)  A[0]:(-6.27040863037e-05) A[1]:(0.810092568398) A[2]:(-0.000290870666504) A[3]:(0.656220793724)\n",
      " state (7)  A[0]:(0.624639749527) A[1]:(-0.247818037868) A[2]:(0.309375226498) A[3]:(0.881532728672)\n",
      " state (8)  A[0]:(0.656655192375) A[1]:(0.000377714604838) A[2]:(0.729100883007) A[3]:(0.590139627457)\n",
      " state (9)  A[0]:(0.656963825226) A[1]:(0.810067355633) A[2]:(0.810092806816) A[3]:(-0.000327900052071)\n",
      " state (10)  A[0]:(0.730012238026) A[1]:(0.900001108646) A[2]:(0.000332832336426) A[3]:(0.729058384895)\n",
      " state (11)  A[0]:(0.525903701782) A[1]:(0.876591086388) A[2]:(-0.62607717514) A[3]:(0.84532636404)\n",
      " state (12)  A[0]:(0.0837001726031) A[1]:(0.823664903641) A[2]:(-0.624048054218) A[3]:(0.794832110405)\n",
      " state (13)  A[0]:(0.00223052129149) A[1]:(0.808140337467) A[2]:(0.900014221668) A[3]:(0.729214668274)\n",
      " state (14)  A[0]:(0.810701072216) A[1]:(0.899838805199) A[2]:(1.0) A[3]:(0.809953093529)\n",
      " state (15)  A[0]:(0.981976211071) A[1]:(0.955674827099) A[2]:(1.0) A[3]:(0.874827325344)\n",
      "Episode 951000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 999.               Steps done: 6871298. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000933417929657.\n",
      " state (0)  A[0]:(0.531402826309) A[1]:(0.590588450432) A[2]:(0.59051835537) A[3]:(0.532336890697)\n",
      " state (1)  A[0]:(0.531485319138) A[1]:(1.17719173431e-06) A[2]:(0.65613090992) A[3]:(0.591386735439)\n",
      " state (2)  A[0]:(0.590470671654) A[1]:(0.729030847549) A[2]:(0.590845227242) A[3]:(0.656768143177)\n",
      " state (3)  A[0]:(0.656072497368) A[1]:(-0.218039184809) A[2]:(0.540400207043) A[3]:(0.518554925919)\n",
      " state (4)  A[0]:(0.590507090092) A[1]:(0.656175971031) A[2]:(0.000281691551208) A[3]:(0.533066809177)\n",
      " state (5)  A[0]:(0.164263665676) A[1]:(0.928750216961) A[2]:(-0.195701569319) A[3]:(0.528907120228)\n",
      " state (6)  A[0]:(0.00139480736107) A[1]:(0.810004115105) A[2]:(-0.000303864479065) A[3]:(0.65771985054)\n",
      " state (7)  A[0]:(0.625209093094) A[1]:(-0.248275279999) A[2]:(0.309451133013) A[3]:(0.88215136528)\n",
      " state (8)  A[0]:(0.656444907188) A[1]:(0.000136062502861) A[2]:(0.728922903538) A[3]:(0.591365158558)\n",
      " state (9)  A[0]:(0.656278610229) A[1]:(0.809979856014) A[2]:(0.809934616089) A[3]:(-9.50694084167e-05)\n",
      " state (10)  A[0]:(0.729443907738) A[1]:(0.899979770184) A[2]:(-0.000328302383423) A[3]:(0.728987157345)\n",
      " state (11)  A[0]:(0.525152146816) A[1]:(0.876590907574) A[2]:(-0.626575350761) A[3]:(0.845324516296)\n",
      " state (12)  A[0]:(0.0827838256955) A[1]:(0.823695719242) A[2]:(-0.624530315399) A[3]:(0.794866800308)\n",
      " state (13)  A[0]:(0.00140440370888) A[1]:(0.808214068413) A[2]:(0.899925231934) A[3]:(0.729295134544)\n",
      " state (14)  A[0]:(0.810446202755) A[1]:(0.899906873703) A[2]:(1.0) A[3]:(0.810063123703)\n",
      " state (15)  A[0]:(0.981944084167) A[1]:(0.955713391304) A[2]:(1.0) A[3]:(0.874913811684)\n",
      "Episode 952000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 1000.               Steps done: 6877299. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000927833262216.\n",
      " state (0)  A[0]:(0.531040668488) A[1]:(0.590517878532) A[2]:(0.590464234352) A[3]:(0.531362295151)\n",
      " state (1)  A[0]:(0.531771481037) A[1]:(0.000425718695624) A[2]:(0.656143784523) A[3]:(0.590102910995)\n",
      " state (2)  A[0]:(0.590794503689) A[1]:(0.729040503502) A[2]:(0.590424537659) A[3]:(0.655588507652)\n",
      " state (3)  A[0]:(0.656249642372) A[1]:(-0.217681139708) A[2]:(0.539546728134) A[3]:(0.517221152782)\n",
      " state (4)  A[0]:(0.590127646923) A[1]:(0.655995607376) A[2]:(-0.000863075023517) A[3]:(0.531700611115)\n",
      " state (5)  A[0]:(0.162488266826) A[1]:(0.928667902946) A[2]:(-0.196674823761) A[3]:(0.527326107025)\n",
      " state (6)  A[0]:(-0.00218504318036) A[1]:(0.810090959072) A[2]:(-0.0015770184109) A[3]:(0.656139373779)\n",
      " state (7)  A[0]:(0.622676372528) A[1]:(-0.247755885124) A[2]:(0.308335036039) A[3]:(0.88149523735)\n",
      " state (8)  A[0]:(0.654692471027) A[1]:(0.000318519771099) A[2]:(0.72884106636) A[3]:(0.589594364166)\n",
      " state (9)  A[0]:(0.655140995979) A[1]:(0.810058951378) A[2]:(0.809992134571) A[3]:(-0.00157736113761)\n",
      " state (10)  A[0]:(0.728505373001) A[1]:(0.900027990341) A[2]:(-0.000204920768738) A[3]:(0.728482723236)\n",
      " state (11)  A[0]:(0.523388445377) A[1]:(0.876671135426) A[2]:(-0.626517891884) A[3]:(0.844953119755)\n",
      " state (12)  A[0]:(0.0799747928977) A[1]:(0.823851466179) A[2]:(-0.624517798424) A[3]:(0.794293999672)\n",
      " state (13)  A[0]:(-0.00173055951018) A[1]:(0.808443665504) A[2]:(0.899899721146) A[3]:(0.728444457054)\n",
      " state (14)  A[0]:(0.809272646904) A[1]:(0.900074899197) A[2]:(1.0) A[3]:(0.809285759926)\n",
      " state (15)  A[0]:(0.981819927692) A[1]:(0.9558147192) A[2]:(1.0) A[3]:(0.874308586121)\n",
      "Episode 953000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 1000.               Steps done: 6883306. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000922276474331.\n",
      " state (0)  A[0]:(0.534609615803) A[1]:(0.590281069279) A[2]:(0.590486168861) A[3]:(0.531478047371)\n",
      " state (1)  A[0]:(0.535169124603) A[1]:(-0.00102704728488) A[2]:(0.656214594841) A[3]:(0.592317521572)\n",
      " state (2)  A[0]:(0.594119787216) A[1]:(0.728797793388) A[2]:(0.591402113438) A[3]:(0.658830165863)\n",
      " state (3)  A[0]:(0.659526705742) A[1]:(-0.21914459765) A[2]:(0.541717648506) A[3]:(0.523733496666)\n",
      " state (4)  A[0]:(0.594504058361) A[1]:(0.655788183212) A[2]:(0.00301777408458) A[3]:(0.540004849434)\n",
      " state (5)  A[0]:(0.170334279537) A[1]:(0.928594648838) A[2]:(-0.192210271955) A[3]:(0.537164092064)\n",
      " state (6)  A[0]:(0.00800145510584) A[1]:(0.809854567051) A[2]:(0.00352512323298) A[3]:(0.665558099747)\n",
      " state (7)  A[0]:(0.630618929863) A[1]:(-0.248214513063) A[2]:(0.312431901693) A[3]:(0.886309504509)\n",
      " state (8)  A[0]:(0.663491904736) A[1]:(0.000502258480992) A[2]:(0.729749917984) A[3]:(0.608354151249)\n",
      " state (9)  A[0]:(0.663977503777) A[1]:(0.810119628906) A[2]:(0.810499787331) A[3]:(0.0313407629728)\n",
      " state (10)  A[0]:(0.735435545444) A[1]:(0.899895370007) A[2]:(0.00393472053111) A[3]:(0.742918133736)\n",
      " state (11)  A[0]:(0.534385919571) A[1]:(0.876288175583) A[2]:(-0.623019099236) A[3]:(0.853519320488)\n",
      " state (12)  A[0]:(0.095641694963) A[1]:(0.822957277298) A[2]:(-0.621322870255) A[3]:(0.805416107178)\n",
      " state (13)  A[0]:(0.013274605386) A[1]:(0.806896448135) A[2]:(0.899856865406) A[3]:(0.742541730404)\n",
      " state (14)  A[0]:(0.813706755638) A[1]:(0.898817718029) A[2]:(0.999999940395) A[3]:(0.81933760643)\n",
      " state (15)  A[0]:(0.98231524229) A[1]:(0.955122292042) A[2]:(1.0) A[3]:(0.881151437759)\n",
      "Episode 954000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 999.               Steps done: 6889312. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00091675388277.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5904,  0.5905,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.0001,  0.6562,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7291,  0.5906,  0.6558]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8100, -0.0005,  0.6556]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000,  0.0004,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.8999,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531210780144) A[1]:(0.590463757515) A[2]:(0.590509653091) A[3]:(0.531069278717)\n",
      " state (1)  A[0]:(0.531350314617) A[1]:(7.13989138603e-05) A[2]:(0.656145811081) A[3]:(0.590336859226)\n",
      " state (2)  A[0]:(0.590502619743) A[1]:(0.729038536549) A[2]:(0.590602278709) A[3]:(0.655817747116)\n",
      " state (3)  A[0]:(0.656542003155) A[1]:(-0.218119844794) A[2]:(0.539931297302) A[3]:(0.516590476036)\n",
      " state (4)  A[0]:(0.591020584106) A[1]:(0.656178593636) A[2]:(-0.00018846988678) A[3]:(0.530763506889)\n",
      " state (5)  A[0]:(0.164501324296) A[1]:(0.928680300713) A[2]:(-0.195707529783) A[3]:(0.526359140873)\n",
      " state (6)  A[0]:(0.000656783464365) A[1]:(0.810036540031) A[2]:(-0.000311732292175) A[3]:(0.65559220314)\n",
      " state (7)  A[0]:(0.624673843384) A[1]:(-0.247974961996) A[2]:(0.309460490942) A[3]:(0.881471812725)\n",
      " state (8)  A[0]:(0.656353592873) A[1]:(0.000144712626934) A[2]:(0.729042649269) A[3]:(0.590185523033)\n",
      " state (9)  A[0]:(0.656223893166) A[1]:(0.810045361519) A[2]:(0.810047447681) A[3]:(-0.000711127999239)\n",
      " state (10)  A[0]:(0.729369044304) A[1]:(0.900036811829) A[2]:(0.000145554542542) A[3]:(0.728857815266)\n",
      " state (11)  A[0]:(0.52505338192) A[1]:(0.876675009727) A[2]:(-0.626234173775) A[3]:(0.845241785049)\n",
      " state (12)  A[0]:(0.0827042087913) A[1]:(0.823819816113) A[2]:(-0.624194383621) A[3]:(0.7947447896)\n",
      " state (13)  A[0]:(0.00130546023138) A[1]:(0.808342397213) A[2]:(0.900002121925) A[3]:(0.729113221169)\n",
      " state (14)  A[0]:(0.810349285603) A[1]:(0.899969160557) A[2]:(1.0) A[3]:(0.809885799885)\n",
      " state (15)  A[0]:(0.981930196285) A[1]:(0.955738782883) A[2]:(1.0) A[3]:(0.874773621559)\n",
      "Episode 955000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5999. Times reached goal: 999.               Steps done: 6895311. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00091127073936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532977461815) A[1]:(0.590335845947) A[2]:(0.590564489365) A[3]:(0.531197726727)\n",
      " state (1)  A[0]:(0.532956242561) A[1]:(-0.000229068100452) A[2]:(0.656100273132) A[3]:(0.590242922306)\n",
      " state (2)  A[0]:(0.591778874397) A[1]:(0.728965044022) A[2]:(0.590614914894) A[3]:(0.655738055706)\n",
      " state (3)  A[0]:(0.657421946526) A[1]:(-0.218598946929) A[2]:(0.540124058723) A[3]:(0.516873002052)\n",
      " state (4)  A[0]:(0.591633439064) A[1]:(0.656011402607) A[2]:(0.000146389007568) A[3]:(0.531256079674)\n",
      " state (5)  A[0]:(0.16478818655) A[1]:(0.928619503975) A[2]:(-0.1953753829) A[3]:(0.526860117912)\n",
      " state (6)  A[0]:(0.000300765037537) A[1]:(0.809962809086) A[2]:(-0.000247955322266) A[3]:(0.655815720558)\n",
      " state (7)  A[0]:(0.624367058277) A[1]:(-0.248069867492) A[2]:(0.309177190065) A[3]:(0.881459593773)\n",
      " state (8)  A[0]:(0.656190395355) A[1]:(-8.49738717079e-05) A[2]:(0.728918731213) A[3]:(0.590081512928)\n",
      " state (9)  A[0]:(0.656064271927) A[1]:(0.809956967831) A[2]:(0.809966444969) A[3]:(-0.000667929532938)\n",
      " state (10)  A[0]:(0.729104399681) A[1]:(0.899980545044) A[2]:(-0.000107526779175) A[3]:(0.728969335556)\n",
      " state (11)  A[0]:(0.524412989616) A[1]:(0.876606822014) A[2]:(-0.626405775547) A[3]:(0.845332145691)\n",
      " state (12)  A[0]:(0.0815543755889) A[1]:(0.823738217354) A[2]:(-0.624377012253) A[3]:(0.79489171505)\n",
      " state (13)  A[0]:(-0.000127375125885) A[1]:(0.808284521103) A[2]:(0.899966776371) A[3]:(0.729347169399)\n",
      " state (14)  A[0]:(0.809707224369) A[1]:(0.899962484837) A[2]:(1.0) A[3]:(0.81008887291)\n",
      " state (15)  A[0]:(0.981846868992) A[1]:(0.955747723579) A[2]:(1.0) A[3]:(0.87493121624)\n",
      "Episode 956000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5998. Times reached goal: 998.               Steps done: 6901309. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000905821296681.\n",
      " state (0)  A[0]:(0.531467199326) A[1]:(0.590626657009) A[2]:(0.590594112873) A[3]:(0.53135985136)\n",
      " state (1)  A[0]:(0.531458497047) A[1]:(0.000438816816313) A[2]:(0.656194925308) A[3]:(0.590482115746)\n",
      " state (2)  A[0]:(0.590690851212) A[1]:(0.729076385498) A[2]:(0.590760946274) A[3]:(0.655846297741)\n",
      " state (3)  A[0]:(0.6565721035) A[1]:(-0.21890771389) A[2]:(0.540367126465) A[3]:(0.516530394554)\n",
      " state (4)  A[0]:(0.591261386871) A[1]:(0.65644711256) A[2]:(0.000180721282959) A[3]:(0.530667424202)\n",
      " state (5)  A[0]:(0.165431067348) A[1]:(0.928762316704) A[2]:(-0.195466652513) A[3]:(0.526006519794)\n",
      " state (6)  A[0]:(0.00269179884344) A[1]:(0.810074210167) A[2]:(-0.000135183334351) A[3]:(0.654907166958)\n",
      " state (7)  A[0]:(0.626362264156) A[1]:(-0.247831478715) A[2]:(0.309375226498) A[3]:(0.880880057812)\n",
      " state (8)  A[0]:(0.657847225666) A[1]:(0.000901467865333) A[2]:(0.729119718075) A[3]:(0.588348388672)\n",
      " state (9)  A[0]:(0.657770395279) A[1]:(0.810176074505) A[2]:(0.810073316097) A[3]:(-0.00165931729134)\n",
      " state (10)  A[0]:(0.730444610119) A[1]:(0.899998247623) A[2]:(0.000464081735117) A[3]:(0.728466272354)\n",
      " state (11)  A[0]:(0.526431918144) A[1]:(0.876540958881) A[2]:(-0.625904023647) A[3]:(0.844844579697)\n",
      " state (12)  A[0]:(0.0844264701009) A[1]:(0.823545694351) A[2]:(-0.623899996281) A[3]:(0.794111251831)\n",
      " state (13)  A[0]:(0.00296246143989) A[1]:(0.807960689068) A[2]:(0.89999961853) A[3]:(0.728285551071)\n",
      " state (14)  A[0]:(0.810903012753) A[1]:(0.899716317654) A[2]:(1.0) A[3]:(0.809394776821)\n",
      " state (15)  A[0]:(0.982001423836) A[1]:(0.955613791943) A[2]:(1.0) A[3]:(0.874572873116)\n",
      "Episode 957000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 1000.               Steps done: 6907309. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000900402641124.\n",
      " state (0)  A[0]:(0.531606793404) A[1]:(0.5906727314) A[2]:(0.590702772141) A[3]:(0.531812608242)\n",
      " state (1)  A[0]:(0.530865430832) A[1]:(-0.00024875998497) A[2]:(0.656190574169) A[3]:(0.591030359268)\n",
      " state (2)  A[0]:(0.589832186699) A[1]:(0.729036450386) A[2]:(0.590816020966) A[3]:(0.656446933746)\n",
      " state (3)  A[0]:(0.655946195126) A[1]:(-0.218866854906) A[2]:(0.540243923664) A[3]:(0.517554283142)\n",
      " state (4)  A[0]:(0.590420305729) A[1]:(0.656313061714) A[2]:(0.000164031982422) A[3]:(0.531856536865)\n",
      " state (5)  A[0]:(0.16377876699) A[1]:(0.928701460361) A[2]:(-0.195193991065) A[3]:(0.527378439903)\n",
      " state (6)  A[0]:(0.000302672386169) A[1]:(0.810064256191) A[2]:(0.000286817550659) A[3]:(0.656226277351)\n",
      " state (7)  A[0]:(0.624899089336) A[1]:(-0.247884169221) A[2]:(0.30980527401) A[3]:(0.88166308403)\n",
      " state (8)  A[0]:(0.65706217289) A[1]:(0.000310003757477) A[2]:(0.728989005089) A[3]:(0.591337561607)\n",
      " state (9)  A[0]:(0.657177567482) A[1]:(0.810090065002) A[2]:(0.810081005096) A[3]:(0.00134061195422)\n",
      " state (10)  A[0]:(0.729995012283) A[1]:(0.900035381317) A[2]:(0.00092053390108) A[3]:(0.729426860809)\n",
      " state (11)  A[0]:(0.525769591331) A[1]:(0.876654326916) A[2]:(-0.625577509403) A[3]:(0.845422267914)\n",
      " state (12)  A[0]:(0.0834007412195) A[1]:(0.823777973652) A[2]:(-0.623636066914) A[3]:(0.794879555702)\n",
      " state (13)  A[0]:(0.00148641958367) A[1]:(0.80827075243) A[2]:(0.899948775768) A[3]:(0.729240179062)\n",
      " state (14)  A[0]:(0.810113191605) A[1]:(0.899914085865) A[2]:(1.0) A[3]:(0.810041069984)\n",
      " state (15)  A[0]:(0.981901824474) A[1]:(0.955725312233) A[2]:(1.0) A[3]:(0.874990165234)\n",
      "Episode 958000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 999.               Steps done: 6913317. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000895009240056.\n",
      " state (0)  A[0]:(0.531317234039) A[1]:(0.590457499027) A[2]:(0.590371131897) A[3]:(0.531658768654)\n",
      " state (1)  A[0]:(0.531229436398) A[1]:(-0.000196471810341) A[2]:(0.655964791775) A[3]:(0.59071290493)\n",
      " state (2)  A[0]:(0.590203404427) A[1]:(0.728830933571) A[2]:(0.590495944023) A[3]:(0.656250953674)\n",
      " state (3)  A[0]:(0.655838608742) A[1]:(-0.219173878431) A[2]:(0.539963662624) A[3]:(0.517673075199)\n",
      " state (4)  A[0]:(0.589976787567) A[1]:(0.655647516251) A[2]:(-0.000111222267151) A[3]:(0.532161474228)\n",
      " state (5)  A[0]:(0.162908032537) A[1]:(0.928567528725) A[2]:(-0.195603772998) A[3]:(0.527836382389)\n",
      " state (6)  A[0]:(-0.000872790580615) A[1]:(0.809825539589) A[2]:(-0.000250816345215) A[3]:(0.656616210938)\n",
      " state (7)  A[0]:(0.624105572701) A[1]:(-0.248451724648) A[2]:(0.309340506792) A[3]:(0.881812632084)\n",
      " state (8)  A[0]:(0.656442344189) A[1]:(-0.000289246439934) A[2]:(0.728770017624) A[3]:(0.591635107994)\n",
      " state (9)  A[0]:(0.656692683697) A[1]:(0.809866130352) A[2]:(0.809803903103) A[3]:(0.0018517205026)\n",
      " state (10)  A[0]:(0.729587078094) A[1]:(0.89988732338) A[2]:(2.41994857788e-05) A[3]:(0.729609370232)\n",
      " state (11)  A[0]:(0.525112211704) A[1]:(0.876450181007) A[2]:(-0.626095533371) A[3]:(0.845500469208)\n",
      " state (12)  A[0]:(0.0825514793396) A[1]:(0.823466539383) A[2]:(-0.624098300934) A[3]:(0.794994115829)\n",
      " state (13)  A[0]:(0.000854491954669) A[1]:(0.807912826538) A[2]:(0.899874806404) A[3]:(0.72952067852)\n",
      " state (14)  A[0]:(0.810054004192) A[1]:(0.899709582329) A[2]:(1.0) A[3]:(0.810509681702)\n",
      " state (15)  A[0]:(0.981905698776) A[1]:(0.955623328686) A[2]:(1.0) A[3]:(0.87548148632)\n",
      "Episode 959000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 998.               Steps done: 6919318. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000889654372955.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5904,  0.5904,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5310, -0.0002,  0.6561,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5900,  0.7290,  0.5908,  0.6559]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  0.8100, -0.0002,  0.6556]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0001,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.8999,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530923247337) A[1]:(0.59036231041) A[2]:(0.590448856354) A[3]:(0.531287074089)\n",
      " state (1)  A[0]:(0.530833482742) A[1]:(-0.000158898532391) A[2]:(0.656090497971) A[3]:(0.590585827827)\n",
      " state (2)  A[0]:(0.589871168137) A[1]:(0.728984475136) A[2]:(0.590749382973) A[3]:(0.656085252762)\n",
      " state (3)  A[0]:(0.65566599369) A[1]:(-0.218563959002) A[2]:(0.540251970291) A[3]:(0.516913473606)\n",
      " state (4)  A[0]:(0.59000647068) A[1]:(0.656048715115) A[2]:(0.000173807144165) A[3]:(0.531121253967)\n",
      " state (5)  A[0]:(0.163250178099) A[1]:(0.928686499596) A[2]:(-0.195529833436) A[3]:(0.526720166206)\n",
      " state (6)  A[0]:(-0.000510752142873) A[1]:(0.809990346432) A[2]:(-0.000161647796631) A[3]:(0.655775487423)\n",
      " state (7)  A[0]:(0.623609662056) A[1]:(-0.248192608356) A[2]:(0.309529066086) A[3]:(0.881433665752)\n",
      " state (8)  A[0]:(0.655387699604) A[1]:(5.7227909565e-05) A[2]:(0.728990733624) A[3]:(0.590171217918)\n",
      " state (9)  A[0]:(0.655599415302) A[1]:(0.81000238657) A[2]:(0.809991598129) A[3]:(-0.000291183590889)\n",
      " state (10)  A[0]:(0.728852272034) A[1]:(0.900000214577) A[2]:(-4.33921813965e-05) A[3]:(0.729057788849)\n",
      " state (11)  A[0]:(0.52411711216) A[1]:(0.87661921978) A[2]:(-0.626372933388) A[3]:(0.845330715179)\n",
      " state (12)  A[0]:(0.0812472552061) A[1]:(0.823736488819) A[2]:(-0.624335765839) A[3]:(0.794835150242)\n",
      " state (13)  A[0]:(-0.000269174575806) A[1]:(0.808263242245) A[2]:(0.899996757507) A[3]:(0.729326725006)\n",
      " state (14)  A[0]:(0.809789121151) A[1]:(0.899944484234) A[2]:(1.0) A[3]:(0.810341715813)\n",
      " state (15)  A[0]:(0.981865644455) A[1]:(0.955733478069) A[2]:(1.0) A[3]:(0.875287950039)\n",
      "Episode 960000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5999. Times reached goal: 999.               Steps done: 6925317. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000884333312849.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532548308372) A[1]:(0.5902094841) A[2]:(0.5903236866) A[3]:(0.531567215919)\n",
      " state (1)  A[0]:(0.532108783722) A[1]:(-5.80847263336e-05) A[2]:(0.655970096588) A[3]:(0.590398967266)\n",
      " state (2)  A[0]:(0.590734601021) A[1]:(0.728972792625) A[2]:(0.590569198132) A[3]:(0.655870437622)\n",
      " state (3)  A[0]:(0.655383110046) A[1]:(-0.21842032671) A[2]:(0.54014647007) A[3]:(0.517076551914)\n",
      " state (4)  A[0]:(0.589042067528) A[1]:(0.655949473381) A[2]:(0.000192761421204) A[3]:(0.531452834606)\n",
      " state (5)  A[0]:(0.161567300558) A[1]:(0.928672432899) A[2]:(-0.195488318801) A[3]:(0.527049064636)\n",
      " state (6)  A[0]:(-0.00209873612039) A[1]:(0.809961080551) A[2]:(-9.16719436646e-05) A[3]:(0.656016469002)\n",
      " state (7)  A[0]:(0.622771084309) A[1]:(-0.248195156455) A[2]:(0.309585750103) A[3]:(0.881521701813)\n",
      " state (8)  A[0]:(0.654206037521) A[1]:(6.63101673126e-05) A[2]:(0.728959083557) A[3]:(0.590328335762)\n",
      " state (9)  A[0]:(0.653820991516) A[1]:(0.809999585152) A[2]:(0.809962809086) A[3]:(-0.000419706077082)\n",
      " state (10)  A[0]:(0.727260231972) A[1]:(0.899996697903) A[2]:(-0.000298976898193) A[3]:(0.728892564774)\n",
      " state (11)  A[0]:(0.521681070328) A[1]:(0.876605331898) A[2]:(-0.626611232758) A[3]:(0.845166027546)\n",
      " state (12)  A[0]:(0.0780736133456) A[1]:(0.82370364666) A[2]:(-0.624565780163) A[3]:(0.794530332088)\n",
      " state (13)  A[0]:(-0.00304149650037) A[1]:(0.808225989342) A[2]:(0.900003969669) A[3]:(0.728830814362)\n",
      " state (14)  A[0]:(0.809120833874) A[1]:(0.899930357933) A[2]:(1.0) A[3]:(0.809884548187)\n",
      " state (15)  A[0]:(0.981818199158) A[1]:(0.955724537373) A[2]:(1.0) A[3]:(0.87490862608)\n",
      "Episode 961000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 999.               Steps done: 6931318. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000879042320141.\n",
      " state (0)  A[0]:(0.53133970499) A[1]:(0.590633153915) A[2]:(0.590451717377) A[3]:(0.531012415886)\n",
      " state (1)  A[0]:(0.531423211098) A[1]:(-0.00020606815815) A[2]:(0.656041383743) A[3]:(0.590292811394)\n",
      " state (2)  A[0]:(0.590476036072) A[1]:(0.728945970535) A[2]:(0.590686500072) A[3]:(0.655898571014)\n",
      " state (3)  A[0]:(0.655932307243) A[1]:(-0.219058215618) A[2]:(0.540168523788) A[3]:(0.516795873642)\n",
      " state (4)  A[0]:(0.590146243572) A[1]:(0.656048417091) A[2]:(-3.3974647522e-05) A[3]:(0.531231701374)\n",
      " state (5)  A[0]:(0.163288518786) A[1]:(0.928671002388) A[2]:(-0.195629924536) A[3]:(0.526901721954)\n",
      " state (6)  A[0]:(-0.000781178299803) A[1]:(0.80997389555) A[2]:(-0.000165462493896) A[3]:(0.655924320221)\n",
      " state (7)  A[0]:(0.623368263245) A[1]:(-0.24822846055) A[2]:(0.309631556273) A[3]:(0.881536543369)\n",
      " state (8)  A[0]:(0.655376434326) A[1]:(-4.35262918472e-05) A[2]:(0.728954017162) A[3]:(0.590692520142)\n",
      " state (9)  A[0]:(0.655741691589) A[1]:(0.809995472431) A[2]:(0.809975683689) A[3]:(0.000172838568687)\n",
      " state (10)  A[0]:(0.728975713253) A[1]:(0.899994254112) A[2]:(9.52482223511e-05) A[3]:(0.729008734226)\n",
      " state (11)  A[0]:(0.524303615093) A[1]:(0.876607596874) A[2]:(-0.62624001503) A[3]:(0.845211148262)\n",
      " state (12)  A[0]:(0.0815258994699) A[1]:(0.82372033596) A[2]:(-0.624222636223) A[3]:(0.794628858566)\n",
      " state (13)  A[0]:(4.76241111755e-05) A[1]:(0.808248698711) A[2]:(0.899997055531) A[3]:(0.729069888592)\n",
      " state (14)  A[0]:(0.809931755066) A[1]:(0.899940609932) A[2]:(1.0) A[3]:(0.810254752636)\n",
      " state (15)  A[0]:(0.981887578964) A[1]:(0.955736875534) A[2]:(1.0) A[3]:(0.875311136246)\n",
      "Episode 962000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 1000.               Steps done: 6937320. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000873782109818.\n",
      " state (0)  A[0]:(0.530847609043) A[1]:(0.590859293938) A[2]:(0.590561747551) A[3]:(0.53180873394)\n",
      " state (1)  A[0]:(0.532327771187) A[1]:(0.00048133727978) A[2]:(0.656254410744) A[3]:(0.58873707056)\n",
      " state (2)  A[0]:(0.591534614563) A[1]:(0.729067385197) A[2]:(0.590835690498) A[3]:(0.653913915157)\n",
      " state (3)  A[0]:(0.656513214111) A[1]:(-0.219581350684) A[2]:(0.540301382542) A[3]:(0.515095949173)\n",
      " state (4)  A[0]:(0.590330243111) A[1]:(0.656296014786) A[2]:(-0.000162959098816) A[3]:(0.530018687248)\n",
      " state (5)  A[0]:(0.162780091166) A[1]:(0.92870759964) A[2]:(-0.19581861794) A[3]:(0.525707006454)\n",
      " state (6)  A[0]:(-0.00205409247428) A[1]:(0.810013651848) A[2]:(-0.000450849503977) A[3]:(0.654950976372)\n",
      " state (7)  A[0]:(0.622060060501) A[1]:(-0.248062089086) A[2]:(0.309471279383) A[3]:(0.881036818027)\n",
      " state (8)  A[0]:(0.653565764427) A[1]:(0.000315934419632) A[2]:(0.729232311249) A[3]:(0.588095664978)\n",
      " state (9)  A[0]:(0.65372723341) A[1]:(0.810053944588) A[2]:(0.810015678406) A[3]:(-0.00386160193011)\n",
      " state (10)  A[0]:(0.727237939835) A[1]:(0.899987459183) A[2]:(-0.000583171786275) A[3]:(0.727380692959)\n",
      " state (11)  A[0]:(0.521408081055) A[1]:(0.876580357552) A[2]:(-0.626855015755) A[3]:(0.844257771969)\n",
      " state (12)  A[0]:(0.0773268863559) A[1]:(0.823679685593) A[2]:(-0.624859035015) A[3]:(0.793416023254)\n",
      " state (13)  A[0]:(-0.00427297130227) A[1]:(0.808231830597) A[2]:(0.899899363518) A[3]:(0.727578401566)\n",
      " state (14)  A[0]:(0.808417499065) A[1]:(0.899959206581) A[2]:(1.0) A[3]:(0.809224426746)\n",
      " state (15)  A[0]:(0.981717586517) A[1]:(0.955753207207) A[2]:(1.0) A[3]:(0.874636173248)\n",
      "Episode 963000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 998.               Steps done: 6943327. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000868549033963.\n",
      " state (0)  A[0]:(0.530964374542) A[1]:(0.590362429619) A[2]:(0.59041851759) A[3]:(0.531220555305)\n",
      " state (1)  A[0]:(0.530905365944) A[1]:(0.000120609998703) A[2]:(0.656087875366) A[3]:(0.590163826942)\n",
      " state (2)  A[0]:(0.590119719505) A[1]:(0.728983461857) A[2]:(0.590537548065) A[3]:(0.655775845051)\n",
      " state (3)  A[0]:(0.656427264214) A[1]:(-0.218889355659) A[2]:(0.54002571106) A[3]:(0.516643702984)\n",
      " state (4)  A[0]:(0.590927600861) A[1]:(0.656054019928) A[2]:(4.97102737427e-05) A[3]:(0.53108894825)\n",
      " state (5)  A[0]:(0.164164364338) A[1]:(0.928631007671) A[2]:(-0.195288702846) A[3]:(0.526729285717)\n",
      " state (6)  A[0]:(-0.00012993812561) A[1]:(0.809992730618) A[2]:(8.60691070557e-05) A[3]:(0.655670762062)\n",
      " state (7)  A[0]:(0.624025523663) A[1]:(-0.24795037508) A[2]:(0.309670895338) A[3]:(0.881410121918)\n",
      " state (8)  A[0]:(0.656025648117) A[1]:(4.76539134979e-05) A[2]:(0.728977322578) A[3]:(0.590398788452)\n",
      " state (9)  A[0]:(0.656005978584) A[1]:(0.81001663208) A[2]:(0.810011744499) A[3]:(-0.000267148017883)\n",
      " state (10)  A[0]:(0.729053199291) A[1]:(0.900001823902) A[2]:(0.000102877616882) A[3]:(0.728869199753)\n",
      " state (11)  A[0]:(0.524401426315) A[1]:(0.876609861851) A[2]:(-0.626311659813) A[3]:(0.845176756382)\n",
      " state (12)  A[0]:(0.0817076638341) A[1]:(0.823714613914) A[2]:(-0.624350190163) A[3]:(0.794636011124)\n",
      " state (13)  A[0]:(0.00021505355835) A[1]:(0.808237075806) A[2]:(0.899939537048) A[3]:(0.729019105434)\n",
      " state (14)  A[0]:(0.809890031815) A[1]:(0.899937331676) A[2]:(1.0) A[3]:(0.809919595718)\n",
      " state (15)  A[0]:(0.981875777245) A[1]:(0.955739498138) A[2]:(1.0) A[3]:(0.87488424778)\n",
      "Episode 964000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6000. Times reached goal: 999.               Steps done: 6949327. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000863353342421.\n",
      "q_values \n",
      "tensor([[ 0.5281,  0.5903,  0.5903,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314, -0.0002,  0.6561,  0.5894]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5928,  0.7288,  0.5906,  0.6547]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0188,  0.8101, -0.0008,  0.6533]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7336,  0.9000, -0.0003,  0.7269]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8120,  0.9000,  1.0000,  0.8091]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530721545219) A[1]:(0.59028917551) A[2]:(0.590452671051) A[3]:(0.532041251659)\n",
      " state (1)  A[0]:(0.533230185509) A[1]:(-0.000147096812725) A[2]:(0.656129002571) A[3]:(0.590143203735)\n",
      " state (2)  A[0]:(0.593948364258) A[1]:(0.728998780251) A[2]:(0.590724229813) A[3]:(0.655356884003)\n",
      " state (3)  A[0]:(0.661698281765) A[1]:(-0.21798273921) A[2]:(0.540142416954) A[3]:(0.5144572258)\n",
      " state (4)  A[0]:(0.599414765835) A[1]:(0.6561216712) A[2]:(-0.000255465507507) A[3]:(0.528186142445)\n",
      " state (5)  A[0]:(0.180055096745) A[1]:(0.92880320549) A[2]:(-0.196439996362) A[3]:(0.523924529552)\n",
      " state (6)  A[0]:(0.0187706947327) A[1]:(0.810087800026) A[2]:(-0.000793337647337) A[3]:(0.653737187386)\n",
      " state (7)  A[0]:(0.635006189346) A[1]:(-0.248238861561) A[2]:(0.309653759003) A[3]:(0.880534410477)\n",
      " state (8)  A[0]:(0.663848400116) A[1]:(0.000244684517384) A[2]:(0.729234814644) A[3]:(0.586794435978)\n",
      " state (9)  A[0]:(0.661832213402) A[1]:(0.810058116913) A[2]:(0.810163617134) A[3]:(-0.0058593377471)\n",
      " state (10)  A[0]:(0.73307955265) A[1]:(0.900062918663) A[2]:(-0.00021755695343) A[3]:(0.726831555367)\n",
      " state (11)  A[0]:(0.529870033264) A[1]:(0.876744031906) A[2]:(-0.626757025719) A[3]:(0.84416437149)\n",
      " state (12)  A[0]:(0.0883073657751) A[1]:(0.823999881744) A[2]:(-0.624897360802) A[3]:(0.793454647064)\n",
      " state (13)  A[0]:(0.0062017836608) A[1]:(0.808680176735) A[2]:(0.89971613884) A[3]:(0.727647185326)\n",
      " state (14)  A[0]:(0.811921596527) A[1]:(0.900274038315) A[2]:(1.0) A[3]:(0.809097647667)\n",
      " state (15)  A[0]:(0.982095241547) A[1]:(0.955943346024) A[2]:(1.0) A[3]:(0.874457061291)\n",
      "Episode 965000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 1000.               Steps done: 6955328. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000858187873504.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531304359436) A[1]:(0.5904494524) A[2]:(0.590490221977) A[3]:(0.531586289406)\n",
      " state (1)  A[0]:(0.530810236931) A[1]:(-3.35574150085e-05) A[2]:(0.656050920486) A[3]:(0.590798139572)\n",
      " state (2)  A[0]:(0.58985555172) A[1]:(0.728936433792) A[2]:(0.590535283089) A[3]:(0.656338691711)\n",
      " state (3)  A[0]:(0.655751943588) A[1]:(-0.219428107142) A[2]:(0.540051102638) A[3]:(0.516860842705)\n",
      " state (4)  A[0]:(0.590129494667) A[1]:(0.656215190887) A[2]:(-6.63995742798e-05) A[3]:(0.53117620945)\n",
      " state (5)  A[0]:(0.163419693708) A[1]:(0.928660988808) A[2]:(-0.195303723216) A[3]:(0.526830077171)\n",
      " state (6)  A[0]:(-0.000511109770741) A[1]:(0.809953510761) A[2]:(0.000290036201477) A[3]:(0.655937552452)\n",
      " state (7)  A[0]:(0.623514294624) A[1]:(-0.24813888967) A[2]:(0.309980928898) A[3]:(0.881610572338)\n",
      " state (8)  A[0]:(0.655362188816) A[1]:(5.11705875397e-05) A[2]:(0.72902238369) A[3]:(0.590889453888)\n",
      " state (9)  A[0]:(0.655264735222) A[1]:(0.810011863708) A[2]:(0.810011029243) A[3]:(-0.000217735767365)\n",
      " state (10)  A[0]:(0.72842335701) A[1]:(0.899976789951) A[2]:(0.0003262758255) A[3]:(0.728632628918)\n",
      " state (11)  A[0]:(0.5234811306) A[1]:(0.876539766788) A[2]:(-0.626078724861) A[3]:(0.844998657703)\n",
      " state (12)  A[0]:(0.0805809721351) A[1]:(0.823540508747) A[2]:(-0.624128401279) A[3]:(0.794420957565)\n",
      " state (13)  A[0]:(-0.000490486563649) A[1]:(0.807933926582) A[2]:(0.89993262291) A[3]:(0.728738069534)\n",
      " state (14)  A[0]:(0.810143351555) A[1]:(0.899694144726) A[2]:(1.0) A[3]:(0.809655189514)\n",
      " state (15)  A[0]:(0.981961131096) A[1]:(0.955594480038) A[2]:(1.0) A[3]:(0.874672114849)\n",
      "Episode 966000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 999.               Steps done: 6961331. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000853051603638.\n",
      " state (0)  A[0]:(0.531813979149) A[1]:(0.590338051319) A[2]:(0.590567111969) A[3]:(0.528980016708)\n",
      " state (1)  A[0]:(0.531390190125) A[1]:(-0.000350430607796) A[2]:(0.656151592731) A[3]:(0.590354084969)\n",
      " state (2)  A[0]:(0.590277552605) A[1]:(0.728984236717) A[2]:(0.590648889542) A[3]:(0.657019078732)\n",
      " state (3)  A[0]:(0.655833125114) A[1]:(-0.218463420868) A[2]:(0.539906859398) A[3]:(0.520451247692)\n",
      " state (4)  A[0]:(0.589969933033) A[1]:(0.655995607376) A[2]:(-0.000175714492798) A[3]:(0.536045193672)\n",
      " state (5)  A[0]:(0.163026988506) A[1]:(0.928599655628) A[2]:(-0.195566400886) A[3]:(0.532174885273)\n",
      " state (6)  A[0]:(-0.00125163723715) A[1]:(0.809988975525) A[2]:(-0.000339150428772) A[3]:(0.659859657288)\n",
      " state (7)  A[0]:(0.623240470886) A[1]:(-0.248171523213) A[2]:(0.309375882149) A[3]:(0.882796287537)\n",
      " state (8)  A[0]:(0.65535402298) A[1]:(-0.000795826141257) A[2]:(0.728935003281) A[3]:(0.59332793951)\n",
      " state (9)  A[0]:(0.655467510223) A[1]:(0.809782147408) A[2]:(0.809950828552) A[3]:(0.0024513702374)\n",
      " state (10)  A[0]:(0.729057610035) A[1]:(0.899991214275) A[2]:(-0.000926732725929) A[3]:(0.730374097824)\n",
      " state (11)  A[0]:(0.524904847145) A[1]:(0.876712322235) A[2]:(-0.627242565155) A[3]:(0.846294879913)\n",
      " state (12)  A[0]:(0.0826871618629) A[1]:(0.823978483677) A[2]:(-0.625206410885) A[3]:(0.796162724495)\n",
      " state (13)  A[0]:(0.0015132415574) A[1]:(0.808656990528) A[2]:(0.89993584156) A[3]:(0.730937957764)\n",
      " state (14)  A[0]:(0.81052005291) A[1]:(0.900249660015) A[2]:(1.0) A[3]:(0.811232686043)\n",
      " state (15)  A[0]:(0.981922745705) A[1]:(0.955892205238) A[2]:(1.0) A[3]:(0.875613927841)\n",
      "Episode 967000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 1000.               Steps done: 6967339. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000847941834719.\n",
      " state (0)  A[0]:(0.530970215797) A[1]:(0.590396881104) A[2]:(0.590489506721) A[3]:(0.531308293343)\n",
      " state (1)  A[0]:(0.530968666077) A[1]:(1.23307108879e-05) A[2]:(0.656067192554) A[3]:(0.590442836285)\n",
      " state (2)  A[0]:(0.590060710907) A[1]:(0.728970110416) A[2]:(0.590579032898) A[3]:(0.656110644341)\n",
      " state (3)  A[0]:(0.655811309814) A[1]:(-0.219059422612) A[2]:(0.540101528168) A[3]:(0.517388820648)\n",
      " state (4)  A[0]:(0.590194702148) A[1]:(0.656009316444) A[2]:(4.60147857666e-05) A[3]:(0.53204959631)\n",
      " state (5)  A[0]:(0.163746982813) A[1]:(0.928628504276) A[2]:(-0.195394992828) A[3]:(0.527788162231)\n",
      " state (6)  A[0]:(0.000370800466044) A[1]:(0.809973120689) A[2]:(-7.11679458618e-05) A[3]:(0.656376481056)\n",
      " state (7)  A[0]:(0.624686360359) A[1]:(-0.248015269637) A[2]:(0.309525609016) A[3]:(0.881512105465)\n",
      " state (8)  A[0]:(0.656419873238) A[1]:(-5.75557351112e-05) A[2]:(0.728928387165) A[3]:(0.590598583221)\n",
      " state (9)  A[0]:(0.655925631523) A[1]:(0.809956550598) A[2]:(0.809968352318) A[3]:(0.000345706939697)\n",
      " state (10)  A[0]:(0.728876709938) A[1]:(0.89999973774) A[2]:(-0.000118613243103) A[3]:(0.729279756546)\n",
      " state (11)  A[0]:(0.524154901505) A[1]:(0.876646518707) A[2]:(-0.626452207565) A[3]:(0.845482468605)\n",
      " state (12)  A[0]:(0.0813751518726) A[1]:(0.823795497417) A[2]:(-0.624428272247) A[3]:(0.795049905777)\n",
      " state (13)  A[0]:(4.92334365845e-05) A[1]:(0.808338940144) A[2]:(0.900006830692) A[3]:(0.729511320591)\n",
      " state (14)  A[0]:(0.810104727745) A[1]:(0.899997889996) A[2]:(1.0) A[3]:(0.810224950314)\n",
      " state (15)  A[0]:(0.981910288334) A[1]:(0.955754220486) A[2]:(1.0) A[3]:(0.87500333786)\n",
      "Episode 968000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 999.               Steps done: 6973340. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000842868573315.\n",
      " state (0)  A[0]:(0.53155708313) A[1]:(0.590339541435) A[2]:(0.590414881706) A[3]:(0.531332731247)\n",
      " state (1)  A[0]:(0.531400680542) A[1]:(-0.000135183334351) A[2]:(0.655979096889) A[3]:(0.590715348721)\n",
      " state (2)  A[0]:(0.590244650841) A[1]:(0.728899002075) A[2]:(0.590616226196) A[3]:(0.656582117081)\n",
      " state (3)  A[0]:(0.655708968639) A[1]:(-0.218868210912) A[2]:(0.540327787399) A[3]:(0.518482148647)\n",
      " state (4)  A[0]:(0.589789092541) A[1]:(0.655834436417) A[2]:(0.000553727091756) A[3]:(0.533363103867)\n",
      " state (5)  A[0]:(0.16283608973) A[1]:(0.928624272346) A[2]:(-0.194990679622) A[3]:(0.529256820679)\n",
      " state (6)  A[0]:(-0.000729501130991) A[1]:(0.809938073158) A[2]:(0.000369548768504) A[3]:(0.657634913921)\n",
      " state (7)  A[0]:(0.623884558678) A[1]:(-0.248096227646) A[2]:(0.309820264578) A[3]:(0.88206499815)\n",
      " state (8)  A[0]:(0.655959248543) A[1]:(5.20348548889e-05) A[2]:(0.728862166405) A[3]:(0.592786252499)\n",
      " state (9)  A[0]:(0.655930221081) A[1]:(0.809990763664) A[2]:(0.809932351112) A[3]:(0.00417399965227)\n",
      " state (10)  A[0]:(0.728767514229) A[1]:(0.899986624718) A[2]:(7.20024108887e-05) A[3]:(0.730886697769)\n",
      " state (11)  A[0]:(0.523606657982) A[1]:(0.876591444016) A[2]:(-0.626280784607) A[3]:(0.846362352371)\n",
      " state (12)  A[0]:(0.0802235901356) A[1]:(0.823678433895) A[2]:(-0.6242826581) A[3]:(0.79612916708)\n",
      " state (13)  A[0]:(-0.00129246641882) A[1]:(0.808181226254) A[2]:(0.900016784668) A[3]:(0.730890095234)\n",
      " state (14)  A[0]:(0.809684574604) A[1]:(0.899902820587) A[2]:(1.0) A[3]:(0.811285257339)\n",
      " state (15)  A[0]:(0.981880784035) A[1]:(0.955712378025) A[2]:(1.0) A[3]:(0.87579536438)\n",
      "Episode 969000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 1000.               Steps done: 6979343. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000837823989736.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5904,  0.5905,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5316, -0.0002,  0.6561,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.7290,  0.5907,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0017,  0.8100,  0.0001,  0.6576]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7279,  0.9000, -0.0005,  0.7264]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8096,  0.8998,  1.0000,  0.8088]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531373023987) A[1]:(0.590416312218) A[2]:(0.590539693832) A[3]:(0.531648933887)\n",
      " state (1)  A[0]:(0.531764149666) A[1]:(-0.00014241039753) A[2]:(0.656115174294) A[3]:(0.590409874916)\n",
      " state (2)  A[0]:(0.590954422951) A[1]:(0.729019999504) A[2]:(0.590705752373) A[3]:(0.656146764755)\n",
      " state (3)  A[0]:(0.657098531723) A[1]:(-0.218201577663) A[2]:(0.54019677639) A[3]:(0.518701434135)\n",
      " state (4)  A[0]:(0.591869473457) A[1]:(0.656116425991) A[2]:(0.00027596950531) A[3]:(0.533825755119)\n",
      " state (5)  A[0]:(0.166150674224) A[1]:(0.92868322134) A[2]:(-0.195288360119) A[3]:(0.529679000378)\n",
      " state (6)  A[0]:(0.00255542434752) A[1]:(0.810032308102) A[2]:(9.1552734375e-05) A[3]:(0.657751441002)\n",
      " state (7)  A[0]:(0.625384688377) A[1]:(-0.247969418764) A[2]:(0.309702783823) A[3]:(0.881786763668)\n",
      " state (8)  A[0]:(0.656342744827) A[1]:(3.11434268951e-06) A[2]:(0.72895950079) A[3]:(0.589645862579)\n",
      " state (9)  A[0]:(0.655425190926) A[1]:(0.809986650944) A[2]:(0.809994637966) A[3]:(-0.00445148395374)\n",
      " state (10)  A[0]:(0.728355050087) A[1]:(0.900011777878) A[2]:(-0.00046861168812) A[3]:(0.726644158363)\n",
      " state (11)  A[0]:(0.523184120655) A[1]:(0.876622974873) A[2]:(-0.626864254475) A[3]:(0.843888044357)\n",
      " state (12)  A[0]:(0.0798735246062) A[1]:(0.823697030544) A[2]:(-0.624835014343) A[3]:(0.793050050735)\n",
      " state (13)  A[0]:(-0.00137668766547) A[1]:(0.808177173138) A[2]:(0.900010287762) A[3]:(0.727183580399)\n",
      " state (14)  A[0]:(0.809714853764) A[1]:(0.899888277054) A[2]:(1.0) A[3]:(0.80887067318)\n",
      " state (15)  A[0]:(0.981870174408) A[1]:(0.955685853958) A[2]:(1.0) A[3]:(0.874318182468)\n",
      "Episode 970000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 999.               Steps done: 6985348. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000832807932463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532156288624) A[1]:(0.590835213661) A[2]:(0.590614080429) A[3]:(0.531884908676)\n",
      " state (1)  A[0]:(0.532041788101) A[1]:(9.87127423286e-05) A[2]:(0.656250834465) A[3]:(0.590854406357)\n",
      " state (2)  A[0]:(0.59099817276) A[1]:(0.728962779045) A[2]:(0.590906977654) A[3]:(0.65637409687)\n",
      " state (3)  A[0]:(0.656483888626) A[1]:(-0.219842657447) A[2]:(0.540453910828) A[3]:(0.517647743225)\n",
      " state (4)  A[0]:(0.590911865234) A[1]:(0.656330943108) A[2]:(0.000195264816284) A[3]:(0.532303571701)\n",
      " state (5)  A[0]:(0.164675176144) A[1]:(0.92869079113) A[2]:(-0.195215553045) A[3]:(0.527878761292)\n",
      " state (6)  A[0]:(0.00090217567049) A[1]:(0.809969544411) A[2]:(0.000244975090027) A[3]:(0.656380951405)\n",
      " state (7)  A[0]:(0.624446153641) A[1]:(-0.248260736465) A[2]:(0.309814333916) A[3]:(0.881525337696)\n",
      " state (8)  A[0]:(0.656477570534) A[1]:(4.98369336128e-05) A[2]:(0.728975176811) A[3]:(0.591047286987)\n",
      " state (9)  A[0]:(0.656720876694) A[1]:(0.810021162033) A[2]:(0.80997300148) A[3]:(0.00181938509922)\n",
      " state (10)  A[0]:(0.729444742203) A[1]:(0.899997234344) A[2]:(0.000316381454468) A[3]:(0.729658842087)\n",
      " state (11)  A[0]:(0.52458691597) A[1]:(0.876616537571) A[2]:(-0.626031517982) A[3]:(0.845474243164)\n",
      " state (12)  A[0]:(0.0814394503832) A[1]:(0.823746562004) A[2]:(-0.62405371666) A[3]:(0.79483371973)\n",
      " state (13)  A[0]:(-0.000315546989441) A[1]:(0.808292269707) A[2]:(0.899996757507) A[3]:(0.729111075401)\n",
      " state (14)  A[0]:(0.809911966324) A[1]:(0.899986982346) A[2]:(1.0) A[3]:(0.80999737978)\n",
      " state (15)  A[0]:(0.981902122498) A[1]:(0.955769181252) A[2]:(1.0) A[3]:(0.874952673912)\n",
      "Episode 971000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5992. Times reached goal: 997.               Steps done: 6991340. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000827832668109.\n",
      " state (0)  A[0]:(0.531886100769) A[1]:(0.590515255928) A[2]:(0.590618729591) A[3]:(0.530255436897)\n",
      " state (1)  A[0]:(0.531999707222) A[1]:(9.47862863541e-05) A[2]:(0.656108081341) A[3]:(0.589626610279)\n",
      " state (2)  A[0]:(0.591002821922) A[1]:(0.728969573975) A[2]:(0.590508937836) A[3]:(0.655590891838)\n",
      " state (3)  A[0]:(0.656728625298) A[1]:(-0.219005122781) A[2]:(0.539975643158) A[3]:(0.51730966568)\n",
      " state (4)  A[0]:(0.591281354427) A[1]:(0.6560125947) A[2]:(-0.000110030174255) A[3]:(0.532296657562)\n",
      " state (5)  A[0]:(0.165233030915) A[1]:(0.928633451462) A[2]:(-0.195477887988) A[3]:(0.528249621391)\n",
      " state (6)  A[0]:(0.00136005796958) A[1]:(0.809959828854) A[2]:(2.74181365967e-06) A[3]:(0.656991302967)\n",
      " state (7)  A[0]:(0.62486243248) A[1]:(-0.248147636652) A[2]:(0.309683948755) A[3]:(0.881932199001)\n",
      " state (8)  A[0]:(0.657036542892) A[1]:(-5.3271651268e-05) A[2]:(0.728877425194) A[3]:(0.592401862144)\n",
      " state (9)  A[0]:(0.657259464264) A[1]:(0.809970498085) A[2]:(0.809942007065) A[3]:(0.00338401109912)\n",
      " state (10)  A[0]:(0.729766011238) A[1]:(0.899953007698) A[2]:(0.0003809928603) A[3]:(0.730462908745)\n",
      " state (11)  A[0]:(0.524892568588) A[1]:(0.876528263092) A[2]:(-0.625948667526) A[3]:(0.846106052399)\n",
      " state (12)  A[0]:(0.081589370966) A[1]:(0.823564648628) A[2]:(-0.623972892761) A[3]:(0.795845508575)\n",
      " state (13)  A[0]:(-0.000573933066335) A[1]:(0.808020472527) A[2]:(0.899972081184) A[3]:(0.730643391609)\n",
      " state (14)  A[0]:(0.809574186802) A[1]:(0.899794101715) A[2]:(1.0) A[3]:(0.811307072639)\n",
      " state (15)  A[0]:(0.981848657131) A[1]:(0.955664873123) A[2]:(1.0) A[3]:(0.875978767872)\n",
      "Episode 972000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 999.               Steps done: 6997340. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000822880543331.\n",
      " state (0)  A[0]:(0.531045436859) A[1]:(0.590546011925) A[2]:(0.59047460556) A[3]:(0.530647873878)\n",
      " state (1)  A[0]:(0.531297206879) A[1]:(0.000148765742779) A[2]:(0.656156778336) A[3]:(0.58991932869)\n",
      " state (2)  A[0]:(0.590421319008) A[1]:(0.729048132896) A[2]:(0.590819478035) A[3]:(0.655791759491)\n",
      " state (3)  A[0]:(0.656237721443) A[1]:(-0.218538224697) A[2]:(0.540505886078) A[3]:(0.517611980438)\n",
      " state (4)  A[0]:(0.590636491776) A[1]:(0.656068921089) A[2]:(0.000748515012674) A[3]:(0.532483816147)\n",
      " state (5)  A[0]:(0.164186581969) A[1]:(0.928663372993) A[2]:(-0.194825649261) A[3]:(0.528289437294)\n",
      " state (6)  A[0]:(0.000469148129923) A[1]:(0.810028016567) A[2]:(0.000466227502329) A[3]:(0.656831502914)\n",
      " state (7)  A[0]:(0.624355077744) A[1]:(-0.247870489955) A[2]:(0.30992770195) A[3]:(0.881713628769)\n",
      " state (8)  A[0]:(0.656235694885) A[1]:(0.000161074101925) A[2]:(0.729060053825) A[3]:(0.591324925423)\n",
      " state (9)  A[0]:(0.656267166138) A[1]:(0.810016155243) A[2]:(0.810062110424) A[3]:(0.0017869155854)\n",
      " state (10)  A[0]:(0.729124903679) A[1]:(0.89999037981) A[2]:(-4.47034835815e-05) A[3]:(0.730098009109)\n",
      " state (11)  A[0]:(0.524126291275) A[1]:(0.87657392025) A[2]:(-0.626535654068) A[3]:(0.845992088318)\n",
      " state (12)  A[0]:(0.0808459669352) A[1]:(0.823624134064) A[2]:(-0.624564230442) A[3]:(0.795699179173)\n",
      " state (13)  A[0]:(-0.000485718221171) A[1]:(0.808110117912) A[2]:(0.900003314018) A[3]:(0.730382800102)\n",
      " state (14)  A[0]:(0.810210585594) A[1]:(0.899870038033) A[2]:(1.0) A[3]:(0.810944616795)\n",
      " state (15)  A[0]:(0.981955170631) A[1]:(0.95569640398) A[2]:(1.0) A[3]:(0.875572323799)\n",
      "Episode 973000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 999.               Steps done: 7003342. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000817956406427.\n",
      " state (0)  A[0]:(0.527114748955) A[1]:(0.590598285198) A[2]:(0.590527951717) A[3]:(0.530458748341)\n",
      " state (1)  A[0]:(0.526749610901) A[1]:(0.000225841999054) A[2]:(0.6560151577) A[3]:(0.588224053383)\n",
      " state (2)  A[0]:(0.585592210293) A[1]:(0.728951752186) A[2]:(0.590511679649) A[3]:(0.653300464153)\n",
      " state (3)  A[0]:(0.651089549065) A[1]:(-0.220766931772) A[2]:(0.540022611618) A[3]:(0.512259244919)\n",
      " state (4)  A[0]:(0.582818865776) A[1]:(0.656033158302) A[2]:(0.000172138214111) A[3]:(0.525952458382)\n",
      " state (5)  A[0]:(0.149816244841) A[1]:(0.928447544575) A[2]:(-0.194345653057) A[3]:(0.520462393761)\n",
      " state (6)  A[0]:(-0.0154579486698) A[1]:(0.809954762459) A[2]:(0.000546097697224) A[3]:(0.649453997612)\n",
      " state (7)  A[0]:(0.617166161537) A[1]:(-0.247531399131) A[2]:(0.309441536665) A[3]:(0.878517925739)\n",
      " state (8)  A[0]:(0.653260052204) A[1]:(-0.0003989636607) A[2]:(0.728887319565) A[3]:(0.581354856491)\n",
      " state (9)  A[0]:(0.656079351902) A[1]:(0.809903800488) A[2]:(0.80981272459) A[3]:(-0.0146266520023)\n",
      " state (10)  A[0]:(0.730044722557) A[1]:(0.899976074696) A[2]:(-0.00119888724294) A[3]:(0.722033977509)\n",
      " state (11)  A[0]:(0.52627915144) A[1]:(0.876604676247) A[2]:(-0.627435922623) A[3]:(0.840980052948)\n",
      " state (12)  A[0]:(0.0844309702516) A[1]:(0.823726177216) A[2]:(-0.62565857172) A[3]:(0.789239287376)\n",
      " state (13)  A[0]:(0.00337754399516) A[1]:(0.80828076601) A[2]:(0.899542629719) A[3]:(0.722322106361)\n",
      " state (14)  A[0]:(0.811339139938) A[1]:(0.899995684624) A[2]:(1.0) A[3]:(0.805249333382)\n",
      " state (15)  A[0]:(0.982070028782) A[1]:(0.955783843994) A[2]:(1.0) A[3]:(0.871915638447)\n",
      "Episode 974000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 1000.               Steps done: 7009345. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000813060922615.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5908,  0.5906,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.6558,  0.0001,  0.5323]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6567, -0.0005,  0.7287,  0.5905]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565,  0.8102,  0.8101, -0.0004]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  0.8085,  0.9001,  0.7290]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.8999,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531652271748) A[1]:(0.590760707855) A[2]:(0.590537905693) A[3]:(0.531473994255)\n",
      " state (1)  A[0]:(0.531571865082) A[1]:(8.41245055199e-05) A[2]:(0.656129956245) A[3]:(0.590539157391)\n",
      " state (2)  A[0]:(0.590574622154) A[1]:(0.728964745998) A[2]:(0.590847849846) A[3]:(0.656174898148)\n",
      " state (3)  A[0]:(0.656413793564) A[1]:(-0.219445675611) A[2]:(0.540131866932) A[3]:(0.517693042755)\n",
      " state (4)  A[0]:(0.590665340424) A[1]:(0.65584897995) A[2]:(-0.000119686126709) A[3]:(0.532264471054)\n",
      " state (5)  A[0]:(0.163871586323) A[1]:(0.928575873375) A[2]:(-0.195550695062) A[3]:(0.527711033821)\n",
      " state (6)  A[0]:(0.000112414360046) A[1]:(0.809873700142) A[2]:(-0.000260353088379) A[3]:(0.656131744385)\n",
      " state (7)  A[0]:(0.62465441227) A[1]:(-0.248307883739) A[2]:(0.309403479099) A[3]:(0.881437838078)\n",
      " state (8)  A[0]:(0.656666398048) A[1]:(-0.000465281278593) A[2]:(0.72874557972) A[3]:(0.590437650681)\n",
      " state (9)  A[0]:(0.656368851662) A[1]:(0.809859156609) A[2]:(0.809783577919) A[3]:(-3.74019145966e-05)\n",
      " state (10)  A[0]:(0.729090094566) A[1]:(0.899927020073) A[2]:(-0.000933408446144) A[3]:(0.729026794434)\n",
      " state (11)  A[0]:(0.524057745934) A[1]:(0.876514554024) A[2]:(-0.627131164074) A[3]:(0.845216035843)\n",
      " state (12)  A[0]:(0.080880433321) A[1]:(0.823576390743) A[2]:(-0.625187933445) A[3]:(0.794583857059)\n",
      " state (13)  A[0]:(-0.000400483579142) A[1]:(0.808116257191) A[2]:(0.89981096983) A[3]:(0.728901326656)\n",
      " state (14)  A[0]:(0.81005191803) A[1]:(0.899912178516) A[2]:(1.0) A[3]:(0.809853255749)\n",
      " state (15)  A[0]:(0.981926083565) A[1]:(0.955740630627) A[2]:(1.0) A[3]:(0.874871611595)\n",
      "Episode 975000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5995. Times reached goal: 997.               Steps done: 7015340. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000808201203946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532789945602) A[1]:(0.590682506561) A[2]:(0.590478658676) A[3]:(0.531740188599)\n",
      " state (1)  A[0]:(0.533221006393) A[1]:(-0.000174306333065) A[2]:(0.656062960625) A[3]:(0.590502262115)\n",
      " state (2)  A[0]:(0.592284679413) A[1]:(0.728952646255) A[2]:(0.591003596783) A[3]:(0.656099081039)\n",
      " state (3)  A[0]:(0.657832205296) A[1]:(-0.219640344381) A[2]:(0.540426790714) A[3]:(0.517800986767)\n",
      " state (4)  A[0]:(0.592524945736) A[1]:(0.655930995941) A[2]:(0.000232577323914) A[3]:(0.53265106678)\n",
      " state (5)  A[0]:(0.167211934924) A[1]:(0.928622722626) A[2]:(-0.195260390639) A[3]:(0.528429508209)\n",
      " state (6)  A[0]:(0.00409654481336) A[1]:(0.809909999371) A[2]:(0.000140190124512) A[3]:(0.656988322735)\n",
      " state (7)  A[0]:(0.626845359802) A[1]:(-0.248132169247) A[2]:(0.309757322073) A[3]:(0.881797134876)\n",
      " state (8)  A[0]:(0.658214986324) A[1]:(-5.3271651268e-06) A[2]:(0.728896021843) A[3]:(0.591342449188)\n",
      " state (9)  A[0]:(0.657523274422) A[1]:(0.80997800827) A[2]:(0.809928715229) A[3]:(0.00109741045162)\n",
      " state (10)  A[0]:(0.730087816715) A[1]:(0.89995932579) A[2]:(-0.000327944755554) A[3]:(0.729722857475)\n",
      " state (11)  A[0]:(0.525915622711) A[1]:(0.876496732235) A[2]:(-0.626663804054) A[3]:(0.845815181732)\n",
      " state (12)  A[0]:(0.0837888941169) A[1]:(0.823437929153) A[2]:(-0.624654591084) A[3]:(0.795573711395)\n",
      " state (13)  A[0]:(0.00264536705799) A[1]:(0.807800829411) A[2]:(0.899980902672) A[3]:(0.730402827263)\n",
      " state (14)  A[0]:(0.811019480228) A[1]:(0.899625360966) A[2]:(1.0) A[3]:(0.811138510704)\n",
      " state (15)  A[0]:(0.982012867928) A[1]:(0.95555049181) A[2]:(1.0) A[3]:(0.875841319561)\n",
      "Episode 976000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6004. Times reached goal: 998.               Steps done: 7021344. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000803363301832.\n",
      " state (0)  A[0]:(0.531607031822) A[1]:(0.590224027634) A[2]:(0.590292453766) A[3]:(0.531708955765)\n",
      " state (1)  A[0]:(0.531342029572) A[1]:(-4.73111867905e-05) A[2]:(0.655815720558) A[3]:(0.59040915966)\n",
      " state (2)  A[0]:(0.590216994286) A[1]:(0.728682279587) A[2]:(0.589823365211) A[3]:(0.655987262726)\n",
      " state (3)  A[0]:(0.655220806599) A[1]:(-0.218322142959) A[2]:(0.538982510567) A[3]:(0.517670750618)\n",
      " state (4)  A[0]:(0.588868141174) A[1]:(0.655487775803) A[2]:(-0.000889658695087) A[3]:(0.5320520401)\n",
      " state (5)  A[0]:(0.16127628088) A[1]:(0.928453266621) A[2]:(-0.195795923471) A[3]:(0.527383089066)\n",
      " state (6)  A[0]:(-0.00240492355078) A[1]:(0.809760928154) A[2]:(-0.000334143638611) A[3]:(0.655683517456)\n",
      " state (7)  A[0]:(0.623774170876) A[1]:(-0.248433887959) A[2]:(0.30934754014) A[3]:(0.881280481815)\n",
      " state (8)  A[0]:(0.656518995762) A[1]:(-0.000869199400768) A[2]:(0.728602290154) A[3]:(0.590852737427)\n",
      " state (9)  A[0]:(0.656136572361) A[1]:(0.80971300602) A[2]:(0.809796869755) A[3]:(0.000854760175571)\n",
      " state (10)  A[0]:(0.728717267513) A[1]:(0.899839997292) A[2]:(0.000132918357849) A[3]:(0.728958904743)\n",
      " state (11)  A[0]:(0.523478746414) A[1]:(0.876407146454) A[2]:(-0.626107811928) A[3]:(0.845005273819)\n",
      " state (12)  A[0]:(0.0801550671458) A[1]:(0.823412120342) A[2]:(-0.624155700207) A[3]:(0.794215917587)\n",
      " state (13)  A[0]:(-0.00133472599555) A[1]:(0.807873904705) A[2]:(0.899900257587) A[3]:(0.728274106979)\n",
      " state (14)  A[0]:(0.809627234936) A[1]:(0.899724900723) A[2]:(1.0) A[3]:(0.809200406075)\n",
      " state (15)  A[0]:(0.981894433498) A[1]:(0.955644369125) A[2]:(1.0) A[3]:(0.874336838722)\n",
      "Episode 977000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 1000.               Steps done: 7027344. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000798557553683.\n",
      " state (0)  A[0]:(0.531533956528) A[1]:(0.590498566628) A[2]:(0.59053593874) A[3]:(0.531604886055)\n",
      " state (1)  A[0]:(0.531351804733) A[1]:(0.000383585662348) A[2]:(0.656150817871) A[3]:(0.590427041054)\n",
      " state (2)  A[0]:(0.590391635895) A[1]:(0.729145765305) A[2]:(0.590293288231) A[3]:(0.656002283096)\n",
      " state (3)  A[0]:(0.656056404114) A[1]:(-0.216796323657) A[2]:(0.539325833321) A[3]:(0.51726359129)\n",
      " state (4)  A[0]:(0.59009796381) A[1]:(0.656213521957) A[2]:(-0.000381588906748) A[3]:(0.531389176846)\n",
      " state (5)  A[0]:(0.163050249219) A[1]:(0.928609609604) A[2]:(-0.195273563266) A[3]:(0.526653528214)\n",
      " state (6)  A[0]:(-0.00111937476322) A[1]:(0.810215830803) A[2]:(0.00018036365509) A[3]:(0.65500664711)\n",
      " state (7)  A[0]:(0.624060094357) A[1]:(-0.247133225203) A[2]:(0.30999904871) A[3]:(0.880933642387)\n",
      " state (8)  A[0]:(0.656417489052) A[1]:(0.000382840604289) A[2]:(0.72918856144) A[3]:(0.589472174644)\n",
      " state (9)  A[0]:(0.65583473444) A[1]:(0.81016522646) A[2]:(0.810138046741) A[3]:(-0.000852584606037)\n",
      " state (10)  A[0]:(0.728603065014) A[1]:(0.900122642517) A[2]:(0.000526308955159) A[3]:(0.728588223457)\n",
      " state (11)  A[0]:(0.523515224457) A[1]:(0.876783192158) A[2]:(-0.626059651375) A[3]:(0.844959974289)\n",
      " state (12)  A[0]:(0.0803982242942) A[1]:(0.823958516121) A[2]:(-0.624231815338) A[3]:(0.794319629669)\n",
      " state (13)  A[0]:(-0.000936388678383) A[1]:(0.808468699455) A[2]:(0.899791359901) A[3]:(0.728607296944)\n",
      " state (14)  A[0]:(0.809869527817) A[1]:(0.900043129921) A[2]:(1.0) A[3]:(0.809662520885)\n",
      " state (15)  A[0]:(0.981937468052) A[1]:(0.955790042877) A[2]:(1.0) A[3]:(0.874806523323)\n",
      "Episode 978000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 998.               Steps done: 7033345. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000793779759912.\n",
      " state (0)  A[0]:(0.532406568527) A[1]:(0.590733230114) A[2]:(0.590113997459) A[3]:(0.532446146011)\n",
      " state (1)  A[0]:(0.532570064068) A[1]:(0.000622890831437) A[2]:(0.656404256821) A[3]:(0.590630054474)\n",
      " state (2)  A[0]:(0.591943025589) A[1]:(0.729081273079) A[2]:(0.590603172779) A[3]:(0.656033039093)\n",
      " state (3)  A[0]:(0.658569455147) A[1]:(-0.218032106757) A[2]:(0.539712905884) A[3]:(0.517541646957)\n",
      " state (4)  A[0]:(0.593435406685) A[1]:(0.655369460583) A[2]:(0.000360488862498) A[3]:(0.531720161438)\n",
      " state (5)  A[0]:(0.167606756091) A[1]:(0.928291678429) A[2]:(-0.194237023592) A[3]:(0.526772141457)\n",
      " state (6)  A[0]:(0.0017434937181) A[1]:(0.80963331461) A[2]:(0.00101208651904) A[3]:(0.654823303223)\n",
      " state (7)  A[0]:(0.625109493732) A[1]:(-0.249641865492) A[2]:(0.310593605042) A[3]:(0.880993247032)\n",
      " state (8)  A[0]:(0.657450556755) A[1]:(-0.00445670681074) A[2]:(0.728695333004) A[3]:(0.590803384781)\n",
      " state (9)  A[0]:(0.656278371811) A[1]:(0.808976054192) A[2]:(0.809673368931) A[3]:(-0.000612884701695)\n",
      " state (10)  A[0]:(0.729395627975) A[1]:(0.899967372417) A[2]:(-0.00249742949381) A[3]:(0.729261696339)\n",
      " state (11)  A[0]:(0.525291800499) A[1]:(0.877100229263) A[2]:(-0.628702282906) A[3]:(0.845745444298)\n",
      " state (12)  A[0]:(0.082946203649) A[1]:(0.825090050697) A[2]:(-0.62655299902) A[3]:(0.79532122612)\n",
      " state (13)  A[0]:(0.00195914250799) A[1]:(0.81057626009) A[2]:(0.90000641346) A[3]:(0.729584097862)\n",
      " state (14)  A[0]:(0.81091016531) A[1]:(0.901753306389) A[2]:(1.0) A[3]:(0.809966206551)\n",
      " state (15)  A[0]:(0.981933474541) A[1]:(0.956754088402) A[2]:(1.0) A[3]:(0.874442279339)\n",
      "Episode 979000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 999.               Steps done: 7039351. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000789026606681.\n",
      "q_values \n",
      "tensor([[ 0.5320,  0.5904,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5324, -0.0003,  0.6561,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5916,  0.7290,  0.5904,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0023,  0.8100, -0.0001,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7297,  0.9000,  0.0001,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8107,  0.9000,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531719446182) A[1]:(0.590376496315) A[2]:(0.590448141098) A[3]:(0.531484365463)\n",
      " state (1)  A[0]:(0.532010912895) A[1]:(-0.000260502099991) A[2]:(0.656083583832) A[3]:(0.590416431427)\n",
      " state (2)  A[0]:(0.591203212738) A[1]:(0.7289955616) A[2]:(0.590367674828) A[3]:(0.656042993069)\n",
      " state (3)  A[0]:(0.656845092773) A[1]:(-0.216553062201) A[2]:(0.539508283138) A[3]:(0.517901778221)\n",
      " state (4)  A[0]:(0.591319441795) A[1]:(0.656063318253) A[2]:(-0.000227212905884) A[3]:(0.532269954681)\n",
      " state (5)  A[0]:(0.165456876159) A[1]:(0.928598940372) A[2]:(-0.195504829288) A[3]:(0.527734398842)\n",
      " state (6)  A[0]:(0.00204348284751) A[1]:(0.810004651546) A[2]:(-0.000167727470398) A[3]:(0.656116843224)\n",
      " state (7)  A[0]:(0.625887393951) A[1]:(-0.247841119766) A[2]:(0.309573560953) A[3]:(0.881397068501)\n",
      " state (8)  A[0]:(0.657706260681) A[1]:(-6.20782375336e-05) A[2]:(0.728990733624) A[3]:(0.590434074402)\n",
      " state (9)  A[0]:(0.65707808733) A[1]:(0.809965848923) A[2]:(0.810008943081) A[3]:(0.000350907444954)\n",
      " state (10)  A[0]:(0.729704856873) A[1]:(0.899994909763) A[2]:(0.000119686126709) A[3]:(0.729195833206)\n",
      " state (11)  A[0]:(0.525363564491) A[1]:(0.876623928547) A[2]:(-0.626276791096) A[3]:(0.845343828201)\n",
      " state (12)  A[0]:(0.0830769017339) A[1]:(0.82374984026) A[2]:(-0.624291598797) A[3]:(0.794791102409)\n",
      " state (13)  A[0]:(0.00182449619751) A[1]:(0.808284759521) A[2]:(0.899973154068) A[3]:(0.729130268097)\n",
      " state (14)  A[0]:(0.810704827309) A[1]:(0.899975776672) A[2]:(1.0) A[3]:(0.80993694067)\n",
      " state (15)  A[0]:(0.981985867023) A[1]:(0.955758690834) A[2]:(1.0) A[3]:(0.874851047993)\n",
      "Episode 980000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 1000.               Steps done: 7045359. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00078430034673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531601786613) A[1]:(0.590443134308) A[2]:(0.590408086777) A[3]:(0.528941273689)\n",
      " state (1)  A[0]:(0.531187653542) A[1]:(0.00046998259495) A[2]:(0.656045436859) A[3]:(0.587685585022)\n",
      " state (2)  A[0]:(0.590318799019) A[1]:(0.729059815407) A[2]:(0.590089440346) A[3]:(0.653233647346)\n",
      " state (3)  A[0]:(0.656860589981) A[1]:(-0.216925159097) A[2]:(0.539077341557) A[3]:(0.512693226337)\n",
      " state (4)  A[0]:(0.591693997383) A[1]:(0.656167387962) A[2]:(-0.00133526243735) A[3]:(0.526328921318)\n",
      " state (5)  A[0]:(0.165799468756) A[1]:(0.928652584553) A[2]:(-0.196968510747) A[3]:(0.521318674088)\n",
      " state (6)  A[0]:(0.00169956521131) A[1]:(0.810080468655) A[2]:(-0.00189912086353) A[3]:(0.65063893795)\n",
      " state (7)  A[0]:(0.624581098557) A[1]:(-0.247704580426) A[2]:(0.308289170265) A[3]:(0.87886351347)\n",
      " state (8)  A[0]:(0.655260443687) A[1]:(4.73856925964e-05) A[2]:(0.72910040617) A[3]:(0.581343531609)\n",
      " state (9)  A[0]:(0.654440522194) A[1]:(0.810010135174) A[2]:(0.810040712357) A[3]:(-0.0126000531018)\n",
      " state (10)  A[0]:(0.727831482887) A[1]:(0.900110781193) A[2]:(-0.00171160534956) A[3]:(0.724501669407)\n",
      " state (11)  A[0]:(0.522479772568) A[1]:(0.876880347729) A[2]:(-0.627987027168) A[3]:(0.842909276485)\n",
      " state (12)  A[0]:(0.0789314880967) A[1]:(0.82427752018) A[2]:(-0.625772297382) A[3]:(0.791842460632)\n",
      " state (13)  A[0]:(-0.00188314693514) A[1]:(0.809134602547) A[2]:(0.900187849998) A[3]:(0.725628137589)\n",
      " state (14)  A[0]:(0.809790790081) A[1]:(0.900637447834) A[2]:(1.0) A[3]:(0.807650566101)\n",
      " state (15)  A[0]:(0.981854021549) A[1]:(0.956100821495) A[2]:(1.0) A[3]:(0.873326063156)\n",
      "Episode 981000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 1000.               Steps done: 7051360. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000779607854255.\n",
      " state (0)  A[0]:(0.532392680645) A[1]:(0.59030687809) A[2]:(0.590383291245) A[3]:(0.530887007713)\n",
      " state (1)  A[0]:(0.532174766064) A[1]:(-0.000507540942635) A[2]:(0.655992865562) A[3]:(0.590091109276)\n",
      " state (2)  A[0]:(0.591088294983) A[1]:(0.728914499283) A[2]:(0.590528607368) A[3]:(0.655891895294)\n",
      " state (3)  A[0]:(0.656505405903) A[1]:(-0.217222347856) A[2]:(0.540022850037) A[3]:(0.517670869827)\n",
      " state (4)  A[0]:(0.590842366219) A[1]:(0.656096875668) A[2]:(0.000414252252085) A[3]:(0.532321214676)\n",
      " state (5)  A[0]:(0.164838463068) A[1]:(0.928658246994) A[2]:(-0.194998472929) A[3]:(0.528223395348)\n",
      " state (6)  A[0]:(0.0019097304903) A[1]:(0.810079395771) A[2]:(0.000477433175547) A[3]:(0.657235145569)\n",
      " state (7)  A[0]:(0.626141130924) A[1]:(-0.247603297234) A[2]:(0.310077577829) A[3]:(0.882276892662)\n",
      " state (8)  A[0]:(0.658529996872) A[1]:(0.000400863558752) A[2]:(0.728953719139) A[3]:(0.593784570694)\n",
      " state (9)  A[0]:(0.658648252487) A[1]:(0.810128986835) A[2]:(0.810104370117) A[3]:(0.00490609323606)\n",
      " state (10)  A[0]:(0.731135606766) A[1]:(0.90003490448) A[2]:(0.000985979684629) A[3]:(0.731086134911)\n",
      " state (11)  A[0]:(0.527568399906) A[1]:(0.876574337482) A[2]:(-0.625608086586) A[3]:(0.846527218819)\n",
      " state (12)  A[0]:(0.0860309749842) A[1]:(0.823517084122) A[2]:(-0.623669922352) A[3]:(0.796422362328)\n",
      " state (13)  A[0]:(0.00467982469127) A[1]:(0.807816565037) A[2]:(0.900010585785) A[3]:(0.731312155724)\n",
      " state (14)  A[0]:(0.811703920364) A[1]:(0.899581730366) A[2]:(1.0) A[3]:(0.811566948891)\n",
      " state (15)  A[0]:(0.982111036777) A[1]:(0.955525815487) A[2]:(1.0) A[3]:(0.876015782356)\n",
      "Episode 982000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 1000.               Steps done: 7057360. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000774944212047.\n",
      " state (0)  A[0]:(0.531340897083) A[1]:(0.590190291405) A[2]:(0.590309381485) A[3]:(0.532364368439)\n",
      " state (1)  A[0]:(0.531499862671) A[1]:(-0.00039421764086) A[2]:(0.656043052673) A[3]:(0.591551542282)\n",
      " state (2)  A[0]:(0.590603649616) A[1]:(0.728902220726) A[2]:(0.590685725212) A[3]:(0.657163262367)\n",
      " state (3)  A[0]:(0.65615606308) A[1]:(-0.218382462859) A[2]:(0.540367603302) A[3]:(0.51886510849)\n",
      " state (4)  A[0]:(0.59043776989) A[1]:(0.656359434128) A[2]:(0.000618457735982) A[3]:(0.533405065536)\n",
      " state (5)  A[0]:(0.164053425193) A[1]:(0.928688168526) A[2]:(-0.194749966264) A[3]:(0.529093503952)\n",
      " state (6)  A[0]:(0.00113201094791) A[1]:(0.81005948782) A[2]:(0.000664949300699) A[3]:(0.657569766045)\n",
      " state (7)  A[0]:(0.62573993206) A[1]:(-0.247840389609) A[2]:(0.310316115618) A[3]:(0.882141649723)\n",
      " state (8)  A[0]:(0.657709717751) A[1]:(0.000299073755741) A[2]:(0.729435920715) A[3]:(0.592832684517)\n",
      " state (9)  A[0]:(0.657538592815) A[1]:(0.810101509094) A[2]:(0.810266375542) A[3]:(0.00519688241184)\n",
      " state (10)  A[0]:(0.73013240099) A[1]:(0.900043189526) A[2]:(0.000578045786824) A[3]:(0.731865286827)\n",
      " state (11)  A[0]:(0.525826454163) A[1]:(0.876640081406) A[2]:(-0.6260638237) A[3]:(0.84706556797)\n",
      " state (12)  A[0]:(0.083428144455) A[1]:(0.823710501194) A[2]:(-0.624054312706) A[3]:(0.79706299305)\n",
      " state (13)  A[0]:(0.00214701564983) A[1]:(0.808186888695) A[2]:(0.90013051033) A[3]:(0.732023656368)\n",
      " state (14)  A[0]:(0.810961544514) A[1]:(0.89989811182) A[2]:(1.0) A[3]:(0.811967968941)\n",
      " state (15)  A[0]:(0.98202008009) A[1]:(0.955705761909) A[2]:(1.0) A[3]:(0.876149952412)\n",
      "Episode 983000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 1000.               Steps done: 7063364. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000770305386686.\n",
      " state (0)  A[0]:(0.530660450459) A[1]:(0.590752720833) A[2]:(0.590616106987) A[3]:(0.531912207603)\n",
      " state (1)  A[0]:(0.530568599701) A[1]:(5.43966889381e-05) A[2]:(0.656151235104) A[3]:(0.590641438961)\n",
      " state (2)  A[0]:(0.589694499969) A[1]:(0.729044139385) A[2]:(0.590718984604) A[3]:(0.656132638454)\n",
      " state (3)  A[0]:(0.655342936516) A[1]:(-0.218037471175) A[2]:(0.54018008709) A[3]:(0.517442822456)\n",
      " state (4)  A[0]:(0.589571356773) A[1]:(0.656110703945) A[2]:(0.000187993049622) A[3]:(0.531831264496)\n",
      " state (5)  A[0]:(0.162923589349) A[1]:(0.928655326366) A[2]:(-0.195435017347) A[3]:(0.527405560017)\n",
      " state (6)  A[0]:(-4.79221343994e-05) A[1]:(0.810012936592) A[2]:(-0.000182390213013) A[3]:(0.656238496304)\n",
      " state (7)  A[0]:(0.624879181385) A[1]:(-0.247968927026) A[2]:(0.309502631426) A[3]:(0.881639480591)\n",
      " state (8)  A[0]:(0.656824588776) A[1]:(3.70815396309e-05) A[2]:(0.728991508484) A[3]:(0.590770840645)\n",
      " state (9)  A[0]:(0.65661495924) A[1]:(0.809998452663) A[2]:(0.809993505478) A[3]:(0.000286474823952)\n",
      " state (10)  A[0]:(0.729341030121) A[1]:(0.899994313717) A[2]:(-0.000234365463257) A[3]:(0.729149222374)\n",
      " state (11)  A[0]:(0.524560570717) A[1]:(0.876591026783) A[2]:(-0.626610040665) A[3]:(0.845310270786)\n",
      " state (12)  A[0]:(0.0817246586084) A[1]:(0.823664844036) A[2]:(-0.624620735645) A[3]:(0.794734537601)\n",
      " state (13)  A[0]:(0.000664412858896) A[1]:(0.808174133301) A[2]:(0.899970054626) A[3]:(0.729043126106)\n",
      " state (14)  A[0]:(0.810630857944) A[1]:(0.899917781353) A[2]:(1.0) A[3]:(0.809803247452)\n",
      " state (15)  A[0]:(0.982005894184) A[1]:(0.955728054047) A[2]:(1.0) A[3]:(0.874707996845)\n",
      "Episode 984000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 1000.               Steps done: 7069365. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000765696626477.\n",
      "q_values \n",
      "tensor([[ 0.5272,  0.5903,  0.5905,  0.5272]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5257,  0.0007,  0.6562,  0.5847]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5841,  0.7290,  0.5901,  0.6495]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0158,  0.8101, -0.0019,  0.6413]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7227,  0.9001, -0.0031,  0.7111]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8069,  0.9007,  1.0000,  0.7986]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.527127027512) A[1]:(0.590476691723) A[2]:(0.590543270111) A[3]:(0.527125716209)\n",
      " state (1)  A[0]:(0.525539815426) A[1]:(0.000910319155082) A[2]:(0.656224131584) A[3]:(0.584588050842)\n",
      " state (2)  A[0]:(0.584077715874) A[1]:(0.729226648808) A[2]:(0.590368270874) A[3]:(0.649405956268)\n",
      " state (3)  A[0]:(0.649035573006) A[1]:(-0.216669633985) A[2]:(0.539363503456) A[3]:(0.505098700523)\n",
      " state (4)  A[0]:(0.580884456635) A[1]:(0.65630543232) A[2]:(-0.000989555963315) A[3]:(0.517094612122)\n",
      " state (5)  A[0]:(0.148426055908) A[1]:(0.928682327271) A[2]:(-0.196750789881) A[3]:(0.510849058628)\n",
      " state (6)  A[0]:(-0.0157264024019) A[1]:(0.810146450996) A[2]:(-0.00191390281543) A[3]:(0.641322851181)\n",
      " state (7)  A[0]:(0.615229487419) A[1]:(-0.247446402907) A[2]:(0.308131337166) A[3]:(0.874527692795)\n",
      " state (8)  A[0]:(0.647479653358) A[1]:(0.000298120081425) A[2]:(0.72923386097) A[3]:(0.565732836723)\n",
      " state (9)  A[0]:(0.647863388062) A[1]:(0.810111641884) A[2]:(0.810089290142) A[3]:(-0.0396620482206)\n",
      " state (10)  A[0]:(0.722867250443) A[1]:(0.900196433067) A[2]:(-0.00281070917845) A[3]:(0.711099505424)\n",
      " state (11)  A[0]:(0.514825165272) A[1]:(0.877002239227) A[2]:(-0.629152655602) A[3]:(0.834730029106)\n",
      " state (12)  A[0]:(0.0685354843736) A[1]:(0.824478149414) A[2]:(-0.62690782547) A[3]:(0.781380236149)\n",
      " state (13)  A[0]:(-0.0113867968321) A[1]:(0.809457957745) A[2]:(0.90023958683) A[3]:(0.71270775795)\n",
      " state (14)  A[0]:(0.80723375082) A[1]:(0.900909364223) A[2]:(1.0) A[3]:(0.79866206646)\n",
      " state (15)  A[0]:(0.98162728548) A[1]:(0.956242144108) A[2]:(1.0) A[3]:(0.86749792099)\n",
      "Episode 985000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5998. Times reached goal: 999.               Steps done: 7075363. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000761117723967.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531928062439) A[1]:(0.590600013733) A[2]:(0.590536952019) A[3]:(0.530130505562)\n",
      " state (1)  A[0]:(0.531933248043) A[1]:(3.51816415787e-05) A[2]:(0.656167387962) A[3]:(0.589769601822)\n",
      " state (2)  A[0]:(0.590911209583) A[1]:(0.729074954987) A[2]:(0.590722799301) A[3]:(0.655700743198)\n",
      " state (3)  A[0]:(0.656171262264) A[1]:(-0.217723205686) A[2]:(0.540204405785) A[3]:(0.517480790615)\n",
      " state (4)  A[0]:(0.590438961983) A[1]:(0.656207501888) A[2]:(0.000309109687805) A[3]:(0.532142043114)\n",
      " state (5)  A[0]:(0.164301767945) A[1]:(0.928704559803) A[2]:(-0.195272311568) A[3]:(0.527749657631)\n",
      " state (6)  A[0]:(0.00120645703282) A[1]:(0.810073077679) A[2]:(0.000144720077515) A[3]:(0.656175911427)\n",
      " state (7)  A[0]:(0.625096559525) A[1]:(-0.247940957546) A[2]:(0.309858202934) A[3]:(0.881325244904)\n",
      " state (8)  A[0]:(0.65663677454) A[1]:(0.000203147530556) A[2]:(0.729167580605) A[3]:(0.590406179428)\n",
      " state (9)  A[0]:(0.65631711483) A[1]:(0.810063719749) A[2]:(0.810126662254) A[3]:(0.00178777985275)\n",
      " state (10)  A[0]:(0.729135632515) A[1]:(0.900056362152) A[2]:(0.000408649415476) A[3]:(0.730210185051)\n",
      " state (11)  A[0]:(0.524385333061) A[1]:(0.876715898514) A[2]:(-0.626089215279) A[3]:(0.845987439156)\n",
      " state (12)  A[0]:(0.081628151238) A[1]:(0.823912739754) A[2]:(-0.624075293541) A[3]:(0.795585095882)\n",
      " state (13)  A[0]:(0.000294506549835) A[1]:(0.808516979218) A[2]:(0.900064706802) A[3]:(0.730045318604)\n",
      " state (14)  A[0]:(0.810109138489) A[1]:(0.900146484375) A[2]:(1.0) A[3]:(0.81044614315)\n",
      " state (15)  A[0]:(0.98191678524) A[1]:(0.955856502056) A[2]:(1.0) A[3]:(0.875096082687)\n",
      "Episode 986000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6000. Times reached goal: 998.               Steps done: 7081363. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000756564690383.\n",
      " state (0)  A[0]:(0.531713366508) A[1]:(0.590504407883) A[2]:(0.590344309807) A[3]:(0.531311869621)\n",
      " state (1)  A[0]:(0.531747639179) A[1]:(0.000178821384907) A[2]:(0.655938327312) A[3]:(0.589579999447)\n",
      " state (2)  A[0]:(0.59062731266) A[1]:(0.728953242302) A[2]:(0.590216875076) A[3]:(0.654622554779)\n",
      " state (3)  A[0]:(0.655683755875) A[1]:(-0.218192398548) A[2]:(0.539356350899) A[3]:(0.51378762722)\n",
      " state (4)  A[0]:(0.589645802975) A[1]:(0.656034648418) A[2]:(-0.00137412466574) A[3]:(0.52708363533)\n",
      " state (5)  A[0]:(0.162635087967) A[1]:(0.928693294525) A[2]:(-0.197280392051) A[3]:(0.521774351597)\n",
      " state (6)  A[0]:(-0.00114208413288) A[1]:(0.809997260571) A[2]:(-0.00208222563379) A[3]:(0.650496244431)\n",
      " state (7)  A[0]:(0.622824549675) A[1]:(-0.248076215386) A[2]:(0.308106422424) A[3]:(0.878288328648)\n",
      " state (8)  A[0]:(0.653236329556) A[1]:(8.04141163826e-05) A[2]:(0.728832483292) A[3]:(0.577580392361)\n",
      " state (9)  A[0]:(0.652308523655) A[1]:(0.80998647213) A[2]:(0.809871613979) A[3]:(-0.0220133699477)\n",
      " state (10)  A[0]:(0.725706100464) A[1]:(0.900026321411) A[2]:(-0.00184905319475) A[3]:(0.718991816044)\n",
      " state (11)  A[0]:(0.518659770489) A[1]:(0.87669223547) A[2]:(-0.627989530563) A[3]:(0.839312076569)\n",
      " state (12)  A[0]:(0.0732486322522) A[1]:(0.823914349079) A[2]:(-0.62594473362) A[3]:(0.787120223045)\n",
      " state (13)  A[0]:(-0.00801921542734) A[1]:(0.808628380299) A[2]:(0.899838268757) A[3]:(0.719612002373)\n",
      " state (14)  A[0]:(0.807562410831) A[1]:(0.900302886963) A[2]:(1.0) A[3]:(0.803222954273)\n",
      " state (15)  A[0]:(0.981653571129) A[1]:(0.955951094627) A[2]:(1.0) A[3]:(0.870414674282)\n",
      "Episode 987000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6004. Times reached goal: 999.               Steps done: 7087367. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00075203588506.\n",
      " state (0)  A[0]:(0.531103610992) A[1]:(0.590196728706) A[2]:(0.590398669243) A[3]:(0.529664874077)\n",
      " state (1)  A[0]:(0.531067490578) A[1]:(-0.000492811144795) A[2]:(0.656054913998) A[3]:(0.589351534843)\n",
      " state (2)  A[0]:(0.590125083923) A[1]:(0.728954672813) A[2]:(0.590480566025) A[3]:(0.655380606651)\n",
      " state (3)  A[0]:(0.656183719635) A[1]:(-0.218501999974) A[2]:(0.539889216423) A[3]:(0.517114937305)\n",
      " state (4)  A[0]:(0.590256214142) A[1]:(0.656320810318) A[2]:(-4.29153442383e-05) A[3]:(0.531869769096)\n",
      " state (5)  A[0]:(0.162807017565) A[1]:(0.928626358509) A[2]:(-0.195160284638) A[3]:(0.527314901352)\n",
      " state (6)  A[0]:(-0.00176936201751) A[1]:(0.810003578663) A[2]:(-6.91413879395e-06) A[3]:(0.655509114265)\n",
      " state (7)  A[0]:(0.622966825962) A[1]:(-0.247821092606) A[2]:(0.309351623058) A[3]:(0.880860030651)\n",
      " state (8)  A[0]:(0.654813826084) A[1]:(-9.43839550018e-05) A[2]:(0.729203820229) A[3]:(0.587820529938)\n",
      " state (9)  A[0]:(0.654783725739) A[1]:(0.809945106506) A[2]:(0.810089588165) A[3]:(-0.00301410839893)\n",
      " state (10)  A[0]:(0.728023409843) A[1]:(0.899993181229) A[2]:(-0.000706910970621) A[3]:(0.728507518768)\n",
      " state (11)  A[0]:(0.522530436516) A[1]:(0.876625955105) A[2]:(-0.627112150192) A[3]:(0.845191895962)\n",
      " state (12)  A[0]:(0.0787701904774) A[1]:(0.823758721352) A[2]:(-0.625121951103) A[3]:(0.794732451439)\n",
      " state (13)  A[0]:(-0.00257008709013) A[1]:(0.808339953423) A[2]:(0.899913668633) A[3]:(0.729128837585)\n",
      " state (14)  A[0]:(0.809319555759) A[1]:(0.900054693222) A[2]:(1.0) A[3]:(0.809842169285)\n",
      " state (15)  A[0]:(0.981838643551) A[1]:(0.955805182457) A[2]:(1.0) A[3]:(0.874690413475)\n",
      "Episode 988000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 1000.               Steps done: 7093370. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000747534936755.\n",
      " state (0)  A[0]:(0.532114803791) A[1]:(0.590528845787) A[2]:(0.590524673462) A[3]:(0.531330108643)\n",
      " state (1)  A[0]:(0.532204627991) A[1]:(-0.000221438705921) A[2]:(0.656101167202) A[3]:(0.590386986732)\n",
      " state (2)  A[0]:(0.591200828552) A[1]:(0.729000329971) A[2]:(0.590580701828) A[3]:(0.656066775322)\n",
      " state (3)  A[0]:(0.656801939011) A[1]:(-0.218552157283) A[2]:(0.540023207664) A[3]:(0.517606317997)\n",
      " state (4)  A[0]:(0.590981602669) A[1]:(0.656094789505) A[2]:(0.000125050544739) A[3]:(0.532162904739)\n",
      " state (5)  A[0]:(0.16425897181) A[1]:(0.928612947464) A[2]:(-0.195104435086) A[3]:(0.527706742287)\n",
      " state (6)  A[0]:(5.36441802979e-05) A[1]:(0.810002326965) A[2]:(0.000149130821228) A[3]:(0.656306743622)\n",
      " state (7)  A[0]:(0.624209523201) A[1]:(-0.247804895043) A[2]:(0.309524953365) A[3]:(0.881607055664)\n",
      " state (8)  A[0]:(0.656084656715) A[1]:(5.67957758904e-05) A[2]:(0.729000687599) A[3]:(0.590859353542)\n",
      " state (9)  A[0]:(0.655875921249) A[1]:(0.810014009476) A[2]:(0.810004234314) A[3]:(0.000996514805593)\n",
      " state (10)  A[0]:(0.728765130043) A[1]:(0.900000631809) A[2]:(-9.76324081421e-05) A[3]:(0.729737401009)\n",
      " state (11)  A[0]:(0.523708164692) A[1]:(0.876595377922) A[2]:(-0.62645637989) A[3]:(0.845730602741)\n",
      " state (12)  A[0]:(0.0805689543486) A[1]:(0.823660850525) A[2]:(-0.624432504177) A[3]:(0.795299708843)\n",
      " state (13)  A[0]:(-0.000768303696532) A[1]:(0.808152675629) A[2]:(0.900007963181) A[3]:(0.729687690735)\n",
      " state (14)  A[0]:(0.809804677963) A[1]:(0.899896740913) A[2]:(1.0) A[3]:(0.810051858425)\n",
      " state (15)  A[0]:(0.981888711452) A[1]:(0.955716013908) A[2]:(1.0) A[3]:(0.874732017517)\n",
      "Episode 989000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 999.               Steps done: 7099370. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000743063155892.\n",
      "q_values \n",
      "tensor([[ 0.5309,  0.5902,  0.5904,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0001,  0.6561,  0.5894]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.7289,  0.5903,  0.6550]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0025,  0.8101, -0.0007,  0.6556]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7306,  0.9000, -0.0003,  0.7278]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8113,  0.9001,  1.0000,  0.8088]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.5308983922) A[1]:(0.590216219425) A[2]:(0.590452075005) A[3]:(0.531290173531)\n",
      " state (1)  A[0]:(0.531342804432) A[1]:(-0.000118166208267) A[2]:(0.656098425388) A[3]:(0.589403748512)\n",
      " state (2)  A[0]:(0.590838015079) A[1]:(0.728854060173) A[2]:(0.590204596519) A[3]:(0.654969215393)\n",
      " state (3)  A[0]:(0.656475901604) A[1]:(-0.218494147062) A[2]:(0.539184331894) A[3]:(0.516195654869)\n",
      " state (4)  A[0]:(0.591086089611) A[1]:(0.65622061491) A[2]:(-0.00134634890128) A[3]:(0.530778884888)\n",
      " state (5)  A[0]:(0.165409222245) A[1]:(0.928664326668) A[2]:(-0.196523085237) A[3]:(0.526377916336)\n",
      " state (6)  A[0]:(0.00257568992674) A[1]:(0.809934973717) A[2]:(-0.000750064733438) A[3]:(0.655580401421)\n",
      " state (7)  A[0]:(0.626293897629) A[1]:(-0.24822846055) A[2]:(0.30943980813) A[3]:(0.881451010704)\n",
      " state (8)  A[0]:(0.65805208683) A[1]:(-0.000204980373383) A[2]:(0.728914499283) A[3]:(0.589828431606)\n",
      " state (9)  A[0]:(0.658107638359) A[1]:(0.809930086136) A[2]:(0.809907436371) A[3]:(-0.00232331035659)\n",
      " state (10)  A[0]:(0.730645656586) A[1]:(0.899973988533) A[2]:(-0.00057911867043) A[3]:(0.727853536606)\n",
      " state (11)  A[0]:(0.526506662369) A[1]:(0.876586854458) A[2]:(-0.626834928989) A[3]:(0.844549655914)\n",
      " state (12)  A[0]:(0.0843048021197) A[1]:(0.823701798916) A[2]:(-0.624763131142) A[3]:(0.79378592968)\n",
      " state (13)  A[0]:(0.00318299653009) A[1]:(0.808294951916) A[2]:(0.900033891201) A[3]:(0.727843046188)\n",
      " state (14)  A[0]:(0.811318874359) A[1]:(0.900050878525) A[2]:(1.0) A[3]:(0.808789134026)\n",
      " state (15)  A[0]:(0.9820535779) A[1]:(0.955815196037) A[2]:(1.0) A[3]:(0.873924434185)\n",
      "Episode 990000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 1000.               Steps done: 7105370. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000738618125384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53119122982) A[1]:(0.590483188629) A[2]:(0.590462386608) A[3]:(0.531013906002)\n",
      " state (1)  A[0]:(0.531282782555) A[1]:(-3.38032841682e-05) A[2]:(0.656093835831) A[3]:(0.590262770653)\n",
      " state (2)  A[0]:(0.590485692024) A[1]:(0.729023694992) A[2]:(0.590473771095) A[3]:(0.656036376953)\n",
      " state (3)  A[0]:(0.656268179417) A[1]:(-0.218322843313) A[2]:(0.539755284786) A[3]:(0.517144203186)\n",
      " state (4)  A[0]:(0.590448081493) A[1]:(0.656111299992) A[2]:(-0.00025475025177) A[3]:(0.531480193138)\n",
      " state (5)  A[0]:(0.163611128926) A[1]:(0.928619205952) A[2]:(-0.19541208446) A[3]:(0.526981294155)\n",
      " state (6)  A[0]:(-0.000303566455841) A[1]:(0.810006976128) A[2]:(5.66244125366e-05) A[3]:(0.655815660954)\n",
      " state (7)  A[0]:(0.624260008335) A[1]:(-0.247785151005) A[2]:(0.309722840786) A[3]:(0.881498098373)\n",
      " state (8)  A[0]:(0.656375169754) A[1]:(-1.83060765266e-05) A[2]:(0.728947997093) A[3]:(0.590626895428)\n",
      " state (9)  A[0]:(0.656374335289) A[1]:(0.809990286827) A[2]:(0.809962689877) A[3]:(-0.000156089663506)\n",
      " state (10)  A[0]:(0.729247808456) A[1]:(0.899994015694) A[2]:(-0.000262975692749) A[3]:(0.729020833969)\n",
      " state (11)  A[0]:(0.524514913559) A[1]:(0.876583099365) A[2]:(-0.626642465591) A[3]:(0.845310091972)\n",
      " state (12)  A[0]:(0.081830047071) A[1]:(0.823643088341) A[2]:(-0.624661982059) A[3]:(0.79483795166)\n",
      " state (13)  A[0]:(0.000720918062143) A[1]:(0.80815076828) A[2]:(0.899952352047) A[3]:(0.729280769825)\n",
      " state (14)  A[0]:(0.810329079628) A[1]:(0.8999158144) A[2]:(1.0) A[3]:(0.809933543205)\n",
      " state (15)  A[0]:(0.981947898865) A[1]:(0.955736458302) A[2]:(1.0) A[3]:(0.874798476696)\n",
      "Episode 991000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 999.               Steps done: 7111370. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000734199685207.\n",
      " state (0)  A[0]:(0.535138487816) A[1]:(0.590357840061) A[2]:(0.590472340584) A[3]:(0.531982779503)\n",
      " state (1)  A[0]:(0.53569316864) A[1]:(-9.48682427406e-05) A[2]:(0.656010866165) A[3]:(0.589894175529)\n",
      " state (2)  A[0]:(0.59462428093) A[1]:(0.728757500648) A[2]:(0.590566575527) A[3]:(0.654635429382)\n",
      " state (3)  A[0]:(0.659103095531) A[1]:(-0.220065325499) A[2]:(0.54013800621) A[3]:(0.512501358986)\n",
      " state (4)  A[0]:(0.593630313873) A[1]:(0.65620881319) A[2]:(-0.000451803178294) A[3]:(0.525410592556)\n",
      " state (5)  A[0]:(0.16888204217) A[1]:(0.928717255592) A[2]:(-0.196176841855) A[3]:(0.51990544796)\n",
      " state (6)  A[0]:(0.00623241951689) A[1]:(0.81004691124) A[2]:(-0.00101578200702) A[3]:(0.649005889893)\n",
      " state (7)  A[0]:(0.628458380699) A[1]:(-0.247899085283) A[2]:(0.30869948864) A[3]:(0.877821087837)\n",
      " state (8)  A[0]:(0.659469842911) A[1]:(0.000197015702724) A[2]:(0.729048132896) A[3]:(0.57746553421)\n",
      " state (9)  A[0]:(0.659297347069) A[1]:(0.809986710548) A[2]:(0.810113847256) A[3]:(-0.0199754927307)\n",
      " state (10)  A[0]:(0.731870353222) A[1]:(0.89999049902) A[2]:(-0.000881910091266) A[3]:(0.720493912697)\n",
      " state (11)  A[0]:(0.528681159019) A[1]:(0.876582980156) A[2]:(-0.627344250679) A[3]:(0.840376317501)\n",
      " state (12)  A[0]:(0.0874912962317) A[1]:(0.823640406132) A[2]:(-0.625297367573) A[3]:(0.788621068001)\n",
      " state (13)  A[0]:(0.0064094979316) A[1]:(0.808170199394) A[2]:(0.900013506413) A[3]:(0.721589565277)\n",
      " state (14)  A[0]:(0.812190294266) A[1]:(0.899950325489) A[2]:(1.0) A[3]:(0.804547250271)\n",
      " state (15)  A[0]:(0.982104301453) A[1]:(0.955742657185) A[2]:(1.0) A[3]:(0.871248364449)\n",
      "Episode 992000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5996. Times reached goal: 997.               Steps done: 7117366. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000729810595535.\n",
      " state (0)  A[0]:(0.531492233276) A[1]:(0.590470612049) A[2]:(0.590489864349) A[3]:(0.530826926231)\n",
      " state (1)  A[0]:(0.531614661217) A[1]:(-5.02243638039e-05) A[2]:(0.656090021133) A[3]:(0.590003967285)\n",
      " state (2)  A[0]:(0.590782642365) A[1]:(0.729000747204) A[2]:(0.590487420559) A[3]:(0.655888080597)\n",
      " state (3)  A[0]:(0.656403303146) A[1]:(-0.21865105629) A[2]:(0.539867758751) A[3]:(0.51742875576)\n",
      " state (4)  A[0]:(0.590498924255) A[1]:(0.656070113182) A[2]:(-0.000124931335449) A[3]:(0.532047510147)\n",
      " state (5)  A[0]:(0.163520351052) A[1]:(0.928591668606) A[2]:(-0.195305451751) A[3]:(0.527597308159)\n",
      " state (6)  A[0]:(-0.000396311254008) A[1]:(0.809981942177) A[2]:(6.43730163574e-06) A[3]:(0.656060218811)\n",
      " state (7)  A[0]:(0.624558329582) A[1]:(-0.247841566801) A[2]:(0.309602230787) A[3]:(0.881387650967)\n",
      " state (8)  A[0]:(0.656662464142) A[1]:(-9.5821917057e-05) A[2]:(0.72901725769) A[3]:(0.58998644352)\n",
      " state (9)  A[0]:(0.656415224075) A[1]:(0.809981822968) A[2]:(0.809985518456) A[3]:(-0.000817790452857)\n",
      " state (10)  A[0]:(0.729101419449) A[1]:(0.899999201298) A[2]:(-0.00033164024353) A[3]:(0.728846371174)\n",
      " state (11)  A[0]:(0.524081349373) A[1]:(0.876612961292) A[2]:(-0.626711606979) A[3]:(0.845250964165)\n",
      " state (12)  A[0]:(0.081029176712) A[1]:(0.823730230331) A[2]:(-0.624711513519) A[3]:(0.794836342335)\n",
      " state (13)  A[0]:(-0.000270247459412) A[1]:(0.80831348896) A[2]:(0.9000005126) A[3]:(0.729404091835)\n",
      " state (14)  A[0]:(0.809866189957) A[1]:(0.900053441525) A[2]:(1.0) A[3]:(0.810139417648)\n",
      " state (15)  A[0]:(0.981880843639) A[1]:(0.955817878246) A[2]:(1.0) A[3]:(0.875002205372)\n",
      "Episode 993000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 1000.               Steps done: 7123367. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000725444116874.\n",
      " state (0)  A[0]:(0.530647158623) A[1]:(0.590517282486) A[2]:(0.590466141701) A[3]:(0.531337738037)\n",
      " state (1)  A[0]:(0.530918955803) A[1]:(2.72989273071e-05) A[2]:(0.656101942062) A[3]:(0.590282201767)\n",
      " state (2)  A[0]:(0.590326666832) A[1]:(0.72907936573) A[2]:(0.590652644634) A[3]:(0.656033039093)\n",
      " state (3)  A[0]:(0.656540572643) A[1]:(-0.218514978886) A[2]:(0.540170431137) A[3]:(0.517426252365)\n",
      " state (4)  A[0]:(0.591108560562) A[1]:(0.656203746796) A[2]:(0.000241279602051) A[3]:(0.531968891621)\n",
      " state (5)  A[0]:(0.16476047039) A[1]:(0.928667724133) A[2]:(-0.195097208023) A[3]:(0.527536511421)\n",
      " state (6)  A[0]:(0.000572085322347) A[1]:(0.810186743736) A[2]:(0.000297546386719) A[3]:(0.656078398228)\n",
      " state (7)  A[0]:(0.624412178993) A[1]:(-0.247319623828) A[2]:(0.310095250607) A[3]:(0.881431937218)\n",
      " state (8)  A[0]:(0.656135618687) A[1]:(0.000455796689494) A[2]:(0.729316353798) A[3]:(0.590161800385)\n",
      " state (9)  A[0]:(0.655796408653) A[1]:(0.810139715672) A[2]:(0.810168385506) A[3]:(-0.000309988856316)\n",
      " state (10)  A[0]:(0.728607416153) A[1]:(0.900078296661) A[2]:(0.000170230865479) A[3]:(0.729109048843)\n",
      " state (11)  A[0]:(0.523365318775) A[1]:(0.876710057259) A[2]:(-0.626379132271) A[3]:(0.845394730568)\n",
      " state (12)  A[0]:(0.080169044435) A[1]:(0.82387727499) A[2]:(-0.624258577824) A[3]:(0.794994771481)\n",
      " state (13)  A[0]:(-0.000804066483397) A[1]:(0.808505237103) A[2]:(0.90030092001) A[3]:(0.729580640793)\n",
      " state (14)  A[0]:(0.809884548187) A[1]:(0.900183796883) A[2]:(1.0) A[3]:(0.810242474079)\n",
      " state (15)  A[0]:(0.981885254383) A[1]:(0.955876290798) A[2]:(1.0) A[3]:(0.875014722347)\n",
      "Episode 994000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5999. Times reached goal: 998.               Steps done: 7129366. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000721105205195.\n",
      "q_values \n",
      "tensor([[ 0.5324,  0.5903,  0.5904,  0.5321]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5324,  0.0001,  0.6560,  0.5907]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5916,  0.7290,  0.5902,  0.6564]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0022,  0.8100,  0.0002,  0.6565]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0004,  0.7286]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8097,  0.9002,  1.0000,  0.8096]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532439231873) A[1]:(0.590313017368) A[2]:(0.590410470963) A[3]:(0.532162308693)\n",
      " state (1)  A[0]:(0.53242623806) A[1]:(1.05798244476e-06) A[2]:(0.656009674072) A[3]:(0.590723574162)\n",
      " state (2)  A[0]:(0.59159219265) A[1]:(0.72901648283) A[2]:(0.590278685093) A[3]:(0.656364440918)\n",
      " state (3)  A[0]:(0.657535731792) A[1]:(-0.217934012413) A[2]:(0.539679825306) A[3]:(0.51802945137)\n",
      " state (4)  A[0]:(0.592108249664) A[1]:(0.65606212616) A[2]:(-0.000222086906433) A[3]:(0.532518863678)\n",
      " state (5)  A[0]:(0.166137218475) A[1]:(0.928611516953) A[2]:(-0.195464476943) A[3]:(0.528056025505)\n",
      " state (6)  A[0]:(0.00218397029676) A[1]:(0.809998273849) A[2]:(-4.52995300293e-06) A[3]:(0.65640604496)\n",
      " state (7)  A[0]:(0.625680446625) A[1]:(-0.247876390815) A[2]:(0.309841275215) A[3]:(0.881473004818)\n",
      " state (8)  A[0]:(0.657301545143) A[1]:(-6.94841146469e-05) A[2]:(0.729136586189) A[3]:(0.589978694916)\n",
      " state (9)  A[0]:(0.656803071499) A[1]:(0.81000995636) A[2]:(0.810083031654) A[3]:(-0.00137512304354)\n",
      " state (10)  A[0]:(0.729295790195) A[1]:(0.900033056736) A[2]:(2.88486480713e-05) A[3]:(0.72848546505)\n",
      " state (11)  A[0]:(0.524252355099) A[1]:(0.876677036285) A[2]:(-0.626464486122) A[3]:(0.845007359982)\n",
      " state (12)  A[0]:(0.0810991078615) A[1]:(0.823854804039) A[2]:(-0.624473810196) A[3]:(0.794480443001)\n",
      " state (13)  A[0]:(-0.000412166089518) A[1]:(0.808488130569) A[2]:(0.900020301342) A[3]:(0.728856801987)\n",
      " state (14)  A[0]:(0.809735715389) A[1]:(0.900175452232) A[2]:(1.0) A[3]:(0.809616208076)\n",
      " state (15)  A[0]:(0.981865644455) A[1]:(0.95589107275) A[2]:(1.0) A[3]:(0.874574303627)\n",
      "Episode 995000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 1000.               Steps done: 7135371. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000716787943988.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532827734947) A[1]:(0.590501189232) A[2]:(0.590471088886) A[3]:(0.531642198563)\n",
      " state (1)  A[0]:(0.53241455555) A[1]:(0.000198662281036) A[2]:(0.6560972929) A[3]:(0.590085268021)\n",
      " state (2)  A[0]:(0.591420412064) A[1]:(0.728976547718) A[2]:(0.590568244457) A[3]:(0.655997395515)\n",
      " state (3)  A[0]:(0.657168030739) A[1]:(-0.218171328306) A[2]:(0.53993088007) A[3]:(0.519045472145)\n",
      " state (4)  A[0]:(0.591712117195) A[1]:(0.655946612358) A[2]:(-0.000321984291077) A[3]:(0.534364104271)\n",
      " state (5)  A[0]:(0.16584572196) A[1]:(0.928639888763) A[2]:(-0.196107491851) A[3]:(0.530310571194)\n",
      " state (6)  A[0]:(0.00261014117859) A[1]:(0.809897780418) A[2]:(-0.000926136679482) A[3]:(0.658368945122)\n",
      " state (7)  A[0]:(0.625910520554) A[1]:(-0.248257100582) A[2]:(0.308763355017) A[3]:(0.882060527802)\n",
      " state (8)  A[0]:(0.65710747242) A[1]:(-0.000345528125763) A[2]:(0.728600144386) A[3]:(0.59000146389)\n",
      " state (9)  A[0]:(0.656688213348) A[1]:(0.809836506844) A[2]:(0.809769392014) A[3]:(-0.00410469528288)\n",
      " state (10)  A[0]:(0.729417741299) A[1]:(0.899904549122) A[2]:(-0.00141656305641) A[3]:(0.727071642876)\n",
      " state (11)  A[0]:(0.524594068527) A[1]:(0.876455843449) A[2]:(-0.627583384514) A[3]:(0.844263672829)\n",
      " state (12)  A[0]:(0.0817557424307) A[1]:(0.823448061943) A[2]:(-0.625467896461) A[3]:(0.793684124947)\n",
      " state (13)  A[0]:(0.000869929557666) A[1]:(0.807976603508) A[2]:(0.90005928278) A[3]:(0.728169739246)\n",
      " state (14)  A[0]:(0.810509979725) A[1]:(0.899865865707) A[2]:(1.0) A[3]:(0.809483587742)\n",
      " state (15)  A[0]:(0.981946229935) A[1]:(0.955711185932) A[2]:(1.0) A[3]:(0.874675393105)\n",
      "Episode 996000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5995. Times reached goal: 998.               Steps done: 7141366. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000712503655251.\n",
      " state (0)  A[0]:(0.530863940716) A[1]:(0.590602636337) A[2]:(0.590543150902) A[3]:(0.531518340111)\n",
      " state (1)  A[0]:(0.530666470528) A[1]:(5.79580664635e-05) A[2]:(0.65619969368) A[3]:(0.590565562248)\n",
      " state (2)  A[0]:(0.589925050735) A[1]:(0.729006171227) A[2]:(0.590839505196) A[3]:(0.656374275684)\n",
      " state (3)  A[0]:(0.655936360359) A[1]:(-0.218680217862) A[2]:(0.540307223797) A[3]:(0.517939865589)\n",
      " state (4)  A[0]:(0.590294003487) A[1]:(0.656048297882) A[2]:(0.000315427780151) A[3]:(0.532521367073)\n",
      " state (5)  A[0]:(0.163652896881) A[1]:(0.92862457037) A[2]:(-0.195147097111) A[3]:(0.528124094009)\n",
      " state (6)  A[0]:(0.000337779521942) A[1]:(0.809950768948) A[2]:(0.000121355056763) A[3]:(0.656493365765)\n",
      " state (7)  A[0]:(0.624969959259) A[1]:(-0.248044684529) A[2]:(0.309690624475) A[3]:(0.881471097469)\n",
      " state (8)  A[0]:(0.656666338444) A[1]:(-0.000119425356388) A[2]:(0.729094564915) A[3]:(0.590053796768)\n",
      " state (9)  A[0]:(0.656297087669) A[1]:(0.809947550297) A[2]:(0.81004679203) A[3]:(-0.000105157494545)\n",
      " state (10)  A[0]:(0.729011893272) A[1]:(0.899967730045) A[2]:(-0.000318288803101) A[3]:(0.729566037655)\n",
      " state (11)  A[0]:(0.523926496506) A[1]:(0.876550376415) A[2]:(-0.626770853996) A[3]:(0.84579885006)\n",
      " state (12)  A[0]:(0.0808549672365) A[1]:(0.823607206345) A[2]:(-0.624765634537) A[3]:(0.795603513718)\n",
      " state (13)  A[0]:(-0.000196814537048) A[1]:(0.808151245117) A[2]:(0.900018215179) A[3]:(0.730406403542)\n",
      " state (14)  A[0]:(0.810071408749) A[1]:(0.899953007698) A[2]:(1.0) A[3]:(0.810798585415)\n",
      " state (15)  A[0]:(0.981922984123) A[1]:(0.955767631531) A[2]:(1.0) A[3]:(0.87541115284)\n",
      "Episode 997000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 998.               Steps done: 7147367. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000708240724532.\n",
      " state (0)  A[0]:(0.533610880375) A[1]:(0.590662896633) A[2]:(0.590609908104) A[3]:(0.529627501965)\n",
      " state (1)  A[0]:(0.532265067101) A[1]:(-0.000445440382464) A[2]:(0.656160593033) A[3]:(0.589444696903)\n",
      " state (2)  A[0]:(0.590299129486) A[1]:(0.72907948494) A[2]:(0.590899944305) A[3]:(0.655909538269)\n",
      " state (3)  A[0]:(0.655043959618) A[1]:(-0.21915525198) A[2]:(0.540398716927) A[3]:(0.518741965294)\n",
      " state (4)  A[0]:(0.587474822998) A[1]:(0.656193375587) A[2]:(0.000652789953165) A[3]:(0.534147024155)\n",
      " state (5)  A[0]:(0.15666282177) A[1]:(0.928530037403) A[2]:(-0.194361820817) A[3]:(0.529927611351)\n",
      " state (6)  A[0]:(-0.0102120963857) A[1]:(0.81000328064) A[2]:(0.000409483880503) A[3]:(0.657900929451)\n",
      " state (7)  A[0]:(0.617297887802) A[1]:(-0.247653514147) A[2]:(0.309226661921) A[3]:(0.882259726524)\n",
      " state (8)  A[0]:(0.649401307106) A[1]:(-0.00026823580265) A[2]:(0.728621840477) A[3]:(0.593502700329)\n",
      " state (9)  A[0]:(0.648191332817) A[1]:(0.809922099113) A[2]:(0.809545874596) A[3]:(0.00615073414519)\n",
      " state (10)  A[0]:(0.72147744894) A[1]:(0.899942994118) A[2]:(-0.00155436864588) A[3]:(0.732127189636)\n",
      " state (11)  A[0]:(0.511317491531) A[1]:(0.876544713974) A[2]:(-0.627467393875) A[3]:(0.847165644169)\n",
      " state (12)  A[0]:(0.0628225281835) A[1]:(0.823687970638) A[2]:(-0.625547885895) A[3]:(0.797292649746)\n",
      " state (13)  A[0]:(-0.0189842116088) A[1]:(0.808379709721) A[2]:(0.899711251259) A[3]:(0.732574462891)\n",
      " state (14)  A[0]:(0.803332209587) A[1]:(0.900177359581) A[2]:(1.0) A[3]:(0.812447249889)\n",
      " state (15)  A[0]:(0.981214225292) A[1]:(0.955927968025) A[2]:(1.0) A[3]:(0.876609742641)\n",
      "Episode 998000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 998.               Steps done: 7153370. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000704001891051.\n",
      " state (0)  A[0]:(0.532232642174) A[1]:(0.590535163879) A[2]:(0.590599656105) A[3]:(0.53161072731)\n",
      " state (1)  A[0]:(0.531455099583) A[1]:(2.10851430893e-06) A[2]:(0.656142294407) A[3]:(0.590364098549)\n",
      " state (2)  A[0]:(0.590451955795) A[1]:(0.729061365128) A[2]:(0.590556740761) A[3]:(0.656058132648)\n",
      " state (3)  A[0]:(0.656129479408) A[1]:(-0.217798218131) A[2]:(0.539659380913) A[3]:(0.517355561256)\n",
      " state (4)  A[0]:(0.590489506721) A[1]:(0.656263232231) A[2]:(-0.000526189745869) A[3]:(0.531800746918)\n",
      " state (5)  A[0]:(0.164217039943) A[1]:(0.928669154644) A[2]:(-0.195792704821) A[3]:(0.52738404274)\n",
      " state (6)  A[0]:(0.000857233768329) A[1]:(0.810073494911) A[2]:(-0.000326156616211) A[3]:(0.656018137932)\n",
      " state (7)  A[0]:(0.624892950058) A[1]:(-0.247633144259) A[2]:(0.309447467327) A[3]:(0.88139128685)\n",
      " state (8)  A[0]:(0.656307697296) A[1]:(0.000501923204865) A[2]:(0.728931665421) A[3]:(0.589835226536)\n",
      " state (9)  A[0]:(0.655518293381) A[1]:(0.810120940208) A[2]:(0.809958636761) A[3]:(-0.00121550203767)\n",
      " state (10)  A[0]:(0.728161633015) A[1]:(0.900018692017) A[2]:(0.000266790390015) A[3]:(0.728368401527)\n",
      " state (11)  A[0]:(0.522620797157) A[1]:(0.8765822649) A[2]:(-0.626082539558) A[3]:(0.844834685326)\n",
      " state (12)  A[0]:(0.0792094618082) A[1]:(0.823609769344) A[2]:(-0.624183535576) A[3]:(0.794230520725)\n",
      " state (13)  A[0]:(-0.00213551195338) A[1]:(0.808058798313) A[2]:(0.899809718132) A[3]:(0.728472530842)\n",
      " state (14)  A[0]:(0.809145450592) A[1]:(0.899829983711) A[2]:(1.0) A[3]:(0.80913579464)\n",
      " state (15)  A[0]:(0.981844246387) A[1]:(0.955708026886) A[2]:(1.0) A[3]:(0.874212503433)\n",
      "Episode 999000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 1000.               Steps done: 7159378. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.000699784928131.\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "# TO BE DELETED_______\n",
    "#steps_done = 40000000\n",
    "#EPS_END = 0.0\n",
    "\n",
    "# TO BE DELETED_______\n",
    "\n",
    "\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    episode_series = []\n",
    "    reward = 0\n",
    "    episode_step = 0\n",
    "    while not done:\n",
    "        # Get action from pi\n",
    "        # action = env.action_space.sample()\n",
    "        np_observation = np.array(observation)\n",
    "        #np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        observation_tensor = FloatTensor(np_observation)\n",
    "        #print(observation_tensor)\n",
    "        #net.eval()\n",
    "        #print(\"before eval\")\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        steps_done += 1\n",
    "        q_values = online_net(observation_tensor)\n",
    "        if sample >= eps_threshold: \n",
    "            #print \"observation_tensor\"\n",
    "            #print observation_tensor.type()\n",
    "            \n",
    "            action = q_values.max(1)[1] # First 1 is the dimension, second 1 is the index (this is argmax)\n",
    "        else:\n",
    "            action = torch.LongTensor([[random.randint(0,3)]], device=device)\n",
    "        \n",
    "            \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        old_state = observation                    \n",
    "            \n",
    "        \n",
    "        observation, reward, done, info = env.step(action.item()) \n",
    "        new_state = observation\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#         if done and reward != 1.0:\n",
    "#             if episode_step > 50:\n",
    "#                 reward = -0.2\n",
    "#             if episode_step > 80:\n",
    "#                 reward = -0.5\n",
    "#             if new_state in [5,7,11,12]:\n",
    "#                 reward = -1.0\n",
    "\n",
    "        \n",
    "        \n",
    "        # Store the transition in memory\n",
    "        #if old_state != new_state:\n",
    "\n",
    "        memory.push(old_state, action, new_state, reward, done)\n",
    "        if k%5000 == 0:\n",
    "            #print(\"old_state != new_state\")\n",
    "            #print(old_state != new_state)\n",
    "            #print(\"oldstate \" + str(old_state) + \" newstate \" + str(new_state))\n",
    "            print(\"q_values \")\n",
    "            print(q_values)\n",
    "            print(\"On state=\"+ str(old_state) + \", selected action=\" + str(action.item()) + \" , \" + \\\n",
    "              \"Random? \" + str( sample < eps_threshold ))\n",
    "            print(\"new state=\"+ str(new_state) + \", done=\"+str(done) + \\\n",
    "             \". Reward: \" + str(reward))\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        if k > BATCH_SIZE :\n",
    "            optimize_model()\n",
    "            times_trained = times_trained + 1\n",
    "\n",
    "        episode_step += 1\n",
    "        #env.render()\n",
    "    if k % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "    \n",
    "    if k % 1000 ==0:\n",
    "        print_q_table()\n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "\n",
    "    if k%1000 == 0:\n",
    "        print(\"Episode {} finished after {} timesteps with r={}. Running score: {}. Times trained: \\\n",
    "              {}. Times reached goal: {}. \\\n",
    "              Steps done: {}. EPS_DECAY: {}. EPS_THRESHOLD: {}.\".format(k, len(episode_series), \\\n",
    "                                                                    reward, np.mean(score), times_trained, \\\n",
    "                                                                       times_reach_goal, steps_done, \\\n",
    "                                                                       EPS_DECAY, eps_threshold))\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "        #print(\"Game finished. \" + \"-\" * 5)\n",
    "        #print(len(episode_series))\n",
    "#         for param in net.parameters():\n",
    "#             print(param.data)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if reward > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(-0.016590623185) A[1]:(-0.0379296652973) A[2]:(0.00880737882107) A[3]:(0.0203321184963)\n",
      " state (1)  A[0]:(-0.0167372319847) A[1]:(-0.0366442278028) A[2]:(0.0104934358969) A[3]:(0.0212363991886)\n",
      " state (2)  A[0]:(-0.0168837141246) A[1]:(-0.0353712290525) A[2]:(0.0121734226122) A[3]:(0.022135829553)\n",
      " state (3)  A[0]:(-0.0170289520174) A[1]:(-0.0341202318668) A[2]:(0.0138428043574) A[3]:(0.0230262875557)\n",
      " state (4)  A[0]:(-0.0171718932688) A[1]:(-0.0329002924263) A[2]:(0.0154972476885) A[3]:(0.0239038150758)\n",
      " state (5)  A[0]:(-0.0173115506768) A[1]:(-0.0317197963595) A[2]:(0.0171326845884) A[3]:(0.0247646775097)\n",
      " state (6)  A[0]:(-0.0174470487982) A[1]:(-0.0305862799287) A[2]:(0.018745386973) A[3]:(0.0256054457277)\n",
      " state (7)  A[0]:(-0.0175776146352) A[1]:(-0.0295063517988) A[2]:(0.0203320086002) A[3]:(0.0264229979366)\n",
      " state (8)  A[0]:(-0.0177025925368) A[1]:(-0.0284855756909) A[2]:(0.0218896120787) A[3]:(0.0272145923227)\n",
      " state (9)  A[0]:(-0.0178214609623) A[1]:(-0.0275284443051) A[2]:(0.0234157033265) A[3]:(0.0279778521508)\n",
      " state (10)  A[0]:(-0.017933819443) A[1]:(-0.0266383886337) A[2]:(0.0249082185328) A[3]:(0.0287107806653)\n",
      " state (11)  A[0]:(-0.0180393848568) A[1]:(-0.0258177798241) A[2]:(0.0263655278832) A[3]:(0.0294117573649)\n",
      " state (12)  A[0]:(-0.0181379895657) A[1]:(-0.0250680148602) A[2]:(0.0277864057571) A[3]:(0.0300795268267)\n",
      " state (13)  A[0]:(-0.0182295646518) A[1]:(-0.0243895780295) A[2]:(0.0291700139642) A[3]:(0.0307131670415)\n",
      " state (14)  A[0]:(-0.0183141361922) A[1]:(-0.023782145232) A[2]:(0.0305158682168) A[3]:(0.0313120670617)\n",
      " state (15)  A[0]:(-0.0183917991817) A[1]:(-0.023244664073) A[2]:(0.0318237841129) A[3]:(0.0318759083748)\n",
      " state (0)  A[0]:(-0.016590623185) A[1]:(-0.0379296652973) A[2]:(0.00880737882107) A[3]:(0.0203321184963)\n",
      " state (1)  A[0]:(-0.0167372319847) A[1]:(-0.0366442278028) A[2]:(0.0104934358969) A[3]:(0.0212363991886)\n",
      " state (2)  A[0]:(-0.0168837141246) A[1]:(-0.0353712290525) A[2]:(0.0121734226122) A[3]:(0.022135829553)\n",
      " state (3)  A[0]:(-0.0170289520174) A[1]:(-0.0341202318668) A[2]:(0.0138428043574) A[3]:(0.0230262875557)\n",
      " state (4)  A[0]:(-0.0171718932688) A[1]:(-0.0329002924263) A[2]:(0.0154972476885) A[3]:(0.0239038150758)\n",
      " state (5)  A[0]:(-0.0173115506768) A[1]:(-0.0317197963595) A[2]:(0.0171326845884) A[3]:(0.0247646775097)\n",
      " state (6)  A[0]:(-0.0174470487982) A[1]:(-0.0305862799287) A[2]:(0.018745386973) A[3]:(0.0256054457277)\n",
      " state (7)  A[0]:(-0.0175776146352) A[1]:(-0.0295063517988) A[2]:(0.0203320086002) A[3]:(0.0264229979366)\n",
      " state (8)  A[0]:(-0.0177025925368) A[1]:(-0.0284855756909) A[2]:(0.0218896120787) A[3]:(0.0272145923227)\n",
      " state (9)  A[0]:(-0.0178214609623) A[1]:(-0.0275284443051) A[2]:(0.0234157033265) A[3]:(0.0279778521508)\n",
      " state (10)  A[0]:(-0.017933819443) A[1]:(-0.0266383886337) A[2]:(0.0249082185328) A[3]:(0.0287107806653)\n",
      " state (11)  A[0]:(-0.0180393848568) A[1]:(-0.0258177798241) A[2]:(0.0263655278832) A[3]:(0.0294117573649)\n",
      " state (12)  A[0]:(-0.0181379895657) A[1]:(-0.0250680148602) A[2]:(0.0277864057571) A[3]:(0.0300795268267)\n",
      " state (13)  A[0]:(-0.0182295646518) A[1]:(-0.0243895780295) A[2]:(0.0291700139642) A[3]:(0.0307131670415)\n",
      " state (14)  A[0]:(-0.0183141361922) A[1]:(-0.023782145232) A[2]:(0.0305158682168) A[3]:(0.0313120670617)\n",
      " state (15)  A[0]:(-0.0183917991817) A[1]:(-0.023244664073) A[2]:(0.0318237841129) A[3]:(0.0318759083748)\n"
     ]
    }
   ],
   "source": [
    "def print_q_table():\n",
    "    for i in range(16):\n",
    "        st = np.array(i)\n",
    "        st = np.expand_dims(st, axis=0)\n",
    "        q_vals = online_net(FloatTensor(st))\n",
    "        outp = \" state (\" +str(i) + \") \"\n",
    "        n = 0\n",
    "        for tensr in q_vals:\n",
    "            for cell in tensr:\n",
    "                outp = outp + \" A[\" + str(n) + \"]:(\" + str(cell.item()) + \")\"\n",
    "                n += 1\n",
    "        print(outp)\n",
    "        \n",
    "print_q_table()\n",
    "print_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
