{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "# register(\n",
    "#    id='FrozenLakeNotSlippery-v0',\n",
    "#    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "#    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "#    max_episode_steps=100,\n",
    "#    reward_threshold=0.78, # optimum = .8196\n",
    "# )\n",
    "\n",
    "#env = gym.make('FrozenLake8x8-v0')\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.render()\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "class q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(q_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 20)\n",
    "        self.linear2 = nn.Linear(20, 4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "#         print(x.shape)\n",
    "#         print(x)\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "        x = x.view(-1,1)\n",
    "        x = F.tanh(self.linear1(x))\n",
    "        #x = F.softmax(self.linear2(x), dim=0)\n",
    "        x = F.tanh(self.linear2(x))\n",
    "        x = x.view(-1,4)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1, out_features=20, bias=True)\n",
      "Linear(in_features=20, out_features=4, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "random.seed(1999)\n",
    "import math\n",
    "\n",
    "# custom weights initialization \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print classname\n",
    "    #print q_net\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        if not m.bias is None:\n",
    "            m.bias.data.normal_(0.0, 0.02)\n",
    "        #m.weight.data.uniform_(0.0, 0.02)        \n",
    "        #m.weight.data.fill_(0.1)\n",
    "        #if not m.bias is None:\n",
    "        #    m.bias.data.fill_(0.1)\n",
    "        print m\n",
    "        \n",
    "\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mse = nn.MSELoss(reduce=False)\n",
    "NUM_EPISODES = 1000000\n",
    "BATCH_SIZE = 500\n",
    "GAMMA = 0.9\n",
    "TARGET_UPDATE = 50\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.0\n",
    "EPS_DECAY = 1000000\n",
    "online_net = q_net().to(device)\n",
    "online_net.apply(weights_init)\n",
    "target_net = q_net().to(device)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "memory = ReplayMemory(100000)\n",
    "#optimizer = optim.RMSprop(online_net.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(online_net.parameters(), lr=0.0001)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda d: d is False,\n",
    "    #                                      batch.done)), device=device, dtype=torch.uint8).unsqueeze(1)\n",
    "    # Compute states that are final.\n",
    "    next_state_final_mask = torch.tensor(tuple(map(lambda d: d in [5,7,11,12],\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8).unsqueeze(1) \n",
    "    next_state_finak_list = [d for d in batch.next_state if d in [5,7,11,12] ]\n",
    "    \n",
    "    \n",
    "    #non_final_next_states = torch.cat([FloatTensor([s]) for s,d in zip(batch.next_state,batch.done)\n",
    "    #                                            if d is False])\n",
    "    \n",
    "    #state_batch = torch.cat([torch.FloatTensor([s]) for s in batch.state])\n",
    "    state_batch = FloatTensor(batch.state)\n",
    "    state_batch = state_batch.view(BATCH_SIZE, 1)\n",
    "    next_state_batch = FloatTensor(batch.next_state)\n",
    "    next_state_batch = next_state_batch.view(BATCH_SIZE, 1)\n",
    "    #action_batch = torch.cat([torch.LongTensor([[a.item()]]) for a in batch.action])\n",
    "    action_batch = LongTensor(batch.action).view(BATCH_SIZE,1)\n",
    "    #reward_batch = torch.cat([torch.tensor([r]) for r in batch.reward])\n",
    "    reward_batch = Tensor(batch.reward).view(BATCH_SIZE,1)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "#     print(\"state_batch \"+\"-\" * 10)\n",
    "#     print(state_batch.shape)\n",
    "#     print(\"action_batch \"+\"-\" * 10)\n",
    "#     print(action_batch.shape)\n",
    "    state_action_values = online_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # next_state_values = torch.zeros(BATCH_SIZE, device=device).view(BATCH_SIZE,1)\n",
    "#     print(\"non_final_mask\")\n",
    "#     print(non_final_mask.shape)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # DDQN ------------------------\n",
    "    # Y_ddqn = R_next + GAMMA * Q_target(S_next, argmax_a Q_online(S_next, a) )\n",
    "    # Below is the argmax_a Q_online(S_next,a)\n",
    "    argmax_a_online_net_next_state = online_net(next_state_batch).max(1)[1].detach().view(BATCH_SIZE,1)\n",
    "    \n",
    "    # Below is the actual Q_target\n",
    "    next_state_values = target_net(next_state_batch).detach().gather(1, argmax_a_online_net_next_state)\n",
    "    # DDQN ------------------------\n",
    "#     print \"next_state_values\"\n",
    "#     print next_state_values.shape\n",
    "#     print next_state_values.type()\n",
    "#     print \"final_mask\"\n",
    "#     print final_mask.shape\n",
    "#     print  final_mask.type()\n",
    "#     print \"next_state_values[final_mask]\"\n",
    "#     print next_state_values[final_mask].shape\n",
    "#     print next_state_values[final_mask].type()\n",
    "    next_state_values[next_state_final_mask] = torch.zeros(len(next_state_finak_list), device=device).view(len(next_state_finak_list))\n",
    "#     print(\"next_state_values\")\n",
    "#     print(next_state_values.shape)\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "#     print(\"expected_state_action_values\")\n",
    "#     print(expected_state_action_values.shape)\n",
    "\n",
    "    # Compute Huber loss (this is like MSE , but less sensitive to outliers )\n",
    "    # loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    #loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    loss = mse( state_action_values, expected_state_action_values)\n",
    "    debug = False\n",
    "    if debug:\n",
    "        print(\"-\" * 40)\n",
    "        print(\"States\")\n",
    "        print(state_batch)\n",
    "        print(\"Target Q\")\n",
    "        print(expected_state_action_values)\n",
    "        print(expected_state_action_values.shape)\n",
    "        print(\"Actual Q\")\n",
    "        print(state_action_values)\n",
    "        print(state_action_values.shape)\n",
    "        print(\"Loss\")\n",
    "        print(loss)\n",
    "        print(loss.shape)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(torch.ones(len(state_action_values), device=device).unsqueeze(1))\n",
    "    \n",
    "    # param in online_net.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if debug: \n",
    "        print(\"AFTER OPTIMIZATION:\")\n",
    "        print(\"New actual Q\")\n",
    "        print(online_net(state_batch).gather(1, action_batch))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 4.6772,  1.5874, -1.5961,  1.0966]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 4.6772,  1.5874, -1.5961,  1.0966]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 4.5986,  1.3899, -1.2254,  1.0709]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 4.5202,  1.1923, -0.8550,  1.0452]], device='cuda:0')\n",
      "On state=2, selected action=3 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 4.5202,  1.1923, -0.8550,  1.0452]], device='cuda:0')\n",
      "On state=2, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 4.4419,  0.9945, -0.4851,  1.0197]], device='cuda:0')\n",
      "On state=3, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 4.4419,  0.9945, -0.4851,  1.0197]], device='cuda:0')\n",
      "On state=3, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 4.4419,  0.9945, -0.4851,  1.0197]], device='cuda:0')\n",
      "On state=3, selected action=3 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 4.4419,  0.9945, -0.4851,  1.0197]], device='cuda:0')\n",
      "On state=3, selected action=0 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 4.5202,  1.1923, -0.8550,  1.0452]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 4.2102,  0.3992,  0.6172,  0.9459]], device='cuda:0')\n",
      "On state=6, selected action=0 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.0467716865242) A[1]:(0.0158739089966) A[2]:(-0.015961188823) A[3]:(0.0109659750015)\n",
      " state (1)  A[0]:(0.0459862612188) A[1]:(0.0138985225931) A[2]:(-0.0122543023899) A[3]:(0.0107089262456)\n",
      " state (2)  A[0]:(0.045201510191) A[1]:(0.0119225373492) A[2]:(-0.00854972098023) A[3]:(0.0104518923908)\n",
      " state (3)  A[0]:(0.0444191098213) A[1]:(0.00994489248842) A[2]:(-0.00485114799812) A[3]:(0.0101966680959)\n",
      " state (4)  A[0]:(0.0436407215893) A[1]:(0.00796458125114) A[2]:(-0.00116223143414) A[3]:(0.00994502007961)\n",
      " state (5)  A[0]:(0.0428679808974) A[1]:(0.00598067371175) A[2]:(0.00251346104778) A[3]:(0.00969867687672)\n",
      " state (6)  A[0]:(0.0421024933457) A[1]:(0.0039923498407) A[2]:(0.0061724819243) A[3]:(0.00945930648595)\n",
      " state (7)  A[0]:(0.0413458012044) A[1]:(0.00199890718795) A[2]:(0.00981149822474) A[3]:(0.00922849774361)\n",
      " state (8)  A[0]:(0.0405994020402) A[1]:(-2.03028321266e-07) A[2]:(0.0134273516014) A[3]:(0.00900775659829)\n",
      " state (9)  A[0]:(0.0398647263646) A[1]:(-0.00200537685305) A[2]:(0.0170170515776) A[3]:(0.00879848189652)\n",
      " state (10)  A[0]:(0.0391431190073) A[1]:(-0.00401683431119) A[2]:(0.020577788353) A[3]:(0.00860196724534)\n",
      " state (11)  A[0]:(0.0384358540177) A[1]:(-0.00603463267908) A[2]:(0.0241069570184) A[3]:(0.00841938331723)\n",
      " state (12)  A[0]:(0.0377441160381) A[1]:(-0.00805865135044) A[2]:(0.0276021603495) A[3]:(0.00825177785009)\n",
      " state (13)  A[0]:(0.0370689891279) A[1]:(-0.0100885927677) A[2]:(0.0310611948371) A[3]:(0.00810006540269)\n",
      " state (14)  A[0]:(0.0364114679396) A[1]:(-0.0121240094304) A[2]:(0.0344820953906) A[3]:(0.00796503666788)\n",
      " state (15)  A[0]:(0.035772446543) A[1]:(-0.0141642903909) A[2]:(0.0378630720079) A[3]:(0.00784734543413)\n",
      "Episode 0 finished after 0 timesteps with r=0.0. Running score: 0.0. Times trained:               0. Times reached goal: 0.               Steps done: 11. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.899991000045.\n",
      " state (0)  A[0]:(0.0609212853014) A[1]:(0.0706380009651) A[2]:(0.0387419946492) A[3]:(0.0611391291022)\n",
      " state (1)  A[0]:(0.066632039845) A[1]:(0.0747750476003) A[2]:(0.0512385368347) A[3]:(0.0624569766223)\n",
      " state (2)  A[0]:(0.0723287314177) A[1]:(0.0788986235857) A[2]:(0.0637034401298) A[3]:(0.0637660697103)\n",
      " state (3)  A[0]:(0.0780070945621) A[1]:(0.083006054163) A[2]:(0.0761259049177) A[3]:(0.0650658905506)\n",
      " state (4)  A[0]:(0.0836629271507) A[1]:(0.087094694376) A[2]:(0.0884952843189) A[3]:(0.0663559809327)\n",
      " state (5)  A[0]:(0.0892920941114) A[1]:(0.0911619365215) A[2]:(0.100801132619) A[3]:(0.06763587147)\n",
      " state (6)  A[0]:(0.094890512526) A[1]:(0.0952052772045) A[2]:(0.113033249974) A[3]:(0.0689051225781)\n",
      " state (7)  A[0]:(0.100454241037) A[1]:(0.0992222651839) A[2]:(0.12518170476) A[3]:(0.0701633095741)\n",
      " state (8)  A[0]:(0.105979420245) A[1]:(0.103210508823) A[2]:(0.137236908078) A[3]:(0.0714100450277)\n",
      " state (9)  A[0]:(0.111462280154) A[1]:(0.107167705894) A[2]:(0.149189561605) A[3]:(0.0726449787617)\n",
      " state (10)  A[0]:(0.11689927429) A[1]:(0.111091665924) A[2]:(0.16103079915) A[3]:(0.0738677158952)\n",
      " state (11)  A[0]:(0.122286871076) A[1]:(0.114980243146) A[2]:(0.172752171755) A[3]:(0.0750779509544)\n",
      " state (12)  A[0]:(0.127621784806) A[1]:(0.118831403553) A[2]:(0.184345543385) A[3]:(0.0762754008174)\n",
      " state (13)  A[0]:(0.132900848985) A[1]:(0.122643247247) A[2]:(0.195803388953) A[3]:(0.0774597376585)\n",
      " state (14)  A[0]:(0.138121008873) A[1]:(0.126413941383) A[2]:(0.207118481398) A[3]:(0.0786307379603)\n",
      " state (15)  A[0]:(0.143279463053) A[1]:(0.130141764879) A[2]:(0.218284159899) A[3]:(0.0797881484032)\n",
      "Episode 1000 finished after 0 timesteps with r=0.0. Running score: 0.0. Times trained:               3264. Times reached goal: 13.               Steps done: 7399. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.89336636806.\n",
      " state (0)  A[0]:(0.0614511221647) A[1]:(0.0715852081776) A[2]:(0.0344305299222) A[3]:(0.0622053071856)\n",
      " state (1)  A[0]:(0.0679487660527) A[1]:(0.0782564133406) A[2]:(0.0502872541547) A[3]:(0.0643433853984)\n",
      " state (2)  A[0]:(0.0744349956512) A[1]:(0.0849127098918) A[2]:(0.0661097019911) A[3]:(0.0664736181498)\n",
      " state (3)  A[0]:(0.0809062197804) A[1]:(0.0915507003665) A[2]:(0.0818828418851) A[3]:(0.0685955807567)\n",
      " state (4)  A[0]:(0.0873589366674) A[1]:(0.0981670171022) A[2]:(0.0975918695331) A[3]:(0.0707088261843)\n",
      " state (5)  A[0]:(0.0937896743417) A[1]:(0.104758344591) A[2]:(0.113222204149) A[3]:(0.0728129446507)\n",
      " state (6)  A[0]:(0.100195012987) A[1]:(0.111321419477) A[2]:(0.128759637475) A[3]:(0.0749075263739)\n",
      " state (7)  A[0]:(0.106571562588) A[1]:(0.117853008211) A[2]:(0.144190311432) A[3]:(0.0769921764731)\n",
      " state (8)  A[0]:(0.112916007638) A[1]:(0.12435002625) A[2]:(0.159500807524) A[3]:(0.0790665149689)\n",
      " state (9)  A[0]:(0.119225129485) A[1]:(0.130809381604) A[2]:(0.174678176641) A[3]:(0.0811301767826)\n",
      " state (10)  A[0]:(0.125495776534) A[1]:(0.137228101492) A[2]:(0.189710006118) A[3]:(0.0831827968359)\n",
      " state (11)  A[0]:(0.131724894047) A[1]:(0.143603265285) A[2]:(0.204584479332) A[3]:(0.0852240473032)\n",
      " state (12)  A[0]:(0.137909471989) A[1]:(0.14993211627) A[2]:(0.219290360808) A[3]:(0.0872535482049)\n",
      " state (13)  A[0]:(0.144046649337) A[1]:(0.156211867929) A[2]:(0.233817011118) A[3]:(0.0892710089684)\n",
      " state (14)  A[0]:(0.150133639574) A[1]:(0.162439957261) A[2]:(0.248154520988) A[3]:(0.0912761166692)\n",
      " state (15)  A[0]:(0.156167805195) A[1]:(0.16861385107) A[2]:(0.262293606997) A[3]:(0.0932685509324)\n",
      "Episode 2000 finished after 0 timesteps with r=0.0. Running score: 0.01. Times trained:               5960. Times reached goal: 22.               Steps done: 13359. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.888057739932.\n",
      " state (0)  A[0]:(0.0583275705576) A[1]:(0.066630743444) A[2]:(0.0291183814406) A[3]:(0.0588286183774)\n",
      " state (1)  A[0]:(0.0646738037467) A[1]:(0.0737071111798) A[2]:(0.0465110577643) A[3]:(0.0606889910996)\n",
      " state (2)  A[0]:(0.0710121095181) A[1]:(0.0807719379663) A[2]:(0.0638736337423) A[3]:(0.062543168664)\n",
      " state (3)  A[0]:(0.0773397088051) A[1]:(0.0878221318126) A[2]:(0.0811889693141) A[3]:(0.0643910765648)\n",
      " state (4)  A[0]:(0.0836538597941) A[1]:(0.0948546677828) A[2]:(0.0984401553869) A[3]:(0.0662326440215)\n",
      " state (5)  A[0]:(0.0899518504739) A[1]:(0.101866528392) A[2]:(0.115610539913) A[3]:(0.0680678188801)\n",
      " state (6)  A[0]:(0.0962309911847) A[1]:(0.108854718506) A[2]:(0.132683828473) A[3]:(0.0698965713382)\n",
      " state (7)  A[0]:(0.102488599718) A[1]:(0.115816310048) A[2]:(0.149644061923) A[3]:(0.0717188343406)\n",
      " state (8)  A[0]:(0.108722090721) A[1]:(0.122748404741) A[2]:(0.166475832462) A[3]:(0.0735345855355)\n",
      " state (9)  A[0]:(0.114928863943) A[1]:(0.129648193717) A[2]:(0.183164224029) A[3]:(0.075343772769)\n",
      " state (10)  A[0]:(0.121106415987) A[1]:(0.136512845755) A[2]:(0.199694886804) A[3]:(0.0771463736892)\n",
      " state (11)  A[0]:(0.127252221107) A[1]:(0.143339678645) A[2]:(0.216054141521) A[3]:(0.078942373395)\n",
      " state (12)  A[0]:(0.133363932371) A[1]:(0.150126039982) A[2]:(0.232229009271) A[3]:(0.0807317271829)\n",
      " state (13)  A[0]:(0.139439120889) A[1]:(0.156869366765) A[2]:(0.248207181692) A[3]:(0.0825143978)\n",
      " state (14)  A[0]:(0.145475566387) A[1]:(0.1635671556) A[2]:(0.263977169991) A[3]:(0.0842903703451)\n",
      " state (15)  A[0]:(0.151470988989) A[1]:(0.170216992497) A[2]:(0.279528111219) A[3]:(0.0860596075654)\n",
      "Episode 3000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               6132. Times reached goal: 18.               Steps done: 19491. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.882628831914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.069325812161) A[1]:(0.078844152391) A[2]:(0.0393220558763) A[3]:(0.0706963539124)\n",
      " state (1)  A[0]:(0.075699865818) A[1]:(0.085556641221) A[2]:(0.0555234476924) A[3]:(0.072701536119)\n",
      " state (2)  A[0]:(0.0820658951998) A[1]:(0.0922584012151) A[2]:(0.0716966986656) A[3]:(0.0747012868524)\n",
      " state (3)  A[0]:(0.0884217917919) A[1]:(0.0989473238587) A[2]:(0.0878288596869) A[3]:(0.0766955837607)\n",
      " state (4)  A[0]:(0.0947655364871) A[1]:(0.105621322989) A[2]:(0.103907123208) A[3]:(0.0786844193935)\n",
      " state (5)  A[0]:(0.101095087826) A[1]:(0.112278327346) A[2]:(0.119918875396) A[3]:(0.0806677639484)\n",
      " state (6)  A[0]:(0.10740840435) A[1]:(0.118916288018) A[2]:(0.135851681232) A[3]:(0.082645624876)\n",
      " state (7)  A[0]:(0.113703519106) A[1]:(0.12553319335) A[2]:(0.151693403721) A[3]:(0.0846179947257)\n",
      " state (8)  A[0]:(0.119978442788) A[1]:(0.132127031684) A[2]:(0.167432203889) A[3]:(0.0865848809481)\n",
      " state (9)  A[0]:(0.126231238246) A[1]:(0.138695821166) A[2]:(0.183056563139) A[3]:(0.0885462611914)\n",
      " state (10)  A[0]:(0.13245998323) A[1]:(0.145237669349) A[2]:(0.198555380106) A[3]:(0.0905021652579)\n",
      " state (11)  A[0]:(0.138662815094) A[1]:(0.151750653982) A[2]:(0.213917911053) A[3]:(0.0924526080489)\n",
      " state (12)  A[0]:(0.144837871194) A[1]:(0.158232942224) A[2]:(0.22913390398) A[3]:(0.0943975672126)\n",
      " state (13)  A[0]:(0.15098336339) A[1]:(0.164682686329) A[2]:(0.244193509221) A[3]:(0.0963370651007)\n",
      " state (14)  A[0]:(0.157097503543) A[1]:(0.171098127961) A[2]:(0.259087473154) A[3]:(0.0982711017132)\n",
      " state (15)  A[0]:(0.163178563118) A[1]:(0.177477493882) A[2]:(0.273806989193) A[3]:(0.100199684501)\n",
      "Episode 4000 finished after 0 timesteps with r=0.0. Running score: 0.01. Times trained:               6680. Times reached goal: 22.               Steps done: 26171. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.87675252005.\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 7.4272,  8.5618,  4.0170,  7.4962]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 7.4204,  8.5637,  4.0125,  7.4949]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor(1.00000e-02 *\n",
      "       [[ 8.1606,  9.3282,  5.8539,  7.7637]], device='cuda:0')\n",
      "On state=1, selected action=1 , Random? False\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.0740710496902) A[1]:(0.0856418535113) A[2]:(0.0400291867554) A[3]:(0.0749134272337)\n",
      " state (1)  A[0]:(0.0815139785409) A[1]:(0.0932680144906) A[2]:(0.0584393814206) A[3]:(0.0776151791215)\n",
      " state (2)  A[0]:(0.0889459773898) A[1]:(0.1008798033) A[2]:(0.0768133699894) A[3]:(0.0803103297949)\n",
      " state (3)  A[0]:(0.0963642448187) A[1]:(0.10847453028) A[2]:(0.0951332524419) A[3]:(0.0829986333847)\n",
      " state (4)  A[0]:(0.103766039014) A[1]:(0.116049520671) A[2]:(0.113381303847) A[3]:(0.085679858923)\n",
      " state (5)  A[0]:(0.111148625612) A[1]:(0.12360214442) A[2]:(0.131540119648) A[3]:(0.0883537754416)\n",
      " state (6)  A[0]:(0.118509285152) A[1]:(0.131129786372) A[2]:(0.149592578411) A[3]:(0.0910201892257)\n",
      " state (7)  A[0]:(0.125845387578) A[1]:(0.138629853725) A[2]:(0.167522087693) A[3]:(0.0936788544059)\n",
      " state (8)  A[0]:(0.133154213428) A[1]:(0.146099850535) A[2]:(0.185312494636) A[3]:(0.0963295921683)\n",
      " state (9)  A[0]:(0.140433266759) A[1]:(0.153537243605) A[2]:(0.202948153019) A[3]:(0.0989721789956)\n",
      " state (10)  A[0]:(0.147679954767) A[1]:(0.160939618945) A[2]:(0.220414102077) A[3]:(0.101606436074)\n",
      " state (11)  A[0]:(0.154891774058) A[1]:(0.168304622173) A[2]:(0.237696036696) A[3]:(0.104232177138)\n",
      " state (12)  A[0]:(0.162066340446) A[1]:(0.175629854202) A[2]:(0.254780322313) A[3]:(0.106849215925)\n",
      " state (13)  A[0]:(0.16920119524) A[1]:(0.182913079858) A[2]:(0.271654158831) A[3]:(0.109457343817)\n",
      " state (14)  A[0]:(0.176294088364) A[1]:(0.190152093768) A[2]:(0.288305431604) A[3]:(0.112056396902)\n",
      " state (15)  A[0]:(0.18334273994) A[1]:(0.197344720364) A[2]:(0.304722875357) A[3]:(0.114646188915)\n",
      "Episode 5000 finished after 0 timesteps with r=0.0. Running score: 0.05. Times trained:               6247. Times reached goal: 34.               Steps done: 32418. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.871292519126.\n",
      " state (0)  A[0]:(0.074963003397) A[1]:(0.0882367938757) A[2]:(0.0371809341013) A[3]:(0.0755342766643)\n",
      " state (1)  A[0]:(0.0829502120614) A[1]:(0.0963413268328) A[2]:(0.0561424382031) A[3]:(0.0782019495964)\n",
      " state (2)  A[0]:(0.0909263938665) A[1]:(0.104431062937) A[2]:(0.0750719383359) A[3]:(0.0808636695147)\n",
      " state (3)  A[0]:(0.0988886281848) A[1]:(0.112503200769) A[2]:(0.093950510025) A[3]:(0.0835192427039)\n",
      " state (4)  A[0]:(0.106833964586) A[1]:(0.120554931462) A[2]:(0.112759396434) A[3]:(0.0861685201526)\n",
      " state (5)  A[0]:(0.114759542048) A[1]:(0.128583490849) A[2]:(0.131480157375) A[3]:(0.0888113230467)\n",
      " state (6)  A[0]:(0.122662492096) A[1]:(0.136586174369) A[2]:(0.150094613433) A[3]:(0.0914474874735)\n",
      " state (7)  A[0]:(0.130539953709) A[1]:(0.14456024766) A[2]:(0.16858510673) A[3]:(0.0940768644214)\n",
      " state (8)  A[0]:(0.13838917017) A[1]:(0.152503103018) A[2]:(0.186934441328) A[3]:(0.0966993197799)\n",
      " state (9)  A[0]:(0.146207377315) A[1]:(0.160412102938) A[2]:(0.205126017332) A[3]:(0.0993146821856)\n",
      " state (10)  A[0]:(0.153991892934) A[1]:(0.168284729123) A[2]:(0.223143815994) A[3]:(0.10192283988)\n",
      " state (11)  A[0]:(0.16174004972) A[1]:(0.176118418574) A[2]:(0.240972578526) A[3]:(0.10452362895)\n",
      " state (12)  A[0]:(0.16944925487) A[1]:(0.183910772204) A[2]:(0.258597701788) A[3]:(0.107116937637)\n",
      " state (13)  A[0]:(0.177116960287) A[1]:(0.191659376025) A[2]:(0.276005506516) A[3]:(0.109702624381)\n",
      " state (14)  A[0]:(0.184740751982) A[1]:(0.199361920357) A[2]:(0.293182969093) A[3]:(0.11228056252)\n",
      " state (15)  A[0]:(0.192318186164) A[1]:(0.207016125321) A[2]:(0.310118168592) A[3]:(0.114850603044)\n",
      "Episode 6000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               6441. Times reached goal: 32.               Steps done: 38859. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.8656985587.\n",
      " state (0)  A[0]:(0.0839370936155) A[1]:(0.0980266258121) A[2]:(0.0445154197514) A[3]:(0.085419587791)\n",
      " state (1)  A[0]:(0.0922731980681) A[1]:(0.106967911124) A[2]:(0.065330862999) A[3]:(0.0882080867887)\n",
      " state (2)  A[0]:(0.100595593452) A[1]:(0.115889653563) A[2]:(0.086102552712) A[3]:(0.090989202261)\n",
      " state (3)  A[0]:(0.108901180327) A[1]:(0.124788485467) A[2]:(0.106806218624) A[3]:(0.0937628373504)\n",
      " state (4)  A[0]:(0.117186836898) A[1]:(0.133661061525) A[2]:(0.127417892218) A[3]:(0.0965289101005)\n",
      " state (5)  A[0]:(0.125449493527) A[1]:(0.142504125834) A[2]:(0.147914022207) A[3]:(0.0992873087525)\n",
      " state (6)  A[0]:(0.13368614018) A[1]:(0.151314377785) A[2]:(0.168271556497) A[3]:(0.102037996054)\n",
      " state (7)  A[0]:(0.141893774271) A[1]:(0.160088658333) A[2]:(0.188468143344) A[3]:(0.104780875146)\n",
      " state (8)  A[0]:(0.150069445372) A[1]:(0.168823853135) A[2]:(0.20848210156) A[3]:(0.107515908778)\n",
      " state (9)  A[0]:(0.158210232854) A[1]:(0.177516832948) A[2]:(0.228292629123) A[3]:(0.110243037343)\n",
      " state (10)  A[0]:(0.166313335299) A[1]:(0.186164602637) A[2]:(0.24787992239) A[3]:(0.112962186337)\n",
      " state (11)  A[0]:(0.174375936389) A[1]:(0.194764241576) A[2]:(0.267225116491) A[3]:(0.115673318505)\n",
      " state (12)  A[0]:(0.182395353913) A[1]:(0.203312858939) A[2]:(0.286310434341) A[3]:(0.118376411498)\n",
      " state (13)  A[0]:(0.190368905663) A[1]:(0.211807638407) A[2]:(0.305119365454) A[3]:(0.121071398258)\n",
      " state (14)  A[0]:(0.198294013739) A[1]:(0.220245927572) A[2]:(0.323636472225) A[3]:(0.123758241534)\n",
      " state (15)  A[0]:(0.206168159842) A[1]:(0.228625103831) A[2]:(0.341847628355) A[3]:(0.12643687427)\n",
      "Episode 7000 finished after 0 timesteps with r=0.0. Running score: 0.01. Times trained:               6393. Times reached goal: 36.               Steps done: 45252. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.860181800919.\n",
      " state (0)  A[0]:(0.0872998684645) A[1]:(0.0994338020682) A[2]:(0.0475921072066) A[3]:(0.0875639766455)\n",
      " state (1)  A[0]:(0.0951793640852) A[1]:(0.108103610575) A[2]:(0.0681543424726) A[3]:(0.0903260484338)\n",
      " state (2)  A[0]:(0.10304684937) A[1]:(0.116756603122) A[2]:(0.0886783078313) A[3]:(0.0930817201734)\n",
      " state (3)  A[0]:(0.110899738967) A[1]:(0.125389769673) A[2]:(0.109140530229) A[3]:(0.0958308950067)\n",
      " state (4)  A[0]:(0.118735454977) A[1]:(0.134000137448) A[2]:(0.129517778754) A[3]:(0.0985734984279)\n",
      " state (5)  A[0]:(0.126551449299) A[1]:(0.142584711313) A[2]:(0.149787157774) A[3]:(0.101309470832)\n",
      " state (6)  A[0]:(0.134345218539) A[1]:(0.151140585542) A[2]:(0.169926255941) A[3]:(0.104038730264)\n",
      " state (7)  A[0]:(0.142114266753) A[1]:(0.159664854407) A[2]:(0.189913183451) A[3]:(0.106761239469)\n",
      " state (8)  A[0]:(0.149856135249) A[1]:(0.168154701591) A[2]:(0.209726750851) A[3]:(0.109476938844)\n",
      " state (9)  A[0]:(0.157568439841) A[1]:(0.176607295871) A[2]:(0.229346603155) A[3]:(0.112185776234)\n",
      " state (10)  A[0]:(0.165248766541) A[1]:(0.185019895434) A[2]:(0.248753145337) A[3]:(0.114887736738)\n",
      " state (11)  A[0]:(0.17289480567) A[1]:(0.193389832973) A[2]:(0.267927885056) A[3]:(0.117582768202)\n",
      " state (12)  A[0]:(0.180504247546) A[1]:(0.201714470983) A[2]:(0.286853253841) A[3]:(0.120270803571)\n",
      " state (13)  A[0]:(0.1880749017) A[1]:(0.209991186857) A[2]:(0.305512696505) A[3]:(0.122951835394)\n",
      " state (14)  A[0]:(0.195604532957) A[1]:(0.218217626214) A[2]:(0.323890984058) A[3]:(0.125625804067)\n",
      " state (15)  A[0]:(0.203091055155) A[1]:(0.226391255856) A[2]:(0.341973870993) A[3]:(0.128292694688)\n",
      "Episode 8000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained:               6279. Times reached goal: 28.               Steps done: 51531. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.854797640652.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.0839584097266) A[1]:(0.100858271122) A[2]:(0.0432215332985) A[3]:(0.0845914110541)\n",
      " state (1)  A[0]:(0.0934173092246) A[1]:(0.111214406788) A[2]:(0.0663720369339) A[3]:(0.0879613310099)\n",
      " state (2)  A[0]:(0.102862127125) A[1]:(0.121547810733) A[2]:(0.089483641088) A[3]:(0.0913261175156)\n",
      " state (3)  A[0]:(0.112288765609) A[1]:(0.131853848696) A[2]:(0.112523272634) A[3]:(0.0946852862835)\n",
      " state (4)  A[0]:(0.12169316411) A[1]:(0.142127916217) A[2]:(0.135458216071) A[3]:(0.0980383455753)\n",
      " state (5)  A[0]:(0.131071299314) A[1]:(0.152365416288) A[2]:(0.158256158233) A[3]:(0.101384781301)\n",
      " state (6)  A[0]:(0.140419140458) A[1]:(0.162561878562) A[2]:(0.18088555336) A[3]:(0.104724124074)\n",
      " state (7)  A[0]:(0.149732753634) A[1]:(0.172712877393) A[2]:(0.203315749764) A[3]:(0.108055897057)\n",
      " state (8)  A[0]:(0.159008294344) A[1]:(0.182814031839) A[2]:(0.225517243147) A[3]:(0.111379608512)\n",
      " state (9)  A[0]:(0.168241903186) A[1]:(0.192861139774) A[2]:(0.247461676598) A[3]:(0.114694781601)\n",
      " state (10)  A[0]:(0.17742985487) A[1]:(0.202849924564) A[2]:(0.269122153521) A[3]:(0.118000984192)\n",
      " state (11)  A[0]:(0.186568468809) A[1]:(0.212776392698) A[2]:(0.290473461151) A[3]:(0.121297724545)\n",
      " state (12)  A[0]:(0.195654168725) A[1]:(0.22263661027) A[2]:(0.311491876841) A[3]:(0.124584577978)\n",
      " state (13)  A[0]:(0.204683497548) A[1]:(0.232426732779) A[2]:(0.332155615091) A[3]:(0.127861082554)\n",
      " state (14)  A[0]:(0.213653057814) A[1]:(0.242143064737) A[2]:(0.352444678545) A[3]:(0.131126791239)\n",
      " state (15)  A[0]:(0.222559586167) A[1]:(0.251782029867) A[2]:(0.372340977192) A[3]:(0.13438129425)\n",
      "Episode 9000 finished after 0 timesteps with r=0.0. Running score: 0.01. Times trained:               6305. Times reached goal: 31.               Steps done: 57836. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.849425096272.\n",
      "q_values \n",
      "tensor([[ 0.0949,  0.1103,  0.0519,  0.0945]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0949,  0.1103,  0.0519,  0.0945]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1038,  0.1198,  0.0742,  0.0976]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1127,  0.1294,  0.0965,  0.1007]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1482,  0.1676,  0.1847,  0.1131]], device='cuda:0')\n",
      "On state=6, selected action=0 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.09501003474) A[1]:(0.10997197032) A[2]:(0.0519210398197) A[3]:(0.0943231284618)\n",
      " state (1)  A[0]:(0.103937871754) A[1]:(0.119609370828) A[2]:(0.0742141902447) A[3]:(0.0974568799138)\n",
      " state (2)  A[0]:(0.112851910293) A[1]:(0.129227772355) A[2]:(0.0964814499021) A[3]:(0.100587144494)\n",
      " state (3)  A[0]:(0.121748775244) A[1]:(0.138823360205) A[2]:(0.118691861629) A[3]:(0.103713490069)\n",
      " state (4)  A[0]:(0.130625128746) A[1]:(0.148392230272) A[2]:(0.140814617276) A[3]:(0.106835447252)\n",
      " state (5)  A[0]:(0.139477670193) A[1]:(0.157930582762) A[2]:(0.16281914711) A[3]:(0.109952531755)\n",
      " state (6)  A[0]:(0.148303091526) A[1]:(0.167434632778) A[2]:(0.184675499797) A[3]:(0.113064311445)\n",
      " state (7)  A[0]:(0.157098174095) A[1]:(0.176900595427) A[2]:(0.206354349852) A[3]:(0.116170287132)\n",
      " state (8)  A[0]:(0.165859699249) A[1]:(0.186324849725) A[2]:(0.227827325463) A[3]:(0.119270026684)\n",
      " state (9)  A[0]:(0.174584522843) A[1]:(0.195703670382) A[2]:(0.249067083001) A[3]:(0.122363053262)\n",
      " state (10)  A[0]:(0.183269515634) A[1]:(0.205033540726) A[2]:(0.270047575235) A[3]:(0.125448897481)\n",
      " state (11)  A[0]:(0.19191172719) A[1]:(0.214310988784) A[2]:(0.290744036436) A[3]:(0.128527119756)\n",
      " state (12)  A[0]:(0.200508102775) A[1]:(0.223532557487) A[2]:(0.3111333251) A[3]:(0.131597280502)\n",
      " state (13)  A[0]:(0.209055796266) A[1]:(0.232695028186) A[2]:(0.331193834543) A[3]:(0.134658917785)\n",
      " state (14)  A[0]:(0.217551991343) A[1]:(0.241795137525) A[2]:(0.350905835629) A[3]:(0.137711569667)\n",
      " state (15)  A[0]:(0.225993931293) A[1]:(0.250829786062) A[2]:(0.370251178741) A[3]:(0.140754848719)\n",
      "Episode 10000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained:               6497. Times reached goal: 53.               Steps done: 64333. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.843924270204.\n",
      " state (0)  A[0]:(0.094320744276) A[1]:(0.108385853469) A[2]:(0.0489730015397) A[3]:(0.094658061862)\n",
      " state (1)  A[0]:(0.103007920086) A[1]:(0.117643930018) A[2]:(0.0712850093842) A[3]:(0.0977109521627)\n",
      " state (2)  A[0]:(0.111687473953) A[1]:(0.126892700791) A[2]:(0.0936119258404) A[3]:(0.100768849254)\n",
      " state (3)  A[0]:(0.120356038213) A[1]:(0.136128202081) A[2]:(0.115919671953) A[3]:(0.103830680251)\n",
      " state (4)  A[0]:(0.129010185599) A[1]:(0.14534637332) A[2]:(0.138173878193) A[3]:(0.106895312667)\n",
      " state (5)  A[0]:(0.137646526098) A[1]:(0.154543191195) A[2]:(0.16034001112) A[3]:(0.109961569309)\n",
      " state (6)  A[0]:(0.146261662245) A[1]:(0.163714617491) A[2]:(0.182383909822) A[3]:(0.113028213382)\n",
      " state (7)  A[0]:(0.154852196574) A[1]:(0.17285656929) A[2]:(0.20427171886) A[3]:(0.11609403044)\n",
      " state (8)  A[0]:(0.16341483593) A[1]:(0.181965067983) A[2]:(0.225970506668) A[3]:(0.119157731533)\n",
      " state (9)  A[0]:(0.171946227551) A[1]:(0.191036090255) A[2]:(0.247448220849) A[3]:(0.122218064964)\n",
      " state (10)  A[0]:(0.180443048477) A[1]:(0.2000657022) A[2]:(0.268674045801) A[3]:(0.125273689628)\n",
      " state (11)  A[0]:(0.188902169466) A[1]:(0.209050059319) A[2]:(0.289618819952) A[3]:(0.128323361278)\n",
      " state (12)  A[0]:(0.197320371866) A[1]:(0.217985376716) A[2]:(0.310254961252) A[3]:(0.131365761161)\n",
      " state (13)  A[0]:(0.205694630742) A[1]:(0.226867944002) A[2]:(0.330556839705) A[3]:(0.134399652481)\n",
      " state (14)  A[0]:(0.214021861553) A[1]:(0.235694140196) A[2]:(0.350500702858) A[3]:(0.137423783541)\n",
      " state (15)  A[0]:(0.222299143672) A[1]:(0.244460567832) A[2]:(0.370065063238) A[3]:(0.140436917543)\n",
      "Episode 11000 finished after 0 timesteps with r=0.0. Running score: 0.05. Times trained:               6439. Times reached goal: 33.               Steps done: 70772. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.838507699194.\n",
      " state (0)  A[0]:(0.0996681675315) A[1]:(0.112066805363) A[2]:(0.0551010333002) A[3]:(0.100726388395)\n",
      " state (1)  A[0]:(0.108630314469) A[1]:(0.121438659728) A[2]:(0.0760525092483) A[3]:(0.103282220662)\n",
      " state (2)  A[0]:(0.117594599724) A[1]:(0.130815029144) A[2]:(0.0970910415053) A[3]:(0.10585962981)\n",
      " state (3)  A[0]:(0.126556813717) A[1]:(0.140190973878) A[2]:(0.118181951344) A[3]:(0.108456507325)\n",
      " state (4)  A[0]:(0.135512664914) A[1]:(0.149561405182) A[2]:(0.139289319515) A[3]:(0.111070603132)\n",
      " state (5)  A[0]:(0.144457757473) A[1]:(0.158921033144) A[2]:(0.160376220942) A[3]:(0.113699443638)\n",
      " state (6)  A[0]:(0.153387606144) A[1]:(0.168264567852) A[2]:(0.181405112147) A[3]:(0.116340465844)\n",
      " state (7)  A[0]:(0.162297770381) A[1]:(0.17758654058) A[2]:(0.202338322997) A[3]:(0.118990913033)\n",
      " state (8)  A[0]:(0.171183690429) A[1]:(0.186881482601) A[2]:(0.223138228059) A[3]:(0.121647931635)\n",
      " state (9)  A[0]:(0.180040895939) A[1]:(0.19614392519) A[2]:(0.243767753243) A[3]:(0.124308638275)\n",
      " state (10)  A[0]:(0.188864842057) A[1]:(0.205368369818) A[2]:(0.264190852642) A[3]:(0.126970037818)\n",
      " state (11)  A[0]:(0.19765111804) A[1]:(0.214549452066) A[2]:(0.284372746944) A[3]:(0.129629120231)\n",
      " state (12)  A[0]:(0.206395357847) A[1]:(0.223681807518) A[2]:(0.304280310869) A[3]:(0.13228289783)\n",
      " state (13)  A[0]:(0.215093255043) A[1]:(0.232760250568) A[2]:(0.323882430792) A[3]:(0.13492834568)\n",
      " state (14)  A[0]:(0.223740682006) A[1]:(0.241779774427) A[2]:(0.343150138855) A[3]:(0.137562572956)\n",
      " state (15)  A[0]:(0.232333555818) A[1]:(0.250735461712) A[2]:(0.362057179213) A[3]:(0.140182748437)\n",
      "Episode 12000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained:               6579. Times reached goal: 25.               Steps done: 77351. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.833009263976.\n",
      " state (0)  A[0]:(0.101598039269) A[1]:(0.115938596427) A[2]:(0.0555336140096) A[3]:(0.101934798062)\n",
      " state (1)  A[0]:(0.111892834306) A[1]:(0.126322701573) A[2]:(0.0796293318272) A[3]:(0.104880124331)\n",
      " state (2)  A[0]:(0.122216843069) A[1]:(0.136744692922) A[2]:(0.104008287191) A[3]:(0.107907064259)\n",
      " state (3)  A[0]:(0.132562413812) A[1]:(0.147196009755) A[2]:(0.128608465195) A[3]:(0.11100962013)\n",
      " state (4)  A[0]:(0.142921358347) A[1]:(0.157667428255) A[2]:(0.15336202085) A[3]:(0.114180743694)\n",
      " state (5)  A[0]:(0.153284981847) A[1]:(0.16814905405) A[2]:(0.178196400404) A[3]:(0.117412418127)\n",
      " state (6)  A[0]:(0.163644134998) A[1]:(0.178630366921) A[2]:(0.203035309911) A[3]:(0.120695725083)\n",
      " state (7)  A[0]:(0.17398943007) A[1]:(0.189100414515) A[2]:(0.227800160646) A[3]:(0.124020956457)\n",
      " state (8)  A[0]:(0.184311106801) A[1]:(0.199547842145) A[2]:(0.252411216497) A[3]:(0.127377763391)\n",
      " state (9)  A[0]:(0.194599255919) A[1]:(0.209961175919) A[2]:(0.276789337397) A[3]:(0.130755245686)\n",
      " state (10)  A[0]:(0.204843997955) A[1]:(0.220328703523) A[2]:(0.300857365131) A[3]:(0.13414222002)\n",
      " state (11)  A[0]:(0.21503546834) A[1]:(0.230638995767) A[2]:(0.324541568756) A[3]:(0.137527272105)\n",
      " state (12)  A[0]:(0.22516399622) A[1]:(0.240880638361) A[2]:(0.347772866488) A[3]:(0.140899032354)\n",
      " state (13)  A[0]:(0.235220178962) A[1]:(0.251042753458) A[2]:(0.370488345623) A[3]:(0.144246354699)\n",
      " state (14)  A[0]:(0.245195090771) A[1]:(0.261114835739) A[2]:(0.392631739378) A[3]:(0.147558435798)\n",
      " state (15)  A[0]:(0.255080163479) A[1]:(0.27108708024) A[2]:(0.414154499769) A[3]:(0.15082500875)\n",
      "Episode 13000 finished after 0 timesteps with r=0.0. Running score: 0.06. Times trained:               6477. Times reached goal: 48.               Steps done: 83828. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.827631298316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.112252555788) A[1]:(0.128187283874) A[2]:(0.0632042512298) A[3]:(0.113169647753)\n",
      " state (1)  A[0]:(0.122474826872) A[1]:(0.137886807323) A[2]:(0.084853336215) A[3]:(0.115588977933)\n",
      " state (2)  A[0]:(0.132815122604) A[1]:(0.147727489471) A[2]:(0.107282154262) A[3]:(0.118247300386)\n",
      " state (3)  A[0]:(0.143262296915) A[1]:(0.157697796822) A[2]:(0.130420953035) A[3]:(0.121134974062)\n",
      " state (4)  A[0]:(0.153802856803) A[1]:(0.167783468962) A[2]:(0.154181465507) A[3]:(0.12423826009)\n",
      " state (5)  A[0]:(0.164421051741) A[1]:(0.177967444062) A[2]:(0.178457513452) A[3]:(0.127539142966)\n",
      " state (6)  A[0]:(0.17509894073) A[1]:(0.188230112195) A[2]:(0.203126862645) A[3]:(0.131015598774)\n",
      " state (7)  A[0]:(0.185816675425) A[1]:(0.19854940474) A[2]:(0.228053450584) A[3]:(0.134641692042)\n",
      " state (8)  A[0]:(0.19655276835) A[1]:(0.208901375532) A[2]:(0.253091037273) A[3]:(0.13838814199)\n",
      " state (9)  A[0]:(0.207284376025) A[1]:(0.219260394573) A[2]:(0.27808713913) A[3]:(0.142222866416)\n",
      " state (10)  A[0]:(0.217987984419) A[1]:(0.229599937797) A[2]:(0.302887797356) A[3]:(0.14611171186)\n",
      " state (11)  A[0]:(0.228639677167) A[1]:(0.239893078804) A[2]:(0.327342182398) A[3]:(0.150019258261)\n",
      " state (12)  A[0]:(0.239215791225) A[1]:(0.250113040209) A[2]:(0.351307421923) A[3]:(0.153909802437)\n",
      " state (13)  A[0]:(0.249693453312) A[1]:(0.260233998299) A[2]:(0.374652534723) A[3]:(0.157748222351)\n",
      " state (14)  A[0]:(0.260051012039) A[1]:(0.270231515169) A[2]:(0.397262066603) A[3]:(0.161500781775)\n",
      " state (15)  A[0]:(0.270268499851) A[1]:(0.280083239079) A[2]:(0.419038504362) A[3]:(0.165136143565)\n",
      "Episode 14000 finished after 0 timesteps with r=0.0. Running score: 0.04. Times trained:               6290. Times reached goal: 45.               Steps done: 90118. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.82244183542.\n",
      "q_values \n",
      "tensor([[ 0.1065,  0.1239,  0.0646,  0.1101]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1494,  0.1700,  0.1472,  0.1166]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1494,  0.1699,  0.1472,  0.1166]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1494,  0.1698,  0.1473,  0.1166]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.106458649039) A[1]:(0.123488306999) A[2]:(0.0645543262362) A[3]:(0.109814748168)\n",
      " state (1)  A[0]:(0.116727650166) A[1]:(0.134317144752) A[2]:(0.0818740427494) A[3]:(0.110301978886)\n",
      " state (2)  A[0]:(0.127312198281) A[1]:(0.145621463656) A[2]:(0.101465985179) A[3]:(0.111589677632)\n",
      " state (3)  A[0]:(0.138198837638) A[1]:(0.157388374209) A[2]:(0.123316712677) A[3]:(0.113692887127)\n",
      " state (4)  A[0]:(0.149366140366) A[1]:(0.169592574239) A[2]:(0.147341266274) A[3]:(0.116606265306)\n",
      " state (5)  A[0]:(0.1607837677) A[1]:(0.182194679976) A[2]:(0.173373937607) A[3]:(0.120301142335)\n",
      " state (6)  A[0]:(0.172411993146) A[1]:(0.195140376687) A[2]:(0.201162874699) A[3]:(0.124723173678)\n",
      " state (7)  A[0]:(0.184201538563) A[1]:(0.208360508084) A[2]:(0.230371847749) A[3]:(0.129791617393)\n",
      " state (8)  A[0]:(0.196094483137) A[1]:(0.221772313118) A[2]:(0.260590463877) A[3]:(0.135400384665)\n",
      " state (9)  A[0]:(0.208025574684) A[1]:(0.235281676054) A[2]:(0.291352421045) A[3]:(0.141421169043)\n",
      " state (10)  A[0]:(0.21992456913) A[1]:(0.248786553741) A[2]:(0.322161197662) A[3]:(0.147708311677)\n",
      " state (11)  A[0]:(0.231718808413) A[1]:(0.26218149066) A[2]:(0.352519571781) A[3]:(0.154105499387)\n",
      " state (12)  A[0]:(0.243336170912) A[1]:(0.275362104177) A[2]:(0.381959885359) A[3]:(0.160453349352)\n",
      " state (13)  A[0]:(0.254708260298) A[1]:(0.288229852915) A[2]:(0.410070300102) A[3]:(0.166597262025)\n",
      " state (14)  A[0]:(0.265773117542) A[1]:(0.300696253777) A[2]:(0.436514407396) A[3]:(0.172394841909)\n",
      " state (15)  A[0]:(0.276477456093) A[1]:(0.312686175108) A[2]:(0.461041986942) A[3]:(0.177721753716)\n",
      "Episode 15000 finished after 0 timesteps with r=0.0. Running score: 0.07. Times trained:               6379. Times reached goal: 45.               Steps done: 96497. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.817212176682.\n",
      " state (0)  A[0]:(0.121654964983) A[1]:(0.145923748612) A[2]:(0.135968506336) A[3]:(0.138449028134)\n",
      " state (1)  A[0]:(0.137365892529) A[1]:(0.158349707723) A[2]:(0.0946485176682) A[3]:(0.129105657339)\n",
      " state (2)  A[0]:(0.153458058834) A[1]:(0.171905323863) A[2]:(0.0579599998891) A[3]:(0.12085133791)\n",
      " state (3)  A[0]:(0.169171541929) A[1]:(0.187904506922) A[2]:(0.041190341115) A[3]:(0.116678386927)\n",
      " state (4)  A[0]:(0.184180647135) A[1]:(0.206915646791) A[2]:(0.0513677187264) A[3]:(0.117972403765)\n",
      " state (5)  A[0]:(0.198587372899) A[1]:(0.228743672371) A[2]:(0.0868906453252) A[3]:(0.124446578324)\n",
      " state (6)  A[0]:(0.212655678391) A[1]:(0.252831786871) A[2]:(0.142080277205) A[3]:(0.135057210922)\n",
      " state (7)  A[0]:(0.226610735059) A[1]:(0.278568804264) A[2]:(0.210638329387) A[3]:(0.148718461394)\n",
      " state (8)  A[0]:(0.240571677685) A[1]:(0.305391430855) A[2]:(0.286885619164) A[3]:(0.16454231739)\n",
      " state (9)  A[0]:(0.254558026791) A[1]:(0.332784175873) A[2]:(0.365934818983) A[3]:(0.181827068329)\n",
      " state (10)  A[0]:(0.268516212702) A[1]:(0.360262423754) A[2]:(0.443682700396) A[3]:(0.199989169836)\n",
      " state (11)  A[0]:(0.282347708941) A[1]:(0.387369304895) A[2]:(0.516891181469) A[3]:(0.218514457345)\n",
      " state (12)  A[0]:(0.295932680368) A[1]:(0.413689434528) A[2]:(0.583303630352) A[3]:(0.236941084266)\n",
      " state (13)  A[0]:(0.309149444103) A[1]:(0.438868790865) A[2]:(0.641677856445) A[3]:(0.254863679409)\n",
      " state (14)  A[0]:(0.321889340878) A[1]:(0.462633341551) A[2]:(0.691679060459) A[3]:(0.271946817636)\n",
      " state (15)  A[0]:(0.334066510201) A[1]:(0.484799474478) A[2]:(0.73366189003) A[3]:(0.287937283516)\n",
      "Episode 16000 finished after 0 timesteps with r=0.0. Running score: 0.05. Times trained:               6461. Times reached goal: 50.               Steps done: 102958. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.811949189198.\n",
      " state (0)  A[0]:(0.168521359563) A[1]:(0.185518249869) A[2]:(0.192248344421) A[3]:(0.179837539792)\n",
      " state (1)  A[0]:(0.183607414365) A[1]:(0.205109804869) A[2]:(0.166840836406) A[3]:(0.17638503015)\n",
      " state (2)  A[0]:(0.197135806084) A[1]:(0.221797868609) A[2]:(0.114829108119) A[3]:(0.169190168381)\n",
      " state (3)  A[0]:(0.210585728288) A[1]:(0.238780915737) A[2]:(0.0618193894625) A[3]:(0.161954253912)\n",
      " state (4)  A[0]:(0.225665196776) A[1]:(0.259699225426) A[2]:(0.039315585047) A[3]:(0.159081682563)\n",
      " state (5)  A[0]:(0.243208244443) A[1]:(0.286275476217) A[2]:(0.0637721270323) A[3]:(0.162872463465)\n",
      " state (6)  A[0]:(0.263019859791) A[1]:(0.317984670401) A[2]:(0.132702961564) A[3]:(0.173036530614)\n",
      " state (7)  A[0]:(0.284441024065) A[1]:(0.353273719549) A[2]:(0.233491718769) A[3]:(0.188073813915)\n",
      " state (8)  A[0]:(0.306809723377) A[1]:(0.390563160181) A[2]:(0.350906193256) A[3]:(0.206443324685)\n",
      " state (9)  A[0]:(0.329607486725) A[1]:(0.428574502468) A[2]:(0.470868617296) A[3]:(0.226947486401)\n",
      " state (10)  A[0]:(0.352445572615) A[1]:(0.46631655097) A[2]:(0.582425177097) A[3]:(0.248704269528)\n",
      " state (11)  A[0]:(0.375020235777) A[1]:(0.503009557724) A[2]:(0.678750514984) A[3]:(0.271034151316)\n",
      " state (12)  A[0]:(0.39708250761) A[1]:(0.538037955761) A[2]:(0.75713211298) A[3]:(0.293380141258)\n",
      " state (13)  A[0]:(0.418426007032) A[1]:(0.570938229561) A[2]:(0.818048477173) A[3]:(0.315273761749)\n",
      " state (14)  A[0]:(0.438886195421) A[1]:(0.601398468018) A[2]:(0.863841533661) A[3]:(0.336330473423)\n",
      " state (15)  A[0]:(0.458343058825) A[1]:(0.629255056381) A[2]:(0.897528648376) A[3]:(0.356256276369)\n",
      "Episode 17000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained:               5688. Times reached goal: 27.               Steps done: 108646. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.807343931978.\n",
      " state (0)  A[0]:(0.180041074753) A[1]:(0.196605950594) A[2]:(0.202157065272) A[3]:(0.192833691835)\n",
      " state (1)  A[0]:(0.197613358498) A[1]:(0.220713451505) A[2]:(0.198905259371) A[3]:(0.192896693945)\n",
      " state (2)  A[0]:(0.212331101298) A[1]:(0.241050243378) A[2]:(0.152041733265) A[3]:(0.185875594616)\n",
      " state (3)  A[0]:(0.225681826472) A[1]:(0.25995734334) A[2]:(0.0838192105293) A[3]:(0.17546467483)\n",
      " state (4)  A[0]:(0.240291833878) A[1]:(0.281429380178) A[2]:(0.0378001146019) A[3]:(0.168474465609)\n",
      " state (5)  A[0]:(0.258035361767) A[1]:(0.308241218328) A[2]:(0.0470726005733) A[3]:(0.169996380806)\n",
      " state (6)  A[0]:(0.279061704874) A[1]:(0.340491086245) A[2]:(0.115827046335) A[3]:(0.180709093809)\n",
      " state (7)  A[0]:(0.302495390177) A[1]:(0.376680225134) A[2]:(0.228870242834) A[3]:(0.198568373919)\n",
      " state (8)  A[0]:(0.32733887434) A[1]:(0.415090948343) A[2]:(0.364608556032) A[3]:(0.221146807075)\n",
      " state (9)  A[0]:(0.352822452784) A[1]:(0.454323112965) A[2]:(0.502447724342) A[3]:(0.246575519443)\n",
      " state (10)  A[0]:(0.378403931856) A[1]:(0.493307977915) A[2]:(0.626938223839) A[3]:(0.273564517498)\n",
      " state (11)  A[0]:(0.40368783474) A[1]:(0.531209647655) A[2]:(0.729636788368) A[3]:(0.301196813583)\n",
      " state (12)  A[0]:(0.428364276886) A[1]:(0.567362606525) A[2]:(0.808581233025) A[3]:(0.328766196966)\n",
      " state (13)  A[0]:(0.452180922031) A[1]:(0.601255059242) A[2]:(0.866140127182) A[3]:(0.355697542429)\n",
      " state (14)  A[0]:(0.474935919046) A[1]:(0.632533550262) A[2]:(0.906593859196) A[3]:(0.381523251534)\n",
      " state (15)  A[0]:(0.496480166912) A[1]:(0.661004424095) A[2]:(0.934394359589) A[3]:(0.405884772539)\n",
      "Episode 18000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained:               5437. Times reached goal: 21.               Steps done: 114083. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.802966314357.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.176157608628) A[1]:(0.197418168187) A[2]:(0.199468404055) A[3]:(0.188027799129)\n",
      " state (1)  A[0]:(0.192781463265) A[1]:(0.217079609632) A[2]:(0.210899293423) A[3]:(0.19127163291)\n",
      " state (2)  A[0]:(0.207278996706) A[1]:(0.236557841301) A[2]:(0.166914969683) A[3]:(0.184860691428)\n",
      " state (3)  A[0]:(0.220786064863) A[1]:(0.257120370865) A[2]:(0.0875595733523) A[3]:(0.17233607173)\n",
      " state (4)  A[0]:(0.235638409853) A[1]:(0.280725240707) A[2]:(0.0260324515402) A[3]:(0.162502288818)\n",
      " state (5)  A[0]:(0.253617465496) A[1]:(0.308727383614) A[2]:(0.0269406344742) A[3]:(0.162597313523)\n",
      " state (6)  A[0]:(0.274859905243) A[1]:(0.341107964516) A[2]:(0.0963604599237) A[3]:(0.173651352525)\n",
      " state (7)  A[0]:(0.298526018858) A[1]:(0.376981586218) A[2]:(0.215389713645) A[3]:(0.192906230688)\n",
      " state (8)  A[0]:(0.323683857918) A[1]:(0.415256500244) A[2]:(0.358612805605) A[3]:(0.217295721173)\n",
      " state (9)  A[0]:(0.349616825581) A[1]:(0.454885870218) A[2]:(0.50305378437) A[3]:(0.24466343224)\n",
      " state (10)  A[0]:(0.375804305077) A[1]:(0.494895577431) A[2]:(0.632250666618) A[3]:(0.27365681529)\n",
      " state (11)  A[0]:(0.401844769716) A[1]:(0.534380078316) A[2]:(0.737666845322) A[3]:(0.303379803896)\n",
      " state (12)  A[0]:(0.427404731512) A[1]:(0.572523117065) A[2]:(0.817712605) A[3]:(0.333153128624)\n",
      " state (13)  A[0]:(0.452198117971) A[1]:(0.608636498451) A[2]:(0.875266849995) A[3]:(0.36240324378)\n",
      " state (14)  A[0]:(0.475985020399) A[1]:(0.642198085785) A[2]:(0.915075182915) A[3]:(0.390633493662)\n",
      " state (15)  A[0]:(0.49857661128) A[1]:(0.67287415266) A[2]:(0.941937029362) A[3]:(0.417432844639)\n",
      "Episode 19000 finished after 0 timesteps with r=0.0. Running score: 0.01. Times trained:               5529. Times reached goal: 22.               Steps done: 119612. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.798538964293.\n",
      "q_values \n",
      "tensor([[ 0.1862,  0.2087,  0.1994,  0.1889]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2381,  0.2817, -0.0022,  0.1651]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2381,  0.2816, -0.0018,  0.1652]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3363,  0.4195,  0.3725,  0.2141]], device='cuda:0')\n",
      "On state=8, selected action=1 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.186088219285) A[1]:(0.208166882396) A[2]:(0.199290722609) A[3]:(0.189061239362)\n",
      " state (1)  A[0]:(0.195490449667) A[1]:(0.188411742449) A[2]:(0.230341628194) A[3]:(0.203290864825)\n",
      " state (2)  A[0]:(0.20629568398) A[1]:(0.198873028159) A[2]:(0.176841378212) A[3]:(0.198780953884)\n",
      " state (3)  A[0]:(0.220123395324) A[1]:(0.236473187804) A[2]:(0.0694432854652) A[3]:(0.180667027831)\n",
      " state (4)  A[0]:(0.238271743059) A[1]:(0.281514853239) A[2]:(-0.00148044421803) A[3]:(0.16549295187)\n",
      " state (5)  A[0]:(0.260484606028) A[1]:(0.321232438087) A[2]:(0.0148542923853) A[3]:(0.163435801864)\n",
      " state (6)  A[0]:(0.285144776106) A[1]:(0.355504184961) A[2]:(0.103162795305) A[3]:(0.17340554297)\n",
      " state (7)  A[0]:(0.310732811689) A[1]:(0.387596964836) A[2]:(0.231165781617) A[3]:(0.191287428141)\n",
      " state (8)  A[0]:(0.336347132921) A[1]:(0.419569760561) A[2]:(0.372752964497) A[3]:(0.214240476489)\n",
      " state (9)  A[0]:(0.361544668674) A[1]:(0.452135622501) A[2]:(0.510003447533) A[3]:(0.2407399863)\n",
      " state (10)  A[0]:(0.386118233204) A[1]:(0.485275506973) A[2]:(0.631665825844) A[3]:(0.269880056381)\n",
      " state (11)  A[0]:(0.40996041894) A[1]:(0.518631756306) A[2]:(0.732074975967) A[3]:(0.300943702459)\n",
      " state (12)  A[0]:(0.432995319366) A[1]:(0.551702916622) A[2]:(0.81012481451) A[3]:(0.33322891593)\n",
      " state (13)  A[0]:(0.455150902271) A[1]:(0.583948612213) A[2]:(0.867866635323) A[3]:(0.366011708975)\n",
      " state (14)  A[0]:(0.47635230422) A[1]:(0.614860534668) A[2]:(0.908957064152) A[3]:(0.398571878672)\n",
      " state (15)  A[0]:(0.496526449919) A[1]:(0.64401191473) A[2]:(0.937383413315) A[3]:(0.430242031813)\n",
      "Episode 20000 finished after 0 timesteps with r=0.0. Running score: 0.06. Times trained:               6442. Times reached goal: 32.               Steps done: 126054. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.793411310191.\n",
      " state (0)  A[0]:(0.223923295736) A[1]:(0.254779487848) A[2]:(0.235474839807) A[3]:(0.218869522214)\n",
      " state (1)  A[0]:(0.225339308381) A[1]:(0.0867895931005) A[2]:(0.266553550959) A[3]:(0.254805386066)\n",
      " state (2)  A[0]:(0.227983802557) A[1]:(0.127356141806) A[2]:(0.1593516469) A[3]:(0.245367735624)\n",
      " state (3)  A[0]:(0.232935935259) A[1]:(0.229249462485) A[2]:(0.024896081537) A[3]:(0.223357662559)\n",
      " state (4)  A[0]:(0.244723588228) A[1]:(0.292459100485) A[2]:(-0.0128888059407) A[3]:(0.206752926111)\n",
      " state (5)  A[0]:(0.261838704348) A[1]:(0.319378733635) A[2]:(0.040334071964) A[3]:(0.199280068278)\n",
      " state (6)  A[0]:(0.281032025814) A[1]:(0.334596544504) A[2]:(0.139355048537) A[3]:(0.200416579843)\n",
      " state (7)  A[0]:(0.30063021183) A[1]:(0.350914776325) A[2]:(0.256577640772) A[3]:(0.209277629852)\n",
      " state (8)  A[0]:(0.320103436708) A[1]:(0.372330874205) A[2]:(0.378212690353) A[3]:(0.224957704544)\n",
      " state (9)  A[0]:(0.339372217655) A[1]:(0.399162262678) A[2]:(0.496032118797) A[3]:(0.246449545026)\n",
      " state (10)  A[0]:(0.358473569155) A[1]:(0.430430650711) A[2]:(0.604032278061) A[3]:(0.272658705711)\n",
      " state (11)  A[0]:(0.377432852983) A[1]:(0.464776366949) A[2]:(0.697914958) A[3]:(0.302445799112)\n",
      " state (12)  A[0]:(0.396222233772) A[1]:(0.500801324844) A[2]:(0.775402069092) A[3]:(0.334661751986)\n",
      " state (13)  A[0]:(0.414759159088) A[1]:(0.537212133408) A[2]:(0.836339235306) A[3]:(0.368182331324)\n",
      " state (14)  A[0]:(0.432919442654) A[1]:(0.572894096375) A[2]:(0.882285714149) A[3]:(0.401950776577)\n",
      " state (15)  A[0]:(0.450558871031) A[1]:(0.60695874691) A[2]:(0.915776610374) A[3]:(0.435028165579)\n",
      "Episode 21000 finished after 0 timesteps with r=0.0. Running score: 0.08. Times trained:               6865. Times reached goal: 24.               Steps done: 132919. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.787983194871.\n",
      " state (0)  A[0]:(0.234035611153) A[1]:(0.251567512751) A[2]:(0.214037910104) A[3]:(0.229614645243)\n",
      " state (1)  A[0]:(0.233569592237) A[1]:(0.0348814278841) A[2]:(0.2410826087) A[3]:(0.221622899175)\n",
      " state (2)  A[0]:(0.228925988078) A[1]:(0.121701709926) A[2]:(0.131118550897) A[3]:(0.226095885038)\n",
      " state (3)  A[0]:(0.226647868752) A[1]:(0.231151252985) A[2]:(0.0185948032886) A[3]:(0.228786334395)\n",
      " state (4)  A[0]:(0.231985092163) A[1]:(0.278561741114) A[2]:(-0.0075203999877) A[3]:(0.217198655009)\n",
      " state (5)  A[0]:(0.243699222803) A[1]:(0.286497890949) A[2]:(0.039107169956) A[3]:(0.199897557497)\n",
      " state (6)  A[0]:(0.259138047695) A[1]:(0.288541853428) A[2]:(0.124411642551) A[3]:(0.188040196896)\n",
      " state (7)  A[0]:(0.276747405529) A[1]:(0.299859255552) A[2]:(0.229631617665) A[3]:(0.186747685075)\n",
      " state (8)  A[0]:(0.295785397291) A[1]:(0.324012249708) A[2]:(0.344831436872) A[3]:(0.19684034586)\n",
      " state (9)  A[0]:(0.315874636173) A[1]:(0.359760344028) A[2]:(0.46232843399) A[3]:(0.217138290405)\n",
      " state (10)  A[0]:(0.336765021086) A[1]:(0.404227226973) A[2]:(0.574725031853) A[3]:(0.245718091726)\n",
      " state (11)  A[0]:(0.358220875263) A[1]:(0.454168081284) A[2]:(0.675449132919) A[3]:(0.280466079712)\n",
      " state (12)  A[0]:(0.379975676537) A[1]:(0.506479799747) A[2]:(0.760027587414) A[3]:(0.319291830063)\n",
      " state (13)  A[0]:(0.401723504066) A[1]:(0.558449625969) A[2]:(0.826858282089) A[3]:(0.360221147537)\n",
      " state (14)  A[0]:(0.423136621714) A[1]:(0.60791438818) A[2]:(0.87696492672) A[3]:(0.40147203207)\n",
      " state (15)  A[0]:(0.443894565105) A[1]:(0.653352856636) A[2]:(0.913013219833) A[3]:(0.441537886858)\n",
      "Episode 22000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained:               7238. Times reached goal: 47.               Steps done: 140157. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.782300363483.\n",
      " state (0)  A[0]:(0.225897550583) A[1]:(0.256672054529) A[2]:(0.215360417962) A[3]:(0.227130666375)\n",
      " state (1)  A[0]:(0.227353960276) A[1]:(0.0281406231225) A[2]:(0.24098457396) A[3]:(0.21089245379)\n",
      " state (2)  A[0]:(0.228168457747) A[1]:(0.141045406461) A[2]:(0.144203126431) A[3]:(0.230447143316)\n",
      " state (3)  A[0]:(0.230074554682) A[1]:(0.251816809177) A[2]:(0.0449300333858) A[3]:(0.242821514606)\n",
      " state (4)  A[0]:(0.234089702368) A[1]:(0.289615511894) A[2]:(0.0143974013627) A[3]:(0.229255050421)\n",
      " state (5)  A[0]:(0.241639629006) A[1]:(0.284865438938) A[2]:(0.0473518036306) A[3]:(0.202944517136)\n",
      " state (6)  A[0]:(0.253477334976) A[1]:(0.278607487679) A[2]:(0.120568558574) A[3]:(0.181907147169)\n",
      " state (7)  A[0]:(0.269479632378) A[1]:(0.290324091911) A[2]:(0.220581501722) A[3]:(0.175351589918)\n",
      " state (8)  A[0]:(0.289074838161) A[1]:(0.324187099934) A[2]:(0.338238090277) A[3]:(0.185331448913)\n",
      " state (9)  A[0]:(0.311557352543) A[1]:(0.377041012049) A[2]:(0.463660061359) A[3]:(0.210217684507)\n",
      " state (10)  A[0]:(0.336209714413) A[1]:(0.442865550518) A[2]:(0.585759878159) A[3]:(0.246932134032)\n",
      " state (11)  A[0]:(0.362324774265) A[1]:(0.514979243279) A[2]:(0.694444179535) A[3]:(0.292020440102)\n",
      " state (12)  A[0]:(0.389200687408) A[1]:(0.58723962307) A[2]:(0.78325766325) A[3]:(0.342085361481)\n",
      " state (13)  A[0]:(0.416150063276) A[1]:(0.654807925224) A[2]:(0.850517630577) A[3]:(0.393994480371)\n",
      " state (14)  A[0]:(0.442529529333) A[1]:(0.714562237263) A[2]:(0.898404479027) A[3]:(0.445052862167)\n",
      " state (15)  A[0]:(0.467783689499) A[1]:(0.765111029148) A[2]:(0.931007444859) A[3]:(0.493165194988)\n",
      "Episode 23000 finished after 0 timesteps with r=0.0. Running score: 0.07. Times trained:               7251. Times reached goal: 51.               Steps done: 147408. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.776648419434.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.222959473729) A[1]:(0.238778591156) A[2]:(0.202690079808) A[3]:(0.220947444439)\n",
      " state (1)  A[0]:(0.219580769539) A[1]:(0.016480801627) A[2]:(0.226674005389) A[3]:(0.19667622447)\n",
      " state (2)  A[0]:(0.220903024077) A[1]:(0.128090664744) A[2]:(0.153815865517) A[3]:(0.219435438514)\n",
      " state (3)  A[0]:(0.221037089825) A[1]:(0.233758836985) A[2]:(0.0661717131734) A[3]:(0.23453117907)\n",
      " state (4)  A[0]:(0.219398990273) A[1]:(0.269446372986) A[2]:(0.0262237545103) A[3]:(0.222194060683)\n",
      " state (5)  A[0]:(0.219833374023) A[1]:(0.259042322636) A[2]:(0.0435846559703) A[3]:(0.19287982583)\n",
      " state (6)  A[0]:(0.226355582476) A[1]:(0.24845097959) A[2]:(0.104979164898) A[3]:(0.166715174913)\n",
      " state (7)  A[0]:(0.240404367447) A[1]:(0.263344734907) A[2]:(0.200534567237) A[3]:(0.156012296677)\n",
      " state (8)  A[0]:(0.261481165886) A[1]:(0.309722989798) A[2]:(0.321313887835) A[3]:(0.164385035634)\n",
      " state (9)  A[0]:(0.288246482611) A[1]:(0.382410258055) A[2]:(0.455183327198) A[3]:(0.190326407552)\n",
      " state (10)  A[0]:(0.31915897131) A[1]:(0.471269726753) A[2]:(0.587323844433) A[3]:(0.230182051659)\n",
      " state (11)  A[0]:(0.352720975876) A[1]:(0.565021038055) A[2]:(0.704107880592) A[3]:(0.279714941978)\n",
      " state (12)  A[0]:(0.387539684772) A[1]:(0.653873383999) A[2]:(0.797270417213) A[3]:(0.334748566151)\n",
      " state (13)  A[0]:(0.422349125147) A[1]:(0.731239676476) A[2]:(0.865323781967) A[3]:(0.391465306282)\n",
      " state (14)  A[0]:(0.456053823233) A[1]:(0.794234573841) A[2]:(0.911749958992) A[3]:(0.446650117636)\n",
      " state (15)  A[0]:(0.487787157297) A[1]:(0.843007683754) A[2]:(0.941986382008) A[3]:(0.497904807329)\n",
      "Episode 24000 finished after 0 timesteps with r=0.0. Running score: 0.05. Times trained:               7473. Times reached goal: 57.               Steps done: 154881. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.770866158124.\n",
      "q_values \n",
      "tensor([[ 0.2168,  0.2389,  0.1944,  0.2166]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2113,  0.0134,  0.2165,  0.1843]], device='cuda:0')\n",
      "On state=1, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2166,  0.2388,  0.1943,  0.2170]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2204,  0.2596,  0.0288,  0.2176]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2558,  0.2834,  0.3013,  0.1534]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2840,  0.3687,  0.4375,  0.1783]], device='cuda:0')\n",
      "On state=9, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2547,  0.2823,  0.2994,  0.1538]], device='cuda:0')\n",
      "On state=8, selected action=1 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.215970307589) A[1]:(0.238514736295) A[2]:(0.194055974483) A[3]:(0.217956736684)\n",
      " state (1)  A[0]:(0.210140198469) A[1]:(0.0131919626147) A[2]:(0.215796038508) A[3]:(0.185775861144)\n",
      " state (2)  A[0]:(0.216475307941) A[1]:(0.12323910743) A[2]:(0.157020270824) A[3]:(0.210023656487)\n",
      " state (3)  A[0]:(0.220373257995) A[1]:(0.226484000683) A[2]:(0.0733435153961) A[3]:(0.227766603231)\n",
      " state (4)  A[0]:(0.218961909413) A[1]:(0.258862048388) A[2]:(0.0267033409327) A[3]:(0.218657374382)\n",
      " state (5)  A[0]:(0.216815307736) A[1]:(0.239412352443) A[2]:(0.0346765145659) A[3]:(0.18988622725)\n",
      " state (6)  A[0]:(0.22049112618) A[1]:(0.219195023179) A[2]:(0.088011495769) A[3]:(0.161660596728)\n",
      " state (7)  A[0]:(0.233065828681) A[1]:(0.230508372188) A[2]:(0.178652733564) A[3]:(0.148058369756)\n",
      " state (8)  A[0]:(0.254455864429) A[1]:(0.282079070807) A[2]:(0.298598766327) A[3]:(0.154059767723)\n",
      " state (9)  A[0]:(0.283072978258) A[1]:(0.36759608984) A[2]:(0.43548732996) A[3]:(0.178679481149)\n",
      " state (10)  A[0]:(0.316918849945) A[1]:(0.473301827908) A[2]:(0.573071360588) A[3]:(0.218251973391)\n",
      " state (11)  A[0]:(0.354042440653) A[1]:(0.583408176899) A[2]:(0.695724248886) A[3]:(0.268321990967)\n",
      " state (12)  A[0]:(0.392661362886) A[1]:(0.684507369995) A[2]:(0.793596386909) A[3]:(0.32443857193)\n",
      " state (13)  A[0]:(0.431192517281) A[1]:(0.768505871296) A[2]:(0.864612519741) A[3]:(0.382503896952)\n",
      " state (14)  A[0]:(0.468296974897) A[1]:(0.833080112934) A[2]:(0.91249358654) A[3]:(0.439045697451)\n",
      " state (15)  A[0]:(0.502946019173) A[1]:(0.88002038002) A[2]:(0.943223595619) A[3]:(0.49146643281)\n",
      "Episode 25000 finished after 0 timesteps with r=0.0. Running score: 0.05. Times trained:               7327. Times reached goal: 48.               Steps done: 162208. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.765238663287.\n",
      " state (0)  A[0]:(0.203803047538) A[1]:(0.224403172731) A[2]:(0.178237989545) A[3]:(0.205880284309)\n",
      " state (1)  A[0]:(0.195520117879) A[1]:(0.0105840787292) A[2]:(0.198083609343) A[3]:(0.172527745366)\n",
      " state (2)  A[0]:(0.203196033835) A[1]:(0.113317422569) A[2]:(0.154628232121) A[3]:(0.194887593389)\n",
      " state (3)  A[0]:(0.208223000169) A[1]:(0.212715685368) A[2]:(0.0761963054538) A[3]:(0.213773906231)\n",
      " state (4)  A[0]:(0.206567496061) A[1]:(0.246861219406) A[2]:(0.0240727495402) A[3]:(0.208862200379)\n",
      " state (5)  A[0]:(0.202290013433) A[1]:(0.225114345551) A[2]:(0.0254226215184) A[3]:(0.182536691427)\n",
      " state (6)  A[0]:(0.203543663025) A[1]:(0.200890615582) A[2]:(0.0741007328033) A[3]:(0.153834760189)\n",
      " state (7)  A[0]:(0.214794591069) A[1]:(0.21292693913) A[2]:(0.162090092897) A[3]:(0.138177230954)\n",
      " state (8)  A[0]:(0.236361935735) A[1]:(0.272876352072) A[2]:(0.281576097012) A[3]:(0.141916200519)\n",
      " state (9)  A[0]:(0.266470402479) A[1]:(0.373104244471) A[2]:(0.420254379511) A[3]:(0.164782419801)\n",
      " state (10)  A[0]:(0.302749037743) A[1]:(0.49556684494) A[2]:(0.561290800571) A[3]:(0.203326642513)\n",
      " state (11)  A[0]:(0.342869132757) A[1]:(0.619373083115) A[2]:(0.687907457352) A[3]:(0.253058373928)\n",
      " state (12)  A[0]:(0.384710878134) A[1]:(0.727805495262) A[2]:(0.78914731741) A[3]:(0.309366762638)\n",
      " state (13)  A[0]:(0.426400095224) A[1]:(0.812569379807) A[2]:(0.862416505814) A[3]:(0.367925494909)\n",
      " state (14)  A[0]:(0.466364115477) A[1]:(0.873383700848) A[2]:(0.911517977715) A[3]:(0.425017058849)\n",
      " state (15)  A[0]:(0.50341552496) A[1]:(0.914544403553) A[2]:(0.942783176899) A[3]:(0.477839529514)\n",
      "Episode 26000 finished after 0 timesteps with r=0.0. Running score: 0.07. Times trained:               7387. Times reached goal: 69.               Steps done: 169595. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.759606672649.\n",
      " state (0)  A[0]:(0.208782508969) A[1]:(0.228920802474) A[2]:(0.17575442791) A[3]:(0.208792120218)\n",
      " state (1)  A[0]:(0.196683377028) A[1]:(0.0107167428359) A[2]:(0.196190327406) A[3]:(0.170268595219)\n",
      " state (2)  A[0]:(0.20600810647) A[1]:(0.115026071668) A[2]:(0.163721367717) A[3]:(0.192396596074)\n",
      " state (3)  A[0]:(0.212904989719) A[1]:(0.218709230423) A[2]:(0.0878184214234) A[3]:(0.213611319661)\n",
      " state (4)  A[0]:(0.21246573329) A[1]:(0.256581872702) A[2]:(0.0333244130015) A[3]:(0.212946563959)\n",
      " state (5)  A[0]:(0.207984343171) A[1]:(0.231332659721) A[2]:(0.0344806052744) A[3]:(0.189334526658)\n",
      " state (6)  A[0]:(0.208558604121) A[1]:(0.200812518597) A[2]:(0.0842568352818) A[3]:(0.161060079932)\n",
      " state (7)  A[0]:(0.219555005431) A[1]:(0.210584297776) A[2]:(0.172923982143) A[3]:(0.144531801343)\n",
      " state (8)  A[0]:(0.241554990411) A[1]:(0.275455206633) A[2]:(0.292468518019) A[3]:(0.147264331579)\n",
      " state (9)  A[0]:(0.272686809301) A[1]:(0.386620312929) A[2]:(0.430860340595) A[3]:(0.169696331024)\n",
      " state (10)  A[0]:(0.310381531715) A[1]:(0.521653592587) A[2]:(0.571370601654) A[3]:(0.208606854081)\n",
      " state (11)  A[0]:(0.352113544941) A[1]:(0.654533028603) A[2]:(0.697120428085) A[3]:(0.259423434734)\n",
      " state (12)  A[0]:(0.395585924387) A[1]:(0.765795230865) A[2]:(0.797089576721) A[3]:(0.317279666662)\n",
      " state (13)  A[0]:(0.438771039248) A[1]:(0.847880959511) A[2]:(0.868824601173) A[3]:(0.377516686916)\n",
      " state (14)  A[0]:(0.479978263378) A[1]:(0.903100848198) A[2]:(0.916393339634) A[3]:(0.436102032661)\n",
      " state (15)  A[0]:(0.517954051495) A[1]:(0.938120126724) A[2]:(0.946340143681) A[3]:(0.490020036697)\n",
      "Episode 27000 finished after 0 timesteps with r=0.0. Running score: 0.06. Times trained:               7133. Times reached goal: 58.               Steps done: 176728. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.754207676664.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.196166902781) A[1]:(0.218773826957) A[2]:(0.171437680721) A[3]:(0.19553412497)\n",
      " state (1)  A[0]:(0.185366824269) A[1]:(0.00712306657806) A[2]:(0.189444214106) A[3]:(0.161801278591)\n",
      " state (2)  A[0]:(0.193024277687) A[1]:(0.100754708052) A[2]:(0.167218491435) A[3]:(0.181390985847)\n",
      " state (3)  A[0]:(0.199299067259) A[1]:(0.196233510971) A[2]:(0.0917164385319) A[3]:(0.203612595797)\n",
      " state (4)  A[0]:(0.199160605669) A[1]:(0.234543636441) A[2]:(0.0270589981228) A[3]:(0.207871571183)\n",
      " state (5)  A[0]:(0.194131568074) A[1]:(0.206962063909) A[2]:(0.0206395871937) A[3]:(0.187911629677)\n",
      " state (6)  A[0]:(0.193840026855) A[1]:(0.172608971596) A[2]:(0.0676652863622) A[3]:(0.160533979535)\n",
      " state (7)  A[0]:(0.204541087151) A[1]:(0.183668792248) A[2]:(0.155562654138) A[3]:(0.143354609609)\n",
      " state (8)  A[0]:(0.226951181889) A[1]:(0.257309168577) A[2]:(0.275120735168) A[3]:(0.145011261106)\n",
      " state (9)  A[0]:(0.259017407894) A[1]:(0.382669866085) A[2]:(0.414505571127) A[3]:(0.16651558876)\n",
      " state (10)  A[0]:(0.298009902239) A[1]:(0.532426774502) A[2]:(0.557348549366) A[3]:(0.204912781715)\n",
      " state (11)  A[0]:(0.341309696436) A[1]:(0.675470411777) A[2]:(0.686505913734) A[3]:(0.255767434835)\n",
      " state (12)  A[0]:(0.3865455091) A[1]:(0.790248990059) A[2]:(0.790088951588) A[3]:(0.314244270325)\n",
      " state (13)  A[0]:(0.431593835354) A[1]:(0.870659947395) A[2]:(0.864809453487) A[3]:(0.375588625669)\n",
      " state (14)  A[0]:(0.47464543581) A[1]:(0.921810686588) A[2]:(0.91440320015) A[3]:(0.435568332672)\n",
      " state (15)  A[0]:(0.514329373837) A[1]:(0.952486038208) A[2]:(0.945529103279) A[3]:(0.490935951471)\n",
      "Episode 28000 finished after 0 timesteps with r=0.0. Running score: 0.06. Times trained:               7473. Times reached goal: 58.               Steps done: 184201. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.748592489973.\n",
      " state (0)  A[0]:(0.199179247022) A[1]:(0.22037909925) A[2]:(0.167199552059) A[3]:(0.197512388229)\n",
      " state (1)  A[0]:(0.187811270356) A[1]:(0.00476276269183) A[2]:(0.179858624935) A[3]:(0.161977112293)\n",
      " state (2)  A[0]:(0.194792181253) A[1]:(0.0993121042848) A[2]:(0.166659116745) A[3]:(0.178819522262)\n",
      " state (3)  A[0]:(0.201118484139) A[1]:(0.197305172682) A[2]:(0.0944221243262) A[3]:(0.200521856546)\n",
      " state (4)  A[0]:(0.200971737504) A[1]:(0.238855183125) A[2]:(0.0271522700787) A[3]:(0.206811219454)\n",
      " state (5)  A[0]:(0.194456607103) A[1]:(0.207312300801) A[2]:(0.0201666429639) A[3]:(0.188194200397)\n",
      " state (6)  A[0]:(0.192517533898) A[1]:(0.16670230031) A[2]:(0.0675479695201) A[3]:(0.160480171442)\n",
      " state (7)  A[0]:(0.20257589221) A[1]:(0.176351323724) A[2]:(0.154237493873) A[3]:(0.141872212291)\n",
      " state (8)  A[0]:(0.225450396538) A[1]:(0.255664050579) A[2]:(0.2710968256) A[3]:(0.141768082976)\n",
      " state (9)  A[0]:(0.258799105883) A[1]:(0.391662031412) A[2]:(0.407642573118) A[3]:(0.161732792854)\n",
      " state (10)  A[0]:(0.299611836672) A[1]:(0.55207824707) A[2]:(0.548902630806) A[3]:(0.199147105217)\n",
      " state (11)  A[0]:(0.345064133406) A[1]:(0.701009571552) A[2]:(0.678300976753) A[3]:(0.249751627445)\n",
      " state (12)  A[0]:(0.39261251688) A[1]:(0.81561756134) A[2]:(0.78346323967) A[3]:(0.308731257915)\n",
      " state (13)  A[0]:(0.439962029457) A[1]:(0.891945898533) A[2]:(0.860152840614) A[3]:(0.371188014746)\n",
      " state (14)  A[0]:(0.485139042139) A[1]:(0.937934458256) A[2]:(0.911415278912) A[3]:(0.432625502348)\n",
      " state (15)  A[0]:(0.526643157005) A[1]:(0.964067935944) A[2]:(0.943692266941) A[3]:(0.489505410194)\n",
      "Episode 29000 finished after 0 timesteps with r=0.0. Running score: 0.06. Times trained:               7417. Times reached goal: 38.               Steps done: 191618. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.743060719406.\n",
      "q_values \n",
      "tensor([[ 0.2072,  0.2328,  0.1768,  0.2051]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2139,  0.2583,  0.0270,  0.2117]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.207216843963) A[1]:(0.232425704598) A[2]:(0.176752403378) A[3]:(0.205072328448)\n",
      " state (1)  A[0]:(0.194285064936) A[1]:(0.0038725391496) A[2]:(0.1869109869) A[3]:(0.166736066341)\n",
      " state (2)  A[0]:(0.202648133039) A[1]:(0.103973910213) A[2]:(0.179311096668) A[3]:(0.181507661939)\n",
      " state (3)  A[0]:(0.21152356267) A[1]:(0.210971847177) A[2]:(0.101812586188) A[3]:(0.203547745943)\n",
      " state (4)  A[0]:(0.213759273291) A[1]:(0.257265418768) A[2]:(0.0268110148609) A[3]:(0.211355745792)\n",
      " state (5)  A[0]:(0.20762065053) A[1]:(0.216824576259) A[2]:(0.0236146952957) A[3]:(0.192257076502)\n",
      " state (6)  A[0]:(0.205574944615) A[1]:(0.164840891957) A[2]:(0.0797637626529) A[3]:(0.163074970245)\n",
      " state (7)  A[0]:(0.216144099832) A[1]:(0.171809583902) A[2]:(0.173837423325) A[3]:(0.143550992012)\n",
      " state (8)  A[0]:(0.240157470107) A[1]:(0.259710639715) A[2]:(0.29512822628) A[3]:(0.143758565187)\n",
      " state (9)  A[0]:(0.27499216795) A[1]:(0.411266237497) A[2]:(0.433194190264) A[3]:(0.165342330933)\n",
      " state (10)  A[0]:(0.317394971848) A[1]:(0.585580825806) A[2]:(0.573385715485) A[3]:(0.205475345254)\n",
      " state (11)  A[0]:(0.364345431328) A[1]:(0.739516496658) A[2]:(0.699614644051) A[3]:(0.259515523911)\n",
      " state (12)  A[0]:(0.413111209869) A[1]:(0.850012302399) A[2]:(0.800338506699) A[3]:(0.322098761797)\n",
      " state (13)  A[0]:(0.461225658655) A[1]:(0.917970955372) A[2]:(0.872379541397) A[3]:(0.38769698143)\n",
      " state (14)  A[0]:(0.506607413292) A[1]:(0.95576775074) A[2]:(0.919637858868) A[3]:(0.451296448708)\n",
      " state (15)  A[0]:(0.54775428772) A[1]:(0.975722372532) A[2]:(0.948920726776) A[3]:(0.50913143158)\n",
      "Episode 30000 finished after 0 timesteps with r=0.0. Running score: 0.08. Times trained:               7644. Times reached goal: 64.               Steps done: 199262. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.73740241685.\n",
      " state (0)  A[0]:(0.237345963717) A[1]:(0.267687410116) A[2]:(0.196950063109) A[3]:(0.23548643291)\n",
      " state (1)  A[0]:(0.223033681512) A[1]:(0.00967898499221) A[2]:(0.20519015193) A[3]:(0.191954791546)\n",
      " state (2)  A[0]:(0.232507973909) A[1]:(0.117383293808) A[2]:(0.204951703548) A[3]:(0.205688521266)\n",
      " state (3)  A[0]:(0.243331506848) A[1]:(0.23616938293) A[2]:(0.120770685375) A[3]:(0.229854539037)\n",
      " state (4)  A[0]:(0.247903332114) A[1]:(0.291950076818) A[2]:(0.0333482883871) A[3]:(0.241207361221)\n",
      " state (5)  A[0]:(0.242124214768) A[1]:(0.243747830391) A[2]:(0.0347657687962) A[3]:(0.221733108163)\n",
      " state (6)  A[0]:(0.239385291934) A[1]:(0.176982581615) A[2]:(0.103742323816) A[3]:(0.189645886421)\n",
      " state (7)  A[0]:(0.249137580395) A[1]:(0.176228478551) A[2]:(0.208302304149) A[3]:(0.16735856235)\n",
      " state (8)  A[0]:(0.272242993116) A[1]:(0.268367767334) A[2]:(0.334616005421) A[3]:(0.166306868196)\n",
      " state (9)  A[0]:(0.305899739265) A[1]:(0.432258218527) A[2]:(0.472322046757) A[3]:(0.188386052847)\n",
      " state (10)  A[0]:(0.346753835678) A[1]:(0.617564916611) A[2]:(0.607985377312) A[3]:(0.230421483517)\n",
      " state (11)  A[0]:(0.39176979661) A[1]:(0.773657500744) A[2]:(0.72726392746) A[3]:(0.287153065205)\n",
      " state (12)  A[0]:(0.438247054815) A[1]:(0.878295242786) A[2]:(0.820491433144) A[3]:(0.352467387915)\n",
      " state (13)  A[0]:(0.483783841133) A[1]:(0.937825381756) A[2]:(0.885957121849) A[3]:(0.420115947723)\n",
      " state (14)  A[0]:(0.526404201984) A[1]:(0.968472957611) A[2]:(0.928248405457) A[3]:(0.484614133835)\n",
      " state (15)  A[0]:(0.564746379852) A[1]:(0.983559608459) A[2]:(0.954161703587) A[3]:(0.542112112045)\n",
      "Episode 31000 finished after 0 timesteps with r=0.0. Running score: 0.06. Times trained:               7316. Times reached goal: 68.               Steps done: 206578. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.732027267042.\n",
      " state (0)  A[0]:(0.261528819799) A[1]:(0.286767542362) A[2]:(0.219972252846) A[3]:(0.262247681618)\n",
      " state (1)  A[0]:(0.246532544494) A[1]:(0.0143410861492) A[2]:(0.228291541338) A[3]:(0.216981053352)\n",
      " state (2)  A[0]:(0.255997717381) A[1]:(0.13073168695) A[2]:(0.233211427927) A[3]:(0.229010269046)\n",
      " state (3)  A[0]:(0.266965389252) A[1]:(0.258326679468) A[2]:(0.136576801538) A[3]:(0.255048662424)\n",
      " state (4)  A[0]:(0.272421747446) A[1]:(0.318787485361) A[2]:(0.0282262675464) A[3]:(0.270016133785)\n",
      " state (5)  A[0]:(0.266558229923) A[1]:(0.255826741457) A[2]:(0.0334913283587) A[3]:(0.249276027083)\n",
      " state (6)  A[0]:(0.26386448741) A[1]:(0.171263486147) A[2]:(0.11763355881) A[3]:(0.213331311941)\n",
      " state (7)  A[0]:(0.274037241936) A[1]:(0.167626023293) A[2]:(0.23377750814) A[3]:(0.187953963876)\n",
      " state (8)  A[0]:(0.297239392996) A[1]:(0.274082988501) A[2]:(0.364524722099) A[3]:(0.185745626688)\n",
      " state (9)  A[0]:(0.330212891102) A[1]:(0.459823638201) A[2]:(0.500691175461) A[3]:(0.20839817822)\n",
      " state (10)  A[0]:(0.369560450315) A[1]:(0.659328460693) A[2]:(0.631300747395) A[3]:(0.252146363258)\n",
      " state (11)  A[0]:(0.412404179573) A[1]:(0.814400732517) A[2]:(0.744349956512) A[3]:(0.311100900173)\n",
      " state (12)  A[0]:(0.456244200468) A[1]:(0.908624529839) A[2]:(0.831811130047) A[3]:(0.378484368324)\n",
      " state (13)  A[0]:(0.4988707304) A[1]:(0.957056820393) A[2]:(0.892781019211) A[3]:(0.447434365749)\n",
      " state (14)  A[0]:(0.538489341736) A[1]:(0.97974896431) A[2]:(0.931980669498) A[3]:(0.512109518051)\n",
      " state (15)  A[0]:(0.573908507824) A[1]:(0.990043640137) A[2]:(0.955959320068) A[3]:(0.568679094315)\n",
      "Episode 32000 finished after 0 timesteps with r=0.0. Running score: 0.1. Times trained:               7734. Times reached goal: 73.               Steps done: 214312. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.726387604843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.285646080971) A[1]:(0.330338418484) A[2]:(0.236838132143) A[3]:(0.284789860249)\n",
      " state (1)  A[0]:(0.268868774176) A[1]:(0.020412158221) A[2]:(0.243645653129) A[3]:(0.234715670347)\n",
      " state (2)  A[0]:(0.280047565699) A[1]:(0.149857252836) A[2]:(0.258383899927) A[3]:(0.245792970061)\n",
      " state (3)  A[0]:(0.294082760811) A[1]:(0.289743810892) A[2]:(0.157454907894) A[3]:(0.274800091982)\n",
      " state (4)  A[0]:(0.304842919111) A[1]:(0.362262636423) A[2]:(0.0280606374145) A[3]:(0.297128140926)\n",
      " state (5)  A[0]:(0.302724808455) A[1]:(0.286252558231) A[2]:(0.0398295708001) A[3]:(0.278912961483)\n",
      " state (6)  A[0]:(0.302416980267) A[1]:(0.176846966147) A[2]:(0.145107463002) A[3]:(0.241396814585)\n",
      " state (7)  A[0]:(0.313854217529) A[1]:(0.162203535438) A[2]:(0.276620090008) A[3]:(0.214155733585)\n",
      " state (8)  A[0]:(0.33661159873) A[1]:(0.279082685709) A[2]:(0.412293195724) A[3]:(0.211763426661)\n",
      " state (9)  A[0]:(0.367296397686) A[1]:(0.486504733562) A[2]:(0.545177161694) A[3]:(0.236230939627)\n",
      " state (10)  A[0]:(0.402847886086) A[1]:(0.699545800686) A[2]:(0.66787135601) A[3]:(0.283271968365)\n",
      " state (11)  A[0]:(0.440865159035) A[1]:(0.851302742958) A[2]:(0.771457016468) A[3]:(0.346075505018)\n",
      " state (12)  A[0]:(0.479299843311) A[1]:(0.933816432953) A[2]:(0.850106060505) A[3]:(0.416740328074)\n",
      " state (13)  A[0]:(0.516341388226) A[1]:(0.97164285183) A[2]:(0.904120802879) A[3]:(0.487439036369)\n",
      " state (14)  A[0]:(0.550542712212) A[1]:(0.987612187862) A[2]:(0.938487946987) A[3]:(0.551934123039)\n",
      " state (15)  A[0]:(0.580994069576) A[1]:(0.994251310825) A[2]:(0.959411561489) A[3]:(0.606660842896)\n",
      "Episode 33000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained:               7912. Times reached goal: 80.               Steps done: 222224. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.720663102109.\n",
      " state (0)  A[0]:(0.322380959988) A[1]:(0.358094364405) A[2]:(0.269424080849) A[3]:(0.322275847197)\n",
      " state (1)  A[0]:(0.30207157135) A[1]:(0.017646715045) A[2]:(0.274491727352) A[3]:(0.266767859459)\n",
      " state (2)  A[0]:(0.311617523432) A[1]:(0.161852449179) A[2]:(0.294057428837) A[3]:(0.274048119783)\n",
      " state (3)  A[0]:(0.322650164366) A[1]:(0.313120603561) A[2]:(0.184611782432) A[3]:(0.302819848061)\n",
      " state (4)  A[0]:(0.331903219223) A[1]:(0.396861463785) A[2]:(0.0240685641766) A[3]:(0.329874396324)\n",
      " state (5)  A[0]:(0.329136222601) A[1]:(0.303930729628) A[2]:(0.0379143320024) A[3]:(0.310652643442)\n",
      " state (6)  A[0]:(0.329313069582) A[1]:(0.164854213595) A[2]:(0.162953644991) A[3]:(0.266905725002)\n",
      " state (7)  A[0]:(0.341380894184) A[1]:(0.138934999704) A[2]:(0.307100117207) A[3]:(0.232863351703)\n",
      " state (8)  A[0]:(0.363332688808) A[1]:(0.270877808332) A[2]:(0.444551199675) A[3]:(0.225876197219)\n",
      " state (9)  A[0]:(0.3912974298) A[1]:(0.506137609482) A[2]:(0.572570443153) A[3]:(0.248607650399)\n",
      " state (10)  A[0]:(0.422568827868) A[1]:(0.734890222549) A[2]:(0.688131809235) A[3]:(0.296288490295)\n",
      " state (11)  A[0]:(0.455327212811) A[1]:(0.882247567177) A[2]:(0.784884929657) A[3]:(0.361118465662)\n",
      " state (12)  A[0]:(0.488073587418) A[1]:(0.953061580658) A[2]:(0.858057498932) A[3]:(0.433950573206)\n",
      " state (13)  A[0]:(0.519461035728) A[1]:(0.981769144535) A[2]:(0.908172905445) A[3]:(0.505867958069)\n",
      " state (14)  A[0]:(0.548410892487) A[1]:(0.992639899254) A[2]:(0.94003111124) A[3]:(0.570148825645)\n",
      " state (15)  A[0]:(0.57426905632) A[1]:(0.996775984764) A[2]:(0.959478199482) A[3]:(0.62341427803)\n",
      "Episode 34000 finished after 0 timesteps with r=0.0. Running score: 0.13. Times trained:               7804. Times reached goal: 75.               Steps done: 230028. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.715060935347.\n",
      "q_values \n",
      "tensor([[ 0.3535,  0.3812,  0.2961,  0.3527]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3576,  0.4361,  0.0162,  0.3590]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.353780835867) A[1]:(0.381704062223) A[2]:(0.296274662018) A[3]:(0.352800488472)\n",
      " state (1)  A[0]:(0.33457878232) A[1]:(0.0120640657842) A[2]:(0.298004567623) A[3]:(0.295932680368)\n",
      " state (2)  A[0]:(0.343122661114) A[1]:(0.174812719226) A[2]:(0.324125915766) A[3]:(0.299117326736)\n",
      " state (3)  A[0]:(0.350649535656) A[1]:(0.335562676191) A[2]:(0.214652150869) A[3]:(0.325840830803)\n",
      " state (4)  A[0]:(0.357770621777) A[1]:(0.43699234724) A[2]:(0.0161089077592) A[3]:(0.359066396952)\n",
      " state (5)  A[0]:(0.354521930218) A[1]:(0.338486909866) A[2]:(0.0232314392924) A[3]:(0.342926561832)\n",
      " state (6)  A[0]:(0.355621188879) A[1]:(0.170085638762) A[2]:(0.170455679297) A[3]:(0.295229673386)\n",
      " state (7)  A[0]:(0.369140863419) A[1]:(0.125966817141) A[2]:(0.331598818302) A[3]:(0.254487127066)\n",
      " state (8)  A[0]:(0.391196757555) A[1]:(0.26668664813) A[2]:(0.47355261445) A[3]:(0.242121100426)\n",
      " state (9)  A[0]:(0.417245239019) A[1]:(0.526172876358) A[2]:(0.59827709198) A[3]:(0.262603849173)\n",
      " state (10)  A[0]:(0.444930464029) A[1]:(0.767056703568) A[2]:(0.707790017128) A[3]:(0.311085909605)\n",
      " state (11)  A[0]:(0.473060458899) A[1]:(0.907494843006) A[2]:(0.798474669456) A[3]:(0.378623306751)\n",
      " state (12)  A[0]:(0.50072491169) A[1]:(0.96707290411) A[2]:(0.866610884666) A[3]:(0.454426258802)\n",
      " state (13)  A[0]:(0.527067005634) A[1]:(0.988420248032) A[2]:(0.912998199463) A[3]:(0.528165102005)\n",
      " state (14)  A[0]:(0.551384985447) A[1]:(0.995676219463) A[2]:(0.942354977131) A[3]:(0.592560470104)\n",
      " state (15)  A[0]:(0.573269605637) A[1]:(0.998208165169) A[2]:(0.960256993771) A[3]:(0.644526124001)\n",
      "Episode 35000 finished after 0 timesteps with r=0.0. Running score: 0.1. Times trained:               8056. Times reached goal: 69.               Steps done: 238084. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.709323545687.\n",
      " state (0)  A[0]:(0.35618185997) A[1]:(0.408131152391) A[2]:(0.307345718145) A[3]:(0.355716913939)\n",
      " state (1)  A[0]:(0.344640374184) A[1]:(0.0117190927267) A[2]:(0.306317806244) A[3]:(0.304934084415)\n",
      " state (2)  A[0]:(0.353944987059) A[1]:(0.189171940088) A[2]:(0.341303050518) A[3]:(0.304153650999)\n",
      " state (3)  A[0]:(0.361562788486) A[1]:(0.346453040838) A[2]:(0.248940736055) A[3]:(0.328735262156)\n",
      " state (4)  A[0]:(0.372044146061) A[1]:(0.458031594753) A[2]:(0.0226042214781) A[3]:(0.371736437082)\n",
      " state (5)  A[0]:(0.371842712164) A[1]:(0.357150733471) A[2]:(0.0134318443015) A[3]:(0.366012573242)\n",
      " state (6)  A[0]:(0.374894946814) A[1]:(0.1601883322) A[2]:(0.181849882007) A[3]:(0.320897191763)\n",
      " state (7)  A[0]:(0.39062705636) A[1]:(0.101345032454) A[2]:(0.362102597952) A[3]:(0.278992205858)\n",
      " state (8)  A[0]:(0.414005190134) A[1]:(0.259190231562) A[2]:(0.510363936424) A[3]:(0.265849649906)\n",
      " state (9)  A[0]:(0.439921081066) A[1]:(0.550914466381) A[2]:(0.633316218853) A[3]:(0.287955850363)\n",
      " state (10)  A[0]:(0.466285288334) A[1]:(0.802787899971) A[2]:(0.737562417984) A[3]:(0.340411692858)\n",
      " state (11)  A[0]:(0.492335647345) A[1]:(0.93159532547) A[2]:(0.821653723717) A[3]:(0.412718147039)\n",
      " state (12)  A[0]:(0.517514467239) A[1]:(0.978562176228) A[2]:(0.883143901825) A[3]:(0.491924911737)\n",
      " state (13)  A[0]:(0.541245341301) A[1]:(0.993192374706) A[2]:(0.923875629902) A[3]:(0.566287219524)\n",
      " state (14)  A[0]:(0.563060641289) A[1]:(0.997631549835) A[2]:(0.949075162411) A[3]:(0.628605127335)\n",
      " state (15)  A[0]:(0.582728028297) A[1]:(0.999058246613) A[2]:(0.964225828648) A[3]:(0.676878869534)\n",
      "Episode 36000 finished after 0 timesteps with r=0.0. Running score: 0.17. Times trained:               8217. Times reached goal: 93.               Steps done: 246301. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.703518915097.\n",
      " state (0)  A[0]:(0.389780580997) A[1]:(0.433192938566) A[2]:(0.341353714466) A[3]:(0.387548953295)\n",
      " state (1)  A[0]:(0.37817505002) A[1]:(0.00689115654677) A[2]:(0.337947070599) A[3]:(0.336410224438)\n",
      " state (2)  A[0]:(0.386118561029) A[1]:(0.203420847654) A[2]:(0.373139321804) A[3]:(0.332084208727)\n",
      " state (3)  A[0]:(0.388864010572) A[1]:(0.365589529276) A[2]:(0.279544532299) A[3]:(0.355642974377)\n",
      " state (4)  A[0]:(0.392990648746) A[1]:(0.481041580439) A[2]:(0.0150942066684) A[3]:(0.40592160821)\n",
      " state (5)  A[0]:(0.387378394604) A[1]:(0.360632777214) A[2]:(-0.0018487253692) A[3]:(0.404536992311)\n",
      " state (6)  A[0]:(0.38850736618) A[1]:(0.122675538063) A[2]:(0.189665094018) A[3]:(0.356162667274)\n",
      " state (7)  A[0]:(0.404946118593) A[1]:(0.0449730120599) A[2]:(0.383650660515) A[3]:(0.307050377131)\n",
      " state (8)  A[0]:(0.428405970335) A[1]:(0.219398140907) A[2]:(0.532055497169) A[3]:(0.287121832371)\n",
      " state (9)  A[0]:(0.452463686466) A[1]:(0.548474669456) A[2]:(0.649706304073) A[3]:(0.305430829525)\n",
      " state (10)  A[0]:(0.475299298763) A[1]:(0.81951880455) A[2]:(0.74858379364) A[3]:(0.357503980398)\n",
      " state (11)  A[0]:(0.496839016676) A[1]:(0.944394230843) A[2]:(0.828795313835) A[3]:(0.431774437428)\n",
      " state (12)  A[0]:(0.517214894295) A[1]:(0.984488964081) A[2]:(0.887665331364) A[3]:(0.513425946236)\n",
      " state (13)  A[0]:(0.536432147026) A[1]:(0.99554258585) A[2]:(0.926591694355) A[3]:(0.589171051979)\n",
      " state (14)  A[0]:(0.554423391819) A[1]:(0.998564183712) A[2]:(0.950562238693) A[3]:(0.651370286942)\n",
      " state (15)  A[0]:(0.571139574051) A[1]:(0.999459207058) A[2]:(0.964913904667) A[3]:(0.698455929756)\n",
      "Episode 37000 finished after 0 timesteps with r=1.0. Running score: 0.13. Times trained:               8139. Times reached goal: 85.               Steps done: 254440. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.697816213272.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.410114854574) A[1]:(0.464584290981) A[2]:(0.362627565861) A[3]:(0.410435378551)\n",
      " state (1)  A[0]:(0.402947753668) A[1]:(0.00673005264252) A[2]:(0.357406884432) A[3]:(0.358437746763)\n",
      " state (2)  A[0]:(0.413579165936) A[1]:(0.223620966077) A[2]:(0.394717782736) A[3]:(0.349259585142)\n",
      " state (3)  A[0]:(0.416905254126) A[1]:(0.391060441732) A[2]:(0.307314753532) A[3]:(0.369366854429)\n",
      " state (4)  A[0]:(0.421249955893) A[1]:(0.513720154762) A[2]:(0.0082191163674) A[3]:(0.426080286503)\n",
      " state (5)  A[0]:(0.415838778019) A[1]:(0.390908718109) A[2]:(-0.025720525533) A[3]:(0.431694239378)\n",
      " state (6)  A[0]:(0.418680638075) A[1]:(0.12199447304) A[2]:(0.191410809755) A[3]:(0.381920516491)\n",
      " state (7)  A[0]:(0.438847720623) A[1]:(0.0213013403118) A[2]:(0.406371206045) A[3]:(0.325521439314)\n",
      " state (8)  A[0]:(0.465098083019) A[1]:(0.203214511275) A[2]:(0.560511648655) A[3]:(0.297640264034)\n",
      " state (9)  A[0]:(0.489609152079) A[1]:(0.5594830513) A[2]:(0.676055729389) A[3]:(0.311324685812)\n",
      " state (10)  A[0]:(0.510842204094) A[1]:(0.839790046215) A[2]:(0.770490527153) A[3]:(0.363173902035)\n",
      " state (11)  A[0]:(0.529466152191) A[1]:(0.956109702587) A[2]:(0.845893383026) A[3]:(0.440351039171)\n",
      " state (12)  A[0]:(0.546360850334) A[1]:(0.989061057568) A[2]:(0.900244891644) A[3]:(0.525564491749)\n",
      " state (13)  A[0]:(0.562166213989) A[1]:(0.997135519981) A[2]:(0.93541520834) A[3]:(0.603526175022)\n",
      " state (14)  A[0]:(0.577249407768) A[1]:(0.99913752079) A[2]:(0.956623792648) A[3]:(0.666096150875)\n",
      " state (15)  A[0]:(0.59175401926) A[1]:(0.999688982964) A[2]:(0.969114899635) A[3]:(0.712275743484)\n",
      "Episode 38000 finished after 0 timesteps with r=0.0. Running score: 0.08. Times trained:               7907. Times reached goal: 83.               Steps done: 262347. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.692320337055.\n",
      " state (0)  A[0]:(0.425056636333) A[1]:(0.481830090284) A[2]:(0.372287482023) A[3]:(0.423158943653)\n",
      " state (1)  A[0]:(0.420159846544) A[1]:(0.00499484268948) A[2]:(0.366705358028) A[3]:(0.371314525604)\n",
      " state (2)  A[0]:(0.431390196085) A[1]:(0.242857649922) A[2]:(0.405790656805) A[3]:(0.357474297285)\n",
      " state (3)  A[0]:(0.43041241169) A[1]:(0.41214582324) A[2]:(0.330728083849) A[3]:(0.373380124569)\n",
      " state (4)  A[0]:(0.425514638424) A[1]:(0.535794615746) A[2]:(0.00668801832944) A[3]:(0.436145663261)\n",
      " state (5)  A[0]:(0.412799119949) A[1]:(0.413125455379) A[2]:(-0.052192337811) A[3]:(0.452031284571)\n",
      " state (6)  A[0]:(0.413515657187) A[1]:(0.118864417076) A[2]:(0.187563478947) A[3]:(0.404011487961)\n",
      " state (7)  A[0]:(0.436052858829) A[1]:(0.00185270397924) A[2]:(0.421729177237) A[3]:(0.342043668032)\n",
      " state (8)  A[0]:(0.464769095182) A[1]:(0.196062579751) A[2]:(0.579242825508) A[3]:(0.306315898895)\n",
      " state (9)  A[0]:(0.489636510611) A[1]:(0.578189611435) A[2]:(0.691386699677) A[3]:(0.314619004726)\n",
      " state (10)  A[0]:(0.509155631065) A[1]:(0.861018061638) A[2]:(0.781804978848) A[3]:(0.365591645241)\n",
      " state (11)  A[0]:(0.524701058865) A[1]:(0.966138362885) A[2]:(0.85421782732) A[3]:(0.445748090744)\n",
      " state (12)  A[0]:(0.53794926405) A[1]:(0.992459893227) A[2]:(0.906372070312) A[3]:(0.535348713398)\n",
      " state (13)  A[0]:(0.55025434494) A[1]:(0.998206853867) A[2]:(0.939831972122) A[3]:(0.616734802723)\n",
      " state (14)  A[0]:(0.562475562096) A[1]:(0.999498665333) A[2]:(0.959755003452) A[3]:(0.680906057358)\n",
      " state (15)  A[0]:(0.574957072735) A[1]:(0.999828338623) A[2]:(0.971344351768) A[3]:(0.727253556252)\n",
      "Episode 39000 finished after 0 timesteps with r=0.0. Running score: 0.05. Times trained:               8018. Times reached goal: 82.               Steps done: 270365. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.68679150729.\n",
      "q_values \n",
      "tensor([[ 0.4287,  0.4825,  0.3875,  0.4269]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4249,  0.5326,  0.0120,  0.4483]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.428859263659) A[1]:(0.482542872429) A[2]:(0.387283533812) A[3]:(0.426863431931)\n",
      " state (1)  A[0]:(0.425442218781) A[1]:(-0.00808279495686) A[2]:(0.380800068378) A[3]:(0.381861925125)\n",
      " state (2)  A[0]:(0.43928360939) A[1]:(0.244397193193) A[2]:(0.419740378857) A[3]:(0.363581568003)\n",
      " state (3)  A[0]:(0.436770230532) A[1]:(0.411781460047) A[2]:(0.357857763767) A[3]:(0.376261472702)\n",
      " state (4)  A[0]:(0.424694418907) A[1]:(0.532157242298) A[2]:(0.0116413934156) A[3]:(0.448237210512)\n",
      " state (5)  A[0]:(0.406976759434) A[1]:(0.405090898275) A[2]:(-0.084423571825) A[3]:(0.478748619556)\n",
      " state (6)  A[0]:(0.40926733613) A[1]:(0.0792318060994) A[2]:(0.175330385566) A[3]:(0.434368044138)\n",
      " state (7)  A[0]:(0.438231706619) A[1]:(-0.0561463870108) A[2]:(0.429879426956) A[3]:(0.366566747427)\n",
      " state (8)  A[0]:(0.472528547049) A[1]:(0.149363562465) A[2]:(0.591156363487) A[3]:(0.321400105953)\n",
      " state (9)  A[0]:(0.499750763178) A[1]:(0.56801199913) A[2]:(0.700414776802) A[3]:(0.322752058506)\n",
      " state (10)  A[0]:(0.518777370453) A[1]:(0.869156479836) A[2]:(0.787968695164) A[3]:(0.372203737497)\n",
      " state (11)  A[0]:(0.531973004341) A[1]:(0.971475541592) A[2]:(0.858920931816) A[3]:(0.455689907074)\n",
      " state (12)  A[0]:(0.542005002499) A[1]:(0.994318842888) A[2]:(0.910246729851) A[3]:(0.550414264202)\n",
      " state (13)  A[0]:(0.551097333431) A[1]:(0.998774170876) A[2]:(0.942933917046) A[3]:(0.63571369648)\n",
      " state (14)  A[0]:(0.560712456703) A[1]:(0.999682188034) A[2]:(0.962126135826) A[3]:(0.701580882072)\n",
      " state (15)  A[0]:(0.571442484856) A[1]:(0.99989682436) A[2]:(0.973125755787) A[3]:(0.747971534729)\n",
      "Episode 40000 finished after 0 timesteps with r=0.0. Running score: 0.11. Times trained:               8331. Times reached goal: 106.               Steps done: 278696. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.68109361477.\n",
      " state (0)  A[0]:(0.451120197773) A[1]:(0.507247805595) A[2]:(0.404868334532) A[3]:(0.451888650656)\n",
      " state (1)  A[0]:(0.450354158878) A[1]:(-0.00299321720377) A[2]:(0.402367711067) A[3]:(0.406058222055)\n",
      " state (2)  A[0]:(0.469468891621) A[1]:(0.266694307327) A[2]:(0.441033005714) A[3]:(0.383520543575)\n",
      " state (3)  A[0]:(0.469203740358) A[1]:(0.440577566624) A[2]:(0.389072060585) A[3]:(0.392106354237)\n",
      " state (4)  A[0]:(0.451640993357) A[1]:(0.565216600895) A[2]:(0.0190275590867) A[3]:(0.471722513437)\n",
      " state (5)  A[0]:(0.428599178791) A[1]:(0.451487392187) A[2]:(-0.118101768196) A[3]:(0.516790151596)\n",
      " state (6)  A[0]:(0.432063877583) A[1]:(0.107986874878) A[2]:(0.169312104583) A[3]:(0.477270662785)\n",
      " state (7)  A[0]:(0.46708163619) A[1]:(-0.0521001704037) A[2]:(0.45368295908) A[3]:(0.404232263565)\n",
      " state (8)  A[0]:(0.506801366806) A[1]:(0.155597418547) A[2]:(0.622475147247) A[3]:(0.348409414291)\n",
      " state (9)  A[0]:(0.53617477417) A[1]:(0.591846227646) A[2]:(0.728947162628) A[3]:(0.340866476297)\n",
      " state (10)  A[0]:(0.554339408875) A[1]:(0.888229906559) A[2]:(0.811306357384) A[3]:(0.387082874775)\n",
      " state (11)  A[0]:(0.564664006233) A[1]:(0.978386521339) A[2]:(0.876976549625) A[3]:(0.472358524799)\n",
      " state (12)  A[0]:(0.570915281773) A[1]:(0.99615752697) A[2]:(0.923501610756) A[3]:(0.570373535156)\n",
      " state (13)  A[0]:(0.576332449913) A[1]:(0.999244511127) A[2]:(0.952309548855) A[3]:(0.657424688339)\n",
      " state (14)  A[0]:(0.583070099354) A[1]:(0.999816596508) A[2]:(0.968739509583) A[3]:(0.722946643829)\n",
      " state (15)  A[0]:(0.591961860657) A[1]:(0.999942839146) A[2]:(0.977928698063) A[3]:(0.767815053463)\n",
      "Episode 41000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               8716. Times reached goal: 108.               Steps done: 287412. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.675182998707.\n",
      " state (0)  A[0]:(0.48010674119) A[1]:(0.528092563152) A[2]:(0.428082376719) A[3]:(0.479975879192)\n",
      " state (1)  A[0]:(0.476889789104) A[1]:(-0.0040373285301) A[2]:(0.425971478224) A[3]:(0.433492928743)\n",
      " state (2)  A[0]:(0.498840868473) A[1]:(0.277589708567) A[2]:(0.459271043539) A[3]:(0.405687898397)\n",
      " state (3)  A[0]:(0.497948139906) A[1]:(0.456275969744) A[2]:(0.409514963627) A[3]:(0.407968431711)\n",
      " state (4)  A[0]:(0.468665033579) A[1]:(0.582787394524) A[2]:(0.00822561327368) A[3]:(0.490786105394)\n",
      " state (5)  A[0]:(0.432544231415) A[1]:(0.474331140518) A[2]:(-0.17004147172) A[3]:(0.544550418854)\n",
      " state (6)  A[0]:(0.433074086905) A[1]:(0.112742930651) A[2]:(0.143887802958) A[3]:(0.503997921944)\n",
      " state (7)  A[0]:(0.471889942884) A[1]:(-0.0610508397222) A[2]:(0.455813258886) A[3]:(0.420550018549)\n",
      " state (8)  A[0]:(0.515490293503) A[1]:(0.158372908831) A[2]:(0.629327893257) A[3]:(0.34995663166)\n",
      " state (9)  A[0]:(0.545951724052) A[1]:(0.613980770111) A[2]:(0.732518196106) A[3]:(0.330876350403)\n",
      " state (10)  A[0]:(0.562481641769) A[1]:(0.903727650642) A[2]:(0.812283456326) A[3]:(0.373331278563)\n",
      " state (11)  A[0]:(0.569229781628) A[1]:(0.983239889145) A[2]:(0.877510309219) A[3]:(0.46246227622)\n",
      " state (12)  A[0]:(0.570963025093) A[1]:(0.997302949429) A[2]:(0.924368202686) A[3]:(0.567853927612)\n",
      " state (13)  A[0]:(0.572000086308) A[1]:(0.999512314796) A[2]:(0.953307986259) A[3]:(0.66127872467)\n",
      " state (14)  A[0]:(0.575320720673) A[1]:(0.999888658524) A[2]:(0.969616889954) A[3]:(0.730448246002)\n",
      " state (15)  A[0]:(0.582082986832) A[1]:(0.999966681004) A[2]:(0.978610515594) A[3]:(0.776807963848)\n",
      "Episode 42000 finished after 0 timesteps with r=0.0. Running score: 0.13. Times trained:               8521. Times reached goal: 89.               Steps done: 295933. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.669454206556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.475173830986) A[1]:(0.526497483253) A[2]:(0.426045805216) A[3]:(0.475923985243)\n",
      " state (1)  A[0]:(0.473769307137) A[1]:(-0.00375745911151) A[2]:(0.424537420273) A[3]:(0.435409545898)\n",
      " state (2)  A[0]:(0.499886959791) A[1]:(0.276305228472) A[2]:(0.452320754528) A[3]:(0.403189659119)\n",
      " state (3)  A[0]:(0.50139605999) A[1]:(0.454052656889) A[2]:(0.413403481245) A[3]:(0.399151623249)\n",
      " state (4)  A[0]:(0.463885992765) A[1]:(0.585770249367) A[2]:(0.0127104828134) A[3]:(0.486172527075)\n",
      " state (5)  A[0]:(0.416241139174) A[1]:(0.504350304604) A[2]:(-0.228758126497) A[3]:(0.559119522572)\n",
      " state (6)  A[0]:(0.416905015707) A[1]:(0.138903871179) A[2]:(0.0983256623149) A[3]:(0.526880502701)\n",
      " state (7)  A[0]:(0.463373839855) A[1]:(-0.061346706003) A[2]:(0.446770131588) A[3]:(0.440426766872)\n",
      " state (8)  A[0]:(0.51483386755) A[1]:(0.149003744125) A[2]:(0.631639003754) A[3]:(0.35923615098)\n",
      " state (9)  A[0]:(0.549943685532) A[1]:(0.615406751633) A[2]:(0.734010279179) A[3]:(0.330003976822)\n",
      " state (10)  A[0]:(0.568028151989) A[1]:(0.909024238586) A[2]:(0.812018036842) A[3]:(0.368871957064)\n",
      " state (11)  A[0]:(0.574156999588) A[1]:(0.985305666924) A[2]:(0.877304077148) A[3]:(0.461720526218)\n",
      " state (12)  A[0]:(0.574084043503) A[1]:(0.997813999653) A[2]:(0.924886524677) A[3]:(0.574065089226)\n",
      " state (13)  A[0]:(0.573100566864) A[1]:(0.999630868435) A[2]:(0.954179108143) A[3]:(0.672993063927)\n",
      " state (14)  A[0]:(0.574915826321) A[1]:(0.999919831753) A[2]:(0.970458745956) A[3]:(0.744721293449)\n",
      " state (15)  A[0]:(0.580983877182) A[1]:(0.999976754189) A[2]:(0.979286909103) A[3]:(0.791592419147)\n",
      "Episode 43000 finished after 0 timesteps with r=0.0. Running score: 0.13. Times trained:               8695. Times reached goal: 114.               Steps done: 304628. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.663658535425.\n",
      " state (0)  A[0]:(0.483195692301) A[1]:(0.532175123692) A[2]:(0.432322531939) A[3]:(0.481352597475)\n",
      " state (1)  A[0]:(0.480652868748) A[1]:(0.00141906645149) A[2]:(0.434425413609) A[3]:(0.442670732737)\n",
      " state (2)  A[0]:(0.510553956032) A[1]:(0.277425080538) A[2]:(0.454916954041) A[3]:(0.407188534737)\n",
      " state (3)  A[0]:(0.515415906906) A[1]:(0.454455971718) A[2]:(0.418134182692) A[3]:(0.397126853466)\n",
      " state (4)  A[0]:(0.471382796764) A[1]:(0.591477751732) A[2]:(0.00940837524831) A[3]:(0.48504832387)\n",
      " state (5)  A[0]:(0.411817848682) A[1]:(0.525907814503) A[2]:(-0.280183285475) A[3]:(0.570663809776)\n",
      " state (6)  A[0]:(0.412061482668) A[1]:(0.144703283906) A[2]:(0.061674091965) A[3]:(0.543261349201)\n",
      " state (7)  A[0]:(0.462832123041) A[1]:(-0.0807539448142) A[2]:(0.440150439739) A[3]:(0.454000502825)\n",
      " state (8)  A[0]:(0.517925143242) A[1]:(0.128224983811) A[2]:(0.630240917206) A[3]:(0.363995194435)\n",
      " state (9)  A[0]:(0.554515063763) A[1]:(0.612719535828) A[2]:(0.729343652725) A[3]:(0.325810939074)\n",
      " state (10)  A[0]:(0.572096347809) A[1]:(0.912934780121) A[2]:(0.806045591831) A[3]:(0.361389905214)\n",
      " state (11)  A[0]:(0.576154053211) A[1]:(0.986855626106) A[2]:(0.873286008835) A[3]:(0.458010584116)\n",
      " state (12)  A[0]:(0.573080182076) A[1]:(0.998175323009) A[2]:(0.923411250114) A[3]:(0.577532708645)\n",
      " state (13)  A[0]:(0.569042265415) A[1]:(0.99970972538) A[2]:(0.954150915146) A[3]:(0.682247161865)\n",
      " state (14)  A[0]:(0.568521380424) A[1]:(0.999939620495) A[2]:(0.970924735069) A[3]:(0.756768763065)\n",
      " state (15)  A[0]:(0.573309600353) A[1]:(0.999982893467) A[2]:(0.979820668697) A[3]:(0.804364442825)\n",
      "Episode 44000 finished after 0 timesteps with r=0.0. Running score: 0.17. Times trained:               8174. Times reached goal: 118.               Steps done: 312802. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.658255901203.\n",
      "q_values \n",
      "tensor([[ 0.4830,  0.5343,  0.4369,  0.4823]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4829,  0.5341,  0.4368,  0.4822]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4828,  0.5339,  0.4367,  0.4822]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4827,  0.5336,  0.4365,  0.4821]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4800,  0.0015,  0.4388,  0.4507]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5139,  0.2731,  0.4525,  0.4135]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3959,  0.2064,  0.0308,  0.5663]], device='cuda:0')\n",
      "On state=6, selected action=3 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5137,  0.2720,  0.4522,  0.4134]], device='cuda:0')\n",
      "On state=2, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5226,  0.4508,  0.4207,  0.3982]], device='cuda:0')\n",
      "On state=3, selected action=0 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5136,  0.2704,  0.4518,  0.4132]], device='cuda:0')\n",
      "On state=2, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5223,  0.4485,  0.4201,  0.3978]], device='cuda:0')\n",
      "On state=3, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5221,  0.4471,  0.4198,  0.3975]], device='cuda:0')\n",
      "On state=3, selected action=0 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5132,  0.2675,  0.4511,  0.4126]], device='cuda:0')\n",
      "On state=2, selected action=0 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4791, -0.0042,  0.4373,  0.4498]], device='cuda:0')\n",
      "On state=1, selected action=0 , Random? False\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4818,  0.5309,  0.4352,  0.4816]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4789, -0.0058,  0.4372,  0.4496]], device='cuda:0')\n",
      "On state=1, selected action=3 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4787, -0.0067,  0.4372,  0.4495]], device='cuda:0')\n",
      "On state=1, selected action=0 , Random? False\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4817,  0.5303,  0.4352,  0.4815]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4699,  0.5891,  0.0232,  0.4834]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5121,  0.1320,  0.6342,  0.3746]], device='cuda:0')\n",
      "On state=8, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5119,  0.1289,  0.6339,  0.3738]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5518,  0.6188,  0.7278,  0.3242]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5705,  0.9181,  0.8017,  0.3543]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5608,  1.0000,  0.9716,  0.7667]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5702,  0.9175,  0.8016,  0.3532]], device='cuda:0')\n",
      "On state=10, selected action=2 , Random? True\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.482122212648) A[1]:(0.530593037605) A[2]:(0.43521040678) A[3]:(0.481386214495)\n",
      " state (1)  A[0]:(0.478979676962) A[1]:(-0.00940015073866) A[2]:(0.437251120806) A[3]:(0.448803335428)\n",
      " state (2)  A[0]:(0.512479186058) A[1]:(0.258871495724) A[2]:(0.450716912746) A[3]:(0.411112368107)\n",
      " state (3)  A[0]:(0.520837366581) A[1]:(0.435564607382) A[2]:(0.419550895691) A[3]:(0.395347714424)\n",
      " state (4)  A[0]:(0.469558417797) A[1]:(0.583965539932) A[2]:(0.023765200749) A[3]:(0.481953978539)\n",
      " state (5)  A[0]:(0.392816364765) A[1]:(0.549606323242) A[2]:(-0.318532317877) A[3]:(0.582301795483)\n",
      " state (6)  A[0]:(0.392279595137) A[1]:(0.168096899986) A[2]:(0.0303684044629) A[3]:(0.561766266823)\n",
      " state (7)  A[0]:(0.449961185455) A[1]:(-0.0823272019625) A[2]:(0.439489305019) A[3]:(0.470572829247)\n",
      " state (8)  A[0]:(0.511413156986) A[1]:(0.120307646692) A[2]:(0.633901238441) A[3]:(0.371146351099)\n",
      " state (9)  A[0]:(0.551631569862) A[1]:(0.615198373795) A[2]:(0.727960288525) A[3]:(0.322259545326)\n",
      " state (10)  A[0]:(0.570399880409) A[1]:(0.91738986969) A[2]:(0.80174267292) A[3]:(0.352754354477)\n",
      " state (11)  A[0]:(0.573806285858) A[1]:(0.988277673721) A[2]:(0.870082080364) A[3]:(0.452095121145)\n",
      " state (12)  A[0]:(0.568960487843) A[1]:(0.998474001884) A[2]:(0.922427475452) A[3]:(0.578505277634)\n",
      " state (13)  A[0]:(0.562915086746) A[1]:(0.999770224094) A[2]:(0.954434096813) A[3]:(0.68895483017)\n",
      " state (14)  A[0]:(0.560910105705) A[1]:(0.999953985214) A[2]:(0.971569120884) A[3]:(0.766183853149)\n",
      " state (15)  A[0]:(0.565067052841) A[1]:(0.999987244606) A[2]:(0.980445444584) A[3]:(0.814410686493)\n",
      "Episode 45000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained:               8444. Times reached goal: 119.               Steps done: 321246. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.652720989657.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.487946182489) A[1]:(0.539795398712) A[2]:(0.431763648987) A[3]:(0.486817896366)\n",
      " state (1)  A[0]:(0.481967747211) A[1]:(0.00889942422509) A[2]:(0.440789908171) A[3]:(0.457436084747)\n",
      " state (2)  A[0]:(0.520014166832) A[1]:(0.268899828196) A[2]:(0.448161572218) A[3]:(0.420836746693)\n",
      " state (3)  A[0]:(0.533056139946) A[1]:(0.443047940731) A[2]:(0.418048650026) A[3]:(0.403461366892)\n",
      " state (4)  A[0]:(0.475201427937) A[1]:(0.601809978485) A[2]:(0.0221679247916) A[3]:(0.490580707788)\n",
      " state (5)  A[0]:(0.382103532553) A[1]:(0.590385138988) A[2]:(-0.347964823246) A[3]:(0.600881099701)\n",
      " state (6)  A[0]:(0.386315226555) A[1]:(0.203974097967) A[2]:(0.0260264500976) A[3]:(0.586496651173)\n",
      " state (7)  A[0]:(0.454073429108) A[1]:(-0.0694350898266) A[2]:(0.459256201982) A[3]:(0.496860295534)\n",
      " state (8)  A[0]:(0.521868944168) A[1]:(0.136913597584) A[2]:(0.647465586662) A[3]:(0.392290562391)\n",
      " state (9)  A[0]:(0.56509244442) A[1]:(0.638982176781) A[2]:(0.730948030949) A[3]:(0.335161805153)\n",
      " state (10)  A[0]:(0.584909319878) A[1]:(0.92779302597) A[2]:(0.799461960793) A[3]:(0.360955089331)\n",
      " state (11)  A[0]:(0.588032722473) A[1]:(0.99049192667) A[2]:(0.868369042873) A[3]:(0.46186119318)\n",
      " state (12)  A[0]:(0.58204293251) A[1]:(0.998848438263) A[2]:(0.922874569893) A[3]:(0.592880964279)\n",
      " state (13)  A[0]:(0.574640154839) A[1]:(0.999836802483) A[2]:(0.955938577652) A[3]:(0.70631980896)\n",
      " state (14)  A[0]:(0.57167327404) A[1]:(0.999968647957) A[2]:(0.973167598248) A[3]:(0.783881485462)\n",
      " state (15)  A[0]:(0.575523674488) A[1]:(0.999991476536) A[2]:(0.98181951046) A[3]:(0.831109642982)\n",
      "Episode 46000 finished after 0 timesteps with r=1.0. Running score: 0.09. Times trained:               8733. Times reached goal: 109.               Steps done: 329979. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.647045594935.\n",
      " state (0)  A[0]:(0.486725687981) A[1]:(0.540479958057) A[2]:(0.436533659697) A[3]:(0.487964093685)\n",
      " state (1)  A[0]:(0.481114625931) A[1]:(0.0110313296318) A[2]:(0.447719812393) A[3]:(0.461435347795)\n",
      " state (2)  A[0]:(0.523609638214) A[1]:(0.263637810946) A[2]:(0.447341859341) A[3]:(0.425185114145)\n",
      " state (3)  A[0]:(0.542911171913) A[1]:(0.437640458345) A[2]:(0.416440725327) A[3]:(0.403533905745)\n",
      " state (4)  A[0]:(0.485548734665) A[1]:(0.608830630779) A[2]:(0.0267573483288) A[3]:(0.484023302794)\n",
      " state (5)  A[0]:(0.380345463753) A[1]:(0.627982020378) A[2]:(-0.386622339487) A[3]:(0.601822257042)\n",
      " state (6)  A[0]:(0.387871623039) A[1]:(0.243406206369) A[2]:(-0.0048811645247) A[3]:(0.592793941498)\n",
      " state (7)  A[0]:(0.46168127656) A[1]:(-0.0607928782701) A[2]:(0.456929683685) A[3]:(0.503885865211)\n",
      " state (8)  A[0]:(0.532469272614) A[1]:(0.14055544138) A[2]:(0.64362680912) A[3]:(0.393167346716)\n",
      " state (9)  A[0]:(0.577258229256) A[1]:(0.649823307991) A[2]:(0.717938184738) A[3]:(0.326049089432)\n",
      " state (10)  A[0]:(0.598062992096) A[1]:(0.933430433273) A[2]:(0.782858490944) A[3]:(0.345501095057)\n",
      " state (11)  A[0]:(0.601589977741) A[1]:(0.991743683815) A[2]:(0.856166243553) A[3]:(0.448258191347)\n",
      " state (12)  A[0]:(0.595403790474) A[1]:(0.999061644077) A[2]:(0.917125344276) A[3]:(0.586990654469)\n",
      " state (13)  A[0]:(0.587488949299) A[1]:(0.999874591827) A[2]:(0.954116344452) A[3]:(0.707797050476)\n",
      " state (14)  A[0]:(0.584116578102) A[1]:(0.999976933002) A[2]:(0.972911417484) A[3]:(0.789437055588)\n",
      " state (15)  A[0]:(0.587919473648) A[1]:(0.999993920326) A[2]:(0.982045888901) A[3]:(0.838232636452)\n",
      "Episode 47000 finished after 0 timesteps with r=0.0. Running score: 0.15. Times trained:               8753. Times reached goal: 109.               Steps done: 338732. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.641406719383.\n",
      " state (0)  A[0]:(0.482120662928) A[1]:(0.529136955738) A[2]:(0.424432724714) A[3]:(0.482365757227)\n",
      " state (1)  A[0]:(0.473941087723) A[1]:(0.00841597747058) A[2]:(0.43538954854) A[3]:(0.460172832012)\n",
      " state (2)  A[0]:(0.51832139492) A[1]:(0.242204591632) A[2]:(0.428261011839) A[3]:(0.425383001566)\n",
      " state (3)  A[0]:(0.540830790997) A[1]:(0.40666911006) A[2]:(0.399260520935) A[3]:(0.401768505573)\n",
      " state (4)  A[0]:(0.479760497808) A[1]:(0.586990118027) A[2]:(0.0286584682763) A[3]:(0.475496560335)\n",
      " state (5)  A[0]:(0.352808743715) A[1]:(0.639237999916) A[2]:(-0.420015841722) A[3]:(0.600707828999)\n",
      " state (6)  A[0]:(0.361005455256) A[1]:(0.252444148064) A[2]:(-0.036296389997) A[3]:(0.598997950554)\n",
      " state (7)  A[0]:(0.444347262383) A[1]:(-0.0799816921353) A[2]:(0.453796684742) A[3]:(0.512271523476)\n",
      " state (8)  A[0]:(0.52121758461) A[1]:(0.118871450424) A[2]:(0.64066529274) A[3]:(0.397091954947)\n",
      " state (9)  A[0]:(0.568990409374) A[1]:(0.647705614567) A[2]:(0.707627892494) A[3]:(0.3221103549)\n",
      " state (10)  A[0]:(0.591176152229) A[1]:(0.936578214169) A[2]:(0.770484566689) A[3]:(0.337159782648)\n",
      " state (11)  A[0]:(0.595212817192) A[1]:(0.992635965347) A[2]:(0.848707914352) A[3]:(0.442107260227)\n",
      " state (12)  A[0]:(0.589185357094) A[1]:(0.999211251736) A[2]:(0.915127754211) A[3]:(0.58640152216)\n",
      " state (13)  A[0]:(0.581585645676) A[1]:(0.999898731709) A[2]:(0.9545378685) A[3]:(0.71111035347)\n",
      " state (14)  A[0]:(0.578972041607) A[1]:(0.999981641769) A[2]:(0.973817646503) A[3]:(0.7938798666)\n",
      " state (15)  A[0]:(0.583929777145) A[1]:(0.999995112419) A[2]:(0.98284894228) A[3]:(0.842393636703)\n",
      "Episode 48000 finished after 0 timesteps with r=0.0. Running score: 0.15. Times trained:               8991. Times reached goal: 134.               Steps done: 347723. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.635665679091.\n",
      " state (0)  A[0]:(0.478910923004) A[1]:(0.530764102936) A[2]:(0.420019119978) A[3]:(0.479164958)\n",
      " state (1)  A[0]:(0.467083573341) A[1]:(0.0165304094553) A[2]:(0.434695929289) A[3]:(0.458525002003)\n",
      " state (2)  A[0]:(0.513980567455) A[1]:(0.239971533418) A[2]:(0.421502768993) A[3]:(0.426535278559)\n",
      " state (3)  A[0]:(0.540676355362) A[1]:(0.401773899794) A[2]:(0.389798045158) A[3]:(0.401744961739)\n",
      " state (4)  A[0]:(0.479284316301) A[1]:(0.594950556755) A[2]:(0.0215220600367) A[3]:(0.464669913054)\n",
      " state (5)  A[0]:(0.337021976709) A[1]:(0.677987337112) A[2]:(-0.458792805672) A[3]:(0.587531685829)\n",
      " state (6)  A[0]:(0.348889261484) A[1]:(0.300753325224) A[2]:(-0.069643586874) A[3]:(0.590523719788)\n",
      " state (7)  A[0]:(0.439662843943) A[1]:(-0.0676280558109) A[2]:(0.453199863434) A[3]:(0.507851362228)\n",
      " state (8)  A[0]:(0.519423961639) A[1]:(0.121147833765) A[2]:(0.641378521919) A[3]:(0.391391515732)\n",
      " state (9)  A[0]:(0.568233728409) A[1]:(0.657743513584) A[2]:(0.700135588646) A[3]:(0.310724377632)\n",
      " state (10)  A[0]:(0.591163039207) A[1]:(0.941833436489) A[2]:(0.759288787842) A[3]:(0.322003990412)\n",
      " state (11)  A[0]:(0.595936834812) A[1]:(0.993677437305) A[2]:(0.841600358486) A[3]:(0.429394185543)\n",
      " state (12)  A[0]:(0.590683698654) A[1]:(0.99936413765) A[2]:(0.913453876972) A[3]:(0.580392122269)\n",
      " state (13)  A[0]:(0.583985209465) A[1]:(0.999922275543) A[2]:(0.955317556858) A[3]:(0.710797309875)\n",
      " state (14)  A[0]:(0.582442522049) A[1]:(0.999986290932) A[2]:(0.975064575672) A[3]:(0.796284496784)\n",
      " state (15)  A[0]:(0.58855754137) A[1]:(0.999996364117) A[2]:(0.98397821188) A[3]:(0.845614790916)\n",
      "Episode 49000 finished after 0 timesteps with r=0.0. Running score: 0.1. Times trained:               8673. Times reached goal: 122.               Steps done: 356396. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.63017638937.\n",
      "q_values \n",
      "tensor([[ 0.4889,  0.5391,  0.4332,  0.4897]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4760,  0.0145,  0.4485,  0.4692]], device='cuda:0')\n",
      "On state=1, selected action=1 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.488742470741) A[1]:(0.538965880871) A[2]:(0.433016330004) A[3]:(0.489599317312)\n",
      " state (1)  A[0]:(0.475935786963) A[1]:(0.0143575640395) A[2]:(0.448367089033) A[3]:(0.469059437513)\n",
      " state (2)  A[0]:(0.522803544998) A[1]:(0.225670680404) A[2]:(0.430586397648) A[3]:(0.441719055176)\n",
      " state (3)  A[0]:(0.551673173904) A[1]:(0.383495807648) A[2]:(0.396209657192) A[3]:(0.418134510517)\n",
      " state (4)  A[0]:(0.492804318666) A[1]:(0.591512739658) A[2]:(0.0279714800417) A[3]:(0.470191657543)\n",
      " state (5)  A[0]:(0.343633979559) A[1]:(0.703508496284) A[2]:(-0.485489666462) A[3]:(0.584495425224)\n",
      " state (6)  A[0]:(0.360691785812) A[1]:(0.325656056404) A[2]:(-0.0910026058555) A[3]:(0.590339064598)\n",
      " state (7)  A[0]:(0.454609423876) A[1]:(-0.0841129571199) A[2]:(0.457862526178) A[3]:(0.513085603714)\n",
      " state (8)  A[0]:(0.531399548054) A[1]:(0.0916435942054) A[2]:(0.642046034336) A[3]:(0.39672896266)\n",
      " state (9)  A[0]:(0.576270401478) A[1]:(0.646858274937) A[2]:(0.688614666462) A[3]:(0.309270143509)\n",
      " state (10)  A[0]:(0.596074223518) A[1]:(0.942054927349) A[2]:(0.741144180298) A[3]:(0.313085198402)\n",
      " state (11)  A[0]:(0.598381698132) A[1]:(0.994016587734) A[2]:(0.828033208847) A[3]:(0.418628245592)\n",
      " state (12)  A[0]:(0.590966165066) A[1]:(0.999431371689) A[2]:(0.908022463322) A[3]:(0.573909819126)\n",
      " state (13)  A[0]:(0.582392454147) A[1]:(0.999934077263) A[2]:(0.954398155212) A[3]:(0.709844470024)\n",
      " state (14)  A[0]:(0.579484224319) A[1]:(0.999988853931) A[2]:(0.975578427315) A[3]:(0.798749804497)\n",
      " state (15)  A[0]:(0.584884643555) A[1]:(0.999997138977) A[2]:(0.984781384468) A[3]:(0.849575936794)\n",
      "Episode 50000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained:               8870. Times reached goal: 132.               Steps done: 365266. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.624611441824.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.483420073986) A[1]:(0.543781161308) A[2]:(0.420714706182) A[3]:(0.485018581152)\n",
      " state (1)  A[0]:(0.467258363962) A[1]:(0.0289596077055) A[2]:(0.443883895874) A[3]:(0.465562403202)\n",
      " state (2)  A[0]:(0.515768408775) A[1]:(0.235465660691) A[2]:(0.424360185862) A[3]:(0.444216370583)\n",
      " state (3)  A[0]:(0.548108518124) A[1]:(0.392869710922) A[2]:(0.390745282173) A[3]:(0.424329847097)\n",
      " state (4)  A[0]:(0.494362354279) A[1]:(0.612008213997) A[2]:(0.0285382252187) A[3]:(0.469241708517)\n",
      " state (5)  A[0]:(0.344861596823) A[1]:(0.74611634016) A[2]:(-0.50357735157) A[3]:(0.576656639576)\n",
      " state (6)  A[0]:(0.369987696409) A[1]:(0.387541383505) A[2]:(-0.0894404873252) A[3]:(0.58834862709)\n",
      " state (7)  A[0]:(0.466684162617) A[1]:(-0.0536997914314) A[2]:(0.48630297184) A[3]:(0.521318733692)\n",
      " state (8)  A[0]:(0.539895951748) A[1]:(0.115007132292) A[2]:(0.662278532982) A[3]:(0.410313636065)\n",
      " state (9)  A[0]:(0.581147789955) A[1]:(0.67019867897) A[2]:(0.694814682007) A[3]:(0.319432497025)\n",
      " state (10)  A[0]:(0.599212050438) A[1]:(0.949224412441) A[2]:(0.736839413643) A[3]:(0.316199332476)\n",
      " state (11)  A[0]:(0.60140144825) A[1]:(0.995084881783) A[2]:(0.823155105114) A[3]:(0.418144613504)\n",
      " state (12)  A[0]:(0.59478521347) A[1]:(0.999563932419) A[2]:(0.907415986061) A[3]:(0.575317203999)\n",
      " state (13)  A[0]:(0.587309718132) A[1]:(0.999952852726) A[2]:(0.956100702286) A[3]:(0.714897274971)\n",
      " state (14)  A[0]:(0.58544921875) A[1]:(0.999992489815) A[2]:(0.977605998516) A[3]:(0.806016266346)\n",
      " state (15)  A[0]:(0.591732382774) A[1]:(0.999998152256) A[2]:(0.986579835415) A[3]:(0.857622861862)\n",
      "Episode 51000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained:               8983. Times reached goal: 135.               Steps done: 374249. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.619025683239.\n",
      " state (0)  A[0]:(0.494556099176) A[1]:(0.544844090939) A[2]:(0.432803213596) A[3]:(0.494963556528)\n",
      " state (1)  A[0]:(0.47635948658) A[1]:(0.0298816096038) A[2]:(0.452354341745) A[3]:(0.472664177418)\n",
      " state (2)  A[0]:(0.524153709412) A[1]:(0.23001909256) A[2]:(0.428868323565) A[3]:(0.456638097763)\n",
      " state (3)  A[0]:(0.556016564369) A[1]:(0.386842817068) A[2]:(0.39189568162) A[3]:(0.440519154072)\n",
      " state (4)  A[0]:(0.500272452831) A[1]:(0.616648435593) A[2]:(0.0300676766783) A[3]:(0.47760117054)\n",
      " state (5)  A[0]:(0.334861397743) A[1]:(0.773465394974) A[2]:(-0.532907128334) A[3]:(0.574310898781)\n",
      " state (6)  A[0]:(0.357986241579) A[1]:(0.435882031918) A[2]:(-0.129518374801) A[3]:(0.589381158352)\n",
      " state (7)  A[0]:(0.461369633675) A[1]:(-0.0360717661679) A[2]:(0.474824905396) A[3]:(0.526128053665)\n",
      " state (8)  A[0]:(0.537113070488) A[1]:(0.118171386421) A[2]:(0.654872298241) A[3]:(0.412995457649)\n",
      " state (9)  A[0]:(0.577599167824) A[1]:(0.678670823574) A[2]:(0.679144382477) A[3]:(0.315197855234)\n",
      " state (10)  A[0]:(0.594453692436) A[1]:(0.953001379967) A[2]:(0.715859293938) A[3]:(0.305888622999)\n",
      " state (11)  A[0]:(0.59624171257) A[1]:(0.99570441246) A[2]:(0.807970285416) A[3]:(0.407309472561)\n",
      " state (12)  A[0]:(0.590164065361) A[1]:(0.999640107155) A[2]:(0.901662945747) A[3]:(0.569308340549)\n",
      " state (13)  A[0]:(0.583847522736) A[1]:(0.999962985516) A[2]:(0.95521825552) A[3]:(0.714333593845)\n",
      " state (14)  A[0]:(0.583372592926) A[1]:(0.999994337559) A[2]:(0.978073596954) A[3]:(0.808552265167)\n",
      " state (15)  A[0]:(0.590980291367) A[1]:(0.999998629093) A[2]:(0.987256705761) A[3]:(0.861388087273)\n",
      "Episode 52000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained:               8521. Times reached goal: 125.               Steps done: 382770. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.613773374632.\n",
      " state (0)  A[0]:(0.494629740715) A[1]:(0.54352080822) A[2]:(0.422871172428) A[3]:(0.494625151157)\n",
      " state (1)  A[0]:(0.476746320724) A[1]:(0.0290649905801) A[2]:(0.444587349892) A[3]:(0.467094004154)\n",
      " state (2)  A[0]:(0.524969816208) A[1]:(0.226157486439) A[2]:(0.417111963034) A[3]:(0.455652952194)\n",
      " state (3)  A[0]:(0.558176040649) A[1]:(0.381809175014) A[2]:(0.378926336765) A[3]:(0.442742973566)\n",
      " state (4)  A[0]:(0.506015062332) A[1]:(0.613125443459) A[2]:(0.0282456222922) A[3]:(0.472668588161)\n",
      " state (5)  A[0]:(0.336120098829) A[1]:(0.784975469112) A[2]:(-0.551535725594) A[3]:(0.559098720551)\n",
      " state (6)  A[0]:(0.364293187857) A[1]:(0.445341974497) A[2]:(-0.146786093712) A[3]:(0.577525496483)\n",
      " state (7)  A[0]:(0.473953425884) A[1]:(-0.0568896122277) A[2]:(0.48199108243) A[3]:(0.517803788185)\n",
      " state (8)  A[0]:(0.54707711935) A[1]:(0.106753021479) A[2]:(0.659286379814) A[3]:(0.403125792742)\n",
      " state (9)  A[0]:(0.58080971241) A[1]:(0.693915963173) A[2]:(0.674670517445) A[3]:(0.300775706768)\n",
      " state (10)  A[0]:(0.590556561947) A[1]:(0.959865987301) A[2]:(0.707093477249) A[3]:(0.289212346077)\n",
      " state (11)  A[0]:(0.586223363876) A[1]:(0.99670279026) A[2]:(0.803556203842) A[3]:(0.393614143133)\n",
      " state (12)  A[0]:(0.575469613075) A[1]:(0.999746799469) A[2]:(0.902248561382) A[3]:(0.561567604542)\n",
      " state (13)  A[0]:(0.566161394119) A[1]:(0.999975442886) A[2]:(0.956927061081) A[3]:(0.711007237434)\n",
      " state (14)  A[0]:(0.564334988594) A[1]:(0.999996304512) A[2]:(0.979373931885) A[3]:(0.807032048702)\n",
      " state (15)  A[0]:(0.571848273277) A[1]:(0.99999910593) A[2]:(0.988114655018) A[3]:(0.860355913639)\n",
      "Episode 53000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained:               8747. Times reached goal: 124.               Steps done: 391517. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.608428110518.\n",
      " state (0)  A[0]:(0.509512782097) A[1]:(0.566323935986) A[2]:(0.441349059343) A[3]:(0.50694835186)\n",
      " state (1)  A[0]:(0.491754382849) A[1]:(0.0263138450682) A[2]:(0.460975438356) A[3]:(0.475408136845)\n",
      " state (2)  A[0]:(0.540016174316) A[1]:(0.225124523044) A[2]:(0.429186761379) A[3]:(0.469288229942)\n",
      " state (3)  A[0]:(0.572656393051) A[1]:(0.38478526473) A[2]:(0.386870414019) A[3]:(0.461463481188)\n",
      " state (4)  A[0]:(0.517221748829) A[1]:(0.624444782734) A[2]:(0.0295695271343) A[3]:(0.490125834942)\n",
      " state (5)  A[0]:(0.334350198507) A[1]:(0.800888299942) A[2]:(-0.562790393829) A[3]:(0.570280611515)\n",
      " state (6)  A[0]:(0.365690559149) A[1]:(0.445940881968) A[2]:(-0.123741954565) A[3]:(0.592528939247)\n",
      " state (7)  A[0]:(0.483160465956) A[1]:(-0.0886242315173) A[2]:(0.524586260319) A[3]:(0.534236073494)\n",
      " state (8)  A[0]:(0.555042743683) A[1]:(0.100013881922) A[2]:(0.687420070171) A[3]:(0.41448777914)\n",
      " state (9)  A[0]:(0.580884039402) A[1]:(0.721867918968) A[2]:(0.691276907921) A[3]:(0.305622965097)\n",
      " state (10)  A[0]:(0.580264925957) A[1]:(0.96901845932) A[2]:(0.717191576958) A[3]:(0.29293397069)\n",
      " state (11)  A[0]:(0.565313398838) A[1]:(0.997805893421) A[2]:(0.812528550625) A[3]:(0.402300029993)\n",
      " state (12)  A[0]:(0.54519701004) A[1]:(0.999849200249) A[2]:(0.909220457077) A[3]:(0.574612140656)\n",
      " state (13)  A[0]:(0.52909052372) A[1]:(0.999986231327) A[2]:(0.960756063461) A[3]:(0.723243057728)\n",
      " state (14)  A[0]:(0.523589372635) A[1]:(0.999997973442) A[2]:(0.981206834316) A[3]:(0.816002190113)\n",
      " state (15)  A[0]:(0.530108809471) A[1]:(0.999999523163) A[2]:(0.9890152812) A[3]:(0.866529345512)\n",
      "Episode 54000 finished after 0 timesteps with r=0.0. Running score: 0.07. Times trained:               8801. Times reached goal: 83.               Steps done: 400318. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.603096829432.\n",
      "q_values \n",
      "tensor([[ 0.5309,  0.5836,  0.4575,  0.5304]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5310,  0.5838,  0.4575,  0.5305]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5841,  0.4575,  0.5306]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5847,  0.4576,  0.5307]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5430,  0.6554,  0.0209,  0.5168]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5890,  0.0993,  0.7024,  0.4322]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6073,  0.7485,  0.6864,  0.3123]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5211,  1.0000,  0.9576,  0.7337]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5123,  1.0000,  0.9801,  0.8271]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5995,  0.9763,  0.6987,  0.2951]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5144,  1.0000,  0.9803,  0.8279]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5154,  1.0000,  0.9803,  0.8281]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5157,  1.0000,  0.9803,  0.8283]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6011,  0.9764,  0.7028,  0.2987]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5150,  1.0000,  0.9805,  0.8284]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? True\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531959295273) A[1]:(0.586192429066) A[2]:(0.45882076025) A[3]:(0.531681418419)\n",
      " state (1)  A[0]:(0.514440178871) A[1]:(0.0364816337824) A[2]:(0.48104351759) A[3]:(0.492829144001)\n",
      " state (2)  A[0]:(0.565161705017) A[1]:(0.246196821332) A[2]:(0.444343954325) A[3]:(0.49329200387)\n",
      " state (3)  A[0]:(0.599667072296) A[1]:(0.416059434414) A[2]:(0.395909160376) A[3]:(0.491477817297)\n",
      " state (4)  A[0]:(0.545183777809) A[1]:(0.659565329552) A[2]:(0.0256931260228) A[3]:(0.519327521324)\n",
      " state (5)  A[0]:(0.356698095798) A[1]:(0.828609645367) A[2]:(-0.581016778946) A[3]:(0.592420935631)\n",
      " state (6)  A[0]:(0.397110849619) A[1]:(0.470622748137) A[2]:(-0.1063028723) A[3]:(0.618132829666)\n",
      " state (7)  A[0]:(0.523021101952) A[1]:(-0.101431824267) A[2]:(0.563802480698) A[3]:(0.562719643116)\n",
      " state (8)  A[0]:(0.592484295368) A[1]:(0.105394534767) A[2]:(0.709758520126) A[3]:(0.438559234142)\n",
      " state (9)  A[0]:(0.61044895649) A[1]:(0.750861644745) A[2]:(0.694759190083) A[3]:(0.319744765759)\n",
      " state (10)  A[0]:(0.60043823719) A[1]:(0.976133942604) A[2]:(0.704943656921) A[3]:(0.299779623747)\n",
      " state (11)  A[0]:(0.576017618179) A[1]:(0.998524367809) A[2]:(0.799238264561) A[3]:(0.40953463316)\n",
      " state (12)  A[0]:(0.547220468521) A[1]:(0.999909460545) A[2]:(0.902951002121) A[3]:(0.586109876633)\n",
      " state (13)  A[0]:(0.524245262146) A[1]:(0.999992370605) A[2]:(0.958723723888) A[3]:(0.736595749855)\n",
      " state (14)  A[0]:(0.514422655106) A[1]:(0.999998927116) A[2]:(0.980517327785) A[3]:(0.82843542099)\n",
      " state (15)  A[0]:(0.519077062607) A[1]:(0.999999761581) A[2]:(0.988681316376) A[3]:(0.877426862717)\n",
      "Episode 55000 finished after 0 timesteps with r=1.0. Running score: 0.08. Times trained:               8601. Times reached goal: 98.               Steps done: 408919. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.59793183749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.537101387978) A[1]:(0.606597542763) A[2]:(0.463999480009) A[3]:(0.536319971085)\n",
      " state (1)  A[0]:(0.521219730377) A[1]:(0.0387692339718) A[2]:(0.491538792849) A[3]:(0.494269311428)\n",
      " state (2)  A[0]:(0.574684143066) A[1]:(0.255062580109) A[2]:(0.450212210417) A[3]:(0.499986112118)\n",
      " state (3)  A[0]:(0.612234890461) A[1]:(0.432106018066) A[2]:(0.39696097374) A[3]:(0.502416789532)\n",
      " state (4)  A[0]:(0.563303947449) A[1]:(0.67722928524) A[2]:(0.0264040827751) A[3]:(0.528536677361)\n",
      " state (5)  A[0]:(0.376052796841) A[1]:(0.846661388874) A[2]:(-0.595972895622) A[3]:(0.596708059311)\n",
      " state (6)  A[0]:(0.426979750395) A[1]:(0.483838230371) A[2]:(-0.0923343300819) A[3]:(0.628352403641)\n",
      " state (7)  A[0]:(0.557501554489) A[1]:(-0.13060733676) A[2]:(0.601316034794) A[3]:(0.579505205154)\n",
      " state (8)  A[0]:(0.620417416096) A[1]:(0.0978950858116) A[2]:(0.731149196625) A[3]:(0.454329907894)\n",
      " state (9)  A[0]:(0.628716111183) A[1]:(0.774380207062) A[2]:(0.697917699814) A[3]:(0.327393978834)\n",
      " state (10)  A[0]:(0.609123766422) A[1]:(0.981513261795) A[2]:(0.692974925041) A[3]:(0.301809966564)\n",
      " state (11)  A[0]:(0.576191544533) A[1]:(0.999002993107) A[2]:(0.787668585777) A[3]:(0.414349377155)\n",
      " state (12)  A[0]:(0.540616989136) A[1]:(0.999945163727) A[2]:(0.898756146431) A[3]:(0.597069859505)\n",
      " state (13)  A[0]:(0.513228237629) A[1]:(0.999995708466) A[2]:(0.958025455475) A[3]:(0.74981200695)\n",
      " state (14)  A[0]:(0.501512229443) A[1]:(0.999999403954) A[2]:(0.980562329292) A[3]:(0.8404687047)\n",
      " state (15)  A[0]:(0.506267786026) A[1]:(0.999999880791) A[2]:(0.98879212141) A[3]:(0.887675583363)\n",
      "Episode 56000 finished after 0 timesteps with r=0.0. Running score: 0.11. Times trained:               8679. Times reached goal: 108.               Steps done: 417598. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.592764841685.\n",
      " state (0)  A[0]:(0.549675047398) A[1]:(0.610745787621) A[2]:(0.476934075356) A[3]:(0.548328518867)\n",
      " state (1)  A[0]:(0.529797554016) A[1]:(0.0249298997223) A[2]:(0.504498124123) A[3]:(0.497286766768)\n",
      " state (2)  A[0]:(0.584322094917) A[1]:(0.241311877966) A[2]:(0.457182049751) A[3]:(0.506764054298)\n",
      " state (3)  A[0]:(0.622273921967) A[1]:(0.423117846251) A[2]:(0.396201103926) A[3]:(0.512594282627)\n",
      " state (4)  A[0]:(0.57218682766) A[1]:(0.67554795742) A[2]:(0.0175800099969) A[3]:(0.537891507149)\n",
      " state (5)  A[0]:(0.367885708809) A[1]:(0.855044126511) A[2]:(-0.627376079559) A[3]:(0.602884292603)\n",
      " state (6)  A[0]:(0.41886395216) A[1]:(0.486981898546) A[2]:(-0.126050159335) A[3]:(0.63948738575)\n",
      " state (7)  A[0]:(0.562282502651) A[1]:(-0.171009659767) A[2]:(0.603230535984) A[3]:(0.588335692883)\n",
      " state (8)  A[0]:(0.627567768097) A[1]:(0.0685221329331) A[2]:(0.728039860725) A[3]:(0.448625326157)\n",
      " state (9)  A[0]:(0.629772901535) A[1]:(0.784327924252) A[2]:(0.675409793854) A[3]:(0.303378492594)\n",
      " state (10)  A[0]:(0.601049780846) A[1]:(0.984545528889) A[2]:(0.655771017075) A[3]:(0.270281344652)\n",
      " state (11)  A[0]:(0.559053599834) A[1]:(0.999259233475) A[2]:(0.759479165077) A[3]:(0.390911996365)\n",
      " state (12)  A[0]:(0.51633143425) A[1]:(0.999962627888) A[2]:(0.88744866848) A[3]:(0.588539719582)\n",
      " state (13)  A[0]:(0.484801709652) A[1]:(0.999997198582) A[2]:(0.954642772675) A[3]:(0.750725328922)\n",
      " state (14)  A[0]:(0.472036838531) A[1]:(0.999999642372) A[2]:(0.979367613792) A[3]:(0.844281852245)\n",
      " state (15)  A[0]:(0.478024691343) A[1]:(0.999999940395) A[2]:(0.988144338131) A[3]:(0.891778349876)\n",
      "Episode 57000 finished after 0 timesteps with r=1.0. Running score: 0.16. Times trained:               8581. Times reached goal: 106.               Steps done: 426179. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.587700087982.\n",
      " state (0)  A[0]:(0.564181447029) A[1]:(0.624209880829) A[2]:(0.492311179638) A[3]:(0.565757572651)\n",
      " state (1)  A[0]:(0.542120754719) A[1]:(0.0300127696246) A[2]:(0.522975444794) A[3]:(0.510234713554)\n",
      " state (2)  A[0]:(0.597214579582) A[1]:(0.248027771711) A[2]:(0.473891794682) A[3]:(0.523805618286)\n",
      " state (3)  A[0]:(0.635673642159) A[1]:(0.434685140848) A[2]:(0.409615248442) A[3]:(0.53290605545)\n",
      " state (4)  A[0]:(0.586222529411) A[1]:(0.691758394241) A[2]:(0.0274910181761) A[3]:(0.554889798164)\n",
      " state (5)  A[0]:(0.372669696808) A[1]:(0.872275292873) A[2]:(-0.638198494911) A[3]:(0.610357105732)\n",
      " state (6)  A[0]:(0.429592430592) A[1]:(0.508006811142) A[2]:(-0.102069661021) A[3]:(0.651162385941)\n",
      " state (7)  A[0]:(0.579171538353) A[1]:(-0.198071032763) A[2]:(0.650843143463) A[3]:(0.605095028877)\n",
      " state (8)  A[0]:(0.638550043106) A[1]:(0.0548741966486) A[2]:(0.758307218552) A[3]:(0.46061205864)\n",
      " state (9)  A[0]:(0.629448652267) A[1]:(0.801445960999) A[2]:(0.686671495438) A[3]:(0.300826221704)\n",
      " state (10)  A[0]:(0.588283479214) A[1]:(0.987661540508) A[2]:(0.645791769028) A[3]:(0.25692898035)\n",
      " state (11)  A[0]:(0.53524017334) A[1]:(0.999476373196) A[2]:(0.747119903564) A[3]:(0.380075454712)\n",
      " state (12)  A[0]:(0.484405338764) A[1]:(0.999976098537) A[2]:(0.883939921856) A[3]:(0.588554680347)\n",
      " state (13)  A[0]:(0.448346823454) A[1]:(0.99999833107) A[2]:(0.955126047134) A[3]:(0.758004128933)\n",
      " state (14)  A[0]:(0.434412032366) A[1]:(0.999999761581) A[2]:(0.98034787178) A[3]:(0.853199005127)\n",
      " state (15)  A[0]:(0.441700994968) A[1]:(0.999999940395) A[2]:(0.988974094391) A[3]:(0.90022790432)\n",
      "Episode 58000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               9051. Times reached goal: 129.               Steps done: 435230. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.582404814396.\n",
      " state (0)  A[0]:(0.57257604599) A[1]:(0.634079217911) A[2]:(0.498950600624) A[3]:(0.573622465134)\n",
      " state (1)  A[0]:(0.55155813694) A[1]:(0.0277684442699) A[2]:(0.532453894615) A[3]:(0.512772202492)\n",
      " state (2)  A[0]:(0.610126793385) A[1]:(0.252245157957) A[2]:(0.480201542377) A[3]:(0.531617999077)\n",
      " state (3)  A[0]:(0.651100039482) A[1]:(0.446305245161) A[2]:(0.411762088537) A[3]:(0.546157360077)\n",
      " state (4)  A[0]:(0.603769421577) A[1]:(0.706201434135) A[2]:(0.0278446804732) A[3]:(0.570420145988)\n",
      " state (5)  A[0]:(0.381187617779) A[1]:(0.885677456856) A[2]:(-0.649643540382) A[3]:(0.625357151031)\n",
      " state (6)  A[0]:(0.444660902023) A[1]:(0.526404380798) A[2]:(-0.0827481299639) A[3]:(0.676033854485)\n",
      " state (7)  A[0]:(0.607418298721) A[1]:(-0.21838709712) A[2]:(0.686984181404) A[3]:(0.635575115681)\n",
      " state (8)  A[0]:(0.667596638203) A[1]:(0.0510112941265) A[2]:(0.777512013912) A[3]:(0.486235737801)\n",
      " state (9)  A[0]:(0.653257727623) A[1]:(0.819827318192) A[2]:(0.682221710682) A[3]:(0.310879200697)\n",
      " state (10)  A[0]:(0.605997443199) A[1]:(0.990184664726) A[2]:(0.612422168255) A[3]:(0.253818422556)\n",
      " state (11)  A[0]:(0.548917174339) A[1]:(0.999626278877) A[2]:(0.713414609432) A[3]:(0.377143353224)\n",
      " state (12)  A[0]:(0.497096061707) A[1]:(0.999984502792) A[2]:(0.870007872581) A[3]:(0.594912111759)\n",
      " state (13)  A[0]:(0.462522953749) A[1]:(0.999998986721) A[2]:(0.952062487602) A[3]:(0.770260035992)\n",
      " state (14)  A[0]:(0.451477080584) A[1]:(0.999999880791) A[2]:(0.980048537254) A[3]:(0.865824460983)\n",
      " state (15)  A[0]:(0.462120562792) A[1]:(1.0) A[2]:(0.989202976227) A[3]:(0.911492466927)\n",
      "Episode 59000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained:               9123. Times reached goal: 136.               Steps done: 444353. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.577115698261.\n",
      "q_values \n",
      "tensor([[ 0.5833,  0.6457,  0.5115,  0.5817]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5833,  0.6460,  0.5115,  0.5817]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5833,  0.6463,  0.5115,  0.5816]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5634,  0.0400,  0.5494,  0.5202]], device='cuda:0')\n",
      "On state=1, selected action=0 , Random? False\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5833,  0.6469,  0.5113,  0.5816]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5833,  0.6471,  0.5113,  0.5815]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6196,  0.7263,  0.0229,  0.5807]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6875,  0.0538,  0.7952,  0.4973]], device='cuda:0')\n",
      "On state=8, selected action=3 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6194,  0.7254,  0.0225,  0.5803]], device='cuda:0')\n",
      "On state=4, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5831,  0.6462,  0.5108,  0.5812]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5830,  0.6458,  0.5106,  0.5811]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5830,  0.6455,  0.5104,  0.5810]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6193,  0.7240,  0.0192,  0.5790]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.583025932312) A[1]:(0.645161509514) A[2]:(0.509993553162) A[3]:(0.580866515636)\n",
      " state (1)  A[0]:(0.563355505466) A[1]:(0.0364091917872) A[2]:(0.547920942307) A[3]:(0.518651485443)\n",
      " state (2)  A[0]:(0.623555064201) A[1]:(0.260770916939) A[2]:(0.493484944105) A[3]:(0.540183961391)\n",
      " state (3)  A[0]:(0.665572822094) A[1]:(0.458310842514) A[2]:(0.419554233551) A[3]:(0.556925415993)\n",
      " state (4)  A[0]:(0.619294047356) A[1]:(0.723614931107) A[2]:(0.0183916203678) A[3]:(0.578666210175)\n",
      " state (5)  A[0]:(0.391196966171) A[1]:(0.901177465916) A[2]:(-0.677926361561) A[3]:(0.626390218735)\n",
      " state (6)  A[0]:(0.460949242115) A[1]:(0.559478640556) A[2]:(-0.102103866637) A[3]:(0.683155417442)\n",
      " state (7)  A[0]:(0.631045758724) A[1]:(-0.228728085756) A[2]:(0.706049799919) A[3]:(0.647231698036)\n",
      " state (8)  A[0]:(0.687359988689) A[1]:(0.0476188845932) A[2]:(0.792098224163) A[3]:(0.492272734642)\n",
      " state (9)  A[0]:(0.663067817688) A[1]:(0.837951302528) A[2]:(0.684037208557) A[3]:(0.303314626217)\n",
      " state (10)  A[0]:(0.603741943836) A[1]:(0.992452085018) A[2]:(0.599785447121) A[3]:(0.239695549011)\n",
      " state (11)  A[0]:(0.536447882652) A[1]:(0.999747872353) A[2]:(0.706150770187) A[3]:(0.37067347765)\n",
      " state (12)  A[0]:(0.479028552771) A[1]:(0.999990463257) A[2]:(0.872171580791) A[3]:(0.600134372711)\n",
      " state (13)  A[0]:(0.443899869919) A[1]:(0.999999403954) A[2]:(0.954938173294) A[3]:(0.779270529747)\n",
      " state (14)  A[0]:(0.435958564281) A[1]:(0.999999940395) A[2]:(0.981683790684) A[3]:(0.873417198658)\n",
      " state (15)  A[0]:(0.451457679272) A[1]:(1.0) A[2]:(0.990112125874) A[3]:(0.917187988758)\n",
      "Episode 60000 finished after 0 timesteps with r=0.0. Running score: 0.15. Times trained:               9147. Times reached goal: 145.               Steps done: 453500. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.571860890471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.586986064911) A[1]:(0.64977812767) A[2]:(0.507264614105) A[3]:(0.585270762444)\n",
      " state (1)  A[0]:(0.561586260796) A[1]:(0.0405246280134) A[2]:(0.551140069962) A[3]:(0.51654779911)\n",
      " state (2)  A[0]:(0.624399185181) A[1]:(0.263654053211) A[2]:(0.494914293289) A[3]:(0.540843069553)\n",
      " state (3)  A[0]:(0.668065965176) A[1]:(0.462550580502) A[2]:(0.417491614819) A[3]:(0.560374438763)\n",
      " state (4)  A[0]:(0.621192455292) A[1]:(0.72964400053) A[2]:(0.0119231650606) A[3]:(0.581732213497)\n",
      " state (5)  A[0]:(0.376270592213) A[1]:(0.90935754776) A[2]:(-0.697003185749) A[3]:(0.625988960266)\n",
      " state (6)  A[0]:(0.446403175592) A[1]:(0.581042528152) A[2]:(-0.120062626898) A[3]:(0.692354738712)\n",
      " state (7)  A[0]:(0.630850553513) A[1]:(-0.239768385887) A[2]:(0.719626426697) A[3]:(0.661326766014)\n",
      " state (8)  A[0]:(0.687367916107) A[1]:(0.0442793928087) A[2]:(0.800271689892) A[3]:(0.500302314758)\n",
      " state (9)  A[0]:(0.652099907398) A[1]:(0.852287232876) A[2]:(0.674373865128) A[3]:(0.296860098839)\n",
      " state (10)  A[0]:(0.577415943146) A[1]:(0.99395686388) A[2]:(0.569223403931) A[3]:(0.225798755884)\n",
      " state (11)  A[0]:(0.497165203094) A[1]:(0.999818086624) A[2]:(0.683480620384) A[3]:(0.364306420088)\n",
      " state (12)  A[0]:(0.433180212975) A[1]:(0.999993622303) A[2]:(0.867516458035) A[3]:(0.606211185455)\n",
      " state (13)  A[0]:(0.39797475934) A[1]:(0.999999642372) A[2]:(0.955661416054) A[3]:(0.789598703384)\n",
      " state (14)  A[0]:(0.394440531731) A[1]:(0.999999940395) A[2]:(0.982602477074) A[3]:(0.882373332977)\n",
      " state (15)  A[0]:(0.416589915752) A[1]:(1.0) A[2]:(0.990730702877) A[3]:(0.92418050766)\n",
      "Episode 61000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               9331. Times reached goal: 122.               Steps done: 462831. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.566549674516.\n",
      " state (0)  A[0]:(0.595625758171) A[1]:(0.662897467613) A[2]:(0.515528321266) A[3]:(0.594661474228)\n",
      " state (1)  A[0]:(0.570309042931) A[1]:(0.0538117326796) A[2]:(0.562765002251) A[3]:(0.524201512337)\n",
      " state (2)  A[0]:(0.63584369421) A[1]:(0.276768803596) A[2]:(0.506842255592) A[3]:(0.550274252892)\n",
      " state (3)  A[0]:(0.680820286274) A[1]:(0.475000083447) A[2]:(0.428397774696) A[3]:(0.572079479694)\n",
      " state (4)  A[0]:(0.631769835949) A[1]:(0.739393234253) A[2]:(0.0140140419826) A[3]:(0.593913912773)\n",
      " state (5)  A[0]:(0.369552016258) A[1]:(0.913332402706) A[2]:(-0.705180287361) A[3]:(0.637518763542)\n",
      " state (6)  A[0]:(0.439100116491) A[1]:(0.577130079269) A[2]:(-0.105398602784) A[3]:(0.712232887745)\n",
      " state (7)  A[0]:(0.638506889343) A[1]:(-0.267946362495) A[2]:(0.740626037121) A[3]:(0.679840624332)\n",
      " state (8)  A[0]:(0.697622776031) A[1]:(0.0313858985901) A[2]:(0.812706053257) A[3]:(0.505622029305)\n",
      " state (9)  A[0]:(0.653732776642) A[1]:(0.863406300545) A[2]:(0.674335360527) A[3]:(0.281361609697)\n",
      " state (10)  A[0]:(0.564970612526) A[1]:(0.995096445084) A[2]:(0.554252207279) A[3]:(0.200019642711)\n",
      " state (11)  A[0]:(0.472358644009) A[1]:(0.999867141247) A[2]:(0.676170408726) A[3]:(0.34621399641)\n",
      " state (12)  A[0]:(0.402631789446) A[1]:(0.999995648861) A[2]:(0.870627403259) A[3]:(0.602687358856)\n",
      " state (13)  A[0]:(0.368712604046) A[1]:(0.999999761581) A[2]:(0.958710134029) A[3]:(0.792655706406)\n",
      " state (14)  A[0]:(0.371312439442) A[1]:(1.0) A[2]:(0.984156012535) A[3]:(0.885806441307)\n",
      " state (15)  A[0]:(0.40186971426) A[1]:(1.0) A[2]:(0.991550147533) A[3]:(0.926776885986)\n",
      "Episode 62000 finished after 0 timesteps with r=1.0. Running score: 0.13. Times trained:               9001. Times reached goal: 142.               Steps done: 471832. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.561473042553.\n",
      " state (0)  A[0]:(0.590945601463) A[1]:(0.657528281212) A[2]:(0.511986196041) A[3]:(0.592295646667)\n",
      " state (1)  A[0]:(0.562731862068) A[1]:(0.0548986233771) A[2]:(0.571068286896) A[3]:(0.519077599049)\n",
      " state (2)  A[0]:(0.636328816414) A[1]:(0.277096092701) A[2]:(0.514048218727) A[3]:(0.54715013504)\n",
      " state (3)  A[0]:(0.687381982803) A[1]:(0.474674373865) A[2]:(0.433521836996) A[3]:(0.571382761002)\n",
      " state (4)  A[0]:(0.6437266469) A[1]:(0.737632513046) A[2]:(0.0203861948103) A[3]:(0.594546973705)\n",
      " state (5)  A[0]:(0.375576972961) A[1]:(0.915481925011) A[2]:(-0.715120315552) A[3]:(0.638884544373)\n",
      " state (6)  A[0]:(0.443230479956) A[1]:(0.589450001717) A[2]:(-0.13180655241) A[3]:(0.724686145782)\n",
      " state (7)  A[0]:(0.655439138412) A[1]:(-0.277172148228) A[2]:(0.742746472359) A[3]:(0.697228550911)\n",
      " state (8)  A[0]:(0.717739403248) A[1]:(0.0183542016894) A[2]:(0.817527890205) A[3]:(0.519425392151)\n",
      " state (9)  A[0]:(0.668154835701) A[1]:(0.87166595459) A[2]:(0.670534312725) A[3]:(0.283855170012)\n",
      " state (10)  A[0]:(0.568265914917) A[1]:(0.995980143547) A[2]:(0.540760278702) A[3]:(0.198471993208)\n",
      " state (11)  A[0]:(0.466052502394) A[1]:(0.999903082848) A[2]:(0.673257708549) A[3]:(0.353697359562)\n",
      " state (12)  A[0]:(0.39432105422) A[1]:(0.999997019768) A[2]:(0.875079154968) A[3]:(0.617361426353)\n",
      " state (13)  A[0]:(0.36673283577) A[1]:(0.999999821186) A[2]:(0.961110830307) A[3]:(0.804179012775)\n",
      " state (14)  A[0]:(0.381040751934) A[1]:(1.0) A[2]:(0.984914243221) A[3]:(0.892406642437)\n",
      " state (15)  A[0]:(0.425000786781) A[1]:(1.0) A[2]:(0.991690695286) A[3]:(0.93042755127)\n",
      "Episode 63000 finished after 0 timesteps with r=1.0. Running score: 0.16. Times trained:               9344. Times reached goal: 141.               Steps done: 481176. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.556251073477.\n",
      " state (0)  A[0]:(0.597301781178) A[1]:(0.668412327766) A[2]:(0.520838677883) A[3]:(0.599393904209)\n",
      " state (1)  A[0]:(0.555024027824) A[1]:(0.0787807926536) A[2]:(0.582359850407) A[3]:(0.524674773216)\n",
      " state (2)  A[0]:(0.637015461922) A[1]:(0.292234063148) A[2]:(0.528168559074) A[3]:(0.552318811417)\n",
      " state (3)  A[0]:(0.693464517593) A[1]:(0.482883632183) A[2]:(0.448011636734) A[3]:(0.57652336359)\n",
      " state (4)  A[0]:(0.646341443062) A[1]:(0.748078465462) A[2]:(0.0135139944032) A[3]:(0.596140503883)\n",
      " state (5)  A[0]:(0.352607011795) A[1]:(0.922175467014) A[2]:(-0.732213020325) A[3]:(0.634978294373)\n",
      " state (6)  A[0]:(0.413783282042) A[1]:(0.61890900135) A[2]:(-0.163753420115) A[3]:(0.73115503788)\n",
      " state (7)  A[0]:(0.644274353981) A[1]:(-0.248665630817) A[2]:(0.735263943672) A[3]:(0.704270362854)\n",
      " state (8)  A[0]:(0.716121137142) A[1]:(0.0266664568335) A[2]:(0.818853259087) A[3]:(0.512919783592)\n",
      " state (9)  A[0]:(0.660019040108) A[1]:(0.879278004169) A[2]:(0.668707132339) A[3]:(0.256297916174)\n",
      " state (10)  A[0]:(0.544495940208) A[1]:(0.996588289738) A[2]:(0.537740826607) A[3]:(0.164791271091)\n",
      " state (11)  A[0]:(0.428921073675) A[1]:(0.999924719334) A[2]:(0.684044957161) A[3]:(0.33329603076)\n",
      " state (12)  A[0]:(0.354530274868) A[1]:(0.999997735023) A[2]:(0.886682331562) A[3]:(0.613129258156)\n",
      " state (13)  A[0]:(0.335486352444) A[1]:(0.999999880791) A[2]:(0.966009914875) A[3]:(0.804854214191)\n",
      " state (14)  A[0]:(0.365483283997) A[1]:(1.0) A[2]:(0.986780941486) A[3]:(0.892660081387)\n",
      " state (15)  A[0]:(0.427445650101) A[1]:(1.0) A[2]:(0.992544829845) A[3]:(0.929862439632)\n",
      "Episode 64000 finished after 0 timesteps with r=1.0. Running score: 0.1. Times trained:               9582. Times reached goal: 129.               Steps done: 490758. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.550946530343.\n",
      "q_values \n",
      "tensor([[ 0.6038,  0.6759,  0.5252,  0.6052]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6536,  0.7523,  0.0184,  0.6010]], device='cuda:0')\n",
      "On state=4, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6036,  0.6759,  0.5254,  0.6051]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5464,  0.0956,  0.5822,  0.5322]], device='cuda:0')\n",
      "On state=1, selected action=3 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5463,  0.0952,  0.5822,  0.5320]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6384,  0.3026,  0.5307,  0.5601]], device='cuda:0')\n",
      "On state=2, selected action=0 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5461,  0.0944,  0.5821,  0.5318]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6384,  0.3028,  0.5305,  0.5601]], device='cuda:0')\n",
      "On state=2, selected action=2 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7019,  0.4874,  0.4532,  0.5855]], device='cuda:0')\n",
      "On state=3, selected action=3 , Random? True\n",
      "new state=3, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7021,  0.4879,  0.4529,  0.5856]], device='cuda:0')\n",
      "On state=3, selected action=1 , Random? True\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.603338360786) A[1]:(0.675930023193) A[2]:(0.52521365881) A[3]:(0.60490000248)\n",
      " state (1)  A[0]:(0.546447396278) A[1]:(0.0958791673183) A[2]:(0.581440448761) A[3]:(0.53195142746)\n",
      " state (2)  A[0]:(0.638833522797) A[1]:(0.303985983133) A[2]:(0.529677093029) A[3]:(0.560191512108)\n",
      " state (3)  A[0]:(0.702239394188) A[1]:(0.488231807947) A[2]:(0.452499866486) A[3]:(0.585627913475)\n",
      " state (4)  A[0]:(0.653997898102) A[1]:(0.751459240913) A[2]:(0.0165257304907) A[3]:(0.600498497486)\n",
      " state (5)  A[0]:(0.335117965937) A[1]:(0.923828125) A[2]:(-0.72890162468) A[3]:(0.626990914345)\n",
      " state (6)  A[0]:(0.392381936312) A[1]:(0.628759980202) A[2]:(-0.148718208075) A[3]:(0.734361588955)\n",
      " state (7)  A[0]:(0.642957210541) A[1]:(-0.235842645168) A[2]:(0.743031620979) A[3]:(0.712306380272)\n",
      " state (8)  A[0]:(0.726037323475) A[1]:(0.0165528431535) A[2]:(0.825548350811) A[3]:(0.518459916115)\n",
      " state (9)  A[0]:(0.669160366058) A[1]:(0.878755271435) A[2]:(0.662917733192) A[3]:(0.249756023288)\n",
      " state (10)  A[0]:(0.545550704002) A[1]:(0.996783554554) A[2]:(0.501854538918) A[3]:(0.1465908885)\n",
      " state (11)  A[0]:(0.423160612583) A[1]:(0.999933719635) A[2]:(0.648319005966) A[3]:(0.315714895725)\n",
      " state (12)  A[0]:(0.35053011775) A[1]:(0.999998152256) A[2]:(0.87452507019) A[3]:(0.606177449226)\n",
      " state (13)  A[0]:(0.341858506203) A[1]:(0.999999880791) A[2]:(0.963549375534) A[3]:(0.804719865322)\n",
      " state (14)  A[0]:(0.387013345957) A[1]:(1.0) A[2]:(0.98620557785) A[3]:(0.894075095654)\n",
      " state (15)  A[0]:(0.464453965425) A[1]:(1.0) A[2]:(0.992326140404) A[3]:(0.931261837482)\n",
      "Episode 65000 finished after 0 timesteps with r=0.0. Running score: 0.08. Times trained:               9437. Times reached goal: 132.               Steps done: 500195. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.54577170376.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.60350304842) A[1]:(0.676231026649) A[2]:(0.524530470371) A[3]:(0.609343886375)\n",
      " state (1)  A[0]:(0.531834483147) A[1]:(0.101094797254) A[2]:(0.584252953529) A[3]:(0.535545349121)\n",
      " state (2)  A[0]:(0.634863495827) A[1]:(0.309965491295) A[2]:(0.536930561066) A[3]:(0.565248250961)\n",
      " state (3)  A[0]:(0.705082774162) A[1]:(0.493155360222) A[2]:(0.464508712292) A[3]:(0.592526733875)\n",
      " state (4)  A[0]:(0.652045607567) A[1]:(0.757440805435) A[2]:(0.016821347177) A[3]:(0.600261569023)\n",
      " state (5)  A[0]:(0.303025960922) A[1]:(0.924710035324) A[2]:(-0.732474803925) A[3]:(0.609389901161)\n",
      " state (6)  A[0]:(0.360585451126) A[1]:(0.637840390205) A[2]:(-0.15568472445) A[3]:(0.730438709259)\n",
      " state (7)  A[0]:(0.637555599213) A[1]:(-0.210503295064) A[2]:(0.737293899059) A[3]:(0.714463531971)\n",
      " state (8)  A[0]:(0.737625837326) A[1]:(0.0166236944497) A[2]:(0.829389214516) A[3]:(0.521251022816)\n",
      " state (9)  A[0]:(0.683654785156) A[1]:(0.881496846676) A[2]:(0.667726755142) A[3]:(0.249467596412)\n",
      " state (10)  A[0]:(0.553567826748) A[1]:(0.997154474258) A[2]:(0.496637910604) A[3]:(0.14472438395)\n",
      " state (11)  A[0]:(0.423381477594) A[1]:(0.999947428703) A[2]:(0.645537078381) A[3]:(0.319116860628)\n",
      " state (12)  A[0]:(0.350880086422) A[1]:(0.999998629093) A[2]:(0.877453744411) A[3]:(0.614396095276)\n",
      " state (13)  A[0]:(0.35131701827) A[1]:(0.999999940395) A[2]:(0.965506374836) A[3]:(0.811322033405)\n",
      " state (14)  A[0]:(0.41022208333) A[1]:(1.0) A[2]:(0.98711681366) A[3]:(0.89796423912)\n",
      " state (15)  A[0]:(0.500873088837) A[1]:(1.0) A[2]:(0.99283516407) A[3]:(0.933558344841)\n",
      "Episode 66000 finished after 0 timesteps with r=0.0. Running score: 0.11. Times trained:               9401. Times reached goal: 127.               Steps done: 509596. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.540664945899.\n",
      " state (0)  A[0]:(0.608641028404) A[1]:(0.679957151413) A[2]:(0.515175223351) A[3]:(0.613512158394)\n",
      " state (1)  A[0]:(0.513300538063) A[1]:(0.104874573648) A[2]:(0.573331475258) A[3]:(0.536458313465)\n",
      " state (2)  A[0]:(0.625527024269) A[1]:(0.315094947815) A[2]:(0.529228329659) A[3]:(0.568706572056)\n",
      " state (3)  A[0]:(0.701961994171) A[1]:(0.497023195028) A[2]:(0.46179574728) A[3]:(0.59860086441)\n",
      " state (4)  A[0]:(0.645919919014) A[1]:(0.755772233009) A[2]:(0.0215237867087) A[3]:(0.598773777485)\n",
      " state (5)  A[0]:(0.257934838533) A[1]:(0.923672676086) A[2]:(-0.736061573029) A[3]:(0.581618905067)\n",
      " state (6)  A[0]:(0.302133589983) A[1]:(0.653229951859) A[2]:(-0.186260461807) A[3]:(0.717310070992)\n",
      " state (7)  A[0]:(0.608811378479) A[1]:(-0.178645730019) A[2]:(0.722593903542) A[3]:(0.713232398033)\n",
      " state (8)  A[0]:(0.72973805666) A[1]:(0.0102226156741) A[2]:(0.827881991863) A[3]:(0.526109337807)\n",
      " state (9)  A[0]:(0.673536300659) A[1]:(0.879271090031) A[2]:(0.656284809113) A[3]:(0.25159496069)\n",
      " state (10)  A[0]:(0.526714086533) A[1]:(0.997281432152) A[2]:(0.451222509146) A[3]:(0.137207701802)\n",
      " state (11)  A[0]:(0.379695653915) A[1]:(0.999953866005) A[2]:(0.598930835724) A[3]:(0.308448404074)\n",
      " state (12)  A[0]:(0.302480250597) A[1]:(0.999998867512) A[2]:(0.862343609333) A[3]:(0.610556602478)\n",
      " state (13)  A[0]:(0.310119181871) A[1]:(0.999999940395) A[2]:(0.963192880154) A[3]:(0.812881708145)\n",
      " state (14)  A[0]:(0.383192867041) A[1]:(1.0) A[2]:(0.986952066422) A[3]:(0.90072208643)\n",
      " state (15)  A[0]:(0.48982834816) A[1]:(1.0) A[2]:(0.992996513844) A[3]:(0.936232626438)\n",
      "Episode 67000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               10079. Times reached goal: 119.               Steps done: 519675. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.535242953938.\n",
      " state (0)  A[0]:(0.607354640961) A[1]:(0.68166834116) A[2]:(0.505348443985) A[3]:(0.611720919609)\n",
      " state (1)  A[0]:(0.493197470903) A[1]:(0.115618512034) A[2]:(0.560964107513) A[3]:(0.529296755791)\n",
      " state (2)  A[0]:(0.616526007652) A[1]:(0.329497247934) A[2]:(0.519529700279) A[3]:(0.565443456173)\n",
      " state (3)  A[0]:(0.700592041016) A[1]:(0.510430932045) A[2]:(0.456650078297) A[3]:(0.599023222923)\n",
      " state (4)  A[0]:(0.644151568413) A[1]:(0.75993335247) A[2]:(0.0219771638513) A[3]:(0.590544462204)\n",
      " state (5)  A[0]:(0.229293033481) A[1]:(0.923098504543) A[2]:(-0.73726785183) A[3]:(0.54104334116)\n",
      " state (6)  A[0]:(0.263619691133) A[1]:(0.669010996819) A[2]:(-0.206971734762) A[3]:(0.693930983543)\n",
      " state (7)  A[0]:(0.594957351685) A[1]:(-0.131021991372) A[2]:(0.709029912949) A[3]:(0.702891290188)\n",
      " state (8)  A[0]:(0.737681210041) A[1]:(0.0237386915833) A[2]:(0.831045985222) A[3]:(0.52248775959)\n",
      " state (9)  A[0]:(0.686719059944) A[1]:(0.882508814335) A[2]:(0.665120601654) A[3]:(0.251660346985)\n",
      " state (10)  A[0]:(0.533840060234) A[1]:(0.997595667839) A[2]:(0.447418540716) A[3]:(0.136247456074)\n",
      " state (11)  A[0]:(0.378757506609) A[1]:(0.999963641167) A[2]:(0.591025531292) A[3]:(0.3080714643)\n",
      " state (12)  A[0]:(0.30344158411) A[1]:(0.99999922514) A[2]:(0.86222910881) A[3]:(0.612570345402)\n",
      " state (13)  A[0]:(0.32298412919) A[1]:(0.999999940395) A[2]:(0.964259982109) A[3]:(0.814988791943)\n",
      " state (14)  A[0]:(0.411534816027) A[1]:(1.0) A[2]:(0.987557649612) A[3]:(0.902062356472)\n",
      " state (15)  A[0]:(0.530721664429) A[1]:(1.0) A[2]:(0.993358552456) A[3]:(0.937104642391)\n",
      "Episode 68000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained:               9987. Times reached goal: 133.               Steps done: 529662. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.52992408653.\n",
      " state (0)  A[0]:(0.610881090164) A[1]:(0.687259197235) A[2]:(0.50477039814) A[3]:(0.615950942039)\n",
      " state (1)  A[0]:(0.472369074821) A[1]:(0.118824444711) A[2]:(0.561432123184) A[3]:(0.525875866413)\n",
      " state (2)  A[0]:(0.604655981064) A[1]:(0.338726907969) A[2]:(0.524580717087) A[3]:(0.567121744156)\n",
      " state (3)  A[0]:(0.694873332977) A[1]:(0.521161794662) A[2]:(0.467775583267) A[3]:(0.605168700218)\n",
      " state (4)  A[0]:(0.634900569916) A[1]:(0.76599162817) A[2]:(0.031944964081) A[3]:(0.588278651237)\n",
      " state (5)  A[0]:(0.198208242655) A[1]:(0.922125756741) A[2]:(-0.733053684235) A[3]:(0.509018242359)\n",
      " state (6)  A[0]:(0.230530261993) A[1]:(0.670364499092) A[2]:(-0.197473958135) A[3]:(0.677899241447)\n",
      " state (7)  A[0]:(0.582755625248) A[1]:(-0.111313395202) A[2]:(0.710281431675) A[3]:(0.697823286057)\n",
      " state (8)  A[0]:(0.742614090443) A[1]:(0.0193039029837) A[2]:(0.839895606041) A[3]:(0.524347066879)\n",
      " state (9)  A[0]:(0.691735744476) A[1]:(0.883186578751) A[2]:(0.68097615242) A[3]:(0.257226288319)\n",
      " state (10)  A[0]:(0.524629533291) A[1]:(0.997831940651) A[2]:(0.452018707991) A[3]:(0.139197036624)\n",
      " state (11)  A[0]:(0.352418571711) A[1]:(0.999970793724) A[2]:(0.588809609413) A[3]:(0.309739023447)\n",
      " state (12)  A[0]:(0.272694081068) A[1]:(0.999999403954) A[2]:(0.864516198635) A[3]:(0.615851163864)\n",
      " state (13)  A[0]:(0.30080935359) A[1]:(1.0) A[2]:(0.966132223606) A[3]:(0.818334519863)\n",
      " state (14)  A[0]:(0.404241085052) A[1]:(1.0) A[2]:(0.988511681557) A[3]:(0.904625415802)\n",
      " state (15)  A[0]:(0.53784686327) A[1]:(1.0) A[2]:(0.993942081928) A[3]:(0.939165353775)\n",
      "Episode 69000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained:               9853. Times reached goal: 139.               Steps done: 539515. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.524728383171.\n",
      "q_values \n",
      "tensor([[ 0.6066,  0.6850,  0.4886,  0.6124]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6289,  0.7621,  0.0372,  0.5824]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6290,  0.7620,  0.0369,  0.5824]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7444,  0.0124,  0.8350,  0.5248]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6945,  0.8807,  0.6687,  0.2588]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5148,  0.9979,  0.4098,  0.1350]], device='cuda:0')\n",
      "On state=10, selected action=0 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6947,  0.8807,  0.6711,  0.2612]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5152,  0.9979,  0.4151,  0.1385]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4128,  1.0000,  0.9885,  0.9076]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4127,  1.0000,  0.9885,  0.9079]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4139,  1.0000,  0.9885,  0.9081]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5182,  0.9980,  0.4217,  0.1459]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4185,  1.0000,  0.9885,  0.9087]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5217,  0.9980,  0.4189,  0.1478]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4220,  1.0000,  0.9884,  0.9089]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4226,  1.0000,  0.9884,  0.9090]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4242,  1.0000,  0.9884,  0.9092]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5242,  0.9980,  0.4141,  0.1496]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4258,  1.0000,  0.9884,  0.9094]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4264,  1.0000,  0.9883,  0.9094]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4269,  1.0000,  0.9883,  0.9094]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5255,  0.9980,  0.4094,  0.1493]], device='cuda:0')\n",
      "On state=10, selected action=3 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2112,  0.6930, -0.2401,  0.6624]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5271,  0.9980,  0.4057,  0.1485]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4314,  1.0000,  0.9881,  0.9093]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5283,  0.9980,  0.4013,  0.1470]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4316,  1.0000,  0.9881,  0.9089]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5268,  0.9979,  0.3969,  0.1427]], device='cuda:0')\n",
      "On state=10, selected action=2 , Random? True\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.609057545662) A[1]:(0.68378329277) A[2]:(0.483144104481) A[3]:(0.61318397522)\n",
      " state (1)  A[0]:(0.444610536098) A[1]:(0.106849603355) A[2]:(0.539809584618) A[3]:(0.51132273674)\n",
      " state (2)  A[0]:(0.588463842869) A[1]:(0.333945453167) A[2]:(0.503863573074) A[3]:(0.559542655945)\n",
      " state (3)  A[0]:(0.687687814236) A[1]:(0.520276904106) A[2]:(0.449562460184) A[3]:(0.603848576546)\n",
      " state (4)  A[0]:(0.634225606918) A[1]:(0.762664139271) A[2]:(0.0239924043417) A[3]:(0.584629833698)\n",
      " state (5)  A[0]:(0.191600009799) A[1]:(0.921797156334) A[2]:(-0.740957021713) A[3]:(0.478400856256)\n",
      " state (6)  A[0]:(0.211747571826) A[1]:(0.687209606171) A[2]:(-0.242914184928) A[3]:(0.660988211632)\n",
      " state (7)  A[0]:(0.574565649033) A[1]:(-0.0728241950274) A[2]:(0.680112361908) A[3]:(0.695215463638)\n",
      " state (8)  A[0]:(0.749534010887) A[1]:(0.0190305374563) A[2]:(0.829273402691) A[3]:(0.530150175095)\n",
      " state (9)  A[0]:(0.701247692108) A[1]:(0.881227791309) A[2]:(0.658391356468) A[3]:(0.264336705208)\n",
      " state (10)  A[0]:(0.526056885719) A[1]:(0.997928082943) A[2]:(0.392426222563) A[3]:(0.139117658138)\n",
      " state (11)  A[0]:(0.345002412796) A[1]:(0.999974310398) A[2]:(0.528359472752) A[3]:(0.30558976531)\n",
      " state (12)  A[0]:(0.268826633692) A[1]:(0.999999523163) A[2]:(0.844426870346) A[3]:(0.615412950516)\n",
      " state (13)  A[0]:(0.310809850693) A[1]:(1.0) A[2]:(0.962797760963) A[3]:(0.821166753769)\n",
      " state (14)  A[0]:(0.430307775736) A[1]:(1.0) A[2]:(0.987939953804) A[3]:(0.908006370068)\n",
      " state (15)  A[0]:(0.574935078621) A[1]:(1.0) A[2]:(0.993822813034) A[3]:(0.942408144474)\n",
      "Episode 70000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               9623. Times reached goal: 132.               Steps done: 549138. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.519703139678.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.61618745327) A[1]:(0.687051892281) A[2]:(0.475677013397) A[3]:(0.619792103767)\n",
      " state (1)  A[0]:(0.426473349333) A[1]:(0.121400691569) A[2]:(0.527474999428) A[3]:(0.501529693604)\n",
      " state (2)  A[0]:(0.578172087669) A[1]:(0.352197736502) A[2]:(0.493627905846) A[3]:(0.556074261665)\n",
      " state (3)  A[0]:(0.683417677879) A[1]:(0.537229776382) A[2]:(0.444167047739) A[3]:(0.606158614159)\n",
      " state (4)  A[0]:(0.632900416851) A[1]:(0.770329415798) A[2]:(0.0346701592207) A[3]:(0.584815263748)\n",
      " state (5)  A[0]:(0.184045478702) A[1]:(0.923689067364) A[2]:(-0.725936710835) A[3]:(0.45635035634)\n",
      " state (6)  A[0]:(0.19408929348) A[1]:(0.704218626022) A[2]:(-0.21617180109) A[3]:(0.649036049843)\n",
      " state (7)  A[0]:(0.566113352776) A[1]:(-0.0198167245835) A[2]:(0.686192154884) A[3]:(0.689296364784)\n",
      " state (8)  A[0]:(0.755412578583) A[1]:(0.0412865281105) A[2]:(0.83801472187) A[3]:(0.523889601231)\n",
      " state (9)  A[0]:(0.708930969238) A[1]:(0.883845329285) A[2]:(0.670429825783) A[3]:(0.252683997154)\n",
      " state (10)  A[0]:(0.523284912109) A[1]:(0.998086452484) A[2]:(0.379658609629) A[3]:(0.116693235934)\n",
      " state (11)  A[0]:(0.32965567708) A[1]:(0.999978125095) A[2]:(0.499910980463) A[3]:(0.277145981789)\n",
      " state (12)  A[0]:(0.254485219717) A[1]:(0.999999642372) A[2]:(0.835356652737) A[3]:(0.596296191216)\n",
      " state (13)  A[0]:(0.309090077877) A[1]:(1.0) A[2]:(0.962481975555) A[3]:(0.813977479935)\n",
      " state (14)  A[0]:(0.444190859795) A[1]:(1.0) A[2]:(0.988451957703) A[3]:(0.906113564968)\n",
      " state (15)  A[0]:(0.599815487862) A[1]:(1.0) A[2]:(0.994289696217) A[3]:(0.942372500896)\n",
      "Episode 71000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               9370. Times reached goal: 152.               Steps done: 558508. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.514856264332.\n",
      " state (0)  A[0]:(0.613796234131) A[1]:(0.69072920084) A[2]:(0.454607218504) A[3]:(0.616480588913)\n",
      " state (1)  A[0]:(0.385930716991) A[1]:(0.127138927579) A[2]:(0.503180503845) A[3]:(0.473824262619)\n",
      " state (2)  A[0]:(0.54785656929) A[1]:(0.358360260725) A[2]:(0.468194842339) A[3]:(0.535978734493)\n",
      " state (3)  A[0]:(0.663106441498) A[1]:(0.54147040844) A[2]:(0.420928180218) A[3]:(0.593453884125)\n",
      " state (4)  A[0]:(0.624979019165) A[1]:(0.764493227005) A[2]:(0.0456515103579) A[3]:(0.576479792595)\n",
      " state (5)  A[0]:(0.177701339126) A[1]:(0.92425596714) A[2]:(-0.720603883266) A[3]:(0.420795023441)\n",
      " state (6)  A[0]:(0.167584255338) A[1]:(0.727216720581) A[2]:(-0.247178211808) A[3]:(0.620989561081)\n",
      " state (7)  A[0]:(0.549156308174) A[1]:(0.0187712907791) A[2]:(0.669668197632) A[3]:(0.678008317947)\n",
      " state (8)  A[0]:(0.754382371902) A[1]:(0.0346604585648) A[2]:(0.83766412735) A[3]:(0.522636890411)\n",
      " state (9)  A[0]:(0.706510424614) A[1]:(0.880191683769) A[2]:(0.669707775116) A[3]:(0.260879874229)\n",
      " state (10)  A[0]:(0.502463579178) A[1]:(0.998154759407) A[2]:(0.353259921074) A[3]:(0.12656083703)\n",
      " state (11)  A[0]:(0.289057910442) A[1]:(0.9999807477) A[2]:(0.463312864304) A[3]:(0.285051822662)\n",
      " state (12)  A[0]:(0.212930470705) A[1]:(0.999999701977) A[2]:(0.823745906353) A[3]:(0.603236913681)\n",
      " state (13)  A[0]:(0.281922727823) A[1]:(1.0) A[2]:(0.961474895477) A[3]:(0.819563210011)\n",
      " state (14)  A[0]:(0.436662107706) A[1]:(1.0) A[2]:(0.988611757755) A[3]:(0.910324931145)\n",
      " state (15)  A[0]:(0.607990324497) A[1]:(1.0) A[2]:(0.994502127171) A[3]:(0.945785880089)\n",
      "Episode 72000 finished after 0 timesteps with r=0.0. Running score: 0.13. Times trained:               9715. Times reached goal: 136.               Steps done: 568223. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.509878653618.\n",
      " state (0)  A[0]:(0.613291144371) A[1]:(0.689191937447) A[2]:(0.426070958376) A[3]:(0.615794897079)\n",
      " state (1)  A[0]:(0.344448268414) A[1]:(0.130832016468) A[2]:(0.480783313513) A[3]:(0.445227652788)\n",
      " state (2)  A[0]:(0.517473578453) A[1]:(0.371425718069) A[2]:(0.445534706116) A[3]:(0.51644384861)\n",
      " state (3)  A[0]:(0.643344521523) A[1]:(0.557091116905) A[2]:(0.401152521372) A[3]:(0.582482159138)\n",
      " state (4)  A[0]:(0.616162657738) A[1]:(0.767952501774) A[2]:(0.0563547722995) A[3]:(0.570266485214)\n",
      " state (5)  A[0]:(0.173737123609) A[1]:(0.92452031374) A[2]:(-0.703867614269) A[3]:(0.392341345549)\n",
      " state (6)  A[0]:(0.148165166378) A[1]:(0.738499283791) A[2]:(-0.242822304368) A[3]:(0.599730491638)\n",
      " state (7)  A[0]:(0.537581503391) A[1]:(0.0463623404503) A[2]:(0.661089420319) A[3]:(0.667828738689)\n",
      " state (8)  A[0]:(0.757429361343) A[1]:(0.0260878596455) A[2]:(0.835876584053) A[3]:(0.515761733055)\n",
      " state (9)  A[0]:(0.709877192974) A[1]:(0.875963568687) A[2]:(0.660186171532) A[3]:(0.255397498608)\n",
      " state (10)  A[0]:(0.49064129591) A[1]:(0.99820369482) A[2]:(0.301927536726) A[3]:(0.115089297295)\n",
      " state (11)  A[0]:(0.260601431131) A[1]:(0.999982833862) A[2]:(0.394172668457) A[3]:(0.265924304724)\n",
      " state (12)  A[0]:(0.187222838402) A[1]:(0.999999761581) A[2]:(0.797740101814) A[3]:(0.588034510612)\n",
      " state (13)  A[0]:(0.273754477501) A[1]:(1.0) A[2]:(0.957991659641) A[3]:(0.813446879387)\n",
      " state (14)  A[0]:(0.44884622097) A[1]:(1.0) A[2]:(0.988336265087) A[3]:(0.908780276775)\n",
      " state (15)  A[0]:(0.632879376411) A[1]:(1.0) A[2]:(0.994606554508) A[3]:(0.94590485096)\n",
      "Episode 73000 finished after 0 timesteps with r=0.0. Running score: 0.23. Times trained:               10108. Times reached goal: 165.               Steps done: 578331. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.50475076022.\n",
      " state (0)  A[0]:(0.617896139622) A[1]:(0.690456986427) A[2]:(0.421054303646) A[3]:(0.62048125267)\n",
      " state (1)  A[0]:(0.326823174953) A[1]:(0.136073485017) A[2]:(0.468845754862) A[3]:(0.424141854048)\n",
      " state (2)  A[0]:(0.504706025124) A[1]:(0.376407533884) A[2]:(0.428474515676) A[3]:(0.502814948559)\n",
      " state (3)  A[0]:(0.636275529861) A[1]:(0.56005448103) A[2]:(0.38149368763) A[3]:(0.576066374779)\n",
      " state (4)  A[0]:(0.619612872601) A[1]:(0.762878298759) A[2]:(0.060675740242) A[3]:(0.569600522518)\n",
      " state (5)  A[0]:(0.183438703418) A[1]:(0.923819065094) A[2]:(-0.695361077785) A[3]:(0.371041744947)\n",
      " state (6)  A[0]:(0.130330115557) A[1]:(0.754333019257) A[2]:(-0.271305501461) A[3]:(0.581747472286)\n",
      " state (7)  A[0]:(0.517987966537) A[1]:(0.0964095294476) A[2]:(0.6292719841) A[3]:(0.663374364376)\n",
      " state (8)  A[0]:(0.75718164444) A[1]:(0.0347914285958) A[2]:(0.827919363976) A[3]:(0.519490838051)\n",
      " state (9)  A[0]:(0.713447451591) A[1]:(0.874038755894) A[2]:(0.656411528587) A[3]:(0.271417915821)\n",
      " state (10)  A[0]:(0.479678243399) A[1]:(0.998318970203) A[2]:(0.28130826354) A[3]:(0.138502553105)\n",
      " state (11)  A[0]:(0.230895385146) A[1]:(0.999985694885) A[2]:(0.364788293839) A[3]:(0.287618547678)\n",
      " state (12)  A[0]:(0.159726426005) A[1]:(0.999999821186) A[2]:(0.788033246994) A[3]:(0.601119697094)\n",
      " state (13)  A[0]:(0.265376567841) A[1]:(1.0) A[2]:(0.956996321678) A[3]:(0.818984627724)\n",
      " state (14)  A[0]:(0.46254709363) A[1]:(1.0) A[2]:(0.988236546516) A[3]:(0.911254525185)\n",
      " state (15)  A[0]:(0.658975958824) A[1]:(1.0) A[2]:(0.994561970234) A[3]:(0.94743180275)\n",
      "Episode 74000 finished after 0 timesteps with r=0.0. Running score: 0.18. Times trained:               10489. Times reached goal: 165.               Steps done: 588820. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.499484098788.\n",
      "q_values \n",
      "tensor([[ 0.6141,  0.6884,  0.4137,  0.6151]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6167,  0.7627,  0.0648,  0.5703]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7586,  0.0401,  0.8355,  0.5267]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7168,  0.8741,  0.6832,  0.2911]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2330,  1.0000,  0.9602,  0.8250]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.614450931549) A[1]:(0.687138855457) A[2]:(0.413118034601) A[3]:(0.615131139755)\n",
      " state (1)  A[0]:(0.31678083539) A[1]:(0.136799171567) A[2]:(0.459466844797) A[3]:(0.406328946352)\n",
      " state (2)  A[0]:(0.49537563324) A[1]:(0.380234569311) A[2]:(0.416391015053) A[3]:(0.491205811501)\n",
      " state (3)  A[0]:(0.629210412502) A[1]:(0.564407348633) A[2]:(0.368719488382) A[3]:(0.570733904839)\n",
      " state (4)  A[0]:(0.617746651173) A[1]:(0.761797308922) A[2]:(0.063703879714) A[3]:(0.57062792778)\n",
      " state (5)  A[0]:(0.182618185878) A[1]:(0.923891425133) A[2]:(-0.68932890892) A[3]:(0.364890724421)\n",
      " state (6)  A[0]:(0.110516540706) A[1]:(0.765256106853) A[2]:(-0.285317093134) A[3]:(0.578611135483)\n",
      " state (7)  A[0]:(0.502673089504) A[1]:(0.131922632456) A[2]:(0.61729156971) A[3]:(0.66618424654)\n",
      " state (8)  A[0]:(0.759628355503) A[1]:(0.0403503328562) A[2]:(0.834060072899) A[3]:(0.526325821877)\n",
      " state (9)  A[0]:(0.717904806137) A[1]:(0.874288618565) A[2]:(0.680599808693) A[3]:(0.29060909152)\n",
      " state (10)  A[0]:(0.465726912022) A[1]:(0.998479485512) A[2]:(0.308136463165) A[3]:(0.166861385107)\n",
      " state (11)  A[0]:(0.190338850021) A[1]:(0.999988675117) A[2]:(0.377303689718) A[3]:(0.313174545765)\n",
      " state (12)  A[0]:(0.114474222064) A[1]:(0.999999880791) A[2]:(0.795687675476) A[3]:(0.615648269653)\n",
      " state (13)  A[0]:(0.235154777765) A[1]:(1.0) A[2]:(0.959992289543) A[3]:(0.824952125549)\n",
      " state (14)  A[0]:(0.454846352339) A[1]:(1.0) A[2]:(0.989337623119) A[3]:(0.913922429085)\n",
      " state (15)  A[0]:(0.667540311813) A[1]:(1.0) A[2]:(0.995122492313) A[3]:(0.949054300785)\n",
      "Episode 75000 finished after 0 timesteps with r=0.0. Running score: 0.17. Times trained:               10131. Times reached goal: 182.               Steps done: 598951. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.494449371855.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.612910449505) A[1]:(0.677452921867) A[2]:(0.413370519876) A[3]:(0.610699772835)\n",
      " state (1)  A[0]:(0.320919364691) A[1]:(0.130300849676) A[2]:(0.456883460283) A[3]:(0.398529797792)\n",
      " state (2)  A[0]:(0.498352259398) A[1]:(0.374121248722) A[2]:(0.410195440054) A[3]:(0.48610880971)\n",
      " state (3)  A[0]:(0.631897807121) A[1]:(0.558805465698) A[2]:(0.359627574682) A[3]:(0.568639159203)\n",
      " state (4)  A[0]:(0.620286941528) A[1]:(0.75928580761) A[2]:(0.0500533990562) A[3]:(0.568217158318)\n",
      " state (5)  A[0]:(0.18534861505) A[1]:(0.923659622669) A[2]:(-0.694049119949) A[3]:(0.356711775064)\n",
      " state (6)  A[0]:(0.10304389894) A[1]:(0.767320394516) A[2]:(-0.302154392004) A[3]:(0.582038760185)\n",
      " state (7)  A[0]:(0.49356251955) A[1]:(0.160236120224) A[2]:(0.586107611656) A[3]:(0.669190764427)\n",
      " state (8)  A[0]:(0.767322540283) A[1]:(0.0358819030225) A[2]:(0.825775682926) A[3]:(0.521872162819)\n",
      " state (9)  A[0]:(0.732862710953) A[1]:(0.866886913776) A[2]:(0.675131082535) A[3]:(0.28476563096)\n",
      " state (10)  A[0]:(0.472267895937) A[1]:(0.998476922512) A[2]:(0.269654512405) A[3]:(0.161147192121)\n",
      " state (11)  A[0]:(0.176247671247) A[1]:(0.999989807606) A[2]:(0.309546142817) A[3]:(0.300181150436)\n",
      " state (12)  A[0]:(0.0994884297252) A[1]:(0.999999880791) A[2]:(0.766079545021) A[3]:(0.600508093834)\n",
      " state (13)  A[0]:(0.238547101617) A[1]:(1.0) A[2]:(0.956269741058) A[3]:(0.815745353699)\n",
      " state (14)  A[0]:(0.47959446907) A[1]:(1.0) A[2]:(0.989111840725) A[3]:(0.909308373928)\n",
      " state (15)  A[0]:(0.70061147213) A[1]:(1.0) A[2]:(0.995258033276) A[3]:(0.946574866772)\n",
      "Episode 76000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               10648. Times reached goal: 164.               Steps done: 609599. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.48921240603.\n",
      " state (0)  A[0]:(0.612870335579) A[1]:(0.679078817368) A[2]:(0.412525177002) A[3]:(0.613742947578)\n",
      " state (1)  A[0]:(0.317058324814) A[1]:(0.13480964303) A[2]:(0.456676393747) A[3]:(0.398500859737)\n",
      " state (2)  A[0]:(0.494342505932) A[1]:(0.374181032181) A[2]:(0.407001435757) A[3]:(0.486305147409)\n",
      " state (3)  A[0]:(0.62874174118) A[1]:(0.554897069931) A[2]:(0.355912715197) A[3]:(0.570356249809)\n",
      " state (4)  A[0]:(0.617805242538) A[1]:(0.752712726593) A[2]:(0.0581486932933) A[3]:(0.574331223965)\n",
      " state (5)  A[0]:(0.175297021866) A[1]:(0.920944273472) A[2]:(-0.678774356842) A[3]:(0.370196342468)\n",
      " state (6)  A[0]:(0.0739925131202) A[1]:(0.764801383018) A[2]:(-0.291230022907) A[3]:(0.597897410393)\n",
      " state (7)  A[0]:(0.466884672642) A[1]:(0.183818727732) A[2]:(0.571598768234) A[3]:(0.677929878235)\n",
      " state (8)  A[0]:(0.763455212116) A[1]:(0.0421745218337) A[2]:(0.824309647083) A[3]:(0.521682620049)\n",
      " state (9)  A[0]:(0.732334494591) A[1]:(0.866100132465) A[2]:(0.681555330753) A[3]:(0.289030790329)\n",
      " state (10)  A[0]:(0.451774150133) A[1]:(0.998592913151) A[2]:(0.258351475) A[3]:(0.177754342556)\n",
      " state (11)  A[0]:(0.13031475246) A[1]:(0.99999165535) A[2]:(0.276037245989) A[3]:(0.31701746583)\n",
      " state (12)  A[0]:(0.0577648691833) A[1]:(0.999999940395) A[2]:(0.753937900066) A[3]:(0.60806453228)\n",
      " state (13)  A[0]:(0.223611965775) A[1]:(1.0) A[2]:(0.956561326981) A[3]:(0.817302286625)\n",
      " state (14)  A[0]:(0.494620084763) A[1]:(1.0) A[2]:(0.989886820316) A[3]:(0.909304261208)\n",
      " state (15)  A[0]:(0.728398680687) A[1]:(1.0) A[2]:(0.995797932148) A[3]:(0.946312904358)\n",
      "Episode 77000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained:               10502. Times reached goal: 172.               Steps done: 620101. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.484101581256.\n",
      " state (0)  A[0]:(0.605473875999) A[1]:(0.672480106354) A[2]:(0.415480434895) A[3]:(0.605822145939)\n",
      " state (1)  A[0]:(0.316209375858) A[1]:(0.128872036934) A[2]:(0.460871040821) A[3]:(0.393464386463)\n",
      " state (2)  A[0]:(0.492907851934) A[1]:(0.3682282269) A[2]:(0.410675764084) A[3]:(0.481304138899)\n",
      " state (3)  A[0]:(0.627373874187) A[1]:(0.549686193466) A[2]:(0.359845995903) A[3]:(0.566598415375)\n",
      " state (4)  A[0]:(0.614868998528) A[1]:(0.753109335899) A[2]:(0.0602947771549) A[3]:(0.573101043701)\n",
      " state (5)  A[0]:(0.16953189671) A[1]:(0.923326909542) A[2]:(-0.672060728073) A[3]:(0.38567712903)\n",
      " state (6)  A[0]:(0.0590546391904) A[1]:(0.772080302238) A[2]:(-0.281043231487) A[3]:(0.624179780483)\n",
      " state (7)  A[0]:(0.448844820261) A[1]:(0.224992454052) A[2]:(0.562264204025) A[3]:(0.69541490078)\n",
      " state (8)  A[0]:(0.762984514236) A[1]:(0.0671158283949) A[2]:(0.827021837234) A[3]:(0.525275945663)\n",
      " state (9)  A[0]:(0.735396385193) A[1]:(0.868926644325) A[2]:(0.696492791176) A[3]:(0.288160413504)\n",
      " state (10)  A[0]:(0.434731483459) A[1]:(0.998728692532) A[2]:(0.257378488779) A[3]:(0.183404117823)\n",
      " state (11)  A[0]:(0.0844114646316) A[1]:(0.99999332428) A[2]:(0.242057919502) A[3]:(0.320008844137)\n",
      " state (12)  A[0]:(0.0146658262238) A[1]:(0.999999940395) A[2]:(0.739924550056) A[3]:(0.604630529881)\n",
      " state (13)  A[0]:(0.2084749192) A[1]:(1.0) A[2]:(0.957477927208) A[3]:(0.813406467438)\n",
      " state (14)  A[0]:(0.511182069778) A[1]:(1.0) A[2]:(0.991061925888) A[3]:(0.906933903694)\n",
      " state (15)  A[0]:(0.75638961792) A[1]:(1.0) A[2]:(0.996570289135) A[3]:(0.944893121719)\n",
      "Episode 78000 finished after 0 timesteps with r=0.0. Running score: 0.21. Times trained:               10385. Times reached goal: 176.               Steps done: 630486. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.479100200951.\n",
      " state (0)  A[0]:(0.606635808945) A[1]:(0.682215213776) A[2]:(0.410914123058) A[3]:(0.605612516403)\n",
      " state (1)  A[0]:(0.317303657532) A[1]:(0.14535908401) A[2]:(0.452755659819) A[3]:(0.388509213924)\n",
      " state (2)  A[0]:(0.492629289627) A[1]:(0.382043093443) A[2]:(0.399340569973) A[3]:(0.476196229458)\n",
      " state (3)  A[0]:(0.627041101456) A[1]:(0.559847652912) A[2]:(0.345846772194) A[3]:(0.562895298004)\n",
      " state (4)  A[0]:(0.617650985718) A[1]:(0.758312284946) A[2]:(0.0455848351121) A[3]:(0.577859520912)\n",
      " state (5)  A[0]:(0.180963426828) A[1]:(0.926105618477) A[2]:(-0.684079170227) A[3]:(0.418399661779)\n",
      " state (6)  A[0]:(0.0601233206689) A[1]:(0.78108561039) A[2]:(-0.326255679131) A[3]:(0.657169342041)\n",
      " state (7)  A[0]:(0.44848844409) A[1]:(0.253113001585) A[2]:(0.513137817383) A[3]:(0.715791463852)\n",
      " state (8)  A[0]:(0.774073839188) A[1]:(0.0666963085532) A[2]:(0.812866449356) A[3]:(0.526531875134)\n",
      " state (9)  A[0]:(0.752702951431) A[1]:(0.86249101162) A[2]:(0.682745873928) A[3]:(0.281143397093)\n",
      " state (10)  A[0]:(0.443220049143) A[1]:(0.998730659485) A[2]:(0.192437753081) A[3]:(0.185274064541)\n",
      " state (11)  A[0]:(0.0640675276518) A[1]:(0.999993979931) A[2]:(0.119781285524) A[3]:(0.320944726467)\n",
      " state (12)  A[0]:(-0.0081139812246) A[1]:(0.999999940395) A[2]:(0.670880675316) A[3]:(0.599871575832)\n",
      " state (13)  A[0]:(0.210271984339) A[1]:(1.0) A[2]:(0.949005305767) A[3]:(0.809272408485)\n",
      " state (14)  A[0]:(0.5396733284) A[1]:(1.0) A[2]:(0.990564703941) A[3]:(0.904913544655)\n",
      " state (15)  A[0]:(0.788465201855) A[1]:(1.0) A[2]:(0.996758162975) A[3]:(0.943922162056)\n",
      "Episode 79000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               10033. Times reached goal: 176.               Steps done: 640519. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.474317421568.\n",
      "q_values \n",
      "tensor([[ 0.6017,  0.6734,  0.4037,  0.6069]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6123,  0.7563,  0.0441,  0.5845]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7791,  0.0796,  0.8146,  0.5389]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7643,  0.8623,  0.7000,  0.2935]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2016,  1.0000,  0.9454,  0.8155]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.2002,  1.0000,  0.9451,  0.8153]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7632,  0.8602,  0.6998,  0.2927]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4388,  0.9988,  0.1890,  0.2138]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5550,  1.0000,  0.9910,  0.9066]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5531,  1.0000,  0.9910,  0.9064]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4338,  0.9988,  0.1880,  0.2107]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5475,  1.0000,  0.9910,  0.9057]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5442,  1.0000,  0.9910,  0.9054]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? True\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.599710226059) A[1]:(0.670433521271) A[2]:(0.406291723251) A[3]:(0.606393396854)\n",
      " state (1)  A[0]:(0.308016389608) A[1]:(0.133503913879) A[2]:(0.452078133821) A[3]:(0.3870677948)\n",
      " state (2)  A[0]:(0.480247378349) A[1]:(0.368889033794) A[2]:(0.397046148777) A[3]:(0.472906947136)\n",
      " state (3)  A[0]:(0.614279150963) A[1]:(0.546424329281) A[2]:(0.342912882566) A[3]:(0.559622168541)\n",
      " state (4)  A[0]:(0.606153964996) A[1]:(0.746492981911) A[2]:(0.051112536341) A[3]:(0.58309841156)\n",
      " state (5)  A[0]:(0.168494090438) A[1]:(0.922597169876) A[2]:(-0.678382158279) A[3]:(0.451481550932)\n",
      " state (6)  A[0]:(0.0329872742295) A[1]:(0.773520827293) A[2]:(-0.337339013815) A[3]:(0.688417613506)\n",
      " state (7)  A[0]:(0.425648927689) A[1]:(0.24249650538) A[2]:(0.4981752038) A[3]:(0.736801981926)\n",
      " state (8)  A[0]:(0.773694992065) A[1]:(0.0388905853033) A[2]:(0.817416250706) A[3]:(0.532618165016)\n",
      " state (9)  A[0]:(0.756716012955) A[1]:(0.85194092989) A[2]:(0.701253533363) A[3]:(0.285060346127)\n",
      " state (10)  A[0]:(0.425444692373) A[1]:(0.998718976974) A[2]:(0.18874783814) A[3]:(0.205940648913)\n",
      " state (11)  A[0]:(0.00637722294778) A[1]:(0.999994575977) A[2]:(0.0542185381055) A[3]:(0.343555808067)\n",
      " state (12)  A[0]:(-0.0709174498916) A[1]:(0.999999940395) A[2]:(0.621434748173) A[3]:(0.609792351723)\n",
      " state (13)  A[0]:(0.173875644803) A[1]:(1.0) A[2]:(0.944415330887) A[3]:(0.811243534088)\n",
      " state (14)  A[0]:(0.540804862976) A[1]:(1.0) A[2]:(0.990981280804) A[3]:(0.905054092407)\n",
      " state (15)  A[0]:(0.804425597191) A[1]:(1.0) A[2]:(0.997246563435) A[3]:(0.943733930588)\n",
      "Episode 80000 finished after 0 timesteps with r=1.0. Running score: 0.17. Times trained:               10024. Times reached goal: 167.               Steps done: 650543. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.469586614154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.600633621216) A[1]:(0.663536190987) A[2]:(0.402676254511) A[3]:(0.599580645561)\n",
      " state (1)  A[0]:(0.309242069721) A[1]:(0.127345755696) A[2]:(0.441431134939) A[3]:(0.373811125755)\n",
      " state (2)  A[0]:(0.476341664791) A[1]:(0.362830042839) A[2]:(0.385910332203) A[3]:(0.457669466734)\n",
      " state (3)  A[0]:(0.608326435089) A[1]:(0.541120767593) A[2]:(0.33123332262) A[3]:(0.544005036354)\n",
      " state (4)  A[0]:(0.604569554329) A[1]:(0.740876674652) A[2]:(0.0470656268299) A[3]:(0.572891831398)\n",
      " state (5)  A[0]:(0.182335063815) A[1]:(0.92150247097) A[2]:(-0.679289996624) A[3]:(0.456190526485)\n",
      " state (6)  A[0]:(0.0276575759053) A[1]:(0.774021148682) A[2]:(-0.370097309351) A[3]:(0.699221372604)\n",
      " state (7)  A[0]:(0.401922911406) A[1]:(0.260435074568) A[2]:(0.453425765038) A[3]:(0.747014164925)\n",
      " state (8)  A[0]:(0.766234993935) A[1]:(0.0631797462702) A[2]:(0.804683089256) A[3]:(0.524045526981)\n",
      " state (9)  A[0]:(0.752454996109) A[1]:(0.857615113258) A[2]:(0.695858895779) A[3]:(0.260582715273)\n",
      " state (10)  A[0]:(0.392603754997) A[1]:(0.998863875866) A[2]:(0.153208166361) A[3]:(0.19522203505)\n",
      " state (11)  A[0]:(-0.0593578182161) A[1]:(0.999995648861) A[2]:(-0.0226116962731) A[3]:(0.340888887644)\n",
      " state (12)  A[0]:(-0.12630200386) A[1]:(1.0) A[2]:(0.573539674282) A[3]:(0.603097200394)\n",
      " state (13)  A[0]:(0.157873436809) A[1]:(1.0) A[2]:(0.942368268967) A[3]:(0.803435862064)\n",
      " state (14)  A[0]:(0.564169287682) A[1]:(1.0) A[2]:(0.991991400719) A[3]:(0.899291753769)\n",
      " state (15)  A[0]:(0.831637859344) A[1]:(1.0) A[2]:(0.997865974903) A[3]:(0.93957811594)\n",
      "Episode 81000 finished after 0 timesteps with r=0.0. Running score: 0.15. Times trained:               10100. Times reached goal: 175.               Steps done: 660643. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.464867660183.\n",
      " state (0)  A[0]:(0.597009301186) A[1]:(0.660126268864) A[2]:(0.395560353994) A[3]:(0.599668383598)\n",
      " state (1)  A[0]:(0.310931295156) A[1]:(0.128055095673) A[2]:(0.442073911428) A[3]:(0.377117723227)\n",
      " state (2)  A[0]:(0.470852315426) A[1]:(0.360152721405) A[2]:(0.385155439377) A[3]:(0.457850068808)\n",
      " state (3)  A[0]:(0.599687457085) A[1]:(0.536771118641) A[2]:(0.329038053751) A[3]:(0.542144656181)\n",
      " state (4)  A[0]:(0.600080013275) A[1]:(0.73382383585) A[2]:(0.0572114996612) A[3]:(0.57477247715)\n",
      " state (5)  A[0]:(0.192875385284) A[1]:(0.919666409492) A[2]:(-0.667692422867) A[3]:(0.467709004879)\n",
      " state (6)  A[0]:(0.0218095164746) A[1]:(0.770030140877) A[2]:(-0.380437165499) A[3]:(0.710920214653)\n",
      " state (7)  A[0]:(0.388521790504) A[1]:(0.247909963131) A[2]:(0.441446900368) A[3]:(0.759030044079)\n",
      " state (8)  A[0]:(0.771183133125) A[1]:(0.0470242314041) A[2]:(0.811746358871) A[3]:(0.52264714241)\n",
      " state (9)  A[0]:(0.766641557217) A[1]:(0.850483775139) A[2]:(0.723233282566) A[3]:(0.253060102463)\n",
      " state (10)  A[0]:(0.396153002977) A[1]:(0.998880505562) A[2]:(0.17441137135) A[3]:(0.210928276181)\n",
      " state (11)  A[0]:(-0.0969573259354) A[1]:(0.999996244907) A[2]:(-0.0734504312277) A[3]:(0.366055846214)\n",
      " state (12)  A[0]:(-0.170283928514) A[1]:(1.0) A[2]:(0.514592409134) A[3]:(0.6155782938)\n",
      " state (13)  A[0]:(0.144082710147) A[1]:(1.0) A[2]:(0.9374371171) A[3]:(0.8052546978)\n",
      " state (14)  A[0]:(0.587105751038) A[1]:(1.0) A[2]:(0.992644190788) A[3]:(0.898344039917)\n",
      " state (15)  A[0]:(0.855868458748) A[1]:(1.0) A[2]:(0.998339474201) A[3]:(0.938278496265)\n",
      "Episode 82000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained:               10505. Times reached goal: 154.               Steps done: 671148. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.460009786071.\n",
      " state (0)  A[0]:(0.593790650368) A[1]:(0.656360864639) A[2]:(0.384732961655) A[3]:(0.600411653519)\n",
      " state (1)  A[0]:(0.305468916893) A[1]:(0.118876978755) A[2]:(0.429660350084) A[3]:(0.379109919071)\n",
      " state (2)  A[0]:(0.458392798901) A[1]:(0.350826919079) A[2]:(0.367706865072) A[3]:(0.458042293787)\n",
      " state (3)  A[0]:(0.585365176201) A[1]:(0.529981970787) A[2]:(0.306501507759) A[3]:(0.541584134102)\n",
      " state (4)  A[0]:(0.593046545982) A[1]:(0.723923444748) A[2]:(0.0530740320683) A[3]:(0.579773306847)\n",
      " state (5)  A[0]:(0.206554919481) A[1]:(0.91639226675) A[2]:(-0.651972293854) A[3]:(0.481500595808)\n",
      " state (6)  A[0]:(0.00811852514744) A[1]:(0.768837094307) A[2]:(-0.398263901472) A[3]:(0.723530471325)\n",
      " state (7)  A[0]:(0.347076296806) A[1]:(0.262226492167) A[2]:(0.398415833712) A[3]:(0.779709398746)\n",
      " state (8)  A[0]:(0.758595347404) A[1]:(0.0855461061001) A[2]:(0.797168016434) A[3]:(0.540601551533)\n",
      " state (9)  A[0]:(0.76921415329) A[1]:(0.856567263603) A[2]:(0.723779916763) A[3]:(0.251911580563)\n",
      " state (10)  A[0]:(0.383033066988) A[1]:(0.998980820179) A[2]:(0.150545850396) A[3]:(0.224204018712)\n",
      " state (11)  A[0]:(-0.150917217135) A[1]:(0.999996900558) A[2]:(-0.164624497294) A[3]:(0.389536082745)\n",
      " state (12)  A[0]:(-0.226179301739) A[1]:(1.0) A[2]:(0.420116692781) A[3]:(0.628109037876)\n",
      " state (13)  A[0]:(0.12227088958) A[1]:(1.0) A[2]:(0.928221940994) A[3]:(0.80726313591)\n",
      " state (14)  A[0]:(0.607301712036) A[1]:(1.0) A[2]:(0.993057012558) A[3]:(0.897403419018)\n",
      " state (15)  A[0]:(0.877089142799) A[1]:(1.0) A[2]:(0.9987154603) A[3]:(0.936957001686)\n",
      "Episode 83000 finished after 0 timesteps with r=0.0. Running score: 0.19. Times trained:               10628. Times reached goal: 168.               Steps done: 681776. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.455146690331.\n",
      " state (0)  A[0]:(0.589180290699) A[1]:(0.650103092194) A[2]:(0.381638139486) A[3]:(0.59304523468)\n",
      " state (1)  A[0]:(0.303872674704) A[1]:(0.125020697713) A[2]:(0.421982586384) A[3]:(0.369045853615)\n",
      " state (2)  A[0]:(0.447022378445) A[1]:(0.355372160673) A[2]:(0.356768459082) A[3]:(0.445626348257)\n",
      " state (3)  A[0]:(0.572592198849) A[1]:(0.540708720684) A[2]:(0.289966374636) A[3]:(0.529161572456)\n",
      " state (4)  A[0]:(0.588752865791) A[1]:(0.732906997204) A[2]:(0.0507169552147) A[3]:(0.566467165947)\n",
      " state (5)  A[0]:(0.235154166818) A[1]:(0.920580387115) A[2]:(-0.624954581261) A[3]:(0.45554164052)\n",
      " state (6)  A[0]:(0.0245052538812) A[1]:(0.776278555393) A[2]:(-0.383721023798) A[3]:(0.708784222603)\n",
      " state (7)  A[0]:(0.338436663151) A[1]:(0.250912338495) A[2]:(0.400657981634) A[3]:(0.781347036362)\n",
      " state (8)  A[0]:(0.763342261314) A[1]:(0.0865827053785) A[2]:(0.797828316689) A[3]:(0.531269550323)\n",
      " state (9)  A[0]:(0.78567391634) A[1]:(0.857385993004) A[2]:(0.731906354427) A[3]:(0.221966162324)\n",
      " state (10)  A[0]:(0.392393290997) A[1]:(0.999055564404) A[2]:(0.12637616694) A[3]:(0.217043697834)\n",
      " state (11)  A[0]:(-0.187397271395) A[1]:(0.999997437) A[2]:(-0.264558166265) A[3]:(0.401981830597)\n",
      " state (12)  A[0]:(-0.269636213779) A[1]:(1.0) A[2]:(0.296149492264) A[3]:(0.635424256325)\n",
      " state (13)  A[0]:(0.112328380346) A[1]:(1.0) A[2]:(0.914530098438) A[3]:(0.806398808956)\n",
      " state (14)  A[0]:(0.636438012123) A[1]:(1.0) A[2]:(0.993388175964) A[3]:(0.894580841064)\n",
      " state (15)  A[0]:(0.899440407753) A[1]:(1.0) A[2]:(0.999038636684) A[3]:(0.934321403503)\n",
      "Episode 84000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               11042. Times reached goal: 174.               Steps done: 692818. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.450148605784.\n",
      "q_values \n",
      "tensor([[ 0.5874,  0.6437,  0.3657,  0.5821]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5840,  0.7242,  0.0519,  0.5568]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7604,  0.0639,  0.7948,  0.5290]], device='cuda:0')\n",
      "On state=8, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7598,  0.0641,  0.7945,  0.5287]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7961,  0.8580,  0.7363,  0.2058]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3942,  0.9992,  0.1074,  0.2271]], device='cuda:0')\n",
      "On state=10, selected action=3 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0224,  0.7753, -0.3757,  0.6853]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3955,  0.9992,  0.1085,  0.2271]], device='cuda:0')\n",
      "On state=10, selected action=0 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7972,  0.8597,  0.7375,  0.2058]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1092,  1.0000,  0.9063,  0.8106]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.1117,  1.0000,  0.9068,  0.8107]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.586674332619) A[1]:(0.64539706707) A[2]:(0.365309298038) A[3]:(0.582138895988)\n",
      " state (1)  A[0]:(0.303031295538) A[1]:(0.141406878829) A[2]:(0.404462814331) A[3]:(0.352340102196)\n",
      " state (2)  A[0]:(0.428466409445) A[1]:(0.356913715601) A[2]:(0.332642257214) A[3]:(0.422864288092)\n",
      " state (3)  A[0]:(0.553687214851) A[1]:(0.548781633377) A[2]:(0.256265729666) A[3]:(0.509463012218)\n",
      " state (4)  A[0]:(0.584339559078) A[1]:(0.72846186161) A[2]:(0.0528358668089) A[3]:(0.556447803974)\n",
      " state (5)  A[0]:(0.268805354834) A[1]:(0.915312707424) A[2]:(-0.567507863045) A[3]:(0.4416025877)\n",
      " state (6)  A[0]:(0.0251767467707) A[1]:(0.778515994549) A[2]:(-0.3725733459) A[3]:(0.685745358467)\n",
      " state (7)  A[0]:(0.311865985394) A[1]:(0.221490085125) A[2]:(0.394393503666) A[3]:(0.780773818493)\n",
      " state (8)  A[0]:(0.761781513691) A[1]:(0.0783162564039) A[2]:(0.796165823936) A[3]:(0.528690218925)\n",
      " state (9)  A[0]:(0.798655331135) A[1]:(0.861707687378) A[2]:(0.739410400391) A[3]:(0.205973178148)\n",
      " state (10)  A[0]:(0.400546967983) A[1]:(0.999179899693) A[2]:(0.115596227348) A[3]:(0.227356761694)\n",
      " state (11)  A[0]:(-0.221179202199) A[1]:(0.999998033047) A[2]:(-0.32841911912) A[3]:(0.432791054249)\n",
      " state (12)  A[0]:(-0.306450128555) A[1]:(1.0) A[2]:(0.209331199527) A[3]:(0.655648708344)\n",
      " state (13)  A[0]:(0.112412258983) A[1]:(1.0) A[2]:(0.907220184803) A[3]:(0.810800790787)\n",
      " state (14)  A[0]:(0.671276926994) A[1]:(1.0) A[2]:(0.994103908539) A[3]:(0.892363786697)\n",
      " state (15)  A[0]:(0.920202732086) A[1]:(1.0) A[2]:(0.9993019104) A[3]:(0.930468857288)\n",
      "Episode 85000 finished after 0 timesteps with r=0.0. Running score: 0.24. Times trained:               11260. Times reached goal: 179.               Steps done: 704078. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.445108362307.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.594429850578) A[1]:(0.649972677231) A[2]:(0.350570559502) A[3]:(0.589591920376)\n",
      " state (1)  A[0]:(0.31400641799) A[1]:(0.151743724942) A[2]:(0.392653614283) A[3]:(0.3545768857)\n",
      " state (2)  A[0]:(0.422284007072) A[1]:(0.356240987778) A[2]:(0.312676548958) A[3]:(0.419571608305)\n",
      " state (3)  A[0]:(0.546881079674) A[1]:(0.559733986855) A[2]:(0.223230436444) A[3]:(0.509933710098)\n",
      " state (4)  A[0]:(0.59096467495) A[1]:(0.731066346169) A[2]:(0.0472714006901) A[3]:(0.565292596817)\n",
      " state (5)  A[0]:(0.316514164209) A[1]:(0.912054181099) A[2]:(-0.498547703028) A[3]:(0.446085840464)\n",
      " state (6)  A[0]:(0.0379868559539) A[1]:(0.79278922081) A[2]:(-0.348931789398) A[3]:(0.665993750095)\n",
      " state (7)  A[0]:(0.282678812742) A[1]:(0.219810053706) A[2]:(0.389907807112) A[3]:(0.789063215256)\n",
      " state (8)  A[0]:(0.756132185459) A[1]:(0.0938853845) A[2]:(0.79187387228) A[3]:(0.552953124046)\n",
      " state (9)  A[0]:(0.814614951611) A[1]:(0.867797315121) A[2]:(0.749493956566) A[3]:(0.213640764356)\n",
      " state (10)  A[0]:(0.428592592478) A[1]:(0.999280869961) A[2]:(0.12444267422) A[3]:(0.251089841127)\n",
      " state (11)  A[0]:(-0.232270762324) A[1]:(0.999998509884) A[2]:(-0.365792274475) A[3]:(0.472755283117)\n",
      " state (12)  A[0]:(-0.327440023422) A[1]:(1.0) A[2]:(0.149406149983) A[3]:(0.682234883308)\n",
      " state (13)  A[0]:(0.121678955853) A[1]:(1.0) A[2]:(0.904815673828) A[3]:(0.8188803792)\n",
      " state (14)  A[0]:(0.704884588718) A[1]:(1.0) A[2]:(0.994992673397) A[3]:(0.891428172588)\n",
      " state (15)  A[0]:(0.936567902565) A[1]:(1.0) A[2]:(0.999511241913) A[3]:(0.92650359869)\n",
      "Episode 86000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               10969. Times reached goal: 177.               Steps done: 715047. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.440252648529.\n",
      " state (0)  A[0]:(0.597895979881) A[1]:(0.649512648582) A[2]:(0.357932627201) A[3]:(0.585354030132)\n",
      " state (1)  A[0]:(0.331339597702) A[1]:(0.148899525404) A[2]:(0.401728749275) A[3]:(0.348602354527)\n",
      " state (2)  A[0]:(0.427188694477) A[1]:(0.345468580723) A[2]:(0.31344166398) A[3]:(0.41298443079)\n",
      " state (3)  A[0]:(0.546785831451) A[1]:(0.555217444897) A[2]:(0.211066246033) A[3]:(0.506859660149)\n",
      " state (4)  A[0]:(0.595978021622) A[1]:(0.725272417068) A[2]:(0.0480873771012) A[3]:(0.566073298454)\n",
      " state (5)  A[0]:(0.34401461482) A[1]:(0.908561885357) A[2]:(-0.429891556501) A[3]:(0.431533247232)\n",
      " state (6)  A[0]:(0.0371923781931) A[1]:(0.798810422421) A[2]:(-0.295359134674) A[3]:(0.632771492004)\n",
      " state (7)  A[0]:(0.248873725533) A[1]:(0.188421666622) A[2]:(0.419013589621) A[3]:(0.785523116589)\n",
      " state (8)  A[0]:(0.748948812485) A[1]:(0.077213011682) A[2]:(0.800139307976) A[3]:(0.553181648254)\n",
      " state (9)  A[0]:(0.823473215103) A[1]:(0.870190620422) A[2]:(0.763121724129) A[3]:(0.197562098503)\n",
      " state (10)  A[0]:(0.429536998272) A[1]:(0.999352753162) A[2]:(0.127471700311) A[3]:(0.26142591238)\n",
      " state (11)  A[0]:(-0.277503103018) A[1]:(0.999998748302) A[2]:(-0.404934763908) A[3]:(0.506440579891)\n",
      " state (12)  A[0]:(-0.380394279957) A[1]:(1.0) A[2]:(0.0925438851118) A[3]:(0.705623865128)\n",
      " state (13)  A[0]:(0.0913884043694) A[1]:(1.0) A[2]:(0.903791248798) A[3]:(0.824880242348)\n",
      " state (14)  A[0]:(0.714331626892) A[1]:(1.0) A[2]:(0.99581360817) A[3]:(0.888052523136)\n",
      " state (15)  A[0]:(0.944502532482) A[1]:(1.0) A[2]:(0.999662458897) A[3]:(0.919486641884)\n",
      "Episode 87000 finished after 0 timesteps with r=0.0. Running score: 0.15. Times trained:               10773. Times reached goal: 185.               Steps done: 725820. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.43553526257.\n",
      " state (0)  A[0]:(0.596559286118) A[1]:(0.654912889004) A[2]:(0.360532432795) A[3]:(0.586092948914)\n",
      " state (1)  A[0]:(0.336063802242) A[1]:(0.153042852879) A[2]:(0.403920263052) A[3]:(0.347756356001)\n",
      " state (2)  A[0]:(0.429390698671) A[1]:(0.354600727558) A[2]:(0.309088945389) A[3]:(0.416757076979)\n",
      " state (3)  A[0]:(0.544312238693) A[1]:(0.562448263168) A[2]:(0.199564635754) A[3]:(0.511151850224)\n",
      " state (4)  A[0]:(0.597655057907) A[1]:(0.726565122604) A[2]:(0.0483538433909) A[3]:(0.570761680603)\n",
      " state (5)  A[0]:(0.364493399858) A[1]:(0.908761799335) A[2]:(-0.367742359638) A[3]:(0.411615967751)\n",
      " state (6)  A[0]:(0.035939257592) A[1]:(0.810524463654) A[2]:(-0.244216486812) A[3]:(0.588694453239)\n",
      " state (7)  A[0]:(0.214180067182) A[1]:(0.165014907718) A[2]:(0.439735472202) A[3]:(0.779297590256)\n",
      " state (8)  A[0]:(0.740921497345) A[1]:(0.0653445497155) A[2]:(0.803433775902) A[3]:(0.556759417057)\n",
      " state (9)  A[0]:(0.834882676601) A[1]:(0.87236058712) A[2]:(0.773297429085) A[3]:(0.185656175017)\n",
      " state (10)  A[0]:(0.442163109779) A[1]:(0.999400913715) A[2]:(0.129870221019) A[3]:(0.275627464056)\n",
      " state (11)  A[0]:(-0.310870617628) A[1]:(0.999998927116) A[2]:(-0.441574603319) A[3]:(0.543525934219)\n",
      " state (12)  A[0]:(-0.424596816301) A[1]:(1.0) A[2]:(0.0363240092993) A[3]:(0.730695128441)\n",
      " state (13)  A[0]:(0.0659031048417) A[1]:(1.0) A[2]:(0.903577387333) A[3]:(0.831442415714)\n",
      " state (14)  A[0]:(0.725593209267) A[1]:(1.0) A[2]:(0.99660217762) A[3]:(0.883702456951)\n",
      " state (15)  A[0]:(0.951952040195) A[1]:(1.0) A[2]:(0.999779999256) A[3]:(0.91005396843)\n",
      "Episode 88000 finished after 0 timesteps with r=0.0. Running score: 0.17. Times trained:               11103. Times reached goal: 185.               Steps done: 736923. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.430726261124.\n",
      " state (0)  A[0]:(0.595156669617) A[1]:(0.650648415089) A[2]:(0.365173190832) A[3]:(0.588068366051)\n",
      " state (1)  A[0]:(0.338453233242) A[1]:(0.148216724396) A[2]:(0.411116212606) A[3]:(0.347849011421)\n",
      " state (2)  A[0]:(0.426654577255) A[1]:(0.348763585091) A[2]:(0.311581015587) A[3]:(0.419229120016)\n",
      " state (3)  A[0]:(0.537191510201) A[1]:(0.556811332703) A[2]:(0.194770082831) A[3]:(0.514438331127)\n",
      " state (4)  A[0]:(0.589859724045) A[1]:(0.72529566288) A[2]:(0.0413710847497) A[3]:(0.570199966431)\n",
      " state (5)  A[0]:(0.352492570877) A[1]:(0.915672600269) A[2]:(-0.347818076611) A[3]:(0.371721416712)\n",
      " state (6)  A[0]:(0.0165968202055) A[1]:(0.817092120647) A[2]:(-0.209155589342) A[3]:(0.555573225021)\n",
      " state (7)  A[0]:(0.177297919989) A[1]:(0.122507527471) A[2]:(0.450038254261) A[3]:(0.779711186886)\n",
      " state (8)  A[0]:(0.734918415546) A[1]:(0.0394450090826) A[2]:(0.800752997398) A[3]:(0.561069488525)\n",
      " state (9)  A[0]:(0.844705343246) A[1]:(0.873075723648) A[2]:(0.770289003849) A[3]:(0.173709884286)\n",
      " state (10)  A[0]:(0.449377596378) A[1]:(0.999431729317) A[2]:(0.0884922966361) A[3]:(0.291729032993)\n",
      " state (11)  A[0]:(-0.344470828772) A[1]:(0.999998986721) A[2]:(-0.513759016991) A[3]:(0.580091238022)\n",
      " state (12)  A[0]:(-0.462909370661) A[1]:(1.0) A[2]:(-0.0740307271481) A[3]:(0.754240870476)\n",
      " state (13)  A[0]:(0.048514932394) A[1]:(1.0) A[2]:(0.891887962818) A[3]:(0.837186455727)\n",
      " state (14)  A[0]:(0.740876793861) A[1]:(1.0) A[2]:(0.996904432774) A[3]:(0.877958714962)\n",
      " state (15)  A[0]:(0.959225594997) A[1]:(1.0) A[2]:(0.999841213226) A[3]:(0.89790558815)\n",
      "Episode 89000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained:               10737. Times reached goal: 191.               Steps done: 747660. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.426126292382.\n",
      "q_values \n",
      "tensor([[ 0.5938,  0.6497,  0.3704,  0.5881]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5913,  0.7243,  0.0354,  0.5660]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7401,  0.0238,  0.8036,  0.5614]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8610,  0.8749,  0.7808,  0.1549]], device='cuda:0')\n",
      "On state=9, selected action=3 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.593361735344) A[1]:(0.648637890816) A[2]:(0.370225667953) A[3]:(0.588344454765)\n",
      " state (1)  A[0]:(0.346498429775) A[1]:(0.13629578054) A[2]:(0.412019222975) A[3]:(0.352112352848)\n",
      " state (2)  A[0]:(0.433316648006) A[1]:(0.337192803621) A[2]:(0.310920953751) A[3]:(0.426028490067)\n",
      " state (3)  A[0]:(0.539987266064) A[1]:(0.545356631279) A[2]:(0.191829323769) A[3]:(0.51985925436)\n",
      " state (4)  A[0]:(0.59101909399) A[1]:(0.724454522133) A[2]:(0.0347612462938) A[3]:(0.566720843315)\n",
      " state (5)  A[0]:(0.350224167109) A[1]:(0.928112626076) A[2]:(-0.347572743893) A[3]:(0.311140179634)\n",
      " state (6)  A[0]:(0.0219947677106) A[1]:(0.834663569927) A[2]:(-0.191215634346) A[3]:(0.515952467918)\n",
      " state (7)  A[0]:(0.163959950209) A[1]:(0.105101287365) A[2]:(0.456497877836) A[3]:(0.77894282341)\n",
      " state (8)  A[0]:(0.738913178444) A[1]:(0.0270967297256) A[2]:(0.802696466446) A[3]:(0.563239216805)\n",
      " state (9)  A[0]:(0.8605017066) A[1]:(0.875523209572) A[2]:(0.780052423477) A[3]:(0.156230852008)\n",
      " state (10)  A[0]:(0.480003267527) A[1]:(0.999465942383) A[2]:(0.100232973695) A[3]:(0.30098927021)\n",
      " state (11)  A[0]:(-0.349578768015) A[1]:(0.99999910593) A[2]:(-0.525950908661) A[3]:(0.610298752785)\n",
      " state (12)  A[0]:(-0.47777569294) A[1]:(1.0) A[2]:(-0.087746873498) A[3]:(0.774084210396)\n",
      " state (13)  A[0]:(0.0538989193738) A[1]:(1.0) A[2]:(0.900477528572) A[3]:(0.841543793678)\n",
      " state (14)  A[0]:(0.762245714664) A[1]:(1.0) A[2]:(0.997656524181) A[3]:(0.870557665825)\n",
      " state (15)  A[0]:(0.966080009937) A[1]:(1.0) A[2]:(0.999900341034) A[3]:(0.882352948189)\n",
      "Episode 90000 finished after 0 timesteps with r=0.0. Running score: 0.18. Times trained:               10443. Times reached goal: 156.               Steps done: 758103. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.421699410705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.600414931774) A[1]:(0.65513241291) A[2]:(0.367557406425) A[3]:(0.593698501587)\n",
      " state (1)  A[0]:(0.353211224079) A[1]:(0.143741950393) A[2]:(0.410561084747) A[3]:(0.352147489786)\n",
      " state (2)  A[0]:(0.439334928989) A[1]:(0.345338046551) A[2]:(0.308473199606) A[3]:(0.429721742868)\n",
      " state (3)  A[0]:(0.542993485928) A[1]:(0.548895001411) A[2]:(0.18827611208) A[3]:(0.523969054222)\n",
      " state (4)  A[0]:(0.594866633415) A[1]:(0.72767329216) A[2]:(0.0346920676529) A[3]:(0.569636642933)\n",
      " state (5)  A[0]:(0.357732415199) A[1]:(0.936040639877) A[2]:(-0.325520992279) A[3]:(0.284343034029)\n",
      " state (6)  A[0]:(0.0203141923994) A[1]:(0.848792612553) A[2]:(-0.159508183599) A[3]:(0.500320196152)\n",
      " state (7)  A[0]:(0.139072537422) A[1]:(0.0956975072622) A[2]:(0.467844873667) A[3]:(0.788323879242)\n",
      " state (8)  A[0]:(0.740165114403) A[1]:(0.0346463471651) A[2]:(0.805236279964) A[3]:(0.575813531876)\n",
      " state (9)  A[0]:(0.874714434147) A[1]:(0.882595300674) A[2]:(0.788484036922) A[3]:(0.148938804865)\n",
      " state (10)  A[0]:(0.509171247482) A[1]:(0.999511182308) A[2]:(0.098254352808) A[3]:(0.321059018373)\n",
      " state (11)  A[0]:(-0.357575148344) A[1]:(0.999999165535) A[2]:(-0.559576749802) A[3]:(0.647459864616)\n",
      " state (12)  A[0]:(-0.49727460742) A[1]:(1.0) A[2]:(-0.149959117174) A[3]:(0.797984004021)\n",
      " state (13)  A[0]:(0.0514713637531) A[1]:(1.0) A[2]:(0.897472143173) A[3]:(0.849189341068)\n",
      " state (14)  A[0]:(0.779593765736) A[1]:(1.0) A[2]:(0.9980224967) A[3]:(0.864692032337)\n",
      " state (15)  A[0]:(0.97160410881) A[1]:(1.0) A[2]:(0.99993211031) A[3]:(0.86552554369)\n",
      "Episode 91000 finished after 0 timesteps with r=1.0. Running score: 0.26. Times trained:               11208. Times reached goal: 192.               Steps done: 769311. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.416999391817.\n",
      " state (0)  A[0]:(0.588190674782) A[1]:(0.656220436096) A[2]:(0.369463860989) A[3]:(0.586330294609)\n",
      " state (1)  A[0]:(0.349752277136) A[1]:(0.150760918856) A[2]:(0.415093123913) A[3]:(0.347276002169)\n",
      " state (2)  A[0]:(0.435103863478) A[1]:(0.346902370453) A[2]:(0.31220754981) A[3]:(0.428650408983)\n",
      " state (3)  A[0]:(0.535695612431) A[1]:(0.54203748703) A[2]:(0.190845444798) A[3]:(0.523292541504)\n",
      " state (4)  A[0]:(0.588083267212) A[1]:(0.719869375229) A[2]:(0.0398302264512) A[3]:(0.568440616131)\n",
      " state (5)  A[0]:(0.352508336306) A[1]:(0.939104616642) A[2]:(-0.300298750401) A[3]:(0.25722360611)\n",
      " state (6)  A[0]:(-0.000928729481529) A[1]:(0.853246808052) A[2]:(-0.127493754029) A[3]:(0.481009066105)\n",
      " state (7)  A[0]:(0.0903951749206) A[1]:(0.0583461485803) A[2]:(0.481153815985) A[3]:(0.792129695415)\n",
      " state (8)  A[0]:(0.727342903614) A[1]:(0.0124367615208) A[2]:(0.809971094131) A[3]:(0.576216578484)\n",
      " state (9)  A[0]:(0.876526117325) A[1]:(0.883857309818) A[2]:(0.795545578003) A[3]:(0.134643286467)\n",
      " state (10)  A[0]:(0.494951695204) A[1]:(0.999530375004) A[2]:(0.0856173485518) A[3]:(0.342584311962)\n",
      " state (11)  A[0]:(-0.405043333769) A[1]:(0.99999922514) A[2]:(-0.593787670135) A[3]:(0.684031963348)\n",
      " state (12)  A[0]:(-0.539315462112) A[1]:(1.0) A[2]:(-0.204407975078) A[3]:(0.821205973625)\n",
      " state (13)  A[0]:(0.0245033912361) A[1]:(1.0) A[2]:(0.895977854729) A[3]:(0.858170688152)\n",
      " state (14)  A[0]:(0.786917388439) A[1]:(1.0) A[2]:(0.998326838017) A[3]:(0.860639035702)\n",
      " state (15)  A[0]:(0.97494494915) A[1]:(1.0) A[2]:(0.99995225668) A[3]:(0.849096536636)\n",
      "Episode 92000 finished after 0 timesteps with r=0.0. Running score: 0.21. Times trained:               10776. Times reached goal: 191.               Steps done: 780087. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.412529931075.\n",
      " state (0)  A[0]:(0.597837924957) A[1]:(0.650088787079) A[2]:(0.370956778526) A[3]:(0.595607817173)\n",
      " state (1)  A[0]:(0.358964234591) A[1]:(0.141890451312) A[2]:(0.418149590492) A[3]:(0.357330232859)\n",
      " state (2)  A[0]:(0.444855332375) A[1]:(0.341531962156) A[2]:(0.31272226572) A[3]:(0.444081395864)\n",
      " state (3)  A[0]:(0.541758537292) A[1]:(0.533637046814) A[2]:(0.18849979341) A[3]:(0.537925839424)\n",
      " state (4)  A[0]:(0.590903639793) A[1]:(0.724740624428) A[2]:(0.0285117812455) A[3]:(0.572805047035)\n",
      " state (5)  A[0]:(0.353519022465) A[1]:(0.952440440655) A[2]:(-0.322061181068) A[3]:(0.204524412751)\n",
      " state (6)  A[0]:(0.0122985299677) A[1]:(0.869484722614) A[2]:(-0.132544800639) A[3]:(0.477484375238)\n",
      " state (7)  A[0]:(0.0808183178306) A[1]:(0.0552358515561) A[2]:(0.472864508629) A[3]:(0.804075181484)\n",
      " state (8)  A[0]:(0.735934495926) A[1]:(0.0083831967786) A[2]:(0.810880661011) A[3]:(0.584679365158)\n",
      " state (9)  A[0]:(0.889653861523) A[1]:(0.884902477264) A[2]:(0.804104089737) A[3]:(0.126610696316)\n",
      " state (10)  A[0]:(0.525838494301) A[1]:(0.999537289143) A[2]:(0.0908382758498) A[3]:(0.367561638355)\n",
      " state (11)  A[0]:(-0.400260567665) A[1]:(0.99999922514) A[2]:(-0.611143112183) A[3]:(0.719663619995)\n",
      " state (12)  A[0]:(-0.543425440788) A[1]:(1.0) A[2]:(-0.233905687928) A[3]:(0.843812346458)\n",
      " state (13)  A[0]:(0.0376353636384) A[1]:(1.0) A[2]:(0.899132609367) A[3]:(0.868278324604)\n",
      " state (14)  A[0]:(0.80450540781) A[1]:(1.0) A[2]:(0.998624145985) A[3]:(0.857789874077)\n",
      " state (15)  A[0]:(0.978694617748) A[1]:(1.0) A[2]:(0.999966204166) A[3]:(0.831761538982)\n",
      "Episode 93000 finished after 0 timesteps with r=0.0. Running score: 0.13. Times trained:               10834. Times reached goal: 189.               Steps done: 790921. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.408084705071.\n",
      " state (0)  A[0]:(0.594960987568) A[1]:(0.649679064751) A[2]:(0.380373805761) A[3]:(0.592495977879)\n",
      " state (1)  A[0]:(0.359962791204) A[1]:(0.142448976636) A[2]:(0.419819056988) A[3]:(0.349762052298)\n",
      " state (2)  A[0]:(0.447147578001) A[1]:(0.338433176279) A[2]:(0.314141631126) A[3]:(0.442515730858)\n",
      " state (3)  A[0]:(0.541806817055) A[1]:(0.522442221642) A[2]:(0.189878702164) A[3]:(0.537843108177)\n",
      " state (4)  A[0]:(0.590724110603) A[1]:(0.720527410507) A[2]:(0.02764724195) A[3]:(0.568123340607)\n",
      " state (5)  A[0]:(0.361814975739) A[1]:(0.958873391151) A[2]:(-0.320407539606) A[3]:(0.158857524395)\n",
      " state (6)  A[0]:(0.016254870221) A[1]:(0.875544548035) A[2]:(-0.122391223907) A[3]:(0.470533430576)\n",
      " state (7)  A[0]:(0.0436210818589) A[1]:(0.0318511240184) A[2]:(0.466702669859) A[3]:(0.81282132864)\n",
      " state (8)  A[0]:(0.720315217972) A[1]:(0.00428971974179) A[2]:(0.807260274887) A[3]:(0.591388463974)\n",
      " state (9)  A[0]:(0.888540327549) A[1]:(0.889118790627) A[2]:(0.804838120937) A[3]:(0.117026604712)\n",
      " state (10)  A[0]:(0.509580731392) A[1]:(0.999562442303) A[2]:(0.0696833282709) A[3]:(0.392394095659)\n",
      " state (11)  A[0]:(-0.433683484793) A[1]:(0.99999922514) A[2]:(-0.637925624847) A[3]:(0.751939713955)\n",
      " state (12)  A[0]:(-0.564690113068) A[1]:(1.0) A[2]:(-0.261760622263) A[3]:(0.862863600254)\n",
      " state (13)  A[0]:(0.0381082445383) A[1]:(1.0) A[2]:(0.904878854752) A[3]:(0.875939369202)\n",
      " state (14)  A[0]:(0.817803144455) A[1]:(1.0) A[2]:(0.998910486698) A[3]:(0.85189718008)\n",
      " state (15)  A[0]:(0.981510281563) A[1]:(1.0) A[2]:(0.999976754189) A[3]:(0.808195233345)\n",
      "Episode 94000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               11770. Times reached goal: 183.               Steps done: 802691. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.403309704098.\n",
      "q_values \n",
      "tensor([[ 0.5810,  0.6456,  0.3827,  0.5822]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5810,  0.6452,  0.3826,  0.5822]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5805,  0.7247,  0.0269,  0.5598]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5802,  0.7240,  0.0268,  0.5598]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7076,  0.0046,  0.8074,  0.5929]], device='cuda:0')\n",
      "On state=8, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7069,  0.0039,  0.8071,  0.5932]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8866,  0.8921,  0.8072,  0.1020]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0140,  1.0000,  0.8994,  0.8868]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0167,  1.0000,  0.9001,  0.8868]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0215,  1.0000,  0.9009,  0.8868]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.580742001534) A[1]:(0.644203424454) A[2]:(0.382431328297) A[3]:(0.582807064056)\n",
      " state (1)  A[0]:(0.349785387516) A[1]:(0.145235106349) A[2]:(0.422379881144) A[3]:(0.341649711132)\n",
      " state (2)  A[0]:(0.43794003129) A[1]:(0.337315648794) A[2]:(0.318094849586) A[3]:(0.439775317907)\n",
      " state (3)  A[0]:(0.531655907631) A[1]:(0.515104055405) A[2]:(0.194490462542) A[3]:(0.537078976631)\n",
      " state (4)  A[0]:(0.580496311188) A[1]:(0.724345207214) A[2]:(0.0273781530559) A[3]:(0.560385346413)\n",
      " state (5)  A[0]:(0.359505176544) A[1]:(0.966751933098) A[2]:(-0.327217400074) A[3]:(0.106350429356)\n",
      " state (6)  A[0]:(0.0151189966127) A[1]:(0.883159935474) A[2]:(-0.115802340209) A[3]:(0.474297046661)\n",
      " state (7)  A[0]:(0.00942011643201) A[1]:(0.0202925354242) A[2]:(0.463773429394) A[3]:(0.823396801949)\n",
      " state (8)  A[0]:(0.708868384361) A[1]:(0.010329535231) A[2]:(0.807695865631) A[3]:(0.593899488449)\n",
      " state (9)  A[0]:(0.887767255306) A[1]:(0.893979370594) A[2]:(0.807824909687) A[3]:(0.103491134942)\n",
      " state (10)  A[0]:(0.496203571558) A[1]:(0.999584972858) A[2]:(0.048351701349) A[3]:(0.4156729877)\n",
      " state (11)  A[0]:(-0.46087872982) A[1]:(0.99999922514) A[2]:(-0.669889450073) A[3]:(0.781372189522)\n",
      " state (12)  A[0]:(-0.58596342802) A[1]:(1.0) A[2]:(-0.31665545702) A[3]:(0.881454706192)\n",
      " state (13)  A[0]:(0.0266163647175) A[1]:(1.0) A[2]:(0.90170788765) A[3]:(0.886735975742)\n",
      " state (14)  A[0]:(0.823439836502) A[1]:(1.0) A[2]:(0.999023795128) A[3]:(0.852737903595)\n",
      " state (15)  A[0]:(0.983009278774) A[1]:(1.0) A[2]:(0.999981582165) A[3]:(0.794078826904)\n",
      "Episode 95000 finished after 0 timesteps with r=0.0. Running score: 0.18. Times trained:               11842. Times reached goal: 192.               Steps done: 814533. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.398561877944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.588307261467) A[1]:(0.650953710079) A[2]:(0.367942899466) A[3]:(0.587812483311)\n",
      " state (1)  A[0]:(0.33731251955) A[1]:(0.164015606046) A[2]:(0.408852249384) A[3]:(0.320940226316)\n",
      " state (2)  A[0]:(0.428476452827) A[1]:(0.362329751253) A[2]:(0.308142513037) A[3]:(0.424655497074)\n",
      " state (3)  A[0]:(0.523578107357) A[1]:(0.53684592247) A[2]:(0.187384024262) A[3]:(0.52620267868)\n",
      " state (4)  A[0]:(0.580594360828) A[1]:(0.728568851948) A[2]:(0.0294542927295) A[3]:(0.563312649727)\n",
      " state (5)  A[0]:(0.393581837416) A[1]:(0.9659537673) A[2]:(-0.304738491774) A[3]:(0.139588743448)\n",
      " state (6)  A[0]:(0.0271589998156) A[1]:(0.888928115368) A[2]:(-0.108188398182) A[3]:(0.486603349447)\n",
      " state (7)  A[0]:(-0.0179401170462) A[1]:(0.0142629006878) A[2]:(0.462098449469) A[3]:(0.834838688374)\n",
      " state (8)  A[0]:(0.694355487823) A[1]:(0.0199519135058) A[2]:(0.808651447296) A[3]:(0.602328181267)\n",
      " state (9)  A[0]:(0.885208070278) A[1]:(0.897220373154) A[2]:(0.813740730286) A[3]:(0.0953354239464)\n",
      " state (10)  A[0]:(0.486183822155) A[1]:(0.999594449997) A[2]:(0.04697201401) A[3]:(0.430935144424)\n",
      " state (11)  A[0]:(-0.47956353426) A[1]:(0.999999284744) A[2]:(-0.687613010406) A[3]:(0.800820887089)\n",
      " state (12)  A[0]:(-0.603142738342) A[1]:(1.0) A[2]:(-0.35120883584) A[3]:(0.893351256847)\n",
      " state (13)  A[0]:(0.0130712399259) A[1]:(1.0) A[2]:(0.90187817812) A[3]:(0.892589569092)\n",
      " state (14)  A[0]:(0.825910866261) A[1]:(1.0) A[2]:(0.999142467976) A[3]:(0.848696947098)\n",
      " state (15)  A[0]:(0.983857452869) A[1]:(1.0) A[2]:(0.999985277653) A[3]:(0.77323102951)\n",
      "Episode 96000 finished after 0 timesteps with r=0.0. Running score: 0.21. Times trained:               11843. Times reached goal: 186.               Steps done: 826376. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.393869550087.\n",
      " state (0)  A[0]:(0.584465444088) A[1]:(0.653735399246) A[2]:(0.373332828283) A[3]:(0.587483763695)\n",
      " state (1)  A[0]:(0.334341406822) A[1]:(0.154331043363) A[2]:(0.413965016603) A[3]:(0.317951440811)\n",
      " state (2)  A[0]:(0.424045562744) A[1]:(0.354408681393) A[2]:(0.319558858871) A[3]:(0.421656966209)\n",
      " state (3)  A[0]:(0.517402052879) A[1]:(0.527767658234) A[2]:(0.201101824641) A[3]:(0.523148238659)\n",
      " state (4)  A[0]:(0.576226234436) A[1]:(0.722783923149) A[2]:(0.041352160275) A[3]:(0.559104800224)\n",
      " state (5)  A[0]:(0.408583581448) A[1]:(0.96840775013) A[2]:(-0.297118663788) A[3]:(0.110847644508)\n",
      " state (6)  A[0]:(0.0416802242398) A[1]:(0.893740773201) A[2]:(-0.10697581619) A[3]:(0.478320449591)\n",
      " state (7)  A[0]:(-0.0373120456934) A[1]:(-0.000115811824799) A[2]:(0.454168707132) A[3]:(0.840830683708)\n",
      " state (8)  A[0]:(0.678970813751) A[1]:(0.0168767049909) A[2]:(0.806031346321) A[3]:(0.610657811165)\n",
      " state (9)  A[0]:(0.88058000803) A[1]:(0.898818671703) A[2]:(0.814452290535) A[3]:(0.099817045033)\n",
      " state (10)  A[0]:(0.472180098295) A[1]:(0.999599814415) A[2]:(0.0301767736673) A[3]:(0.45589441061)\n",
      " state (11)  A[0]:(-0.492956161499) A[1]:(0.999999284744) A[2]:(-0.708529233932) A[3]:(0.821021735668)\n",
      " state (12)  A[0]:(-0.609942317009) A[1]:(1.0) A[2]:(-0.383368492126) A[3]:(0.904622256756)\n",
      " state (13)  A[0]:(0.0182110201567) A[1]:(1.0) A[2]:(0.902660608292) A[3]:(0.898356437683)\n",
      " state (14)  A[0]:(0.83301115036) A[1]:(1.0) A[2]:(0.999229431152) A[3]:(0.845029473305)\n",
      " state (15)  A[0]:(0.984890580177) A[1]:(1.0) A[2]:(0.999987304211) A[3]:(0.75244396925)\n",
      "Episode 97000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained:               12021. Times reached goal: 175.               Steps done: 838397. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.389163188485.\n",
      " state (0)  A[0]:(0.593093037605) A[1]:(0.650989472866) A[2]:(0.366941154003) A[3]:(0.590825438499)\n",
      " state (1)  A[0]:(0.351615488529) A[1]:(0.154239460826) A[2]:(0.41051915288) A[3]:(0.323535650969)\n",
      " state (2)  A[0]:(0.439516097307) A[1]:(0.364294558764) A[2]:(0.321373701096) A[3]:(0.427917778492)\n",
      " state (3)  A[0]:(0.528512060642) A[1]:(0.533909261227) A[2]:(0.198954820633) A[3]:(0.529295563698)\n",
      " state (4)  A[0]:(0.586027026176) A[1]:(0.72391295433) A[2]:(0.0306634120643) A[3]:(0.565398931503)\n",
      " state (5)  A[0]:(0.435269981623) A[1]:(0.970274984837) A[2]:(-0.304623633623) A[3]:(0.102978311479)\n",
      " state (6)  A[0]:(0.0650709718466) A[1]:(0.89141523838) A[2]:(-0.110717751086) A[3]:(0.493467628956)\n",
      " state (7)  A[0]:(-0.0265108197927) A[1]:(-0.0322278551757) A[2]:(0.446132063866) A[3]:(0.849310398102)\n",
      " state (8)  A[0]:(0.693941652775) A[1]:(0.00131291081198) A[2]:(0.80866008997) A[3]:(0.606374144554)\n",
      " state (9)  A[0]:(0.889040231705) A[1]:(0.894429683685) A[2]:(0.822741985321) A[3]:(0.0817564874887)\n",
      " state (10)  A[0]:(0.505990505219) A[1]:(0.999565720558) A[2]:(0.0433268919587) A[3]:(0.460137426853)\n",
      " state (11)  A[0]:(-0.467874914408) A[1]:(0.99999922514) A[2]:(-0.717967510223) A[3]:(0.831925570965)\n",
      " state (12)  A[0]:(-0.601257324219) A[1]:(1.0) A[2]:(-0.412745296955) A[3]:(0.912263512611)\n",
      " state (13)  A[0]:(0.0259821638465) A[1]:(1.0) A[2]:(0.900906980038) A[3]:(0.903042733669)\n",
      " state (14)  A[0]:(0.836195230484) A[1]:(1.0) A[2]:(0.999269783497) A[3]:(0.842778742313)\n",
      " state (15)  A[0]:(0.98523324728) A[1]:(1.0) A[2]:(0.99998819828) A[3]:(0.736036658287)\n",
      "Episode 98000 finished after 0 timesteps with r=1.0. Running score: 0.16. Times trained:               11792. Times reached goal: 207.               Steps done: 850189. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.384601126945.\n",
      " state (0)  A[0]:(0.583142995834) A[1]:(0.653710126877) A[2]:(0.356549888849) A[3]:(0.587526381016)\n",
      " state (1)  A[0]:(0.33535438776) A[1]:(0.160475552082) A[2]:(0.401785194874) A[3]:(0.319203585386)\n",
      " state (2)  A[0]:(0.426479667425) A[1]:(0.389782756567) A[2]:(0.327866882086) A[3]:(0.425164163113)\n",
      " state (3)  A[0]:(0.515324115753) A[1]:(0.550840377808) A[2]:(0.203135922551) A[3]:(0.527632951736)\n",
      " state (4)  A[0]:(0.575828552246) A[1]:(0.726534307003) A[2]:(0.0281210895628) A[3]:(0.568084001541)\n",
      " state (5)  A[0]:(0.441798448563) A[1]:(0.971076667309) A[2]:(-0.299013853073) A[3]:(0.101241908967)\n",
      " state (6)  A[0]:(0.0674798712134) A[1]:(0.894092023373) A[2]:(-0.111565336585) A[3]:(0.499627739191)\n",
      " state (7)  A[0]:(-0.0384631156921) A[1]:(-0.0381257422268) A[2]:(0.436891674995) A[3]:(0.857635200024)\n",
      " state (8)  A[0]:(0.69005471468) A[1]:(0.0103738158941) A[2]:(0.809272170067) A[3]:(0.614279270172)\n",
      " state (9)  A[0]:(0.888722598553) A[1]:(0.896178603172) A[2]:(0.828313291073) A[3]:(0.0873933807015)\n",
      " state (10)  A[0]:(0.51145529747) A[1]:(0.999562501907) A[2]:(0.047966953367) A[3]:(0.480184912682)\n",
      " state (11)  A[0]:(-0.463428765535) A[1]:(0.999999284744) A[2]:(-0.729949712753) A[3]:(0.846741437912)\n",
      " state (12)  A[0]:(-0.601750135422) A[1]:(1.0) A[2]:(-0.443117946386) A[3]:(0.921343326569)\n",
      " state (13)  A[0]:(0.0289373360574) A[1]:(1.0) A[2]:(0.899248540401) A[3]:(0.9099573493)\n",
      " state (14)  A[0]:(0.839975178242) A[1]:(1.0) A[2]:(0.999315321445) A[3]:(0.845318675041)\n",
      " state (15)  A[0]:(0.985777795315) A[1]:(1.0) A[2]:(0.999989330769) A[3]:(0.728250622749)\n",
      "Episode 99000 finished after 0 timesteps with r=0.0. Running score: 0.19. Times trained:               12574. Times reached goal: 179.               Steps done: 862763. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.379795429114.\n",
      "q_values \n",
      "tensor([[ 0.5923,  0.6614,  0.3666,  0.5902]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3535,  0.1501,  0.4094,  0.3190]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4394,  0.4066,  0.3560,  0.4259]], device='cuda:0')\n",
      "On state=2, selected action=0 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3533,  0.1508,  0.4087,  0.3193]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4395,  0.4081,  0.3550,  0.4264]], device='cuda:0')\n",
      "On state=2, selected action=3 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.4392,  0.4085,  0.3545,  0.4266]], device='cuda:0')\n",
      "On state=2, selected action=0 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.3528,  0.1521,  0.4075,  0.3198]], device='cuda:0')\n",
      "On state=1, selected action=1 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.5916441679) A[1]:(0.661684393883) A[2]:(0.365011245012) A[3]:(0.59071624279)\n",
      " state (1)  A[0]:(0.352576345205) A[1]:(0.151355996728) A[2]:(0.40716663003) A[3]:(0.319881886244)\n",
      " state (2)  A[0]:(0.438574105501) A[1]:(0.407695800066) A[2]:(0.353706300259) A[3]:(0.426816940308)\n",
      " state (3)  A[0]:(0.521840333939) A[1]:(0.559656500816) A[2]:(0.221270382404) A[3]:(0.530768573284)\n",
      " state (4)  A[0]:(0.580067932606) A[1]:(0.730918645859) A[2]:(0.0287279691547) A[3]:(0.568368196487)\n",
      " state (5)  A[0]:(0.456264793873) A[1]:(0.974808990955) A[2]:(-0.297300159931) A[3]:(0.0607370175421)\n",
      " state (6)  A[0]:(0.0841539427638) A[1]:(0.895578801632) A[2]:(-0.103871688247) A[3]:(0.504633009434)\n",
      " state (7)  A[0]:(-0.0360210500658) A[1]:(-0.0530399158597) A[2]:(0.432085156441) A[3]:(0.865139067173)\n",
      " state (8)  A[0]:(0.689801990986) A[1]:(0.00997200328857) A[2]:(0.809885859489) A[3]:(0.615860462189)\n",
      " state (9)  A[0]:(0.886915624142) A[1]:(0.895572781563) A[2]:(0.830924272537) A[3]:(0.0782711729407)\n",
      " state (10)  A[0]:(0.505684912205) A[1]:(0.999543011189) A[2]:(0.0385714061558) A[3]:(0.486307889223)\n",
      " state (11)  A[0]:(-0.473212480545) A[1]:(0.99999922514) A[2]:(-0.747294485569) A[3]:(0.854975104332)\n",
      " state (12)  A[0]:(-0.615414023399) A[1]:(1.0) A[2]:(-0.480796098709) A[3]:(0.926704764366)\n",
      " state (13)  A[0]:(0.00588663574308) A[1]:(1.0) A[2]:(0.894823670387) A[3]:(0.91362786293)\n",
      " state (14)  A[0]:(0.833715438843) A[1]:(1.0) A[2]:(0.999323606491) A[3]:(0.844538331032)\n",
      " state (15)  A[0]:(0.985119760036) A[1]:(1.0) A[2]:(0.999989449978) A[3]:(0.717422842979)\n",
      "Episode 100000 finished after 0 timesteps with r=0.0. Running score: 0.32. Times trained:               12443. Times reached goal: 207.               Steps done: 875206. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.375098914551.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.583859026432) A[1]:(0.648202240467) A[2]:(0.352131158113) A[3]:(0.580727159977)\n",
      " state (1)  A[0]:(0.341851323843) A[1]:(0.117167107761) A[2]:(0.391101151705) A[3]:(0.304753661156)\n",
      " state (2)  A[0]:(0.420737475157) A[1]:(0.421030879021) A[2]:(0.380370169878) A[3]:(0.413061261177)\n",
      " state (3)  A[0]:(0.504525661469) A[1]:(0.56476598978) A[2]:(0.244007185102) A[3]:(0.522736430168)\n",
      " state (4)  A[0]:(0.568556547165) A[1]:(0.723448038101) A[2]:(0.0335542298853) A[3]:(0.566432833672)\n",
      " state (5)  A[0]:(0.463764369488) A[1]:(0.975046098232) A[2]:(-0.287330776453) A[3]:(0.0373304001987)\n",
      " state (6)  A[0]:(0.0963690504432) A[1]:(0.893931865692) A[2]:(-0.0970702022314) A[3]:(0.498807013035)\n",
      " state (7)  A[0]:(-0.0310792345554) A[1]:(-0.080633148551) A[2]:(0.428334116936) A[3]:(0.869045078754)\n",
      " state (8)  A[0]:(0.693099439144) A[1]:(-0.00388477277011) A[2]:(0.811832427979) A[3]:(0.616925358772)\n",
      " state (9)  A[0]:(0.887497127056) A[1]:(0.892490983009) A[2]:(0.835795462132) A[3]:(0.0773866921663)\n",
      " state (10)  A[0]:(0.512325108051) A[1]:(0.999505400658) A[2]:(0.0440480411053) A[3]:(0.496595859528)\n",
      " state (11)  A[0]:(-0.462900966406) A[1]:(0.999999165535) A[2]:(-0.750947237015) A[3]:(0.862463533878)\n",
      " state (12)  A[0]:(-0.606813192368) A[1]:(1.0) A[2]:(-0.480995237827) A[3]:(0.930991053581)\n",
      " state (13)  A[0]:(0.0237797368318) A[1]:(1.0) A[2]:(0.90155684948) A[3]:(0.916527748108)\n",
      " state (14)  A[0]:(0.839898586273) A[1]:(1.0) A[2]:(0.999400258064) A[3]:(0.844239413738)\n",
      " state (15)  A[0]:(0.985608279705) A[1]:(1.0) A[2]:(0.999990522861) A[3]:(0.710074186325)\n",
      "Episode 101000 finished after 0 timesteps with r=0.0. Running score: 0.16. Times trained:               12058. Times reached goal: 207.               Steps done: 887264. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.370603131388.\n",
      " state (0)  A[0]:(0.595073461533) A[1]:(0.653020501137) A[2]:(0.368039011955) A[3]:(0.592404961586)\n",
      " state (1)  A[0]:(0.378320634365) A[1]:(0.114471308887) A[2]:(0.414887577295) A[3]:(0.329989463091)\n",
      " state (2)  A[0]:(0.433868587017) A[1]:(0.450666546822) A[2]:(0.439129471779) A[3]:(0.432514846325)\n",
      " state (3)  A[0]:(0.515795230865) A[1]:(0.568866729736) A[2]:(0.278836101294) A[3]:(0.543820619583)\n",
      " state (4)  A[0]:(0.581857323647) A[1]:(0.722882151604) A[2]:(0.026082796976) A[3]:(0.578305959702)\n",
      " state (5)  A[0]:(0.477926552296) A[1]:(0.979850292206) A[2]:(-0.292521595955) A[3]:(-0.0315686129034)\n",
      " state (6)  A[0]:(0.104320801795) A[1]:(0.893693447113) A[2]:(-0.0870421677828) A[3]:(0.499355375767)\n",
      " state (7)  A[0]:(-0.0409456305206) A[1]:(-0.0862331911922) A[2]:(0.418594390154) A[3]:(0.874034762383)\n",
      " state (8)  A[0]:(0.682504415512) A[1]:(0.022467110306) A[2]:(0.804553329945) A[3]:(0.624636650085)\n",
      " state (9)  A[0]:(0.884019851685) A[1]:(0.898706316948) A[2]:(0.833148062229) A[3]:(0.0713318288326)\n",
      " state (10)  A[0]:(0.504752874374) A[1]:(0.999505221844) A[2]:(0.0212563443929) A[3]:(0.48756852746)\n",
      " state (11)  A[0]:(-0.471979856491) A[1]:(0.99999910593) A[2]:(-0.770408391953) A[3]:(0.859755277634)\n",
      " state (12)  A[0]:(-0.616946876049) A[1]:(1.0) A[2]:(-0.518919110298) A[3]:(0.929368853569)\n",
      " state (13)  A[0]:(0.00605799304321) A[1]:(1.0) A[2]:(0.896271288395) A[3]:(0.91326135397)\n",
      " state (14)  A[0]:(0.834442079067) A[1]:(1.0) A[2]:(0.999398887157) A[3]:(0.8353074193)\n",
      " state (15)  A[0]:(0.984988808632) A[1]:(1.0) A[2]:(0.999990642071) A[3]:(0.690587759018)\n",
      "Episode 102000 finished after 0 timesteps with r=0.0. Running score: 0.24. Times trained:               12179. Times reached goal: 231.               Steps done: 899443. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.366116930026.\n",
      " state (0)  A[0]:(0.588279604912) A[1]:(0.650550365448) A[2]:(0.404850929976) A[3]:(0.586052775383)\n",
      " state (1)  A[0]:(0.423341393471) A[1]:(0.0880390405655) A[2]:(0.454353243113) A[3]:(0.365899562836)\n",
      " state (2)  A[0]:(0.448297649622) A[1]:(0.440964341164) A[2]:(0.505559742451) A[3]:(0.457742959261)\n",
      " state (3)  A[0]:(0.532808423042) A[1]:(0.52088779211) A[2]:(0.319014191628) A[3]:(0.566681742668)\n",
      " state (4)  A[0]:(0.597592234612) A[1]:(0.726505279541) A[2]:(0.0174230579287) A[3]:(0.552484154701)\n",
      " state (5)  A[0]:(0.480031520128) A[1]:(0.987519741058) A[2]:(-0.302900195122) A[3]:(-0.193984776735)\n",
      " state (6)  A[0]:(0.108616106212) A[1]:(0.887293577194) A[2]:(-0.0809984430671) A[3]:(0.509907841682)\n",
      " state (7)  A[0]:(-0.0398323088884) A[1]:(-0.125189736485) A[2]:(0.411186367273) A[3]:(0.878649711609)\n",
      " state (8)  A[0]:(0.695268034935) A[1]:(0.00686932727695) A[2]:(0.809449076653) A[3]:(0.617043375969)\n",
      " state (9)  A[0]:(0.89017945528) A[1]:(0.895393371582) A[2]:(0.841964960098) A[3]:(0.0519439391792)\n",
      " state (10)  A[0]:(0.523326516151) A[1]:(0.999443769455) A[2]:(0.0325047858059) A[3]:(0.478437542915)\n",
      " state (11)  A[0]:(-0.456788778305) A[1]:(0.999998867512) A[2]:(-0.775074362755) A[3]:(0.858285307884)\n",
      " state (12)  A[0]:(-0.609269380569) A[1]:(1.0) A[2]:(-0.529465556145) A[3]:(0.929078102112)\n",
      " state (13)  A[0]:(0.0185337588191) A[1]:(1.0) A[2]:(0.89900803566) A[3]:(0.912576854229)\n",
      " state (14)  A[0]:(0.83998554945) A[1]:(1.0) A[2]:(0.999458909035) A[3]:(0.832523941994)\n",
      " state (15)  A[0]:(0.985725998878) A[1]:(1.0) A[2]:(0.999991953373) A[3]:(0.683186411858)\n",
      "Episode 103000 finished after 0 timesteps with r=1.0. Running score: 0.23. Times trained:               12554. Times reached goal: 190.               Steps done: 911997. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.361549428283.\n",
      " state (0)  A[0]:(0.588922023773) A[1]:(0.6532548666) A[2]:(0.457239151001) A[3]:(0.59040158987)\n",
      " state (1)  A[0]:(0.481598645449) A[1]:(0.0819766372442) A[2]:(0.513287484646) A[3]:(0.413807749748)\n",
      " state (2)  A[0]:(0.477179139853) A[1]:(0.412569880486) A[2]:(0.573214173317) A[3]:(0.492929011583)\n",
      " state (3)  A[0]:(0.560232758522) A[1]:(0.428229272366) A[2]:(0.367054373026) A[3]:(0.598814964294)\n",
      " state (4)  A[0]:(0.615672230721) A[1]:(0.719274938107) A[2]:(0.0228143669665) A[3]:(0.542614817619)\n",
      " state (5)  A[0]:(0.485960036516) A[1]:(0.990774691105) A[2]:(-0.318488895893) A[3]:(-0.173009604216)\n",
      " state (6)  A[0]:(0.0994373038411) A[1]:(0.88336545229) A[2]:(-0.115626037121) A[3]:(0.593054234982)\n",
      " state (7)  A[0]:(-0.0649498924613) A[1]:(-0.138319447637) A[2]:(0.368211984634) A[3]:(0.895639717579)\n",
      " state (8)  A[0]:(0.693480193615) A[1]:(0.0073171146214) A[2]:(0.804383575916) A[3]:(0.634732723236)\n",
      " state (9)  A[0]:(0.889014661312) A[1]:(0.895109415054) A[2]:(0.842189788818) A[3]:(0.0711040496826)\n",
      " state (10)  A[0]:(0.51592540741) A[1]:(0.999398171902) A[2]:(0.0224761050195) A[3]:(0.496892243624)\n",
      " state (11)  A[0]:(-0.461890876293) A[1]:(0.999998688698) A[2]:(-0.78075748682) A[3]:(0.864221632481)\n",
      " state (12)  A[0]:(-0.611415922642) A[1]:(1.0) A[2]:(-0.530018389225) A[3]:(0.93187892437)\n",
      " state (13)  A[0]:(0.0212408546358) A[1]:(1.0) A[2]:(0.905086159706) A[3]:(0.915550231934)\n",
      " state (14)  A[0]:(0.843429148197) A[1]:(1.0) A[2]:(0.999523460865) A[3]:(0.837334692478)\n",
      " state (15)  A[0]:(0.986319482327) A[1]:(1.0) A[2]:(0.999993026257) A[3]:(0.691345334053)\n",
      "Episode 104000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               12597. Times reached goal: 199.               Steps done: 924594. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.357023556189.\n",
      "q_values \n",
      "tensor([[ 0.5932,  0.6518,  0.4831,  0.5921]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6244,  0.7243,  0.0117,  0.5179]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6247,  0.7247,  0.0118,  0.5180]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7041,  0.0055,  0.8127,  0.6305]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8905,  0.8942,  0.8474,  0.0554]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0106,  1.0000,  0.8979,  0.9148]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.593534350395) A[1]:(0.651445388794) A[2]:(0.48285356164) A[3]:(0.592523217201)\n",
      " state (1)  A[0]:(0.528670191765) A[1]:(0.106968745589) A[2]:(0.535213828087) A[3]:(0.426940232515)\n",
      " state (2)  A[0]:(0.499542474747) A[1]:(0.398617655039) A[2]:(0.607513070107) A[3]:(0.485160917044)\n",
      " state (3)  A[0]:(0.582602500916) A[1]:(0.3114990592) A[2]:(0.387278527021) A[3]:(0.593478798866)\n",
      " state (4)  A[0]:(0.624648809433) A[1]:(0.723058700562) A[2]:(0.0109475050122) A[3]:(0.518228292465)\n",
      " state (5)  A[0]:(0.480133712292) A[1]:(0.992449104786) A[2]:(-0.337301760912) A[3]:(-0.00139573868364)\n",
      " state (6)  A[0]:(0.0771002620459) A[1]:(0.879102349281) A[2]:(-0.158574223518) A[3]:(0.704409241676)\n",
      " state (7)  A[0]:(-0.0892250537872) A[1]:(-0.153345525265) A[2]:(0.334482580423) A[3]:(0.912405371666)\n",
      " state (8)  A[0]:(0.704161942005) A[1]:(0.00192212825641) A[2]:(0.812673151493) A[3]:(0.630011498928)\n",
      " state (9)  A[0]:(0.890632390976) A[1]:(0.89377117157) A[2]:(0.847481489182) A[3]:(0.0548118837178)\n",
      " state (10)  A[0]:(0.522555589676) A[1]:(0.999364733696) A[2]:(0.0280873794109) A[3]:(0.484889507294)\n",
      " state (11)  A[0]:(-0.452412515879) A[1]:(0.999998629093) A[2]:(-0.785764455795) A[3]:(0.858661651611)\n",
      " state (12)  A[0]:(-0.612483918667) A[1]:(1.0) A[2]:(-0.555723905563) A[3]:(0.929860532284)\n",
      " state (13)  A[0]:(0.0121625326574) A[1]:(1.0) A[2]:(0.898290932178) A[3]:(0.914702296257)\n",
      " state (14)  A[0]:(0.84265422821) A[1]:(1.0) A[2]:(0.999506354332) A[3]:(0.838295400143)\n",
      " state (15)  A[0]:(0.986643731594) A[1]:(1.0) A[2]:(0.999993026257) A[3]:(0.697168588638)\n",
      "Episode 105000 finished after 0 timesteps with r=0.0. Running score: 0.12. Times trained:               12814. Times reached goal: 185.               Steps done: 937408. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.352477842924.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.583298683167) A[1]:(0.652479588985) A[2]:(0.522564530373) A[3]:(0.586648225784)\n",
      " state (1)  A[0]:(0.57822483778) A[1]:(0.109016790986) A[2]:(0.542176961899) A[3]:(0.461890399456)\n",
      " state (2)  A[0]:(0.516962230206) A[1]:(0.419797837734) A[2]:(0.618807792664) A[3]:(0.473988801241)\n",
      " state (3)  A[0]:(0.597016572952) A[1]:(0.209849029779) A[2]:(0.391527622938) A[3]:(0.576438724995)\n",
      " state (4)  A[0]:(0.631334900856) A[1]:(0.720776796341) A[2]:(0.0133075006306) A[3]:(0.528937280178)\n",
      " state (5)  A[0]:(0.491274356842) A[1]:(0.993114709854) A[2]:(-0.309836119413) A[3]:(0.269436269999)\n",
      " state (6)  A[0]:(0.0776072144508) A[1]:(0.881423711777) A[2]:(-0.182799339294) A[3]:(0.797469496727)\n",
      " state (7)  A[0]:(-0.118300653994) A[1]:(-0.154543012381) A[2]:(0.291799902916) A[3]:(0.92780148983)\n",
      " state (8)  A[0]:(0.693757891655) A[1]:(0.00445553706959) A[2]:(0.808800339699) A[3]:(0.640762090683)\n",
      " state (9)  A[0]:(0.882284998894) A[1]:(0.895146548748) A[2]:(0.841265797615) A[3]:(0.0663911476731)\n",
      " state (10)  A[0]:(0.504786133766) A[1]:(0.999366521835) A[2]:(0.0103894313797) A[3]:(0.489415645599)\n",
      " state (11)  A[0]:(-0.460744142532) A[1]:(0.999998748302) A[2]:(-0.790650904179) A[3]:(0.85785150528)\n",
      " state (12)  A[0]:(-0.61999976635) A[1]:(1.0) A[2]:(-0.558245360851) A[3]:(0.92972868681)\n",
      " state (13)  A[0]:(0.00646063638851) A[1]:(1.0) A[2]:(0.9022783041) A[3]:(0.915150284767)\n",
      " state (14)  A[0]:(0.844533383846) A[1]:(1.0) A[2]:(0.999527573586) A[3]:(0.840520143509)\n",
      " state (15)  A[0]:(0.987079918385) A[1]:(1.0) A[2]:(0.999992847443) A[3]:(0.70473253727)\n",
      "Episode 106000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained:               13494. Times reached goal: 218.               Steps done: 950902. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.347753454051.\n",
      " state (0)  A[0]:(0.588296294212) A[1]:(0.649814844131) A[2]:(0.551976203918) A[3]:(0.589566230774)\n",
      " state (1)  A[0]:(0.611672759056) A[1]:(0.0797678604722) A[2]:(0.53035980463) A[3]:(0.521392583847)\n",
      " state (2)  A[0]:(0.526682496071) A[1]:(0.485189110041) A[2]:(0.608495950699) A[3]:(0.482449024916)\n",
      " state (3)  A[0]:(0.601963639259) A[1]:(0.165703296661) A[2]:(0.383875280619) A[3]:(0.569576382637)\n",
      " state (4)  A[0]:(0.6316614151) A[1]:(0.72176861763) A[2]:(0.00688911508769) A[3]:(0.546721339226)\n",
      " state (5)  A[0]:(0.497007787228) A[1]:(0.993409872055) A[2]:(-0.320070773363) A[3]:(0.430098325014)\n",
      " state (6)  A[0]:(0.0709391534328) A[1]:(0.876268029213) A[2]:(-0.212254688144) A[3]:(0.841287791729)\n",
      " state (7)  A[0]:(-0.133880704641) A[1]:(-0.17553396523) A[2]:(0.25992795825) A[3]:(0.933818936348)\n",
      " state (8)  A[0]:(0.69179058075) A[1]:(-0.000749111059122) A[2]:(0.809195041656) A[3]:(0.635367691517)\n",
      " state (9)  A[0]:(0.879800081253) A[1]:(0.89178186655) A[2]:(0.842929244041) A[3]:(0.0597343929112)\n",
      " state (10)  A[0]:(0.509558558464) A[1]:(0.999288499355) A[2]:(0.0225452128798) A[3]:(0.478956580162)\n",
      " state (11)  A[0]:(-0.451185643673) A[1]:(0.999998569489) A[2]:(-0.789538741112) A[3]:(0.852078199387)\n",
      " state (12)  A[0]:(-0.621687531471) A[1]:(1.0) A[2]:(-0.568864822388) A[3]:(0.927546679974)\n",
      " state (13)  A[0]:(-0.00434684986249) A[1]:(1.0) A[2]:(0.899120151997) A[3]:(0.913863956928)\n",
      " state (14)  A[0]:(0.841984450817) A[1]:(1.0) A[2]:(0.999512672424) A[3]:(0.840091884136)\n",
      " state (15)  A[0]:(0.986970961094) A[1]:(1.0) A[2]:(0.99999243021) A[3]:(0.706906080246)\n",
      "Episode 107000 finished after 0 timesteps with r=0.0. Running score: 0.26. Times trained:               12833. Times reached goal: 202.               Steps done: 963735. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.34331924692.\n",
      " state (0)  A[0]:(0.583196878433) A[1]:(0.654694914818) A[2]:(0.550303816795) A[3]:(0.585610985756)\n",
      " state (1)  A[0]:(0.601119816303) A[1]:(0.0534594133496) A[2]:(0.51961761713) A[3]:(0.544826745987)\n",
      " state (2)  A[0]:(0.50726467371) A[1]:(0.541743040085) A[2]:(0.589708030224) A[3]:(0.472964823246)\n",
      " state (3)  A[0]:(0.588572919369) A[1]:(0.101643249393) A[2]:(0.387457311153) A[3]:(0.551185131073)\n",
      " state (4)  A[0]:(0.629707098007) A[1]:(0.726804852486) A[2]:(0.0226041916758) A[3]:(0.55303800106)\n",
      " state (5)  A[0]:(0.531246185303) A[1]:(0.993792116642) A[2]:(-0.324559628963) A[3]:(0.536242485046)\n",
      " state (6)  A[0]:(0.108300350606) A[1]:(0.873366951942) A[2]:(-0.220919981599) A[3]:(0.866655528545)\n",
      " state (7)  A[0]:(-0.118492096663) A[1]:(-0.193113058805) A[2]:(0.247021347284) A[3]:(0.937953770161)\n",
      " state (8)  A[0]:(0.695748865604) A[1]:(0.00707995984703) A[2]:(0.808751821518) A[3]:(0.640280008316)\n",
      " state (9)  A[0]:(0.881158351898) A[1]:(0.895159900188) A[2]:(0.844025373459) A[3]:(0.0620886385441)\n",
      " state (10)  A[0]:(0.524344682693) A[1]:(0.999273240566) A[2]:(0.0300221797079) A[3]:(0.47120565176)\n",
      " state (11)  A[0]:(-0.429194748402) A[1]:(0.999998509884) A[2]:(-0.788087248802) A[3]:(0.84575176239)\n",
      " state (12)  A[0]:(-0.607111096382) A[1]:(1.0) A[2]:(-0.566645503044) A[3]:(0.92416459322)\n",
      " state (13)  A[0]:(0.0198820549995) A[1]:(1.0) A[2]:(0.903396070004) A[3]:(0.910825371742)\n",
      " state (14)  A[0]:(0.850877642632) A[1]:(1.0) A[2]:(0.999554872513) A[3]:(0.837059557438)\n",
      " state (15)  A[0]:(0.987973630428) A[1]:(1.0) A[2]:(0.99999320507) A[3]:(0.70628619194)\n",
      "Episode 108000 finished after 0 timesteps with r=1.0. Running score: 0.29. Times trained:               13187. Times reached goal: 210.               Steps done: 976922. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.338821616315.\n",
      " state (0)  A[0]:(0.584415435791) A[1]:(0.65572309494) A[2]:(0.537777781487) A[3]:(0.588215529919)\n",
      " state (1)  A[0]:(0.596211791039) A[1]:(0.0523624308407) A[2]:(0.533419370651) A[3]:(0.558337807655)\n",
      " state (2)  A[0]:(0.494379758835) A[1]:(0.630374073982) A[2]:(0.568140149117) A[3]:(0.465681344271)\n",
      " state (3)  A[0]:(0.583678603172) A[1]:(0.0789770334959) A[2]:(0.384577721357) A[3]:(0.537417888641)\n",
      " state (4)  A[0]:(0.63579750061) A[1]:(0.729454755783) A[2]:(0.0157137103379) A[3]:(0.556215405464)\n",
      " state (5)  A[0]:(0.560205340385) A[1]:(0.994327127934) A[2]:(-0.365126699209) A[3]:(0.586576163769)\n",
      " state (6)  A[0]:(0.132639229298) A[1]:(0.872417867184) A[2]:(-0.254286944866) A[3]:(0.874623954296)\n",
      " state (7)  A[0]:(-0.122346572578) A[1]:(-0.219190180302) A[2]:(0.228157997131) A[3]:(0.936517715454)\n",
      " state (8)  A[0]:(0.687510073185) A[1]:(0.000585079134908) A[2]:(0.809208095074) A[3]:(0.629386425018)\n",
      " state (9)  A[0]:(0.875239491463) A[1]:(0.895545125008) A[2]:(0.8449036479) A[3]:(0.0588112063706)\n",
      " state (10)  A[0]:(0.511442184448) A[1]:(0.999237239361) A[2]:(0.0309563875198) A[3]:(0.466868311167)\n",
      " state (11)  A[0]:(-0.437426894903) A[1]:(0.999998390675) A[2]:(-0.790759503841) A[3]:(0.841574668884)\n",
      " state (12)  A[0]:(-0.613211035728) A[1]:(1.0) A[2]:(-0.57478672266) A[3]:(0.921817958355)\n",
      " state (13)  A[0]:(0.0154013074934) A[1]:(1.0) A[2]:(0.904540538788) A[3]:(0.908675253391)\n",
      " state (14)  A[0]:(0.853081822395) A[1]:(1.0) A[2]:(0.999588608742) A[3]:(0.834735512733)\n",
      " state (15)  A[0]:(0.988514363766) A[1]:(1.0) A[2]:(0.999994039536) A[3]:(0.705576777458)\n",
      "Episode 109000 finished after 0 timesteps with r=0.0. Running score: 0.19. Times trained:               12975. Times reached goal: 206.               Steps done: 989897. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.334453803307.\n",
      "q_values \n",
      "tensor([[ 0.5891,  0.6556,  0.5389,  0.5914]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6518,  0.7277,  0.0188,  0.5685]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6831, -0.0036,  0.8098,  0.6254]], device='cuda:0')\n",
      "On state=8, selected action=3 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6513,  0.7277,  0.0172,  0.5690]], device='cuda:0')\n",
      "On state=4, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5888,  0.6557,  0.5383,  0.5916]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5888,  0.6561,  0.5381,  0.5916]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6516,  0.7303,  0.0153,  0.5698]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6826,  0.0058,  0.8076,  0.6283]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8750,  0.9000,  0.8432,  0.0726]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0139,  1.0000,  0.9028,  0.9094]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0191,  1.0000,  0.9035,  0.9093]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0215,  1.0000,  0.9037,  0.9093]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8572,  1.0000,  0.9996,  0.8385]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8575,  1.0000,  0.9996,  0.8386]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8577,  1.0000,  0.9996,  0.8388]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8590,  1.0000,  0.9996,  0.8389]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8596,  1.0000,  0.9996,  0.8389]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? True\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.589314341545) A[1]:(0.656162261963) A[2]:(0.539386153221) A[3]:(0.591277360916)\n",
      " state (1)  A[0]:(0.601546764374) A[1]:(0.0193391758949) A[2]:(0.60568779707) A[3]:(0.598810553551)\n",
      " state (2)  A[0]:(0.524860918522) A[1]:(0.700889587402) A[2]:(0.588513374329) A[3]:(0.514382362366)\n",
      " state (3)  A[0]:(0.620677113533) A[1]:(0.0294243972749) A[2]:(0.423668563366) A[3]:(0.571412205696)\n",
      " state (4)  A[0]:(0.654858767986) A[1]:(0.728398442268) A[2]:(0.0179861467332) A[3]:(0.571369051933)\n",
      " state (5)  A[0]:(0.552738904953) A[1]:(0.994762718678) A[2]:(-0.432215541601) A[3]:(0.590070366859)\n",
      " state (6)  A[0]:(0.137439653277) A[1]:(0.873565852642) A[2]:(-0.296607643366) A[3]:(0.869601666927)\n",
      " state (7)  A[0]:(-0.117147244513) A[1]:(-0.232554018497) A[2]:(0.221188023686) A[3]:(0.933201849461)\n",
      " state (8)  A[0]:(0.68890786171) A[1]:(0.00347219500691) A[2]:(0.808981895447) A[3]:(0.63080072403)\n",
      " state (9)  A[0]:(0.878474295139) A[1]:(0.898827672005) A[2]:(0.845123529434) A[3]:(0.0750519037247)\n",
      " state (10)  A[0]:(0.530349969864) A[1]:(0.999242603779) A[2]:(0.0265413168818) A[3]:(0.473551005125)\n",
      " state (11)  A[0]:(-0.417111814022) A[1]:(0.999998450279) A[2]:(-0.798315286636) A[3]:(0.841384589672)\n",
      " state (12)  A[0]:(-0.602810263634) A[1]:(1.0) A[2]:(-0.593796491623) A[3]:(0.921784758568)\n",
      " state (13)  A[0]:(0.0321101397276) A[1]:(1.0) A[2]:(0.90288490057) A[3]:(0.90968298912)\n",
      " state (14)  A[0]:(0.859956681728) A[1]:(1.0) A[2]:(0.999604523182) A[3]:(0.839025616646)\n",
      " state (15)  A[0]:(0.989353716373) A[1]:(1.0) A[2]:(0.999994397163) A[3]:(0.717875599861)\n",
      "Episode 110000 finished after 0 timesteps with r=1.0. Running score: 0.27. Times trained:               13599. Times reached goal: 243.               Steps done: 1003496. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.329936352064.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.595442295074) A[1]:(0.65973263979) A[2]:(0.606308460236) A[3]:(0.593898415565)\n",
      " state (1)  A[0]:(0.606136918068) A[1]:(0.00921008735895) A[2]:(0.675893366337) A[3]:(0.639067292213)\n",
      " state (2)  A[0]:(0.572421431541) A[1]:(0.765737771988) A[2]:(0.61293566227) A[3]:(0.571828007698)\n",
      " state (3)  A[0]:(0.667270481586) A[1]:(0.0127172768116) A[2]:(0.456813961267) A[3]:(0.609335780144)\n",
      " state (4)  A[0]:(0.669374704361) A[1]:(0.733496308327) A[2]:(0.0236238390207) A[3]:(0.570122122765)\n",
      " state (5)  A[0]:(0.50576364994) A[1]:(0.995674371719) A[2]:(-0.464240789413) A[3]:(0.542743444443)\n",
      " state (6)  A[0]:(0.0903025344014) A[1]:(0.875144600868) A[2]:(-0.269573867321) A[3]:(0.850373625755)\n",
      " state (7)  A[0]:(-0.119273811579) A[1]:(-0.234346359968) A[2]:(0.273749470711) A[3]:(0.924187481403)\n",
      " state (8)  A[0]:(0.698609888554) A[1]:(0.0043135615997) A[2]:(0.814029812813) A[3]:(0.621925830841)\n",
      " state (9)  A[0]:(0.891453385353) A[1]:(0.898350775242) A[2]:(0.84918987751) A[3]:(0.0613927282393)\n",
      " state (10)  A[0]:(0.575510382652) A[1]:(0.999232172966) A[2]:(0.0345164462924) A[3]:(0.451173752546)\n",
      " state (11)  A[0]:(-0.38628745079) A[1]:(0.999998509884) A[2]:(-0.803612887859) A[3]:(0.830903112888)\n",
      " state (12)  A[0]:(-0.600222885609) A[1]:(1.0) A[2]:(-0.610064148903) A[3]:(0.917014718056)\n",
      " state (13)  A[0]:(0.0197194572538) A[1]:(1.0) A[2]:(0.901258587837) A[3]:(0.905818223953)\n",
      " state (14)  A[0]:(0.855437517166) A[1]:(1.0) A[2]:(0.99960744381) A[3]:(0.836070537567)\n",
      " state (15)  A[0]:(0.988987386227) A[1]:(1.0) A[2]:(0.999994337559) A[3]:(0.720145881176)\n",
      "Episode 111000 finished after 0 timesteps with r=0.0. Running score: 0.28. Times trained:               13161. Times reached goal: 241.               Steps done: 1016657. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.325622509244.\n",
      " state (0)  A[0]:(0.591877877712) A[1]:(0.659572601318) A[2]:(0.634215533733) A[3]:(0.592409968376)\n",
      " state (1)  A[0]:(0.598246693611) A[1]:(0.00359164131805) A[2]:(0.706601142883) A[3]:(0.663994073868)\n",
      " state (2)  A[0]:(0.59829723835) A[1]:(0.78787624836) A[2]:(0.621848702431) A[3]:(0.61648619175)\n",
      " state (3)  A[0]:(0.690492987633) A[1]:(-0.0030251648277) A[2]:(0.456577003002) A[3]:(0.644916057587)\n",
      " state (4)  A[0]:(0.67014759779) A[1]:(0.730608224869) A[2]:(0.0137706631795) A[3]:(0.578873276711)\n",
      " state (5)  A[0]:(0.441459923983) A[1]:(0.996840059757) A[2]:(-0.470394790173) A[3]:(0.498145580292)\n",
      " state (6)  A[0]:(0.0280035212636) A[1]:(0.881291210651) A[2]:(-0.205235406756) A[3]:(0.832479476929)\n",
      " state (7)  A[0]:(-0.118070855737) A[1]:(-0.226366683841) A[2]:(0.33513957262) A[3]:(0.915099680424)\n",
      " state (8)  A[0]:(0.705199658871) A[1]:(-0.00121557654347) A[2]:(0.816836714745) A[3]:(0.624970436096)\n",
      " state (9)  A[0]:(0.901026308537) A[1]:(0.896305441856) A[2]:(0.853402018547) A[3]:(0.068654499948)\n",
      " state (10)  A[0]:(0.596505582333) A[1]:(0.999242007732) A[2]:(0.0327178239822) A[3]:(0.454003155231)\n",
      " state (11)  A[0]:(-0.390005975962) A[1]:(0.999998569489) A[2]:(-0.813193261623) A[3]:(0.832888424397)\n",
      " state (12)  A[0]:(-0.611824989319) A[1]:(1.0) A[2]:(-0.628702759743) A[3]:(0.91830432415)\n",
      " state (13)  A[0]:(0.0112494844943) A[1]:(1.0) A[2]:(0.900021493435) A[3]:(0.907625317574)\n",
      " state (14)  A[0]:(0.85831618309) A[1]:(1.0) A[2]:(0.999624669552) A[3]:(0.840386152267)\n",
      " state (15)  A[0]:(0.989600300789) A[1]:(1.0) A[2]:(0.999994695187) A[3]:(0.730624437332)\n",
      "Episode 112000 finished after 0 timesteps with r=0.0. Running score: 0.19. Times trained:               13646. Times reached goal: 236.               Steps done: 1030303. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.321209244671.\n",
      " state (0)  A[0]:(0.597719073296) A[1]:(0.657039642334) A[2]:(0.643045783043) A[3]:(0.596868038177)\n",
      " state (1)  A[0]:(0.598610758781) A[1]:(0.00176083855331) A[2]:(0.71831792593) A[3]:(0.687748193741)\n",
      " state (2)  A[0]:(0.610235214233) A[1]:(0.798327028751) A[2]:(0.61400604248) A[3]:(0.661671876907)\n",
      " state (3)  A[0]:(0.697802662849) A[1]:(-0.00266420212574) A[2]:(0.425999999046) A[3]:(0.683929085732)\n",
      " state (4)  A[0]:(0.667356193066) A[1]:(0.730488061905) A[2]:(0.00912092626095) A[3]:(0.58957028389)\n",
      " state (5)  A[0]:(0.40863147378) A[1]:(0.997545421124) A[2]:(-0.412252545357) A[3]:(0.435341864824)\n",
      " state (6)  A[0]:(0.02160776034) A[1]:(0.889924585819) A[2]:(-0.124698631465) A[3]:(0.804505228996)\n",
      " state (7)  A[0]:(-0.0943746864796) A[1]:(-0.211309969425) A[2]:(0.368478238583) A[3]:(0.903301358223)\n",
      " state (8)  A[0]:(0.689199328423) A[1]:(0.00731681659818) A[2]:(0.809028148651) A[3]:(0.623504757881)\n",
      " state (9)  A[0]:(0.894693017006) A[1]:(0.898911356926) A[2]:(0.852012872696) A[3]:(0.0626500174403)\n",
      " state (10)  A[0]:(0.583204090595) A[1]:(0.999267578125) A[2]:(0.0294881183654) A[3]:(0.435978978872)\n",
      " state (11)  A[0]:(-0.405318975449) A[1]:(0.999998629093) A[2]:(-0.817946493626) A[3]:(0.823933124542)\n",
      " state (12)  A[0]:(-0.619059205055) A[1]:(1.0) A[2]:(-0.632907092571) A[3]:(0.913652420044)\n",
      " state (13)  A[0]:(0.0154538964853) A[1]:(1.0) A[2]:(0.90300399065) A[3]:(0.903069257736)\n",
      " state (14)  A[0]:(0.863499879837) A[1]:(1.0) A[2]:(0.999637901783) A[3]:(0.83543753624)\n",
      " state (15)  A[0]:(0.990213811398) A[1]:(1.0) A[2]:(0.999994635582) A[3]:(0.728712022305)\n",
      "Episode 113000 finished after 0 timesteps with r=0.0. Running score: 0.17. Times trained:               13743. Times reached goal: 239.               Steps done: 1044046. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.316825060943.\n",
      " state (0)  A[0]:(0.596036791801) A[1]:(0.66148352623) A[2]:(0.648841977119) A[3]:(0.593319296837)\n",
      " state (1)  A[0]:(0.598627328873) A[1]:(0.000392556161387) A[2]:(0.722021102905) A[3]:(0.691151976585)\n",
      " state (2)  A[0]:(0.621993124485) A[1]:(0.802304029465) A[2]:(0.613963007927) A[3]:(0.681702256203)\n",
      " state (3)  A[0]:(0.706042587757) A[1]:(-0.000500559748616) A[2]:(0.39908900857) A[3]:(0.703733980656)\n",
      " state (4)  A[0]:(0.666651844978) A[1]:(0.733718276024) A[2]:(0.00631097517908) A[3]:(0.592491149902)\n",
      " state (5)  A[0]:(0.383752048016) A[1]:(0.997948408127) A[2]:(-0.34813657403) A[3]:(0.383800923824)\n",
      " state (6)  A[0]:(0.0272167455405) A[1]:(0.896286189556) A[2]:(-0.0812235698104) A[3]:(0.786480665207)\n",
      " state (7)  A[0]:(-0.0610376857221) A[1]:(-0.222551286221) A[2]:(0.37677565217) A[3]:(0.898397564888)\n",
      " state (8)  A[0]:(0.694312632084) A[1]:(0.00205170828849) A[2]:(0.811121225357) A[3]:(0.623106777668)\n",
      " state (9)  A[0]:(0.88978600502) A[1]:(0.899850130081) A[2]:(0.852023899555) A[3]:(0.0865153670311)\n",
      " state (10)  A[0]:(0.565063774586) A[1]:(0.999268829823) A[2]:(0.00848451722413) A[3]:(0.457108318806)\n",
      " state (11)  A[0]:(-0.424502193928) A[1]:(0.999998688698) A[2]:(-0.830247402191) A[3]:(0.829563617706)\n",
      " state (12)  A[0]:(-0.63095843792) A[1]:(1.0) A[2]:(-0.651993989944) A[3]:(0.915762841702)\n",
      " state (13)  A[0]:(0.00639605754986) A[1]:(1.0) A[2]:(0.901848077774) A[3]:(0.905379652977)\n",
      " state (14)  A[0]:(0.863858699799) A[1]:(1.0) A[2]:(0.999634981155) A[3]:(0.840934753418)\n",
      " state (15)  A[0]:(0.990366518497) A[1]:(1.0) A[2]:(0.999994218349) A[3]:(0.742168664932)\n",
      "Episode 114000 finished after 0 timesteps with r=0.0. Running score: 0.2. Times trained:               14042. Times reached goal: 243.               Steps done: 1058088. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.312407293175.\n",
      "q_values \n",
      "tensor([[ 0.5937,  0.6580,  0.6537,  0.5916]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5937,  0.6577,  0.6537,  0.5917]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6636,  0.7298,  0.0004,  0.5898]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6971,  0.0035,  0.8085,  0.6203]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8897,  0.8991,  0.8530,  0.0861]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0060,  1.0000,  0.8997,  0.9032]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0073,  1.0000,  0.9004,  0.9031]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0117,  1.0000,  0.9014,  0.9030]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0131,  1.0000,  0.9019,  0.9030]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0125,  1.0000,  0.9022,  0.9030]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0103,  1.0000,  0.9023,  0.9030]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0081,  1.0000,  0.9022,  0.9031]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8622,  1.0000,  0.9996,  0.8405]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8619,  1.0000,  0.9996,  0.8406]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8621,  1.0000,  0.9996,  0.8406]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8625,  1.0000,  0.9996,  0.8404]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? True\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.593306064606) A[1]:(0.658076107502) A[2]:(0.656018674374) A[3]:(0.592011868954)\n",
      " state (1)  A[0]:(0.596727669239) A[1]:(-0.000614643038716) A[2]:(0.729429721832) A[3]:(0.686192035675)\n",
      " state (2)  A[0]:(0.628882527351) A[1]:(0.807545959949) A[2]:(0.619629502296) A[3]:(0.686611175537)\n",
      " state (3)  A[0]:(0.710380315781) A[1]:(0.00216745981015) A[2]:(0.380773931742) A[3]:(0.70950037241)\n",
      " state (4)  A[0]:(0.663107633591) A[1]:(0.731668114662) A[2]:(0.00698292814195) A[3]:(0.58986967802)\n",
      " state (5)  A[0]:(0.354288518429) A[1]:(0.998128890991) A[2]:(-0.289345353842) A[3]:(0.350342661142)\n",
      " state (6)  A[0]:(0.0125159043819) A[1]:(0.896587133408) A[2]:(-0.0574051141739) A[3]:(0.779851198196)\n",
      " state (7)  A[0]:(-0.0536312647164) A[1]:(-0.247804671526) A[2]:(0.365902721882) A[3]:(0.898585796356)\n",
      " state (8)  A[0]:(0.697692275047) A[1]:(1.59740447998e-05) A[2]:(0.811249613762) A[3]:(0.620344281197)\n",
      " state (9)  A[0]:(0.890231847763) A[1]:(0.898279249668) A[2]:(0.855338215828) A[3]:(0.0849668458104)\n",
      " state (10)  A[0]:(0.589129686356) A[1]:(0.999184191227) A[2]:(0.030686635524) A[3]:(0.446023672819)\n",
      " state (11)  A[0]:(-0.385826766491) A[1]:(0.999998569489) A[2]:(-0.827307462692) A[3]:(0.821246504784)\n",
      " state (12)  A[0]:(-0.618477344513) A[1]:(1.0) A[2]:(-0.657393276691) A[3]:(0.912191390991)\n",
      " state (13)  A[0]:(0.0077615310438) A[1]:(1.0) A[2]:(0.901215732098) A[3]:(0.903032839298)\n",
      " state (14)  A[0]:(0.862483978271) A[1]:(1.0) A[2]:(0.999623537064) A[3]:(0.84036886692)\n",
      " state (15)  A[0]:(0.990228176117) A[1]:(1.0) A[2]:(0.999993562698) A[3]:(0.747373461723)\n",
      "Episode 115000 finished after 0 timesteps with r=1.0. Running score: 0.35. Times trained:               13924. Times reached goal: 260.               Steps done: 1072012. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.308087478369.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.589377760887) A[1]:(0.656042814255) A[2]:(0.657961726189) A[3]:(0.589171767235)\n",
      " state (1)  A[0]:(0.592823803425) A[1]:(-0.00109887076542) A[2]:(0.731409907341) A[3]:(0.675138473511)\n",
      " state (2)  A[0]:(0.632155299187) A[1]:(0.807196736336) A[2]:(0.621630787849) A[3]:(0.682110309601)\n",
      " state (3)  A[0]:(0.711728692055) A[1]:(0.00210785563104) A[2]:(0.359915733337) A[3]:(0.706269860268)\n",
      " state (4)  A[0]:(0.658393621445) A[1]:(0.733059763908) A[2]:(0.00829248502851) A[3]:(0.580626487732)\n",
      " state (5)  A[0]:(0.329234957695) A[1]:(0.998295962811) A[2]:(-0.225675299764) A[3]:(0.319733142853)\n",
      " state (6)  A[0]:(0.00116583646741) A[1]:(0.896985828876) A[2]:(-0.0424217358232) A[3]:(0.774035573006)\n",
      " state (7)  A[0]:(-0.0455477759242) A[1]:(-0.282271027565) A[2]:(0.349060237408) A[3]:(0.897815227509)\n",
      " state (8)  A[0]:(0.702072620392) A[1]:(0.000464439362986) A[2]:(0.814671635628) A[3]:(0.602374970913)\n",
      " state (9)  A[0]:(0.885759592056) A[1]:(0.898301124573) A[2]:(0.85435795784) A[3]:(0.0772812217474)\n",
      " state (10)  A[0]:(0.589925885201) A[1]:(0.999105334282) A[2]:(0.0211059693247) A[3]:(0.443561166525)\n",
      " state (11)  A[0]:(-0.369437098503) A[1]:(0.999998450279) A[2]:(-0.831655919552) A[3]:(0.817497014999)\n",
      " state (12)  A[0]:(-0.612802922726) A[1]:(1.0) A[2]:(-0.66392660141) A[3]:(0.910387277603)\n",
      " state (13)  A[0]:(0.0111513286829) A[1]:(1.0) A[2]:(0.903339862823) A[3]:(0.901745915413)\n",
      " state (14)  A[0]:(0.863614439964) A[1]:(1.0) A[2]:(0.999624431133) A[3]:(0.840684056282)\n",
      " state (15)  A[0]:(0.990359604359) A[1]:(1.0) A[2]:(0.999992907047) A[3]:(0.753543972969)\n",
      "Episode 116000 finished after 0 timesteps with r=0.0. Running score: 0.31. Times trained:               14366. Times reached goal: 298.               Steps done: 1086378. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.303693133807.\n",
      " state (0)  A[0]:(0.590201497078) A[1]:(0.657977938652) A[2]:(0.656266212463) A[3]:(0.58923125267)\n",
      " state (1)  A[0]:(0.59665864706) A[1]:(-0.00209438498132) A[2]:(0.728980302811) A[3]:(0.672218501568)\n",
      " state (2)  A[0]:(0.640831232071) A[1]:(0.805634558201) A[2]:(0.626187860966) A[3]:(0.685588955879)\n",
      " state (3)  A[0]:(0.718171715736) A[1]:(-0.00659158220515) A[2]:(0.345651715994) A[3]:(0.711201667786)\n",
      " state (4)  A[0]:(0.662577033043) A[1]:(0.727251768112) A[2]:(0.00375161785632) A[3]:(0.590180039406)\n",
      " state (5)  A[0]:(0.321516960859) A[1]:(0.998420357704) A[2]:(-0.188906311989) A[3]:(0.330435425043)\n",
      " state (6)  A[0]:(0.00202089268714) A[1]:(0.895966053009) A[2]:(-0.0436461269855) A[3]:(0.788297951221)\n",
      " state (7)  A[0]:(-0.03537087515) A[1]:(-0.320760816336) A[2]:(0.323687493801) A[3]:(0.907475829124)\n",
      " state (8)  A[0]:(0.70316028595) A[1]:(0.00320123531856) A[2]:(0.811893343925) A[3]:(0.621084749699)\n",
      " state (9)  A[0]:(0.881354570389) A[1]:(0.899834036827) A[2]:(0.849409997463) A[3]:(0.117774784565)\n",
      " state (10)  A[0]:(0.593657076359) A[1]:(0.999034225941) A[2]:(0.00260984315537) A[3]:(0.476530611515)\n",
      " state (11)  A[0]:(-0.350368827581) A[1]:(0.99999833107) A[2]:(-0.839085102081) A[3]:(0.828363418579)\n",
      " state (12)  A[0]:(-0.607370257378) A[1]:(1.0) A[2]:(-0.681863307953) A[3]:(0.915882110596)\n",
      " state (13)  A[0]:(0.0120522202924) A[1]:(1.0) A[2]:(0.900405466557) A[3]:(0.907665908337)\n",
      " state (14)  A[0]:(0.864858925343) A[1]:(1.0) A[2]:(0.999615967274) A[3]:(0.850397646427)\n",
      " state (15)  A[0]:(0.990603625774) A[1]:(1.0) A[2]:(0.99999243021) A[3]:(0.770722687244)\n",
      "Episode 117000 finished after 0 timesteps with r=0.0. Running score: 0.33. Times trained:               14160. Times reached goal: 251.               Steps done: 1100538. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.299423141921.\n",
      " state (0)  A[0]:(0.590482115746) A[1]:(0.657330036163) A[2]:(0.660449445248) A[3]:(0.590196013451)\n",
      " state (1)  A[0]:(0.595405042171) A[1]:(-0.000172853469849) A[2]:(0.728570997715) A[3]:(0.668846845627)\n",
      " state (2)  A[0]:(0.642441153526) A[1]:(0.806529641151) A[2]:(0.632905483246) A[3]:(0.686278581619)\n",
      " state (3)  A[0]:(0.718201160431) A[1]:(-0.000671863439493) A[2]:(0.34004637599) A[3]:(0.710478305817)\n",
      " state (4)  A[0]:(0.660842418671) A[1]:(0.73451924324) A[2]:(0.00956792198122) A[3]:(0.586648464203)\n",
      " state (5)  A[0]:(0.311909765005) A[1]:(0.99862998724) A[2]:(-0.14569735527) A[3]:(0.315682590008)\n",
      " state (6)  A[0]:(0.00138288643211) A[1]:(0.899100065231) A[2]:(-0.0297195389867) A[3]:(0.78780579567)\n",
      " state (7)  A[0]:(-0.030703606084) A[1]:(-0.353896915913) A[2]:(0.313150644302) A[3]:(0.909238278866)\n",
      " state (8)  A[0]:(0.698511123657) A[1]:(0.00328336958773) A[2]:(0.811788916588) A[3]:(0.610362172127)\n",
      " state (9)  A[0]:(0.876946628094) A[1]:(0.899326205254) A[2]:(0.848734736443) A[3]:(0.0957753285766)\n",
      " state (10)  A[0]:(0.604662895203) A[1]:(0.998902916908) A[2]:(0.00998296961188) A[3]:(0.454731166363)\n",
      " state (11)  A[0]:(-0.319849967957) A[1]:(0.999998033047) A[2]:(-0.837730050087) A[3]:(0.816712856293)\n",
      " state (12)  A[0]:(-0.598571956158) A[1]:(1.0) A[2]:(-0.686432242393) A[3]:(0.910342097282)\n",
      " state (13)  A[0]:(0.0109411282465) A[1]:(1.0) A[2]:(0.900697410107) A[3]:(0.902357339859)\n",
      " state (14)  A[0]:(0.864749312401) A[1]:(1.0) A[2]:(0.999614238739) A[3]:(0.843852996826)\n",
      " state (15)  A[0]:(0.990730285645) A[1]:(1.0) A[2]:(0.999991893768) A[3]:(0.76594388485)\n",
      "Episode 118000 finished after 0 timesteps with r=0.0. Running score: 0.36. Times trained:               14482. Times reached goal: 287.               Steps done: 1115020. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.295118143712.\n",
      " state (0)  A[0]:(0.590675473213) A[1]:(0.656430125237) A[2]:(0.653000235558) A[3]:(0.589869618416)\n",
      " state (1)  A[0]:(0.593088269234) A[1]:(0.0017268640222) A[2]:(0.727751970291) A[3]:(0.667585074902)\n",
      " state (2)  A[0]:(0.642761468887) A[1]:(0.809740662575) A[2]:(0.63965857029) A[3]:(0.690394282341)\n",
      " state (3)  A[0]:(0.718368709087) A[1]:(0.00560015533119) A[2]:(0.335353165865) A[3]:(0.71315407753)\n",
      " state (4)  A[0]:(0.661474406719) A[1]:(0.728537201881) A[2]:(0.00687850592658) A[3]:(0.588830947876)\n",
      " state (5)  A[0]:(0.302467614412) A[1]:(0.998773932457) A[2]:(-0.122642032802) A[3]:(0.302785634995)\n",
      " state (6)  A[0]:(0.00473126210272) A[1]:(0.898917198181) A[2]:(-0.0258707478642) A[3]:(0.790006697178)\n",
      " state (7)  A[0]:(-0.00895827915519) A[1]:(-0.3906904459) A[2]:(0.299118638039) A[3]:(0.912806868553)\n",
      " state (8)  A[0]:(0.712959647179) A[1]:(0.0038871569559) A[2]:(0.812495827675) A[3]:(0.612738072872)\n",
      " state (9)  A[0]:(0.883036732674) A[1]:(0.899430155754) A[2]:(0.848453521729) A[3]:(0.107627257705)\n",
      " state (10)  A[0]:(0.634329080582) A[1]:(0.998780786991) A[2]:(0.00961142964661) A[3]:(0.466162055731)\n",
      " state (11)  A[0]:(-0.27683365345) A[1]:(0.999997735023) A[2]:(-0.840440809727) A[3]:(0.819643497467)\n",
      " state (12)  A[0]:(-0.587905406952) A[1]:(1.0) A[2]:(-0.698323011398) A[3]:(0.911877632141)\n",
      " state (13)  A[0]:(0.00746051548049) A[1]:(1.0) A[2]:(0.899146795273) A[3]:(0.904010534286)\n",
      " state (14)  A[0]:(0.863689601421) A[1]:(1.0) A[2]:(0.999615371227) A[3]:(0.847110033035)\n",
      " state (15)  A[0]:(0.990804314613) A[1]:(1.0) A[2]:(0.99999165535) A[3]:(0.774311423302)\n",
      "Episode 119000 finished after 0 timesteps with r=0.0. Running score: 0.28. Times trained:               14519. Times reached goal: 285.               Steps done: 1129539. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.29086427904.\n",
      "q_values \n",
      "tensor([[ 0.5947,  0.6593,  0.6557,  0.5934]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6668,  0.7343,  0.0025,  0.5886]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7274,  0.0058,  0.8067,  0.6216]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8987,  0.9006,  0.8553,  0.1119]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0223,  1.0000,  0.9028,  0.9035]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0209,  1.0000,  0.9033,  0.9035]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0162,  1.0000,  0.9031,  0.9035]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0110,  1.0000,  0.9029,  0.9036]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0048,  1.0000,  0.9024,  0.9038]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0026,  1.0000,  0.9025,  0.9038]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0021,  1.0000,  0.9027,  0.9038]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0045,  1.0000,  0.9033,  0.9037]], device='cuda:0')\n",
      "On state=13, selected action=0 , Random? True\n",
      "new state=12, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.594518899918) A[1]:(0.660409033298) A[2]:(0.657164275646) A[3]:(0.593648552895)\n",
      " state (1)  A[0]:(0.597331523895) A[1]:(-0.00019371509552) A[2]:(0.730008721352) A[3]:(0.671950697899)\n",
      " state (2)  A[0]:(0.650540590286) A[1]:(0.811441779137) A[2]:(0.647630870342) A[3]:(0.701552510262)\n",
      " state (3)  A[0]:(0.724629759789) A[1]:(0.0231974218041) A[2]:(0.338473707438) A[3]:(0.721721947193)\n",
      " state (4)  A[0]:(0.665965974331) A[1]:(0.735883712769) A[2]:(0.00941689778119) A[3]:(0.588420748711)\n",
      " state (5)  A[0]:(0.292884618044) A[1]:(0.998870670795) A[2]:(-0.109163679183) A[3]:(0.269078671932)\n",
      " state (6)  A[0]:(-0.0121213821694) A[1]:(0.899373710155) A[2]:(-0.0202941447496) A[3]:(0.779694139957)\n",
      " state (7)  A[0]:(-0.0188075173646) A[1]:(-0.397652447224) A[2]:(0.277709841728) A[3]:(0.911862850189)\n",
      " state (8)  A[0]:(0.726688861847) A[1]:(0.00178873352706) A[2]:(0.810555815697) A[3]:(0.620407819748)\n",
      " state (9)  A[0]:(0.897511005402) A[1]:(0.899755775928) A[2]:(0.857283294201) A[3]:(0.11186747998)\n",
      " state (10)  A[0]:(0.66524207592) A[1]:(0.998742222786) A[2]:(0.0164998099208) A[3]:(0.475188195705)\n",
      " state (11)  A[0]:(-0.264496743679) A[1]:(0.999997615814) A[2]:(-0.847743153572) A[3]:(0.824395656586)\n",
      " state (12)  A[0]:(-0.593810558319) A[1]:(1.0) A[2]:(-0.707441568375) A[3]:(0.913329064846)\n",
      " state (13)  A[0]:(0.00729035353288) A[1]:(1.0) A[2]:(0.903731942177) A[3]:(0.903702914715)\n",
      " state (14)  A[0]:(0.868853092194) A[1]:(1.0) A[2]:(0.999646604061) A[3]:(0.845952928066)\n",
      " state (15)  A[0]:(0.991525352001) A[1]:(1.0) A[2]:(0.999992072582) A[3]:(0.776317954063)\n",
      "Episode 120000 finished after 0 timesteps with r=0.0. Running score: 0.32. Times trained:               15057. Times reached goal: 282.               Steps done: 1144596. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.286517542121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.58940076828) A[1]:(0.66010671854) A[2]:(0.65281188488) A[3]:(0.589022874832)\n",
      " state (1)  A[0]:(0.591449737549) A[1]:(0.00173532788176) A[2]:(0.726582884789) A[3]:(0.663359880447)\n",
      " state (2)  A[0]:(0.646556556225) A[1]:(0.810031235218) A[2]:(0.652004957199) A[3]:(0.699109077454)\n",
      " state (3)  A[0]:(0.719492435455) A[1]:(0.0242918618023) A[2]:(0.336612820625) A[3]:(0.719139933586)\n",
      " state (4)  A[0]:(0.662893652916) A[1]:(0.734236776829) A[2]:(0.0115528926253) A[3]:(0.578117966652)\n",
      " state (5)  A[0]:(0.298429280519) A[1]:(0.998885691166) A[2]:(-0.0912911742926) A[3]:(0.229633897543)\n",
      " state (6)  A[0]:(-0.0116869257763) A[1]:(0.901164650917) A[2]:(-0.0231259353459) A[3]:(0.761833548546)\n",
      " state (7)  A[0]:(-0.0332370698452) A[1]:(-0.415366500616) A[2]:(0.263230472803) A[3]:(0.907006859779)\n",
      " state (8)  A[0]:(0.72036331892) A[1]:(-0.0042114011012) A[2]:(0.815032422543) A[3]:(0.604122281075)\n",
      " state (9)  A[0]:(0.894807398319) A[1]:(0.89764124155) A[2]:(0.86106723547) A[3]:(0.103994764388)\n",
      " state (10)  A[0]:(0.659453034401) A[1]:(0.998602330685) A[2]:(0.00618044612929) A[3]:(0.483737826347)\n",
      " state (11)  A[0]:(-0.276057779789) A[1]:(0.999997198582) A[2]:(-0.854878008366) A[3]:(0.829453885555)\n",
      " state (12)  A[0]:(-0.601815581322) A[1]:(1.0) A[2]:(-0.713260889053) A[3]:(0.915094852448)\n",
      " state (13)  A[0]:(0.00794394034892) A[1]:(1.0) A[2]:(0.905875086784) A[3]:(0.903848230839)\n",
      " state (14)  A[0]:(0.872563719749) A[1]:(1.0) A[2]:(0.9996342659) A[3]:(0.846202909946)\n",
      " state (15)  A[0]:(0.991975903511) A[1]:(1.0) A[2]:(0.999990701675) A[3]:(0.781511902809)\n",
      "Episode 121000 finished after 0 timesteps with r=0.0. Running score: 0.37. Times trained:               15094. Times reached goal: 332.               Steps done: 1159690. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.282225321222.\n",
      " state (0)  A[0]:(0.59439009428) A[1]:(0.657354474068) A[2]:(0.655259013176) A[3]:(0.594415843487)\n",
      " state (1)  A[0]:(0.593286693096) A[1]:(0.00320767262019) A[2]:(0.729608654976) A[3]:(0.661305725574)\n",
      " state (2)  A[0]:(0.646349668503) A[1]:(0.810344934464) A[2]:(0.659045100212) A[3]:(0.697727918625)\n",
      " state (3)  A[0]:(0.716401100159) A[1]:(0.0342778339982) A[2]:(0.339275121689) A[3]:(0.717434406281)\n",
      " state (4)  A[0]:(0.664012312889) A[1]:(0.731203079224) A[2]:(0.013326571323) A[3]:(0.580446839333)\n",
      " state (5)  A[0]:(0.302751868963) A[1]:(0.99898815155) A[2]:(-0.0750728696585) A[3]:(0.223812937737)\n",
      " state (6)  A[0]:(-0.0167615506798) A[1]:(0.899395883083) A[2]:(-0.0144586022943) A[3]:(0.763693332672)\n",
      " state (7)  A[0]:(-0.0411412231624) A[1]:(-0.443100690842) A[2]:(0.247290804982) A[3]:(0.909002900124)\n",
      " state (8)  A[0]:(0.709249138832) A[1]:(0.00276076095179) A[2]:(0.812258422375) A[3]:(0.605981826782)\n",
      " state (9)  A[0]:(0.890167713165) A[1]:(0.898460268974) A[2]:(0.862346112728) A[3]:(0.105610243976)\n",
      " state (10)  A[0]:(0.662239789963) A[1]:(0.998410820961) A[2]:(0.0148016130552) A[3]:(0.489037662745)\n",
      " state (11)  A[0]:(-0.261577606201) A[1]:(0.999996483326) A[2]:(-0.85281932354) A[3]:(0.83095407486)\n",
      " state (12)  A[0]:(-0.601338624954) A[1]:(1.0) A[2]:(-0.709116458893) A[3]:(0.915168642998)\n",
      " state (13)  A[0]:(-0.0019262409769) A[1]:(1.0) A[2]:(0.907194137573) A[3]:(0.902840852737)\n",
      " state (14)  A[0]:(0.869684934616) A[1]:(1.0) A[2]:(0.999602377415) A[3]:(0.845759153366)\n",
      " state (15)  A[0]:(0.991841077805) A[1]:(1.0) A[2]:(0.99998831749) A[3]:(0.786633372307)\n",
      "Episode 122000 finished after 0 timesteps with r=0.0. Running score: 0.27. Times trained:               15159. Times reached goal: 272.               Steps done: 1174849. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.277979331367.\n",
      " state (0)  A[0]:(0.58971208334) A[1]:(0.659552335739) A[2]:(0.653333067894) A[3]:(0.590661108494)\n",
      " state (1)  A[0]:(0.593522429466) A[1]:(0.00080323201837) A[2]:(0.727335512638) A[3]:(0.660398840904)\n",
      " state (2)  A[0]:(0.64975130558) A[1]:(0.807287156582) A[2]:(0.660061776638) A[3]:(0.697823822498)\n",
      " state (3)  A[0]:(0.717615962029) A[1]:(0.0274596307427) A[2]:(0.336523503065) A[3]:(0.71730774641)\n",
      " state (4)  A[0]:(0.667330205441) A[1]:(0.735343933105) A[2]:(0.00814348366112) A[3]:(0.583032727242)\n",
      " state (5)  A[0]:(0.313719600439) A[1]:(0.999080061913) A[2]:(-0.0794329941273) A[3]:(0.226786419749)\n",
      " state (6)  A[0]:(0.0041139787063) A[1]:(0.89805150032) A[2]:(-0.0232670027763) A[3]:(0.770487606525)\n",
      " state (7)  A[0]:(-0.0124645037577) A[1]:(-0.480516970158) A[2]:(0.230811595917) A[3]:(0.913560211658)\n",
      " state (8)  A[0]:(0.717433750629) A[1]:(0.00115692568943) A[2]:(0.814033865929) A[3]:(0.612205982208)\n",
      " state (9)  A[0]:(0.890961170197) A[1]:(0.896869599819) A[2]:(0.862196803093) A[3]:(0.126755520701)\n",
      " state (10)  A[0]:(0.676017582417) A[1]:(0.998126268387) A[2]:(0.00773390615359) A[3]:(0.51268684864)\n",
      " state (11)  A[0]:(-0.231669172645) A[1]:(0.999995470047) A[2]:(-0.855421185493) A[3]:(0.838935494423)\n",
      " state (12)  A[0]:(-0.591213583946) A[1]:(1.0) A[2]:(-0.711512923241) A[3]:(0.917964458466)\n",
      " state (13)  A[0]:(0.00420463597402) A[1]:(1.0) A[2]:(0.906835079193) A[3]:(0.904714345932)\n",
      " state (14)  A[0]:(0.871195197105) A[1]:(1.0) A[2]:(0.999559104443) A[3]:(0.85042399168)\n",
      " state (15)  A[0]:(0.992042124271) A[1]:(1.0) A[2]:(0.999985039234) A[3]:(0.799618244171)\n",
      "Episode 123000 finished after 0 timesteps with r=0.0. Running score: 0.33. Times trained:               15870. Times reached goal: 327.               Steps done: 1190719. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.273602620437.\n",
      " state (0)  A[0]:(0.592065989971) A[1]:(0.654944777489) A[2]:(0.65466606617) A[3]:(0.592080712318)\n",
      " state (1)  A[0]:(0.593546390533) A[1]:(-0.00122225226369) A[2]:(0.726353406906) A[3]:(0.659124016762)\n",
      " state (2)  A[0]:(0.649883329868) A[1]:(0.808756411076) A[2]:(0.660125255585) A[3]:(0.693036198616)\n",
      " state (3)  A[0]:(0.71477997303) A[1]:(0.0307018794119) A[2]:(0.336840420961) A[3]:(0.710263371468)\n",
      " state (4)  A[0]:(0.665120363235) A[1]:(0.72944766283) A[2]:(0.00821774546057) A[3]:(0.578675627708)\n",
      " state (5)  A[0]:(0.300506323576) A[1]:(0.999198675156) A[2]:(-0.0753002241254) A[3]:(0.209643512964)\n",
      " state (6)  A[0]:(-0.00274991290644) A[1]:(0.898441195488) A[2]:(-0.019058899954) A[3]:(0.766954541206)\n",
      " state (7)  A[0]:(-0.0134764499962) A[1]:(-0.515654027462) A[2]:(0.215727835894) A[3]:(0.914053201675)\n",
      " state (8)  A[0]:(0.699207365513) A[1]:(0.000181198120117) A[2]:(0.809778094292) A[3]:(0.607411384583)\n",
      " state (9)  A[0]:(0.880046904087) A[1]:(0.897568583488) A[2]:(0.861016869545) A[3]:(0.11107724905)\n",
      " state (10)  A[0]:(0.66345089674) A[1]:(0.997819423676) A[2]:(-0.002267714357) A[3]:(0.503907561302)\n",
      " state (11)  A[0]:(-0.238830044866) A[1]:(0.999993979931) A[2]:(-0.860215246677) A[3]:(0.833829820156)\n",
      " state (12)  A[0]:(-0.599261760712) A[1]:(1.0) A[2]:(-0.72018969059) A[3]:(0.91405838728)\n",
      " state (13)  A[0]:(-0.00607000291348) A[1]:(1.0) A[2]:(0.903135716915) A[3]:(0.899775028229)\n",
      " state (14)  A[0]:(0.871103346348) A[1]:(1.0) A[2]:(0.999486267567) A[3]:(0.846877038479)\n",
      " state (15)  A[0]:(0.992235183716) A[1]:(1.0) A[2]:(0.999979734421) A[3]:(0.804172754288)\n",
      "Episode 124000 finished after 0 timesteps with r=1.0. Running score: 0.27. Times trained:               16102. Times reached goal: 307.               Steps done: 1206821. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.269232350511.\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.6565,  0.6569,  0.5902]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5928, -0.0028,  0.7291,  0.6586]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6509,  0.8088,  0.6630,  0.6906]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0053,  0.8957, -0.0164,  0.7706]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6766,  0.9973,  0.0114,  0.5078]], device='cuda:0')\n",
      "On state=10, selected action=2 , Random? True\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.590495109558) A[1]:(0.656139492989) A[2]:(0.656584620476) A[3]:(0.590337753296)\n",
      " state (1)  A[0]:(0.5922498703) A[1]:(-0.00469538103789) A[2]:(0.728622019291) A[3]:(0.658433914185)\n",
      " state (2)  A[0]:(0.650338351727) A[1]:(0.808253526688) A[2]:(0.6624969244) A[3]:(0.690334558487)\n",
      " state (3)  A[0]:(0.713865578175) A[1]:(0.0331597290933) A[2]:(0.340619236231) A[3]:(0.70619058609)\n",
      " state (4)  A[0]:(0.664136826992) A[1]:(0.728590607643) A[2]:(0.0105919456109) A[3]:(0.578926861286)\n",
      " state (5)  A[0]:(0.284343868494) A[1]:(0.999318242073) A[2]:(-0.072220377624) A[3]:(0.200642079115)\n",
      " state (6)  A[0]:(0.0037127265241) A[1]:(0.895756542683) A[2]:(-0.0169085841626) A[3]:(0.769831299782)\n",
      " state (7)  A[0]:(0.0194294434041) A[1]:(-0.561110854149) A[2]:(0.200815498829) A[3]:(0.91667330265)\n",
      " state (8)  A[0]:(0.699390888214) A[1]:(0.00150585058145) A[2]:(0.809410393238) A[3]:(0.605525493622)\n",
      " state (9)  A[0]:(0.875698626041) A[1]:(0.896838963032) A[2]:(0.861378669739) A[3]:(0.112179905176)\n",
      " state (10)  A[0]:(0.67483150959) A[1]:(0.997306346893) A[2]:(0.0105328243226) A[3]:(0.507063984871)\n",
      " state (11)  A[0]:(-0.197201684117) A[1]:(0.999991357327) A[2]:(-0.85205757618) A[3]:(0.830719709396)\n",
      " state (12)  A[0]:(-0.580872297287) A[1]:(1.0) A[2]:(-0.696417093277) A[3]:(0.90988779068)\n",
      " state (13)  A[0]:(0.00416774116457) A[1]:(1.0) A[2]:(0.910295307636) A[3]:(0.894400954247)\n",
      " state (14)  A[0]:(0.871610760689) A[1]:(1.0) A[2]:(0.999441921711) A[3]:(0.844618558884)\n",
      " state (15)  A[0]:(0.992300927639) A[1]:(1.0) A[2]:(0.999973237514) A[3]:(0.812312483788)\n",
      "Episode 125000 finished after 0 timesteps with r=0.0. Running score: 0.34. Times trained:               16346. Times reached goal: 331.               Steps done: 1223167. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.264867251655.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.588005185127) A[1]:(0.651626944542) A[2]:(0.652550876141) A[3]:(0.587279081345)\n",
      " state (1)  A[0]:(0.587818384171) A[1]:(0.00191914802417) A[2]:(0.72439044714) A[3]:(0.656234025955)\n",
      " state (2)  A[0]:(0.647345483303) A[1]:(0.805639743805) A[2]:(0.659904837608) A[3]:(0.686629772186)\n",
      " state (3)  A[0]:(0.71068072319) A[1]:(0.0290668364614) A[2]:(0.340260028839) A[3]:(0.701318264008)\n",
      " state (4)  A[0]:(0.658910274506) A[1]:(0.730126142502) A[2]:(0.00992682762444) A[3]:(0.574991881847)\n",
      " state (5)  A[0]:(0.256752789021) A[1]:(0.999439716339) A[2]:(-0.0753226280212) A[3]:(0.177148386836)\n",
      " state (6)  A[0]:(0.00630441913381) A[1]:(0.897925436497) A[2]:(-0.0177273061126) A[3]:(0.765756607056)\n",
      " state (7)  A[0]:(0.0490858629346) A[1]:(-0.599282681942) A[2]:(0.191369816661) A[3]:(0.917760491371)\n",
      " state (8)  A[0]:(0.696438491344) A[1]:(0.00326763419434) A[2]:(0.812143683434) A[3]:(0.601987123489)\n",
      " state (9)  A[0]:(0.869161248207) A[1]:(0.895573019981) A[2]:(0.86239862442) A[3]:(0.119503997266)\n",
      " state (10)  A[0]:(0.678628504276) A[1]:(0.996613383293) A[2]:(0.0100400652736) A[3]:(0.522702217102)\n",
      " state (11)  A[0]:(-0.170420274138) A[1]:(0.999986767769) A[2]:(-0.852521598339) A[3]:(0.83556419611)\n",
      " state (12)  A[0]:(-0.566733121872) A[1]:(1.0) A[2]:(-0.694248259068) A[3]:(0.910655677319)\n",
      " state (13)  A[0]:(0.0238946788013) A[1]:(1.0) A[2]:(0.911847233772) A[3]:(0.894275605679)\n",
      " state (14)  A[0]:(0.878464698792) A[1]:(1.0) A[2]:(0.999402523041) A[3]:(0.848482728004)\n",
      " state (15)  A[0]:(0.992935836315) A[1]:(1.0) A[2]:(0.999967634678) A[3]:(0.825759351254)\n",
      "Episode 126000 finished after 0 timesteps with r=1.0. Running score: 0.35. Times trained:               16385. Times reached goal: 309.               Steps done: 1239552. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.260562762565.\n",
      " state (0)  A[0]:(0.588722288609) A[1]:(0.653599083424) A[2]:(0.652436971664) A[3]:(0.588039278984)\n",
      " state (1)  A[0]:(0.589617967606) A[1]:(0.00123524607625) A[2]:(0.724989056587) A[3]:(0.657821178436)\n",
      " state (2)  A[0]:(0.650101065636) A[1]:(0.805912971497) A[2]:(0.661956191063) A[3]:(0.688102304935)\n",
      " state (3)  A[0]:(0.712630748749) A[1]:(0.030654836446) A[2]:(0.346285134554) A[3]:(0.701504826546)\n",
      " state (4)  A[0]:(0.658966422081) A[1]:(0.728788375854) A[2]:(0.0112496633083) A[3]:(0.577653288841)\n",
      " state (5)  A[0]:(0.227329030633) A[1]:(0.999544322491) A[2]:(-0.0833475887775) A[3]:(0.15705345571)\n",
      " state (6)  A[0]:(0.00359739293344) A[1]:(0.895769953728) A[2]:(-0.0142606366426) A[3]:(0.763114511967)\n",
      " state (7)  A[0]:(0.0783580243587) A[1]:(-0.641872286797) A[2]:(0.176300793886) A[3]:(0.919313311577)\n",
      " state (8)  A[0]:(0.695120573044) A[1]:(0.000927209563088) A[2]:(0.806432008743) A[3]:(0.603232860565)\n",
      " state (9)  A[0]:(0.865831613541) A[1]:(0.896060943604) A[2]:(0.861089348793) A[3]:(0.111990705132)\n",
      " state (10)  A[0]:(0.686650693417) A[1]:(0.995666325092) A[2]:(-0.00469693075866) A[3]:(0.523350059986)\n",
      " state (11)  A[0]:(-0.152772694826) A[1]:(0.999977886677) A[2]:(-0.862587809563) A[3]:(0.834216952324)\n",
      " state (12)  A[0]:(-0.572336554527) A[1]:(1.0) A[2]:(-0.722995281219) A[3]:(0.907875776291)\n",
      " state (13)  A[0]:(0.00267576542683) A[1]:(1.0) A[2]:(0.901050150394) A[3]:(0.889700651169)\n",
      " state (14)  A[0]:(0.875368833542) A[1]:(1.0) A[2]:(0.999281644821) A[3]:(0.845855593681)\n",
      " state (15)  A[0]:(0.992986798286) A[1]:(1.0) A[2]:(0.99995714426) A[3]:(0.831502318382)\n",
      "Episode 127000 finished after 0 timesteps with r=0.0. Running score: 0.33. Times trained:               16977. Times reached goal: 290.               Steps done: 1256529. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.256176526459.\n",
      " state (0)  A[0]:(0.588889837265) A[1]:(0.656124293804) A[2]:(0.651142477989) A[3]:(0.587090492249)\n",
      " state (1)  A[0]:(0.590944051743) A[1]:(-0.00072962034028) A[2]:(0.722393155098) A[3]:(0.658516347408)\n",
      " state (2)  A[0]:(0.654508054256) A[1]:(0.805480360985) A[2]:(0.657688736916) A[3]:(0.693497657776)\n",
      " state (3)  A[0]:(0.716750204563) A[1]:(0.0258056428283) A[2]:(0.343591660261) A[3]:(0.706522345543)\n",
      " state (4)  A[0]:(0.659314632416) A[1]:(0.724121332169) A[2]:(0.00600247317925) A[3]:(0.581139922142)\n",
      " state (5)  A[0]:(0.197325393558) A[1]:(0.999613821507) A[2]:(-0.0940383970737) A[3]:(0.130227208138)\n",
      " state (6)  A[0]:(0.00655963551253) A[1]:(0.894713819027) A[2]:(-0.0135670918971) A[3]:(0.757049798965)\n",
      " state (7)  A[0]:(0.112562008202) A[1]:(-0.676645636559) A[2]:(0.164975315332) A[3]:(0.92046558857)\n",
      " state (8)  A[0]:(0.696479201317) A[1]:(0.0035356732551) A[2]:(0.803529858589) A[3]:(0.603757321835)\n",
      " state (9)  A[0]:(0.863612055779) A[1]:(0.897096693516) A[2]:(0.862479150295) A[3]:(0.101363442838)\n",
      " state (10)  A[0]:(0.695666909218) A[1]:(0.994298934937) A[2]:(-0.00834839046001) A[3]:(0.520986318588)\n",
      " state (11)  A[0]:(-0.131094053388) A[1]:(0.999960005283) A[2]:(-0.865731656551) A[3]:(0.830081582069)\n",
      " state (12)  A[0]:(-0.567079544067) A[1]:(0.999999940395) A[2]:(-0.722290754318) A[3]:(0.902150630951)\n",
      " state (13)  A[0]:(0.00996449403465) A[1]:(1.0) A[2]:(0.902312159538) A[3]:(0.882202029228)\n",
      " state (14)  A[0]:(0.879672467709) A[1]:(1.0) A[2]:(0.999188244343) A[3]:(0.843617141247)\n",
      " state (15)  A[0]:(0.993465602398) A[1]:(1.0) A[2]:(0.999942660332) A[3]:(0.841525435448)\n",
      "Episode 128000 finished after 0 timesteps with r=0.0. Running score: 0.27. Times trained:               16615. Times reached goal: 272.               Steps done: 1273144. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.251955318267.\n",
      " state (0)  A[0]:(0.587951660156) A[1]:(0.651025652885) A[2]:(0.653126001358) A[3]:(0.58570343256)\n",
      " state (1)  A[0]:(0.586995482445) A[1]:(-0.000103235244751) A[2]:(0.723424911499) A[3]:(0.655167818069)\n",
      " state (2)  A[0]:(0.652099967003) A[1]:(0.802963614464) A[2]:(0.65614771843) A[3]:(0.69896531105)\n",
      " state (3)  A[0]:(0.715575397015) A[1]:(0.0107896905392) A[2]:(0.344230979681) A[3]:(0.715135812759)\n",
      " state (4)  A[0]:(0.650129258633) A[1]:(0.723017573357) A[2]:(0.0123741272837) A[3]:(0.584350466728)\n",
      " state (5)  A[0]:(0.15413364768) A[1]:(0.999668896198) A[2]:(-0.077122181654) A[3]:(0.0945107340813)\n",
      " state (6)  A[0]:(0.00632748520002) A[1]:(0.891532540321) A[2]:(-0.00394807197154) A[3]:(0.74889844656)\n",
      " state (7)  A[0]:(0.140323117375) A[1]:(-0.729941725731) A[2]:(0.161949813366) A[3]:(0.92221558094)\n",
      " state (8)  A[0]:(0.682413220406) A[1]:(0.00684250658378) A[2]:(0.808407723904) A[3]:(0.599071085453)\n",
      " state (9)  A[0]:(0.850804328918) A[1]:(0.893600583076) A[2]:(0.866629183292) A[3]:(0.100405916572)\n",
      " state (10)  A[0]:(0.700573265553) A[1]:(0.990176200867) A[2]:(0.0161441843957) A[3]:(0.530388116837)\n",
      " state (11)  A[0]:(-0.0898877158761) A[1]:(0.999875068665) A[2]:(-0.857159137726) A[3]:(0.829517126083)\n",
      " state (12)  A[0]:(-0.553231179714) A[1]:(0.999999701977) A[2]:(-0.701260209084) A[3]:(0.897196650505)\n",
      " state (13)  A[0]:(0.00951788853854) A[1]:(1.0) A[2]:(0.910148501396) A[3]:(0.874547481537)\n",
      " state (14)  A[0]:(0.879552423954) A[1]:(1.0) A[2]:(0.9991543293) A[3]:(0.841573834419)\n",
      " state (15)  A[0]:(0.993612289429) A[1]:(1.0) A[2]:(0.999929904938) A[3]:(0.851801156998)\n",
      "Episode 129000 finished after 0 timesteps with r=1.0. Running score: 0.42. Times trained:               18449. Times reached goal: 343.               Steps done: 1291593. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.247349610585.\n",
      "q_values \n",
      "tensor([[ 0.5813,  0.6496,  0.6423,  0.5826]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6417,  0.7211,  0.0066,  0.5833]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6419,  0.7214,  0.0077,  0.5834]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6719,  0.0106,  0.8016,  0.6039]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8369,  0.8894,  0.8621,  0.1029]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0086,  1.0000,  0.9069,  0.8660]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0087,  1.0000,  0.9071,  0.8660]], device='cuda:0')\n",
      "On state=13, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0075,  1.0000,  0.9074,  0.8658]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8787,  1.0000,  0.9991,  0.8360]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8782,  1.0000,  0.9991,  0.8358]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7021,  0.9804,  0.0019,  0.5393]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8784,  1.0000,  0.9991,  0.8355]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8792,  1.0000,  0.9991,  0.8356]], device='cuda:0')\n",
      "On state=14, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8795,  1.0000,  0.9991,  0.8355]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? True\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.581781327724) A[1]:(0.647975623608) A[2]:(0.643907189369) A[3]:(0.582595229149)\n",
      " state (1)  A[0]:(0.581081748009) A[1]:(0.000334978103638) A[2]:(0.717440128326) A[3]:(0.643844246864)\n",
      " state (2)  A[0]:(0.64787954092) A[1]:(0.798112988472) A[2]:(0.653657674789) A[3]:(0.691617012024)\n",
      " state (3)  A[0]:(0.71298211813) A[1]:(0.0099283773452) A[2]:(0.344718277454) A[3]:(0.713765382767)\n",
      " state (4)  A[0]:(0.642653107643) A[1]:(0.717946469784) A[2]:(0.00983158871531) A[3]:(0.584315776825)\n",
      " state (5)  A[0]:(0.113333866) A[1]:(0.999717175961) A[2]:(-0.0845504552126) A[3]:(0.0616368353367)\n",
      " state (6)  A[0]:(0.0156076392159) A[1]:(0.884600281715) A[2]:(-0.00524158449844) A[3]:(0.745857000351)\n",
      " state (7)  A[0]:(0.186400011182) A[1]:(-0.781806707382) A[2]:(0.146724030375) A[3]:(0.925971806049)\n",
      " state (8)  A[0]:(0.671404778957) A[1]:(0.0141823319718) A[2]:(0.802554368973) A[3]:(0.604009687901)\n",
      " state (9)  A[0]:(0.836264193058) A[1]:(0.890126049519) A[2]:(0.862962424755) A[3]:(0.102313108742)\n",
      " state (10)  A[0]:(0.703186035156) A[1]:(0.980566799641) A[2]:(0.00562900304794) A[3]:(0.539072394371)\n",
      " state (11)  A[0]:(-0.052518222481) A[1]:(0.999436974525) A[2]:(-0.860418438911) A[3]:(0.828533530235)\n",
      " state (12)  A[0]:(-0.541739583015) A[1]:(0.999997079372) A[2]:(-0.708473801613) A[3]:(0.891536712646)\n",
      " state (13)  A[0]:(0.00609396304935) A[1]:(1.0) A[2]:(0.910439491272) A[3]:(0.864846169949)\n",
      " state (14)  A[0]:(0.878515899181) A[1]:(1.0) A[2]:(0.999112010002) A[3]:(0.835315704346)\n",
      " state (15)  A[0]:(0.993589758873) A[1]:(1.0) A[2]:(0.999918401241) A[3]:(0.855711817741)\n",
      "Episode 130000 finished after 0 timesteps with r=1.0. Running score: 0.3. Times trained:               16630. Times reached goal: 299.               Steps done: 1308223. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.243270200868.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.575821518898) A[1]:(0.642447769642) A[2]:(0.627429366112) A[3]:(0.578132033348)\n",
      " state (1)  A[0]:(0.569804787636) A[1]:(0.00181644957047) A[2]:(0.695714116096) A[3]:(0.624256312847)\n",
      " state (2)  A[0]:(0.635613918304) A[1]:(0.775691986084) A[2]:(0.641427278519) A[3]:(0.668027281761)\n",
      " state (3)  A[0]:(0.702926278114) A[1]:(0.000547587813344) A[2]:(0.346341997385) A[3]:(0.701782822609)\n",
      " state (4)  A[0]:(0.626072406769) A[1]:(0.719391107559) A[2]:(0.0076142270118) A[3]:(0.575438261032)\n",
      " state (5)  A[0]:(0.059463661164) A[1]:(0.999750077724) A[2]:(-0.117511466146) A[3]:(0.0162339247763)\n",
      " state (6)  A[0]:(0.0250896606594) A[1]:(0.85430586338) A[2]:(0.00728510832414) A[3]:(0.743026733398)\n",
      " state (7)  A[0]:(0.24902099371) A[1]:(-0.847897529602) A[2]:(0.147541418672) A[3]:(0.928748250008)\n",
      " state (8)  A[0]:(0.65802359581) A[1]:(0.0159790627658) A[2]:(0.797316789627) A[3]:(0.598624110222)\n",
      " state (9)  A[0]:(0.815447568893) A[1]:(0.886812388897) A[2]:(0.861276984215) A[3]:(0.0707229301333)\n",
      " state (10)  A[0]:(0.70556807518) A[1]:(0.94910132885) A[2]:(0.00307737803087) A[3]:(0.523841381073)\n",
      " state (11)  A[0]:(-0.0044319042936) A[1]:(0.994835674763) A[2]:(-0.865184664726) A[3]:(0.817160844803)\n",
      " state (12)  A[0]:(-0.52655172348) A[1]:(0.999916255474) A[2]:(-0.729450702667) A[3]:(0.878446102142)\n",
      " state (13)  A[0]:(0.00115549517795) A[1]:(0.999998867512) A[2]:(0.91011941433) A[3]:(0.843922495842)\n",
      " state (14)  A[0]:(0.875291705132) A[1]:(0.999999880791) A[2]:(0.999157905579) A[3]:(0.813734650612)\n",
      " state (15)  A[0]:(0.993108928204) A[1]:(0.999999940395) A[2]:(0.999918162823) A[3]:(0.845373034477)\n",
      "Episode 131000 finished after 0 timesteps with r=0.0. Running score: 0.27. Times trained:               18560. Times reached goal: 269.               Steps done: 1326783. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.238796747997.\n",
      " state (0)  A[0]:(0.580814361572) A[1]:(0.651145279408) A[2]:(0.602982461452) A[3]:(0.582327246666)\n",
      " state (1)  A[0]:(0.566553115845) A[1]:(-0.00301306531765) A[2]:(0.666216135025) A[3]:(0.598708331585)\n",
      " state (2)  A[0]:(0.62407630682) A[1]:(0.737939953804) A[2]:(0.627144217491) A[3]:(0.639043211937)\n",
      " state (3)  A[0]:(0.705396056175) A[1]:(0.00375632662326) A[2]:(0.357772141695) A[3]:(0.694249749184)\n",
      " state (4)  A[0]:(0.632722198963) A[1]:(0.718986332417) A[2]:(0.000167846679688) A[3]:(0.581031739712)\n",
      " state (5)  A[0]:(0.0235435962677) A[1]:(0.999758958817) A[2]:(-0.211181163788) A[3]:(-0.0252632368356)\n",
      " state (6)  A[0]:(0.0536101981997) A[1]:(0.818996846676) A[2]:(0.020092882216) A[3]:(0.741962254047)\n",
      " state (7)  A[0]:(0.322433888912) A[1]:(-0.898265361786) A[2]:(0.205083876848) A[3]:(0.934071242809)\n",
      " state (8)  A[0]:(0.664987802505) A[1]:(-0.00117915810551) A[2]:(0.802272737026) A[3]:(0.614132642746)\n",
      " state (9)  A[0]:(0.806056857109) A[1]:(0.895146012306) A[2]:(0.859450697899) A[3]:(0.0683815330267)\n",
      " state (10)  A[0]:(0.709244608879) A[1]:(0.907588601112) A[2]:(-0.0168935675174) A[3]:(0.533455967903)\n",
      " state (11)  A[0]:(0.00977303273976) A[1]:(0.975290954113) A[2]:(-0.878938317299) A[3]:(0.819388210773)\n",
      " state (12)  A[0]:(-0.536658346653) A[1]:(0.998965263367) A[2]:(-0.769786596298) A[3]:(0.874642193317)\n",
      " state (13)  A[0]:(-0.00551921501756) A[1]:(0.999972820282) A[2]:(0.908386230469) A[3]:(0.83120059967)\n",
      " state (14)  A[0]:(0.883509755135) A[1]:(0.999996304512) A[2]:(0.999340355396) A[3]:(0.796283125877)\n",
      " state (15)  A[0]:(0.994058728218) A[1]:(0.99999666214) A[2]:(0.999945104122) A[3]:(0.836387336254)\n",
      "Episode 132000 finished after 0 timesteps with r=1.0. Running score: 0.22. Times trained:               17592. Times reached goal: 273.               Steps done: 1344375. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.2346325711.\n",
      " state (0)  A[0]:(0.593586683273) A[1]:(0.654909491539) A[2]:(0.589362919331) A[3]:(0.59251755476)\n",
      " state (1)  A[0]:(0.573183894157) A[1]:(-0.00155582895968) A[2]:(0.655143618584) A[3]:(0.590030550957)\n",
      " state (2)  A[0]:(0.610257387161) A[1]:(0.727301836014) A[2]:(0.633073687553) A[3]:(0.626838207245)\n",
      " state (3)  A[0]:(0.709913730621) A[1]:(0.0429393202066) A[2]:(0.390217453241) A[3]:(0.696750760078)\n",
      " state (4)  A[0]:(0.642488002777) A[1]:(0.727051436901) A[2]:(0.000364422769053) A[3]:(0.588556289673)\n",
      " state (5)  A[0]:(-0.0235771052539) A[1]:(0.999755680561) A[2]:(-0.332275271416) A[3]:(-0.0864497721195)\n",
      " state (6)  A[0]:(0.0781478732824) A[1]:(0.809701800346) A[2]:(0.024520130828) A[3]:(0.733502984047)\n",
      " state (7)  A[0]:(0.392445802689) A[1]:(-0.911597490311) A[2]:(0.298747062683) A[3]:(0.938682258129)\n",
      " state (8)  A[0]:(0.681284070015) A[1]:(-0.00111833168194) A[2]:(0.810007452965) A[3]:(0.617878317833)\n",
      " state (9)  A[0]:(0.808067798615) A[1]:(0.902192115784) A[2]:(0.852233171463) A[3]:(0.047415856272)\n",
      " state (10)  A[0]:(0.737596273422) A[1]:(0.896105766296) A[2]:(-0.00635937228799) A[3]:(0.536025881767)\n",
      " state (11)  A[0]:(0.0956199318171) A[1]:(0.961962819099) A[2]:(-0.874128699303) A[3]:(0.82297629118)\n",
      " state (12)  A[0]:(-0.505982279778) A[1]:(0.9977388978) A[2]:(-0.783924221992) A[3]:(0.875961482525)\n",
      " state (13)  A[0]:(-0.00314619089477) A[1]:(0.999925136566) A[2]:(0.905869543552) A[3]:(0.828440904617)\n",
      " state (14)  A[0]:(0.887783169746) A[1]:(0.999989032745) A[2]:(0.99948400259) A[3]:(0.788701653481)\n",
      " state (15)  A[0]:(0.994799315929) A[1]:(0.999989807606) A[2]:(0.999966323376) A[3]:(0.833218216896)\n",
      "Episode 133000 finished after 0 timesteps with r=0.0. Running score: 0.32. Times trained:               18066. Times reached goal: 283.               Steps done: 1362441. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.230431759258.\n",
      " state (0)  A[0]:(0.592581748962) A[1]:(0.655881643295) A[2]:(0.586663722992) A[3]:(0.592945814133)\n",
      " state (1)  A[0]:(0.575826644897) A[1]:(0.000409007043345) A[2]:(0.652130246162) A[3]:(0.585810899734)\n",
      " state (2)  A[0]:(0.598965644836) A[1]:(0.722969293594) A[2]:(0.638252973557) A[3]:(0.619592308998)\n",
      " state (3)  A[0]:(0.712279319763) A[1]:(0.0625425949693) A[2]:(0.418470352888) A[3]:(0.699008703232)\n",
      " state (4)  A[0]:(0.64450275898) A[1]:(0.73069524765) A[2]:(0.000288605690002) A[3]:(0.59295797348)\n",
      " state (5)  A[0]:(-0.0821755900979) A[1]:(0.999743938446) A[2]:(-0.443485468626) A[3]:(-0.131420388818)\n",
      " state (6)  A[0]:(0.0945269167423) A[1]:(0.800894618034) A[2]:(0.0274686831981) A[3]:(0.733749508858)\n",
      " state (7)  A[0]:(0.455375850201) A[1]:(-0.912934243679) A[2]:(0.375747919083) A[3]:(0.94428896904)\n",
      " state (8)  A[0]:(0.684966921806) A[1]:(-0.00168835953809) A[2]:(0.808824121952) A[3]:(0.636824607849)\n",
      " state (9)  A[0]:(0.793217897415) A[1]:(0.904655277729) A[2]:(0.837246656418) A[3]:(0.0466626919806)\n",
      " state (10)  A[0]:(0.736638665199) A[1]:(0.886894404888) A[2]:(-0.0270502120256) A[3]:(0.550813555717)\n",
      " state (11)  A[0]:(0.118385516107) A[1]:(0.945957005024) A[2]:(-0.877141714096) A[3]:(0.831639409065)\n",
      " state (12)  A[0]:(-0.498683422804) A[1]:(0.995099425316) A[2]:(-0.804107308388) A[3]:(0.880490243435)\n",
      " state (13)  A[0]:(0.000714868190698) A[1]:(0.999754190445) A[2]:(0.898839533329) A[3]:(0.830436706543)\n",
      " state (14)  A[0]:(0.893937349319) A[1]:(0.999954283237) A[2]:(0.999562442303) A[3]:(0.787237167358)\n",
      " state (15)  A[0]:(0.995392918587) A[1]:(0.999952971935) A[2]:(0.999977111816) A[3]:(0.834616780281)\n",
      "Episode 134000 finished after 0 timesteps with r=0.0. Running score: 0.35. Times trained:               18544. Times reached goal: 284.               Steps done: 1380985. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.226198009368.\n",
      "q_values \n",
      "tensor([[ 0.5842,  0.6500,  0.5648,  0.5843]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6122,  0.7244, -0.0002,  0.5647]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7212, -0.0648,  0.8107,  0.6577]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7961,  0.9044,  0.8604,  0.0178]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6878,  0.7696, -0.0348,  0.5007]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8535,  0.8967,  0.9999,  0.6981]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.585253119469) A[1]:(0.650131702423) A[2]:(0.564120829105) A[3]:(0.584547519684)\n",
      " state (1)  A[0]:(0.631959795952) A[1]:(-0.0169129632413) A[2]:(0.599576234818) A[3]:(0.573873281479)\n",
      " state (2)  A[0]:(0.646203637123) A[1]:(0.640063047409) A[2]:(0.557451486588) A[3]:(0.603042602539)\n",
      " state (3)  A[0]:(0.70536339283) A[1]:(0.0203700792044) A[2]:(0.385926961899) A[3]:(0.680252790451)\n",
      " state (4)  A[0]:(0.613255620003) A[1]:(0.722401976585) A[2]:(-0.00207233126275) A[3]:(0.566029548645)\n",
      " state (5)  A[0]:(-0.121819742024) A[1]:(0.999717831612) A[2]:(-0.505089640617) A[3]:(-0.182802826166)\n",
      " state (6)  A[0]:(0.0657880306244) A[1]:(0.710112214088) A[2]:(0.155349761248) A[3]:(0.705786824226)\n",
      " state (7)  A[0]:(0.496406257153) A[1]:(-0.89226526022) A[2]:(0.45904764533) A[3]:(0.92416882515)\n",
      " state (8)  A[0]:(0.72246325016) A[1]:(-0.0689640045166) A[2]:(0.809867203236) A[3]:(0.659327030182)\n",
      " state (9)  A[0]:(0.797757327557) A[1]:(0.90358042717) A[2]:(0.860019564629) A[3]:(0.020647212863)\n",
      " state (10)  A[0]:(0.69113445282) A[1]:(0.768076300621) A[2]:(-0.0343774929643) A[3]:(0.502629339695)\n",
      " state (11)  A[0]:(-0.0680428370833) A[1]:(0.538587629795) A[2]:(-0.91671872139) A[3]:(0.820298552513)\n",
      " state (12)  A[0]:(-0.559659481049) A[1]:(0.581417322159) A[2]:(-0.88725912571) A[3]:(0.885080873966)\n",
      " state (13)  A[0]:(-0.0114356065169) A[1]:(0.805049359798) A[2]:(0.894099771976) A[3]:(0.826375484467)\n",
      " state (14)  A[0]:(0.854164600372) A[1]:(0.896327137947) A[2]:(0.999903738499) A[3]:(0.698740065098)\n",
      " state (15)  A[0]:(0.985737740993) A[1]:(0.863547325134) A[2]:(0.999998629093) A[3]:(0.671797990799)\n",
      "Episode 135000 finished after 0 timesteps with r=1.0. Running score: 0.77. Times trained:               10807. Times reached goal: 594.               Steps done: 1391792. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.223766649.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.578549861908) A[1]:(0.642172217369) A[2]:(0.532141327858) A[3]:(0.572833359241)\n",
      " state (1)  A[0]:(0.590659677982) A[1]:(0.00104182923678) A[2]:(0.565233945847) A[3]:(0.568517923355)\n",
      " state (2)  A[0]:(0.62568962574) A[1]:(0.635421276093) A[2]:(0.542397499084) A[3]:(0.6116989851)\n",
      " state (3)  A[0]:(0.69570940733) A[1]:(0.0214559882879) A[2]:(0.392374038696) A[3]:(0.686295092106)\n",
      " state (4)  A[0]:(0.613779783249) A[1]:(0.71454679966) A[2]:(-0.00149118795525) A[3]:(0.565793633461)\n",
      " state (5)  A[0]:(-0.124444171786) A[1]:(0.999696016312) A[2]:(-0.584967017174) A[3]:(-0.227180793881)\n",
      " state (6)  A[0]:(0.0222529973835) A[1]:(0.706771731377) A[2]:(0.0592140220106) A[3]:(0.665444970131)\n",
      " state (7)  A[0]:(0.460301816463) A[1]:(-0.88370680809) A[2]:(0.42824998498) A[3]:(0.907549440861)\n",
      " state (8)  A[0]:(0.706905722618) A[1]:(-0.0770322978497) A[2]:(0.792858242989) A[3]:(0.6229211092)\n",
      " state (9)  A[0]:(0.788986265659) A[1]:(0.88624215126) A[2]:(0.83435344696) A[3]:(0.0258711054921)\n",
      " state (10)  A[0]:(0.672017872334) A[1]:(0.769986331463) A[2]:(-0.0987855866551) A[3]:(0.487010717392)\n",
      " state (11)  A[0]:(-0.0914832726121) A[1]:(0.595023155212) A[2]:(-0.920242011547) A[3]:(0.803070545197)\n",
      " state (12)  A[0]:(-0.541341304779) A[1]:(0.625271201134) A[2]:(-0.884884238243) A[3]:(0.874056398869)\n",
      " state (13)  A[0]:(-0.002825550735) A[1]:(0.807056903839) A[2]:(0.900850057602) A[3]:(0.81708997488)\n",
      " state (14)  A[0]:(0.834470152855) A[1]:(0.896161735058) A[2]:(0.999933123589) A[3]:(0.682438492775)\n",
      " state (15)  A[0]:(0.981057822704) A[1]:(0.873539209366) A[2]:(0.999999344349) A[3]:(0.651780486107)\n",
      "Episode 136000 finished after 0 timesteps with r=1.0. Running score: 0.82. Times trained:               6621. Times reached goal: 777.               Steps done: 1398413. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.222289983911.\n",
      " state (0)  A[0]:(0.575262248516) A[1]:(0.636811375618) A[2]:(0.510883808136) A[3]:(0.570740938187)\n",
      " state (1)  A[0]:(0.569999396801) A[1]:(0.00373103981838) A[2]:(0.567415475845) A[3]:(0.554673135281)\n",
      " state (2)  A[0]:(0.610726475716) A[1]:(0.636162281036) A[2]:(0.534340858459) A[3]:(0.611304700375)\n",
      " state (3)  A[0]:(0.689943790436) A[1]:(0.0225706361234) A[2]:(0.400794804096) A[3]:(0.68469119072)\n",
      " state (4)  A[0]:(0.621736884117) A[1]:(0.701330900192) A[2]:(0.00912521779537) A[3]:(0.567937135696)\n",
      " state (5)  A[0]:(-0.101514264941) A[1]:(0.999676048756) A[2]:(-0.635069727898) A[3]:(-0.213650599122)\n",
      " state (6)  A[0]:(0.0184121783823) A[1]:(0.710755705833) A[2]:(-0.00481566041708) A[3]:(0.675971508026)\n",
      " state (7)  A[0]:(0.452836751938) A[1]:(-0.863256037235) A[2]:(0.413719415665) A[3]:(0.905762910843)\n",
      " state (8)  A[0]:(0.696129024029) A[1]:(-0.0495280511677) A[2]:(0.780891001225) A[3]:(0.615824580193)\n",
      " state (9)  A[0]:(0.773301005363) A[1]:(0.871388137341) A[2]:(0.813994765282) A[3]:(0.0339921228588)\n",
      " state (10)  A[0]:(0.63416492939) A[1]:(0.781070291996) A[2]:(-0.10589646548) A[3]:(0.451809763908)\n",
      " state (11)  A[0]:(-0.148108676076) A[1]:(0.661997437477) A[2]:(-0.912019133568) A[3]:(0.768195986748)\n",
      " state (12)  A[0]:(-0.545889377594) A[1]:(0.680180430412) A[2]:(-0.875781297684) A[3]:(0.854057312012)\n",
      " state (13)  A[0]:(-0.000657230499201) A[1]:(0.81474506855) A[2]:(0.8987454772) A[3]:(0.809787511826)\n",
      " state (14)  A[0]:(0.828436434269) A[1]:(0.898278355598) A[2]:(0.999949157238) A[3]:(0.687066197395)\n",
      " state (15)  A[0]:(0.979959785938) A[1]:(0.886144936085) A[2]:(0.999999701977) A[3]:(0.663695335388)\n",
      "Episode 137000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained:               6618. Times reached goal: 768.               Steps done: 1405031. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.220823725996.\n",
      " state (0)  A[0]:(0.560741007328) A[1]:(0.626272797585) A[2]:(0.513192415237) A[3]:(0.558950662613)\n",
      " state (1)  A[0]:(0.552383363247) A[1]:(0.00332476454787) A[2]:(0.569111406803) A[3]:(0.540006279945)\n",
      " state (2)  A[0]:(0.587162613869) A[1]:(0.641689836979) A[2]:(0.518520593643) A[3]:(0.605736613274)\n",
      " state (3)  A[0]:(0.673771798611) A[1]:(0.0192175395787) A[2]:(0.396748125553) A[3]:(0.676726937294)\n",
      " state (4)  A[0]:(0.611680865288) A[1]:(0.69137108326) A[2]:(0.0113707650453) A[3]:(0.554922640324)\n",
      " state (5)  A[0]:(-0.112404346466) A[1]:(0.999659717083) A[2]:(-0.662900805473) A[3]:(-0.232401043177)\n",
      " state (6)  A[0]:(-0.00940092559904) A[1]:(0.715171933174) A[2]:(-0.0268311463296) A[3]:(0.666651964188)\n",
      " state (7)  A[0]:(0.433958590031) A[1]:(-0.841196715832) A[2]:(0.42565754056) A[3]:(0.898627638817)\n",
      " state (8)  A[0]:(0.682595491409) A[1]:(-0.0522927343845) A[2]:(0.770077109337) A[3]:(0.60775911808)\n",
      " state (9)  A[0]:(0.757248163223) A[1]:(0.854491591454) A[2]:(0.796617150307) A[3]:(0.0319156683981)\n",
      " state (10)  A[0]:(0.60288977623) A[1]:(0.787435829639) A[2]:(-0.0702310204506) A[3]:(0.40504348278)\n",
      " state (11)  A[0]:(-0.182018741965) A[1]:(0.705884218216) A[2]:(-0.8896022439) A[3]:(0.722423195839)\n",
      " state (12)  A[0]:(-0.543692350388) A[1]:(0.713899552822) A[2]:(-0.850958824158) A[3]:(0.827400803566)\n",
      " state (13)  A[0]:(-0.0056317448616) A[1]:(0.814642250538) A[2]:(0.898333609104) A[3]:(0.798872292042)\n",
      " state (14)  A[0]:(0.815777719021) A[1]:(0.898248612881) A[2]:(0.999960541725) A[3]:(0.685168504715)\n",
      " state (15)  A[0]:(0.977563738823) A[1]:(0.899567246437) A[2]:(0.999999880791) A[3]:(0.658866286278)\n",
      "Episode 138000 finished after 0 timesteps with r=1.0. Running score: 0.7. Times trained:               6576. Times reached goal: 763.               Steps done: 1411607. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.219376353351.\n",
      " state (0)  A[0]:(0.551166057587) A[1]:(0.611534714699) A[2]:(0.516545772552) A[3]:(0.547802269459)\n",
      " state (1)  A[0]:(0.544470310211) A[1]:(-5.87552785873e-05) A[2]:(0.577512264252) A[3]:(0.52948486805)\n",
      " state (2)  A[0]:(0.571696400642) A[1]:(0.641274273396) A[2]:(0.509577393532) A[3]:(0.600671589375)\n",
      " state (3)  A[0]:(0.663401961327) A[1]:(0.00418125651777) A[2]:(0.395056903362) A[3]:(0.670034527779)\n",
      " state (4)  A[0]:(0.606190979481) A[1]:(0.679830551147) A[2]:(0.0118027208373) A[3]:(0.542825460434)\n",
      " state (5)  A[0]:(-0.114918142557) A[1]:(0.999631285667) A[2]:(-0.680524289608) A[3]:(-0.248852834105)\n",
      " state (6)  A[0]:(-0.0237889103591) A[1]:(0.70542371273) A[2]:(-0.0251556001604) A[3]:(0.658306717873)\n",
      " state (7)  A[0]:(0.427394509315) A[1]:(-0.819325149059) A[2]:(0.442766278982) A[3]:(0.891169428825)\n",
      " state (8)  A[0]:(0.673441529274) A[1]:(-0.0399404242635) A[2]:(0.754242479801) A[3]:(0.599313616753)\n",
      " state (9)  A[0]:(0.740784108639) A[1]:(0.84136235714) A[2]:(0.7658842206) A[3]:(0.0368272922933)\n",
      " state (10)  A[0]:(0.565659761429) A[1]:(0.792798876762) A[2]:(-0.0774856135249) A[3]:(0.372669160366)\n",
      " state (11)  A[0]:(-0.223453760147) A[1]:(0.735861539841) A[2]:(-0.871109724045) A[3]:(0.681976377964)\n",
      " state (12)  A[0]:(-0.542908370495) A[1]:(0.736478865147) A[2]:(-0.822616219521) A[3]:(0.801297962666)\n",
      " state (13)  A[0]:(0.00482790870592) A[1]:(0.812722802162) A[2]:(0.901785194874) A[3]:(0.786485314369)\n",
      " state (14)  A[0]:(0.81264102459) A[1]:(0.896180450916) A[2]:(0.999967813492) A[3]:(0.678873479366)\n",
      " state (15)  A[0]:(0.976532697678) A[1]:(0.908025801182) A[2]:(0.999999940395) A[3]:(0.64432156086)\n",
      "Episode 139000 finished after 0 timesteps with r=1.0. Running score: 0.74. Times trained:               6457. Times reached goal: 766.               Steps done: 1418064. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.217964403623.\n",
      "q_values \n",
      "tensor([[ 0.5496,  0.6060,  0.5298,  0.5497]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6007,  0.6724,  0.0051,  0.5421]], device='cuda:0')\n",
      "On state=4, selected action=2 , Random? True\n",
      "new state=5, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.549811065197) A[1]:(0.606053352356) A[2]:(0.529515862465) A[3]:(0.549480676651)\n",
      " state (1)  A[0]:(0.540304660797) A[1]:(-0.00055065745255) A[2]:(0.587314486504) A[3]:(0.532369792461)\n",
      " state (2)  A[0]:(0.557627677917) A[1]:(0.64815568924) A[2]:(0.505656898022) A[3]:(0.604885816574)\n",
      " state (3)  A[0]:(0.653097987175) A[1]:(0.00150379422121) A[2]:(0.392181277275) A[3]:(0.671732842922)\n",
      " state (4)  A[0]:(0.600931167603) A[1]:(0.672127664089) A[2]:(0.0049390392378) A[3]:(0.541959524155)\n",
      " state (5)  A[0]:(-0.1140845716) A[1]:(0.999633252621) A[2]:(-0.70168441534) A[3]:(-0.265954136848)\n",
      " state (6)  A[0]:(-0.0319399796426) A[1]:(0.724235773087) A[2]:(-0.0187311917543) A[3]:(0.641174256802)\n",
      " state (7)  A[0]:(0.422740519047) A[1]:(-0.79417437315) A[2]:(0.474145829678) A[3]:(0.883543431759)\n",
      " state (8)  A[0]:(0.669257760048) A[1]:(-0.0347944945097) A[2]:(0.748873770237) A[3]:(0.598305583)\n",
      " state (9)  A[0]:(0.737644255161) A[1]:(0.830619454384) A[2]:(0.751785874367) A[3]:(0.0491521321237)\n",
      " state (10)  A[0]:(0.574573218822) A[1]:(0.792373180389) A[2]:(-0.02947906591) A[3]:(0.361210763454)\n",
      " state (11)  A[0]:(-0.19263830781) A[1]:(0.747017145157) A[2]:(-0.837861418724) A[3]:(0.661296606064)\n",
      " state (12)  A[0]:(-0.521528422832) A[1]:(0.745329022408) A[2]:(-0.786737799644) A[3]:(0.789102554321)\n",
      " state (13)  A[0]:(0.00693393545225) A[1]:(0.80801486969) A[2]:(0.900953710079) A[3]:(0.788067817688)\n",
      " state (14)  A[0]:(0.80393487215) A[1]:(0.892248094082) A[2]:(0.999969065189) A[3]:(0.692768871784)\n",
      " state (15)  A[0]:(0.974967360497) A[1]:(0.912047505379) A[2]:(0.999999940395) A[3]:(0.65233027935)\n",
      "Episode 140000 finished after 0 timesteps with r=0.0. Running score: 0.73. Times trained:               6467. Times reached goal: 750.               Steps done: 1424531. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.216559375879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.546432316303) A[1]:(0.605182647705) A[2]:(0.529899060726) A[3]:(0.542344033718)\n",
      " state (1)  A[0]:(0.540632367134) A[1]:(-0.00211111898534) A[2]:(0.589354753494) A[3]:(0.525046110153)\n",
      " state (2)  A[0]:(0.551217556) A[1]:(0.656264305115) A[2]:(0.506115198135) A[3]:(0.598216414452)\n",
      " state (3)  A[0]:(0.649520993233) A[1]:(0.0126508427784) A[2]:(0.394633352757) A[3]:(0.664917886257)\n",
      " state (4)  A[0]:(0.602982103825) A[1]:(0.673067927361) A[2]:(0.00131583132315) A[3]:(0.532898962498)\n",
      " state (5)  A[0]:(-0.1042457968) A[1]:(0.999625921249) A[2]:(-0.719715297222) A[3]:(-0.28315231204)\n",
      " state (6)  A[0]:(-0.0318724252284) A[1]:(0.728100538254) A[2]:(8.64267349243e-05) A[3]:(0.623732924461)\n",
      " state (7)  A[0]:(0.427599281073) A[1]:(-0.768102169037) A[2]:(0.504383563995) A[3]:(0.87425661087)\n",
      " state (8)  A[0]:(0.670659184456) A[1]:(-0.0227785483003) A[2]:(0.744960486889) A[3]:(0.593038260937)\n",
      " state (9)  A[0]:(0.7326785326) A[1]:(0.829520046711) A[2]:(0.738500952721) A[3]:(0.0422171801329)\n",
      " state (10)  A[0]:(0.569071531296) A[1]:(0.802952051163) A[2]:(-0.00555390352383) A[3]:(0.334936916828)\n",
      " state (11)  A[0]:(-0.191983744502) A[1]:(0.766053140163) A[2]:(-0.813473701477) A[3]:(0.633292913437)\n",
      " state (12)  A[0]:(-0.517221331596) A[1]:(0.760456562042) A[2]:(-0.761032342911) A[3]:(0.77221763134)\n",
      " state (13)  A[0]:(0.0035143350251) A[1]:(0.810686230659) A[2]:(0.89794498682) A[3]:(0.783717572689)\n",
      " state (14)  A[0]:(0.798824071884) A[1]:(0.892880916595) A[2]:(0.999968886375) A[3]:(0.698328912258)\n",
      " state (15)  A[0]:(0.974321484566) A[1]:(0.918289542198) A[2]:(1.0) A[3]:(0.654057860374)\n",
      "Episode 141000 finished after 0 timesteps with r=1.0. Running score: 0.69. Times trained:               6447. Times reached goal: 758.               Steps done: 1430978. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.215167708443.\n",
      " state (0)  A[0]:(0.546576619148) A[1]:(0.604408860207) A[2]:(0.535361886024) A[3]:(0.546190440655)\n",
      " state (1)  A[0]:(0.539888858795) A[1]:(-0.00145897164475) A[2]:(0.593070149422) A[3]:(0.530705153942)\n",
      " state (2)  A[0]:(0.543631851673) A[1]:(0.655774593353) A[2]:(0.516606211662) A[3]:(0.602285146713)\n",
      " state (3)  A[0]:(0.644429206848) A[1]:(0.0103733688593) A[2]:(0.406510412693) A[3]:(0.667815864086)\n",
      " state (4)  A[0]:(0.603030443192) A[1]:(0.668216228485) A[2]:(0.00496812537313) A[3]:(0.536155879498)\n",
      " state (5)  A[0]:(-0.0947073996067) A[1]:(0.999620318413) A[2]:(-0.735899090767) A[3]:(-0.291503220797)\n",
      " state (6)  A[0]:(-0.0320274792612) A[1]:(0.734027385712) A[2]:(0.0147158009931) A[3]:(0.607262134552)\n",
      " state (7)  A[0]:(0.426953017712) A[1]:(-0.75042951107) A[2]:(0.531857728958) A[3]:(0.866758942604)\n",
      " state (8)  A[0]:(0.664579987526) A[1]:(-0.0208901464939) A[2]:(0.746206581593) A[3]:(0.594963788986)\n",
      " state (9)  A[0]:(0.720770120621) A[1]:(0.829670011997) A[2]:(0.732673048973) A[3]:(0.0502533726394)\n",
      " state (10)  A[0]:(0.5606123209) A[1]:(0.811335861683) A[2]:(0.0179487261921) A[3]:(0.330448508263)\n",
      " state (11)  A[0]:(-0.190065994859) A[1]:(0.778854846954) A[2]:(-0.793313682079) A[3]:(0.623436808586)\n",
      " state (12)  A[0]:(-0.514573216438) A[1]:(0.772021710873) A[2]:(-0.73934674263) A[3]:(0.765707790852)\n",
      " state (13)  A[0]:(0.00117653550114) A[1]:(0.815672934055) A[2]:(0.899347305298) A[3]:(0.783802270889)\n",
      " state (14)  A[0]:(0.797772943974) A[1]:(0.89467972517) A[2]:(0.999969780445) A[3]:(0.705201387405)\n",
      " state (15)  A[0]:(0.974654555321) A[1]:(0.921790540218) A[2]:(1.0) A[3]:(0.66061258316)\n",
      "Episode 142000 finished after 0 timesteps with r=1.0. Running score: 0.77. Times trained:               6416. Times reached goal: 770.               Steps done: 1437394. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.213791611665.\n",
      " state (0)  A[0]:(0.54120272398) A[1]:(0.605056166649) A[2]:(0.537242054939) A[3]:(0.541904568672)\n",
      " state (1)  A[0]:(0.537860393524) A[1]:(7.4066221714e-05) A[2]:(0.597990393639) A[3]:(0.526276350021)\n",
      " state (2)  A[0]:(0.536084055901) A[1]:(0.661497950554) A[2]:(0.528011322021) A[3]:(0.597683310509)\n",
      " state (3)  A[0]:(0.639105558395) A[1]:(0.0228176210076) A[2]:(0.417140573263) A[3]:(0.664000034332)\n",
      " state (4)  A[0]:(0.602277219296) A[1]:(0.668372869492) A[2]:(0.0051027094014) A[3]:(0.534951627254)\n",
      " state (5)  A[0]:(-0.091245546937) A[1]:(0.999605357647) A[2]:(-0.752540946007) A[3]:(-0.291505038738)\n",
      " state (6)  A[0]:(-0.0357762426138) A[1]:(0.728230118752) A[2]:(0.0170356240124) A[3]:(0.60067397356)\n",
      " state (7)  A[0]:(0.431494444609) A[1]:(-0.735872983932) A[2]:(0.540103316307) A[3]:(0.86223256588)\n",
      " state (8)  A[0]:(0.667883753777) A[1]:(-0.0234938170761) A[2]:(0.743832826614) A[3]:(0.597659528255)\n",
      " state (9)  A[0]:(0.721596002579) A[1]:(0.824883878231) A[2]:(0.728402137756) A[3]:(0.0564842373133)\n",
      " state (10)  A[0]:(0.572286963463) A[1]:(0.810087800026) A[2]:(0.0292658656836) A[3]:(0.331250905991)\n",
      " state (11)  A[0]:(-0.16426859796) A[1]:(0.777972698212) A[2]:(-0.782255351543) A[3]:(0.620927155018)\n",
      " state (12)  A[0]:(-0.499031841755) A[1]:(0.770306110382) A[2]:(-0.725234508514) A[3]:(0.763533711433)\n",
      " state (13)  A[0]:(0.0108396932483) A[1]:(0.81278437376) A[2]:(0.903469800949) A[3]:(0.784240365028)\n",
      " state (14)  A[0]:(0.799474537373) A[1]:(0.892180621624) A[2]:(0.999971866608) A[3]:(0.709421873093)\n",
      " state (15)  A[0]:(0.975076079369) A[1]:(0.920059859753) A[2]:(1.0) A[3]:(0.667037844658)\n",
      "Episode 143000 finished after 0 timesteps with r=1.0. Running score: 0.77. Times trained:               6283. Times reached goal: 727.               Steps done: 1443677. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.212452569973.\n",
      " state (0)  A[0]:(0.543841362) A[1]:(0.603024482727) A[2]:(0.538369059563) A[3]:(0.544157624245)\n",
      " state (1)  A[0]:(0.539397001266) A[1]:(-0.000813752238173) A[2]:(0.597295284271) A[3]:(0.5248234272)\n",
      " state (2)  A[0]:(0.530586004257) A[1]:(0.657486319542) A[2]:(0.5350664258) A[3]:(0.593337655067)\n",
      " state (3)  A[0]:(0.635132312775) A[1]:(0.0242211818695) A[2]:(0.423806607723) A[3]:(0.660593628883)\n",
      " state (4)  A[0]:(0.60293340683) A[1]:(0.669657289982) A[2]:(0.0030963323079) A[3]:(0.533423960209)\n",
      " state (5)  A[0]:(-0.080615632236) A[1]:(0.999603271484) A[2]:(-0.766565680504) A[3]:(-0.297219723463)\n",
      " state (6)  A[0]:(-0.0308155715466) A[1]:(0.731426119804) A[2]:(0.0165577307343) A[3]:(0.586593806744)\n",
      " state (7)  A[0]:(0.439964950085) A[1]:(-0.721256315708) A[2]:(0.547625601292) A[3]:(0.855481624603)\n",
      " state (8)  A[0]:(0.670366644859) A[1]:(-0.0169847663492) A[2]:(0.742497384548) A[3]:(0.592617332935)\n",
      " state (9)  A[0]:(0.721197724342) A[1]:(0.824872195721) A[2]:(0.724670708179) A[3]:(0.0516438484192)\n",
      " state (10)  A[0]:(0.587003469467) A[1]:(0.812597453594) A[2]:(0.033127579838) A[3]:(0.327444821596)\n",
      " state (11)  A[0]:(-0.130925297737) A[1]:(0.780126333237) A[2]:(-0.779571592808) A[3]:(0.618158102036)\n",
      " state (12)  A[0]:(-0.482941299677) A[1]:(0.771815419197) A[2]:(-0.728537082672) A[3]:(0.76135802269)\n",
      " state (13)  A[0]:(0.0147961452603) A[1]:(0.812939941883) A[2]:(0.89961284399) A[3]:(0.78320145607)\n",
      " state (14)  A[0]:(0.799034714699) A[1]:(0.891088962555) A[2]:(0.999971270561) A[3]:(0.70936602354)\n",
      " state (15)  A[0]:(0.975320577621) A[1]:(0.918507099152) A[2]:(1.0) A[3]:(0.667376279831)\n",
      "Episode 144000 finished after 0 timesteps with r=1.0. Running score: 0.8. Times trained:               6560. Times reached goal: 770.               Steps done: 1450237. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.211063442434.\n",
      "q_values \n",
      "tensor([[ 0.5407,  0.6050,  0.5400,  0.5410]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6037,  0.6681,  0.0067,  0.5330]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6663, -0.0156,  0.7409,  0.5970]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7059,  0.8247,  0.7229,  0.0545]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0031,  0.8161,  0.8991,  0.7867]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7951,  0.8916,  1.0000,  0.7134]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.54026722908) A[1]:(0.604009747505) A[2]:(0.538573026657) A[3]:(0.540605425835)\n",
      " state (1)  A[0]:(0.538279891014) A[1]:(-0.00107670528814) A[2]:(0.599030017853) A[3]:(0.518778681755)\n",
      " state (2)  A[0]:(0.523653388023) A[1]:(0.659816086292) A[2]:(0.542886257172) A[3]:(0.584731578827)\n",
      " state (3)  A[0]:(0.629555165768) A[1]:(0.033314999193) A[2]:(0.430567890406) A[3]:(0.653343677521)\n",
      " state (4)  A[0]:(0.602112233639) A[1]:(0.667215704918) A[2]:(0.00337504060008) A[3]:(0.531363010406)\n",
      " state (5)  A[0]:(-0.0773836225271) A[1]:(0.999596595764) A[2]:(-0.780365109444) A[3]:(-0.294464707375)\n",
      " state (6)  A[0]:(-0.0365503244102) A[1]:(0.732892632484) A[2]:(0.0041721817106) A[3]:(0.580206990242)\n",
      " state (7)  A[0]:(0.438553601503) A[1]:(-0.708258509636) A[2]:(0.54384458065) A[3]:(0.852167487144)\n",
      " state (8)  A[0]:(0.663243055344) A[1]:(-0.0158339682966) A[2]:(0.739686727524) A[3]:(0.593912661076)\n",
      " state (9)  A[0]:(0.703676462173) A[1]:(0.824888765812) A[2]:(0.722306013107) A[3]:(0.0501135364175)\n",
      " state (10)  A[0]:(0.561365485191) A[1]:(0.818156421185) A[2]:(0.0224688369781) A[3]:(0.325675487518)\n",
      " state (11)  A[0]:(-0.164923846722) A[1]:(0.788003385067) A[2]:(-0.785680532455) A[3]:(0.618158817291)\n",
      " state (12)  A[0]:(-0.503177821636) A[1]:(0.778491139412) A[2]:(-0.7340285182) A[3]:(0.762376904488)\n",
      " state (13)  A[0]:(-0.00441776309162) A[1]:(0.816533327103) A[2]:(0.899968862534) A[3]:(0.785112202168)\n",
      " state (14)  A[0]:(0.795094966888) A[1]:(0.891649365425) A[2]:(0.999972701073) A[3]:(0.712455749512)\n",
      " state (15)  A[0]:(0.975303173065) A[1]:(0.917566299438) A[2]:(1.0) A[3]:(0.671629667282)\n",
      "Episode 145000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained:               6533. Times reached goal: 785.               Steps done: 1456770. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.209689059275.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.541141748428) A[1]:(0.5994322896) A[2]:(0.538497149944) A[3]:(0.542191326618)\n",
      " state (1)  A[0]:(0.537945330143) A[1]:(0.000381320685847) A[2]:(0.598175287247) A[3]:(0.518375754356)\n",
      " state (2)  A[0]:(0.51657307148) A[1]:(0.661029815674) A[2]:(0.548994719982) A[3]:(0.579997301102)\n",
      " state (3)  A[0]:(0.623262643814) A[1]:(0.0393955260515) A[2]:(0.435349017382) A[3]:(0.648938596249)\n",
      " state (4)  A[0]:(0.599638402462) A[1]:(0.668207645416) A[2]:(0.00264119496569) A[3]:(0.530079007149)\n",
      " state (5)  A[0]:(-0.072245888412) A[1]:(0.999597728252) A[2]:(-0.789746522903) A[3]:(-0.294638514519)\n",
      " state (6)  A[0]:(-0.0360987782478) A[1]:(0.73670899868) A[2]:(-0.00411400850862) A[3]:(0.56968152523)\n",
      " state (7)  A[0]:(0.44427421689) A[1]:(-0.698144316673) A[2]:(0.543836951256) A[3]:(0.847264766693)\n",
      " state (8)  A[0]:(0.663568854332) A[1]:(-0.010516166687) A[2]:(0.74049282074) A[3]:(0.588874995708)\n",
      " state (9)  A[0]:(0.700586855412) A[1]:(0.82531362772) A[2]:(0.725213766098) A[3]:(0.0448315739632)\n",
      " state (10)  A[0]:(0.570469856262) A[1]:(0.821432411671) A[2]:(0.0301375892013) A[3]:(0.325701653957)\n",
      " state (11)  A[0]:(-0.141452714801) A[1]:(0.79263561964) A[2]:(-0.784566700459) A[3]:(0.619967460632)\n",
      " state (12)  A[0]:(-0.490996897221) A[1]:(0.783772528172) A[2]:(-0.735253989697) A[3]:(0.762786626816)\n",
      " state (13)  A[0]:(-0.000605419219937) A[1]:(0.820627331734) A[2]:(0.90075725317) A[3]:(0.78369820118)\n",
      " state (14)  A[0]:(0.794805943966) A[1]:(0.892621815205) A[2]:(0.999973893166) A[3]:(0.708169817924)\n",
      " state (15)  A[0]:(0.975409567356) A[1]:(0.916395187378) A[2]:(1.0) A[3]:(0.665872097015)\n",
      "Episode 146000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained:               6535. Times reached goal: 791.               Steps done: 1463305. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.208323209049.\n",
      " state (0)  A[0]:(0.541332781315) A[1]:(0.598881423473) A[2]:(0.534708321095) A[3]:(0.540231347084)\n",
      " state (1)  A[0]:(0.538516104221) A[1]:(0.000145211815834) A[2]:(0.594723343849) A[3]:(0.515218675137)\n",
      " state (2)  A[0]:(0.513092517853) A[1]:(0.663915395737) A[2]:(0.55113607645) A[3]:(0.572852134705)\n",
      " state (3)  A[0]:(0.620213806629) A[1]:(0.0454943776131) A[2]:(0.434829562902) A[3]:(0.642792761326)\n",
      " state (4)  A[0]:(0.600412130356) A[1]:(0.668140649796) A[2]:(-0.00321613624692) A[3]:(0.528746128082)\n",
      " state (5)  A[0]:(-0.0627738311887) A[1]:(0.999598681927) A[2]:(-0.79838681221) A[3]:(-0.287193715572)\n",
      " state (6)  A[0]:(-0.0307817198336) A[1]:(0.740133941174) A[2]:(-0.0143179642037) A[3]:(0.56761944294)\n",
      " state (7)  A[0]:(0.454074084759) A[1]:(-0.688809633255) A[2]:(0.540205359459) A[3]:(0.846177339554)\n",
      " state (8)  A[0]:(0.667850077152) A[1]:(-0.00782019365579) A[2]:(0.738863050938) A[3]:(0.588185071945)\n",
      " state (9)  A[0]:(0.702938079834) A[1]:(0.823841094971) A[2]:(0.725614964962) A[3]:(0.0409171581268)\n",
      " state (10)  A[0]:(0.588120102882) A[1]:(0.82189142704) A[2]:(0.0313009247184) A[3]:(0.329200714827)\n",
      " state (11)  A[0]:(-0.106621958315) A[1]:(0.794092357159) A[2]:(-0.787388086319) A[3]:(0.626173853874)\n",
      " state (12)  A[0]:(-0.474832892418) A[1]:(0.786162376404) A[2]:(-0.743773937225) A[3]:(0.767111718655)\n",
      " state (13)  A[0]:(0.00369645468891) A[1]:(0.821996033192) A[2]:(0.897686004639) A[3]:(0.786688089371)\n",
      " state (14)  A[0]:(0.794650971889) A[1]:(0.891208171844) A[2]:(0.999973773956) A[3]:(0.710985779762)\n",
      " state (15)  A[0]:(0.97564214468) A[1]:(0.912590920925) A[2]:(1.0) A[3]:(0.669641256332)\n",
      "Episode 147000 finished after 0 timesteps with r=1.0. Running score: 0.7. Times trained:               6452. Times reached goal: 775.               Steps done: 1469757. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.206983434465.\n",
      " state (0)  A[0]:(0.534867882729) A[1]:(0.599551439285) A[2]:(0.532725691795) A[3]:(0.534257769585)\n",
      " state (1)  A[0]:(0.536345362663) A[1]:(-0.000177957117558) A[2]:(0.594191193581) A[3]:(0.504305839539)\n",
      " state (2)  A[0]:(0.508333563805) A[1]:(0.666171729565) A[2]:(0.55419421196) A[3]:(0.556715369225)\n",
      " state (3)  A[0]:(0.616120040417) A[1]:(0.053848721087) A[2]:(0.436800807714) A[3]:(0.629695892334)\n",
      " state (4)  A[0]:(0.600301980972) A[1]:(0.665959239006) A[2]:(7.02142715454e-05) A[3]:(0.523418188095)\n",
      " state (5)  A[0]:(-0.0603186935186) A[1]:(0.999595880508) A[2]:(-0.801439285278) A[3]:(-0.278382211924)\n",
      " state (6)  A[0]:(-0.036251373589) A[1]:(0.741357386112) A[2]:(-0.0148682361469) A[3]:(0.566301167011)\n",
      " state (7)  A[0]:(0.454135715961) A[1]:(-0.679848909378) A[2]:(0.539899468422) A[3]:(0.845380961895)\n",
      " state (8)  A[0]:(0.663504421711) A[1]:(-0.00492411619052) A[2]:(0.739916205406) A[3]:(0.589632153511)\n",
      " state (9)  A[0]:(0.693314909935) A[1]:(0.82337731123) A[2]:(0.72874712944) A[3]:(0.0415026433766)\n",
      " state (10)  A[0]:(0.583943724632) A[1]:(0.824500918388) A[2]:(0.0325008556247) A[3]:(0.337395220995)\n",
      " state (11)  A[0]:(-0.104576267302) A[1]:(0.798475503922) A[2]:(-0.790155768394) A[3]:(0.636517882347)\n",
      " state (12)  A[0]:(-0.474546432495) A[1]:(0.790175080299) A[2]:(-0.747275352478) A[3]:(0.775228440762)\n",
      " state (13)  A[0]:(0.00083743018331) A[1]:(0.822484731674) A[2]:(0.89916074276) A[3]:(0.793935537338)\n",
      " state (14)  A[0]:(0.794630110264) A[1]:(0.887362599373) A[2]:(0.999975085258) A[3]:(0.72021818161)\n",
      " state (15)  A[0]:(0.976012289524) A[1]:(0.90517783165) A[2]:(1.0) A[3]:(0.681613504887)\n",
      "Episode 148000 finished after 0 timesteps with r=1.0. Running score: 0.77. Times trained:               6450. Times reached goal: 756.               Steps done: 1476207. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.205652687585.\n",
      " state (0)  A[0]:(0.537029743195) A[1]:(0.60113286972) A[2]:(0.538629353046) A[3]:(0.539429783821)\n",
      " state (1)  A[0]:(0.538829803467) A[1]:(0.00428037717938) A[2]:(0.60152053833) A[3]:(0.506789982319)\n",
      " state (2)  A[0]:(0.50709939003) A[1]:(0.667923212051) A[2]:(0.560164988041) A[3]:(0.550554990768)\n",
      " state (3)  A[0]:(0.61389118433) A[1]:(0.0616676062346) A[2]:(0.440322935581) A[3]:(0.624547660351)\n",
      " state (4)  A[0]:(0.600989460945) A[1]:(0.667865812778) A[2]:(0.00178050808609) A[3]:(0.526162147522)\n",
      " state (5)  A[0]:(-0.0517353340983) A[1]:(0.999598443508) A[2]:(-0.805621743202) A[3]:(-0.256714850664)\n",
      " state (6)  A[0]:(-0.0312385521829) A[1]:(0.742617607117) A[2]:(-0.0275306254625) A[3]:(0.572683215141)\n",
      " state (7)  A[0]:(0.463589847088) A[1]:(-0.675784468651) A[2]:(0.530231118202) A[3]:(0.847155570984)\n",
      " state (8)  A[0]:(0.667913079262) A[1]:(-0.0113269463181) A[2]:(0.736577868462) A[3]:(0.591789245605)\n",
      " state (9)  A[0]:(0.69590651989) A[1]:(0.819230139256) A[2]:(0.729206204414) A[3]:(0.0384118705988)\n",
      " state (10)  A[0]:(0.599504947662) A[1]:(0.824756860733) A[2]:(0.0310875102878) A[3]:(0.340198308229)\n",
      " state (11)  A[0]:(-0.073327742517) A[1]:(0.803460776806) A[2]:(-0.793571412563) A[3]:(0.640842676163)\n",
      " state (12)  A[0]:(-0.458762824535) A[1]:(0.798034787178) A[2]:(-0.7519723773) A[3]:(0.776970744133)\n",
      " state (13)  A[0]:(0.00720580853522) A[1]:(0.827163279057) A[2]:(0.90008187294) A[3]:(0.79312056303)\n",
      " state (14)  A[0]:(0.795241117477) A[1]:(0.885145783424) A[2]:(0.999975979328) A[3]:(0.717386603355)\n",
      " state (15)  A[0]:(0.976248383522) A[1]:(0.89720261097) A[2]:(1.0) A[3]:(0.679911851883)\n",
      "Episode 149000 finished after 0 timesteps with r=0.0. Running score: 0.8. Times trained:               6405. Times reached goal: 776.               Steps done: 1482612. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.20433969148.\n",
      "q_values \n",
      "tensor([[ 0.5390,  0.5967,  0.5455,  0.5385]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6007,  0.6644,  0.0022,  0.5285]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6677, -0.0091,  0.7371,  0.5932]], device='cuda:0')\n",
      "On state=8, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6688, -0.0093,  0.7377,  0.5937]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6963,  0.8191,  0.7363,  0.0400]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0182,  0.8532,  0.8999,  0.8004]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8002,  0.8961,  1.0000,  0.7265]], device='cuda:0')\n",
      "On state=14, selected action=3 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6158,  0.8340,  0.0381,  0.3524]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7991,  0.8961,  1.0000,  0.7255]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.538662791252) A[1]:(0.596801877022) A[2]:(0.546600997448) A[3]:(0.538823306561)\n",
      " state (1)  A[0]:(0.538743495941) A[1]:(0.00225118547678) A[2]:(0.607262015343) A[3]:(0.507733464241)\n",
      " state (2)  A[0]:(0.503341138363) A[1]:(0.675647616386) A[2]:(0.563621640205) A[3]:(0.542487978935)\n",
      " state (3)  A[0]:(0.609734654427) A[1]:(0.0743889212608) A[2]:(0.439464390278) A[3]:(0.617283463478)\n",
      " state (4)  A[0]:(0.600711882114) A[1]:(0.664081573486) A[2]:(0.0039416346699) A[3]:(0.528441905975)\n",
      " state (5)  A[0]:(-0.0462700016797) A[1]:(0.999598622322) A[2]:(-0.805318415165) A[3]:(-0.235259488225)\n",
      " state (6)  A[0]:(-0.0275447405875) A[1]:(0.74592590332) A[2]:(-0.0359526835382) A[3]:(0.578271627426)\n",
      " state (7)  A[0]:(0.471987038851) A[1]:(-0.670596122742) A[2]:(0.52259105444) A[3]:(0.849202394485)\n",
      " state (8)  A[0]:(0.670057892799) A[1]:(-0.00970948860049) A[2]:(0.738152265549) A[3]:(0.592520356178)\n",
      " state (9)  A[0]:(0.697184085846) A[1]:(0.818621039391) A[2]:(0.736431837082) A[3]:(0.0379157923162)\n",
      " state (10)  A[0]:(0.61438280344) A[1]:(0.833657741547) A[2]:(0.0363271050155) A[3]:(0.351479232311)\n",
      " state (11)  A[0]:(-0.0421013981104) A[1]:(0.825643062592) A[2]:(-0.798112988472) A[3]:(0.653727769852)\n",
      " state (12)  A[0]:(-0.442879766226) A[1]:(0.828851401806) A[2]:(-0.760095238686) A[3]:(0.785665690899)\n",
      " state (13)  A[0]:(0.0154584553093) A[1]:(0.852873444557) A[2]:(0.899201750755) A[3]:(0.799533486366)\n",
      " state (14)  A[0]:(0.79842787981) A[1]:(0.895832180977) A[2]:(0.999976038933) A[3]:(0.725558876991)\n",
      " state (15)  A[0]:(0.977161765099) A[1]:(0.898186504841) A[2]:(1.0) A[3]:(0.692611455917)\n",
      "Episode 150000 finished after 0 timesteps with r=1.0. Running score: 0.73. Times trained:               6474. Times reached goal: 773.               Steps done: 1489086. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.203021069303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.537417709827) A[1]:(0.600171804428) A[2]:(0.546798348427) A[3]:(0.537755608559)\n",
      " state (1)  A[0]:(0.540374815464) A[1]:(0.00398940732703) A[2]:(0.609711587429) A[3]:(0.506675839424)\n",
      " state (2)  A[0]:(0.504884719849) A[1]:(0.674034237862) A[2]:(0.563156247139) A[3]:(0.531329810619)\n",
      " state (3)  A[0]:(0.609074056149) A[1]:(0.0750735551119) A[2]:(0.434215992689) A[3]:(0.607990562916)\n",
      " state (4)  A[0]:(0.601000905037) A[1]:(0.662428855896) A[2]:(0.00256895460188) A[3]:(0.526164293289)\n",
      " state (5)  A[0]:(-0.0449275225401) A[1]:(0.999607980251) A[2]:(-0.800484001637) A[3]:(-0.224865213037)\n",
      " state (6)  A[0]:(-0.031719263643) A[1]:(0.752242028713) A[2]:(-0.0396261028945) A[3]:(0.576106190681)\n",
      " state (7)  A[0]:(0.46981421113) A[1]:(-0.665069937706) A[2]:(0.512971878052) A[3]:(0.848360776901)\n",
      " state (8)  A[0]:(0.659380197525) A[1]:(-0.00507734809071) A[2]:(0.737032055855) A[3]:(0.588446855545)\n",
      " state (9)  A[0]:(0.681080639362) A[1]:(0.818899810314) A[2]:(0.738893270493) A[3]:(0.033464834094)\n",
      " state (10)  A[0]:(0.604306519032) A[1]:(0.838376045227) A[2]:(0.0249810684472) A[3]:(0.359610080719)\n",
      " state (11)  A[0]:(-0.0495240241289) A[1]:(0.83584779501) A[2]:(-0.808348298073) A[3]:(0.663460254669)\n",
      " state (12)  A[0]:(-0.449734151363) A[1]:(0.840632140636) A[2]:(-0.769622027874) A[3]:(0.79136300087)\n",
      " state (13)  A[0]:(0.0066277012229) A[1]:(0.858382642269) A[2]:(0.900869846344) A[3]:(0.80198943615)\n",
      " state (14)  A[0]:(0.79717373848) A[1]:(0.890437185764) A[2]:(0.999976694584) A[3]:(0.727650761604)\n",
      " state (15)  A[0]:(0.977376937866) A[1]:(0.881029725075) A[2]:(1.0) A[3]:(0.69895029068)\n",
      "Episode 151000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained:               6394. Times reached goal: 788.               Steps done: 1495480. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.201727093834.\n",
      " state (0)  A[0]:(0.53491795063) A[1]:(0.597916483879) A[2]:(0.547952651978) A[3]:(0.537648081779)\n",
      " state (1)  A[0]:(0.540486693382) A[1]:(0.00139991845936) A[2]:(0.609252810478) A[3]:(0.509773671627)\n",
      " state (2)  A[0]:(0.504738986492) A[1]:(0.676775455475) A[2]:(0.564143419266) A[3]:(0.525212883949)\n",
      " state (3)  A[0]:(0.606349468231) A[1]:(0.0797483623028) A[2]:(0.430376946926) A[3]:(0.602048397064)\n",
      " state (4)  A[0]:(0.599052190781) A[1]:(0.661087036133) A[2]:(0.00249945605174) A[3]:(0.526534795761)\n",
      " state (5)  A[0]:(-0.0437077283859) A[1]:(0.999613404274) A[2]:(-0.793928742409) A[3]:(-0.209796860814)\n",
      " state (6)  A[0]:(-0.0304250661284) A[1]:(0.754778623581) A[2]:(-0.0440218672156) A[3]:(0.577928423882)\n",
      " state (7)  A[0]:(0.476571857929) A[1]:(-0.664784431458) A[2]:(0.501111030579) A[3]:(0.8495439291)\n",
      " state (8)  A[0]:(0.658184409142) A[1]:(-0.00837957579643) A[2]:(0.734556794167) A[3]:(0.589763641357)\n",
      " state (9)  A[0]:(0.676239967346) A[1]:(0.818181872368) A[2]:(0.741317987442) A[3]:(0.0348240807652)\n",
      " state (10)  A[0]:(0.61118555069) A[1]:(0.841943919659) A[2]:(0.0182178132236) A[3]:(0.371376723051)\n",
      " state (11)  A[0]:(-0.0336108207703) A[1]:(0.844249665737) A[2]:(-0.816900730133) A[3]:(0.673638820648)\n",
      " state (12)  A[0]:(-0.44828376174) A[1]:(0.851027071476) A[2]:(-0.781212449074) A[3]:(0.795879364014)\n",
      " state (13)  A[0]:(-0.00396303227171) A[1]:(0.864212334156) A[2]:(0.898923575878) A[3]:(0.802364110947)\n",
      " state (14)  A[0]:(0.792748093605) A[1]:(0.886656343937) A[2]:(0.999976158142) A[3]:(0.726033091545)\n",
      " state (15)  A[0]:(0.977045178413) A[1]:(0.865400373936) A[2]:(1.0) A[3]:(0.699996590614)\n",
      "Episode 152000 finished after 0 timesteps with r=1.0. Running score: 0.8. Times trained:               6456. Times reached goal: 774.               Steps done: 1501936. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.20042893867.\n",
      " state (0)  A[0]:(0.534912467003) A[1]:(0.592584371567) A[2]:(0.551874518394) A[3]:(0.536915063858)\n",
      " state (1)  A[0]:(0.540143847466) A[1]:(0.000132162123919) A[2]:(0.612172544003) A[3]:(0.516327500343)\n",
      " state (2)  A[0]:(0.504537582397) A[1]:(0.680828034878) A[2]:(0.568130254745) A[3]:(0.52455163002)\n",
      " state (3)  A[0]:(0.603830695152) A[1]:(0.0796376392245) A[2]:(0.429303854704) A[3]:(0.600442886353)\n",
      " state (4)  A[0]:(0.595776438713) A[1]:(0.66376209259) A[2]:(0.00157117715571) A[3]:(0.528076767921)\n",
      " state (5)  A[0]:(-0.0425279960036) A[1]:(0.99962669611) A[2]:(-0.785898566246) A[3]:(-0.194122105837)\n",
      " state (6)  A[0]:(-0.0239849574864) A[1]:(0.759555757046) A[2]:(-0.0402463115752) A[3]:(0.582714676857)\n",
      " state (7)  A[0]:(0.488893121481) A[1]:(-0.666282176971) A[2]:(0.499436199665) A[3]:(0.852632045746)\n",
      " state (8)  A[0]:(0.663409233093) A[1]:(-0.00757528888062) A[2]:(0.737851262093) A[3]:(0.591773927212)\n",
      " state (9)  A[0]:(0.681932806969) A[1]:(0.817664980888) A[2]:(0.745843052864) A[3]:(0.0419859960675)\n",
      " state (10)  A[0]:(0.628975868225) A[1]:(0.84423750639) A[2]:(0.0112749328837) A[3]:(0.393657803535)\n",
      " state (11)  A[0]:(-0.00265135732479) A[1]:(0.850060164928) A[2]:(-0.825506091118) A[3]:(0.69311195612)\n",
      " state (12)  A[0]:(-0.431263178587) A[1]:(0.857765316963) A[2]:(-0.791637778282) A[3]:(0.808860659599)\n",
      " state (13)  A[0]:(0.0121542485431) A[1]:(0.866994082928) A[2]:(0.897838830948) A[3]:(0.813205897808)\n",
      " state (14)  A[0]:(0.799970030785) A[1]:(0.881454706192) A[2]:(0.999975919724) A[3]:(0.740431666374)\n",
      " state (15)  A[0]:(0.978164315224) A[1]:(0.848647356033) A[2]:(1.0) A[3]:(0.719069778919)\n",
      "Episode 153000 finished after 0 timesteps with r=0.0. Running score: 0.81. Times trained:               6343. Times reached goal: 782.               Steps done: 1508279. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.199161641394.\n",
      " state (0)  A[0]:(0.535238683224) A[1]:(0.596832156181) A[2]:(0.554772019386) A[3]:(0.533428311348)\n",
      " state (1)  A[0]:(0.547648429871) A[1]:(-0.00174955662806) A[2]:(0.617769241333) A[3]:(0.520369291306)\n",
      " state (2)  A[0]:(0.515953540802) A[1]:(0.680544555187) A[2]:(0.574853003025) A[3]:(0.520266294479)\n",
      " state (3)  A[0]:(0.610346674919) A[1]:(0.0706494599581) A[2]:(0.431230813265) A[3]:(0.594057142735)\n",
      " state (4)  A[0]:(0.599310159683) A[1]:(0.661762833595) A[2]:(0.00270377937704) A[3]:(0.522809505463)\n",
      " state (5)  A[0]:(-0.0401067957282) A[1]:(0.999628245831) A[2]:(-0.778714537621) A[3]:(-0.18572050333)\n",
      " state (6)  A[0]:(-0.0245035402477) A[1]:(0.758161246777) A[2]:(-0.0464164018631) A[3]:(0.582364499569)\n",
      " state (7)  A[0]:(0.491817474365) A[1]:(-0.666322827339) A[2]:(0.484865665436) A[3]:(0.851904571056)\n",
      " state (8)  A[0]:(0.657132029533) A[1]:(-0.00251225405373) A[2]:(0.734468460083) A[3]:(0.581176996231)\n",
      " state (9)  A[0]:(0.673569500446) A[1]:(0.818454802036) A[2]:(0.747354269028) A[3]:(0.022728908807)\n",
      " state (10)  A[0]:(0.632262706757) A[1]:(0.847798168659) A[2]:(0.00913558807224) A[3]:(0.386141747236)\n",
      " state (11)  A[0]:(0.00703394738957) A[1]:(0.857244729996) A[2]:(-0.829030811787) A[3]:(0.689037024975)\n",
      " state (12)  A[0]:(-0.434588998556) A[1]:(0.866825819016) A[2]:(-0.79373216629) A[3]:(0.802956223488)\n",
      " state (13)  A[0]:(-0.00142815615982) A[1]:(0.873774647713) A[2]:(0.901773035526) A[3]:(0.803541064262)\n",
      " state (14)  A[0]:(0.795423686504) A[1]:(0.881186425686) A[2]:(0.999976813793) A[3]:(0.727080345154)\n",
      " state (15)  A[0]:(0.97779583931) A[1]:(0.837807476521) A[2]:(1.0) A[3]:(0.70972931385)\n",
      "Episode 154000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained:               6430. Times reached goal: 795.               Steps done: 1514709. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.197885140389.\n",
      "q_values \n",
      "tensor([[ 0.5342,  0.5951,  0.5547,  0.5371]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5976,  0.6615,  0.0024,  0.5331]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6634, -0.0035,  0.7357,  0.5946]], device='cuda:0')\n",
      "On state=8, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6626, -0.0027,  0.7359,  0.5943]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6762,  0.8184,  0.7510,  0.0389]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0030,  0.8759,  0.9010,  0.8103]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7992,  0.8768,  1.0000,  0.7387]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533365666866) A[1]:(0.594773769379) A[2]:(0.553619027138) A[3]:(0.537015914917)\n",
      " state (1)  A[0]:(0.544761240482) A[1]:(-0.00171651528217) A[2]:(0.615899384022) A[3]:(0.536566257477)\n",
      " state (2)  A[0]:(0.516834497452) A[1]:(0.682961702347) A[2]:(0.575227618217) A[3]:(0.531879782677)\n",
      " state (3)  A[0]:(0.608980774879) A[1]:(0.064506098628) A[2]:(0.427355617285) A[3]:(0.60257101059)\n",
      " state (4)  A[0]:(0.596764445305) A[1]:(0.659875392914) A[2]:(0.0021003454458) A[3]:(0.534542798996)\n",
      " state (5)  A[0]:(-0.0420227982104) A[1]:(0.999636530876) A[2]:(-0.767292737961) A[3]:(-0.158244863153)\n",
      " state (6)  A[0]:(-0.0234779175371) A[1]:(0.762569487095) A[2]:(-0.0392466560006) A[3]:(0.595968306065)\n",
      " state (7)  A[0]:(0.50125426054) A[1]:(-0.664844751358) A[2]:(0.483357608318) A[3]:(0.858847916126)\n",
      " state (8)  A[0]:(0.66163623333) A[1]:(-0.0045550968498) A[2]:(0.736391365528) A[3]:(0.595913946629)\n",
      " state (9)  A[0]:(0.676574707031) A[1]:(0.817194283009) A[2]:(0.751248478889) A[3]:(0.0432107634842)\n",
      " state (10)  A[0]:(0.645657002926) A[1]:(0.849365770817) A[2]:(0.00765535188839) A[3]:(0.412089973688)\n",
      " state (11)  A[0]:(0.0303385406733) A[1]:(0.861105799675) A[2]:(-0.833928227425) A[3]:(0.70593470335)\n",
      " state (12)  A[0]:(-0.425023019314) A[1]:(0.871001005173) A[2]:(-0.80014693737) A[3]:(0.812593758106)\n",
      " state (13)  A[0]:(0.0050497520715) A[1]:(0.87512254715) A[2]:(0.901354849339) A[3]:(0.811686873436)\n",
      " state (14)  A[0]:(0.79933321476) A[1]:(0.876497387886) A[2]:(0.999976694584) A[3]:(0.739610135555)\n",
      " state (15)  A[0]:(0.978508889675) A[1]:(0.822699368) A[2]:(1.0) A[3]:(0.727366030216)\n",
      "Episode 155000 finished after 0 timesteps with r=1.0. Running score: 0.78. Times trained:               6540. Times reached goal: 813.               Steps done: 1521249. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.196595194292.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.533135175705) A[1]:(0.59325492382) A[2]:(0.553500294685) A[3]:(0.532641291618)\n",
      " state (1)  A[0]:(0.543678641319) A[1]:(-0.00084885652177) A[2]:(0.614696860313) A[3]:(0.536845207214)\n",
      " state (2)  A[0]:(0.520544826984) A[1]:(0.683093905449) A[2]:(0.57465326786) A[3]:(0.526537597179)\n",
      " state (3)  A[0]:(0.60973918438) A[1]:(0.0576057061553) A[2]:(0.422400921583) A[3]:(0.595269322395)\n",
      " state (4)  A[0]:(0.5955555439) A[1]:(0.658794164658) A[2]:(-0.000823736016173) A[3]:(0.526776969433)\n",
      " state (5)  A[0]:(-0.0433917678893) A[1]:(0.999638676643) A[2]:(-0.757601320744) A[3]:(-0.159199401736)\n",
      " state (6)  A[0]:(-0.0258566010743) A[1]:(0.762346506119) A[2]:(-0.0410629734397) A[3]:(0.590632975101)\n",
      " state (7)  A[0]:(0.503878831863) A[1]:(-0.663504481316) A[2]:(0.471197962761) A[3]:(0.856742739677)\n",
      " state (8)  A[0]:(0.656004905701) A[1]:(-0.00218518939801) A[2]:(0.732781052589) A[3]:(0.584952175617)\n",
      " state (9)  A[0]:(0.666377544403) A[1]:(0.817655682564) A[2]:(0.751513123512) A[3]:(0.0200043749064)\n",
      " state (10)  A[0]:(0.6447879076) A[1]:(0.851966321468) A[2]:(0.0019085383974) A[3]:(0.397838294506)\n",
      " state (11)  A[0]:(0.0340124554932) A[1]:(0.865393698215) A[2]:(-0.838845610619) A[3]:(0.696759819984)\n",
      " state (12)  A[0]:(-0.427207380533) A[1]:(0.875558912754) A[2]:(-0.806098520756) A[3]:(0.804121255875)\n",
      " state (13)  A[0]:(0.00153678539209) A[1]:(0.877671837807) A[2]:(0.900902986526) A[3]:(0.801957249641)\n",
      " state (14)  A[0]:(0.800577044487) A[1]:(0.874194860458) A[2]:(0.999976456165) A[3]:(0.729469776154)\n",
      " state (15)  A[0]:(0.978929162025) A[1]:(0.811822295189) A[2]:(1.0) A[3]:(0.722442090511)\n",
      "Episode 156000 finished after 0 timesteps with r=0.0. Running score: 0.74. Times trained:               6281. Times reached goal: 767.               Steps done: 1527530. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.195364249705.\n",
      " state (0)  A[0]:(0.533987164497) A[1]:(0.593375444412) A[2]:(0.555580556393) A[3]:(0.537772774696)\n",
      " state (1)  A[0]:(0.544219970703) A[1]:(0.00122299918439) A[2]:(0.619075417519) A[3]:(0.550032258034)\n",
      " state (2)  A[0]:(0.526019632816) A[1]:(0.68844640255) A[2]:(0.57984817028) A[3]:(0.537146627903)\n",
      " state (3)  A[0]:(0.612015247345) A[1]:(0.0561520159245) A[2]:(0.423678308725) A[3]:(0.602380871773)\n",
      " state (4)  A[0]:(0.595510363579) A[1]:(0.661519110203) A[2]:(0.00156891217921) A[3]:(0.533468127251)\n",
      " state (5)  A[0]:(-0.0399309173226) A[1]:(0.999647557735) A[2]:(-0.745723307133) A[3]:(-0.144512802362)\n",
      " state (6)  A[0]:(-0.0170003194362) A[1]:(0.764977395535) A[2]:(-0.0399888791144) A[3]:(0.596653699875)\n",
      " state (7)  A[0]:(0.518135786057) A[1]:(-0.667723417282) A[2]:(0.465461045504) A[3]:(0.860740303993)\n",
      " state (8)  A[0]:(0.664454340935) A[1]:(-0.0113330679014) A[2]:(0.733686387539) A[3]:(0.592039465904)\n",
      " state (9)  A[0]:(0.676249980927) A[1]:(0.812998473644) A[2]:(0.755389213562) A[3]:(0.0332334414124)\n",
      " state (10)  A[0]:(0.666090846062) A[1]:(0.849668502808) A[2]:(0.00481828302145) A[3]:(0.417566686869)\n",
      " state (11)  A[0]:(0.0700160488486) A[1]:(0.865032374859) A[2]:(-0.841513693333) A[3]:(0.708388805389)\n",
      " state (12)  A[0]:(-0.412174582481) A[1]:(0.876092851162) A[2]:(-0.811206579208) A[3]:(0.80963331461)\n",
      " state (13)  A[0]:(0.00793690700084) A[1]:(0.877027213573) A[2]:(0.899567484856) A[3]:(0.806314587593)\n",
      " state (14)  A[0]:(0.802608013153) A[1]:(0.869651675224) A[2]:(0.999975860119) A[3]:(0.73789870739)\n",
      " state (15)  A[0]:(0.979259490967) A[1]:(0.798895359039) A[2]:(1.0) A[3]:(0.735962092876)\n",
      "Episode 157000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained:               6394. Times reached goal: 793.               Steps done: 1533924. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.194119075756.\n",
      " state (0)  A[0]:(0.535185098648) A[1]:(0.596762418747) A[2]:(0.560638964176) A[3]:(0.535269856453)\n",
      " state (1)  A[0]:(0.544390201569) A[1]:(0.000242205103859) A[2]:(0.622130393982) A[3]:(0.552141427994)\n",
      " state (2)  A[0]:(0.532074809074) A[1]:(0.689405322075) A[2]:(0.579319119453) A[3]:(0.536051094532)\n",
      " state (3)  A[0]:(0.61490893364) A[1]:(0.0516985356808) A[2]:(0.418462395668) A[3]:(0.59953558445)\n",
      " state (4)  A[0]:(0.596279382706) A[1]:(0.661465644836) A[2]:(0.000180244445801) A[3]:(0.531429588795)\n",
      " state (5)  A[0]:(-0.039785861969) A[1]:(0.999654650688) A[2]:(-0.732418358326) A[3]:(-0.137661963701)\n",
      " state (6)  A[0]:(-0.018190106377) A[1]:(0.768563747406) A[2]:(-0.0335208885372) A[3]:(0.598214387894)\n",
      " state (7)  A[0]:(0.520871520042) A[1]:(-0.666662573814) A[2]:(0.463562518358) A[3]:(0.862370491028)\n",
      " state (8)  A[0]:(0.660573840141) A[1]:(-0.0036819097586) A[2]:(0.736506342888) A[3]:(0.590449512005)\n",
      " state (9)  A[0]:(0.674150526524) A[1]:(0.814139544964) A[2]:(0.760550737381) A[3]:(0.03355595842)\n",
      " state (10)  A[0]:(0.677095532417) A[1]:(0.851019799709) A[2]:(0.0133507661521) A[3]:(0.426463723183)\n",
      " state (11)  A[0]:(0.0942699760199) A[1]:(0.866852641106) A[2]:(-0.841522932053) A[3]:(0.713054299355)\n",
      " state (12)  A[0]:(-0.40377035737) A[1]:(0.878149092197) A[2]:(-0.812293589115) A[3]:(0.810270130634)\n",
      " state (13)  A[0]:(0.0085517168045) A[1]:(0.878158509731) A[2]:(0.900894105434) A[3]:(0.806112825871)\n",
      " state (14)  A[0]:(0.803125560284) A[1]:(0.867548406124) A[2]:(0.999975919724) A[3]:(0.741563677788)\n",
      " state (15)  A[0]:(0.979425489902) A[1]:(0.789968550205) A[2]:(1.0) A[3]:(0.74583864212)\n",
      "Episode 158000 finished after 0 timesteps with r=1.0. Running score: 0.77. Times trained:               6395. Times reached goal: 786.               Steps done: 1540319. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.192881645168.\n",
      " state (0)  A[0]:(0.537680625916) A[1]:(0.592661082745) A[2]:(0.561686635017) A[3]:(0.537552237511)\n",
      " state (1)  A[0]:(0.54393684864) A[1]:(0.00139501853846) A[2]:(0.623157143593) A[3]:(0.559276044369)\n",
      " state (2)  A[0]:(0.53618824482) A[1]:(0.691688418388) A[2]:(0.579770684242) A[3]:(0.540999233723)\n",
      " state (3)  A[0]:(0.616182565689) A[1]:(0.0475555844605) A[2]:(0.416022866964) A[3]:(0.601114928722)\n",
      " state (4)  A[0]:(0.595280766487) A[1]:(0.663505315781) A[2]:(0.0012129539391) A[3]:(0.531131148338)\n",
      " state (5)  A[0]:(-0.0382803678513) A[1]:(0.999659836292) A[2]:(-0.719166457653) A[3]:(-0.133801549673)\n",
      " state (6)  A[0]:(-0.0137127386406) A[1]:(0.770058274269) A[2]:(-0.0314640812576) A[3]:(0.595644235611)\n",
      " state (7)  A[0]:(0.529780030251) A[1]:(-0.666334450245) A[2]:(0.45651614666) A[3]:(0.861849009991)\n",
      " state (8)  A[0]:(0.663506448269) A[1]:(-0.00048729518312) A[2]:(0.735004067421) A[3]:(0.585003137589)\n",
      " state (9)  A[0]:(0.673471450806) A[1]:(0.817219734192) A[2]:(0.761252582073) A[3]:(0.0182264819741)\n",
      " state (10)  A[0]:(0.681474804878) A[1]:(0.857009887695) A[2]:(0.00783153250813) A[3]:(0.415416628122)\n",
      " state (11)  A[0]:(0.100010573864) A[1]:(0.874386310577) A[2]:(-0.84616625309) A[3]:(0.703597784042)\n",
      " state (12)  A[0]:(-0.4053157866) A[1]:(0.885712683201) A[2]:(-0.818502724171) A[3]:(0.801319122314)\n",
      " state (13)  A[0]:(0.00604875478894) A[1]:(0.884902536869) A[2]:(0.898907244205) A[3]:(0.797641634941)\n",
      " state (14)  A[0]:(0.804347097874) A[1]:(0.872464954853) A[2]:(0.999975442886) A[3]:(0.735921382904)\n",
      " state (15)  A[0]:(0.979798614979) A[1]:(0.793762087822) A[2]:(1.0) A[3]:(0.746334969997)\n",
      "Episode 159000 finished after 0 timesteps with r=0.0. Running score: 0.85. Times trained:               6516. Times reached goal: 822.               Steps done: 1546835. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.191628914198.\n",
      "q_values \n",
      "tensor([[ 0.5354,  0.5965,  0.5608,  0.5362]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5962,  0.6610, -0.0011,  0.5330]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6615, -0.0120,  0.7351,  0.5874]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6709,  0.8107,  0.7639,  0.0232]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0074,  0.8795,  0.8968,  0.7961]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8061,  0.8643,  1.0000,  0.7392]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.535711884499) A[1]:(0.596280574799) A[2]:(0.561407625675) A[3]:(0.53575450182)\n",
      " state (1)  A[0]:(0.542430639267) A[1]:(0.0011050873436) A[2]:(0.622940659523) A[3]:(0.562678277493)\n",
      " state (2)  A[0]:(0.540459990501) A[1]:(0.69510447979) A[2]:(0.577201068401) A[3]:(0.543932020664)\n",
      " state (3)  A[0]:(0.618178069592) A[1]:(0.046698693186) A[2]:(0.410036146641) A[3]:(0.602057337761)\n",
      " state (4)  A[0]:(0.595760405064) A[1]:(0.661717295647) A[2]:(-0.000571727694478) A[3]:(0.532982051373)\n",
      " state (5)  A[0]:(-0.0390106551349) A[1]:(0.999663233757) A[2]:(-0.708736419678) A[3]:(-0.124725975096)\n",
      " state (6)  A[0]:(-0.0162973571569) A[1]:(0.772160172462) A[2]:(-0.0354287102818) A[3]:(0.598828315735)\n",
      " state (7)  A[0]:(0.530690968037) A[1]:(-0.669956207275) A[2]:(0.448076426983) A[3]:(0.864149332047)\n",
      " state (8)  A[0]:(0.658344447613) A[1]:(-0.00849057920277) A[2]:(0.735104978085) A[3]:(0.58613717556)\n",
      " state (9)  A[0]:(0.667121291161) A[1]:(0.811950802803) A[2]:(0.763618588448) A[3]:(0.0207398291677)\n",
      " state (10)  A[0]:(0.681077599525) A[1]:(0.853809714317) A[2]:(0.00560397002846) A[3]:(0.419903755188)\n",
      " state (11)  A[0]:(0.0987041369081) A[1]:(0.872104227543) A[2]:(-0.849609911442) A[3]:(0.702382206917)\n",
      " state (12)  A[0]:(-0.40971198678) A[1]:(0.883066952229) A[2]:(-0.822772502899) A[3]:(0.797630667686)\n",
      " state (13)  A[0]:(0.00269820634276) A[1]:(0.880708515644) A[2]:(0.898083746433) A[3]:(0.794764816761)\n",
      " state (14)  A[0]:(0.805308043957) A[1]:(0.865197658539) A[2]:(0.999975264072) A[3]:(0.738084733486)\n",
      " state (15)  A[0]:(0.980086505413) A[1]:(0.778813898563) A[2]:(0.999999940395) A[3]:(0.754679739475)\n",
      "Episode 160000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6411. Times reached goal: 813.               Steps done: 1553246. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.190404310889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53162920475) A[1]:(0.59415948391) A[2]:(0.562401413918) A[3]:(0.534675061703)\n",
      " state (1)  A[0]:(0.539504289627) A[1]:(-0.00169174396433) A[2]:(0.623425006866) A[3]:(0.566086053848)\n",
      " state (2)  A[0]:(0.542816877365) A[1]:(0.690091907978) A[2]:(0.575669765472) A[3]:(0.546398758888)\n",
      " state (3)  A[0]:(0.618256092072) A[1]:(0.03273492679) A[2]:(0.406363934278) A[3]:(0.601886153221)\n",
      " state (4)  A[0]:(0.59384727478) A[1]:(0.658623337746) A[2]:(0.00160598615184) A[3]:(0.531526088715)\n",
      " state (5)  A[0]:(-0.039760004729) A[1]:(0.999659478664) A[2]:(-0.69219660759) A[3]:(-0.120529688895)\n",
      " state (6)  A[0]:(-0.0152144609019) A[1]:(0.769080340862) A[2]:(-0.0300494544208) A[3]:(0.59939622879)\n",
      " state (7)  A[0]:(0.537885904312) A[1]:(-0.669413268566) A[2]:(0.439147025347) A[3]:(0.86532998085)\n",
      " state (8)  A[0]:(0.661204397678) A[1]:(-0.00781989749521) A[2]:(0.73054087162) A[3]:(0.589452385902)\n",
      " state (9)  A[0]:(0.664249360561) A[1]:(0.814720988274) A[2]:(0.762008309364) A[3]:(0.0168396104127)\n",
      " state (10)  A[0]:(0.682538211346) A[1]:(0.858983874321) A[2]:(-9.77516174316e-05) A[3]:(0.415065348148)\n",
      " state (11)  A[0]:(0.100626587868) A[1]:(0.877902984619) A[2]:(-0.852069914341) A[3]:(0.694927573204)\n",
      " state (12)  A[0]:(-0.413355648518) A[1]:(0.888678729534) A[2]:(-0.823711037636) A[3]:(0.788921892643)\n",
      " state (13)  A[0]:(-0.00281428545713) A[1]:(0.885998606682) A[2]:(0.900554835796) A[3]:(0.786332845688)\n",
      " state (14)  A[0]:(0.804554104805) A[1]:(0.869532227516) A[2]:(0.999975979328) A[3]:(0.733447313309)\n",
      " state (15)  A[0]:(0.980097711086) A[1]:(0.782805204391) A[2]:(0.999999940395) A[3]:(0.756938457489)\n",
      "Episode 161000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6370. Times reached goal: 787.               Steps done: 1559616. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.189195290247.\n",
      " state (0)  A[0]:(0.533846855164) A[1]:(0.592845857143) A[2]:(0.562716722488) A[3]:(0.533913254738)\n",
      " state (1)  A[0]:(0.541120767593) A[1]:(0.00048186533968) A[2]:(0.625388622284) A[3]:(0.570525467396)\n",
      " state (2)  A[0]:(0.549613833427) A[1]:(0.696031928062) A[2]:(0.576034069061) A[3]:(0.551488399506)\n",
      " state (3)  A[0]:(0.622301459312) A[1]:(0.0336284376681) A[2]:(0.403374373913) A[3]:(0.603659451008)\n",
      " state (4)  A[0]:(0.595597624779) A[1]:(0.660217881203) A[2]:(0.00134944834281) A[3]:(0.531770467758)\n",
      " state (5)  A[0]:(-0.036332283169) A[1]:(0.999663233757) A[2]:(-0.679298341274) A[3]:(-0.11335260421)\n",
      " state (6)  A[0]:(-0.0121856555343) A[1]:(0.770374536514) A[2]:(-0.0270908325911) A[3]:(0.602582395077)\n",
      " state (7)  A[0]:(0.543554186821) A[1]:(-0.670344829559) A[2]:(0.436179637909) A[3]:(0.867623329163)\n",
      " state (8)  A[0]:(0.663725376129) A[1]:(-0.00456399377435) A[2]:(0.733038723469) A[3]:(0.591599822044)\n",
      " state (9)  A[0]:(0.669606626034) A[1]:(0.815242350101) A[2]:(0.766177237034) A[3]:(0.0228650942445)\n",
      " state (10)  A[0]:(0.693784952164) A[1]:(0.861055135727) A[2]:(0.00468942057341) A[3]:(0.423814058304)\n",
      " state (11)  A[0]:(0.120375111699) A[1]:(0.880437016487) A[2]:(-0.852799296379) A[3]:(0.697558164597)\n",
      " state (12)  A[0]:(-0.401929289103) A[1]:(0.890749573708) A[2]:(-0.825157165527) A[3]:(0.789112269878)\n",
      " state (13)  A[0]:(0.00871353130788) A[1]:(0.887378633022) A[2]:(0.900785684586) A[3]:(0.788834095001)\n",
      " state (14)  A[0]:(0.809048950672) A[1]:(0.869969069958) A[2]:(0.999976217747) A[3]:(0.743207335472)\n",
      " state (15)  A[0]:(0.980630278587) A[1]:(0.782417237759) A[2]:(0.999999940395) A[3]:(0.77183920145)\n",
      "Episode 162000 finished after 0 timesteps with r=0.0. Running score: 0.83. Times trained:               6385. Times reached goal: 811.               Steps done: 1566001. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.187991126702.\n",
      " state (0)  A[0]:(0.535266876221) A[1]:(0.592105150223) A[2]:(0.562261819839) A[3]:(0.532324552536)\n",
      " state (1)  A[0]:(0.539988279343) A[1]:(-6.10630959272e-05) A[2]:(0.625758886337) A[3]:(0.572328329086)\n",
      " state (2)  A[0]:(0.552266716957) A[1]:(0.695625424385) A[2]:(0.57504594326) A[3]:(0.55385440588)\n",
      " state (3)  A[0]:(0.622700572014) A[1]:(0.0274486429989) A[2]:(0.399209856987) A[3]:(0.603310108185)\n",
      " state (4)  A[0]:(0.594029009342) A[1]:(0.660392701626) A[2]:(0.000345826148987) A[3]:(0.528659105301)\n",
      " state (5)  A[0]:(-0.0331245735288) A[1]:(0.999667108059) A[2]:(-0.666303038597) A[3]:(-0.116794444621)\n",
      " state (6)  A[0]:(-0.0054140156135) A[1]:(0.772227883339) A[2]:(-0.0296066273004) A[3]:(0.597425103188)\n",
      " state (7)  A[0]:(0.549144804478) A[1]:(-0.673024654388) A[2]:(0.427077591419) A[3]:(0.866289377213)\n",
      " state (8)  A[0]:(0.658884108067) A[1]:(-0.00244691292755) A[2]:(0.731757640839) A[3]:(0.58145403862)\n",
      " state (9)  A[0]:(0.666088342667) A[1]:(0.813837409019) A[2]:(0.766813933849) A[3]:(0.0143451681361)\n",
      " state (10)  A[0]:(0.70090842247) A[1]:(0.86001676321) A[2]:(0.00579469883814) A[3]:(0.424700051546)\n",
      " state (11)  A[0]:(0.137224540114) A[1]:(0.879511356354) A[2]:(-0.853698074818) A[3]:(0.696234107018)\n",
      " state (12)  A[0]:(-0.398555517197) A[1]:(0.889928519726) A[2]:(-0.827524065971) A[3]:(0.785149693489)\n",
      " state (13)  A[0]:(0.00225293263793) A[1]:(0.886454284191) A[2]:(0.900249183178) A[3]:(0.784602940083)\n",
      " state (14)  A[0]:(0.805864453316) A[1]:(0.868068814278) A[2]:(0.999976038933) A[3]:(0.742096602917)\n",
      " state (15)  A[0]:(0.980252504349) A[1]:(0.777840018272) A[2]:(0.999999940395) A[3]:(0.775283038616)\n",
      "Episode 163000 finished after 0 timesteps with r=0.0. Running score: 0.76. Times trained:               6339. Times reached goal: 792.               Steps done: 1572340. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.186803219998.\n",
      " state (0)  A[0]:(0.534549236298) A[1]:(0.593424916267) A[2]:(0.565178871155) A[3]:(0.534586071968)\n",
      " state (1)  A[0]:(0.538883984089) A[1]:(-0.000140184536576) A[2]:(0.626878082752) A[3]:(0.58214867115)\n",
      " state (2)  A[0]:(0.556217074394) A[1]:(0.693510770798) A[2]:(0.573699831963) A[3]:(0.567499279976)\n",
      " state (3)  A[0]:(0.625007629395) A[1]:(0.0146011523902) A[2]:(0.394716084003) A[3]:(0.613514780998)\n",
      " state (4)  A[0]:(0.593385696411) A[1]:(0.657398760319) A[2]:(0.000227093696594) A[3]:(0.536278128624)\n",
      " state (5)  A[0]:(-0.0361980237067) A[1]:(0.99966865778) A[2]:(-0.649417340755) A[3]:(-0.108260445297)\n",
      " state (6)  A[0]:(-0.00659086694941) A[1]:(0.774143755436) A[2]:(-0.0243458319455) A[3]:(0.602301239967)\n",
      " state (7)  A[0]:(0.551501810551) A[1]:(-0.674806594849) A[2]:(0.424248963594) A[3]:(0.869924426079)\n",
      " state (8)  A[0]:(0.655177950859) A[1]:(0.000381236866815) A[2]:(0.733272194862) A[3]:(0.587354719639)\n",
      " state (9)  A[0]:(0.659688949585) A[1]:(0.815708756447) A[2]:(0.768847346306) A[3]:(0.023371970281)\n",
      " state (10)  A[0]:(0.698006331921) A[1]:(0.8626973629) A[2]:(0.00100946391467) A[3]:(0.436087489128)\n",
      " state (11)  A[0]:(0.128451094031) A[1]:(0.88157337904) A[2]:(-0.857453227043) A[3]:(0.700610637665)\n",
      " state (12)  A[0]:(-0.407586038113) A[1]:(0.890759408474) A[2]:(-0.830915808678) A[3]:(0.786002039909)\n",
      " state (13)  A[0]:(-0.00310238194652) A[1]:(0.886375546455) A[2]:(0.901066422462) A[3]:(0.785966992378)\n",
      " state (14)  A[0]:(0.806704044342) A[1]:(0.867094397545) A[2]:(0.999976634979) A[3]:(0.747653722763)\n",
      " state (15)  A[0]:(0.980497717857) A[1]:(0.775579154491) A[2]:(0.999999940395) A[3]:(0.784142255783)\n",
      "Episode 164000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained:               6466. Times reached goal: 816.               Steps done: 1578806. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.185599247017.\n",
      "q_values \n",
      "tensor([[ 0.5346,  0.5936,  0.5647,  0.5366]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5939,  0.6588, -0.0003,  0.5392]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6562, -0.0005,  0.7327,  0.5914]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6588,  0.8159,  0.7699,  0.0281]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0094,  0.8885,  0.9034,  0.7827]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8052,  0.8697,  1.0000,  0.7474]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532953679562) A[1]:(0.593801498413) A[2]:(0.565004825592) A[3]:(0.535699129105)\n",
      " state (1)  A[0]:(0.536921858788) A[1]:(-0.00124732335098) A[2]:(0.626900911331) A[3]:(0.588937759399)\n",
      " state (2)  A[0]:(0.559753656387) A[1]:(0.69700717926) A[2]:(0.574849843979) A[3]:(0.578337550163)\n",
      " state (3)  A[0]:(0.627213180065) A[1]:(0.00898822024465) A[2]:(0.393174946308) A[3]:(0.620425999165)\n",
      " state (4)  A[0]:(0.592162549496) A[1]:(0.65866625309) A[2]:(-0.000464320153696) A[3]:(0.537568449974)\n",
      " state (5)  A[0]:(-0.0379309691489) A[1]:(0.999669730663) A[2]:(-0.636336445808) A[3]:(-0.109948143363)\n",
      " state (6)  A[0]:(-0.0056739137508) A[1]:(0.774511873722) A[2]:(-0.0243311766535) A[3]:(0.602317929268)\n",
      " state (7)  A[0]:(0.555362582207) A[1]:(-0.677579522133) A[2]:(0.417932927608) A[3]:(0.871757864952)\n",
      " state (8)  A[0]:(0.653475642204) A[1]:(-0.000669158878736) A[2]:(0.732841551304) A[3]:(0.588359117508)\n",
      " state (9)  A[0]:(0.658265829086) A[1]:(0.815060257912) A[2]:(0.76991122961) A[3]:(0.0265161208808)\n",
      " state (10)  A[0]:(0.703142404556) A[1]:(0.863298296928) A[2]:(0.000466823548777) A[3]:(0.443169474602)\n",
      " state (11)  A[0]:(0.137646496296) A[1]:(0.882284641266) A[2]:(-0.85885733366) A[3]:(0.701931536198)\n",
      " state (12)  A[0]:(-0.407138884068) A[1]:(0.891451060772) A[2]:(-0.832158982754) A[3]:(0.783622324467)\n",
      " state (13)  A[0]:(-0.00684176152572) A[1]:(0.887574851513) A[2]:(0.902595102787) A[3]:(0.783168315887)\n",
      " state (14)  A[0]:(0.805719673634) A[1]:(0.869003891945) A[2]:(0.99997729063) A[3]:(0.747869968414)\n",
      " state (15)  A[0]:(0.980419337749) A[1]:(0.779309749603) A[2]:(0.999999940395) A[3]:(0.788190186024)\n",
      "Episode 165000 finished after 0 timesteps with r=1.0. Running score: 0.76. Times trained:               6369. Times reached goal: 783.               Steps done: 1585175. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.184420921773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.535491347313) A[1]:(0.594402074814) A[2]:(0.564969480038) A[3]:(0.535480678082)\n",
      " state (1)  A[0]:(0.537945628166) A[1]:(0.000456450477941) A[2]:(0.627325057983) A[3]:(0.590838193893)\n",
      " state (2)  A[0]:(0.565348803997) A[1]:(0.700749635696) A[2]:(0.575574815273) A[3]:(0.58473098278)\n",
      " state (3)  A[0]:(0.632101655006) A[1]:(0.00807732436806) A[2]:(0.392044484615) A[3]:(0.624703526497)\n",
      " state (4)  A[0]:(0.595214486122) A[1]:(0.661691188812) A[2]:(0.000414848298533) A[3]:(0.538120210171)\n",
      " state (5)  A[0]:(-0.0321574769914) A[1]:(0.999671936035) A[2]:(-0.623004794121) A[3]:(-0.111497998238)\n",
      " state (6)  A[0]:(0.00568809919059) A[1]:(0.77526974678) A[2]:(-0.0204534605145) A[3]:(0.603007078171)\n",
      " state (7)  A[0]:(0.570265293121) A[1]:(-0.67934346199) A[2]:(0.414949864149) A[3]:(0.874608635902)\n",
      " state (8)  A[0]:(0.666362047195) A[1]:(-0.0053959582001) A[2]:(0.733059287071) A[3]:(0.595788836479)\n",
      " state (9)  A[0]:(0.672364473343) A[1]:(0.813343048096) A[2]:(0.772106707096) A[3]:(0.036832947284)\n",
      " state (10)  A[0]:(0.722074747086) A[1]:(0.862267494202) A[2]:(0.00446686148643) A[3]:(0.457714468241)\n",
      " state (11)  A[0]:(0.17306779325) A[1]:(0.880299389362) A[2]:(-0.859698116779) A[3]:(0.710094332695)\n",
      " state (12)  A[0]:(-0.388285040855) A[1]:(0.88861310482) A[2]:(-0.835764169693) A[3]:(0.787784636021)\n",
      " state (13)  A[0]:(0.00602434715256) A[1]:(0.884464740753) A[2]:(0.900365948677) A[3]:(0.787686705589)\n",
      " state (14)  A[0]:(0.809485018253) A[1]:(0.865902543068) A[2]:(0.999976933002) A[3]:(0.756075561047)\n",
      " state (15)  A[0]:(0.980811059475) A[1]:(0.775879025459) A[2]:(0.999999940395) A[3]:(0.797906279564)\n",
      "Episode 166000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained:               6417. Times reached goal: 803.               Steps done: 1591592. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.183241281641.\n",
      " state (0)  A[0]:(0.534416437149) A[1]:(0.592792987823) A[2]:(0.565981626511) A[3]:(0.535852074623)\n",
      " state (1)  A[0]:(0.535105109215) A[1]:(-0.002421352081) A[2]:(0.628699064255) A[3]:(0.594293951988)\n",
      " state (2)  A[0]:(0.565598964691) A[1]:(0.698230147362) A[2]:(0.575140476227) A[3]:(0.593255639076)\n",
      " state (3)  A[0]:(0.631785154343) A[1]:(-0.00633515138179) A[2]:(0.389508754015) A[3]:(0.630393087864)\n",
      " state (4)  A[0]:(0.591840445995) A[1]:(0.65789604187) A[2]:(0.000820040528197) A[3]:(0.539014935493)\n",
      " state (5)  A[0]:(-0.0387530475855) A[1]:(0.999671697617) A[2]:(-0.608862519264) A[3]:(-0.115955352783)\n",
      " state (6)  A[0]:(-0.00139233376831) A[1]:(0.777102828026) A[2]:(-0.0152082638815) A[3]:(0.600277781487)\n",
      " state (7)  A[0]:(0.565155148506) A[1]:(-0.680039942265) A[2]:(0.415595918894) A[3]:(0.8748306036)\n",
      " state (8)  A[0]:(0.654812932014) A[1]:(-0.00109002692625) A[2]:(0.73635160923) A[3]:(0.589052498341)\n",
      " state (9)  A[0]:(0.659263253212) A[1]:(0.814282894135) A[2]:(0.775160193443) A[3]:(0.0254471190274)\n",
      " state (10)  A[0]:(0.713433265686) A[1]:(0.863311231136) A[2]:(0.00639465684071) A[3]:(0.452203333378)\n",
      " state (11)  A[0]:(0.155377045274) A[1]:(0.880057871342) A[2]:(-0.860180854797) A[3]:(0.70405626297)\n",
      " state (12)  A[0]:(-0.401121139526) A[1]:(0.886703431606) A[2]:(-0.834598600864) A[3]:(0.781440019608)\n",
      " state (13)  A[0]:(-0.00134992517997) A[1]:(0.881782054901) A[2]:(0.903828322887) A[3]:(0.782784223557)\n",
      " state (14)  A[0]:(0.809332609177) A[1]:(0.863315165043) A[2]:(0.999978363514) A[3]:(0.754755377769)\n",
      " state (15)  A[0]:(0.980855107307) A[1]:(0.773705899715) A[2]:(1.0) A[3]:(0.800207018852)\n",
      "Episode 167000 finished after 0 timesteps with r=1.0. Running score: 0.79. Times trained:               6384. Times reached goal: 826.               Steps done: 1597976. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.182075195407.\n",
      " state (0)  A[0]:(0.536725759506) A[1]:(0.594187736511) A[2]:(0.567768871784) A[3]:(0.534885644913)\n",
      " state (1)  A[0]:(0.537289619446) A[1]:(-0.00141154509038) A[2]:(0.629179120064) A[3]:(0.593089222908)\n",
      " state (2)  A[0]:(0.570907354355) A[1]:(0.698478341103) A[2]:(0.574699044228) A[3]:(0.597057938576)\n",
      " state (3)  A[0]:(0.63672208786) A[1]:(-0.0122334705666) A[2]:(0.387832671404) A[3]:(0.6325045228)\n",
      " state (4)  A[0]:(0.595145583153) A[1]:(0.658457040787) A[2]:(0.00118195952382) A[3]:(0.537312150002)\n",
      " state (5)  A[0]:(-0.0344374142587) A[1]:(0.999672353268) A[2]:(-0.599421024323) A[3]:(-0.120720140636)\n",
      " state (6)  A[0]:(0.00302558206022) A[1]:(0.778336942196) A[2]:(-0.0206565074623) A[3]:(0.598795771599)\n",
      " state (7)  A[0]:(0.570814788342) A[1]:(-0.680007576942) A[2]:(0.405325472355) A[3]:(0.876201450825)\n",
      " state (8)  A[0]:(0.656794548035) A[1]:(-0.000291455537081) A[2]:(0.733025550842) A[3]:(0.589942932129)\n",
      " state (9)  A[0]:(0.660593152046) A[1]:(0.814495325089) A[2]:(0.774062812328) A[3]:(0.0237030982971)\n",
      " state (10)  A[0]:(0.718838870525) A[1]:(0.864117622375) A[2]:(0.00277005927637) A[3]:(0.452992141247)\n",
      " state (11)  A[0]:(0.165239751339) A[1]:(0.879800438881) A[2]:(-0.862459242344) A[3]:(0.701252579689)\n",
      " state (12)  A[0]:(-0.397792339325) A[1]:(0.885254561901) A[2]:(-0.839432537556) A[3]:(0.776878893375)\n",
      " state (13)  A[0]:(-0.000885188346729) A[1]:(0.880280137062) A[2]:(0.900520801544) A[3]:(0.779472589493)\n",
      " state (14)  A[0]:(0.809429943562) A[1]:(0.863275647163) A[2]:(0.999978065491) A[3]:(0.754725754261)\n",
      " state (15)  A[0]:(0.980840444565) A[1]:(0.777567267418) A[2]:(1.0) A[3]:(0.802676141262)\n",
      "Episode 168000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained:               6415. Times reached goal: 807.               Steps done: 1604391. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.18091092143.\n",
      " state (0)  A[0]:(0.535820960999) A[1]:(0.592269659042) A[2]:(0.565705239773) A[3]:(0.534301280975)\n",
      " state (1)  A[0]:(0.534192919731) A[1]:(0.00153028476052) A[2]:(0.629911124706) A[3]:(0.591205835342)\n",
      " state (2)  A[0]:(0.568728923798) A[1]:(0.698957800865) A[2]:(0.572976469994) A[3]:(0.599982857704)\n",
      " state (3)  A[0]:(0.634606838226) A[1]:(-0.0178891438991) A[2]:(0.383809536695) A[3]:(0.634114861488)\n",
      " state (4)  A[0]:(0.591360747814) A[1]:(0.658124268055) A[2]:(-6.97374343872e-05) A[3]:(0.535514235497)\n",
      " state (5)  A[0]:(-0.0366198793054) A[1]:(0.999669492245) A[2]:(-0.58763730526) A[3]:(-0.124963730574)\n",
      " state (6)  A[0]:(0.00363092008047) A[1]:(0.776673257351) A[2]:(-0.0198042728007) A[3]:(0.59681391716)\n",
      " state (7)  A[0]:(0.57266920805) A[1]:(-0.682108998299) A[2]:(0.400923490524) A[3]:(0.876638352871)\n",
      " state (8)  A[0]:(0.652668952942) A[1]:(0.00115882931277) A[2]:(0.732273757458) A[3]:(0.585604548454)\n",
      " state (9)  A[0]:(0.658725559711) A[1]:(0.813300609589) A[2]:(0.773102045059) A[3]:(0.0222770962864)\n",
      " state (10)  A[0]:(0.722175598145) A[1]:(0.86384087801) A[2]:(0.000101447105408) A[3]:(0.456532388926)\n",
      " state (11)  A[0]:(0.171782746911) A[1]:(0.879162371159) A[2]:(-0.863440275192) A[3]:(0.700541853905)\n",
      " state (12)  A[0]:(-0.396653085947) A[1]:(0.884059011936) A[2]:(-0.841131925583) A[3]:(0.774076700211)\n",
      " state (13)  A[0]:(-0.00397408893332) A[1]:(0.879686772823) A[2]:(0.899969875813) A[3]:(0.778093934059)\n",
      " state (14)  A[0]:(0.807411432266) A[1]:(0.864873707294) A[2]:(0.999978482723) A[3]:(0.757116675377)\n",
      " state (15)  A[0]:(0.980503439903) A[1]:(0.784457683563) A[2]:(1.0) A[3]:(0.807367861271)\n",
      "Episode 169000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained:               6335. Times reached goal: 792.               Steps done: 1610726. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.179768473268.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5927,  0.5649,  0.5344]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5910,  0.6586,  0.0022,  0.5348]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6586, -0.0054,  0.7337,  0.5888]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6618,  0.8128,  0.7762,  0.0167]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0054,  0.8767,  0.9002,  0.7715]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8126,  0.8622,  1.0000,  0.7535]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532995045185) A[1]:(0.594503164291) A[2]:(0.565175592899) A[3]:(0.534402370453)\n",
      " state (1)  A[0]:(0.534234344959) A[1]:(0.0025806853082) A[2]:(0.630704879761) A[3]:(0.590254127979)\n",
      " state (2)  A[0]:(0.571326851845) A[1]:(0.702004432678) A[2]:(0.57322704792) A[3]:(0.603154957294)\n",
      " state (3)  A[0]:(0.637319207191) A[1]:(-0.0163199231029) A[2]:(0.382596611977) A[3]:(0.63592427969)\n",
      " state (4)  A[0]:(0.592799663544) A[1]:(0.66028380394) A[2]:(0.00193667167332) A[3]:(0.534513711929)\n",
      " state (5)  A[0]:(-0.035009406507) A[1]:(0.999670922756) A[2]:(-0.574814736843) A[3]:(-0.12854744494)\n",
      " state (6)  A[0]:(0.00930041074753) A[1]:(0.777428328991) A[2]:(-0.0136150056496) A[3]:(0.594341158867)\n",
      " state (7)  A[0]:(0.5817720294) A[1]:(-0.685512900352) A[2]:(0.402112960815) A[3]:(0.877891182899)\n",
      " state (8)  A[0]:(0.662697792053) A[1]:(-0.0114821624011) A[2]:(0.733458161354) A[3]:(0.590682983398)\n",
      " state (9)  A[0]:(0.664917469025) A[1]:(0.811005830765) A[2]:(0.776028990746) A[3]:(0.0185177605599)\n",
      " state (10)  A[0]:(0.728255748749) A[1]:(0.863234519958) A[2]:(0.00495680095628) A[3]:(0.452327519655)\n",
      " state (11)  A[0]:(0.182426437736) A[1]:(0.877176403999) A[2]:(-0.863649129868) A[3]:(0.694474577904)\n",
      " state (12)  A[0]:(-0.387409329414) A[1]:(0.88019156456) A[2]:(-0.84266269207) A[3]:(0.767328083515)\n",
      " state (13)  A[0]:(0.0100895315409) A[1]:(0.875237762928) A[2]:(0.899204730988) A[3]:(0.772671937943)\n",
      " state (14)  A[0]:(0.813252151012) A[1]:(0.861688375473) A[2]:(0.999978899956) A[3]:(0.754001140594)\n",
      " state (15)  A[0]:(0.981121659279) A[1]:(0.784003198147) A[2]:(1.0) A[3]:(0.80636048317)\n",
      "Episode 170000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained:               6291. Times reached goal: 780.               Steps done: 1617017. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.178641099674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.533923327923) A[1]:(0.593353807926) A[2]:(0.567809760571) A[3]:(0.533634364605)\n",
      " state (1)  A[0]:(0.534087657928) A[1]:(-0.000368088454707) A[2]:(0.630014061928) A[3]:(0.588204622269)\n",
      " state (2)  A[0]:(0.572226762772) A[1]:(0.697846293449) A[2]:(0.573328256607) A[3]:(0.604667544365)\n",
      " state (3)  A[0]:(0.638501286507) A[1]:(-0.0277526322752) A[2]:(0.380630105734) A[3]:(0.636756956577)\n",
      " state (4)  A[0]:(0.592315196991) A[1]:(0.657783091068) A[2]:(5.59091567993e-05) A[3]:(0.53372824192)\n",
      " state (5)  A[0]:(-0.036269839853) A[1]:(0.999667167664) A[2]:(-0.569245934486) A[3]:(-0.127320826054)\n",
      " state (6)  A[0]:(0.012655059807) A[1]:(0.77520430088) A[2]:(-0.0179385971278) A[3]:(0.596077740192)\n",
      " state (7)  A[0]:(0.585539460182) A[1]:(-0.688104748726) A[2]:(0.395092099905) A[3]:(0.879729568958)\n",
      " state (8)  A[0]:(0.658802211285) A[1]:(-0.00494067836553) A[2]:(0.731551289558) A[3]:(0.589280903339)\n",
      " state (9)  A[0]:(0.664706468582) A[1]:(0.812540769577) A[2]:(0.774648725986) A[3]:(0.0240671932697)\n",
      " state (10)  A[0]:(0.735990047455) A[1]:(0.864319801331) A[2]:(0.00491078710184) A[3]:(0.465054631233)\n",
      " state (11)  A[0]:(0.200441718102) A[1]:(0.877139508724) A[2]:(-0.863480567932) A[3]:(0.700605750084)\n",
      " state (12)  A[0]:(-0.38156208396) A[1]:(0.879648387432) A[2]:(-0.843448519707) A[3]:(0.768389821053)\n",
      " state (13)  A[0]:(0.00697869667783) A[1]:(0.875859081745) A[2]:(0.89933848381) A[3]:(0.772260248661)\n",
      " state (14)  A[0]:(0.81104695797) A[1]:(0.864826798439) A[2]:(0.999979257584) A[3]:(0.754792928696)\n",
      " state (15)  A[0]:(0.980811417103) A[1]:(0.792488455772) A[2]:(1.0) A[3]:(0.808561205864)\n",
      "Episode 171000 finished after 0 timesteps with r=0.0. Running score: 0.79. Times trained:               6387. Times reached goal: 802.               Steps done: 1623404. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.177503754947.\n",
      " state (0)  A[0]:(0.532537817955) A[1]:(0.592517495155) A[2]:(0.565400063992) A[3]:(0.531120061874)\n",
      " state (1)  A[0]:(0.533330202103) A[1]:(-0.0020100818947) A[2]:(0.628457665443) A[3]:(0.584507703781)\n",
      " state (2)  A[0]:(0.573013663292) A[1]:(0.700224757195) A[2]:(0.572566747665) A[3]:(0.605278611183)\n",
      " state (3)  A[0]:(0.639820337296) A[1]:(-0.0242142789066) A[2]:(0.378058433533) A[3]:(0.637155890465)\n",
      " state (4)  A[0]:(0.59157127142) A[1]:(0.65936934948) A[2]:(-0.00194477790501) A[3]:(0.533246219158)\n",
      " state (5)  A[0]:(-0.0460840761662) A[1]:(0.999671280384) A[2]:(-0.565245270729) A[3]:(-0.129890888929)\n",
      " state (6)  A[0]:(0.00279074185528) A[1]:(0.778895735741) A[2]:(-0.0189243610948) A[3]:(0.593080997467)\n",
      " state (7)  A[0]:(0.578687131405) A[1]:(-0.683980226517) A[2]:(0.393037080765) A[3]:(0.879789173603)\n",
      " state (8)  A[0]:(0.646284103394) A[1]:(0.0067357220687) A[2]:(0.732046365738) A[3]:(0.584115743637)\n",
      " state (9)  A[0]:(0.647494912148) A[1]:(0.816899001598) A[2]:(0.774181544781) A[3]:(0.0144633399323)\n",
      " state (10)  A[0]:(0.718538045883) A[1]:(0.868831634521) A[2]:(-0.00562638090923) A[3]:(0.461466789246)\n",
      " state (11)  A[0]:(0.156928628683) A[1]:(0.880697369576) A[2]:(-0.867247998714) A[3]:(0.696805953979)\n",
      " state (12)  A[0]:(-0.414939552546) A[1]:(0.881838560104) A[2]:(-0.844654858112) A[3]:(0.764904379845)\n",
      " state (13)  A[0]:(-0.0197155848145) A[1]:(0.878163397312) A[2]:(0.902536928654) A[3]:(0.771128118038)\n",
      " state (14)  A[0]:(0.805405199528) A[1]:(0.868976891041) A[2]:(0.999980688095) A[3]:(0.757573485374)\n",
      " state (15)  A[0]:(0.980350375175) A[1]:(0.801817953587) A[2]:(1.0) A[3]:(0.813573658466)\n",
      "Episode 172000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6360. Times reached goal: 818.               Steps done: 1629764. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.176378413445.\n",
      " state (0)  A[0]:(0.533587396145) A[1]:(0.593611061573) A[2]:(0.566754937172) A[3]:(0.532927632332)\n",
      " state (1)  A[0]:(0.534112453461) A[1]:(0.00163924170192) A[2]:(0.630785346031) A[3]:(0.584107041359)\n",
      " state (2)  A[0]:(0.574649453163) A[1]:(0.700729131699) A[2]:(0.576643466949) A[3]:(0.607527256012)\n",
      " state (3)  A[0]:(0.641987681389) A[1]:(-0.0252730697393) A[2]:(0.381510794163) A[3]:(0.638649642467)\n",
      " state (4)  A[0]:(0.593055009842) A[1]:(0.660213649273) A[2]:(0.0017223340692) A[3]:(0.534085035324)\n",
      " state (5)  A[0]:(-0.0435313694179) A[1]:(0.999670028687) A[2]:(-0.558954238892) A[3]:(-0.125386595726)\n",
      " state (6)  A[0]:(0.011662133038) A[1]:(0.777162432671) A[2]:(-0.0184074118733) A[3]:(0.596302807331)\n",
      " state (7)  A[0]:(0.589939713478) A[1]:(-0.687790036201) A[2]:(0.391268402338) A[3]:(0.882467567921)\n",
      " state (8)  A[0]:(0.657592058182) A[1]:(-0.00340425036848) A[2]:(0.73221963644) A[3]:(0.5903891325)\n",
      " state (9)  A[0]:(0.661274790764) A[1]:(0.812848627567) A[2]:(0.776214540005) A[3]:(0.0204570963979)\n",
      " state (10)  A[0]:(0.735424041748) A[1]:(0.865823507309) A[2]:(0.00290321488865) A[3]:(0.469281077385)\n",
      " state (11)  A[0]:(0.192914843559) A[1]:(0.876188337803) A[2]:(-0.865732192993) A[3]:(0.69943010807)\n",
      " state (12)  A[0]:(-0.390636414289) A[1]:(0.875786304474) A[2]:(-0.845972537994) A[3]:(0.764457464218)\n",
      " state (13)  A[0]:(0.00103339517955) A[1]:(0.872432291508) A[2]:(0.900156438351) A[3]:(0.771011948586)\n",
      " state (14)  A[0]:(0.810740232468) A[1]:(0.865776240826) A[2]:(0.999980568886) A[3]:(0.759239912033)\n",
      " state (15)  A[0]:(0.980737507343) A[1]:(0.802775621414) A[2]:(1.0) A[3]:(0.815664112568)\n",
      "Episode 173000 finished after 0 timesteps with r=1.0. Running score: 0.79. Times trained:               6316. Times reached goal: 803.               Steps done: 1636080. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.175267918022.\n",
      " state (0)  A[0]:(0.53535592556) A[1]:(0.592659831047) A[2]:(0.569440603256) A[3]:(0.533032715321)\n",
      " state (1)  A[0]:(0.534264564514) A[1]:(-0.00338331074454) A[2]:(0.631210803986) A[3]:(0.580941200256)\n",
      " state (2)  A[0]:(0.57471036911) A[1]:(0.70098555088) A[2]:(0.577200651169) A[3]:(0.607154250145)\n",
      " state (3)  A[0]:(0.642677903175) A[1]:(-0.0241670478135) A[2]:(0.380323320627) A[3]:(0.638339519501)\n",
      " state (4)  A[0]:(0.59300506115) A[1]:(0.659692764282) A[2]:(0.00152432802133) A[3]:(0.534801840782)\n",
      " state (5)  A[0]:(-0.0467152372003) A[1]:(0.9996727705) A[2]:(-0.554120182991) A[3]:(-0.121282853186)\n",
      " state (6)  A[0]:(0.0148729737848) A[1]:(0.779606223106) A[2]:(-0.0182752534747) A[3]:(0.598003387451)\n",
      " state (7)  A[0]:(0.595234394073) A[1]:(-0.68926692009) A[2]:(0.390745192766) A[3]:(0.884549975395)\n",
      " state (8)  A[0]:(0.660390973091) A[1]:(-0.00688629876822) A[2]:(0.73272806406) A[3]:(0.593829393387)\n",
      " state (9)  A[0]:(0.665222525597) A[1]:(0.810828208923) A[2]:(0.776567816734) A[3]:(0.0266830455512)\n",
      " state (10)  A[0]:(0.741665959358) A[1]:(0.864000558853) A[2]:(0.00158297887538) A[3]:(0.480081170797)\n",
      " state (11)  A[0]:(0.204778671265) A[1]:(0.872301697731) A[2]:(-0.867043316364) A[3]:(0.705471813679)\n",
      " state (12)  A[0]:(-0.383249461651) A[1]:(0.869591116905) A[2]:(-0.848564743996) A[3]:(0.767783164978)\n",
      " state (13)  A[0]:(0.00888038147241) A[1]:(0.865894913673) A[2]:(0.898833870888) A[3]:(0.774708747864)\n",
      " state (14)  A[0]:(0.81356137991) A[1]:(0.861089527607) A[2]:(0.9999807477) A[3]:(0.764602661133)\n",
      " state (15)  A[0]:(0.98097217083) A[1]:(0.80056643486) A[2]:(1.0) A[3]:(0.820592939854)\n",
      "Episode 174000 finished after 0 timesteps with r=1.0. Running score: 0.74. Times trained:               6349. Times reached goal: 807.               Steps done: 1642429. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.174158667054.\n",
      "q_values \n",
      "tensor([[ 0.5320,  0.5936,  0.5683,  0.5320]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5931,  0.6582,  0.0019,  0.5329]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566, -0.0009,  0.7333,  0.5916]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6580,  0.8146,  0.7772,  0.0185]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0063,  0.8716,  0.9013,  0.7681]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8094,  0.8698,  1.0000,  0.7593]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531352639198) A[1]:(0.593716084957) A[2]:(0.567551374435) A[3]:(0.532336592674)\n",
      " state (1)  A[0]:(0.532546877861) A[1]:(-0.000110447406769) A[2]:(0.6309851408) A[3]:(0.578348696232)\n",
      " state (2)  A[0]:(0.574324250221) A[1]:(0.701155185699) A[2]:(0.57747387886) A[3]:(0.605706930161)\n",
      " state (3)  A[0]:(0.642976164818) A[1]:(-0.0243108645082) A[2]:(0.378871202469) A[3]:(0.635941803455)\n",
      " state (4)  A[0]:(0.59225076437) A[1]:(0.658003091812) A[2]:(0.001033067354) A[3]:(0.532424330711)\n",
      " state (5)  A[0]:(-0.0527621917427) A[1]:(0.999673545361) A[2]:(-0.550039708614) A[3]:(-0.12100353837)\n",
      " state (6)  A[0]:(0.0129252923653) A[1]:(0.781510591507) A[2]:(-0.0176786836237) A[3]:(0.596146166325)\n",
      " state (7)  A[0]:(0.594684302807) A[1]:(-0.687147855759) A[2]:(0.390405356884) A[3]:(0.884630858898)\n",
      " state (8)  A[0]:(0.654687047005) A[1]:(0.000871121650562) A[2]:(0.733236670494) A[3]:(0.589280962944)\n",
      " state (9)  A[0]:(0.656660914421) A[1]:(0.814946830273) A[2]:(0.777159571648) A[3]:(0.0156237287447)\n",
      " state (10)  A[0]:(0.735565662384) A[1]:(0.868923902512) A[2]:(0.00143456365913) A[3]:(0.474213391542)\n",
      " state (11)  A[0]:(0.189201503992) A[1]:(0.876957118511) A[2]:(-0.867399990559) A[3]:(0.69899392128)\n",
      " state (12)  A[0]:(-0.398242890835) A[1]:(0.873989164829) A[2]:(-0.848187565804) A[3]:(0.760055780411)\n",
      " state (13)  A[0]:(-0.00653656944633) A[1]:(0.871653735638) A[2]:(0.900973558426) A[3]:(0.767395615578)\n",
      " state (14)  A[0]:(0.809404432774) A[1]:(0.869861900806) A[2]:(0.999981760979) A[3]:(0.758989393711)\n",
      " state (15)  A[0]:(0.980555772781) A[1]:(0.817064225674) A[2]:(1.0) A[3]:(0.817673802376)\n",
      "Episode 175000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6380. Times reached goal: 824.               Steps done: 1648809. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.173051071744.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.5315579772) A[1]:(0.593366146088) A[2]:(0.566385388374) A[3]:(0.532701075077)\n",
      " state (1)  A[0]:(0.531375646591) A[1]:(-0.00048008558224) A[2]:(0.630488038063) A[3]:(0.577462852001)\n",
      " state (2)  A[0]:(0.573419034481) A[1]:(0.700312972069) A[2]:(0.578287124634) A[3]:(0.606201291084)\n",
      " state (3)  A[0]:(0.642923295498) A[1]:(-0.0251005906612) A[2]:(0.378313630819) A[3]:(0.635614335537)\n",
      " state (4)  A[0]:(0.590478539467) A[1]:(0.658446669579) A[2]:(0.00169265107252) A[3]:(0.531758666039)\n",
      " state (5)  A[0]:(-0.0613666586578) A[1]:(0.999672114849) A[2]:(-0.543418288231) A[3]:(-0.119217522442)\n",
      " state (6)  A[0]:(0.0112162893638) A[1]:(0.779173493385) A[2]:(-0.0164919439703) A[3]:(0.595487058163)\n",
      " state (7)  A[0]:(0.598212182522) A[1]:(-0.688874363899) A[2]:(0.386893302202) A[3]:(0.885417580605)\n",
      " state (8)  A[0]:(0.655964493752) A[1]:(-0.0041991523467) A[2]:(0.731456398964) A[3]:(0.589845061302)\n",
      " state (9)  A[0]:(0.657416701317) A[1]:(0.81280040741) A[2]:(0.777194142342) A[3]:(0.0128605132923)\n",
      " state (10)  A[0]:(0.741385996342) A[1]:(0.867264449596) A[2]:(0.00539392977953) A[3]:(0.476475924253)\n",
      " state (11)  A[0]:(0.204996228218) A[1]:(0.873413026333) A[2]:(-0.866360545158) A[3]:(0.698451340199)\n",
      " state (12)  A[0]:(-0.388325065374) A[1]:(0.868507146835) A[2]:(-0.848112940788) A[3]:(0.756476461887)\n",
      " state (13)  A[0]:(0.00122797430959) A[1]:(0.866385757923) A[2]:(0.901239871979) A[3]:(0.763108968735)\n",
      " state (14)  A[0]:(0.811524987221) A[1]:(0.866890609264) A[2]:(0.999982178211) A[3]:(0.755527257919)\n",
      " state (15)  A[0]:(0.980672121048) A[1]:(0.817180514336) A[2]:(1.0) A[3]:(0.815640807152)\n",
      "Episode 176000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained:               6488. Times reached goal: 839.               Steps done: 1655297. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.171931950745.\n",
      " state (0)  A[0]:(0.532076358795) A[1]:(0.591651618481) A[2]:(0.566208481789) A[3]:(0.531846225262)\n",
      " state (1)  A[0]:(0.529966473579) A[1]:(0.000586669833865) A[2]:(0.628066062927) A[3]:(0.576237857342)\n",
      " state (2)  A[0]:(0.57183098793) A[1]:(0.70055603981) A[2]:(0.576943278313) A[3]:(0.606515407562)\n",
      " state (3)  A[0]:(0.642445921898) A[1]:(-0.0239825341851) A[2]:(0.375150382519) A[3]:(0.635564684868)\n",
      " state (4)  A[0]:(0.589133858681) A[1]:(0.657270491123) A[2]:(0.000129699707031) A[3]:(0.532635986805)\n",
      " state (5)  A[0]:(-0.0680271461606) A[1]:(0.999672889709) A[2]:(-0.537820279598) A[3]:(-0.116081036627)\n",
      " state (6)  A[0]:(0.0117981620133) A[1]:(0.77991515398) A[2]:(-0.0133909322321) A[3]:(0.595594704151)\n",
      " state (7)  A[0]:(0.600225806236) A[1]:(-0.689306616783) A[2]:(0.388126641512) A[3]:(0.886356711388)\n",
      " state (8)  A[0]:(0.651504576206) A[1]:(-0.0013495265739) A[2]:(0.732784807682) A[3]:(0.587187051773)\n",
      " state (9)  A[0]:(0.65369695425) A[1]:(0.812640666962) A[2]:(0.777169167995) A[3]:(0.0144129246473)\n",
      " state (10)  A[0]:(0.741170525551) A[1]:(0.867228746414) A[2]:(0.00118815840688) A[3]:(0.486726343632)\n",
      " state (11)  A[0]:(0.202464058995) A[1]:(0.871869087219) A[2]:(-0.868502140045) A[3]:(0.705179750919)\n",
      " state (12)  A[0]:(-0.393775463104) A[1]:(0.865354657173) A[2]:(-0.851091206074) A[3]:(0.760691046715)\n",
      " state (13)  A[0]:(-0.0049381153658) A[1]:(0.863875448704) A[2]:(0.900417149067) A[3]:(0.767553210258)\n",
      " state (14)  A[0]:(0.810268938541) A[1]:(0.867133438587) A[2]:(0.999982476234) A[3]:(0.761527657509)\n",
      " state (15)  A[0]:(0.980546891689) A[1]:(0.821905195713) A[2]:(1.0) A[3]:(0.821087479591)\n",
      "Episode 177000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained:               6386. Times reached goal: 836.               Steps done: 1661683. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.170837491635.\n",
      " state (0)  A[0]:(0.536819219589) A[1]:(0.593632996082) A[2]:(0.568454504013) A[3]:(0.534912645817)\n",
      " state (1)  A[0]:(0.533584833145) A[1]:(0.0029774592258) A[2]:(0.631494998932) A[3]:(0.579211890697)\n",
      " state (2)  A[0]:(0.575415313244) A[1]:(0.70537674427) A[2]:(0.580082535744) A[3]:(0.61107480526)\n",
      " state (3)  A[0]:(0.646539509296) A[1]:(-0.0150958979502) A[2]:(0.376665502787) A[3]:(0.639163017273)\n",
      " state (4)  A[0]:(0.592486977577) A[1]:(0.660934925079) A[2]:(0.00127494265325) A[3]:(0.537176132202)\n",
      " state (5)  A[0]:(-0.0671827569604) A[1]:(0.999676346779) A[2]:(-0.53372335434) A[3]:(-0.105369783938)\n",
      " state (6)  A[0]:(0.0196668468416) A[1]:(0.781221687794) A[2]:(-0.0141514632851) A[3]:(0.601961016655)\n",
      " state (7)  A[0]:(0.609420180321) A[1]:(-0.690691530704) A[2]:(0.387020021677) A[3]:(0.889832258224)\n",
      " state (8)  A[0]:(0.661078929901) A[1]:(-0.00297202030197) A[2]:(0.734156131744) A[3]:(0.595343947411)\n",
      " state (9)  A[0]:(0.668964087963) A[1]:(0.811338126659) A[2]:(0.779858589172) A[3]:(0.030609132722)\n",
      " state (10)  A[0]:(0.757995009422) A[1]:(0.865933179855) A[2]:(0.011118311435) A[3]:(0.506033778191)\n",
      " state (11)  A[0]:(0.237826555967) A[1]:(0.868340134621) A[2]:(-0.86644166708) A[3]:(0.716754555702)\n",
      " state (12)  A[0]:(-0.370821088552) A[1]:(0.859493434429) A[2]:(-0.851225376129) A[3]:(0.768190264702)\n",
      " state (13)  A[0]:(0.0124185848981) A[1]:(0.858458578587) A[2]:(0.899371027946) A[3]:(0.774967908859)\n",
      " state (14)  A[0]:(0.814136266708) A[1]:(0.865038156509) A[2]:(0.999982714653) A[3]:(0.770340979099)\n",
      " state (15)  A[0]:(0.980769753456) A[1]:(0.824630379677) A[2]:(1.0) A[3]:(0.828218221664)\n",
      "Episode 178000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6486. Times reached goal: 821.               Steps done: 1668169. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.16973302532.\n",
      " state (0)  A[0]:(0.531583249569) A[1]:(0.59299659729) A[2]:(0.569152116776) A[3]:(0.532927632332)\n",
      " state (1)  A[0]:(0.531481027603) A[1]:(0.000763714138884) A[2]:(0.632576107979) A[3]:(0.577044427395)\n",
      " state (2)  A[0]:(0.574186146259) A[1]:(0.703720808029) A[2]:(0.581400692463) A[3]:(0.609894514084)\n",
      " state (3)  A[0]:(0.64616549015) A[1]:(-0.0175612401217) A[2]:(0.376285493374) A[3]:(0.636928677559)\n",
      " state (4)  A[0]:(0.590668320656) A[1]:(0.658445835114) A[2]:(0.000722885015421) A[3]:(0.535109519958)\n",
      " state (5)  A[0]:(-0.0778046771884) A[1]:(0.999674618244) A[2]:(-0.532325387001) A[3]:(-0.104250915349)\n",
      " state (6)  A[0]:(0.0119636757299) A[1]:(0.780386805534) A[2]:(-0.0184627063572) A[3]:(0.600933790207)\n",
      " state (7)  A[0]:(0.606547474861) A[1]:(-0.68900346756) A[2]:(0.38037532568) A[3]:(0.890253663063)\n",
      " state (8)  A[0]:(0.655360937119) A[1]:(-0.000274837017059) A[2]:(0.730923891068) A[3]:(0.594588577747)\n",
      " state (9)  A[0]:(0.659867942333) A[1]:(0.812531113625) A[2]:(0.77796959877) A[3]:(0.0246646739542)\n",
      " state (10)  A[0]:(0.751062989235) A[1]:(0.86769926548) A[2]:(0.00617448566481) A[3]:(0.502916514874)\n",
      " state (11)  A[0]:(0.220309615135) A[1]:(0.868671774864) A[2]:(-0.867653012276) A[3]:(0.711493313313)\n",
      " state (12)  A[0]:(-0.386373072863) A[1]:(0.857998967171) A[2]:(-0.851629137993) A[3]:(0.761097848415)\n",
      " state (13)  A[0]:(-0.00288810534403) A[1]:(0.857421159744) A[2]:(0.900838434696) A[3]:(0.768276453018)\n",
      " state (14)  A[0]:(0.809461593628) A[1]:(0.866756439209) A[2]:(0.999983549118) A[3]:(0.765303254128)\n",
      " state (15)  A[0]:(0.980170667171) A[1]:(0.831100463867) A[2]:(1.0) A[3]:(0.825418770313)\n",
      "Episode 179000 finished after 0 timesteps with r=1.0. Running score: 0.8. Times trained:               6360. Times reached goal: 821.               Steps done: 1674529. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.16865694883.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5940,  0.5706,  0.5340]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.6589,  0.0013,  0.5340]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560, -0.0069,  0.7324,  0.5915]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6586,  0.8112,  0.7798,  0.0142]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0074,  0.8554,  0.9001,  0.7651]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8141,  0.8673,  1.0000,  0.7650]], device='cuda:0')\n",
      "On state=14, selected action=0 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0083,  0.8546,  0.8987,  0.7672]], device='cuda:0')\n",
      "On state=13, selected action=3 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6644,  0.8103,  0.7798,  0.0247]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0100,  0.8541,  0.8984,  0.7699]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8134,  0.8675,  1.0000,  0.7680]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533568918705) A[1]:(0.592750191689) A[2]:(0.569489896297) A[3]:(0.534219741821)\n",
      " state (1)  A[0]:(0.532888948917) A[1]:(-0.00130002875812) A[2]:(0.631800889969) A[3]:(0.57807970047)\n",
      " state (2)  A[0]:(0.575521111488) A[1]:(0.702818393707) A[2]:(0.581081390381) A[3]:(0.611979961395)\n",
      " state (3)  A[0]:(0.648310422897) A[1]:(-0.0183994788677) A[2]:(0.374243110418) A[3]:(0.638100504875)\n",
      " state (4)  A[0]:(0.591877698898) A[1]:(0.658865988255) A[2]:(-0.000889181857929) A[3]:(0.536370754242)\n",
      " state (5)  A[0]:(-0.0796720981598) A[1]:(0.999676048756) A[2]:(-0.528290212154) A[3]:(-0.0999225974083)\n",
      " state (6)  A[0]:(0.0178904067725) A[1]:(0.781080365181) A[2]:(-0.0155178960413) A[3]:(0.601501822472)\n",
      " state (7)  A[0]:(0.61271905899) A[1]:(-0.689673185349) A[2]:(0.382245987654) A[3]:(0.891346514225)\n",
      " state (8)  A[0]:(0.659122347832) A[1]:(-0.00310413283296) A[2]:(0.73207116127) A[3]:(0.59617960453)\n",
      " state (9)  A[0]:(0.662765204906) A[1]:(0.811949133873) A[2]:(0.779097914696) A[3]:(0.0252033136785)\n",
      " state (10)  A[0]:(0.754768252373) A[1]:(0.868348002434) A[2]:(0.00679911626503) A[3]:(0.50776296854)\n",
      " state (11)  A[0]:(0.227134659886) A[1]:(0.868240118027) A[2]:(-0.868523359299) A[3]:(0.713235497475)\n",
      " state (12)  A[0]:(-0.381666094065) A[1]:(0.856062412262) A[2]:(-0.853732168674) A[3]:(0.760790646076)\n",
      " state (13)  A[0]:(0.00332029443234) A[1]:(0.856257081032) A[2]:(0.899695217609) A[3]:(0.768478691578)\n",
      " state (14)  A[0]:(0.81209397316) A[1]:(0.868677854538) A[2]:(0.999983787537) A[3]:(0.766956925392)\n",
      " state (15)  A[0]:(0.980458319187) A[1]:(0.838128685951) A[2]:(1.0) A[3]:(0.827232718468)\n",
      "Episode 180000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained:               6350. Times reached goal: 824.               Steps done: 1680879. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.167589370353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.533360242844) A[1]:(0.593225598335) A[2]:(0.568751573563) A[3]:(0.533393979073)\n",
      " state (1)  A[0]:(0.532644033432) A[1]:(0.000510454119649) A[2]:(0.633770287037) A[3]:(0.576318025589)\n",
      " state (2)  A[0]:(0.575337588787) A[1]:(0.704137444496) A[2]:(0.58266723156) A[3]:(0.611111819744)\n",
      " state (3)  A[0]:(0.64896351099) A[1]:(-0.0160270314664) A[2]:(0.37405487895) A[3]:(0.636350572109)\n",
      " state (4)  A[0]:(0.591307163239) A[1]:(0.657887458801) A[2]:(3.7670135498e-05) A[3]:(0.535088002682)\n",
      " state (5)  A[0]:(-0.0878951475024) A[1]:(0.99967610836) A[2]:(-0.522779226303) A[3]:(-0.0979104340076)\n",
      " state (6)  A[0]:(0.0139875672758) A[1]:(0.781258106232) A[2]:(-0.0155630661175) A[3]:(0.600583672523)\n",
      " state (7)  A[0]:(0.611184358597) A[1]:(-0.689234972) A[2]:(0.378906935453) A[3]:(0.891634702682)\n",
      " state (8)  A[0]:(0.655138671398) A[1]:(-0.00297068664804) A[2]:(0.730996608734) A[3]:(0.594294786453)\n",
      " state (9)  A[0]:(0.659067988396) A[1]:(0.811942815781) A[2]:(0.778880953789) A[3]:(0.0220785010606)\n",
      " state (10)  A[0]:(0.753704309464) A[1]:(0.869453549385) A[2]:(0.00534231355414) A[3]:(0.509921848774)\n",
      " state (11)  A[0]:(0.223586842418) A[1]:(0.868430793285) A[2]:(-0.869525372982) A[3]:(0.713084101677)\n",
      " state (12)  A[0]:(-0.386975079775) A[1]:(0.854839742184) A[2]:(-0.855668306351) A[3]:(0.758965373039)\n",
      " state (13)  A[0]:(-0.00362287368625) A[1]:(0.855615198612) A[2]:(0.898757696152) A[3]:(0.767747759819)\n",
      " state (14)  A[0]:(0.80984210968) A[1]:(0.870659708977) A[2]:(0.999984145164) A[3]:(0.768558740616)\n",
      " state (15)  A[0]:(0.980188429356) A[1]:(0.844423830509) A[2]:(1.0) A[3]:(0.829671800137)\n",
      "Episode 181000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6306. Times reached goal: 827.               Steps done: 1687185. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.16653587694.\n",
      " state (0)  A[0]:(0.533633828163) A[1]:(0.590821027756) A[2]:(0.570999920368) A[3]:(0.533363938332)\n",
      " state (1)  A[0]:(0.531391263008) A[1]:(-0.00254678912461) A[2]:(0.632576406002) A[3]:(0.575851976871)\n",
      " state (2)  A[0]:(0.574038624763) A[1]:(0.70421653986) A[2]:(0.58242225647) A[3]:(0.611215174198)\n",
      " state (3)  A[0]:(0.648821115494) A[1]:(-0.0155474608764) A[2]:(0.372764438391) A[3]:(0.635586500168)\n",
      " state (4)  A[0]:(0.590695381165) A[1]:(0.657264351845) A[2]:(0.000420689553721) A[3]:(0.534764885902)\n",
      " state (5)  A[0]:(-0.0906322002411) A[1]:(0.999677121639) A[2]:(-0.517177283764) A[3]:(-0.0939034968615)\n",
      " state (6)  A[0]:(0.0203694980592) A[1]:(0.782003045082) A[2]:(-0.0129594700411) A[3]:(0.600804150105)\n",
      " state (7)  A[0]:(0.618227243423) A[1]:(-0.689826667309) A[2]:(0.379844486713) A[3]:(0.892395675182)\n",
      " state (8)  A[0]:(0.660169005394) A[1]:(-0.00440762331709) A[2]:(0.732145309448) A[3]:(0.594356656075)\n",
      " state (9)  A[0]:(0.661779761314) A[1]:(0.812310218811) A[2]:(0.780004262924) A[3]:(0.0174992699176)\n",
      " state (10)  A[0]:(0.754553377628) A[1]:(0.870459854603) A[2]:(0.00362192001194) A[3]:(0.507552623749)\n",
      " state (11)  A[0]:(0.221919521689) A[1]:(0.867678880692) A[2]:(-0.870814979076) A[3]:(0.708485007286)\n",
      " state (12)  A[0]:(-0.38484954834) A[1]:(0.851402103901) A[2]:(-0.856430530548) A[3]:(0.753461062908)\n",
      " state (13)  A[0]:(0.00605766521767) A[1]:(0.851876735687) A[2]:(0.899887979031) A[3]:(0.764137625694)\n",
      " state (14)  A[0]:(0.814406573772) A[1]:(0.869451761246) A[2]:(0.99998486042) A[3]:(0.767727851868)\n",
      " state (15)  A[0]:(0.980672776699) A[1]:(0.846655249596) A[2]:(1.0) A[3]:(0.830397903919)\n",
      "Episode 182000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained:               6339. Times reached goal: 830.               Steps done: 1693524. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.165483544906.\n",
      " state (0)  A[0]:(0.534671783447) A[1]:(0.591597437859) A[2]:(0.569154858589) A[3]:(0.533455014229)\n",
      " state (1)  A[0]:(0.533343315125) A[1]:(-0.000978164025582) A[2]:(0.633535206318) A[3]:(0.575204491615)\n",
      " state (2)  A[0]:(0.576390087605) A[1]:(0.704003930092) A[2]:(0.583669543266) A[3]:(0.610728025436)\n",
      " state (3)  A[0]:(0.651792883873) A[1]:(-0.0155690768734) A[2]:(0.372135341167) A[3]:(0.634211599827)\n",
      " state (4)  A[0]:(0.592179775238) A[1]:(0.657648921013) A[2]:(-0.000820874993224) A[3]:(0.534088611603)\n",
      " state (5)  A[0]:(-0.0960535034537) A[1]:(0.999680995941) A[2]:(-0.515857934952) A[3]:(-0.0883086100221)\n",
      " state (6)  A[0]:(0.0214980635792) A[1]:(0.785134732723) A[2]:(-0.016794770956) A[3]:(0.602448225021)\n",
      " state (7)  A[0]:(0.619116783142) A[1]:(-0.688532948494) A[2]:(0.375632345676) A[3]:(0.893330097198)\n",
      " state (8)  A[0]:(0.656614243984) A[1]:(-0.00317398877814) A[2]:(0.731392264366) A[3]:(0.59143692255)\n",
      " state (9)  A[0]:(0.659756064415) A[1]:(0.811864733696) A[2]:(0.780020952225) A[3]:(0.0138903865591)\n",
      " state (10)  A[0]:(0.755836844444) A[1]:(0.870947241783) A[2]:(0.0034688571468) A[3]:(0.510368704796)\n",
      " state (11)  A[0]:(0.223581969738) A[1]:(0.867336690426) A[2]:(-0.871397078037) A[3]:(0.708443164825)\n",
      " state (12)  A[0]:(-0.388553202152) A[1]:(0.849942743778) A[2]:(-0.857935905457) A[3]:(0.750495672226)\n",
      " state (13)  A[0]:(-0.00232597766444) A[1]:(0.851397454739) A[2]:(0.899515390396) A[3]:(0.76094865799)\n",
      " state (14)  A[0]:(0.811317443848) A[1]:(0.871555626392) A[2]:(0.999985218048) A[3]:(0.766073107719)\n",
      " state (15)  A[0]:(0.98032104969) A[1]:(0.852337419987) A[2]:(1.0) A[3]:(0.830206334591)\n",
      "Episode 183000 finished after 0 timesteps with r=1.0. Running score: 0.75. Times trained:               6399. Times reached goal: 831.               Steps done: 1699923. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.164427996531.\n",
      " state (0)  A[0]:(0.533646404743) A[1]:(0.592285394669) A[2]:(0.570335388184) A[3]:(0.532677292824)\n",
      " state (1)  A[0]:(0.532637238503) A[1]:(-0.000399745971663) A[2]:(0.634590744972) A[3]:(0.575523734093)\n",
      " state (2)  A[0]:(0.576059043407) A[1]:(0.70433229208) A[2]:(0.586212217808) A[3]:(0.611165523529)\n",
      " state (3)  A[0]:(0.652231216431) A[1]:(-0.0153893372044) A[2]:(0.373559653759) A[3]:(0.633160412312)\n",
      " state (4)  A[0]:(0.590728700161) A[1]:(0.657732009888) A[2]:(0.000251770019531) A[3]:(0.53273832798)\n",
      " state (5)  A[0]:(-0.106128081679) A[1]:(0.999681532383) A[2]:(-0.512910366058) A[3]:(-0.085642196238)\n",
      " state (6)  A[0]:(0.0166267026216) A[1]:(0.785000383854) A[2]:(-0.0169340875) A[3]:(0.601989507675)\n",
      " state (7)  A[0]:(0.617214500904) A[1]:(-0.686253368855) A[2]:(0.373182415962) A[3]:(0.893524646759)\n",
      " state (8)  A[0]:(0.651053071022) A[1]:(0.00207789684646) A[2]:(0.730846762657) A[3]:(0.58913731575)\n",
      " state (9)  A[0]:(0.653740286827) A[1]:(0.813954293728) A[2]:(0.7807071805) A[3]:(0.0097725559026)\n",
      " state (10)  A[0]:(0.75423181057) A[1]:(0.873405218124) A[2]:(0.00706017203629) A[3]:(0.511299550533)\n",
      " state (11)  A[0]:(0.221753016114) A[1]:(0.868797898293) A[2]:(-0.870486140251) A[3]:(0.70679295063)\n",
      " state (12)  A[0]:(-0.392347514629) A[1]:(0.850049257278) A[2]:(-0.857094943523) A[3]:(0.746205151081)\n",
      " state (13)  A[0]:(-0.0089811058715) A[1]:(0.851951003075) A[2]:(0.901096582413) A[3]:(0.756959676743)\n",
      " state (14)  A[0]:(0.808708310127) A[1]:(0.874076664448) A[2]:(0.999985873699) A[3]:(0.764343857765)\n",
      " state (15)  A[0]:(0.979977071285) A[1]:(0.857951045036) A[2]:(1.0) A[3]:(0.830427110195)\n",
      "Episode 184000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6343. Times reached goal: 835.               Steps done: 1706266. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.163388330536.\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.5917,  0.5702,  0.5305]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5911,  0.6560, -0.0004,  0.5302]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6529,  0.0002,  0.7311,  0.5886]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.8133,  0.7818,  0.0111]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0035,  0.8501,  0.9015,  0.7579]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8113,  0.8738,  1.0000,  0.7681]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532064557076) A[1]:(0.592684984207) A[2]:(0.571324586868) A[3]:(0.531249523163)\n",
      " state (1)  A[0]:(0.532196998596) A[1]:(0.00134394981433) A[2]:(0.635833799839) A[3]:(0.574726641178)\n",
      " state (2)  A[0]:(0.576511025429) A[1]:(0.704114377499) A[2]:(0.588582873344) A[3]:(0.610727071762)\n",
      " state (3)  A[0]:(0.654033660889) A[1]:(-0.016602197662) A[2]:(0.374953210354) A[3]:(0.631675064564)\n",
      " state (4)  A[0]:(0.591457009315) A[1]:(0.65660482645) A[2]:(0.00196134787984) A[3]:(0.531227111816)\n",
      " state (5)  A[0]:(-0.111948542297) A[1]:(0.999681770802) A[2]:(-0.507970452309) A[3]:(-0.0841622874141)\n",
      " state (6)  A[0]:(0.0203539337963) A[1]:(0.784961819649) A[2]:(-0.0131264533848) A[3]:(0.60103905201)\n",
      " state (7)  A[0]:(0.622939586639) A[1]:(-0.686886370182) A[2]:(0.374547839165) A[3]:(0.894224047661)\n",
      " state (8)  A[0]:(0.655298709869) A[1]:(-0.00230146548711) A[2]:(0.73179101944) A[3]:(0.591448843479)\n",
      " state (9)  A[0]:(0.658196687698) A[1]:(0.812422394753) A[2]:(0.782298922539) A[3]:(0.014004775323)\n",
      " state (10)  A[0]:(0.760671377182) A[1]:(0.8731123209) A[2]:(0.0104019464925) A[3]:(0.520437836647)\n",
      " state (11)  A[0]:(0.236478999257) A[1]:(0.867228507996) A[2]:(-0.870625138283) A[3]:(0.711796879768)\n",
      " state (12)  A[0]:(-0.383745878935) A[1]:(0.846599519253) A[2]:(-0.858571648598) A[3]:(0.748109340668)\n",
      " state (13)  A[0]:(-0.00211709411815) A[1]:(0.848901391029) A[2]:(0.900564074516) A[3]:(0.758952617645)\n",
      " state (14)  A[0]:(0.811283171177) A[1]:(0.873313903809) A[2]:(0.999986112118) A[3]:(0.768456220627)\n",
      " state (15)  A[0]:(0.980331003666) A[1]:(0.859499573708) A[2]:(1.0) A[3]:(0.834985375404)\n",
      "Episode 185000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6406. Times reached goal: 843.               Steps done: 1712672. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.162345010213.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.535652220249) A[1]:(0.592059135437) A[2]:(0.574836194515) A[3]:(0.534681916237)\n",
      " state (1)  A[0]:(0.534392952919) A[1]:(-0.00103855098132) A[2]:(0.636511564255) A[3]:(0.578423261642)\n",
      " state (2)  A[0]:(0.578503012657) A[1]:(0.706110715866) A[2]:(0.589342832565) A[3]:(0.615114927292)\n",
      " state (3)  A[0]:(0.656709551811) A[1]:(-0.012661495246) A[2]:(0.373471736908) A[3]:(0.634862959385)\n",
      " state (4)  A[0]:(0.593326568604) A[1]:(0.65515267849) A[2]:(0.000165104866028) A[3]:(0.53584253788)\n",
      " state (5)  A[0]:(-0.117517098784) A[1]:(0.999680638313) A[2]:(-0.505014657974) A[3]:(-0.0750279799104)\n",
      " state (6)  A[0]:(0.0230497866869) A[1]:(0.783829569817) A[2]:(-0.0106410542503) A[3]:(0.6046667099)\n",
      " state (7)  A[0]:(0.627982020378) A[1]:(-0.68721550703) A[2]:(0.37383544445) A[3]:(0.896190106869)\n",
      " state (8)  A[0]:(0.659399688244) A[1]:(-0.00520725315437) A[2]:(0.730926156044) A[3]:(0.596612095833)\n",
      " state (9)  A[0]:(0.663805425167) A[1]:(0.811372160912) A[2]:(0.782197296619) A[3]:(0.0215251576155)\n",
      " state (10)  A[0]:(0.769415974617) A[1]:(0.872929692268) A[2]:(0.00994661450386) A[3]:(0.533241987228)\n",
      " state (11)  A[0]:(0.258718848228) A[1]:(0.865561425686) A[2]:(-0.871796905994) A[3]:(0.720111548901)\n",
      " state (12)  A[0]:(-0.370109856129) A[1]:(0.842994213104) A[2]:(-0.861850976944) A[3]:(0.752735674381)\n",
      " state (13)  A[0]:(0.00689264666289) A[1]:(0.845927715302) A[2]:(0.898267865181) A[3]:(0.762518405914)\n",
      " state (14)  A[0]:(0.813663184643) A[1]:(0.872885286808) A[2]:(0.999986112118) A[3]:(0.772866785526)\n",
      " state (15)  A[0]:(0.980584144592) A[1]:(0.861609101295) A[2]:(1.0) A[3]:(0.838984727859)\n",
      "Episode 186000 finished after 0 timesteps with r=0.0. Running score: 0.82. Times trained:               6290. Times reached goal: 823.               Steps done: 1718962. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.161327064893.\n",
      " state (0)  A[0]:(0.533348083496) A[1]:(0.591397404671) A[2]:(0.573845386505) A[3]:(0.533413767815)\n",
      " state (1)  A[0]:(0.531766533852) A[1]:(0.000231899321079) A[2]:(0.636524796486) A[3]:(0.577244877815)\n",
      " state (2)  A[0]:(0.575843691826) A[1]:(0.708273410797) A[2]:(0.589101433754) A[3]:(0.614545881748)\n",
      " state (3)  A[0]:(0.655378222466) A[1]:(-0.00712557742372) A[2]:(0.372082769871) A[3]:(0.633292913437)\n",
      " state (4)  A[0]:(0.590637922287) A[1]:(0.657900214195) A[2]:(-0.000130295753479) A[3]:(0.534097254276)\n",
      " state (5)  A[0]:(-0.12630559504) A[1]:(0.99968367815) A[2]:(-0.500420570374) A[3]:(-0.0744580477476)\n",
      " state (6)  A[0]:(0.0242894794792) A[1]:(0.785197854042) A[2]:(-0.0109936334193) A[3]:(0.60237121582)\n",
      " state (7)  A[0]:(0.62993991375) A[1]:(-0.684806704521) A[2]:(0.369877129793) A[3]:(0.895610630512)\n",
      " state (8)  A[0]:(0.654469490051) A[1]:(-0.00146158679854) A[2]:(0.730407059193) A[3]:(0.588656425476)\n",
      " state (9)  A[0]:(0.657501220703) A[1]:(0.811541974545) A[2]:(0.781846404076) A[3]:(0.0110010830685)\n",
      " state (10)  A[0]:(0.76590526104) A[1]:(0.873785495758) A[2]:(0.00372586911544) A[3]:(0.530865073204)\n",
      " state (11)  A[0]:(0.249738246202) A[1]:(0.865426361561) A[2]:(-0.874222159386) A[3]:(0.716405391693)\n",
      " state (12)  A[0]:(-0.378185629845) A[1]:(0.841289401054) A[2]:(-0.863379061222) A[3]:(0.746663868427)\n",
      " state (13)  A[0]:(0.000719651463442) A[1]:(0.844782531261) A[2]:(0.900227725506) A[3]:(0.756712496281)\n",
      " state (14)  A[0]:(0.812700152397) A[1]:(0.873714804649) A[2]:(0.999986886978) A[3]:(0.769453704357)\n",
      " state (15)  A[0]:(0.98052418232) A[1]:(0.864471018314) A[2]:(1.0) A[3]:(0.838204324245)\n",
      "Episode 187000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained:               6300. Times reached goal: 806.               Steps done: 1725262. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.160313899207.\n",
      " state (0)  A[0]:(0.535241246223) A[1]:(0.593993484974) A[2]:(0.575449824333) A[3]:(0.534250974655)\n",
      " state (1)  A[0]:(0.535034179688) A[1]:(3.57329845428e-05) A[2]:(0.639548420906) A[3]:(0.578737139702)\n",
      " state (2)  A[0]:(0.579607725143) A[1]:(0.709035456181) A[2]:(0.590258955956) A[3]:(0.61622095108)\n",
      " state (3)  A[0]:(0.659631609917) A[1]:(-0.00496481731534) A[2]:(0.371385097504) A[3]:(0.633656859398)\n",
      " state (4)  A[0]:(0.59339761734) A[1]:(0.659504890442) A[2]:(-0.000500917376485) A[3]:(0.534708619118)\n",
      " state (5)  A[0]:(-0.131395608187) A[1]:(0.99968701601) A[2]:(-0.495918810368) A[3]:(-0.0698903650045)\n",
      " state (6)  A[0]:(0.0276883691549) A[1]:(0.787427723408) A[2]:(-0.00978736579418) A[3]:(0.603413283825)\n",
      " state (7)  A[0]:(0.634098172188) A[1]:(-0.682772994041) A[2]:(0.36818215251) A[3]:(0.896824121475)\n",
      " state (8)  A[0]:(0.656709730625) A[1]:(0.00249358499423) A[2]:(0.731057465076) A[3]:(0.590641081333)\n",
      " state (9)  A[0]:(0.660179078579) A[1]:(0.814155399799) A[2]:(0.784208416939) A[3]:(0.0132841104642)\n",
      " state (10)  A[0]:(0.770830392838) A[1]:(0.876980602741) A[2]:(0.00890398304909) A[3]:(0.539257884026)\n",
      " state (11)  A[0]:(0.259890973568) A[1]:(0.868138253689) A[2]:(-0.873950481415) A[3]:(0.72089189291)\n",
      " state (12)  A[0]:(-0.377215534449) A[1]:(0.843472957611) A[2]:(-0.863891363144) A[3]:(0.747062981129)\n",
      " state (13)  A[0]:(-0.00501065235585) A[1]:(0.847984552383) A[2]:(0.901461720467) A[3]:(0.755729854107)\n",
      " state (14)  A[0]:(0.810429215431) A[1]:(0.878353774548) A[2]:(0.99998742342) A[3]:(0.769738256931)\n",
      " state (15)  A[0]:(0.9803160429) A[1]:(0.871390342712) A[2]:(1.0) A[3]:(0.839721858501)\n",
      "Episode 188000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6348. Times reached goal: 839.               Steps done: 1731610. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.159299449844.\n",
      " state (0)  A[0]:(0.531658530235) A[1]:(0.593090295792) A[2]:(0.576718091965) A[3]:(0.533042669296)\n",
      " state (1)  A[0]:(0.530307412148) A[1]:(0.000421851844294) A[2]:(0.638375163078) A[3]:(0.578857660294)\n",
      " state (2)  A[0]:(0.57538163662) A[1]:(0.711421251297) A[2]:(0.590778112411) A[3]:(0.617009818554)\n",
      " state (3)  A[0]:(0.657164573669) A[1]:(-0.00121679843869) A[2]:(0.371146023273) A[3]:(0.63317912817)\n",
      " state (4)  A[0]:(0.588926970959) A[1]:(0.657465577126) A[2]:(4.67300415039e-05) A[3]:(0.535050690174)\n",
      " state (5)  A[0]:(-0.149905577302) A[1]:(0.999686539173) A[2]:(-0.492895960808) A[3]:(-0.0655919760466)\n",
      " state (6)  A[0]:(0.018894540146) A[1]:(0.78780412674) A[2]:(-0.00960785429925) A[3]:(0.604941725731)\n",
      " state (7)  A[0]:(0.632889270782) A[1]:(-0.682703018188) A[2]:(0.366357505322) A[3]:(0.898351550102)\n",
      " state (8)  A[0]:(0.654817938805) A[1]:(-0.00208535487764) A[2]:(0.731364250183) A[3]:(0.594712615013)\n",
      " state (9)  A[0]:(0.658009409904) A[1]:(0.812018632889) A[2]:(0.785643100739) A[3]:(0.0180877391249)\n",
      " state (10)  A[0]:(0.771248936653) A[1]:(0.876048982143) A[2]:(0.00926885288209) A[3]:(0.54738175869)\n",
      " state (11)  A[0]:(0.261570215225) A[1]:(0.865529716015) A[2]:(-0.875288248062) A[3]:(0.724758386612)\n",
      " state (12)  A[0]:(-0.376570433378) A[1]:(0.838086724281) A[2]:(-0.866567790508) A[3]:(0.748310923576)\n",
      " state (13)  A[0]:(-0.00376073736697) A[1]:(0.842623889446) A[2]:(0.900122463703) A[3]:(0.757609963417)\n",
      " state (14)  A[0]:(0.81142359972) A[1]:(0.875709354877) A[2]:(0.999987602234) A[3]:(0.773901462555)\n",
      " state (15)  A[0]:(0.980460166931) A[1]:(0.870944976807) A[2]:(1.0) A[3]:(0.844112217426)\n",
      "Episode 189000 finished after 0 timesteps with r=1.0. Running score: 0.77. Times trained:               6237. Times reached goal: 811.               Steps done: 1737847. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.158308991131.\n",
      "q_values \n",
      "tensor([[ 0.5333,  0.5932,  0.5752,  0.5337]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5924,  0.6579,  0.0021,  0.5354]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6588, -0.0033,  0.7324,  0.5977]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6581,  0.8131,  0.7867,  0.0170]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0069,  0.8454,  0.8998,  0.7566]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.8809,  1.0000,  0.7746]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533031761646) A[1]:(0.592416286469) A[2]:(0.574288368225) A[3]:(0.533473730087)\n",
      " state (1)  A[0]:(0.532195925713) A[1]:(-0.00163578835782) A[2]:(0.638125658035) A[3]:(0.579413115978)\n",
      " state (2)  A[0]:(0.57793545723) A[1]:(0.709096550941) A[2]:(0.590775489807) A[3]:(0.617944717407)\n",
      " state (3)  A[0]:(0.660701572895) A[1]:(-0.00442120525986) A[2]:(0.369987130165) A[3]:(0.632901191711)\n",
      " state (4)  A[0]:(0.590953469276) A[1]:(0.656745910645) A[2]:(-1.19209289551e-05) A[3]:(0.534386277199)\n",
      " state (5)  A[0]:(-0.157507553697) A[1]:(0.999688386917) A[2]:(-0.487827390432) A[3]:(-0.0654833689332)\n",
      " state (6)  A[0]:(0.0182414241135) A[1]:(0.78945016861) A[2]:(-0.00495596649125) A[3]:(0.601991415024)\n",
      " state (7)  A[0]:(0.633334040642) A[1]:(-0.680321931839) A[2]:(0.368147850037) A[3]:(0.897950410843)\n",
      " state (8)  A[0]:(0.651640474796) A[1]:(0.000337362289429) A[2]:(0.73191678524) A[3]:(0.591160655022)\n",
      " state (9)  A[0]:(0.651714384556) A[1]:(0.813946545124) A[2]:(0.786074280739) A[3]:(0.00879488606006)\n",
      " state (10)  A[0]:(0.767624497414) A[1]:(0.879135012627) A[2]:(0.00588195677847) A[3]:(0.545604586601)\n",
      " state (11)  A[0]:(0.252931892872) A[1]:(0.868562161922) A[2]:(-0.877278089523) A[3]:(0.722320795059)\n",
      " state (12)  A[0]:(-0.385251820087) A[1]:(0.840817451477) A[2]:(-0.86884355545) A[3]:(0.744058489799)\n",
      " state (13)  A[0]:(-0.012679612264) A[1]:(0.846486210823) A[2]:(0.900261938572) A[3]:(0.754462003708)\n",
      " state (14)  A[0]:(0.809147238731) A[1]:(0.881230533123) A[2]:(0.999988019466) A[3]:(0.773759901524)\n",
      " state (15)  A[0]:(0.980268478394) A[1]:(0.879245221615) A[2]:(1.0) A[3]:(0.845800817013)\n",
      "Episode 190000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained:               6386. Times reached goal: 850.               Steps done: 1744233. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.157301251053.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.534053325653) A[1]:(0.591721534729) A[2]:(0.575708448887) A[3]:(0.532418966293)\n",
      " state (1)  A[0]:(0.532739400864) A[1]:(0.000366933614714) A[2]:(0.639484763145) A[3]:(0.579478919506)\n",
      " state (2)  A[0]:(0.578301072121) A[1]:(0.71120095253) A[2]:(0.592907786369) A[3]:(0.618966698647)\n",
      " state (3)  A[0]:(0.662223935127) A[1]:(9.91895794868e-05) A[2]:(0.371403992176) A[3]:(0.632897675037)\n",
      " state (4)  A[0]:(0.591434180737) A[1]:(0.658730208874) A[2]:(0.000427722901804) A[3]:(0.53437268734)\n",
      " state (5)  A[0]:(-0.163088485599) A[1]:(0.999689877033) A[2]:(-0.487380862236) A[3]:(-0.0621987096965)\n",
      " state (6)  A[0]:(0.0242599025369) A[1]:(0.789580643177) A[2]:(-0.00903104990721) A[3]:(0.603219151497)\n",
      " state (7)  A[0]:(0.641944646835) A[1]:(-0.679408669472) A[2]:(0.362282931805) A[3]:(0.899437427521)\n",
      " state (8)  A[0]:(0.660370230675) A[1]:(-0.00183056830429) A[2]:(0.730698347092) A[3]:(0.595715284348)\n",
      " state (9)  A[0]:(0.662251234055) A[1]:(0.812170267105) A[2]:(0.787113428116) A[3]:(0.0170419998467)\n",
      " state (10)  A[0]:(0.778228759766) A[1]:(0.878059923649) A[2]:(0.00899433717132) A[3]:(0.557049274445)\n",
      " state (11)  A[0]:(0.278035610914) A[1]:(0.865307986736) A[2]:(-0.877327620983) A[3]:(0.7283872962)\n",
      " state (12)  A[0]:(-0.36722278595) A[1]:(0.834294319153) A[2]:(-0.870239794254) A[3]:(0.746556520462)\n",
      " state (13)  A[0]:(0.00260558142327) A[1]:(0.840245366096) A[2]:(0.899573981762) A[3]:(0.756687521935)\n",
      " state (14)  A[0]:(0.813217520714) A[1]:(0.878341794014) A[2]:(0.999988257885) A[3]:(0.777724504471)\n",
      " state (15)  A[0]:(0.98061466217) A[1]:(0.878856420517) A[2]:(1.0) A[3]:(0.849792599678)\n",
      "Episode 191000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6268. Times reached goal: 841.               Steps done: 1750501. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.156318370377.\n",
      " state (0)  A[0]:(0.531962871552) A[1]:(0.591646432877) A[2]:(0.576878547668) A[3]:(0.532725334167)\n",
      " state (1)  A[0]:(0.530256807804) A[1]:(-0.000120364129543) A[2]:(0.640051662922) A[3]:(0.578796744347)\n",
      " state (2)  A[0]:(0.576179027557) A[1]:(0.710257709026) A[2]:(0.594617247581) A[3]:(0.618166923523)\n",
      " state (3)  A[0]:(0.661333918571) A[1]:(-0.00238319044001) A[2]:(0.372598528862) A[3]:(0.630347967148)\n",
      " state (4)  A[0]:(0.589127421379) A[1]:(0.656204044819) A[2]:(0.00328027014621) A[3]:(0.531357526779)\n",
      " state (5)  A[0]:(-0.172978192568) A[1]:(0.99968880415) A[2]:(-0.480846226215) A[3]:(-0.0624291971326)\n",
      " state (6)  A[0]:(0.0224767159671) A[1]:(0.789352476597) A[2]:(-0.0060725659132) A[3]:(0.600409626961)\n",
      " state (7)  A[0]:(0.641129732132) A[1]:(-0.679829955101) A[2]:(0.361470937729) A[3]:(0.898617684841)\n",
      " state (8)  A[0]:(0.65399992466) A[1]:(-0.00618899147958) A[2]:(0.730685114861) A[3]:(0.587248563766)\n",
      " state (9)  A[0]:(0.654324412346) A[1]:(0.810303866863) A[2]:(0.78780323267) A[3]:(0.000425040692789)\n",
      " state (10)  A[0]:(0.77593255043) A[1]:(0.877742707729) A[2]:(0.0100760627538) A[3]:(0.550531804562)\n",
      " state (11)  A[0]:(0.277422994375) A[1]:(0.863784968853) A[2]:(-0.877802848816) A[3]:(0.722565948963)\n",
      " state (12)  A[0]:(-0.367761850357) A[1]:(0.830467998981) A[2]:(-0.871852874756) A[3]:(0.738389015198)\n",
      " state (13)  A[0]:(0.00125764240511) A[1]:(0.836873114109) A[2]:(0.89879077673) A[3]:(0.749050319195)\n",
      " state (14)  A[0]:(0.812818050385) A[1]:(0.877802014351) A[2]:(0.999988555908) A[3]:(0.772767722607)\n",
      " state (15)  A[0]:(0.980563282967) A[1]:(0.880779981613) A[2]:(1.0) A[3]:(0.847600579262)\n",
      "Episode 192000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained:               6328. Times reached goal: 835.               Steps done: 1756829. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.155332310912.\n",
      " state (0)  A[0]:(0.531355023384) A[1]:(0.591025292873) A[2]:(0.57662332058) A[3]:(0.532942533493)\n",
      " state (1)  A[0]:(0.530642986298) A[1]:(-0.00145739212167) A[2]:(0.63957130909) A[3]:(0.57967877388)\n",
      " state (2)  A[0]:(0.577188253403) A[1]:(0.711862444878) A[2]:(0.594124555588) A[3]:(0.62017250061)\n",
      " state (3)  A[0]:(0.663215339184) A[1]:(0.000995695241727) A[2]:(0.369766294956) A[3]:(0.631421387196)\n",
      " state (4)  A[0]:(0.588936328888) A[1]:(0.657939553261) A[2]:(-0.000841736618895) A[3]:(0.533003211021)\n",
      " state (5)  A[0]:(-0.183382898569) A[1]:(0.999692380428) A[2]:(-0.481125891209) A[3]:(-0.0558326169848)\n",
      " state (6)  A[0]:(0.022579645738) A[1]:(0.791920483112) A[2]:(-0.00858869869262) A[3]:(0.603395342827)\n",
      " state (7)  A[0]:(0.643461465836) A[1]:(-0.677040696144) A[2]:(0.358296185732) A[3]:(0.900274395943)\n",
      " state (8)  A[0]:(0.652603149414) A[1]:(0.00106936646625) A[2]:(0.730451345444) A[3]:(0.588897645473)\n",
      " state (9)  A[0]:(0.655431747437) A[1]:(0.813064515591) A[2]:(0.788003206253) A[3]:(0.00960147660226)\n",
      " state (10)  A[0]:(0.779599189758) A[1]:(0.880575060844) A[2]:(0.00965195707977) A[3]:(0.564995646477)\n",
      " state (11)  A[0]:(0.284192740917) A[1]:(0.866011023521) A[2]:(-0.878387272358) A[3]:(0.730731010437)\n",
      " state (12)  A[0]:(-0.370138972998) A[1]:(0.831766605377) A[2]:(-0.87254011631) A[3]:(0.741530776024)\n",
      " state (13)  A[0]:(-0.00837348215282) A[1]:(0.838665127754) A[2]:(0.900467813015) A[3]:(0.750160694122)\n",
      " state (14)  A[0]:(0.809307157993) A[1]:(0.880449354649) A[2]:(0.99998909235) A[3]:(0.77503734827)\n",
      " state (15)  A[0]:(0.980237305164) A[1]:(0.884347915649) A[2]:(1.0) A[3]:(0.850661218166)\n",
      "Episode 193000 finished after 0 timesteps with r=1.0. Running score: 0.8. Times trained:               6379. Times reached goal: 830.               Steps done: 1763208. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.154344599755.\n",
      " state (0)  A[0]:(0.533275783062) A[1]:(0.593655943871) A[2]:(0.579074263573) A[3]:(0.531478703022)\n",
      " state (1)  A[0]:(0.534587264061) A[1]:(0.00158130249474) A[2]:(0.643092870712) A[3]:(0.577981948853)\n",
      " state (2)  A[0]:(0.582345724106) A[1]:(0.712792396545) A[2]:(0.597880840302) A[3]:(0.618511795998)\n",
      " state (3)  A[0]:(0.66880607605) A[1]:(0.00256119854748) A[2]:(0.372891813517) A[3]:(0.628685951233)\n",
      " state (4)  A[0]:(0.593409180641) A[1]:(0.657400727272) A[2]:(0.0024759718217) A[3]:(0.530730485916)\n",
      " state (5)  A[0]:(-0.189048796892) A[1]:(0.999691426754) A[2]:(-0.475991457701) A[3]:(-0.053742762655)\n",
      " state (6)  A[0]:(0.0254569333047) A[1]:(0.79059535265) A[2]:(-0.00357483304106) A[3]:(0.603199005127)\n",
      " state (7)  A[0]:(0.64801800251) A[1]:(-0.676906049252) A[2]:(0.359149068594) A[3]:(0.90107178688)\n",
      " state (8)  A[0]:(0.656435012817) A[1]:(-0.0035303758923) A[2]:(0.73091840744) A[3]:(0.591296195984)\n",
      " state (9)  A[0]:(0.655251026154) A[1]:(0.812141776085) A[2]:(0.789327383041) A[3]:(0.00511340796947)\n",
      " state (10)  A[0]:(0.779608011246) A[1]:(0.88110178709) A[2]:(0.00646677520126) A[3]:(0.565936088562)\n",
      " state (11)  A[0]:(0.284567385912) A[1]:(0.865519702435) A[2]:(-0.881046593189) A[3]:(0.730566680431)\n",
      " state (12)  A[0]:(-0.367784947157) A[1]:(0.829224705696) A[2]:(-0.876122713089) A[3]:(0.739935219288)\n",
      " state (13)  A[0]:(-0.00120812596288) A[1]:(0.836497187614) A[2]:(0.898918390274) A[3]:(0.74992120266)\n",
      " state (14)  A[0]:(0.813134789467) A[1]:(0.880608558655) A[2]:(0.999989211559) A[3]:(0.777465462685)\n",
      " state (15)  A[0]:(0.980746388435) A[1]:(0.886523008347) A[2]:(1.0) A[3]:(0.853781282902)\n",
      "Episode 194000 finished after 0 timesteps with r=1.0. Running score: 0.78. Times trained:               6276. Times reached goal: 821.               Steps done: 1769484. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.153378966374.\n",
      "q_values \n",
      "tensor([[ 0.5302,  0.5915,  0.5772,  0.5310]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5893,  0.6569,  0.0012,  0.5323]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6563, -0.0044,  0.7296,  0.5937]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6554,  0.8110,  0.7888,  0.0104]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0030,  0.8320,  0.8995,  0.7504]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8144,  0.8793,  1.0000,  0.7788]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530357718468) A[1]:(0.591431081295) A[2]:(0.576873958111) A[3]:(0.531494855881)\n",
      " state (1)  A[0]:(0.529907941818) A[1]:(0.000166177749634) A[2]:(0.640539884567) A[3]:(0.579515814781)\n",
      " state (2)  A[0]:(0.578215837479) A[1]:(0.71218419075) A[2]:(0.59688770771) A[3]:(0.620485603809)\n",
      " state (3)  A[0]:(0.666540086269) A[1]:(0.00253009260632) A[2]:(0.370912879705) A[3]:(0.629670619965)\n",
      " state (4)  A[0]:(0.589420258999) A[1]:(0.657520294189) A[2]:(0.00105440581683) A[3]:(0.53234732151)\n",
      " state (5)  A[0]:(-0.202214241028) A[1]:(0.999694228172) A[2]:(-0.473772406578) A[3]:(-0.0483044274151)\n",
      " state (6)  A[0]:(0.02348222211) A[1]:(0.792763531208) A[2]:(-0.0041244989261) A[3]:(0.604602575302)\n",
      " state (7)  A[0]:(0.64966160059) A[1]:(-0.674144864082) A[2]:(0.356306642294) A[3]:(0.902057766914)\n",
      " state (8)  A[0]:(0.654502093792) A[1]:(-0.00100159610156) A[2]:(0.730199098587) A[3]:(0.59118103981)\n",
      " state (9)  A[0]:(0.653301715851) A[1]:(0.812320649624) A[2]:(0.789220273495) A[3]:(0.00632486259565)\n",
      " state (10)  A[0]:(0.780956327915) A[1]:(0.881629168987) A[2]:(0.00453349808231) A[3]:(0.573082923889)\n",
      " state (11)  A[0]:(0.289542108774) A[1]:(0.864609181881) A[2]:(-0.881950080395) A[3]:(0.733843803406)\n",
      " state (12)  A[0]:(-0.365500420332) A[1]:(0.826034188271) A[2]:(-0.876732587814) A[3]:(0.739362478256)\n",
      " state (13)  A[0]:(0.000209480524063) A[1]:(0.833762645721) A[2]:(0.900823175907) A[3]:(0.74840259552)\n",
      " state (14)  A[0]:(0.813924312592) A[1]:(0.880057692528) A[2]:(0.999989748001) A[3]:(0.777760148048)\n",
      " state (15)  A[0]:(0.98084884882) A[1]:(0.887373387814) A[2]:(1.0) A[3]:(0.855400383472)\n",
      "Episode 195000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained:               6206. Times reached goal: 817.               Steps done: 1775690. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.15243004406.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531760334969) A[1]:(0.591589331627) A[2]:(0.578543066978) A[3]:(0.531670808792)\n",
      " state (1)  A[0]:(0.531016886234) A[1]:(-1.38208270073e-05) A[2]:(0.641907215118) A[3]:(0.579650163651)\n",
      " state (2)  A[0]:(0.579162240028) A[1]:(0.712234258652) A[2]:(0.598407626152) A[3]:(0.620392024517)\n",
      " state (3)  A[0]:(0.668048679829) A[1]:(0.00316452677362) A[2]:(0.370939493179) A[3]:(0.628174185753)\n",
      " state (4)  A[0]:(0.589240193367) A[1]:(0.65622407198) A[2]:(-0.000163555145264) A[3]:(0.531262040138)\n",
      " state (5)  A[0]:(-0.213017195463) A[1]:(0.999692738056) A[2]:(-0.474666953087) A[3]:(-0.0444490797818)\n",
      " state (6)  A[0]:(0.0186244025826) A[1]:(0.79141664505) A[2]:(-0.00980047788471) A[3]:(0.60562735796)\n",
      " state (7)  A[0]:(0.648077726364) A[1]:(-0.673751235008) A[2]:(0.348872214556) A[3]:(0.90263158083)\n",
      " state (8)  A[0]:(0.650038123131) A[1]:(-0.0028297752142) A[2]:(0.728987932205) A[3]:(0.587592363358)\n",
      " state (9)  A[0]:(0.652535974979) A[1]:(0.810140252113) A[2]:(0.790133237839) A[3]:(0.004905810114)\n",
      " state (10)  A[0]:(0.784224033356) A[1]:(0.881070494652) A[2]:(0.00847104750574) A[3]:(0.579043388367)\n",
      " state (11)  A[0]:(0.298435240984) A[1]:(0.863364219666) A[2]:(-0.881614267826) A[3]:(0.736383974552)\n",
      " state (12)  A[0]:(-0.364531129599) A[1]:(0.823444485664) A[2]:(-0.877939522266) A[3]:(0.738041758537)\n",
      " state (13)  A[0]:(-0.00794575735927) A[1]:(0.832492768764) A[2]:(0.899873435497) A[3]:(0.746584057808)\n",
      " state (14)  A[0]:(0.809328138828) A[1]:(0.881482839584) A[2]:(0.999989926815) A[3]:(0.778273224831)\n",
      " state (15)  A[0]:(0.980228602886) A[1]:(0.891004443169) A[2]:(1.0) A[3]:(0.857226192951)\n",
      "Episode 196000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained:               6264. Times reached goal: 836.               Steps done: 1781954. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.151478206531.\n",
      " state (0)  A[0]:(0.534585654736) A[1]:(0.592652916908) A[2]:(0.581528365612) A[3]:(0.534032464027)\n",
      " state (1)  A[0]:(0.534236907959) A[1]:(-0.000177755951881) A[2]:(0.645108938217) A[3]:(0.582466840744)\n",
      " state (2)  A[0]:(0.582749962807) A[1]:(0.714698672295) A[2]:(0.600807726383) A[3]:(0.62355530262)\n",
      " state (3)  A[0]:(0.672122359276) A[1]:(0.00823084264994) A[2]:(0.371881246567) A[3]:(0.630254507065)\n",
      " state (4)  A[0]:(0.592739522457) A[1]:(0.657474756241) A[2]:(0.000156164169312) A[3]:(0.534436941147)\n",
      " state (5)  A[0]:(-0.215515449643) A[1]:(0.999696731567) A[2]:(-0.472362697124) A[3]:(-0.0353776328266)\n",
      " state (6)  A[0]:(0.025480221957) A[1]:(0.794602394104) A[2]:(-0.00974767282605) A[3]:(0.609756350517)\n",
      " state (7)  A[0]:(0.654094576836) A[1]:(-0.673029482365) A[2]:(0.34863704443) A[3]:(0.90449655056)\n",
      " state (8)  A[0]:(0.655131220818) A[1]:(-0.00195897859521) A[2]:(0.73059129715) A[3]:(0.590629458427)\n",
      " state (9)  A[0]:(0.661745548248) A[1]:(0.810891270638) A[2]:(0.792573928833) A[3]:(0.0137792751193)\n",
      " state (10)  A[0]:(0.794285178185) A[1]:(0.882272422314) A[2]:(0.0155925033614) A[3]:(0.592975735664)\n",
      " state (11)  A[0]:(0.324534863234) A[1]:(0.863443136215) A[2]:(-0.88066226244) A[3]:(0.74543595314)\n",
      " state (12)  A[0]:(-0.346089363098) A[1]:(0.821547210217) A[2]:(-0.878472268581) A[3]:(0.743765473366)\n",
      " state (13)  A[0]:(0.00479496316984) A[1]:(0.831036508083) A[2]:(0.899652600288) A[3]:(0.751412391663)\n",
      " state (14)  A[0]:(0.812156796455) A[1]:(0.882212460041) A[2]:(0.999990165234) A[3]:(0.784135699272)\n",
      " state (15)  A[0]:(0.980457127094) A[1]:(0.89354455471) A[2]:(1.0) A[3]:(0.862218558788)\n",
      "Episode 197000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained:               6257. Times reached goal: 841.               Steps done: 1788211. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.150533366408.\n",
      " state (0)  A[0]:(0.530954003334) A[1]:(0.590229153633) A[2]:(0.577317714691) A[3]:(0.530443131924)\n",
      " state (1)  A[0]:(0.529785037041) A[1]:(-0.000105641782284) A[2]:(0.642563581467) A[3]:(0.580451130867)\n",
      " state (2)  A[0]:(0.578911483288) A[1]:(0.717076778412) A[2]:(0.601484417915) A[3]:(0.622155308723)\n",
      " state (3)  A[0]:(0.670058310032) A[1]:(0.0141568034887) A[2]:(0.37217900157) A[3]:(0.627619147301)\n",
      " state (4)  A[0]:(0.589217782021) A[1]:(0.657950758934) A[2]:(0.00092387170298) A[3]:(0.531676888466)\n",
      " state (5)  A[0]:(-0.229230985045) A[1]:(0.999697268009) A[2]:(-0.469004303217) A[3]:(-0.0367903597653)\n",
      " state (6)  A[0]:(0.0225039571524) A[1]:(0.7945125103) A[2]:(-0.00643816636875) A[3]:(0.606553971767)\n",
      " state (7)  A[0]:(0.655461192131) A[1]:(-0.670885801315) A[2]:(0.348875761032) A[3]:(0.904209017754)\n",
      " state (8)  A[0]:(0.652676224709) A[1]:(-8.10846686363e-05) A[2]:(0.730782091618) A[3]:(0.588098883629)\n",
      " state (9)  A[0]:(0.654750823975) A[1]:(0.811614751816) A[2]:(0.792142748833) A[3]:(0.00896879844368)\n",
      " state (10)  A[0]:(0.787829518318) A[1]:(0.883779346943) A[2]:(0.00562280463055) A[3]:(0.593458175659)\n",
      " state (11)  A[0]:(0.30503988266) A[1]:(0.864387392998) A[2]:(-0.884401679039) A[3]:(0.743428468704)\n",
      " state (12)  A[0]:(-0.362163573503) A[1]:(0.821291983128) A[2]:(-0.881250858307) A[3]:(0.739289462566)\n",
      " state (13)  A[0]:(-0.00498444167897) A[1]:(0.831486940384) A[2]:(0.900713264942) A[3]:(0.747969269753)\n",
      " state (14)  A[0]:(0.811479091644) A[1]:(0.88416069746) A[2]:(0.999990642071) A[3]:(0.783810138702)\n",
      " state (15)  A[0]:(0.980552613735) A[1]:(0.896681249142) A[2]:(1.0) A[3]:(0.863683998585)\n",
      "Episode 198000 finished after 0 timesteps with r=1.0. Running score: 0.82. Times trained:               6184. Times reached goal: 823.               Steps done: 1794395. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.149605340484.\n",
      " state (0)  A[0]:(0.533860862255) A[1]:(0.591414690018) A[2]:(0.580572247505) A[3]:(0.533160328865)\n",
      " state (1)  A[0]:(0.531247138977) A[1]:(-0.000611901225056) A[2]:(0.644688546658) A[3]:(0.583710551262)\n",
      " state (2)  A[0]:(0.580018818378) A[1]:(0.716365337372) A[2]:(0.601397633553) A[3]:(0.625100970268)\n",
      " state (3)  A[0]:(0.671613991261) A[1]:(0.0130051197484) A[2]:(0.370026230812) A[3]:(0.629114985466)\n",
      " state (4)  A[0]:(0.589384436607) A[1]:(0.655691564083) A[2]:(-0.000105142593384) A[3]:(0.534138798714)\n",
      " state (5)  A[0]:(-0.239279553294) A[1]:(0.999696969986) A[2]:(-0.464359730482) A[3]:(-0.0302355792373)\n",
      " state (6)  A[0]:(0.0192195195705) A[1]:(0.79499745369) A[2]:(-0.00640860386193) A[3]:(0.608749389648)\n",
      " state (7)  A[0]:(0.655267834663) A[1]:(-0.668995141983) A[2]:(0.344534814358) A[3]:(0.90532296896)\n",
      " state (8)  A[0]:(0.650988817215) A[1]:(-0.000838875595946) A[2]:(0.730475306511) A[3]:(0.588878691196)\n",
      " state (9)  A[0]:(0.65502178669) A[1]:(0.810442149639) A[2]:(0.793683767319) A[3]:(0.0113059235737)\n",
      " state (10)  A[0]:(0.791205942631) A[1]:(0.883781909943) A[2]:(0.00848725903779) A[3]:(0.601627707481)\n",
      " state (11)  A[0]:(0.315301030874) A[1]:(0.863514065742) A[2]:(-0.884731829166) A[3]:(0.747994840145)\n",
      " state (12)  A[0]:(-0.357682168484) A[1]:(0.818689405918) A[2]:(-0.8828805089) A[3]:(0.740215361118)\n",
      " state (13)  A[0]:(-0.0066914903) A[1]:(0.829713106155) A[2]:(0.899985492229) A[3]:(0.747916162014)\n",
      " state (14)  A[0]:(0.809594511986) A[1]:(0.884726345539) A[2]:(0.999990820885) A[3]:(0.785264253616)\n",
      " state (15)  A[0]:(0.980257093906) A[1]:(0.898900091648) A[2]:(1.0) A[3]:(0.865704953671)\n",
      "Episode 199000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6286. Times reached goal: 821.               Steps done: 1800681. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.148667870867.\n",
      "q_values \n",
      "tensor([[ 0.5323,  0.5915,  0.5797,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5910,  0.6566, -0.0005,  0.5326]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6546, -0.0014,  0.7299,  0.5909]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6538,  0.8116,  0.7946,  0.0038]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0016,  0.8296,  0.8998,  0.7432]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8127,  0.8863,  1.0000,  0.7836]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531949877739) A[1]:(0.592221498489) A[2]:(0.57944715023) A[3]:(0.531564474106)\n",
      " state (1)  A[0]:(0.531209230423) A[1]:(-0.00109501136467) A[2]:(0.643345952034) A[3]:(0.582722663879)\n",
      " state (2)  A[0]:(0.581224322319) A[1]:(0.716575026512) A[2]:(0.59995496273) A[3]:(0.624273300171)\n",
      " state (3)  A[0]:(0.673946201801) A[1]:(0.0145535180345) A[2]:(0.367705971003) A[3]:(0.626859486103)\n",
      " state (4)  A[0]:(0.59078335762) A[1]:(0.656651616096) A[2]:(-0.000350475311279) A[3]:(0.531499743462)\n",
      " state (5)  A[0]:(-0.246124193072) A[1]:(0.99969959259) A[2]:(-0.458995521069) A[3]:(-0.0310488343239)\n",
      " state (6)  A[0]:(0.019887432456) A[1]:(0.796828567982) A[2]:(-0.00368641130626) A[3]:(0.605703890324)\n",
      " state (7)  A[0]:(0.657432556152) A[1]:(-0.666861593723) A[2]:(0.343349307775) A[3]:(0.905041873455)\n",
      " state (8)  A[0]:(0.652504920959) A[1]:(-0.0013122104574) A[2]:(0.730125844479) A[3]:(0.587714552879)\n",
      " state (9)  A[0]:(0.651319205761) A[1]:(0.811642706394) A[2]:(0.794547438622) A[3]:(-0.000717997434549)\n",
      " state (10)  A[0]:(0.78775370121) A[1]:(0.885813057423) A[2]:(0.00420830165967) A[3]:(0.595678985119)\n",
      " state (11)  A[0]:(0.306974798441) A[1]:(0.864998698235) A[2]:(-0.88711553812) A[3]:(0.741921246052)\n",
      " state (12)  A[0]:(-0.360938280821) A[1]:(0.818667471409) A[2]:(-0.885176062584) A[3]:(0.732260346413)\n",
      " state (13)  A[0]:(-0.00276903109625) A[1]:(0.82980376482) A[2]:(0.900193750858) A[3]:(0.742027163506)\n",
      " state (14)  A[0]:(0.81286662817) A[1]:(0.886349976063) A[2]:(0.999991238117) A[3]:(0.783321738243)\n",
      " state (15)  A[0]:(0.980731368065) A[1]:(0.901969254017) A[2]:(1.0) A[3]:(0.866105318069)\n",
      "Episode 200000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6283. Times reached goal: 848.               Steps done: 1806964. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.147736718912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531282901764) A[1]:(0.590628147125) A[2]:(0.580497384071) A[3]:(0.529854953289)\n",
      " state (1)  A[0]:(0.530311346054) A[1]:(-0.0010801400058) A[2]:(0.645193696022) A[3]:(0.581759691238)\n",
      " state (2)  A[0]:(0.58028280735) A[1]:(0.717637777328) A[2]:(0.601076841354) A[3]:(0.62373316288)\n",
      " state (3)  A[0]:(0.67378693819) A[1]:(0.0170480329543) A[2]:(0.367090165615) A[3]:(0.62517541647)\n",
      " state (4)  A[0]:(0.589077472687) A[1]:(0.656987428665) A[2]:(-0.000437736482127) A[3]:(0.530824303627)\n",
      " state (5)  A[0]:(-0.255230993032) A[1]:(0.999699831009) A[2]:(-0.454375952482) A[3]:(-0.023773541674)\n",
      " state (6)  A[0]:(0.020526073873) A[1]:(0.796594202518) A[2]:(-0.00613264506683) A[3]:(0.609642267227)\n",
      " state (7)  A[0]:(0.658786416054) A[1]:(-0.665908515453) A[2]:(0.337120771408) A[3]:(0.906331121922)\n",
      " state (8)  A[0]:(0.6480281353) A[1]:(0.00126589764841) A[2]:(0.729893326759) A[3]:(0.584227442741)\n",
      " state (9)  A[0]:(0.651997685432) A[1]:(0.810813605785) A[2]:(0.794889569283) A[3]:(0.0071905804798)\n",
      " state (10)  A[0]:(0.79152739048) A[1]:(0.885830640793) A[2]:(0.00509639130905) A[3]:(0.610128879547)\n",
      " state (11)  A[0]:(0.317092239857) A[1]:(0.864213228226) A[2]:(-0.887288987637) A[3]:(0.751030683517)\n",
      " state (12)  A[0]:(-0.357320845127) A[1]:(0.81612598896) A[2]:(-0.8864620924) A[3]:(0.737840056419)\n",
      " state (13)  A[0]:(-0.00683691864833) A[1]:(0.827893078327) A[2]:(0.899352908134) A[3]:(0.746313691139)\n",
      " state (14)  A[0]:(0.809726476669) A[1]:(0.88673132658) A[2]:(0.999991416931) A[3]:(0.788205981255)\n",
      " state (15)  A[0]:(0.980272293091) A[1]:(0.9039670825) A[2]:(1.0) A[3]:(0.870068192482)\n",
      "Episode 201000 finished after 0 timesteps with r=0.0. Running score: 0.84. Times trained:               6345. Times reached goal: 843.               Steps done: 1813309. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.146802297019.\n",
      " state (0)  A[0]:(0.532243013382) A[1]:(0.589999198914) A[2]:(0.58220243454) A[3]:(0.531709313393)\n",
      " state (1)  A[0]:(0.531407833099) A[1]:(-0.00040470060776) A[2]:(0.646711826324) A[3]:(0.584795236588)\n",
      " state (2)  A[0]:(0.581760823727) A[1]:(0.717621684074) A[2]:(0.602285027504) A[3]:(0.626465916634)\n",
      " state (3)  A[0]:(0.675866484642) A[1]:(0.0167703554034) A[2]:(0.367162257433) A[3]:(0.625980377197)\n",
      " state (4)  A[0]:(0.5898873806) A[1]:(0.657158195972) A[2]:(0.000236868858337) A[3]:(0.531772315502)\n",
      " state (5)  A[0]:(-0.260736018419) A[1]:(0.999701440334) A[2]:(-0.450645327568) A[3]:(-0.0159739367664)\n",
      " state (6)  A[0]:(0.025933098048) A[1]:(0.797872006893) A[2]:(-0.00927374046296) A[3]:(0.61325699091)\n",
      " state (7)  A[0]:(0.665609061718) A[1]:(-0.665748596191) A[2]:(0.331369549036) A[3]:(0.908284068108)\n",
      " state (8)  A[0]:(0.657669186592) A[1]:(-0.00202928204089) A[2]:(0.729306221008) A[3]:(0.591640233994)\n",
      " state (9)  A[0]:(0.657976031303) A[1]:(0.810887336731) A[2]:(0.796810746193) A[3]:(0.00894615054131)\n",
      " state (10)  A[0]:(0.791759669781) A[1]:(0.886939644814) A[2]:(0.00452551152557) A[3]:(0.611137747765)\n",
      " state (11)  A[0]:(0.311883300543) A[1]:(0.864430010319) A[2]:(-0.888797044754) A[3]:(0.748526930809)\n",
      " state (12)  A[0]:(-0.35815012455) A[1]:(0.814136743546) A[2]:(-0.888061523438) A[3]:(0.733442425728)\n",
      " state (13)  A[0]:(-0.000764339987654) A[1]:(0.825870454311) A[2]:(0.899064898491) A[3]:(0.744007945061)\n",
      " state (14)  A[0]:(0.812687397003) A[1]:(0.887088537216) A[2]:(0.999991714954) A[3]:(0.788744866848)\n",
      " state (15)  A[0]:(0.980573952198) A[1]:(0.906333208084) A[2]:(1.0) A[3]:(0.871342182159)\n",
      "Episode 202000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6443. Times reached goal: 851.               Steps done: 1819752. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.145859490332.\n",
      " state (0)  A[0]:(0.532680749893) A[1]:(0.592164635658) A[2]:(0.582984089851) A[3]:(0.53314769268)\n",
      " state (1)  A[0]:(0.532237648964) A[1]:(-8.05854797363e-05) A[2]:(0.647618055344) A[3]:(0.587609291077)\n",
      " state (2)  A[0]:(0.58287858963) A[1]:(0.718979716301) A[2]:(0.602641880512) A[3]:(0.629363298416)\n",
      " state (3)  A[0]:(0.677513837814) A[1]:(0.0199005547911) A[2]:(0.36508217454) A[3]:(0.626952528954)\n",
      " state (4)  A[0]:(0.590646743774) A[1]:(0.657330513) A[2]:(-0.000882148509845) A[3]:(0.533462285995)\n",
      " state (5)  A[0]:(-0.267776727676) A[1]:(0.999702453613) A[2]:(-0.445105373859) A[3]:(-0.00727280089632)\n",
      " state (6)  A[0]:(0.0229581184685) A[1]:(0.79813349247) A[2]:(-0.00831978209317) A[3]:(0.616993665695)\n",
      " state (7)  A[0]:(0.663866877556) A[1]:(-0.663443446159) A[2]:(0.328253030777) A[3]:(0.90940207243)\n",
      " state (8)  A[0]:(0.653367877007) A[1]:(0.000269174575806) A[2]:(0.728478729725) A[3]:(0.590200662613)\n",
      " state (9)  A[0]:(0.653101980686) A[1]:(0.811301529408) A[2]:(0.796426892281) A[3]:(0.00450622942299)\n",
      " state (10)  A[0]:(0.787838637829) A[1]:(0.888154685497) A[2]:(0.000657081487589) A[3]:(0.611244678497)\n",
      " state (11)  A[0]:(0.301400691271) A[1]:(0.865354418755) A[2]:(-0.889985859394) A[3]:(0.746289432049)\n",
      " state (12)  A[0]:(-0.365556627512) A[1]:(0.814041376114) A[2]:(-0.888743042946) A[3]:(0.728732228279)\n",
      " state (13)  A[0]:(-0.00630419561639) A[1]:(0.826263427734) A[2]:(0.900082707405) A[3]:(0.740503549576)\n",
      " state (14)  A[0]:(0.810795783997) A[1]:(0.888989448547) A[2]:(0.999992072582) A[3]:(0.78814804554)\n",
      " state (15)  A[0]:(0.980286002159) A[1]:(0.909527778625) A[2]:(1.0) A[3]:(0.872143864632)\n",
      "Episode 203000 finished after 0 timesteps with r=1.0. Running score: 0.79. Times trained:               6305. Times reached goal: 850.               Steps done: 1826057. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.14494273934.\n",
      " state (0)  A[0]:(0.531942248344) A[1]:(0.590781867504) A[2]:(0.582590460777) A[3]:(0.532077372074)\n",
      " state (1)  A[0]:(0.529561877251) A[1]:(-0.000925168104004) A[2]:(0.64660692215) A[3]:(0.586516141891)\n",
      " state (2)  A[0]:(0.579873800278) A[1]:(0.719787120819) A[2]:(0.602122664452) A[3]:(0.629074335098)\n",
      " state (3)  A[0]:(0.675753116608) A[1]:(0.0227897185832) A[2]:(0.363495260477) A[3]:(0.625335097313)\n",
      " state (4)  A[0]:(0.588438153267) A[1]:(0.656724333763) A[2]:(-0.000353693932993) A[3]:(0.532140493393)\n",
      " state (5)  A[0]:(-0.273831963539) A[1]:(0.999703288078) A[2]:(-0.438945025206) A[3]:(-0.00457238964736)\n",
      " state (6)  A[0]:(0.0253372937441) A[1]:(0.798997104168) A[2]:(-0.00917861890048) A[3]:(0.617020726204)\n",
      " state (7)  A[0]:(0.666969299316) A[1]:(-0.66332256794) A[2]:(0.324878185987) A[3]:(0.909859001637)\n",
      " state (8)  A[0]:(0.655838668346) A[1]:(-0.00104652310256) A[2]:(0.729764997959) A[3]:(0.589374661446)\n",
      " state (9)  A[0]:(0.654155194759) A[1]:(0.810802459717) A[2]:(0.798634946346) A[3]:(0.00348390708677)\n",
      " state (10)  A[0]:(0.784821271896) A[1]:(0.888455867767) A[2]:(0.000509619654622) A[3]:(0.612138271332)\n",
      " state (11)  A[0]:(0.289205431938) A[1]:(0.864596128464) A[2]:(-0.89121067524) A[3]:(0.744329690933)\n",
      " state (12)  A[0]:(-0.369716763496) A[1]:(0.810807585716) A[2]:(-0.890049278736) A[3]:(0.726268410683)\n",
      " state (13)  A[0]:(-0.0014445920242) A[1]:(0.823075711727) A[2]:(0.899440824986) A[3]:(0.74126124382)\n",
      " state (14)  A[0]:(0.813507974148) A[1]:(0.888745844364) A[2]:(0.999992311001) A[3]:(0.79152315855)\n",
      " state (15)  A[0]:(0.980547010899) A[1]:(0.911571502686) A[2]:(1.0) A[3]:(0.875039935112)\n",
      "Episode 204000 finished after 0 timesteps with r=1.0. Running score: 0.78. Times trained:               6308. Times reached goal: 868.               Steps done: 1832365. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.144031318185.\n",
      "q_values \n",
      "tensor([[ 0.5322,  0.5918,  0.5846,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5328,  0.0011,  0.6485,  0.5883]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5837,  0.7204,  0.6035,  0.6318]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0286,  0.8010, -0.0080,  0.6223]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7893,  0.8902,  0.0051,  0.6256]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8093,  0.8914,  1.0000,  0.7925]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53261834383) A[1]:(0.59021115303) A[2]:(0.583990812302) A[3]:(0.532071948051)\n",
      " state (1)  A[0]:(0.532188415527) A[1]:(-0.00137604691554) A[2]:(0.648048698902) A[3]:(0.588032305241)\n",
      " state (2)  A[0]:(0.582768440247) A[1]:(0.719143033028) A[2]:(0.603156089783) A[3]:(0.631181955338)\n",
      " state (3)  A[0]:(0.678398251534) A[1]:(0.0213574916124) A[2]:(0.362365871668) A[3]:(0.625874876976)\n",
      " state (4)  A[0]:(0.59021127224) A[1]:(0.65529602766) A[2]:(-0.000151753425598) A[3]:(0.533281087875)\n",
      " state (5)  A[0]:(-0.277799665928) A[1]:(0.99970382452) A[2]:(-0.431564867496) A[3]:(0.00260814442299)\n",
      " state (6)  A[0]:(0.023987563327) A[1]:(0.799734413624) A[2]:(-0.00719081889838) A[3]:(0.62069529295)\n",
      " state (7)  A[0]:(0.664707303047) A[1]:(-0.661400437355) A[2]:(0.322634190321) A[3]:(0.911080002785)\n",
      " state (8)  A[0]:(0.650158286095) A[1]:(0.00324973813258) A[2]:(0.729150950909) A[3]:(0.588357150555)\n",
      " state (9)  A[0]:(0.65170955658) A[1]:(0.812320649624) A[2]:(0.798415660858) A[3]:(0.0084937857464)\n",
      " state (10)  A[0]:(0.785587489605) A[1]:(0.890427589417) A[2]:(0.00335429841653) A[3]:(0.622575998306)\n",
      " state (11)  A[0]:(0.292827397585) A[1]:(0.866697311401) A[2]:(-0.890148878098) A[3]:(0.750828504562)\n",
      " state (12)  A[0]:(-0.371087759733) A[1]:(0.812644839287) A[2]:(-0.88930606842) A[3]:(0.729010283947)\n",
      " state (13)  A[0]:(-0.0106608252972) A[1]:(0.825298964977) A[2]:(0.900686979294) A[3]:(0.741831898689)\n",
      " state (14)  A[0]:(0.808455467224) A[1]:(0.891507506371) A[2]:(0.999992609024) A[3]:(0.792200088501)\n",
      " state (15)  A[0]:(0.979836344719) A[1]:(0.914939165115) A[2]:(1.0) A[3]:(0.875819087029)\n",
      "Episode 205000 finished after 0 timesteps with r=1.0. Running score: 0.8. Times trained:               6316. Times reached goal: 844.               Steps done: 1838681. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.143124483179.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532712101936) A[1]:(0.590852737427) A[2]:(0.582499027252) A[3]:(0.532290697098)\n",
      " state (1)  A[0]:(0.532133221626) A[1]:(-0.00110028637573) A[2]:(0.64780318737) A[3]:(0.587882637978)\n",
      " state (2)  A[0]:(0.58344244957) A[1]:(0.720860183239) A[2]:(0.604016184807) A[3]:(0.632215857506)\n",
      " state (3)  A[0]:(0.679967880249) A[1]:(0.024354185909) A[2]:(0.362165391445) A[3]:(0.625715434551)\n",
      " state (4)  A[0]:(0.591174244881) A[1]:(0.657016038895) A[2]:(0.000684261205606) A[3]:(0.533620238304)\n",
      " state (5)  A[0]:(-0.280367851257) A[1]:(0.999707460403) A[2]:(-0.425971984863) A[3]:(0.0102446665987)\n",
      " state (6)  A[0]:(0.0311890374869) A[1]:(0.802335679531) A[2]:(-0.00622014142573) A[3]:(0.624816596508)\n",
      " state (7)  A[0]:(0.670883893967) A[1]:(-0.662372112274) A[2]:(0.322025388479) A[3]:(0.912939488888)\n",
      " state (8)  A[0]:(0.657574415207) A[1]:(-0.00472612120211) A[2]:(0.730993390083) A[3]:(0.593775629997)\n",
      " state (9)  A[0]:(0.654615402222) A[1]:(0.809661746025) A[2]:(0.800769627094) A[3]:(0.00770020158961)\n",
      " state (10)  A[0]:(0.781805872917) A[1]:(0.889364242554) A[2]:(-0.00100398028735) A[3]:(0.621290385723)\n",
      " state (11)  A[0]:(0.277028501034) A[1]:(0.864292740822) A[2]:(-0.893072366714) A[3]:(0.747087001801)\n",
      " state (12)  A[0]:(-0.376081675291) A[1]:(0.80710542202) A[2]:(-0.892219483852) A[3]:(0.725189566612)\n",
      " state (13)  A[0]:(-0.00201123673469) A[1]:(0.819659531116) A[2]:(0.899120211601) A[3]:(0.741815924644)\n",
      " state (14)  A[0]:(0.81391108036) A[1]:(0.889264583588) A[2]:(0.999992787838) A[3]:(0.795075237751)\n",
      " state (15)  A[0]:(0.980537652969) A[1]:(0.914842188358) A[2]:(1.0) A[3]:(0.878442704678)\n",
      "Episode 206000 finished after 0 timesteps with r=1.0. Running score: 0.82. Times trained:               6358. Times reached goal: 866.               Steps done: 1845039. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.142217384437.\n",
      " state (0)  A[0]:(0.530610442162) A[1]:(0.589752972126) A[2]:(0.584198832512) A[3]:(0.532100856304)\n",
      " state (1)  A[0]:(0.529220581055) A[1]:(-0.00136986293364) A[2]:(0.648375153542) A[3]:(0.587730407715)\n",
      " state (2)  A[0]:(0.580685973167) A[1]:(0.720819830894) A[2]:(0.604347348213) A[3]:(0.632405638695)\n",
      " state (3)  A[0]:(0.678106725216) A[1]:(0.0238510128111) A[2]:(0.36029368639) A[3]:(0.624205946922)\n",
      " state (4)  A[0]:(0.588203430176) A[1]:(0.656303524971) A[2]:(-3.67164611816e-05) A[3]:(0.532379150391)\n",
      " state (5)  A[0]:(-0.287119328976) A[1]:(0.999706268311) A[2]:(-0.419376969337) A[3]:(0.0154533600435)\n",
      " state (6)  A[0]:(0.0315976142883) A[1]:(0.80074352026) A[2]:(-0.0052595846355) A[3]:(0.627351462841)\n",
      " state (7)  A[0]:(0.672053098679) A[1]:(-0.660826086998) A[2]:(0.31783798337) A[3]:(0.913837373257)\n",
      " state (8)  A[0]:(0.656898915768) A[1]:(-0.000853448873386) A[2]:(0.729564964771) A[3]:(0.591914236546)\n",
      " state (9)  A[0]:(0.656899809837) A[1]:(0.810739696026) A[2]:(0.800437629223) A[3]:(0.00880755018443)\n",
      " state (10)  A[0]:(0.785854160786) A[1]:(0.890728473663) A[2]:(0.00174653355498) A[3]:(0.628324627876)\n",
      " state (11)  A[0]:(0.289227515459) A[1]:(0.865383207798) A[2]:(-0.892241597176) A[3]:(0.751048982143)\n",
      " state (12)  A[0]:(-0.36880287528) A[1]:(0.807193934917) A[2]:(-0.892197370529) A[3]:(0.72535520792)\n",
      " state (13)  A[0]:(-0.00106209481601) A[1]:(0.819767057896) A[2]:(0.898987293243) A[3]:(0.740166544914)\n",
      " state (14)  A[0]:(0.812245965004) A[1]:(0.890371859074) A[2]:(0.999992907047) A[3]:(0.794125676155)\n",
      " state (15)  A[0]:(0.980202496052) A[1]:(0.916705787182) A[2]:(1.0) A[3]:(0.878265380859)\n",
      "Episode 207000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6309. Times reached goal: 850.               Steps done: 1851348. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.14132295939.\n",
      " state (0)  A[0]:(0.530604958534) A[1]:(0.591173708439) A[2]:(0.584899842739) A[3]:(0.531362295151)\n",
      " state (1)  A[0]:(0.531144976616) A[1]:(-0.00099135900382) A[2]:(0.649849414825) A[3]:(0.58730751276)\n",
      " state (2)  A[0]:(0.58324021101) A[1]:(0.722103238106) A[2]:(0.606445670128) A[3]:(0.632418990135)\n",
      " state (3)  A[0]:(0.680749058723) A[1]:(0.0269202347845) A[2]:(0.361039012671) A[3]:(0.622265100479)\n",
      " state (4)  A[0]:(0.589931547642) A[1]:(0.656281769276) A[2]:(0.00120794714894) A[3]:(0.530086815357)\n",
      " state (5)  A[0]:(-0.293654173613) A[1]:(0.999707341194) A[2]:(-0.414322823286) A[3]:(0.0160748828202)\n",
      " state (6)  A[0]:(0.0299300234765) A[1]:(0.801300644875) A[2]:(-0.00473471917212) A[3]:(0.625981032848)\n",
      " state (7)  A[0]:(0.672159612179) A[1]:(-0.659343421459) A[2]:(0.314426749945) A[3]:(0.913910865784)\n",
      " state (8)  A[0]:(0.657245993614) A[1]:(-0.00201870221645) A[2]:(0.728866457939) A[3]:(0.590380191803)\n",
      " state (9)  A[0]:(0.654873728752) A[1]:(0.810614705086) A[2]:(0.801667571068) A[3]:(-0.00083830935182)\n",
      " state (10)  A[0]:(0.783717155457) A[1]:(0.89144974947) A[2]:(0.003197301412) A[3]:(0.625175833702)\n",
      " state (11)  A[0]:(0.284434407949) A[1]:(0.865773320198) A[2]:(-0.89257311821) A[3]:(0.747123897076)\n",
      " state (12)  A[0]:(-0.371008694172) A[1]:(0.806634187698) A[2]:(-0.892435967922) A[3]:(0.71835142374)\n",
      " state (13)  A[0]:(-0.00115720881149) A[1]:(0.819747328758) A[2]:(0.900195240974) A[3]:(0.733323812485)\n",
      " state (14)  A[0]:(0.812388300896) A[1]:(0.891628205776) A[2]:(0.999993145466) A[3]:(0.789818167686)\n",
      " state (15)  A[0]:(0.980177640915) A[1]:(0.918698549271) A[2]:(1.0) A[3]:(0.876334607601)\n",
      "Episode 208000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6325. Times reached goal: 856.               Steps done: 1857673. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.140431912578.\n",
      " state (0)  A[0]:(0.531900465488) A[1]:(0.590137720108) A[2]:(0.585608422756) A[3]:(0.531851410866)\n",
      " state (1)  A[0]:(0.530194759369) A[1]:(-0.00107003701851) A[2]:(0.65042245388) A[3]:(0.589030265808)\n",
      " state (2)  A[0]:(0.581950306892) A[1]:(0.723056435585) A[2]:(0.606952250004) A[3]:(0.635402262211)\n",
      " state (3)  A[0]:(0.680294036865) A[1]:(0.0294364113361) A[2]:(0.35971057415) A[3]:(0.624131977558)\n",
      " state (4)  A[0]:(0.588016808033) A[1]:(0.65646982193) A[2]:(-0.000368952722056) A[3]:(0.532667517662)\n",
      " state (5)  A[0]:(-0.303860723972) A[1]:(0.999707996845) A[2]:(-0.412215262651) A[3]:(0.0239643454552)\n",
      " state (6)  A[0]:(0.0244944859296) A[1]:(0.801143884659) A[2]:(-0.00758609501645) A[3]:(0.629798233509)\n",
      " state (7)  A[0]:(0.669107377529) A[1]:(-0.658612132072) A[2]:(0.310066819191) A[3]:(0.914942502975)\n",
      " state (8)  A[0]:(0.650429129601) A[1]:(0.00102637673263) A[2]:(0.72973382473) A[3]:(0.585448503494)\n",
      " state (9)  A[0]:(0.653210282326) A[1]:(0.810324311256) A[2]:(0.802020251751) A[3]:(0.00474365986884)\n",
      " state (10)  A[0]:(0.782631516457) A[1]:(0.891749918461) A[2]:(0.00131511606742) A[3]:(0.635723233223)\n",
      " state (11)  A[0]:(0.279075175524) A[1]:(0.865792632103) A[2]:(-0.893412828445) A[3]:(0.753740489483)\n",
      " state (12)  A[0]:(-0.376993507147) A[1]:(0.805778086185) A[2]:(-0.893298983574) A[3]:(0.723437726498)\n",
      " state (13)  A[0]:(-0.00882729329169) A[1]:(0.819412827492) A[2]:(0.900412917137) A[3]:(0.738611876965)\n",
      " state (14)  A[0]:(0.809290409088) A[1]:(0.892606079578) A[2]:(0.999993383884) A[3]:(0.795762956142)\n",
      " state (15)  A[0]:(0.979765057564) A[1]:(0.92045289278) A[2]:(1.0) A[3]:(0.880785644054)\n",
      "Episode 209000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6258. Times reached goal: 833.               Steps done: 1863931. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.139555833778.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5915,  0.5853,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.6568,  0.0003,  0.5335]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6555, -0.0014,  0.7302,  0.5933]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6547,  0.8101,  0.8032,  0.0104]], device='cuda:0')\n",
      "On state=9, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6544, -0.0006,  0.7308,  0.5916]], device='cuda:0')\n",
      "On state=8, selected action=3 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5899,  0.6570,  0.0006,  0.5330]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6537, -0.0007,  0.7306,  0.5900]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6534,  0.8098,  0.8028,  0.0061]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0038,  0.8174,  0.8999,  0.7400]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8117,  0.8926,  1.0000,  0.7978]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530462443829) A[1]:(0.591518580914) A[2]:(0.585034132004) A[3]:(0.530748665333)\n",
      " state (1)  A[0]:(0.529698371887) A[1]:(-0.000205367803574) A[2]:(0.650812625885) A[3]:(0.587514996529)\n",
      " state (2)  A[0]:(0.58235681057) A[1]:(0.722834825516) A[2]:(0.607294023037) A[3]:(0.634919643402)\n",
      " state (3)  A[0]:(0.681626439095) A[1]:(0.0295482967049) A[2]:(0.35846889019) A[3]:(0.622420191765)\n",
      " state (4)  A[0]:(0.588784337044) A[1]:(0.657262504101) A[2]:(-0.000890135532245) A[3]:(0.53125846386)\n",
      " state (5)  A[0]:(-0.308309793472) A[1]:(0.999711811543) A[2]:(-0.408068835735) A[3]:(0.0272554885596)\n",
      " state (6)  A[0]:(0.0259630586952) A[1]:(0.803740203381) A[2]:(-0.00565940048546) A[3]:(0.630474328995)\n",
      " state (7)  A[0]:(0.670720756054) A[1]:(-0.656682729721) A[2]:(0.310295343399) A[3]:(0.915557086468)\n",
      " state (8)  A[0]:(0.651750147343) A[1]:(0.00054517382523) A[2]:(0.730164051056) A[3]:(0.585884809494)\n",
      " state (9)  A[0]:(0.65192759037) A[1]:(0.810412943363) A[2]:(0.802902460098) A[3]:(8.79764556885e-05)\n",
      " state (10)  A[0]:(0.780151069164) A[1]:(0.892456710339) A[2]:(0.000534176768269) A[3]:(0.635750770569)\n",
      " state (11)  A[0]:(0.273066997528) A[1]:(0.866081595421) A[2]:(-0.894253313541) A[3]:(0.752755522728)\n",
      " state (12)  A[0]:(-0.377566874027) A[1]:(0.804771125317) A[2]:(-0.894065976143) A[3]:(0.721306860447)\n",
      " state (13)  A[0]:(-0.00389126967639) A[1]:(0.818351745605) A[2]:(0.900682747364) A[3]:(0.737904071808)\n",
      " state (14)  A[0]:(0.811709403992) A[1]:(0.892876029015) A[2]:(0.999993562698) A[3]:(0.796966791153)\n",
      " state (15)  A[0]:(0.980040669441) A[1]:(0.921605527401) A[2]:(1.0) A[3]:(0.882223963737)\n",
      "Episode 210000 finished after 0 timesteps with r=1.0. Running score: 0.82. Times trained:               6170. Times reached goal: 846.               Steps done: 1870101. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.138697425198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530369877815) A[1]:(0.590857446194) A[2]:(0.5854357481) A[3]:(0.529961168766)\n",
      " state (1)  A[0]:(0.529215812683) A[1]:(6.6950917244e-05) A[2]:(0.650351166725) A[3]:(0.586843967438)\n",
      " state (2)  A[0]:(0.58161509037) A[1]:(0.724367618561) A[2]:(0.608171463013) A[3]:(0.635228872299)\n",
      " state (3)  A[0]:(0.681737184525) A[1]:(0.0340910851955) A[2]:(0.357986927032) A[3]:(0.621374726295)\n",
      " state (4)  A[0]:(0.587862372398) A[1]:(0.656880021095) A[2]:(-0.00114154769108) A[3]:(0.53088504076)\n",
      " state (5)  A[0]:(-0.319445341825) A[1]:(0.999711930752) A[2]:(-0.404958695173) A[3]:(0.0318507067859)\n",
      " state (6)  A[0]:(0.0185451544821) A[1]:(0.803041875362) A[2]:(-0.00601296313107) A[3]:(0.632141113281)\n",
      " state (7)  A[0]:(0.668456554413) A[1]:(-0.656354963779) A[2]:(0.307253271341) A[3]:(0.916477143764)\n",
      " state (8)  A[0]:(0.650569140911) A[1]:(0.000360920996172) A[2]:(0.729197025299) A[3]:(0.587437033653)\n",
      " state (9)  A[0]:(0.653076767921) A[1]:(0.810011208057) A[2]:(0.802520096302) A[3]:(0.00490062497556)\n",
      " state (10)  A[0]:(0.781346321106) A[1]:(0.892629146576) A[2]:(-0.00130104948767) A[3]:(0.642630934715)\n",
      " state (11)  A[0]:(0.276037931442) A[1]:(0.865647614002) A[2]:(-0.895029783249) A[3]:(0.756167054176)\n",
      " state (12)  A[0]:(-0.374764889479) A[1]:(0.802844405174) A[2]:(-0.895334124565) A[3]:(0.722259819508)\n",
      " state (13)  A[0]:(-0.00162097672001) A[1]:(0.816570162773) A[2]:(0.899829447269) A[3]:(0.738549590111)\n",
      " state (14)  A[0]:(0.811809778214) A[1]:(0.892767488956) A[2]:(0.999993681908) A[3]:(0.798598885536)\n",
      " state (15)  A[0]:(0.979969084263) A[1]:(0.92247068882) A[2]:(1.0) A[3]:(0.883758962154)\n",
      "Episode 211000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6223. Times reached goal: 849.               Steps done: 1876324. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.137836991138.\n",
      " state (0)  A[0]:(0.530924260616) A[1]:(0.591020822525) A[2]:(0.584965825081) A[3]:(0.530484318733)\n",
      " state (1)  A[0]:(0.530079603195) A[1]:(-0.000488653720822) A[2]:(0.650313615799) A[3]:(0.587524175644)\n",
      " state (2)  A[0]:(0.582485556602) A[1]:(0.723467826843) A[2]:(0.608984827995) A[3]:(0.636253118515)\n",
      " state (3)  A[0]:(0.683432877064) A[1]:(0.0329192951322) A[2]:(0.357854217291) A[3]:(0.620914459229)\n",
      " state (4)  A[0]:(0.589542031288) A[1]:(0.655164361) A[2]:(-0.000208377838135) A[3]:(0.531161427498)\n",
      " state (5)  A[0]:(-0.323495090008) A[1]:(0.999713778496) A[2]:(-0.400370776653) A[3]:(0.0369613692164)\n",
      " state (6)  A[0]:(0.0205378849059) A[1]:(0.804692149162) A[2]:(-0.00310610723682) A[3]:(0.633443593979)\n",
      " state (7)  A[0]:(0.670469760895) A[1]:(-0.655157327652) A[2]:(0.30824559927) A[3]:(0.917221903801)\n",
      " state (8)  A[0]:(0.651399254799) A[1]:(0.000369176239474) A[2]:(0.730185508728) A[3]:(0.588229060173)\n",
      " state (9)  A[0]:(0.651247084141) A[1]:(0.810579299927) A[2]:(0.803771257401) A[3]:(0.00287138624117)\n",
      " state (10)  A[0]:(0.778710722923) A[1]:(0.893779635429) A[2]:(-0.00116109801456) A[3]:(0.645167410374)\n",
      " state (11)  A[0]:(0.268641233444) A[1]:(0.866860091686) A[2]:(-0.895812571049) A[3]:(0.756999135017)\n",
      " state (12)  A[0]:(-0.379838973284) A[1]:(0.803670406342) A[2]:(-0.896269083023) A[3]:(0.72105127573)\n",
      " state (13)  A[0]:(-0.00457454985008) A[1]:(0.817442953587) A[2]:(0.900081694126) A[3]:(0.737413048744)\n",
      " state (14)  A[0]:(0.811561524868) A[1]:(0.894031226635) A[2]:(0.999993860722) A[3]:(0.798760652542)\n",
      " state (15)  A[0]:(0.980001270771) A[1]:(0.924009859562) A[2]:(1.0) A[3]:(0.884378194809)\n",
      "Episode 212000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6340. Times reached goal: 867.               Steps done: 1882664. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.136965868989.\n",
      " state (0)  A[0]:(0.531125664711) A[1]:(0.591989398003) A[2]:(0.587918639183) A[3]:(0.530831694603)\n",
      " state (1)  A[0]:(0.530623435974) A[1]:(0.000609219016042) A[2]:(0.6531945467) A[3]:(0.588281154633)\n",
      " state (2)  A[0]:(0.582752466202) A[1]:(0.725197315216) A[2]:(0.611674070358) A[3]:(0.637825369835)\n",
      " state (3)  A[0]:(0.684308171272) A[1]:(0.0380923524499) A[2]:(0.35944712162) A[3]:(0.62100070715)\n",
      " state (4)  A[0]:(0.589509427547) A[1]:(0.656966984272) A[2]:(0.000919580226764) A[3]:(0.531642079353)\n",
      " state (5)  A[0]:(-0.330928206444) A[1]:(0.999716103077) A[2]:(-0.397740155458) A[3]:(0.0429423861206)\n",
      " state (6)  A[0]:(0.0183184370399) A[1]:(0.805098712444) A[2]:(-0.00441107293591) A[3]:(0.635760068893)\n",
      " state (7)  A[0]:(0.670926809311) A[1]:(-0.654467999935) A[2]:(0.305095851421) A[3]:(0.918224513531)\n",
      " state (8)  A[0]:(0.652985095978) A[1]:(-5.73992729187e-05) A[2]:(0.730273365974) A[3]:(0.589532196522)\n",
      " state (9)  A[0]:(0.653613984585) A[1]:(0.810164988041) A[2]:(0.804911136627) A[3]:(0.00448945118114)\n",
      " state (10)  A[0]:(0.779064655304) A[1]:(0.893968582153) A[2]:(0.000176787376404) A[3]:(0.649387001991)\n",
      " state (11)  A[0]:(0.267895519733) A[1]:(0.866606533527) A[2]:(-0.896079480648) A[3]:(0.75874710083)\n",
      " state (12)  A[0]:(-0.378207534552) A[1]:(0.802259087563) A[2]:(-0.896642208099) A[3]:(0.721325278282)\n",
      " state (13)  A[0]:(-0.000524759234395) A[1]:(0.816300868988) A[2]:(0.900373101234) A[3]:(0.738325953484)\n",
      " state (14)  A[0]:(0.812668442726) A[1]:(0.89437687397) A[2]:(0.999993979931) A[3]:(0.800797879696)\n",
      " state (15)  A[0]:(0.980042695999) A[1]:(0.925237119198) A[2]:(1.0) A[3]:(0.886087179184)\n",
      "Episode 213000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6300. Times reached goal: 861.               Steps done: 1888964. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.136105696403.\n",
      " state (0)  A[0]:(0.532250642776) A[1]:(0.590763688087) A[2]:(0.587144374847) A[3]:(0.531911253929)\n",
      " state (1)  A[0]:(0.530622124672) A[1]:(-0.00126977195032) A[2]:(0.651623606682) A[3]:(0.589661002159)\n",
      " state (2)  A[0]:(0.582165241241) A[1]:(0.724135160446) A[2]:(0.612738609314) A[3]:(0.639921784401)\n",
      " state (3)  A[0]:(0.684360146523) A[1]:(0.0380242504179) A[2]:(0.359939694405) A[3]:(0.621997117996)\n",
      " state (4)  A[0]:(0.588793694973) A[1]:(0.656077384949) A[2]:(0.000478863687022) A[3]:(0.533243894577)\n",
      " state (5)  A[0]:(-0.337525933981) A[1]:(0.999716818333) A[2]:(-0.398449599743) A[3]:(0.0494731552899)\n",
      " state (6)  A[0]:(0.0160682071) A[1]:(0.804749250412) A[2]:(-0.00614385027438) A[3]:(0.637775361538)\n",
      " state (7)  A[0]:(0.670218467712) A[1]:(-0.654391646385) A[2]:(0.303030759096) A[3]:(0.918977797031)\n",
      " state (8)  A[0]:(0.651216089725) A[1]:(-0.000373318762286) A[2]:(0.729762196541) A[3]:(0.589557468891)\n",
      " state (9)  A[0]:(0.651999473572) A[1]:(0.810152232647) A[2]:(0.804922997952) A[3]:(0.00646153045818)\n",
      " state (10)  A[0]:(0.7775182724) A[1]:(0.894680023193) A[2]:(0.00106418097857) A[3]:(0.655024647713)\n",
      " state (11)  A[0]:(0.263681560755) A[1]:(0.867408275604) A[2]:(-0.895870804787) A[3]:(0.761829972267)\n",
      " state (12)  A[0]:(-0.382365226746) A[1]:(0.802767157555) A[2]:(-0.896440267563) A[3]:(0.722066819668)\n",
      " state (13)  A[0]:(-0.0065185399726) A[1]:(0.816991209984) A[2]:(0.9011464715) A[3]:(0.738328456879)\n",
      " state (14)  A[0]:(0.809976994991) A[1]:(0.895507931709) A[2]:(0.999994158745) A[3]:(0.801498472691)\n",
      " state (15)  A[0]:(0.979659855366) A[1]:(0.926642596722) A[2]:(1.0) A[3]:(0.886922836304)\n",
      "Episode 214000 finished after 0 timesteps with r=1.0. Running score: 0.79. Times trained:               6252. Times reached goal: 848.               Steps done: 1895216. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.135257418071.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5923,  0.5881,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.6559, -0.0010,  0.5311]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6519,  0.0006,  0.7293,  0.5868]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6543,  0.8104,  0.8053,  0.0043]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0049,  0.8152,  0.9000,  0.7380]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8138,  0.8950,  1.0000,  0.8021]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531628787518) A[1]:(0.592341065407) A[2]:(0.588497877121) A[3]:(0.531566977501)\n",
      " state (1)  A[0]:(0.532248437405) A[1]:(-4.63128089905e-05) A[2]:(0.65358299017) A[3]:(0.5889544487)\n",
      " state (2)  A[0]:(0.584154725075) A[1]:(0.724413871765) A[2]:(0.615938067436) A[3]:(0.639626860619)\n",
      " state (3)  A[0]:(0.686706602573) A[1]:(0.0393982343376) A[2]:(0.361990749836) A[3]:(0.620280683041)\n",
      " state (4)  A[0]:(0.591552495956) A[1]:(0.655592083931) A[2]:(2.74181365967e-05) A[3]:(0.531797766685)\n",
      " state (5)  A[0]:(-0.338095545769) A[1]:(0.999719023705) A[2]:(-0.401593893766) A[3]:(0.0522800125182)\n",
      " state (6)  A[0]:(0.0201007910073) A[1]:(0.805766284466) A[2]:(-0.00932583026588) A[3]:(0.638555347919)\n",
      " state (7)  A[0]:(0.673891425133) A[1]:(-0.652777194977) A[2]:(0.300445050001) A[3]:(0.919871509075)\n",
      " state (8)  A[0]:(0.656874775887) A[1]:(0.000760972325224) A[2]:(0.729165792465) A[3]:(0.593247890472)\n",
      " state (9)  A[0]:(0.657660245895) A[1]:(0.810799598694) A[2]:(0.80582255125) A[3]:(0.00975998025388)\n",
      " state (10)  A[0]:(0.781560301781) A[1]:(0.895369529724) A[2]:(0.00419399654493) A[3]:(0.660723805428)\n",
      " state (11)  A[0]:(0.274771273136) A[1]:(0.867544591427) A[2]:(-0.895689308643) A[3]:(0.765052318573)\n",
      " state (12)  A[0]:(-0.370966374874) A[1]:(0.801485002041) A[2]:(-0.897175312042) A[3]:(0.723024785519)\n",
      " state (13)  A[0]:(0.00648137787357) A[1]:(0.815403878689) A[2]:(0.899885058403) A[3]:(0.738389194012)\n",
      " state (14)  A[0]:(0.81387847662) A[1]:(0.895185530186) A[2]:(0.999994158745) A[3]:(0.801884829998)\n",
      " state (15)  A[0]:(0.98003757) A[1]:(0.927124142647) A[2]:(1.0) A[3]:(0.887258827686)\n",
      "Episode 215000 finished after 0 timesteps with r=1.0. Running score: 0.83. Times trained:               6312. Times reached goal: 855.               Steps done: 1901528. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.134406362006.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532120943069) A[1]:(0.591546416283) A[2]:(0.587957441807) A[3]:(0.531192541122)\n",
      " state (1)  A[0]:(0.531532049179) A[1]:(0.000263661146164) A[2]:(0.653056144714) A[3]:(0.589860796928)\n",
      " state (2)  A[0]:(0.582690238953) A[1]:(0.725313901901) A[2]:(0.617444038391) A[3]:(0.641254842281)\n",
      " state (3)  A[0]:(0.685995101929) A[1]:(0.0423123463988) A[2]:(0.363479733467) A[3]:(0.620447993279)\n",
      " state (4)  A[0]:(0.590054214001) A[1]:(0.656535804272) A[2]:(0.00102484191302) A[3]:(0.531660437584)\n",
      " state (5)  A[0]:(-0.344224274158) A[1]:(0.999721050262) A[2]:(-0.400624483824) A[3]:(0.0549783706665)\n",
      " state (6)  A[0]:(0.0170011818409) A[1]:(0.805634975433) A[2]:(-0.00839463993907) A[3]:(0.637959957123)\n",
      " state (7)  A[0]:(0.672046661377) A[1]:(-0.65252327919) A[2]:(0.300313293934) A[3]:(0.919840753078)\n",
      " state (8)  A[0]:(0.65399312973) A[1]:(0.000103682279587) A[2]:(0.728812217712) A[3]:(0.589353919029)\n",
      " state (9)  A[0]:(0.652331709862) A[1]:(0.810897529125) A[2]:(0.805191218853) A[3]:(-0.000318050384521)\n",
      " state (10)  A[0]:(0.77497202158) A[1]:(0.89614957571) A[2]:(-0.0016485437518) A[3]:(0.657425403595)\n",
      " state (11)  A[0]:(0.255956053734) A[1]:(0.868531525135) A[2]:(-0.897411048412) A[3]:(0.761227428913)\n",
      " state (12)  A[0]:(-0.384989798069) A[1]:(0.802406907082) A[2]:(-0.898370444775) A[3]:(0.717138290405)\n",
      " state (13)  A[0]:(-0.00474538840353) A[1]:(0.816378772259) A[2]:(0.900130748749) A[3]:(0.73365008831)\n",
      " state (14)  A[0]:(0.810914337635) A[1]:(0.896232306957) A[2]:(0.999994277954) A[3]:(0.799742877483)\n",
      " state (15)  A[0]:(0.979723989964) A[1]:(0.928211510181) A[2]:(1.0) A[3]:(0.886734247208)\n",
      "Episode 216000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6333. Times reached goal: 872.               Steps done: 1907861. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.133557856145.\n",
      " state (0)  A[0]:(0.530357480049) A[1]:(0.59054017067) A[2]:(0.586849570274) A[3]:(0.530774712563)\n",
      " state (1)  A[0]:(0.529533982277) A[1]:(0.000722661498003) A[2]:(0.653139710426) A[3]:(0.588892102242)\n",
      " state (2)  A[0]:(0.580312609673) A[1]:(0.725195169449) A[2]:(0.618143796921) A[3]:(0.641164302826)\n",
      " state (3)  A[0]:(0.68460047245) A[1]:(0.0445314534009) A[2]:(0.363683223724) A[3]:(0.619535088539)\n",
      " state (4)  A[0]:(0.588655948639) A[1]:(0.656610369682) A[2]:(0.000565290392842) A[3]:(0.531520009041)\n",
      " state (5)  A[0]:(-0.348223894835) A[1]:(0.999723851681) A[2]:(-0.401404708624) A[3]:(0.0595546513796)\n",
      " state (6)  A[0]:(0.0168843753636) A[1]:(0.806837618351) A[2]:(-0.0101506803185) A[3]:(0.639521479607)\n",
      " state (7)  A[0]:(0.672938883305) A[1]:(-0.65115249157) A[2]:(0.298973560333) A[3]:(0.920680999756)\n",
      " state (8)  A[0]:(0.655574798584) A[1]:(0.000432983011706) A[2]:(0.729641675949) A[3]:(0.590948700905)\n",
      " state (9)  A[0]:(0.65401506424) A[1]:(0.810474574566) A[2]:(0.806430459023) A[3]:(0.00273501197807)\n",
      " state (10)  A[0]:(0.774132490158) A[1]:(0.896123051643) A[2]:(-0.000850796466693) A[3]:(0.661502480507)\n",
      " state (11)  A[0]:(0.251852333546) A[1]:(0.867965996265) A[2]:(-0.897822856903) A[3]:(0.762382566929)\n",
      " state (12)  A[0]:(-0.385119915009) A[1]:(0.800539731979) A[2]:(-0.89877641201) A[3]:(0.716808915138)\n",
      " state (13)  A[0]:(-0.000849932199344) A[1]:(0.814572155476) A[2]:(0.900232434273) A[3]:(0.734371185303)\n",
      " state (14)  A[0]:(0.812378287315) A[1]:(0.895884513855) A[2]:(0.999994397163) A[3]:(0.801947236061)\n",
      " state (15)  A[0]:(0.97983366251) A[1]:(0.928675353527) A[2]:(1.0) A[3]:(0.888688921928)\n",
      "Episode 217000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6333. Times reached goal: 875.               Steps done: 1914194. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.132714706892.\n",
      " state (0)  A[0]:(0.533065140247) A[1]:(0.590682864189) A[2]:(0.587345838547) A[3]:(0.532291650772)\n",
      " state (1)  A[0]:(0.532148241997) A[1]:(-0.000883862143382) A[2]:(0.652574777603) A[3]:(0.589502096176)\n",
      " state (2)  A[0]:(0.581995308399) A[1]:(0.726203858852) A[2]:(0.61820602417) A[3]:(0.642164945602)\n",
      " state (3)  A[0]:(0.685982465744) A[1]:(0.0486485026777) A[2]:(0.362993150949) A[3]:(0.61932694912)\n",
      " state (4)  A[0]:(0.589859724045) A[1]:(0.656550824642) A[2]:(-0.000543713511433) A[3]:(0.531759500504)\n",
      " state (5)  A[0]:(-0.351563870907) A[1]:(0.999724805355) A[2]:(-0.401521593332) A[3]:(0.0638036057353)\n",
      " state (6)  A[0]:(0.0160021688789) A[1]:(0.806145489216) A[2]:(-0.0115589713678) A[3]:(0.639798641205)\n",
      " state (7)  A[0]:(0.672966122627) A[1]:(-0.651209831238) A[2]:(0.29646012187) A[3]:(0.920974612236)\n",
      " state (8)  A[0]:(0.656184077263) A[1]:(0.00108118308708) A[2]:(0.728732407093) A[3]:(0.590262293816)\n",
      " state (9)  A[0]:(0.653477609158) A[1]:(0.811236560345) A[2]:(0.805799543858) A[3]:(-0.000396788091166)\n",
      " state (10)  A[0]:(0.770879149437) A[1]:(0.89698523283) A[2]:(-0.00667502498254) A[3]:(0.661242902279)\n",
      " state (11)  A[0]:(0.241120398045) A[1]:(0.868649840355) A[2]:(-0.899556815624) A[3]:(0.760212063789)\n",
      " state (12)  A[0]:(-0.390478938818) A[1]:(0.800573289394) A[2]:(-0.900144815445) A[3]:(0.712936937809)\n",
      " state (13)  A[0]:(-0.00139467325062) A[1]:(0.814453005791) A[2]:(0.89988297224) A[3]:(0.73204344511)\n",
      " state (14)  A[0]:(0.812817156315) A[1]:(0.896344900131) A[2]:(0.999994456768) A[3]:(0.80190962553)\n",
      " state (15)  A[0]:(0.979859828949) A[1]:(0.929525852203) A[2]:(1.0) A[3]:(0.889375448227)\n",
      "Episode 218000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6283. Times reached goal: 884.               Steps done: 1920477. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.13188347444.\n",
      " state (0)  A[0]:(0.532003641129) A[1]:(0.591866135597) A[2]:(0.588406383991) A[3]:(0.532213807106)\n",
      " state (1)  A[0]:(0.531888246536) A[1]:(0.00161780277267) A[2]:(0.653980493546) A[3]:(0.590122461319)\n",
      " state (2)  A[0]:(0.582785725594) A[1]:(0.726749300957) A[2]:(0.620957672596) A[3]:(0.643399596214)\n",
      " state (3)  A[0]:(0.687180995941) A[1]:(0.0487102866173) A[2]:(0.365201205015) A[3]:(0.619256913662)\n",
      " state (4)  A[0]:(0.590418159962) A[1]:(0.657160282135) A[2]:(0.00041818615864) A[3]:(0.532127380371)\n",
      " state (5)  A[0]:(-0.354829132557) A[1]:(0.999727845192) A[2]:(-0.400606572628) A[3]:(0.0713660493493)\n",
      " state (6)  A[0]:(0.0150584522635) A[1]:(0.807076275349) A[2]:(-0.00983325764537) A[3]:(0.643112778664)\n",
      " state (7)  A[0]:(0.671928942204) A[1]:(-0.650491118431) A[2]:(0.298198521137) A[3]:(0.921911001205)\n",
      " state (8)  A[0]:(0.654580116272) A[1]:(-0.000476703018649) A[2]:(0.72981274128) A[3]:(0.589887440205)\n",
      " state (9)  A[0]:(0.652992367744) A[1]:(0.810012340546) A[2]:(0.806477963924) A[3]:(0.000386208266718)\n",
      " state (10)  A[0]:(0.770118713379) A[1]:(0.896743178368) A[2]:(-0.00506146391854) A[3]:(0.666181445122)\n",
      " state (11)  A[0]:(0.238701120019) A[1]:(0.868406951427) A[2]:(-0.899396657944) A[3]:(0.763519644737)\n",
      " state (12)  A[0]:(-0.392815232277) A[1]:(0.79989951849) A[2]:(-0.900116980076) A[3]:(0.714754581451)\n",
      " state (13)  A[0]:(-0.0049842777662) A[1]:(0.814029037952) A[2]:(0.900327026844) A[3]:(0.73322892189)\n",
      " state (14)  A[0]:(0.811066448689) A[1]:(0.896620631218) A[2]:(0.999994575977) A[3]:(0.803310036659)\n",
      " state (15)  A[0]:(0.979598343372) A[1]:(0.930113434792) A[2]:(1.0) A[3]:(0.890443742275)\n",
      "Episode 219000 finished after 0 timesteps with r=0.0. Running score: 0.87. Times trained:               6309. Times reached goal: 874.               Steps done: 1926786. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.1310540408.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5909,  0.5879,  0.5309]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5893,  0.6565, -0.0021,  0.5306]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6533,  0.0013,  0.7291,  0.5873]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6548,  0.8103,  0.8074,  0.0017]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8137,  0.9003,  0.7348]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8117,  0.8967,  1.0000,  0.8047]], device='cuda:0')\n",
      "On state=14, selected action=0 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0008,  0.8130,  0.8996,  0.7354]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8116,  0.8963,  1.0000,  0.8048]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53125333786) A[1]:(0.590834736824) A[2]:(0.589146733284) A[3]:(0.532079994678)\n",
      " state (1)  A[0]:(0.530526161194) A[1]:(-0.000199437141418) A[2]:(0.654277563095) A[3]:(0.590291798115)\n",
      " state (2)  A[0]:(0.580889344215) A[1]:(0.727275490761) A[2]:(0.621605098248) A[3]:(0.644551277161)\n",
      " state (3)  A[0]:(0.686139464378) A[1]:(0.0499979257584) A[2]:(0.365528941154) A[3]:(0.619002103806)\n",
      " state (4)  A[0]:(0.589045524597) A[1]:(0.656792879105) A[2]:(0.00107145262882) A[3]:(0.531910300255)\n",
      " state (5)  A[0]:(-0.3600063622) A[1]:(0.99972987175) A[2]:(-0.398239970207) A[3]:(0.0744071304798)\n",
      " state (6)  A[0]:(0.0115995267406) A[1]:(0.807173669338) A[2]:(-0.00879038590938) A[3]:(0.64280962944)\n",
      " state (7)  A[0]:(0.670608520508) A[1]:(-0.650542259216) A[2]:(0.297913700342) A[3]:(0.922156691551)\n",
      " state (8)  A[0]:(0.655322790146) A[1]:(-0.00218760618009) A[2]:(0.730407297611) A[3]:(0.590918183327)\n",
      " state (9)  A[0]:(0.656405091286) A[1]:(0.809428453445) A[2]:(0.808503627777) A[3]:(0.00444120215252)\n",
      " state (10)  A[0]:(0.774597346783) A[1]:(0.896784305573) A[2]:(0.00504787452519) A[3]:(0.674068927765)\n",
      " state (11)  A[0]:(0.252008497715) A[1]:(0.868217468262) A[2]:(-0.897449553013) A[3]:(0.769492149353)\n",
      " state (12)  A[0]:(-0.383632451296) A[1]:(0.798742413521) A[2]:(-0.89957600832) A[3]:(0.718656420708)\n",
      " state (13)  A[0]:(0.000248640775681) A[1]:(0.812652051449) A[2]:(0.899582326412) A[3]:(0.73511505127)\n",
      " state (14)  A[0]:(0.811511337757) A[1]:(0.896273195744) A[2]:(0.999994575977) A[3]:(0.804582774639)\n",
      " state (15)  A[0]:(0.979570686817) A[1]:(0.930384874344) A[2]:(1.0) A[3]:(0.891153454781)\n",
      "Episode 220000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6190. Times reached goal: 845.               Steps done: 1932976. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.130245321854.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532485842705) A[1]:(0.590715050697) A[2]:(0.588331699371) A[3]:(0.532047212124)\n",
      " state (1)  A[0]:(0.531502366066) A[1]:(-0.000373065442545) A[2]:(0.654326081276) A[3]:(0.59097135067)\n",
      " state (2)  A[0]:(0.580593347549) A[1]:(0.726557195187) A[2]:(0.6229211092) A[3]:(0.645660102367)\n",
      " state (3)  A[0]:(0.685939908028) A[1]:(0.0516018494964) A[2]:(0.366295784712) A[3]:(0.618726491928)\n",
      " state (4)  A[0]:(0.589324533939) A[1]:(0.656149625778) A[2]:(8.77380371094e-05) A[3]:(0.53193539381)\n",
      " state (5)  A[0]:(-0.360539317131) A[1]:(0.999732553959) A[2]:(-0.400927692652) A[3]:(0.0788783207536)\n",
      " state (6)  A[0]:(0.0126653397456) A[1]:(0.80776655674) A[2]:(-0.0115662422031) A[3]:(0.642965853214)\n",
      " state (7)  A[0]:(0.671186566353) A[1]:(-0.649141788483) A[2]:(0.294469237328) A[3]:(0.922423362732)\n",
      " state (8)  A[0]:(0.656443893909) A[1]:(-0.000832021061797) A[2]:(0.727967262268) A[3]:(0.591432750225)\n",
      " state (9)  A[0]:(0.65524661541) A[1]:(0.810113072395) A[2]:(0.806530237198) A[3]:(0.00144457712304)\n",
      " state (10)  A[0]:(0.771153926849) A[1]:(0.897425711155) A[2]:(-0.0039614229463) A[3]:(0.674073696136)\n",
      " state (11)  A[0]:(0.241845473647) A[1]:(0.868673086166) A[2]:(-0.899561047554) A[3]:(0.768064558506)\n",
      " state (12)  A[0]:(-0.388623356819) A[1]:(0.798732042313) A[2]:(-0.900935351849) A[3]:(0.715376377106)\n",
      " state (13)  A[0]:(-5.51342964172e-06) A[1]:(0.812698125839) A[2]:(0.899739384651) A[3]:(0.73221373558)\n",
      " state (14)  A[0]:(0.81212580204) A[1]:(0.896729052067) A[2]:(0.999994695187) A[3]:(0.8030782938)\n",
      " state (15)  A[0]:(0.979619264603) A[1]:(0.931031227112) A[2]:(1.0) A[3]:(0.890528678894)\n",
      "Episode 221000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6304. Times reached goal: 860.               Steps done: 1939280. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.129426837917.\n",
      " state (0)  A[0]:(0.530836224556) A[1]:(0.590120971203) A[2]:(0.588609576225) A[3]:(0.532017230988)\n",
      " state (1)  A[0]:(0.530945837498) A[1]:(-0.00360460509546) A[2]:(0.653941392899) A[3]:(0.591029882431)\n",
      " state (2)  A[0]:(0.579726219177) A[1]:(0.726939082146) A[2]:(0.624170720577) A[3]:(0.646605372429)\n",
      " state (3)  A[0]:(0.685182154179) A[1]:(0.056598700583) A[2]:(0.367872565985) A[3]:(0.618366956711)\n",
      " state (4)  A[0]:(0.588558673859) A[1]:(0.656755387783) A[2]:(0.000715374830179) A[3]:(0.531999707222)\n",
      " state (5)  A[0]:(-0.363759696484) A[1]:(0.999735116959) A[2]:(-0.40116918087) A[3]:(0.085058003664)\n",
      " state (6)  A[0]:(0.00993414316326) A[1]:(0.80751979351) A[2]:(-0.0101713007316) A[3]:(0.643913149834)\n",
      " state (7)  A[0]:(0.669264912605) A[1]:(-0.648985385895) A[2]:(0.296032845974) A[3]:(0.922742128372)\n",
      " state (8)  A[0]:(0.655237793922) A[1]:(-0.000582426728215) A[2]:(0.729121804237) A[3]:(0.591213941574)\n",
      " state (9)  A[0]:(0.653600811958) A[1]:(0.810540556908) A[2]:(0.807654380798) A[3]:(0.000955134339165)\n",
      " state (10)  A[0]:(0.767576098442) A[1]:(0.898188114166) A[2]:(-0.00247621024027) A[3]:(0.67664372921)\n",
      " state (11)  A[0]:(0.230706378818) A[1]:(0.86973118782) A[2]:(-0.899569392204) A[3]:(0.769010066986)\n",
      " state (12)  A[0]:(-0.3962854743) A[1]:(0.800063073635) A[2]:(-0.900797903538) A[3]:(0.714986920357)\n",
      " state (13)  A[0]:(-0.00585228996351) A[1]:(0.814101994038) A[2]:(0.900332152843) A[3]:(0.731969833374)\n",
      " state (14)  A[0]:(0.810421764851) A[1]:(0.898024141788) A[2]:(0.999994814396) A[3]:(0.803508102894)\n",
      " state (15)  A[0]:(0.979400038719) A[1]:(0.932344019413) A[2]:(1.0) A[3]:(0.890969634056)\n",
      "Episode 222000 finished after 0 timesteps with r=0.0. Running score: 0.85. Times trained:               6312. Times reached goal: 871.               Steps done: 1945592. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.12861246857.\n",
      " state (0)  A[0]:(0.532046437263) A[1]:(0.589546382427) A[2]:(0.589498519897) A[3]:(0.531497895718)\n",
      " state (1)  A[0]:(0.532094120979) A[1]:(0.000135973095894) A[2]:(0.65463244915) A[3]:(0.590637505054)\n",
      " state (2)  A[0]:(0.580208003521) A[1]:(0.725987017155) A[2]:(0.623579978943) A[3]:(0.64673435688)\n",
      " state (3)  A[0]:(0.685577869415) A[1]:(0.0576122812927) A[2]:(0.367151737213) A[3]:(0.617404341698)\n",
      " state (4)  A[0]:(0.589870214462) A[1]:(0.655862212181) A[2]:(0.000798106018919) A[3]:(0.532012104988)\n",
      " state (5)  A[0]:(-0.362233579159) A[1]:(0.999737560749) A[2]:(-0.399143636227) A[3]:(0.0907972455025)\n",
      " state (6)  A[0]:(0.0129529889673) A[1]:(0.807784497738) A[2]:(-0.00882793404162) A[3]:(0.644822955132)\n",
      " state (7)  A[0]:(0.670622646809) A[1]:(-0.647744417191) A[2]:(0.295907765627) A[3]:(0.923084259033)\n",
      " state (8)  A[0]:(0.657532811165) A[1]:(-0.000588342489209) A[2]:(0.729301691055) A[3]:(0.591649770737)\n",
      " state (9)  A[0]:(0.656913638115) A[1]:(0.809787809849) A[2]:(0.808077216148) A[3]:(0.00427535548806)\n",
      " state (10)  A[0]:(0.769238829613) A[1]:(0.897853732109) A[2]:(-0.00174617592711) A[3]:(0.682223916054)\n",
      " state (11)  A[0]:(0.234959095716) A[1]:(0.868959605694) A[2]:(-0.899676084518) A[3]:(0.772671103477)\n",
      " state (12)  A[0]:(-0.389706224203) A[1]:(0.798088669777) A[2]:(-0.90129339695) A[3]:(0.717870175838)\n",
      " state (13)  A[0]:(0.00357942236587) A[1]:(0.811984121799) A[2]:(0.899405002594) A[3]:(0.734699249268)\n",
      " state (14)  A[0]:(0.813188433647) A[1]:(0.897290349007) A[2]:(0.999994814396) A[3]:(0.806035220623)\n",
      " state (15)  A[0]:(0.979625940323) A[1]:(0.932443678379) A[2]:(1.0) A[3]:(0.892445683479)\n",
      "Episode 223000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained:               6202. Times reached goal: 860.               Steps done: 1951794. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.127817282461.\n",
      " state (0)  A[0]:(0.531513810158) A[1]:(0.589576840401) A[2]:(0.588521957397) A[3]:(0.531362771988)\n",
      " state (1)  A[0]:(0.530949473381) A[1]:(0.000272780656815) A[2]:(0.654588460922) A[3]:(0.590338528156)\n",
      " state (2)  A[0]:(0.578679800034) A[1]:(0.72659933567) A[2]:(0.622995972633) A[3]:(0.64720416069)\n",
      " state (3)  A[0]:(0.684233486652) A[1]:(0.0577372312546) A[2]:(0.365866005421) A[3]:(0.616385817528)\n",
      " state (4)  A[0]:(0.587997317314) A[1]:(0.655506372452) A[2]:(-0.000337839126587) A[3]:(0.531419217587)\n",
      " state (5)  A[0]:(-0.366115182638) A[1]:(0.999740719795) A[2]:(-0.398390412331) A[3]:(0.0962221324444)\n",
      " state (6)  A[0]:(0.00807741563767) A[1]:(0.808179020882) A[2]:(-0.00975327566266) A[3]:(0.645117878914)\n",
      " state (7)  A[0]:(0.666367888451) A[1]:(-0.647112369537) A[2]:(0.293716549873) A[3]:(0.923027276993)\n",
      " state (8)  A[0]:(0.653043985367) A[1]:(-0.000410526961787) A[2]:(0.728786110878) A[3]:(0.588134527206)\n",
      " state (9)  A[0]:(0.653657913208) A[1]:(0.80948972702) A[2]:(0.807974159718) A[3]:(0.00235190545209)\n",
      " state (10)  A[0]:(0.765830874443) A[1]:(0.898006260395) A[2]:(-0.00176524929702) A[3]:(0.685529649258)\n",
      " state (11)  A[0]:(0.226102694869) A[1]:(0.869232594967) A[2]:(-0.899611830711) A[3]:(0.774684429169)\n",
      " state (12)  A[0]:(-0.396340638399) A[1]:(0.798273861408) A[2]:(-0.901022613049) A[3]:(0.71817612648)\n",
      " state (13)  A[0]:(-0.002877749037) A[1]:(0.812205791473) A[2]:(0.900194168091) A[3]:(0.734037160873)\n",
      " state (14)  A[0]:(0.811040401459) A[1]:(0.897710323334) A[2]:(0.999994874001) A[3]:(0.805455803871)\n",
      " state (15)  A[0]:(0.979354679585) A[1]:(0.932966172695) A[2]:(1.0) A[3]:(0.892060875893)\n",
      "Episode 224000 finished after 0 timesteps with r=0.0. Running score: 0.79. Times trained:               6244. Times reached goal: 844.               Steps done: 1958038. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.127021677812.\n",
      "q_values \n",
      "tensor([[ 0.5299,  0.5890,  0.5887,  0.5302]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5297, -0.0033,  0.6542,  0.5892]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5770,  0.7274,  0.6232,  0.6472]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0081,  0.8095, -0.0058,  0.6443]], device='cuda:0')\n",
      "On state=6, selected action=2 , Random? True\n",
      "new state=7, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.530124306679) A[1]:(0.589667737484) A[2]:(0.587556958199) A[3]:(0.530150949955)\n",
      " state (1)  A[0]:(0.529891729355) A[1]:(-0.00270470324904) A[2]:(0.653412103653) A[3]:(0.589105010033)\n",
      " state (2)  A[0]:(0.577023267746) A[1]:(0.727129876614) A[2]:(0.622507274151) A[3]:(0.647098183632)\n",
      " state (3)  A[0]:(0.683047890663) A[1]:(0.0589783675969) A[2]:(0.366110384464) A[3]:(0.614748179913)\n",
      " state (4)  A[0]:(0.587158918381) A[1]:(0.656154751778) A[2]:(0.00143587496132) A[3]:(0.529968142509)\n",
      " state (5)  A[0]:(-0.366106957197) A[1]:(0.999744832516) A[2]:(-0.394832730293) A[3]:(0.099935784936)\n",
      " state (6)  A[0]:(0.00784887652844) A[1]:(0.808951795101) A[2]:(-0.00635067047551) A[3]:(0.644135773182)\n",
      " state (7)  A[0]:(0.66552811861) A[1]:(-0.646467804909) A[2]:(0.29558968544) A[3]:(0.922895669937)\n",
      " state (8)  A[0]:(0.653971016407) A[1]:(-0.000261396169662) A[2]:(0.729154288769) A[3]:(0.587961673737)\n",
      " state (9)  A[0]:(0.653057456017) A[1]:(0.81004011631) A[2]:(0.808381319046) A[3]:(-0.00148156180512)\n",
      " state (10)  A[0]:(0.762572050095) A[1]:(0.898663699627) A[2]:(-0.00269233528525) A[3]:(0.685186982155)\n",
      " state (11)  A[0]:(0.216277658939) A[1]:(0.869979918003) A[2]:(-0.900118052959) A[3]:(0.773253500462)\n",
      " state (12)  A[0]:(-0.401254117489) A[1]:(0.79890614748) A[2]:(-0.901332974434) A[3]:(0.715214014053)\n",
      " state (13)  A[0]:(-0.00397239020094) A[1]:(0.812591552734) A[2]:(0.900266706944) A[3]:(0.731775045395)\n",
      " state (14)  A[0]:(0.811257302761) A[1]:(0.898194551468) A[2]:(0.99999499321) A[3]:(0.804588317871)\n",
      " state (15)  A[0]:(0.979387760162) A[1]:(0.93361800909) A[2]:(1.0) A[3]:(0.891807317734)\n",
      "Episode 225000 finished after 0 timesteps with r=0.0. Running score: 0.83. Times trained:               6173. Times reached goal: 848.               Steps done: 1964211. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.126239988162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53243637085) A[1]:(0.590105414391) A[2]:(0.591650664806) A[3]:(0.533183693886)\n",
      " state (1)  A[0]:(0.532798111439) A[1]:(-3.80575656891e-05) A[2]:(0.655589699745) A[3]:(0.592491567135)\n",
      " state (2)  A[0]:(0.579528331757) A[1]:(0.727803587914) A[2]:(0.62197214365) A[3]:(0.65079587698)\n",
      " state (3)  A[0]:(0.684824943542) A[1]:(0.0585905537009) A[2]:(0.364819198847) A[3]:(0.616946697235)\n",
      " state (4)  A[0]:(0.589117765427) A[1]:(0.655382931232) A[2]:(0.00116455496754) A[3]:(0.533373177052)\n",
      " state (5)  A[0]:(-0.365808516741) A[1]:(0.999746978283) A[2]:(-0.391967207193) A[3]:(0.111073069274)\n",
      " state (6)  A[0]:(0.00670470669866) A[1]:(0.808134555817) A[2]:(-0.00413284311071) A[3]:(0.647513628006)\n",
      " state (7)  A[0]:(0.663824081421) A[1]:(-0.646151483059) A[2]:(0.295377522707) A[3]:(0.923771440983)\n",
      " state (8)  A[0]:(0.653917431831) A[1]:(-0.00160255888477) A[2]:(0.728841304779) A[3]:(0.590409874916)\n",
      " state (9)  A[0]:(0.654077410698) A[1]:(0.80910474062) A[2]:(0.808818936348) A[3]:(0.00124826957472)\n",
      " state (10)  A[0]:(0.763354301453) A[1]:(0.898313999176) A[2]:(-0.000112295150757) A[3]:(0.69094479084)\n",
      " state (11)  A[0]:(0.219454154372) A[1]:(0.869480609894) A[2]:(-0.899647474289) A[3]:(0.777526259422)\n",
      " state (12)  A[0]:(-0.397484242916) A[1]:(0.797822117805) A[2]:(-0.901226043701) A[3]:(0.718138813972)\n",
      " state (13)  A[0]:(-0.000375032395823) A[1]:(0.811590909958) A[2]:(0.900082707405) A[3]:(0.733190774918)\n",
      " state (14)  A[0]:(0.811599373817) A[1]:(0.898041427135) A[2]:(0.99999499321) A[3]:(0.805228888988)\n",
      " state (15)  A[0]:(0.979313373566) A[1]:(0.933931052685) A[2]:(1.0) A[3]:(0.891885459423)\n",
      "Episode 226000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6335. Times reached goal: 866.               Steps done: 1970546. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.125442785642.\n",
      " state (0)  A[0]:(0.531787872314) A[1]:(0.591373682022) A[2]:(0.588667631149) A[3]:(0.532752990723)\n",
      " state (1)  A[0]:(0.532804012299) A[1]:(0.000914707547054) A[2]:(0.655645608902) A[3]:(0.591979980469)\n",
      " state (2)  A[0]:(0.581210255623) A[1]:(0.727715849876) A[2]:(0.622466087341) A[3]:(0.651816010475)\n",
      " state (3)  A[0]:(0.686181604862) A[1]:(0.0545167028904) A[2]:(0.364777028561) A[3]:(0.61635106802)\n",
      " state (4)  A[0]:(0.59011721611) A[1]:(0.655844449997) A[2]:(0.000703334691934) A[3]:(0.532733559608)\n",
      " state (5)  A[0]:(-0.363514184952) A[1]:(0.999750375748) A[2]:(-0.392022997141) A[3]:(0.117233127356)\n",
      " state (6)  A[0]:(0.00842925347388) A[1]:(0.807874023914) A[2]:(-0.00589661905542) A[3]:(0.647997260094)\n",
      " state (7)  A[0]:(0.663322806358) A[1]:(-0.645042598248) A[2]:(0.29248008132) A[3]:(0.923908352852)\n",
      " state (8)  A[0]:(0.654455006123) A[1]:(0.000839128915686) A[2]:(0.727607011795) A[3]:(0.59067261219)\n",
      " state (9)  A[0]:(0.652117609978) A[1]:(0.810173034668) A[2]:(0.807797312737) A[3]:(-0.000856041675434)\n",
      " state (10)  A[0]:(0.756280064583) A[1]:(0.899076282978) A[2]:(-0.00780447386205) A[3]:(0.689577102661)\n",
      " state (11)  A[0]:(0.198287799954) A[1]:(0.870270967484) A[2]:(-0.901453971863) A[3]:(0.774113178253)\n",
      " state (12)  A[0]:(-0.40818554163) A[1]:(0.798568665981) A[2]:(-0.902026832104) A[3]:(0.713369965553)\n",
      " state (13)  A[0]:(-0.0032984346617) A[1]:(0.81214427948) A[2]:(0.900206565857) A[3]:(0.730172693729)\n",
      " state (14)  A[0]:(0.811672449112) A[1]:(0.898707270622) A[2]:(0.999995052814) A[3]:(0.803987979889)\n",
      " state (15)  A[0]:(0.979295611382) A[1]:(0.934814155102) A[2]:(1.0) A[3]:(0.891360104084)\n",
      "Episode 227000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6259. Times reached goal: 872.               Steps done: 1976805. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.124660091244.\n",
      " state (0)  A[0]:(0.534347057343) A[1]:(0.591019332409) A[2]:(0.589459657669) A[3]:(0.533453524113)\n",
      " state (1)  A[0]:(0.532384872437) A[1]:(0.000979393371381) A[2]:(0.655079424381) A[3]:(0.592132151127)\n",
      " state (2)  A[0]:(0.579478263855) A[1]:(0.728145599365) A[2]:(0.622747898102) A[3]:(0.653015971184)\n",
      " state (3)  A[0]:(0.684273958206) A[1]:(0.0553297102451) A[2]:(0.365021824837) A[3]:(0.616466403008)\n",
      " state (4)  A[0]:(0.588292181492) A[1]:(0.656458497047) A[2]:(-2.55107879639e-05) A[3]:(0.533854842186)\n",
      " state (5)  A[0]:(-0.362256020308) A[1]:(0.999756038189) A[2]:(-0.393691420555) A[3]:(0.127047881484)\n",
      " state (6)  A[0]:(0.00939883943647) A[1]:(0.810310006142) A[2]:(-0.00561350630596) A[3]:(0.65045273304)\n",
      " state (7)  A[0]:(0.6626496315) A[1]:(-0.64443141222) A[2]:(0.294721692801) A[3]:(0.924516558647)\n",
      " state (8)  A[0]:(0.655331969261) A[1]:(0.00154611340258) A[2]:(0.729306101799) A[3]:(0.591173708439)\n",
      " state (9)  A[0]:(0.655225992203) A[1]:(0.810470819473) A[2]:(0.808962523937) A[3]:(0.00456607155502)\n",
      " state (10)  A[0]:(0.757051765919) A[1]:(0.899329841137) A[2]:(-0.00526137277484) A[3]:(0.696833252907)\n",
      " state (11)  A[0]:(0.198245540261) A[1]:(0.870351135731) A[2]:(-0.901145815849) A[3]:(0.779239177704)\n",
      " state (12)  A[0]:(-0.406424164772) A[1]:(0.798049330711) A[2]:(-0.90194940567) A[3]:(0.718064963818)\n",
      " state (13)  A[0]:(0.000247240066528) A[1]:(0.811279177666) A[2]:(0.899870693684) A[3]:(0.73424577713)\n",
      " state (14)  A[0]:(0.812550246716) A[1]:(0.898443579674) A[2]:(0.999995112419) A[3]:(0.807149767876)\n",
      " state (15)  A[0]:(0.979327261448) A[1]:(0.935017466545) A[2]:(1.0) A[3]:(0.893063187599)\n",
      "Episode 228000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6395. Times reached goal: 882.               Steps done: 1983200. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.123865433587.\n",
      " state (0)  A[0]:(0.532420396805) A[1]:(0.590596318245) A[2]:(0.592082977295) A[3]:(0.532996296883)\n",
      " state (1)  A[0]:(0.532336354256) A[1]:(0.000430598825915) A[2]:(0.656764149666) A[3]:(0.591408610344)\n",
      " state (2)  A[0]:(0.579285860062) A[1]:(0.729549527168) A[2]:(0.622758448124) A[3]:(0.652889490128)\n",
      " state (3)  A[0]:(0.683675169945) A[1]:(0.05890648067) A[2]:(0.36572393775) A[3]:(0.614872097969)\n",
      " state (4)  A[0]:(0.588244915009) A[1]:(0.657025098801) A[2]:(0.00194310897496) A[3]:(0.533009171486)\n",
      " state (5)  A[0]:(-0.361790955067) A[1]:(0.9997600317) A[2]:(-0.390966266394) A[3]:(0.13140873611)\n",
      " state (6)  A[0]:(0.00728549575433) A[1]:(0.811011254787) A[2]:(-0.00187575595919) A[3]:(0.648976922035)\n",
      " state (7)  A[0]:(0.660907804966) A[1]:(-0.644598603249) A[2]:(0.297047048807) A[3]:(0.924516797066)\n",
      " state (8)  A[0]:(0.657576441765) A[1]:(0.00040532645653) A[2]:(0.729197740555) A[3]:(0.594386816025)\n",
      " state (9)  A[0]:(0.656022012234) A[1]:(0.81104940176) A[2]:(0.809173345566) A[3]:(0.00343658169731)\n",
      " state (10)  A[0]:(0.754924178123) A[1]:(0.899827063084) A[2]:(-0.00700176227838) A[3]:(0.698519706726)\n",
      " state (11)  A[0]:(0.191852971911) A[1]:(0.870753228664) A[2]:(-0.901871144772) A[3]:(0.77987664938)\n",
      " state (12)  A[0]:(-0.407664835453) A[1]:(0.798200428486) A[2]:(-0.902465999126) A[3]:(0.717631220818)\n",
      " state (13)  A[0]:(0.00428398326039) A[1]:(0.811326622963) A[2]:(0.899792730808) A[3]:(0.733911037445)\n",
      " state (14)  A[0]:(0.814754724503) A[1]:(0.898761153221) A[2]:(0.999995172024) A[3]:(0.80722618103)\n",
      " state (15)  A[0]:(0.979610145092) A[1]:(0.935547530651) A[2]:(1.0) A[3]:(0.893060684204)\n",
      "Episode 229000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6227. Times reached goal: 862.               Steps done: 1989427. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.123096520029.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5911,  0.5897,  0.5305]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5910,  0.5893,  0.5305]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5883,  0.6563, -0.0003,  0.5309]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6532, -0.0009,  0.7288,  0.5887]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6539,  0.8100,  0.8099,  0.0000]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0007,  0.8113,  0.9009,  0.7344]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8127,  0.8991,  1.0000,  0.8081]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533264279366) A[1]:(0.591205239296) A[2]:(0.589381933212) A[3]:(0.531695485115)\n",
      " state (1)  A[0]:(0.533852934837) A[1]:(-0.000575676502194) A[2]:(0.655851364136) A[3]:(0.591020226479)\n",
      " state (2)  A[0]:(0.581059515476) A[1]:(0.729402244091) A[2]:(0.619156599045) A[3]:(0.652988195419)\n",
      " state (3)  A[0]:(0.684371054173) A[1]:(0.0611956864595) A[2]:(0.362278461456) A[3]:(0.613284707069)\n",
      " state (4)  A[0]:(0.589997649193) A[1]:(0.656619429588) A[2]:(0.000416994065745) A[3]:(0.532858133316)\n",
      " state (5)  A[0]:(-0.358445763588) A[1]:(0.999761879444) A[2]:(-0.390852868557) A[3]:(0.139703497291)\n",
      " state (6)  A[0]:(0.00768615072593) A[1]:(0.809705495834) A[2]:(-0.00318693043664) A[3]:(0.650606274605)\n",
      " state (7)  A[0]:(0.659190654755) A[1]:(-0.644627273083) A[2]:(0.294609367847) A[3]:(0.92490285635)\n",
      " state (8)  A[0]:(0.655999779701) A[1]:(0.000382289261324) A[2]:(0.729257464409) A[3]:(0.592948317528)\n",
      " state (9)  A[0]:(0.656046390533) A[1]:(0.810366928577) A[2]:(0.810142815113) A[3]:(0.00561720179394)\n",
      " state (10)  A[0]:(0.754136502743) A[1]:(0.899589061737) A[2]:(-0.00204968173057) A[3]:(0.704542517662)\n",
      " state (11)  A[0]:(0.188764050603) A[1]:(0.870543062687) A[2]:(-0.900833368301) A[3]:(0.784047245979)\n",
      " state (12)  A[0]:(-0.410689979792) A[1]:(0.797749817371) A[2]:(-0.901875913143) A[3]:(0.720403373241)\n",
      " state (13)  A[0]:(-0.000625699700322) A[1]:(0.810854792595) A[2]:(0.899677455425) A[3]:(0.73563426733)\n",
      " state (14)  A[0]:(0.812409281731) A[1]:(0.898836135864) A[2]:(0.999995172024) A[3]:(0.808645367622)\n",
      " state (15)  A[0]:(0.979265332222) A[1]:(0.936006486416) A[2]:(1.0) A[3]:(0.893844246864)\n",
      "Episode 230000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6248. Times reached goal: 854.               Steps done: 1995675. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.122329810666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.52953183651) A[1]:(0.590241611004) A[2]:(0.589809536934) A[3]:(0.531173467636)\n",
      " state (1)  A[0]:(0.529358744621) A[1]:(0.000557482184377) A[2]:(0.655255794525) A[3]:(0.590777337551)\n",
      " state (2)  A[0]:(0.577124714851) A[1]:(0.727951169014) A[2]:(0.618479549885) A[3]:(0.653612792492)\n",
      " state (3)  A[0]:(0.68052482605) A[1]:(0.0560364201665) A[2]:(0.362083554268) A[3]:(0.612037062645)\n",
      " state (4)  A[0]:(0.585750460625) A[1]:(0.655662059784) A[2]:(0.000949859328102) A[3]:(0.532183945179)\n",
      " state (5)  A[0]:(-0.359077304602) A[1]:(0.999764919281) A[2]:(-0.390719741583) A[3]:(0.147151440382)\n",
      " state (6)  A[0]:(0.00507648382336) A[1]:(0.809128820896) A[2]:(-0.00226020417176) A[3]:(0.651149630547)\n",
      " state (7)  A[0]:(0.656475007534) A[1]:(-0.644416987896) A[2]:(0.295146405697) A[3]:(0.925098359585)\n",
      " state (8)  A[0]:(0.655039072037) A[1]:(0.000689089181833) A[2]:(0.728988051414) A[3]:(0.593868255615)\n",
      " state (9)  A[0]:(0.654195189476) A[1]:(0.81045627594) A[2]:(0.809693396091) A[3]:(0.0055090226233)\n",
      " state (10)  A[0]:(0.749453663826) A[1]:(0.899727523327) A[2]:(-0.00529165100306) A[3]:(0.706039905548)\n",
      " state (11)  A[0]:(0.176362782717) A[1]:(0.870594501495) A[2]:(-0.9014929533) A[3]:(0.783592283726)\n",
      " state (12)  A[0]:(-0.41567003727) A[1]:(0.797516047955) A[2]:(-0.901955485344) A[3]:(0.71828186512)\n",
      " state (13)  A[0]:(-0.00036862489651) A[1]:(0.810536563396) A[2]:(0.9000248909) A[3]:(0.733663856983)\n",
      " state (14)  A[0]:(0.813032209873) A[1]:(0.898982584476) A[2]:(0.999995231628) A[3]:(0.807398021221)\n",
      " state (15)  A[0]:(0.979304909706) A[1]:(0.936503350735) A[2]:(1.0) A[3]:(0.893015503883)\n",
      "Episode 231000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6253. Times reached goal: 857.               Steps done: 2001928. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.121567268931.\n",
      " state (0)  A[0]:(0.533132970333) A[1]:(0.588773787022) A[2]:(0.589775800705) A[3]:(0.532507777214)\n",
      " state (1)  A[0]:(0.532205104828) A[1]:(-2.07871198654e-05) A[2]:(0.655458688736) A[3]:(0.591206014156)\n",
      " state (2)  A[0]:(0.578899979591) A[1]:(0.727856278419) A[2]:(0.61647939682) A[3]:(0.654164195061)\n",
      " state (3)  A[0]:(0.680865049362) A[1]:(0.0561789162457) A[2]:(0.360061109066) A[3]:(0.610850095749)\n",
      " state (4)  A[0]:(0.586940467358) A[1]:(0.655168294907) A[2]:(0.000309824943542) A[3]:(0.532228708267)\n",
      " state (5)  A[0]:(-0.354888647795) A[1]:(0.999768555164) A[2]:(-0.391002118587) A[3]:(0.154318645597)\n",
      " state (6)  A[0]:(0.0049118748866) A[1]:(0.809245467186) A[2]:(-0.00326393870637) A[3]:(0.650030195713)\n",
      " state (7)  A[0]:(0.653622508049) A[1]:(-0.644173145294) A[2]:(0.293781667948) A[3]:(0.924497485161)\n",
      " state (8)  A[0]:(0.652433991432) A[1]:(-0.000220343470573) A[2]:(0.7283436656) A[3]:(0.588584899902)\n",
      " state (9)  A[0]:(0.651846170425) A[1]:(0.809615075588) A[2]:(0.809463381767) A[3]:(-0.000691115739755)\n",
      " state (10)  A[0]:(0.745835900307) A[1]:(0.899536728859) A[2]:(-0.00410959776491) A[3]:(0.707285642624)\n",
      " state (11)  A[0]:(0.167351782322) A[1]:(0.870662748814) A[2]:(-0.900933384895) A[3]:(0.784460306168)\n",
      " state (12)  A[0]:(-0.421226114035) A[1]:(0.797751426697) A[2]:(-0.901390075684) A[3]:(0.717729568481)\n",
      " state (13)  A[0]:(-0.00613385206088) A[1]:(0.810820519924) A[2]:(0.900061428547) A[3]:(0.73216432333)\n",
      " state (14)  A[0]:(0.810497462749) A[1]:(0.899544000626) A[2]:(0.999995291233) A[3]:(0.805685162544)\n",
      " state (15)  A[0]:(0.978908896446) A[1]:(0.937348127365) A[2]:(1.0) A[3]:(0.891532540321)\n",
      "Episode 232000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6269. Times reached goal: 866.               Steps done: 2008197. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.120807547557.\n",
      " state (0)  A[0]:(0.531850218773) A[1]:(0.591062903404) A[2]:(0.589474916458) A[3]:(0.531027674675)\n",
      " state (1)  A[0]:(0.5318557024) A[1]:(0.000544011534657) A[2]:(0.655080080032) A[3]:(0.589880049229)\n",
      " state (2)  A[0]:(0.58020401001) A[1]:(0.729914903641) A[2]:(0.614958286285) A[3]:(0.653681397438)\n",
      " state (3)  A[0]:(0.680661201477) A[1]:(0.0587184689939) A[2]:(0.358066767454) A[3]:(0.60845631361)\n",
      " state (4)  A[0]:(0.587037563324) A[1]:(0.656332790852) A[2]:(-0.000984310754575) A[3]:(0.531266152859)\n",
      " state (5)  A[0]:(-0.351655960083) A[1]:(0.99977350235) A[2]:(-0.391537904739) A[3]:(0.163235530257)\n",
      " state (6)  A[0]:(0.0031653388869) A[1]:(0.810082435608) A[2]:(-0.00339554436505) A[3]:(0.650254368782)\n",
      " state (7)  A[0]:(0.649795174599) A[1]:(-0.643869638443) A[2]:(0.294247031212) A[3]:(0.924286603928)\n",
      " state (8)  A[0]:(0.649633049965) A[1]:(0.000257432460785) A[2]:(0.729108631611) A[3]:(0.584342718124)\n",
      " state (9)  A[0]:(0.650683820248) A[1]:(0.809534132481) A[2]:(0.810123980045) A[3]:(-0.0034527045209)\n",
      " state (10)  A[0]:(0.742962360382) A[1]:(0.899662613869) A[2]:(-0.0015565144131) A[3]:(0.710642695427)\n",
      " state (11)  A[0]:(0.159532785416) A[1]:(0.870988726616) A[2]:(-0.900275349617) A[3]:(0.786930084229)\n",
      " state (12)  A[0]:(-0.424916684628) A[1]:(0.798224329948) A[2]:(-0.900694131851) A[3]:(0.719618439674)\n",
      " state (13)  A[0]:(-0.00794584676623) A[1]:(0.81120377779) A[2]:(0.900302052498) A[3]:(0.733869373798)\n",
      " state (14)  A[0]:(0.809852600098) A[1]:(0.90007930994) A[2]:(0.999995350838) A[3]:(0.807193517685)\n",
      " state (15)  A[0]:(0.978791594505) A[1]:(0.938115298748) A[2]:(1.0) A[3]:(0.892303824425)\n",
      "Episode 233000 finished after 0 timesteps with r=0.0. Running score: 0.9. Times trained:               6338. Times reached goal: 883.               Steps done: 2014535. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.120044290637.\n",
      " state (0)  A[0]:(0.531948804855) A[1]:(0.589480519295) A[2]:(0.589902341366) A[3]:(0.531666040421)\n",
      " state (1)  A[0]:(0.53296482563) A[1]:(-0.00125455786474) A[2]:(0.655004322529) A[3]:(0.59027493)\n",
      " state (2)  A[0]:(0.582015872002) A[1]:(0.728399038315) A[2]:(0.614299416542) A[3]:(0.654557228088)\n",
      " state (3)  A[0]:(0.681192874908) A[1]:(0.0541414543986) A[2]:(0.358410686255) A[3]:(0.607863068581)\n",
      " state (4)  A[0]:(0.588516354561) A[1]:(0.655326545238) A[2]:(0.000865101581439) A[3]:(0.532417416573)\n",
      " state (5)  A[0]:(-0.344414204359) A[1]:(0.999776661396) A[2]:(-0.390602856874) A[3]:(0.174493834376)\n",
      " state (6)  A[0]:(0.00695528788492) A[1]:(0.80929672718) A[2]:(-0.000371575326426) A[3]:(0.652620911598)\n",
      " state (7)  A[0]:(0.651023030281) A[1]:(-0.644589722157) A[2]:(0.297444701195) A[3]:(0.925281405449)\n",
      " state (8)  A[0]:(0.654964983463) A[1]:(-0.000956296629738) A[2]:(0.729578733444) A[3]:(0.59179008007)\n",
      " state (9)  A[0]:(0.654945909977) A[1]:(0.8093354702) A[2]:(0.810278296471) A[3]:(0.00397519161925)\n",
      " state (10)  A[0]:(0.742161273956) A[1]:(0.899448215961) A[2]:(-0.00240838062018) A[3]:(0.715516805649)\n",
      " state (11)  A[0]:(0.154840916395) A[1]:(0.870335102081) A[2]:(-0.900423049927) A[3]:(0.789071977139)\n",
      " state (12)  A[0]:(-0.423248708248) A[1]:(0.796649456024) A[2]:(-0.900499284267) A[3]:(0.720875501633)\n",
      " state (13)  A[0]:(-9.52631235123e-05) A[1]:(0.809390306473) A[2]:(0.90017926693) A[3]:(0.73497647047)\n",
      " state (14)  A[0]:(0.812747001648) A[1]:(0.899453401566) A[2]:(0.999995350838) A[3]:(0.807834744453)\n",
      " state (15)  A[0]:(0.979071676731) A[1]:(0.938307285309) A[2]:(1.0) A[3]:(0.892211616039)\n",
      "Episode 234000 finished after 0 timesteps with r=1.0. Running score: 0.82. Times trained:               6302. Times reached goal: 866.               Steps done: 2020837. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.119290150309.\n",
      "q_values \n",
      "tensor([[ 0.5327,  0.5907,  0.5902,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5340, -0.0003,  0.6554,  0.5901]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5844,  0.7289,  0.6129,  0.6541]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0076,  0.8103, -0.0013,  0.6519]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7398,  0.8997, -0.0031,  0.7161]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8110,  0.8997,  1.0000,  0.8069]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533391654491) A[1]:(0.590730428696) A[2]:(0.590602874756) A[3]:(0.531749010086)\n",
      " state (1)  A[0]:(0.534340262413) A[1]:(-0.000560954154935) A[2]:(0.65553021431) A[3]:(0.590219199657)\n",
      " state (2)  A[0]:(0.584327101707) A[1]:(0.72800719738) A[2]:(0.61271739006) A[3]:(0.654084742069)\n",
      " state (3)  A[0]:(0.681161284447) A[1]:(0.054208535701) A[2]:(0.356373250484) A[3]:(0.605110704899)\n",
      " state (4)  A[0]:(0.588930130005) A[1]:(0.655828237534) A[2]:(-0.000520944537129) A[3]:(0.530740261078)\n",
      " state (5)  A[0]:(-0.33872500062) A[1]:(0.999780595303) A[2]:(-0.392004340887) A[3]:(0.181872740388)\n",
      " state (6)  A[0]:(0.00676773581654) A[1]:(0.809185683727) A[2]:(-0.00161838391796) A[3]:(0.651745200157)\n",
      " state (7)  A[0]:(0.648391842842) A[1]:(-0.644666433334) A[2]:(0.296603083611) A[3]:(0.924917399883)\n",
      " state (8)  A[0]:(0.654786467552) A[1]:(-0.0011333967559) A[2]:(0.72898209095) A[3]:(0.589349985123)\n",
      " state (9)  A[0]:(0.655522942543) A[1]:(0.809230804443) A[2]:(0.810064971447) A[3]:(-3.9130449295e-05)\n",
      " state (10)  A[0]:(0.739910840988) A[1]:(0.899616837502) A[2]:(-0.0016304240562) A[3]:(0.716025769711)\n",
      " state (11)  A[0]:(0.147422358394) A[1]:(0.870716094971) A[2]:(-0.899943470955) A[3]:(0.788300693035)\n",
      " state (12)  A[0]:(-0.426964432001) A[1]:(0.797191441059) A[2]:(-0.899857163429) A[3]:(0.718240559101)\n",
      " state (13)  A[0]:(-0.00318283261731) A[1]:(0.809761762619) A[2]:(0.90026974678) A[3]:(0.732530772686)\n",
      " state (14)  A[0]:(0.810924351215) A[1]:(0.899949610233) A[2]:(0.999995410442) A[3]:(0.806545615196)\n",
      " state (15)  A[0]:(0.978718340397) A[1]:(0.93906378746) A[2]:(1.0) A[3]:(0.891533613205)\n",
      "Episode 235000 finished after 0 timesteps with r=1.0. Running score: 0.84. Times trained:               6293. Times reached goal: 879.               Steps done: 2027130. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.118541814501.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.533200860023) A[1]:(0.590961575508) A[2]:(0.589601635933) A[3]:(0.531596064568)\n",
      " state (1)  A[0]:(0.532898962498) A[1]:(6.63846731186e-05) A[2]:(0.655284941196) A[3]:(0.590008735657)\n",
      " state (2)  A[0]:(0.583029508591) A[1]:(0.729339957237) A[2]:(0.612241506577) A[3]:(0.654558777809)\n",
      " state (3)  A[0]:(0.678638219833) A[1]:(0.0580022484064) A[2]:(0.356283962727) A[3]:(0.60407102108)\n",
      " state (4)  A[0]:(0.587918281555) A[1]:(0.655876278877) A[2]:(-0.000208735466003) A[3]:(0.531712770462)\n",
      " state (5)  A[0]:(-0.33293774724) A[1]:(0.99978518486) A[2]:(-0.393722742796) A[3]:(0.192280992866)\n",
      " state (6)  A[0]:(0.0051041697152) A[1]:(0.810322880745) A[2]:(-0.00116813124623) A[3]:(0.652001976967)\n",
      " state (7)  A[0]:(0.6446570158) A[1]:(-0.644064247608) A[2]:(0.298043310642) A[3]:(0.924860239029)\n",
      " state (8)  A[0]:(0.653017699718) A[1]:(1.46478414536e-05) A[2]:(0.729095697403) A[3]:(0.588782608509)\n",
      " state (9)  A[0]:(0.652844846249) A[1]:(0.809990882874) A[2]:(0.80999648571) A[3]:(-0.00423968257383)\n",
      " state (10)  A[0]:(0.734816789627) A[1]:(0.900054574013) A[2]:(-0.00346647296101) A[3]:(0.716364145279)\n",
      " state (11)  A[0]:(0.135122045875) A[1]:(0.871197938919) A[2]:(-0.900391101837) A[3]:(0.788061261177)\n",
      " state (12)  A[0]:(-0.431857675314) A[1]:(0.79776930809) A[2]:(-0.900034248829) A[3]:(0.717508673668)\n",
      " state (13)  A[0]:(-0.00330080394633) A[1]:(0.81027418375) A[2]:(0.90001565218) A[3]:(0.732682049274)\n",
      " state (14)  A[0]:(0.811483561993) A[1]:(0.900606989861) A[2]:(0.999995410442) A[3]:(0.807353973389)\n",
      " state (15)  A[0]:(0.978771567345) A[1]:(0.939938247204) A[2]:(1.0) A[3]:(0.892010390759)\n",
      "Episode 236000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6284. Times reached goal: 874.               Steps done: 2033414. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.117799233373.\n",
      " state (0)  A[0]:(0.530324161053) A[1]:(0.589560866356) A[2]:(0.590220630169) A[3]:(0.531472682953)\n",
      " state (1)  A[0]:(0.530375123024) A[1]:(-1.01774930954e-05) A[2]:(0.656352698803) A[3]:(0.591081738472)\n",
      " state (2)  A[0]:(0.58120983839) A[1]:(0.728174030781) A[2]:(0.611984372139) A[3]:(0.655300021172)\n",
      " state (3)  A[0]:(0.675192832947) A[1]:(0.057042196393) A[2]:(0.356522887945) A[3]:(0.602788090706)\n",
      " state (4)  A[0]:(0.584935188293) A[1]:(0.654755830765) A[2]:(0.000216484069824) A[3]:(0.532118976116)\n",
      " state (5)  A[0]:(-0.329953104258) A[1]:(0.999788105488) A[2]:(-0.396045476198) A[3]:(0.203740119934)\n",
      " state (6)  A[0]:(0.00174040917773) A[1]:(0.809682846069) A[2]:(-0.00219452031888) A[3]:(0.653313875198)\n",
      " state (7)  A[0]:(0.641143918037) A[1]:(-0.64549946785) A[2]:(0.298447310925) A[3]:(0.925168275833)\n",
      " state (8)  A[0]:(0.652768373489) A[1]:(-0.000789090816397) A[2]:(0.729425191879) A[3]:(0.590658783913)\n",
      " state (9)  A[0]:(0.654823541641) A[1]:(0.809789896011) A[2]:(0.810360789299) A[3]:(0.00175264303107)\n",
      " state (10)  A[0]:(0.734786629677) A[1]:(0.899825811386) A[2]:(-0.00110578490421) A[3]:(0.721685886383)\n",
      " state (11)  A[0]:(0.132889315486) A[1]:(0.870652198792) A[2]:(-0.899719178677) A[3]:(0.790716469288)\n",
      " state (12)  A[0]:(-0.432363569736) A[1]:(0.796555459499) A[2]:(-0.899417579174) A[3]:(0.71862745285)\n",
      " state (13)  A[0]:(-0.00306380330585) A[1]:(0.808877110481) A[2]:(0.900080323219) A[3]:(0.733300447464)\n",
      " state (14)  A[0]:(0.811111629009) A[1]:(0.900093317032) A[2]:(0.999995470047) A[3]:(0.808195650578)\n",
      " state (15)  A[0]:(0.978647351265) A[1]:(0.940019547939) A[2]:(1.0) A[3]:(0.892532229424)\n",
      "Episode 237000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6318. Times reached goal: 890.               Steps done: 2039732. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.117057323976.\n",
      " state (0)  A[0]:(0.534285068512) A[1]:(0.589839339256) A[2]:(0.591284036636) A[3]:(0.532263278961)\n",
      " state (1)  A[0]:(0.535105705261) A[1]:(-0.00218326994218) A[2]:(0.655394434929) A[3]:(0.591472625732)\n",
      " state (2)  A[0]:(0.586384296417) A[1]:(0.728624999523) A[2]:(0.610753774643) A[3]:(0.655969560146)\n",
      " state (3)  A[0]:(0.677702307701) A[1]:(0.0584295317531) A[2]:(0.356356799603) A[3]:(0.601469635963)\n",
      " state (4)  A[0]:(0.589101076126) A[1]:(0.656024634838) A[2]:(0.000860333209857) A[3]:(0.532428324223)\n",
      " state (5)  A[0]:(-0.317150712013) A[1]:(0.999792873859) A[2]:(-0.397611886263) A[3]:(0.214777857065)\n",
      " state (6)  A[0]:(0.00614775437862) A[1]:(0.810094416142) A[2]:(-0.00130450655706) A[3]:(0.652462720871)\n",
      " state (7)  A[0]:(0.640710055828) A[1]:(-0.644386768341) A[2]:(0.299754530191) A[3]:(0.924601614475)\n",
      " state (8)  A[0]:(0.654994130135) A[1]:(0.000275775790215) A[2]:(0.729114532471) A[3]:(0.590229451656)\n",
      " state (9)  A[0]:(0.656868338585) A[1]:(0.810285449028) A[2]:(0.810391426086) A[3]:(-0.00129991699941)\n",
      " state (10)  A[0]:(0.73501431942) A[1]:(0.900238633156) A[2]:(0.000153541564941) A[3]:(0.722710490227)\n",
      " state (11)  A[0]:(0.132605716586) A[1]:(0.871299147606) A[2]:(-0.899343430996) A[3]:(0.791267514229)\n",
      " state (12)  A[0]:(-0.431036382914) A[1]:(0.797515392303) A[2]:(-0.899157524109) A[3]:(0.718197703362)\n",
      " state (13)  A[0]:(-0.000699594500475) A[1]:(0.809641242027) A[2]:(0.899902522564) A[3]:(0.732867121696)\n",
      " state (14)  A[0]:(0.81151342392) A[1]:(0.900715827942) A[2]:(0.999995529652) A[3]:(0.808264374733)\n",
      " state (15)  A[0]:(0.978639662266) A[1]:(0.940735042095) A[2]:(1.0) A[3]:(0.892525196075)\n",
      "Episode 238000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6369. Times reached goal: 890.               Steps done: 2046101. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.116314155013.\n",
      " state (0)  A[0]:(0.533463716507) A[1]:(0.591471552849) A[2]:(0.590767621994) A[3]:(0.53165435791)\n",
      " state (1)  A[0]:(0.534461975098) A[1]:(0.00111655844375) A[2]:(0.656083106995) A[3]:(0.590905368328)\n",
      " state (2)  A[0]:(0.586406230927) A[1]:(0.728624403477) A[2]:(0.610816836357) A[3]:(0.655762910843)\n",
      " state (3)  A[0]:(0.67636525631) A[1]:(0.0533353574574) A[2]:(0.35722643137) A[3]:(0.599352777004)\n",
      " state (4)  A[0]:(0.588229894638) A[1]:(0.656929016113) A[2]:(0.0011707538506) A[3]:(0.53237092495)\n",
      " state (5)  A[0]:(-0.308220475912) A[1]:(0.999797284603) A[2]:(-0.400581151247) A[3]:(0.229967534542)\n",
      " state (6)  A[0]:(0.00634031463414) A[1]:(0.809743225574) A[2]:(-0.000141620635986) A[3]:(0.654269278049)\n",
      " state (7)  A[0]:(0.638003230095) A[1]:(-0.644507646561) A[2]:(0.302592873573) A[3]:(0.924615502357)\n",
      " state (8)  A[0]:(0.65563082695) A[1]:(7.51465559006e-05) A[2]:(0.729712724686) A[3]:(0.591963648796)\n",
      " state (9)  A[0]:(0.657968580723) A[1]:(0.81014162302) A[2]:(0.810301423073) A[3]:(-0.000186592340469)\n",
      " state (10)  A[0]:(0.733223438263) A[1]:(0.900061607361) A[2]:(-0.00198960048147) A[3]:(0.723904252052)\n",
      " state (11)  A[0]:(0.125900164247) A[1]:(0.870970726013) A[2]:(-0.899953603745) A[3]:(0.791039764881)\n",
      " state (12)  A[0]:(-0.433973520994) A[1]:(0.796851217747) A[2]:(-0.899783849716) A[3]:(0.717217087746)\n",
      " state (13)  A[0]:(-0.0014037033543) A[1]:(0.808803319931) A[2]:(0.898986935616) A[3]:(0.732803106308)\n",
      " state (14)  A[0]:(0.811404824257) A[1]:(0.900453388691) A[2]:(0.999995529652) A[3]:(0.809124827385)\n",
      " state (15)  A[0]:(0.978616952896) A[1]:(0.940909147263) A[2]:(1.0) A[3]:(0.893166184425)\n",
      "Episode 239000 finished after 0 timesteps with r=1.0. Running score: 0.81. Times trained:               6304. Times reached goal: 882.               Steps done: 2052405. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.115583216918.\n",
      "q_values \n",
      "tensor([[ 0.5325,  0.5903,  0.5901,  0.5320]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5876,  0.6559,  0.0029,  0.5321]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6554, -0.0003,  0.7287,  0.5927]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6578,  0.8103,  0.8097,  0.0009]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? True\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7331,  0.9002, -0.0015,  0.7261]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8113,  0.9009,  1.0000,  0.8087]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531783342361) A[1]:(0.590959966183) A[2]:(0.588910520077) A[3]:(0.53110408783)\n",
      " state (1)  A[0]:(0.532459914684) A[1]:(-0.00121924222913) A[2]:(0.655257642269) A[3]:(0.589471578598)\n",
      " state (2)  A[0]:(0.585349142551) A[1]:(0.728827178478) A[2]:(0.610917925835) A[3]:(0.654870152473)\n",
      " state (3)  A[0]:(0.67359995842) A[1]:(0.0549097619951) A[2]:(0.358791202307) A[3]:(0.596132814884)\n",
      " state (4)  A[0]:(0.58680164814) A[1]:(0.656287312508) A[2]:(0.00145673647057) A[3]:(0.530849933624)\n",
      " state (5)  A[0]:(-0.300146520138) A[1]:(0.999800860882) A[2]:(-0.406085908413) A[3]:(0.242132112384)\n",
      " state (6)  A[0]:(0.00466020032763) A[1]:(0.809895634651) A[2]:(-0.00160896638408) A[3]:(0.652417182922)\n",
      " state (7)  A[0]:(0.634024143219) A[1]:(-0.643390238285) A[2]:(0.303271502256) A[3]:(0.923369467258)\n",
      " state (8)  A[0]:(0.653939723969) A[1]:(0.00196197372861) A[2]:(0.729573965073) A[3]:(0.589223504066)\n",
      " state (9)  A[0]:(0.657554268837) A[1]:(0.810722470284) A[2]:(0.810405910015) A[3]:(-0.00120952667203)\n",
      " state (10)  A[0]:(0.732992172241) A[1]:(0.900370836258) A[2]:(0.000377178163035) A[3]:(0.725477218628)\n",
      " state (11)  A[0]:(0.126187428832) A[1]:(0.871382713318) A[2]:(-0.899161040783) A[3]:(0.791696190834)\n",
      " state (12)  A[0]:(-0.432972401381) A[1]:(0.797488510609) A[2]:(-0.89889138937) A[3]:(0.716403007507)\n",
      " state (13)  A[0]:(-0.000578835548367) A[1]:(0.809421539307) A[2]:(0.899930417538) A[3]:(0.731511712074)\n",
      " state (14)  A[0]:(0.811143875122) A[1]:(0.900973200798) A[2]:(0.999995589256) A[3]:(0.808463990688)\n",
      " state (15)  A[0]:(0.978515863419) A[1]:(0.941439390182) A[2]:(1.0) A[3]:(0.892813265324)\n",
      "Episode 240000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6328. Times reached goal: 877.               Steps done: 2058733. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.11485411563.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532566547394) A[1]:(0.590492904186) A[2]:(0.58952832222) A[3]:(0.531683921814)\n",
      " state (1)  A[0]:(0.532769799232) A[1]:(0.000368654698832) A[2]:(0.656133174896) A[3]:(0.590465247631)\n",
      " state (2)  A[0]:(0.58545345068) A[1]:(0.728501439095) A[2]:(0.611442446709) A[3]:(0.655670523643)\n",
      " state (3)  A[0]:(0.6720495224) A[1]:(0.0539023652673) A[2]:(0.36170810461) A[3]:(0.595124006271)\n",
      " state (4)  A[0]:(0.586598396301) A[1]:(0.655735015869) A[2]:(0.00329111819156) A[3]:(0.532451868057)\n",
      " state (5)  A[0]:(-0.290084928274) A[1]:(0.999804913998) A[2]:(-0.411155194044) A[3]:(0.261655032635)\n",
      " state (6)  A[0]:(0.00400965753943) A[1]:(0.810255765915) A[2]:(-0.002545947209) A[3]:(0.654263675213)\n",
      " state (7)  A[0]:(0.630178689957) A[1]:(-0.643251538277) A[2]:(0.304231107235) A[3]:(0.922825753689)\n",
      " state (8)  A[0]:(0.652673721313) A[1]:(0.000318586826324) A[2]:(0.728929400444) A[3]:(0.590571761131)\n",
      " state (9)  A[0]:(0.655610144138) A[1]:(0.810091435909) A[2]:(0.809759378433) A[3]:(-0.00106459821109)\n",
      " state (10)  A[0]:(0.729663252831) A[1]:(0.900008261204) A[2]:(-0.00240027438849) A[3]:(0.725650548935)\n",
      " state (11)  A[0]:(0.118128940463) A[1]:(0.87095528841) A[2]:(-0.899731099606) A[3]:(0.791209995747)\n",
      " state (12)  A[0]:(-0.437162965536) A[1]:(0.796844780445) A[2]:(-0.899230480194) A[3]:(0.715480446815)\n",
      " state (13)  A[0]:(-0.00302264653146) A[1]:(0.808683037758) A[2]:(0.899883508682) A[3]:(0.731332778931)\n",
      " state (14)  A[0]:(0.810622990131) A[1]:(0.900634884834) A[2]:(0.999995648861) A[3]:(0.809037268162)\n",
      " state (15)  A[0]:(0.978465735912) A[1]:(0.941389203072) A[2]:(1.0) A[3]:(0.893238306046)\n",
      "Episode 241000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6255. Times reached goal: 881.               Steps done: 2064988. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.114137945295.\n",
      " state (0)  A[0]:(0.531716287136) A[1]:(0.591032028198) A[2]:(0.590017914772) A[3]:(0.531716108322)\n",
      " state (1)  A[0]:(0.531379938126) A[1]:(0.00184458284639) A[2]:(0.656383991241) A[3]:(0.590635538101)\n",
      " state (2)  A[0]:(0.584306180477) A[1]:(0.728939712048) A[2]:(0.609645664692) A[3]:(0.656078338623)\n",
      " state (3)  A[0]:(0.66975736618) A[1]:(0.0537610389292) A[2]:(0.360743105412) A[3]:(0.593252897263)\n",
      " state (4)  A[0]:(0.585546135902) A[1]:(0.656381905079) A[2]:(0.00112724257633) A[3]:(0.53267967701)\n",
      " state (5)  A[0]:(-0.281407445669) A[1]:(0.999808847904) A[2]:(-0.416716396809) A[3]:(0.278192996979)\n",
      " state (6)  A[0]:(0.00196714443155) A[1]:(0.810242176056) A[2]:(-0.00182294647675) A[3]:(0.65435731411)\n",
      " state (7)  A[0]:(0.626388192177) A[1]:(-0.641683697701) A[2]:(0.306961447001) A[3]:(0.921967625618)\n",
      " state (8)  A[0]:(0.653265118599) A[1]:(0.00142997410148) A[2]:(0.729068040848) A[3]:(0.59247815609)\n",
      " state (9)  A[0]:(0.657351315022) A[1]:(0.810398161411) A[2]:(0.810181677341) A[3]:(0.000647216918878)\n",
      " state (10)  A[0]:(0.731508791447) A[1]:(0.900151610374) A[2]:(0.000443935365183) A[3]:(0.727426528931)\n",
      " state (11)  A[0]:(0.122687749565) A[1]:(0.871154725552) A[2]:(-0.89912301302) A[3]:(0.792554855347)\n",
      " state (12)  A[0]:(-0.433892667294) A[1]:(0.797164916992) A[2]:(-0.898934483528) A[3]:(0.716216027737)\n",
      " state (13)  A[0]:(-0.000634297670331) A[1]:(0.808957338333) A[2]:(0.899959504604) A[3]:(0.731419801712)\n",
      " state (14)  A[0]:(0.810852944851) A[1]:(0.90093511343) A[2]:(0.999995708466) A[3]:(0.808922290802)\n",
      " state (15)  A[0]:(0.978453338146) A[1]:(0.941770732403) A[2]:(1.0) A[3]:(0.892873764038)\n",
      "Episode 242000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6250. Times reached goal: 887.               Steps done: 2071238. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.113426807756.\n",
      " state (0)  A[0]:(0.531532049179) A[1]:(0.590767145157) A[2]:(0.590620875359) A[3]:(0.53162163496)\n",
      " state (1)  A[0]:(0.531869649887) A[1]:(0.000921174651012) A[2]:(0.655721604824) A[3]:(0.590630590916)\n",
      " state (2)  A[0]:(0.585738062859) A[1]:(0.729134082794) A[2]:(0.607261300087) A[3]:(0.655805230141)\n",
      " state (3)  A[0]:(0.669116735458) A[1]:(0.0551521107554) A[2]:(0.360613882542) A[3]:(0.590442955494)\n",
      " state (4)  A[0]:(0.586196303368) A[1]:(0.656357884407) A[2]:(0.000887393718585) A[3]:(0.531982898712)\n",
      " state (5)  A[0]:(-0.271515130997) A[1]:(0.999812602997) A[2]:(-0.421743452549) A[3]:(0.294530361891)\n",
      " state (6)  A[0]:(0.00212283106521) A[1]:(0.810554563999) A[2]:(-0.00170326069929) A[3]:(0.654129505157)\n",
      " state (7)  A[0]:(0.623998105526) A[1]:(-0.641152858734) A[2]:(0.309241563082) A[3]:(0.920840144157)\n",
      " state (8)  A[0]:(0.654111862183) A[1]:(0.000769540492911) A[2]:(0.72920191288) A[3]:(0.593271374702)\n",
      " state (9)  A[0]:(0.657800197601) A[1]:(0.810300767422) A[2]:(0.810290157795) A[3]:(0.000905930763111)\n",
      " state (10)  A[0]:(0.730514883995) A[1]:(0.900106728077) A[2]:(-9.29832458496e-06) A[3]:(0.727275848389)\n",
      " state (11)  A[0]:(0.119530446827) A[1]:(0.87106102705) A[2]:(-0.89931344986) A[3]:(0.791973471642)\n",
      " state (12)  A[0]:(-0.434723168612) A[1]:(0.796864569187) A[2]:(-0.899007618427) A[3]:(0.715457379818)\n",
      " state (13)  A[0]:(1.25169754028e-06) A[1]:(0.808389723301) A[2]:(0.900168061256) A[3]:(0.731291413307)\n",
      " state (14)  A[0]:(0.81104683876) A[1]:(0.900637865067) A[2]:(0.99999576807) A[3]:(0.809198617935)\n",
      " state (15)  A[0]:(0.978460669518) A[1]:(0.941739082336) A[2]:(1.0) A[3]:(0.892904520035)\n",
      "Episode 243000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6348. Times reached goal: 887.               Steps done: 2077586. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.112709054939.\n",
      " state (0)  A[0]:(0.534376263618) A[1]:(0.590109467506) A[2]:(0.592739999294) A[3]:(0.533263742924)\n",
      " state (1)  A[0]:(0.534200251102) A[1]:(0.000281780958176) A[2]:(0.656201124191) A[3]:(0.592621803284)\n",
      " state (2)  A[0]:(0.587412118912) A[1]:(0.72828823328) A[2]:(0.607288956642) A[3]:(0.657326817513)\n",
      " state (3)  A[0]:(0.668897211552) A[1]:(0.0523210279644) A[2]:(0.363402545452) A[3]:(0.590032815933)\n",
      " state (4)  A[0]:(0.586577475071) A[1]:(0.656685113907) A[2]:(0.00264179101214) A[3]:(0.533349633217)\n",
      " state (5)  A[0]:(-0.261815100908) A[1]:(0.999815523624) A[2]:(-0.426394104958) A[3]:(0.31156924367)\n",
      " state (6)  A[0]:(0.00288750929758) A[1]:(0.809410512447) A[2]:(-0.00132632174063) A[3]:(0.654521644115)\n",
      " state (7)  A[0]:(0.621687531471) A[1]:(-0.641431808472) A[2]:(0.311233371496) A[3]:(0.91963160038)\n",
      " state (8)  A[0]:(0.653764009476) A[1]:(-0.00100424850825) A[2]:(0.729281544685) A[3]:(0.590570211411)\n",
      " state (9)  A[0]:(0.658535480499) A[1]:(0.809280991554) A[2]:(0.810098469257) A[3]:(-0.00103691185359)\n",
      " state (10)  A[0]:(0.731409192085) A[1]:(0.899590313435) A[2]:(0.00020158290863) A[3]:(0.727504014969)\n",
      " state (11)  A[0]:(0.121419362724) A[1]:(0.870643436909) A[2]:(-0.899212419987) A[3]:(0.79227155447)\n",
      " state (12)  A[0]:(-0.433916121721) A[1]:(0.796499192715) A[2]:(-0.899049818516) A[3]:(0.715303659439)\n",
      " state (13)  A[0]:(-0.000513017119374) A[1]:(0.808095514774) A[2]:(0.900268673897) A[3]:(0.730888366699)\n",
      " state (14)  A[0]:(0.810454726219) A[1]:(0.900575101376) A[2]:(0.999995827675) A[3]:(0.808898627758)\n",
      " state (15)  A[0]:(0.978377282619) A[1]:(0.94182151556) A[2]:(1.0) A[3]:(0.892496407032)\n",
      "Episode 244000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6211. Times reached goal: 877.               Steps done: 2083797. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.112011188466.\n",
      "q_values \n",
      "tensor([[ 0.5305,  0.5904,  0.5898,  0.5300]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5854,  0.6563,  0.0023,  0.5309]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6545, -0.0003,  0.7287,  0.5935]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6574,  0.8099,  0.8099,  0.0000]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9001, -0.0003,  0.7268]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8106,  0.9011,  1.0000,  0.8090]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530042409897) A[1]:(0.590730786324) A[2]:(0.590335309505) A[3]:(0.529908597469)\n",
      " state (1)  A[0]:(0.529958486557) A[1]:(0.00114519847557) A[2]:(0.656131267548) A[3]:(0.589558005333)\n",
      " state (2)  A[0]:(0.584925532341) A[1]:(0.728737354279) A[2]:(0.606947362423) A[3]:(0.654811024666)\n",
      " state (3)  A[0]:(0.66579002142) A[1]:(0.0529238507152) A[2]:(0.365283429623) A[3]:(0.585422635078)\n",
      " state (4)  A[0]:(0.584697842598) A[1]:(0.657231032848) A[2]:(0.00350128184073) A[3]:(0.530593752861)\n",
      " state (5)  A[0]:(-0.25408616662) A[1]:(0.999819338322) A[2]:(-0.431071162224) A[3]:(0.323783248663)\n",
      " state (6)  A[0]:(0.00197140616365) A[1]:(0.810445308685) A[2]:(-0.000562667788472) A[3]:(0.653661131859)\n",
      " state (7)  A[0]:(0.618877768517) A[1]:(-0.639412164688) A[2]:(0.314247131348) A[3]:(0.91849899292)\n",
      " state (8)  A[0]:(0.653265595436) A[1]:(0.00123746634927) A[2]:(0.729942321777) A[3]:(0.591026484966)\n",
      " state (9)  A[0]:(0.657058238983) A[1]:(0.810306787491) A[2]:(0.810738325119) A[3]:(-0.00202542264014)\n",
      " state (10)  A[0]:(0.729532241821) A[1]:(0.900175631046) A[2]:(0.00181591312867) A[3]:(0.726678490639)\n",
      " state (11)  A[0]:(0.117606811225) A[1]:(0.871429979801) A[2]:(-0.89901381731) A[3]:(0.7914083004)\n",
      " state (12)  A[0]:(-0.435435056686) A[1]:(0.797696828842) A[2]:(-0.898882329464) A[3]:(0.71427667141)\n",
      " state (13)  A[0]:(-0.000449076265795) A[1]:(0.809100270271) A[2]:(0.900691390038) A[3]:(0.730510532856)\n",
      " state (14)  A[0]:(0.810957193375) A[1]:(0.901155233383) A[2]:(0.99999588728) A[3]:(0.809088230133)\n",
      " state (15)  A[0]:(0.978499352932) A[1]:(0.942272245884) A[2]:(1.0) A[3]:(0.892577648163)\n",
      "Episode 245000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6268. Times reached goal: 873.               Steps done: 2090065. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.111311298085.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531949520111) A[1]:(0.591548204422) A[2]:(0.591288864613) A[3]:(0.531660974026)\n",
      " state (1)  A[0]:(0.532295107841) A[1]:(-0.000506311596837) A[2]:(0.656564772129) A[3]:(0.591223120689)\n",
      " state (2)  A[0]:(0.588045954704) A[1]:(0.72909963131) A[2]:(0.606538534164) A[3]:(0.656210780144)\n",
      " state (3)  A[0]:(0.667281150818) A[1]:(0.0520363003016) A[2]:(0.366389811039) A[3]:(0.585006058216)\n",
      " state (4)  A[0]:(0.586902976036) A[1]:(0.657131612301) A[2]:(0.00244914996438) A[3]:(0.531885027885)\n",
      " state (5)  A[0]:(-0.245308056474) A[1]:(0.999822199345) A[2]:(-0.43735703826) A[3]:(0.340035945177)\n",
      " state (6)  A[0]:(0.000589191855397) A[1]:(0.810097694397) A[2]:(-0.00158095231745) A[3]:(0.654078662395)\n",
      " state (7)  A[0]:(0.615149140358) A[1]:(-0.639483809471) A[2]:(0.315096288919) A[3]:(0.917362809181)\n",
      " state (8)  A[0]:(0.651754617691) A[1]:(0.000626891793218) A[2]:(0.729178130627) A[3]:(0.589127898216)\n",
      " state (9)  A[0]:(0.656164526939) A[1]:(0.810210585594) A[2]:(0.809593439102) A[3]:(-0.00341959460638)\n",
      " state (10)  A[0]:(0.728176951408) A[1]:(0.900127470493) A[2]:(-0.00281941145658) A[3]:(0.726751923561)\n",
      " state (11)  A[0]:(0.112986803055) A[1]:(0.871471762657) A[2]:(-0.900067746639) A[3]:(0.791529893875)\n",
      " state (12)  A[0]:(-0.439941853285) A[1]:(0.797930240631) A[2]:(-0.899767041206) A[3]:(0.714155733585)\n",
      " state (13)  A[0]:(-0.00617841957137) A[1]:(0.809375047684) A[2]:(0.900698125362) A[3]:(0.73015075922)\n",
      " state (14)  A[0]:(0.809106111526) A[1]:(0.901306331158) A[2]:(0.999996006489) A[3]:(0.808612585068)\n",
      " state (15)  A[0]:(0.978310763836) A[1]:(0.942340970039) A[2]:(1.0) A[3]:(0.891959071159)\n",
      "Episode 246000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6298. Times reached goal: 871.               Steps done: 2096363. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.110612462473.\n",
      " state (0)  A[0]:(0.532356619835) A[1]:(0.590907931328) A[2]:(0.591559588909) A[3]:(0.532984018326)\n",
      " state (1)  A[0]:(0.532767295837) A[1]:(4.36156988144e-05) A[2]:(0.656999409199) A[3]:(0.591822743416)\n",
      " state (2)  A[0]:(0.588226556778) A[1]:(0.728997051716) A[2]:(0.605730891228) A[3]:(0.656866252422)\n",
      " state (3)  A[0]:(0.666269779205) A[1]:(0.0523600354791) A[2]:(0.368282020092) A[3]:(0.584052443504)\n",
      " state (4)  A[0]:(0.587373554707) A[1]:(0.656428933144) A[2]:(0.00385902402923) A[3]:(0.53328371048)\n",
      " state (5)  A[0]:(-0.23553737998) A[1]:(0.999824881554) A[2]:(-0.44132438302) A[3]:(0.357415884733)\n",
      " state (6)  A[0]:(0.00167007592972) A[1]:(0.809943258762) A[2]:(-6.05583190918e-05) A[3]:(0.655346989632)\n",
      " state (7)  A[0]:(0.613697052002) A[1]:(-0.638751506805) A[2]:(0.317814409733) A[3]:(0.916623055935)\n",
      " state (8)  A[0]:(0.652856469154) A[1]:(-0.00094296009047) A[2]:(0.728927969933) A[3]:(0.593209266663)\n",
      " state (9)  A[0]:(0.655621647835) A[1]:(0.809772968292) A[2]:(0.809603750706) A[3]:(-0.000225275754929)\n",
      " state (10)  A[0]:(0.727442860603) A[1]:(0.899929821491) A[2]:(-0.00267934147269) A[3]:(0.727750360966)\n",
      " state (11)  A[0]:(0.112457029521) A[1]:(0.871276855469) A[2]:(-0.900235652924) A[3]:(0.792354106903)\n",
      " state (12)  A[0]:(-0.439199179411) A[1]:(0.797535717487) A[2]:(-0.900343060493) A[3]:(0.715310692787)\n",
      " state (13)  A[0]:(-0.00461963983253) A[1]:(0.80868345499) A[2]:(0.899888038635) A[3]:(0.731558442116)\n",
      " state (14)  A[0]:(0.809694647789) A[1]:(0.900927484035) A[2]:(0.999996006489) A[3]:(0.809796690941)\n",
      " state (15)  A[0]:(0.978407144547) A[1]:(0.942262709141) A[2]:(1.0) A[3]:(0.892398238182)\n",
      "Episode 247000 finished after 0 timesteps with r=0.0. Running score: 0.86. Times trained:               6349. Times reached goal: 897.               Steps done: 2102712. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.109912408621.\n",
      " state (0)  A[0]:(0.532169759274) A[1]:(0.590302348137) A[2]:(0.590558052063) A[3]:(0.531747937202)\n",
      " state (1)  A[0]:(0.532834291458) A[1]:(0.000453770131571) A[2]:(0.655735611916) A[3]:(0.590683460236)\n",
      " state (2)  A[0]:(0.587945461273) A[1]:(0.728578805923) A[2]:(0.604818940163) A[3]:(0.655925214291)\n",
      " state (3)  A[0]:(0.665389478207) A[1]:(0.0489147640765) A[2]:(0.369870841503) A[3]:(0.581304907799)\n",
      " state (4)  A[0]:(0.587484359741) A[1]:(0.656134486198) A[2]:(0.002658956917) A[3]:(0.53187417984)\n",
      " state (5)  A[0]:(-0.225373968482) A[1]:(0.999827682972) A[2]:(-0.450333178043) A[3]:(0.370469778776)\n",
      " state (6)  A[0]:(0.00288366479799) A[1]:(0.810106277466) A[2]:(-0.00286518735811) A[3]:(0.654578447342)\n",
      " state (7)  A[0]:(0.612121105194) A[1]:(-0.637296915054) A[2]:(0.318717986345) A[3]:(0.915271162987)\n",
      " state (8)  A[0]:(0.653576374054) A[1]:(4.41670417786e-05) A[2]:(0.729075849056) A[3]:(0.591994643211)\n",
      " state (9)  A[0]:(0.657079815865) A[1]:(0.809970200062) A[2]:(0.810091793537) A[3]:(-0.000663488986902)\n",
      " state (10)  A[0]:(0.729301512241) A[1]:(0.899976849556) A[2]:(0.000128746032715) A[3]:(0.727677941322)\n",
      " state (11)  A[0]:(0.117062881589) A[1]:(0.871318340302) A[2]:(-0.899659574032) A[3]:(0.791957139969)\n",
      " state (12)  A[0]:(-0.4360871315) A[1]:(0.797483146191) A[2]:(-0.900016486645) A[3]:(0.714012265205)\n",
      " state (13)  A[0]:(-0.00233539519832) A[1]:(0.808360338211) A[2]:(0.900301516056) A[3]:(0.730109333992)\n",
      " state (14)  A[0]:(0.810086607933) A[1]:(0.900682091713) A[2]:(0.999996066093) A[3]:(0.808903872967)\n",
      " state (15)  A[0]:(0.978454947472) A[1]:(0.94214195013) A[2]:(1.0) A[3]:(0.891784250736)\n",
      "Episode 248000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6407. Times reached goal: 900.               Steps done: 2109119. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.109210450942.\n",
      " state (0)  A[0]:(0.531198859215) A[1]:(0.590290725231) A[2]:(0.589358925819) A[3]:(0.530153155327)\n",
      " state (1)  A[0]:(0.531471610069) A[1]:(0.000219210982323) A[2]:(0.655734658241) A[3]:(0.589230656624)\n",
      " state (2)  A[0]:(0.587236344814) A[1]:(0.729198098183) A[2]:(0.60538226366) A[3]:(0.655118703842)\n",
      " state (3)  A[0]:(0.664143502712) A[1]:(0.0489521510899) A[2]:(0.373855233192) A[3]:(0.578732788563)\n",
      " state (4)  A[0]:(0.587109386921) A[1]:(0.656004071236) A[2]:(0.00546855246648) A[3]:(0.530994296074)\n",
      " state (5)  A[0]:(-0.218727767467) A[1]:(0.999830245972) A[2]:(-0.454635858536) A[3]:(0.383784413338)\n",
      " state (6)  A[0]:(0.00161996344104) A[1]:(0.810130178928) A[2]:(-0.00167226640042) A[3]:(0.653606414795)\n",
      " state (7)  A[0]:(0.609561443329) A[1]:(-0.635611891747) A[2]:(0.321208626032) A[3]:(0.913812816143)\n",
      " state (8)  A[0]:(0.653666198254) A[1]:(0.00102411175612) A[2]:(0.729127645493) A[3]:(0.591315984726)\n",
      " state (9)  A[0]:(0.65728443861) A[1]:(0.810347318649) A[2]:(0.81017768383) A[3]:(-0.0017113967333)\n",
      " state (10)  A[0]:(0.729916632175) A[1]:(0.900195658207) A[2]:(0.000756740453653) A[3]:(0.727529525757)\n",
      " state (11)  A[0]:(0.119275450706) A[1]:(0.8717212677) A[2]:(-0.899536907673) A[3]:(0.792009294033)\n",
      " state (12)  A[0]:(-0.434100627899) A[1]:(0.798289775848) A[2]:(-0.899795293808) A[3]:(0.713797569275)\n",
      " state (13)  A[0]:(0.000119984149933) A[1]:(0.809226632118) A[2]:(0.901243269444) A[3]:(0.729752779007)\n",
      " state (14)  A[0]:(0.811047911644) A[1]:(0.901225328445) A[2]:(0.999996185303) A[3]:(0.808615088463)\n",
      " state (15)  A[0]:(0.978614032269) A[1]:(0.942508339882) A[2]:(1.0) A[3]:(0.891346037388)\n",
      "Episode 249000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6290. Times reached goal: 892.               Steps done: 2115409. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.108525673089.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5895,  0.5905,  0.5324]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.0002,  0.6542,  0.5910]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5873,  0.7280,  0.6040,  0.6559]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0023,  0.8096, -0.0019,  0.6541]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7298,  0.9000, -0.0009,  0.7282]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9010,  1.0000,  0.8089]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533171772957) A[1]:(0.589277148247) A[2]:(0.591212451458) A[3]:(0.532609641552)\n",
      " state (1)  A[0]:(0.532829105854) A[1]:(0.000451594562037) A[2]:(0.654514193535) A[3]:(0.591107666492)\n",
      " state (2)  A[0]:(0.588022828102) A[1]:(0.727935791016) A[2]:(0.603890180588) A[3]:(0.655762374401)\n",
      " state (3)  A[0]:(0.663566827774) A[1]:(0.0439549125731) A[2]:(0.374838352203) A[3]:(0.578152537346)\n",
      " state (4)  A[0]:(0.586506307125) A[1]:(0.655985116959) A[2]:(0.00479122297838) A[3]:(0.53239107132)\n",
      " state (5)  A[0]:(-0.211485475302) A[1]:(0.999832630157) A[2]:(-0.460908591747) A[3]:(0.399168103933)\n",
      " state (6)  A[0]:(0.0020025644917) A[1]:(0.809669435024) A[2]:(-0.00305961607955) A[3]:(0.653796136379)\n",
      " state (7)  A[0]:(0.607746243477) A[1]:(-0.635213375092) A[2]:(0.321488171816) A[3]:(0.912531852722)\n",
      " state (8)  A[0]:(0.653573393822) A[1]:(-0.000218361616135) A[2]:(0.727982461452) A[3]:(0.591341257095)\n",
      " state (9)  A[0]:(0.656398832798) A[1]:(0.809887886047) A[2]:(0.809176921844) A[3]:(-0.0013213447528)\n",
      " state (10)  A[0]:(0.7287992239) A[1]:(0.89991736412) A[2]:(-0.0023130136542) A[3]:(0.72754740715)\n",
      " state (11)  A[0]:(0.116664454341) A[1]:(0.87147808075) A[2]:(-0.900241911411) A[3]:(0.791966855526)\n",
      " state (12)  A[0]:(-0.436358779669) A[1]:(0.798032522202) A[2]:(-0.900649905205) A[3]:(0.713603317738)\n",
      " state (13)  A[0]:(-0.002726130886) A[1]:(0.808886766434) A[2]:(0.900642752647) A[3]:(0.729779303074)\n",
      " state (14)  A[0]:(0.810229539871) A[1]:(0.901030778885) A[2]:(0.999996244907) A[3]:(0.808893918991)\n",
      " state (15)  A[0]:(0.978569030762) A[1]:(0.942433238029) A[2]:(1.0) A[3]:(0.891371667385)\n",
      "Episode 250000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6231. Times reached goal: 888.               Steps done: 2121640. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.107851552025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530382215977) A[1]:(0.589246630669) A[2]:(0.589701890945) A[3]:(0.530612587929)\n",
      " state (1)  A[0]:(0.530330181122) A[1]:(0.000936701602768) A[2]:(0.655102610588) A[3]:(0.589594125748)\n",
      " state (2)  A[0]:(0.586606800556) A[1]:(0.728582620621) A[2]:(0.602016806602) A[3]:(0.654931306839)\n",
      " state (3)  A[0]:(0.662168562412) A[1]:(0.0397867374122) A[2]:(0.373895734549) A[3]:(0.57486230135)\n",
      " state (4)  A[0]:(0.585772454739) A[1]:(0.655824661255) A[2]:(0.00169467763044) A[3]:(0.530715465546)\n",
      " state (5)  A[0]:(-0.204487204552) A[1]:(0.999835193157) A[2]:(-0.467041999102) A[3]:(0.412597119808)\n",
      " state (6)  A[0]:(0.000865056877956) A[1]:(0.809794425964) A[2]:(-0.00232266960666) A[3]:(0.653732657433)\n",
      " state (7)  A[0]:(0.604635596275) A[1]:(-0.63362544775) A[2]:(0.324747532606) A[3]:(0.911183416843)\n",
      " state (8)  A[0]:(0.653187334538) A[1]:(0.000359967321856) A[2]:(0.728912591934) A[3]:(0.590841472149)\n",
      " state (9)  A[0]:(0.656941175461) A[1]:(0.809979319572) A[2]:(0.809945523739) A[3]:(-0.000359564990504)\n",
      " state (10)  A[0]:(0.729787230492) A[1]:(0.89996856451) A[2]:(-8.48770141602e-05) A[3]:(0.72829836607)\n",
      " state (11)  A[0]:(0.119705602527) A[1]:(0.871583163738) A[2]:(-0.900022625923) A[3]:(0.79264497757)\n",
      " state (12)  A[0]:(-0.433687448502) A[1]:(0.79800593853) A[2]:(-0.900861382484) A[3]:(0.714298844337)\n",
      " state (13)  A[0]:(-0.000228613615036) A[1]:(0.808372616768) A[2]:(0.900220692158) A[3]:(0.730405569077)\n",
      " state (14)  A[0]:(0.810756981373) A[1]:(0.900577425957) A[2]:(0.999996244907) A[3]:(0.809311568737)\n",
      " state (15)  A[0]:(0.97863805294) A[1]:(0.942181885242) A[2]:(1.0) A[3]:(0.891298770905)\n",
      "Episode 251000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6227. Times reached goal: 878.               Steps done: 2127867. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.107182047078.\n",
      " state (0)  A[0]:(0.530651628971) A[1]:(0.591470122337) A[2]:(0.59086817503) A[3]:(0.531874120235)\n",
      " state (1)  A[0]:(0.53055524826) A[1]:(5.89638948441e-05) A[2]:(0.655961632729) A[3]:(0.59063833952)\n",
      " state (2)  A[0]:(0.587037563324) A[1]:(0.729189395905) A[2]:(0.603319525719) A[3]:(0.656171381474)\n",
      " state (3)  A[0]:(0.662522792816) A[1]:(0.0369888544083) A[2]:(0.378320455551) A[3]:(0.57475399971)\n",
      " state (4)  A[0]:(0.586756289005) A[1]:(0.656575679779) A[2]:(0.003864626633) A[3]:(0.532154202461)\n",
      " state (5)  A[0]:(-0.197019383311) A[1]:(0.999837994576) A[2]:(-0.472599178553) A[3]:(0.426948040724)\n",
      " state (6)  A[0]:(-0.000157549977303) A[1]:(0.810065507889) A[2]:(-0.00202738959342) A[3]:(0.655173957348)\n",
      " state (7)  A[0]:(0.602004647255) A[1]:(-0.632811129093) A[2]:(0.327211230993) A[3]:(0.910626232624)\n",
      " state (8)  A[0]:(0.653536319733) A[1]:(0.00039060410927) A[2]:(0.729142785072) A[3]:(0.592680215836)\n",
      " state (9)  A[0]:(0.657600283623) A[1]:(0.810306191444) A[2]:(0.810134410858) A[3]:(0.000684141996317)\n",
      " state (10)  A[0]:(0.730482935905) A[1]:(0.900074064732) A[2]:(0.000205159187317) A[3]:(0.729190826416)\n",
      " state (11)  A[0]:(0.121035322547) A[1]:(0.871710598469) A[2]:(-0.900169968605) A[3]:(0.793651938438)\n",
      " state (12)  A[0]:(-0.433239161968) A[1]:(0.798232853413) A[2]:(-0.901230156422) A[3]:(0.715446829796)\n",
      " state (13)  A[0]:(-0.000251039862633) A[1]:(0.808543860912) A[2]:(0.900177538395) A[3]:(0.73129940033)\n",
      " state (14)  A[0]:(0.810874581337) A[1]:(0.900712549686) A[2]:(0.999996304512) A[3]:(0.809795558453)\n",
      " state (15)  A[0]:(0.978714048862) A[1]:(0.942323565483) A[2]:(1.0) A[3]:(0.891184985638)\n",
      "Episode 252000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6276. Times reached goal: 897.               Steps done: 2134143. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.106511478994.\n",
      " state (0)  A[0]:(0.532351732254) A[1]:(0.591275691986) A[2]:(0.590167045593) A[3]:(0.532086253166)\n",
      " state (1)  A[0]:(0.531914412975) A[1]:(0.000272244215012) A[2]:(0.655885398388) A[3]:(0.590258359909)\n",
      " state (2)  A[0]:(0.587652981281) A[1]:(0.729015350342) A[2]:(0.60301399231) A[3]:(0.656146228313)\n",
      " state (3)  A[0]:(0.66290396452) A[1]:(0.0307219773531) A[2]:(0.3803807199) A[3]:(0.573258221149)\n",
      " state (4)  A[0]:(0.58772790432) A[1]:(0.656727552414) A[2]:(0.00389693188481) A[3]:(0.532416403294)\n",
      " state (5)  A[0]:(-0.187970593572) A[1]:(0.999840557575) A[2]:(-0.478234827518) A[3]:(0.439580827951)\n",
      " state (6)  A[0]:(0.00133196928073) A[1]:(0.810297131538) A[2]:(-0.00169801549055) A[3]:(0.655005931854)\n",
      " state (7)  A[0]:(0.600949645042) A[1]:(-0.631495416164) A[2]:(0.329997777939) A[3]:(0.909437060356)\n",
      " state (8)  A[0]:(0.654597401619) A[1]:(0.00191780691966) A[2]:(0.729753732681) A[3]:(0.592334866524)\n",
      " state (9)  A[0]:(0.658843159676) A[1]:(0.810973346233) A[2]:(0.810319840908) A[3]:(0.00133877911139)\n",
      " state (10)  A[0]:(0.731055319309) A[1]:(0.900321722031) A[2]:(8.78572463989e-05) A[3]:(0.729236721992)\n",
      " state (11)  A[0]:(0.121342651546) A[1]:(0.871986508369) A[2]:(-0.900305628777) A[3]:(0.793301045895)\n",
      " state (12)  A[0]:(-0.433167934418) A[1]:(0.798650920391) A[2]:(-0.901300787926) A[3]:(0.714590668678)\n",
      " state (13)  A[0]:(-0.000113919377327) A[1]:(0.808851540089) A[2]:(0.90071606636) A[3]:(0.730525135994)\n",
      " state (14)  A[0]:(0.81105697155) A[1]:(0.900868654251) A[2]:(0.999996423721) A[3]:(0.809350967407)\n",
      " state (15)  A[0]:(0.978785872459) A[1]:(0.942434370518) A[2]:(1.0) A[3]:(0.890691637993)\n",
      "Episode 253000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6280. Times reached goal: 898.               Steps done: 2140423. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.105844682837.\n",
      " state (0)  A[0]:(0.53078609705) A[1]:(0.589884519577) A[2]:(0.590482592583) A[3]:(0.531819581985)\n",
      " state (1)  A[0]:(0.531278133392) A[1]:(3.63737344742e-05) A[2]:(0.65608048439) A[3]:(0.590709269047)\n",
      " state (2)  A[0]:(0.587846577168) A[1]:(0.728554308414) A[2]:(0.602530419827) A[3]:(0.656026244164)\n",
      " state (3)  A[0]:(0.662157654762) A[1]:(0.0254339110106) A[2]:(0.381947398186) A[3]:(0.571112155914)\n",
      " state (4)  A[0]:(0.587192773819) A[1]:(0.655454158783) A[2]:(0.00371633260511) A[3]:(0.532300174236)\n",
      " state (5)  A[0]:(-0.181179240346) A[1]:(0.999842166901) A[2]:(-0.483503580093) A[3]:(0.453928053379)\n",
      " state (6)  A[0]:(0.00191466277465) A[1]:(0.809651732445) A[2]:(-0.0020866363775) A[3]:(0.655277609825)\n",
      " state (7)  A[0]:(0.599048435688) A[1]:(-0.629756033421) A[2]:(0.330484420061) A[3]:(0.907965421677)\n",
      " state (8)  A[0]:(0.65435063839) A[1]:(-3.42726707458e-07) A[2]:(0.728656172752) A[3]:(0.593075156212)\n",
      " state (9)  A[0]:(0.65788859129) A[1]:(0.809727787971) A[2]:(0.809957385063) A[3]:(0.00274395244196)\n",
      " state (10)  A[0]:(0.731247007847) A[1]:(0.899874269962) A[2]:(0.00101411307696) A[3]:(0.72985291481)\n",
      " state (11)  A[0]:(0.124460496008) A[1]:(0.871834158897) A[2]:(-0.900054037571) A[3]:(0.793938457966)\n",
      " state (12)  A[0]:(-0.430526077747) A[1]:(0.798564851284) A[2]:(-0.901468276978) A[3]:(0.714939594269)\n",
      " state (13)  A[0]:(0.00126118888147) A[1]:(0.808330833912) A[2]:(0.900214254856) A[3]:(0.730669379234)\n",
      " state (14)  A[0]:(0.810863256454) A[1]:(0.90033531189) A[2]:(0.999996423721) A[3]:(0.809577047825)\n",
      " state (15)  A[0]:(0.978741943836) A[1]:(0.942075014114) A[2]:(1.0) A[3]:(0.890657126904)\n",
      "Episode 254000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6254. Times reached goal: 895.               Steps done: 2146677. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.105184795809.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5901,  0.5894,  0.5307]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5901,  0.5895,  0.5308]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5876,  0.6564,  0.0017,  0.5313]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6545,  0.0009,  0.7301,  0.5921]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6576,  0.8099,  0.8108,  0.0010]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.8997, -0.0001,  0.7282]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9004,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530330896378) A[1]:(0.590028166771) A[2]:(0.590403437614) A[3]:(0.530512034893)\n",
      " state (1)  A[0]:(0.530345618725) A[1]:(-0.000224232673645) A[2]:(0.656831026077) A[3]:(0.589530110359)\n",
      " state (2)  A[0]:(0.58730494976) A[1]:(0.728912472725) A[2]:(0.602287650108) A[3]:(0.655678391457)\n",
      " state (3)  A[0]:(0.661383271217) A[1]:(0.0244767647237) A[2]:(0.383876502514) A[3]:(0.568885803223)\n",
      " state (4)  A[0]:(0.587070822716) A[1]:(0.656028091908) A[2]:(0.003805618966) A[3]:(0.5314245224)\n",
      " state (5)  A[0]:(-0.174910858274) A[1]:(0.999844372272) A[2]:(-0.488330841064) A[3]:(0.465207368135)\n",
      " state (6)  A[0]:(0.000807449046988) A[1]:(0.80968862772) A[2]:(-0.000402808160288) A[3]:(0.655121386051)\n",
      " state (7)  A[0]:(0.596107304096) A[1]:(-0.629251360893) A[2]:(0.333984524012) A[3]:(0.906711220741)\n",
      " state (8)  A[0]:(0.653121232986) A[1]:(-0.00159060815349) A[2]:(0.729094445705) A[3]:(0.591869950294)\n",
      " state (9)  A[0]:(0.655154466629) A[1]:(0.809205293655) A[2]:(0.809572219849) A[3]:(-0.00124546815641)\n",
      " state (10)  A[0]:(0.726987481117) A[1]:(0.899645507336) A[2]:(-0.00371096818708) A[3]:(0.727212309837)\n",
      " state (11)  A[0]:(0.113523222506) A[1]:(0.87176579237) A[2]:(-0.901461243629) A[3]:(0.791900932789)\n",
      " state (12)  A[0]:(-0.43747112155) A[1]:(0.798734307289) A[2]:(-0.90278595686) A[3]:(0.713454902172)\n",
      " state (13)  A[0]:(-0.0042386543937) A[1]:(0.808531284332) A[2]:(0.899405479431) A[3]:(0.730674564838)\n",
      " state (14)  A[0]:(0.809588074684) A[1]:(0.900533080101) A[2]:(0.999996423721) A[3]:(0.810213625431)\n",
      " state (15)  A[0]:(0.978652656078) A[1]:(0.942313730717) A[2]:(1.0) A[3]:(0.890906453133)\n",
      "Episode 255000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6329. Times reached goal: 893.               Steps done: 2153006. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.104521183452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530981183052) A[1]:(0.590333938599) A[2]:(0.58961558342) A[3]:(0.531454086304)\n",
      " state (1)  A[0]:(0.53094291687) A[1]:(-0.000337556004524) A[2]:(0.655599951744) A[3]:(0.589496970177)\n",
      " state (2)  A[0]:(0.587678074837) A[1]:(0.728787541389) A[2]:(0.599636793137) A[3]:(0.65504950285)\n",
      " state (3)  A[0]:(0.660926938057) A[1]:(0.0234647076577) A[2]:(0.382988721132) A[3]:(0.566864490509)\n",
      " state (4)  A[0]:(0.587098360062) A[1]:(0.655955910683) A[2]:(0.00174844090361) A[3]:(0.53081703186)\n",
      " state (5)  A[0]:(-0.169438436627) A[1]:(0.999846339226) A[2]:(-0.49428832531) A[3]:(0.473990947008)\n",
      " state (6)  A[0]:(0.000488027901156) A[1]:(0.810014784336) A[2]:(-0.00176834873855) A[3]:(0.653532028198)\n",
      " state (7)  A[0]:(0.594790101051) A[1]:(-0.627358794212) A[2]:(0.334824979305) A[3]:(0.905153155327)\n",
      " state (8)  A[0]:(0.653660655022) A[1]:(0.00117497087922) A[2]:(0.729282200336) A[3]:(0.589228510857)\n",
      " state (9)  A[0]:(0.657115399837) A[1]:(0.81031024456) A[2]:(0.810291945934) A[3]:(-0.00302385352552)\n",
      " state (10)  A[0]:(0.73060464859) A[1]:(0.900150716305) A[2]:(0.00124120654073) A[3]:(0.727724254131)\n",
      " state (11)  A[0]:(0.123471736908) A[1]:(0.872304677963) A[2]:(-0.900398552418) A[3]:(0.792706489563)\n",
      " state (12)  A[0]:(-0.430654704571) A[1]:(0.799327135086) A[2]:(-0.902299284935) A[3]:(0.71363645792)\n",
      " state (13)  A[0]:(0.00148014619481) A[1]:(0.808821558952) A[2]:(0.899548590183) A[3]:(0.729987621307)\n",
      " state (14)  A[0]:(0.811042428017) A[1]:(0.900714814663) A[2]:(0.999996483326) A[3]:(0.809299409389)\n",
      " state (15)  A[0]:(0.978822946548) A[1]:(0.942555308342) A[2]:(1.0) A[3]:(0.889915764332)\n",
      "Episode 256000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6233. Times reached goal: 880.               Steps done: 2159239. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.103871729043.\n",
      " state (0)  A[0]:(0.531912684441) A[1]:(0.591119945049) A[2]:(0.590244650841) A[3]:(0.530633687973)\n",
      " state (1)  A[0]:(0.531278371811) A[1]:(0.00115427328274) A[2]:(0.655652344227) A[3]:(0.590055704117)\n",
      " state (2)  A[0]:(0.588320374489) A[1]:(0.729467630386) A[2]:(0.599106788635) A[3]:(0.655578970909)\n",
      " state (3)  A[0]:(0.661044418812) A[1]:(0.023315584287) A[2]:(0.384462624788) A[3]:(0.566113471985)\n",
      " state (4)  A[0]:(0.587791383266) A[1]:(0.656982719898) A[2]:(0.00180816452485) A[3]:(0.531586289406)\n",
      " state (5)  A[0]:(-0.162949889898) A[1]:(0.999848604202) A[2]:(-0.498672217131) A[3]:(0.485192298889)\n",
      " state (6)  A[0]:(0.000382497877581) A[1]:(0.810387790203) A[2]:(-0.00169944600202) A[3]:(0.654984354973)\n",
      " state (7)  A[0]:(0.592955231667) A[1]:(-0.626412570477) A[2]:(0.336203962564) A[3]:(0.904706478119)\n",
      " state (8)  A[0]:(0.653732895851) A[1]:(0.000600188912358) A[2]:(0.728969693184) A[3]:(0.591715037823)\n",
      " state (9)  A[0]:(0.657220125198) A[1]:(0.810016989708) A[2]:(0.809924125671) A[3]:(0.00158923736308)\n",
      " state (10)  A[0]:(0.730499446392) A[1]:(0.899921357632) A[2]:(-8.85725021362e-05) A[3]:(0.729803085327)\n",
      " state (11)  A[0]:(0.122611634433) A[1]:(0.872026205063) A[2]:(-0.900763750076) A[3]:(0.794050693512)\n",
      " state (12)  A[0]:(-0.431972265244) A[1]:(0.798903822899) A[2]:(-0.902662277222) A[3]:(0.714815378189)\n",
      " state (13)  A[0]:(-0.000774100248236) A[1]:(0.808308303356) A[2]:(0.899757564068) A[3]:(0.730885624886)\n",
      " state (14)  A[0]:(0.810164690018) A[1]:(0.9004124403) A[2]:(0.999996542931) A[3]:(0.809931695461)\n",
      " state (15)  A[0]:(0.978733003139) A[1]:(0.942391335964) A[2]:(1.0) A[3]:(0.890013873577)\n",
      "Episode 257000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6256. Times reached goal: 897.               Steps done: 2165495. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.103223935916.\n",
      " state (0)  A[0]:(0.530393958092) A[1]:(0.59104681015) A[2]:(0.588938653469) A[3]:(0.531102061272)\n",
      " state (1)  A[0]:(0.530767798424) A[1]:(-0.00182558398228) A[2]:(0.655893564224) A[3]:(0.588859438896)\n",
      " state (2)  A[0]:(0.587872207165) A[1]:(0.728986203671) A[2]:(0.599519252777) A[3]:(0.65505695343)\n",
      " state (3)  A[0]:(0.660437583923) A[1]:(0.0216023530811) A[2]:(0.387469112873) A[3]:(0.564287185669)\n",
      " state (4)  A[0]:(0.588150262833) A[1]:(0.655718445778) A[2]:(0.00401602033526) A[3]:(0.531065702438)\n",
      " state (5)  A[0]:(-0.157000720501) A[1]:(0.999849557877) A[2]:(-0.501729607582) A[3]:(0.493596017361)\n",
      " state (6)  A[0]:(0.000502631010022) A[1]:(0.809830546379) A[2]:(0.000679850461893) A[3]:(0.653866648674)\n",
      " state (7)  A[0]:(0.591573417187) A[1]:(-0.625359177589) A[2]:(0.339534282684) A[3]:(0.903349041939)\n",
      " state (8)  A[0]:(0.654006958008) A[1]:(0.000693067791872) A[2]:(0.729622662067) A[3]:(0.590640127659)\n",
      " state (9)  A[0]:(0.65623486042) A[1]:(0.810327649117) A[2]:(0.810391426086) A[3]:(-0.00280212634243)\n",
      " state (10)  A[0]:(0.72890818119) A[1]:(0.900252580643) A[2]:(0.000434756249888) A[3]:(0.727283000946)\n",
      " state (11)  A[0]:(0.119381152093) A[1]:(0.872710704803) A[2]:(-0.900778472424) A[3]:(0.792254090309)\n",
      " state (12)  A[0]:(-0.432914495468) A[1]:(0.800205826759) A[2]:(-0.902424812317) A[3]:(0.713056325912)\n",
      " state (13)  A[0]:(0.000106185674667) A[1]:(0.809567272663) A[2]:(0.900826513767) A[3]:(0.729926109314)\n",
      " state (14)  A[0]:(0.810886502266) A[1]:(0.901102542877) A[2]:(0.99999666214) A[3]:(0.809563457966)\n",
      " state (15)  A[0]:(0.978869736195) A[1]:(0.942825734615) A[2]:(1.0) A[3]:(0.889629006386)\n",
      "Episode 258000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6290. Times reached goal: 897.               Steps done: 2171785. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.102576695065.\n",
      " state (0)  A[0]:(0.531697392464) A[1]:(0.590374469757) A[2]:(0.590447902679) A[3]:(0.531028151512)\n",
      " state (1)  A[0]:(0.531056702137) A[1]:(0.00073161709588) A[2]:(0.656801342964) A[3]:(0.59046292305)\n",
      " state (2)  A[0]:(0.588192939758) A[1]:(0.728365540504) A[2]:(0.599982857704) A[3]:(0.656377911568)\n",
      " state (3)  A[0]:(0.660186648369) A[1]:(0.0171841457486) A[2]:(0.389685600996) A[3]:(0.564236462116)\n",
      " state (4)  A[0]:(0.587971031666) A[1]:(0.655090153217) A[2]:(0.00392685318366) A[3]:(0.53246563673)\n",
      " state (5)  A[0]:(-0.151835471392) A[1]:(0.999850928783) A[2]:(-0.507126450539) A[3]:(0.505537986755)\n",
      " state (6)  A[0]:(0.000595897377934) A[1]:(0.809620261192) A[2]:(-0.00102722609881) A[3]:(0.655882000923)\n",
      " state (7)  A[0]:(0.590064108372) A[1]:(-0.625248789787) A[2]:(0.339448958635) A[3]:(0.902882277966)\n",
      " state (8)  A[0]:(0.653768420219) A[1]:(-0.000788539473433) A[2]:(0.728788852692) A[3]:(0.592279195786)\n",
      " state (9)  A[0]:(0.656343698502) A[1]:(0.809584319592) A[2]:(0.809399425983) A[3]:(0.00267382827587)\n",
      " state (10)  A[0]:(0.728800475597) A[1]:(0.899656295776) A[2]:(-0.00367484823801) A[3]:(0.729509592056)\n",
      " state (11)  A[0]:(0.118762649596) A[1]:(0.871755897999) A[2]:(-0.901939809322) A[3]:(0.793427824974)\n",
      " state (12)  A[0]:(-0.433469176292) A[1]:(0.798285722733) A[2]:(-0.904165267944) A[3]:(0.714088797569)\n",
      " state (13)  A[0]:(-0.00110256625339) A[1]:(0.807064056396) A[2]:(0.898559689522) A[3]:(0.730977773666)\n",
      " state (14)  A[0]:(0.810237288475) A[1]:(0.899563074112) A[2]:(0.999996602535) A[3]:(0.810462355614)\n",
      " state (15)  A[0]:(0.978796362877) A[1]:(0.941986262798) A[2]:(1.0) A[3]:(0.889908015728)\n",
      "Episode 259000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6298. Times reached goal: 900.               Steps done: 2178083. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.101932697118.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5905,  0.5902,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5888,  0.6549,  0.0031,  0.5317]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6548, -0.0007,  0.7288,  0.5920]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6573,  0.8105,  0.8101, -0.0008]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0011,  0.8091,  0.8997,  0.7292]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8109,  0.9007,  1.0000,  0.8091]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531593203545) A[1]:(0.590924203396) A[2]:(0.590502142906) A[3]:(0.532038927078)\n",
      " state (1)  A[0]:(0.531895399094) A[1]:(0.000682353856973) A[2]:(0.655974924564) A[3]:(0.590664863586)\n",
      " state (2)  A[0]:(0.588981032372) A[1]:(0.728748261929) A[2]:(0.598722577095) A[3]:(0.65618288517)\n",
      " state (3)  A[0]:(0.660457849503) A[1]:(0.0156876090914) A[2]:(0.390503555536) A[3]:(0.562688112259)\n",
      " state (4)  A[0]:(0.588673710823) A[1]:(0.655901074409) A[2]:(0.0033264036756) A[3]:(0.532254099846)\n",
      " state (5)  A[0]:(-0.145535841584) A[1]:(0.9998524189) A[2]:(-0.511940419674) A[3]:(0.514038681984)\n",
      " state (6)  A[0]:(0.00037686523865) A[1]:(0.809372901917) A[2]:(-0.0004448890395) A[3]:(0.655351400375)\n",
      " state (7)  A[0]:(0.588374137878) A[1]:(-0.624060750008) A[2]:(0.341523230076) A[3]:(0.901810348034)\n",
      " state (8)  A[0]:(0.654452860355) A[1]:(-0.000814676110167) A[2]:(0.728914499283) A[3]:(0.592400431633)\n",
      " state (9)  A[0]:(0.656758964062) A[1]:(0.809790372849) A[2]:(0.809929132462) A[3]:(-0.000306159257889)\n",
      " state (10)  A[0]:(0.729615330696) A[1]:(0.899826169014) A[2]:(-0.00080847722711) A[3]:(0.728287935257)\n",
      " state (11)  A[0]:(0.121459648013) A[1]:(0.872173905373) A[2]:(-0.901329934597) A[3]:(0.792905449867)\n",
      " state (12)  A[0]:(-0.431537210941) A[1]:(0.799257218838) A[2]:(-0.903522074223) A[3]:(0.713205218315)\n",
      " state (13)  A[0]:(0.000829845492262) A[1]:(0.808267116547) A[2]:(0.899899363518) A[3]:(0.729798913002)\n",
      " state (14)  A[0]:(0.810984909534) A[1]:(0.900402545929) A[2]:(0.999996721745) A[3]:(0.809335112572)\n",
      " state (15)  A[0]:(0.978924810886) A[1]:(0.942585289478) A[2]:(1.0) A[3]:(0.888796567917)\n",
      "Episode 260000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6268. Times reached goal: 909.               Steps done: 2184351. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.101295781153.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531405806541) A[1]:(0.590559303761) A[2]:(0.591791093349) A[3]:(0.531961143017)\n",
      " state (1)  A[0]:(0.531650066376) A[1]:(0.000704958918504) A[2]:(0.656346559525) A[3]:(0.59159809351)\n",
      " state (2)  A[0]:(0.589452981949) A[1]:(0.728928565979) A[2]:(0.598206996918) A[3]:(0.656694173813)\n",
      " state (3)  A[0]:(0.660528659821) A[1]:(0.0127408318222) A[2]:(0.391108363867) A[3]:(0.561656057835)\n",
      " state (4)  A[0]:(0.588923692703) A[1]:(0.656148016453) A[2]:(0.00154340139125) A[3]:(0.532066404819)\n",
      " state (5)  A[0]:(-0.140389159322) A[1]:(0.999854028225) A[2]:(-0.517796039581) A[3]:(0.521546840668)\n",
      " state (6)  A[0]:(0.00141544546932) A[1]:(0.809978067875) A[2]:(-0.00165414658841) A[3]:(0.655592024326)\n",
      " state (7)  A[0]:(0.587964117527) A[1]:(-0.622002840042) A[2]:(0.342384129763) A[3]:(0.901150047779)\n",
      " state (8)  A[0]:(0.655191302299) A[1]:(0.000413849920733) A[2]:(0.728577136993) A[3]:(0.593229651451)\n",
      " state (9)  A[0]:(0.65711915493) A[1]:(0.810033261776) A[2]:(0.809708833694) A[3]:(0.000632971466985)\n",
      " state (10)  A[0]:(0.73008286953) A[1]:(0.89993917942) A[2]:(-0.00109004927799) A[3]:(0.728880882263)\n",
      " state (11)  A[0]:(0.122704535723) A[1]:(0.872410416603) A[2]:(-0.901471138) A[3]:(0.793432593346)\n",
      " state (12)  A[0]:(-0.431101590395) A[1]:(0.799681067467) A[2]:(-0.903837144375) A[3]:(0.713514447212)\n",
      " state (13)  A[0]:(0.000404164165957) A[1]:(0.808558106422) A[2]:(0.899872601032) A[3]:(0.729853272438)\n",
      " state (14)  A[0]:(0.810694932938) A[1]:(0.900522828102) A[2]:(0.999996781349) A[3]:(0.809294223785)\n",
      " state (15)  A[0]:(0.978910565376) A[1]:(0.942664325237) A[2]:(1.0) A[3]:(0.888487160206)\n",
      "Episode 261000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6178. Times reached goal: 888.               Steps done: 2190529. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.100671904954.\n",
      " state (0)  A[0]:(0.530911624432) A[1]:(0.58974134922) A[2]:(0.591232717037) A[3]:(0.53097641468)\n",
      " state (1)  A[0]:(0.531309843063) A[1]:(-0.000209450721741) A[2]:(0.656601190567) A[3]:(0.590713024139)\n",
      " state (2)  A[0]:(0.589197278023) A[1]:(0.72894102335) A[2]:(0.59942483902) A[3]:(0.656176865101)\n",
      " state (3)  A[0]:(0.659875035286) A[1]:(0.0104706035927) A[2]:(0.395081728697) A[3]:(0.560497164726)\n",
      " state (4)  A[0]:(0.588044285774) A[1]:(0.656373739243) A[2]:(0.00399790052325) A[3]:(0.532041430473)\n",
      " state (5)  A[0]:(-0.137311935425) A[1]:(0.999855339527) A[2]:(-0.521076142788) A[3]:(0.528936326504)\n",
      " state (6)  A[0]:(0.000599861086812) A[1]:(0.809697389603) A[2]:(0.00016987323761) A[3]:(0.655557334423)\n",
      " state (7)  A[0]:(0.585588932037) A[1]:(-0.621484398842) A[2]:(0.345524400473) A[3]:(0.899983167648)\n",
      " state (8)  A[0]:(0.653909921646) A[1]:(-0.000932573981117) A[2]:(0.729474425316) A[3]:(0.591373443604)\n",
      " state (9)  A[0]:(0.655638754368) A[1]:(0.809509158134) A[2]:(0.809822618961) A[3]:(-0.000685900333337)\n",
      " state (10)  A[0]:(0.727403879166) A[1]:(0.899884164333) A[2]:(-0.00333987432532) A[3]:(0.727661371231)\n",
      " state (11)  A[0]:(0.115539856255) A[1]:(0.872684180737) A[2]:(-0.902287364006) A[3]:(0.792266905308)\n",
      " state (12)  A[0]:(-0.435663729906) A[1]:(0.800171911716) A[2]:(-0.904705762863) A[3]:(0.71272110939)\n",
      " state (13)  A[0]:(-0.00353392981924) A[1]:(0.80852329731) A[2]:(0.899209260941) A[3]:(0.730365037918)\n",
      " state (14)  A[0]:(0.809592664242) A[1]:(0.900170862675) A[2]:(0.999996781349) A[3]:(0.810490489006)\n",
      " state (15)  A[0]:(0.978830337524) A[1]:(0.942332029343) A[2]:(1.0) A[3]:(0.889334559441)\n",
      "Episode 262000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6283. Times reached goal: 888.               Steps done: 2196812. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.100041366287.\n",
      " state (0)  A[0]:(0.530848443508) A[1]:(0.591140568256) A[2]:(0.590348184109) A[3]:(0.530869007111)\n",
      " state (1)  A[0]:(0.530699789524) A[1]:(0.00017374753952) A[2]:(0.655968427658) A[3]:(0.590287685394)\n",
      " state (2)  A[0]:(0.588792443275) A[1]:(0.72930419445) A[2]:(0.597248196602) A[3]:(0.655964612961)\n",
      " state (3)  A[0]:(0.659667730331) A[1]:(0.00863024033606) A[2]:(0.393773823977) A[3]:(0.558936476707)\n",
      " state (4)  A[0]:(0.588570713997) A[1]:(0.656455755234) A[2]:(0.000585317553487) A[3]:(0.531481564045)\n",
      " state (5)  A[0]:(-0.132102742791) A[1]:(0.999856829643) A[2]:(-0.527261376381) A[3]:(0.535765349865)\n",
      " state (6)  A[0]:(0.000192105770111) A[1]:(0.810038328171) A[2]:(-0.00156307092402) A[3]:(0.655596852303)\n",
      " state (7)  A[0]:(0.584055542946) A[1]:(-0.619688391685) A[2]:(0.345854282379) A[3]:(0.899277031422)\n",
      " state (8)  A[0]:(0.654478669167) A[1]:(0.00071901071351) A[2]:(0.729189395905) A[3]:(0.592121481895)\n",
      " state (9)  A[0]:(0.657212018967) A[1]:(0.810159683228) A[2]:(0.81017768383) A[3]:(0.000771850172896)\n",
      " state (10)  A[0]:(0.730076789856) A[1]:(0.900112807751) A[2]:(0.000166893005371) A[3]:(0.729098796844)\n",
      " state (11)  A[0]:(0.12273838371) A[1]:(0.872870206833) A[2]:(-0.901449799538) A[3]:(0.793723583221)\n",
      " state (12)  A[0]:(-0.430939882994) A[1]:(0.800351560116) A[2]:(-0.904038965702) A[3]:(0.713850021362)\n",
      " state (13)  A[0]:(0.000202342867851) A[1]:(0.80868601799) A[2]:(0.900248944759) A[3]:(0.730523467064)\n",
      " state (14)  A[0]:(0.810456633568) A[1]:(0.90039730072) A[2]:(0.999996900558) A[3]:(0.810052752495)\n",
      " state (15)  A[0]:(0.978917300701) A[1]:(0.942592084408) A[2]:(1.0) A[3]:(0.888529777527)\n",
      "Episode 263000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6261. Times reached goal: 885.               Steps done: 2203073. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0994169640238.\n",
      " state (0)  A[0]:(0.532419919968) A[1]:(0.591035962105) A[2]:(0.591475009918) A[3]:(0.532266676426)\n",
      " state (1)  A[0]:(0.532104969025) A[1]:(0.000172898173332) A[2]:(0.657598018646) A[3]:(0.591133952141)\n",
      " state (2)  A[0]:(0.589967250824) A[1]:(0.729440808296) A[2]:(0.599411666393) A[3]:(0.656739294529)\n",
      " state (3)  A[0]:(0.660338759422) A[1]:(0.00814805738628) A[2]:(0.399017125368) A[3]:(0.559302330017)\n",
      " state (4)  A[0]:(0.589734196663) A[1]:(0.656132102013) A[2]:(0.00527102826163) A[3]:(0.533220410347)\n",
      " state (5)  A[0]:(-0.126848235726) A[1]:(0.999858260155) A[2]:(-0.529010653496) A[3]:(0.544215679169)\n",
      " state (6)  A[0]:(0.00142064597458) A[1]:(0.810489177704) A[2]:(-0.000276446342468) A[3]:(0.656686306)\n",
      " state (7)  A[0]:(0.583296597004) A[1]:(-0.61987555027) A[2]:(0.347673863173) A[3]:(0.898578882217)\n",
      " state (8)  A[0]:(0.653622031212) A[1]:(-0.000464871496661) A[2]:(0.729260325432) A[3]:(0.590306520462)\n",
      " state (9)  A[0]:(0.656293869019) A[1]:(0.809793353081) A[2]:(0.809427022934) A[3]:(0.000421196193201)\n",
      " state (10)  A[0]:(0.729036569595) A[1]:(0.899738132954) A[2]:(-0.00416467152536) A[3]:(0.729490220547)\n",
      " state (11)  A[0]:(0.120524227619) A[1]:(0.872352361679) A[2]:(-0.902541041374) A[3]:(0.794300675392)\n",
      " state (12)  A[0]:(-0.431489288807) A[1]:(0.799596190453) A[2]:(-0.905047476292) A[3]:(0.715095043182)\n",
      " state (13)  A[0]:(0.00149458530359) A[1]:(0.808017849922) A[2]:(0.899884939194) A[3]:(0.732260465622)\n",
      " state (14)  A[0]:(0.811471819878) A[1]:(0.900179445744) A[2]:(0.999996960163) A[3]:(0.81155449152)\n",
      " state (15)  A[0]:(0.979100048542) A[1]:(0.942588508129) A[2]:(1.0) A[3]:(0.889189720154)\n",
      "Episode 264000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6214. Times reached goal: 891.               Steps done: 2209287. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0988011024729.\n",
      "q_values \n",
      "tensor([[ 0.5326,  0.5907,  0.5913,  0.5322]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5329,  0.0019,  0.6563,  0.5914]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.7292,  0.5971,  0.6570]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0020,  0.8099, -0.0014,  0.6563]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7300,  0.8999, -0.0008,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8105,  0.9002,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533239722252) A[1]:(0.590969324112) A[2]:(0.590806603432) A[3]:(0.533081531525)\n",
      " state (1)  A[0]:(0.53334581852) A[1]:(0.00111058307812) A[2]:(0.655988752842) A[3]:(0.591695129871)\n",
      " state (2)  A[0]:(0.590755939484) A[1]:(0.729176163673) A[2]:(0.596868157387) A[3]:(0.657021045685)\n",
      " state (3)  A[0]:(0.660500407219) A[1]:(0.00598545698449) A[2]:(0.397474586964) A[3]:(0.558613359928)\n",
      " state (4)  A[0]:(0.589924573898) A[1]:(0.656340837479) A[2]:(0.00183677463792) A[3]:(0.533347606659)\n",
      " state (5)  A[0]:(-0.122786581516) A[1]:(0.999859452248) A[2]:(-0.535028100014) A[3]:(0.5501973629)\n",
      " state (6)  A[0]:(0.001536591677) A[1]:(0.81068944931) A[2]:(-0.00137150206137) A[3]:(0.655932962894)\n",
      " state (7)  A[0]:(0.582538545132) A[1]:(-0.618033111095) A[2]:(0.349047660828) A[3]:(0.897597372532)\n",
      " state (8)  A[0]:(0.65442943573) A[1]:(0.00138404872268) A[2]:(0.729688763618) A[3]:(0.590495586395)\n",
      " state (9)  A[0]:(0.65652358532) A[1]:(0.810685217381) A[2]:(0.810374081135) A[3]:(-0.00108906580135)\n",
      " state (10)  A[0]:(0.729148745537) A[1]:(0.900239646435) A[2]:(0.000115633010864) A[3]:(0.728317379951)\n",
      " state (11)  A[0]:(0.120418071747) A[1]:(0.872993409634) A[2]:(-0.901788890362) A[3]:(0.793097376823)\n",
      " state (12)  A[0]:(-0.432737141848) A[1]:(0.80044400692) A[2]:(-0.904694139957) A[3]:(0.712804019451)\n",
      " state (13)  A[0]:(-0.00144068791997) A[1]:(0.808543920517) A[2]:(0.900177001953) A[3]:(0.729723751545)\n",
      " state (14)  A[0]:(0.810293197632) A[1]:(0.900401830673) A[2]:(0.999997019768) A[3]:(0.809603452682)\n",
      " state (15)  A[0]:(0.978980839252) A[1]:(0.94275444746) A[2]:(1.0) A[3]:(0.887744069099)\n",
      "Episode 265000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6290. Times reached goal: 901.               Steps done: 2215577. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0981815939352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531219482422) A[1]:(0.590481817722) A[2]:(0.590024709702) A[3]:(0.53149574995)\n",
      " state (1)  A[0]:(0.531129479408) A[1]:(0.000601291598286) A[2]:(0.656137526035) A[3]:(0.590212225914)\n",
      " state (2)  A[0]:(0.588878571987) A[1]:(0.729033887386) A[2]:(0.596512556076) A[3]:(0.656029105186)\n",
      " state (3)  A[0]:(0.658952713013) A[1]:(0.00512929214165) A[2]:(0.39871430397) A[3]:(0.55631005764)\n",
      " state (4)  A[0]:(0.588947176933) A[1]:(0.656160593033) A[2]:(0.00238132034428) A[3]:(0.531578540802)\n",
      " state (5)  A[0]:(-0.120607458055) A[1]:(0.999860227108) A[2]:(-0.537611722946) A[3]:(0.553706586361)\n",
      " state (6)  A[0]:(0.000471800536616) A[1]:(0.810420572758) A[2]:(0.000120878219604) A[3]:(0.655240297318)\n",
      " state (7)  A[0]:(0.581444144249) A[1]:(-0.616359174252) A[2]:(0.350993722677) A[3]:(0.897014796734)\n",
      " state (8)  A[0]:(0.655455350876) A[1]:(0.00155687204096) A[2]:(0.729692161083) A[3]:(0.592938244343)\n",
      " state (9)  A[0]:(0.657072067261) A[1]:(0.810661256313) A[2]:(0.810656666756) A[3]:(0.000305622816086)\n",
      " state (10)  A[0]:(0.729726016521) A[1]:(0.900355994701) A[2]:(0.00187730567995) A[3]:(0.728292822838)\n",
      " state (11)  A[0]:(0.122587315738) A[1]:(0.873387336731) A[2]:(-0.901451706886) A[3]:(0.792949318886)\n",
      " state (12)  A[0]:(-0.430715084076) A[1]:(0.80125528574) A[2]:(-0.904451012611) A[3]:(0.712283968925)\n",
      " state (13)  A[0]:(0.000598043145146) A[1]:(0.809293985367) A[2]:(0.90074133873) A[3]:(0.729129731655)\n",
      " state (14)  A[0]:(0.810792326927) A[1]:(0.900815546513) A[2]:(0.999997079372) A[3]:(0.809170663357)\n",
      " state (15)  A[0]:(0.979044854641) A[1]:(0.943037450314) A[2]:(1.0) A[3]:(0.887250721455)\n",
      "Episode 266000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6181. Times reached goal: 894.               Steps done: 2221758. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0975766051471.\n",
      " state (0)  A[0]:(0.531466722488) A[1]:(0.590227365494) A[2]:(0.589914560318) A[3]:(0.531033933163)\n",
      " state (1)  A[0]:(0.531638264656) A[1]:(2.97427177429e-05) A[2]:(0.656039714813) A[3]:(0.589477777481)\n",
      " state (2)  A[0]:(0.589285135269) A[1]:(0.729067504406) A[2]:(0.597321987152) A[3]:(0.655683815479)\n",
      " state (3)  A[0]:(0.65889197588) A[1]:(0.00465169176459) A[2]:(0.401570081711) A[3]:(0.555491089821)\n",
      " state (4)  A[0]:(0.589004278183) A[1]:(0.656163930893) A[2]:(0.00387928914279) A[3]:(0.531816244125)\n",
      " state (5)  A[0]:(-0.117421545088) A[1]:(0.999861419201) A[2]:(-0.541235387325) A[3]:(0.559417009354)\n",
      " state (6)  A[0]:(0.000947922177147) A[1]:(0.810883939266) A[2]:(-0.000250458717346) A[3]:(0.65518951416)\n",
      " state (7)  A[0]:(0.580660521984) A[1]:(-0.614997386932) A[2]:(0.351543813944) A[3]:(0.896192669868)\n",
      " state (8)  A[0]:(0.654985904694) A[1]:(0.00165623275097) A[2]:(0.729219198227) A[3]:(0.59269297123)\n",
      " state (9)  A[0]:(0.655742585659) A[1]:(0.810699999332) A[2]:(0.810293614864) A[3]:(0.00019845366478)\n",
      " state (10)  A[0]:(0.728504478931) A[1]:(0.900484144688) A[2]:(0.000695466878824) A[3]:(0.728381276131)\n",
      " state (11)  A[0]:(0.120330162346) A[1]:(0.87379771471) A[2]:(-0.901772260666) A[3]:(0.793214380741)\n",
      " state (12)  A[0]:(-0.43245396018) A[1]:(0.802105724812) A[2]:(-0.904756963253) A[3]:(0.712666273117)\n",
      " state (13)  A[0]:(-0.00151266041212) A[1]:(0.810096740723) A[2]:(0.901042461395) A[3]:(0.729680776596)\n",
      " state (14)  A[0]:(0.810134828091) A[1]:(0.901210725307) A[2]:(0.999997138977) A[3]:(0.809777081013)\n",
      " state (15)  A[0]:(0.978997766972) A[1]:(0.943233728409) A[2]:(1.0) A[3]:(0.887508928776)\n",
      "Episode 267000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6273. Times reached goal: 900.               Steps done: 2228031. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0969664229404.\n",
      " state (0)  A[0]:(0.531952381134) A[1]:(0.590649485588) A[2]:(0.590565085411) A[3]:(0.531077027321)\n",
      " state (1)  A[0]:(0.531534433365) A[1]:(0.00158564618323) A[2]:(0.656130433083) A[3]:(0.590192735195)\n",
      " state (2)  A[0]:(0.589514315128) A[1]:(0.728957235813) A[2]:(0.596371889114) A[3]:(0.656024217606)\n",
      " state (3)  A[0]:(0.659022092819) A[1]:(0.00388255249709) A[2]:(0.402093172073) A[3]:(0.554850041866)\n",
      " state (4)  A[0]:(0.589444696903) A[1]:(0.655379414558) A[2]:(0.0036279996857) A[3]:(0.531214475632)\n",
      " state (5)  A[0]:(-0.115188166499) A[1]:(0.999861776829) A[2]:(-0.54433786869) A[3]:(0.562416434288)\n",
      " state (6)  A[0]:(-0.000213533639908) A[1]:(0.80999225378) A[2]:(9.25064086914e-05) A[3]:(0.654186666012)\n",
      " state (7)  A[0]:(0.579148411751) A[1]:(-0.615319550037) A[2]:(0.352815806866) A[3]:(0.895276129246)\n",
      " state (8)  A[0]:(0.65458637476) A[1]:(0.000983640202321) A[2]:(0.729592561722) A[3]:(0.590393185616)\n",
      " state (9)  A[0]:(0.656443417072) A[1]:(0.810515880585) A[2]:(0.810463666916) A[3]:(-0.00160705903545)\n",
      " state (10)  A[0]:(0.729496479034) A[1]:(0.900276839733) A[2]:(0.0011173482053) A[3]:(0.727894067764)\n",
      " state (11)  A[0]:(0.122490204871) A[1]:(0.87343364954) A[2]:(-0.90187472105) A[3]:(0.792718231678)\n",
      " state (12)  A[0]:(-0.431170523167) A[1]:(0.801287233829) A[2]:(-0.905192255974) A[3]:(0.711530804634)\n",
      " state (13)  A[0]:(-0.000780880276579) A[1]:(0.808932542801) A[2]:(0.900640010834) A[3]:(0.728311061859)\n",
      " state (14)  A[0]:(0.810188651085) A[1]:(0.900486290455) A[2]:(0.999997138977) A[3]:(0.808593928814)\n",
      " state (15)  A[0]:(0.979009628296) A[1]:(0.942838549614) A[2]:(1.0) A[3]:(0.886448383331)\n",
      "Episode 268000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6256. Times reached goal: 898.               Steps done: 2234287. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0963616945612.\n",
      " state (0)  A[0]:(0.531503677368) A[1]:(0.590100288391) A[2]:(0.590803563595) A[3]:(0.530397295952)\n",
      " state (1)  A[0]:(0.531691789627) A[1]:(0.000388830871088) A[2]:(0.656400978565) A[3]:(0.589771151543)\n",
      " state (2)  A[0]:(0.589744031429) A[1]:(0.729092001915) A[2]:(0.596303701401) A[3]:(0.65600413084)\n",
      " state (3)  A[0]:(0.658936917782) A[1]:(0.0051471432671) A[2]:(0.403456807137) A[3]:(0.554917275906)\n",
      " state (4)  A[0]:(0.589546978474) A[1]:(0.656679749489) A[2]:(0.00426850095391) A[3]:(0.532087802887)\n",
      " state (5)  A[0]:(-0.112050674856) A[1]:(0.999863028526) A[2]:(-0.54627007246) A[3]:(0.566780686378)\n",
      " state (6)  A[0]:(0.00179569225293) A[1]:(0.810162782669) A[2]:(0.00103855098132) A[3]:(0.654926896095)\n",
      " state (7)  A[0]:(0.580270409584) A[1]:(-0.614876508713) A[2]:(0.353833943605) A[3]:(0.895184755325)\n",
      " state (8)  A[0]:(0.656587123871) A[1]:(-0.000596314610448) A[2]:(0.729056358337) A[3]:(0.593420624733)\n",
      " state (9)  A[0]:(0.657642245293) A[1]:(0.809937000275) A[2]:(0.809927582741) A[3]:(0.0023184674792)\n",
      " state (10)  A[0]:(0.729982376099) A[1]:(0.899939358234) A[2]:(-0.0010455843294) A[3]:(0.729452252388)\n",
      " state (11)  A[0]:(0.123295001686) A[1]:(0.873078227043) A[2]:(-0.902530431747) A[3]:(0.794048964977)\n",
      " state (12)  A[0]:(-0.43015819788) A[1]:(0.80077123642) A[2]:(-0.906123578548) A[3]:(0.713507711887)\n",
      " state (13)  A[0]:(0.000889554386958) A[1]:(0.808340489864) A[2]:(0.899731993675) A[3]:(0.730516850948)\n",
      " state (14)  A[0]:(0.810884952545) A[1]:(0.900249302387) A[2]:(0.999997138977) A[3]:(0.810347795486)\n",
      " state (15)  A[0]:(0.979120016098) A[1]:(0.942831754684) A[2]:(1.0) A[3]:(0.887306451797)\n",
      "Episode 269000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6250. Times reached goal: 899.               Steps done: 2240537. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0957613121197.\n",
      "q_values \n",
      "tensor([[ 0.5310,  0.5902,  0.5901,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5889,  0.6558,  0.0006,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6547,  0.0000,  0.7289,  0.5912]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560,  0.8100,  0.8098, -0.0003]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0006,  0.8089,  0.8999,  0.7299]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9006,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531591057777) A[1]:(0.590178906918) A[2]:(0.589991509914) A[3]:(0.531239748001)\n",
      " state (1)  A[0]:(0.531521916389) A[1]:(-0.000104442238808) A[2]:(0.655751109123) A[3]:(0.590016126633)\n",
      " state (2)  A[0]:(0.58940911293) A[1]:(0.728698253632) A[2]:(0.594272077084) A[3]:(0.656016647816)\n",
      " state (3)  A[0]:(0.658473849297) A[1]:(0.00307986652479) A[2]:(0.401385307312) A[3]:(0.553943037987)\n",
      " state (4)  A[0]:(0.58927154541) A[1]:(0.655781745911) A[2]:(0.000514388026204) A[3]:(0.531499743462)\n",
      " state (5)  A[0]:(-0.110290892422) A[1]:(0.999863505363) A[2]:(-0.550912857056) A[3]:(0.570333003998)\n",
      " state (6)  A[0]:(0.000807255331893) A[1]:(0.809896588326) A[2]:(-0.00065410125535) A[3]:(0.655424952507)\n",
      " state (7)  A[0]:(0.578404903412) A[1]:(-0.614111542702) A[2]:(0.353752791882) A[3]:(0.894701004028)\n",
      " state (8)  A[0]:(0.654633522034) A[1]:(1.19060277939e-05) A[2]:(0.72910964489) A[3]:(0.590943932533)\n",
      " state (9)  A[0]:(0.656230270863) A[1]:(0.809968590736) A[2]:(0.810081958771) A[3]:(-0.000312805175781)\n",
      " state (10)  A[0]:(0.729468643665) A[1]:(0.900009393692) A[2]:(6.13927841187e-05) A[3]:(0.728833794594)\n",
      " state (11)  A[0]:(0.123098492622) A[1]:(0.873357653618) A[2]:(-0.902364134789) A[3]:(0.793805003166)\n",
      " state (12)  A[0]:(-0.4304895401) A[1]:(0.801316797733) A[2]:(-0.906134963036) A[3]:(0.713054299355)\n",
      " state (13)  A[0]:(-0.000253200531006) A[1]:(0.808804392815) A[2]:(0.900031864643) A[3]:(0.730003476143)\n",
      " state (14)  A[0]:(0.810265421867) A[1]:(0.900517582893) A[2]:(0.999997198582) A[3]:(0.809940814972)\n",
      " state (15)  A[0]:(0.979040563107) A[1]:(0.943024098873) A[2]:(1.0) A[3]:(0.886844336987)\n",
      "Episode 270000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6216. Times reached goal: 906.               Steps done: 2246753. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0951679060204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532477021217) A[1]:(0.591099381447) A[2]:(0.591215848923) A[3]:(0.531775593758)\n",
      " state (1)  A[0]:(0.532699227333) A[1]:(0.000270798802376) A[2]:(0.656355142593) A[3]:(0.590887963772)\n",
      " state (2)  A[0]:(0.590923666954) A[1]:(0.729601383209) A[2]:(0.59442615509) A[3]:(0.65661740303)\n",
      " state (3)  A[0]:(0.659584403038) A[1]:(0.00462834211066) A[2]:(0.402419418097) A[3]:(0.554154872894)\n",
      " state (4)  A[0]:(0.590637207031) A[1]:(0.656066060066) A[2]:(0.000392675370676) A[3]:(0.531999588013)\n",
      " state (5)  A[0]:(-0.107346661389) A[1]:(0.999864339828) A[2]:(-0.55381667614) A[3]:(0.57371878624)\n",
      " state (6)  A[0]:(0.00139461364597) A[1]:(0.809942364693) A[2]:(-0.00112271262333) A[3]:(0.655727148056)\n",
      " state (7)  A[0]:(0.578327655792) A[1]:(-0.613158941269) A[2]:(0.353986173868) A[3]:(0.894449234009)\n",
      " state (8)  A[0]:(0.655241250992) A[1]:(0.000689223292284) A[2]:(0.728796124458) A[3]:(0.591789543629)\n",
      " state (9)  A[0]:(0.656611263752) A[1]:(0.810254216194) A[2]:(0.809928059578) A[3]:(0.000599682272878)\n",
      " state (10)  A[0]:(0.729626119137) A[1]:(0.900104880333) A[2]:(-0.000343322753906) A[3]:(0.729016900063)\n",
      " state (11)  A[0]:(0.123380452394) A[1]:(0.873442411423) A[2]:(-0.902574419975) A[3]:(0.793840944767)\n",
      " state (12)  A[0]:(-0.43015474081) A[1]:(0.801314949989) A[2]:(-0.906494915485) A[3]:(0.712955713272)\n",
      " state (13)  A[0]:(0.000267058610916) A[1]:(0.808587908745) A[2]:(0.899986684322) A[3]:(0.729946434498)\n",
      " state (14)  A[0]:(0.810483217239) A[1]:(0.900375962257) A[2]:(0.999997258186) A[3]:(0.809920608997)\n",
      " state (15)  A[0]:(0.979081332684) A[1]:(0.942972958088) A[2]:(1.0) A[3]:(0.886630952358)\n",
      "Episode 271000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6246. Times reached goal: 913.               Steps done: 2252999. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0945753397902.\n",
      " state (0)  A[0]:(0.53129196167) A[1]:(0.590424001217) A[2]:(0.590007901192) A[3]:(0.531298398972)\n",
      " state (1)  A[0]:(0.531016588211) A[1]:(-4.82797622681e-06) A[2]:(0.655777812004) A[3]:(0.589735865593)\n",
      " state (2)  A[0]:(0.589024305344) A[1]:(0.728690624237) A[2]:(0.594532966614) A[3]:(0.655728816986)\n",
      " state (3)  A[0]:(0.657981395721) A[1]:(0.0025286024902) A[2]:(0.404117017984) A[3]:(0.553200483322)\n",
      " state (4)  A[0]:(0.589046239853) A[1]:(0.655897736549) A[2]:(0.00121247710194) A[3]:(0.531412959099)\n",
      " state (5)  A[0]:(-0.107478633523) A[1]:(0.999865174294) A[2]:(-0.556405544281) A[3]:(0.57516002655)\n",
      " state (6)  A[0]:(-0.00021630525589) A[1]:(0.810185551643) A[2]:(-0.00108706904575) A[3]:(0.655079364777)\n",
      " state (7)  A[0]:(0.576920747757) A[1]:(-0.613137841225) A[2]:(0.355362534523) A[3]:(0.893917262554)\n",
      " state (8)  A[0]:(0.654557824135) A[1]:(0.000613868178334) A[2]:(0.729414105415) A[3]:(0.590523838997)\n",
      " state (9)  A[0]:(0.656450748444) A[1]:(0.810381650925) A[2]:(0.810288131237) A[3]:(-0.000249624252319)\n",
      " state (10)  A[0]:(0.729581952095) A[1]:(0.900076508522) A[2]:(0.000489950121846) A[3]:(0.728843688965)\n",
      " state (11)  A[0]:(0.123219236732) A[1]:(0.873366236687) A[2]:(-0.902558088303) A[3]:(0.793720960617)\n",
      " state (12)  A[0]:(-0.430294632912) A[1]:(0.80114120245) A[2]:(-0.906603217125) A[3]:(0.712761282921)\n",
      " state (13)  A[0]:(0.000297039747238) A[1]:(0.80832862854) A[2]:(0.900307416916) A[3]:(0.729917168617)\n",
      " state (14)  A[0]:(0.810672402382) A[1]:(0.900264978409) A[2]:(0.999997317791) A[3]:(0.810043096542)\n",
      " state (15)  A[0]:(0.979140877724) A[1]:(0.94294321537) A[2]:(1.0) A[3]:(0.886584281921)\n",
      "Episode 272000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6202. Times reached goal: 888.               Steps done: 2259201. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0939905986893.\n",
      " state (0)  A[0]:(0.531937479973) A[1]:(0.590399026871) A[2]:(0.590168356895) A[3]:(0.532412588596)\n",
      " state (1)  A[0]:(0.531946778297) A[1]:(-7.82310962677e-05) A[2]:(0.655722022057) A[3]:(0.592140614986)\n",
      " state (2)  A[0]:(0.590107381344) A[1]:(0.729076147079) A[2]:(0.594710469246) A[3]:(0.658493638039)\n",
      " state (3)  A[0]:(0.658837795258) A[1]:(0.0030092804227) A[2]:(0.405545324087) A[3]:(0.556339323521)\n",
      " state (4)  A[0]:(0.590137243271) A[1]:(0.656231045723) A[2]:(0.00161647656932) A[3]:(0.534741282463)\n",
      " state (5)  A[0]:(-0.104426570237) A[1]:(0.999865889549) A[2]:(-0.558943510056) A[3]:(0.580059409142)\n",
      " state (6)  A[0]:(0.00116226030514) A[1]:(0.81006360054) A[2]:(-0.000638604105916) A[3]:(0.656889081001)\n",
      " state (7)  A[0]:(0.577210426331) A[1]:(-0.612052559853) A[2]:(0.356191962957) A[3]:(0.894142508507)\n",
      " state (8)  A[0]:(0.655058383942) A[1]:(0.00078126770677) A[2]:(0.729122281075) A[3]:(0.59253180027)\n",
      " state (9)  A[0]:(0.656279802322) A[1]:(0.810389399529) A[2]:(0.809976696968) A[3]:(0.00165074912366)\n",
      " state (10)  A[0]:(0.729156494141) A[1]:(0.900198221207) A[2]:(-0.000508785189595) A[3]:(0.729355812073)\n",
      " state (11)  A[0]:(0.122293524444) A[1]:(0.87379527092) A[2]:(-0.902859807014) A[3]:(0.794023931026)\n",
      " state (12)  A[0]:(-0.431170850992) A[1]:(0.802038788795) A[2]:(-0.906999766827) A[3]:(0.712900698185)\n",
      " state (13)  A[0]:(-0.00103220308665) A[1]:(0.809155344963) A[2]:(0.900290310383) A[3]:(0.729903101921)\n",
      " state (14)  A[0]:(0.810222029686) A[1]:(0.900661349297) A[2]:(0.999997377396) A[3]:(0.809947431087)\n",
      " state (15)  A[0]:(0.979117810726) A[1]:(0.943139672279) A[2]:(1.0) A[3]:(0.886304855347)\n",
      "Episode 273000 finished after 0 timesteps with r=1.0. Running score: 0.87. Times trained:               6245. Times reached goal: 892.               Steps done: 2265446. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0934054564089.\n",
      " state (0)  A[0]:(0.531856894493) A[1]:(0.590518116951) A[2]:(0.592036724091) A[3]:(0.532185077667)\n",
      " state (1)  A[0]:(0.532064914703) A[1]:(-0.000173807144165) A[2]:(0.657120347023) A[3]:(0.591010332108)\n",
      " state (2)  A[0]:(0.590157210827) A[1]:(0.728770017624) A[2]:(0.59620308876) A[3]:(0.656983673573)\n",
      " state (3)  A[0]:(0.658743619919) A[1]:(0.00178241543472) A[2]:(0.408472567797) A[3]:(0.554421663284)\n",
      " state (4)  A[0]:(0.589693009853) A[1]:(0.656525969505) A[2]:(0.00351236807182) A[3]:(0.532719969749)\n",
      " state (5)  A[0]:(-0.104574806988) A[1]:(0.9998665452) A[2]:(-0.560536503792) A[3]:(0.579530775547)\n",
      " state (6)  A[0]:(0.000126793980598) A[1]:(0.809715390205) A[2]:(0.000855803256854) A[3]:(0.65591365099)\n",
      " state (7)  A[0]:(0.576185107231) A[1]:(-0.612337231636) A[2]:(0.357992142439) A[3]:(0.893724381924)\n",
      " state (8)  A[0]:(0.654917776585) A[1]:(-0.00170230702497) A[2]:(0.729219079018) A[3]:(0.592572927475)\n",
      " state (9)  A[0]:(0.65538918972) A[1]:(0.809475719929) A[2]:(0.809763073921) A[3]:(-0.000611424387898)\n",
      " state (10)  A[0]:(0.727581262589) A[1]:(0.899754822254) A[2]:(-0.00257408060133) A[3]:(0.72790569067)\n",
      " state (11)  A[0]:(0.118534497917) A[1]:(0.873414337635) A[2]:(-0.903595685959) A[3]:(0.793180942535)\n",
      " state (12)  A[0]:(-0.433404058218) A[1]:(0.801526546478) A[2]:(-0.908018946648) A[3]:(0.712541103363)\n",
      " state (13)  A[0]:(-0.002850495046) A[1]:(0.808433830738) A[2]:(0.899167776108) A[3]:(0.730262875557)\n",
      " state (14)  A[0]:(0.809748053551) A[1]:(0.900208890438) A[2]:(0.999997377396) A[3]:(0.810455739498)\n",
      " state (15)  A[0]:(0.979092597961) A[1]:(0.942912101746) A[2]:(1.0) A[3]:(0.88648557663)\n",
      "Episode 274000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6202. Times reached goal: 902.               Steps done: 2271648. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0928279484715.\n",
      "q_values \n",
      "tensor([[ 0.5307,  0.5905,  0.5892,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5888,  0.6556,  0.0015,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6555,  0.0002,  0.7287,  0.5928]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.8104,  0.8099,  0.0002]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0012,  0.8091,  0.8994,  0.7291]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8107,  0.9005,  1.0000,  0.8093]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531628131866) A[1]:(0.590859770775) A[2]:(0.589529156685) A[3]:(0.530897676945)\n",
      " state (1)  A[0]:(0.531613171101) A[1]:(0.000610455812421) A[2]:(0.655699968338) A[3]:(0.589326858521)\n",
      " state (2)  A[0]:(0.589672267437) A[1]:(0.728881776333) A[2]:(0.595291852951) A[3]:(0.655643343925)\n",
      " state (3)  A[0]:(0.658419132233) A[1]:(0.00212393375114) A[2]:(0.408503234386) A[3]:(0.553160190582)\n",
      " state (4)  A[0]:(0.58987402916) A[1]:(0.656207442284) A[2]:(0.00224792584777) A[3]:(0.53113847971)\n",
      " state (5)  A[0]:(-0.102061182261) A[1]:(0.999867141247) A[2]:(-0.564281404018) A[3]:(0.577963232994)\n",
      " state (6)  A[0]:(0.00140438880771) A[1]:(0.809736669064) A[2]:(-0.000331521034241) A[3]:(0.653999686241)\n",
      " state (7)  A[0]:(0.576832354069) A[1]:(-0.611146450043) A[2]:(0.357849955559) A[3]:(0.893026351929)\n",
      " state (8)  A[0]:(0.655568063259) A[1]:(-7.69048929214e-05) A[2]:(0.729141950607) A[3]:(0.590777277946)\n",
      " state (9)  A[0]:(0.656971812248) A[1]:(0.809997975826) A[2]:(0.810100734234) A[3]:(-0.00130230118521)\n",
      " state (10)  A[0]:(0.730518996716) A[1]:(0.899912834167) A[2]:(0.000813007180113) A[3]:(0.728689432144)\n",
      " state (11)  A[0]:(0.126723513007) A[1]:(0.873557209969) A[2]:(-0.90279686451) A[3]:(0.794086694717)\n",
      " state (12)  A[0]:(-0.427827090025) A[1]:(0.801673054695) A[2]:(-0.907489836216) A[3]:(0.712837159634)\n",
      " state (13)  A[0]:(0.00195674342103) A[1]:(0.808526098728) A[2]:(0.899951279163) A[3]:(0.729569792747)\n",
      " state (14)  A[0]:(0.811082839966) A[1]:(0.900309979916) A[2]:(0.999997437) A[3]:(0.809463500977)\n",
      " state (15)  A[0]:(0.979250192642) A[1]:(0.943010628223) A[2]:(1.0) A[3]:(0.885513365269)\n",
      "Episode 275000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6234. Times reached goal: 897.               Steps done: 2277882. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0922510590733.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532264232635) A[1]:(0.590279698372) A[2]:(0.5900426507) A[3]:(0.53277361393)\n",
      " state (1)  A[0]:(0.53181540966) A[1]:(-8.14497470856e-05) A[2]:(0.656155109406) A[3]:(0.590648531914)\n",
      " state (2)  A[0]:(0.589464843273) A[1]:(0.728831589222) A[2]:(0.5961830616) A[3]:(0.657077848911)\n",
      " state (3)  A[0]:(0.658139467239) A[1]:(0.00192674761638) A[2]:(0.410553365946) A[3]:(0.555286109447)\n",
      " state (4)  A[0]:(0.589285373688) A[1]:(0.655787527561) A[2]:(0.00271581928246) A[3]:(0.532974362373)\n",
      " state (5)  A[0]:(-0.102921456099) A[1]:(0.999867856503) A[2]:(-0.567566037178) A[3]:(0.57991361618)\n",
      " state (6)  A[0]:(0.000358909339411) A[1]:(0.809993326664) A[2]:(-0.00179290573578) A[3]:(0.65523314476)\n",
      " state (7)  A[0]:(0.575357913971) A[1]:(-0.609649419785) A[2]:(0.356820881367) A[3]:(0.893057823181)\n",
      " state (8)  A[0]:(0.653859674931) A[1]:(0.00082907063188) A[2]:(0.728380441666) A[3]:(0.589923739433)\n",
      " state (9)  A[0]:(0.655594825745) A[1]:(0.810089051723) A[2]:(0.80930441618) A[3]:(-0.00113487197086)\n",
      " state (10)  A[0]:(0.729021728039) A[1]:(0.90016579628) A[2]:(-0.00255309976637) A[3]:(0.728804826736)\n",
      " state (11)  A[0]:(0.122805736959) A[1]:(0.874272108078) A[2]:(-0.903636455536) A[3]:(0.79403424263)\n",
      " state (12)  A[0]:(-0.431380838156) A[1]:(0.803052723408) A[2]:(-0.908366024494) A[3]:(0.712608575821)\n",
      " state (13)  A[0]:(-0.00282113999128) A[1]:(0.809723258018) A[2]:(0.899449646473) A[3]:(0.729447484016)\n",
      " state (14)  A[0]:(0.80931353569) A[1]:(0.900768756866) A[2]:(0.999997437) A[3]:(0.809510409832)\n",
      " state (15)  A[0]:(0.979061722755) A[1]:(0.943131446838) A[2]:(1.0) A[3]:(0.88549810648)\n",
      "Episode 276000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6246. Times reached goal: 911.               Steps done: 2284128. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0916766546906.\n",
      " state (0)  A[0]:(0.531123995781) A[1]:(0.590132236481) A[2]:(0.590681016445) A[3]:(0.529713511467)\n",
      " state (1)  A[0]:(0.531136512756) A[1]:(0.000756636145525) A[2]:(0.656005978584) A[3]:(0.589739561081)\n",
      " state (2)  A[0]:(0.58965408802) A[1]:(0.728579759598) A[2]:(0.595031023026) A[3]:(0.656207382679)\n",
      " state (3)  A[0]:(0.658324062824) A[1]:(0.00174511794467) A[2]:(0.410147100687) A[3]:(0.554269790649)\n",
      " state (4)  A[0]:(0.589287400246) A[1]:(0.655959367752) A[2]:(0.00140511896461) A[3]:(0.531277894974)\n",
      " state (5)  A[0]:(-0.102926850319) A[1]:(0.999868690968) A[2]:(-0.570071578026) A[3]:(0.578388094902)\n",
      " state (6)  A[0]:(0.000913277035579) A[1]:(0.810012698174) A[2]:(-0.00209986860864) A[3]:(0.654726564884)\n",
      " state (7)  A[0]:(0.576172113419) A[1]:(-0.609061837196) A[2]:(0.356987327337) A[3]:(0.893139243126)\n",
      " state (8)  A[0]:(0.65505874157) A[1]:(0.000721111777239) A[2]:(0.728091180325) A[3]:(0.592418015003)\n",
      " state (9)  A[0]:(0.655768632889) A[1]:(0.81023222208) A[2]:(0.809469401836) A[3]:(0.000758826558013)\n",
      " state (10)  A[0]:(0.728973865509) A[1]:(0.900052309036) A[2]:(-0.00140321161598) A[3]:(0.729070782661)\n",
      " state (11)  A[0]:(0.123283922672) A[1]:(0.873864173889) A[2]:(-0.903468966484) A[3]:(0.794090986252)\n",
      " state (12)  A[0]:(-0.430147409439) A[1]:(0.802035033703) A[2]:(-0.908324956894) A[3]:(0.712622046471)\n",
      " state (13)  A[0]:(-0.000394403905375) A[1]:(0.808422684669) A[2]:(0.899720489979) A[3]:(0.729610323906)\n",
      " state (14)  A[0]:(0.810323357582) A[1]:(0.900059640408) A[2]:(0.999997496605) A[3]:(0.80973559618)\n",
      " state (15)  A[0]:(0.979189753532) A[1]:(0.942781805992) A[2]:(1.0) A[3]:(0.885522425175)\n",
      "Episode 277000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6175. Times reached goal: 900.               Steps done: 2290303. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0911122955999.\n",
      " state (0)  A[0]:(0.531622171402) A[1]:(0.591238021851) A[2]:(0.590808153152) A[3]:(0.531373500824)\n",
      " state (1)  A[0]:(0.531427145004) A[1]:(-0.000972002453636) A[2]:(0.656530380249) A[3]:(0.590827465057)\n",
      " state (2)  A[0]:(0.590033054352) A[1]:(0.72901558876) A[2]:(0.595652878284) A[3]:(0.657211661339)\n",
      " state (3)  A[0]:(0.658893346786) A[1]:(0.0017726700753) A[2]:(0.411718785763) A[3]:(0.555642366409)\n",
      " state (4)  A[0]:(0.58984464407) A[1]:(0.65638422966) A[2]:(0.00118780077901) A[3]:(0.532206773758)\n",
      " state (5)  A[0]:(-0.102030932903) A[1]:(0.999869346619) A[2]:(-0.57331609726) A[3]:(0.57915109396)\n",
      " state (6)  A[0]:(0.0004149377055) A[1]:(0.809899568558) A[2]:(-0.00176763348281) A[3]:(0.655296802521)\n",
      " state (7)  A[0]:(0.575042724609) A[1]:(-0.609167933464) A[2]:(0.358671545982) A[3]:(0.893034815788)\n",
      " state (8)  A[0]:(0.654206037521) A[1]:(6.94394111633e-06) A[2]:(0.728887081146) A[3]:(0.590017676353)\n",
      " state (9)  A[0]:(0.656575262547) A[1]:(0.809901535511) A[2]:(0.809830307961) A[3]:(-0.00145506754052)\n",
      " state (10)  A[0]:(0.730526685715) A[1]:(0.899932444096) A[2]:(-0.000173091888428) A[3]:(0.729074239731)\n",
      " state (11)  A[0]:(0.127039209008) A[1]:(0.873989701271) A[2]:(-0.9033613801) A[3]:(0.794579863548)\n",
      " state (12)  A[0]:(-0.428140968084) A[1]:(0.802515983582) A[2]:(-0.908533394337) A[3]:(0.713131189346)\n",
      " state (13)  A[0]:(0.000551670731511) A[1]:(0.808959066868) A[2]:(0.899607658386) A[3]:(0.729809701443)\n",
      " state (14)  A[0]:(0.810443282127) A[1]:(0.900379955769) A[2]:(0.999997496605) A[3]:(0.809720635414)\n",
      " state (15)  A[0]:(0.979224443436) A[1]:(0.942968010902) A[2]:(1.0) A[3]:(0.885303199291)\n",
      "Episode 278000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6228. Times reached goal: 908.               Steps done: 2296531. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0905466115914.\n",
      " state (0)  A[0]:(0.532013475895) A[1]:(0.589967012405) A[2]:(0.591035485268) A[3]:(0.532971978188)\n",
      " state (1)  A[0]:(0.53264939785) A[1]:(2.39312648773e-05) A[2]:(0.655873775482) A[3]:(0.59194034338)\n",
      " state (2)  A[0]:(0.590440750122) A[1]:(0.728504836559) A[2]:(0.595076799393) A[3]:(0.657784461975)\n",
      " state (3)  A[0]:(0.659011244774) A[1]:(0.000989287742414) A[2]:(0.412372767925) A[3]:(0.556781291962)\n",
      " state (4)  A[0]:(0.589740157127) A[1]:(0.655855774879) A[2]:(0.000722885015421) A[3]:(0.53339278698)\n",
      " state (5)  A[0]:(-0.101649045944) A[1]:(0.999869823456) A[2]:(-0.576327681541) A[3]:(0.581063449383)\n",
      " state (6)  A[0]:(0.000811606470961) A[1]:(0.809566259384) A[2]:(-0.00172948662657) A[3]:(0.655598342419)\n",
      " state (7)  A[0]:(0.575143992901) A[1]:(-0.607844233513) A[2]:(0.358970463276) A[3]:(0.892872333527)\n",
      " state (8)  A[0]:(0.654369711876) A[1]:(3.13222408295e-05) A[2]:(0.728438854218) A[3]:(0.591328680515)\n",
      " state (9)  A[0]:(0.655632257462) A[1]:(0.809750437737) A[2]:(0.80959713459) A[3]:(-0.000432431668742)\n",
      " state (10)  A[0]:(0.729146003723) A[1]:(0.899915516376) A[2]:(-0.000798225228209) A[3]:(0.728800535202)\n",
      " state (11)  A[0]:(0.124153055251) A[1]:(0.874154686928) A[2]:(-0.903506994247) A[3]:(0.794143676758)\n",
      " state (12)  A[0]:(-0.429835110903) A[1]:(0.802888929844) A[2]:(-0.908602297306) A[3]:(0.712480306625)\n",
      " state (13)  A[0]:(-0.000927194661926) A[1]:(0.809221386909) A[2]:(0.900015234947) A[3]:(0.729307889938)\n",
      " state (14)  A[0]:(0.809955179691) A[1]:(0.900460422039) A[2]:(0.99999755621) A[3]:(0.809423089027)\n",
      " state (15)  A[0]:(0.979173779488) A[1]:(0.942978799343) A[2]:(1.0) A[3]:(0.885006189346)\n",
      "Episode 279000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6278. Times reached goal: 912.               Steps done: 2302809. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0899799406052.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5906,  0.5902,  0.5310]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5900,  0.6559,  0.0043,  0.5324]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6553, -0.0020,  0.7291,  0.5920]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6580,  0.8104,  0.8106,  0.0014]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7309,  0.9003,  0.0007,  0.7303]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8110,  0.9008,  1.0000,  0.8111]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530806899071) A[1]:(0.590495824814) A[2]:(0.590216159821) A[3]:(0.531475901604)\n",
      " state (1)  A[0]:(0.530860722065) A[1]:(-0.00158087781165) A[2]:(0.657346010208) A[3]:(0.589971542358)\n",
      " state (2)  A[0]:(0.58885705471) A[1]:(0.728727459908) A[2]:(0.598214387894) A[3]:(0.657072722912)\n",
      " state (3)  A[0]:(0.658373236656) A[1]:(0.00120814086404) A[2]:(0.417975604534) A[3]:(0.556769013405)\n",
      " state (4)  A[0]:(0.589718103409) A[1]:(0.655227959156) A[2]:(0.00541550572962) A[3]:(0.533302545547)\n",
      " state (5)  A[0]:(-0.100697018206) A[1]:(0.999870359898) A[2]:(-0.577728569508) A[3]:(0.581067442894)\n",
      " state (6)  A[0]:(0.00107279373333) A[1]:(0.809539616108) A[2]:(0.000728964689188) A[3]:(0.656291306019)\n",
      " state (7)  A[0]:(0.575933933258) A[1]:(-0.608204126358) A[2]:(0.361490935087) A[3]:(0.893651425838)\n",
      " state (8)  A[0]:(0.657179832458) A[1]:(-0.00166742352303) A[2]:(0.728321254253) A[3]:(0.597864151001)\n",
      " state (9)  A[0]:(0.657091081142) A[1]:(0.80992013216) A[2]:(0.809164464474) A[3]:(0.00528938602656)\n",
      " state (10)  A[0]:(0.728639602661) A[1]:(0.899981915951) A[2]:(-0.00430068699643) A[3]:(0.730278432369)\n",
      " state (11)  A[0]:(0.120832487941) A[1]:(0.874235630035) A[2]:(-0.904473125935) A[3]:(0.795171499252)\n",
      " state (12)  A[0]:(-0.432149916887) A[1]:(0.803155839443) A[2]:(-0.909318447113) A[3]:(0.714252591133)\n",
      " state (13)  A[0]:(-0.00190530484542) A[1]:(0.80966168642) A[2]:(0.900191366673) A[3]:(0.731402933598)\n",
      " state (14)  A[0]:(0.810305356979) A[1]:(0.900806128979) A[2]:(0.999997615814) A[3]:(0.811031460762)\n",
      " state (15)  A[0]:(0.979295313358) A[1]:(0.943188428879) A[2]:(1.0) A[3]:(0.885819792747)\n",
      "Episode 280000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6201. Times reached goal: 903.               Steps done: 2309010. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0894237013956.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530717849731) A[1]:(0.590174674988) A[2]:(0.590598464012) A[3]:(0.530569195747)\n",
      " state (1)  A[0]:(0.530584454536) A[1]:(-0.00207003648393) A[2]:(0.656377732754) A[3]:(0.590510368347)\n",
      " state (2)  A[0]:(0.588970303535) A[1]:(0.72890651226) A[2]:(0.598432838917) A[3]:(0.657543301582)\n",
      " state (3)  A[0]:(0.658219456673) A[1]:(0.00220194109716) A[2]:(0.419968217611) A[3]:(0.557593941689)\n",
      " state (4)  A[0]:(0.589284181595) A[1]:(0.655821502209) A[2]:(0.0059273741208) A[3]:(0.533644318581)\n",
      " state (5)  A[0]:(-0.100094474852) A[1]:(0.999871253967) A[2]:(-0.5816822052) A[3]:(0.58171248436)\n",
      " state (6)  A[0]:(0.00161370495334) A[1]:(0.809817612171) A[2]:(-0.000871300464496) A[3]:(0.655973255634)\n",
      " state (7)  A[0]:(0.575562596321) A[1]:(-0.606880784035) A[2]:(0.361100286245) A[3]:(0.892981231213)\n",
      " state (8)  A[0]:(0.655919253826) A[1]:(-0.000101894140244) A[2]:(0.728442311287) A[3]:(0.594190001488)\n",
      " state (9)  A[0]:(0.656932711601) A[1]:(0.810299634933) A[2]:(0.809472978115) A[3]:(0.00323616317473)\n",
      " state (10)  A[0]:(0.729821622372) A[1]:(0.900124311447) A[2]:(-0.00195896369405) A[3]:(0.730319440365)\n",
      " state (11)  A[0]:(0.125102952123) A[1]:(0.874428153038) A[2]:(-0.903980970383) A[3]:(0.795481920242)\n",
      " state (12)  A[0]:(-0.429045498371) A[1]:(0.803278326988) A[2]:(-0.909134089947) A[3]:(0.71428912878)\n",
      " state (13)  A[0]:(0.000655934098177) A[1]:(0.809417307377) A[2]:(0.900334060192) A[3]:(0.731184840202)\n",
      " state (14)  A[0]:(0.810903549194) A[1]:(0.900505244732) A[2]:(0.999997615814) A[3]:(0.810909986496)\n",
      " state (15)  A[0]:(0.979363918304) A[1]:(0.942947268486) A[2]:(1.0) A[3]:(0.885689258575)\n",
      "Episode 281000 finished after 0 timesteps with r=1.0. Running score: 0.86. Times trained:               6230. Times reached goal: 899.               Steps done: 2315240. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0888683235343.\n",
      " state (0)  A[0]:(0.531217753887) A[1]:(0.590423941612) A[2]:(0.591146469116) A[3]:(0.531034946442)\n",
      " state (1)  A[0]:(0.531651496887) A[1]:(0.000352367729647) A[2]:(0.656376481056) A[3]:(0.59066927433)\n",
      " state (2)  A[0]:(0.589838862419) A[1]:(0.728976368904) A[2]:(0.598088741302) A[3]:(0.657086730003)\n",
      " state (3)  A[0]:(0.658956289291) A[1]:(0.00127886165865) A[2]:(0.420737087727) A[3]:(0.557025074959)\n",
      " state (4)  A[0]:(0.58963906765) A[1]:(0.656159877777) A[2]:(0.00471350038424) A[3]:(0.53243970871)\n",
      " state (5)  A[0]:(-0.0996842235327) A[1]:(0.999872028828) A[2]:(-0.58568918705) A[3]:(0.580954790115)\n",
      " state (6)  A[0]:(0.00107590807602) A[1]:(0.809836030006) A[2]:(-1.21593475342e-05) A[3]:(0.654964148998)\n",
      " state (7)  A[0]:(0.574307203293) A[1]:(-0.604694843292) A[2]:(0.363373041153) A[3]:(0.89219391346)\n",
      " state (8)  A[0]:(0.65467685461) A[1]:(0.000699505093507) A[2]:(0.729656934738) A[3]:(0.591035842896)\n",
      " state (9)  A[0]:(0.6564925313) A[1]:(0.809833467007) A[2]:(0.810396552086) A[3]:(8.76188278198e-05)\n",
      " state (10)  A[0]:(0.729878902435) A[1]:(0.899942755699) A[2]:(0.000918149715289) A[3]:(0.729005217552)\n",
      " state (11)  A[0]:(0.126095220447) A[1]:(0.874446749687) A[2]:(-0.903612196445) A[3]:(0.794497072697)\n",
      " state (12)  A[0]:(-0.42813155055) A[1]:(0.803298056126) A[2]:(-0.909250199795) A[3]:(0.712847590446)\n",
      " state (13)  A[0]:(0.000723615172319) A[1]:(0.8089889884) A[2]:(0.899728059769) A[3]:(0.729765951633)\n",
      " state (14)  A[0]:(0.810386002064) A[1]:(0.900056242943) A[2]:(0.999997615814) A[3]:(0.809813261032)\n",
      " state (15)  A[0]:(0.979272544384) A[1]:(0.942649543285) A[2]:(1.0) A[3]:(0.884821474552)\n",
      "Episode 282000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6263. Times reached goal: 920.               Steps done: 2321503. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0883134805285.\n",
      " state (0)  A[0]:(0.532152295113) A[1]:(0.590649485588) A[2]:(0.59030008316) A[3]:(0.530786454678)\n",
      " state (1)  A[0]:(0.532042741776) A[1]:(0.00112000061199) A[2]:(0.656140565872) A[3]:(0.589798569679)\n",
      " state (2)  A[0]:(0.589901566505) A[1]:(0.729196190834) A[2]:(0.597165226936) A[3]:(0.65601503849)\n",
      " state (3)  A[0]:(0.659107327461) A[1]:(0.000640302780084) A[2]:(0.420347929001) A[3]:(0.556166648865)\n",
      " state (4)  A[0]:(0.589727282524) A[1]:(0.656299233437) A[2]:(0.00163543072995) A[3]:(0.531550526619)\n",
      " state (5)  A[0]:(-0.0984796136618) A[1]:(0.999872863293) A[2]:(-0.591298103333) A[3]:(0.581030726433)\n",
      " state (6)  A[0]:(0.00165368465241) A[1]:(0.810127496719) A[2]:(-0.00220834859647) A[3]:(0.654841184616)\n",
      " state (7)  A[0]:(0.574282646179) A[1]:(-0.604304909706) A[2]:(0.362945973873) A[3]:(0.892018437386)\n",
      " state (8)  A[0]:(0.655164301395) A[1]:(-0.000109285116196) A[2]:(0.728994786739) A[3]:(0.590818405151)\n",
      " state (9)  A[0]:(0.65729290247) A[1]:(0.809647977352) A[2]:(0.809963822365) A[3]:(-0.000692903879099)\n",
      " state (10)  A[0]:(0.730717182159) A[1]:(0.89979404211) A[2]:(-0.000142335891724) A[3]:(0.729128837585)\n",
      " state (11)  A[0]:(0.127586901188) A[1]:(0.874330639839) A[2]:(-0.903882205486) A[3]:(0.794935941696)\n",
      " state (12)  A[0]:(-0.42778173089) A[1]:(0.803245067596) A[2]:(-0.909569978714) A[3]:(0.71326059103)\n",
      " state (13)  A[0]:(0.000560536922421) A[1]:(0.809044539928) A[2]:(0.899852395058) A[3]:(0.729858517647)\n",
      " state (14)  A[0]:(0.810408651829) A[1]:(0.900166034698) A[2]:(0.999997675419) A[3]:(0.80967092514)\n",
      " state (15)  A[0]:(0.979305326939) A[1]:(0.942721009254) A[2]:(1.0) A[3]:(0.884488999844)\n",
      "Episode 283000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6196. Times reached goal: 911.               Steps done: 2327699. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.087767981903.\n",
      " state (0)  A[0]:(0.531048715115) A[1]:(0.590398430824) A[2]:(0.589493751526) A[3]:(0.530434608459)\n",
      " state (1)  A[0]:(0.531269669533) A[1]:(-0.00029356777668) A[2]:(0.655729055405) A[3]:(0.59002584219)\n",
      " state (2)  A[0]:(0.589623332024) A[1]:(0.728847980499) A[2]:(0.597158670425) A[3]:(0.656708598137)\n",
      " state (3)  A[0]:(0.658924341202) A[1]:(0.000188514590263) A[2]:(0.42209854722) A[3]:(0.557194828987)\n",
      " state (4)  A[0]:(0.589456558228) A[1]:(0.655877768993) A[2]:(0.00210082228296) A[3]:(0.532373905182)\n",
      " state (5)  A[0]:(-0.0983943492174) A[1]:(0.999873280525) A[2]:(-0.594686388969) A[3]:(0.582742094994)\n",
      " state (6)  A[0]:(0.00091192102991) A[1]:(0.809747338295) A[2]:(-0.0019222474657) A[3]:(0.654839515686)\n",
      " state (7)  A[0]:(0.573427438736) A[1]:(-0.603241801262) A[2]:(0.363698720932) A[3]:(0.891700923443)\n",
      " state (8)  A[0]:(0.655030012131) A[1]:(0.000488921941724) A[2]:(0.728651940823) A[3]:(0.591926932335)\n",
      " state (9)  A[0]:(0.656283259392) A[1]:(0.810230195522) A[2]:(0.809756398201) A[3]:(-0.000471830338938)\n",
      " state (10)  A[0]:(0.729126691818) A[1]:(0.900143623352) A[2]:(-0.00117742957082) A[3]:(0.72835123539)\n",
      " state (11)  A[0]:(0.124023750424) A[1]:(0.874775528908) A[2]:(-0.904191255569) A[3]:(0.794147133827)\n",
      " state (12)  A[0]:(-0.429666757584) A[1]:(0.803780138493) A[2]:(-0.90987175703) A[3]:(0.712402164936)\n",
      " state (13)  A[0]:(-0.000295013189316) A[1]:(0.809278726578) A[2]:(0.899811506271) A[3]:(0.729479134083)\n",
      " state (14)  A[0]:(0.810442328453) A[1]:(0.900188148022) A[2]:(0.999997675419) A[3]:(0.809676706791)\n",
      " state (15)  A[0]:(0.979343056679) A[1]:(0.942703723907) A[2]:(1.0) A[3]:(0.884480178356)\n",
      "Episode 284000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6232. Times reached goal: 911.               Steps done: 2333931. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0872227126635.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5906,  0.5900,  0.5307]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5900,  0.6562,  0.0045,  0.5324]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.0033,  0.7297,  0.5929]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6582,  0.8111,  0.8109,  0.0023]], device='cuda:0')\n",
      "On state=9, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556,  0.0001,  0.7292,  0.5930]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6551,  0.8095,  0.8093, -0.0003]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0034,  0.8095,  0.8990,  0.7305]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8095,  0.9008,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531666278839) A[1]:(0.590916156769) A[2]:(0.590754508972) A[3]:(0.531182050705)\n",
      " state (1)  A[0]:(0.531641960144) A[1]:(-0.000434219808085) A[2]:(0.656679749489) A[3]:(0.590476214886)\n",
      " state (2)  A[0]:(0.589736819267) A[1]:(0.729735255241) A[2]:(0.599024534225) A[3]:(0.657066404819)\n",
      " state (3)  A[0]:(0.658895015717) A[1]:(0.00278227799572) A[2]:(0.426602214575) A[3]:(0.558111071587)\n",
      " state (4)  A[0]:(0.589368283749) A[1]:(0.657058358192) A[2]:(0.00609664525837) A[3]:(0.533367395401)\n",
      " state (5)  A[0]:(-0.0976116284728) A[1]:(0.999874711037) A[2]:(-0.596320927143) A[3]:(0.58468401432)\n",
      " state (6)  A[0]:(0.000365287036402) A[1]:(0.810961782932) A[2]:(0.00039434430073) A[3]:(0.654874444008)\n",
      " state (7)  A[0]:(0.571951985359) A[1]:(-0.600787580013) A[2]:(0.366318166256) A[3]:(0.891100168228)\n",
      " state (8)  A[0]:(0.653466939926) A[1]:(0.0021609631367) A[2]:(0.729549109936) A[3]:(0.590011060238)\n",
      " state (9)  A[0]:(0.655190229416) A[1]:(0.810665726662) A[2]:(0.810378789902) A[3]:(-0.00170373753645)\n",
      " state (10)  A[0]:(0.729032218456) A[1]:(0.90047121048) A[2]:(0.0013109438587) A[3]:(0.728693246841)\n",
      " state (11)  A[0]:(0.124958276749) A[1]:(0.875451505184) A[2]:(-0.903659820557) A[3]:(0.794963955879)\n",
      " state (12)  A[0]:(-0.429350972176) A[1]:(0.805106937885) A[2]:(-0.909245848656) A[3]:(0.713342607021)\n",
      " state (13)  A[0]:(-0.000756800058298) A[1]:(0.810727715492) A[2]:(0.901263773441) A[3]:(0.730040311813)\n",
      " state (14)  A[0]:(0.810136198997) A[1]:(0.900991976261) A[2]:(0.999997735023) A[3]:(0.809927463531)\n",
      " state (15)  A[0]:(0.979311347008) A[1]:(0.943101763725) A[2]:(1.0) A[3]:(0.884456276894)\n",
      "Episode 285000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6249. Times reached goal: 924.               Steps done: 2340180. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0866793574137.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53163009882) A[1]:(0.590271234512) A[2]:(0.591106295586) A[3]:(0.532067894936)\n",
      " state (1)  A[0]:(0.531460404396) A[1]:(0.000593945325818) A[2]:(0.656593263149) A[3]:(0.590746045113)\n",
      " state (2)  A[0]:(0.589212298393) A[1]:(0.728944778442) A[2]:(0.599145114422) A[3]:(0.657116770744)\n",
      " state (3)  A[0]:(0.658518016338) A[1]:(0.000379845470889) A[2]:(0.428229629993) A[3]:(0.558183550835)\n",
      " state (4)  A[0]:(0.588937401772) A[1]:(0.656048893929) A[2]:(0.00572985084727) A[3]:(0.533215165138)\n",
      " state (5)  A[0]:(-0.097122721374) A[1]:(0.999874651432) A[2]:(-0.600882411003) A[3]:(0.585576176643)\n",
      " state (6)  A[0]:(-8.77678394318e-06) A[1]:(0.809881031513) A[2]:(-0.000645637395792) A[3]:(0.655395507812)\n",
      " state (7)  A[0]:(0.571454942226) A[1]:(-0.60108423233) A[2]:(0.36615395546) A[3]:(0.891270399094)\n",
      " state (8)  A[0]:(0.653468370438) A[1]:(-0.000122979283333) A[2]:(0.728634476662) A[3]:(0.591839313507)\n",
      " state (9)  A[0]:(0.654634237289) A[1]:(0.809706330299) A[2]:(0.809402883053) A[3]:(0.000275671482086)\n",
      " state (10)  A[0]:(0.727828323841) A[1]:(0.899917066097) A[2]:(-0.00261735310778) A[3]:(0.729071855545)\n",
      " state (11)  A[0]:(0.121577933431) A[1]:(0.87482714653) A[2]:(-0.904658973217) A[3]:(0.79501324892)\n",
      " state (12)  A[0]:(-0.43198749423) A[1]:(0.804124236107) A[2]:(-0.910527765751) A[3]:(0.713239908218)\n",
      " state (13)  A[0]:(-0.00361490156502) A[1]:(0.809531211853) A[2]:(0.899631083012) A[3]:(0.729896366596)\n",
      " state (14)  A[0]:(0.80928426981) A[1]:(0.90029156208) A[2]:(0.999997735023) A[3]:(0.809698522091)\n",
      " state (15)  A[0]:(0.979243099689) A[1]:(0.94273775816) A[2]:(1.0) A[3]:(0.884053647518)\n",
      "Episode 286000 finished after 0 timesteps with r=0.0. Running score: 0.87. Times trained:               6241. Times reached goal: 916.               Steps done: 2346421. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0861400761218.\n",
      " state (0)  A[0]:(0.531528949738) A[1]:(0.590646862984) A[2]:(0.590132832527) A[3]:(0.531729757786)\n",
      " state (1)  A[0]:(0.53119468689) A[1]:(-0.000207126140594) A[2]:(0.655853629112) A[3]:(0.590714812279)\n",
      " state (2)  A[0]:(0.589258551598) A[1]:(0.728702545166) A[2]:(0.596309423447) A[3]:(0.657034516335)\n",
      " state (3)  A[0]:(0.658630251884) A[1]:(-1.16229057312e-05) A[2]:(0.425580233335) A[3]:(0.557932853699)\n",
      " state (4)  A[0]:(0.589124321938) A[1]:(0.656081855297) A[2]:(0.00119185389485) A[3]:(0.532667458057)\n",
      " state (5)  A[0]:(-0.0958980247378) A[1]:(0.999875307083) A[2]:(-0.605558753014) A[3]:(0.586201310158)\n",
      " state (6)  A[0]:(0.000328689813614) A[1]:(0.809732496738) A[2]:(-0.0016713127261) A[3]:(0.654826045036)\n",
      " state (7)  A[0]:(0.571405887604) A[1]:(-0.6003074646) A[2]:(0.366646021605) A[3]:(0.890689134598)\n",
      " state (8)  A[0]:(0.654245913029) A[1]:(0.000704109552316) A[2]:(0.728947520256) A[3]:(0.590408623219)\n",
      " state (9)  A[0]:(0.656686902046) A[1]:(0.810182988644) A[2]:(0.810110151768) A[3]:(-0.000816404644866)\n",
      " state (10)  A[0]:(0.730562567711) A[1]:(0.900054812431) A[2]:(0.00107216788456) A[3]:(0.729121804237)\n",
      " state (11)  A[0]:(0.128353342414) A[1]:(0.874836444855) A[2]:(-0.903970241547) A[3]:(0.795312166214)\n",
      " state (12)  A[0]:(-0.427096933126) A[1]:(0.803824305534) A[2]:(-0.910198152065) A[3]:(0.713367342949)\n",
      " state (13)  A[0]:(0.00114923669025) A[1]:(0.808974087238) A[2]:(0.899916052818) A[3]:(0.729891359806)\n",
      " state (14)  A[0]:(0.810595154762) A[1]:(0.899994850159) A[2]:(0.999997735023) A[3]:(0.809788823128)\n",
      " state (15)  A[0]:(0.979371726513) A[1]:(0.9426176548) A[2]:(1.0) A[3]:(0.884074449539)\n",
      "Episode 287000 finished after 0 timesteps with r=0.0. Running score: 0.84. Times trained:               6280. Times reached goal: 914.               Steps done: 2352701. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.085600811507.\n",
      " state (0)  A[0]:(0.532131671906) A[1]:(0.591195881367) A[2]:(0.591254591942) A[3]:(0.531324863434)\n",
      " state (1)  A[0]:(0.532200813293) A[1]:(0.000388219923479) A[2]:(0.65620046854) A[3]:(0.590809226036)\n",
      " state (2)  A[0]:(0.590122282505) A[1]:(0.729258358479) A[2]:(0.596755743027) A[3]:(0.656986296177)\n",
      " state (3)  A[0]:(0.659324526787) A[1]:(0.000482395262225) A[2]:(0.427383303642) A[3]:(0.557878017426)\n",
      " state (4)  A[0]:(0.589903593063) A[1]:(0.655834794044) A[2]:(0.00142085459083) A[3]:(0.532219409943)\n",
      " state (5)  A[0]:(-0.0943366736174) A[1]:(0.99987590313) A[2]:(-0.609448432922) A[3]:(0.586167037487)\n",
      " state (6)  A[0]:(0.00027135014534) A[1]:(0.810163199902) A[2]:(-0.00204038340598) A[3]:(0.65510058403)\n",
      " state (7)  A[0]:(0.571176886559) A[1]:(-0.600208997726) A[2]:(0.368307054043) A[3]:(0.890949249268)\n",
      " state (8)  A[0]:(0.654344677925) A[1]:(0.00126011599787) A[2]:(0.729499816895) A[3]:(0.591948568821)\n",
      " state (9)  A[0]:(0.656184673309) A[1]:(0.810806751251) A[2]:(0.810237407684) A[3]:(0.000735401990823)\n",
      " state (10)  A[0]:(0.729226529598) A[1]:(0.900154531002) A[2]:(0.000249743461609) A[3]:(0.729338645935)\n",
      " state (11)  A[0]:(0.124133273959) A[1]:(0.874707221985) A[2]:(-0.904273569584) A[3]:(0.795185804367)\n",
      " state (12)  A[0]:(-0.430070400238) A[1]:(0.803482294083) A[2]:(-0.910317242146) A[3]:(0.713235914707)\n",
      " state (13)  A[0]:(-0.000760659400839) A[1]:(0.808729708195) A[2]:(0.90052819252) A[3]:(0.729984164238)\n",
      " state (14)  A[0]:(0.810517430305) A[1]:(0.900020480156) A[2]:(0.999997794628) A[3]:(0.809941112995)\n",
      " state (15)  A[0]:(0.979408800602) A[1]:(0.942700445652) A[2]:(1.0) A[3]:(0.884061694145)\n",
      "Episode 288000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6195. Times reached goal: 899.               Steps done: 2358896. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.085072153688.\n",
      " state (0)  A[0]:(0.531310081482) A[1]:(0.590013504028) A[2]:(0.590357303619) A[3]:(0.531088709831)\n",
      " state (1)  A[0]:(0.531460285187) A[1]:(0.000240787863731) A[2]:(0.65558052063) A[3]:(0.58984965086)\n",
      " state (2)  A[0]:(0.589282810688) A[1]:(0.728585124016) A[2]:(0.596005320549) A[3]:(0.655888199806)\n",
      " state (3)  A[0]:(0.658520340919) A[1]:(-7.86930322647e-05) A[2]:(0.428141117096) A[3]:(0.556726872921)\n",
      " state (4)  A[0]:(0.589047789574) A[1]:(0.655717074871) A[2]:(0.00134479918052) A[3]:(0.531182527542)\n",
      " state (5)  A[0]:(-0.0941295996308) A[1]:(0.999876379967) A[2]:(-0.612075567245) A[3]:(0.586815059185)\n",
      " state (6)  A[0]:(0.000785350624938) A[1]:(0.80978924036) A[2]:(-0.0012984268833) A[3]:(0.654053449631)\n",
      " state (7)  A[0]:(0.570921301842) A[1]:(-0.598429679871) A[2]:(0.368636518717) A[3]:(0.889936864376)\n",
      " state (8)  A[0]:(0.653968036175) A[1]:(0.000227198004723) A[2]:(0.728992462158) A[3]:(0.589555621147)\n",
      " state (9)  A[0]:(0.655617117882) A[1]:(0.809834182262) A[2]:(0.810040473938) A[3]:(-0.00283341831528)\n",
      " state (10)  A[0]:(0.729317903519) A[1]:(0.899894237518) A[2]:(0.000544309557881) A[3]:(0.727895021439)\n",
      " state (11)  A[0]:(0.126011654735) A[1]:(0.874930500984) A[2]:(-0.904232680798) A[3]:(0.794511079788)\n",
      " state (12)  A[0]:(-0.428292781115) A[1]:(0.804249286652) A[2]:(-0.910521805286) A[3]:(0.712392687798)\n",
      " state (13)  A[0]:(0.000519156397786) A[1]:(0.809398293495) A[2]:(0.900221645832) A[3]:(0.729125320911)\n",
      " state (14)  A[0]:(0.810551524162) A[1]:(0.900271594524) A[2]:(0.999997794628) A[3]:(0.809280991554)\n",
      " state (15)  A[0]:(0.979398548603) A[1]:(0.942797660828) A[2]:(1.0) A[3]:(0.88351213932)\n",
      "Episode 289000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6211. Times reached goal: 897.               Steps done: 2365107. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0845454080434.\n",
      "q_values \n",
      "tensor([[ 0.5323,  0.5901,  0.5902,  0.5326]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5324,  0.0005,  0.6561,  0.5912]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.7290,  0.5969,  0.6571]], device='cuda:0')\n",
      "On state=2, selected action=3 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5897,  0.7288,  0.5970,  0.6568]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  0.8095, -0.0001,  0.6556]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7278,  0.8999, -0.0020,  0.7280]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8095,  0.9010,  1.0000,  0.8094]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530688166618) A[1]:(0.590130329132) A[2]:(0.589859604836) A[3]:(0.530271410942)\n",
      " state (1)  A[0]:(0.530790090561) A[1]:(8.225440979e-05) A[2]:(0.655881762505) A[3]:(0.589386224747)\n",
      " state (2)  A[0]:(0.588836789131) A[1]:(0.729191660881) A[2]:(0.596504986286) A[3]:(0.655840277672)\n",
      " state (3)  A[0]:(0.658092081547) A[1]:(0.00204777438194) A[2]:(0.430165290833) A[3]:(0.55668592453)\n",
      " state (4)  A[0]:(0.588540434837) A[1]:(0.656417369843) A[2]:(0.00287579698488) A[3]:(0.531403422356)\n",
      " state (5)  A[0]:(-0.094763495028) A[1]:(0.999877393246) A[2]:(-0.613830387592) A[3]:(0.589063823223)\n",
      " state (6)  A[0]:(0.000266030430794) A[1]:(0.8104839921) A[2]:(-7.00950622559e-05) A[3]:(0.655120968819)\n",
      " state (7)  A[0]:(0.570879340172) A[1]:(-0.597432613373) A[2]:(0.370125412941) A[3]:(0.890234947205)\n",
      " state (8)  A[0]:(0.655095815659) A[1]:(0.000329256057739) A[2]:(0.728999614716) A[3]:(0.592906236649)\n",
      " state (9)  A[0]:(0.656049847603) A[1]:(0.810312628746) A[2]:(0.81010299921) A[3]:(0.000131964683533)\n",
      " state (10)  A[0]:(0.72892999649) A[1]:(0.900276303291) A[2]:(0.00014328956604) A[3]:(0.728637158871)\n",
      " state (11)  A[0]:(0.123867802322) A[1]:(0.875576555729) A[2]:(-0.904489159584) A[3]:(0.794891357422)\n",
      " state (12)  A[0]:(-0.430567145348) A[1]:(0.805451631546) A[2]:(-0.910766065121) A[3]:(0.712701916695)\n",
      " state (13)  A[0]:(-0.00224934145808) A[1]:(0.810719311237) A[2]:(0.900510132313) A[3]:(0.729382872581)\n",
      " state (14)  A[0]:(0.809707283974) A[1]:(0.90107691288) A[2]:(0.999997854233) A[3]:(0.809467315674)\n",
      " state (15)  A[0]:(0.979325652122) A[1]:(0.94327712059) A[2]:(1.0) A[3]:(0.883510828018)\n",
      "Episode 290000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6237. Times reached goal: 926.               Steps done: 2371344. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0840197393353.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530860841274) A[1]:(0.58996963501) A[2]:(0.589877724648) A[3]:(0.530342340469)\n",
      " state (1)  A[0]:(0.530999422073) A[1]:(0.000365793675883) A[2]:(0.655830383301) A[3]:(0.590034544468)\n",
      " state (2)  A[0]:(0.589368224144) A[1]:(0.729064583778) A[2]:(0.595289885998) A[3]:(0.656376600266)\n",
      " state (3)  A[0]:(0.658483147621) A[1]:(0.0022230707109) A[2]:(0.429689019918) A[3]:(0.556958079338)\n",
      " state (4)  A[0]:(0.589232087135) A[1]:(0.655808329582) A[2]:(0.00147914781701) A[3]:(0.531509459019)\n",
      " state (5)  A[0]:(-0.0928387716413) A[1]:(0.999877691269) A[2]:(-0.617009043694) A[3]:(0.590548157692)\n",
      " state (6)  A[0]:(0.000567689479794) A[1]:(0.810133755207) A[2]:(-0.00119435728993) A[3]:(0.655198693275)\n",
      " state (7)  A[0]:(0.570280909538) A[1]:(-0.596957564354) A[2]:(0.369919329882) A[3]:(0.889788925648)\n",
      " state (8)  A[0]:(0.654143810272) A[1]:(0.000183880329132) A[2]:(0.729025959969) A[3]:(0.590959191322)\n",
      " state (9)  A[0]:(0.655814051628) A[1]:(0.80986726284) A[2]:(0.810043036938) A[3]:(0.000474870175822)\n",
      " state (10)  A[0]:(0.728951156139) A[1]:(0.899980664253) A[2]:(-0.000202536582947) A[3]:(0.729174911976)\n",
      " state (11)  A[0]:(0.124070994556) A[1]:(0.875197947025) A[2]:(-0.904699385166) A[3]:(0.795259356499)\n",
      " state (12)  A[0]:(-0.430208355188) A[1]:(0.804625689983) A[2]:(-0.911236286163) A[3]:(0.713121414185)\n",
      " state (13)  A[0]:(-0.00196593743749) A[1]:(0.809452414513) A[2]:(0.899901211262) A[3]:(0.729900538921)\n",
      " state (14)  A[0]:(0.809592962265) A[1]:(0.900205016136) A[2]:(0.999997854233) A[3]:(0.80995875597)\n",
      " state (15)  A[0]:(0.979295611382) A[1]:(0.94272851944) A[2]:(1.0) A[3]:(0.883781731129)\n",
      "Episode 291000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6276. Times reached goal: 925.               Steps done: 2377620. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0834940826872.\n",
      " state (0)  A[0]:(0.530993580818) A[1]:(0.590204238892) A[2]:(0.590240895748) A[3]:(0.533062338829)\n",
      " state (1)  A[0]:(0.530958652496) A[1]:(-0.000499308051076) A[2]:(0.655845165253) A[3]:(0.592060148716)\n",
      " state (2)  A[0]:(0.589377522469) A[1]:(0.729195713997) A[2]:(0.594742774963) A[3]:(0.6583122015)\n",
      " state (3)  A[0]:(0.658256232738) A[1]:(0.00380651303567) A[2]:(0.430070638657) A[3]:(0.559279620647)\n",
      " state (4)  A[0]:(0.589012205601) A[1]:(0.656364798546) A[2]:(0.00142264273018) A[3]:(0.533864021301)\n",
      " state (5)  A[0]:(-0.0923718139529) A[1]:(0.99987834692) A[2]:(-0.618747830391) A[3]:(0.593394935131)\n",
      " state (6)  A[0]:(0.000150635838509) A[1]:(0.810195505619) A[2]:(-0.000358581513865) A[3]:(0.655656218529)\n",
      " state (7)  A[0]:(0.569493532181) A[1]:(-0.596429347992) A[2]:(0.371040046215) A[3]:(0.889340817928)\n",
      " state (8)  A[0]:(0.653310060501) A[1]:(0.00016063451767) A[2]:(0.729189872742) A[3]:(0.588657140732)\n",
      " state (9)  A[0]:(0.654862165451) A[1]:(0.80985981226) A[2]:(0.809948086739) A[3]:(-0.00404540495947)\n",
      " state (10)  A[0]:(0.728048980236) A[1]:(0.899968504906) A[2]:(-0.00094783277018) A[3]:(0.726829469204)\n",
      " state (11)  A[0]:(0.122496619821) A[1]:(0.875251114368) A[2]:(-0.904948771) A[3]:(0.793509364128)\n",
      " state (12)  A[0]:(-0.430542826653) A[1]:(0.804728090763) A[2]:(-0.911503851414) A[3]:(0.711127996445)\n",
      " state (13)  A[0]:(-0.00123588682618) A[1]:(0.809510886669) A[2]:(0.899955868721) A[3]:(0.728439331055)\n",
      " state (14)  A[0]:(0.810031652451) A[1]:(0.900292754173) A[2]:(0.999997913837) A[3]:(0.809130609035)\n",
      " state (15)  A[0]:(0.979356050491) A[1]:(0.942827105522) A[2]:(1.0) A[3]:(0.88325792551)\n",
      "Episode 292000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6219. Times reached goal: 906.               Steps done: 2383839. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.082976444252.\n",
      " state (0)  A[0]:(0.531980872154) A[1]:(0.590559005737) A[2]:(0.591331362724) A[3]:(0.531892299652)\n",
      " state (1)  A[0]:(0.532335877419) A[1]:(-0.000340506434441) A[2]:(0.655893087387) A[3]:(0.591120004654)\n",
      " state (2)  A[0]:(0.5905200243) A[1]:(0.728897452354) A[2]:(0.593915402889) A[3]:(0.656510353088)\n",
      " state (3)  A[0]:(0.659108638763) A[1]:(0.00403802888468) A[2]:(0.429894298315) A[3]:(0.556973338127)\n",
      " state (4)  A[0]:(0.590341389179) A[1]:(0.655356407166) A[2]:(0.000934958166908) A[3]:(0.532008469105)\n",
      " state (5)  A[0]:(-0.0898408144712) A[1]:(0.999878585339) A[2]:(-0.620741903782) A[3]:(0.593976736069)\n",
      " state (6)  A[0]:(0.00128884543665) A[1]:(0.810058295727) A[2]:(-0.000852465396747) A[3]:(0.655639111996)\n",
      " state (7)  A[0]:(0.57048368454) A[1]:(-0.596272826195) A[2]:(0.370982587337) A[3]:(0.889488697052)\n",
      " state (8)  A[0]:(0.654631257057) A[1]:(0.00062842661282) A[2]:(0.728942751884) A[3]:(0.59087741375)\n",
      " state (9)  A[0]:(0.656005382538) A[1]:(0.810294628143) A[2]:(0.809957504272) A[3]:(-1.51991844177e-05)\n",
      " state (10)  A[0]:(0.729133546352) A[1]:(0.900064885616) A[2]:(-0.000522732676473) A[3]:(0.729022860527)\n",
      " state (11)  A[0]:(0.124442629516) A[1]:(0.875185668468) A[2]:(-0.904988825321) A[3]:(0.795211851597)\n",
      " state (12)  A[0]:(-0.429644227028) A[1]:(0.804449081421) A[2]:(-0.911758840084) A[3]:(0.712999224663)\n",
      " state (13)  A[0]:(-0.000662192585878) A[1]:(0.809239923954) A[2]:(0.899884164333) A[3]:(0.729848265648)\n",
      " state (14)  A[0]:(0.810189366341) A[1]:(0.90031337738) A[2]:(0.999997913837) A[3]:(0.80992603302)\n",
      " state (15)  A[0]:(0.979371547699) A[1]:(0.942965447903) A[2]:(1.0) A[3]:(0.883532583714)\n",
      "Episode 293000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6185. Times reached goal: 908.               Steps done: 2390024. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0824648187771.\n",
      " state (0)  A[0]:(0.530867099762) A[1]:(0.590844392776) A[2]:(0.591455578804) A[3]:(0.530494809151)\n",
      " state (1)  A[0]:(0.53064763546) A[1]:(-0.00172172312159) A[2]:(0.656519532204) A[3]:(0.589383125305)\n",
      " state (2)  A[0]:(0.589156389236) A[1]:(0.728554308414) A[2]:(0.594318509102) A[3]:(0.655295610428)\n",
      " state (3)  A[0]:(0.658082962036) A[1]:(0.00359356356785) A[2]:(0.431107848883) A[3]:(0.555046916008)\n",
      " state (4)  A[0]:(0.589509487152) A[1]:(0.654049754143) A[2]:(0.00204550940543) A[3]:(0.529588222504)\n",
      " state (5)  A[0]:(-0.0915461927652) A[1]:(0.999878168106) A[2]:(-0.621478497982) A[3]:(0.591833770275)\n",
      " state (6)  A[0]:(-0.0023239213042) A[1]:(0.808819591999) A[2]:(0.00117492617574) A[3]:(0.652181982994)\n",
      " state (7)  A[0]:(0.567737817764) A[1]:(-0.596352338791) A[2]:(0.372658491135) A[3]:(0.887880206108)\n",
      " state (8)  A[0]:(0.652976989746) A[1]:(-0.000503882707562) A[2]:(0.729316234589) A[3]:(0.585850000381)\n",
      " state (9)  A[0]:(0.654318630695) A[1]:(0.809881031513) A[2]:(0.810158312321) A[3]:(-0.0100890547037)\n",
      " state (10)  A[0]:(0.727576732635) A[1]:(0.900063872337) A[2]:(-0.000205755233765) A[3]:(0.723890066147)\n",
      " state (11)  A[0]:(0.121798038483) A[1]:(0.875564575195) A[2]:(-0.904919624329) A[3]:(0.791395306587)\n",
      " state (12)  A[0]:(-0.430344760418) A[1]:(0.805468082428) A[2]:(-0.911327779293) A[3]:(0.708469390869)\n",
      " state (13)  A[0]:(8.824467659e-05) A[1]:(0.810526311398) A[2]:(0.901487469673) A[3]:(0.726062655449)\n",
      " state (14)  A[0]:(0.810656428337) A[1]:(0.901108622551) A[2]:(0.999997973442) A[3]:(0.807393789291)\n",
      " state (15)  A[0]:(0.979431927204) A[1]:(0.943395793438) A[2]:(1.0) A[3]:(0.881962299347)\n",
      "Episode 294000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6241. Times reached goal: 919.               Steps done: 2396265. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.081951758513.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5907,  0.5904,  0.5327]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5891,  0.6562,  0.0012,  0.5334]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6534,  0.0016,  0.7296,  0.5905]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6555,  0.8103,  0.8100,  0.0004]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  0.8091,  0.9000,  0.7301]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8112,  0.9002,  1.0000,  0.8104]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531471669674) A[1]:(0.590658664703) A[2]:(0.590039253235) A[3]:(0.532918930054)\n",
      " state (1)  A[0]:(0.531384885311) A[1]:(0.000301107764244) A[2]:(0.655813097954) A[3]:(0.591700017452)\n",
      " state (2)  A[0]:(0.589678049088) A[1]:(0.729495167732) A[2]:(0.593202769756) A[3]:(0.658009409904)\n",
      " state (3)  A[0]:(0.658239305019) A[1]:(0.00707422336563) A[2]:(0.43024417758) A[3]:(0.558994531631)\n",
      " state (4)  A[0]:(0.589734196663) A[1]:(0.656467020512) A[2]:(0.00033175945282) A[3]:(0.533873915672)\n",
      " state (5)  A[0]:(-0.0891659110785) A[1]:(0.999879658222) A[2]:(-0.623730123043) A[3]:(0.596032738686)\n",
      " state (6)  A[0]:(0.000987648614682) A[1]:(0.810161709785) A[2]:(-0.000513434351888) A[3]:(0.656782507896)\n",
      " state (7)  A[0]:(0.570310711861) A[1]:(-0.595800995827) A[2]:(0.37220159173) A[3]:(0.889656424522)\n",
      " state (8)  A[0]:(0.654994785786) A[1]:(0.000233873724937) A[2]:(0.72932446003) A[3]:(0.591019034386)\n",
      " state (9)  A[0]:(0.657106995583) A[1]:(0.810029149055) A[2]:(0.810165941715) A[3]:(0.000556290091481)\n",
      " state (10)  A[0]:(0.73033362627) A[1]:(0.899890601635) A[2]:(0.000125050544739) A[3]:(0.729262173176)\n",
      " state (11)  A[0]:(0.127203404903) A[1]:(0.875073611736) A[2]:(-0.905078172684) A[3]:(0.795316696167)\n",
      " state (12)  A[0]:(-0.427088469267) A[1]:(0.804295241833) A[2]:(-0.912088394165) A[3]:(0.713059902191)\n",
      " state (13)  A[0]:(0.00264906254597) A[1]:(0.808978259563) A[2]:(0.900141239166) A[3]:(0.730056285858)\n",
      " state (14)  A[0]:(0.811268687248) A[1]:(0.900248467922) A[2]:(0.999997973442) A[3]:(0.810206532478)\n",
      " state (15)  A[0]:(0.97949719429) A[1]:(0.942984700203) A[2]:(1.0) A[3]:(0.883624315262)\n",
      "Episode 295000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6222. Times reached goal: 915.               Steps done: 2402487. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0814434376975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531375408173) A[1]:(0.590453624725) A[2]:(0.590602695942) A[3]:(0.530358672142)\n",
      " state (1)  A[0]:(0.531242609024) A[1]:(-0.000175565481186) A[2]:(0.656047463417) A[3]:(0.589915812016)\n",
      " state (2)  A[0]:(0.589649736881) A[1]:(0.728846132755) A[2]:(0.592641592026) A[3]:(0.656233727932)\n",
      " state (3)  A[0]:(0.658114671707) A[1]:(0.0060315891169) A[2]:(0.429996043444) A[3]:(0.556669473648)\n",
      " state (4)  A[0]:(0.589682579041) A[1]:(0.65616196394) A[2]:(-0.000285387039185) A[3]:(0.531470775604)\n",
      " state (5)  A[0]:(-0.0887135714293) A[1]:(0.999879956245) A[2]:(-0.624903857708) A[3]:(0.594442248344)\n",
      " state (6)  A[0]:(0.000773086969275) A[1]:(0.809924960136) A[2]:(-0.000439882249339) A[3]:(0.655074477196)\n",
      " state (7)  A[0]:(0.570292830467) A[1]:(-0.595533549786) A[2]:(0.372316390276) A[3]:(0.889144361019)\n",
      " state (8)  A[0]:(0.655331969261) A[1]:(-0.00020107626915) A[2]:(0.728951573372) A[3]:(0.59096544981)\n",
      " state (9)  A[0]:(0.656910359859) A[1]:(0.80992358923) A[2]:(0.810008168221) A[3]:(-0.000113368034363)\n",
      " state (10)  A[0]:(0.730183124542) A[1]:(0.899930596352) A[2]:(0.000252246856689) A[3]:(0.729129254818)\n",
      " state (11)  A[0]:(0.126817137003) A[1]:(0.875270664692) A[2]:(-0.905123472214) A[3]:(0.795414626598)\n",
      " state (12)  A[0]:(-0.427931308746) A[1]:(0.804679989815) A[2]:(-0.912276089191) A[3]:(0.713127374649)\n",
      " state (13)  A[0]:(0.000969692773651) A[1]:(0.809290885925) A[2]:(0.900289833546) A[3]:(0.729969799519)\n",
      " state (14)  A[0]:(0.810569763184) A[1]:(0.900394976139) A[2]:(0.999998033047) A[3]:(0.810020148754)\n",
      " state (15)  A[0]:(0.979416668415) A[1]:(0.943041324615) A[2]:(1.0) A[3]:(0.883389174938)\n",
      "Episode 296000 finished after 0 timesteps with r=0.0. Running score: 0.93. Times trained:               6198. Times reached goal: 924.               Steps done: 2408685. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0809402123768.\n",
      " state (0)  A[0]:(0.531583964825) A[1]:(0.590708494186) A[2]:(0.590091466904) A[3]:(0.531250715256)\n",
      " state (1)  A[0]:(0.5317081213) A[1]:(0.000271573662758) A[2]:(0.655981183052) A[3]:(0.590037107468)\n",
      " state (2)  A[0]:(0.589883744717) A[1]:(0.728972911835) A[2]:(0.592307031155) A[3]:(0.656224489212)\n",
      " state (3)  A[0]:(0.658116519451) A[1]:(0.00671878783032) A[2]:(0.429935604334) A[3]:(0.556707143784)\n",
      " state (4)  A[0]:(0.589761972427) A[1]:(0.656164526939) A[2]:(-0.000311493873596) A[3]:(0.531615138054)\n",
      " state (5)  A[0]:(-0.0882207676768) A[1]:(0.999880254269) A[2]:(-0.62517952919) A[3]:(0.595009803772)\n",
      " state (6)  A[0]:(0.000567972601857) A[1]:(0.809593200684) A[2]:(0.000108599662781) A[3]:(0.655256569386)\n",
      " state (7)  A[0]:(0.570140242577) A[1]:(-0.595960140228) A[2]:(0.372685402632) A[3]:(0.889188408852)\n",
      " state (8)  A[0]:(0.655067443848) A[1]:(-0.000744089367799) A[2]:(0.729114174843) A[3]:(0.590984463692)\n",
      " state (9)  A[0]:(0.656328141689) A[1]:(0.809826374054) A[2]:(0.810119688511) A[3]:(-0.000298082828522)\n",
      " state (10)  A[0]:(0.729364037514) A[1]:(0.899864494801) A[2]:(0.000118851661682) A[3]:(0.728842437267)\n",
      " state (11)  A[0]:(0.12456279248) A[1]:(0.875234901905) A[2]:(-0.905355870724) A[3]:(0.795002281666)\n",
      " state (12)  A[0]:(-0.429542988539) A[1]:(0.804648399353) A[2]:(-0.912666022778) A[3]:(0.712586522102)\n",
      " state (13)  A[0]:(-0.000426501006586) A[1]:(0.809230208397) A[2]:(0.90015399456) A[3]:(0.729701638222)\n",
      " state (14)  A[0]:(0.810244739056) A[1]:(0.900433242321) A[2]:(0.999998033047) A[3]:(0.810006380081)\n",
      " state (15)  A[0]:(0.979397177696) A[1]:(0.943113565445) A[2]:(1.0) A[3]:(0.883394122124)\n",
      "Episode 297000 finished after 0 timesteps with r=0.0. Running score: 0.94. Times trained:               6290. Times reached goal: 935.               Steps done: 2414975. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0804326962525.\n",
      " state (0)  A[0]:(0.531052589417) A[1]:(0.590237975121) A[2]:(0.590333580971) A[3]:(0.531370997429)\n",
      " state (1)  A[0]:(0.530911564827) A[1]:(-0.000196799635887) A[2]:(0.655670881271) A[3]:(0.590157747269)\n",
      " state (2)  A[0]:(0.58908367157) A[1]:(0.728738307953) A[2]:(0.591767430305) A[3]:(0.656079411507)\n",
      " state (3)  A[0]:(0.657242059708) A[1]:(0.00750946300104) A[2]:(0.429449290037) A[3]:(0.556456625462)\n",
      " state (4)  A[0]:(0.589141249657) A[1]:(0.655516088009) A[2]:(-0.000235438346863) A[3]:(0.531452000141)\n",
      " state (5)  A[0]:(-0.0885741561651) A[1]:(0.999880731106) A[2]:(-0.62502425909) A[3]:(0.595412611961)\n",
      " state (6)  A[0]:(0.000280886888504) A[1]:(0.809809207916) A[2]:(-0.000636339129414) A[3]:(0.655144095421)\n",
      " state (7)  A[0]:(0.570360839367) A[1]:(-0.595422506332) A[2]:(0.371589690447) A[3]:(0.889117717743)\n",
      " state (8)  A[0]:(0.655233025551) A[1]:(-0.000374183029635) A[2]:(0.728576779366) A[3]:(0.591073691845)\n",
      " state (9)  A[0]:(0.656312763691) A[1]:(0.80980181694) A[2]:(0.809796214104) A[3]:(0.000367343396647)\n",
      " state (10)  A[0]:(0.729187130928) A[1]:(0.89987796545) A[2]:(-0.000854849582538) A[3]:(0.728810191154)\n",
      " state (11)  A[0]:(0.124249212444) A[1]:(0.875298857689) A[2]:(-0.905726015568) A[3]:(0.794628024101)\n",
      " state (12)  A[0]:(-0.428945988417) A[1]:(0.804589152336) A[2]:(-0.913239359856) A[3]:(0.712084531784)\n",
      " state (13)  A[0]:(0.000824346963782) A[1]:(0.808803975582) A[2]:(0.89980340004) A[3]:(0.729462385178)\n",
      " state (14)  A[0]:(0.810537457466) A[1]:(0.900126993656) A[2]:(0.999998092651) A[3]:(0.809899032116)\n",
      " state (15)  A[0]:(0.979413449764) A[1]:(0.9429718256) A[2]:(1.0) A[3]:(0.883248925209)\n",
      "Episode 298000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6228. Times reached goal: 920.               Steps done: 2421203. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.079933318098.\n",
      " state (0)  A[0]:(0.531553089619) A[1]:(0.590569317341) A[2]:(0.590589642525) A[3]:(0.531681418419)\n",
      " state (1)  A[0]:(0.531660795212) A[1]:(-0.00050984317204) A[2]:(0.65616941452) A[3]:(0.590505897999)\n",
      " state (2)  A[0]:(0.590076386929) A[1]:(0.728841304779) A[2]:(0.591803610325) A[3]:(0.65643799305)\n",
      " state (3)  A[0]:(0.657896101475) A[1]:(0.00890864711255) A[2]:(0.429399341345) A[3]:(0.556789755821)\n",
      " state (4)  A[0]:(0.589921236038) A[1]:(0.65584564209) A[2]:(-0.00023627281189) A[3]:(0.531665802002)\n",
      " state (5)  A[0]:(-0.0876568928361) A[1]:(0.999881267548) A[2]:(-0.625039339066) A[3]:(0.595708608627)\n",
      " state (6)  A[0]:(0.000485524506075) A[1]:(0.810001015663) A[2]:(-0.000387191743357) A[3]:(0.655719637871)\n",
      " state (7)  A[0]:(0.570623397827) A[1]:(-0.595697760582) A[2]:(0.372082859278) A[3]:(0.889396727085)\n",
      " state (8)  A[0]:(0.655352592468) A[1]:(-0.000415667862399) A[2]:(0.728878498077) A[3]:(0.591051697731)\n",
      " state (9)  A[0]:(0.656622171402) A[1]:(0.809885978699) A[2]:(0.810020625591) A[3]:(8.10623168945e-05)\n",
      " state (10)  A[0]:(0.729664623737) A[1]:(0.89990913868) A[2]:(0.000226616859436) A[3]:(0.728849411011)\n",
      " state (11)  A[0]:(0.125003755093) A[1]:(0.875374138355) A[2]:(-0.905699253082) A[3]:(0.794610381126)\n",
      " state (12)  A[0]:(-0.428772479296) A[1]:(0.804703056812) A[2]:(-0.913424968719) A[3]:(0.71193921566)\n",
      " state (13)  A[0]:(0.000684544327669) A[1]:(0.808889508247) A[2]:(0.900210857391) A[3]:(0.729327142239)\n",
      " state (14)  A[0]:(0.810483217239) A[1]:(0.90026396513) A[2]:(0.999998152256) A[3]:(0.809817314148)\n",
      " state (15)  A[0]:(0.979417145252) A[1]:(0.943089842796) A[2]:(1.0) A[3]:(0.883157074451)\n",
      "Episode 299000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6245. Times reached goal: 923.               Steps done: 2427448. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0794356899875.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5904,  0.5904,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315, -0.0001,  0.6561,  0.5902]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5898,  0.7292,  0.5917,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  0.8101, -0.0001,  0.6558]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7294,  0.9000,  0.0000,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9005,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531300425529) A[1]:(0.590407669544) A[2]:(0.590364098549) A[3]:(0.531273961067)\n",
      " state (1)  A[0]:(0.531283259392) A[1]:(-5.27501106262e-05) A[2]:(0.656067728996) A[3]:(0.590188264847)\n",
      " state (2)  A[0]:(0.589704871178) A[1]:(0.729201257229) A[2]:(0.591785550117) A[3]:(0.656005263329)\n",
      " state (3)  A[0]:(0.657360970974) A[1]:(0.0116096884012) A[2]:(0.429405450821) A[3]:(0.556397557259)\n",
      " state (4)  A[0]:(0.589719831944) A[1]:(0.655777215958) A[2]:(6.91413879395e-05) A[3]:(0.531287670135)\n",
      " state (5)  A[0]:(-0.0874841660261) A[1]:(0.999881565571) A[2]:(-0.625004529953) A[3]:(0.595514774323)\n",
      " state (6)  A[0]:(0.00019858777523) A[1]:(0.809988319874) A[2]:(0.000154137611389) A[3]:(0.655624747276)\n",
      " state (7)  A[0]:(0.570592045784) A[1]:(-0.595580756664) A[2]:(0.372407883406) A[3]:(0.889415621758)\n",
      " state (8)  A[0]:(0.654963970184) A[1]:(-0.000531017722096) A[2]:(0.729027330875) A[3]:(0.590700089931)\n",
      " state (9)  A[0]:(0.655891656876) A[1]:(0.809766769409) A[2]:(0.81007874012) A[3]:(-0.000517487467732)\n",
      " state (10)  A[0]:(0.728939712048) A[1]:(0.899915635586) A[2]:(-0.000143051147461) A[3]:(0.728560209274)\n",
      " state (11)  A[0]:(0.123260371387) A[1]:(0.875564754009) A[2]:(-0.90606033802) A[3]:(0.794303834438)\n",
      " state (12)  A[0]:(-0.43002474308) A[1]:(0.805128157139) A[2]:(-0.913992762566) A[3]:(0.711606800556)\n",
      " state (13)  A[0]:(-0.000748380902223) A[1]:(0.809258043766) A[2]:(0.900076568127) A[3]:(0.729184865952)\n",
      " state (14)  A[0]:(0.809955298901) A[1]:(0.900483846664) A[2]:(0.999998152256) A[3]:(0.809764206409)\n",
      " state (15)  A[0]:(0.979363977909) A[1]:(0.943219661713) A[2]:(1.0) A[3]:(0.883081495762)\n",
      "Episode 300000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6213. Times reached goal: 932.               Steps done: 2433661. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0789436860385.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531604290009) A[1]:(0.590389490128) A[2]:(0.590176701546) A[3]:(0.530448675156)\n",
      " state (1)  A[0]:(0.53149998188) A[1]:(8.48025083542e-05) A[2]:(0.656018853188) A[3]:(0.589340209961)\n",
      " state (2)  A[0]:(0.58971118927) A[1]:(0.729096293449) A[2]:(0.59223151207) A[3]:(0.655498981476)\n",
      " state (3)  A[0]:(0.657310962677) A[1]:(0.0119798863307) A[2]:(0.430357903242) A[3]:(0.556101441383)\n",
      " state (4)  A[0]:(0.589675188065) A[1]:(0.656156718731) A[2]:(0.000642418744974) A[3]:(0.530959129333)\n",
      " state (5)  A[0]:(-0.0873483344913) A[1]:(0.999881982803) A[2]:(-0.625939190388) A[3]:(0.594773411751)\n",
      " state (6)  A[0]:(0.00030680000782) A[1]:(0.810076773167) A[2]:(-1.10864639282e-05) A[3]:(0.654773533344)\n",
      " state (7)  A[0]:(0.57077050209) A[1]:(-0.595146894455) A[2]:(0.372552543879) A[3]:(0.889102220535)\n",
      " state (8)  A[0]:(0.654972553253) A[1]:(0.00018946826458) A[2]:(0.729087650776) A[3]:(0.589572191238)\n",
      " state (9)  A[0]:(0.655840277672) A[1]:(0.809991300106) A[2]:(0.810037195683) A[3]:(-0.0015645014355)\n",
      " state (10)  A[0]:(0.728669643402) A[1]:(0.900016546249) A[2]:(-0.000526547373738) A[3]:(0.728136777878)\n",
      " state (11)  A[0]:(0.122215181589) A[1]:(0.875701367855) A[2]:(-0.906270205975) A[3]:(0.793923676014)\n",
      " state (12)  A[0]:(-0.430858910084) A[1]:(0.805268108845) A[2]:(-0.914261341095) A[3]:(0.711169600487)\n",
      " state (13)  A[0]:(-0.00152026000433) A[1]:(0.80923050642) A[2]:(0.90027064085) A[3]:(0.728984117508)\n",
      " state (14)  A[0]:(0.809770226479) A[1]:(0.900399565697) A[2]:(0.999998211861) A[3]:(0.809782445431)\n",
      " state (15)  A[0]:(0.979357481003) A[1]:(0.943108916283) A[2]:(1.0) A[3]:(0.883144915104)\n",
      "Episode 301000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6177. Times reached goal: 925.               Steps done: 2439838. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0784575538548.\n",
      " state (0)  A[0]:(0.531557381153) A[1]:(0.589950680733) A[2]:(0.590667009354) A[3]:(0.531661868095)\n",
      " state (1)  A[0]:(0.531407833099) A[1]:(1.71363353729e-06) A[2]:(0.656202375889) A[3]:(0.590549886227)\n",
      " state (2)  A[0]:(0.58979421854) A[1]:(0.729036808014) A[2]:(0.592460870743) A[3]:(0.656374752522)\n",
      " state (3)  A[0]:(0.657176971436) A[1]:(0.0119912093505) A[2]:(0.430687606335) A[3]:(0.556910097599)\n",
      " state (4)  A[0]:(0.589186787605) A[1]:(0.656270265579) A[2]:(0.000287294387817) A[3]:(0.53162330389)\n",
      " state (5)  A[0]:(-0.088745623827) A[1]:(0.999882340431) A[2]:(-0.627114534378) A[3]:(0.595723450184)\n",
      " state (6)  A[0]:(-0.000269666314125) A[1]:(0.810151159763) A[2]:(-0.000438451737864) A[3]:(0.655743062496)\n",
      " state (7)  A[0]:(0.570841312408) A[1]:(-0.595082998276) A[2]:(0.372547835112) A[3]:(0.889477670193)\n",
      " state (8)  A[0]:(0.654974102974) A[1]:(0.000996872433461) A[2]:(0.72916662693) A[3]:(0.590091586113)\n",
      " state (9)  A[0]:(0.656057476997) A[1]:(0.810454130173) A[2]:(0.810138761997) A[3]:(-0.000702559831552)\n",
      " state (10)  A[0]:(0.729141592979) A[1]:(0.900143027306) A[2]:(-0.000251650810242) A[3]:(0.728784203529)\n",
      " state (11)  A[0]:(0.123515173793) A[1]:(0.875694036484) A[2]:(-0.906390845776) A[3]:(0.794452428818)\n",
      " state (12)  A[0]:(-0.429656118155) A[1]:(0.804998159409) A[2]:(-0.914607584476) A[3]:(0.711751461029)\n",
      " state (13)  A[0]:(0.000279024243355) A[1]:(0.808754980564) A[2]:(0.900090038776) A[3]:(0.729454338551)\n",
      " state (14)  A[0]:(0.810542702675) A[1]:(0.90014398098) A[2]:(0.999998211861) A[3]:(0.810060918331)\n",
      " state (15)  A[0]:(0.979463279247) A[1]:(0.94298607111) A[2]:(1.0) A[3]:(0.883226037025)\n",
      "Episode 302000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6247. Times reached goal: 918.               Steps done: 2446085. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0779689572364.\n",
      " state (0)  A[0]:(0.531180083752) A[1]:(0.590775609016) A[2]:(0.590377748013) A[3]:(0.531872153282)\n",
      " state (1)  A[0]:(0.531196832657) A[1]:(0.000200763344765) A[2]:(0.656224012375) A[3]:(0.590448141098)\n",
      " state (2)  A[0]:(0.589624166489) A[1]:(0.728970110416) A[2]:(0.59266602993) A[3]:(0.656472086906)\n",
      " state (3)  A[0]:(0.657357811928) A[1]:(0.0114676840603) A[2]:(0.431454092264) A[3]:(0.557217597961)\n",
      " state (4)  A[0]:(0.589813113213) A[1]:(0.655917346478) A[2]:(0.000750899198465) A[3]:(0.532025337219)\n",
      " state (5)  A[0]:(-0.0873762071133) A[1]:(0.999882340431) A[2]:(-0.627989590168) A[3]:(0.595998048782)\n",
      " state (6)  A[0]:(9.71555709839e-05) A[1]:(0.809611976147) A[2]:(-0.000253081321716) A[3]:(0.655848622322)\n",
      " state (7)  A[0]:(0.570931434631) A[1]:(-0.595676541328) A[2]:(0.372755616903) A[3]:(0.889562308788)\n",
      " state (8)  A[0]:(0.655453324318) A[1]:(-0.00112198246643) A[2]:(0.728871166706) A[3]:(0.591361105442)\n",
      " state (9)  A[0]:(0.656472086906) A[1]:(0.809602499008) A[2]:(0.809989452362) A[3]:(0.000697731855325)\n",
      " state (10)  A[0]:(0.729747533798) A[1]:(0.899779319763) A[2]:(4.75645065308e-05) A[3]:(0.729427158833)\n",
      " state (11)  A[0]:(0.125188961625) A[1]:(0.875491440296) A[2]:(-0.906420648098) A[3]:(0.795012414455)\n",
      " state (12)  A[0]:(-0.42899531126) A[1]:(0.80492323637) A[2]:(-0.914915978909) A[3]:(0.71218931675)\n",
      " state (13)  A[0]:(-0.000182405114174) A[1]:(0.80869269371) A[2]:(0.899842083454) A[3]:(0.72958445549)\n",
      " state (14)  A[0]:(0.810111224651) A[1]:(0.90010613203) A[2]:(0.999998211861) A[3]:(0.810041666031)\n",
      " state (15)  A[0]:(0.979414582253) A[1]:(0.942941069603) A[2]:(1.0) A[3]:(0.883131444454)\n",
      "Episode 303000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6198. Times reached goal: 928.               Steps done: 2452283. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0774872001469.\n",
      " state (0)  A[0]:(0.530891060829) A[1]:(0.590851306915) A[2]:(0.590398192406) A[3]:(0.531287670135)\n",
      " state (1)  A[0]:(0.531034946442) A[1]:(0.000176027417183) A[2]:(0.656313598156) A[3]:(0.590222716331)\n",
      " state (2)  A[0]:(0.589854300022) A[1]:(0.72956520319) A[2]:(0.592573285103) A[3]:(0.656346857548)\n",
      " state (3)  A[0]:(0.657558083534) A[1]:(0.0123523892835) A[2]:(0.431531786919) A[3]:(0.556988239288)\n",
      " state (4)  A[0]:(0.590085148811) A[1]:(0.656090497971) A[2]:(0.000201821327209) A[3]:(0.53157055378)\n",
      " state (5)  A[0]:(-0.0870930626988) A[1]:(0.999882936478) A[2]:(-0.629455268383) A[3]:(0.595487713814)\n",
      " state (6)  A[0]:(-6.0647726059e-06) A[1]:(0.810336768627) A[2]:(-0.000437259644968) A[3]:(0.655737519264)\n",
      " state (7)  A[0]:(0.570907473564) A[1]:(-0.594700753689) A[2]:(0.373569905758) A[3]:(0.889592707157)\n",
      " state (8)  A[0]:(0.655371546745) A[1]:(0.00109608424827) A[2]:(0.729413628578) A[3]:(0.591032385826)\n",
      " state (9)  A[0]:(0.656336426735) A[1]:(0.81055021286) A[2]:(0.810279726982) A[3]:(0.000214099884033)\n",
      " state (10)  A[0]:(0.729365468025) A[1]:(0.900166392326) A[2]:(0.000176072120667) A[3]:(0.7291213274)\n",
      " state (11)  A[0]:(0.124162942171) A[1]:(0.875807464123) A[2]:(-0.906585276127) A[3]:(0.794722020626)\n",
      " state (12)  A[0]:(-0.429104089737) A[1]:(0.805183112621) A[2]:(-0.91513979435) A[3]:(0.712019205093)\n",
      " state (13)  A[0]:(0.000897541409358) A[1]:(0.808783888817) A[2]:(0.900003910065) A[3]:(0.729710459709)\n",
      " state (14)  A[0]:(0.810792386532) A[1]:(0.900157868862) A[2]:(0.999998271465) A[3]:(0.810251355171)\n",
      " state (15)  A[0]:(0.979517519474) A[1]:(0.942973613739) A[2]:(1.0) A[3]:(0.883255779743)\n",
      "Episode 304000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6225. Times reached goal: 916.               Steps done: 2458508. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0770063405543.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5905,  0.5905,  0.5306]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5900,  0.6561,  0.0005,  0.5308]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.0023,  0.7294,  0.5905]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6571,  0.8109,  0.8102,  0.0014]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8082,  0.9000,  0.7296]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8105,  0.8998,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53116106987) A[1]:(0.590646028519) A[2]:(0.590367794037) A[3]:(0.531181931496)\n",
      " state (1)  A[0]:(0.531179308891) A[1]:(0.000262856483459) A[2]:(0.656061530113) A[3]:(0.59065258503)\n",
      " state (2)  A[0]:(0.589903831482) A[1]:(0.729378700256) A[2]:(0.592935562134) A[3]:(0.656436562538)\n",
      " state (3)  A[0]:(0.657443761826) A[1]:(0.0130120320246) A[2]:(0.432624936104) A[3]:(0.557286679745)\n",
      " state (4)  A[0]:(0.58992421627) A[1]:(0.656330943108) A[2]:(0.000927090353798) A[3]:(0.531766474247)\n",
      " state (5)  A[0]:(-0.0870369896293) A[1]:(0.999883234501) A[2]:(-0.630645990372) A[3]:(0.595512270927)\n",
      " state (6)  A[0]:(8.59946012497e-05) A[1]:(0.810243606567) A[2]:(-0.000697016599588) A[3]:(0.655763506889)\n",
      " state (7)  A[0]:(0.571033716202) A[1]:(-0.59457296133) A[2]:(0.373608678579) A[3]:(0.889611780643)\n",
      " state (8)  A[0]:(0.655262351036) A[1]:(0.00147688284051) A[2]:(0.729321479797) A[3]:(0.590972423553)\n",
      " state (9)  A[0]:(0.656067609787) A[1]:(0.810694098473) A[2]:(0.810099959373) A[3]:(0.00075083959382)\n",
      " state (10)  A[0]:(0.728840708733) A[1]:(0.900145053864) A[2]:(-0.000380873651011) A[3]:(0.729278087616)\n",
      " state (11)  A[0]:(0.122585304081) A[1]:(0.875706613064) A[2]:(-0.906762778759) A[3]:(0.794689893723)\n",
      " state (12)  A[0]:(-0.430388599634) A[1]:(0.804912567139) A[2]:(-0.915341854095) A[3]:(0.71185362339)\n",
      " state (13)  A[0]:(-0.000327125191689) A[1]:(0.808398842812) A[2]:(0.900264918804) A[3]:(0.729539573193)\n",
      " state (14)  A[0]:(0.810515344143) A[1]:(0.899937689304) A[2]:(0.999998271465) A[3]:(0.810121536255)\n",
      " state (15)  A[0]:(0.979499280453) A[1]:(0.942821741104) A[2]:(1.0) A[3]:(0.883131980896)\n",
      "Episode 305000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6258. Times reached goal: 935.               Steps done: 2464766. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0765259396174.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531860649586) A[1]:(0.591534733772) A[2]:(0.591534495354) A[3]:(0.531703829765)\n",
      " state (1)  A[0]:(0.532125294209) A[1]:(-0.00014354288578) A[2]:(0.657184839249) A[3]:(0.590858578682)\n",
      " state (2)  A[0]:(0.590735912323) A[1]:(0.729922652245) A[2]:(0.594791591167) A[3]:(0.656805634499)\n",
      " state (3)  A[0]:(0.65839111805) A[1]:(0.0130948536098) A[2]:(0.435576140881) A[3]:(0.557960808277)\n",
      " state (4)  A[0]:(0.591016829014) A[1]:(0.656281650066) A[2]:(0.00336943776347) A[3]:(0.532379209995)\n",
      " state (5)  A[0]:(-0.0860180333257) A[1]:(0.999883532524) A[2]:(-0.631407499313) A[3]:(0.595683693886)\n",
      " state (6)  A[0]:(7.58171081543e-05) A[1]:(0.810316085815) A[2]:(0.000687241437845) A[3]:(0.656064271927)\n",
      " state (7)  A[0]:(0.570553898811) A[1]:(-0.594957113266) A[2]:(0.374955058098) A[3]:(0.889722824097)\n",
      " state (8)  A[0]:(0.655396282673) A[1]:(-0.00172497157473) A[2]:(0.729336500168) A[3]:(0.592325389385)\n",
      " state (9)  A[0]:(0.655245363712) A[1]:(0.809551596642) A[2]:(0.810102701187) A[3]:(-0.000992416986264)\n",
      " state (10)  A[0]:(0.72773194313) A[1]:(0.899677276611) A[2]:(-0.0015518652508) A[3]:(0.728237390518)\n",
      " state (11)  A[0]:(0.120397202671) A[1]:(0.875471293926) A[2]:(-0.907309710979) A[3]:(0.794425666332)\n",
      " state (12)  A[0]:(-0.431685060263) A[1]:(0.804926156998) A[2]:(-0.916090905666) A[3]:(0.71220266819)\n",
      " state (13)  A[0]:(-0.00144192471635) A[1]:(0.80851829052) A[2]:(0.899530529976) A[3]:(0.730280995369)\n",
      " state (14)  A[0]:(0.810250878334) A[1]:(0.900025784969) A[2]:(0.999998271465) A[3]:(0.810741305351)\n",
      " state (15)  A[0]:(0.979499518871) A[1]:(0.942858576775) A[2]:(1.0) A[3]:(0.88345015049)\n",
      "Episode 306000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6193. Times reached goal: 916.               Steps done: 2470959. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0760534789578.\n",
      " state (0)  A[0]:(0.531313538551) A[1]:(0.590855836868) A[2]:(0.590589106083) A[3]:(0.531234264374)\n",
      " state (1)  A[0]:(0.531399607658) A[1]:(-0.000102713704109) A[2]:(0.656340479851) A[3]:(0.590619683266)\n",
      " state (2)  A[0]:(0.589844465256) A[1]:(0.729197621346) A[2]:(0.593647241592) A[3]:(0.656698226929)\n",
      " state (3)  A[0]:(0.657626390457) A[1]:(0.0106149949133) A[2]:(0.434530347586) A[3]:(0.557724058628)\n",
      " state (4)  A[0]:(0.589937090874) A[1]:(0.656285226345) A[2]:(0.00104486907367) A[3]:(0.532102823257)\n",
      " state (5)  A[0]:(-0.0868562310934) A[1]:(0.999883770943) A[2]:(-0.633769750595) A[3]:(0.595941007137)\n",
      " state (6)  A[0]:(0.000449597806437) A[1]:(0.810046195984) A[2]:(-0.000459313363535) A[3]:(0.656082391739)\n",
      " state (7)  A[0]:(0.570999979973) A[1]:(-0.593800187111) A[2]:(0.37435656786) A[3]:(0.889551520348)\n",
      " state (8)  A[0]:(0.655945777893) A[1]:(0.000302836298943) A[2]:(0.729215860367) A[3]:(0.591484904289)\n",
      " state (9)  A[0]:(0.65700173378) A[1]:(0.810169041157) A[2]:(0.810235619545) A[3]:(0.000107169151306)\n",
      " state (10)  A[0]:(0.729966521263) A[1]:(0.900052011013) A[2]:(0.000319838523865) A[3]:(0.729099273682)\n",
      " state (11)  A[0]:(0.125649437308) A[1]:(0.87600928545) A[2]:(-0.906882226467) A[3]:(0.794906675816)\n",
      " state (12)  A[0]:(-0.428506404161) A[1]:(0.805694580078) A[2]:(-0.915811240673) A[3]:(0.712005376816)\n",
      " state (13)  A[0]:(0.000669762375765) A[1]:(0.809066653252) A[2]:(0.900154232979) A[3]:(0.729492425919)\n",
      " state (14)  A[0]:(0.810559630394) A[1]:(0.900165915489) A[2]:(0.99999833107) A[3]:(0.810025691986)\n",
      " state (15)  A[0]:(0.979519844055) A[1]:(0.942804813385) A[2]:(1.0) A[3]:(0.882975101471)\n",
      "Episode 307000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6184. Times reached goal: 931.               Steps done: 2477143. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0755846154641.\n",
      " state (0)  A[0]:(0.532619953156) A[1]:(0.590992689133) A[2]:(0.592731237411) A[3]:(0.532252788544)\n",
      " state (1)  A[0]:(0.532807707787) A[1]:(-0.000290334224701) A[2]:(0.658410668373) A[3]:(0.591384410858)\n",
      " state (2)  A[0]:(0.591188907623) A[1]:(0.729576230049) A[2]:(0.595958828926) A[3]:(0.657241344452)\n",
      " state (3)  A[0]:(0.658897042274) A[1]:(0.0115573620424) A[2]:(0.438228338957) A[3]:(0.558583498001)\n",
      " state (4)  A[0]:(0.591360151768) A[1]:(0.656811654568) A[2]:(0.00514133227989) A[3]:(0.53299933672)\n",
      " state (5)  A[0]:(-0.0851597860456) A[1]:(0.999884128571) A[2]:(-0.63262963295) A[3]:(0.596677541733)\n",
      " state (6)  A[0]:(0.00169551209547) A[1]:(0.809900820255) A[2]:(0.00360093940981) A[3]:(0.656695902348)\n",
      " state (7)  A[0]:(0.571780502796) A[1]:(-0.594821810722) A[2]:(0.378002434969) A[3]:(0.889855980873)\n",
      " state (8)  A[0]:(0.656586527824) A[1]:(-0.00141137745231) A[2]:(0.73036891222) A[3]:(0.592844903469)\n",
      " state (9)  A[0]:(0.656186163425) A[1]:(0.809891581535) A[2]:(0.810416221619) A[3]:(-0.000165343284607)\n",
      " state (10)  A[0]:(0.727610468864) A[1]:(0.899821996689) A[2]:(-0.00187933223788) A[3]:(0.728253006935)\n",
      " state (11)  A[0]:(0.118812941015) A[1]:(0.875707805157) A[2]:(-0.907688379288) A[3]:(0.79425984621)\n",
      " state (12)  A[0]:(-0.432955056429) A[1]:(0.805296421051) A[2]:(-0.916630923748) A[3]:(0.711984276772)\n",
      " state (13)  A[0]:(-0.00231091259047) A[1]:(0.808770656586) A[2]:(0.89950221777) A[3]:(0.730256199837)\n",
      " state (14)  A[0]:(0.810231089592) A[1]:(0.900162637234) A[2]:(0.99999833107) A[3]:(0.810826480389)\n",
      " state (15)  A[0]:(0.979539811611) A[1]:(0.942905306816) A[2]:(1.0) A[3]:(0.883440971375)\n",
      "Episode 308000 finished after 0 timesteps with r=1.0. Running score: 0.85. Times trained:               6173. Times reached goal: 914.               Steps done: 2483316. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0751194687851.\n",
      " state (0)  A[0]:(0.532784342766) A[1]:(0.588999390602) A[2]:(0.590138852596) A[3]:(0.53326022625)\n",
      " state (1)  A[0]:(0.532585322857) A[1]:(-0.000275209546089) A[2]:(0.65506696701) A[3]:(0.590984046459)\n",
      " state (2)  A[0]:(0.58978831768) A[1]:(0.728089928627) A[2]:(0.593540430069) A[3]:(0.656461060047)\n",
      " state (3)  A[0]:(0.656854152679) A[1]:(0.00590572366491) A[2]:(0.436008870602) A[3]:(0.557843327522)\n",
      " state (4)  A[0]:(0.588477134705) A[1]:(0.65448641777) A[2]:(0.00100278819446) A[3]:(0.532874763012)\n",
      " state (5)  A[0]:(-0.0872874855995) A[1]:(0.999883890152) A[2]:(-0.637547492981) A[3]:(0.597716212273)\n",
      " state (6)  A[0]:(6.56098127365e-05) A[1]:(0.809645354748) A[2]:(-0.00178932957351) A[3]:(0.655659079552)\n",
      " state (7)  A[0]:(0.570144176483) A[1]:(-0.593315243721) A[2]:(0.373681396246) A[3]:(0.888954222202)\n",
      " state (8)  A[0]:(0.654788315296) A[1]:(-0.00074847030919) A[2]:(0.728200078011) A[3]:(0.591182887554)\n",
      " state (9)  A[0]:(0.655077457428) A[1]:(0.809696793556) A[2]:(0.80947893858) A[3]:(0.000341832637787)\n",
      " state (10)  A[0]:(0.728515565395) A[1]:(0.89982932806) A[2]:(-0.00168871716596) A[3]:(0.729219675064)\n",
      " state (11)  A[0]:(0.123758561909) A[1]:(0.875948607922) A[2]:(-0.907467246056) A[3]:(0.79520046711)\n",
      " state (12)  A[0]:(-0.429724782705) A[1]:(0.805684030056) A[2]:(-0.916785299778) A[3]:(0.71230661869)\n",
      " state (13)  A[0]:(-0.000953852839302) A[1]:(0.808815598488) A[2]:(0.899080812931) A[3]:(0.729706048965)\n",
      " state (14)  A[0]:(0.80998814106) A[1]:(0.899951159954) A[2]:(0.99999833107) A[3]:(0.810153305531)\n",
      " state (15)  A[0]:(0.979474306107) A[1]:(0.942652404308) A[2]:(1.0) A[3]:(0.882911086082)\n",
      "Episode 309000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6164. Times reached goal: 928.               Steps done: 2489480. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0746578565301.\n",
      "q_values \n",
      "tensor([[ 0.5302,  0.5902,  0.5904,  0.5303]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5307,  0.0004,  0.6558,  0.5898]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5896,  0.7288,  0.5936,  0.6557]], device='cuda:0')\n",
      "On state=2, selected action=3 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5896,  0.7288,  0.5935,  0.6557]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0008,  0.8101, -0.0006,  0.6548]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7297,  0.9004,  0.0004,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8112,  0.9000,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530505537987) A[1]:(0.590166449547) A[2]:(0.590205729008) A[3]:(0.530485868454)\n",
      " state (1)  A[0]:(0.530922889709) A[1]:(0.00017137825489) A[2]:(0.655659198761) A[3]:(0.589872181416)\n",
      " state (2)  A[0]:(0.589787364006) A[1]:(0.728959441185) A[2]:(0.59342110157) A[3]:(0.65585398674)\n",
      " state (3)  A[0]:(0.657618224621) A[1]:(0.00589693197981) A[2]:(0.436655044556) A[3]:(0.556189417839)\n",
      " state (4)  A[0]:(0.589800834656) A[1]:(0.655953288078) A[2]:(0.000746726873331) A[3]:(0.53101170063)\n",
      " state (5)  A[0]:(-0.0845361500978) A[1]:(0.999884605408) A[2]:(-0.639082670212) A[3]:(0.597828149796)\n",
      " state (6)  A[0]:(0.000992893823422) A[1]:(0.809926867485) A[2]:(-0.000915765529498) A[3]:(0.65505194664)\n",
      " state (7)  A[0]:(0.570370256901) A[1]:(-0.592531919479) A[2]:(0.375326573849) A[3]:(0.888562560081)\n",
      " state (8)  A[0]:(0.65586745739) A[1]:(0.000570729316678) A[2]:(0.728946268559) A[3]:(0.590568423271)\n",
      " state (9)  A[0]:(0.656811952591) A[1]:(0.810293793678) A[2]:(0.809961259365) A[3]:(-0.000122845172882)\n",
      " state (10)  A[0]:(0.729792714119) A[1]:(0.900020837784) A[2]:(-0.00055789941689) A[3]:(0.728906691074)\n",
      " state (11)  A[0]:(0.126143246889) A[1]:(0.876014113426) A[2]:(-0.907323896885) A[3]:(0.794966459274)\n",
      " state (12)  A[0]:(-0.427431106567) A[1]:(0.805564582348) A[2]:(-0.916552484035) A[3]:(0.712137699127)\n",
      " state (13)  A[0]:(0.00253953994252) A[1]:(0.808568477631) A[2]:(0.900009810925) A[3]:(0.729769527912)\n",
      " state (14)  A[0]:(0.811334431171) A[1]:(0.899795651436) A[2]:(0.999998390675) A[3]:(0.810359120369)\n",
      " state (15)  A[0]:(0.979638338089) A[1]:(0.942516505718) A[2]:(1.0) A[3]:(0.883060216904)\n",
      "Episode 310000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6195. Times reached goal: 928.               Steps done: 2495675. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0741967807656.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531593084335) A[1]:(0.590716063976) A[2]:(0.591352105141) A[3]:(0.531466960907)\n",
      " state (1)  A[0]:(0.531853556633) A[1]:(-0.000507891119923) A[2]:(0.657451629639) A[3]:(0.590541362762)\n",
      " state (2)  A[0]:(0.590403199196) A[1]:(0.729140520096) A[2]:(0.595515072346) A[3]:(0.656608462334)\n",
      " state (3)  A[0]:(0.658346891403) A[1]:(0.00359861482866) A[2]:(0.439835995436) A[3]:(0.556603193283)\n",
      " state (4)  A[0]:(0.590501189232) A[1]:(0.65619134903) A[2]:(0.00279390090145) A[3]:(0.532267093658)\n",
      " state (5)  A[0]:(-0.0831335484982) A[1]:(0.999884963036) A[2]:(-0.640222251415) A[3]:(0.601750612259)\n",
      " state (6)  A[0]:(0.000729560735635) A[1]:(0.809948146343) A[2]:(0.000935673422646) A[3]:(0.656137526035)\n",
      " state (7)  A[0]:(0.569545924664) A[1]:(-0.592277526855) A[2]:(0.377296686172) A[3]:(0.888621032238)\n",
      " state (8)  A[0]:(0.655798077583) A[1]:(-0.00130993057974) A[2]:(0.728917598724) A[3]:(0.59264755249)\n",
      " state (9)  A[0]:(0.655348420143) A[1]:(0.809643030167) A[2]:(0.809594213963) A[3]:(-0.000186860561371)\n",
      " state (10)  A[0]:(0.7273914814) A[1]:(0.899833679199) A[2]:(-0.00328468088992) A[3]:(0.728229165077)\n",
      " state (11)  A[0]:(0.119949966669) A[1]:(0.876101613045) A[2]:(-0.908075869083) A[3]:(0.794556498528)\n",
      " state (12)  A[0]:(-0.432213187218) A[1]:(0.806094348431) A[2]:(-0.917207062244) A[3]:(0.711934149265)\n",
      " state (13)  A[0]:(-0.00260592414998) A[1]:(0.809298694134) A[2]:(0.899790763855) A[3]:(0.729742527008)\n",
      " state (14)  A[0]:(0.809724271297) A[1]:(0.900247573853) A[2]:(0.999998390675) A[3]:(0.810250282288)\n",
      " state (15)  A[0]:(0.979474723339) A[1]:(0.942754030228) A[2]:(1.0) A[3]:(0.882814705372)\n",
      "Episode 311000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6254. Times reached goal: 930.               Steps done: 2501929. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.07373420209.\n",
      " state (0)  A[0]:(0.531008780003) A[1]:(0.589731931686) A[2]:(0.58920443058) A[3]:(0.53108048439)\n",
      " state (1)  A[0]:(0.531107783318) A[1]:(-0.00114148808643) A[2]:(0.656247735023) A[3]:(0.589807748795)\n",
      " state (2)  A[0]:(0.589260697365) A[1]:(0.728819012642) A[2]:(0.595749139786) A[3]:(0.65573066473)\n",
      " state (3)  A[0]:(0.657290875912) A[1]:(0.00169402197935) A[2]:(0.441976428032) A[3]:(0.555371761322)\n",
      " state (4)  A[0]:(0.589577674866) A[1]:(0.655338287354) A[2]:(0.0045523326844) A[3]:(0.531856417656)\n",
      " state (5)  A[0]:(-0.0822367295623) A[1]:(0.999884903431) A[2]:(-0.642350912094) A[3]:(0.604003727436)\n",
      " state (6)  A[0]:(0.000523403228726) A[1]:(0.809585690498) A[2]:(0.000517010630574) A[3]:(0.655375957489)\n",
      " state (7)  A[0]:(0.569086134434) A[1]:(-0.591885507107) A[2]:(0.377322971821) A[3]:(0.887862980366)\n",
      " state (8)  A[0]:(0.656211912632) A[1]:(-0.00148314132821) A[2]:(0.728648304939) A[3]:(0.592185258865)\n",
      " state (9)  A[0]:(0.656619787216) A[1]:(0.809704065323) A[2]:(0.809777379036) A[3]:(0.00070190418046)\n",
      " state (10)  A[0]:(0.729670703411) A[1]:(0.899916112423) A[2]:(-0.000742792966776) A[3]:(0.729226469994)\n",
      " state (11)  A[0]:(0.126316294074) A[1]:(0.876322925091) A[2]:(-0.907531142235) A[3]:(0.79558712244)\n",
      " state (12)  A[0]:(-0.428128182888) A[1]:(0.80652320385) A[2]:(-0.916974365711) A[3]:(0.712646842003)\n",
      " state (13)  A[0]:(0.00043053922127) A[1]:(0.809723615646) A[2]:(0.900099277496) A[3]:(0.729796171188)\n",
      " state (14)  A[0]:(0.810423970222) A[1]:(0.900505781174) A[2]:(0.999998390675) A[3]:(0.810129582882)\n",
      " state (15)  A[0]:(0.979550361633) A[1]:(0.942906439304) A[2]:(1.0) A[3]:(0.882635295391)\n",
      "Episode 312000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6204. Times reached goal: 918.               Steps done: 2508133. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0732781711708.\n",
      " state (0)  A[0]:(0.531345129013) A[1]:(0.590686798096) A[2]:(0.591039896011) A[3]:(0.531589210033)\n",
      " state (1)  A[0]:(0.531331479549) A[1]:(3.73423099518e-05) A[2]:(0.656108438969) A[3]:(0.590741753578)\n",
      " state (2)  A[0]:(0.589656174183) A[1]:(0.728961586952) A[2]:(0.594842255116) A[3]:(0.656101703644)\n",
      " state (3)  A[0]:(0.657343506813) A[1]:(0.000395685405238) A[2]:(0.441973149776) A[3]:(0.554777383804)\n",
      " state (4)  A[0]:(0.589418053627) A[1]:(0.655523657799) A[2]:(0.00328301195987) A[3]:(0.531639456749)\n",
      " state (5)  A[0]:(-0.0812219753861) A[1]:(0.999885082245) A[2]:(-0.645591139793) A[3]:(0.606997966766)\n",
      " state (6)  A[0]:(-0.000634863914456) A[1]:(0.809472322464) A[2]:(-0.00131750025321) A[3]:(0.655696988106)\n",
      " state (7)  A[0]:(0.567223668098) A[1]:(-0.590958237648) A[2]:(0.376324594021) A[3]:(0.887397587299)\n",
      " state (8)  A[0]:(0.654800772667) A[1]:(-0.00208338792436) A[2]:(0.727886557579) A[3]:(0.591663062572)\n",
      " state (9)  A[0]:(0.655597507954) A[1]:(0.809096157551) A[2]:(0.809489309788) A[3]:(0.000886559253559)\n",
      " state (10)  A[0]:(0.729643642902) A[1]:(0.899624168873) A[2]:(-0.000347852706909) A[3]:(0.729824960232)\n",
      " state (11)  A[0]:(0.12760129571) A[1]:(0.876105487347) A[2]:(-0.907459020615) A[3]:(0.796389758587)\n",
      " state (12)  A[0]:(-0.427664250135) A[1]:(0.806214094162) A[2]:(-0.9171397686) A[3]:(0.713383078575)\n",
      " state (13)  A[0]:(-0.000472173065646) A[1]:(0.809246480465) A[2]:(0.899993777275) A[3]:(0.730039775372)\n",
      " state (14)  A[0]:(0.809692382812) A[1]:(0.900141596794) A[2]:(0.999998450279) A[3]:(0.810087859631)\n",
      " state (15)  A[0]:(0.97943764925) A[1]:(0.942622601986) A[2]:(1.0) A[3]:(0.88246512413)\n",
      "Episode 313000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6214. Times reached goal: 927.               Steps done: 2514347. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0728242324634.\n",
      " state (0)  A[0]:(0.531519889832) A[1]:(0.590188682079) A[2]:(0.59130781889) A[3]:(0.531494259834)\n",
      " state (1)  A[0]:(0.531828999519) A[1]:(-0.000310227274895) A[2]:(0.656915009022) A[3]:(0.590418636799)\n",
      " state (2)  A[0]:(0.590187370777) A[1]:(0.728957533836) A[2]:(0.595033705235) A[3]:(0.656170785427)\n",
      " state (3)  A[0]:(0.657938420773) A[1]:(-0.00172500137705) A[2]:(0.442809313536) A[3]:(0.554408073425)\n",
      " state (4)  A[0]:(0.590278983116) A[1]:(0.655791401863) A[2]:(0.00274502532557) A[3]:(0.531849145889)\n",
      " state (5)  A[0]:(-0.0778503715992) A[1]:(0.999885380268) A[2]:(-0.647501468658) A[3]:(0.609426617622)\n",
      " state (6)  A[0]:(0.00116309465375) A[1]:(0.809528887272) A[2]:(0.00114953471348) A[3]:(0.655609488487)\n",
      " state (7)  A[0]:(0.567942380905) A[1]:(-0.590046286583) A[2]:(0.379761457443) A[3]:(0.8870459795)\n",
      " state (8)  A[0]:(0.655857264996) A[1]:(-0.00131396879442) A[2]:(0.728996634483) A[3]:(0.591897964478)\n",
      " state (9)  A[0]:(0.655555844307) A[1]:(0.809439301491) A[2]:(0.809751927853) A[3]:(-0.000510156096425)\n",
      " state (10)  A[0]:(0.728054702282) A[1]:(0.899703681469) A[2]:(-0.0020071240142) A[3]:(0.728219032288)\n",
      " state (11)  A[0]:(0.122777499259) A[1]:(0.876065671444) A[2]:(-0.908116340637) A[3]:(0.794952511787)\n",
      " state (12)  A[0]:(-0.430117666721) A[1]:(0.805937170982) A[2]:(-0.917801380157) A[3]:(0.712137401104)\n",
      " state (13)  A[0]:(-0.000902354484424) A[1]:(0.808750987053) A[2]:(0.899387419224) A[3]:(0.729662358761)\n",
      " state (14)  A[0]:(0.810124695301) A[1]:(0.899855494499) A[2]:(0.999998450279) A[3]:(0.810154020786)\n",
      " state (15)  A[0]:(0.979527652264) A[1]:(0.942489326) A[2]:(1.0) A[3]:(0.882522881031)\n",
      "Episode 314000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6194. Times reached goal: 934.               Steps done: 2520541. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0723745532617.\n",
      "q_values \n",
      "tensor([[ 0.5308,  0.5903,  0.5908,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5311, -0.0006,  0.6562,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5899,  0.7287,  0.5943,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  0.8102, -0.0007,  0.6562]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7303,  0.9000,  0.0008,  0.7295]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9000,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531614542007) A[1]:(0.590307831764) A[2]:(0.590245008469) A[3]:(0.533286094666)\n",
      " state (1)  A[0]:(0.531682610512) A[1]:(3.27825546265e-06) A[2]:(0.655689835548) A[3]:(0.592702865601)\n",
      " state (2)  A[0]:(0.590417325497) A[1]:(0.728814423084) A[2]:(0.593800485134) A[3]:(0.658199846745)\n",
      " state (3)  A[0]:(0.657997488976) A[1]:(-0.00343208154663) A[2]:(0.442498087883) A[3]:(0.556121110916)\n",
      " state (4)  A[0]:(0.590077996254) A[1]:(0.656253695488) A[2]:(0.000866651302204) A[3]:(0.534028887749)\n",
      " state (5)  A[0]:(-0.0770422667265) A[1]:(0.999885797501) A[2]:(-0.651188135147) A[3]:(0.613910079002)\n",
      " state (6)  A[0]:(0.000563651265111) A[1]:(0.809793353081) A[2]:(-0.000876188045368) A[3]:(0.657004833221)\n",
      " state (7)  A[0]:(0.566722273827) A[1]:(-0.589022755623) A[2]:(0.379455566406) A[3]:(0.886819720268)\n",
      " state (8)  A[0]:(0.654842495918) A[1]:(0.000497281493153) A[2]:(0.729241728783) A[3]:(0.589824199677)\n",
      " state (9)  A[0]:(0.656336903572) A[1]:(0.809939801693) A[2]:(0.810035347939) A[3]:(-0.000293016433716)\n",
      " state (10)  A[0]:(0.729691743851) A[1]:(0.899937689304) A[2]:(-0.000328898429871) A[3]:(0.72930264473)\n",
      " state (11)  A[0]:(0.126575812697) A[1]:(0.876382887363) A[2]:(-0.90782135725) A[3]:(0.795910358429)\n",
      " state (12)  A[0]:(-0.428282737732) A[1]:(0.806400120258) A[2]:(-0.917677819729) A[3]:(0.712786078453)\n",
      " state (13)  A[0]:(-0.000206619501114) A[1]:(0.809154331684) A[2]:(0.899885773659) A[3]:(0.729729652405)\n",
      " state (14)  A[0]:(0.810124516487) A[1]:(0.900053501129) A[2]:(0.999998450279) A[3]:(0.810040652752)\n",
      " state (15)  A[0]:(0.979523122311) A[1]:(0.942548036575) A[2]:(1.0) A[3]:(0.882361888885)\n",
      "Episode 315000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6161. Times reached goal: 914.               Steps done: 2526702. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0719300244163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531932830811) A[1]:(0.590511083603) A[2]:(0.591182172298) A[3]:(0.531566083431)\n",
      " state (1)  A[0]:(0.531927108765) A[1]:(-8.04662704468e-06) A[2]:(0.656013131142) A[3]:(0.591054558754)\n",
      " state (2)  A[0]:(0.590115487576) A[1]:(0.728681325912) A[2]:(0.594430804253) A[3]:(0.657116889954)\n",
      " state (3)  A[0]:(0.657614707947) A[1]:(-0.00575172528625) A[2]:(0.444455593824) A[3]:(0.554547131062)\n",
      " state (4)  A[0]:(0.589631080627) A[1]:(0.655905723572) A[2]:(0.00126957823522) A[3]:(0.532808542252)\n",
      " state (5)  A[0]:(-0.0757975131273) A[1]:(0.999885857105) A[2]:(-0.654242396355) A[3]:(0.614408731461)\n",
      " state (6)  A[0]:(0.000257223844528) A[1]:(0.809825778008) A[2]:(-0.00102305377368) A[3]:(0.655497193336)\n",
      " state (7)  A[0]:(0.566159665585) A[1]:(-0.587534666061) A[2]:(0.380067557096) A[3]:(0.886129438877)\n",
      " state (8)  A[0]:(0.65530538559) A[1]:(0.00111371232197) A[2]:(0.728718996048) A[3]:(0.591031432152)\n",
      " state (9)  A[0]:(0.6557751894) A[1]:(0.810314297676) A[2]:(0.809816420078) A[3]:(-0.000588655413594)\n",
      " state (10)  A[0]:(0.728955149651) A[1]:(0.900137543678) A[2]:(-0.000697493436746) A[3]:(0.72863805294)\n",
      " state (11)  A[0]:(0.125378057361) A[1]:(0.876619398594) A[2]:(-0.907952845097) A[3]:(0.795462489128)\n",
      " state (12)  A[0]:(-0.429068356752) A[1]:(0.806722760201) A[2]:(-0.917826116085) A[3]:(0.712136983871)\n",
      " state (13)  A[0]:(-0.00085374689661) A[1]:(0.809420585632) A[2]:(0.900139153004) A[3]:(0.729115009308)\n",
      " state (14)  A[0]:(0.810046494007) A[1]:(0.900189995766) A[2]:(0.999998450279) A[3]:(0.809658885002)\n",
      " state (15)  A[0]:(0.979529738426) A[1]:(0.942590713501) A[2]:(1.0) A[3]:(0.882086634636)\n",
      "Episode 316000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6189. Times reached goal: 931.               Steps done: 2532891. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0714862242514.\n",
      " state (0)  A[0]:(0.53122150898) A[1]:(0.590751051903) A[2]:(0.590492248535) A[3]:(0.531193852425)\n",
      " state (1)  A[0]:(0.53113758564) A[1]:(0.00190658634529) A[2]:(0.656095743179) A[3]:(0.590189933777)\n",
      " state (2)  A[0]:(0.589663982391) A[1]:(0.729085206985) A[2]:(0.595025599003) A[3]:(0.655934512615)\n",
      " state (3)  A[0]:(0.65758150816) A[1]:(-0.00524282129481) A[2]:(0.446833640337) A[3]:(0.552881360054)\n",
      " state (4)  A[0]:(0.589826703072) A[1]:(0.656751930714) A[2]:(0.00293933507055) A[3]:(0.531395316124)\n",
      " state (5)  A[0]:(-0.0746859833598) A[1]:(0.999886333942) A[2]:(-0.656446635723) A[3]:(0.615170717239)\n",
      " state (6)  A[0]:(0.000508606375661) A[1]:(0.810045421124) A[2]:(-0.000921249156818) A[3]:(0.655857086182)\n",
      " state (7)  A[0]:(0.566107153893) A[1]:(-0.587786138058) A[2]:(0.381322979927) A[3]:(0.886105418205)\n",
      " state (8)  A[0]:(0.656018435955) A[1]:(0.000159457325935) A[2]:(0.729187905788) A[3]:(0.591193974018)\n",
      " state (9)  A[0]:(0.657340884209) A[1]:(0.81005769968) A[2]:(0.809998571873) A[3]:(0.000430464715464)\n",
      " state (10)  A[0]:(0.730359435081) A[1]:(0.899902582169) A[2]:(-0.000653505208902) A[3]:(0.729239761829)\n",
      " state (11)  A[0]:(0.128491774201) A[1]:(0.876280128956) A[2]:(-0.908083796501) A[3]:(0.795972287655)\n",
      " state (12)  A[0]:(-0.426171481609) A[1]:(0.806146681309) A[2]:(-0.918065369129) A[3]:(0.712810575962)\n",
      " state (13)  A[0]:(0.00299122044817) A[1]:(0.808812439442) A[2]:(0.900042533875) A[3]:(0.729782342911)\n",
      " state (14)  A[0]:(0.811378002167) A[1]:(0.899927735329) A[2]:(0.999998509884) A[3]:(0.810152173042)\n",
      " state (15)  A[0]:(0.979683756828) A[1]:(0.942492246628) A[2]:(1.0) A[3]:(0.882302999496)\n",
      "Episode 317000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6228. Times reached goal: 913.               Steps done: 2539119. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0710423915763.\n",
      " state (0)  A[0]:(0.53181540966) A[1]:(0.590525865555) A[2]:(0.590256214142) A[3]:(0.531583428383)\n",
      " state (1)  A[0]:(0.531857311726) A[1]:(0.000238567590714) A[2]:(0.65597474575) A[3]:(0.590323448181)\n",
      " state (2)  A[0]:(0.590016186237) A[1]:(0.729131340981) A[2]:(0.594339847565) A[3]:(0.656314134598)\n",
      " state (3)  A[0]:(0.65770304203) A[1]:(-0.00521377986297) A[2]:(0.447117805481) A[3]:(0.553276002407)\n",
      " state (4)  A[0]:(0.590054750443) A[1]:(0.656486749649) A[2]:(0.00239264499396) A[3]:(0.531849384308)\n",
      " state (5)  A[0]:(-0.0731633007526) A[1]:(0.999886572361) A[2]:(-0.658901810646) A[3]:(0.616353094578)\n",
      " state (6)  A[0]:(0.000553220452275) A[1]:(0.810241937637) A[2]:(-0.000676870229654) A[3]:(0.655740737915)\n",
      " state (7)  A[0]:(0.565553188324) A[1]:(-0.586501598358) A[2]:(0.382337093353) A[3]:(0.885825574398)\n",
      " state (8)  A[0]:(0.656009376049) A[1]:(0.00105085934047) A[2]:(0.729417145252) A[3]:(0.591601371765)\n",
      " state (9)  A[0]:(0.657316207886) A[1]:(0.810417830944) A[2]:(0.810333430767) A[3]:(0.00103437865619)\n",
      " state (10)  A[0]:(0.730303287506) A[1]:(0.900146603584) A[2]:(0.000774860207457) A[3]:(0.729556560516)\n",
      " state (11)  A[0]:(0.128073394299) A[1]:(0.876682758331) A[2]:(-0.907936811447) A[3]:(0.796304583549)\n",
      " state (12)  A[0]:(-0.427353858948) A[1]:(0.806812405586) A[2]:(-0.91816842556) A[3]:(0.713018417358)\n",
      " state (13)  A[0]:(0.000773563806433) A[1]:(0.809407234192) A[2]:(0.899972736835) A[3]:(0.729812860489)\n",
      " state (14)  A[0]:(0.810573935509) A[1]:(0.900233209133) A[2]:(0.999998509884) A[3]:(0.810169577599)\n",
      " state (15)  A[0]:(0.979604244232) A[1]:(0.942645668983) A[2]:(1.0) A[3]:(0.882282972336)\n",
      "Episode 318000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6207. Times reached goal: 928.               Steps done: 2545326. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0706027971445.\n",
      " state (0)  A[0]:(0.531841516495) A[1]:(0.588480293751) A[2]:(0.589731574059) A[3]:(0.531283080578)\n",
      " state (1)  A[0]:(0.531735420227) A[1]:(-8.4400177002e-05) A[2]:(0.655272722244) A[3]:(0.589745640755)\n",
      " state (2)  A[0]:(0.589066147804) A[1]:(0.728629052639) A[2]:(0.593765616417) A[3]:(0.655492901802)\n",
      " state (3)  A[0]:(0.656631231308) A[1]:(-0.00611273758113) A[2]:(0.447570264339) A[3]:(0.552443861961)\n",
      " state (4)  A[0]:(0.589005231857) A[1]:(0.656080842018) A[2]:(0.00206768210046) A[3]:(0.531620979309)\n",
      " state (5)  A[0]:(-0.0725315138698) A[1]:(0.999886631966) A[2]:(-0.661003172398) A[3]:(0.617604136467)\n",
      " state (6)  A[0]:(0.000911593204364) A[1]:(0.810018122196) A[2]:(-0.000285029411316) A[3]:(0.654350996017)\n",
      " state (7)  A[0]:(0.565461874008) A[1]:(-0.585674285889) A[2]:(0.38242483139) A[3]:(0.884946525097)\n",
      " state (8)  A[0]:(0.655972599983) A[1]:(-9.79900360107e-05) A[2]:(0.728283047676) A[3]:(0.592053771019)\n",
      " state (9)  A[0]:(0.655371665955) A[1]:(0.810094237328) A[2]:(0.809555947781) A[3]:(-0.000387012929423)\n",
      " state (10)  A[0]:(0.728313207626) A[1]:(0.900044441223) A[2]:(-0.00106334651355) A[3]:(0.72840064764)\n",
      " state (11)  A[0]:(0.124723985791) A[1]:(0.876711070538) A[2]:(-0.908227801323) A[3]:(0.79561316967)\n",
      " state (12)  A[0]:(-0.429332315922) A[1]:(0.807042181492) A[2]:(-0.918326735497) A[3]:(0.712189972401)\n",
      " state (13)  A[0]:(-0.000890969997272) A[1]:(0.809764683247) A[2]:(0.900318026543) A[3]:(0.729007124901)\n",
      " state (14)  A[0]:(0.810145735741) A[1]:(0.900503635406) A[2]:(0.999998509884) A[3]:(0.809524953365)\n",
      " state (15)  A[0]:(0.979558587074) A[1]:(0.942815124989) A[2]:(1.0) A[3]:(0.88175624609)\n",
      "Episode 319000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6126. Times reached goal: 922.               Steps done: 2551452. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0701716064946.\n",
      "q_values \n",
      "tensor([[ 0.5322,  0.5901,  0.5906,  0.5320]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5321, -0.0001,  0.6554,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5899,  0.7285,  0.5932,  0.6559]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0000,  0.8100, -0.0011,  0.6546]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7285,  0.9001, -0.0000,  0.7281]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.9001,  1.0000,  0.8096]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532172679901) A[1]:(0.590007781982) A[2]:(0.590427398682) A[3]:(0.532147943974)\n",
      " state (1)  A[0]:(0.531981408596) A[1]:(3.52263450623e-05) A[2]:(0.655285000801) A[3]:(0.590457201004)\n",
      " state (2)  A[0]:(0.589855909348) A[1]:(0.728543460369) A[2]:(0.593146443367) A[3]:(0.655982971191)\n",
      " state (3)  A[0]:(0.657078266144) A[1]:(-0.00621507503092) A[2]:(0.447810739279) A[3]:(0.552676677704)\n",
      " state (4)  A[0]:(0.589373588562) A[1]:(0.655518889427) A[2]:(0.00140857603401) A[3]:(0.532092630863)\n",
      " state (5)  A[0]:(-0.071805588901) A[1]:(0.999886572361) A[2]:(-0.663703024387) A[3]:(0.61989402771)\n",
      " state (6)  A[0]:(0.000371560425265) A[1]:(0.809683084488) A[2]:(-0.000959753699135) A[3]:(0.654860079288)\n",
      " state (7)  A[0]:(0.564491510391) A[1]:(-0.585066974163) A[2]:(0.382757365704) A[3]:(0.884586751461)\n",
      " state (8)  A[0]:(0.655471801758) A[1]:(-0.000236093997955) A[2]:(0.728594064713) A[3]:(0.590824902058)\n",
      " state (9)  A[0]:(0.656069040298) A[1]:(0.809677839279) A[2]:(0.809869706631) A[3]:(2.19345092773e-05)\n",
      " state (10)  A[0]:(0.729408323765) A[1]:(0.899830102921) A[2]:(0.000227212905884) A[3]:(0.72878921032)\n",
      " state (11)  A[0]:(0.127780422568) A[1]:(0.876527309418) A[2]:(-0.908049821854) A[3]:(0.79591345787)\n",
      " state (12)  A[0]:(-0.426726937294) A[1]:(0.806669950485) A[2]:(-0.918319702148) A[3]:(0.71245777607)\n",
      " state (13)  A[0]:(0.001678182045) A[1]:(0.809074878693) A[2]:(0.900381088257) A[3]:(0.729273974895)\n",
      " state (14)  A[0]:(0.810649394989) A[1]:(0.8999376297) A[2]:(0.999998569489) A[3]:(0.809834301472)\n",
      " state (15)  A[0]:(0.979584336281) A[1]:(0.942386984825) A[2]:(1.0) A[3]:(0.881993710995)\n",
      "Episode 320000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6176. Times reached goal: 934.               Steps done: 2557628. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.069739562179.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531934022903) A[1]:(0.589994907379) A[2]:(0.590466499329) A[3]:(0.53232383728)\n",
      " state (1)  A[0]:(0.531920731068) A[1]:(0.000291272997856) A[2]:(0.655816555023) A[3]:(0.591048240662)\n",
      " state (2)  A[0]:(0.589871942997) A[1]:(0.729204893112) A[2]:(0.593119502068) A[3]:(0.656980276108)\n",
      " state (3)  A[0]:(0.657174348831) A[1]:(-0.00427417829633) A[2]:(0.448330849409) A[3]:(0.553581655025)\n",
      " state (4)  A[0]:(0.589573740959) A[1]:(0.656447887421) A[2]:(0.000723719480447) A[3]:(0.532599687576)\n",
      " state (5)  A[0]:(-0.07106359303) A[1]:(0.999886929989) A[2]:(-0.665971159935) A[3]:(0.620620727539)\n",
      " state (6)  A[0]:(0.000591412128415) A[1]:(0.809808552265) A[2]:(-0.000342965126038) A[3]:(0.655785560608)\n",
      " state (7)  A[0]:(0.564375221729) A[1]:(-0.58572602272) A[2]:(0.384454488754) A[3]:(0.884863913059)\n",
      " state (8)  A[0]:(0.655498743057) A[1]:(-0.00174458150286) A[2]:(0.729201555252) A[3]:(0.590742111206)\n",
      " state (9)  A[0]:(0.656488656998) A[1]:(0.809287369251) A[2]:(0.810038745403) A[3]:(-0.000364840001566)\n",
      " state (10)  A[0]:(0.72977411747) A[1]:(0.899680912495) A[2]:(-0.000250697135925) A[3]:(0.729171812534)\n",
      " state (11)  A[0]:(0.127704501152) A[1]:(0.876559376717) A[2]:(-0.908396720886) A[3]:(0.796498119831)\n",
      " state (12)  A[0]:(-0.427838653326) A[1]:(0.807053744793) A[2]:(-0.918826878071) A[3]:(0.713161051273)\n",
      " state (13)  A[0]:(-0.000383511156542) A[1]:(0.809735655785) A[2]:(0.900060713291) A[3]:(0.729685902596)\n",
      " state (14)  A[0]:(0.809976637363) A[1]:(0.900442361832) A[2]:(0.999998569489) A[3]:(0.809913814068)\n",
      " state (15)  A[0]:(0.979532837868) A[1]:(0.942707002163) A[2]:(1.0) A[3]:(0.881872057915)\n",
      "Episode 321000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6186. Times reached goal: 931.               Steps done: 2563814. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.069309484848.\n",
      " state (0)  A[0]:(0.531215071678) A[1]:(0.590265274048) A[2]:(0.590372383595) A[3]:(0.531253814697)\n",
      " state (1)  A[0]:(0.531381428242) A[1]:(-0.000128537416458) A[2]:(0.656044840813) A[3]:(0.590597689152)\n",
      " state (2)  A[0]:(0.589936375618) A[1]:(0.728676795959) A[2]:(0.59327507019) A[3]:(0.656525969505)\n",
      " state (3)  A[0]:(0.657200098038) A[1]:(-0.00482534617186) A[2]:(0.449230223894) A[3]:(0.552782595158)\n",
      " state (4)  A[0]:(0.589719831944) A[1]:(0.655529022217) A[2]:(0.000824928109068) A[3]:(0.53165268898)\n",
      " state (5)  A[0]:(-0.0706064626575) A[1]:(0.999886989594) A[2]:(-0.668167114258) A[3]:(0.620842456818)\n",
      " state (6)  A[0]:(0.00033988058567) A[1]:(0.809916257858) A[2]:(-0.00112092448398) A[3]:(0.655899226665)\n",
      " state (7)  A[0]:(0.563973903656) A[1]:(-0.584388494492) A[2]:(0.384265333414) A[3]:(0.884814441204)\n",
      " state (8)  A[0]:(0.655322670937) A[1]:(-0.000173598527908) A[2]:(0.728945314884) A[3]:(0.591445446014)\n",
      " state (9)  A[0]:(0.655940949917) A[1]:(0.809919118881) A[2]:(0.810025990009) A[3]:(0.000167965888977)\n",
      " state (10)  A[0]:(0.729059696198) A[1]:(0.899963140488) A[2]:(-0.000131487846375) A[3]:(0.728843927383)\n",
      " state (11)  A[0]:(0.12632958591) A[1]:(0.876765429974) A[2]:(-0.908450067043) A[3]:(0.795990824699)\n",
      " state (12)  A[0]:(-0.428327858448) A[1]:(0.807048380375) A[2]:(-0.918973505497) A[3]:(0.71244341135)\n",
      " state (13)  A[0]:(-0.000208362936974) A[1]:(0.809364259243) A[2]:(0.899998605251) A[3]:(0.729282855988)\n",
      " state (14)  A[0]:(0.810140311718) A[1]:(0.900114417076) A[2]:(0.999998569489) A[3]:(0.8099142313)\n",
      " state (15)  A[0]:(0.979551494122) A[1]:(0.942479729652) A[2]:(1.0) A[3]:(0.881985485554)\n",
      "Episode 322000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6165. Times reached goal: 927.               Steps done: 2569979. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.068883506302.\n",
      " state (0)  A[0]:(0.531827032566) A[1]:(0.59026414156) A[2]:(0.590842783451) A[3]:(0.530566096306)\n",
      " state (1)  A[0]:(0.531636178493) A[1]:(-0.000216841697693) A[2]:(0.655960261822) A[3]:(0.590234160423)\n",
      " state (2)  A[0]:(0.590070843697) A[1]:(0.728421211243) A[2]:(0.592666387558) A[3]:(0.656290173531)\n",
      " state (3)  A[0]:(0.657163381577) A[1]:(-0.004291865509) A[2]:(0.449227929115) A[3]:(0.552706360817)\n",
      " state (4)  A[0]:(0.589424014091) A[1]:(0.655869185925) A[2]:(0.000219941139221) A[3]:(0.531605124474)\n",
      " state (5)  A[0]:(-0.071459338069) A[1]:(0.999887287617) A[2]:(-0.66975569725) A[3]:(0.620999693871)\n",
      " state (6)  A[0]:(-0.000324755907059) A[1]:(0.809973955154) A[2]:(-0.000764608208556) A[3]:(0.655177175999)\n",
      " state (7)  A[0]:(0.563412070274) A[1]:(-0.583605468273) A[2]:(0.38477241993) A[3]:(0.884314835072)\n",
      " state (8)  A[0]:(0.654900312424) A[1]:(-8.5711479187e-05) A[2]:(0.728848338127) A[3]:(0.590680003166)\n",
      " state (9)  A[0]:(0.65535569191) A[1]:(0.809851706028) A[2]:(0.809869766235) A[3]:(-0.000291407108307)\n",
      " state (10)  A[0]:(0.728324770927) A[1]:(0.899959802628) A[2]:(-0.000877976184711) A[3]:(0.728644549847)\n",
      " state (11)  A[0]:(0.124481663108) A[1]:(0.876854419708) A[2]:(-0.908692896366) A[3]:(0.795884609222)\n",
      " state (12)  A[0]:(-0.42987498641) A[1]:(0.807226061821) A[2]:(-0.919233262539) A[3]:(0.712345719337)\n",
      " state (13)  A[0]:(-0.00206824834459) A[1]:(0.809477448463) A[2]:(0.900056958199) A[3]:(0.72930085659)\n",
      " state (14)  A[0]:(0.809452176094) A[1]:(0.900139093399) A[2]:(0.999998569489) A[3]:(0.810023665428)\n",
      " state (15)  A[0]:(0.979467272758) A[1]:(0.942448973656) A[2]:(1.0) A[3]:(0.882068097591)\n",
      "Episode 323000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6115. Times reached goal: 910.               Steps done: 2576094. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.068463568928.\n",
      " state (0)  A[0]:(0.531366348267) A[1]:(0.590074300766) A[2]:(0.590907514095) A[3]:(0.531383514404)\n",
      " state (1)  A[0]:(0.531394124031) A[1]:(-0.000326007604599) A[2]:(0.656773805618) A[3]:(0.590477824211)\n",
      " state (2)  A[0]:(0.589956521988) A[1]:(0.728881597519) A[2]:(0.594083726406) A[3]:(0.656499564648)\n",
      " state (3)  A[0]:(0.657320976257) A[1]:(-0.00356848514639) A[2]:(0.451942533255) A[3]:(0.553149104118)\n",
      " state (4)  A[0]:(0.589947462082) A[1]:(0.655626296997) A[2]:(0.00232553062961) A[3]:(0.532230138779)\n",
      " state (5)  A[0]:(-0.0699721127748) A[1]:(0.999887287617) A[2]:(-0.671185135841) A[3]:(0.622308790684)\n",
      " state (6)  A[0]:(0.000834077422041) A[1]:(0.809622108936) A[2]:(7.15255737305e-07) A[3]:(0.656618237495)\n",
      " state (7)  A[0]:(0.564014732838) A[1]:(-0.584128856659) A[2]:(0.38553661108) A[3]:(0.88487046957)\n",
      " state (8)  A[0]:(0.655444741249) A[1]:(-0.00214346917346) A[2]:(0.728507816792) A[3]:(0.59270375967)\n",
      " state (9)  A[0]:(0.655182182789) A[1]:(0.809122800827) A[2]:(0.809193134308) A[3]:(0.00139659550041)\n",
      " state (10)  A[0]:(0.727454423904) A[1]:(0.899526596069) A[2]:(-0.00458487635478) A[3]:(0.729061245918)\n",
      " state (11)  A[0]:(0.122135199606) A[1]:(0.876362204552) A[2]:(-0.909653246403) A[3]:(0.796229660511)\n",
      " state (12)  A[0]:(-0.430937856436) A[1]:(0.806487977505) A[2]:(-0.92027503252) A[3]:(0.713111996651)\n",
      " state (13)  A[0]:(-0.00195616227575) A[1]:(0.808665037155) A[2]:(0.898819804192) A[3]:(0.730302929878)\n",
      " state (14)  A[0]:(0.809862613678) A[1]:(0.899721026421) A[2]:(0.999998569489) A[3]:(0.810787677765)\n",
      " state (15)  A[0]:(0.979550004005) A[1]:(0.942249774933) A[2]:(1.0) A[3]:(0.882438659668)\n",
      "Episode 324000 finished after 0 timesteps with r=1.0. Running score: 0.89. Times trained:               6187. Times reached goal: 930.               Steps done: 2582281. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0680412924863.\n",
      "q_values \n",
      "tensor([[ 0.5309,  0.5901,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5311, -0.0013,  0.6560,  0.5897]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5897,  0.7287,  0.5928,  0.6559]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0002,  0.8101, -0.0000,  0.6547]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7276,  0.9001, -0.0023,  0.7284]], device='cuda:0')\n",
      "On state=10, selected action=0 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.8097,  0.8096,  0.0015]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0025,  0.8087,  0.9002,  0.7301]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8114,  0.8997,  1.0000,  0.8104]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531605422497) A[1]:(0.590449690819) A[2]:(0.590230047703) A[3]:(0.532042264938)\n",
      " state (1)  A[0]:(0.531805753708) A[1]:(7.9944729805e-05) A[2]:(0.656044840813) A[3]:(0.590737104416)\n",
      " state (2)  A[0]:(0.590345740318) A[1]:(0.728975534439) A[2]:(0.592841744423) A[3]:(0.656952440739)\n",
      " state (3)  A[0]:(0.657663345337) A[1]:(-0.0018188188551) A[2]:(0.451418668032) A[3]:(0.553779125214)\n",
      " state (4)  A[0]:(0.590561151505) A[1]:(0.656011581421) A[2]:(0.00167727307416) A[3]:(0.532555818558)\n",
      " state (5)  A[0]:(-0.0687660425901) A[1]:(0.999887645245) A[2]:(-0.672719478607) A[3]:(0.622223973274)\n",
      " state (6)  A[0]:(0.00134424783755) A[1]:(0.809836924076) A[2]:(6.60419464111e-05) A[3]:(0.655982375145)\n",
      " state (7)  A[0]:(0.564269185066) A[1]:(-0.583174586296) A[2]:(0.38649648428) A[3]:(0.884396195412)\n",
      " state (8)  A[0]:(0.655780911446) A[1]:(-0.000491544546094) A[2]:(0.729569137096) A[3]:(0.590678930283)\n",
      " state (9)  A[0]:(0.657177567482) A[1]:(0.809619069099) A[2]:(0.810582876205) A[3]:(0.00104069674853)\n",
      " state (10)  A[0]:(0.731010317802) A[1]:(0.899716198444) A[2]:(0.00257205404341) A[3]:(0.730061769485)\n",
      " state (11)  A[0]:(0.131688654423) A[1]:(0.876522958279) A[2]:(-0.908185899258) A[3]:(0.797316372395)\n",
      " state (12)  A[0]:(-0.424431532621) A[1]:(0.806581795216) A[2]:(-0.9192507267) A[3]:(0.713772654533)\n",
      " state (13)  A[0]:(0.00358475698158) A[1]:(0.808672785759) A[2]:(0.900054931641) A[3]:(0.730121552944)\n",
      " state (14)  A[0]:(0.811219751835) A[1]:(0.899783432484) A[2]:(0.999998629093) A[3]:(0.810374677181)\n",
      " state (15)  A[0]:(0.979664504528) A[1]:(0.942328929901) A[2]:(1.0) A[3]:(0.882074832916)\n",
      "Episode 325000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6232. Times reached goal: 932.               Steps done: 2588513. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0676185776989.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53154027462) A[1]:(0.590387940407) A[2]:(0.590379953384) A[3]:(0.531897425652)\n",
      " state (1)  A[0]:(0.531356692314) A[1]:(-0.000424578756792) A[2]:(0.656071662903) A[3]:(0.59056854248)\n",
      " state (2)  A[0]:(0.589865088463) A[1]:(0.72878408432) A[2]:(0.593222498894) A[3]:(0.656413018703)\n",
      " state (3)  A[0]:(0.657228708267) A[1]:(-0.00173754815478) A[2]:(0.452456265688) A[3]:(0.553000330925)\n",
      " state (4)  A[0]:(0.589926958084) A[1]:(0.655940055847) A[2]:(0.00219022878446) A[3]:(0.531777262688)\n",
      " state (5)  A[0]:(-0.0704565793276) A[1]:(0.999887824059) A[2]:(-0.673992156982) A[3]:(0.62224316597)\n",
      " state (6)  A[0]:(-0.000609025300946) A[1]:(0.809912502766) A[2]:(5.8650970459e-05) A[3]:(0.655529022217)\n",
      " state (7)  A[0]:(0.562873363495) A[1]:(-0.582253217697) A[2]:(0.386366903782) A[3]:(0.884145498276)\n",
      " state (8)  A[0]:(0.655037641525) A[1]:(-0.000103428959846) A[2]:(0.72890329361) A[3]:(0.591065764427)\n",
      " state (9)  A[0]:(0.655488610268) A[1]:(0.810024857521) A[2]:(0.810211956501) A[3]:(-0.00121194066014)\n",
      " state (10)  A[0]:(0.729042410851) A[1]:(0.900092661381) A[2]:(0.00144767656457) A[3]:(0.7284745574)\n",
      " state (11)  A[0]:(0.126990631223) A[1]:(0.877173066139) A[2]:(-0.908493995667) A[3]:(0.796123683453)\n",
      " state (12)  A[0]:(-0.428447037935) A[1]:(0.807737350464) A[2]:(-0.91956871748) A[3]:(0.712245941162)\n",
      " state (13)  A[0]:(-0.00114990724251) A[1]:(0.809853553772) A[2]:(0.900071799755) A[3]:(0.728754639626)\n",
      " state (14)  A[0]:(0.809698164463) A[1]:(0.900427818298) A[2]:(0.999998629093) A[3]:(0.809418261051)\n",
      " state (15)  A[0]:(0.979505360126) A[1]:(0.9426664114) A[2]:(1.0) A[3]:(0.881445109844)\n",
      "Episode 326000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6160. Times reached goal: 924.               Steps done: 2594673. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0672033275438.\n",
      " state (0)  A[0]:(0.533047437668) A[1]:(0.590167045593) A[2]:(0.590222239494) A[3]:(0.531900167465)\n",
      " state (1)  A[0]:(0.532733142376) A[1]:(0.000301748514175) A[2]:(0.655562281609) A[3]:(0.590056538582)\n",
      " state (2)  A[0]:(0.590256512165) A[1]:(0.728528499603) A[2]:(0.593323528767) A[3]:(0.656068682671)\n",
      " state (3)  A[0]:(0.65717536211) A[1]:(-0.00373251503333) A[2]:(0.453049808741) A[3]:(0.552973270416)\n",
      " state (4)  A[0]:(0.589594721794) A[1]:(0.655102491379) A[2]:(0.00110614253208) A[3]:(0.53223246336)\n",
      " state (5)  A[0]:(-0.0697117820382) A[1]:(0.999887704849) A[2]:(-0.676894187927) A[3]:(0.623100161552)\n",
      " state (6)  A[0]:(7.0333480835e-05) A[1]:(0.809783995152) A[2]:(-0.00104570353869) A[3]:(0.654846191406)\n",
      " state (7)  A[0]:(0.562924325466) A[1]:(-0.580801665783) A[2]:(0.385983377695) A[3]:(0.883595764637)\n",
      " state (8)  A[0]:(0.654539823532) A[1]:(0.000939041085076) A[2]:(0.728391885757) A[3]:(0.590326845646)\n",
      " state (9)  A[0]:(0.654198765755) A[1]:(0.810100078583) A[2]:(0.809719800949) A[3]:(-0.00151836755686)\n",
      " state (10)  A[0]:(0.727516174316) A[1]:(0.900118172169) A[2]:(-0.000566363276448) A[3]:(0.728036403656)\n",
      " state (11)  A[0]:(0.123910494149) A[1]:(0.877222836018) A[2]:(-0.90892881155) A[3]:(0.795688688755)\n",
      " state (12)  A[0]:(-0.430015265942) A[1]:(0.807720422745) A[2]:(-0.919853746891) A[3]:(0.711825966835)\n",
      " state (13)  A[0]:(-0.00201834458858) A[1]:(0.809628367424) A[2]:(0.90029489994) A[3]:(0.728676319122)\n",
      " state (14)  A[0]:(0.809492766857) A[1]:(0.900173902512) A[2]:(0.999998629093) A[3]:(0.809586405754)\n",
      " state (15)  A[0]:(0.979476332664) A[1]:(0.942416965961) A[2]:(1.0) A[3]:(0.881615996361)\n",
      "Episode 327000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6161. Times reached goal: 935.               Steps done: 2600834. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0667905606768.\n",
      " state (0)  A[0]:(0.531742453575) A[1]:(0.59032022953) A[2]:(0.590728402138) A[3]:(0.531897902489)\n",
      " state (1)  A[0]:(0.531278133392) A[1]:(0.00172245327849) A[2]:(0.656081140041) A[3]:(0.590918064117)\n",
      " state (2)  A[0]:(0.589670419693) A[1]:(0.728915452957) A[2]:(0.592626154423) A[3]:(0.656747460365)\n",
      " state (3)  A[0]:(0.65701264143) A[1]:(-0.00220143445767) A[2]:(0.453008711338) A[3]:(0.553352117538)\n",
      " state (4)  A[0]:(0.58958286047) A[1]:(0.656303405762) A[2]:(0.000723838689737) A[3]:(0.532416820526)\n",
      " state (5)  A[0]:(-0.0699390023947) A[1]:(0.999888420105) A[2]:(-0.677964985371) A[3]:(0.624262034893)\n",
      " state (6)  A[0]:(-0.000510081590619) A[1]:(0.81032127142) A[2]:(-1.31130218506e-05) A[3]:(0.656305611134)\n",
      " state (7)  A[0]:(0.562706708908) A[1]:(-0.580973207951) A[2]:(0.387758940458) A[3]:(0.884167551994)\n",
      " state (8)  A[0]:(0.655330896378) A[1]:(0.00120246352162) A[2]:(0.72921705246) A[3]:(0.59195125103)\n",
      " state (9)  A[0]:(0.65635073185) A[1]:(0.810474634171) A[2]:(0.810085475445) A[3]:(0.00257080234587)\n",
      " state (10)  A[0]:(0.729336380959) A[1]:(0.900240838528) A[2]:(-0.000179529190063) A[3]:(0.730281591415)\n",
      " state (11)  A[0]:(0.126317843795) A[1]:(0.877306103706) A[2]:(-0.90913850069) A[3]:(0.797310948372)\n",
      " state (12)  A[0]:(-0.429530203342) A[1]:(0.807810306549) A[2]:(-0.920415580273) A[3]:(0.713542580605)\n",
      " state (13)  A[0]:(-0.00248443568125) A[1]:(0.809768438339) A[2]:(0.899477422237) A[3]:(0.729930520058)\n",
      " state (14)  A[0]:(0.809303820133) A[1]:(0.900404572487) A[2]:(0.999998629093) A[3]:(0.810298621655)\n",
      " state (15)  A[0]:(0.979472219944) A[1]:(0.942656457424) A[2]:(1.0) A[3]:(0.881914138794)\n",
      "Episode 328000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6218. Times reached goal: 946.               Steps done: 2607052. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0663765454777.\n",
      " state (0)  A[0]:(0.531412601471) A[1]:(0.590021371841) A[2]:(0.589866399765) A[3]:(0.53063929081)\n",
      " state (1)  A[0]:(0.531076610088) A[1]:(0.000578209699597) A[2]:(0.655770301819) A[3]:(0.589938342571)\n",
      " state (2)  A[0]:(0.589584290981) A[1]:(0.729020118713) A[2]:(0.592528641224) A[3]:(0.656254172325)\n",
      " state (3)  A[0]:(0.656635284424) A[1]:(-0.0011493708007) A[2]:(0.453595936298) A[3]:(0.552887141705)\n",
      " state (4)  A[0]:(0.589007377625) A[1]:(0.656165719032) A[2]:(0.000946521467995) A[3]:(0.531507492065)\n",
      " state (5)  A[0]:(-0.0705013871193) A[1]:(0.99988847971) A[2]:(-0.679427862167) A[3]:(0.622813344002)\n",
      " state (6)  A[0]:(-0.000640436890535) A[1]:(0.810249865055) A[2]:(-0.000642537954263) A[3]:(0.654789090157)\n",
      " state (7)  A[0]:(0.562101304531) A[1]:(-0.580466508865) A[2]:(0.387401551008) A[3]:(0.88320595026)\n",
      " state (8)  A[0]:(0.653835892677) A[1]:(0.000890776282176) A[2]:(0.729247689247) A[3]:(0.58768093586)\n",
      " state (9)  A[0]:(0.655099272728) A[1]:(0.810012757778) A[2]:(0.810287356377) A[3]:(-0.00257825269364)\n",
      " state (10)  A[0]:(0.729082882404) A[1]:(0.900046288967) A[2]:(0.00114071322605) A[3]:(0.72851729393)\n",
      " state (11)  A[0]:(0.127472117543) A[1]:(0.877251565456) A[2]:(-0.908877491951) A[3]:(0.796348273754)\n",
      " state (12)  A[0]:(-0.427982926369) A[1]:(0.807830750942) A[2]:(-0.920190930367) A[3]:(0.712454199791)\n",
      " state (13)  A[0]:(-0.000544518174138) A[1]:(0.809730947018) A[2]:(0.900227606297) A[3]:(0.729082345963)\n",
      " state (14)  A[0]:(0.809853136539) A[1]:(0.900321602821) A[2]:(0.999998688698) A[3]:(0.80988317728)\n",
      " state (15)  A[0]:(0.979522943497) A[1]:(0.942526817322) A[2]:(1.0) A[3]:(0.881750524044)\n",
      "Episode 329000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6200. Times reached goal: 931.               Steps done: 2613252. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0659662840205.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5906,  0.5898,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.6556,  0.0019,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6573, -0.0005,  0.7288,  0.5931]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6580,  0.8104,  0.8104,  0.0015]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0017,  0.8100,  0.9000,  0.7296]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9006,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531326651573) A[1]:(0.591036617756) A[2]:(0.590602636337) A[3]:(0.531316816807)\n",
      " state (1)  A[0]:(0.531121253967) A[1]:(0.000639080943074) A[2]:(0.656435668468) A[3]:(0.590247690678)\n",
      " state (2)  A[0]:(0.589711248875) A[1]:(0.728939294815) A[2]:(0.593696832657) A[3]:(0.656028151512)\n",
      " state (3)  A[0]:(0.657150745392) A[1]:(-0.00171840016264) A[2]:(0.455677568913) A[3]:(0.552623033524)\n",
      " state (4)  A[0]:(0.589914083481) A[1]:(0.655749440193) A[2]:(0.00241684447974) A[3]:(0.531306147575)\n",
      " state (5)  A[0]:(-0.0693570673466) A[1]:(0.999888300896) A[2]:(-0.680589079857) A[3]:(0.623016893864)\n",
      " state (6)  A[0]:(-0.000805616204161) A[1]:(0.809548735619) A[2]:(6.84261322021e-05) A[3]:(0.655251741409)\n",
      " state (7)  A[0]:(0.561954915524) A[1]:(-0.581581473351) A[2]:(0.388019800186) A[3]:(0.883620858192)\n",
      " state (8)  A[0]:(0.654488563538) A[1]:(-0.00164711324032) A[2]:(0.728507399559) A[3]:(0.590529203415)\n",
      " state (9)  A[0]:(0.654393076897) A[1]:(0.809546172619) A[2]:(0.809521496296) A[3]:(-0.00258916034363)\n",
      " state (10)  A[0]:(0.727561712265) A[1]:(0.899817883968) A[2]:(-0.00185847072862) A[3]:(0.727767348289)\n",
      " state (11)  A[0]:(0.123584039509) A[1]:(0.877037167549) A[2]:(-0.909542024136) A[3]:(0.795938014984)\n",
      " state (12)  A[0]:(-0.430649876595) A[1]:(0.807668924332) A[2]:(-0.92073905468) A[3]:(0.71232765913)\n",
      " state (13)  A[0]:(-0.00252898992039) A[1]:(0.809785187244) A[2]:(0.90007853508) A[3]:(0.729190826416)\n",
      " state (14)  A[0]:(0.809599816799) A[1]:(0.90051817894) A[2]:(0.999998688698) A[3]:(0.809931755066)\n",
      " state (15)  A[0]:(0.979529619217) A[1]:(0.942704260349) A[2]:(1.0) A[3]:(0.8816614151)\n",
      "Episode 330000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6187. Times reached goal: 945.               Steps done: 2619439. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0655594105821.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532457232475) A[1]:(0.589504003525) A[2]:(0.590725064278) A[3]:(0.531780362129)\n",
      " state (1)  A[0]:(0.532180309296) A[1]:(3.7282705307e-05) A[2]:(0.655657470226) A[3]:(0.590408980846)\n",
      " state (2)  A[0]:(0.590181648731) A[1]:(0.728869259357) A[2]:(0.592578291893) A[3]:(0.656409502029)\n",
      " state (3)  A[0]:(0.657236099243) A[1]:(0.000163644552231) A[2]:(0.4552295506) A[3]:(0.553703010082)\n",
      " state (4)  A[0]:(0.589912772179) A[1]:(0.65567690134) A[2]:(0.00231277523562) A[3]:(0.532153725624)\n",
      " state (5)  A[0]:(-0.0694208815694) A[1]:(0.999888658524) A[2]:(-0.681479096413) A[3]:(0.622719526291)\n",
      " state (6)  A[0]:(0.00057612353703) A[1]:(0.809930562973) A[2]:(-0.000338554382324) A[3]:(0.65545040369)\n",
      " state (7)  A[0]:(0.563438594341) A[1]:(-0.580628633499) A[2]:(0.387907713652) A[3]:(0.883720517159)\n",
      " state (8)  A[0]:(0.65568780899) A[1]:(0.000121206045151) A[2]:(0.728859186172) A[3]:(0.590627908707)\n",
      " state (9)  A[0]:(0.656543254852) A[1]:(0.810006797314) A[2]:(0.809819817543) A[3]:(0.00113493157551)\n",
      " state (10)  A[0]:(0.729424476624) A[1]:(0.899948120117) A[2]:(-0.000993251451291) A[3]:(0.729615330696)\n",
      " state (11)  A[0]:(0.12688036263) A[1]:(0.877071619034) A[2]:(-0.909492254257) A[3]:(0.796886026859)\n",
      " state (12)  A[0]:(-0.428452193737) A[1]:(0.807497680187) A[2]:(-0.920831024647) A[3]:(0.71298879385)\n",
      " state (13)  A[0]:(-0.000405594677432) A[1]:(0.809381246567) A[2]:(0.900161504745) A[3]:(0.729528784752)\n",
      " state (14)  A[0]:(0.810119986534) A[1]:(0.900230824947) A[2]:(0.999998688698) A[3]:(0.810104548931)\n",
      " state (15)  A[0]:(0.979571759701) A[1]:(0.942504644394) A[2]:(1.0) A[3]:(0.881737291813)\n",
      "Episode 331000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6210. Times reached goal: 930.               Steps done: 2625649. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0651535481496.\n",
      " state (0)  A[0]:(0.532581686974) A[1]:(0.590244054794) A[2]:(0.590553998947) A[3]:(0.53108716011)\n",
      " state (1)  A[0]:(0.532426595688) A[1]:(0.000356063217623) A[2]:(0.655922412872) A[3]:(0.590403020382)\n",
      " state (2)  A[0]:(0.590613245964) A[1]:(0.729282855988) A[2]:(0.592914104462) A[3]:(0.656509637833)\n",
      " state (3)  A[0]:(0.657638251781) A[1]:(0.000506505311932) A[2]:(0.455637991428) A[3]:(0.553742468357)\n",
      " state (4)  A[0]:(0.590103447437) A[1]:(0.656352162361) A[2]:(0.00151646020822) A[3]:(0.5319480896)\n",
      " state (5)  A[0]:(-0.0696914047003) A[1]:(0.999889016151) A[2]:(-0.683077692986) A[3]:(0.622417211533)\n",
      " state (6)  A[0]:(0.000408545107348) A[1]:(0.810139596462) A[2]:(-0.000136494636536) A[3]:(0.655617117882)\n",
      " state (7)  A[0]:(0.563142299652) A[1]:(-0.580583274364) A[2]:(0.388983905315) A[3]:(0.883754014969)\n",
      " state (8)  A[0]:(0.655510485172) A[1]:(-0.000700130942278) A[2]:(0.729164004326) A[3]:(0.590573787689)\n",
      " state (9)  A[0]:(0.655794739723) A[1]:(0.809787511826) A[2]:(0.809953987598) A[3]:(-0.000959932513069)\n",
      " state (10)  A[0]:(0.728206932545) A[1]:(0.899923324585) A[2]:(-0.00127756525762) A[3]:(0.72833275795)\n",
      " state (11)  A[0]:(0.123706810176) A[1]:(0.877182483673) A[2]:(-0.909729480743) A[3]:(0.796041488647)\n",
      " state (12)  A[0]:(-0.430822521448) A[1]:(0.807735681534) A[2]:(-0.92115175724) A[3]:(0.712277412415)\n",
      " state (13)  A[0]:(-0.00291234930046) A[1]:(0.809522211552) A[2]:(0.89999127388) A[3]:(0.729221045971)\n",
      " state (14)  A[0]:(0.809282422066) A[1]:(0.900259613991) A[2]:(0.999998688698) A[3]:(0.810021460056)\n",
      " state (15)  A[0]:(0.979479730129) A[1]:(0.942484557629) A[2]:(1.0) A[3]:(0.881699919701)\n",
      "Episode 332000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6180. Times reached goal: 924.               Steps done: 2631829. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0647521408481.\n",
      " state (0)  A[0]:(0.531419157982) A[1]:(0.590638160706) A[2]:(0.590422272682) A[3]:(0.53166437149)\n",
      " state (1)  A[0]:(0.531366109848) A[1]:(-0.000237300992012) A[2]:(0.656147360802) A[3]:(0.590511322021)\n",
      " state (2)  A[0]:(0.589863419533) A[1]:(0.729066133499) A[2]:(0.593276500702) A[3]:(0.656445741653)\n",
      " state (3)  A[0]:(0.657312750816) A[1]:(-0.000843837682623) A[2]:(0.456399857998) A[3]:(0.553502559662)\n",
      " state (4)  A[0]:(0.589833021164) A[1]:(0.656558513641) A[2]:(0.0010182854021) A[3]:(0.531575202942)\n",
      " state (5)  A[0]:(-0.0700578168035) A[1]:(0.99988925457) A[2]:(-0.684869766235) A[3]:(0.622096061707)\n",
      " state (6)  A[0]:(-0.000744461896829) A[1]:(0.810248792171) A[2]:(-0.000324964523315) A[3]:(0.655126094818)\n",
      " state (7)  A[0]:(0.562305808067) A[1]:(-0.580325901508) A[2]:(0.389438480139) A[3]:(0.883571147919)\n",
      " state (8)  A[0]:(0.655450224876) A[1]:(-0.00112302554771) A[2]:(0.729131817818) A[3]:(0.590746343136)\n",
      " state (9)  A[0]:(0.655929625034) A[1]:(0.809694170952) A[2]:(0.81011390686) A[3]:(-0.00178122334182)\n",
      " state (10)  A[0]:(0.728801608086) A[1]:(0.899834036827) A[2]:(-0.000190258026123) A[3]:(0.728118777275)\n",
      " state (11)  A[0]:(0.125753819942) A[1]:(0.877098679543) A[2]:(-0.909610927105) A[3]:(0.796165466309)\n",
      " state (12)  A[0]:(-0.429341584444) A[1]:(0.807663321495) A[2]:(-0.921257078648) A[3]:(0.712379574776)\n",
      " state (13)  A[0]:(-0.00140873994678) A[1]:(0.809527754784) A[2]:(0.899937331676) A[3]:(0.729140758514)\n",
      " state (14)  A[0]:(0.8098269701) A[1]:(0.90037882328) A[2]:(0.999998688698) A[3]:(0.809857964516)\n",
      " state (15)  A[0]:(0.979552388191) A[1]:(0.942617535591) A[2]:(1.0) A[3]:(0.881487846375)\n",
      "Episode 333000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6126. Times reached goal: 934.               Steps done: 2637955. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0643566817612.\n",
      " state (0)  A[0]:(0.531804084778) A[1]:(0.590324997902) A[2]:(0.590539157391) A[3]:(0.531256556511)\n",
      " state (1)  A[0]:(0.531848430634) A[1]:(-6.90966844559e-05) A[2]:(0.656172037125) A[3]:(0.590106368065)\n",
      " state (2)  A[0]:(0.590407729149) A[1]:(0.729465842247) A[2]:(0.593348860741) A[3]:(0.656208455563)\n",
      " state (3)  A[0]:(0.657709360123) A[1]:(-3.73870134354e-05) A[2]:(0.457068741322) A[3]:(0.553188204765)\n",
      " state (4)  A[0]:(0.590361773968) A[1]:(0.656097888947) A[2]:(0.00135278620292) A[3]:(0.53114426136)\n",
      " state (5)  A[0]:(-0.0689620450139) A[1]:(0.99988925457) A[2]:(-0.686000823975) A[3]:(0.622117757797)\n",
      " state (6)  A[0]:(0.000755295041017) A[1]:(0.810144066811) A[2]:(-0.000158548355103) A[3]:(0.655390024185)\n",
      " state (7)  A[0]:(0.563270926476) A[1]:(-0.579194068909) A[2]:(0.389501065016) A[3]:(0.883615911007)\n",
      " state (8)  A[0]:(0.655807554722) A[1]:(0.000484988064272) A[2]:(0.729000449181) A[3]:(0.590905189514)\n",
      " state (9)  A[0]:(0.655899405479) A[1]:(0.81025147438) A[2]:(0.809913218021) A[3]:(-0.000973701186012)\n",
      " state (10)  A[0]:(0.728461384773) A[1]:(0.900140166283) A[2]:(-0.00132477201987) A[3]:(0.728167176247)\n",
      " state (11)  A[0]:(0.124923184514) A[1]:(0.877464294434) A[2]:(-0.909900307655) A[3]:(0.795894980431)\n",
      " state (12)  A[0]:(-0.429471343756) A[1]:(0.80810123682) A[2]:(-0.921417891979) A[3]:(0.711824893951)\n",
      " state (13)  A[0]:(-0.000893771415576) A[1]:(0.809759259224) A[2]:(0.900392770767) A[3]:(0.728645026684)\n",
      " state (14)  A[0]:(0.81005191803) A[1]:(0.900366425514) A[2]:(0.999998748302) A[3]:(0.809568881989)\n",
      " state (15)  A[0]:(0.979576587677) A[1]:(0.942486703396) A[2]:(1.0) A[3]:(0.881322562695)\n",
      "Episode 334000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6182. Times reached goal: 946.               Steps done: 2644137. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.063960055988.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5901,  0.5906,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=3 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5902,  0.5906,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.0001,  0.6557,  0.5902]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5899,  0.7292,  0.5922,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0005,  0.8099, -0.0002,  0.6551]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0002,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53150087595) A[1]:(0.590216040611) A[2]:(0.590503454208) A[3]:(0.531767845154)\n",
      " state (1)  A[0]:(0.531519293785) A[1]:(0.00018784403801) A[2]:(0.655716240406) A[3]:(0.590361595154)\n",
      " state (2)  A[0]:(0.589682817459) A[1]:(0.729233562946) A[2]:(0.592134475708) A[3]:(0.65635484457)\n",
      " state (3)  A[0]:(0.656964838505) A[1]:(0.000432223052485) A[2]:(0.456166356802) A[3]:(0.553873896599)\n",
      " state (4)  A[0]:(0.589658498764) A[1]:(0.655763983727) A[2]:(7.65323638916e-05) A[3]:(0.531815648079)\n",
      " state (5)  A[0]:(-0.0694513693452) A[1]:(0.999889314175) A[2]:(-0.687493562698) A[3]:(0.622052907944)\n",
      " state (6)  A[0]:(0.000424742669566) A[1]:(0.809958577156) A[2]:(-0.0008325575036) A[3]:(0.65544629097)\n",
      " state (7)  A[0]:(0.562958240509) A[1]:(-0.578625679016) A[2]:(0.388905376196) A[3]:(0.883583128452)\n",
      " state (8)  A[0]:(0.655301094055) A[1]:(0.000352114409907) A[2]:(0.728571474552) A[3]:(0.590791106224)\n",
      " state (9)  A[0]:(0.655564308167) A[1]:(0.809881806374) A[2]:(0.809602737427) A[3]:(0.000129759311676)\n",
      " state (10)  A[0]:(0.728419542313) A[1]:(0.899952530861) A[2]:(-0.00197446090169) A[3]:(0.728912353516)\n",
      " state (11)  A[0]:(0.125336170197) A[1]:(0.877336204052) A[2]:(-0.910116434097) A[3]:(0.796534776688)\n",
      " state (12)  A[0]:(-0.429088622332) A[1]:(0.807907581329) A[2]:(-0.921871006489) A[3]:(0.712562918663)\n",
      " state (13)  A[0]:(-0.000737219932489) A[1]:(0.809405684471) A[2]:(0.899712920189) A[3]:(0.729243516922)\n",
      " state (14)  A[0]:(0.809947669506) A[1]:(0.900139093399) A[2]:(0.999998748302) A[3]:(0.809942662716)\n",
      " state (15)  A[0]:(0.979553043842) A[1]:(0.94236856699) A[2]:(1.0) A[3]:(0.881489872932)\n",
      "Episode 335000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6241. Times reached goal: 949.               Steps done: 2650378. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0635621243159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531221747398) A[1]:(0.590623140335) A[2]:(0.591555535793) A[3]:(0.53231137991)\n",
      " state (1)  A[0]:(0.53174674511) A[1]:(-1.93119049072e-05) A[2]:(0.657239735126) A[3]:(0.590945422649)\n",
      " state (2)  A[0]:(0.590302944183) A[1]:(0.729273438454) A[2]:(0.594506025314) A[3]:(0.65741622448)\n",
      " state (3)  A[0]:(0.657700479031) A[1]:(0.000984668382443) A[2]:(0.45998314023) A[3]:(0.555333137512)\n",
      " state (4)  A[0]:(0.590621352196) A[1]:(0.656204402447) A[2]:(0.00432023685426) A[3]:(0.532728433609)\n",
      " state (5)  A[0]:(-0.0679795295) A[1]:(0.999889552593) A[2]:(-0.686804771423) A[3]:(0.621558904648)\n",
      " state (6)  A[0]:(0.00130492378958) A[1]:(0.810085773468) A[2]:(0.00267135468312) A[3]:(0.655675053596)\n",
      " state (7)  A[0]:(0.563603401184) A[1]:(-0.578281104565) A[2]:(0.39214104414) A[3]:(0.883807122707)\n",
      " state (8)  A[0]:(0.656525969505) A[1]:(0.000923394865822) A[2]:(0.729886412621) A[3]:(0.591721594334)\n",
      " state (9)  A[0]:(0.656390845776) A[1]:(0.810546636581) A[2]:(0.810536265373) A[3]:(-0.00172143999953)\n",
      " state (10)  A[0]:(0.728612780571) A[1]:(0.900323688984) A[2]:(0.000415325135691) A[3]:(0.727353036404)\n",
      " state (11)  A[0]:(0.125293120742) A[1]:(0.877764225006) A[2]:(-0.909811317921) A[3]:(0.795267939568)\n",
      " state (12)  A[0]:(-0.428663104773) A[1]:(0.808548688889) A[2]:(-0.921556651592) A[3]:(0.711082458496)\n",
      " state (13)  A[0]:(0.000901594525203) A[1]:(0.810117959976) A[2]:(0.900719940662) A[3]:(0.728150844574)\n",
      " state (14)  A[0]:(0.810860335827) A[1]:(0.900628328323) A[2]:(0.999998748302) A[3]:(0.809341669083)\n",
      " state (15)  A[0]:(0.979683816433) A[1]:(0.942671716213) A[2]:(1.0) A[3]:(0.881157398224)\n",
      "Episode 336000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6184. Times reached goal: 959.               Steps done: 2656562. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0631702690046.\n",
      " state (0)  A[0]:(0.531626105309) A[1]:(0.590825080872) A[2]:(0.590560674667) A[3]:(0.53156375885)\n",
      " state (1)  A[0]:(0.531741976738) A[1]:(-9.71555709839e-06) A[2]:(0.656147181988) A[3]:(0.590388953686)\n",
      " state (2)  A[0]:(0.590483188629) A[1]:(0.729018330574) A[2]:(0.592931449413) A[3]:(0.656306445599)\n",
      " state (3)  A[0]:(0.657688975334) A[1]:(0.000343859195709) A[2]:(0.458313435316) A[3]:(0.554185688496)\n",
      " state (4)  A[0]:(0.590145587921) A[1]:(0.656365156174) A[2]:(0.00108897639439) A[3]:(0.531621694565)\n",
      " state (5)  A[0]:(-0.0694028958678) A[1]:(0.999889731407) A[2]:(-0.689708948135) A[3]:(0.620907783508)\n",
      " state (6)  A[0]:(0.000302150845528) A[1]:(0.809821307659) A[2]:(-0.00073897826951) A[3]:(0.655702829361)\n",
      " state (7)  A[0]:(0.562720954418) A[1]:(-0.578357219696) A[2]:(0.389472544193) A[3]:(0.88373875618)\n",
      " state (8)  A[0]:(0.655311405659) A[1]:(0.000256836414337) A[2]:(0.728642582893) A[3]:(0.590889811516)\n",
      " state (9)  A[0]:(0.656021118164) A[1]:(0.810036659241) A[2]:(0.809883356094) A[3]:(-0.000301480293274)\n",
      " state (10)  A[0]:(0.729366660118) A[1]:(0.900016844273) A[2]:(4.83989715576e-05) A[3]:(0.728957116604)\n",
      " state (11)  A[0]:(0.127897232771) A[1]:(0.877416312695) A[2]:(-0.909847736359) A[3]:(0.796759605408)\n",
      " state (12)  A[0]:(-0.427645772696) A[1]:(0.807888865471) A[2]:(-0.921919584274) A[3]:(0.712533593178)\n",
      " state (13)  A[0]:(0.000336736440659) A[1]:(0.809181928635) A[2]:(0.900092124939) A[3]:(0.728935003281)\n",
      " state (14)  A[0]:(0.810280382633) A[1]:(0.900002837181) A[2]:(0.999998748302) A[3]:(0.809655487537)\n",
      " state (15)  A[0]:(0.979596614838) A[1]:(0.942259788513) A[2]:(1.0) A[3]:(0.881236374378)\n",
      "Episode 337000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6139. Times reached goal: 922.               Steps done: 2662701. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0627836546501.\n",
      " state (0)  A[0]:(0.53219628334) A[1]:(0.590562641621) A[2]:(0.590271234512) A[3]:(0.531822383404)\n",
      " state (1)  A[0]:(0.532001614571) A[1]:(3.56584787369e-05) A[2]:(0.65602183342) A[3]:(0.590170860291)\n",
      " state (2)  A[0]:(0.589774787426) A[1]:(0.728842437267) A[2]:(0.59327352047) A[3]:(0.656193852425)\n",
      " state (3)  A[0]:(0.657166004181) A[1]:(-0.000915869837627) A[2]:(0.459335416555) A[3]:(0.554227113724)\n",
      " state (4)  A[0]:(0.589870333672) A[1]:(0.655577421188) A[2]:(0.00134909071494) A[3]:(0.531876683235)\n",
      " state (5)  A[0]:(-0.0686930790544) A[1]:(0.999889731407) A[2]:(-0.691358566284) A[3]:(0.621345996857)\n",
      " state (6)  A[0]:(8.28504562378e-05) A[1]:(0.80982285738) A[2]:(-0.00107538653538) A[3]:(0.655903577805)\n",
      " state (7)  A[0]:(0.562365293503) A[1]:(-0.578448414803) A[2]:(0.389811366796) A[3]:(0.883835375309)\n",
      " state (8)  A[0]:(0.655314445496) A[1]:(-0.000843152229208) A[2]:(0.728570282459) A[3]:(0.59179276228)\n",
      " state (9)  A[0]:(0.655876517296) A[1]:(0.809656083584) A[2]:(0.809884488583) A[3]:(0.000341951847076)\n",
      " state (10)  A[0]:(0.729307711124) A[1]:(0.899850487709) A[2]:(-1.19209289551e-06) A[3]:(0.729448080063)\n",
      " state (11)  A[0]:(0.127870514989) A[1]:(0.877345919609) A[2]:(-0.909970939159) A[3]:(0.797407090664)\n",
      " state (12)  A[0]:(-0.427863776684) A[1]:(0.807964980602) A[2]:(-0.922148704529) A[3]:(0.713469445705)\n",
      " state (13)  A[0]:(-0.000259831547737) A[1]:(0.809412360191) A[2]:(0.900038838387) A[3]:(0.729708433151)\n",
      " state (14)  A[0]:(0.809966087341) A[1]:(0.900256812572) A[2]:(0.999998807907) A[3]:(0.810059249401)\n",
      " state (15)  A[0]:(0.97955173254) A[1]:(0.942467391491) A[2]:(1.0) A[3]:(0.881340324879)\n",
      "Episode 338000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6141. Times reached goal: 923.               Steps done: 2668842. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0623992816521.\n",
      " state (0)  A[0]:(0.531263947487) A[1]:(0.590267717838) A[2]:(0.590470790863) A[3]:(0.531461536884)\n",
      " state (1)  A[0]:(0.531302690506) A[1]:(-4.92185354233e-05) A[2]:(0.655954480171) A[3]:(0.590364873409)\n",
      " state (2)  A[0]:(0.589682579041) A[1]:(0.728801190853) A[2]:(0.592849135399) A[3]:(0.656209647655)\n",
      " state (3)  A[0]:(0.656902134418) A[1]:(-0.00104914570693) A[2]:(0.459435492754) A[3]:(0.553965210915)\n",
      " state (4)  A[0]:(0.58928924799) A[1]:(0.656131148338) A[2]:(0.000830530945677) A[3]:(0.531535625458)\n",
      " state (5)  A[0]:(-0.0693953037262) A[1]:(0.99989002943) A[2]:(-0.692566394806) A[3]:(0.621633589268)\n",
      " state (6)  A[0]:(-0.000791102473158) A[1]:(0.80987226963) A[2]:(-0.000692963483743) A[3]:(0.655531585217)\n",
      " state (7)  A[0]:(0.561595320702) A[1]:(-0.578241705894) A[2]:(0.390668720007) A[3]:(0.883438885212)\n",
      " state (8)  A[0]:(0.654584288597) A[1]:(-0.000207290053368) A[2]:(0.728910923004) A[3]:(0.590065121651)\n",
      " state (9)  A[0]:(0.655474185944) A[1]:(0.809922337532) A[2]:(0.80988162756) A[3]:(-0.00126767088659)\n",
      " state (10)  A[0]:(0.728814601898) A[1]:(0.899900734425) A[2]:(-0.0005737542524) A[3]:(0.728678643703)\n",
      " state (11)  A[0]:(0.126429051161) A[1]:(0.877337574959) A[2]:(-0.910210072994) A[3]:(0.796750426292)\n",
      " state (12)  A[0]:(-0.428851038218) A[1]:(0.807860553265) A[2]:(-0.922452569008) A[3]:(0.712671041489)\n",
      " state (13)  A[0]:(-0.000751063111238) A[1]:(0.809266865253) A[2]:(0.899922251701) A[3]:(0.72919100523)\n",
      " state (14)  A[0]:(0.810048162937) A[1]:(0.900243103504) A[2]:(0.999998807907) A[3]:(0.809880614281)\n",
      " state (15)  A[0]:(0.979579746723) A[1]:(0.942494869232) A[2]:(1.0) A[3]:(0.881276249886)\n",
      "Episode 339000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6207. Times reached goal: 939.               Steps done: 2675049. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0620131688517.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5915,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.6572,  0.0020,  0.5318]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6569,  0.0024,  0.5319]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6555,  0.0011,  0.7295,  0.5926]], device='cuda:0')\n",
      "On state=8, selected action=0 , Random? True\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6548, -0.0011,  0.7289,  0.5919]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6551,  0.8095,  0.8099,  0.0004]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 7.2825e-01,  8.9990e-01,  7.8678e-06,  7.2896e-01]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9005,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531921625137) A[1]:(0.591603696346) A[2]:(0.590859651566) A[3]:(0.531509399414)\n",
      " state (1)  A[0]:(0.53162831068) A[1]:(0.000779092137236) A[2]:(0.656367540359) A[3]:(0.590461611748)\n",
      " state (2)  A[0]:(0.590328335762) A[1]:(0.730091273785) A[2]:(0.593259930611) A[3]:(0.656358420849)\n",
      " state (3)  A[0]:(0.65771895647) A[1]:(0.00190871721134) A[2]:(0.460851877928) A[3]:(0.554184913635)\n",
      " state (4)  A[0]:(0.59071367979) A[1]:(0.656972467899) A[2]:(0.00217592366971) A[3]:(0.531676769257)\n",
      " state (5)  A[0]:(-0.0667936205864) A[1]:(0.999890863895) A[2]:(-0.693838119507) A[3]:(0.621985435486)\n",
      " state (6)  A[0]:(-6.29723072052e-05) A[1]:(0.811221957207) A[2]:(-0.00116634310689) A[3]:(0.655764520168)\n",
      " state (7)  A[0]:(0.561444044113) A[1]:(-0.576445996761) A[2]:(0.39107221365) A[3]:(0.883426189423)\n",
      " state (8)  A[0]:(0.65433883667) A[1]:(0.001621900592) A[2]:(0.729171812534) A[3]:(0.590241789818)\n",
      " state (9)  A[0]:(0.654939353466) A[1]:(0.81048977375) A[2]:(0.810147762299) A[3]:(-0.000990628846921)\n",
      " state (10)  A[0]:(0.728148341179) A[1]:(0.90016824007) A[2]:(-2.07424163818e-05) A[3]:(0.728682100773)\n",
      " state (11)  A[0]:(0.124902166426) A[1]:(0.877646684647) A[2]:(-0.910177886486) A[3]:(0.796718776226)\n",
      " state (12)  A[0]:(-0.429699391127) A[1]:(0.808297753334) A[2]:(-0.922323822975) A[3]:(0.712659358978)\n",
      " state (13)  A[0]:(-0.00104288721923) A[1]:(0.809670507908) A[2]:(0.900790095329) A[3]:(0.729220151901)\n",
      " state (14)  A[0]:(0.810126662254) A[1]:(0.900442659855) A[2]:(0.999998807907) A[3]:(0.809893488884)\n",
      " state (15)  A[0]:(0.979598224163) A[1]:(0.942546129227) A[2]:(1.0) A[3]:(0.881244421005)\n",
      "Episode 340000 finished after 0 timesteps with r=1.0. Running score: 0.88. Times trained:               6167. Times reached goal: 936.               Steps done: 2681216. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0616319104579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531404018402) A[1]:(0.590294361115) A[2]:(0.589737713337) A[3]:(0.531356036663)\n",
      " state (1)  A[0]:(0.531183362007) A[1]:(9.5322728157e-05) A[2]:(0.655615210533) A[3]:(0.590064108372)\n",
      " state (2)  A[0]:(0.589786052704) A[1]:(0.729160428047) A[2]:(0.592602729797) A[3]:(0.656280517578)\n",
      " state (3)  A[0]:(0.657022356987) A[1]:(-0.000441119045718) A[2]:(0.460548073053) A[3]:(0.55418074131)\n",
      " state (4)  A[0]:(0.58970618248) A[1]:(0.655651390553) A[2]:(0.00109064532444) A[3]:(0.531455338001)\n",
      " state (5)  A[0]:(-0.0678895711899) A[1]:(0.999890208244) A[2]:(-0.695346534252) A[3]:(0.6213504076)\n",
      " state (6)  A[0]:(-0.000106289982796) A[1]:(0.809872865677) A[2]:(-0.00141966249794) A[3]:(0.655406594276)\n",
      " state (7)  A[0]:(0.56170630455) A[1]:(-0.577185809612) A[2]:(0.390728026628) A[3]:(0.883280873299)\n",
      " state (8)  A[0]:(0.654710292816) A[1]:(0.00015015900135) A[2]:(0.728846013546) A[3]:(0.589888453484)\n",
      " state (9)  A[0]:(0.655883967876) A[1]:(0.809868335724) A[2]:(0.809996366501) A[3]:(-0.000513255537953)\n",
      " state (10)  A[0]:(0.729467570782) A[1]:(0.899894416332) A[2]:(0.000216960906982) A[3]:(0.729269206524)\n",
      " state (11)  A[0]:(0.128340035677) A[1]:(0.877427458763) A[2]:(-0.910210549831) A[3]:(0.79733222723)\n",
      " state (12)  A[0]:(-0.427396774292) A[1]:(0.807918965816) A[2]:(-0.922684550285) A[3]:(0.713280558586)\n",
      " state (13)  A[0]:(0.000500321330037) A[1]:(0.809009253979) A[2]:(0.900020897388) A[3]:(0.729628920555)\n",
      " state (14)  A[0]:(0.810250520706) A[1]:(0.89994263649) A[2]:(0.999998807907) A[3]:(0.810157775879)\n",
      " state (15)  A[0]:(0.979590952396) A[1]:(0.942208170891) A[2]:(1.0) A[3]:(0.88138461113)\n",
      "Episode 341000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6170. Times reached goal: 945.               Steps done: 2687386. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0612528122909.\n",
      " state (0)  A[0]:(0.532500743866) A[1]:(0.591546714306) A[2]:(0.590531110764) A[3]:(0.532046437263)\n",
      " state (1)  A[0]:(0.53241288662) A[1]:(0.000410020322306) A[2]:(0.656500101089) A[3]:(0.59062230587)\n",
      " state (2)  A[0]:(0.590598940849) A[1]:(0.729425191879) A[2]:(0.594158053398) A[3]:(0.656538486481)\n",
      " state (3)  A[0]:(0.658031821251) A[1]:(-0.000904142623767) A[2]:(0.463219702244) A[3]:(0.554568111897)\n",
      " state (4)  A[0]:(0.590910434723) A[1]:(0.656264066696) A[2]:(0.00309990835376) A[3]:(0.53201341629)\n",
      " state (5)  A[0]:(-0.0660883337259) A[1]:(0.999890506268) A[2]:(-0.696170866489) A[3]:(0.622042179108)\n",
      " state (6)  A[0]:(0.000455170840723) A[1]:(0.809850871563) A[2]:(0.000237345695496) A[3]:(0.655486762524)\n",
      " state (7)  A[0]:(0.561893641949) A[1]:(-0.577268719673) A[2]:(0.39254617691) A[3]:(0.883300423622)\n",
      " state (8)  A[0]:(0.655639708042) A[1]:(-0.000877916580066) A[2]:(0.729078173637) A[3]:(0.590920090675)\n",
      " state (9)  A[0]:(0.656211853027) A[1]:(0.809762299061) A[2]:(0.809994161129) A[3]:(-0.00188851135317)\n",
      " state (10)  A[0]:(0.729139089584) A[1]:(0.899845778942) A[2]:(-0.000436544389231) A[3]:(0.728239655495)\n",
      " state (11)  A[0]:(0.127064853907) A[1]:(0.87740778923) A[2]:(-0.91049349308) A[3]:(0.796778559685)\n",
      " state (12)  A[0]:(-0.428225427866) A[1]:(0.807950496674) A[2]:(-0.923053920269) A[3]:(0.712939500809)\n",
      " state (13)  A[0]:(0.000151976943016) A[1]:(0.809091329575) A[2]:(0.89966326952) A[3]:(0.729542553425)\n",
      " state (14)  A[0]:(0.810357093811) A[1]:(0.900088250637) A[2]:(0.999998807907) A[3]:(0.810109078884)\n",
      " state (15)  A[0]:(0.979624092579) A[1]:(0.942363917828) A[2]:(1.0) A[3]:(0.881252765656)\n",
      "Episode 342000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6062. Times reached goal: 930.               Steps done: 2693448. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0608826209265.\n",
      " state (0)  A[0]:(0.531352698803) A[1]:(0.590839505196) A[2]:(0.590741991997) A[3]:(0.531521558762)\n",
      " state (1)  A[0]:(0.531431078911) A[1]:(0.000400096148951) A[2]:(0.656271100044) A[3]:(0.590483188629)\n",
      " state (2)  A[0]:(0.590091586113) A[1]:(0.729198515415) A[2]:(0.592431008816) A[3]:(0.656433343887)\n",
      " state (3)  A[0]:(0.657276988029) A[1]:(-0.00146338238847) A[2]:(0.461344659328) A[3]:(0.554108917713)\n",
      " state (4)  A[0]:(0.589753866196) A[1]:(0.656535387039) A[2]:(0.000127553939819) A[3]:(0.531473577023)\n",
      " state (5)  A[0]:(-0.067196212709) A[1]:(0.999890804291) A[2]:(-0.698035955429) A[3]:(0.622382521629)\n",
      " state (6)  A[0]:(-0.000221848487854) A[1]:(0.810027241707) A[2]:(-0.00031590461731) A[3]:(0.655538499355)\n",
      " state (7)  A[0]:(0.561393857002) A[1]:(-0.576561331749) A[2]:(0.392628163099) A[3]:(0.883146107197)\n",
      " state (8)  A[0]:(0.655195593834) A[1]:(-0.000196188688278) A[2]:(0.729117512703) A[3]:(0.59070456028)\n",
      " state (9)  A[0]:(0.656037926674) A[1]:(0.809938967228) A[2]:(0.810054421425) A[3]:(-0.000847518269438)\n",
      " state (10)  A[0]:(0.729070186615) A[1]:(0.899956703186) A[2]:(-0.000263690948486) A[3]:(0.728692531586)\n",
      " state (11)  A[0]:(0.126724138856) A[1]:(0.877601981163) A[2]:(-0.9105437994) A[3]:(0.796918570995)\n",
      " state (12)  A[0]:(-0.428779870272) A[1]:(0.808274745941) A[2]:(-0.923120975494) A[3]:(0.712790727615)\n",
      " state (13)  A[0]:(-0.000733226421289) A[1]:(0.809397816658) A[2]:(0.900034427643) A[3]:(0.729268729687)\n",
      " state (14)  A[0]:(0.80997389555) A[1]:(0.900246798992) A[2]:(0.999998867512) A[3]:(0.809926509857)\n",
      " state (15)  A[0]:(0.97957187891) A[1]:(0.942412137985) A[2]:(1.0) A[3]:(0.881138086319)\n",
      "Episode 343000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6140. Times reached goal: 932.               Steps done: 2699588. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.060509946914.\n",
      " state (0)  A[0]:(0.531004428864) A[1]:(0.58938485384) A[2]:(0.5897564888) A[3]:(0.532335698605)\n",
      " state (1)  A[0]:(0.531368613243) A[1]:(-0.000563844980206) A[2]:(0.655414223671) A[3]:(0.591559290886)\n",
      " state (2)  A[0]:(0.59009885788) A[1]:(0.728833675385) A[2]:(0.592301607132) A[3]:(0.657533288002)\n",
      " state (3)  A[0]:(0.657086253166) A[1]:(-0.00217698165216) A[2]:(0.461980998516) A[3]:(0.555848896503)\n",
      " state (4)  A[0]:(0.589642047882) A[1]:(0.655415892601) A[2]:(0.000547885836568) A[3]:(0.533597111702)\n",
      " state (5)  A[0]:(-0.0662535950541) A[1]:(0.999890565872) A[2]:(-0.699129879475) A[3]:(0.62451851368)\n",
      " state (6)  A[0]:(0.00111995590851) A[1]:(0.809519767761) A[2]:(-0.000354170770152) A[3]:(0.656348824501)\n",
      " state (7)  A[0]:(0.561971306801) A[1]:(-0.576427638531) A[2]:(0.392479926348) A[3]:(0.882951319218)\n",
      " state (8)  A[0]:(0.654986977577) A[1]:(-0.000759348098654) A[2]:(0.729105710983) A[3]:(0.589066743851)\n",
      " state (9)  A[0]:(0.656344175339) A[1]:(0.809343755245) A[2]:(0.81000828743) A[3]:(-0.00121164263692)\n",
      " state (10)  A[0]:(0.729888498783) A[1]:(0.899601638317) A[2]:(-0.000229954719543) A[3]:(0.728798627853)\n",
      " state (11)  A[0]:(0.1296428442) A[1]:(0.877240300179) A[2]:(-0.910587191582) A[3]:(0.7970687747)\n",
      " state (12)  A[0]:(-0.425788938999) A[1]:(0.807661473751) A[2]:(-0.923302173615) A[3]:(0.712979912758)\n",
      " state (13)  A[0]:(0.00299496063963) A[1]:(0.808533549309) A[2]:(0.899820268154) A[3]:(0.729561924934)\n",
      " state (14)  A[0]:(0.811107635498) A[1]:(0.899648308754) A[2]:(0.999998867512) A[3]:(0.810303449631)\n",
      " state (15)  A[0]:(0.97968775034) A[1]:(0.942009568214) A[2]:(1.0) A[3]:(0.881431818008)\n",
      "Episode 344000 finished after 0 timesteps with r=0.0. Running score: 0.94. Times trained:               6151. Times reached goal: 932.               Steps done: 2705739. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.060138892578.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5905,  0.5909,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.0001,  0.6569,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.7285,  0.5942,  0.6563]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0012,  0.8098,  0.0002,  0.6561]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7283,  0.9001, -0.0023,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8107,  0.9006,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530986189842) A[1]:(0.590231537819) A[2]:(0.589458346367) A[3]:(0.531038999557)\n",
      " state (1)  A[0]:(0.531270623207) A[1]:(-4.64469194412e-05) A[2]:(0.655744552612) A[3]:(0.59018266201)\n",
      " state (2)  A[0]:(0.590034782887) A[1]:(0.728657364845) A[2]:(0.593262672424) A[3]:(0.655922651291)\n",
      " state (3)  A[0]:(0.657437801361) A[1]:(-0.00401824060827) A[2]:(0.464127153158) A[3]:(0.55347764492)\n",
      " state (4)  A[0]:(0.590223908424) A[1]:(0.655502438545) A[2]:(0.00193881744053) A[3]:(0.53125500679)\n",
      " state (5)  A[0]:(-0.0651081576943) A[1]:(0.999890863895) A[2]:(-0.700467646122) A[3]:(0.623544871807)\n",
      " state (6)  A[0]:(0.00131149520166) A[1]:(0.810003399849) A[2]:(0.000437498063548) A[3]:(0.655550122261)\n",
      " state (7)  A[0]:(0.562475442886) A[1]:(-0.57556283474) A[2]:(0.393906652927) A[3]:(0.883049488068)\n",
      " state (8)  A[0]:(0.656602203846) A[1]:(0.000226303935051) A[2]:(0.729014396667) A[3]:(0.592567563057)\n",
      " state (9)  A[0]:(0.656355142593) A[1]:(0.81036901474) A[2]:(0.809950351715) A[3]:(0.000175833702087)\n",
      " state (10)  A[0]:(0.728752076626) A[1]:(0.900160849094) A[2]:(-0.000907778507099) A[3]:(0.728364467621)\n",
      " state (11)  A[0]:(0.125999510288) A[1]:(0.877842962742) A[2]:(-0.910777330399) A[3]:(0.796519577503)\n",
      " state (12)  A[0]:(-0.428508579731) A[1]:(0.808622598648) A[2]:(-0.923274159431) A[3]:(0.71227222681)\n",
      " state (13)  A[0]:(0.000953525013756) A[1]:(0.809733867645) A[2]:(0.90066421032) A[3]:(0.728924036026)\n",
      " state (14)  A[0]:(0.810855388641) A[1]:(0.900494754314) A[2]:(0.999998867512) A[3]:(0.809767305851)\n",
      " state (15)  A[0]:(0.979689002037) A[1]:(0.94256311655) A[2]:(1.0) A[3]:(0.880959808826)\n",
      "Episode 345000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6194. Times reached goal: 947.               Steps done: 2711933. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0597675435327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531565189362) A[1]:(0.590370953083) A[2]:(0.589179635048) A[3]:(0.531427264214)\n",
      " state (1)  A[0]:(0.531389296055) A[1]:(0.000768169586081) A[2]:(0.655469059944) A[3]:(0.58994948864)\n",
      " state (2)  A[0]:(0.589614033699) A[1]:(0.729092359543) A[2]:(0.594074845314) A[3]:(0.655775427818)\n",
      " state (3)  A[0]:(0.657050967216) A[1]:(-0.00441764388233) A[2]:(0.466215223074) A[3]:(0.553268909454)\n",
      " state (4)  A[0]:(0.589590191841) A[1]:(0.656260609627) A[2]:(0.00301479385234) A[3]:(0.531441092491)\n",
      " state (5)  A[0]:(-0.0654329657555) A[1]:(0.999891161919) A[2]:(-0.702382802963) A[3]:(0.624596595764)\n",
      " state (6)  A[0]:(0.000301793217659) A[1]:(0.81004601717) A[2]:(-0.000497341097798) A[3]:(0.654679775238)\n",
      " state (7)  A[0]:(0.561167418957) A[1]:(-0.575365662575) A[2]:(0.39362180233) A[3]:(0.882157087326)\n",
      " state (8)  A[0]:(0.655227541924) A[1]:(-0.00035049021244) A[2]:(0.728891909122) A[3]:(0.588907837868)\n",
      " state (9)  A[0]:(0.656040430069) A[1]:(0.809820532799) A[2]:(0.809844732285) A[3]:(-0.00305103301071)\n",
      " state (10)  A[0]:(0.729523599148) A[1]:(0.89982187748) A[2]:(-0.000709295156412) A[3]:(0.727963924408)\n",
      " state (11)  A[0]:(0.12890483439) A[1]:(0.877521336079) A[2]:(-0.910835027695) A[3]:(0.796780884266)\n",
      " state (12)  A[0]:(-0.426592856646) A[1]:(0.808130264282) A[2]:(-0.923705816269) A[3]:(0.712599515915)\n",
      " state (13)  A[0]:(0.00220517464913) A[1]:(0.809049189091) A[2]:(0.899717748165) A[3]:(0.729007840157)\n",
      " state (14)  A[0]:(0.811069905758) A[1]:(0.900049805641) A[2]:(0.999998867512) A[3]:(0.809713423252)\n",
      " state (15)  A[0]:(0.979710638523) A[1]:(0.942302525043) A[2]:(1.0) A[3]:(0.88080573082)\n",
      "Episode 346000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6121. Times reached goal: 944.               Steps done: 2718054. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0594028237623.\n",
      " state (0)  A[0]:(0.532165646553) A[1]:(0.590172529221) A[2]:(0.590905845165) A[3]:(0.532359302044)\n",
      " state (1)  A[0]:(0.532222151756) A[1]:(-0.000253021717072) A[2]:(0.655811786652) A[3]:(0.590689897537)\n",
      " state (2)  A[0]:(0.590027213097) A[1]:(0.728622317314) A[2]:(0.593159079552) A[3]:(0.656311631203)\n",
      " state (3)  A[0]:(0.656947374344) A[1]:(-0.00639149779454) A[2]:(0.465548813343) A[3]:(0.553467214108)\n",
      " state (4)  A[0]:(0.58938729763) A[1]:(0.654978871346) A[2]:(0.00134992517997) A[3]:(0.532205224037)\n",
      " state (5)  A[0]:(-0.0643783733249) A[1]:(0.999890863895) A[2]:(-0.704384565353) A[3]:(0.627474188805)\n",
      " state (6)  A[0]:(5.37931919098e-05) A[1]:(0.80965846777) A[2]:(-0.00108611537144) A[3]:(0.655506849289)\n",
      " state (7)  A[0]:(0.560349345207) A[1]:(-0.573830425739) A[2]:(0.393106609583) A[3]:(0.882094025612)\n",
      " state (8)  A[0]:(0.654809355736) A[1]:(0.000183284282684) A[2]:(0.728227376938) A[3]:(0.590868115425)\n",
      " state (9)  A[0]:(0.655199825764) A[1]:(0.809792995453) A[2]:(0.809698343277) A[3]:(1.31130218506e-06)\n",
      " state (10)  A[0]:(0.728920221329) A[1]:(0.899974405766) A[2]:(-1.49011611938e-05) A[3]:(0.728918194771)\n",
      " state (11)  A[0]:(0.128077507019) A[1]:(0.877948403358) A[2]:(-0.910690426826) A[3]:(0.797284960747)\n",
      " state (12)  A[0]:(-0.42785820365) A[1]:(0.808939576149) A[2]:(-0.923650979996) A[3]:(0.712677776814)\n",
      " state (13)  A[0]:(-0.000561863125768) A[1]:(0.809792339802) A[2]:(0.900069177151) A[3]:(0.728688776493)\n",
      " state (14)  A[0]:(0.80969274044) A[1]:(0.900350570679) A[2]:(0.999998867512) A[3]:(0.80940258503)\n",
      " state (15)  A[0]:(0.97951900959) A[1]:(0.942373931408) A[2]:(1.0) A[3]:(0.880589306355)\n",
      "Episode 347000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6184. Times reached goal: 940.               Steps done: 2724238. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0590366101995.\n",
      " state (0)  A[0]:(0.531347632408) A[1]:(0.590812206268) A[2]:(0.591014027596) A[3]:(0.530481338501)\n",
      " state (1)  A[0]:(0.531174659729) A[1]:(9.90033149719e-05) A[2]:(0.656028568745) A[3]:(0.590927481651)\n",
      " state (2)  A[0]:(0.589766979218) A[1]:(0.728891253471) A[2]:(0.593697667122) A[3]:(0.65706974268)\n",
      " state (3)  A[0]:(0.657188534737) A[1]:(-0.00815272144973) A[2]:(0.467381507158) A[3]:(0.553922653198)\n",
      " state (4)  A[0]:(0.589502811432) A[1]:(0.656674444675) A[2]:(0.00136732973624) A[3]:(0.532164216042)\n",
      " state (5)  A[0]:(-0.0635316222906) A[1]:(0.999891340733) A[2]:(-0.706923544407) A[3]:(0.626978635788)\n",
      " state (6)  A[0]:(-6.75469636917e-05) A[1]:(0.809851527214) A[2]:(-0.00128650595434) A[3]:(0.654872536659)\n",
      " state (7)  A[0]:(0.560261845589) A[1]:(-0.574248552322) A[2]:(0.394813805819) A[3]:(0.881912589073)\n",
      " state (8)  A[0]:(0.656024217606) A[1]:(-5.42402267456e-06) A[2]:(0.729009151459) A[3]:(0.59055274725)\n",
      " state (9)  A[0]:(0.657403349876) A[1]:(0.810089826584) A[2]:(0.810142397881) A[3]:(-0.000850438838825)\n",
      " state (10)  A[0]:(0.730582118034) A[1]:(0.900046885014) A[2]:(0.000912904506549) A[3]:(0.7288646698)\n",
      " state (11)  A[0]:(0.13018065691) A[1]:(0.877887248993) A[2]:(-0.910683512688) A[3]:(0.797458529472)\n",
      " state (12)  A[0]:(-0.427275180817) A[1]:(0.808662593365) A[2]:(-0.923786699772) A[3]:(0.712947905064)\n",
      " state (13)  A[0]:(-0.000293806195259) A[1]:(0.809403061867) A[2]:(0.90008944273) A[3]:(0.72894102335)\n",
      " state (14)  A[0]:(0.809946000576) A[1]:(0.900129318237) A[2]:(0.999998867512) A[3]:(0.809612631798)\n",
      " state (15)  A[0]:(0.979577600956) A[1]:(0.942220509052) A[2]:(1.0) A[3]:(0.88068985939)\n",
      "Episode 348000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6135. Times reached goal: 939.               Steps done: 2730373. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.058675529344.\n",
      " state (0)  A[0]:(0.530912876129) A[1]:(0.590460479259) A[2]:(0.590423762798) A[3]:(0.53160315752)\n",
      " state (1)  A[0]:(0.531199514866) A[1]:(0.000373184651835) A[2]:(0.656477093697) A[3]:(0.590578198433)\n",
      " state (2)  A[0]:(0.58969438076) A[1]:(0.729264259338) A[2]:(0.593214452267) A[3]:(0.656500220299)\n",
      " state (3)  A[0]:(0.657024800777) A[1]:(-0.00461879046634) A[2]:(0.467741131783) A[3]:(0.553654193878)\n",
      " state (4)  A[0]:(0.589861690998) A[1]:(0.656830906868) A[2]:(0.00278925173916) A[3]:(0.532347917557)\n",
      " state (5)  A[0]:(-0.0620680265129) A[1]:(0.999891757965) A[2]:(-0.706601500511) A[3]:(0.628417849541)\n",
      " state (6)  A[0]:(0.000897049671039) A[1]:(0.810399949551) A[2]:(0.00112104369327) A[3]:(0.656041204929)\n",
      " state (7)  A[0]:(0.560574054718) A[1]:(-0.573140144348) A[2]:(0.396172314882) A[3]:(0.88221424818)\n",
      " state (8)  A[0]:(0.655895471573) A[1]:(0.000247538089752) A[2]:(0.728996872902) A[3]:(0.592126727104)\n",
      " state (9)  A[0]:(0.655741453171) A[1]:(0.810232758522) A[2]:(0.809775650501) A[3]:(-7.24792480469e-05)\n",
      " state (10)  A[0]:(0.727887392044) A[1]:(0.900165557861) A[2]:(-0.00202142912894) A[3]:(0.728321909904)\n",
      " state (11)  A[0]:(0.123599283397) A[1]:(0.878131151199) A[2]:(-0.911406636238) A[3]:(0.796842217445)\n",
      " state (12)  A[0]:(-0.430968463421) A[1]:(0.80915927887) A[2]:(-0.92429035902) A[3]:(0.712652385235)\n",
      " state (13)  A[0]:(-0.00223181769252) A[1]:(0.809989690781) A[2]:(0.899958252907) A[3]:(0.729259312153)\n",
      " state (14)  A[0]:(0.809757888317) A[1]:(0.900554895401) A[2]:(0.999998927116) A[3]:(0.810042262077)\n",
      " state (15)  A[0]:(0.979574263096) A[1]:(0.942539691925) A[2]:(1.0) A[3]:(0.880941390991)\n",
      "Episode 349000 finished after 0 timesteps with r=0.0. Running score: 0.92. Times trained:               6147. Times reached goal: 937.               Steps done: 2736520. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0583159571425.\n",
      "q_values \n",
      "tensor([[ 0.5309,  0.5902,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.3095e-01, -8.8215e-06,  6.5582e-01,  5.9024e-01]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5896,  0.7289,  0.5922,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  0.8101, -0.0003,  0.6552]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7296,  0.8999, -0.0001,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9002,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531357169151) A[1]:(0.590221405029) A[2]:(0.590462565422) A[3]:(0.53146481514)\n",
      " state (1)  A[0]:(0.531532406807) A[1]:(0.000145301222801) A[2]:(0.655851662159) A[3]:(0.590438842773)\n",
      " state (2)  A[0]:(0.590198040009) A[1]:(0.728819668293) A[2]:(0.592256426811) A[3]:(0.656324505806)\n",
      " state (3)  A[0]:(0.65717792511) A[1]:(-0.00515279080719) A[2]:(0.4669085145) A[3]:(0.553294539452)\n",
      " state (4)  A[0]:(0.589842796326) A[1]:(0.655906677246) A[2]:(0.000917315250263) A[3]:(0.531540453434)\n",
      " state (5)  A[0]:(-0.0622557401657) A[1]:(0.999891519547) A[2]:(-0.708799481392) A[3]:(0.627439618111)\n",
      " state (6)  A[0]:(0.000598251761403) A[1]:(0.809935986996) A[2]:(-0.000117421150208) A[3]:(0.65543627739)\n",
      " state (7)  A[0]:(0.560337662697) A[1]:(-0.573212862015) A[2]:(0.395730227232) A[3]:(0.881909608841)\n",
      " state (8)  A[0]:(0.655725240707) A[1]:(-0.000417366594775) A[2]:(0.729067444801) A[3]:(0.590968489647)\n",
      " state (9)  A[0]:(0.656794905663) A[1]:(0.809726238251) A[2]:(0.810159087181) A[3]:(0.000666737440042)\n",
      " state (10)  A[0]:(0.730048894882) A[1]:(0.899892091751) A[2]:(0.000392317742808) A[3]:(0.729491949081)\n",
      " state (11)  A[0]:(0.129484012723) A[1]:(0.877872526646) A[2]:(-0.911000013351) A[3]:(0.797892689705)\n",
      " state (12)  A[0]:(-0.426995933056) A[1]:(0.80875235796) A[2]:(-0.924198627472) A[3]:(0.713497161865)\n",
      " state (13)  A[0]:(0.000861316686496) A[1]:(0.809465765953) A[2]:(0.900038838387) A[3]:(0.729562759399)\n",
      " state (14)  A[0]:(0.810305297375) A[1]:(0.90022444725) A[2]:(0.999998927116) A[3]:(0.810121119022)\n",
      " state (15)  A[0]:(0.97959882021) A[1]:(0.942320644855) A[2]:(1.0) A[3]:(0.880941152573)\n",
      "Episode 350000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6158. Times reached goal: 937.               Steps done: 2742678. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.057957950911.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530264616013) A[1]:(0.589669823647) A[2]:(0.590136229992) A[3]:(0.530510902405)\n",
      " state (1)  A[0]:(0.530468344688) A[1]:(-5.51491975784e-05) A[2]:(0.655766665936) A[3]:(0.589847207069)\n",
      " state (2)  A[0]:(0.589222431183) A[1]:(0.72862893343) A[2]:(0.592234075069) A[3]:(0.655864536762)\n",
      " state (3)  A[0]:(0.656335115433) A[1]:(-0.00508979056031) A[2]:(0.467378050089) A[3]:(0.552779495716)\n",
      " state (4)  A[0]:(0.589137732983) A[1]:(0.655452251434) A[2]:(0.000608920992818) A[3]:(0.530945062637)\n",
      " state (5)  A[0]:(-0.0624694079161) A[1]:(0.999891519547) A[2]:(-0.710605502129) A[3]:(0.627148509026)\n",
      " state (6)  A[0]:(0.000131040811539) A[1]:(0.809905827045) A[2]:(-0.00086498237215) A[3]:(0.655243575573)\n",
      " state (7)  A[0]:(0.559784531593) A[1]:(-0.572738289833) A[2]:(0.395621746778) A[3]:(0.881722092628)\n",
      " state (8)  A[0]:(0.654739737511) A[1]:(-0.000116184353828) A[2]:(0.728841662407) A[3]:(0.589827120304)\n",
      " state (9)  A[0]:(0.655664861202) A[1]:(0.80970710516) A[2]:(0.809891581535) A[3]:(-0.000600993575063)\n",
      " state (10)  A[0]:(0.729305326939) A[1]:(0.899901211262) A[2]:(-9.65595245361e-05) A[3]:(0.729149460793)\n",
      " state (11)  A[0]:(0.12830029428) A[1]:(0.877982199192) A[2]:(-0.911102175713) A[3]:(0.797771334648)\n",
      " state (12)  A[0]:(-0.427925527096) A[1]:(0.809004426003) A[2]:(-0.924375891685) A[3]:(0.7133051157)\n",
      " state (13)  A[0]:(-0.000262886285782) A[1]:(0.809734880924) A[2]:(0.899957001209) A[3]:(0.729306161404)\n",
      " state (14)  A[0]:(0.809931695461) A[1]:(0.900416374207) A[2]:(0.999998927116) A[3]:(0.809884309769)\n",
      " state (15)  A[0]:(0.979556679726) A[1]:(0.942460894585) A[2]:(1.0) A[3]:(0.880723059177)\n",
      "Episode 351000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6151. Times reached goal: 947.               Steps done: 2748829. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0576025457241.\n",
      " state (0)  A[0]:(0.531552910805) A[1]:(0.590489268303) A[2]:(0.590670228004) A[3]:(0.531486868858)\n",
      " state (1)  A[0]:(0.531435966492) A[1]:(-0.000166207551956) A[2]:(0.656241416931) A[3]:(0.590419530869)\n",
      " state (2)  A[0]:(0.590195655823) A[1]:(0.728835225105) A[2]:(0.591954708099) A[3]:(0.656486868858)\n",
      " state (3)  A[0]:(0.657207012177) A[1]:(-0.0035498291254) A[2]:(0.46753808856) A[3]:(0.553894281387)\n",
      " state (4)  A[0]:(0.590054035187) A[1]:(0.65588760376) A[2]:(0.000481009454234) A[3]:(0.531749367714)\n",
      " state (5)  A[0]:(-0.061898894608) A[1]:(0.99989181757) A[2]:(-0.711515784264) A[3]:(0.626997709274)\n",
      " state (6)  A[0]:(0.00047434863518) A[1]:(0.81004267931) A[2]:(-0.000299453735352) A[3]:(0.655730426311)\n",
      " state (7)  A[0]:(0.560126543045) A[1]:(-0.572309434414) A[2]:(0.39629971981) A[3]:(0.882023096085)\n",
      " state (8)  A[0]:(0.655438899994) A[1]:(0.000351637601852) A[2]:(0.729061841965) A[3]:(0.591250956059)\n",
      " state (9)  A[0]:(0.656183838844) A[1]:(0.810114324093) A[2]:(0.810137748718) A[3]:(0.000671863439493)\n",
      " state (10)  A[0]:(0.729282319546) A[1]:(0.900049090385) A[2]:(0.000170588493347) A[3]:(0.72933280468)\n",
      " state (11)  A[0]:(0.12754970789) A[1]:(0.877993941307) A[2]:(-0.911202192307) A[3]:(0.797726750374)\n",
      " state (12)  A[0]:(-0.428286463022) A[1]:(0.8087272048) A[2]:(-0.924499452114) A[3]:(0.713272154331)\n",
      " state (13)  A[0]:(-1.62571668625e-05) A[1]:(0.809181213379) A[2]:(0.900116086006) A[3]:(0.729467272758)\n",
      " state (14)  A[0]:(0.810126543045) A[1]:(0.900021374226) A[2]:(0.999998927116) A[3]:(0.810131847858)\n",
      " state (15)  A[0]:(0.979579627514) A[1]:(0.942180097103) A[2]:(1.0) A[3]:(0.88091558218)\n",
      "Episode 352000 finished after 0 timesteps with r=1.0. Running score: 0.9. Times trained:               6177. Times reached goal: 941.               Steps done: 2755006. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.057247831462.\n",
      " state (0)  A[0]:(0.531170248985) A[1]:(0.590556502342) A[2]:(0.590502262115) A[3]:(0.531112253666)\n",
      " state (1)  A[0]:(0.531231999397) A[1]:(8.39084386826e-05) A[2]:(0.65612924099) A[3]:(0.590027332306)\n",
      " state (2)  A[0]:(0.589737415314) A[1]:(0.729025185108) A[2]:(0.592513740063) A[3]:(0.656119585037)\n",
      " state (3)  A[0]:(0.656930685043) A[1]:(-0.00344991800375) A[2]:(0.468665659428) A[3]:(0.553540229797)\n",
      " state (4)  A[0]:(0.589757323265) A[1]:(0.656186640263) A[2]:(0.000685691717081) A[3]:(0.531423330307)\n",
      " state (5)  A[0]:(-0.0623280256987) A[1]:(0.999891996384) A[2]:(-0.713025331497) A[3]:(0.626835048199)\n",
      " state (6)  A[0]:(-4.76688146591e-05) A[1]:(0.810040771961) A[2]:(-0.000553488673177) A[3]:(0.655462265015)\n",
      " state (7)  A[0]:(0.559809684753) A[1]:(-0.57196366787) A[2]:(0.396595239639) A[3]:(0.881854891777)\n",
      " state (8)  A[0]:(0.655382275581) A[1]:(0.000365003914339) A[2]:(0.728993415833) A[3]:(0.590875864029)\n",
      " state (9)  A[0]:(0.656211614609) A[1]:(0.810075938702) A[2]:(0.810029566288) A[3]:(-3.70144844055e-05)\n",
      " state (10)  A[0]:(0.729279756546) A[1]:(0.900026082993) A[2]:(-7.17639923096e-05) A[3]:(0.728926002979)\n",
      " state (11)  A[0]:(0.127519205213) A[1]:(0.878019928932) A[2]:(-0.911302924156) A[3]:(0.797449529171)\n",
      " state (12)  A[0]:(-0.428258687258) A[1]:(0.808805525303) A[2]:(-0.924678087234) A[3]:(0.712913036346)\n",
      " state (13)  A[0]:(0.000220209360123) A[1]:(0.809247970581) A[2]:(0.90003246069) A[3]:(0.729156017303)\n",
      " state (14)  A[0]:(0.810286402702) A[1]:(0.900095105171) A[2]:(0.999998927116) A[3]:(0.809921741486)\n",
      " state (15)  A[0]:(0.979606449604) A[1]:(0.942248225212) A[2]:(1.0) A[3]:(0.88074016571)\n",
      "Episode 353000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6158. Times reached goal: 950.               Steps done: 2761164. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0568963825377.\n",
      " state (0)  A[0]:(0.531699776649) A[1]:(0.591065227985) A[2]:(0.59108710289) A[3]:(0.531460762024)\n",
      " state (1)  A[0]:(0.531441807747) A[1]:(-0.000157341361046) A[2]:(0.656142055988) A[3]:(0.590721666813)\n",
      " state (2)  A[0]:(0.589953541756) A[1]:(0.729105830193) A[2]:(0.593038916588) A[3]:(0.656647920609)\n",
      " state (3)  A[0]:(0.657156169415) A[1]:(-0.00417896173894) A[2]:(0.469987243414) A[3]:(0.554163455963)\n",
      " state (4)  A[0]:(0.589856624603) A[1]:(0.656011581421) A[2]:(0.00121915282216) A[3]:(0.531911432743)\n",
      " state (5)  A[0]:(-0.0625511929393) A[1]:(0.999891936779) A[2]:(-0.714410126209) A[3]:(0.627004146576)\n",
      " state (6)  A[0]:(-0.000700503471307) A[1]:(0.809677839279) A[2]:(-0.000610232295003) A[3]:(0.655850827694)\n",
      " state (7)  A[0]:(0.559218525887) A[1]:(-0.572433531284) A[2]:(0.397100329399) A[3]:(0.881949365139)\n",
      " state (8)  A[0]:(0.654199004173) A[1]:(0.000463262171252) A[2]:(0.729332387447) A[3]:(0.589396357536)\n",
      " state (9)  A[0]:(0.655433416367) A[1]:(0.810020625591) A[2]:(0.809947252274) A[3]:(-0.000757098081522)\n",
      " state (10)  A[0]:(0.72886300087) A[1]:(0.899805307388) A[2]:(-0.000989675172605) A[3]:(0.729204654694)\n",
      " state (11)  A[0]:(0.126690119505) A[1]:(0.877646028996) A[2]:(-0.911535680294) A[3]:(0.797882437706)\n",
      " state (12)  A[0]:(-0.428698986769) A[1]:(0.808219909668) A[2]:(-0.924808442593) A[3]:(0.713628053665)\n",
      " state (13)  A[0]:(0.000353991956217) A[1]:(0.808798491955) A[2]:(0.900478124619) A[3]:(0.729850530624)\n",
      " state (14)  A[0]:(0.810550630093) A[1]:(0.899982571602) A[2]:(0.999998986721) A[3]:(0.810372233391)\n",
      " state (15)  A[0]:(0.979644954205) A[1]:(0.942219078541) A[2]:(1.0) A[3]:(0.880932450294)\n",
      "Episode 354000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6122. Times reached goal: 945.               Steps done: 2767286. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0565491269178.\n",
      "q_values \n",
      "tensor([[ 0.5306,  0.5909,  0.5896,  0.5307]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5898,  0.6562,  0.0015,  0.5312]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557,  0.0015,  0.7286,  0.5915]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6568,  0.8107,  0.8099,  0.0020]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0005,  0.8095,  0.9005,  0.7290]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.9001,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530564069748) A[1]:(0.590747475624) A[2]:(0.589800715446) A[3]:(0.530688345432)\n",
      " state (1)  A[0]:(0.530420303345) A[1]:(9.486079216e-05) A[2]:(0.655561923981) A[3]:(0.589601159096)\n",
      " state (2)  A[0]:(0.589393019676) A[1]:(0.728934586048) A[2]:(0.593014597893) A[3]:(0.655643701553)\n",
      " state (3)  A[0]:(0.656808257103) A[1]:(-0.00426745787263) A[2]:(0.470851689577) A[3]:(0.553128480911)\n",
      " state (4)  A[0]:(0.589671850204) A[1]:(0.656361579895) A[2]:(0.00163137773052) A[3]:(0.531013607979)\n",
      " state (5)  A[0]:(-0.0618179664016) A[1]:(0.999892175198) A[2]:(-0.71562564373) A[3]:(0.626615464687)\n",
      " state (6)  A[0]:(0.000136688351631) A[1]:(0.809770345688) A[2]:(-0.00096118421061) A[3]:(0.655113458633)\n",
      " state (7)  A[0]:(0.559771716595) A[1]:(-0.571763396263) A[2]:(0.396670877934) A[3]:(0.881583094597)\n",
      " state (8)  A[0]:(0.655020594597) A[1]:(0.000306814908981) A[2]:(0.728648781776) A[3]:(0.589676856995)\n",
      " state (9)  A[0]:(0.655645012856) A[1]:(0.810063004494) A[2]:(0.809569180012) A[3]:(-0.00153064611368)\n",
      " state (10)  A[0]:(0.728651404381) A[1]:(0.899885296822) A[2]:(-0.00196194392629) A[3]:(0.728291273117)\n",
      " state (11)  A[0]:(0.126007318497) A[1]:(0.877824544907) A[2]:(-0.911801874638) A[3]:(0.797059297562)\n",
      " state (12)  A[0]:(-0.429289966822) A[1]:(0.808489918709) A[2]:(-0.925217449665) A[3]:(0.712429642677)\n",
      " state (13)  A[0]:(-0.000378817290766) A[1]:(0.80890417099) A[2]:(0.89986115694) A[3]:(0.72874891758)\n",
      " state (14)  A[0]:(0.810295283794) A[1]:(0.899974763393) A[2]:(0.999998986721) A[3]:(0.809636712074)\n",
      " state (15)  A[0]:(0.979622900486) A[1]:(0.942201733589) A[2]:(1.0) A[3]:(0.880444705486)\n",
      "Episode 355000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6157. Times reached goal: 946.               Steps done: 2773443. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0562020235975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531646609306) A[1]:(0.590304017067) A[2]:(0.590690255165) A[3]:(0.53154116869)\n",
      " state (1)  A[0]:(0.531623244286) A[1]:(-8.19563865662e-05) A[2]:(0.655955553055) A[3]:(0.590420842171)\n",
      " state (2)  A[0]:(0.589949250221) A[1]:(0.728944420815) A[2]:(0.592906117439) A[3]:(0.65641617775)\n",
      " state (3)  A[0]:(0.657070875168) A[1]:(-0.00489487312734) A[2]:(0.471086293459) A[3]:(0.55392229557)\n",
      " state (4)  A[0]:(0.589630305767) A[1]:(0.656233310699) A[2]:(0.00102174247149) A[3]:(0.53165769577)\n",
      " state (5)  A[0]:(-0.0620199777186) A[1]:(0.999892354012) A[2]:(-0.717031598091) A[3]:(0.627222418785)\n",
      " state (6)  A[0]:(0.000173956155777) A[1]:(0.810018181801) A[2]:(-0.000815152947325) A[3]:(0.65558296442)\n",
      " state (7)  A[0]:(0.559801220894) A[1]:(-0.571207046509) A[2]:(0.397581070662) A[3]:(0.881700158119)\n",
      " state (8)  A[0]:(0.655485630035) A[1]:(0.000412523717387) A[2]:(0.72901058197) A[3]:(0.590810418129)\n",
      " state (9)  A[0]:(0.656240403652) A[1]:(0.810199677944) A[2]:(0.810019075871) A[3]:(-8.69631767273e-05)\n",
      " state (10)  A[0]:(0.729338407516) A[1]:(0.90004748106) A[2]:(-0.000142455101013) A[3]:(0.728998482227)\n",
      " state (11)  A[0]:(0.127699971199) A[1]:(0.87811923027) A[2]:(-0.91155642271) A[3]:(0.79771065712)\n",
      " state (12)  A[0]:(-0.428270667791) A[1]:(0.808946013451) A[2]:(-0.925173163414) A[3]:(0.713171601295)\n",
      " state (13)  A[0]:(0.000183969736099) A[1]:(0.809228777885) A[2]:(0.899947822094) A[3]:(0.72930085659)\n",
      " state (14)  A[0]:(0.810271382332) A[1]:(0.900082588196) A[2]:(0.999998986721) A[3]:(0.809973239899)\n",
      " state (15)  A[0]:(0.979606211185) A[1]:(0.942225337029) A[2]:(1.0) A[3]:(0.880612134933)\n",
      "Episode 356000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6130. Times reached goal: 941.               Steps done: 2779573. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0558585589874.\n",
      " state (0)  A[0]:(0.531114339828) A[1]:(0.590602397919) A[2]:(0.590588450432) A[3]:(0.532901287079)\n",
      " state (1)  A[0]:(0.531026244164) A[1]:(-3.21120023727e-05) A[2]:(0.656134784222) A[3]:(0.592135131359)\n",
      " state (2)  A[0]:(0.589800953865) A[1]:(0.728981614113) A[2]:(0.592704534531) A[3]:(0.658033132553)\n",
      " state (3)  A[0]:(0.657086968422) A[1]:(-0.00570593541488) A[2]:(0.471291422844) A[3]:(0.55611884594)\n",
      " state (4)  A[0]:(0.589766979218) A[1]:(0.656135439873) A[2]:(0.000119686126709) A[3]:(0.533776402473)\n",
      " state (5)  A[0]:(-0.0612862855196) A[1]:(0.999892473221) A[2]:(-0.718620896339) A[3]:(0.628414154053)\n",
      " state (6)  A[0]:(-0.000209331512451) A[1]:(0.810010671616) A[2]:(-0.000388622254832) A[3]:(0.656991124153)\n",
      " state (7)  A[0]:(0.559031367302) A[1]:(-0.570699572563) A[2]:(0.398357212543) A[3]:(0.882195949554)\n",
      " state (8)  A[0]:(0.654793381691) A[1]:(0.000399425596697) A[2]:(0.729080438614) A[3]:(0.591792821884)\n",
      " state (9)  A[0]:(0.655496478081) A[1]:(0.810103714466) A[2]:(0.809903264046) A[3]:(0.000576674879994)\n",
      " state (10)  A[0]:(0.728606164455) A[1]:(0.900003910065) A[2]:(-0.000885724788532) A[3]:(0.729251146317)\n",
      " state (11)  A[0]:(0.12600453198) A[1]:(0.878177642822) A[2]:(-0.911792874336) A[3]:(0.798001825809)\n",
      " state (12)  A[0]:(-0.429701030254) A[1]:(0.809155702591) A[2]:(-0.925442874432) A[3]:(0.713566064835)\n",
      " state (13)  A[0]:(-0.00128632714041) A[1]:(0.809454143047) A[2]:(0.899846196175) A[3]:(0.729616045952)\n",
      " state (14)  A[0]:(0.809951066971) A[1]:(0.900192797184) A[2]:(0.999998986721) A[3]:(0.810134708881)\n",
      " state (15)  A[0]:(0.979594349861) A[1]:(0.942252516747) A[2]:(1.0) A[3]:(0.880635261536)\n",
      "Episode 357000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6138. Times reached goal: 945.               Steps done: 2785711. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0555167492396.\n",
      " state (0)  A[0]:(0.53190767765) A[1]:(0.590915262699) A[2]:(0.59090089798) A[3]:(0.532188951969)\n",
      " state (1)  A[0]:(0.531859993935) A[1]:(0.000177979469299) A[2]:(0.656450390816) A[3]:(0.591189444065)\n",
      " state (2)  A[0]:(0.590190887451) A[1]:(0.729268431664) A[2]:(0.593158721924) A[3]:(0.657248139381)\n",
      " state (3)  A[0]:(0.657499790192) A[1]:(-0.00597314862534) A[2]:(0.472418278456) A[3]:(0.555191993713)\n",
      " state (4)  A[0]:(0.590031325817) A[1]:(0.656322836876) A[2]:(0.000275254249573) A[3]:(0.532560706139)\n",
      " state (5)  A[0]:(-0.0613211579621) A[1]:(0.999892652035) A[2]:(-0.720187902451) A[3]:(0.627001881599)\n",
      " state (6)  A[0]:(1.35600566864e-05) A[1]:(0.810139238834) A[2]:(-0.000581026019063) A[3]:(0.6560536623)\n",
      " state (7)  A[0]:(0.559328138828) A[1]:(-0.570332884789) A[2]:(0.398781567812) A[3]:(0.881893515587)\n",
      " state (8)  A[0]:(0.655588388443) A[1]:(4.56571578979e-05) A[2]:(0.729192256927) A[3]:(0.591687321663)\n",
      " state (9)  A[0]:(0.656442344189) A[1]:(0.810051202774) A[2]:(0.810248553753) A[3]:(-0.000171959400177)\n",
      " state (10)  A[0]:(0.729605913162) A[1]:(0.900016486645) A[2]:(0.000650167348795) A[3]:(0.729002356529)\n",
      " state (11)  A[0]:(0.128238722682) A[1]:(0.87826102972) A[2]:(-0.911597728729) A[3]:(0.797966718674)\n",
      " state (12)  A[0]:(-0.428413152695) A[1]:(0.809291481972) A[2]:(-0.925431728363) A[3]:(0.713379979134)\n",
      " state (13)  A[0]:(-0.000394031376345) A[1]:(0.809495925903) A[2]:(0.899957597256) A[3]:(0.729285359383)\n",
      " state (14)  A[0]:(0.810129165649) A[1]:(0.900163352489) A[2]:(0.999998986721) A[3]:(0.809852063656)\n",
      " state (15)  A[0]:(0.979613006115) A[1]:(0.942191004753) A[2]:(1.0) A[3]:(0.880410790443)\n",
      "Episode 358000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6101. Times reached goal: 942.               Steps done: 2791812. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0551790726822.\n",
      " state (0)  A[0]:(0.530840992928) A[1]:(0.590328335762) A[2]:(0.590955376625) A[3]:(0.531005382538)\n",
      " state (1)  A[0]:(0.530736267567) A[1]:(0.000357091397746) A[2]:(0.656370997429) A[3]:(0.589767813683)\n",
      " state (2)  A[0]:(0.589325428009) A[1]:(0.728933095932) A[2]:(0.593412876129) A[3]:(0.655833482742)\n",
      " state (3)  A[0]:(0.656525969505) A[1]:(-0.00554681103677) A[2]:(0.473732829094) A[3]:(0.553703904152)\n",
      " state (4)  A[0]:(0.588981509209) A[1]:(0.655922472477) A[2]:(0.00174617592711) A[3]:(0.530727088451)\n",
      " state (5)  A[0]:(-0.0620373301208) A[1]:(0.99989259243) A[2]:(-0.720788061619) A[3]:(0.624980568886)\n",
      " state (6)  A[0]:(-0.000348493456841) A[1]:(0.809790790081) A[2]:(9.89437103271e-06) A[3]:(0.654672920704)\n",
      " state (7)  A[0]:(0.558984816074) A[1]:(-0.569371879101) A[2]:(0.398533821106) A[3]:(0.881317198277)\n",
      " state (8)  A[0]:(0.655266284943) A[1]:(0.00034087896347) A[2]:(0.728694200516) A[3]:(0.590559363365)\n",
      " state (9)  A[0]:(0.656294465065) A[1]:(0.809991002083) A[2]:(0.809945464134) A[3]:(-0.000916421180591)\n",
      " state (10)  A[0]:(0.729874849319) A[1]:(0.900094389915) A[2]:(0.000454425782664) A[3]:(0.728772044182)\n",
      " state (11)  A[0]:(0.129884004593) A[1]:(0.878542423248) A[2]:(-0.911569714546) A[3]:(0.79790365696)\n",
      " state (12)  A[0]:(-0.426698714495) A[1]:(0.80983799696) A[2]:(-0.925394058228) A[3]:(0.713211417198)\n",
      " state (13)  A[0]:(0.00140321161598) A[1]:(0.809997797012) A[2]:(0.900324106216) A[3]:(0.729064047337)\n",
      " state (14)  A[0]:(0.810443580151) A[1]:(0.900396466255) A[2]:(0.999998986721) A[3]:(0.80971378088)\n",
      " state (15)  A[0]:(0.979613423347) A[1]:(0.942295253277) A[2]:(1.0) A[3]:(0.880304813385)\n",
      "Episode 359000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6172. Times reached goal: 951.               Steps done: 2797984. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0548395562711.\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.5906,  0.5909,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5316, -0.0004,  0.6569,  0.5902]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5897,  0.7291,  0.5939,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=3 , Random? True\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5895,  0.7292,  0.5938,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0002,  0.8100,  0.0007,  0.6553]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7281,  0.8999, -0.0004,  0.7285]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9003,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53066188097) A[1]:(0.59107863903) A[2]:(0.589543163776) A[3]:(0.530937194824)\n",
      " state (1)  A[0]:(0.530733942986) A[1]:(0.000792845909018) A[2]:(0.656083583832) A[3]:(0.589835226536)\n",
      " state (2)  A[0]:(0.589494168758) A[1]:(0.729259729385) A[2]:(0.593374252319) A[3]:(0.655952692032)\n",
      " state (3)  A[0]:(0.657033741474) A[1]:(-0.00435315305367) A[2]:(0.474703878164) A[3]:(0.554404735565)\n",
      " state (4)  A[0]:(0.590048074722) A[1]:(0.656353652477) A[2]:(0.00239014159888) A[3]:(0.531453847885)\n",
      " state (5)  A[0]:(-0.0595783777535) A[1]:(0.999893069267) A[2]:(-0.722096443176) A[3]:(0.62520468235)\n",
      " state (6)  A[0]:(0.00137135304976) A[1]:(0.810478568077) A[2]:(0.000500321330037) A[3]:(0.655579209328)\n",
      " state (7)  A[0]:(0.560095012188) A[1]:(-0.569041013718) A[2]:(0.399838596582) A[3]:(0.881834864616)\n",
      " state (8)  A[0]:(0.656775355339) A[1]:(7.54445791245e-05) A[2]:(0.729140758514) A[3]:(0.59322142601)\n",
      " state (9)  A[0]:(0.657305598259) A[1]:(0.810242533684) A[2]:(0.810445785522) A[3]:(0.00141239073128)\n",
      " state (10)  A[0]:(0.730237960815) A[1]:(0.90022367239) A[2]:(0.00185382156633) A[3]:(0.729464173317)\n",
      " state (11)  A[0]:(0.129723995924) A[1]:(0.878673434258) A[2]:(-0.911517500877) A[3]:(0.798386216164)\n",
      " state (12)  A[0]:(-0.427293032408) A[1]:(0.809970796108) A[2]:(-0.925666332245) A[3]:(0.71378749609)\n",
      " state (13)  A[0]:(0.000774800602812) A[1]:(0.810029685497) A[2]:(0.899614334106) A[3]:(0.729587376118)\n",
      " state (14)  A[0]:(0.810446560383) A[1]:(0.900467634201) A[2]:(0.999998986721) A[3]:(0.81007027626)\n",
      " state (15)  A[0]:(0.979645252228) A[1]:(0.942423403263) A[2]:(1.0) A[3]:(0.880453228951)\n",
      "Episode 360000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6115. Times reached goal: 934.               Steps done: 2804099. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0545052356117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531172990799) A[1]:(0.590508460999) A[2]:(0.590607762337) A[3]:(0.531101584435)\n",
      " state (1)  A[0]:(0.531447410583) A[1]:(-0.00053730601212) A[2]:(0.656233429909) A[3]:(0.591647624969)\n",
      " state (2)  A[0]:(0.590324044228) A[1]:(0.729330658913) A[2]:(0.592612862587) A[3]:(0.658153414726)\n",
      " state (3)  A[0]:(0.657573461533) A[1]:(-0.00368656031787) A[2]:(0.473945736885) A[3]:(0.557176232338)\n",
      " state (4)  A[0]:(0.590339899063) A[1]:(0.656237185001) A[2]:(0.000554680766072) A[3]:(0.533604025841)\n",
      " state (5)  A[0]:(-0.0601215399802) A[1]:(0.999893069267) A[2]:(-0.724006831646) A[3]:(0.625577807426)\n",
      " state (6)  A[0]:(0.000666692736559) A[1]:(0.810122251511) A[2]:(-0.000630736292806) A[3]:(0.65626013279)\n",
      " state (7)  A[0]:(0.55899399519) A[1]:(-0.569443583488) A[2]:(0.399529397488) A[3]:(0.881711125374)\n",
      " state (8)  A[0]:(0.654423952103) A[1]:(0.000308126211166) A[2]:(0.729359507561) A[3]:(0.588924825191)\n",
      " state (9)  A[0]:(0.655641615391) A[1]:(0.809977114201) A[2]:(0.81009888649) A[3]:(-0.00286167068407)\n",
      " state (10)  A[0]:(0.728715181351) A[1]:(0.899977385998) A[2]:(-0.000484108895762) A[3]:(0.728033185005)\n",
      " state (11)  A[0]:(0.12599542737) A[1]:(0.878433406353) A[2]:(-0.912012636662) A[3]:(0.797444880009)\n",
      " state (12)  A[0]:(-0.429755687714) A[1]:(0.809710502625) A[2]:(-0.925930202007) A[3]:(0.712998390198)\n",
      " state (13)  A[0]:(-0.00118923129048) A[1]:(0.809867441654) A[2]:(0.899984240532) A[3]:(0.729298532009)\n",
      " state (14)  A[0]:(0.809917092323) A[1]:(0.900422930717) A[2]:(0.999998986721) A[3]:(0.810078680515)\n",
      " state (15)  A[0]:(0.979581832886) A[1]:(0.942366421223) A[2]:(1.0) A[3]:(0.880522310734)\n",
      "Episode 361000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6110. Times reached goal: 940.               Steps done: 2810209. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0541732239506.\n",
      " state (0)  A[0]:(0.530477643013) A[1]:(0.590186476707) A[2]:(0.588965654373) A[3]:(0.530622720718)\n",
      " state (1)  A[0]:(0.530743360519) A[1]:(-9.52631235123e-05) A[2]:(0.655813276768) A[3]:(0.589916050434)\n",
      " state (2)  A[0]:(0.5895190835) A[1]:(0.728923559189) A[2]:(0.593637168407) A[3]:(0.656414985657)\n",
      " state (3)  A[0]:(0.657098650932) A[1]:(-0.00319493212737) A[2]:(0.476335287094) A[3]:(0.555736899376)\n",
      " state (4)  A[0]:(0.590048968792) A[1]:(0.656242132187) A[2]:(0.0033199666068) A[3]:(0.531891584396)\n",
      " state (5)  A[0]:(-0.0605823136866) A[1]:(0.999893128872) A[2]:(-0.724169015884) A[3]:(0.622905611992)\n",
      " state (6)  A[0]:(0.00065210449975) A[1]:(0.809970796108) A[2]:(0.00125658442266) A[3]:(0.655290842056)\n",
      " state (7)  A[0]:(0.559885203838) A[1]:(-0.568928956985) A[2]:(0.400546640158) A[3]:(0.882007420063)\n",
      " state (8)  A[0]:(0.656262636185) A[1]:(0.000119552016258) A[2]:(0.728777229786) A[3]:(0.593184113503)\n",
      " state (9)  A[0]:(0.655660390854) A[1]:(0.810463070869) A[2]:(0.809644937515) A[3]:(-0.00054204458138)\n",
      " state (10)  A[0]:(0.727509617805) A[1]:(0.900263905525) A[2]:(-0.00215267809108) A[3]:(0.727704763412)\n",
      " state (11)  A[0]:(0.122494101524) A[1]:(0.878720760345) A[2]:(-0.912307858467) A[3]:(0.796797692776)\n",
      " state (12)  A[0]:(-0.431859493256) A[1]:(0.810147404671) A[2]:(-0.925938308239) A[3]:(0.712099194527)\n",
      " state (13)  A[0]:(-0.00213829847053) A[1]:(0.810421347618) A[2]:(0.900792717934) A[3]:(0.728490352631)\n",
      " state (14)  A[0]:(0.810020685196) A[1]:(0.900828123093) A[2]:(0.999999046326) A[3]:(0.809426665306)\n",
      " state (15)  A[0]:(0.979615986347) A[1]:(0.942625522614) A[2]:(1.0) A[3]:(0.879995524883)\n",
      "Episode 362000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6127. Times reached goal: 931.               Steps done: 2816336. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0538423193688.\n",
      " state (0)  A[0]:(0.531996130943) A[1]:(0.591014862061) A[2]:(0.59065747261) A[3]:(0.53154861927)\n",
      " state (1)  A[0]:(0.531713664532) A[1]:(0.000112056732178) A[2]:(0.656544148922) A[3]:(0.590420842171)\n",
      " state (2)  A[0]:(0.589878261089) A[1]:(0.729301333427) A[2]:(0.59315776825) A[3]:(0.657509803772)\n",
      " state (3)  A[0]:(0.657291293144) A[1]:(-0.00190377002582) A[2]:(0.475484311581) A[3]:(0.557580709457)\n",
      " state (4)  A[0]:(0.589967131615) A[1]:(0.656455457211) A[2]:(0.000941395468544) A[3]:(0.532755970955)\n",
      " state (5)  A[0]:(-0.061665289104) A[1]:(0.99989336729) A[2]:(-0.726411759853) A[3]:(0.620986580849)\n",
      " state (6)  A[0]:(-0.000142157077789) A[1]:(0.810096740723) A[2]:(-0.000508904398885) A[3]:(0.655647397041)\n",
      " state (7)  A[0]:(0.559148311615) A[1]:(-0.569465398788) A[2]:(0.400310695171) A[3]:(0.882173001766)\n",
      " state (8)  A[0]:(0.655310750008) A[1]:(-0.000336408615112) A[2]:(0.729415476322) A[3]:(0.591200232506)\n",
      " state (9)  A[0]:(0.656692326069) A[1]:(0.810022413731) A[2]:(0.810242652893) A[3]:(-6.87837600708e-05)\n",
      " state (10)  A[0]:(0.729459404945) A[1]:(0.899937033653) A[2]:(0.000205159187317) A[3]:(0.728962302208)\n",
      " state (11)  A[0]:(0.126875132322) A[1]:(0.878317952156) A[2]:(-0.912020206451) A[3]:(0.797894239426)\n",
      " state (12)  A[0]:(-0.429578661919) A[1]:(0.809399843216) A[2]:(-0.926104485989) A[3]:(0.713171720505)\n",
      " state (13)  A[0]:(-0.0012256646296) A[1]:(0.809400677681) A[2]:(0.900107145309) A[3]:(0.729206442833)\n",
      " state (14)  A[0]:(0.809867203236) A[1]:(0.900171339512) A[2]:(0.999999046326) A[3]:(0.80990666151)\n",
      " state (15)  A[0]:(0.979571998119) A[1]:(0.942221999168) A[2]:(1.0) A[3]:(0.880305290222)\n",
      "Episode 363000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6085. Times reached goal: 947.               Steps done: 2822421. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0535156836525.\n",
      " state (0)  A[0]:(0.531366586685) A[1]:(0.590506911278) A[2]:(0.590906023979) A[3]:(0.533411383629)\n",
      " state (1)  A[0]:(0.531870424747) A[1]:(-1.31577253342e-05) A[2]:(0.656118512154) A[3]:(0.592884361744)\n",
      " state (2)  A[0]:(0.590599060059) A[1]:(0.729045629501) A[2]:(0.59320306778) A[3]:(0.659057378769)\n",
      " state (3)  A[0]:(0.657949030399) A[1]:(-0.00127558340319) A[2]:(0.476229667664) A[3]:(0.559808850288)\n",
      " state (4)  A[0]:(0.590546488762) A[1]:(0.656108140945) A[2]:(0.00167000142392) A[3]:(0.534457921982)\n",
      " state (5)  A[0]:(-0.0620839558542) A[1]:(0.999893426895) A[2]:(-0.727145195007) A[3]:(0.620988547802)\n",
      " state (6)  A[0]:(0.000617250741925) A[1]:(0.810091853142) A[2]:(-0.00028932094574) A[3]:(0.656659841537)\n",
      " state (7)  A[0]:(0.559616446495) A[1]:(-0.568027853966) A[2]:(0.40002655983) A[3]:(0.882441997528)\n",
      " state (8)  A[0]:(0.654625177383) A[1]:(0.000566437782254) A[2]:(0.729202747345) A[3]:(0.59065747261)\n",
      " state (9)  A[0]:(0.655397057533) A[1]:(0.809932649136) A[2]:(0.810055077076) A[3]:(-0.00112831545994)\n",
      " state (10)  A[0]:(0.72836959362) A[1]:(0.899957358837) A[2]:(-0.000516891421285) A[3]:(0.728356838226)\n",
      " state (11)  A[0]:(0.125551059842) A[1]:(0.878493666649) A[2]:(-0.912169218063) A[3]:(0.797524333)\n",
      " state (12)  A[0]:(-0.429340302944) A[1]:(0.809664845467) A[2]:(-0.926212191582) A[3]:(0.712954521179)\n",
      " state (13)  A[0]:(5.07086515427e-05) A[1]:(0.809402883053) A[2]:(0.900209963322) A[3]:(0.729206740856)\n",
      " state (14)  A[0]:(0.810266315937) A[1]:(0.899991869926) A[2]:(0.999999046326) A[3]:(0.80993437767)\n",
      " state (15)  A[0]:(0.979601740837) A[1]:(0.942024588585) A[2]:(1.0) A[3]:(0.880284667015)\n",
      "Episode 364000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6170. Times reached goal: 957.               Steps done: 2828591. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0531865084342.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5900,  0.5904,  0.5329]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5309, -0.0005,  0.6560,  0.5917]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5895,  0.7292,  0.5922,  0.6580]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  0.8100, -0.0001,  0.6562]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7303,  0.9000,  0.0004,  0.7295]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8114,  0.8998,  1.0000,  0.8104]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531203448772) A[1]:(0.590149044991) A[2]:(0.590344905853) A[3]:(0.531918227673)\n",
      " state (1)  A[0]:(0.531202912331) A[1]:(-0.000386819214327) A[2]:(0.655918478966) A[3]:(0.591028690338)\n",
      " state (2)  A[0]:(0.589710474014) A[1]:(0.72910296917) A[2]:(0.592172503471) A[3]:(0.657500565052)\n",
      " state (3)  A[0]:(0.657142937183) A[1]:(4.90099191666e-05) A[2]:(0.475340366364) A[3]:(0.55860221386)\n",
      " state (4)  A[0]:(0.589749038219) A[1]:(0.656288087368) A[2]:(0.000421524018748) A[3]:(0.533061623573)\n",
      " state (5)  A[0]:(-0.0628908202052) A[1]:(0.999893665314) A[2]:(-0.728162050247) A[3]:(0.618839979172)\n",
      " state (6)  A[0]:(0.000946983403992) A[1]:(0.81014919281) A[2]:(-0.000428438157542) A[3]:(0.656013906002)\n",
      " state (7)  A[0]:(0.560436069965) A[1]:(-0.568035125732) A[2]:(0.399971276522) A[3]:(0.882426261902)\n",
      " state (8)  A[0]:(0.656005501747) A[1]:(0.000142946839333) A[2]:(0.729074001312) A[3]:(0.591457366943)\n",
      " state (9)  A[0]:(0.657287836075) A[1]:(0.809951543808) A[2]:(0.810185492039) A[3]:(0.000477135152323)\n",
      " state (10)  A[0]:(0.730181455612) A[1]:(0.899954319) A[2]:(0.000418901414378) A[3]:(0.729109942913)\n",
      " state (11)  A[0]:(0.129455775023) A[1]:(0.878466308117) A[2]:(-0.912063002586) A[3]:(0.798067927361)\n",
      " state (12)  A[0]:(-0.42632612586) A[1]:(0.809543251991) A[2]:(-0.926294267178) A[3]:(0.713506102562)\n",
      " state (13)  A[0]:(0.00347182247788) A[1]:(0.809192597866) A[2]:(0.900042116642) A[3]:(0.729663193226)\n",
      " state (14)  A[0]:(0.811337172985) A[1]:(0.899922430515) A[2]:(0.999999046326) A[3]:(0.8102902174)\n",
      " state (15)  A[0]:(0.979715287685) A[1]:(0.942045867443) A[2]:(1.0) A[3]:(0.880493462086)\n",
      "Episode 365000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6092. Times reached goal: 945.               Steps done: 2834683. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0528634811649.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531456589699) A[1]:(0.589976191521) A[2]:(0.590551137924) A[3]:(0.531252741814)\n",
      " state (1)  A[0]:(0.531582951546) A[1]:(-0.000217542052269) A[2]:(0.655821800232) A[3]:(0.589919030666)\n",
      " state (2)  A[0]:(0.589904844761) A[1]:(0.729329407215) A[2]:(0.592178165913) A[3]:(0.656433463097)\n",
      " state (3)  A[0]:(0.657141089439) A[1]:(0.000894918804988) A[2]:(0.475603938103) A[3]:(0.557808876038)\n",
      " state (4)  A[0]:(0.589655458927) A[1]:(0.65595459938) A[2]:(0.000130772590637) A[3]:(0.53160315752)\n",
      " state (5)  A[0]:(-0.0633329823613) A[1]:(0.999893665314) A[2]:(-0.729239702225) A[3]:(0.615591406822)\n",
      " state (6)  A[0]:(0.000468969315989) A[1]:(0.809951722622) A[2]:(-0.000656843069009) A[3]:(0.65467530489)\n",
      " state (7)  A[0]:(0.560097455978) A[1]:(-0.567799985409) A[2]:(0.399596899748) A[3]:(0.882150888443)\n",
      " state (8)  A[0]:(0.655249357224) A[1]:(0.000343933701515) A[2]:(0.728766679764) A[3]:(0.590288162231)\n",
      " state (9)  A[0]:(0.656590461731) A[1]:(0.809975326061) A[2]:(0.809956610203) A[3]:(-0.000281989574432)\n",
      " state (10)  A[0]:(0.729958176613) A[1]:(0.899960577488) A[2]:(2.74181365967e-06) A[3]:(0.729145050049)\n",
      " state (11)  A[0]:(0.129133507609) A[1]:(0.878542542458) A[2]:(-0.912167906761) A[3]:(0.798181116581)\n",
      " state (12)  A[0]:(-0.427157402039) A[1]:(0.809736251831) A[2]:(-0.926440298557) A[3]:(0.713396966457)\n",
      " state (13)  A[0]:(0.00181841652375) A[1]:(0.809423744678) A[2]:(0.900185346603) A[3]:(0.729283213615)\n",
      " state (14)  A[0]:(0.810655236244) A[1]:(0.9000659585) A[2]:(0.999999046326) A[3]:(0.809881865978)\n",
      " state (15)  A[0]:(0.979630827904) A[1]:(0.942103981972) A[2]:(1.0) A[3]:(0.880151033401)\n",
      "Episode 366000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6148. Times reached goal: 944.               Steps done: 2840831. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0525394735028.\n",
      " state (0)  A[0]:(0.531316876411) A[1]:(0.590075850487) A[2]:(0.59057456255) A[3]:(0.530671477318)\n",
      " state (1)  A[0]:(0.531107246876) A[1]:(-0.000100985169411) A[2]:(0.656367659569) A[3]:(0.589805841446)\n",
      " state (2)  A[0]:(0.589132905006) A[1]:(0.72900134325) A[2]:(0.593351721764) A[3]:(0.656598329544)\n",
      " state (3)  A[0]:(0.656759738922) A[1]:(-0.000634327472653) A[2]:(0.477413147688) A[3]:(0.558356285095)\n",
      " state (4)  A[0]:(0.588930010796) A[1]:(0.656320691109) A[2]:(0.000781178299803) A[3]:(0.531758666039)\n",
      " state (5)  A[0]:(-0.0653470158577) A[1]:(0.999893963337) A[2]:(-0.73053085804) A[3]:(0.614424407482)\n",
      " state (6)  A[0]:(-0.000711560132913) A[1]:(0.809994339943) A[2]:(-0.000464081735117) A[3]:(0.65511661768)\n",
      " state (7)  A[0]:(0.559549093246) A[1]:(-0.568583130836) A[2]:(0.400775283575) A[3]:(0.8825070858)\n",
      " state (8)  A[0]:(0.654827058315) A[1]:(-0.00042380389641) A[2]:(0.729304850101) A[3]:(0.590599060059)\n",
      " state (9)  A[0]:(0.656034469604) A[1]:(0.810010790825) A[2]:(0.810213387012) A[3]:(-0.000817119900603)\n",
      " state (10)  A[0]:(0.729063272476) A[1]:(0.899956166744) A[2]:(0.000370860070689) A[3]:(0.7287940979)\n",
      " state (11)  A[0]:(0.126162990928) A[1]:(0.878536403179) A[2]:(-0.912189483643) A[3]:(0.797937452793)\n",
      " state (12)  A[0]:(-0.429915219545) A[1]:(0.809757113457) A[2]:(-0.926469564438) A[3]:(0.713183283806)\n",
      " state (13)  A[0]:(-0.00111138774082) A[1]:(0.809507727623) A[2]:(0.90048199892) A[3]:(0.729219675064)\n",
      " state (14)  A[0]:(0.809919655323) A[1]:(0.900183141232) A[2]:(0.999999046326) A[3]:(0.809920608997)\n",
      " state (15)  A[0]:(0.979572474957) A[1]:(0.942187964916) A[2]:(1.0) A[3]:(0.880174696445)\n",
      "Episode 367000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6107. Times reached goal: 943.               Steps done: 2846938. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0522195926884.\n",
      " state (0)  A[0]:(0.531488180161) A[1]:(0.590489566326) A[2]:(0.590251326561) A[3]:(0.531675815582)\n",
      " state (1)  A[0]:(0.531322836876) A[1]:(-0.000159561634064) A[2]:(0.656287074089) A[3]:(0.590439796448)\n",
      " state (2)  A[0]:(0.589740037918) A[1]:(0.729419887066) A[2]:(0.593321681023) A[3]:(0.657013177872)\n",
      " state (3)  A[0]:(0.657462596893) A[1]:(0.000418126553996) A[2]:(0.477991759777) A[3]:(0.559198260307)\n",
      " state (4)  A[0]:(0.589728474617) A[1]:(0.656704485416) A[2]:(0.000986456521787) A[3]:(0.532368600368)\n",
      " state (5)  A[0]:(-0.0647352412343) A[1]:(0.999894320965) A[2]:(-0.731476068497) A[3]:(0.614189267159)\n",
      " state (6)  A[0]:(0.00061981374165) A[1]:(0.81031191349) A[2]:(-0.000196099281311) A[3]:(0.656185388565)\n",
      " state (7)  A[0]:(0.560892224312) A[1]:(-0.567962408066) A[2]:(0.401289790869) A[3]:(0.883091926575)\n",
      " state (8)  A[0]:(0.656161129475) A[1]:(0.000440537900431) A[2]:(0.729529321194) A[3]:(0.592059135437)\n",
      " state (9)  A[0]:(0.657641708851) A[1]:(0.810351967812) A[2]:(0.810317099094) A[3]:(0.0015855418751)\n",
      " state (10)  A[0]:(0.73026663065) A[1]:(0.900073766708) A[2]:(0.000249147415161) A[3]:(0.729756891727)\n",
      " state (11)  A[0]:(0.128237515688) A[1]:(0.878606617451) A[2]:(-0.912342846394) A[3]:(0.798487126827)\n",
      " state (12)  A[0]:(-0.428160846233) A[1]:(0.809724628925) A[2]:(-0.926749706268) A[3]:(0.713747262955)\n",
      " state (13)  A[0]:(0.00124208570924) A[1]:(0.80931854248) A[2]:(0.900066196918) A[3]:(0.729654848576)\n",
      " state (14)  A[0]:(0.810719609261) A[1]:(0.900084376335) A[2]:(0.999999046326) A[3]:(0.810136675835)\n",
      " state (15)  A[0]:(0.979660332203) A[1]:(0.942175865173) A[2]:(1.0) A[3]:(0.880194723606)\n",
      "Episode 368000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6175. Times reached goal: 959.               Steps done: 2853113. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0518981302403.\n",
      " state (0)  A[0]:(0.531623721123) A[1]:(0.590402007103) A[2]:(0.590815901756) A[3]:(0.531386494637)\n",
      " state (1)  A[0]:(0.531650900841) A[1]:(0.000253885984421) A[2]:(0.656430780888) A[3]:(0.590490758419)\n",
      " state (2)  A[0]:(0.590249896049) A[1]:(0.72908949852) A[2]:(0.592947840691) A[3]:(0.656631708145)\n",
      " state (3)  A[0]:(0.657787919044) A[1]:(-0.000325232744217) A[2]:(0.477711945772) A[3]:(0.558752655983)\n",
      " state (4)  A[0]:(0.590013027191) A[1]:(0.656436920166) A[2]:(-0.000196814537048) A[3]:(0.531514525414)\n",
      " state (5)  A[0]:(-0.0642525330186) A[1]:(0.99989426136) A[2]:(-0.732772767544) A[3]:(0.612682580948)\n",
      " state (6)  A[0]:(0.000502526701894) A[1]:(0.810042440891) A[2]:(-0.000662684324197) A[3]:(0.655387878418)\n",
      " state (7)  A[0]:(0.560274243355) A[1]:(-0.56720495224) A[2]:(0.400621503592) A[3]:(0.882689237595)\n",
      " state (8)  A[0]:(0.654755592346) A[1]:(0.000710457446985) A[2]:(0.728883385658) A[3]:(0.59012401104)\n",
      " state (9)  A[0]:(0.655567824841) A[1]:(0.810218811035) A[2]:(0.809792876244) A[3]:(-0.00175005022902)\n",
      " state (10)  A[0]:(0.728556394577) A[1]:(0.900090813637) A[2]:(-0.00098299945239) A[3]:(0.728340983391)\n",
      " state (11)  A[0]:(0.124967455864) A[1]:(0.878859877586) A[2]:(-0.912573337555) A[3]:(0.797709643841)\n",
      " state (12)  A[0]:(-0.430856525898) A[1]:(0.810334086418) A[2]:(-0.927050232887) A[3]:(0.712929308414)\n",
      " state (13)  A[0]:(-0.00218024500646) A[1]:(0.80997043848) A[2]:(0.899727106094) A[3]:(0.728984832764)\n",
      " state (14)  A[0]:(0.809510171413) A[1]:(0.900447845459) A[2]:(0.999999046326) A[3]:(0.809694230556)\n",
      " state (15)  A[0]:(0.979523599148) A[1]:(0.942385733128) A[2]:(1.0) A[3]:(0.879894256592)\n",
      "Episode 369000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6158. Times reached goal: 943.               Steps done: 2859271. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0515795235511.\n",
      "q_values \n",
      "tensor([[ 0.5310,  0.5906,  0.5892,  0.5292]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6573,  0.0020,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565,  0.0000,  0.7292,  0.5906]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6592,  0.8092,  0.8098,  0.0013]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7336,  0.8997,  0.0015,  0.7307]], device='cuda:0')\n",
      "On state=10, selected action=0 , Random? True\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6581,  0.8103,  0.8099,  0.0002]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0009,  0.8110,  0.9009,  0.7288]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8096,  0.9010,  1.0000,  0.8094]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53142696619) A[1]:(0.590533792973) A[2]:(0.590531110764) A[3]:(0.531554341316)\n",
      " state (1)  A[0]:(0.530488848686) A[1]:(-0.000598996819463) A[2]:(0.655896484852) A[3]:(0.591076374054)\n",
      " state (2)  A[0]:(0.58901065588) A[1]:(0.728695392609) A[2]:(0.592763483524) A[3]:(0.657125592232)\n",
      " state (3)  A[0]:(0.656862318516) A[1]:(-0.000763028685469) A[2]:(0.478502869606) A[3]:(0.559609293938)\n",
      " state (4)  A[0]:(0.589052677155) A[1]:(0.656217455864) A[2]:(0.000457763642771) A[3]:(0.532178878784)\n",
      " state (5)  A[0]:(-0.0660429224372) A[1]:(0.999894320965) A[2]:(-0.733751296997) A[3]:(0.612446784973)\n",
      " state (6)  A[0]:(-0.00208250875585) A[1]:(0.80995464325) A[2]:(-0.000830054108519) A[3]:(0.65530860424)\n",
      " state (7)  A[0]:(0.55846375227) A[1]:(-0.566268444061) A[2]:(0.400403290987) A[3]:(0.882584035397)\n",
      " state (8)  A[0]:(0.653499364853) A[1]:(0.00213232310489) A[2]:(0.728668093681) A[3]:(0.589342236519)\n",
      " state (9)  A[0]:(0.655102372169) A[1]:(0.810680389404) A[2]:(0.809539318085) A[3]:(-0.00196772557683)\n",
      " state (10)  A[0]:(0.728542923927) A[1]:(0.900400042534) A[2]:(-0.00171291665174) A[3]:(0.728360652924)\n",
      " state (11)  A[0]:(0.125453352928) A[1]:(0.879339396954) A[2]:(-0.912738919258) A[3]:(0.79771655798)\n",
      " state (12)  A[0]:(-0.430088192225) A[1]:(0.81111317873) A[2]:(-0.927276134491) A[3]:(0.712843000889)\n",
      " state (13)  A[0]:(-0.000992968329228) A[1]:(0.810681402683) A[2]:(0.899491965771) A[3]:(0.728799581528)\n",
      " state (14)  A[0]:(0.809904336929) A[1]:(0.900825500488) A[2]:(0.999999046326) A[3]:(0.809465289116)\n",
      " state (15)  A[0]:(0.979566037655) A[1]:(0.942622363567) A[2]:(1.0) A[3]:(0.879643261433)\n",
      "Episode 370000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6140. Times reached goal: 951.               Steps done: 2865411. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0512637955533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531325519085) A[1]:(0.590613365173) A[2]:(0.590295433998) A[3]:(0.531248152256)\n",
      " state (1)  A[0]:(0.531186044216) A[1]:(-0.000604495347943) A[2]:(0.65624332428) A[3]:(0.590065598488)\n",
      " state (2)  A[0]:(0.589624047279) A[1]:(0.729311108589) A[2]:(0.593289852142) A[3]:(0.656103014946)\n",
      " state (3)  A[0]:(0.657390713692) A[1]:(4.56720590591e-05) A[2]:(0.479700088501) A[3]:(0.558618783951)\n",
      " state (4)  A[0]:(0.589898049831) A[1]:(0.656248748302) A[2]:(0.00139701273292) A[3]:(0.531309247017)\n",
      " state (5)  A[0]:(-0.063485018909) A[1]:(0.999894618988) A[2]:(-0.734556496143) A[3]:(0.611978948116)\n",
      " state (6)  A[0]:(0.000486686796648) A[1]:(0.81026506424) A[2]:(-0.000118851661682) A[3]:(0.655261158943)\n",
      " state (7)  A[0]:(0.560308158398) A[1]:(-0.567173957825) A[2]:(0.401918888092) A[3]:(0.882687568665)\n",
      " state (8)  A[0]:(0.655388951302) A[1]:(-0.00048695501755) A[2]:(0.729318141937) A[3]:(0.590873658657)\n",
      " state (9)  A[0]:(0.656230032444) A[1]:(0.809912264347) A[2]:(0.810158550739) A[3]:(-0.00105333293322)\n",
      " state (10)  A[0]:(0.728902220726) A[1]:(0.899997472763) A[2]:(-0.000280022621155) A[3]:(0.728508889675)\n",
      " state (11)  A[0]:(0.125364497304) A[1]:(0.878888487816) A[2]:(-0.912588119507) A[3]:(0.797874927521)\n",
      " state (12)  A[0]:(-0.430089890957) A[1]:(0.810489058495) A[2]:(-0.927063703537) A[3]:(0.713255465031)\n",
      " state (13)  A[0]:(-0.000539436878171) A[1]:(0.810129106045) A[2]:(0.900349915028) A[3]:(0.729360938072)\n",
      " state (14)  A[0]:(0.810105860233) A[1]:(0.90057259798) A[2]:(0.99999910593) A[3]:(0.809920549393)\n",
      " state (15)  A[0]:(0.979582846165) A[1]:(0.942463994026) A[2]:(1.0) A[3]:(0.879906773567)\n",
      "Episode 371000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6129. Times reached goal: 953.               Steps done: 2871540. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0509505606393.\n",
      " state (0)  A[0]:(0.531084179878) A[1]:(0.589806318283) A[2]:(0.590130805969) A[3]:(0.531988620758)\n",
      " state (1)  A[0]:(0.53183555603) A[1]:(-0.00030243396759) A[2]:(0.655406236649) A[3]:(0.591412067413)\n",
      " state (2)  A[0]:(0.590113162994) A[1]:(0.728543102741) A[2]:(0.59282040596) A[3]:(0.657330751419)\n",
      " state (3)  A[0]:(0.657591223717) A[1]:(-0.00130093027838) A[2]:(0.480129778385) A[3]:(0.560376763344)\n",
      " state (4)  A[0]:(0.589782416821) A[1]:(0.656062185764) A[2]:(0.0014450539602) A[3]:(0.532985210419)\n",
      " state (5)  A[0]:(-0.0636998713017) A[1]:(0.999894499779) A[2]:(-0.735622525215) A[3]:(0.612647891045)\n",
      " state (6)  A[0]:(0.000568792165723) A[1]:(0.809691369534) A[2]:(-0.000171899795532) A[3]:(0.655549168587)\n",
      " state (7)  A[0]:(0.560642182827) A[1]:(-0.567127346992) A[2]:(0.401569873095) A[3]:(0.882787048817)\n",
      " state (8)  A[0]:(0.656133770943) A[1]:(-0.00112903071567) A[2]:(0.728598117828) A[3]:(0.592249393463)\n",
      " state (9)  A[0]:(0.65650755167) A[1]:(0.809793949127) A[2]:(0.809646368027) A[3]:(-0.000345826148987)\n",
      " state (10)  A[0]:(0.728731870651) A[1]:(0.899946510792) A[2]:(-0.00154518953059) A[3]:(0.72823536396)\n",
      " state (11)  A[0]:(0.124670229852) A[1]:(0.878840684891) A[2]:(-0.912831068039) A[3]:(0.797555267811)\n",
      " state (12)  A[0]:(-0.430453777313) A[1]:(0.810364544392) A[2]:(-0.927351534367) A[3]:(0.712875068188)\n",
      " state (13)  A[0]:(-0.000513493956532) A[1]:(0.809883832932) A[2]:(0.899977326393) A[3]:(0.729154229164)\n",
      " state (14)  A[0]:(0.810223996639) A[1]:(0.900435805321) A[2]:(0.99999910593) A[3]:(0.809886097908)\n",
      " state (15)  A[0]:(0.979601085186) A[1]:(0.942415833473) A[2]:(1.0) A[3]:(0.87988960743)\n",
      "Episode 372000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6138. Times reached goal: 939.               Steps done: 2877678. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0506387839197.\n",
      " state (0)  A[0]:(0.530822753906) A[1]:(0.590604305267) A[2]:(0.590444386005) A[3]:(0.531621694565)\n",
      " state (1)  A[0]:(0.530908584595) A[1]:(-4.06205654144e-05) A[2]:(0.656197547913) A[3]:(0.590447187424)\n",
      " state (2)  A[0]:(0.58939242363) A[1]:(0.728966712952) A[2]:(0.59235060215) A[3]:(0.656267046928)\n",
      " state (3)  A[0]:(0.657054543495) A[1]:(-0.000403016776545) A[2]:(0.479563981295) A[3]:(0.559013962746)\n",
      " state (4)  A[0]:(0.589488983154) A[1]:(0.656251847744) A[2]:(0.00010347366333) A[3]:(0.531701862812)\n",
      " state (5)  A[0]:(-0.0635017454624) A[1]:(0.999894857407) A[2]:(-0.736757516861) A[3]:(0.612460851669)\n",
      " state (6)  A[0]:(-0.000296711921692) A[1]:(0.810064911842) A[2]:(-0.000168681144714) A[3]:(0.655657291412)\n",
      " state (7)  A[0]:(0.559652268887) A[1]:(-0.566445231438) A[2]:(0.40210288763) A[3]:(0.882678449154)\n",
      " state (8)  A[0]:(0.654722929001) A[1]:(0.00033512711525) A[2]:(0.729120790958) A[3]:(0.590749502182)\n",
      " state (9)  A[0]:(0.655606627464) A[1]:(0.810217261314) A[2]:(0.809971570969) A[3]:(-0.000489532889333)\n",
      " state (10)  A[0]:(0.728338241577) A[1]:(0.900087237358) A[2]:(-0.000599265040364) A[3]:(0.72862136364)\n",
      " state (11)  A[0]:(0.123644709587) A[1]:(0.878956317902) A[2]:(-0.912745714188) A[3]:(0.797812104225)\n",
      " state (12)  A[0]:(-0.431840896606) A[1]:(0.810453116894) A[2]:(-0.927386343479) A[3]:(0.712930381298)\n",
      " state (13)  A[0]:(-0.0025566611439) A[1]:(0.809892296791) A[2]:(0.900101542473) A[3]:(0.729015111923)\n",
      " state (14)  A[0]:(0.809515118599) A[1]:(0.90043848753) A[2]:(0.99999910593) A[3]:(0.809739470482)\n",
      " state (15)  A[0]:(0.97952067852) A[1]:(0.942410707474) A[2]:(1.0) A[3]:(0.879754722118)\n",
      "Episode 373000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6155. Times reached goal: 940.               Steps done: 2883833. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0503280594403.\n",
      " state (0)  A[0]:(0.531209349632) A[1]:(0.590146780014) A[2]:(0.59012401104) A[3]:(0.531213641167)\n",
      " state (1)  A[0]:(0.531280636787) A[1]:(-1.8835067749e-05) A[2]:(0.65595126152) A[3]:(0.589916288853)\n",
      " state (2)  A[0]:(0.589734792709) A[1]:(0.728502631187) A[2]:(0.591905474663) A[3]:(0.656174063683)\n",
      " state (3)  A[0]:(0.657177150249) A[1]:(-2.31266021729e-05) A[2]:(0.479474753141) A[3]:(0.559161007404)\n",
      " state (4)  A[0]:(0.589694738388) A[1]:(0.656047344208) A[2]:(-1.25169754028e-05) A[3]:(0.531467914581)\n",
      " state (5)  A[0]:(-0.0627601593733) A[1]:(0.999894917011) A[2]:(-0.737420499325) A[3]:(0.611224591732)\n",
      " state (6)  A[0]:(0.00101444090251) A[1]:(0.8098320961) A[2]:(-0.000577092112508) A[3]:(0.65528011322)\n",
      " state (7)  A[0]:(0.561044573784) A[1]:(-0.566297888756) A[2]:(0.401465684175) A[3]:(0.882717967033)\n",
      " state (8)  A[0]:(0.656063437462) A[1]:(0.000131845474243) A[2]:(0.728663086891) A[3]:(0.591599702835)\n",
      " state (9)  A[0]:(0.656924009323) A[1]:(0.810158610344) A[2]:(0.809875905514) A[3]:(0.0010393258417)\n",
      " state (10)  A[0]:(0.72978079319) A[1]:(0.900022625923) A[2]:(0.000107049942017) A[3]:(0.729356527328)\n",
      " state (11)  A[0]:(0.127196401358) A[1]:(0.878824055195) A[2]:(-0.912606358528) A[3]:(0.79832392931)\n",
      " state (12)  A[0]:(-0.428935706615) A[1]:(0.810116171837) A[2]:(-0.92740547657) A[3]:(0.713356256485)\n",
      " state (13)  A[0]:(0.00074833619874) A[1]:(0.809421420097) A[2]:(0.900072991848) A[3]:(0.729225754738)\n",
      " state (14)  A[0]:(0.810488343239) A[1]:(0.900216042995) A[2]:(0.99999910593) A[3]:(0.809810459614)\n",
      " state (15)  A[0]:(0.979609310627) A[1]:(0.942347347736) A[2]:(1.0) A[3]:(0.879726231098)\n",
      "Episode 374000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6143. Times reached goal: 948.               Steps done: 2889976. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0500198418307.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5907,  0.5906,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.6561, -0.0000,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6554,  0.0004,  0.7291,  0.5905]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.8101,  0.8099,  0.0000]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0007,  0.8093,  0.9000,  0.7295]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531776070595) A[1]:(0.590641140938) A[2]:(0.590717017651) A[3]:(0.531520247459)\n",
      " state (1)  A[0]:(0.531905412674) A[1]:(-0.000214979052544) A[2]:(0.656240463257) A[3]:(0.590327382088)\n",
      " state (2)  A[0]:(0.590521395206) A[1]:(0.729121029377) A[2]:(0.591828584671) A[3]:(0.656367182732)\n",
      " state (3)  A[0]:(0.657867670059) A[1]:(0.00196630996652) A[2]:(0.479450881481) A[3]:(0.559572815895)\n",
      " state (4)  A[0]:(0.590456843376) A[1]:(0.656074285507) A[2]:(-6.74724578857e-05) A[3]:(0.531424641609)\n",
      " state (5)  A[0]:(-0.0628222003579) A[1]:(0.999895095825) A[2]:(-0.737857282162) A[3]:(0.610145449638)\n",
      " state (6)  A[0]:(0.000941142148804) A[1]:(0.80996131897) A[2]:(-0.000214815139771) A[3]:(0.655438542366)\n",
      " state (7)  A[0]:(0.561094522476) A[1]:(-0.566240310669) A[2]:(0.401936560869) A[3]:(0.882865786552)\n",
      " state (8)  A[0]:(0.655827641487) A[1]:(0.000127628445625) A[2]:(0.728913545609) A[3]:(0.591022372246)\n",
      " state (9)  A[0]:(0.656705379486) A[1]:(0.810127735138) A[2]:(0.80992859602) A[3]:(0.000100016593933)\n",
      " state (10)  A[0]:(0.729497313499) A[1]:(0.900006592274) A[2]:(-0.000273585319519) A[3]:(0.729058861732)\n",
      " state (11)  A[0]:(0.126224026084) A[1]:(0.878834426403) A[2]:(-0.912798643112) A[3]:(0.798174321651)\n",
      " state (12)  A[0]:(-0.429584622383) A[1]:(0.810109376907) A[2]:(-0.927618801594) A[3]:(0.713348507881)\n",
      " state (13)  A[0]:(0.00029245018959) A[1]:(0.809330165386) A[2]:(0.900079011917) A[3]:(0.729388237)\n",
      " state (14)  A[0]:(0.810345411301) A[1]:(0.900146305561) A[2]:(0.99999910593) A[3]:(0.80997467041)\n",
      " state (15)  A[0]:(0.979589819908) A[1]:(0.942290186882) A[2]:(1.0) A[3]:(0.879818022251)\n",
      "Episode 375000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6131. Times reached goal: 944.               Steps done: 2896107. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0497141083641.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531994223595) A[1]:(0.59044110775) A[2]:(0.590394258499) A[3]:(0.531554162502)\n",
      " state (1)  A[0]:(0.531924366951) A[1]:(-0.00025986135006) A[2]:(0.655970454216) A[3]:(0.590473771095)\n",
      " state (2)  A[0]:(0.590277194977) A[1]:(0.729068517685) A[2]:(0.592417478561) A[3]:(0.656582951546)\n",
      " state (3)  A[0]:(0.657430768013) A[1]:(0.00258327438496) A[2]:(0.480408579111) A[3]:(0.560153841972)\n",
      " state (4)  A[0]:(0.589865326881) A[1]:(0.656168162823) A[2]:(0.000712156179361) A[3]:(0.531659603119)\n",
      " state (5)  A[0]:(-0.0635273754597) A[1]:(0.999895274639) A[2]:(-0.738523483276) A[3]:(0.60915017128)\n",
      " state (6)  A[0]:(0.000508010329213) A[1]:(0.809935092926) A[2]:(-0.00133848108817) A[3]:(0.655472993851)\n",
      " state (7)  A[0]:(0.561037540436) A[1]:(-0.566368818283) A[2]:(0.401172369719) A[3]:(0.883018672466)\n",
      " state (8)  A[0]:(0.655494809151) A[1]:(0.000287070870399) A[2]:(0.728805959225) A[3]:(0.590974152088)\n",
      " state (9)  A[0]:(0.656410932541) A[1]:(0.810212850571) A[2]:(0.809986531734) A[3]:(0.000614285410848)\n",
      " state (10)  A[0]:(0.729290246964) A[1]:(0.90000385046) A[2]:(-8.78572463989e-05) A[3]:(0.729213535786)\n",
      " state (11)  A[0]:(0.125603482127) A[1]:(0.878743052483) A[2]:(-0.912873744965) A[3]:(0.798036038876)\n",
      " state (12)  A[0]:(-0.429953962564) A[1]:(0.809764683247) A[2]:(-0.927814483643) A[3]:(0.713006079197)\n",
      " state (13)  A[0]:(-1.60932540894e-05) A[1]:(0.808745622635) A[2]:(0.899895966053) A[3]:(0.729128837585)\n",
      " state (14)  A[0]:(0.81015586853) A[1]:(0.899782896042) A[2]:(0.99999910593) A[3]:(0.809877336025)\n",
      " state (15)  A[0]:(0.979553103447) A[1]:(0.942089259624) A[2]:(1.0) A[3]:(0.879777371883)\n",
      "Episode 376000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6156. Times reached goal: 949.               Steps done: 2902263. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0494090083743.\n",
      " state (0)  A[0]:(0.531773746014) A[1]:(0.590710520744) A[2]:(0.590387940407) A[3]:(0.531577348709)\n",
      " state (1)  A[0]:(0.531793832779) A[1]:(-0.000115349888802) A[2]:(0.656197190285) A[3]:(0.590196728706)\n",
      " state (2)  A[0]:(0.590199410915) A[1]:(0.729058742523) A[2]:(0.591987967491) A[3]:(0.656338214874)\n",
      " state (3)  A[0]:(0.657579421997) A[1]:(0.00256947614253) A[2]:(0.480085462332) A[3]:(0.560269892216)\n",
      " state (4)  A[0]:(0.590285658836) A[1]:(0.656192481518) A[2]:(3.36170196533e-05) A[3]:(0.531735420227)\n",
      " state (5)  A[0]:(-0.0628985241055) A[1]:(0.999895393848) A[2]:(-0.739053606987) A[3]:(0.608436942101)\n",
      " state (6)  A[0]:(0.000564962567296) A[1]:(0.809967517853) A[2]:(-0.000557064951863) A[3]:(0.655715763569)\n",
      " state (7)  A[0]:(0.561034560204) A[1]:(-0.565935194492) A[2]:(0.401819437742) A[3]:(0.883213400841)\n",
      " state (8)  A[0]:(0.655492126942) A[1]:(0.000211045145988) A[2]:(0.72885710001) A[3]:(0.59102332592)\n",
      " state (9)  A[0]:(0.656297564507) A[1]:(0.810070633888) A[2]:(0.809890329838) A[3]:(-0.000346183776855)\n",
      " state (10)  A[0]:(0.729145407677) A[1]:(0.90001142025) A[2]:(-0.000495195330586) A[3]:(0.728805601597)\n",
      " state (11)  A[0]:(0.125197708607) A[1]:(0.87897258997) A[2]:(-0.913006663322) A[3]:(0.797930181026)\n",
      " state (12)  A[0]:(-0.430278509855) A[1]:(0.810395300388) A[2]:(-0.927965044975) A[3]:(0.713050127029)\n",
      " state (13)  A[0]:(-0.000232070684433) A[1]:(0.809541881084) A[2]:(0.899975061417) A[3]:(0.729216337204)\n",
      " state (14)  A[0]:(0.810175120831) A[1]:(0.900291144848) A[2]:(0.99999910593) A[3]:(0.809892952442)\n",
      " state (15)  A[0]:(0.97957110405) A[1]:(0.942403495312) A[2]:(1.0) A[3]:(0.879716217518)\n",
      "Episode 377000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6148. Times reached goal: 944.               Steps done: 2908411. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0491061736586.\n",
      " state (0)  A[0]:(0.531309723854) A[1]:(0.590470254421) A[2]:(0.590351104736) A[3]:(0.531435370445)\n",
      " state (1)  A[0]:(0.530940294266) A[1]:(6.99609518051e-05) A[2]:(0.656024813652) A[3]:(0.590361237526)\n",
      " state (2)  A[0]:(0.58931517601) A[1]:(0.728981494904) A[2]:(0.592057824135) A[3]:(0.656178474426)\n",
      " state (3)  A[0]:(0.656726717949) A[1]:(0.00204764027148) A[2]:(0.480623483658) A[3]:(0.560090780258)\n",
      " state (4)  A[0]:(0.589353203773) A[1]:(0.656089603901) A[2]:(0.000312089920044) A[3]:(0.531693458557)\n",
      " state (5)  A[0]:(-0.0634960159659) A[1]:(0.999895572662) A[2]:(-0.739583015442) A[3]:(0.60890853405)\n",
      " state (6)  A[0]:(9.84817743301e-05) A[1]:(0.809925019741) A[2]:(-7.68899917603e-05) A[3]:(0.656101822853)\n",
      " state (7)  A[0]:(0.560903072357) A[1]:(-0.565880775452) A[2]:(0.402379661798) A[3]:(0.883328855038)\n",
      " state (8)  A[0]:(0.655689835548) A[1]:(-0.000208273530006) A[2]:(0.729080498219) A[3]:(0.591796994209)\n",
      " state (9)  A[0]:(0.656812727451) A[1]:(0.809868514538) A[2]:(0.810070633888) A[3]:(0.00153314950876)\n",
      " state (10)  A[0]:(0.729729413986) A[1]:(0.899908006191) A[2]:(1.31130218506e-06) A[3]:(0.729639291763)\n",
      " state (11)  A[0]:(0.12658803165) A[1]:(0.878860116005) A[2]:(-0.912986040115) A[3]:(0.798453330994)\n",
      " state (12)  A[0]:(-0.428969860077) A[1]:(0.810153007507) A[2]:(-0.927995026112) A[3]:(0.71359038353)\n",
      " state (13)  A[0]:(0.00135092355777) A[1]:(0.809142768383) A[2]:(0.900142490864) A[3]:(0.729655385017)\n",
      " state (14)  A[0]:(0.810561656952) A[1]:(0.900006115437) A[2]:(0.999999165535) A[3]:(0.810186743736)\n",
      " state (15)  A[0]:(0.979594886303) A[1]:(0.942206323147) A[2]:(1.0) A[3]:(0.87986177206)\n",
      "Episode 378000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6128. Times reached goal: 948.               Steps done: 2914539. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0488061711729.\n",
      " state (0)  A[0]:(0.531530022621) A[1]:(0.590409219265) A[2]:(0.590447425842) A[3]:(0.531537294388)\n",
      " state (1)  A[0]:(0.531559944153) A[1]:(-4.9501657486e-05) A[2]:(0.656252264977) A[3]:(0.590376079082)\n",
      " state (2)  A[0]:(0.589928090572) A[1]:(0.728908538818) A[2]:(0.592037498951) A[3]:(0.656215131283)\n",
      " state (3)  A[0]:(0.657188117504) A[1]:(0.00176247768104) A[2]:(0.480963081121) A[3]:(0.559927284718)\n",
      " state (4)  A[0]:(0.589737296104) A[1]:(0.656067609787) A[2]:(0.000274062156677) A[3]:(0.53167784214)\n",
      " state (5)  A[0]:(-0.0634614378214) A[1]:(0.999895691872) A[2]:(-0.740388095379) A[3]:(0.609592795372)\n",
      " state (6)  A[0]:(-0.00045438107918) A[1]:(0.809911429882) A[2]:(-0.000329732894897) A[3]:(0.655823230743)\n",
      " state (7)  A[0]:(0.560472548008) A[1]:(-0.565740942955) A[2]:(0.402367562056) A[3]:(0.883048474789)\n",
      " state (8)  A[0]:(0.655047595501) A[1]:(6.23166561127e-05) A[2]:(0.728992342949) A[3]:(0.59063154459)\n",
      " state (9)  A[0]:(0.655817806721) A[1]:(0.810024142265) A[2]:(0.810008049011) A[3]:(-0.000539660395589)\n",
      " state (10)  A[0]:(0.728903651237) A[1]:(0.899961352348) A[2]:(-0.000163078308105) A[3]:(0.728888750076)\n",
      " state (11)  A[0]:(0.124845735729) A[1]:(0.87893140316) A[2]:(-0.913087189198) A[3]:(0.798080921173)\n",
      " state (12)  A[0]:(-0.430301249027) A[1]:(0.810285151005) A[2]:(-0.928171932697) A[3]:(0.713245511055)\n",
      " state (13)  A[0]:(0.000122606754303) A[1]:(0.809322953224) A[2]:(0.900044322014) A[3]:(0.729400515556)\n",
      " state (14)  A[0]:(0.8103043437) A[1]:(0.900200664997) A[2]:(0.999999165535) A[3]:(0.810007333755)\n",
      " state (15)  A[0]:(0.979577362537) A[1]:(0.942382693291) A[2]:(1.0) A[3]:(0.879695951939)\n",
      "Episode 379000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6128. Times reached goal: 952.               Steps done: 2920667. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0485080014809.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5909,  0.5912,  0.5300]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0005,  0.6565,  0.5900]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5898,  0.7291,  0.5922,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0005,  0.8100,  0.0000,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7298,  0.8999,  0.0004,  0.7296]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9002,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532065153122) A[1]:(0.590855360031) A[2]:(0.590907335281) A[3]:(0.529886782169)\n",
      " state (1)  A[0]:(0.531709551811) A[1]:(-0.000236332416534) A[2]:(0.656443476677) A[3]:(0.590143680573)\n",
      " state (2)  A[0]:(0.590253531933) A[1]:(0.729080557823) A[2]:(0.59226167202) A[3]:(0.6562063694)\n",
      " state (3)  A[0]:(0.657469034195) A[1]:(0.00122731865849) A[2]:(0.481495916843) A[3]:(0.559637665749)\n",
      " state (4)  A[0]:(0.590234279633) A[1]:(0.656232833862) A[2]:(0.000118136405945) A[3]:(0.531537652016)\n",
      " state (5)  A[0]:(-0.0615300424397) A[1]:(0.999895870686) A[2]:(-0.741272211075) A[3]:(0.610327601433)\n",
      " state (6)  A[0]:(0.000286266207695) A[1]:(0.81007540226) A[2]:(1.14440917969e-05) A[3]:(0.655872821808)\n",
      " state (7)  A[0]:(0.560689806938) A[1]:(-0.565536141396) A[2]:(0.403158545494) A[3]:(0.882992684841)\n",
      " state (8)  A[0]:(0.65532130003) A[1]:(-0.000315248966217) A[2]:(0.729097902775) A[3]:(0.590925872326)\n",
      " state (9)  A[0]:(0.655714631081) A[1]:(0.809843242168) A[2]:(0.809905648232) A[3]:(-0.000477552384837)\n",
      " state (10)  A[0]:(0.728425502777) A[1]:(0.899897992611) A[2]:(-0.00105690921191) A[3]:(0.728837490082)\n",
      " state (11)  A[0]:(0.12306971103) A[1]:(0.878936290741) A[2]:(-0.913364708424) A[3]:(0.798070192337)\n",
      " state (12)  A[0]:(-0.432027280331) A[1]:(0.810371398926) A[2]:(-0.928433716297) A[3]:(0.71331346035)\n",
      " state (13)  A[0]:(-0.00208399887197) A[1]:(0.809408903122) A[2]:(0.900016605854) A[3]:(0.72955095768)\n",
      " state (14)  A[0]:(0.809502124786) A[1]:(0.900228261948) A[2]:(0.999999165535) A[3]:(0.810159683228)\n",
      " state (15)  A[0]:(0.979483425617) A[1]:(0.942360281944) A[2]:(1.0) A[3]:(0.879783093929)\n",
      "Episode 380000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6094. Times reached goal: 954.               Steps done: 2926761. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0482132926099.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53230214119) A[1]:(0.590612113476) A[2]:(0.590755283833) A[3]:(0.531538963318)\n",
      " state (1)  A[0]:(0.532317578793) A[1]:(-0.000257670879364) A[2]:(0.656357288361) A[3]:(0.590528130531)\n",
      " state (2)  A[0]:(0.590926110744) A[1]:(0.729009270668) A[2]:(0.592121243477) A[3]:(0.656205654144)\n",
      " state (3)  A[0]:(0.657941222191) A[1]:(0.00118333043065) A[2]:(0.481890678406) A[3]:(0.559338748455)\n",
      " state (4)  A[0]:(0.590854942799) A[1]:(0.656182765961) A[2]:(0.000470757455332) A[3]:(0.531316876411)\n",
      " state (5)  A[0]:(-0.0599289834499) A[1]:(0.999895870686) A[2]:(-0.741809010506) A[3]:(0.611027121544)\n",
      " state (6)  A[0]:(0.00105312431697) A[1]:(0.809966564178) A[2]:(5.57899475098e-05) A[3]:(0.655598640442)\n",
      " state (7)  A[0]:(0.560958981514) A[1]:(-0.5650177598) A[2]:(0.403166651726) A[3]:(0.882633328438)\n",
      " state (8)  A[0]:(0.6556635499) A[1]:(6.44326210022e-05) A[2]:(0.729071855545) A[3]:(0.59017932415)\n",
      " state (9)  A[0]:(0.656045913696) A[1]:(0.810025036335) A[2]:(0.810155808926) A[3]:(-0.00197577220388)\n",
      " state (10)  A[0]:(0.728978157043) A[1]:(0.90004914999) A[2]:(0.000556707323994) A[3]:(0.728057920933)\n",
      " state (11)  A[0]:(0.124756075442) A[1]:(0.87917214632) A[2]:(-0.913090825081) A[3]:(0.797523915768)\n",
      " state (12)  A[0]:(-0.430710047483) A[1]:(0.810695827007) A[2]:(-0.92831325531) A[3]:(0.712496578693)\n",
      " state (13)  A[0]:(-0.000678777578287) A[1]:(0.809608995914) A[2]:(0.900290071964) A[3]:(0.728780627251)\n",
      " state (14)  A[0]:(0.809925019741) A[1]:(0.900296270847) A[2]:(0.999999165535) A[3]:(0.809697031975)\n",
      " state (15)  A[0]:(0.979527056217) A[1]:(0.94238191843) A[2]:(1.0) A[3]:(0.879526257515)\n",
      "Episode 381000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6126. Times reached goal: 956.               Steps done: 2932887. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0479188408061.\n",
      " state (0)  A[0]:(0.530979990959) A[1]:(0.590247035027) A[2]:(0.590104341507) A[3]:(0.531008839607)\n",
      " state (1)  A[0]:(0.531046509743) A[1]:(4.8816204071e-05) A[2]:(0.65563595295) A[3]:(0.590033173561)\n",
      " state (2)  A[0]:(0.589556336403) A[1]:(0.728749871254) A[2]:(0.591575026512) A[3]:(0.655882120132)\n",
      " state (3)  A[0]:(0.656687259674) A[1]:(0.000122010707855) A[2]:(0.481838494539) A[3]:(0.559050738811)\n",
      " state (4)  A[0]:(0.589594006538) A[1]:(0.656056404114) A[2]:(-0.000105023384094) A[3]:(0.531394004822)\n",
      " state (5)  A[0]:(-0.0603907331824) A[1]:(0.999895989895) A[2]:(-0.742848396301) A[3]:(0.611686110497)\n",
      " state (6)  A[0]:(0.000435709924204) A[1]:(0.809928357601) A[2]:(-0.000345349311829) A[3]:(0.655741631985)\n",
      " state (7)  A[0]:(0.560790300369) A[1]:(-0.564921081066) A[2]:(0.403139770031) A[3]:(0.882752835751)\n",
      " state (8)  A[0]:(0.655905306339) A[1]:(-2.31862068176e-05) A[2]:(0.72887635231) A[3]:(0.591662883759)\n",
      " state (9)  A[0]:(0.656562447548) A[1]:(0.809969365597) A[2]:(0.809998869896) A[3]:(0.00163489428815)\n",
      " state (10)  A[0]:(0.729659557343) A[1]:(0.899984300137) A[2]:(0.000338315963745) A[3]:(0.72986972332)\n",
      " state (11)  A[0]:(0.126292258501) A[1]:(0.879098773003) A[2]:(-0.913189291954) A[3]:(0.798796713352)\n",
      " state (12)  A[0]:(-0.429822921753) A[1]:(0.810608208179) A[2]:(-0.928539216518) A[3]:(0.713806271553)\n",
      " state (13)  A[0]:(-0.000136464834213) A[1]:(0.809558212757) A[2]:(0.900026917458) A[3]:(0.729682803154)\n",
      " state (14)  A[0]:(0.809964060783) A[1]:(0.900359392166) A[2]:(0.999999165535) A[3]:(0.810181617737)\n",
      " state (15)  A[0]:(0.979518711567) A[1]:(0.942486047745) A[2]:(1.0) A[3]:(0.879716396332)\n",
      "Episode 382000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6118. Times reached goal: 957.               Steps done: 2939005. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0476265683112.\n",
      " state (0)  A[0]:(0.530982613564) A[1]:(0.5905854702) A[2]:(0.590045452118) A[3]:(0.530014038086)\n",
      " state (1)  A[0]:(0.531095385551) A[1]:(-0.00038810071419) A[2]:(0.655952095985) A[3]:(0.589830517769)\n",
      " state (2)  A[0]:(0.589689075947) A[1]:(0.72892677784) A[2]:(0.591982841492) A[3]:(0.655819118023)\n",
      " state (3)  A[0]:(0.656686067581) A[1]:(-0.000518277229276) A[2]:(0.482771962881) A[3]:(0.558629512787)\n",
      " state (4)  A[0]:(0.589590787888) A[1]:(0.655874848366) A[2]:(0.000229716300964) A[3]:(0.53093624115)\n",
      " state (5)  A[0]:(-0.0596289373934) A[1]:(0.999895989895) A[2]:(-0.743878364563) A[3]:(0.611878573895)\n",
      " state (6)  A[0]:(0.000143125653267) A[1]:(0.809878110886) A[2]:(-0.000682830694132) A[3]:(0.655116558075)\n",
      " state (7)  A[0]:(0.560113191605) A[1]:(-0.564201593399) A[2]:(0.403127104044) A[3]:(0.882251620293)\n",
      " state (8)  A[0]:(0.655229389668) A[1]:(0.000462815136416) A[2]:(0.728768527508) A[3]:(0.58997631073)\n",
      " state (9)  A[0]:(0.655711770058) A[1]:(0.810022592545) A[2]:(0.809920072556) A[3]:(-0.00149440648966)\n",
      " state (10)  A[0]:(0.728893876076) A[1]:(0.900058209896) A[2]:(-5.80549240112e-05) A[3]:(0.728307127953)\n",
      " state (11)  A[0]:(0.125036388636) A[1]:(0.879255056381) A[2]:(-0.913341104984) A[3]:(0.797746181488)\n",
      " state (12)  A[0]:(-0.430241584778) A[1]:(0.810793638229) A[2]:(-0.928724884987) A[3]:(0.712720274925)\n",
      " state (13)  A[0]:(-0.000118359923363) A[1]:(0.809527873993) A[2]:(0.899965941906) A[3]:(0.728987932205)\n",
      " state (14)  A[0]:(0.80997222662) A[1]:(0.900214612484) A[2]:(0.999999165535) A[3]:(0.809877693653)\n",
      " state (15)  A[0]:(0.979513764381) A[1]:(0.942330539227) A[2]:(1.0) A[3]:(0.879585146904)\n",
      "Episode 383000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6105. Times reached goal: 940.               Steps done: 2945110. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0473366938538.\n",
      " state (0)  A[0]:(0.531431555748) A[1]:(0.590663731098) A[2]:(0.590378701687) A[3]:(0.531582772732)\n",
      " state (1)  A[0]:(0.531407475471) A[1]:(-7.94380903244e-05) A[2]:(0.656087875366) A[3]:(0.590412735939)\n",
      " state (2)  A[0]:(0.589904725552) A[1]:(0.729185461998) A[2]:(0.591700792313) A[3]:(0.656221151352)\n",
      " state (3)  A[0]:(0.656843423843) A[1]:(0.000184968113899) A[2]:(0.48284637928) A[3]:(0.559045910835)\n",
      " state (4)  A[0]:(0.589812636375) A[1]:(0.656349241734) A[2]:(8.97645950317e-05) A[3]:(0.531658887863)\n",
      " state (5)  A[0]:(-0.0590905100107) A[1]:(0.999896287918) A[2]:(-0.744463920593) A[3]:(0.613437414169)\n",
      " state (6)  A[0]:(0.000131577253342) A[1]:(0.810086548328) A[2]:(-0.000416398019297) A[3]:(0.656079053879)\n",
      " state (7)  A[0]:(0.560302138329) A[1]:(-0.564307451248) A[2]:(0.403773933649) A[3]:(0.882607042789)\n",
      " state (8)  A[0]:(0.655871450901) A[1]:(0.00066895771306) A[2]:(0.729075193405) A[3]:(0.591162323952)\n",
      " state (9)  A[0]:(0.656814455986) A[1]:(0.810217559338) A[2]:(0.810030221939) A[3]:(0.000913977390155)\n",
      " state (10)  A[0]:(0.729683160782) A[1]:(0.900039374828) A[2]:(-0.000348687171936) A[3]:(0.72939658165)\n",
      " state (11)  A[0]:(0.126026719809) A[1]:(0.879073023796) A[2]:(-0.913559496403) A[3]:(0.798353552818)\n",
      " state (12)  A[0]:(-0.429485708475) A[1]:(0.810340106487) A[2]:(-0.929022371769) A[3]:(0.713310241699)\n",
      " state (13)  A[0]:(0.0011700236937) A[1]:(0.809004664421) A[2]:(0.899731040001) A[3]:(0.72945356369)\n",
      " state (14)  A[0]:(0.810521364212) A[1]:(0.900005578995) A[2]:(0.999999165535) A[3]:(0.810158729553)\n",
      " state (15)  A[0]:(0.9795794487) A[1]:(0.942271530628) A[2]:(1.0) A[3]:(0.879688620567)\n",
      "Episode 384000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6143. Times reached goal: 947.               Steps done: 2951253. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0470467958767.\n",
      "q_values \n",
      "tensor([[ 0.5330,  0.5906,  0.5908,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5325, -0.0005,  0.6563,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=3 , Random? True\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5322, -0.0003,  0.6563,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.7289,  0.5916,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0005,  0.8101, -0.0000,  0.6555]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7294,  0.9001,  0.0001,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9004,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53078854084) A[1]:(0.590609073639) A[2]:(0.590710997581) A[3]:(0.531499862671)\n",
      " state (1)  A[0]:(0.530548095703) A[1]:(0.000242426991463) A[2]:(0.656189441681) A[3]:(0.590929269791)\n",
      " state (2)  A[0]:(0.58948302269) A[1]:(0.729003787041) A[2]:(0.59181201458) A[3]:(0.656578540802)\n",
      " state (3)  A[0]:(0.656484901905) A[1]:(0.000350192189217) A[2]:(0.483534365892) A[3]:(0.559364616871)\n",
      " state (4)  A[0]:(0.589647769928) A[1]:(0.656188845634) A[2]:(0.00105571711902) A[3]:(0.532198786736)\n",
      " state (5)  A[0]:(-0.0587268471718) A[1]:(0.999896287918) A[2]:(-0.74457359314) A[3]:(0.614779829979)\n",
      " state (6)  A[0]:(-6.04689121246e-05) A[1]:(0.809947788715) A[2]:(0.000146508216858) A[3]:(0.656901299953)\n",
      " state (7)  A[0]:(0.56017267704) A[1]:(-0.563955783844) A[2]:(0.403878897429) A[3]:(0.882858753204)\n",
      " state (8)  A[0]:(0.655994176865) A[1]:(0.000868037110195) A[2]:(0.728876113892) A[3]:(0.592446446419)\n",
      " state (9)  A[0]:(0.657044053078) A[1]:(0.810311973095) A[2]:(0.809901416302) A[3]:(0.00321136810817)\n",
      " state (10)  A[0]:(0.730017483234) A[1]:(0.900150537491) A[2]:(-0.000408172578318) A[3]:(0.730461895466)\n",
      " state (11)  A[0]:(0.126688748598) A[1]:(0.879317939281) A[2]:(-0.913626372814) A[3]:(0.799095094204)\n",
      " state (12)  A[0]:(-0.429456800222) A[1]:(0.810830831528) A[2]:(-0.929177165031) A[3]:(0.71396791935)\n",
      " state (13)  A[0]:(0.000649064662866) A[1]:(0.80955350399) A[2]:(0.899762511253) A[3]:(0.72978913784)\n",
      " state (14)  A[0]:(0.810258865356) A[1]:(0.900340974331) A[2]:(0.999999165535) A[3]:(0.810275375843)\n",
      " state (15)  A[0]:(0.979549825191) A[1]:(0.942463994026) A[2]:(1.0) A[3]:(0.879692673683)\n",
      "Episode 385000 finished after 0 timesteps with r=1.0. Running score: 0.91. Times trained:               6103. Times reached goal: 947.               Steps done: 2957356. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0467605436686.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531803965569) A[1]:(0.590809464455) A[2]:(0.590462207794) A[3]:(0.53157043457)\n",
      " state (1)  A[0]:(0.531773924828) A[1]:(0.000133693218231) A[2]:(0.656260728836) A[3]:(0.590402841568)\n",
      " state (2)  A[0]:(0.590221345425) A[1]:(0.728999853134) A[2]:(0.591969251633) A[3]:(0.656197190285)\n",
      " state (3)  A[0]:(0.657103657722) A[1]:(-0.000700294855051) A[2]:(0.483967632055) A[3]:(0.55876326561)\n",
      " state (4)  A[0]:(0.590318799019) A[1]:(0.656263589859) A[2]:(0.000476241082652) A[3]:(0.53155040741)\n",
      " state (5)  A[0]:(-0.0571445599198) A[1]:(0.999896347523) A[2]:(-0.745941996574) A[3]:(0.614292740822)\n",
      " state (6)  A[0]:(0.000463470787508) A[1]:(0.809955179691) A[2]:(-0.000492215098348) A[3]:(0.656020998955)\n",
      " state (7)  A[0]:(0.560068964958) A[1]:(-0.564097821712) A[2]:(0.404254436493) A[3]:(0.882402002811)\n",
      " state (8)  A[0]:(0.655722379684) A[1]:(8.60095024109e-05) A[2]:(0.72902148962) A[3]:(0.590976119041)\n",
      " state (9)  A[0]:(0.656271755695) A[1]:(0.809986352921) A[2]:(0.810014784336) A[3]:(-6.66975975037e-05)\n",
      " state (10)  A[0]:(0.729136645794) A[1]:(0.899988353252) A[2]:(-0.000137805938721) A[3]:(0.728900253773)\n",
      " state (11)  A[0]:(0.124902874231) A[1]:(0.87917381525) A[2]:(-0.913622617722) A[3]:(0.798108875751)\n",
      " state (12)  A[0]:(-0.430424332619) A[1]:(0.810626864433) A[2]:(-0.929141104221) A[3]:(0.713012039661)\n",
      " state (13)  A[0]:(2.35885381699e-05) A[1]:(0.809297323227) A[2]:(0.900219202042) A[3]:(0.729167222977)\n",
      " state (14)  A[0]:(0.81008130312) A[1]:(0.900172054768) A[2]:(0.99999922514) A[3]:(0.809939920902)\n",
      " state (15)  A[0]:(0.979522705078) A[1]:(0.942328035831) A[2]:(1.0) A[3]:(0.879493892193)\n",
      "Episode 386000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6091. Times reached goal: 957.               Steps done: 2963447. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0464765908533.\n",
      " state (0)  A[0]:(0.531476378441) A[1]:(0.590596854687) A[2]:(0.590478420258) A[3]:(0.531452178955)\n",
      " state (1)  A[0]:(0.531386852264) A[1]:(-0.000104010105133) A[2]:(0.656262159348) A[3]:(0.590342760086)\n",
      " state (2)  A[0]:(0.590036511421) A[1]:(0.728891015053) A[2]:(0.591508209705) A[3]:(0.656168937683)\n",
      " state (3)  A[0]:(0.656834483147) A[1]:(-0.000623524130788) A[2]:(0.483673721552) A[3]:(0.558650374413)\n",
      " state (4)  A[0]:(0.5900349617) A[1]:(0.656043767929) A[2]:(-3.21865081787e-06) A[3]:(0.531403005123)\n",
      " state (5)  A[0]:(-0.0574672557414) A[1]:(0.999896466732) A[2]:(-0.746504306793) A[3]:(0.614367187023)\n",
      " state (6)  A[0]:(-0.000198781490326) A[1]:(0.810056209564) A[2]:(-0.000302195549011) A[3]:(0.65595459938)\n",
      " state (7)  A[0]:(0.559642314911) A[1]:(-0.564073324203) A[2]:(0.404633432627) A[3]:(0.882323265076)\n",
      " state (8)  A[0]:(0.655179142952) A[1]:(0.000665128114633) A[2]:(0.729166686535) A[3]:(0.590199708939)\n",
      " state (9)  A[0]:(0.6557315588) A[1]:(0.810291528702) A[2]:(0.809957504272) A[3]:(-0.00100332463626)\n",
      " state (10)  A[0]:(0.728624224663) A[1]:(0.900017857552) A[2]:(-0.000937938399147) A[3]:(0.728528618813)\n",
      " state (11)  A[0]:(0.123555749655) A[1]:(0.879077911377) A[2]:(-0.913919210434) A[3]:(0.797825217247)\n",
      " state (12)  A[0]:(-0.431324332952) A[1]:(0.81035476923) A[2]:(-0.929497539997) A[3]:(0.712730228901)\n",
      " state (13)  A[0]:(-0.000332981348038) A[1]:(0.808984279633) A[2]:(0.899918496609) A[3]:(0.729066431522)\n",
      " state (14)  A[0]:(0.810250759125) A[1]:(0.900067865849) A[2]:(0.99999922514) A[3]:(0.809973299503)\n",
      " state (15)  A[0]:(0.979565560818) A[1]:(0.942308962345) A[2]:(1.0) A[3]:(0.879529178143)\n",
      "Episode 387000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6124. Times reached goal: 948.               Steps done: 2969571. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0461928379491.\n",
      " state (0)  A[0]:(0.531470835209) A[1]:(0.590204298496) A[2]:(0.590561568737) A[3]:(0.531575202942)\n",
      " state (1)  A[0]:(0.531623125076) A[1]:(9.54121351242e-05) A[2]:(0.656086444855) A[3]:(0.590744435787)\n",
      " state (2)  A[0]:(0.590204060078) A[1]:(0.729136824608) A[2]:(0.591340065002) A[3]:(0.656413316727)\n",
      " state (3)  A[0]:(0.656809091568) A[1]:(0.000121161341667) A[2]:(0.483837723732) A[3]:(0.559015154839)\n",
      " state (4)  A[0]:(0.590082705021) A[1]:(0.655736804008) A[2]:(0.00016987323761) A[3]:(0.531810224056)\n",
      " state (5)  A[0]:(-0.0568971000612) A[1]:(0.999896466732) A[2]:(-0.747007608414) A[3]:(0.614866197109)\n",
      " state (6)  A[0]:(0.000324204564095) A[1]:(0.810125291348) A[2]:(-0.000198006629944) A[3]:(0.65626001358)\n",
      " state (7)  A[0]:(0.560106873512) A[1]:(-0.563401937485) A[2]:(0.404744774103) A[3]:(0.882453024387)\n",
      " state (8)  A[0]:(0.655952572823) A[1]:(0.00108341826126) A[2]:(0.729237437248) A[3]:(0.591725349426)\n",
      " state (9)  A[0]:(0.656593620777) A[1]:(0.810436367989) A[2]:(0.810284793377) A[3]:(0.0017955879448)\n",
      " state (10)  A[0]:(0.729450345039) A[1]:(0.900114893913) A[2]:(0.000543117464986) A[3]:(0.729706406593)\n",
      " state (11)  A[0]:(0.125453189015) A[1]:(0.879189968109) A[2]:(-0.913717389107) A[3]:(0.798601686954)\n",
      " state (12)  A[0]:(-0.429943799973) A[1]:(0.810417771339) A[2]:(-0.929417252541) A[3]:(0.713500797749)\n",
      " state (13)  A[0]:(0.000974774069618) A[1]:(0.808851957321) A[2]:(0.900242745876) A[3]:(0.729636311531)\n",
      " state (14)  A[0]:(0.810536384583) A[1]:(0.899894356728) A[2]:(0.99999922514) A[3]:(0.810354232788)\n",
      " state (15)  A[0]:(0.979583919048) A[1]:(0.94214206934) A[2]:(1.0) A[3]:(0.879764795303)\n",
      "Episode 388000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6193. Times reached goal: 964.               Steps done: 2975764. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0459076497006.\n",
      " state (0)  A[0]:(0.53128182888) A[1]:(0.590314149857) A[2]:(0.590505242348) A[3]:(0.531394720078)\n",
      " state (1)  A[0]:(0.531288981438) A[1]:(7.97510147095e-05) A[2]:(0.65600335598) A[3]:(0.590505123138)\n",
      " state (2)  A[0]:(0.589733839035) A[1]:(0.728980779648) A[2]:(0.591717779636) A[3]:(0.656116008759)\n",
      " state (3)  A[0]:(0.65649831295) A[1]:(-0.000399470300181) A[2]:(0.484778493643) A[3]:(0.558525681496)\n",
      " state (4)  A[0]:(0.589808106422) A[1]:(0.656078219414) A[2]:(0.000706195714884) A[3]:(0.53150498867)\n",
      " state (5)  A[0]:(-0.0568140558898) A[1]:(0.999896585941) A[2]:(-0.747664928436) A[3]:(0.615256845951)\n",
      " state (6)  A[0]:(-2.23070383072e-05) A[1]:(0.809949874878) A[2]:(-0.000192046165466) A[3]:(0.65598320961)\n",
      " state (7)  A[0]:(0.559539437294) A[1]:(-0.563959240913) A[2]:(0.404887318611) A[3]:(0.882124364376)\n",
      " state (8)  A[0]:(0.655360937119) A[1]:(-0.00060045713326) A[2]:(0.728957653046) A[3]:(0.590724885464)\n",
      " state (9)  A[0]:(0.655906319618) A[1]:(0.809811353683) A[2]:(0.809976696968) A[3]:(0.000153660774231)\n",
      " state (10)  A[0]:(0.729119896889) A[1]:(0.899958491325) A[2]:(-6.37769699097e-05) A[3]:(0.729192852974)\n",
      " state (11)  A[0]:(0.12525048852) A[1]:(0.879378914833) A[2]:(-0.913851678371) A[3]:(0.798499882221)\n",
      " state (12)  A[0]:(-0.430392503738) A[1]:(0.811196446419) A[2]:(-0.92962872982) A[3]:(0.713364958763)\n",
      " state (13)  A[0]:(-0.000148922204971) A[1]:(0.809955060482) A[2]:(0.900111854076) A[3]:(0.729311347008)\n",
      " state (14)  A[0]:(0.810056447983) A[1]:(0.900626957417) A[2]:(0.99999922514) A[3]:(0.809950113297)\n",
      " state (15)  A[0]:(0.979535400867) A[1]:(0.942607700825) A[2]:(1.0) A[3]:(0.879382550716)\n",
      "Episode 389000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6130. Times reached goal: 951.               Steps done: 2981894. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0456270965818.\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.5908,  0.5908,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.6565,  0.0009,  0.5321]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556, -0.0004,  0.7285,  0.5901]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560,  0.8102,  0.8095,  0.0003]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0031,  0.8103,  0.9005,  0.7289]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8091,  0.9006,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531661689281) A[1]:(0.590302526951) A[2]:(0.590579926968) A[3]:(0.531632184982)\n",
      " state (1)  A[0]:(0.531414270401) A[1]:(-0.000815734092612) A[2]:(0.655870556831) A[3]:(0.590680360794)\n",
      " state (2)  A[0]:(0.590005993843) A[1]:(0.72867000103) A[2]:(0.591582775116) A[3]:(0.656323671341)\n",
      " state (3)  A[0]:(0.656718254089) A[1]:(-0.00100408459548) A[2]:(0.485094308853) A[3]:(0.558700084686)\n",
      " state (4)  A[0]:(0.590011775494) A[1]:(0.655757188797) A[2]:(0.000901341205463) A[3]:(0.532025635242)\n",
      " state (5)  A[0]:(-0.0568281374872) A[1]:(0.999896526337) A[2]:(-0.748209416866) A[3]:(0.616668581963)\n",
      " state (6)  A[0]:(-0.00064237404149) A[1]:(0.809792757034) A[2]:(-0.000152587890625) A[3]:(0.656309127808)\n",
      " state (7)  A[0]:(0.559036850929) A[1]:(-0.563743591309) A[2]:(0.405104368925) A[3]:(0.882049202919)\n",
      " state (8)  A[0]:(0.654797136784) A[1]:(0.000436440081103) A[2]:(0.729105949402) A[3]:(0.589761912823)\n",
      " state (9)  A[0]:(0.655690670013) A[1]:(0.810062229633) A[2]:(0.80973392725) A[3]:(0.000396192044718)\n",
      " state (10)  A[0]:(0.728411793709) A[1]:(0.899958074093) A[2]:(-0.00187718647067) A[3]:(0.729343593121)\n",
      " state (11)  A[0]:(0.122337706387) A[1]:(0.879234910011) A[2]:(-0.91433775425) A[3]:(0.798410832882)\n",
      " state (12)  A[0]:(-0.433004587889) A[1]:(0.810773432255) A[2]:(-0.930090904236) A[3]:(0.713260531425)\n",
      " state (13)  A[0]:(-0.00292237754911) A[1]:(0.809338390827) A[2]:(0.899758458138) A[3]:(0.729393959045)\n",
      " state (14)  A[0]:(0.809269309044) A[1]:(0.90022701025) A[2]:(0.99999922514) A[3]:(0.810138642788)\n",
      " state (15)  A[0]:(0.979459106922) A[1]:(0.942328155041) A[2]:(1.0) A[3]:(0.879546940327)\n",
      "Episode 390000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6105. Times reached goal: 956.               Steps done: 2987999. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0453493917138.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531114041805) A[1]:(0.590425729752) A[2]:(0.590477824211) A[3]:(0.531292498112)\n",
      " state (1)  A[0]:(0.531262993813) A[1]:(-4.00841236115e-05) A[2]:(0.656075358391) A[3]:(0.590372383595)\n",
      " state (2)  A[0]:(0.590055465698) A[1]:(0.728955507278) A[2]:(0.591191887856) A[3]:(0.655957102776)\n",
      " state (3)  A[0]:(0.656591594219) A[1]:(-0.000664502265863) A[2]:(0.484639048576) A[3]:(0.557721495628)\n",
      " state (4)  A[0]:(0.590078294277) A[1]:(0.655893564224) A[2]:(-4.98294830322e-05) A[3]:(0.531254589558)\n",
      " state (5)  A[0]:(-0.055278070271) A[1]:(0.999896705151) A[2]:(-0.748941779137) A[3]:(0.617543339729)\n",
      " state (6)  A[0]:(0.000125497579575) A[1]:(0.810042321682) A[2]:(-3.01599502563e-05) A[3]:(0.655791401863)\n",
      " state (7)  A[0]:(0.559516787529) A[1]:(-0.563143491745) A[2]:(0.405506551266) A[3]:(0.881710767746)\n",
      " state (8)  A[0]:(0.655821800232) A[1]:(0.000167340040207) A[2]:(0.72898286581) A[3]:(0.590701282024)\n",
      " state (9)  A[0]:(0.656105518341) A[1]:(0.810119748116) A[2]:(0.810005009174) A[3]:(-7.53998756409e-05)\n",
      " state (10)  A[0]:(0.729029655457) A[1]:(0.900017499924) A[2]:(-0.000219464302063) A[3]:(0.728865623474)\n",
      " state (11)  A[0]:(0.124812059104) A[1]:(0.879288673401) A[2]:(-0.914099216461) A[3]:(0.798194468021)\n",
      " state (12)  A[0]:(-0.43049415946) A[1]:(0.810763657093) A[2]:(-0.930014073849) A[3]:(0.712983846664)\n",
      " state (13)  A[0]:(0.000294208526611) A[1]:(0.809220314026) A[2]:(0.900005459785) A[3]:(0.729122042656)\n",
      " state (14)  A[0]:(0.8103120327) A[1]:(0.900162160397) A[2]:(0.99999922514) A[3]:(0.809960484505)\n",
      " state (15)  A[0]:(0.979566216469) A[1]:(0.942299723625) A[2]:(1.0) A[3]:(0.879420280457)\n",
      "Episode 391000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6144. Times reached goal: 958.               Steps done: 2994143. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0450716192419.\n",
      " state (0)  A[0]:(0.531660795212) A[1]:(0.590808153152) A[2]:(0.590414166451) A[3]:(0.531548380852)\n",
      " state (1)  A[0]:(0.531583666801) A[1]:(0.000106826424599) A[2]:(0.656172931194) A[3]:(0.590215325356)\n",
      " state (2)  A[0]:(0.590164482594) A[1]:(0.729241728783) A[2]:(0.591467022896) A[3]:(0.656226992607)\n",
      " state (3)  A[0]:(0.656710386276) A[1]:(-0.000931963033509) A[2]:(0.485271781683) A[3]:(0.557865262032)\n",
      " state (4)  A[0]:(0.59032022953) A[1]:(0.656175792217) A[2]:(-9.82284545898e-05) A[3]:(0.531678438187)\n",
      " state (5)  A[0]:(-0.0541842319071) A[1]:(0.999896764755) A[2]:(-0.749848306179) A[3]:(0.618696212769)\n",
      " state (6)  A[0]:(4.57018613815e-05) A[1]:(0.810128927231) A[2]:(-9.9778175354e-05) A[3]:(0.655990719795)\n",
      " state (7)  A[0]:(0.559193372726) A[1]:(-0.562916517258) A[2]:(0.405878037214) A[3]:(0.88169914484)\n",
      " state (8)  A[0]:(0.655741691589) A[1]:(-2.69263982773e-05) A[2]:(0.729004919529) A[3]:(0.591045975685)\n",
      " state (9)  A[0]:(0.655939817429) A[1]:(0.810046136379) A[2]:(0.810055434704) A[3]:(-0.000108301639557)\n",
      " state (10)  A[0]:(0.729066312313) A[1]:(0.900020837784) A[2]:(-8.01086425781e-05) A[3]:(0.729030370712)\n",
      " state (11)  A[0]:(0.125113978982) A[1]:(0.879411160946) A[2]:(-0.914189755917) A[3]:(0.798479914665)\n",
      " state (12)  A[0]:(-0.430564910173) A[1]:(0.81111574173) A[2]:(-0.930190980434) A[3]:(0.713276028633)\n",
      " state (13)  A[0]:(-0.000158235430717) A[1]:(0.809700250626) A[2]:(0.900079488754) A[3]:(0.729265332222)\n",
      " state (14)  A[0]:(0.810141980648) A[1]:(0.900483965874) A[2]:(0.99999922514) A[3]:(0.810016155243)\n",
      " state (15)  A[0]:(0.979554116726) A[1]:(0.942476689816) A[2]:(1.0) A[3]:(0.879419505596)\n",
      "Episode 392000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6120. Times reached goal: 953.               Steps done: 3000263. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0447966232781.\n",
      " state (0)  A[0]:(0.531774938107) A[1]:(0.590179443359) A[2]:(0.590736448765) A[3]:(0.530992388725)\n",
      " state (1)  A[0]:(0.53172147274) A[1]:(0.000558465661015) A[2]:(0.655866682529) A[3]:(0.590615153313)\n",
      " state (2)  A[0]:(0.590248167515) A[1]:(0.728878736496) A[2]:(0.590792417526) A[3]:(0.656599283218)\n",
      " state (3)  A[0]:(0.656532168388) A[1]:(0.000201791524887) A[2]:(0.484665304422) A[3]:(0.558566451073)\n",
      " state (4)  A[0]:(0.590116858482) A[1]:(0.65583384037) A[2]:(-0.000101327896118) A[3]:(0.531947851181)\n",
      " state (5)  A[0]:(-0.0546688362956) A[1]:(0.999896705151) A[2]:(-0.749789476395) A[3]:(0.617695808411)\n",
      " state (6)  A[0]:(-3.32444906235e-05) A[1]:(0.809920012951) A[2]:(-0.000348329544067) A[3]:(0.65561068058)\n",
      " state (7)  A[0]:(0.559314370155) A[1]:(-0.562782227993) A[2]:(0.405320495367) A[3]:(0.881595671177)\n",
      " state (8)  A[0]:(0.655822813511) A[1]:(0.00040158626507) A[2]:(0.728750228882) A[3]:(0.590707540512)\n",
      " state (9)  A[0]:(0.656195104122) A[1]:(0.810177385807) A[2]:(0.809877991676) A[3]:(0.000182569026947)\n",
      " state (10)  A[0]:(0.729148864746) A[1]:(0.90003824234) A[2]:(-0.000448703736765) A[3]:(0.72892844677)\n",
      " state (11)  A[0]:(0.125090047717) A[1]:(0.879355490208) A[2]:(-0.914351642132) A[3]:(0.798166811466)\n",
      " state (12)  A[0]:(-0.430408477783) A[1]:(0.810840904713) A[2]:(-0.930443584919) A[3]:(0.712796807289)\n",
      " state (13)  A[0]:(0.000244200229645) A[1]:(0.809189915657) A[2]:(0.89997023344) A[3]:(0.728956282139)\n",
      " state (14)  A[0]:(0.810256838799) A[1]:(0.900143265724) A[2]:(0.99999922514) A[3]:(0.809923052788)\n",
      " state (15)  A[0]:(0.97956264019) A[1]:(0.942245185375) A[2]:(1.0) A[3]:(0.879433214664)\n",
      "Episode 393000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6139. Times reached goal: 958.               Steps done: 3006402. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0445224592155.\n",
      " state (0)  A[0]:(0.531771600246) A[1]:(0.59093439579) A[2]:(0.590584635735) A[3]:(0.532060086727)\n",
      " state (1)  A[0]:(0.531879425049) A[1]:(-0.000269740819931) A[2]:(0.656571805477) A[3]:(0.59077000618)\n",
      " state (2)  A[0]:(0.5906291008) A[1]:(0.72861468792) A[2]:(0.591201543808) A[3]:(0.656536459923)\n",
      " state (3)  A[0]:(0.657019674778) A[1]:(0.00011220574379) A[2]:(0.485154211521) A[3]:(0.558574199677)\n",
      " state (4)  A[0]:(0.590683460236) A[1]:(0.656322181225) A[2]:(0.000446796388132) A[3]:(0.532022595406)\n",
      " state (5)  A[0]:(-0.0541390478611) A[1]:(0.999896764755) A[2]:(-0.749464690685) A[3]:(0.6178855896)\n",
      " state (6)  A[0]:(0.000699654105119) A[1]:(0.809721112251) A[2]:(0.00112390471622) A[3]:(0.656472504139)\n",
      " state (7)  A[0]:(0.559986829758) A[1]:(-0.563248753548) A[2]:(0.40674880147) A[3]:(0.882022619247)\n",
      " state (8)  A[0]:(0.65680038929) A[1]:(-0.000878423219547) A[2]:(0.729363322258) A[3]:(0.591942727566)\n",
      " state (9)  A[0]:(0.657549262047) A[1]:(0.809719443321) A[2]:(0.81040430069) A[3]:(0.00132876553107)\n",
      " state (10)  A[0]:(0.730704665184) A[1]:(0.899928450584) A[2]:(0.00201701838523) A[3]:(0.729638934135)\n",
      " state (11)  A[0]:(0.128605097532) A[1]:(0.879445672035) A[2]:(-0.914029061794) A[3]:(0.798829257488)\n",
      " state (12)  A[0]:(-0.428541511297) A[1]:(0.811161279678) A[2]:(-0.930464982986) A[3]:(0.713378369808)\n",
      " state (13)  A[0]:(0.00112980557606) A[1]:(0.809493422508) A[2]:(0.899906516075) A[3]:(0.729143023491)\n",
      " state (14)  A[0]:(0.810341775417) A[1]:(0.9002841115) A[2]:(0.999999284744) A[3]:(0.809867978096)\n",
      " state (15)  A[0]:(0.979580938816) A[1]:(0.942295730114) A[2]:(1.0) A[3]:(0.879324257374)\n",
      "Episode 394000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6156. Times reached goal: 958.               Steps done: 3012558. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0442492208471.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5906,  0.5905,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6560,  0.0000,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558, -0.0002,  0.7289,  0.5907]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.8099,  0.8102, -0.0000]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000,  0.0001,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9004,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531300425529) A[1]:(0.590583145618) A[2]:(0.59051322937) A[3]:(0.531404614449)\n",
      " state (1)  A[0]:(0.531313061714) A[1]:(0.000185370445251) A[2]:(0.656205296516) A[3]:(0.59020036459)\n",
      " state (2)  A[0]:(0.590269207954) A[1]:(0.729146540165) A[2]:(0.591015815735) A[3]:(0.656187593937)\n",
      " state (3)  A[0]:(0.656554043293) A[1]:(0.00179683964234) A[2]:(0.484710633755) A[3]:(0.558351874352)\n",
      " state (4)  A[0]:(0.590281009674) A[1]:(0.656051754951) A[2]:(2.20537185669e-05) A[3]:(0.531577825546)\n",
      " state (5)  A[0]:(-0.0547728762031) A[1]:(0.999897003174) A[2]:(-0.749840378761) A[3]:(0.61675632)\n",
      " state (6)  A[0]:(7.91996717453e-05) A[1]:(0.810183942318) A[2]:(-0.000224709510803) A[3]:(0.655844211578)\n",
      " state (7)  A[0]:(0.559454977512) A[1]:(-0.562711834908) A[2]:(0.405417501926) A[3]:(0.881809294224)\n",
      " state (8)  A[0]:(0.65572977066) A[1]:(-0.000131011009216) A[2]:(0.72896194458) A[3]:(0.590800881386)\n",
      " state (9)  A[0]:(0.656015634537) A[1]:(0.809907257557) A[2]:(0.810175418854) A[3]:(0.000116109848022)\n",
      " state (10)  A[0]:(0.729018807411) A[1]:(0.899995267391) A[2]:(0.000131368637085) A[3]:(0.729077458382)\n",
      " state (11)  A[0]:(0.124723523855) A[1]:(0.879543364048) A[2]:(-0.914559543133) A[3]:(0.798393845558)\n",
      " state (12)  A[0]:(-0.430934995413) A[1]:(0.811339318752) A[2]:(-0.930858135223) A[3]:(0.713135242462)\n",
      " state (13)  A[0]:(-0.000706240418367) A[1]:(0.809683322906) A[2]:(0.900035738945) A[3]:(0.729273438454)\n",
      " state (14)  A[0]:(0.809877038002) A[1]:(0.900390744209) A[2]:(0.999999284744) A[3]:(0.810105144978)\n",
      " state (15)  A[0]:(0.97953414917) A[1]:(0.942312598228) A[2]:(1.0) A[3]:(0.879517197609)\n",
      "Episode 395000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6132. Times reached goal: 958.               Steps done: 3018690. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0439787148439.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531618118286) A[1]:(0.590518593788) A[2]:(0.590465307236) A[3]:(0.531623661518)\n",
      " state (1)  A[0]:(0.53157299757) A[1]:(-0.000415161222918) A[2]:(0.655943632126) A[3]:(0.590431451797)\n",
      " state (2)  A[0]:(0.590253829956) A[1]:(0.728739976883) A[2]:(0.59071457386) A[3]:(0.656238794327)\n",
      " state (3)  A[0]:(0.65655195713) A[1]:(0.00121800543275) A[2]:(0.484443932772) A[3]:(0.558498620987)\n",
      " state (4)  A[0]:(0.59033036232) A[1]:(0.655668258667) A[2]:(-0.00025463104248) A[3]:(0.531647443771)\n",
      " state (5)  A[0]:(-0.0548138804734) A[1]:(0.999896883965) A[2]:(-0.750079154968) A[3]:(0.616464138031)\n",
      " state (6)  A[0]:(7.79330730438e-06) A[1]:(0.809885144234) A[2]:(-0.000404119462473) A[3]:(0.656075239182)\n",
      " state (7)  A[0]:(0.55958199501) A[1]:(-0.563403367996) A[2]:(0.40555498004) A[3]:(0.88203471899)\n",
      " state (8)  A[0]:(0.655797421932) A[1]:(-0.000467136473162) A[2]:(0.72886800766) A[3]:(0.591214418411)\n",
      " state (9)  A[0]:(0.656003117561) A[1]:(0.809947371483) A[2]:(0.809903740883) A[3]:(0.000567376555409)\n",
      " state (10)  A[0]:(0.728860378265) A[1]:(0.899955272675) A[2]:(-0.000599622668233) A[3]:(0.729175806046)\n",
      " state (11)  A[0]:(0.124058865011) A[1]:(0.879426598549) A[2]:(-0.914772033691) A[3]:(0.798336088657)\n",
      " state (12)  A[0]:(-0.431457966566) A[1]:(0.811070263386) A[2]:(-0.931139349937) A[3]:(0.712991833687)\n",
      " state (13)  A[0]:(-0.00101907516364) A[1]:(0.809353351593) A[2]:(0.899890601635) A[3]:(0.729167819023)\n",
      " state (14)  A[0]:(0.80992436409) A[1]:(0.900260865688) A[2]:(0.999999284744) A[3]:(0.8100451231)\n",
      " state (15)  A[0]:(0.979555428028) A[1]:(0.942264020443) A[2]:(1.0) A[3]:(0.879471182823)\n",
      "Episode 396000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6089. Times reached goal: 957.               Steps done: 3024779. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0437117420727.\n",
      " state (0)  A[0]:(0.531570255756) A[1]:(0.590397119522) A[2]:(0.590391337872) A[3]:(0.531374454498)\n",
      " state (1)  A[0]:(0.531640768051) A[1]:(0.000212535262108) A[2]:(0.656038045883) A[3]:(0.590471744537)\n",
      " state (2)  A[0]:(0.590456783772) A[1]:(0.72901058197) A[2]:(0.590732514858) A[3]:(0.656150102615)\n",
      " state (3)  A[0]:(0.656716227531) A[1]:(0.00203691143543) A[2]:(0.484460443258) A[3]:(0.558480143547)\n",
      " state (4)  A[0]:(0.590540766716) A[1]:(0.655955672264) A[2]:(-3.44514846802e-05) A[3]:(0.531451165676)\n",
      " state (5)  A[0]:(-0.0545165538788) A[1]:(0.999897003174) A[2]:(-0.749826192856) A[3]:(0.615816116333)\n",
      " state (6)  A[0]:(0.000875949626788) A[1]:(0.80996888876) A[2]:(-4.57763671875e-05) A[3]:(0.656121850014)\n",
      " state (7)  A[0]:(0.560279607773) A[1]:(-0.563020169735) A[2]:(0.405681073666) A[3]:(0.882069706917)\n",
      " state (8)  A[0]:(0.656142890453) A[1]:(-3.44663858414e-05) A[2]:(0.729065895081) A[3]:(0.590703010559)\n",
      " state (9)  A[0]:(0.656714677811) A[1]:(0.809908509254) A[2]:(0.810073852539) A[3]:(0.000722646596842)\n",
      " state (10)  A[0]:(0.729816198349) A[1]:(0.899928033352) A[2]:(0.000243902206421) A[3]:(0.729462981224)\n",
      " state (11)  A[0]:(0.126672923565) A[1]:(0.879439294338) A[2]:(-0.914693653584) A[3]:(0.798578202724)\n",
      " state (12)  A[0]:(-0.429197698832) A[1]:(0.811070263386) A[2]:(-0.931167602539) A[3]:(0.713227510452)\n",
      " state (13)  A[0]:(0.00145475461613) A[1]:(0.809244573116) A[2]:(0.900181174278) A[3]:(0.729316949844)\n",
      " state (14)  A[0]:(0.810580015182) A[1]:(0.900159955025) A[2]:(0.999999284744) A[3]:(0.810108423233)\n",
      " state (15)  A[0]:(0.979614078999) A[1]:(0.942164778709) A[2]:(1.0) A[3]:(0.879487037659)\n",
      "Episode 397000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6110. Times reached goal: 960.               Steps done: 3030889. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0434454775949.\n",
      " state (0)  A[0]:(0.531001746655) A[1]:(0.590354204178) A[2]:(0.590154290199) A[3]:(0.531594514847)\n",
      " state (1)  A[0]:(0.531135082245) A[1]:(0.000281140208244) A[2]:(0.655786275864) A[3]:(0.590412557125)\n",
      " state (2)  A[0]:(0.589922726154) A[1]:(0.728725492954) A[2]:(0.590653896332) A[3]:(0.656154692173)\n",
      " state (3)  A[0]:(0.656173825264) A[1]:(0.00177387706935) A[2]:(0.484298020601) A[3]:(0.55866765976)\n",
      " state (4)  A[0]:(0.589782416821) A[1]:(0.655923604965) A[2]:(-0.000279903411865) A[3]:(0.531421840191)\n",
      " state (5)  A[0]:(-0.0560309961438) A[1]:(0.999896883965) A[2]:(-0.749771952629) A[3]:(0.614906251431)\n",
      " state (6)  A[0]:(-0.00013093650341) A[1]:(0.809533894062) A[2]:(0.000606298388448) A[3]:(0.655835390091)\n",
      " state (7)  A[0]:(0.560024321079) A[1]:(-0.563257098198) A[2]:(0.406033456326) A[3]:(0.8821195364)\n",
      " state (8)  A[0]:(0.65661907196) A[1]:(-0.000141337513924) A[2]:(0.729038476944) A[3]:(0.59103924036)\n",
      " state (9)  A[0]:(0.657507777214) A[1]:(0.810039103031) A[2]:(0.810018539429) A[3]:(0.000233888626099)\n",
      " state (10)  A[0]:(0.73032283783) A[1]:(0.900009274483) A[2]:(0.000115871429443) A[3]:(0.729015946388)\n",
      " state (11)  A[0]:(0.127214387059) A[1]:(0.879521906376) A[2]:(-0.914813995361) A[3]:(0.79817456007)\n",
      " state (12)  A[0]:(-0.429144263268) A[1]:(0.811128914356) A[2]:(-0.931329011917) A[3]:(0.712680995464)\n",
      " state (13)  A[0]:(0.00109370006248) A[1]:(0.809208512306) A[2]:(0.900334477425) A[3]:(0.728864789009)\n",
      " state (14)  A[0]:(0.810308694839) A[1]:(0.900099515915) A[2]:(0.999999284744) A[3]:(0.809836804867)\n",
      " state (15)  A[0]:(0.97957611084) A[1]:(0.942070007324) A[2]:(1.0) A[3]:(0.87935012579)\n",
      "Episode 398000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6128. Times reached goal: 952.               Steps done: 3037017. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0431800577851.\n",
      " state (0)  A[0]:(0.531098783016) A[1]:(0.590443313122) A[2]:(0.590697407722) A[3]:(0.530305206776)\n",
      " state (1)  A[0]:(0.531046688557) A[1]:(0.000783666793723) A[2]:(0.656160891056) A[3]:(0.589650154114)\n",
      " state (2)  A[0]:(0.589765548706) A[1]:(0.728856801987) A[2]:(0.590860247612) A[3]:(0.655805468559)\n",
      " state (3)  A[0]:(0.656120359898) A[1]:(0.00281474739313) A[2]:(0.484478235245) A[3]:(0.558533191681)\n",
      " state (4)  A[0]:(0.589850902557) A[1]:(0.656181931496) A[2]:(-0.000127792358398) A[3]:(0.530955553055)\n",
      " state (5)  A[0]:(-0.0562981367111) A[1]:(0.999897062778) A[2]:(-0.750048995018) A[3]:(0.613268852234)\n",
      " state (6)  A[0]:(-0.000323802232742) A[1]:(0.809773445129) A[2]:(-0.000192761421204) A[3]:(0.6551232934)\n",
      " state (7)  A[0]:(0.559907436371) A[1]:(-0.562576711178) A[2]:(0.405311614275) A[3]:(0.881952166557)\n",
      " state (8)  A[0]:(0.655814886093) A[1]:(0.000910028582439) A[2]:(0.728860974312) A[3]:(0.589908361435)\n",
      " state (9)  A[0]:(0.65634894371) A[1]:(0.810172498226) A[2]:(0.809842824936) A[3]:(-0.000644564512186)\n",
      " state (10)  A[0]:(0.729213058949) A[1]:(0.899964928627) A[2]:(-0.000815272156615) A[3]:(0.728621482849)\n",
      " state (11)  A[0]:(0.125187590718) A[1]:(0.879387617111) A[2]:(-0.915090680122) A[3]:(0.797834038734)\n",
      " state (12)  A[0]:(-0.429653584957) A[1]:(0.810811936855) A[2]:(-0.931592583656) A[3]:(0.712468743324)\n",
      " state (13)  A[0]:(0.00197463971563) A[1]:(0.808797955513) A[2]:(0.900325596333) A[3]:(0.728929758072)\n",
      " state (14)  A[0]:(0.810945391655) A[1]:(0.899903714657) A[2]:(0.999999284744) A[3]:(0.809957325459)\n",
      " state (15)  A[0]:(0.979669213295) A[1]:(0.941975057125) A[2]:(1.0) A[3]:(0.879423260689)\n",
      "Episode 399000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6120. Times reached goal: 955.               Steps done: 3043137. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.042916602826.\n",
      "q_values \n",
      "tensor([[ 0.5308,  0.5905,  0.5903,  0.5307]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.6554,  0.0002,  0.5312]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556, -0.0005,  0.7288,  0.5910]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6555,  0.8096,  0.8102,  0.0005]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7283,  0.8999, -0.0000,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8095,  0.9007,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530883073807) A[1]:(0.590462327003) A[2]:(0.590277075768) A[3]:(0.530659914017)\n",
      " state (1)  A[0]:(0.530901968479) A[1]:(-0.000160858035088) A[2]:(0.655935049057) A[3]:(0.589908480644)\n",
      " state (2)  A[0]:(0.589845180511) A[1]:(0.729638695717) A[2]:(0.590678930283) A[3]:(0.655976295471)\n",
      " state (3)  A[0]:(0.656177043915) A[1]:(0.00520518189296) A[2]:(0.484177201986) A[3]:(0.559101581573)\n",
      " state (4)  A[0]:(0.590177059174) A[1]:(0.655391573906) A[2]:(0.000185489654541) A[3]:(0.531218230724)\n",
      " state (5)  A[0]:(-0.0564283132553) A[1]:(0.999897301197) A[2]:(-0.749951004982) A[3]:(0.612381100655)\n",
      " state (6)  A[0]:(-0.000517025531735) A[1]:(0.810414791107) A[2]:(-0.000701546552591) A[3]:(0.655408024788)\n",
      " state (7)  A[0]:(0.559362769127) A[1]:(-0.562533140182) A[2]:(0.404566645622) A[3]:(0.88212364912)\n",
      " state (8)  A[0]:(0.654634952545) A[1]:(-0.000175639986992) A[2]:(0.728611826897) A[3]:(0.590420484543)\n",
      " state (9)  A[0]:(0.654343366623) A[1]:(0.809863686562) A[2]:(0.810053110123) A[3]:(-0.000453829736216)\n",
      " state (10)  A[0]:(0.727667927742) A[1]:(0.900009870529) A[2]:(-0.000140309333801) A[3]:(0.728990077972)\n",
      " state (11)  A[0]:(0.122203618288) A[1]:(0.879836201668) A[2]:(-0.915171444416) A[3]:(0.798428535461)\n",
      " state (12)  A[0]:(-0.432726532221) A[1]:(0.811978697777) A[2]:(-0.931882798672) A[3]:(0.713242173195)\n",
      " state (13)  A[0]:(-0.00252513051964) A[1]:(0.81023311615) A[2]:(0.900075733662) A[3]:(0.729451060295)\n",
      " state (14)  A[0]:(0.809424519539) A[1]:(0.900748133659) A[2]:(0.999999284744) A[3]:(0.810175538063)\n",
      " state (15)  A[0]:(0.979528725147) A[1]:(0.942439258099) A[2]:(1.0) A[3]:(0.879488110542)\n",
      "Episode 400000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6131. Times reached goal: 961.               Steps done: 3049268. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0426542860877.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531695008278) A[1]:(0.589746713638) A[2]:(0.590380907059) A[3]:(0.530908346176)\n",
      " state (1)  A[0]:(0.531611382961) A[1]:(0.00057338172337) A[2]:(0.655998170376) A[3]:(0.590086221695)\n",
      " state (2)  A[0]:(0.590082168579) A[1]:(0.728902816772) A[2]:(0.590939640999) A[3]:(0.656110227108)\n",
      " state (3)  A[0]:(0.656280755997) A[1]:(0.00386616145261) A[2]:(0.484454423189) A[3]:(0.559619069099)\n",
      " state (4)  A[0]:(0.589912056923) A[1]:(0.655851721764) A[2]:(0.000197291374207) A[3]:(0.531388640404)\n",
      " state (5)  A[0]:(-0.0570364929736) A[1]:(0.999897360802) A[2]:(-0.74983048439) A[3]:(0.611045718193)\n",
      " state (6)  A[0]:(0.000306144356728) A[1]:(0.810174763203) A[2]:(-0.000109553337097) A[3]:(0.655498862267)\n",
      " state (7)  A[0]:(0.560367703438) A[1]:(-0.562773942947) A[2]:(0.405228227377) A[3]:(0.88231408596)\n",
      " state (8)  A[0]:(0.655580818653) A[1]:(0.00023578107357) A[2]:(0.728931427002) A[3]:(0.589959144592)\n",
      " state (9)  A[0]:(0.656052827835) A[1]:(0.809960186481) A[2]:(0.809928774834) A[3]:(-0.000282347202301)\n",
      " state (10)  A[0]:(0.729019701481) A[1]:(0.900002002716) A[2]:(-0.000820159737486) A[3]:(0.728927850723)\n",
      " state (11)  A[0]:(0.12460680306) A[1]:(0.879740476608) A[2]:(-0.915392458439) A[3]:(0.798089146614)\n",
      " state (12)  A[0]:(-0.430734813213) A[1]:(0.811620116234) A[2]:(-0.932166516781) A[3]:(0.712693810463)\n",
      " state (13)  A[0]:(-0.000249743461609) A[1]:(0.80959379673) A[2]:(0.899840176105) A[3]:(0.729080736637)\n",
      " state (14)  A[0]:(0.809980452061) A[1]:(0.900300204754) A[2]:(0.999999344349) A[3]:(0.810050785542)\n",
      " state (15)  A[0]:(0.979569137096) A[1]:(0.942132771015) A[2]:(1.0) A[3]:(0.879481613636)\n",
      "Episode 401000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6164. Times reached goal: 968.               Steps done: 3055432. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0423921737284.\n",
      " state (0)  A[0]:(0.531710386276) A[1]:(0.590549945831) A[2]:(0.590430617332) A[3]:(0.531520724297)\n",
      " state (1)  A[0]:(0.531742215157) A[1]:(-2.92211771011e-05) A[2]:(0.656137526035) A[3]:(0.59042775631)\n",
      " state (2)  A[0]:(0.590466141701) A[1]:(0.729084670544) A[2]:(0.591100394726) A[3]:(0.656408190727)\n",
      " state (3)  A[0]:(0.656829237938) A[1]:(0.00493556726724) A[2]:(0.484348386526) A[3]:(0.560339391232)\n",
      " state (4)  A[0]:(0.590560793877) A[1]:(0.65605866909) A[2]:(-0.000298023223877) A[3]:(0.531937837601)\n",
      " state (5)  A[0]:(-0.0573438033462) A[1]:(0.999897480011) A[2]:(-0.750327169895) A[3]:(0.610626280308)\n",
      " state (6)  A[0]:(0.000512853206601) A[1]:(0.810160756111) A[2]:(-0.000665306928568) A[3]:(0.656730055809)\n",
      " state (7)  A[0]:(0.56098395586) A[1]:(-0.563264131546) A[2]:(0.405043274164) A[3]:(0.883116364479)\n",
      " state (8)  A[0]:(0.656214237213) A[1]:(7.11530447006e-05) A[2]:(0.728773474693) A[3]:(0.591867208481)\n",
      " state (9)  A[0]:(0.656861305237) A[1]:(0.810073077679) A[2]:(0.809785842896) A[3]:(0.0022651811596)\n",
      " state (10)  A[0]:(0.729805827141) A[1]:(0.899998009205) A[2]:(-0.000810503785033) A[3]:(0.730271220207)\n",
      " state (11)  A[0]:(0.125979498029) A[1]:(0.879672825336) A[2]:(-0.915427565575) A[3]:(0.799127161503)\n",
      " state (12)  A[0]:(-0.43009352684) A[1]:(0.811480164528) A[2]:(-0.932273209095) A[3]:(0.713894128799)\n",
      " state (13)  A[0]:(0.000309750437737) A[1]:(0.809504747391) A[2]:(0.899983346462) A[3]:(0.729971051216)\n",
      " state (14)  A[0]:(0.810247421265) A[1]:(0.900358021259) A[2]:(0.999999344349) A[3]:(0.810513198376)\n",
      " state (15)  A[0]:(0.979609966278) A[1]:(0.942217350006) A[2]:(1.0) A[3]:(0.879675626755)\n",
      "Episode 402000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6134. Times reached goal: 954.               Steps done: 3061566. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0421329360297.\n",
      " state (0)  A[0]:(0.533101737499) A[1]:(0.59033203125) A[2]:(0.590330004692) A[3]:(0.530808925629)\n",
      " state (1)  A[0]:(0.533630549908) A[1]:(0.000471264094813) A[2]:(0.655759811401) A[3]:(0.589854717255)\n",
      " state (2)  A[0]:(0.592370271683) A[1]:(0.728827953339) A[2]:(0.590637087822) A[3]:(0.655836462975)\n",
      " state (3)  A[0]:(0.658393502235) A[1]:(0.0055201984942) A[2]:(0.484041392803) A[3]:(0.559925913811)\n",
      " state (4)  A[0]:(0.592151641846) A[1]:(0.656770467758) A[2]:(-0.00034499168396) A[3]:(0.530961155891)\n",
      " state (5)  A[0]:(-0.0553765781224) A[1]:(0.999897778034) A[2]:(-0.75020891428) A[3]:(0.608107686043)\n",
      " state (6)  A[0]:(0.00241389404982) A[1]:(0.810258507729) A[2]:(-0.000473737687571) A[3]:(0.655053496361)\n",
      " state (7)  A[0]:(0.561849474907) A[1]:(-0.564349293709) A[2]:(0.40519374609) A[3]:(0.882204115391)\n",
      " state (8)  A[0]:(0.65648317337) A[1]:(-0.00386078236625) A[2]:(0.728620767593) A[3]:(0.587784647942)\n",
      " state (9)  A[0]:(0.656984567642) A[1]:(0.808129966259) A[2]:(0.80956029892) A[3]:(-0.00614444632083)\n",
      " state (10)  A[0]:(0.730794787407) A[1]:(0.89880490303) A[2]:(-0.00156521669123) A[3]:(0.726268291473)\n",
      " state (11)  A[0]:(0.132306054235) A[1]:(0.87821573019) A[2]:(-0.915662944317) A[3]:(0.79682725668)\n",
      " state (12)  A[0]:(-0.420585244894) A[1]:(0.809046030045) A[2]:(-0.932742595673) A[3]:(0.712312161922)\n",
      " state (13)  A[0]:(0.0149606922641) A[1]:(0.806639909744) A[2]:(0.898624360561) A[3]:(0.729622125626)\n",
      " state (14)  A[0]:(0.815312623978) A[1]:(0.898782074451) A[2]:(0.999999344349) A[3]:(0.810667872429)\n",
      " state (15)  A[0]:(0.980156898499) A[1]:(0.941466510296) A[2]:(1.0) A[3]:(0.879819512367)\n",
      "Episode 403000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6104. Times reached goal: 952.               Steps done: 3067670. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0418765399052.\n",
      " state (0)  A[0]:(0.531293570995) A[1]:(0.590593934059) A[2]:(0.590559303761) A[3]:(0.530123233795)\n",
      " state (1)  A[0]:(0.531407475471) A[1]:(-0.000658511999063) A[2]:(0.656074404716) A[3]:(0.590129733086)\n",
      " state (2)  A[0]:(0.590261101723) A[1]:(0.728814899921) A[2]:(0.59093773365) A[3]:(0.656448364258)\n",
      " state (3)  A[0]:(0.656566619873) A[1]:(0.00535316113383) A[2]:(0.48421305418) A[3]:(0.56097304821)\n",
      " state (4)  A[0]:(0.590277314186) A[1]:(0.655787229538) A[2]:(-2.89678573608e-05) A[3]:(0.531613826752)\n",
      " state (5)  A[0]:(-0.0579826459289) A[1]:(0.999897539616) A[2]:(-0.750042200089) A[3]:(0.607078790665)\n",
      " state (6)  A[0]:(0.000249370932579) A[1]:(0.810064554214) A[2]:(-0.000369906396372) A[3]:(0.655480146408)\n",
      " state (7)  A[0]:(0.560888648033) A[1]:(-0.563238739967) A[2]:(0.404882133007) A[3]:(0.882943809032)\n",
      " state (8)  A[0]:(0.655967116356) A[1]:(-0.000468015641673) A[2]:(0.728662252426) A[3]:(0.591443896294)\n",
      " state (9)  A[0]:(0.6561409235) A[1]:(0.809939146042) A[2]:(0.809931695461) A[3]:(0.000449478597147)\n",
      " state (10)  A[0]:(0.729187071323) A[1]:(0.900017261505) A[2]:(1.81198120117e-05) A[3]:(0.729209542274)\n",
      " state (11)  A[0]:(0.124881334603) A[1]:(0.879822134972) A[2]:(-0.915493369102) A[3]:(0.798335194588)\n",
      " state (12)  A[0]:(-0.430996328592) A[1]:(0.811693787575) A[2]:(-0.932595312595) A[3]:(0.712895154953)\n",
      " state (13)  A[0]:(-0.000947490043472) A[1]:(0.80947047472) A[2]:(0.89979660511) A[3]:(0.729169249535)\n",
      " state (14)  A[0]:(0.809778809547) A[1]:(0.900212883949) A[2]:(0.999999344349) A[3]:(0.810039281845)\n",
      " state (15)  A[0]:(0.979569137096) A[1]:(0.9420363307) A[2]:(1.0) A[3]:(0.879429996014)\n",
      "Episode 404000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6121. Times reached goal: 963.               Steps done: 3073791. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0416209964929.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5907,  0.5905,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6562,  0.0002,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559, -0.0000,  0.7288,  0.5909]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.8102,  0.8099,  0.0002]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  0.8094,  0.9000,  0.7289]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9002,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531829178333) A[1]:(0.590665817261) A[2]:(0.590463101864) A[3]:(0.531684160233)\n",
      " state (1)  A[0]:(0.531711578369) A[1]:(5.99175691605e-05) A[2]:(0.656201124191) A[3]:(0.590306520462)\n",
      " state (2)  A[0]:(0.590345621109) A[1]:(0.729228019714) A[2]:(0.59129178524) A[3]:(0.65637087822)\n",
      " state (3)  A[0]:(0.656659960747) A[1]:(0.00641249259934) A[2]:(0.484571635723) A[3]:(0.561241030693)\n",
      " state (4)  A[0]:(0.590352773666) A[1]:(0.656230688095) A[2]:(0.000388860673411) A[3]:(0.531584143639)\n",
      " state (5)  A[0]:(-0.0582697577775) A[1]:(0.99989759922) A[2]:(-0.749809265137) A[3]:(0.605747818947)\n",
      " state (6)  A[0]:(0.000328838825226) A[1]:(0.809834480286) A[2]:(0.000155806541443) A[3]:(0.65576338768)\n",
      " state (7)  A[0]:(0.561105787754) A[1]:(-0.563476443291) A[2]:(0.405162662268) A[3]:(0.883229255676)\n",
      " state (8)  A[0]:(0.655734419823) A[1]:(5.53876161575e-05) A[2]:(0.728924632072) A[3]:(0.590871334076)\n",
      " state (9)  A[0]:(0.656174182892) A[1]:(0.810095787048) A[2]:(0.809939324856) A[3]:(7.47442245483e-05)\n",
      " state (10)  A[0]:(0.729185223579) A[1]:(0.900003731251) A[2]:(-0.000119924545288) A[3]:(0.729045629501)\n",
      " state (11)  A[0]:(0.124731756747) A[1]:(0.879757463932) A[2]:(-0.915574431419) A[3]:(0.798103988171)\n",
      " state (12)  A[0]:(-0.43080663681) A[1]:(0.81155872345) A[2]:(-0.932676494122) A[3]:(0.712592124939)\n",
      " state (13)  A[0]:(-0.000129550695419) A[1]:(0.809329092503) A[2]:(0.900049448013) A[3]:(0.728923082352)\n",
      " state (14)  A[0]:(0.810217499733) A[1]:(0.900189042091) A[2]:(0.999999344349) A[3]:(0.80983376503)\n",
      " state (15)  A[0]:(0.979630410671) A[1]:(0.942040920258) A[2]:(1.0) A[3]:(0.879253029823)\n",
      "Episode 405000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6061. Times reached goal: 949.               Steps done: 3079852. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0413694945796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531706690788) A[1]:(0.590206742287) A[2]:(0.590611577034) A[3]:(0.531698405743)\n",
      " state (1)  A[0]:(0.531710326672) A[1]:(7.08401203156e-05) A[2]:(0.655949115753) A[3]:(0.590399503708)\n",
      " state (2)  A[0]:(0.590253472328) A[1]:(0.729044497013) A[2]:(0.590849518776) A[3]:(0.656183719635)\n",
      " state (3)  A[0]:(0.656512379646) A[1]:(0.0058626756072) A[2]:(0.48401710391) A[3]:(0.561323106289)\n",
      " state (4)  A[0]:(0.590066790581) A[1]:(0.655902862549) A[2]:(-0.000242352485657) A[3]:(0.531496286392)\n",
      " state (5)  A[0]:(-0.0590564161539) A[1]:(0.99989771843) A[2]:(-0.749982118607) A[3]:(0.604669034481)\n",
      " state (6)  A[0]:(-5.35696744919e-05) A[1]:(0.810011029243) A[2]:(-0.000456094712717) A[3]:(0.655595839024)\n",
      " state (7)  A[0]:(0.560862779617) A[1]:(-0.562965869904) A[2]:(0.40454480052) A[3]:(0.883241176605)\n",
      " state (8)  A[0]:(0.655355274677) A[1]:(0.000395327777369) A[2]:(0.728742361069) A[3]:(0.590605854988)\n",
      " state (9)  A[0]:(0.655897498131) A[1]:(0.81007283926) A[2]:(0.809911251068) A[3]:(-1.54972076416e-06)\n",
      " state (10)  A[0]:(0.729094326496) A[1]:(0.900068342686) A[2]:(-0.000141978263855) A[3]:(0.729087591171)\n",
      " state (11)  A[0]:(0.124746762216) A[1]:(0.879953920841) A[2]:(-0.915679752827) A[3]:(0.798144340515)\n",
      " state (12)  A[0]:(-0.430893778801) A[1]:(0.811890661716) A[2]:(-0.932863235474) A[3]:(0.712599754333)\n",
      " state (13)  A[0]:(-0.000784680072684) A[1]:(0.809527039528) A[2]:(0.900049805641) A[3]:(0.728886723518)\n",
      " state (14)  A[0]:(0.809721946716) A[1]:(0.900195121765) A[2]:(0.999999344349) A[3]:(0.809769153595)\n",
      " state (15)  A[0]:(0.97955429554) A[1]:(0.941960573196) A[2]:(1.0) A[3]:(0.879195451736)\n",
      "Episode 406000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6140. Times reached goal: 955.               Steps done: 3085992. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0411162640961.\n",
      " state (0)  A[0]:(0.531135857105) A[1]:(0.590751588345) A[2]:(0.590124249458) A[3]:(0.531081557274)\n",
      " state (1)  A[0]:(0.531195402145) A[1]:(0.000118598341942) A[2]:(0.65601426363) A[3]:(0.590240240097)\n",
      " state (2)  A[0]:(0.590299487114) A[1]:(0.729333519936) A[2]:(0.590608716011) A[3]:(0.656105995178)\n",
      " state (3)  A[0]:(0.656719624996) A[1]:(0.00670051993802) A[2]:(0.483772903681) A[3]:(0.561440706253)\n",
      " state (4)  A[0]:(0.590351343155) A[1]:(0.656257808208) A[2]:(-0.000357747048838) A[3]:(0.53134560585)\n",
      " state (5)  A[0]:(-0.0592166371644) A[1]:(0.999897956848) A[2]:(-0.749897360802) A[3]:(0.603686630726)\n",
      " state (6)  A[0]:(0.000118970870972) A[1]:(0.810147166252) A[2]:(-0.000494360865559) A[3]:(0.656071007252)\n",
      " state (7)  A[0]:(0.561213254929) A[1]:(-0.563835322857) A[2]:(0.40463244915) A[3]:(0.883635699749)\n",
      " state (8)  A[0]:(0.655641973019) A[1]:(-0.000546112598386) A[2]:(0.728847146034) A[3]:(0.591217279434)\n",
      " state (9)  A[0]:(0.655990242958) A[1]:(0.809931516647) A[2]:(0.80991601944) A[3]:(0.000110626220703)\n",
      " state (10)  A[0]:(0.728725731373) A[1]:(0.899925589561) A[2]:(-0.000973343558144) A[3]:(0.728952109814)\n",
      " state (11)  A[0]:(0.123211279511) A[1]:(0.87974768877) A[2]:(-0.916012525558) A[3]:(0.798020780087)\n",
      " state (12)  A[0]:(-0.431832879782) A[1]:(0.811585307121) A[2]:(-0.933230578899) A[3]:(0.712723970413)\n",
      " state (13)  A[0]:(-0.00097441644175) A[1]:(0.809284687042) A[2]:(0.899718523026) A[3]:(0.729322016239)\n",
      " state (14)  A[0]:(0.809995353222) A[1]:(0.900181531906) A[2]:(0.999999344349) A[3]:(0.810219764709)\n",
      " state (15)  A[0]:(0.979618310928) A[1]:(0.942015647888) A[2]:(1.0) A[3]:(0.879510760307)\n",
      "Episode 407000 finished after 0 timesteps with r=0.0. Running score: 0.93. Times trained:               6118. Times reached goal: 959.               Steps done: 3092110. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0408654827148.\n",
      " state (0)  A[0]:(0.531661391258) A[1]:(0.590570271015) A[2]:(0.59043610096) A[3]:(0.531779289246)\n",
      " state (1)  A[0]:(0.53167539835) A[1]:(-2.55703926086e-05) A[2]:(0.656231403351) A[3]:(0.590501785278)\n",
      " state (2)  A[0]:(0.590342342854) A[1]:(0.728752732277) A[2]:(0.591141104698) A[3]:(0.656419038773)\n",
      " state (3)  A[0]:(0.656779646873) A[1]:(0.0057216854766) A[2]:(0.484254598618) A[3]:(0.562206268311)\n",
      " state (4)  A[0]:(0.590375125408) A[1]:(0.655979275703) A[2]:(2.90870666504e-05) A[3]:(0.531863510609)\n",
      " state (5)  A[0]:(-0.0595539808273) A[1]:(0.999897897243) A[2]:(-0.749763846397) A[3]:(0.602630674839)\n",
      " state (6)  A[0]:(0.000139236450195) A[1]:(0.809960246086) A[2]:(-0.000215649604797) A[3]:(0.655944108963)\n",
      " state (7)  A[0]:(0.561286091805) A[1]:(-0.563331365585) A[2]:(0.404488176107) A[3]:(0.883671283722)\n",
      " state (8)  A[0]:(0.655360877514) A[1]:(-4.22596931458e-05) A[2]:(0.728747308254) A[3]:(0.59089076519)\n",
      " state (9)  A[0]:(0.655651569366) A[1]:(0.809958934784) A[2]:(0.809913992882) A[3]:(-0.000350654125214)\n",
      " state (10)  A[0]:(0.72874891758) A[1]:(0.900000214577) A[2]:(-0.000253796577454) A[3]:(0.72880512476)\n",
      " state (11)  A[0]:(0.123961515725) A[1]:(0.879960477352) A[2]:(-0.915892243385) A[3]:(0.79796475172)\n",
      " state (12)  A[0]:(-0.431148171425) A[1]:(0.811980366707) A[2]:(-0.933217227459) A[3]:(0.712558269501)\n",
      " state (13)  A[0]:(-0.00055652851006) A[1]:(0.80962562561) A[2]:(0.89996021986) A[3]:(0.729019165039)\n",
      " state (14)  A[0]:(0.809913158417) A[1]:(0.900338411331) A[2]:(0.999999344349) A[3]:(0.809877753258)\n",
      " state (15)  A[0]:(0.979589939117) A[1]:(0.94207918644) A[2]:(1.0) A[3]:(0.879220724106)\n",
      "Episode 408000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6041. Times reached goal: 951.               Steps done: 3098151. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0406193585004.\n",
      " state (0)  A[0]:(0.530691802502) A[1]:(0.590486526489) A[2]:(0.590406119823) A[3]:(0.531341195107)\n",
      " state (1)  A[0]:(0.530854582787) A[1]:(-0.000157058238983) A[2]:(0.655955672264) A[3]:(0.59034371376)\n",
      " state (2)  A[0]:(0.589800834656) A[1]:(0.72899889946) A[2]:(0.590914011002) A[3]:(0.656086921692)\n",
      " state (3)  A[0]:(0.656279802322) A[1]:(0.00621912814677) A[2]:(0.484039664268) A[3]:(0.561854958534)\n",
      " state (4)  A[0]:(0.589741170406) A[1]:(0.655991792679) A[2]:(5.56707382202e-05) A[3]:(0.531411230564)\n",
      " state (5)  A[0]:(-0.0607735924423) A[1]:(0.999898016453) A[2]:(-0.749538064003) A[3]:(0.602042555809)\n",
      " state (6)  A[0]:(-0.000158235430717) A[1]:(0.80997890234) A[2]:(0.000151038169861) A[3]:(0.655860960484)\n",
      " state (7)  A[0]:(0.561313509941) A[1]:(-0.563508033752) A[2]:(0.404821932316) A[3]:(0.88363713026)\n",
      " state (8)  A[0]:(0.655259728432) A[1]:(-0.000201165676117) A[2]:(0.729028940201) A[3]:(0.590281367302)\n",
      " state (9)  A[0]:(0.655815720558) A[1]:(0.809853672981) A[2]:(0.810031354427) A[3]:(-0.00035852190922)\n",
      " state (10)  A[0]:(0.728850007057) A[1]:(0.899976670742) A[2]:(-0.000205636024475) A[3]:(0.7289083004)\n",
      " state (11)  A[0]:(0.123910598457) A[1]:(0.879999697208) A[2]:(-0.91600894928) A[3]:(0.797977387905)\n",
      " state (12)  A[0]:(-0.431186527014) A[1]:(0.812049329281) A[2]:(-0.933379709721) A[3]:(0.712564706802)\n",
      " state (13)  A[0]:(-0.000529080571141) A[1]:(0.809603691101) A[2]:(0.90007609129) A[3]:(0.729114413261)\n",
      " state (14)  A[0]:(0.809919834137) A[1]:(0.900271236897) A[2]:(0.999999344349) A[3]:(0.810028493404)\n",
      " state (15)  A[0]:(0.979596316814) A[1]:(0.94197845459) A[2]:(1.0) A[3]:(0.879362404346)\n",
      "Episode 409000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6131. Times reached goal: 948.               Steps done: 3104282. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0403710830794.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5906,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.6562,  0.0002,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556,  0.0001,  0.7290,  0.5908]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.8100,  0.8100, -0.0000]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 7.2910e-01,  8.9998e-01,  7.8678e-06,  7.2891e-01]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9002,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531418204308) A[1]:(0.590640246868) A[2]:(0.590451955795) A[3]:(0.531126022339)\n",
      " state (1)  A[0]:(0.531524181366) A[1]:(3.12924385071e-05) A[2]:(0.656199336052) A[3]:(0.590183138847)\n",
      " state (2)  A[0]:(0.590367674828) A[1]:(0.729042887688) A[2]:(0.591336846352) A[3]:(0.656193733215)\n",
      " state (3)  A[0]:(0.656789124012) A[1]:(0.00599979097024) A[2]:(0.484479874372) A[3]:(0.562022686005)\n",
      " state (4)  A[0]:(0.590221166611) A[1]:(0.65617275238) A[2]:(0.000154137611389) A[3]:(0.531286358833)\n",
      " state (5)  A[0]:(-0.0605306476355) A[1]:(0.999898135662) A[2]:(-0.74987578392) A[3]:(0.601079940796)\n",
      " state (6)  A[0]:(-0.000224530696869) A[1]:(0.810007214546) A[2]:(-0.000217914581299) A[3]:(0.655707359314)\n",
      " state (7)  A[0]:(0.561414182186) A[1]:(-0.563484311104) A[2]:(0.404765218496) A[3]:(0.88375389576)\n",
      " state (8)  A[0]:(0.655563592911) A[1]:(2.77161598206e-05) A[2]:(0.728982329369) A[3]:(0.590674996376)\n",
      " state (9)  A[0]:(0.656160771847) A[1]:(0.809989094734) A[2]:(0.810021162033) A[3]:(-0.000220358371735)\n",
      " state (10)  A[0]:(0.729083180428) A[1]:(0.899999499321) A[2]:(-2.06232070923e-05) A[3]:(0.728834033012)\n",
      " state (11)  A[0]:(0.124223813415) A[1]:(0.879960298538) A[2]:(-0.916044414043) A[3]:(0.797843635082)\n",
      " state (12)  A[0]:(-0.430930674076) A[1]:(0.811886370182) A[2]:(-0.933503866196) A[3]:(0.712351560593)\n",
      " state (13)  A[0]:(-8.10325145721e-05) A[1]:(0.809359073639) A[2]:(0.900032103062) A[3]:(0.728935837746)\n",
      " state (14)  A[0]:(0.810114741325) A[1]:(0.900165379047) A[2]:(0.999999403954) A[3]:(0.809906005859)\n",
      " state (15)  A[0]:(0.979621529579) A[1]:(0.941939413548) A[2]:(1.0) A[3]:(0.879268348217)\n",
      "Episode 410000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6117. Times reached goal: 957.               Steps done: 3110399. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0401248869229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531237185001) A[1]:(0.59009206295) A[2]:(0.59048974514) A[3]:(0.531264662743)\n",
      " state (1)  A[0]:(0.531277954578) A[1]:(5.49107789993e-05) A[2]:(0.656188845634) A[3]:(0.590412318707)\n",
      " state (2)  A[0]:(0.590211093426) A[1]:(0.729000866413) A[2]:(0.592006087303) A[3]:(0.65618622303)\n",
      " state (3)  A[0]:(0.656621456146) A[1]:(0.00484613282606) A[2]:(0.485769331455) A[3]:(0.562008142471)\n",
      " state (4)  A[0]:(0.589827656746) A[1]:(0.656253039837) A[2]:(0.00116109801456) A[3]:(0.531413078308)\n",
      " state (5)  A[0]:(-0.0608581863344) A[1]:(0.999898254871) A[2]:(-0.750212907791) A[3]:(0.601505041122)\n",
      " state (6)  A[0]:(0.000262051820755) A[1]:(0.810064435005) A[2]:(-0.000134587287903) A[3]:(0.655982673168)\n",
      " state (7)  A[0]:(0.5617800951) A[1]:(-0.563168644905) A[2]:(0.404847055674) A[3]:(0.883722186089)\n",
      " state (8)  A[0]:(0.65560901165) A[1]:(0.000245645642281) A[2]:(0.729040026665) A[3]:(0.589971423149)\n",
      " state (9)  A[0]:(0.656443595886) A[1]:(0.809932470322) A[2]:(0.810057222843) A[3]:(-0.000774353567977)\n",
      " state (10)  A[0]:(0.729735374451) A[1]:(0.899965286255) A[2]:(0.000168561935425) A[3]:(0.728891849518)\n",
      " state (11)  A[0]:(0.126318112016) A[1]:(0.879980623722) A[2]:(-0.916080713272) A[3]:(0.798085451126)\n",
      " state (12)  A[0]:(-0.429249972105) A[1]:(0.811939477921) A[2]:(-0.933661222458) A[3]:(0.712659955025)\n",
      " state (13)  A[0]:(0.00147582485806) A[1]:(0.809340775013) A[2]:(0.899837255478) A[3]:(0.729129195213)\n",
      " state (14)  A[0]:(0.810440838337) A[1]:(0.900118529797) A[2]:(0.999999403954) A[3]:(0.809989571571)\n",
      " state (15)  A[0]:(0.979643464088) A[1]:(0.941883862019) A[2]:(1.0) A[3]:(0.879272997379)\n",
      "Episode 411000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6107. Times reached goal: 962.               Steps done: 3116506. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0398805909554.\n",
      " state (0)  A[0]:(0.530898332596) A[1]:(0.590213656425) A[2]:(0.58994692564) A[3]:(0.530812621117)\n",
      " state (1)  A[0]:(0.531100869179) A[1]:(-1.99526548386e-05) A[2]:(0.656086802483) A[3]:(0.589902877808)\n",
      " state (2)  A[0]:(0.589877724648) A[1]:(0.728845775127) A[2]:(0.591246843338) A[3]:(0.655846953392)\n",
      " state (3)  A[0]:(0.656462311745) A[1]:(0.00342089100741) A[2]:(0.48507553339) A[3]:(0.56170219183)\n",
      " state (4)  A[0]:(0.589878380299) A[1]:(0.655867099762) A[2]:(-5.6266784668e-05) A[3]:(0.531210541725)\n",
      " state (5)  A[0]:(-0.0599504373968) A[1]:(0.999898254871) A[2]:(-0.750871598721) A[3]:(0.601232528687)\n",
      " state (6)  A[0]:(0.000255107879639) A[1]:(0.809955716133) A[2]:(-0.0002760887146) A[3]:(0.655686020851)\n",
      " state (7)  A[0]:(0.561546206474) A[1]:(-0.563288807869) A[2]:(0.404964148998) A[3]:(0.883664011955)\n",
      " state (8)  A[0]:(0.655430018902) A[1]:(8.16136598587e-05) A[2]:(0.728947758675) A[3]:(0.590403914452)\n",
      " state (9)  A[0]:(0.655866503716) A[1]:(0.809991300106) A[2]:(0.809980928898) A[3]:(-0.000303864479065)\n",
      " state (10)  A[0]:(0.728934168816) A[1]:(0.89999371767) A[2]:(-7.42673873901e-05) A[3]:(0.728944540024)\n",
      " state (11)  A[0]:(0.124151013792) A[1]:(0.880033373833) A[2]:(-0.916162490845) A[3]:(0.798092603683)\n",
      " state (12)  A[0]:(-0.431147009134) A[1]:(0.812058329582) A[2]:(-0.933728456497) A[3]:(0.712638497353)\n",
      " state (13)  A[0]:(-0.000525385083165) A[1]:(0.809499680996) A[2]:(0.900064229965) A[3]:(0.729081094265)\n",
      " state (14)  A[0]:(0.809963464737) A[1]:(0.900235414505) A[2]:(0.999999403954) A[3]:(0.809927046299)\n",
      " state (15)  A[0]:(0.979609668255) A[1]:(0.94194072485) A[2]:(1.0) A[3]:(0.879195451736)\n",
      "Episode 412000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6091. Times reached goal: 963.               Steps done: 3122597. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0396384165667.\n",
      " state (0)  A[0]:(0.531329631805) A[1]:(0.590792059898) A[2]:(0.590748488903) A[3]:(0.532262086868)\n",
      " state (1)  A[0]:(0.531473040581) A[1]:(0.000326856970787) A[2]:(0.656628847122) A[3]:(0.591311693192)\n",
      " state (2)  A[0]:(0.590525329113) A[1]:(0.729783117771) A[2]:(0.592415332794) A[3]:(0.65711915493)\n",
      " state (3)  A[0]:(0.657143592834) A[1]:(0.00380226620473) A[2]:(0.486734181643) A[3]:(0.562897801399)\n",
      " state (4)  A[0]:(0.59049808979) A[1]:(0.656294286251) A[2]:(0.00101590121631) A[3]:(0.532119870186)\n",
      " state (5)  A[0]:(-0.0596308968961) A[1]:(0.999898552895) A[2]:(-0.751581370831) A[3]:(0.601823329926)\n",
      " state (6)  A[0]:(-2.44230031967e-05) A[1]:(0.810399770737) A[2]:(-0.000630140246358) A[3]:(0.656215548515)\n",
      " state (7)  A[0]:(0.561295926571) A[1]:(-0.56325095892) A[2]:(0.405260205269) A[3]:(0.883840203285)\n",
      " state (8)  A[0]:(0.655648469925) A[1]:(-8.94218683243e-05) A[2]:(0.729285955429) A[3]:(0.590476691723)\n",
      " state (9)  A[0]:(0.656153202057) A[1]:(0.809982538223) A[2]:(0.810261487961) A[3]:(-0.00150659563951)\n",
      " state (10)  A[0]:(0.728617787361) A[1]:(0.8999132514) A[2]:(-0.000694513204508) A[3]:(0.728151917458)\n",
      " state (11)  A[0]:(0.122480534017) A[1]:(0.879860281944) A[2]:(-0.916526138783) A[3]:(0.797522544861)\n",
      " state (12)  A[0]:(-0.432169318199) A[1]:(0.811687707901) A[2]:(-0.934096932411) A[3]:(0.712294995785)\n",
      " state (13)  A[0]:(-0.000781223003287) A[1]:(0.809003949165) A[2]:(0.899725615978) A[3]:(0.729194104671)\n",
      " state (14)  A[0]:(0.81015175581) A[1]:(0.899922668934) A[2]:(0.999999403954) A[3]:(0.810213804245)\n",
      " state (15)  A[0]:(0.979653954506) A[1]:(0.941722273827) A[2]:(1.0) A[3]:(0.879429340363)\n",
      "Episode 413000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6125. Times reached goal: 961.               Steps done: 3128722. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0393963732795.\n",
      " state (0)  A[0]:(0.532132446766) A[1]:(0.590878903866) A[2]:(0.590460300446) A[3]:(0.531742215157)\n",
      " state (1)  A[0]:(0.53219974041) A[1]:(-0.0001480281353) A[2]:(0.656272470951) A[3]:(0.59042930603)\n",
      " state (2)  A[0]:(0.590777635574) A[1]:(0.728854656219) A[2]:(0.592652261257) A[3]:(0.656280994415)\n",
      " state (3)  A[0]:(0.657470703125) A[1]:(4.98592853546e-05) A[2]:(0.487608253956) A[3]:(0.562058866024)\n",
      " state (4)  A[0]:(0.590755999088) A[1]:(0.65633392334) A[2]:(0.000794052903075) A[3]:(0.531751096249)\n",
      " state (5)  A[0]:(-0.0581322088838) A[1]:(0.999898433685) A[2]:(-0.752480387688) A[3]:(0.602079510689)\n",
      " state (6)  A[0]:(0.00120939256158) A[1]:(0.809872746468) A[2]:(-0.000376582116587) A[3]:(0.656068563461)\n",
      " state (7)  A[0]:(0.562073290348) A[1]:(-0.56308478117) A[2]:(0.405559957027) A[3]:(0.883789122105)\n",
      " state (8)  A[0]:(0.656280577183) A[1]:(0.00013592839241) A[2]:(0.728968024254) A[3]:(0.590932071209)\n",
      " state (9)  A[0]:(0.656788289547) A[1]:(0.810057163239) A[2]:(0.8098706007) A[3]:(-0.000285595655441)\n",
      " state (10)  A[0]:(0.729570567608) A[1]:(0.899993896484) A[2]:(-0.000899076228961) A[3]:(0.728889882565)\n",
      " state (11)  A[0]:(0.125135377049) A[1]:(0.880037009716) A[2]:(-0.916466712952) A[3]:(0.798129141331)\n",
      " state (12)  A[0]:(-0.430546104908) A[1]:(0.812025010586) A[2]:(-0.934090673923) A[3]:(0.712668001652)\n",
      " state (13)  A[0]:(0.00025600194931) A[1]:(0.809352040291) A[2]:(0.899906635284) A[3]:(0.729068160057)\n",
      " state (14)  A[0]:(0.810304105282) A[1]:(0.900106191635) A[2]:(0.999999403954) A[3]:(0.809892475605)\n",
      " state (15)  A[0]:(0.979657948017) A[1]:(0.94180804491) A[2]:(1.0) A[3]:(0.879095554352)\n",
      "Episode 414000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6104. Times reached goal: 961.               Steps done: 3134826. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0391566302571.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5907,  0.5902,  0.5304]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5909,  0.6562,  0.0007,  0.5322]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6573, -0.0011,  0.7290,  0.5934]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6581,  0.8100,  0.8101,  0.0038]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7300,  0.9002, -0.0004,  0.7300]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9003,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53097820282) A[1]:(0.5904083848) A[2]:(0.590425729752) A[3]:(0.530754208565)\n",
      " state (1)  A[0]:(0.531434178352) A[1]:(-0.000123664736748) A[2]:(0.656293511391) A[3]:(0.590095937252)\n",
      " state (2)  A[0]:(0.590482890606) A[1]:(0.729035615921) A[2]:(0.592498004436) A[3]:(0.656175732613)\n",
      " state (3)  A[0]:(0.657249331474) A[1]:(-0.00154678395484) A[2]:(0.487972259521) A[3]:(0.561777949333)\n",
      " state (4)  A[0]:(0.590554714203) A[1]:(0.655758976936) A[2]:(0.000368595094187) A[3]:(0.531566858292)\n",
      " state (5)  A[0]:(-0.0574975088239) A[1]:(0.999898374081) A[2]:(-0.753542482853) A[3]:(0.602439522743)\n",
      " state (6)  A[0]:(0.00128285516985) A[1]:(0.80998557806) A[2]:(-0.000203609466553) A[3]:(0.655803561211)\n",
      " state (7)  A[0]:(0.561818480492) A[1]:(-0.562258422375) A[2]:(0.406049489975) A[3]:(0.883533656597)\n",
      " state (8)  A[0]:(0.656380534172) A[1]:(0.000642135622911) A[2]:(0.729099690914) A[3]:(0.590912401676)\n",
      " state (9)  A[0]:(0.65688085556) A[1]:(0.810157954693) A[2]:(0.80998390913) A[3]:(-8.31782817841e-05)\n",
      " state (10)  A[0]:(0.729274511337) A[1]:(0.900068998337) A[2]:(-0.00095057458384) A[3]:(0.728689968586)\n",
      " state (11)  A[0]:(0.124159567058) A[1]:(0.880196809769) A[2]:(-0.916535615921) A[3]:(0.797934949398)\n",
      " state (12)  A[0]:(-0.431033998728) A[1]:(0.812320411205) A[2]:(-0.934089422226) A[3]:(0.712547183037)\n",
      " state (13)  A[0]:(0.00017999112606) A[1]:(0.809627354145) A[2]:(0.900318324566) A[3]:(0.729190707207)\n",
      " state (14)  A[0]:(0.810337424278) A[1]:(0.900214850903) A[2]:(0.999999403954) A[3]:(0.810168266296)\n",
      " state (15)  A[0]:(0.979659199715) A[1]:(0.941816210747) A[2]:(1.0) A[3]:(0.879332959652)\n",
      "Episode 415000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6039. Times reached goal: 952.               Steps done: 3140865. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0389208759436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531060576439) A[1]:(0.590686321259) A[2]:(0.590226233006) A[3]:(0.530850887299)\n",
      " state (1)  A[0]:(0.53144967556) A[1]:(9.3474984169e-05) A[2]:(0.656561374664) A[3]:(0.590445280075)\n",
      " state (2)  A[0]:(0.590587735176) A[1]:(0.729279518127) A[2]:(0.593701839447) A[3]:(0.656133890152)\n",
      " state (3)  A[0]:(0.657406210899) A[1]:(-0.00305697857402) A[2]:(0.490455061197) A[3]:(0.561079204082)\n",
      " state (4)  A[0]:(0.590433478355) A[1]:(0.656656622887) A[2]:(0.00226246914826) A[3]:(0.531342625618)\n",
      " state (5)  A[0]:(-0.0572786331177) A[1]:(0.999898731709) A[2]:(-0.754221558571) A[3]:(0.604652404785)\n",
      " state (6)  A[0]:(0.00112399412319) A[1]:(0.810351371765) A[2]:(0.000573277415242) A[3]:(0.656496882439)\n",
      " state (7)  A[0]:(0.561473608017) A[1]:(-0.561701059341) A[2]:(0.407132565975) A[3]:(0.883540570736)\n",
      " state (8)  A[0]:(0.656381309032) A[1]:(0.00016762316227) A[2]:(0.729094386101) A[3]:(0.592050731182)\n",
      " state (9)  A[0]:(0.656177639961) A[1]:(0.810055315495) A[2]:(0.809843301773) A[3]:(5.79357147217e-05)\n",
      " state (10)  A[0]:(0.728448867798) A[1]:(0.900138020515) A[2]:(-0.00176262669265) A[3]:(0.728357076645)\n",
      " state (11)  A[0]:(0.122921325266) A[1]:(0.880452990532) A[2]:(-0.916721463203) A[3]:(0.797766029835)\n",
      " state (12)  A[0]:(-0.431269466877) A[1]:(0.812882900238) A[2]:(-0.934219419956) A[3]:(0.712445378304)\n",
      " state (13)  A[0]:(0.000470072001917) A[1]:(0.810253441334) A[2]:(0.900375425816) A[3]:(0.729073882103)\n",
      " state (14)  A[0]:(0.810419917107) A[1]:(0.900574803352) A[2]:(0.999999403954) A[3]:(0.809966206551)\n",
      " state (15)  A[0]:(0.979655981064) A[1]:(0.942033529282) A[2]:(1.0) A[3]:(0.879059016705)\n",
      "Episode 416000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6129. Times reached goal: 961.               Steps done: 3146994. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0386830594281.\n",
      " state (0)  A[0]:(0.530285716057) A[1]:(0.590500831604) A[2]:(0.590511918068) A[3]:(0.531418204308)\n",
      " state (1)  A[0]:(0.530800700188) A[1]:(9.39816236496e-05) A[2]:(0.656027674675) A[3]:(0.59158718586)\n",
      " state (2)  A[0]:(0.590190649033) A[1]:(0.729049921036) A[2]:(0.593629360199) A[3]:(0.657120943069)\n",
      " state (3)  A[0]:(0.656917512417) A[1]:(-0.00565828289837) A[2]:(0.491272419691) A[3]:(0.561833977699)\n",
      " state (4)  A[0]:(0.589806675911) A[1]:(0.655948221684) A[2]:(0.00198948127218) A[3]:(0.532772302628)\n",
      " state (5)  A[0]:(-0.0566208027303) A[1]:(0.99989849329) A[2]:(-0.755960583687) A[3]:(0.60817694664)\n",
      " state (6)  A[0]:(0.00126933981664) A[1]:(0.80996966362) A[2]:(-0.00125527312048) A[3]:(0.657966971397)\n",
      " state (7)  A[0]:(0.561566948891) A[1]:(-0.561874747276) A[2]:(0.406053364277) A[3]:(0.883837640285)\n",
      " state (8)  A[0]:(0.657123982906) A[1]:(-0.000199511647224) A[2]:(0.728755831718) A[3]:(0.593967080116)\n",
      " state (9)  A[0]:(0.658507049084) A[1]:(0.809844970703) A[2]:(0.810118019581) A[3]:(0.00629357155412)\n",
      " state (10)  A[0]:(0.731991291046) A[1]:(0.899958372116) A[2]:(0.00121974886861) A[3]:(0.732315897942)\n",
      " state (11)  A[0]:(0.131501942873) A[1]:(0.88017809391) A[2]:(-0.916203439236) A[3]:(0.800967991352)\n",
      " state (12)  A[0]:(-0.42657917738) A[1]:(0.812309741974) A[2]:(-0.934213340282) A[3]:(0.715623855591)\n",
      " state (13)  A[0]:(0.00297901639715) A[1]:(0.809474110603) A[2]:(0.899850845337) A[3]:(0.730942249298)\n",
      " state (14)  A[0]:(0.81071472168) A[1]:(0.900089979172) A[2]:(0.999999403954) A[3]:(0.810870230198)\n",
      " state (15)  A[0]:(0.979664742947) A[1]:(0.941738665104) A[2]:(1.0) A[3]:(0.879457652569)\n",
      "Episode 417000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6112. Times reached goal: 967.               Steps done: 3153106. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0384473496318.\n",
      " state (0)  A[0]:(0.531409144402) A[1]:(0.590515851974) A[2]:(0.59102332592) A[3]:(0.531405329704)\n",
      " state (1)  A[0]:(0.531555294991) A[1]:(5.84125518799e-06) A[2]:(0.656305611134) A[3]:(0.590830922127)\n",
      " state (2)  A[0]:(0.590387940407) A[1]:(0.728882551193) A[2]:(0.593843281269) A[3]:(0.656319141388)\n",
      " state (3)  A[0]:(0.657208919525) A[1]:(-0.00881088804454) A[2]:(0.492290526628) A[3]:(0.560264706612)\n",
      " state (4)  A[0]:(0.589949846268) A[1]:(0.656191587448) A[2]:(0.00147771730553) A[3]:(0.531479418278)\n",
      " state (5)  A[0]:(-0.0558084174991) A[1]:(0.99989849329) A[2]:(-0.757537841797) A[3]:(0.608518779278)\n",
      " state (6)  A[0]:(-0.000100016593933) A[1]:(0.809954285622) A[2]:(-0.00101947749499) A[3]:(0.656182050705)\n",
      " state (7)  A[0]:(0.559751629829) A[1]:(-0.561242282391) A[2]:(0.407103419304) A[3]:(0.882748961449)\n",
      " state (8)  A[0]:(0.655382752419) A[1]:(0.000124096870422) A[2]:(0.728844344616) A[3]:(0.590913891792)\n",
      " state (9)  A[0]:(0.655719518661) A[1]:(0.809920191765) A[2]:(0.809876561165) A[3]:(-0.00010946393013)\n",
      " state (10)  A[0]:(0.728900194168) A[1]:(0.899964332581) A[2]:(-0.000509500445332) A[3]:(0.728996634483)\n",
      " state (11)  A[0]:(0.124723099172) A[1]:(0.880178570747) A[2]:(-0.916579961777) A[3]:(0.798685729504)\n",
      " state (12)  A[0]:(-0.430990546942) A[1]:(0.812291026115) A[2]:(-0.934445977211) A[3]:(0.713262498379)\n",
      " state (13)  A[0]:(-0.000733196618967) A[1]:(0.809420466423) A[2]:(0.89983278513) A[3]:(0.729297280312)\n",
      " state (14)  A[0]:(0.809786021709) A[1]:(0.900070786476) A[2]:(0.999999403954) A[3]:(0.809933423996)\n",
      " state (15)  A[0]:(0.979566395283) A[1]:(0.941731750965) A[2]:(1.0) A[3]:(0.878879547119)\n",
      "Episode 418000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6116. Times reached goal: 967.               Steps done: 3159222. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0382129232481.\n",
      " state (0)  A[0]:(0.531232714653) A[1]:(0.590399682522) A[2]:(0.590399265289) A[3]:(0.531156420708)\n",
      " state (1)  A[0]:(0.531537294388) A[1]:(0.000887155300006) A[2]:(0.656534194946) A[3]:(0.589935302734)\n",
      " state (2)  A[0]:(0.590253472328) A[1]:(0.729038476944) A[2]:(0.594487369061) A[3]:(0.655489444733)\n",
      " state (3)  A[0]:(0.657060265541) A[1]:(-0.0103796571493) A[2]:(0.494074344635) A[3]:(0.559038221836)\n",
      " state (4)  A[0]:(0.589985966682) A[1]:(0.656341195107) A[2]:(0.00256621278822) A[3]:(0.531036734581)\n",
      " state (5)  A[0]:(-0.0534066073596) A[1]:(0.99989849329) A[2]:(-0.758435368538) A[3]:(0.610245585442)\n",
      " state (6)  A[0]:(0.000917121535167) A[1]:(0.809916973114) A[2]:(0.00017249584198) A[3]:(0.655370354652)\n",
      " state (7)  A[0]:(0.560473144054) A[1]:(-0.56001919508) A[2]:(0.408224612474) A[3]:(0.882282674313)\n",
      " state (8)  A[0]:(0.657193422318) A[1]:(0.000937938399147) A[2]:(0.72887802124) A[3]:(0.592303752899)\n",
      " state (9)  A[0]:(0.65686404705) A[1]:(0.810342490673) A[2]:(0.810035645962) A[3]:(4.34815883636e-05)\n",
      " state (10)  A[0]:(0.729200482368) A[1]:(0.900030910969) A[2]:(-0.000399947137339) A[3]:(0.728229522705)\n",
      " state (11)  A[0]:(0.125224858522) A[1]:(0.879945278168) A[2]:(-0.916669428349) A[3]:(0.797981262207)\n",
      " state (12)  A[0]:(-0.429493248463) A[1]:(0.811479926109) A[2]:(-0.934579432011) A[3]:(0.712559223175)\n",
      " state (13)  A[0]:(0.00253940583207) A[1]:(0.808237731457) A[2]:(0.899594664574) A[3]:(0.728975772858)\n",
      " state (14)  A[0]:(0.811119019985) A[1]:(0.89937633276) A[2]:(0.999999403954) A[3]:(0.809896469116)\n",
      " state (15)  A[0]:(0.979708015919) A[1]:(0.941373348236) A[2]:(1.0) A[3]:(0.878855526447)\n",
      "Episode 419000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6107. Times reached goal: 968.               Steps done: 3165329. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0379802680615.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5911,  0.5905,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.6561,  0.0005,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557, -0.0004,  0.7289,  0.5911]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558,  0.8098,  0.8100, -0.0008]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0001,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9004,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531520605087) A[1]:(0.591086447239) A[2]:(0.590496778488) A[3]:(0.531311511993)\n",
      " state (1)  A[0]:(0.531700849533) A[1]:(-2.51531600952e-05) A[2]:(0.65634137392) A[3]:(0.590579390526)\n",
      " state (2)  A[0]:(0.590551495552) A[1]:(0.729107320309) A[2]:(0.594104647636) A[3]:(0.65620392561)\n",
      " state (3)  A[0]:(0.657609701157) A[1]:(-0.0147409485653) A[2]:(0.494249403477) A[3]:(0.559026896954)\n",
      " state (4)  A[0]:(0.590587615967) A[1]:(0.65614926815) A[2]:(0.000569820345845) A[3]:(0.531483829021)\n",
      " state (5)  A[0]:(-0.0511602014303) A[1]:(0.999898552895) A[2]:(-0.760607421398) A[3]:(0.612844824791)\n",
      " state (6)  A[0]:(0.000921725993976) A[1]:(0.810108065605) A[2]:(-0.000911950832233) A[3]:(0.656128048897)\n",
      " state (7)  A[0]:(0.559317231178) A[1]:(-0.560243427753) A[2]:(0.408764719963) A[3]:(0.882123529911)\n",
      " state (8)  A[0]:(0.656072795391) A[1]:(-3.93390655518e-06) A[2]:(0.728967428207) A[3]:(0.591562688351)\n",
      " state (9)  A[0]:(0.656213700771) A[1]:(0.809993267059) A[2]:(0.810069382191) A[3]:(-5.33163547516e-05)\n",
      " state (10)  A[0]:(0.729494929314) A[1]:(0.900063633919) A[2]:(0.000666260602884) A[3]:(0.729211807251)\n",
      " state (11)  A[0]:(0.126445859671) A[1]:(0.880433619022) A[2]:(-0.916446685791) A[3]:(0.799347698689)\n",
      " state (12)  A[0]:(-0.4302097857) A[1]:(0.812834620476) A[2]:(-0.93447971344) A[3]:(0.71402990818)\n",
      " state (13)  A[0]:(-0.000302657485008) A[1]:(0.81006282568) A[2]:(0.900097608566) A[3]:(0.729648828506)\n",
      " state (14)  A[0]:(0.810029685497) A[1]:(0.900483727455) A[2]:(0.999999403954) A[3]:(0.81000828743)\n",
      " state (15)  A[0]:(0.979606628418) A[1]:(0.941972792149) A[2]:(1.0) A[3]:(0.878743827343)\n",
      "Episode 420000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6110. Times reached goal: 969.               Steps done: 3171439. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0377489161236.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531304121017) A[1]:(0.590706169605) A[2]:(0.590871572495) A[3]:(0.53149408102)\n",
      " state (1)  A[0]:(0.531579613686) A[1]:(0.000381395191653) A[2]:(0.656227827072) A[3]:(0.590896248817)\n",
      " state (2)  A[0]:(0.590356707573) A[1]:(0.729196548462) A[2]:(0.59483730793) A[3]:(0.656365394592)\n",
      " state (3)  A[0]:(0.657486140728) A[1]:(-0.0178714580834) A[2]:(0.49661397934) A[3]:(0.558477520943)\n",
      " state (4)  A[0]:(0.590341031551) A[1]:(0.656511604786) A[2]:(0.00207602675073) A[3]:(0.531612753868)\n",
      " state (5)  A[0]:(-0.0496196299791) A[1]:(0.999898731709) A[2]:(-0.761733829975) A[3]:(0.615668296814)\n",
      " state (6)  A[0]:(0.000647097709589) A[1]:(0.810199677944) A[2]:(-0.000820517365355) A[3]:(0.656266510487)\n",
      " state (7)  A[0]:(0.558432102203) A[1]:(-0.56020462513) A[2]:(0.409547805786) A[3]:(0.881669282913)\n",
      " state (8)  A[0]:(0.655975878239) A[1]:(-0.000513628066983) A[2]:(0.729176223278) A[3]:(0.591054499149)\n",
      " state (9)  A[0]:(0.656538009644) A[1]:(0.809807479382) A[2]:(0.810070157051) A[3]:(-2.29179859161e-05)\n",
      " state (10)  A[0]:(0.729473590851) A[1]:(0.899952411652) A[2]:(-0.000236392021179) A[3]:(0.728996157646)\n",
      " state (11)  A[0]:(0.126024708152) A[1]:(0.880338549614) A[2]:(-0.91672372818) A[3]:(0.799108862877)\n",
      " state (12)  A[0]:(-0.43031540513) A[1]:(0.812718987465) A[2]:(-0.934711635113) A[3]:(0.713761687279)\n",
      " state (13)  A[0]:(-0.000105530023575) A[1]:(0.809923350811) A[2]:(0.900010704994) A[3]:(0.729569196701)\n",
      " state (14)  A[0]:(0.810078442097) A[1]:(0.900396823883) A[2]:(0.999999403954) A[3]:(0.810132861137)\n",
      " state (15)  A[0]:(0.979600012302) A[1]:(0.941901028156) A[2]:(1.0) A[3]:(0.878853082657)\n",
      "Episode 421000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6106. Times reached goal: 960.               Steps done: 3177545. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0375191235125.\n",
      " state (0)  A[0]:(0.530316412449) A[1]:(0.590329289436) A[2]:(0.590205192566) A[3]:(0.531337141991)\n",
      " state (1)  A[0]:(0.530204355717) A[1]:(0.000222817063332) A[2]:(0.655947804451) A[3]:(0.590589404106)\n",
      " state (2)  A[0]:(0.588838398457) A[1]:(0.728924393654) A[2]:(0.59454035759) A[3]:(0.656143784523)\n",
      " state (3)  A[0]:(0.656210660934) A[1]:(-0.0215445645154) A[2]:(0.497536748648) A[3]:(0.557629346848)\n",
      " state (4)  A[0]:(0.588933169842) A[1]:(0.656262040138) A[2]:(0.00209843809716) A[3]:(0.531402587891)\n",
      " state (5)  A[0]:(-0.0494716838002) A[1]:(0.999898552895) A[2]:(-0.76283043623) A[3]:(0.61769592762)\n",
      " state (6)  A[0]:(-0.000278294086456) A[1]:(0.80985057354) A[2]:(-0.000425577134592) A[3]:(0.655926525593)\n",
      " state (7)  A[0]:(0.55727159977) A[1]:(-0.560023546219) A[2]:(0.410144120455) A[3]:(0.881101071835)\n",
      " state (8)  A[0]:(0.655384898186) A[1]:(-0.00132499553729) A[2]:(0.729056954384) A[3]:(0.590659737587)\n",
      " state (9)  A[0]:(0.655987381935) A[1]:(0.809442639351) A[2]:(0.809924483299) A[3]:(-4.72962856293e-05)\n",
      " state (10)  A[0]:(0.72915494442) A[1]:(0.899880290031) A[2]:(-0.000631213129964) A[3]:(0.729008734226)\n",
      " state (11)  A[0]:(0.125737369061) A[1]:(0.88047760725) A[2]:(-0.916836202145) A[3]:(0.799237966537)\n",
      " state (12)  A[0]:(-0.430728971958) A[1]:(0.813154935837) A[2]:(-0.934855937958) A[3]:(0.713754832745)\n",
      " state (13)  A[0]:(-0.000827774230856) A[1]:(0.810418248177) A[2]:(0.90000975132) A[3]:(0.729305624962)\n",
      " state (14)  A[0]:(0.809881985188) A[1]:(0.900626599789) A[2]:(0.999999403954) A[3]:(0.809816718102)\n",
      " state (15)  A[0]:(0.979592144489) A[1]:(0.941969335079) A[2]:(1.0) A[3]:(0.878551244736)\n",
      "Episode 422000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6103. Times reached goal: 960.               Steps done: 3183648. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0372908416125.\n",
      " state (0)  A[0]:(0.53118622303) A[1]:(0.590038478374) A[2]:(0.590534806252) A[3]:(0.531181335449)\n",
      " state (1)  A[0]:(0.531477928162) A[1]:(-0.000235512852669) A[2]:(0.655875086784) A[3]:(0.590432047844)\n",
      " state (2)  A[0]:(0.590133190155) A[1]:(0.729236364365) A[2]:(0.594216346741) A[3]:(0.656023383141)\n",
      " state (3)  A[0]:(0.657180547714) A[1]:(-0.0249860268086) A[2]:(0.498056650162) A[3]:(0.556715726852)\n",
      " state (4)  A[0]:(0.589862942696) A[1]:(0.65597319603) A[2]:(0.000927447981667) A[3]:(0.531251013279)\n",
      " state (5)  A[0]:(-0.0463152192533) A[1]:(0.999898612499) A[2]:(-0.764649271965) A[3]:(0.620268464088)\n",
      " state (6)  A[0]:(0.000421062082751) A[1]:(0.810090065002) A[2]:(-0.000810503785033) A[3]:(0.655665457249)\n",
      " state (7)  A[0]:(0.5569845438) A[1]:(-0.558606386185) A[2]:(0.410579025745) A[3]:(0.880602300167)\n",
      " state (8)  A[0]:(0.655517160892) A[1]:(0.00016288459301) A[2]:(0.729064941406) A[3]:(0.590537667274)\n",
      " state (9)  A[0]:(0.656029820442) A[1]:(0.809913635254) A[2]:(0.810067296028) A[3]:(-5.45382499695e-05)\n",
      " state (10)  A[0]:(0.729315280914) A[1]:(0.89999884367) A[2]:(3.55243682861e-05) A[3]:(0.729107379913)\n",
      " state (11)  A[0]:(0.126544445753) A[1]:(0.880422592163) A[2]:(-0.91676568985) A[3]:(0.799488246441)\n",
      " state (12)  A[0]:(-0.429975777864) A[1]:(0.812765419483) A[2]:(-0.934878587723) A[3]:(0.714052438736)\n",
      " state (13)  A[0]:(3.38703393936e-05) A[1]:(0.809762120247) A[2]:(0.900039494038) A[3]:(0.72953081131)\n",
      " state (14)  A[0]:(0.810070574284) A[1]:(0.900199592113) A[2]:(0.999999403954) A[3]:(0.810004711151)\n",
      " state (15)  A[0]:(0.979584276676) A[1]:(0.941704452038) A[2]:(1.0) A[3]:(0.878625750542)\n",
      "Episode 423000 finished after 0 timesteps with r=0.0. Running score: 0.93. Times trained:               6063. Times reached goal: 958.               Steps done: 3189711. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0370654312618.\n",
      " state (0)  A[0]:(0.532202541828) A[1]:(0.591234147549) A[2]:(0.590587198734) A[3]:(0.532114028931)\n",
      " state (1)  A[0]:(0.532090127468) A[1]:(-9.0092420578e-05) A[2]:(0.656225681305) A[3]:(0.590707182884)\n",
      " state (2)  A[0]:(0.590216696262) A[1]:(0.729172825813) A[2]:(0.595483481884) A[3]:(0.656472980976)\n",
      " state (3)  A[0]:(0.657692909241) A[1]:(-0.0296972375363) A[2]:(0.500874638557) A[3]:(0.55663383007)\n",
      " state (4)  A[0]:(0.590437829494) A[1]:(0.65659224987) A[2]:(0.00198244792409) A[3]:(0.532019436359)\n",
      " state (5)  A[0]:(-0.0439918227494) A[1]:(0.999898672104) A[2]:(-0.766209065914) A[3]:(0.623221933842)\n",
      " state (6)  A[0]:(9.09864902496e-05) A[1]:(0.809962689877) A[2]:(-0.000689148786478) A[3]:(0.656224370003)\n",
      " state (7)  A[0]:(0.556416869164) A[1]:(-0.558854699135) A[2]:(0.411884397268) A[3]:(0.880679965019)\n",
      " state (8)  A[0]:(0.655816555023) A[1]:(0.00047996637295) A[2]:(0.729385852814) A[3]:(0.591416656971)\n",
      " state (9)  A[0]:(0.656195998192) A[1]:(0.81035733223) A[2]:(0.810157954693) A[3]:(-0.000283390283585)\n",
      " state (10)  A[0]:(0.729142189026) A[1]:(0.900027573109) A[2]:(8.86917114258e-05) A[3]:(0.728944897652)\n",
      " state (11)  A[0]:(0.125567615032) A[1]:(0.880144000053) A[2]:(-0.916802406311) A[3]:(0.79948425293)\n",
      " state (12)  A[0]:(-0.430897504091) A[1]:(0.812024116516) A[2]:(-0.934953808784) A[3]:(0.714078426361)\n",
      " state (13)  A[0]:(-0.000554293335881) A[1]:(0.808922410011) A[2]:(0.900008797646) A[3]:(0.72951900959)\n",
      " state (14)  A[0]:(0.810160934925) A[1]:(0.899844408035) A[2]:(0.999999403954) A[3]:(0.809979081154)\n",
      " state (15)  A[0]:(0.979601383209) A[1]:(0.941594958305) A[2]:(1.0) A[3]:(0.878500938416)\n",
      "Episode 424000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6113. Times reached goal: 956.               Steps done: 3195824. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0368395414162.\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.5905,  0.5906,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5320, -0.0007,  0.6558,  0.5909]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.7287,  0.5947,  0.6564]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0014,  0.8102, -0.0014,  0.6564]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7311,  0.9000,  0.0007,  0.7303]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.8999,  1.0000,  0.8104]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531610429287) A[1]:(0.590476989746) A[2]:(0.590500771999) A[3]:(0.531347095966)\n",
      " state (1)  A[0]:(0.531567454338) A[1]:(-0.000674515846185) A[2]:(0.655919909477) A[3]:(0.590328931808)\n",
      " state (2)  A[0]:(0.590128064156) A[1]:(0.728903889656) A[2]:(0.594903945923) A[3]:(0.655995965004)\n",
      " state (3)  A[0]:(0.657458126545) A[1]:(-0.033983759582) A[2]:(0.501308202744) A[3]:(0.555303752422)\n",
      " state (4)  A[0]:(0.590004086494) A[1]:(0.655862569809) A[2]:(0.00100243056659) A[3]:(0.531578660011)\n",
      " state (5)  A[0]:(-0.0426903031766) A[1]:(0.999898433685) A[2]:(-0.767763137817) A[3]:(0.625806450844)\n",
      " state (6)  A[0]:(-0.000223025679588) A[1]:(0.809649944305) A[2]:(-0.00089156604372) A[3]:(0.655642151833)\n",
      " state (7)  A[0]:(0.555155873299) A[1]:(-0.557553529739) A[2]:(0.411723732948) A[3]:(0.879720687866)\n",
      " state (8)  A[0]:(0.654645621777) A[1]:(0.00112888170406) A[2]:(0.729042649269) A[3]:(0.589417934418)\n",
      " state (9)  A[0]:(0.655360102654) A[1]:(0.810189962387) A[2]:(0.80989998579) A[3]:(-0.000870287185535)\n",
      " state (10)  A[0]:(0.728784263134) A[1]:(0.900074005127) A[2]:(-0.000513672770467) A[3]:(0.729084968567)\n",
      " state (11)  A[0]:(0.125404059887) A[1]:(0.880503892899) A[2]:(-0.916943311691) A[3]:(0.799857020378)\n",
      " state (12)  A[0]:(-0.431540697813) A[1]:(0.812832176685) A[2]:(-0.935183942318) A[3]:(0.714439630508)\n",
      " state (13)  A[0]:(-0.00237005506642) A[1]:(0.809701442719) A[2]:(0.899660468102) A[3]:(0.729694366455)\n",
      " state (14)  A[0]:(0.809265136719) A[1]:(0.900133311749) A[2]:(0.999999403954) A[3]:(0.810140967369)\n",
      " state (15)  A[0]:(0.979486048222) A[1]:(0.941654026508) A[2]:(1.0) A[3]:(0.878610193729)\n",
      "Episode 425000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6071. Times reached goal: 960.               Steps done: 3201895. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0366165660867.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531187534332) A[1]:(0.59070622921) A[2]:(0.590789675713) A[3]:(0.53068125248)\n",
      " state (1)  A[0]:(0.531227767467) A[1]:(1.18166208267e-05) A[2]:(0.656507730484) A[3]:(0.589607357979)\n",
      " state (2)  A[0]:(0.589918732643) A[1]:(0.728906035423) A[2]:(0.595743060112) A[3]:(0.655794382095)\n",
      " state (3)  A[0]:(0.657428860664) A[1]:(-0.0378100387752) A[2]:(0.503576874733) A[3]:(0.554364323616)\n",
      " state (4)  A[0]:(0.589991807938) A[1]:(0.655674338341) A[2]:(0.00222789868712) A[3]:(0.530619442463)\n",
      " state (5)  A[0]:(-0.0409576520324) A[1]:(0.999898433685) A[2]:(-0.768759250641) A[3]:(0.625727415085)\n",
      " state (6)  A[0]:(-0.000484496325953) A[1]:(0.809604287148) A[2]:(0.000218868255615) A[3]:(0.654378771782)\n",
      " state (7)  A[0]:(0.554394125938) A[1]:(-0.557858288288) A[2]:(0.412940710783) A[3]:(0.879066407681)\n",
      " state (8)  A[0]:(0.654880523682) A[1]:(-0.00136910297442) A[2]:(0.728743910789) A[3]:(0.589238882065)\n",
      " state (9)  A[0]:(0.654997467995) A[1]:(0.80939912796) A[2]:(0.809644937515) A[3]:(-0.00424027862027)\n",
      " state (10)  A[0]:(0.728599846363) A[1]:(0.899707317352) A[2]:(-0.00113666011021) A[3]:(0.727332472801)\n",
      " state (11)  A[0]:(0.126244992018) A[1]:(0.880248427391) A[2]:(-0.917021155357) A[3]:(0.799059510231)\n",
      " state (12)  A[0]:(-0.430090993643) A[1]:(0.812753081322) A[2]:(-0.935156404972) A[3]:(0.713721573353)\n",
      " state (13)  A[0]:(0.000122964382172) A[1]:(0.809931457043) A[2]:(0.900187909603) A[3]:(0.729012787342)\n",
      " state (14)  A[0]:(0.810305833817) A[1]:(0.900413274765) A[2]:(0.999999463558) A[3]:(0.80954927206)\n",
      " state (15)  A[0]:(0.979608714581) A[1]:(0.94186347723) A[2]:(1.0) A[3]:(0.878072917461)\n",
      "Episode 426000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6087. Times reached goal: 963.               Steps done: 3207982. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0363943580253.\n",
      " state (0)  A[0]:(0.531259000301) A[1]:(0.590742707253) A[2]:(0.590337395668) A[3]:(0.530102312565)\n",
      " state (1)  A[0]:(0.531103134155) A[1]:(-0.000190883874893) A[2]:(0.656071186066) A[3]:(0.589509010315)\n",
      " state (2)  A[0]:(0.589756965637) A[1]:(0.729137063026) A[2]:(0.594185590744) A[3]:(0.65601670742)\n",
      " state (3)  A[0]:(0.657151460648) A[1]:(-0.038802832365) A[2]:(0.502929449081) A[3]:(0.554541826248)\n",
      " state (4)  A[0]:(0.58962136507) A[1]:(0.655888497829) A[2]:(0.00082707387628) A[3]:(0.531002640724)\n",
      " state (5)  A[0]:(-0.039908464998) A[1]:(0.999898731709) A[2]:(-0.77030724287) A[3]:(0.626929938793)\n",
      " state (6)  A[0]:(7.83056020737e-05) A[1]:(0.810118317604) A[2]:(-0.00106382335071) A[3]:(0.655355930328)\n",
      " state (7)  A[0]:(0.554594814777) A[1]:(-0.556753754616) A[2]:(0.412841320038) A[3]:(0.879421532154)\n",
      " state (8)  A[0]:(0.655599117279) A[1]:(0.000583976448979) A[2]:(0.72904664278) A[3]:(0.591248214245)\n",
      " state (9)  A[0]:(0.656313478947) A[1]:(0.810190081596) A[2]:(0.81006103754) A[3]:(0.000845312897582)\n",
      " state (10)  A[0]:(0.729257583618) A[1]:(0.900048494339) A[2]:(-0.000198125839233) A[3]:(0.729375004768)\n",
      " state (11)  A[0]:(0.126404926181) A[1]:(0.880408704281) A[2]:(-0.916975319386) A[3]:(0.800039470196)\n",
      " state (12)  A[0]:(-0.430225908756) A[1]:(0.812527179718) A[2]:(-0.935203909874) A[3]:(0.714560508728)\n",
      " state (13)  A[0]:(-3.46750020981e-05) A[1]:(0.809202373028) A[2]:(0.900066018105) A[3]:(0.729773640633)\n",
      " state (14)  A[0]:(0.810142755508) A[1]:(0.899803578854) A[2]:(0.999999463558) A[3]:(0.810252189636)\n",
      " state (15)  A[0]:(0.979567945004) A[1]:(0.941429972649) A[2]:(1.0) A[3]:(0.878601074219)\n",
      "Episode 427000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6109. Times reached goal: 964.               Steps done: 3214091. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0361727026279.\n",
      " state (0)  A[0]:(0.531713962555) A[1]:(0.590380191803) A[2]:(0.590441703796) A[3]:(0.531186461449)\n",
      " state (1)  A[0]:(0.531669437885) A[1]:(-0.000427350372775) A[2]:(0.656017839909) A[3]:(0.5898655653)\n",
      " state (2)  A[0]:(0.590095281601) A[1]:(0.729130327702) A[2]:(0.594214081764) A[3]:(0.656057536602)\n",
      " state (3)  A[0]:(0.657521724701) A[1]:(-0.0411517396569) A[2]:(0.503953695297) A[3]:(0.554532051086)\n",
      " state (4)  A[0]:(0.589932203293) A[1]:(0.656131029129) A[2]:(0.000704169156961) A[3]:(0.531346201897)\n",
      " state (5)  A[0]:(-0.0385496504605) A[1]:(0.999898791313) A[2]:(-0.77154314518) A[3]:(0.627779364586)\n",
      " state (6)  A[0]:(0.000304013490677) A[1]:(0.810007929802) A[2]:(-0.00124716688879) A[3]:(0.655404806137)\n",
      " state (7)  A[0]:(0.554418325424) A[1]:(-0.556264936924) A[2]:(0.41292449832) A[3]:(0.879290223122)\n",
      " state (8)  A[0]:(0.655742049217) A[1]:(0.000741377356462) A[2]:(0.728912293911) A[3]:(0.591001987457)\n",
      " state (9)  A[0]:(0.657158493996) A[1]:(0.810103178024) A[2]:(0.810069322586) A[3]:(0.00136965431739)\n",
      " state (10)  A[0]:(0.730690598488) A[1]:(0.900023341179) A[2]:(0.000877141719684) A[3]:(0.730241954327)\n",
      " state (11)  A[0]:(0.130070745945) A[1]:(0.880492687225) A[2]:(-0.916760444641) A[3]:(0.801031053066)\n",
      " state (12)  A[0]:(-0.428198456764) A[1]:(0.812766432762) A[2]:(-0.935210227966) A[3]:(0.715582609177)\n",
      " state (13)  A[0]:(0.00111949397251) A[1]:(0.809475362301) A[2]:(0.89987808466) A[3]:(0.730289936066)\n",
      " state (14)  A[0]:(0.810329973698) A[1]:(0.899988949299) A[2]:(0.999999463558) A[3]:(0.810422956944)\n",
      " state (15)  A[0]:(0.979578316212) A[1]:(0.941572904587) A[2]:(1.0) A[3]:(0.878582715988)\n",
      "Episode 428000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6110. Times reached goal: 962.               Steps done: 3220201. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0359523612432.\n",
      " state (0)  A[0]:(0.531517982483) A[1]:(0.590608716011) A[2]:(0.590297162533) A[3]:(0.531590223312)\n",
      " state (1)  A[0]:(0.531358718872) A[1]:(-0.000503331364598) A[2]:(0.655836582184) A[3]:(0.590068817139)\n",
      " state (2)  A[0]:(0.589726984501) A[1]:(0.728858113289) A[2]:(0.5941157341) A[3]:(0.655940532684)\n",
      " state (3)  A[0]:(0.657262802124) A[1]:(-0.0434106886387) A[2]:(0.505173683167) A[3]:(0.554321169853)\n",
      " state (4)  A[0]:(0.589600920677) A[1]:(0.655994176865) A[2]:(0.00156295171473) A[3]:(0.531363010406)\n",
      " state (5)  A[0]:(-0.0381988435984) A[1]:(0.999898850918) A[2]:(-0.772346735001) A[3]:(0.628291130066)\n",
      " state (6)  A[0]:(-0.000401928991778) A[1]:(0.810089707375) A[2]:(-0.000610709132161) A[3]:(0.655357241631)\n",
      " state (7)  A[0]:(0.553499042988) A[1]:(-0.556069493294) A[2]:(0.413908392191) A[3]:(0.879122495651)\n",
      " state (8)  A[0]:(0.654991269112) A[1]:(0.000427931518061) A[2]:(0.729185581207) A[3]:(0.590554594994)\n",
      " state (9)  A[0]:(0.65576928854) A[1]:(0.81003254652) A[2]:(0.810112476349) A[3]:(-0.000779211346526)\n",
      " state (10)  A[0]:(0.728792905807) A[1]:(0.90001052618) A[2]:(-6.61611557007e-05) A[3]:(0.728750407696)\n",
      " state (11)  A[0]:(0.12553037703) A[1]:(0.880570054054) A[2]:(-0.917019188404) A[3]:(0.799874186516)\n",
      " state (12)  A[0]:(-0.430939972401) A[1]:(0.813024461269) A[2]:(-0.935348510742) A[3]:(0.714371800423)\n",
      " state (13)  A[0]:(-0.000744894030504) A[1]:(0.809855759144) A[2]:(0.899967610836) A[3]:(0.729423880577)\n",
      " state (14)  A[0]:(0.80995798111) A[1]:(0.90029156208) A[2]:(0.999999463558) A[3]:(0.809826254845)\n",
      " state (15)  A[0]:(0.979539811611) A[1]:(0.941809177399) A[2]:(1.0) A[3]:(0.878102958202)\n",
      "Episode 429000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6117. Times reached goal: 966.               Steps done: 3226318. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0357331119073.\n",
      "q_values \n",
      "tensor([[ 0.5310,  0.5901,  0.5902,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5309, -0.0004,  0.6555,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5894,  0.7286,  0.5934,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0006,  0.8105, -0.0004,  0.6556]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7288,  0.9001,  0.0002,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.8999,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53088504076) A[1]:(0.590249061584) A[2]:(0.590277194977) A[3]:(0.5313372612)\n",
      " state (1)  A[0]:(0.530757427216) A[1]:(-0.000400230259402) A[2]:(0.655534982681) A[3]:(0.590133607388)\n",
      " state (2)  A[0]:(0.5892765522) A[1]:(0.728444218636) A[2]:(0.593264102936) A[3]:(0.655797123909)\n",
      " state (3)  A[0]:(0.656781673431) A[1]:(-0.0462910719216) A[2]:(0.504919528961) A[3]:(0.553790092468)\n",
      " state (4)  A[0]:(0.589023470879) A[1]:(0.655188500881) A[2]:(0.00025463104248) A[3]:(0.531139314175)\n",
      " state (5)  A[0]:(-0.0377416238189) A[1]:(0.999898672104) A[2]:(-0.773649215698) A[3]:(0.629317641258)\n",
      " state (6)  A[0]:(-0.000618696154561) A[1]:(0.809586584568) A[2]:(-0.00124239863362) A[3]:(0.655366182327)\n",
      " state (7)  A[0]:(0.553005635738) A[1]:(-0.557098209858) A[2]:(0.413711190224) A[3]:(0.878854393959)\n",
      " state (8)  A[0]:(0.654442191124) A[1]:(-0.00104562903289) A[2]:(0.728801846504) A[3]:(0.589764118195)\n",
      " state (9)  A[0]:(0.655381321907) A[1]:(0.809554696083) A[2]:(0.809674739838) A[3]:(-0.000950604386162)\n",
      " state (10)  A[0]:(0.728712677956) A[1]:(0.899652302265) A[2]:(-0.00126373698004) A[3]:(0.728895843029)\n",
      " state (11)  A[0]:(0.125864639878) A[1]:(0.880076706409) A[2]:(-0.917207300663) A[3]:(0.800117373466)\n",
      " state (12)  A[0]:(-0.430363446474) A[1]:(0.812185406685) A[2]:(-0.935525476933) A[3]:(0.714720606804)\n",
      " state (13)  A[0]:(0.000367149681551) A[1]:(0.808944523335) A[2]:(0.89981508255) A[3]:(0.729812145233)\n",
      " state (14)  A[0]:(0.81043869257) A[1]:(0.899826109409) A[2]:(0.999999463558) A[3]:(0.810221076012)\n",
      " state (15)  A[0]:(0.979590415955) A[1]:(0.941569983959) A[2]:(1.0) A[3]:(0.87837177515)\n",
      "Episode 430000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6073. Times reached goal: 955.               Steps done: 3232391. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0355167623291.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531659245491) A[1]:(0.590831398964) A[2]:(0.590946435928) A[3]:(0.531079649925)\n",
      " state (1)  A[0]:(0.531465888023) A[1]:(-3.1054019928e-05) A[2]:(0.656542658806) A[3]:(0.590390920639)\n",
      " state (2)  A[0]:(0.590478301048) A[1]:(0.729457736015) A[2]:(0.593738913536) A[3]:(0.656044900417)\n",
      " state (3)  A[0]:(0.657804250717) A[1]:(-0.0451974868774) A[2]:(0.505949378014) A[3]:(0.553604066372)\n",
      " state (4)  A[0]:(0.589935541153) A[1]:(0.656737446785) A[2]:(0.000669479253702) A[3]:(0.530773699284)\n",
      " state (5)  A[0]:(-0.0364989116788) A[1]:(0.999898970127) A[2]:(-0.77399533987) A[3]:(0.629457354546)\n",
      " state (6)  A[0]:(-0.000529631914105) A[1]:(0.809738755226) A[2]:(6.78300857544e-05) A[3]:(0.655131340027)\n",
      " state (7)  A[0]:(0.552313804626) A[1]:(-0.555745542049) A[2]:(0.414275318384) A[3]:(0.878477334976)\n",
      " state (8)  A[0]:(0.653571724892) A[1]:(-0.000451147527201) A[2]:(0.728726744652) A[3]:(0.588214159012)\n",
      " state (9)  A[0]:(0.653925001621) A[1]:(0.809647321701) A[2]:(0.809610784054) A[3]:(-0.00483306450769)\n",
      " state (10)  A[0]:(0.727106273174) A[1]:(0.900014102459) A[2]:(-0.00184428482316) A[3]:(0.727093696594)\n",
      " state (11)  A[0]:(0.121663026512) A[1]:(0.881013691425) A[2]:(-0.917431592941) A[3]:(0.799058973789)\n",
      " state (12)  A[0]:(-0.434832662344) A[1]:(0.814169287682) A[2]:(-0.935800552368) A[3]:(0.71343767643)\n",
      " state (13)  A[0]:(-0.00621014321223) A[1]:(0.811179041862) A[2]:(0.899563848972) A[3]:(0.72859442234)\n",
      " state (14)  A[0]:(0.807980000973) A[1]:(0.901006817818) A[2]:(0.999999463558) A[3]:(0.809339463711)\n",
      " state (15)  A[0]:(0.979316949844) A[1]:(0.942169070244) A[2]:(1.0) A[3]:(0.877786397934)\n",
      "Episode 431000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6076. Times reached goal: 963.               Steps done: 3238467. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0353016167553.\n",
      " state (0)  A[0]:(0.531789362431) A[1]:(0.590790271759) A[2]:(0.590368390083) A[3]:(0.531351447105)\n",
      " state (1)  A[0]:(0.531440734863) A[1]:(-0.000291347503662) A[2]:(0.6559060812) A[3]:(0.590154528618)\n",
      " state (2)  A[0]:(0.590067028999) A[1]:(0.729082107544) A[2]:(0.593131244183) A[3]:(0.656197965145)\n",
      " state (3)  A[0]:(0.657639741898) A[1]:(-0.0465057343245) A[2]:(0.506277143955) A[3]:(0.553946137428)\n",
      " state (4)  A[0]:(0.590221524239) A[1]:(0.656301319599) A[2]:(0.000818133179564) A[3]:(0.531351566315)\n",
      " state (5)  A[0]:(-0.0344848781824) A[1]:(0.999899208546) A[2]:(-0.774964332581) A[3]:(0.630222678185)\n",
      " state (6)  A[0]:(0.000746458652429) A[1]:(0.810190439224) A[2]:(-0.000345468521118) A[3]:(0.655584216118)\n",
      " state (7)  A[0]:(0.553660273552) A[1]:(-0.555527329445) A[2]:(0.415006488562) A[3]:(0.878827393055)\n",
      " state (8)  A[0]:(0.655651807785) A[1]:(0.000407218904002) A[2]:(0.72923541069) A[3]:(0.590716600418)\n",
      " state (9)  A[0]:(0.65669631958) A[1]:(0.810189604759) A[2]:(0.810198664665) A[3]:(0.000157296657562)\n",
      " state (10)  A[0]:(0.729830145836) A[1]:(0.900070786476) A[2]:(0.000746250036173) A[3]:(0.729351639748)\n",
      " state (11)  A[0]:(0.127619042993) A[1]:(0.880663275719) A[2]:(-0.916965007782) A[3]:(0.800408840179)\n",
      " state (12)  A[0]:(-0.429945021868) A[1]:(0.813123583794) A[2]:(-0.935469508171) A[3]:(0.714688658714)\n",
      " state (13)  A[0]:(-0.000113621354103) A[1]:(0.809878826141) A[2]:(0.900163114071) A[3]:(0.729494214058)\n",
      " state (14)  A[0]:(0.810031175613) A[1]:(0.900364637375) A[2]:(0.999999463558) A[3]:(0.809960186481)\n",
      " state (15)  A[0]:(0.979520797729) A[1]:(0.94189208746) A[2]:(1.0) A[3]:(0.878147482872)\n",
      "Episode 432000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6066. Times reached goal: 962.               Steps done: 3244533. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0350881253222.\n",
      " state (0)  A[0]:(0.532306730747) A[1]:(0.590753793716) A[2]:(0.590395092964) A[3]:(0.532157123089)\n",
      " state (1)  A[0]:(0.532228410244) A[1]:(-4.82946634293e-05) A[2]:(0.656427264214) A[3]:(0.59057199955)\n",
      " state (2)  A[0]:(0.590271949768) A[1]:(0.729127645493) A[2]:(0.593811631203) A[3]:(0.656617164612)\n",
      " state (3)  A[0]:(0.657657802105) A[1]:(-0.0485076345503) A[2]:(0.50726723671) A[3]:(0.554413080215)\n",
      " state (4)  A[0]:(0.590022325516) A[1]:(0.656377136707) A[2]:(0.000371813745005) A[3]:(0.532192468643)\n",
      " state (5)  A[0]:(-0.0338824875653) A[1]:(0.999899208546) A[2]:(-0.776132464409) A[3]:(0.631552517414)\n",
      " state (6)  A[0]:(0.000414818496211) A[1]:(0.809979319572) A[2]:(-0.000420212716563) A[3]:(0.656449079514)\n",
      " state (7)  A[0]:(0.553226351738) A[1]:(-0.55608856678) A[2]:(0.415645033121) A[3]:(0.879099547863)\n",
      " state (8)  A[0]:(0.655520796776) A[1]:(-0.000171795487404) A[2]:(0.729304671288) A[3]:(0.591158509254)\n",
      " state (9)  A[0]:(0.656614661217) A[1]:(0.810105204582) A[2]:(0.809992134571) A[3]:(0.000109761953354)\n",
      " state (10)  A[0]:(0.729399621487) A[1]:(0.899982094765) A[2]:(-0.000495552958455) A[3]:(0.729161143303)\n",
      " state (11)  A[0]:(0.125971615314) A[1]:(0.880534946918) A[2]:(-0.917244493961) A[3]:(0.800257146358)\n",
      " state (12)  A[0]:(-0.431188374758) A[1]:(0.812916100025) A[2]:(-0.935690760612) A[3]:(0.714628398418)\n",
      " state (13)  A[0]:(-0.000930279202294) A[1]:(0.809670388699) A[2]:(0.900036215782) A[3]:(0.729593992233)\n",
      " state (14)  A[0]:(0.810000479221) A[1]:(0.900290429592) A[2]:(0.999999463558) A[3]:(0.810105919838)\n",
      " state (15)  A[0]:(0.979532539845) A[1]:(0.941872596741) A[2]:(1.0) A[3]:(0.87822508812)\n",
      "Episode 433000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6059. Times reached goal: 958.               Steps done: 3250592. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0348761691406.\n",
      " state (0)  A[0]:(0.531019985676) A[1]:(0.590345919132) A[2]:(0.590362668037) A[3]:(0.530995249748)\n",
      " state (1)  A[0]:(0.531076073647) A[1]:(-0.000429600448115) A[2]:(0.656038224697) A[3]:(0.59047305584)\n",
      " state (2)  A[0]:(0.589738965034) A[1]:(0.728973448277) A[2]:(0.592948079109) A[3]:(0.656497657299)\n",
      " state (3)  A[0]:(0.657254219055) A[1]:(-0.0480633787811) A[2]:(0.507181167603) A[3]:(0.554176867008)\n",
      " state (4)  A[0]:(0.589705705643) A[1]:(0.655921578407) A[2]:(0.00100791419391) A[3]:(0.5315361619)\n",
      " state (5)  A[0]:(-0.034633487463) A[1]:(0.999899208546) A[2]:(-0.776136636734) A[3]:(0.630536794662)\n",
      " state (6)  A[0]:(-0.000599890889134) A[1]:(0.80992937088) A[2]:(0.000266075134277) A[3]:(0.655375540257)\n",
      " state (7)  A[0]:(0.552398920059) A[1]:(-0.555150091648) A[2]:(0.415549874306) A[3]:(0.878565967083)\n",
      " state (8)  A[0]:(0.654811382294) A[1]:(-0.000473946303828) A[2]:(0.729053139687) A[3]:(0.590666770935)\n",
      " state (9)  A[0]:(0.655419588089) A[1]:(0.809802591801) A[2]:(0.81004101038) A[3]:(-0.000782877032179)\n",
      " state (10)  A[0]:(0.728451192379) A[1]:(0.900003015995) A[2]:(3.62396240234e-05) A[3]:(0.728500902653)\n",
      " state (11)  A[0]:(0.124554105103) A[1]:(0.880788326263) A[2]:(-0.917193353176) A[3]:(0.799727916718)\n",
      " state (12)  A[0]:(-0.432089865208) A[1]:(0.813402593136) A[2]:(-0.93573397398) A[3]:(0.713853120804)\n",
      " state (13)  A[0]:(-0.00233199773356) A[1]:(0.809977948666) A[2]:(0.899972200394) A[3]:(0.728879332542)\n",
      " state (14)  A[0]:(0.809219956398) A[1]:(0.900293469429) A[2]:(0.999999463558) A[3]:(0.809656500816)\n",
      " state (15)  A[0]:(0.979414582253) A[1]:(0.941788077354) A[2]:(1.0) A[3]:(0.877959489822)\n",
      "Episode 434000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6066. Times reached goal: 959.               Steps done: 3256658. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0346652506611.\n",
      "q_values \n",
      "tensor([[ 0.5325,  0.5909,  0.5910,  0.5319]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5322,  0.0002,  0.6563,  0.5906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7292,  0.5926,  0.6569]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0007,  0.8096,  0.0001,  0.6549]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7287,  0.9000,  0.0012,  0.7285]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8094,  0.9001,  1.0000,  0.8096]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532713651657) A[1]:(0.59075152874) A[2]:(0.590586185455) A[3]:(0.532122850418)\n",
      " state (1)  A[0]:(0.53243458271) A[1]:(-0.000303938984871) A[2]:(0.656229972839) A[3]:(0.590679049492)\n",
      " state (2)  A[0]:(0.590699076653) A[1]:(0.729005813599) A[2]:(0.592682123184) A[3]:(0.656962394714)\n",
      " state (3)  A[0]:(0.658113360405) A[1]:(-0.0479301586747) A[2]:(0.507076084614) A[3]:(0.554900109768)\n",
      " state (4)  A[0]:(0.590683221817) A[1]:(0.656199574471) A[2]:(0.000283598899841) A[3]:(0.531981229782)\n",
      " state (5)  A[0]:(-0.0333186089993) A[1]:(0.999899446964) A[2]:(-0.77699893713) A[3]:(0.630014777184)\n",
      " state (6)  A[0]:(6.2882900238e-05) A[1]:(0.810183048248) A[2]:(-0.000328421592712) A[3]:(0.655166268349)\n",
      " state (7)  A[0]:(0.552913427353) A[1]:(-0.555289506912) A[2]:(0.415527284145) A[3]:(0.87858736515)\n",
      " state (8)  A[0]:(0.655431389809) A[1]:(-0.00113315833732) A[2]:(0.729094147682) A[3]:(0.591173708439)\n",
      " state (9)  A[0]:(0.656063079834) A[1]:(0.809661090374) A[2]:(0.810420095921) A[3]:(-0.000419855088694)\n",
      " state (10)  A[0]:(0.729547560215) A[1]:(0.899862170219) A[2]:(0.00215398939326) A[3]:(0.728994369507)\n",
      " state (11)  A[0]:(0.127459943295) A[1]:(0.880536198616) A[2]:(-0.916854858398) A[3]:(0.800299823284)\n",
      " state (12)  A[0]:(-0.43017321825) A[1]:(0.812908411026) A[2]:(-0.935623705387) A[3]:(0.71443092823)\n",
      " state (13)  A[0]:(-0.000650822999887) A[1]:(0.809439778328) A[2]:(0.900056421757) A[3]:(0.729168772697)\n",
      " state (14)  A[0]:(0.809645414352) A[1]:(0.900083959103) A[2]:(0.999999463558) A[3]:(0.80974149704)\n",
      " state (15)  A[0]:(0.97944355011) A[1]:(0.941744804382) A[2]:(1.0) A[3]:(0.877929210663)\n",
      "Episode 435000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6117. Times reached goal: 970.               Steps done: 3262775. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0344538505492.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531446456909) A[1]:(0.590704202652) A[2]:(0.590574979782) A[3]:(0.531042456627)\n",
      " state (1)  A[0]:(0.531466662884) A[1]:(0.000344961881638) A[2]:(0.656097054482) A[3]:(0.589978575706)\n",
      " state (2)  A[0]:(0.590210855007) A[1]:(0.729052901268) A[2]:(0.592297315598) A[3]:(0.656386375427)\n",
      " state (3)  A[0]:(0.657646775246) A[1]:(-0.047490876168) A[2]:(0.506848692894) A[3]:(0.554486870766)\n",
      " state (4)  A[0]:(0.590208232403) A[1]:(0.656168699265) A[2]:(-0.00023627281189) A[3]:(0.531303822994)\n",
      " state (5)  A[0]:(-0.0336108505726) A[1]:(0.999899446964) A[2]:(-0.777472317219) A[3]:(0.628742694855)\n",
      " state (6)  A[0]:(0.000771984283347) A[1]:(0.809952795506) A[2]:(-0.000245571136475) A[3]:(0.655202150345)\n",
      " state (7)  A[0]:(0.55381667614) A[1]:(-0.554814696312) A[2]:(0.415345013142) A[3]:(0.878758788109)\n",
      " state (8)  A[0]:(0.655867159367) A[1]:(0.000510424317326) A[2]:(0.728934049606) A[3]:(0.590692520142)\n",
      " state (9)  A[0]:(0.656501352787) A[1]:(0.810244262218) A[2]:(0.809905529022) A[3]:(-0.000330328941345)\n",
      " state (10)  A[0]:(0.729170441628) A[1]:(0.900091290474) A[2]:(-0.00072824943345) A[3]:(0.728676319122)\n",
      " state (11)  A[0]:(0.125454947352) A[1]:(0.880703866482) A[2]:(-0.917425513268) A[3]:(0.799694776535)\n",
      " state (12)  A[0]:(-0.431018024683) A[1]:(0.813024163246) A[2]:(-0.935953080654) A[3]:(0.713787138462)\n",
      " state (13)  A[0]:(-9.50396060944e-05) A[1]:(0.809449911118) A[2]:(0.900016963482) A[3]:(0.728988289833)\n",
      " state (14)  A[0]:(0.810162842274) A[1]:(0.900065422058) A[2]:(0.999999463558) A[3]:(0.809883236885)\n",
      " state (15)  A[0]:(0.979518353939) A[1]:(0.94171756506) A[2]:(1.0) A[3]:(0.878119528294)\n",
      "Episode 436000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6084. Times reached goal: 966.               Steps done: 3268859. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0342448696869.\n",
      " state (0)  A[0]:(0.531267166138) A[1]:(0.590464234352) A[2]:(0.590405583382) A[3]:(0.532485187054)\n",
      " state (1)  A[0]:(0.531583070755) A[1]:(3.84896993637e-05) A[2]:(0.656056642532) A[3]:(0.591620087624)\n",
      " state (2)  A[0]:(0.589950323105) A[1]:(0.729125976562) A[2]:(0.592318475246) A[3]:(0.65772330761)\n",
      " state (3)  A[0]:(0.657417535782) A[1]:(-0.0468593873084) A[2]:(0.507150650024) A[3]:(0.556511580944)\n",
      " state (4)  A[0]:(0.590061664581) A[1]:(0.656275033951) A[2]:(-0.000253081321716) A[3]:(0.532937884331)\n",
      " state (5)  A[0]:(-0.0336751677096) A[1]:(0.999899625778) A[2]:(-0.778145670891) A[3]:(0.628463387489)\n",
      " state (6)  A[0]:(0.000426515907748) A[1]:(0.810194611549) A[2]:(-0.00074696529191) A[3]:(0.655504465103)\n",
      " state (7)  A[0]:(0.553517878056) A[1]:(-0.554278314114) A[2]:(0.414929121733) A[3]:(0.878886461258)\n",
      " state (8)  A[0]:(0.655543267727) A[1]:(0.000574931444135) A[2]:(0.728774666786) A[3]:(0.590559303761)\n",
      " state (9)  A[0]:(0.656312942505) A[1]:(0.810180783272) A[2]:(0.810085535049) A[3]:(-0.00115027977154)\n",
      " state (10)  A[0]:(0.729599118233) A[1]:(0.900127708912) A[2]:(0.000795841042418) A[3]:(0.728532373905)\n",
      " state (11)  A[0]:(0.127381771803) A[1]:(0.880924403667) A[2]:(-0.91712552309) A[3]:(0.799839556217)\n",
      " state (12)  A[0]:(-0.429464191198) A[1]:(0.813581883907) A[2]:(-0.935786128044) A[3]:(0.713917255402)\n",
      " state (13)  A[0]:(0.00141598191112) A[1]:(0.810169696808) A[2]:(0.900331795216) A[3]:(0.728883981705)\n",
      " state (14)  A[0]:(0.810560703278) A[1]:(0.900555610657) A[2]:(0.999999463558) A[3]:(0.809642076492)\n",
      " state (15)  A[0]:(0.97955340147) A[1]:(0.942061722279) A[2]:(1.0) A[3]:(0.87784922123)\n",
      "Episode 437000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6082. Times reached goal: 958.               Steps done: 3274941. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0340372244786.\n",
      " state (0)  A[0]:(0.531157970428) A[1]:(0.590499520302) A[2]:(0.590200066566) A[3]:(0.531342387199)\n",
      " state (1)  A[0]:(0.531246185303) A[1]:(0.000248864293098) A[2]:(0.655988574028) A[3]:(0.590319633484)\n",
      " state (2)  A[0]:(0.5901222229) A[1]:(0.728945195675) A[2]:(0.592524528503) A[3]:(0.656344413757)\n",
      " state (3)  A[0]:(0.65746486187) A[1]:(-0.0466706454754) A[2]:(0.507920980453) A[3]:(0.555255889893)\n",
      " state (4)  A[0]:(0.589915931225) A[1]:(0.656213343143) A[2]:(0.000799417321105) A[3]:(0.531632184982)\n",
      " state (5)  A[0]:(-0.0343172289431) A[1]:(0.999899625778) A[2]:(-0.778105199337) A[3]:(0.627246975899)\n",
      " state (6)  A[0]:(0.000126466155052) A[1]:(0.809878647327) A[2]:(0.00031590461731) A[3]:(0.655995130539)\n",
      " state (7)  A[0]:(0.553377866745) A[1]:(-0.555106878281) A[2]:(0.415771454573) A[3]:(0.879285693169)\n",
      " state (8)  A[0]:(0.655286312103) A[1]:(-0.000599428953137) A[2]:(0.729101121426) A[3]:(0.591501712799)\n",
      " state (9)  A[0]:(0.655938982964) A[1]:(0.809785842896) A[2]:(0.810103952885) A[3]:(0.000786691729445)\n",
      " state (10)  A[0]:(0.728684544563) A[1]:(0.899855911732) A[2]:(0.000231385231018) A[3]:(0.729145348072)\n",
      " state (11)  A[0]:(0.124254986644) A[1]:(0.880528271198) A[2]:(-0.917311429977) A[3]:(0.800050318241)\n",
      " state (12)  A[0]:(-0.432057380676) A[1]:(0.812817454338) A[2]:(-0.935982048512) A[3]:(0.714203238487)\n",
      " state (13)  A[0]:(-0.00131286610849) A[1]:(0.809165000916) A[2]:(0.900058448315) A[3]:(0.72933614254)\n",
      " state (14)  A[0]:(0.809713900089) A[1]:(0.89992171526) A[2]:(0.999999463558) A[3]:(0.810101985931)\n",
      " state (15)  A[0]:(0.979457616806) A[1]:(0.941668748856) A[2]:(1.0) A[3]:(0.878189325333)\n",
      "Episode 438000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6029. Times reached goal: 963.               Steps done: 3280970. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0338326314177.\n",
      " state (0)  A[0]:(0.531071484089) A[1]:(0.590490758419) A[2]:(0.590397596359) A[3]:(0.53143465519)\n",
      " state (1)  A[0]:(0.530868053436) A[1]:(-0.000280722975731) A[2]:(0.65613424778) A[3]:(0.590228915215)\n",
      " state (2)  A[0]:(0.589379549026) A[1]:(0.728959441185) A[2]:(0.591821849346) A[3]:(0.656553566456)\n",
      " state (3)  A[0]:(0.656846761703) A[1]:(-0.0452346503735) A[2]:(0.507047176361) A[3]:(0.556037902832)\n",
      " state (4)  A[0]:(0.58932030201) A[1]:(0.656123399734) A[2]:(-8.64267349243e-05) A[3]:(0.531820416451)\n",
      " state (5)  A[0]:(-0.0358331203461) A[1]:(0.999899744987) A[2]:(-0.778523802757) A[3]:(0.625391840935)\n",
      " state (6)  A[0]:(-0.000931501097512) A[1]:(0.809976935387) A[2]:(-4.39882278442e-05) A[3]:(0.655458688736)\n",
      " state (7)  A[0]:(0.552803516388) A[1]:(-0.555025339127) A[2]:(0.415562480688) A[3]:(0.879153132439)\n",
      " state (8)  A[0]:(0.654236078262) A[1]:(-8.06897878647e-05) A[2]:(0.729158639908) A[3]:(0.589564025402)\n",
      " state (9)  A[0]:(0.655049562454) A[1]:(0.80989420414) A[2]:(0.809963464737) A[3]:(-0.00140127446502)\n",
      " state (10)  A[0]:(0.727947950363) A[1]:(0.899913668633) A[2]:(-0.000773668114562) A[3]:(0.728412389755)\n",
      " state (11)  A[0]:(0.122598320246) A[1]:(0.880664348602) A[2]:(-0.917553842068) A[3]:(0.799557924271)\n",
      " state (12)  A[0]:(-0.43295276165) A[1]:(0.813112139702) A[2]:(-0.936150074005) A[3]:(0.713748455048)\n",
      " state (13)  A[0]:(-0.00171688036062) A[1]:(0.80953091383) A[2]:(0.900177955627) A[3]:(0.729078054428)\n",
      " state (14)  A[0]:(0.809698760509) A[1]:(0.900169610977) A[2]:(0.999999463558) A[3]:(0.809951901436)\n",
      " state (15)  A[0]:(0.979463517666) A[1]:(0.94181984663) A[2]:(1.0) A[3]:(0.878082692623)\n",
      "Episode 439000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6069. Times reached goal: 971.               Steps done: 3287039. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0336279229937.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5899,  0.5902,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.0001,  0.6559,  0.5899]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5897,  0.7286,  0.5913,  0.6559]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  0.8099, -0.0004,  0.6555]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7289,  0.9000, -0.0001,  0.7286]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9004,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531374514103) A[1]:(0.58991163969) A[2]:(0.590258479118) A[3]:(0.531209230423)\n",
      " state (1)  A[0]:(0.531192958355) A[1]:(-9.63062047958e-05) A[2]:(0.655852496624) A[3]:(0.58985799551)\n",
      " state (2)  A[0]:(0.589518785477) A[1]:(0.728567659855) A[2]:(0.59131115675) A[3]:(0.655818343163)\n",
      " state (3)  A[0]:(0.656726837158) A[1]:(-0.043861348182) A[2]:(0.506462216377) A[3]:(0.555620789528)\n",
      " state (4)  A[0]:(0.589330732822) A[1]:(0.655651807785) A[2]:(-1.57356262207e-05) A[3]:(0.531141996384)\n",
      " state (5)  A[0]:(-0.0357297807932) A[1]:(0.999899744987) A[2]:(-0.778371453285) A[3]:(0.623883545399)\n",
      " state (6)  A[0]:(-0.000270456075668) A[1]:(0.809917211533) A[2]:(-0.000200629234314) A[3]:(0.655228257179)\n",
      " state (7)  A[0]:(0.553620755672) A[1]:(-0.554831504822) A[2]:(0.414826601744) A[3]:(0.879235327244)\n",
      " state (8)  A[0]:(0.654714107513) A[1]:(8.13156366348e-05) A[2]:(0.728750824928) A[3]:(0.589688181877)\n",
      " state (9)  A[0]:(0.655306816101) A[1]:(0.810019075871) A[2]:(0.809862494469) A[3]:(-0.00159859517589)\n",
      " state (10)  A[0]:(0.728379368782) A[1]:(0.900011181831) A[2]:(-0.000355482072337) A[3]:(0.728259801865)\n",
      " state (11)  A[0]:(0.123810008168) A[1]:(0.880830705166) A[2]:(-0.917504012585) A[3]:(0.799424648285)\n",
      " state (12)  A[0]:(-0.432034224272) A[1]:(0.81339007616) A[2]:(-0.936248898506) A[3]:(0.713500380516)\n",
      " state (13)  A[0]:(-0.000921964412555) A[1]:(0.809790015221) A[2]:(0.899933278561) A[3]:(0.728829205036)\n",
      " state (14)  A[0]:(0.809805452824) A[1]:(0.900370657444) A[2]:(0.999999463558) A[3]:(0.809789657593)\n",
      " state (15)  A[0]:(0.979459345341) A[1]:(0.942010939121) A[2]:(1.0) A[3]:(0.877970337868)\n",
      "Episode 440000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6086. Times reached goal: 965.               Steps done: 3293125. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0334238849718.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531536698341) A[1]:(0.59005188942) A[2]:(0.590469419956) A[3]:(0.530434966087)\n",
      " state (1)  A[0]:(0.53124320507) A[1]:(0.000218749046326) A[2]:(0.655988514423) A[3]:(0.589738845825)\n",
      " state (2)  A[0]:(0.58968770504) A[1]:(0.728749990463) A[2]:(0.591253101826) A[3]:(0.656292557716)\n",
      " state (3)  A[0]:(0.656789541245) A[1]:(-0.0417217873037) A[2]:(0.506197392941) A[3]:(0.556789159775)\n",
      " state (4)  A[0]:(0.58955591917) A[1]:(0.655588805676) A[2]:(1.4066696167e-05) A[3]:(0.531722903252)\n",
      " state (5)  A[0]:(-0.0358403660357) A[1]:(0.999899744987) A[2]:(-0.778404116631) A[3]:(0.622304975986)\n",
      " state (6)  A[0]:(-0.000477820605738) A[1]:(0.809810161591) A[2]:(0.000146865844727) A[3]:(0.655273914337)\n",
      " state (7)  A[0]:(0.553860425949) A[1]:(-0.554580330849) A[2]:(0.415021389723) A[3]:(0.879507601261)\n",
      " state (8)  A[0]:(0.655122995377) A[1]:(0.000425934762461) A[2]:(0.729011893272) A[3]:(0.589985847473)\n",
      " state (9)  A[0]:(0.656349301338) A[1]:(0.81001085043) A[2]:(0.810075998306) A[3]:(-0.000666797044687)\n",
      " state (10)  A[0]:(0.729635238647) A[1]:(0.899985611439) A[2]:(0.000255107879639) A[3]:(0.728731215)\n",
      " state (11)  A[0]:(0.126937448978) A[1]:(0.880769371986) A[2]:(-0.917490720749) A[3]:(0.799671292305)\n",
      " state (12)  A[0]:(-0.428949147463) A[1]:(0.813185453415) A[2]:(-0.936300933361) A[3]:(0.713799476624)\n",
      " state (13)  A[0]:(0.00328797404654) A[1]:(0.809444069862) A[2]:(0.900092959404) A[3]:(0.72918087244)\n",
      " state (14)  A[0]:(0.811228394508) A[1]:(0.900157928467) A[2]:(0.999999463558) A[3]:(0.810090780258)\n",
      " state (15)  A[0]:(0.979615628719) A[1]:(0.941879868507) A[2]:(1.0) A[3]:(0.878186643124)\n",
      "Episode 441000 finished after 0 timesteps with r=1.0. Running score: 0.92. Times trained:               6077. Times reached goal: 960.               Steps done: 3299202. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0332213839454.\n",
      " state (0)  A[0]:(0.531931996346) A[1]:(0.590811431408) A[2]:(0.590946853161) A[3]:(0.531727850437)\n",
      " state (1)  A[0]:(0.531781673431) A[1]:(3.10391187668e-05) A[2]:(0.656236290932) A[3]:(0.590446591377)\n",
      " state (2)  A[0]:(0.590131521225) A[1]:(0.729126095772) A[2]:(0.591938018799) A[3]:(0.656277894974)\n",
      " state (3)  A[0]:(0.657077312469) A[1]:(-0.0365387350321) A[2]:(0.506804645061) A[3]:(0.557300627232)\n",
      " state (4)  A[0]:(0.590056061745) A[1]:(0.656574368477) A[2]:(0.000473022431834) A[3]:(0.531908154488)\n",
      " state (5)  A[0]:(-0.0358760841191) A[1]:(0.999900043011) A[2]:(-0.779461920261) A[3]:(0.621738195419)\n",
      " state (6)  A[0]:(-0.0001021027565) A[1]:(0.810172855854) A[2]:(-2.05039978027e-05) A[3]:(0.65588581562)\n",
      " state (7)  A[0]:(0.554550766945) A[1]:(-0.554680883884) A[2]:(0.415675610304) A[3]:(0.879950582981)\n",
      " state (8)  A[0]:(0.656075835228) A[1]:(0.0002451390028) A[2]:(0.729329943657) A[3]:(0.591455340385)\n",
      " state (9)  A[0]:(0.65730291605) A[1]:(0.810157239437) A[2]:(0.810283660889) A[3]:(0.00139015819877)\n",
      " state (10)  A[0]:(0.729775607586) A[1]:(0.900025486946) A[2]:(0.0004996060743) A[3]:(0.729401826859)\n",
      " state (11)  A[0]:(0.125310406089) A[1]:(0.880719423294) A[2]:(-0.917579352856) A[3]:(0.799885571003)\n",
      " state (12)  A[0]:(-0.431191504002) A[1]:(0.812984287739) A[2]:(-0.936454832554) A[3]:(0.713955879211)\n",
      " state (13)  A[0]:(0.000180616974831) A[1]:(0.80914825201) A[2]:(0.899940907955) A[3]:(0.72937899828)\n",
      " state (14)  A[0]:(0.810127973557) A[1]:(0.900004804134) A[2]:(0.999999523163) A[3]:(0.810294389725)\n",
      " state (15)  A[0]:(0.979489088058) A[1]:(0.941806137562) A[2]:(1.0) A[3]:(0.878356695175)\n",
      "Episode 442000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6084. Times reached goal: 963.               Steps done: 3305286. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0330198786462.\n",
      " state (0)  A[0]:(0.53185737133) A[1]:(0.590539395809) A[2]:(0.590810537338) A[3]:(0.531747341156)\n",
      " state (1)  A[0]:(0.531743764877) A[1]:(-8.25673341751e-05) A[2]:(0.656280994415) A[3]:(0.590752840042)\n",
      " state (2)  A[0]:(0.590476512909) A[1]:(0.729349374771) A[2]:(0.591956734657) A[3]:(0.656453669071)\n",
      " state (3)  A[0]:(0.657137513161) A[1]:(-0.0315688364208) A[2]:(0.506655693054) A[3]:(0.557693243027)\n",
      " state (4)  A[0]:(0.590393066406) A[1]:(0.656407177448) A[2]:(0.000531792582478) A[3]:(0.531703531742)\n",
      " state (5)  A[0]:(-0.035958442837) A[1]:(0.999899923801) A[2]:(-0.780373036861) A[3]:(0.620842337608)\n",
      " state (6)  A[0]:(0.000431612104876) A[1]:(0.810122728348) A[2]:(-5.90085983276e-05) A[3]:(0.65577352047)\n",
      " state (7)  A[0]:(0.554991960526) A[1]:(-0.554417192936) A[2]:(0.415669381618) A[3]:(0.879840016365)\n",
      " state (8)  A[0]:(0.655586361885) A[1]:(-8.37594270706e-05) A[2]:(0.729229748249) A[3]:(0.590245485306)\n",
      " state (9)  A[0]:(0.656259536743) A[1]:(0.809881389141) A[2]:(0.810083448887) A[3]:(-0.000200629234314)\n",
      " state (10)  A[0]:(0.728808224201) A[1]:(0.899973869324) A[2]:(-0.0006216763868) A[3]:(0.728811740875)\n",
      " state (11)  A[0]:(0.123377710581) A[1]:(0.880855679512) A[2]:(-0.917846143246) A[3]:(0.799528241158)\n",
      " state (12)  A[0]:(-0.432251363993) A[1]:(0.813425004482) A[2]:(-0.936648905277) A[3]:(0.713639199734)\n",
      " state (13)  A[0]:(-0.000487104029162) A[1]:(0.809728145599) A[2]:(0.899967670441) A[3]:(0.729110717773)\n",
      " state (14)  A[0]:(0.809989690781) A[1]:(0.900360405445) A[2]:(0.999999523163) A[3]:(0.809990525246)\n",
      " state (15)  A[0]:(0.979481816292) A[1]:(0.942012071609) A[2]:(1.0) A[3]:(0.878081560135)\n",
      "Episode 443000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6051. Times reached goal: 960.               Steps done: 3311337. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0328206786479.\n",
      " state (0)  A[0]:(0.531347811222) A[1]:(0.59037822485) A[2]:(0.590331435204) A[3]:(0.531087636948)\n",
      " state (1)  A[0]:(0.531322598457) A[1]:(-5.6728720665e-05) A[2]:(0.656100749969) A[3]:(0.590274512768)\n",
      " state (2)  A[0]:(0.590190112591) A[1]:(0.728905558586) A[2]:(0.591408848763) A[3]:(0.65590941906)\n",
      " state (3)  A[0]:(0.656693994999) A[1]:(-0.0288558788598) A[2]:(0.506085038185) A[3]:(0.557343125343)\n",
      " state (4)  A[0]:(0.590105772018) A[1]:(0.656010508537) A[2]:(-3.75509262085e-05) A[3]:(0.531151533127)\n",
      " state (5)  A[0]:(-0.036596160382) A[1]:(0.999899744987) A[2]:(-0.781302928925) A[3]:(0.620231151581)\n",
      " state (6)  A[0]:(6.67572021484e-05) A[1]:(0.809991776943) A[2]:(-7.56978988647e-05) A[3]:(0.655544936657)\n",
      " state (7)  A[0]:(0.554877758026) A[1]:(-0.554000973701) A[2]:(0.415630728006) A[3]:(0.879706025124)\n",
      " state (8)  A[0]:(0.655261933804) A[1]:(0.000145688652992) A[2]:(0.728982746601) A[3]:(0.589925408363)\n",
      " state (9)  A[0]:(0.655735015869) A[1]:(0.809995293617) A[2]:(0.809988737106) A[3]:(-0.000812738959212)\n",
      " state (10)  A[0]:(0.728727579117) A[1]:(0.900003314018) A[2]:(-0.000118970870972) A[3]:(0.728569984436)\n",
      " state (11)  A[0]:(0.123776786029) A[1]:(0.880848109722) A[2]:(-0.917754173279) A[3]:(0.799407064915)\n",
      " state (12)  A[0]:(-0.432127535343) A[1]:(0.813320755959) A[2]:(-0.936669826508) A[3]:(0.713379502296)\n",
      " state (13)  A[0]:(-0.000680178287439) A[1]:(0.809519708157) A[2]:(0.899991214275) A[3]:(0.728854417801)\n",
      " state (14)  A[0]:(0.809851169586) A[1]:(0.900225698948) A[2]:(0.999999523163) A[3]:(0.809912383556)\n",
      " state (15)  A[0]:(0.979459524155) A[1]:(0.941926002502) A[2]:(1.0) A[3]:(0.878110527992)\n",
      "Episode 444000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6065. Times reached goal: 965.               Steps done: 3317402. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.032622223655.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5907,  0.5902,  0.5294]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6560,  0.0002,  0.5308]], device='cuda:0')\n",
      "On state=4, selected action=0 , Random? True\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.6557,  0.0000,  0.5310]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6552,  0.0001,  0.7287,  0.5897]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6547,  0.8100,  0.8098, -0.0025]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0033,  0.8103,  0.9002,  0.7280]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9004,  1.0000,  0.8095]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531596422195) A[1]:(0.590522050858) A[2]:(0.590260028839) A[3]:(0.531808137894)\n",
      " state (1)  A[0]:(0.531710147858) A[1]:(0.000643432023935) A[2]:(0.655806303024) A[3]:(0.591090083122)\n",
      " state (2)  A[0]:(0.590556621552) A[1]:(0.728868722916) A[2]:(0.591210722923) A[3]:(0.656723022461)\n",
      " state (3)  A[0]:(0.656948566437) A[1]:(-0.0251288544387) A[2]:(0.506091237068) A[3]:(0.558685183525)\n",
      " state (4)  A[0]:(0.590605854988) A[1]:(0.655892729759) A[2]:(0.00035989281605) A[3]:(0.531967639923)\n",
      " state (5)  A[0]:(-0.0367801636457) A[1]:(0.999899625778) A[2]:(-0.781999111176) A[3]:(0.619633197784)\n",
      " state (6)  A[0]:(0.000280156731606) A[1]:(0.809858083725) A[2]:(0.000393629044993) A[3]:(0.655304551125)\n",
      " state (7)  A[0]:(0.555382013321) A[1]:(-0.553749322891) A[2]:(0.416118472815) A[3]:(0.879616498947)\n",
      " state (8)  A[0]:(0.656086325645) A[1]:(-0.000897466903552) A[2]:(0.729139447212) A[3]:(0.590261220932)\n",
      " state (9)  A[0]:(0.656794846058) A[1]:(0.80953836441) A[2]:(0.810517430305) A[3]:(-0.00101539457683)\n",
      " state (10)  A[0]:(0.730456709862) A[1]:(0.899843394756) A[2]:(0.00300227687694) A[3]:(0.72876060009)\n",
      " state (11)  A[0]:(0.128541126847) A[1]:(0.880786538124) A[2]:(-0.917247414589) A[3]:(0.799826562405)\n",
      " state (12)  A[0]:(-0.429026961327) A[1]:(0.813304901123) A[2]:(-0.9365234375) A[3]:(0.713685035706)\n",
      " state (13)  A[0]:(0.00168447033502) A[1]:(0.809453964233) A[2]:(0.899941205978) A[3]:(0.728771209717)\n",
      " state (14)  A[0]:(0.810354888439) A[1]:(0.900171160698) A[2]:(0.999999523163) A[3]:(0.809676647186)\n",
      " state (15)  A[0]:(0.979506731033) A[1]:(0.94189530611) A[2]:(1.0) A[3]:(0.877901315689)\n",
      "Episode 445000 finished after 0 timesteps with r=1.0. Running score: 0.94. Times trained:               6079. Times reached goal: 959.               Steps done: 3323481. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0324245147026.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531551122665) A[1]:(0.590324938297) A[2]:(0.590426146984) A[3]:(0.531664967537)\n",
      " state (1)  A[0]:(0.531511306763) A[1]:(7.1719288826e-05) A[2]:(0.656129837036) A[3]:(0.590473413467)\n",
      " state (2)  A[0]:(0.590302109718) A[1]:(0.729006052017) A[2]:(0.591244578362) A[3]:(0.656299173832)\n",
      " state (3)  A[0]:(0.656484484673) A[1]:(-0.0208016876131) A[2]:(0.506001710892) A[3]:(0.558656394482)\n",
      " state (4)  A[0]:(0.590105175972) A[1]:(0.65632379055) A[2]:(0.00035035610199) A[3]:(0.531724095345)\n",
      " state (5)  A[0]:(-0.0382729284465) A[1]:(0.999899685383) A[2]:(-0.782898008823) A[3]:(0.619052290916)\n",
      " state (6)  A[0]:(7.13616609573e-05) A[1]:(0.810075163841) A[2]:(-0.000409722299082) A[3]:(0.656034111977)\n",
      " state (7)  A[0]:(0.55567240715) A[1]:(-0.553497314453) A[2]:(0.415867686272) A[3]:(0.88004219532)\n",
      " state (8)  A[0]:(0.655725896358) A[1]:(0.000744059565477) A[2]:(0.729089200497) A[3]:(0.590201497078)\n",
      " state (9)  A[0]:(0.656200766563) A[1]:(0.810265243053) A[2]:(0.810028195381) A[3]:(-0.000220865011215)\n",
      " state (10)  A[0]:(0.729038119316) A[1]:(0.900087058544) A[2]:(-0.000332236289978) A[3]:(0.728880226612)\n",
      " state (11)  A[0]:(0.124007642269) A[1]:(0.88084423542) A[2]:(-0.917949795723) A[3]:(0.799538731575)\n",
      " state (12)  A[0]:(-0.431819558144) A[1]:(0.813149094582) A[2]:(-0.936902225018) A[3]:(0.713559031487)\n",
      " state (13)  A[0]:(8.97347927094e-05) A[1]:(0.809218943119) A[2]:(0.90005171299) A[3]:(0.729084551334)\n",
      " state (14)  A[0]:(0.8101323843) A[1]:(0.900064468384) A[2]:(0.999999523163) A[3]:(0.8100695014)\n",
      " state (15)  A[0]:(0.979487419128) A[1]:(0.941826939583) A[2]:(1.0) A[3]:(0.878210783005)\n",
      "Episode 446000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6048. Times reached goal: 953.               Steps done: 3329529. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0322290030608.\n",
      " state (0)  A[0]:(0.531731247902) A[1]:(0.590054392815) A[2]:(0.590770423412) A[3]:(0.531551241875)\n",
      " state (1)  A[0]:(0.531706988811) A[1]:(0.00013579428196) A[2]:(0.656109452248) A[3]:(0.590420782566)\n",
      " state (2)  A[0]:(0.590416550636) A[1]:(0.729002952576) A[2]:(0.591383159161) A[3]:(0.656322777271)\n",
      " state (3)  A[0]:(0.656411886215) A[1]:(-0.0173671506345) A[2]:(0.506121337414) A[3]:(0.559150576591)\n",
      " state (4)  A[0]:(0.590197980404) A[1]:(0.656236767769) A[2]:(0.000669002416544) A[3]:(0.53179949522)\n",
      " state (5)  A[0]:(-0.038623675704) A[1]:(0.999899506569) A[2]:(-0.783425569534) A[3]:(0.617921829224)\n",
      " state (6)  A[0]:(0.000371471018298) A[1]:(0.809919655323) A[2]:(-0.00028657913208) A[3]:(0.655743122101)\n",
      " state (7)  A[0]:(0.556123137474) A[1]:(-0.552369356155) A[2]:(0.415219724178) A[3]:(0.880021989346)\n",
      " state (8)  A[0]:(0.655869841576) A[1]:(0.00112642301247) A[2]:(0.728494286537) A[3]:(0.590604364872)\n",
      " state (9)  A[0]:(0.655789375305) A[1]:(0.810162305832) A[2]:(0.8097615242) A[3]:(-0.000263839960098)\n",
      " state (10)  A[0]:(0.72876906395) A[1]:(0.900017082691) A[2]:(-0.00078475457849) A[3]:(0.728622555733)\n",
      " state (11)  A[0]:(0.12425108999) A[1]:(0.880763292313) A[2]:(-0.918030321598) A[3]:(0.799321591854)\n",
      " state (12)  A[0]:(-0.430912554264) A[1]:(0.81297647953) A[2]:(-0.936955809593) A[3]:(0.713275432587)\n",
      " state (13)  A[0]:(0.0016645923024) A[1]:(0.808958411217) A[2]:(0.900256037712) A[3]:(0.728841423988)\n",
      " state (14)  A[0]:(0.810613930225) A[1]:(0.899892091751) A[2]:(0.999999523163) A[3]:(0.809889912605)\n",
      " state (15)  A[0]:(0.979524612427) A[1]:(0.941706001759) A[2]:(1.0) A[3]:(0.878083050251)\n",
      "Episode 447000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6070. Times reached goal: 968.               Steps done: 3335599. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.03203396555.\n",
      " state (0)  A[0]:(0.531315922737) A[1]:(0.589839220047) A[2]:(0.590412735939) A[3]:(0.53129529953)\n",
      " state (1)  A[0]:(0.531311988831) A[1]:(-0.000172525644302) A[2]:(0.65598988533) A[3]:(0.589874744415)\n",
      " state (2)  A[0]:(0.590111374855) A[1]:(0.728967607021) A[2]:(0.591015636921) A[3]:(0.655982077122)\n",
      " state (3)  A[0]:(0.656113982201) A[1]:(-0.0151836229488) A[2]:(0.505810320377) A[3]:(0.559283614159)\n",
      " state (4)  A[0]:(0.590023100376) A[1]:(0.655868649483) A[2]:(0.000126957893372) A[3]:(0.531621515751)\n",
      " state (5)  A[0]:(-0.0392080619931) A[1]:(0.999899506569) A[2]:(-0.784508407116) A[3]:(0.616743326187)\n",
      " state (6)  A[0]:(-2.820789814e-05) A[1]:(0.810050189495) A[2]:(-0.000451087922556) A[3]:(0.655914902687)\n",
      " state (7)  A[0]:(0.555922031403) A[1]:(-0.55317646265) A[2]:(0.416010737419) A[3]:(0.880270838737)\n",
      " state (8)  A[0]:(0.65605288744) A[1]:(-8.52644443512e-05) A[2]:(0.728762388229) A[3]:(0.591597378254)\n",
      " state (9)  A[0]:(0.656317591667) A[1]:(0.810012578964) A[2]:(0.809841990471) A[3]:(0.00161114195362)\n",
      " state (10)  A[0]:(0.729122042656) A[1]:(0.900030612946) A[2]:(-0.000855207210407) A[3]:(0.729545772076)\n",
      " state (11)  A[0]:(0.124109871686) A[1]:(0.880918800831) A[2]:(-0.91818267107) A[3]:(0.799925982952)\n",
      " state (12)  A[0]:(-0.432008415461) A[1]:(0.813409328461) A[2]:(-0.937209665775) A[3]:(0.713819861412)\n",
      " state (13)  A[0]:(-0.000278681516647) A[1]:(0.809539318085) A[2]:(0.899948298931) A[3]:(0.729177474976)\n",
      " state (14)  A[0]:(0.810043692589) A[1]:(0.900261044502) A[2]:(0.999999523163) A[3]:(0.810096204281)\n",
      " state (15)  A[0]:(0.979492068291) A[1]:(0.941911041737) A[2]:(1.0) A[3]:(0.878229558468)\n",
      "Episode 448000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6110. Times reached goal: 972.               Steps done: 3341709. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0318388347521.\n",
      " state (0)  A[0]:(0.531582474709) A[1]:(0.590343952179) A[2]:(0.590828418732) A[3]:(0.531776726246)\n",
      " state (1)  A[0]:(0.531759023666) A[1]:(0.000147700309753) A[2]:(0.656081438065) A[3]:(0.590647816658)\n",
      " state (2)  A[0]:(0.590381741524) A[1]:(0.728986263275) A[2]:(0.591338455677) A[3]:(0.656266212463)\n",
      " state (3)  A[0]:(0.656313061714) A[1]:(-0.0140837654471) A[2]:(0.506376266479) A[3]:(0.559693455696)\n",
      " state (4)  A[0]:(0.589972913265) A[1]:(0.656584382057) A[2]:(0.000244379043579) A[3]:(0.531720280647)\n",
      " state (5)  A[0]:(-0.0402226001024) A[1]:(0.999899625778) A[2]:(-0.785207033157) A[3]:(0.616154432297)\n",
      " state (6)  A[0]:(-0.000246703624725) A[1]:(0.810146212578) A[2]:(-0.000541090907063) A[3]:(0.655696392059)\n",
      " state (7)  A[0]:(0.555863380432) A[1]:(-0.552906692028) A[2]:(0.416378855705) A[3]:(0.880109786987)\n",
      " state (8)  A[0]:(0.65554857254) A[1]:(0.000742792966776) A[2]:(0.729112744331) A[3]:(0.589807510376)\n",
      " state (9)  A[0]:(0.65583640337) A[1]:(0.810244858265) A[2]:(0.810032308102) A[3]:(-0.00109526468441)\n",
      " state (10)  A[0]:(0.7286850214) A[1]:(0.900015473366) A[2]:(-0.000467061967356) A[3]:(0.728527069092)\n",
      " state (11)  A[0]:(0.123126082122) A[1]:(0.88076043129) A[2]:(-0.918193519115) A[3]:(0.799357771873)\n",
      " state (12)  A[0]:(-0.432594418526) A[1]:(0.812990188599) A[2]:(-0.937288761139) A[3]:(0.713362336159)\n",
      " state (13)  A[0]:(-0.000438243121607) A[1]:(0.808988690376) A[2]:(0.899931192398) A[3]:(0.728995978832)\n",
      " state (14)  A[0]:(0.810177445412) A[1]:(0.899961531162) A[2]:(0.999999523163) A[3]:(0.810089588165)\n",
      " state (15)  A[0]:(0.979517459869) A[1]:(0.941745877266) A[2]:(1.0) A[3]:(0.878264725208)\n",
      "Episode 449000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6073. Times reached goal: 973.               Steps done: 3347782. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0316460634512.\n",
      "q_values \n",
      "tensor([[ 0.5324,  0.5905,  0.5909,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315, -0.0007,  0.6559,  0.5907]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5897,  0.7286,  0.5905,  0.6565]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0027,  0.8104,  0.0003,  0.6555]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7253,  0.8997, -0.0035,  0.7249]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8114,  0.8999,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=0 , Random? True\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0005,  0.8092,  0.9011,  0.7288]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8110,  0.9002,  1.0000,  0.8106]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532030701637) A[1]:(0.590641736984) A[2]:(0.590859770775) A[3]:(0.532143652439)\n",
      " state (1)  A[0]:(0.531107246876) A[1]:(-0.00101548398379) A[2]:(0.656211256981) A[3]:(0.591471195221)\n",
      " state (2)  A[0]:(0.589659571648) A[1]:(0.728758215904) A[2]:(0.590966403484) A[3]:(0.657304704189)\n",
      " state (3)  A[0]:(0.655547142029) A[1]:(-0.0123440315947) A[2]:(0.505981981754) A[3]:(0.561079323292)\n",
      " state (4)  A[0]:(0.588968396187) A[1]:(0.655954241753) A[2]:(0.00032901763916) A[3]:(0.532650053501)\n",
      " state (5)  A[0]:(-0.0434370562434) A[1]:(0.99989926815) A[2]:(-0.785138607025) A[3]:(0.616169989109)\n",
      " state (6)  A[0]:(-0.00265925494023) A[1]:(0.809712767601) A[2]:(0.00136935629416) A[3]:(0.657204508781)\n",
      " state (7)  A[0]:(0.555363535881) A[1]:(-0.552259922028) A[2]:(0.417427659035) A[3]:(0.881291270256)\n",
      " state (8)  A[0]:(0.65704870224) A[1]:(0.000987708219327) A[2]:(0.729094743729) A[3]:(0.596650600433)\n",
      " state (9)  A[0]:(0.656876087189) A[1]:(0.810897767544) A[2]:(0.810397267342) A[3]:(0.00592370843515)\n",
      " state (10)  A[0]:(0.729149341583) A[1]:(0.900438129902) A[2]:(0.000775575463194) A[3]:(0.73046028614)\n",
      " state (11)  A[0]:(0.124335974455) A[1]:(0.881137430668) A[2]:(-0.918049931526) A[3]:(0.800359010696)\n",
      " state (12)  A[0]:(-0.43061003089) A[1]:(0.813356339931) A[2]:(-0.937086939812) A[3]:(0.714543640614)\n",
      " state (13)  A[0]:(0.00299102673307) A[1]:(0.80923974514) A[2]:(0.900803983212) A[3]:(0.730160057545)\n",
      " state (14)  A[0]:(0.811358213425) A[1]:(0.900080323219) A[2]:(0.999999523163) A[3]:(0.810961961746)\n",
      " state (15)  A[0]:(0.979632616043) A[1]:(0.94179058075) A[2]:(1.0) A[3]:(0.87884747982)\n",
      "Episode 450000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6101. Times reached goal: 971.               Steps done: 3353883. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0314535785902.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531325399876) A[1]:(0.590527236462) A[2]:(0.590422272682) A[3]:(0.53131878376)\n",
      " state (1)  A[0]:(0.531344354153) A[1]:(-9.90480184555e-05) A[2]:(0.656093478203) A[3]:(0.590280592442)\n",
      " state (2)  A[0]:(0.590144515038) A[1]:(0.728920340538) A[2]:(0.591774642467) A[3]:(0.655955672264)\n",
      " state (3)  A[0]:(0.656199097633) A[1]:(-0.010977646336) A[2]:(0.507098078728) A[3]:(0.559892654419)\n",
      " state (4)  A[0]:(0.590038239956) A[1]:(0.656123161316) A[2]:(0.000852346187457) A[3]:(0.53134149313)\n",
      " state (5)  A[0]:(-0.0413735806942) A[1]:(0.999899327755) A[2]:(-0.786350369453) A[3]:(0.614360332489)\n",
      " state (6)  A[0]:(-0.000316932797432) A[1]:(0.809944450855) A[2]:(-0.000282287597656) A[3]:(0.655894160271)\n",
      " state (7)  A[0]:(0.556060194969) A[1]:(-0.552561163902) A[2]:(0.416741132736) A[3]:(0.880385160446)\n",
      " state (8)  A[0]:(0.655537426472) A[1]:(0.000160351395607) A[2]:(0.729030251503) A[3]:(0.590791583061)\n",
      " state (9)  A[0]:(0.65556794405) A[1]:(0.810004353523) A[2]:(0.810107767582) A[3]:(-0.000103235244751)\n",
      " state (10)  A[0]:(0.728823184967) A[1]:(0.900022864342) A[2]:(0.000514507235494) A[3]:(0.729079723358)\n",
      " state (11)  A[0]:(0.124186694622) A[1]:(0.881000936031) A[2]:(-0.91810297966) A[3]:(0.799913644791)\n",
      " state (12)  A[0]:(-0.432146191597) A[1]:(0.813566327095) A[2]:(-0.937367916107) A[3]:(0.713859438896)\n",
      " state (13)  A[0]:(-0.000820785586257) A[1]:(0.809601485729) A[2]:(0.900150299072) A[3]:(0.729132354259)\n",
      " state (14)  A[0]:(0.80979347229) A[1]:(0.900261640549) A[2]:(0.999999523163) A[3]:(0.80999994278)\n",
      " state (15)  A[0]:(0.979464411736) A[1]:(0.94184833765) A[2]:(1.0) A[3]:(0.878134608269)\n",
      "Episode 451000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6095. Times reached goal: 974.               Steps done: 3359978. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0312624520784.\n",
      " state (0)  A[0]:(0.5309933424) A[1]:(0.590299367905) A[2]:(0.590434074402) A[3]:(0.53109484911)\n",
      " state (1)  A[0]:(0.531099379063) A[1]:(-0.000274643301964) A[2]:(0.655967354774) A[3]:(0.590390563011)\n",
      " state (2)  A[0]:(0.590164780617) A[1]:(0.728609144688) A[2]:(0.590605735779) A[3]:(0.655912637711)\n",
      " state (3)  A[0]:(0.656222343445) A[1]:(-0.00983157381415) A[2]:(0.505959689617) A[3]:(0.559742867947)\n",
      " state (4)  A[0]:(0.589993178844) A[1]:(0.657113969326) A[2]:(-0.000570297183003) A[3]:(0.531344890594)\n",
      " state (5)  A[0]:(-0.0417468361557) A[1]:(0.999899446964) A[2]:(-0.786789655685) A[3]:(0.615100264549)\n",
      " state (6)  A[0]:(-1.26510858536e-05) A[1]:(0.809787034988) A[2]:(-0.000215768814087) A[3]:(0.655612707138)\n",
      " state (7)  A[0]:(0.556522846222) A[1]:(-0.552097797394) A[2]:(0.416135132313) A[3]:(0.880076885223)\n",
      " state (8)  A[0]:(0.656421363354) A[1]:(-0.000266298651695) A[2]:(0.728314459324) A[3]:(0.591197669506)\n",
      " state (9)  A[0]:(0.656069397926) A[1]:(0.809857726097) A[2]:(0.809693455696) A[3]:(-0.0002800822258)\n",
      " state (10)  A[0]:(0.728833496571) A[1]:(0.899969518185) A[2]:(-0.000797271553893) A[3]:(0.728418588638)\n",
      " state (11)  A[0]:(0.124037526548) A[1]:(0.880998313427) A[2]:(-0.918389439583) A[3]:(0.799318015575)\n",
      " state (12)  A[0]:(-0.431957602501) A[1]:(0.813628196716) A[2]:(-0.937611877918) A[3]:(0.713166534901)\n",
      " state (13)  A[0]:(-0.000244721770287) A[1]:(0.809683740139) A[2]:(0.900009155273) A[3]:(0.728651702404)\n",
      " state (14)  A[0]:(0.809986591339) A[1]:(0.900322556496) A[2]:(0.999999523163) A[3]:(0.809755921364)\n",
      " state (15)  A[0]:(0.979484200478) A[1]:(0.941875338554) A[2]:(1.0) A[3]:(0.878009140491)\n",
      "Episode 452000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6071. Times reached goal: 972.               Steps done: 3366049. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0310732326885.\n",
      " state (0)  A[0]:(0.531456828117) A[1]:(0.590722799301) A[2]:(0.590380311012) A[3]:(0.531500577927)\n",
      " state (1)  A[0]:(0.531424283981) A[1]:(4.55379486084e-05) A[2]:(0.655985951424) A[3]:(0.590433359146)\n",
      " state (2)  A[0]:(0.590318381786) A[1]:(0.728793859482) A[2]:(0.59120118618) A[3]:(0.656016349792)\n",
      " state (3)  A[0]:(0.656363368034) A[1]:(-0.00964325573295) A[2]:(0.506884157658) A[3]:(0.559759020805)\n",
      " state (4)  A[0]:(0.590458869934) A[1]:(0.655674815178) A[2]:(0.000335812568665) A[3]:(0.531488656998)\n",
      " state (5)  A[0]:(-0.0407984480262) A[1]:(0.99989926815) A[2]:(-0.787514030933) A[3]:(0.616003990173)\n",
      " state (6)  A[0]:(9.96887683868e-05) A[1]:(0.809976935387) A[2]:(-0.000234603881836) A[3]:(0.656270623207)\n",
      " state (7)  A[0]:(0.55641746521) A[1]:(-0.552370071411) A[2]:(0.417261302471) A[3]:(0.880263566971)\n",
      " state (8)  A[0]:(0.656025111675) A[1]:(0.000699519994669) A[2]:(0.729076266289) A[3]:(0.590832948685)\n",
      " state (9)  A[0]:(0.656183123589) A[1]:(0.810343146324) A[2]:(0.810011029243) A[3]:(0.00064748513978)\n",
      " state (10)  A[0]:(0.729288935661) A[1]:(0.900036811829) A[2]:(-0.000176072120667) A[3]:(0.729238033295)\n",
      " state (11)  A[0]:(0.125225707889) A[1]:(0.880823016167) A[2]:(-0.918386399746) A[3]:(0.799912393093)\n",
      " state (12)  A[0]:(-0.430929720402) A[1]:(0.813076674938) A[2]:(-0.937703609467) A[3]:(0.713812410831)\n",
      " state (13)  A[0]:(0.00137138285208) A[1]:(0.809001505375) A[2]:(0.900015294552) A[3]:(0.729163110256)\n",
      " state (14)  A[0]:(0.810742199421) A[1]:(0.899986028671) A[2]:(0.999999523163) A[3]:(0.810097277164)\n",
      " state (15)  A[0]:(0.979584038258) A[1]:(0.941701173782) A[2]:(1.0) A[3]:(0.878209173679)\n",
      "Episode 453000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6106. Times reached goal: 975.               Steps done: 3372155. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0308840776078.\n",
      " state (0)  A[0]:(0.531533718109) A[1]:(0.590709924698) A[2]:(0.590609908104) A[3]:(0.531674027443)\n",
      " state (1)  A[0]:(0.531480252743) A[1]:(-4.72366809845e-05) A[2]:(0.656263589859) A[3]:(0.590657711029)\n",
      " state (2)  A[0]:(0.59044110775) A[1]:(0.729143261909) A[2]:(0.591265559196) A[3]:(0.656243562698)\n",
      " state (3)  A[0]:(0.656465530396) A[1]:(-0.00919247604907) A[2]:(0.507205843925) A[3]:(0.559798717499)\n",
      " state (4)  A[0]:(0.590428352356) A[1]:(0.656306326389) A[2]:(2.14576721191e-05) A[3]:(0.531602621078)\n",
      " state (5)  A[0]:(-0.040856834501) A[1]:(0.999899327755) A[2]:(-0.78818410635) A[3]:(0.616735458374)\n",
      " state (6)  A[0]:(-0.000109747052193) A[1]:(0.810014903545) A[2]:(7.80820846558e-05) A[3]:(0.656194031239)\n",
      " state (7)  A[0]:(0.556016147137) A[1]:(-0.552057921886) A[2]:(0.417847812176) A[3]:(0.880029201508)\n",
      " state (8)  A[0]:(0.656003773212) A[1]:(0.000128716230392) A[2]:(0.729153573513) A[3]:(0.590576171875)\n",
      " state (9)  A[0]:(0.656166672707) A[1]:(0.810060501099) A[2]:(0.810058057308) A[3]:(-0.000244975090027)\n",
      " state (10)  A[0]:(0.729154706001) A[1]:(0.900010287762) A[2]:(-0.000169396400452) A[3]:(0.728835523129)\n",
      " state (11)  A[0]:(0.124778419733) A[1]:(0.880988478661) A[2]:(-0.918483436108) A[3]:(0.799782395363)\n",
      " state (12)  A[0]:(-0.431691348553) A[1]:(0.813505113125) A[2]:(-0.937841117382) A[3]:(0.713713645935)\n",
      " state (13)  A[0]:(-0.000138536095619) A[1]:(0.809443235397) A[2]:(0.900063574314) A[3]:(0.729056358337)\n",
      " state (14)  A[0]:(0.810064971447) A[1]:(0.900146484375) A[2]:(0.999999523163) A[3]:(0.809997141361)\n",
      " state (15)  A[0]:(0.979503035545) A[1]:(0.941692948341) A[2]:(1.0) A[3]:(0.878139555454)\n",
      "Episode 454000 finished after 0 timesteps with r=1.0. Running score: 0.93. Times trained:               6050. Times reached goal: 958.               Steps done: 3378205. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0306977930174.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5908,  0.5909,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=0 , Random? True\n",
      "new state=0, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5908,  0.5908,  0.5320]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314, -0.0007,  0.6562,  0.5910]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.7291,  0.5913,  0.6566]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  0.8100, -0.0004,  0.6564]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7287,  0.9000, -0.0007,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9001,  1.0000,  0.8095]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531579256058) A[1]:(0.590841531754) A[2]:(0.590967774391) A[3]:(0.532137155533)\n",
      " state (1)  A[0]:(0.531580209732) A[1]:(-0.000870674615726) A[2]:(0.656338870525) A[3]:(0.59087228775)\n",
      " state (2)  A[0]:(0.590549588203) A[1]:(0.728929340839) A[2]:(0.591565728188) A[3]:(0.656348228455)\n",
      " state (3)  A[0]:(0.656518578529) A[1]:(-0.00897380430251) A[2]:(0.508043527603) A[3]:(0.559875845909)\n",
      " state (4)  A[0]:(0.590502679348) A[1]:(0.656088232994) A[2]:(0.000967979140114) A[3]:(0.531733155251)\n",
      " state (5)  A[0]:(-0.0406890287995) A[1]:(0.999899208546) A[2]:(-0.788614153862) A[3]:(0.617213606834)\n",
      " state (6)  A[0]:(-3.40938568115e-05) A[1]:(0.809966206551) A[2]:(9.07182693481e-05) A[3]:(0.656042993069)\n",
      " state (7)  A[0]:(0.555921077728) A[1]:(-0.551966905594) A[2]:(0.418124854565) A[3]:(0.879813373089)\n",
      " state (8)  A[0]:(0.655897974968) A[1]:(-0.000163525342941) A[2]:(0.729231357574) A[3]:(0.590190887451)\n",
      " state (9)  A[0]:(0.656035661697) A[1]:(0.809875249863) A[2]:(0.810110449791) A[3]:(-0.000550329627004)\n",
      " state (10)  A[0]:(0.729098439217) A[1]:(0.899893641472) A[2]:(4.27961349487e-05) A[3]:(0.728653490543)\n",
      " state (11)  A[0]:(0.124940745533) A[1]:(0.880844712257) A[2]:(-0.918502390385) A[3]:(0.799599051476)\n",
      " state (12)  A[0]:(-0.431403964758) A[1]:(0.813248038292) A[2]:(-0.937871396542) A[3]:(0.713352620602)\n",
      " state (13)  A[0]:(0.000333800911903) A[1]:(0.809129834175) A[2]:(0.900386214256) A[3]:(0.728631734848)\n",
      " state (14)  A[0]:(0.810226678848) A[1]:(0.899944901466) A[2]:(0.999999523163) A[3]:(0.809644460678)\n",
      " state (15)  A[0]:(0.979518890381) A[1]:(0.941518723965) A[2]:(1.0) A[3]:(0.877889335155)\n",
      "Episode 455000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6088. Times reached goal: 976.               Steps done: 3384293. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0305114725884.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531516671181) A[1]:(0.590526580811) A[2]:(0.590561747551) A[3]:(0.531374335289)\n",
      " state (1)  A[0]:(0.531426072121) A[1]:(0.000117063522339) A[2]:(0.656165838242) A[3]:(0.590319275856)\n",
      " state (2)  A[0]:(0.590375483036) A[1]:(0.728941440582) A[2]:(0.591352105141) A[3]:(0.655873417854)\n",
      " state (3)  A[0]:(0.656319618225) A[1]:(-0.00939484592527) A[2]:(0.507878780365) A[3]:(0.559185624123)\n",
      " state (4)  A[0]:(0.590386748314) A[1]:(0.655733346939) A[2]:(8.17775726318e-05) A[3]:(0.5312063694)\n",
      " state (5)  A[0]:(-0.0402601771057) A[1]:(0.999899148941) A[2]:(-0.78945350647) A[3]:(0.6173787117)\n",
      " state (6)  A[0]:(-0.000475943059428) A[1]:(0.810046672821) A[2]:(-7.2717666626e-05) A[3]:(0.655517578125)\n",
      " state (7)  A[0]:(0.555238187313) A[1]:(-0.551455259323) A[2]:(0.418229401112) A[3]:(0.879473686218)\n",
      " state (8)  A[0]:(0.655403971672) A[1]:(2.50339508057e-05) A[2]:(0.729103267193) A[3]:(0.589779615402)\n",
      " state (9)  A[0]:(0.655314385891) A[1]:(0.809990942478) A[2]:(0.810121059418) A[3]:(-0.00162976840511)\n",
      " state (10)  A[0]:(0.728629589081) A[1]:(0.900030910969) A[2]:(0.000120639801025) A[3]:(0.728313148022)\n",
      " state (11)  A[0]:(0.124407932162) A[1]:(0.881152629852) A[2]:(-0.918611586094) A[3]:(0.799665570259)\n",
      " state (12)  A[0]:(-0.431960612535) A[1]:(0.81388002634) A[2]:(-0.938133180141) A[3]:(0.713626861572)\n",
      " state (13)  A[0]:(-0.000574618519749) A[1]:(0.809837460518) A[2]:(0.899908661842) A[3]:(0.728931486607)\n",
      " state (14)  A[0]:(0.809941411018) A[1]:(0.900363385677) A[2]:(0.999999523163) A[3]:(0.809866786003)\n",
      " state (15)  A[0]:(0.979500234127) A[1]:(0.94177031517) A[2]:(1.0) A[3]:(0.878025114536)\n",
      "Episode 456000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6076. Times reached goal: 972.               Steps done: 3390369. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0303266469498.\n",
      " state (0)  A[0]:(0.532063245773) A[1]:(0.590937972069) A[2]:(0.590636909008) A[3]:(0.5319262743)\n",
      " state (1)  A[0]:(0.532021164894) A[1]:(-1.77174806595e-05) A[2]:(0.656313121319) A[3]:(0.590636730194)\n",
      " state (2)  A[0]:(0.590719521046) A[1]:(0.729045033455) A[2]:(0.591204166412) A[3]:(0.656338095665)\n",
      " state (3)  A[0]:(0.656669557095) A[1]:(-0.00815142504871) A[2]:(0.507958889008) A[3]:(0.559719800949)\n",
      " state (4)  A[0]:(0.590852022171) A[1]:(0.6563372612) A[2]:(-8.35657119751e-05) A[3]:(0.531863987446)\n",
      " state (5)  A[0]:(-0.0395382940769) A[1]:(0.999899208546) A[2]:(-0.790007412434) A[3]:(0.618505001068)\n",
      " state (6)  A[0]:(0.000256717205048) A[1]:(0.810022592545) A[2]:(-9.54866409302e-05) A[3]:(0.656327486038)\n",
      " state (7)  A[0]:(0.555883288383) A[1]:(-0.551598072052) A[2]:(0.418485492468) A[3]:(0.879765927792)\n",
      " state (8)  A[0]:(0.656385421753) A[1]:(-0.000127390027046) A[2]:(0.72903585434) A[3]:(0.591082811356)\n",
      " state (9)  A[0]:(0.656545162201) A[1]:(0.810053348541) A[2]:(0.810021281242) A[3]:(0.000362575025065)\n",
      " state (10)  A[0]:(0.729613542557) A[1]:(0.899989843369) A[2]:(8.46385955811e-06) A[3]:(0.729078650475)\n",
      " state (11)  A[0]:(0.126177713275) A[1]:(0.880992114544) A[2]:(-0.918673098087) A[3]:(0.800108134747)\n",
      " state (12)  A[0]:(-0.430889129639) A[1]:(0.813508868217) A[2]:(-0.938233137131) A[3]:(0.714004635811)\n",
      " state (13)  A[0]:(0.000569611729588) A[1]:(0.809411227703) A[2]:(0.899979650974) A[3]:(0.729199051857)\n",
      " state (14)  A[0]:(0.810388565063) A[1]:(0.900151491165) A[2]:(0.999999523163) A[3]:(0.810094535351)\n",
      " state (15)  A[0]:(0.979559004307) A[1]:(0.941638410091) A[2]:(1.0) A[3]:(0.878206253052)\n",
      "Episode 457000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6079. Times reached goal: 975.               Steps done: 3396448. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0301428504784.\n",
      " state (0)  A[0]:(0.531384050846) A[1]:(0.590657830238) A[2]:(0.590527892113) A[3]:(0.531393289566)\n",
      " state (1)  A[0]:(0.531389713287) A[1]:(-8.35061073303e-05) A[2]:(0.65621316433) A[3]:(0.590452611446)\n",
      " state (2)  A[0]:(0.590407729149) A[1]:(0.729158639908) A[2]:(0.590893447399) A[3]:(0.656076490879)\n",
      " state (3)  A[0]:(0.656352519989) A[1]:(-0.00756565202028) A[2]:(0.507941365242) A[3]:(0.559227287769)\n",
      " state (4)  A[0]:(0.590467810631) A[1]:(0.65616953373) A[2]:(-4.79221343994e-05) A[3]:(0.53138756752)\n",
      " state (5)  A[0]:(-0.0402421317995) A[1]:(0.999899148941) A[2]:(-0.790354669094) A[3]:(0.618698954582)\n",
      " state (6)  A[0]:(-0.00013467669487) A[1]:(0.810019671917) A[2]:(8.78572463989e-05) A[3]:(0.656129837036)\n",
      " state (7)  A[0]:(0.555494904518) A[1]:(-0.551292538643) A[2]:(0.418759822845) A[3]:(0.879534721375)\n",
      " state (8)  A[0]:(0.655917048454) A[1]:(3.18884849548e-05) A[2]:(0.7291431427) A[3]:(0.590407967567)\n",
      " state (9)  A[0]:(0.656113684177) A[1]:(0.810025036335) A[2]:(0.810055613518) A[3]:(3.37660312653e-05)\n",
      " state (10)  A[0]:(0.729292929173) A[1]:(0.900003492832) A[2]:(-9.5009803772e-05) A[3]:(0.729096591473)\n",
      " state (11)  A[0]:(0.125672787428) A[1]:(0.88108509779) A[2]:(-0.918790698051) A[3]:(0.800184369087)\n",
      " state (12)  A[0]:(-0.431341618299) A[1]:(0.813713550568) A[2]:(-0.938394188881) A[3]:(0.714047789574)\n",
      " state (13)  A[0]:(-0.000195011496544) A[1]:(0.809617996216) A[2]:(0.899973154068) A[3]:(0.729137539864)\n",
      " state (14)  A[0]:(0.810033380985) A[1]:(0.900243163109) A[2]:(0.999999523163) A[3]:(0.809964299202)\n",
      " state (15)  A[0]:(0.9795140028) A[1]:(0.941640198231) A[2]:(1.0) A[3]:(0.878079652786)\n",
      "Episode 458000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6062. Times reached goal: 967.               Steps done: 3402510. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0299606772437.\n",
      " state (0)  A[0]:(0.531777799129) A[1]:(0.590542793274) A[2]:(0.590262413025) A[3]:(0.531783938408)\n",
      " state (1)  A[0]:(0.531618177891) A[1]:(0.000601783336606) A[2]:(0.656124174595) A[3]:(0.59083378315)\n",
      " state (2)  A[0]:(0.590498805046) A[1]:(0.729126155376) A[2]:(0.590656995773) A[3]:(0.656486988068)\n",
      " state (3)  A[0]:(0.656344771385) A[1]:(-0.00716600986198) A[2]:(0.50786960125) A[3]:(0.559809803963)\n",
      " state (4)  A[0]:(0.590445399284) A[1]:(0.656393885612) A[2]:(0.00016462802887) A[3]:(0.532181620598)\n",
      " state (5)  A[0]:(-0.0401772670448) A[1]:(0.99989926815) A[2]:(-0.790292680264) A[3]:(0.619688749313)\n",
      " state (6)  A[0]:(-0.000362083286745) A[1]:(0.81018435955) A[2]:(6.83069229126e-05) A[3]:(0.656722784042)\n",
      " state (7)  A[0]:(0.555094838142) A[1]:(-0.55088865757) A[2]:(0.418629735708) A[3]:(0.879696071148)\n",
      " state (8)  A[0]:(0.655592560768) A[1]:(7.77393579483e-05) A[2]:(0.728970646858) A[3]:(0.591262578964)\n",
      " state (9)  A[0]:(0.655441820621) A[1]:(0.810002088547) A[2]:(0.809973597527) A[3]:(0.000745922210626)\n",
      " state (10)  A[0]:(0.728424489498) A[1]:(0.900013506413) A[2]:(-0.00034511089325) A[3]:(0.729117870331)\n",
      " state (11)  A[0]:(0.123523272574) A[1]:(0.881133615971) A[2]:(-0.918926477432) A[3]:(0.800081849098)\n",
      " state (12)  A[0]:(-0.433186620474) A[1]:(0.813791573048) A[2]:(-0.938543558121) A[3]:(0.713875770569)\n",
      " state (13)  A[0]:(-0.00244394945912) A[1]:(0.809650540352) A[2]:(0.900125622749) A[3]:(0.729077458382)\n",
      " state (14)  A[0]:(0.809269428253) A[1]:(0.900227308273) A[2]:(0.999999523163) A[3]:(0.810040056705)\n",
      " state (15)  A[0]:(0.979429900646) A[1]:(0.941567122936) A[2]:(1.0) A[3]:(0.878208875656)\n",
      "Episode 459000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6070. Times reached goal: 966.               Steps done: 3408580. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0297793667669.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5903,  0.5904,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315, -0.0000,  0.6559,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7290,  0.5906,  0.6559]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8100, -0.0001,  0.6558]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7298,  0.9001,  0.0001,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8105,  0.9000,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531362652779) A[1]:(0.590354382992) A[2]:(0.590406298637) A[3]:(0.531615495682)\n",
      " state (1)  A[0]:(0.531377434731) A[1]:(2.321600914e-05) A[2]:(0.655967950821) A[3]:(0.590540766716)\n",
      " state (2)  A[0]:(0.590300321579) A[1]:(0.72900313139) A[2]:(0.59057533741) A[3]:(0.656122386456)\n",
      " state (3)  A[0]:(0.656144142151) A[1]:(-0.00727941654623) A[2]:(0.507719576359) A[3]:(0.559243321419)\n",
      " state (4)  A[0]:(0.590268671513) A[1]:(0.656211018562) A[2]:(-8.49962234497e-05) A[3]:(0.531602978706)\n",
      " state (5)  A[0]:(-0.0401653684676) A[1]:(0.999899148941) A[2]:(-0.790440201759) A[3]:(0.619358122349)\n",
      " state (6)  A[0]:(0.000367566914065) A[1]:(0.809931874275) A[2]:(-0.000183939933777) A[3]:(0.656101346016)\n",
      " state (7)  A[0]:(0.555996119976) A[1]:(-0.551028132439) A[2]:(0.418415278196) A[3]:(0.879473149776)\n",
      " state (8)  A[0]:(0.656662464142) A[1]:(0.00036725398968) A[2]:(0.728895902634) A[3]:(0.590951919556)\n",
      " state (9)  A[0]:(0.65692281723) A[1]:(0.810205698013) A[2]:(0.810012698174) A[3]:(0.000932872004341)\n",
      " state (10)  A[0]:(0.72998213768) A[1]:(0.900050342083) A[2]:(0.000161170959473) A[3]:(0.729455709457)\n",
      " state (11)  A[0]:(0.127278372645) A[1]:(0.881054162979) A[2]:(-0.918918669224) A[3]:(0.800374984741)\n",
      " state (12)  A[0]:(-0.430061340332) A[1]:(0.813486814499) A[2]:(-0.938666522503) A[3]:(0.714129388332)\n",
      " state (13)  A[0]:(0.00133123912383) A[1]:(0.809221625328) A[2]:(0.900040984154) A[3]:(0.72919178009)\n",
      " state (14)  A[0]:(0.810545086861) A[1]:(0.900016725063) A[2]:(0.999999523163) A[3]:(0.810056269169)\n",
      " state (15)  A[0]:(0.979573726654) A[1]:(0.941462993622) A[2]:(1.0) A[3]:(0.878182947636)\n",
      "Episode 460000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6066. Times reached goal: 971.               Steps done: 3414646. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.029599271908.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531355261803) A[1]:(0.590653896332) A[2]:(0.590574622154) A[3]:(0.53131878376)\n",
      " state (1)  A[0]:(0.531479358673) A[1]:(1.42753124237e-05) A[2]:(0.656067490578) A[3]:(0.590376794338)\n",
      " state (2)  A[0]:(0.590523838997) A[1]:(0.728987753391) A[2]:(0.590715527534) A[3]:(0.6560754776)\n",
      " state (3)  A[0]:(0.656366467476) A[1]:(-0.00723447697237) A[2]:(0.507908344269) A[3]:(0.559145271778)\n",
      " state (4)  A[0]:(0.590564370155) A[1]:(0.656248271465) A[2]:(0.000117063522339) A[3]:(0.531506001949)\n",
      " state (5)  A[0]:(-0.039784476161) A[1]:(0.999899148941) A[2]:(-0.790521562099) A[3]:(0.619373083115)\n",
      " state (6)  A[0]:(0.00018709897995) A[1]:(0.810025274754) A[2]:(-0.000174522399902) A[3]:(0.655933141708)\n",
      " state (7)  A[0]:(0.55551815033) A[1]:(-0.550552487373) A[2]:(0.418456882238) A[3]:(0.87932240963)\n",
      " state (8)  A[0]:(0.656098842621) A[1]:(0.00038835403393) A[2]:(0.728804886341) A[3]:(0.590797662735)\n",
      " state (9)  A[0]:(0.65588927269) A[1]:(0.810120582581) A[2]:(0.80997979641) A[3]:(-0.000113010406494)\n",
      " state (10)  A[0]:(0.729051947594) A[1]:(0.900037825108) A[2]:(5.91278076172e-05) A[3]:(0.728754818439)\n",
      " state (11)  A[0]:(0.125682681799) A[1]:(0.881090044975) A[2]:(-0.919032156467) A[3]:(0.799886643887)\n",
      " state (12)  A[0]:(-0.431145608425) A[1]:(0.813532710075) A[2]:(-0.938829481602) A[3]:(0.713594913483)\n",
      " state (13)  A[0]:(4.3511390686e-06) A[1]:(0.809162735939) A[2]:(0.900021612644) A[3]:(0.728831768036)\n",
      " state (14)  A[0]:(0.810026943684) A[1]:(0.899903655052) A[2]:(0.999999582767) A[3]:(0.809891760349)\n",
      " state (15)  A[0]:(0.979512929916) A[1]:(0.94131308794) A[2]:(1.0) A[3]:(0.87814104557)\n",
      "Episode 461000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6027. Times reached goal: 955.               Steps done: 3420673. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0294214136106.\n",
      " state (0)  A[0]:(0.531340479851) A[1]:(0.590532422066) A[2]:(0.590657234192) A[3]:(0.531487941742)\n",
      " state (1)  A[0]:(0.531292557716) A[1]:(-0.0001230686903) A[2]:(0.65610730648) A[3]:(0.590556740761)\n",
      " state (2)  A[0]:(0.59042596817) A[1]:(0.729284763336) A[2]:(0.59046459198) A[3]:(0.656092524529)\n",
      " state (3)  A[0]:(0.656237483025) A[1]:(-0.00630613230169) A[2]:(0.507605195045) A[3]:(0.559041857719)\n",
      " state (4)  A[0]:(0.590369701385) A[1]:(0.656372666359) A[2]:(-2.69412994385e-05) A[3]:(0.53140437603)\n",
      " state (5)  A[0]:(-0.0404906198382) A[1]:(0.99989926815) A[2]:(-0.790508627892) A[3]:(0.619554638863)\n",
      " state (6)  A[0]:(-0.000131234526634) A[1]:(0.810196101665) A[2]:(-0.000174522399902) A[3]:(0.65594792366)\n",
      " state (7)  A[0]:(0.555468916893) A[1]:(-0.550596058369) A[2]:(0.418633580208) A[3]:(0.87930393219)\n",
      " state (8)  A[0]:(0.656103372574) A[1]:(0.00105781818274) A[2]:(0.729066252708) A[3]:(0.590384244919)\n",
      " state (9)  A[0]:(0.656259894371) A[1]:(0.810463964939) A[2]:(0.810135662556) A[3]:(0.000169515609741)\n",
      " state (10)  A[0]:(0.729325532913) A[1]:(0.900144696236) A[2]:(0.000213027000427) A[3]:(0.729112923145)\n",
      " state (11)  A[0]:(0.125809803605) A[1]:(0.88113039732) A[2]:(-0.919121205807) A[3]:(0.800117790699)\n",
      " state (12)  A[0]:(-0.43127065897) A[1]:(0.81352853775) A[2]:(-0.938957154751) A[3]:(0.713805794716)\n",
      " state (13)  A[0]:(-2.17854976654e-05) A[1]:(0.80919444561) A[2]:(0.900116384029) A[3]:(0.728973925114)\n",
      " state (14)  A[0]:(0.810152769089) A[1]:(0.900001585484) A[2]:(0.999999582767) A[3]:(0.809962928295)\n",
      " state (15)  A[0]:(0.979540765285) A[1]:(0.941393852234) A[2]:(1.0) A[3]:(0.878166913986)\n",
      "Episode 462000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6067. Times reached goal: 968.               Steps done: 3426740. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0292434542797.\n",
      " state (0)  A[0]:(0.531466245651) A[1]:(0.590287804604) A[2]:(0.590473175049) A[3]:(0.531803965569)\n",
      " state (1)  A[0]:(0.531581640244) A[1]:(-1.57207250595e-05) A[2]:(0.656048297882) A[3]:(0.590869426727)\n",
      " state (2)  A[0]:(0.590509653091) A[1]:(0.729006290436) A[2]:(0.590605378151) A[3]:(0.656449794769)\n",
      " state (3)  A[0]:(0.656322896481) A[1]:(-0.00698219798505) A[2]:(0.507744908333) A[3]:(0.559500217438)\n",
      " state (4)  A[0]:(0.590492248535) A[1]:(0.656344771385) A[2]:(-0.000112414360046) A[3]:(0.532014966011)\n",
      " state (5)  A[0]:(-0.0399667546153) A[1]:(0.99989926815) A[2]:(-0.790676772594) A[3]:(0.620223879814)\n",
      " state (6)  A[0]:(0.000682577374391) A[1]:(0.810122013092) A[2]:(-0.000367283792002) A[3]:(0.656490087509)\n",
      " state (7)  A[0]:(0.555944561958) A[1]:(-0.550874710083) A[2]:(0.418536245823) A[3]:(0.879470646381)\n",
      " state (8)  A[0]:(0.6563346982) A[1]:(0.000159278512001) A[2]:(0.728821277618) A[3]:(0.590751826763)\n",
      " state (9)  A[0]:(0.656424999237) A[1]:(0.810059547424) A[2]:(0.809894561768) A[3]:(0.00057387346169)\n",
      " state (10)  A[0]:(0.729719638824) A[1]:(0.900006175041) A[2]:(-8.15391540527e-05) A[3]:(0.729448914528)\n",
      " state (11)  A[0]:(0.127238944173) A[1]:(0.881146490574) A[2]:(-0.919198930264) A[3]:(0.800490617752)\n",
      " state (12)  A[0]:(-0.430228024721) A[1]:(0.813768386841) A[2]:(-0.939101934433) A[3]:(0.714198291302)\n",
      " state (13)  A[0]:(0.000798314635176) A[1]:(0.80956530571) A[2]:(0.900096476078) A[3]:(0.72916328907)\n",
      " state (14)  A[0]:(0.810332894325) A[1]:(0.90026062727) A[2]:(0.999999582767) A[3]:(0.809983074665)\n",
      " state (15)  A[0]:(0.979560673237) A[1]:(0.941546678543) A[2]:(1.0) A[3]:(0.878123223782)\n",
      "Episode 463000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6089. Times reached goal: 969.               Steps done: 3432829. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0290659319019.\n",
      " state (0)  A[0]:(0.531236171722) A[1]:(0.590574502945) A[2]:(0.590400338173) A[3]:(0.53186994791)\n",
      " state (1)  A[0]:(0.531021595001) A[1]:(-1.81794166565e-06) A[2]:(0.655688643456) A[3]:(0.590706348419)\n",
      " state (2)  A[0]:(0.58992767334) A[1]:(0.72831261158) A[2]:(0.590438425541) A[3]:(0.656136274338)\n",
      " state (3)  A[0]:(0.655791461468) A[1]:(-0.00837965030223) A[2]:(0.507672548294) A[3]:(0.559159994125)\n",
      " state (4)  A[0]:(0.589909315109) A[1]:(0.655538678169) A[2]:(1.85966491699e-05) A[3]:(0.531774818897)\n",
      " state (5)  A[0]:(-0.0408308617771) A[1]:(0.999899148941) A[2]:(-0.790609538555) A[3]:(0.620207071304)\n",
      " state (6)  A[0]:(-0.000730439904146) A[1]:(0.809920608997) A[2]:(5.00679016113e-05) A[3]:(0.656228840351)\n",
      " state (7)  A[0]:(0.554440140724) A[1]:(-0.552051663399) A[2]:(0.41921967268) A[3]:(0.879207670689)\n",
      " state (8)  A[0]:(0.654527425766) A[1]:(-0.00123539508786) A[2]:(0.728940367699) A[3]:(0.589681625366)\n",
      " state (9)  A[0]:(0.653999209404) A[1]:(0.809767842293) A[2]:(0.80970287323) A[3]:(-0.00147038593423)\n",
      " state (10)  A[0]:(0.727417588234) A[1]:(0.899841248989) A[2]:(-0.000999569543637) A[3]:(0.728296458721)\n",
      " state (11)  A[0]:(0.122444838285) A[1]:(0.880993902683) A[2]:(-0.919411897659) A[3]:(0.799646854401)\n",
      " state (12)  A[0]:(-0.433655947447) A[1]:(0.813642501831) A[2]:(-0.939242005348) A[3]:(0.713312745094)\n",
      " state (13)  A[0]:(-0.00257266499102) A[1]:(0.809588611126) A[2]:(0.900358498096) A[3]:(0.728610217571)\n",
      " state (14)  A[0]:(0.809450149536) A[1]:(0.900374054909) A[2]:(0.999999582767) A[3]:(0.809755086899)\n",
      " state (15)  A[0]:(0.97948217392) A[1]:(0.94161772728) A[2]:(1.0) A[3]:(0.878055930138)\n",
      "Episode 464000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6064. Times reached goal: 964.               Steps done: 3438893. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0288902094199.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5904,  0.5906,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.0001,  0.6562,  0.5908]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.7291,  0.5906,  0.6565]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0000,  0.8099,  0.0000,  0.6565]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7299,  0.9000,  0.0010,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9000,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531384050846) A[1]:(0.590471088886) A[2]:(0.59049141407) A[3]:(0.531501054764)\n",
      " state (1)  A[0]:(0.531279325485) A[1]:(-5.88595867157e-05) A[2]:(0.656069159508) A[3]:(0.590740680695)\n",
      " state (2)  A[0]:(0.5905341506) A[1]:(0.728967547417) A[2]:(0.59058701992) A[3]:(0.656349658966)\n",
      " state (3)  A[0]:(0.65625834465) A[1]:(-0.00710710836574) A[2]:(0.507674634457) A[3]:(0.559162020683)\n",
      " state (4)  A[0]:(0.590300500393) A[1]:(0.6560972929) A[2]:(-0.000155329704285) A[3]:(0.531599640846)\n",
      " state (5)  A[0]:(-0.0406263098121) A[1]:(0.999899208546) A[2]:(-0.790720939636) A[3]:(0.620168924332)\n",
      " state (6)  A[0]:(7.33137130737e-06) A[1]:(0.810057997704) A[2]:(-0.000142693519592) A[3]:(0.656291723251)\n",
      " state (7)  A[0]:(0.555324554443) A[1]:(-0.550832390785) A[2]:(0.418857038021) A[3]:(0.879303514957)\n",
      " state (8)  A[0]:(0.656159281731) A[1]:(-0.000180929899216) A[2]:(0.728932619095) A[3]:(0.590695023537)\n",
      " state (9)  A[0]:(0.656350314617) A[1]:(0.809934139252) A[2]:(0.809983849525) A[3]:(0.000117003917694)\n",
      " state (10)  A[0]:(0.729516983032) A[1]:(0.899963796139) A[2]:(-0.000297427177429) A[3]:(0.728967905045)\n",
      " state (11)  A[0]:(0.126893594861) A[1]:(0.881128311157) A[2]:(-0.91951841116) A[3]:(0.800091505051)\n",
      " state (12)  A[0]:(-0.430299669504) A[1]:(0.813650846481) A[2]:(-0.939613878727) A[3]:(0.713787257671)\n",
      " state (13)  A[0]:(0.000859141116962) A[1]:(0.809200763702) A[2]:(0.899408876896) A[3]:(0.729021072388)\n",
      " state (14)  A[0]:(0.810383677483) A[1]:(0.899949967861) A[2]:(0.999999582767) A[3]:(0.81005769968)\n",
      " state (15)  A[0]:(0.979584395885) A[1]:(0.941271901131) A[2]:(1.0) A[3]:(0.878278791904)\n",
      "Episode 465000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6091. Times reached goal: 972.               Steps done: 3444984. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0287147739854.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532288432121) A[1]:(0.590458631516) A[2]:(0.590700805187) A[3]:(0.532313704491)\n",
      " state (1)  A[0]:(0.532079994678) A[1]:(-0.000269249081612) A[2]:(0.655653059483) A[3]:(0.590836286545)\n",
      " state (2)  A[0]:(0.59072381258) A[1]:(0.729240119457) A[2]:(0.590119719505) A[3]:(0.656188845634)\n",
      " state (3)  A[0]:(0.656500935555) A[1]:(-0.00636862544343) A[2]:(0.507323145866) A[3]:(0.559243559837)\n",
      " state (4)  A[0]:(0.590629696846) A[1]:(0.656291127205) A[2]:(-0.000449418992503) A[3]:(0.531893193722)\n",
      " state (5)  A[0]:(-0.0403292961419) A[1]:(0.999899208546) A[2]:(-0.790878653526) A[3]:(0.620257616043)\n",
      " state (6)  A[0]:(0.000318437814713) A[1]:(0.809767484665) A[2]:(0.000139474868774) A[3]:(0.656291842461)\n",
      " state (7)  A[0]:(0.555752515793) A[1]:(-0.552936136723) A[2]:(0.419931590557) A[3]:(0.879401385784)\n",
      " state (8)  A[0]:(0.656520485878) A[1]:(-0.0012985013891) A[2]:(0.729498267174) A[3]:(0.589942991734)\n",
      " state (9)  A[0]:(0.657159030437) A[1]:(0.809966683388) A[2]:(0.809898436069) A[3]:(-0.000109672546387)\n",
      " state (10)  A[0]:(0.729866147041) A[1]:(0.899809420109) A[2]:(-0.0017497522058) A[3]:(0.729345798492)\n",
      " state (11)  A[0]:(0.126417651772) A[1]:(0.880747377872) A[2]:(-0.919898688793) A[3]:(0.800442814827)\n",
      " state (12)  A[0]:(-0.430951684713) A[1]:(0.812995433807) A[2]:(-0.939866185188) A[3]:(0.714408993721)\n",
      " state (13)  A[0]:(0.000398024887545) A[1]:(0.808750092983) A[2]:(0.899627685547) A[3]:(0.729667901993)\n",
      " state (14)  A[0]:(0.810386717319) A[1]:(0.899930894375) A[2]:(0.999999582767) A[3]:(0.810452878475)\n",
      " state (15)  A[0]:(0.979590952396) A[1]:(0.941340208054) A[2]:(1.0) A[3]:(0.878466725349)\n",
      "Episode 466000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6064. Times reached goal: 982.               Steps done: 3451048. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0285411744816.\n",
      " state (0)  A[0]:(0.531849265099) A[1]:(0.590530395508) A[2]:(0.590528428555) A[3]:(0.531655669212)\n",
      " state (1)  A[0]:(0.532896459103) A[1]:(-0.000723883393221) A[2]:(0.656172633171) A[3]:(0.590392231941)\n",
      " state (2)  A[0]:(0.592055559158) A[1]:(0.728988409042) A[2]:(0.590727686882) A[3]:(0.655947387218)\n",
      " state (3)  A[0]:(0.657858669758) A[1]:(-0.00664419587702) A[2]:(0.507877111435) A[3]:(0.558735191822)\n",
      " state (4)  A[0]:(0.59265422821) A[1]:(0.656411349773) A[2]:(-7.93933868408e-05) A[3]:(0.531279325485)\n",
      " state (5)  A[0]:(-0.0358117781579) A[1]:(0.999899208546) A[2]:(-0.790974140167) A[3]:(0.619811654091)\n",
      " state (6)  A[0]:(0.00547659862787) A[1]:(0.809952974319) A[2]:(-0.000489115656819) A[3]:(0.655657589436)\n",
      " state (7)  A[0]:(0.55982863903) A[1]:(-0.550635933876) A[2]:(0.418507039547) A[3]:(0.879242658615)\n",
      " state (8)  A[0]:(0.661057770252) A[1]:(-0.000399634212954) A[2]:(0.728664875031) A[3]:(0.592512965202)\n",
      " state (9)  A[0]:(0.662117362022) A[1]:(0.810091495514) A[2]:(0.810583889484) A[3]:(0.00270109716803)\n",
      " state (10)  A[0]:(0.736103534698) A[1]:(0.90008097887) A[2]:(0.00500078778714) A[3]:(0.731003284454)\n",
      " state (11)  A[0]:(0.143304482102) A[1]:(0.881293296814) A[2]:(-0.918587446213) A[3]:(0.802103281021)\n",
      " state (12)  A[0]:(-0.41847422719) A[1]:(0.813915729523) A[2]:(-0.939255177975) A[3]:(0.715648174286)\n",
      " state (13)  A[0]:(0.0121309468523) A[1]:(0.809545576572) A[2]:(0.900045871735) A[3]:(0.729671537876)\n",
      " state (14)  A[0]:(0.813571810722) A[1]:(0.900283396244) A[2]:(0.999999582767) A[3]:(0.809940457344)\n",
      " state (15)  A[0]:(0.979916036129) A[1]:(0.941526293755) A[2]:(1.0) A[3]:(0.877954542637)\n",
      "Episode 467000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6059. Times reached goal: 969.               Steps done: 3457107. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0283687663433.\n",
      " state (0)  A[0]:(0.53156042099) A[1]:(0.590573549271) A[2]:(0.590794444084) A[3]:(0.531980037689)\n",
      " state (1)  A[0]:(0.53149330616) A[1]:(-0.000248998403549) A[2]:(0.657052159309) A[3]:(0.590840756893)\n",
      " state (2)  A[0]:(0.590579390526) A[1]:(0.729396820068) A[2]:(0.591871380806) A[3]:(0.656464099884)\n",
      " state (3)  A[0]:(0.656452775002) A[1]:(-0.00636720983312) A[2]:(0.509058833122) A[3]:(0.559163331985)\n",
      " state (4)  A[0]:(0.590364694595) A[1]:(0.657691836357) A[2]:(0.000653028371744) A[3]:(0.531652748585)\n",
      " state (5)  A[0]:(-0.0409389063716) A[1]:(0.99989938736) A[2]:(-0.79096531868) A[3]:(0.620241999626)\n",
      " state (6)  A[0]:(-0.000102296471596) A[1]:(0.810003399849) A[2]:(0.000399351090891) A[3]:(0.655800044537)\n",
      " state (7)  A[0]:(0.555051803589) A[1]:(-0.550942897797) A[2]:(0.419987171888) A[3]:(0.878951013088)\n",
      " state (8)  A[0]:(0.655768334866) A[1]:(-8.21053981781e-05) A[2]:(0.729483664036) A[3]:(0.588627576828)\n",
      " state (9)  A[0]:(0.656029820442) A[1]:(0.809867739677) A[2]:(0.81006705761) A[3]:(-0.00299533316866)\n",
      " state (10)  A[0]:(0.728739798069) A[1]:(0.899935483932) A[2]:(-0.000532746256795) A[3]:(0.727783679962)\n",
      " state (11)  A[0]:(0.12404859066) A[1]:(0.881215691566) A[2]:(-0.91979432106) A[3]:(0.799384772778)\n",
      " state (12)  A[0]:(-0.433331459761) A[1]:(0.81394135952) A[2]:(-0.939878344536) A[3]:(0.713044762611)\n",
      " state (13)  A[0]:(-0.00300700054504) A[1]:(0.809606552124) A[2]:(0.900231778622) A[3]:(0.728441238403)\n",
      " state (14)  A[0]:(0.809203505516) A[1]:(0.900186419487) A[2]:(0.999999582767) A[3]:(0.809638679028)\n",
      " state (15)  A[0]:(0.979479908943) A[1]:(0.941260695457) A[2]:(1.0) A[3]:(0.878026127815)\n",
      "Episode 468000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6056. Times reached goal: 971.               Steps done: 3463163. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0281974842599.\n",
      " state (0)  A[0]:(0.531356692314) A[1]:(0.590641021729) A[2]:(0.590506315231) A[3]:(0.531570076942)\n",
      " state (1)  A[0]:(0.531288862228) A[1]:(-5.83231449127e-05) A[2]:(0.65602850914) A[3]:(0.590548694134)\n",
      " state (2)  A[0]:(0.590240597725) A[1]:(0.729022026062) A[2]:(0.590720057487) A[3]:(0.656105041504)\n",
      " state (3)  A[0]:(0.656119585037) A[1]:(-0.00670843198895) A[2]:(0.507932901382) A[3]:(0.558793485165)\n",
      " state (4)  A[0]:(0.590299963951) A[1]:(0.656194865704) A[2]:(0.000178933143616) A[3]:(0.531495690346)\n",
      " state (5)  A[0]:(-0.0408885963261) A[1]:(0.999899208546) A[2]:(-0.790900826454) A[3]:(0.620656251907)\n",
      " state (6)  A[0]:(-0.000437572569354) A[1]:(0.810060918331) A[2]:(0.000206589698792) A[3]:(0.656016469002)\n",
      " state (7)  A[0]:(0.554877877235) A[1]:(-0.550651907921) A[2]:(0.419463336468) A[3]:(0.879098236561)\n",
      " state (8)  A[0]:(0.655924260616) A[1]:(-0.000262409448624) A[2]:(0.729061484337) A[3]:(0.59071803093)\n",
      " state (9)  A[0]:(0.655789256096) A[1]:(0.809940338135) A[2]:(0.810049831867) A[3]:(-0.000273823738098)\n",
      " state (10)  A[0]:(0.728805601597) A[1]:(0.899973869324) A[2]:(-0.000175952911377) A[3]:(0.728677272797)\n",
      " state (11)  A[0]:(0.125445991755) A[1]:(0.881238043308) A[2]:(-0.919806063175) A[3]:(0.799933314323)\n",
      " state (12)  A[0]:(-0.431382477283) A[1]:(0.813923597336) A[2]:(-0.939988791943) A[3]:(0.713628053665)\n",
      " state (13)  A[0]:(-0.000242605805397) A[1]:(0.809544444084) A[2]:(0.900114357471) A[3]:(0.728933751583)\n",
      " state (14)  A[0]:(0.81009376049) A[1]:(0.900194764137) A[2]:(0.999999582767) A[3]:(0.809986114502)\n",
      " state (15)  A[0]:(0.979568481445) A[1]:(0.941307127476) A[2]:(1.0) A[3]:(0.878235340118)\n",
      "Episode 469000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6046. Times reached goal: 971.               Steps done: 3469209. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0280275166001.\n",
      "q_values \n",
      "tensor([[ 0.5322,  0.5903,  0.5901,  0.5305]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.6559, -0.0004,  0.5308]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6553,  0.0000,  0.7290,  0.5895]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557,  0.8099,  0.8101,  0.0003]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000,  0.0000,  0.7297]], device='cuda:0')\n",
      "On state=10, selected action=2 , Random? True\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.530231654644) A[1]:(0.590253472328) A[2]:(0.590067207813) A[3]:(0.531270623207)\n",
      " state (1)  A[0]:(0.530376851559) A[1]:(0.000120788812637) A[2]:(0.655966520309) A[3]:(0.590662479401)\n",
      " state (2)  A[0]:(0.589722633362) A[1]:(0.729054331779) A[2]:(0.590528726578) A[3]:(0.656339526176)\n",
      " state (3)  A[0]:(0.655573487282) A[1]:(-0.00657391035929) A[2]:(0.507634162903) A[3]:(0.559010982513)\n",
      " state (4)  A[0]:(0.589621424675) A[1]:(0.656139314175) A[2]:(-0.000201463699341) A[3]:(0.531867682934)\n",
      " state (5)  A[0]:(-0.0417810641229) A[1]:(0.99989926815) A[2]:(-0.791050255299) A[3]:(0.621532022953)\n",
      " state (6)  A[0]:(-0.000725671532564) A[1]:(0.810165941715) A[2]:(-0.000393390626414) A[3]:(0.656688690186)\n",
      " state (7)  A[0]:(0.554646968842) A[1]:(-0.550392568111) A[2]:(0.418940484524) A[3]:(0.879245042801)\n",
      " state (8)  A[0]:(0.655620455742) A[1]:(1.85072422028e-05) A[2]:(0.728941917419) A[3]:(0.590921282768)\n",
      " state (9)  A[0]:(0.656011343002) A[1]:(0.809943437576) A[2]:(0.810129523277) A[3]:(0.00128668476827)\n",
      " state (10)  A[0]:(0.729646086693) A[1]:(0.900012493134) A[2]:(0.000755905988626) A[3]:(0.729889750481)\n",
      " state (11)  A[0]:(0.127915516496) A[1]:(0.881353974342) A[2]:(-0.9197255373) A[3]:(0.800979733467)\n",
      " state (12)  A[0]:(-0.430051058531) A[1]:(0.814107120037) A[2]:(-0.940118789673) A[3]:(0.714729905128)\n",
      " state (13)  A[0]:(0.000244542956352) A[1]:(0.809632301331) A[2]:(0.899889230728) A[3]:(0.729680776596)\n",
      " state (14)  A[0]:(0.810036540031) A[1]:(0.90017837286) A[2]:(0.999999582767) A[3]:(0.810440063477)\n",
      " state (15)  A[0]:(0.979558289051) A[1]:(0.941240370274) A[2]:(1.0) A[3]:(0.878529369831)\n",
      "Episode 470000 finished after 0 timesteps with r=0.0. Running score: 0.95. Times trained:               6087. Times reached goal: 975.               Steps done: 3475296. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0278574312864.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531313061714) A[1]:(0.590445280075) A[2]:(0.590447545052) A[3]:(0.531303822994)\n",
      " state (1)  A[0]:(0.531350433826) A[1]:(-5.73247671127e-05) A[2]:(0.656130433083) A[3]:(0.590391635895)\n",
      " state (2)  A[0]:(0.590433955193) A[1]:(0.729122281075) A[2]:(0.590529441833) A[3]:(0.656068682671)\n",
      " state (3)  A[0]:(0.656178355217) A[1]:(-0.00632478808984) A[2]:(0.507694005966) A[3]:(0.558589696884)\n",
      " state (4)  A[0]:(0.590338528156) A[1]:(0.656164824963) A[2]:(1.33514404297e-05) A[3]:(0.531333446503)\n",
      " state (5)  A[0]:(-0.0406981483102) A[1]:(0.999899208546) A[2]:(-0.790966749191) A[3]:(0.620966613293)\n",
      " state (6)  A[0]:(4.96357679367e-05) A[1]:(0.810094475746) A[2]:(8.86917114258e-05) A[3]:(0.656038403511)\n",
      " state (7)  A[0]:(0.555045604706) A[1]:(-0.550368309021) A[2]:(0.419413715601) A[3]:(0.878987312317)\n",
      " state (8)  A[0]:(0.656002819538) A[1]:(0.000190630555153) A[2]:(0.729104399681) A[3]:(0.590359032154)\n",
      " state (9)  A[0]:(0.656126379967) A[1]:(0.81005614996) A[2]:(0.810111641884) A[3]:(2.95639038086e-05)\n",
      " state (10)  A[0]:(0.729219973087) A[1]:(0.900022745132) A[2]:(-2.14576721191e-06) A[3]:(0.729028344154)\n",
      " state (11)  A[0]:(0.126449242234) A[1]:(0.881293594837) A[2]:(-0.919976830482) A[3]:(0.800216376781)\n",
      " state (12)  A[0]:(-0.430877745152) A[1]:(0.813932240009) A[2]:(-0.940295040607) A[3]:(0.713844060898)\n",
      " state (13)  A[0]:(-8.87811183929e-05) A[1]:(0.809433937073) A[2]:(0.900076925755) A[3]:(0.729027867317)\n",
      " state (14)  A[0]:(0.810049057007) A[1]:(0.900091290474) A[2]:(0.999999582767) A[3]:(0.810018181801)\n",
      " state (15)  A[0]:(0.979565858841) A[1]:(0.941161215305) A[2]:(1.0) A[3]:(0.87826615572)\n",
      "Episode 471000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6052. Times reached goal: 968.               Steps done: 3481348. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0276893472476.\n",
      " state (0)  A[0]:(0.531041502953) A[1]:(0.590759158134) A[2]:(0.590521335602) A[3]:(0.531710624695)\n",
      " state (1)  A[0]:(0.531137049198) A[1]:(1.04308128357e-07) A[2]:(0.656217694283) A[3]:(0.59074395895)\n",
      " state (2)  A[0]:(0.590344429016) A[1]:(0.729178249836) A[2]:(0.590697944164) A[3]:(0.656337201595)\n",
      " state (3)  A[0]:(0.65619456768) A[1]:(-0.00610262015834) A[2]:(0.507877469063) A[3]:(0.558882236481)\n",
      " state (4)  A[0]:(0.590435922146) A[1]:(0.656376183033) A[2]:(0.000178575515747) A[3]:(0.531714081764)\n",
      " state (5)  A[0]:(-0.0405631139874) A[1]:(0.99989926815) A[2]:(-0.790976107121) A[3]:(0.621491611004)\n",
      " state (6)  A[0]:(0.000443845958216) A[1]:(0.810184955597) A[2]:(0.000348091125488) A[3]:(0.656485557556)\n",
      " state (7)  A[0]:(0.555494427681) A[1]:(-0.550304412842) A[2]:(0.419815629721) A[3]:(0.879188299179)\n",
      " state (8)  A[0]:(0.656580209732) A[1]:(0.000448808044894) A[2]:(0.729248166084) A[3]:(0.591085493565)\n",
      " state (9)  A[0]:(0.65678691864) A[1]:(0.810212254524) A[2]:(0.810150623322) A[3]:(0.00101399386767)\n",
      " state (10)  A[0]:(0.729716300964) A[1]:(0.900069594383) A[2]:(-1.76429748535e-05) A[3]:(0.729350209236)\n",
      " state (11)  A[0]:(0.127426579595) A[1]:(0.881299138069) A[2]:(-0.920072615147) A[3]:(0.800343036652)\n",
      " state (12)  A[0]:(-0.429977446795) A[1]:(0.813887953758) A[2]:(-0.940436184406) A[3]:(0.713930726051)\n",
      " state (13)  A[0]:(0.00122189463582) A[1]:(0.80939000845) A[2]:(0.900046646595) A[3]:(0.729120492935)\n",
      " state (14)  A[0]:(0.810556948185) A[1]:(0.90012806654) A[2]:(0.999999582767) A[3]:(0.810111761093)\n",
      " state (15)  A[0]:(0.979628741741) A[1]:(0.94120824337) A[2]:(1.0) A[3]:(0.878335654736)\n",
      "Episode 472000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6044. Times reached goal: 974.               Steps done: 3487392. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0275224975605.\n",
      " state (0)  A[0]:(0.53147995472) A[1]:(0.590397596359) A[2]:(0.5904083848) A[3]:(0.531224012375)\n",
      " state (1)  A[0]:(0.531435549259) A[1]:(-4.30643558502e-05) A[2]:(0.656034827232) A[3]:(0.590358316898)\n",
      " state (2)  A[0]:(0.590476155281) A[1]:(0.729006588459) A[2]:(0.590484142303) A[3]:(0.655982255936)\n",
      " state (3)  A[0]:(0.656235933304) A[1]:(-0.00638133566827) A[2]:(0.507658600807) A[3]:(0.558465600014)\n",
      " state (4)  A[0]:(0.590418696404) A[1]:(0.656022012234) A[2]:(-4.81605529785e-05) A[3]:(0.531319320202)\n",
      " state (5)  A[0]:(-0.0407949350774) A[1]:(0.999899148941) A[2]:(-0.791117966175) A[3]:(0.621166944504)\n",
      " state (6)  A[0]:(0.000159740447998) A[1]:(0.809942305088) A[2]:(-0.000100135803223) A[3]:(0.655933380127)\n",
      " state (7)  A[0]:(0.555047750473) A[1]:(-0.550474524498) A[2]:(0.419267237186) A[3]:(0.87887275219)\n",
      " state (8)  A[0]:(0.6559715271) A[1]:(-0.000171333551407) A[2]:(0.728879213333) A[3]:(0.590257287025)\n",
      " state (9)  A[0]:(0.655977845192) A[1]:(0.809920191765) A[2]:(0.809943437576) A[3]:(-0.000152140855789)\n",
      " state (10)  A[0]:(0.729092597961) A[1]:(0.899960398674) A[2]:(-0.000203251838684) A[3]:(0.728922486305)\n",
      " state (11)  A[0]:(0.126381933689) A[1]:(0.881283342838) A[2]:(-0.92013245821) A[3]:(0.800192534924)\n",
      " state (12)  A[0]:(-0.430996477604) A[1]:(0.813980102539) A[2]:(-0.940543889999) A[3]:(0.713813960552)\n",
      " state (13)  A[0]:(-0.000330910086632) A[1]:(0.809523940086) A[2]:(0.900103986263) A[3]:(0.72903752327)\n",
      " state (14)  A[0]:(0.809984087944) A[1]:(0.900201737881) A[2]:(0.999999582767) A[3]:(0.810072183609)\n",
      " state (15)  A[0]:(0.979568004608) A[1]:(0.941212117672) A[2]:(1.0) A[3]:(0.87833327055)\n",
      "Episode 473000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6066. Times reached goal: 975.               Steps done: 3493458. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0273560514317.\n",
      " state (0)  A[0]:(0.531371712685) A[1]:(0.590660631657) A[2]:(0.590369403362) A[3]:(0.531324028969)\n",
      " state (1)  A[0]:(0.531338751316) A[1]:(-2.22325325012e-05) A[2]:(0.656085371971) A[3]:(0.59039503336)\n",
      " state (2)  A[0]:(0.59043687582) A[1]:(0.728938221931) A[2]:(0.590552508831) A[3]:(0.656025290489)\n",
      " state (3)  A[0]:(0.656239509583) A[1]:(-0.00655147014186) A[2]:(0.507781624794) A[3]:(0.558477878571)\n",
      " state (4)  A[0]:(0.590460956097) A[1]:(0.65610563755) A[2]:(2.63452529907e-05) A[3]:(0.531377911568)\n",
      " state (5)  A[0]:(-0.0406668782234) A[1]:(0.999899148941) A[2]:(-0.791205644608) A[3]:(0.62133795023)\n",
      " state (6)  A[0]:(0.000137254595757) A[1]:(0.809985399246) A[2]:(2.24113464355e-05) A[3]:(0.656091094017)\n",
      " state (7)  A[0]:(0.555029690266) A[1]:(-0.550415635109) A[2]:(0.419656723738) A[3]:(0.878965973854)\n",
      " state (8)  A[0]:(0.656085252762) A[1]:(7.55190849304e-05) A[2]:(0.729030549526) A[3]:(0.590593099594)\n",
      " state (9)  A[0]:(0.656082928181) A[1]:(0.810060977936) A[2]:(0.809991955757) A[3]:(1.01327896118e-06)\n",
      " state (10)  A[0]:(0.729103088379) A[1]:(0.900000214577) A[2]:(-0.000173449516296) A[3]:(0.72890818119)\n",
      " state (11)  A[0]:(0.126326560974) A[1]:(0.881269216537) A[2]:(-0.920232176781) A[3]:(0.800149381161)\n",
      " state (12)  A[0]:(-0.430973321199) A[1]:(0.813863515854) A[2]:(-0.940710306168) A[3]:(0.713755130768)\n",
      " state (13)  A[0]:(-0.000110730528831) A[1]:(0.809344351292) A[2]:(0.900003731251) A[3]:(0.728989541531)\n",
      " state (14)  A[0]:(0.810130000114) A[1]:(0.900130391121) A[2]:(0.999999582767) A[3]:(0.810006201267)\n",
      " state (15)  A[0]:(0.979590713978) A[1]:(0.94117641449) A[2]:(1.0) A[3]:(0.878266215324)\n",
      "Episode 474000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6065. Times reached goal: 973.               Steps done: 3499523. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0271906390998.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5907,  0.5907,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5316, -0.0000,  0.6563,  0.5907]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7290,  0.5908,  0.6563]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0002,  0.8099, -0.0000,  0.6563]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7287,  0.9000, -0.0005,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531785547733) A[1]:(0.590645670891) A[2]:(0.590687870979) A[3]:(0.531776785851)\n",
      " state (1)  A[0]:(0.531635046005) A[1]:(-0.000150427222252) A[2]:(0.656277418137) A[3]:(0.590801477432)\n",
      " state (2)  A[0]:(0.590571522713) A[1]:(0.728915572166) A[2]:(0.590903401375) A[3]:(0.656367242336)\n",
      " state (3)  A[0]:(0.656408250332) A[1]:(-0.00672090379521) A[2]:(0.50807338953) A[3]:(0.558802723885)\n",
      " state (4)  A[0]:(0.590590596199) A[1]:(0.656001269817) A[2]:(0.000169277191162) A[3]:(0.531745553017)\n",
      " state (5)  A[0]:(-0.0410244613886) A[1]:(0.999899148941) A[2]:(-0.791316092014) A[3]:(0.621823072433)\n",
      " state (6)  A[0]:(-0.0001050978899) A[1]:(0.809976875782) A[2]:(0.000124096870422) A[3]:(0.656392037868)\n",
      " state (7)  A[0]:(0.554828643799) A[1]:(-0.550705552101) A[2]:(0.419941812754) A[3]:(0.879054546356)\n",
      " state (8)  A[0]:(0.655844807625) A[1]:(-0.000571951211896) A[2]:(0.729085445404) A[3]:(0.590708971024)\n",
      " state (9)  A[0]:(0.655618667603) A[1]:(0.809856653214) A[2]:(0.809973716736) A[3]:(-0.000423788995249)\n",
      " state (10)  A[0]:(0.728668570518) A[1]:(0.899913728237) A[2]:(-0.000534295977559) A[3]:(0.728834867477)\n",
      " state (11)  A[0]:(0.125461265445) A[1]:(0.881219267845) A[2]:(-0.920411646366) A[3]:(0.800217866898)\n",
      " state (12)  A[0]:(-0.431640684605) A[1]:(0.813851118088) A[2]:(-0.940911531448) A[3]:(0.713904857635)\n",
      " state (13)  A[0]:(-0.00083206576528) A[1]:(0.809388756752) A[2]:(0.899968028069) A[3]:(0.729102611542)\n",
      " state (14)  A[0]:(0.809930324554) A[1]:(0.900194048882) A[2]:(0.999999582767) A[3]:(0.810013353825)\n",
      " state (15)  A[0]:(0.979576766491) A[1]:(0.941197931767) A[2]:(1.0) A[3]:(0.878227412701)\n",
      "Episode 475000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6060. Times reached goal: 967.               Steps done: 3505583. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0270263620889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531647086143) A[1]:(0.590373754501) A[2]:(0.590479850769) A[3]:(0.531388342381)\n",
      " state (1)  A[0]:(0.531648993492) A[1]:(-3.90559434891e-05) A[2]:(0.656037867069) A[3]:(0.590546607971)\n",
      " state (2)  A[0]:(0.59061819315) A[1]:(0.729143619537) A[2]:(0.590297400951) A[3]:(0.656142473221)\n",
      " state (3)  A[0]:(0.656357645988) A[1]:(-0.00585592538118) A[2]:(0.507446944714) A[3]:(0.558530092239)\n",
      " state (4)  A[0]:(0.59056699276) A[1]:(0.656136989594) A[2]:(-0.000290989875793) A[3]:(0.531591773033)\n",
      " state (5)  A[0]:(-0.0407893434167) A[1]:(0.999899089336) A[2]:(-0.791330695152) A[3]:(0.622057557106)\n",
      " state (6)  A[0]:(0.000501006783452) A[1]:(0.809882760048) A[2]:(-8.69035720825e-05) A[3]:(0.656388342381)\n",
      " state (7)  A[0]:(0.555351376534) A[1]:(-0.550364017487) A[2]:(0.419524222612) A[3]:(0.879026949406)\n",
      " state (8)  A[0]:(0.656504869461) A[1]:(-6.57588243484e-05) A[2]:(0.728844165802) A[3]:(0.591122329235)\n",
      " state (9)  A[0]:(0.656625628471) A[1]:(0.809995174408) A[2]:(0.809839189053) A[3]:(0.00127366115339)\n",
      " state (10)  A[0]:(0.729536652565) A[1]:(0.899992763996) A[2]:(-0.000630378664937) A[3]:(0.729606747627)\n",
      " state (11)  A[0]:(0.127143353224) A[1]:(0.881301045418) A[2]:(-0.920489668846) A[3]:(0.8006311059)\n",
      " state (12)  A[0]:(-0.43052136898) A[1]:(0.813894212246) A[2]:(-0.941056072712) A[3]:(0.714179873466)\n",
      " state (13)  A[0]:(0.000385820836527) A[1]:(0.809314489365) A[2]:(0.899941504002) A[3]:(0.729226529598)\n",
      " state (14)  A[0]:(0.810380339622) A[1]:(0.900109171867) A[2]:(0.999999582767) A[3]:(0.810086488724)\n",
      " state (15)  A[0]:(0.979638457298) A[1]:(0.941103458405) A[2]:(1.0) A[3]:(0.878289282322)\n",
      "Episode 476000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6125. Times reached goal: 987.               Steps done: 3511708. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0268613315431.\n",
      " state (0)  A[0]:(0.531365036964) A[1]:(0.590446352959) A[2]:(0.59040927887) A[3]:(0.531375348568)\n",
      " state (1)  A[0]:(0.531312942505) A[1]:(0.000116750597954) A[2]:(0.656084418297) A[3]:(0.590431690216)\n",
      " state (2)  A[0]:(0.590381681919) A[1]:(0.729012966156) A[2]:(0.590511798859) A[3]:(0.656060755253)\n",
      " state (3)  A[0]:(0.656111836433) A[1]:(-0.00607395172119) A[2]:(0.507679581642) A[3]:(0.558333694935)\n",
      " state (4)  A[0]:(0.590265154839) A[1]:(0.656113684177) A[2]:(-2.15768814087e-05) A[3]:(0.531371712685)\n",
      " state (5)  A[0]:(-0.0412254333496) A[1]:(0.999899208546) A[2]:(-0.791308581829) A[3]:(0.621984183788)\n",
      " state (6)  A[0]:(-2.84165143967e-05) A[1]:(0.810080111027) A[2]:(1.93119049072e-05) A[3]:(0.656046390533)\n",
      " state (7)  A[0]:(0.554766178131) A[1]:(-0.549979090691) A[2]:(0.419693261385) A[3]:(0.878763616085)\n",
      " state (8)  A[0]:(0.655889391899) A[1]:(0.000129863619804) A[2]:(0.728963255882) A[3]:(0.590388000011)\n",
      " state (9)  A[0]:(0.655892729759) A[1]:(0.810024499893) A[2]:(0.810016393661) A[3]:(-0.000154674053192)\n",
      " state (10)  A[0]:(0.729159832001) A[1]:(0.900039672852) A[2]:(-4.76837158203e-05) A[3]:(0.728945195675)\n",
      " state (11)  A[0]:(0.127070859075) A[1]:(0.881415128708) A[2]:(-0.920493304729) A[3]:(0.800245165825)\n",
      " state (12)  A[0]:(-0.43041151762) A[1]:(0.814106225967) A[2]:(-0.94114279747) A[3]:(0.713758647442)\n",
      " state (13)  A[0]:(0.000159576535225) A[1]:(0.809533774853) A[2]:(0.900041460991) A[3]:(0.728924751282)\n",
      " state (14)  A[0]:(0.810052096844) A[1]:(0.900239050388) A[2]:(0.999999582767) A[3]:(0.809945702553)\n",
      " state (15)  A[0]:(0.979577183723) A[1]:(0.941154718399) A[2]:(1.0) A[3]:(0.878247022629)\n",
      "Episode 477000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6096. Times reached goal: 973.               Steps done: 3517804. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0266980829532.\n",
      " state (0)  A[0]:(0.531512081623) A[1]:(0.590491890907) A[2]:(0.590479910374) A[3]:(0.53158146143)\n",
      " state (1)  A[0]:(0.531455397606) A[1]:(-8.60393047333e-05) A[2]:(0.656086623669) A[3]:(0.590551257133)\n",
      " state (2)  A[0]:(0.590302109718) A[1]:(0.729018330574) A[2]:(0.590812683105) A[3]:(0.656090378761)\n",
      " state (3)  A[0]:(0.656090855598) A[1]:(-0.00614489335567) A[2]:(0.50801050663) A[3]:(0.558369934559)\n",
      " state (4)  A[0]:(0.590235650539) A[1]:(0.656187057495) A[2]:(0.000141501426697) A[3]:(0.531447887421)\n",
      " state (5)  A[0]:(-0.0415270254016) A[1]:(0.999899148941) A[2]:(-0.791474819183) A[3]:(0.621989488602)\n",
      " state (6)  A[0]:(-0.00054554635426) A[1]:(0.809992194176) A[2]:(0.000166535377502) A[3]:(0.65593200922)\n",
      " state (7)  A[0]:(0.554369270802) A[1]:(-0.550250351429) A[2]:(0.420149058104) A[3]:(0.878726959229)\n",
      " state (8)  A[0]:(0.655461192131) A[1]:(-4.16934490204e-05) A[2]:(0.729168772697) A[3]:(0.589921236038)\n",
      " state (9)  A[0]:(0.655315160751) A[1]:(0.80997800827) A[2]:(0.810027360916) A[3]:(-0.00122874916997)\n",
      " state (10)  A[0]:(0.728474497795) A[1]:(0.89997535944) A[2]:(-0.000408053369028) A[3]:(0.728506803513)\n",
      " state (11)  A[0]:(0.125337198377) A[1]:(0.881316065788) A[2]:(-0.920655846596) A[3]:(0.799993097782)\n",
      " state (12)  A[0]:(-0.431691527367) A[1]:(0.813953995705) A[2]:(-0.941305160522) A[3]:(0.713580012321)\n",
      " state (13)  A[0]:(-0.000903218751773) A[1]:(0.809429526329) A[2]:(0.900072693825) A[3]:(0.728848934174)\n",
      " state (14)  A[0]:(0.809895515442) A[1]:(0.900253593922) A[2]:(0.999999642372) A[3]:(0.809873104095)\n",
      " state (15)  A[0]:(0.979577720165) A[1]:(0.941178917885) A[2]:(1.0) A[3]:(0.878172397614)\n",
      "Episode 478000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6035. Times reached goal: 968.               Steps done: 3523839. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0265374452345.\n",
      " state (0)  A[0]:(0.531603455544) A[1]:(0.590624451637) A[2]:(0.590502202511) A[3]:(0.531658828259)\n",
      " state (1)  A[0]:(0.531494140625) A[1]:(-7.75456428528e-05) A[2]:(0.656202077866) A[3]:(0.590685009956)\n",
      " state (2)  A[0]:(0.590369403362) A[1]:(0.729040801525) A[2]:(0.59081619978) A[3]:(0.656297624111)\n",
      " state (3)  A[0]:(0.656142771244) A[1]:(-0.00598643999547) A[2]:(0.507954597473) A[3]:(0.558561205864)\n",
      " state (4)  A[0]:(0.590367674828) A[1]:(0.656090974808) A[2]:(0.000126123428345) A[3]:(0.531681239605)\n",
      " state (5)  A[0]:(-0.0412337966263) A[1]:(0.999899148941) A[2]:(-0.791472554207) A[3]:(0.622358679771)\n",
      " state (6)  A[0]:(-0.000350520014763) A[1]:(0.809984385967) A[2]:(0.000234365463257) A[3]:(0.656073212624)\n",
      " state (7)  A[0]:(0.55456507206) A[1]:(-0.550174057484) A[2]:(0.420197159052) A[3]:(0.878796696663)\n",
      " state (8)  A[0]:(0.656015813351) A[1]:(-4.58359718323e-05) A[2]:(0.729125380516) A[3]:(0.590760231018)\n",
      " state (9)  A[0]:(0.655975461006) A[1]:(0.810040593147) A[2]:(0.810079097748) A[3]:(-6.04391098022e-05)\n",
      " state (10)  A[0]:(0.728977382183) A[1]:(0.900002241135) A[2]:(-3.68356704712e-05) A[3]:(0.728935539722)\n",
      " state (11)  A[0]:(0.126329988241) A[1]:(0.881318688393) A[2]:(-0.920657396317) A[3]:(0.800273656845)\n",
      " state (12)  A[0]:(-0.430984199047) A[1]:(0.813888728619) A[2]:(-0.94138199091) A[3]:(0.713890552521)\n",
      " state (13)  A[0]:(-0.000199869275093) A[1]:(0.8092892766) A[2]:(0.90005928278) A[3]:(0.729117870331)\n",
      " state (14)  A[0]:(0.810061812401) A[1]:(0.900173544884) A[2]:(0.999999642372) A[3]:(0.810061693192)\n",
      " state (15)  A[0]:(0.979589641094) A[1]:(0.941127419472) A[2]:(1.0) A[3]:(0.878292381763)\n",
      "Episode 479000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6053. Times reached goal: 973.               Steps done: 3529892. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0263772992493.\n",
      "q_values \n",
      "tensor([[ 0.5322,  0.5907,  0.5908,  0.5296]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.0014,  0.6556,  0.5892]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.7289,  0.5898,  0.6549]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  0.8103, -0.0004,  0.6570]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7341,  0.9000,  0.0020,  0.7340]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8130,  0.8995,  1.0000,  0.8109]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531554222107) A[1]:(0.590480566025) A[2]:(0.590352773666) A[3]:(0.529979944229)\n",
      " state (1)  A[0]:(0.531324028969) A[1]:(-4.1127204895e-06) A[2]:(0.655819892883) A[3]:(0.588929891586)\n",
      " state (2)  A[0]:(0.590359210968) A[1]:(0.729066371918) A[2]:(0.59013068676) A[3]:(0.654621243477)\n",
      " state (3)  A[0]:(0.656018972397) A[1]:(-0.00564455939457) A[2]:(0.507220447063) A[3]:(0.556394815445)\n",
      " state (4)  A[0]:(0.590028822422) A[1]:(0.656184792519) A[2]:(-0.00056052202126) A[3]:(0.529579162598)\n",
      " state (5)  A[0]:(-0.0424582958221) A[1]:(0.999899148941) A[2]:(-0.791580975056) A[3]:(0.62132024765)\n",
      " state (6)  A[0]:(-0.00162008265033) A[1]:(0.810013830662) A[2]:(-0.000327229499817) A[3]:(0.654971718788)\n",
      " state (7)  A[0]:(0.553468823433) A[1]:(-0.549738645554) A[2]:(0.419598668814) A[3]:(0.878110527992)\n",
      " state (8)  A[0]:(0.65473985672) A[1]:(0.000456511945231) A[2]:(0.729005634785) A[3]:(0.587752759457)\n",
      " state (9)  A[0]:(0.655501484871) A[1]:(0.809834063053) A[2]:(0.80994451046) A[3]:(-0.00291838403791)\n",
      " state (10)  A[0]:(0.729530215263) A[1]:(0.899791240692) A[2]:(-0.000189542770386) A[3]:(0.72803914547)\n",
      " state (11)  A[0]:(0.129871547222) A[1]:(0.881007730961) A[2]:(-0.920687377453) A[3]:(0.799826562405)\n",
      " state (12)  A[0]:(-0.426359862089) A[1]:(0.81324917078) A[2]:(-0.94145655632) A[3]:(0.713579416275)\n",
      " state (13)  A[0]:(0.00647711614147) A[1]:(0.808433532715) A[2]:(0.90000551939) A[3]:(0.729082524776)\n",
      " state (14)  A[0]:(0.812292039394) A[1]:(0.899668157101) A[2]:(0.999999642372) A[3]:(0.810175895691)\n",
      " state (15)  A[0]:(0.979818701744) A[1]:(0.94084405899) A[2]:(1.0) A[3]:(0.878407180309)\n",
      "Episode 480000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6069. Times reached goal: 976.               Steps done: 3535961. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0262177002133.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531344294548) A[1]:(0.590575814247) A[2]:(0.590509176254) A[3]:(0.531330049038)\n",
      " state (1)  A[0]:(0.531370520592) A[1]:(1.01327896118e-06) A[2]:(0.656113028526) A[3]:(0.590427815914)\n",
      " state (2)  A[0]:(0.590441465378) A[1]:(0.728985249996) A[2]:(0.590594470501) A[3]:(0.656019747257)\n",
      " state (3)  A[0]:(0.656188845634) A[1]:(-0.00599347334355) A[2]:(0.507777988911) A[3]:(0.558090150356)\n",
      " state (4)  A[0]:(0.590377807617) A[1]:(0.656108319759) A[2]:(1.65700912476e-05) A[3]:(0.531306147575)\n",
      " state (5)  A[0]:(-0.0413200743496) A[1]:(0.999899148941) A[2]:(-0.791538774967) A[3]:(0.622596621513)\n",
      " state (6)  A[0]:(-0.000163242220879) A[1]:(0.810002088547) A[2]:(0.000197529792786) A[3]:(0.656051039696)\n",
      " state (7)  A[0]:(0.554563760757) A[1]:(-0.550089895725) A[2]:(0.420301884413) A[3]:(0.87865871191)\n",
      " state (8)  A[0]:(0.655936837196) A[1]:(-0.000152766704559) A[2]:(0.729156017303) A[3]:(0.590352892876)\n",
      " state (9)  A[0]:(0.65593457222) A[1]:(0.809944450855) A[2]:(0.810068845749) A[3]:(-0.00044202801655)\n",
      " state (10)  A[0]:(0.729002416134) A[1]:(0.899984061718) A[2]:(-0.000173449516296) A[3]:(0.728754818439)\n",
      " state (11)  A[0]:(0.126540109515) A[1]:(0.881362736225) A[2]:(-0.920838773251) A[3]:(0.800133705139)\n",
      " state (12)  A[0]:(-0.430886358023) A[1]:(0.81397163868) A[2]:(-0.941628754139) A[3]:(0.713664412498)\n",
      " state (13)  A[0]:(-0.000201418995857) A[1]:(0.809316754341) A[2]:(0.90014231205) A[3]:(0.728954315186)\n",
      " state (14)  A[0]:(0.810039520264) A[1]:(0.900149583817) A[2]:(0.999999642372) A[3]:(0.810023903847)\n",
      " state (15)  A[0]:(0.979593873024) A[1]:(0.941028118134) A[2]:(1.0) A[3]:(0.878334760666)\n",
      "Episode 481000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6041. Times reached goal: 967.               Steps done: 3542002. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0260597965146.\n",
      " state (0)  A[0]:(0.531863033772) A[1]:(0.590790987015) A[2]:(0.590374708176) A[3]:(0.531789660454)\n",
      " state (1)  A[0]:(0.53178024292) A[1]:(-7.34627246857e-06) A[2]:(0.656471014023) A[3]:(0.590683162212)\n",
      " state (2)  A[0]:(0.590665102005) A[1]:(0.728431522846) A[2]:(0.591014027596) A[3]:(0.656353116035)\n",
      " state (3)  A[0]:(0.656409740448) A[1]:(-0.00706441886723) A[2]:(0.508191406727) A[3]:(0.558565735817)\n",
      " state (4)  A[0]:(0.5905418396) A[1]:(0.65625333786) A[2]:(0.000174522399902) A[3]:(0.531957149506)\n",
      " state (5)  A[0]:(-0.0411814190447) A[1]:(0.999899208546) A[2]:(-0.791618287563) A[3]:(0.623255133629)\n",
      " state (6)  A[0]:(-5.09470701218e-05) A[1]:(0.810053229332) A[2]:(8.98838043213e-05) A[3]:(0.656482934952)\n",
      " state (7)  A[0]:(0.554596185684) A[1]:(-0.550022900105) A[2]:(0.420293241739) A[3]:(0.878816008568)\n",
      " state (8)  A[0]:(0.656096696854) A[1]:(-0.000130370259285) A[2]:(0.729097783566) A[3]:(0.591030061245)\n",
      " state (9)  A[0]:(0.656025886536) A[1]:(0.810002207756) A[2]:(0.810077786446) A[3]:(0.000167429447174)\n",
      " state (10)  A[0]:(0.728998363018) A[1]:(0.9000172019) A[2]:(2.55107879639e-05) A[3]:(0.728925585747)\n",
      " state (11)  A[0]:(0.126375675201) A[1]:(0.881413519382) A[2]:(-0.920871078968) A[3]:(0.800232231617)\n",
      " state (12)  A[0]:(-0.431271851063) A[1]:(0.814066290855) A[2]:(-0.941728413105) A[3]:(0.71372371912)\n",
      " state (13)  A[0]:(-0.000900268321857) A[1]:(0.809440374374) A[2]:(0.900126755238) A[3]:(0.72897040844)\n",
      " state (14)  A[0]:(0.809781908989) A[1]:(0.900259017944) A[2]:(0.999999642372) A[3]:(0.810022115707)\n",
      " state (15)  A[0]:(0.979568362236) A[1]:(0.941095888615) A[2]:(1.0) A[3]:(0.878332853317)\n",
      "Episode 482000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6060. Times reached goal: 977.               Steps done: 3548062. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0259023516874.\n",
      " state (0)  A[0]:(0.531307220459) A[1]:(0.590366721153) A[2]:(0.590529859066) A[3]:(0.53176343441)\n",
      " state (1)  A[0]:(0.531408309937) A[1]:(0.000540465058293) A[2]:(0.656061708927) A[3]:(0.590596437454)\n",
      " state (2)  A[0]:(0.590446352959) A[1]:(0.728980541229) A[2]:(0.590598106384) A[3]:(0.656104683876)\n",
      " state (3)  A[0]:(0.656170845032) A[1]:(-0.00570246344432) A[2]:(0.507820665836) A[3]:(0.558168768883)\n",
      " state (4)  A[0]:(0.59035295248) A[1]:(0.656234145164) A[2]:(0.000102758407593) A[3]:(0.531471252441)\n",
      " state (5)  A[0]:(-0.0414011627436) A[1]:(0.99989926815) A[2]:(-0.791640460491) A[3]:(0.6229377985)\n",
      " state (6)  A[0]:(-0.000158339738846) A[1]:(0.810130238533) A[2]:(-3.19480895996e-05) A[3]:(0.655992507935)\n",
      " state (7)  A[0]:(0.554396271706) A[1]:(-0.550123214722) A[2]:(0.420271843672) A[3]:(0.878516495228)\n",
      " state (8)  A[0]:(0.655854225159) A[1]:(-0.000245600938797) A[2]:(0.72910618782) A[3]:(0.589997291565)\n",
      " state (9)  A[0]:(0.65604698658) A[1]:(0.809933543205) A[2]:(0.810058236122) A[3]:(-0.000553369463887)\n",
      " state (10)  A[0]:(0.729129195213) A[1]:(0.899969696999) A[2]:(0.000168919563293) A[3]:(0.728798747063)\n",
      " state (11)  A[0]:(0.126566872001) A[1]:(0.881373941898) A[2]:(-0.920907318592) A[3]:(0.80015194416)\n",
      " state (12)  A[0]:(-0.431547880173) A[1]:(0.814005613327) A[2]:(-0.94184333086) A[3]:(0.713449895382)\n",
      " state (13)  A[0]:(-0.00159457186237) A[1]:(0.809354543686) A[2]:(0.900087714195) A[3]:(0.728561401367)\n",
      " state (14)  A[0]:(0.809563577175) A[1]:(0.90020686388) A[2]:(0.999999642372) A[3]:(0.809659779072)\n",
      " state (15)  A[0]:(0.979554116726) A[1]:(0.9410353899) A[2]:(1.0) A[3]:(0.878077626228)\n",
      "Episode 483000 finished after 0 timesteps with r=0.0. Running score: 0.97. Times trained:               6052. Times reached goal: 972.               Steps done: 3554114. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0257460640584.\n",
      " state (0)  A[0]:(0.531153321266) A[1]:(0.590499699116) A[2]:(0.590507149696) A[3]:(0.5311383605)\n",
      " state (1)  A[0]:(0.530987858772) A[1]:(1.81049108505e-05) A[2]:(0.656485676765) A[3]:(0.590077280998)\n",
      " state (2)  A[0]:(0.590110301971) A[1]:(0.729222416878) A[2]:(0.590913414955) A[3]:(0.655690550804)\n",
      " state (3)  A[0]:(0.655867576599) A[1]:(-0.00523969205096) A[2]:(0.508096575737) A[3]:(0.55749976635)\n",
      " state (4)  A[0]:(0.589929938316) A[1]:(0.656582713127) A[2]:(0.000357031793101) A[3]:(0.530720710754)\n",
      " state (5)  A[0]:(-0.0423975028098) A[1]:(0.99989938736) A[2]:(-0.791568517685) A[3]:(0.622474312782)\n",
      " state (6)  A[0]:(-0.00113874627277) A[1]:(0.810395300388) A[2]:(0.000715613248758) A[3]:(0.655639648438)\n",
      " state (7)  A[0]:(0.553872942924) A[1]:(-0.549430966377) A[2]:(0.421183675528) A[3]:(0.878440260887)\n",
      " state (8)  A[0]:(0.655708432198) A[1]:(0.000937923497986) A[2]:(0.729599833488) A[3]:(0.590032637119)\n",
      " state (9)  A[0]:(0.65586078167) A[1]:(0.810429334641) A[2]:(0.810380101204) A[3]:(-0.00119358242955)\n",
      " state (10)  A[0]:(0.728867888451) A[1]:(0.900184929371) A[2]:(0.000543117464986) A[3]:(0.728274524212)\n",
      " state (11)  A[0]:(0.12637399137) A[1]:(0.881501495838) A[2]:(-0.920980215073) A[3]:(0.799719929695)\n",
      " state (12)  A[0]:(-0.430731803179) A[1]:(0.814000189304) A[2]:(-0.941940963268) A[3]:(0.713115215302)\n",
      " state (13)  A[0]:(0.000397056312067) A[1]:(0.809198558331) A[2]:(0.900111436844) A[3]:(0.728529930115)\n",
      " state (14)  A[0]:(0.810336351395) A[1]:(0.900114059448) A[2]:(0.999999642372) A[3]:(0.809755086899)\n",
      " state (15)  A[0]:(0.979635119438) A[1]:(0.940989434719) A[2]:(1.0) A[3]:(0.878171920776)\n",
      "Episode 484000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6052. Times reached goal: 969.               Steps done: 3560166. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0255907194257.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5905,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.6561,  0.0001,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 6.5615e-01, -4.4256e-06,  7.2910e-01,  5.9070e-01]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6562,  0.8100,  0.8101,  0.0005]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7294,  0.9000,  0.0002,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531366109848) A[1]:(0.590515136719) A[2]:(0.590494632721) A[3]:(0.531393527985)\n",
      " state (1)  A[0]:(0.531332314014) A[1]:(2.95042991638e-06) A[2]:(0.656078398228) A[3]:(0.590463042259)\n",
      " state (2)  A[0]:(0.59026825428) A[1]:(0.729114949703) A[2]:(0.590533554554) A[3]:(0.656000256538)\n",
      " state (3)  A[0]:(0.655991971493) A[1]:(-0.00534639647231) A[2]:(0.507718205452) A[3]:(0.557925581932)\n",
      " state (4)  A[0]:(0.590214729309) A[1]:(0.656127631664) A[2]:(2.26497650146e-05) A[3]:(0.531295180321)\n",
      " state (5)  A[0]:(-0.0415879674256) A[1]:(0.999899148941) A[2]:(-0.791710436344) A[3]:(0.623152375221)\n",
      " state (6)  A[0]:(-0.000301733613014) A[1]:(0.810002565384) A[2]:(0.000112056732178) A[3]:(0.656069397926)\n",
      " state (7)  A[0]:(0.554271996021) A[1]:(-0.54987347126) A[2]:(0.42031750083) A[3]:(0.878537476063)\n",
      " state (8)  A[0]:(0.655728578568) A[1]:(-7.33435153961e-05) A[2]:(0.729032814503) A[3]:(0.590405404568)\n",
      " state (9)  A[0]:(0.655809640884) A[1]:(0.809976696968) A[2]:(0.810032010078) A[3]:(3.15606594086e-05)\n",
      " state (10)  A[0]:(0.729137659073) A[1]:(0.900001823902) A[2]:(0.000128388404846) A[3]:(0.729131698608)\n",
      " state (11)  A[0]:(0.127361372113) A[1]:(0.881434679031) A[2]:(-0.921062886715) A[3]:(0.800488054752)\n",
      " state (12)  A[0]:(-0.430509358644) A[1]:(0.814077973366) A[2]:(-0.942094504833) A[3]:(0.713917613029)\n",
      " state (13)  A[0]:(-0.000231295824051) A[1]:(0.80939656496) A[2]:(0.900046169758) A[3]:(0.729010462761)\n",
      " state (14)  A[0]:(0.809961736202) A[1]:(0.900268018246) A[2]:(0.999999642372) A[3]:(0.809974431992)\n",
      " state (15)  A[0]:(0.979590773582) A[1]:(0.941060960293) A[2]:(1.0) A[3]:(0.878274738789)\n",
      "Episode 485000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6070. Times reached goal: 977.               Steps done: 3566236. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0254358542501.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531474113464) A[1]:(0.59080851078) A[2]:(0.590183436871) A[3]:(0.531534790993)\n",
      " state (1)  A[0]:(0.531396150589) A[1]:(-0.000186130404472) A[2]:(0.656453490257) A[3]:(0.590465009212)\n",
      " state (2)  A[0]:(0.590431272984) A[1]:(0.729233801365) A[2]:(0.590803980827) A[3]:(0.656324148178)\n",
      " state (3)  A[0]:(0.656322538853) A[1]:(-0.00512754870579) A[2]:(0.507845401764) A[3]:(0.558413267136)\n",
      " state (4)  A[0]:(0.590770483017) A[1]:(0.656069397926) A[2]:(-2.27689743042e-05) A[3]:(0.532035410404)\n",
      " state (5)  A[0]:(-0.0409297607839) A[1]:(0.999899208546) A[2]:(-0.791848182678) A[3]:(0.624092817307)\n",
      " state (6)  A[0]:(0.000331118702888) A[1]:(0.81016600132) A[2]:(-0.000518560351338) A[3]:(0.656718194485)\n",
      " state (7)  A[0]:(0.554585814476) A[1]:(-0.549668550491) A[2]:(0.419431209564) A[3]:(0.878750801086)\n",
      " state (8)  A[0]:(0.656254768372) A[1]:(-0.00122849585023) A[2]:(0.728567242622) A[3]:(0.59195458889)\n",
      " state (9)  A[0]:(0.65614938736) A[1]:(0.809524059296) A[2]:(0.81014329195) A[3]:(0.00181090633851)\n",
      " state (10)  A[0]:(0.729610919952) A[1]:(0.900017440319) A[2]:(0.00107443286106) A[3]:(0.730119287968)\n",
      " state (11)  A[0]:(0.128629699349) A[1]:(0.881876647472) A[2]:(-0.920985102654) A[3]:(0.801503181458)\n",
      " state (12)  A[0]:(-0.430330872536) A[1]:(0.815279126167) A[2]:(-0.94215798378) A[3]:(0.715144634247)\n",
      " state (13)  A[0]:(-0.00127948750742) A[1]:(0.810951113701) A[2]:(0.900091946125) A[3]:(0.729824662209)\n",
      " state (14)  A[0]:(0.809395313263) A[1]:(0.901193082333) A[2]:(0.999999642372) A[3]:(0.810320734978)\n",
      " state (15)  A[0]:(0.979535162449) A[1]:(0.941587150097) A[2]:(1.0) A[3]:(0.878394305706)\n",
      "Episode 486000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6033. Times reached goal: 969.               Steps done: 3572269. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0252828617074.\n",
      " state (0)  A[0]:(0.531573534012) A[1]:(0.590687155724) A[2]:(0.590674221516) A[3]:(0.531582295895)\n",
      " state (1)  A[0]:(0.531639873981) A[1]:(0.000131174921989) A[2]:(0.656285047531) A[3]:(0.590723276138)\n",
      " state (2)  A[0]:(0.590780794621) A[1]:(0.729032278061) A[2]:(0.59073472023) A[3]:(0.656303405762)\n",
      " state (3)  A[0]:(0.6564925313) A[1]:(-0.00556649500504) A[2]:(0.507874429226) A[3]:(0.558121919632)\n",
      " state (4)  A[0]:(0.590611636639) A[1]:(0.656390786171) A[2]:(-3.26633453369e-05) A[3]:(0.531523942947)\n",
      " state (5)  A[0]:(-0.0416662283242) A[1]:(0.999899208546) A[2]:(-0.79181855917) A[3]:(0.623638927937)\n",
      " state (6)  A[0]:(5.55068254471e-05) A[1]:(0.810025155544) A[2]:(0.000137209892273) A[3]:(0.65608716011)\n",
      " state (7)  A[0]:(0.554518342018) A[1]:(-0.549821615219) A[2]:(0.420552819967) A[3]:(0.878418982029)\n",
      " state (8)  A[0]:(0.655976653099) A[1]:(1.60187482834e-05) A[2]:(0.729124426842) A[3]:(0.589866161346)\n",
      " state (9)  A[0]:(0.655994415283) A[1]:(0.810023665428) A[2]:(0.81003767252) A[3]:(-0.00118380726781)\n",
      " state (10)  A[0]:(0.729031562805) A[1]:(0.900009870529) A[2]:(-0.000219821929932) A[3]:(0.728508353233)\n",
      " state (11)  A[0]:(0.126791626215) A[1]:(0.881407380104) A[2]:(-0.921300709248) A[3]:(0.800060272217)\n",
      " state (12)  A[0]:(-0.43091994524) A[1]:(0.813925921917) A[2]:(-0.94238448143) A[3]:(0.713502168655)\n",
      " state (13)  A[0]:(-0.000394657225115) A[1]:(0.809117436409) A[2]:(0.89998292923) A[3]:(0.728816151619)\n",
      " state (14)  A[0]:(0.810030937195) A[1]:(0.900086641312) A[2]:(0.999999642372) A[3]:(0.809944927692)\n",
      " state (15)  A[0]:(0.979613006115) A[1]:(0.940892219543) A[2]:(1.0) A[3]:(0.878314673901)\n",
      "Episode 487000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6049. Times reached goal: 969.               Steps done: 3578318. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0251303873007.\n",
      " state (0)  A[0]:(0.531491041183) A[1]:(0.590707063675) A[2]:(0.590583443642) A[3]:(0.531468868256)\n",
      " state (1)  A[0]:(0.531345009804) A[1]:(7.58469104767e-06) A[2]:(0.656107664108) A[3]:(0.590598404408)\n",
      " state (2)  A[0]:(0.590575516224) A[1]:(0.728981494904) A[2]:(0.590509235859) A[3]:(0.65617698431)\n",
      " state (3)  A[0]:(0.656307935715) A[1]:(-0.00540804071352) A[2]:(0.50768917799) A[3]:(0.558001041412)\n",
      " state (4)  A[0]:(0.590532839298) A[1]:(0.65610229969) A[2]:(-7.49826431274e-05) A[3]:(0.531457662582)\n",
      " state (5)  A[0]:(-0.041670139879) A[1]:(0.999899089336) A[2]:(-0.791884899139) A[3]:(0.623700857162)\n",
      " state (6)  A[0]:(-0.000169321894646) A[1]:(0.809914112091) A[2]:(5.20944595337e-05) A[3]:(0.656283140182)\n",
      " state (7)  A[0]:(0.554487705231) A[1]:(-0.549568116665) A[2]:(0.420446246862) A[3]:(0.878600478172)\n",
      " state (8)  A[0]:(0.656089842319) A[1]:(0.000862076645717) A[2]:(0.72906845808) A[3]:(0.59056532383)\n",
      " state (9)  A[0]:(0.656178534031) A[1]:(0.810351669788) A[2]:(0.810017704964) A[3]:(-0.0001400411129)\n",
      " state (10)  A[0]:(0.729198396206) A[1]:(0.900013685226) A[2]:(-9.46521759033e-05) A[3]:(0.728889405727)\n",
      " state (11)  A[0]:(0.127128958702) A[1]:(0.8811622262) A[2]:(-0.921332597733) A[3]:(0.800231635571)\n",
      " state (12)  A[0]:(-0.430606752634) A[1]:(0.813258767128) A[2]:(-0.942471802235) A[3]:(0.713638782501)\n",
      " state (13)  A[0]:(0.00014728307724) A[1]:(0.808312952518) A[2]:(0.89998626709) A[3]:(0.728964447975)\n",
      " state (14)  A[0]:(0.810253381729) A[1]:(0.899699389935) A[2]:(0.999999642372) A[3]:(0.810099959373)\n",
      " state (15)  A[0]:(0.979632377625) A[1]:(0.940697610378) A[2]:(1.0) A[3]:(0.878440916538)\n",
      "Episode 488000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6069. Times reached goal: 974.               Steps done: 3584387. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0249783328561.\n",
      " state (0)  A[0]:(0.531566500664) A[1]:(0.590307176113) A[2]:(0.590556800365) A[3]:(0.53148317337)\n",
      " state (1)  A[0]:(0.531572699547) A[1]:(-2.94297933578e-05) A[2]:(0.655966579914) A[3]:(0.590397715569)\n",
      " state (2)  A[0]:(0.590440273285) A[1]:(0.7289031744) A[2]:(0.59056609869) A[3]:(0.655962944031)\n",
      " state (3)  A[0]:(0.656117618084) A[1]:(-0.00557856447995) A[2]:(0.507765114307) A[3]:(0.557779431343)\n",
      " state (4)  A[0]:(0.590303719044) A[1]:(0.656062722206) A[2]:(-7.22408294678e-05) A[3]:(0.531308412552)\n",
      " state (5)  A[0]:(-0.041731365025) A[1]:(0.999899089336) A[2]:(-0.792008042336) A[3]:(0.623569011688)\n",
      " state (6)  A[0]:(-0.000111266970634) A[1]:(0.809935212135) A[2]:(-0.00015652179718) A[3]:(0.65579777956)\n",
      " state (7)  A[0]:(0.554378330708) A[1]:(-0.549579024315) A[2]:(0.42032122612) A[3]:(0.878307282925)\n",
      " state (8)  A[0]:(0.655877649784) A[1]:(0.000416889757616) A[2]:(0.728892087936) A[3]:(0.590076863766)\n",
      " state (9)  A[0]:(0.655806541443) A[1]:(0.810178518295) A[2]:(0.809878110886) A[3]:(-0.000671505811624)\n",
      " state (10)  A[0]:(0.728990077972) A[1]:(0.90003657341) A[2]:(-0.000466823548777) A[3]:(0.72865664959)\n",
      " state (11)  A[0]:(0.127116784453) A[1]:(0.881375968456) A[2]:(-0.921458363533) A[3]:(0.800131261349)\n",
      " state (12)  A[0]:(-0.430520057678) A[1]:(0.813788473606) A[2]:(-0.942626059055) A[3]:(0.713508725166)\n",
      " state (13)  A[0]:(0.000133946537971) A[1]:(0.808962166309) A[2]:(0.89995777607) A[3]:(0.728764772415)\n",
      " state (14)  A[0]:(0.810197293758) A[1]:(0.900091409683) A[2]:(0.999999642372) A[3]:(0.809853494167)\n",
      " state (15)  A[0]:(0.979626595974) A[1]:(0.940918207169) A[2]:(1.0) A[3]:(0.878224611282)\n",
      "Episode 489000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6089. Times reached goal: 981.               Steps done: 3590476. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0248267018963.\n",
      "q_values \n",
      "tensor([[ 0.5308,  0.5905,  0.5904,  0.5322]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5899,  0.6561,  0.0006,  0.5332]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6574, -0.0017,  0.7289,  0.5949]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6593,  0.8098,  0.8104,  0.0085]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7309,  0.8999,  0.0008,  0.7302]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8109,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53209066391) A[1]:(0.590266942978) A[2]:(0.590550661087) A[3]:(0.529952287674)\n",
      " state (1)  A[0]:(0.532278895378) A[1]:(9.93311405182e-05) A[2]:(0.65598988533) A[3]:(0.588821947575)\n",
      " state (2)  A[0]:(0.59104514122) A[1]:(0.728899478912) A[2]:(0.590506374836) A[3]:(0.654519915581)\n",
      " state (3)  A[0]:(0.656762421131) A[1]:(-0.00543025741354) A[2]:(0.507723629475) A[3]:(0.55597281456)\n",
      " state (4)  A[0]:(0.591273725033) A[1]:(0.656018733978) A[2]:(-3.95774841309e-05) A[3]:(0.529467821121)\n",
      " state (5)  A[0]:(-0.0398298539221) A[1]:(0.99989926815) A[2]:(-0.792111754417) A[3]:(0.622189164162)\n",
      " state (6)  A[0]:(0.00177655927837) A[1]:(0.810367882252) A[2]:(-0.000185608863831) A[3]:(0.654238581657)\n",
      " state (7)  A[0]:(0.555199861526) A[1]:(-0.549090623856) A[2]:(0.420680880547) A[3]:(0.877395272255)\n",
      " state (8)  A[0]:(0.65572053194) A[1]:(0.001199632301) A[2]:(0.729225635529) A[3]:(0.585911631584)\n",
      " state (9)  A[0]:(0.655445218086) A[1]:(0.810268461704) A[2]:(0.809863805771) A[3]:(-0.00652670534328)\n",
      " state (10)  A[0]:(0.728448987007) A[1]:(0.900115311146) A[2]:(-0.00131821550895) A[3]:(0.726191282272)\n",
      " state (11)  A[0]:(0.126160562038) A[1]:(0.88162958622) A[2]:(-0.921652317047) A[3]:(0.798614144325)\n",
      " state (12)  A[0]:(-0.430308997631) A[1]:(0.814428329468) A[2]:(-0.942659497261) A[3]:(0.712171018124)\n",
      " state (13)  A[0]:(0.00153018417768) A[1]:(0.809862494469) A[2]:(0.900553524494) A[3]:(0.728004217148)\n",
      " state (14)  A[0]:(0.810845434666) A[1]:(0.900692105293) A[2]:(0.999999642372) A[3]:(0.809458971024)\n",
      " state (15)  A[0]:(0.979707837105) A[1]:(0.9412779212) A[2]:(1.0) A[3]:(0.878003716469)\n",
      "Episode 490000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6051. Times reached goal: 970.               Steps done: 3596527. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0246769291177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532081961632) A[1]:(0.589985251427) A[2]:(0.590876340866) A[3]:(0.531726956367)\n",
      " state (1)  A[0]:(0.531925916672) A[1]:(0.000197157263756) A[2]:(0.656191468239) A[3]:(0.590633451939)\n",
      " state (2)  A[0]:(0.590703964233) A[1]:(0.72896194458) A[2]:(0.591637253761) A[3]:(0.656264424324)\n",
      " state (3)  A[0]:(0.656368851662) A[1]:(-0.00584978656843) A[2]:(0.509185135365) A[3]:(0.558093428612)\n",
      " state (4)  A[0]:(0.590626120567) A[1]:(0.655649900436) A[2]:(0.00146126642358) A[3]:(0.531747400761)\n",
      " state (5)  A[0]:(-0.0414230749011) A[1]:(0.999898910522) A[2]:(-0.792007565498) A[3]:(0.624139666557)\n",
      " state (6)  A[0]:(1.35600566864e-05) A[1]:(0.809815347195) A[2]:(4.97102737427e-05) A[3]:(0.655780792236)\n",
      " state (7)  A[0]:(0.554224133492) A[1]:(-0.548360705376) A[2]:(0.420056372881) A[3]:(0.878179013729)\n",
      " state (8)  A[0]:(0.655331671238) A[1]:(0.00161026278511) A[2]:(0.728850364685) A[3]:(0.589555740356)\n",
      " state (9)  A[0]:(0.65517771244) A[1]:(0.810193538666) A[2]:(0.809868097305) A[3]:(-0.000376343698008)\n",
      " state (10)  A[0]:(0.728454351425) A[1]:(0.900057375431) A[2]:(-0.000877260928974) A[3]:(0.729011416435)\n",
      " state (11)  A[0]:(0.126229733229) A[1]:(0.881492197514) A[2]:(-0.921670079231) A[3]:(0.800431966782)\n",
      " state (12)  A[0]:(-0.431062251329) A[1]:(0.814014911652) A[2]:(-0.942813158035) A[3]:(0.713870406151)\n",
      " state (13)  A[0]:(-0.000655948999338) A[1]:(0.809141993523) A[2]:(0.900319814682) A[3]:(0.729025602341)\n",
      " state (14)  A[0]:(0.80973803997) A[1]:(0.900114178658) A[2]:(0.999999642372) A[3]:(0.809928953648)\n",
      " state (15)  A[0]:(0.979555845261) A[1]:(0.940808832645) A[2]:(1.0) A[3]:(0.878217995167)\n",
      "Episode 491000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6070. Times reached goal: 972.               Steps done: 3602597. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.024527593849.\n",
      " state (0)  A[0]:(0.531566262245) A[1]:(0.590308129787) A[2]:(0.590552985668) A[3]:(0.531352043152)\n",
      " state (1)  A[0]:(0.531535029411) A[1]:(0.000122413039207) A[2]:(0.656117558479) A[3]:(0.590293049812)\n",
      " state (2)  A[0]:(0.590329051018) A[1]:(0.728889167309) A[2]:(0.591798603535) A[3]:(0.655992865562)\n",
      " state (3)  A[0]:(0.656237483025) A[1]:(-0.00874891877174) A[2]:(0.509389996529) A[3]:(0.557478010654)\n",
      " state (4)  A[0]:(0.590326189995) A[1]:(0.655861020088) A[2]:(9.53674316406e-07) A[3]:(0.53139770031)\n",
      " state (5)  A[0]:(-0.0412148870528) A[1]:(0.999899029732) A[2]:(-0.793070554733) A[3]:(0.624655544758)\n",
      " state (6)  A[0]:(-3.82661819458e-05) A[1]:(0.809941411018) A[2]:(-0.000257611274719) A[3]:(0.655891299248)\n",
      " state (7)  A[0]:(0.554087996483) A[1]:(-0.54937016964) A[2]:(0.421039313078) A[3]:(0.878239691257)\n",
      " state (8)  A[0]:(0.656018435955) A[1]:(-0.000296965241432) A[2]:(0.729009270668) A[3]:(0.590739548206)\n",
      " state (9)  A[0]:(0.655930161476) A[1]:(0.809822440147) A[2]:(0.810005068779) A[3]:(-0.000105530023575)\n",
      " state (10)  A[0]:(0.729220271111) A[1]:(0.899931371212) A[2]:(4.19616699219e-05) A[3]:(0.729186177254)\n",
      " state (11)  A[0]:(0.127941980958) A[1]:(0.881429076195) A[2]:(-0.921581506729) A[3]:(0.800832808018)\n",
      " state (12)  A[0]:(-0.430380493402) A[1]:(0.814015328884) A[2]:(-0.942921161652) A[3]:(0.714371740818)\n",
      " state (13)  A[0]:(-0.000442877382739) A[1]:(0.809185743332) A[2]:(0.899944782257) A[3]:(0.729354918003)\n",
      " state (14)  A[0]:(0.80990755558) A[1]:(0.900173783302) A[2]:(0.999999642372) A[3]:(0.810124874115)\n",
      " state (15)  A[0]:(0.979601264) A[1]:(0.940858900547) A[2]:(1.0) A[3]:(0.878320634365)\n",
      "Episode 492000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6060. Times reached goal: 971.               Steps done: 3608657. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0243794060927.\n",
      " state (0)  A[0]:(0.530770242214) A[1]:(0.590168356895) A[2]:(0.590192437172) A[3]:(0.531206250191)\n",
      " state (1)  A[0]:(0.530674159527) A[1]:(0.000299394130707) A[2]:(0.655922055244) A[3]:(0.590336561203)\n",
      " state (2)  A[0]:(0.589579105377) A[1]:(0.728888750076) A[2]:(0.591950416565) A[3]:(0.656018614769)\n",
      " state (3)  A[0]:(0.655683338642) A[1]:(-0.0103494403884) A[2]:(0.510349810123) A[3]:(0.557317376137)\n",
      " state (4)  A[0]:(0.58966088295) A[1]:(0.655701935291) A[2]:(0.000667691114359) A[3]:(0.531514167786)\n",
      " state (5)  A[0]:(-0.0417297892272) A[1]:(0.999898970127) A[2]:(-0.793451309204) A[3]:(0.625610589981)\n",
      " state (6)  A[0]:(-0.000873639946803) A[1]:(0.809889554977) A[2]:(-6.31809234619e-05) A[3]:(0.656064033508)\n",
      " state (7)  A[0]:(0.553275406361) A[1]:(-0.548640727997) A[2]:(0.421113729477) A[3]:(0.87813526392)\n",
      " state (8)  A[0]:(0.655295550823) A[1]:(0.000203549861908) A[2]:(0.729026257992) A[3]:(0.590911746025)\n",
      " state (9)  A[0]:(0.655221760273) A[1]:(0.809826493263) A[2]:(0.810136377811) A[3]:(0.000695019843988)\n",
      " state (10)  A[0]:(0.728918075562) A[1]:(0.899933815002) A[2]:(0.000569939555135) A[3]:(0.729483366013)\n",
      " state (11)  A[0]:(0.128051742911) A[1]:(0.881468057632) A[2]:(-0.921573400497) A[3]:(0.801031470299)\n",
      " state (12)  A[0]:(-0.430231362581) A[1]:(0.814042389393) A[2]:(-0.943062722683) A[3]:(0.714456737041)\n",
      " state (13)  A[0]:(-0.000650346162729) A[1]:(0.809063613415) A[2]:(0.899515092373) A[3]:(0.7293009758)\n",
      " state (14)  A[0]:(0.8096575737) A[1]:(0.900030434132) A[2]:(0.999999642372) A[3]:(0.81005936861)\n",
      " state (15)  A[0]:(0.979555606842) A[1]:(0.940746963024) A[2]:(1.0) A[3]:(0.878248333931)\n",
      "Episode 493000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6026. Times reached goal: 971.               Steps done: 3614683. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0242329375445.\n",
      " state (0)  A[0]:(0.531487643719) A[1]:(0.590507030487) A[2]:(0.590490937233) A[3]:(0.531479597092)\n",
      " state (1)  A[0]:(0.531481981277) A[1]:(8.23736190796e-05) A[2]:(0.656169116497) A[3]:(0.59052169323)\n",
      " state (2)  A[0]:(0.590353608131) A[1]:(0.729015290737) A[2]:(0.591646790504) A[3]:(0.65615016222)\n",
      " state (3)  A[0]:(0.656450033188) A[1]:(-0.0115311993286) A[2]:(0.510404825211) A[3]:(0.557216405869)\n",
      " state (4)  A[0]:(0.590440750122) A[1]:(0.656067252159) A[2]:(-2.83718109131e-05) A[3]:(0.531568288803)\n",
      " state (5)  A[0]:(-0.0401184894145) A[1]:(0.999899148941) A[2]:(-0.79410213232) A[3]:(0.626311659813)\n",
      " state (6)  A[0]:(0.000157803297043) A[1]:(0.810008704662) A[2]:(-0.000263690948486) A[3]:(0.656072795391)\n",
      " state (7)  A[0]:(0.553691148758) A[1]:(-0.548884510994) A[2]:(0.421436429024) A[3]:(0.878010988235)\n",
      " state (8)  A[0]:(0.65600335598) A[1]:(-0.000111222267151) A[2]:(0.728984594345) A[3]:(0.590866327286)\n",
      " state (9)  A[0]:(0.65598654747) A[1]:(0.809917271137) A[2]:(0.810039401054) A[3]:(0.00017574429512)\n",
      " state (10)  A[0]:(0.729265034199) A[1]:(0.899980604649) A[2]:(0.000198483467102) A[3]:(0.729221105576)\n",
      " state (11)  A[0]:(0.128164663911) A[1]:(0.881523489952) A[2]:(-0.92165350914) A[3]:(0.800923585892)\n",
      " state (12)  A[0]:(-0.430476486683) A[1]:(0.814164638519) A[2]:(-0.94306665659) A[3]:(0.714339375496)\n",
      " state (13)  A[0]:(-0.000733494642191) A[1]:(0.8092867136) A[2]:(0.90002477169) A[3]:(0.729179382324)\n",
      " state (14)  A[0]:(0.809876918793) A[1]:(0.9001942873) A[2]:(0.999999642372) A[3]:(0.809976935387)\n",
      " state (15)  A[0]:(0.979604721069) A[1]:(0.940808176994) A[2]:(1.0) A[3]:(0.878187000751)\n",
      "Episode 494000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6069. Times reached goal: 976.               Steps done: 3620752. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0240863132281.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5902,  0.5904,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0001,  0.6560,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7289,  0.5914,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0000,  0.8099, -0.0001,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0003,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.8998,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531216263771) A[1]:(0.590280890465) A[2]:(0.590428709984) A[3]:(0.531241416931)\n",
      " state (1)  A[0]:(0.531280875206) A[1]:(0.000159651041031) A[2]:(0.656002104282) A[3]:(0.590424954891)\n",
      " state (2)  A[0]:(0.59033125639) A[1]:(0.728906989098) A[2]:(0.591435194016) A[3]:(0.655993461609)\n",
      " state (3)  A[0]:(0.656323075294) A[1]:(-0.0127218952402) A[2]:(0.510647535324) A[3]:(0.556837916374)\n",
      " state (4)  A[0]:(0.590139627457) A[1]:(0.655909538269) A[2]:(-7.80820846558e-05) A[3]:(0.53148317337)\n",
      " state (5)  A[0]:(-0.0402024239302) A[1]:(0.999899089336) A[2]:(-0.794429957867) A[3]:(0.627164423466)\n",
      " state (6)  A[0]:(-7.80820846558e-06) A[1]:(0.809932947159) A[2]:(-0.000130772590637) A[3]:(0.656034231186)\n",
      " state (7)  A[0]:(0.553487300873) A[1]:(-0.548187494278) A[2]:(0.421406149864) A[3]:(0.877827525139)\n",
      " state (8)  A[0]:(0.65568614006) A[1]:(0.00107875419781) A[2]:(0.728893995285) A[3]:(0.590508341789)\n",
      " state (9)  A[0]:(0.655689775944) A[1]:(0.810260415077) A[2]:(0.80992937088) A[3]:(0.000386089057429)\n",
      " state (10)  A[0]:(0.729142129421) A[1]:(0.900046646595) A[2]:(-0.000184893608093) A[3]:(0.729246735573)\n",
      " state (11)  A[0]:(0.128364622593) A[1]:(0.881437897682) A[2]:(-0.921755194664) A[3]:(0.800846397877)\n",
      " state (12)  A[0]:(-0.429955780506) A[1]:(0.813782215118) A[2]:(-0.94318395853) A[3]:(0.714143514633)\n",
      " state (13)  A[0]:(9.04500484467e-05) A[1]:(0.808678030968) A[2]:(0.899937212467) A[3]:(0.729035675526)\n",
      " state (14)  A[0]:(0.810079157352) A[1]:(0.899800300598) A[2]:(0.999999642372) A[3]:(0.809974372387)\n",
      " state (15)  A[0]:(0.97960460186) A[1]:(0.940553426743) A[2]:(1.0) A[3]:(0.878219783306)\n",
      "Episode 495000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6080. Times reached goal: 978.               Steps done: 3626832. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0239403127349.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531192779541) A[1]:(0.59057199955) A[2]:(0.590454816818) A[3]:(0.53143632412)\n",
      " state (1)  A[0]:(0.531243085861) A[1]:(4.96059656143e-05) A[2]:(0.656103610992) A[3]:(0.59057366848)\n",
      " state (2)  A[0]:(0.590093076229) A[1]:(0.729061245918) A[2]:(0.591438889503) A[3]:(0.656192779541)\n",
      " state (3)  A[0]:(0.656335234642) A[1]:(-0.0138738639653) A[2]:(0.511144757271) A[3]:(0.556810498238)\n",
      " state (4)  A[0]:(0.590292811394) A[1]:(0.65626347065) A[2]:(-0.000218510627747) A[3]:(0.531610429287)\n",
      " state (5)  A[0]:(-0.0391172878444) A[1]:(0.999899208546) A[2]:(-0.795035541058) A[3]:(0.627846479416)\n",
      " state (6)  A[0]:(0.000438839168055) A[1]:(0.810113191605) A[2]:(-0.000133156776428) A[3]:(0.656122148037)\n",
      " state (7)  A[0]:(0.553736388683) A[1]:(-0.548393130302) A[2]:(0.422075420618) A[3]:(0.877856492996)\n",
      " state (8)  A[0]:(0.656633853912) A[1]:(0.000463262171252) A[2]:(0.72908270359) A[3]:(0.591514408588)\n",
      " state (9)  A[0]:(0.656864881516) A[1]:(0.810230731964) A[2]:(0.810162365437) A[3]:(0.00138324405998)\n",
      " state (10)  A[0]:(0.73017013073) A[1]:(0.900098145008) A[2]:(0.000849246745929) A[3]:(0.729817867279)\n",
      " state (11)  A[0]:(0.130350247025) A[1]:(0.881604135036) A[2]:(-0.921645522118) A[3]:(0.801447212696)\n",
      " state (12)  A[0]:(-0.428982526064) A[1]:(0.814200997353) A[2]:(-0.943211376667) A[3]:(0.714839577675)\n",
      " state (13)  A[0]:(0.000742703559808) A[1]:(0.809263885021) A[2]:(0.899887800217) A[3]:(0.729445159435)\n",
      " state (14)  A[0]:(0.810341715813) A[1]:(0.900221705437) A[2]:(0.999999642372) A[3]:(0.810095787048)\n",
      " state (15)  A[0]:(0.979649722576) A[1]:(0.940847933292) A[2]:(1.0) A[3]:(0.878190696239)\n",
      "Episode 496000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6072. Times reached goal: 981.               Steps done: 3632904. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.023795387594.\n",
      " state (0)  A[0]:(0.53274512291) A[1]:(0.590068101883) A[2]:(0.590980947018) A[3]:(0.530213475227)\n",
      " state (1)  A[0]:(0.532439053059) A[1]:(0.000547125877347) A[2]:(0.656018972397) A[3]:(0.589996576309)\n",
      " state (2)  A[0]:(0.591025829315) A[1]:(0.728693366051) A[2]:(0.592136621475) A[3]:(0.655697643757)\n",
      " state (3)  A[0]:(0.656981468201) A[1]:(-0.0152421556413) A[2]:(0.512742996216) A[3]:(0.556170701981)\n",
      " state (4)  A[0]:(0.590930342674) A[1]:(0.655393719673) A[2]:(0.00170409516431) A[3]:(0.531279921532)\n",
      " state (5)  A[0]:(-0.0380528010428) A[1]:(0.999898791313) A[2]:(-0.795111477375) A[3]:(0.628291487694)\n",
      " state (6)  A[0]:(0.00063952797791) A[1]:(0.809418439865) A[2]:(-0.000286936759949) A[3]:(0.655950903893)\n",
      " state (7)  A[0]:(0.554092168808) A[1]:(-0.547752022743) A[2]:(0.42139339447) A[3]:(0.877914130688)\n",
      " state (8)  A[0]:(0.657408356667) A[1]:(0.0029084153939) A[2]:(0.729040265083) A[3]:(0.592015743256)\n",
      " state (9)  A[0]:(0.658805608749) A[1]:(0.811169743538) A[2]:(0.810513019562) A[3]:(0.0044150063768)\n",
      " state (10)  A[0]:(0.732542872429) A[1]:(0.900372922421) A[2]:(0.00344441947527) A[3]:(0.732050180435)\n",
      " state (11)  A[0]:(0.135297417641) A[1]:(0.881526470184) A[2]:(-0.921193122864) A[3]:(0.803171396255)\n",
      " state (12)  A[0]:(-0.426760077477) A[1]:(0.813521981239) A[2]:(-0.943044602871) A[3]:(0.716467142105)\n",
      " state (13)  A[0]:(0.00109526468441) A[1]:(0.808192610741) A[2]:(0.900069415569) A[3]:(0.730401158333)\n",
      " state (14)  A[0]:(0.809878051281) A[1]:(0.899557888508) A[2]:(0.999999642372) A[3]:(0.810644805431)\n",
      " state (15)  A[0]:(0.979535758495) A[1]:(0.940436422825) A[2]:(1.0) A[3]:(0.878517985344)\n",
      "Episode 497000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6084. Times reached goal: 982.               Steps done: 3638988. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0236510559579.\n",
      " state (0)  A[0]:(0.531467378139) A[1]:(0.590262293816) A[2]:(0.590381860733) A[3]:(0.531412601471)\n",
      " state (1)  A[0]:(0.531607925892) A[1]:(-3.84896993637e-05) A[2]:(0.655982971191) A[3]:(0.590508103371)\n",
      " state (2)  A[0]:(0.590450763702) A[1]:(0.728830099106) A[2]:(0.59130603075) A[3]:(0.656101346016)\n",
      " state (3)  A[0]:(0.656575918198) A[1]:(-0.0165767837316) A[2]:(0.512058377266) A[3]:(0.556198239326)\n",
      " state (4)  A[0]:(0.59035885334) A[1]:(0.656136453152) A[2]:(3.11136245728e-05) A[3]:(0.531495571136)\n",
      " state (5)  A[0]:(-0.0380312278867) A[1]:(0.999899148941) A[2]:(-0.795792460442) A[3]:(0.629567801952)\n",
      " state (6)  A[0]:(0.000418424577219) A[1]:(0.80993360281) A[2]:(-2.08616256714e-05) A[3]:(0.656036496162)\n",
      " state (7)  A[0]:(0.552984714508) A[1]:(-0.548294425011) A[2]:(0.422354400158) A[3]:(0.877399682999)\n",
      " state (8)  A[0]:(0.656053423882) A[1]:(-0.000151500105858) A[2]:(0.728909492493) A[3]:(0.590738832951)\n",
      " state (9)  A[0]:(0.656034827232) A[1]:(0.809960722923) A[2]:(0.809954762459) A[3]:(8.97943973541e-05)\n",
      " state (10)  A[0]:(0.729197442532) A[1]:(0.900013387203) A[2]:(-3.44514846802e-05) A[3]:(0.729019284248)\n",
      " state (11)  A[0]:(0.128422662616) A[1]:(0.881649971008) A[2]:(-0.921864151955) A[3]:(0.80093818903)\n",
      " state (12)  A[0]:(-0.430258899927) A[1]:(0.814399242401) A[2]:(-0.94338530302) A[3]:(0.714284360409)\n",
      " state (13)  A[0]:(-0.00055202835938) A[1]:(0.809464693069) A[2]:(0.900051236153) A[3]:(0.729063987732)\n",
      " state (14)  A[0]:(0.809877991676) A[1]:(0.900288939476) A[2]:(0.999999642372) A[3]:(0.809943974018)\n",
      " state (15)  A[0]:(0.979586184025) A[1]:(0.940816819668) A[2]:(1.0) A[3]:(0.878118574619)\n",
      "Episode 498000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6102. Times reached goal: 979.               Steps done: 3645090. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0235071766367.\n",
      " state (0)  A[0]:(0.53135240078) A[1]:(0.59033870697) A[2]:(0.590439498425) A[3]:(0.531956911087)\n",
      " state (1)  A[0]:(0.531411170959) A[1]:(-9.3087553978e-05) A[2]:(0.655990600586) A[3]:(0.590688884258)\n",
      " state (2)  A[0]:(0.59020960331) A[1]:(0.728895902634) A[2]:(0.591584563255) A[3]:(0.656167566776)\n",
      " state (3)  A[0]:(0.65626424551) A[1]:(-0.0179730225354) A[2]:(0.512799143791) A[3]:(0.555953025818)\n",
      " state (4)  A[0]:(0.589856863022) A[1]:(0.656191885471) A[2]:(0.000100135803223) A[3]:(0.531436920166)\n",
      " state (5)  A[0]:(-0.0381555445492) A[1]:(0.999899208546) A[2]:(-0.796430110931) A[3]:(0.630314230919)\n",
      " state (6)  A[0]:(-0.000533759535756) A[1]:(0.810028910637) A[2]:(-0.000277400016785) A[3]:(0.65604019165)\n",
      " state (7)  A[0]:(0.552097320557) A[1]:(-0.547760903835) A[2]:(0.422554284334) A[3]:(0.877313792706)\n",
      " state (8)  A[0]:(0.65557205677) A[1]:(0.000327959656715) A[2]:(0.728980064392) A[3]:(0.590898394585)\n",
      " state (9)  A[0]:(0.65562993288) A[1]:(0.810040771961) A[2]:(0.810044765472) A[3]:(0.000113040208817)\n",
      " state (10)  A[0]:(0.728950738907) A[1]:(0.899989366531) A[2]:(0.000136256217957) A[3]:(0.728935837746)\n",
      " state (11)  A[0]:(0.128399431705) A[1]:(0.881527900696) A[2]:(-0.921901524067) A[3]:(0.800869762897)\n",
      " state (12)  A[0]:(-0.429826110601) A[1]:(0.814057648182) A[2]:(-0.943457663059) A[3]:(0.714194655418)\n",
      " state (13)  A[0]:(0.000222340226173) A[1]:(0.808973610401) A[2]:(0.900049805641) A[3]:(0.729009032249)\n",
      " state (14)  A[0]:(0.810060918331) A[1]:(0.899985849857) A[2]:(0.999999642372) A[3]:(0.809927821159)\n",
      " state (15)  A[0]:(0.979586660862) A[1]:(0.940621495247) A[2]:(1.0) A[3]:(0.878094613552)\n",
      "Episode 499000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6043. Times reached goal: 978.               Steps done: 3651133. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0233655511208.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5905,  0.5903,  0.5310]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.6554,  0.0008,  0.5312]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557, -0.0007,  0.7286,  0.5914]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6553,  0.8099,  0.8099,  0.0004]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7286,  0.9001,  0.0002,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9003,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531120538712) A[1]:(0.590388178825) A[2]:(0.590221583843) A[3]:(0.530991196632)\n",
      " state (1)  A[0]:(0.531146883965) A[1]:(-0.000257775187492) A[2]:(0.655801773071) A[3]:(0.590291976929)\n",
      " state (2)  A[0]:(0.59016597271) A[1]:(0.728474974632) A[2]:(0.591285288334) A[3]:(0.655878484249)\n",
      " state (3)  A[0]:(0.656263172626) A[1]:(-0.0195631310344) A[2]:(0.513025462627) A[3]:(0.555415391922)\n",
      " state (4)  A[0]:(0.590094566345) A[1]:(0.655045151711) A[2]:(0.000490546168294) A[3]:(0.531096458435)\n",
      " state (5)  A[0]:(-0.0369484983385) A[1]:(0.999898970127) A[2]:(-0.79670125246) A[3]:(0.630827665329)\n",
      " state (6)  A[0]:(-8.74847173691e-05) A[1]:(0.809800982475) A[2]:(-0.000490784586873) A[3]:(0.65577185154)\n",
      " state (7)  A[0]:(0.552007317543) A[1]:(-0.548059880733) A[2]:(0.422302603722) A[3]:(0.877022862434)\n",
      " state (8)  A[0]:(0.655445218086) A[1]:(-0.000734135392122) A[2]:(0.728615164757) A[3]:(0.590753078461)\n",
      " state (9)  A[0]:(0.655067205429) A[1]:(0.809715867043) A[2]:(0.809853971004) A[3]:(-0.000512480677571)\n",
      " state (10)  A[0]:(0.728459239006) A[1]:(0.899888277054) A[2]:(-0.00019896030426) A[3]:(0.7285836339)\n",
      " state (11)  A[0]:(0.127679809928) A[1]:(0.88153886795) A[2]:(-0.921986222267) A[3]:(0.800765872002)\n",
      " state (12)  A[0]:(-0.430375188589) A[1]:(0.814213871956) A[2]:(-0.943567931652) A[3]:(0.714155554771)\n",
      " state (13)  A[0]:(-0.000383242935641) A[1]:(0.80918687582) A[2]:(0.900001883507) A[3]:(0.729030966759)\n",
      " state (14)  A[0]:(0.809931635857) A[1]:(0.900104999542) A[2]:(0.999999642372) A[3]:(0.810005486012)\n",
      " state (15)  A[0]:(0.979583740234) A[1]:(0.94066798687) A[2]:(1.0) A[3]:(0.878169059753)\n",
      "Episode 500000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6042. Times reached goal: 975.               Steps done: 3657175. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0232248020919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531638026237) A[1]:(0.590511322021) A[2]:(0.590396404266) A[3]:(0.531436026096)\n",
      " state (1)  A[0]:(0.531627893448) A[1]:(-4.22447919846e-05) A[2]:(0.656114280224) A[3]:(0.590476870537)\n",
      " state (2)  A[0]:(0.590259552002) A[1]:(0.729055523872) A[2]:(0.591602444649) A[3]:(0.656206130981)\n",
      " state (3)  A[0]:(0.656478881836) A[1]:(-0.0199291985482) A[2]:(0.513643562794) A[3]:(0.555662155151)\n",
      " state (4)  A[0]:(0.590307950974) A[1]:(0.656071305275) A[2]:(7.164478302e-05) A[3]:(0.531655550003)\n",
      " state (5)  A[0]:(-0.0360830612481) A[1]:(0.999899148941) A[2]:(-0.797411561012) A[3]:(0.631959974766)\n",
      " state (6)  A[0]:(-0.000111699104309) A[1]:(0.809956967831) A[2]:(-0.000545144022908) A[3]:(0.65629529953)\n",
      " state (7)  A[0]:(0.551779866219) A[1]:(-0.547733545303) A[2]:(0.422857612371) A[3]:(0.87718296051)\n",
      " state (8)  A[0]:(0.655460655689) A[1]:(0.00010247528553) A[2]:(0.728940665722) A[3]:(0.591000318527)\n",
      " state (9)  A[0]:(0.65560823679) A[1]:(0.809963166714) A[2]:(0.809969186783) A[3]:(0.000900596147403)\n",
      " state (10)  A[0]:(0.728899359703) A[1]:(0.899979710579) A[2]:(-4.60147857666e-05) A[3]:(0.729557275772)\n",
      " state (11)  A[0]:(0.127918407321) A[1]:(0.881634712219) A[2]:(-0.922028422356) A[3]:(0.801481544971)\n",
      " state (12)  A[0]:(-0.43100592494) A[1]:(0.814382195473) A[2]:(-0.943643271923) A[3]:(0.714841723442)\n",
      " state (13)  A[0]:(-0.00171713368036) A[1]:(0.809413313866) A[2]:(0.900099277496) A[3]:(0.729414820671)\n",
      " state (14)  A[0]:(0.80949139595) A[1]:(0.900254547596) A[2]:(0.999999642372) A[3]:(0.810159802437)\n",
      " state (15)  A[0]:(0.979543447495) A[1]:(0.940732002258) A[2]:(1.0) A[3]:(0.878210484982)\n",
      "Episode 501000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6029. Times reached goal: 977.               Steps done: 3663204. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0230852010104.\n",
      " state (0)  A[0]:(0.531741678715) A[1]:(0.590614557266) A[2]:(0.590687155724) A[3]:(0.532296657562)\n",
      " state (1)  A[0]:(0.531877875328) A[1]:(-6.59674406052e-05) A[2]:(0.656212687492) A[3]:(0.591161489487)\n",
      " state (2)  A[0]:(0.590575754642) A[1]:(0.729132890701) A[2]:(0.591935992241) A[3]:(0.656695246696)\n",
      " state (3)  A[0]:(0.656879782677) A[1]:(-0.0206940881908) A[2]:(0.514780402184) A[3]:(0.555974960327)\n",
      " state (4)  A[0]:(0.590734362602) A[1]:(0.656243145466) A[2]:(0.00127839972265) A[3]:(0.532147288322)\n",
      " state (5)  A[0]:(-0.0353057198226) A[1]:(0.999899208546) A[2]:(-0.797512054443) A[3]:(0.633083701134)\n",
      " state (6)  A[0]:(0.000312745571136) A[1]:(0.81003266573) A[2]:(0.00017249584198) A[3]:(0.656347990036)\n",
      " state (7)  A[0]:(0.552049040794) A[1]:(-0.547642827034) A[2]:(0.423625290394) A[3]:(0.877027928829)\n",
      " state (8)  A[0]:(0.656096100807) A[1]:(6.42091035843e-05) A[2]:(0.72921192646) A[3]:(0.590854465961)\n",
      " state (9)  A[0]:(0.656426787376) A[1]:(0.810019016266) A[2]:(0.810118079185) A[3]:(0.000413835019572)\n",
      " state (10)  A[0]:(0.729566335678) A[1]:(0.899995684624) A[2]:(3.57627868652e-05) A[3]:(0.729319155216)\n",
      " state (11)  A[0]:(0.129410102963) A[1]:(0.881636977196) A[2]:(-0.922107338905) A[3]:(0.801350831985)\n",
      " state (12)  A[0]:(-0.429562330246) A[1]:(0.814353823662) A[2]:(-0.943742573261) A[3]:(0.714674890041)\n",
      " state (13)  A[0]:(0.000370144814951) A[1]:(0.809352934361) A[2]:(0.900102496147) A[3]:(0.729216277599)\n",
      " state (14)  A[0]:(0.810285210609) A[1]:(0.900218486786) A[2]:(0.999999642372) A[3]:(0.809957146645)\n",
      " state (15)  A[0]:(0.979638814926) A[1]:(0.940695822239) A[2]:(1.0) A[3]:(0.878016650677)\n",
      "Episode 502000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6049. Times reached goal: 980.               Steps done: 3669253. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0229459801276.\n",
      " state (0)  A[0]:(0.531507611275) A[1]:(0.590584874153) A[2]:(0.590626120567) A[3]:(0.531487524509)\n",
      " state (1)  A[0]:(0.531522154808) A[1]:(-0.000207930803299) A[2]:(0.656226336956) A[3]:(0.590748488903)\n",
      " state (2)  A[0]:(0.590388655663) A[1]:(0.729039907455) A[2]:(0.591993451118) A[3]:(0.656468033791)\n",
      " state (3)  A[0]:(0.65664434433) A[1]:(-0.0220613740385) A[2]:(0.515265464783) A[3]:(0.555418491364)\n",
      " state (4)  A[0]:(0.590325832367) A[1]:(0.656194210052) A[2]:(0.00122308672871) A[3]:(0.531877458096)\n",
      " state (5)  A[0]:(-0.0354030244052) A[1]:(0.99989926815) A[2]:(-0.798085510731) A[3]:(0.63395357132)\n",
      " state (6)  A[0]:(-6.95437192917e-05) A[1]:(0.810062050819) A[2]:(-0.000325441360474) A[3]:(0.656482875347)\n",
      " state (7)  A[0]:(0.551611065865) A[1]:(-0.547588706017) A[2]:(0.423461824656) A[3]:(0.87695556879)\n",
      " state (8)  A[0]:(0.655938029289) A[1]:(-0.000232011079788) A[2]:(0.72898375988) A[3]:(0.591362714767)\n",
      " state (9)  A[0]:(0.656212449074) A[1]:(0.809949994087) A[2]:(0.810031056404) A[3]:(0.00117036642041)\n",
      " state (10)  A[0]:(0.729478359222) A[1]:(0.899961054325) A[2]:(6.79492950439e-06) A[3]:(0.72960036993)\n",
      " state (11)  A[0]:(0.129610851407) A[1]:(0.881605505943) A[2]:(-0.922158956528) A[3]:(0.801585197449)\n",
      " state (12)  A[0]:(-0.429358869791) A[1]:(0.814280629158) A[2]:(-0.943876206875) A[3]:(0.714937210083)\n",
      " state (13)  A[0]:(0.00042879578541) A[1]:(0.809208035469) A[2]:(0.899829983711) A[3]:(0.729500055313)\n",
      " state (14)  A[0]:(0.810197710991) A[1]:(0.900127291679) A[2]:(0.999999642372) A[3]:(0.810275435448)\n",
      " state (15)  A[0]:(0.97961628437) A[1]:(0.940643191338) A[2]:(1.0) A[3]:(0.878274679184)\n",
      "Episode 503000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6094. Times reached goal: 983.               Steps done: 3675347. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.022806572531.\n",
      " state (0)  A[0]:(0.530978679657) A[1]:(0.590359091759) A[2]:(0.590443134308) A[3]:(0.53091442585)\n",
      " state (1)  A[0]:(0.53112077713) A[1]:(0.00120554806199) A[2]:(0.656313061714) A[3]:(0.590114712715)\n",
      " state (2)  A[0]:(0.590015590191) A[1]:(0.729165196419) A[2]:(0.592799544334) A[3]:(0.655707538128)\n",
      " state (3)  A[0]:(0.656364560127) A[1]:(-0.0229477081448) A[2]:(0.516726911068) A[3]:(0.554341673851)\n",
      " state (4)  A[0]:(0.590185880661) A[1]:(0.65643286705) A[2]:(0.00248574698344) A[3]:(0.531098246574)\n",
      " state (5)  A[0]:(-0.0342294611037) A[1]:(0.999899327755) A[2]:(-0.798205852509) A[3]:(0.634005308151)\n",
      " state (6)  A[0]:(0.000415846676333) A[1]:(0.810297727585) A[2]:(0.000484943360789) A[3]:(0.655367970467)\n",
      " state (7)  A[0]:(0.551674365997) A[1]:(-0.54629868269) A[2]:(0.423797130585) A[3]:(0.876389503479)\n",
      " state (8)  A[0]:(0.656121969223) A[1]:(0.000470742554171) A[2]:(0.728784799576) A[3]:(0.591242611408)\n",
      " state (9)  A[0]:(0.655353188515) A[1]:(0.810220301151) A[2]:(0.810063242912) A[3]:(-0.000757396104746)\n",
      " state (10)  A[0]:(0.728366494179) A[1]:(0.900165259838) A[2]:(0.000152111053467) A[3]:(0.728300452232)\n",
      " state (11)  A[0]:(0.127463936806) A[1]:(0.881915390491) A[2]:(-0.922191679478) A[3]:(0.800759553909)\n",
      " state (12)  A[0]:(-0.430662155151) A[1]:(0.814802408218) A[2]:(-0.943940341473) A[3]:(0.714076220989)\n",
      " state (13)  A[0]:(-0.000623792351689) A[1]:(0.809712946415) A[2]:(0.899798333645) A[3]:(0.728826522827)\n",
      " state (14)  A[0]:(0.809950470924) A[1]:(0.900387167931) A[2]:(0.999999642372) A[3]:(0.809806525707)\n",
      " state (15)  A[0]:(0.979592084885) A[1]:(0.940792024136) A[2]:(1.0) A[3]:(0.877931475639)\n",
      "Episode 504000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6043. Times reached goal: 975.               Steps done: 3681390. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0226691679992.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5903,  0.5906,  0.5306]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.0003,  0.6561,  0.5894]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5895,  0.7291,  0.5913,  0.6549]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0021,  0.8095,  0.0004,  0.6548]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7270,  0.9002, -0.0007,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8082,  0.8999,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531312704086) A[1]:(0.590475440025) A[2]:(0.589915215969) A[3]:(0.531817793846)\n",
      " state (1)  A[0]:(0.531068205833) A[1]:(-0.000636130513158) A[2]:(0.655925929546) A[3]:(0.591024041176)\n",
      " state (2)  A[0]:(0.589721918106) A[1]:(0.728896021843) A[2]:(0.591257631779) A[3]:(0.656700611115)\n",
      " state (3)  A[0]:(0.656092762947) A[1]:(-0.0253935400397) A[2]:(0.515128195286) A[3]:(0.555437684059)\n",
      " state (4)  A[0]:(0.58975225687) A[1]:(0.656113028526) A[2]:(-0.000627398432698) A[3]:(0.532771706581)\n",
      " state (5)  A[0]:(-0.0342011973262) A[1]:(0.99989926815) A[2]:(-0.799567937851) A[3]:(0.636704802513)\n",
      " state (6)  A[0]:(-0.000386968225939) A[1]:(0.809979498386) A[2]:(-0.00110697699711) A[3]:(0.657753825188)\n",
      " state (7)  A[0]:(0.550872683525) A[1]:(-0.54767537117) A[2]:(0.423667728901) A[3]:(0.877339661121)\n",
      " state (8)  A[0]:(0.655813455582) A[1]:(-0.00113418651745) A[2]:(0.728869915009) A[3]:(0.593881368637)\n",
      " state (9)  A[0]:(0.656350016594) A[1]:(0.809451878071) A[2]:(0.809891283512) A[3]:(0.00667907809839)\n",
      " state (10)  A[0]:(0.729354977608) A[1]:(0.899672985077) A[2]:(-0.000424146623118) A[3]:(0.732225179672)\n",
      " state (11)  A[0]:(0.128255024552) A[1]:(0.881272673607) A[2]:(-0.922347128391) A[3]:(0.803371906281)\n",
      " state (12)  A[0]:(-0.431855112314) A[1]:(0.813695073128) A[2]:(-0.944176971912) A[3]:(0.716735184193)\n",
      " state (13)  A[0]:(-0.0039653419517) A[1]:(0.808388471603) A[2]:(0.899343788624) A[3]:(0.730655252934)\n",
      " state (14)  A[0]:(0.808452010155) A[1]:(0.899532318115) A[2]:(0.999999642372) A[3]:(0.810835003853)\n",
      " state (15)  A[0]:(0.979405343533) A[1]:(0.940182566643) A[2]:(1.0) A[3]:(0.878497600555)\n",
      "Episode 505000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6049. Times reached goal: 978.               Steps done: 3687439. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0225324561039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531345844269) A[1]:(0.589976191521) A[2]:(0.590211331844) A[3]:(0.531391918659)\n",
      " state (1)  A[0]:(0.531030893326) A[1]:(-0.000784590665717) A[2]:(0.655886411667) A[3]:(0.59022963047)\n",
      " state (2)  A[0]:(0.589664399624) A[1]:(0.728900909424) A[2]:(0.591956496239) A[3]:(0.655807375908)\n",
      " state (3)  A[0]:(0.655924201012) A[1]:(-0.026072030887) A[2]:(0.516793251038) A[3]:(0.553976237774)\n",
      " state (4)  A[0]:(0.589377045631) A[1]:(0.655892372131) A[2]:(0.00148665800225) A[3]:(0.531329274178)\n",
      " state (5)  A[0]:(-0.0348527468741) A[1]:(0.999899148941) A[2]:(-0.799410820007) A[3]:(0.636080861092)\n",
      " state (6)  A[0]:(-0.00151863577776) A[1]:(0.809857845306) A[2]:(2.00271606445e-05) A[3]:(0.655675768852)\n",
      " state (7)  A[0]:(0.549735665321) A[1]:(-0.546864807606) A[2]:(0.424140542746) A[3]:(0.876044332981)\n",
      " state (8)  A[0]:(0.654665827751) A[1]:(-0.000466063589556) A[2]:(0.728972792625) A[3]:(0.589560627937)\n",
      " state (9)  A[0]:(0.654839634895) A[1]:(0.809654951096) A[2]:(0.81006282568) A[3]:(-0.00183811574243)\n",
      " state (10)  A[0]:(0.728377223015) A[1]:(0.899836421013) A[2]:(0.000206708908081) A[3]:(0.728228747845)\n",
      " state (11)  A[0]:(0.128010392189) A[1]:(0.881612718105) A[2]:(-0.922221064568) A[3]:(0.800894796848)\n",
      " state (12)  A[0]:(-0.430279165506) A[1]:(0.81441283226) A[2]:(-0.944001913071) A[3]:(0.714207231998)\n",
      " state (13)  A[0]:(-0.000433504552348) A[1]:(0.809299647808) A[2]:(0.900024950504) A[3]:(0.72885966301)\n",
      " state (14)  A[0]:(0.809880137444) A[1]:(0.900137245655) A[2]:(0.999999701977) A[3]:(0.809823453426)\n",
      " state (15)  A[0]:(0.979563415051) A[1]:(0.940599918365) A[2]:(1.0) A[3]:(0.877907633781)\n",
      "Episode 506000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6037. Times reached goal: 973.               Steps done: 3693476. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0223968374433.\n",
      " state (0)  A[0]:(0.531084895134) A[1]:(0.5906727314) A[2]:(0.590662956238) A[3]:(0.531014025211)\n",
      " state (1)  A[0]:(0.531188607216) A[1]:(0.00113147450611) A[2]:(0.656427741051) A[3]:(0.590281605721)\n",
      " state (2)  A[0]:(0.59028083086) A[1]:(0.729064047337) A[2]:(0.592351973057) A[3]:(0.655806541443)\n",
      " state (3)  A[0]:(0.656671285629) A[1]:(-0.0275037642568) A[2]:(0.517532646656) A[3]:(0.553523004055)\n",
      " state (4)  A[0]:(0.590440988541) A[1]:(0.655940115452) A[2]:(0.00153481843881) A[3]:(0.531091094017)\n",
      " state (5)  A[0]:(-0.0321588590741) A[1]:(0.999899089336) A[2]:(-0.799819588661) A[3]:(0.636922419071)\n",
      " state (6)  A[0]:(0.000308245420456) A[1]:(0.809653401375) A[2]:(0.000413417787058) A[3]:(0.655672848225)\n",
      " state (7)  A[0]:(0.550931155682) A[1]:(-0.547205209732) A[2]:(0.424413353205) A[3]:(0.876028716564)\n",
      " state (8)  A[0]:(0.656389176846) A[1]:(-0.00186568277422) A[2]:(0.728407502174) A[3]:(0.591201663017)\n",
      " state (9)  A[0]:(0.65577852726) A[1]:(0.809375107288) A[2]:(0.809490263462) A[3]:(-0.00154173257761)\n",
      " state (10)  A[0]:(0.728331625462) A[1]:(0.899744033813) A[2]:(-0.00219106324948) A[3]:(0.72764313221)\n",
      " state (11)  A[0]:(0.127175241709) A[1]:(0.881553173065) A[2]:(-0.922656238079) A[3]:(0.800405442715)\n",
      " state (12)  A[0]:(-0.430490881205) A[1]:(0.814397931099) A[2]:(-0.944231152534) A[3]:(0.713817834854)\n",
      " state (13)  A[0]:(0.000307247042656) A[1]:(0.809347033501) A[2]:(0.900084614754) A[3]:(0.728773236275)\n",
      " state (14)  A[0]:(0.810391485691) A[1]:(0.900172650814) A[2]:(0.999999701977) A[3]:(0.809899508953)\n",
      " state (15)  A[0]:(0.979637861252) A[1]:(0.940590679646) A[2]:(1.0) A[3]:(0.877984762192)\n",
      "Episode 507000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6080. Times reached goal: 985.               Steps done: 3699556. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0222610777991.\n",
      " state (0)  A[0]:(0.531376600266) A[1]:(0.590545535088) A[2]:(0.590427160263) A[3]:(0.531378865242)\n",
      " state (1)  A[0]:(0.531515717506) A[1]:(-1.37239694595e-05) A[2]:(0.656059622765) A[3]:(0.590348601341)\n",
      " state (2)  A[0]:(0.590321183205) A[1]:(0.729083657265) A[2]:(0.590773701668) A[3]:(0.655971884727)\n",
      " state (3)  A[0]:(0.656580924988) A[1]:(-0.0272944271564) A[2]:(0.516022443771) A[3]:(0.553624629974)\n",
      " state (4)  A[0]:(0.59047293663) A[1]:(0.656038880348) A[2]:(-8.92877578735e-05) A[3]:(0.531434893608)\n",
      " state (5)  A[0]:(-0.0312826149166) A[1]:(0.999899208546) A[2]:(-0.80024087429) A[3]:(0.637871265411)\n",
      " state (6)  A[0]:(0.000411331624491) A[1]:(0.80989575386) A[2]:(0.000263929367065) A[3]:(0.65594291687)\n",
      " state (7)  A[0]:(0.550764322281) A[1]:(-0.546697676182) A[2]:(0.424893438816) A[3]:(0.875988841057)\n",
      " state (8)  A[0]:(0.656076848507) A[1]:(-8.73953104019e-05) A[2]:(0.728962481022) A[3]:(0.590713381767)\n",
      " state (9)  A[0]:(0.656297981739) A[1]:(0.810020744801) A[2]:(0.8099167943) A[3]:(0.000425994367106)\n",
      " state (10)  A[0]:(0.729404091835) A[1]:(0.900025546551) A[2]:(-0.000119209289551) A[3]:(0.729212641716)\n",
      " state (11)  A[0]:(0.129651144147) A[1]:(0.881818413734) A[2]:(-0.922343850136) A[3]:(0.801537632942)\n",
      " state (12)  A[0]:(-0.429452210665) A[1]:(0.814724624157) A[2]:(-0.944167256355) A[3]:(0.714742779732)\n",
      " state (13)  A[0]:(0.000278860330582) A[1]:(0.809668779373) A[2]:(0.900020539761) A[3]:(0.729071557522)\n",
      " state (14)  A[0]:(0.810119628906) A[1]:(0.90042245388) A[2]:(0.999999701977) A[3]:(0.809822678566)\n",
      " state (15)  A[0]:(0.979588508606) A[1]:(0.940800964832) A[2]:(1.0) A[3]:(0.877799093723)\n",
      "Episode 508000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6061. Times reached goal: 972.               Steps done: 3705617. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0221265614701.\n",
      " state (0)  A[0]:(0.531546592712) A[1]:(0.590497493744) A[2]:(0.590538144112) A[3]:(0.531532406807)\n",
      " state (1)  A[0]:(0.531620502472) A[1]:(0.000113621354103) A[2]:(0.656139373779) A[3]:(0.590578317642)\n",
      " state (2)  A[0]:(0.590426564217) A[1]:(0.729009032249) A[2]:(0.590866804123) A[3]:(0.656207382679)\n",
      " state (3)  A[0]:(0.656574010849) A[1]:(-0.0272398386151) A[2]:(0.516066610813) A[3]:(0.55372262001)\n",
      " state (4)  A[0]:(0.590486943722) A[1]:(0.656011819839) A[2]:(-0.000140786170959) A[3]:(0.531593680382)\n",
      " state (5)  A[0]:(-0.0310217700899) A[1]:(0.999899208546) A[2]:(-0.800447940826) A[3]:(0.63853096962)\n",
      " state (6)  A[0]:(0.000452518434031) A[1]:(0.809976816177) A[2]:(-0.000410914391978) A[3]:(0.656436502934)\n",
      " state (7)  A[0]:(0.550857663155) A[1]:(-0.546487629414) A[2]:(0.424396455288) A[3]:(0.876189053059)\n",
      " state (8)  A[0]:(0.656377971172) A[1]:(4.32878732681e-05) A[2]:(0.728851079941) A[3]:(0.591367602348)\n",
      " state (9)  A[0]:(0.656889081001) A[1]:(0.809989213943) A[2]:(0.809974193573) A[3]:(0.00176006369293)\n",
      " state (10)  A[0]:(0.730065464973) A[1]:(0.899990439415) A[2]:(4.24385070801e-05) A[3]:(0.730037093163)\n",
      " state (11)  A[0]:(0.131101936102) A[1]:(0.881740808487) A[2]:(-0.922403454781) A[3]:(0.802176713943)\n",
      " state (12)  A[0]:(-0.428392678499) A[1]:(0.814518451691) A[2]:(-0.944272577763) A[3]:(0.715517103672)\n",
      " state (13)  A[0]:(0.00134694494773) A[1]:(0.809355556965) A[2]:(0.900023818016) A[3]:(0.729759156704)\n",
      " state (14)  A[0]:(0.810387909412) A[1]:(0.90020442009) A[2]:(0.999999701977) A[3]:(0.810327649117)\n",
      " state (15)  A[0]:(0.979612112045) A[1]:(0.940624415874) A[2]:(1.0) A[3]:(0.878147661686)\n",
      "Episode 509000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6047. Times reached goal: 979.               Steps done: 3711664. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.021993165881.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5908,  0.5904,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6562,  0.0000,  0.5317]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560, -0.0003,  0.7292,  0.5908]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6563,  0.8100,  0.8101,  0.0002]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0003,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9002,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531674087048) A[1]:(0.590785384178) A[2]:(0.590446949005) A[3]:(0.531614005566)\n",
      " state (1)  A[0]:(0.531614542007) A[1]:(-2.72840261459e-05) A[2]:(0.656244695187) A[3]:(0.5906047225)\n",
      " state (2)  A[0]:(0.59042173624) A[1]:(0.729184925556) A[2]:(0.590797543526) A[3]:(0.656221747398)\n",
      " state (3)  A[0]:(0.656522274017) A[1]:(-0.0265939552337) A[2]:(0.516051888466) A[3]:(0.553649663925)\n",
      " state (4)  A[0]:(0.590489625931) A[1]:(0.656216740608) A[2]:(4.94718551636e-05) A[3]:(0.531597137451)\n",
      " state (5)  A[0]:(-0.030815185979) A[1]:(0.999899327755) A[2]:(-0.80037856102) A[3]:(0.638888478279)\n",
      " state (6)  A[0]:(0.000115543603897) A[1]:(0.810070514679) A[2]:(0.000213503837585) A[3]:(0.656367719173)\n",
      " state (7)  A[0]:(0.550383329391) A[1]:(-0.54659986496) A[2]:(0.425254017115) A[3]:(0.876045703888)\n",
      " state (8)  A[0]:(0.655972838402) A[1]:(-7.61598348618e-05) A[2]:(0.72922283411) A[3]:(0.590710043907)\n",
      " state (9)  A[0]:(0.65627092123) A[1]:(0.810012876987) A[2]:(0.810083508492) A[3]:(8.82744789124e-05)\n",
      " state (10)  A[0]:(0.729138195515) A[1]:(0.900019526482) A[2]:(-0.00012469291687) A[3]:(0.72908949852)\n",
      " state (11)  A[0]:(0.128438159823) A[1]:(0.881799459457) A[2]:(-0.922538876534) A[3]:(0.801428318024)\n",
      " state (12)  A[0]:(-0.43057307601) A[1]:(0.814629971981) A[2]:(-0.944401264191) A[3]:(0.714675962925)\n",
      " state (13)  A[0]:(-0.000747367623262) A[1]:(0.809471130371) A[2]:(0.900063872337) A[3]:(0.729235649109)\n",
      " state (14)  A[0]:(0.809949040413) A[1]:(0.900264620781) A[2]:(0.999999701977) A[3]:(0.810152173042)\n",
      " state (15)  A[0]:(0.979594171047) A[1]:(0.940623760223) A[2]:(1.0) A[3]:(0.878143966198)\n",
      "Episode 510000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6043. Times reached goal: 972.               Steps done: 3717707. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0218606619434.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531637012959) A[1]:(0.590532660484) A[2]:(0.590422272682) A[3]:(0.531413912773)\n",
      " state (1)  A[0]:(0.531650304794) A[1]:(4.96506690979e-05) A[2]:(0.656182050705) A[3]:(0.590347290039)\n",
      " state (2)  A[0]:(0.590443313122) A[1]:(0.729000091553) A[2]:(0.591041147709) A[3]:(0.656035840511)\n",
      " state (3)  A[0]:(0.65651011467) A[1]:(-0.0266319103539) A[2]:(0.516336798668) A[3]:(0.553434848785)\n",
      " state (4)  A[0]:(0.590567290783) A[1]:(0.656233668327) A[2]:(0.000331282615662) A[3]:(0.531512737274)\n",
      " state (5)  A[0]:(-0.0304265394807) A[1]:(0.999899327755) A[2]:(-0.800456106663) A[3]:(0.638996958733)\n",
      " state (6)  A[0]:(0.000322341918945) A[1]:(0.810075163841) A[2]:(5.13792037964e-05) A[3]:(0.656060814857)\n",
      " state (7)  A[0]:(0.550606787205) A[1]:(-0.546500384808) A[2]:(0.425091415644) A[3]:(0.87590867281)\n",
      " state (8)  A[0]:(0.656124889851) A[1]:(0.000107109546661) A[2]:(0.729110240936) A[3]:(0.590540289879)\n",
      " state (9)  A[0]:(0.656314313412) A[1]:(0.810097932816) A[2]:(0.810066580772) A[3]:(-0.000153601169586)\n",
      " state (10)  A[0]:(0.729379892349) A[1]:(0.90001475811) A[2]:(0.00010347366333) A[3]:(0.729086637497)\n",
      " state (11)  A[0]:(0.12942931056) A[1]:(0.881745696068) A[2]:(-0.922547757626) A[3]:(0.801478981972)\n",
      " state (12)  A[0]:(-0.429606854916) A[1]:(0.814503192902) A[2]:(-0.944495916367) A[3]:(0.714696645737)\n",
      " state (13)  A[0]:(0.000407978863223) A[1]:(0.809347033501) A[2]:(0.899912118912) A[3]:(0.729180395603)\n",
      " state (14)  A[0]:(0.810283541679) A[1]:(0.900272905827) A[2]:(0.999999701977) A[3]:(0.810044765472)\n",
      " state (15)  A[0]:(0.97962230444) A[1]:(0.940685033798) A[2]:(1.0) A[3]:(0.878022432327)\n",
      "Episode 511000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6059. Times reached goal: 972.               Steps done: 3723766. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0217286086521.\n",
      " state (0)  A[0]:(0.531607866287) A[1]:(0.590319275856) A[2]:(0.590471625328) A[3]:(0.531347751617)\n",
      " state (1)  A[0]:(0.53145968914) A[1]:(4.65959310532e-05) A[2]:(0.655953526497) A[3]:(0.590246081352)\n",
      " state (2)  A[0]:(0.590190470219) A[1]:(0.729003071785) A[2]:(0.590595841408) A[3]:(0.655802071095)\n",
      " state (3)  A[0]:(0.656136393547) A[1]:(-0.0263078436255) A[2]:(0.51582056284) A[3]:(0.552941083908)\n",
      " state (4)  A[0]:(0.59013569355) A[1]:(0.655940651894) A[2]:(-2.11000442505e-05) A[3]:(0.530941307545)\n",
      " state (5)  A[0]:(-0.0310445781797) A[1]:(0.999899208546) A[2]:(-0.800437569618) A[3]:(0.638716340065)\n",
      " state (6)  A[0]:(-0.000204548239708) A[1]:(0.809900283813) A[2]:(-4.52995300293e-06) A[3]:(0.65521723032)\n",
      " state (7)  A[0]:(0.550222873688) A[1]:(-0.546040654182) A[2]:(0.424636274576) A[3]:(0.875393152237)\n",
      " state (8)  A[0]:(0.655638337135) A[1]:(0.000570163072553) A[2]:(0.728832364082) A[3]:(0.58894854784)\n",
      " state (9)  A[0]:(0.655797958374) A[1]:(0.810107946396) A[2]:(0.809882998466) A[3]:(-0.00234454451129)\n",
      " state (10)  A[0]:(0.729073286057) A[1]:(0.899999856949) A[2]:(-0.000267505645752) A[3]:(0.728025078773)\n",
      " state (11)  A[0]:(0.129268884659) A[1]:(0.88173353672) A[2]:(-0.922621250153) A[3]:(0.800691902637)\n",
      " state (12)  A[0]:(-0.429347515106) A[1]:(0.814460158348) A[2]:(-0.9445515275) A[3]:(0.713678717613)\n",
      " state (13)  A[0]:(0.0010419782484) A[1]:(0.809247136116) A[2]:(0.900080800056) A[3]:(0.728277981281)\n",
      " state (14)  A[0]:(0.810520589352) A[1]:(0.900188982487) A[2]:(0.999999701977) A[3]:(0.809427380562)\n",
      " state (15)  A[0]:(0.979646921158) A[1]:(0.940600514412) A[2]:(1.0) A[3]:(0.87763184309)\n",
      "Episode 512000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6082. Times reached goal: 981.               Steps done: 3729848. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0215968563193.\n",
      " state (0)  A[0]:(0.53149753809) A[1]:(0.590374708176) A[2]:(0.590580880642) A[3]:(0.531554222107)\n",
      " state (1)  A[0]:(0.531379818916) A[1]:(-0.00011995434761) A[2]:(0.656151175499) A[3]:(0.590693593025)\n",
      " state (2)  A[0]:(0.590333819389) A[1]:(0.728894352913) A[2]:(0.590840756893) A[3]:(0.656240224838)\n",
      " state (3)  A[0]:(0.656249463558) A[1]:(-0.026668690145) A[2]:(0.516084074974) A[3]:(0.553423523903)\n",
      " state (4)  A[0]:(0.590270519257) A[1]:(0.656113743782) A[2]:(8.4400177002e-05) A[3]:(0.531574368477)\n",
      " state (5)  A[0]:(-0.0305029414594) A[1]:(0.999899327755) A[2]:(-0.800567507744) A[3]:(0.639659285545)\n",
      " state (6)  A[0]:(-6.45071268082e-05) A[1]:(0.8100669384) A[2]:(0.000101923942566) A[3]:(0.656215190887)\n",
      " state (7)  A[0]:(0.55016797781) A[1]:(-0.546534359455) A[2]:(0.425285071135) A[3]:(0.875842034817)\n",
      " state (8)  A[0]:(0.655874609947) A[1]:(-0.000453263492091) A[2]:(0.729087293148) A[3]:(0.590856432915)\n",
      " state (9)  A[0]:(0.655991315842) A[1]:(0.809908986092) A[2]:(0.810075044632) A[3]:(0.000219464302063)\n",
      " state (10)  A[0]:(0.729243993759) A[1]:(0.900018632412) A[2]:(0.000229477882385) A[3]:(0.729207634926)\n",
      " state (11)  A[0]:(0.129377767444) A[1]:(0.881904661655) A[2]:(-0.922656178474) A[3]:(0.801581561565)\n",
      " state (12)  A[0]:(-0.429941385984) A[1]:(0.814882278442) A[2]:(-0.944674491882) A[3]:(0.714717626572)\n",
      " state (13)  A[0]:(-0.000528886856046) A[1]:(0.809749960899) A[2]:(0.90002310276) A[3]:(0.729109525681)\n",
      " state (14)  A[0]:(0.809824287891) A[1]:(0.900451898575) A[2]:(0.999999701977) A[3]:(0.80996966362)\n",
      " state (15)  A[0]:(0.979570686817) A[1]:(0.940700471401) A[2]:(1.0) A[3]:(0.877989530563)\n",
      "Episode 513000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6052. Times reached goal: 976.               Steps done: 3735900. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.021466546859.\n",
      " state (0)  A[0]:(0.531442403793) A[1]:(0.590767621994) A[2]:(0.590300917625) A[3]:(0.53178024292)\n",
      " state (1)  A[0]:(0.531636536121) A[1]:(-0.000811517063994) A[2]:(0.655956864357) A[3]:(0.590792775154)\n",
      " state (2)  A[0]:(0.590416789055) A[1]:(0.729027271271) A[2]:(0.591049432755) A[3]:(0.656406760216)\n",
      " state (3)  A[0]:(0.656239509583) A[1]:(-0.02666121535) A[2]:(0.516559898853) A[3]:(0.553447425365)\n",
      " state (4)  A[0]:(0.590285718441) A[1]:(0.655964255333) A[2]:(0.000702619436197) A[3]:(0.531438469887)\n",
      " state (5)  A[0]:(-0.0300907380879) A[1]:(0.999898791313) A[2]:(-0.800373017788) A[3]:(0.639251947403)\n",
      " state (6)  A[0]:(1.13993883133e-05) A[1]:(0.809050440788) A[2]:(0.00135958113242) A[3]:(0.655166089535)\n",
      " state (7)  A[0]:(0.549696564674) A[1]:(-0.546494543552) A[2]:(0.426052570343) A[3]:(0.875157535076)\n",
      " state (8)  A[0]:(0.653532862663) A[1]:(0.0011304016225) A[2]:(0.729922294617) A[3]:(0.586475968361)\n",
      " state (9)  A[0]:(0.652686595917) A[1]:(0.81011557579) A[2]:(0.810247659683) A[3]:(-0.00485973712057)\n",
      " state (10)  A[0]:(0.725630164146) A[1]:(0.899944365025) A[2]:(-0.00110149336979) A[3]:(0.727004289627)\n",
      " state (11)  A[0]:(0.121329002082) A[1]:(0.881667077541) A[2]:(-0.922994375229) A[3]:(0.800011992455)\n",
      " state (12)  A[0]:(-0.434623986483) A[1]:(0.814327955246) A[2]:(-0.944798767567) A[3]:(0.713496625423)\n",
      " state (13)  A[0]:(-0.0036679788027) A[1]:(0.809036672115) A[2]:(0.900308728218) A[3]:(0.728834569454)\n",
      " state (14)  A[0]:(0.809190928936) A[1]:(0.900033473969) A[2]:(0.999999701977) A[3]:(0.81014329195)\n",
      " state (15)  A[0]:(0.979505956173) A[1]:(0.940430998802) A[2]:(1.0) A[3]:(0.878221571445)\n",
      "Episode 514000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6088. Times reached goal: 981.               Steps done: 3741988. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.021336255531.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5904,  0.5906,  0.5323]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5309,  0.0000,  0.6561,  0.5917]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5899,  0.7290,  0.5922,  0.6575]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  0.8101, -0.0006,  0.6584]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0005,  0.7306]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8091,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531947016716) A[1]:(0.590442180634) A[2]:(0.590642929077) A[3]:(0.531836271286)\n",
      " state (1)  A[0]:(0.531705200672) A[1]:(5.49852848053e-05) A[2]:(0.656126976013) A[3]:(0.590825557709)\n",
      " state (2)  A[0]:(0.590343177319) A[1]:(0.728964149952) A[2]:(0.592329144478) A[3]:(0.656451284885)\n",
      " state (3)  A[0]:(0.656261026859) A[1]:(-0.0281822830439) A[2]:(0.518410563469) A[3]:(0.553375601768)\n",
      " state (4)  A[0]:(0.590125203133) A[1]:(0.655936717987) A[2]:(0.00197612983175) A[3]:(0.531677544117)\n",
      " state (5)  A[0]:(-0.0305449236184) A[1]:(0.99989926815) A[2]:(-0.80107998848) A[3]:(0.640311837196)\n",
      " state (6)  A[0]:(-0.00050584966084) A[1]:(0.809973061085) A[2]:(0.000177025794983) A[3]:(0.656067252159)\n",
      " state (7)  A[0]:(0.549683451653) A[1]:(-0.546530246735) A[2]:(0.425857901573) A[3]:(0.875643074512)\n",
      " state (8)  A[0]:(0.655038237572) A[1]:(1.81794166565e-06) A[2]:(0.729402542114) A[3]:(0.589649021626)\n",
      " state (9)  A[0]:(0.654667615891) A[1]:(0.810018658638) A[2]:(0.810107946396) A[3]:(-0.00199717027135)\n",
      " state (10)  A[0]:(0.727669596672) A[1]:(0.899922132492) A[2]:(-0.000589489878621) A[3]:(0.72799295187)\n",
      " state (11)  A[0]:(0.125605583191) A[1]:(0.881644248962) A[2]:(-0.922927260399) A[3]:(0.80064022541)\n",
      " state (12)  A[0]:(-0.432410389185) A[1]:(0.814341902733) A[2]:(-0.944862425327) A[3]:(0.713729381561)\n",
      " state (13)  A[0]:(-0.00226904056035) A[1]:(0.809152424335) A[2]:(0.900248944759) A[3]:(0.72849881649)\n",
      " state (14)  A[0]:(0.809582471848) A[1]:(0.90016412735) A[2]:(0.999999701977) A[3]:(0.809685766697)\n",
      " state (15)  A[0]:(0.979557514191) A[1]:(0.940518677235) A[2]:(1.0) A[3]:(0.87783318758)\n",
      "Episode 515000 finished after 0 timesteps with r=1.0. Running score: 0.95. Times trained:               6031. Times reached goal: 975.               Steps done: 3748019. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0212079638263.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531144559383) A[1]:(0.590648293495) A[2]:(0.590356826782) A[3]:(0.531015694141)\n",
      " state (1)  A[0]:(0.53108561039) A[1]:(0.000297784805298) A[2]:(0.655527591705) A[3]:(0.590106606483)\n",
      " state (2)  A[0]:(0.590066850185) A[1]:(0.728675603867) A[2]:(0.591995477676) A[3]:(0.655625879765)\n",
      " state (3)  A[0]:(0.656115174294) A[1]:(-0.0297925490886) A[2]:(0.518708646297) A[3]:(0.552313327789)\n",
      " state (4)  A[0]:(0.590126037598) A[1]:(0.655337750912) A[2]:(0.00214242609218) A[3]:(0.53098076582)\n",
      " state (5)  A[0]:(-0.0297426152974) A[1]:(0.999899089336) A[2]:(-0.801490306854) A[3]:(0.640564739704)\n",
      " state (6)  A[0]:(-0.000354588002665) A[1]:(0.809840917587) A[2]:(-0.000178813934326) A[3]:(0.65554612875)\n",
      " state (7)  A[0]:(0.54958319664) A[1]:(-0.546028971672) A[2]:(0.425410538912) A[3]:(0.875338792801)\n",
      " state (8)  A[0]:(0.655349254608) A[1]:(7.34329223633e-05) A[2]:(0.728986382484) A[3]:(0.59000992775)\n",
      " state (9)  A[0]:(0.655405521393) A[1]:(0.809953451157) A[2]:(0.809988975525) A[3]:(-0.000209450721741)\n",
      " state (10)  A[0]:(0.728837966919) A[1]:(0.899974882603) A[2]:(0.000198841094971) A[3]:(0.729194164276)\n",
      " state (11)  A[0]:(0.128704220057) A[1]:(0.881832540035) A[2]:(-0.922768235207) A[3]:(0.801719844341)\n",
      " state (12)  A[0]:(-0.430738180876) A[1]:(0.814707756042) A[2]:(-0.944876551628) A[3]:(0.714824080467)\n",
      " state (13)  A[0]:(-0.0014498521341) A[1]:(0.809462189674) A[2]:(0.900108635426) A[3]:(0.729100704193)\n",
      " state (14)  A[0]:(0.809705138206) A[1]:(0.900253355503) A[2]:(0.999999701977) A[3]:(0.809941589832)\n",
      " state (15)  A[0]:(0.979575037956) A[1]:(0.940512537956) A[2]:(1.0) A[3]:(0.877937555313)\n",
      "Episode 516000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6051. Times reached goal: 980.               Steps done: 3754070. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0210800219159.\n",
      " state (0)  A[0]:(0.531385779381) A[1]:(0.590487718582) A[2]:(0.590428829193) A[3]:(0.531275987625)\n",
      " state (1)  A[0]:(0.531462907791) A[1]:(-6.29127025604e-05) A[2]:(0.656094074249) A[3]:(0.59045702219)\n",
      " state (2)  A[0]:(0.590371012688) A[1]:(0.729004979134) A[2]:(0.591460287571) A[3]:(0.656066954136)\n",
      " state (3)  A[0]:(0.656449317932) A[1]:(-0.0306824371219) A[2]:(0.5180362463) A[3]:(0.552480340004)\n",
      " state (4)  A[0]:(0.590381622314) A[1]:(0.656085371971) A[2]:(0.000107407569885) A[3]:(0.531305074692)\n",
      " state (5)  A[0]:(-0.0290136989206) A[1]:(0.99989926815) A[2]:(-0.802315235138) A[3]:(0.641528487206)\n",
      " state (6)  A[0]:(-5.96195459366e-05) A[1]:(0.80999815464) A[2]:(-0.000251531600952) A[3]:(0.655939221382)\n",
      " state (7)  A[0]:(0.549692630768) A[1]:(-0.545804977417) A[2]:(0.42587351799) A[3]:(0.875403761864)\n",
      " state (8)  A[0]:(0.655872344971) A[1]:(-9.2476606369e-05) A[2]:(0.728992164135) A[3]:(0.590660274029)\n",
      " state (9)  A[0]:(0.655738115311) A[1]:(0.809963405132) A[2]:(0.809944927692) A[3]:(-0.00056189292809)\n",
      " state (10)  A[0]:(0.728854835033) A[1]:(0.899998426437) A[2]:(-0.000401854485972) A[3]:(0.728697896004)\n",
      " state (11)  A[0]:(0.128802701831) A[1]:(0.881888449192) A[2]:(-0.922939062119) A[3]:(0.801418066025)\n",
      " state (12)  A[0]:(-0.43010699749) A[1]:(0.81481719017) A[2]:(-0.945011079311) A[3]:(0.714676141739)\n",
      " state (13)  A[0]:(-0.000191822648048) A[1]:(0.809566915035) A[2]:(0.900012254715) A[3]:(0.729132056236)\n",
      " state (14)  A[0]:(0.810112476349) A[1]:(0.900320470333) A[2]:(0.999999701977) A[3]:(0.809979736805)\n",
      " state (15)  A[0]:(0.979609310627) A[1]:(0.940558373928) A[2]:(1.0) A[3]:(0.877921819687)\n",
      "Episode 517000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6074. Times reached goal: 978.               Steps done: 3760144. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0209523699343.\n",
      " state (0)  A[0]:(0.531548917294) A[1]:(0.590371131897) A[2]:(0.590410351753) A[3]:(0.531364798546)\n",
      " state (1)  A[0]:(0.531528711319) A[1]:(-8.32080841064e-05) A[2]:(0.656001389027) A[3]:(0.59044110775)\n",
      " state (2)  A[0]:(0.590297579765) A[1]:(0.728856801987) A[2]:(0.591427266598) A[3]:(0.656058192253)\n",
      " state (3)  A[0]:(0.656391859055) A[1]:(-0.0328979939222) A[2]:(0.518763303757) A[3]:(0.552182912827)\n",
      " state (4)  A[0]:(0.590221583843) A[1]:(0.656038761139) A[2]:(9.95397567749e-05) A[3]:(0.531285047531)\n",
      " state (5)  A[0]:(-0.0282953828573) A[1]:(0.999899208546) A[2]:(-0.803029894829) A[3]:(0.642348945141)\n",
      " state (6)  A[0]:(6.7412853241e-05) A[1]:(0.809957146645) A[2]:(-0.000466704339487) A[3]:(0.655893921852)\n",
      " state (7)  A[0]:(0.549550771713) A[1]:(-0.545290708542) A[2]:(0.425995200872) A[3]:(0.87524998188)\n",
      " state (8)  A[0]:(0.655938208103) A[1]:(0.000438213319285) A[2]:(0.728966891766) A[3]:(0.590792059898)\n",
      " state (9)  A[0]:(0.655899107456) A[1]:(0.810085892677) A[2]:(0.809985637665) A[3]:(-0.00010746717453)\n",
      " state (10)  A[0]:(0.729122579098) A[1]:(0.89999884367) A[2]:(-4.95910644531e-05) A[3]:(0.728880286217)\n",
      " state (11)  A[0]:(0.129516705871) A[1]:(0.881810545921) A[2]:(-0.922915697098) A[3]:(0.801565408707)\n",
      " state (12)  A[0]:(-0.429834365845) A[1]:(0.814569115639) A[2]:(-0.945054829121) A[3]:(0.714730143547)\n",
      " state (13)  A[0]:(-0.000185668468475) A[1]:(0.80918443203) A[2]:(0.900018215179) A[3]:(0.729098796844)\n",
      " state (14)  A[0]:(0.810084581375) A[1]:(0.900052011013) A[2]:(0.999999701977) A[3]:(0.809990882874)\n",
      " state (15)  A[0]:(0.979601502419) A[1]:(0.940350592136) A[2]:(1.0) A[3]:(0.877947151661)\n",
      "Episode 518000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6021. Times reached goal: 968.               Steps done: 3766165. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0208265947411.\n",
      " state (0)  A[0]:(0.531473994255) A[1]:(0.590539693832) A[2]:(0.590428590775) A[3]:(0.531446218491)\n",
      " state (1)  A[0]:(0.531491458416) A[1]:(0.000106230378151) A[2]:(0.656094670296) A[3]:(0.590450763702)\n",
      " state (2)  A[0]:(0.590158641338) A[1]:(0.728653311729) A[2]:(0.591253578663) A[3]:(0.656032085419)\n",
      " state (3)  A[0]:(0.656345486641) A[1]:(-0.0340271741152) A[2]:(0.518969774246) A[3]:(0.552050828934)\n",
      " state (4)  A[0]:(0.590366184711) A[1]:(0.655521631241) A[2]:(0.000359296769602) A[3]:(0.53151845932)\n",
      " state (5)  A[0]:(-0.0274086631835) A[1]:(0.999899327755) A[2]:(-0.803161561489) A[3]:(0.643505930901)\n",
      " state (6)  A[0]:(0.00013467669487) A[1]:(0.8103916049) A[2]:(0.000254273414612) A[3]:(0.656402170658)\n",
      " state (7)  A[0]:(0.549250364304) A[1]:(-0.5450091362) A[2]:(0.427201658487) A[3]:(0.875266909599)\n",
      " state (8)  A[0]:(0.655712902546) A[1]:(0.00149597111158) A[2]:(0.729685783386) A[3]:(0.590460717678)\n",
      " state (9)  A[0]:(0.6561871171) A[1]:(0.810547828674) A[2]:(0.810366153717) A[3]:(0.000655323150568)\n",
      " state (10)  A[0]:(0.729453682899) A[1]:(0.900174856186) A[2]:(0.000925779051613) A[3]:(0.72946703434)\n",
      " state (11)  A[0]:(0.130029857159) A[1]:(0.881948947906) A[2]:(-0.922820985317) A[3]:(0.801976501942)\n",
      " state (12)  A[0]:(-0.429654181004) A[1]:(0.814723968506) A[2]:(-0.945048213005) A[3]:(0.715105652809)\n",
      " state (13)  A[0]:(2.95788049698e-05) A[1]:(0.809346914291) A[2]:(0.900094330311) A[3]:(0.729292392731)\n",
      " state (14)  A[0]:(0.810267806053) A[1]:(0.900188684464) A[2]:(0.999999701977) A[3]:(0.810049653053)\n",
      " state (15)  A[0]:(0.979633808136) A[1]:(0.940457463264) A[2]:(1.0) A[3]:(0.877929568291)\n",
      "Episode 519000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6053. Times reached goal: 982.               Steps done: 3772218. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0207009121253.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5905,  0.5905,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.0007,  0.6561,  0.5902]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.7291,  0.5918,  0.6559]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0006,  0.8104,  0.0016,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.8997, -0.0005,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8096,  0.9003,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530550181866) A[1]:(0.590513825417) A[2]:(0.589895367622) A[3]:(0.531226634979)\n",
      " state (1)  A[0]:(0.53081035614) A[1]:(0.00179283134639) A[2]:(0.656189322472) A[3]:(0.590392947197)\n",
      " state (2)  A[0]:(0.58981192112) A[1]:(0.729422688484) A[2]:(0.592337012291) A[3]:(0.656069397926)\n",
      " state (3)  A[0]:(0.656177043915) A[1]:(-0.0338799729943) A[2]:(0.521002411842) A[3]:(0.551989078522)\n",
      " state (4)  A[0]:(0.59010720253) A[1]:(0.656873941422) A[2]:(0.00202452857047) A[3]:(0.531736195087)\n",
      " state (5)  A[0]:(-0.0270504504442) A[1]:(0.999899685383) A[2]:(-0.803433775902) A[3]:(0.64416128397)\n",
      " state (6)  A[0]:(0.000152438879013) A[1]:(0.810745954514) A[2]:(0.000690102460794) A[3]:(0.656125068665)\n",
      " state (7)  A[0]:(0.549127340317) A[1]:(-0.544368386269) A[2]:(0.427330374718) A[3]:(0.87513512373)\n",
      " state (8)  A[0]:(0.656307458878) A[1]:(0.000418350071413) A[2]:(0.729073941708) A[3]:(0.592349529266)\n",
      " state (9)  A[0]:(0.655554831028) A[1]:(0.810323119164) A[2]:(0.810010313988) A[3]:(0.000525623501744)\n",
      " state (10)  A[0]:(0.728116929531) A[1]:(0.900273621082) A[2]:(-0.000767230812926) A[3]:(0.728689014912)\n",
      " state (11)  A[0]:(0.126748412848) A[1]:(0.882353842258) A[2]:(-0.923181295395) A[3]:(0.801513850689)\n",
      " state (12)  A[0]:(-0.432159364223) A[1]:(0.815705180168) A[2]:(-0.945257246494) A[3]:(0.714803099632)\n",
      " state (13)  A[0]:(-0.00283326930366) A[1]:(0.81055521965) A[2]:(0.900162696838) A[3]:(0.729184746742)\n",
      " state (14)  A[0]:(0.809283137321) A[1]:(0.900837063789) A[2]:(0.999999701977) A[3]:(0.809992074966)\n",
      " state (15)  A[0]:(0.97952735424) A[1]:(0.940765619278) A[2]:(1.0) A[3]:(0.877872228622)\n",
      "Episode 520000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6058. Times reached goal: 986.               Steps done: 3778276. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0205758850889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531153559685) A[1]:(0.59050488472) A[2]:(0.590464949608) A[3]:(0.531225681305)\n",
      " state (1)  A[0]:(0.531073272228) A[1]:(-0.000388115615351) A[2]:(0.656131446362) A[3]:(0.590309977531)\n",
      " state (2)  A[0]:(0.590013027191) A[1]:(0.728896856308) A[2]:(0.591612875462) A[3]:(0.655970215797)\n",
      " state (3)  A[0]:(0.656306385994) A[1]:(-0.0365332737565) A[2]:(0.520456969738) A[3]:(0.551468133926)\n",
      " state (4)  A[0]:(0.590153515339) A[1]:(0.656142473221) A[2]:(0.000462412805064) A[3]:(0.531363904476)\n",
      " state (5)  A[0]:(-0.0264961831272) A[1]:(0.999899208546) A[2]:(-0.804271399975) A[3]:(0.644639015198)\n",
      " state (6)  A[0]:(8.78274440765e-05) A[1]:(0.809974551201) A[2]:(3.7670135498e-05) A[3]:(0.656040549278)\n",
      " state (7)  A[0]:(0.549030780792) A[1]:(-0.544667243958) A[2]:(0.427003234625) A[3]:(0.874999165535)\n",
      " state (8)  A[0]:(0.656284272671) A[1]:(0.000641852500848) A[2]:(0.729073405266) A[3]:(0.591314554214)\n",
      " state (9)  A[0]:(0.656490623951) A[1]:(0.810176610947) A[2]:(0.810024142265) A[3]:(0.000625878514256)\n",
      " state (10)  A[0]:(0.72961640358) A[1]:(0.900021135807) A[2]:(-0.000130534172058) A[3]:(0.729328513145)\n",
      " state (11)  A[0]:(0.130670189857) A[1]:(0.881848812103) A[2]:(-0.923076450825) A[3]:(0.802083194256)\n",
      " state (12)  A[0]:(-0.429128855467) A[1]:(0.814606785774) A[2]:(-0.945284307003) A[3]:(0.715287446976)\n",
      " state (13)  A[0]:(0.000501885951962) A[1]:(0.809133529663) A[2]:(0.900035440922) A[3]:(0.729390203953)\n",
      " state (14)  A[0]:(0.810344696045) A[1]:(0.899963498116) A[2]:(0.999999701977) A[3]:(0.810068011284)\n",
      " state (15)  A[0]:(0.979634284973) A[1]:(0.940210223198) A[2]:(1.0) A[3]:(0.877885758877)\n",
      "Episode 521000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6045. Times reached goal: 984.               Steps done: 3784321. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0204518790494.\n",
      " state (0)  A[0]:(0.531843900681) A[1]:(0.590761780739) A[2]:(0.59083199501) A[3]:(0.531934082508)\n",
      " state (1)  A[0]:(0.531788706779) A[1]:(0.000193193554878) A[2]:(0.655987501144) A[3]:(0.590768277645)\n",
      " state (2)  A[0]:(0.590367913246) A[1]:(0.729188680649) A[2]:(0.591555297375) A[3]:(0.656397998333)\n",
      " state (3)  A[0]:(0.656642079353) A[1]:(-0.0365575142205) A[2]:(0.520921349525) A[3]:(0.551907062531)\n",
      " state (4)  A[0]:(0.590501010418) A[1]:(0.656750798225) A[2]:(0.000645637395792) A[3]:(0.532076537609)\n",
      " state (5)  A[0]:(-0.0257173683494) A[1]:(0.999899566174) A[2]:(-0.804724693298) A[3]:(0.645750641823)\n",
      " state (6)  A[0]:(-7.72327184677e-05) A[1]:(0.810393571854) A[2]:(-0.00033712387085) A[3]:(0.656307220459)\n",
      " state (7)  A[0]:(0.548706948757) A[1]:(-0.544823944569) A[2]:(0.427158087492) A[3]:(0.875020682812)\n",
      " state (8)  A[0]:(0.656460821629) A[1]:(7.54147768021e-05) A[2]:(0.728999853134) A[3]:(0.591964840889)\n",
      " state (9)  A[0]:(0.656674206257) A[1]:(0.810141742229) A[2]:(0.810042798519) A[3]:(0.000963002152275)\n",
      " state (10)  A[0]:(0.729744315147) A[1]:(0.899976015091) A[2]:(0.000103235244751) A[3]:(0.72951489687)\n",
      " state (11)  A[0]:(0.130813971162) A[1]:(0.881777882576) A[2]:(-0.923070847988) A[3]:(0.802305221558)\n",
      " state (12)  A[0]:(-0.42924040556) A[1]:(0.814530313015) A[2]:(-0.945311844349) A[3]:(0.715516626835)\n",
      " state (13)  A[0]:(0.00041419264744) A[1]:(0.809156298637) A[2]:(0.900158703327) A[3]:(0.729499816895)\n",
      " state (14)  A[0]:(0.810452222824) A[1]:(0.900069415569) A[2]:(0.999999701977) A[3]:(0.81009376049)\n",
      " state (15)  A[0]:(0.979657590389) A[1]:(0.940314471722) A[2]:(1.0) A[3]:(0.877854287624)\n",
      "Episode 522000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6045. Times reached goal: 977.               Steps done: 3790366. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0203286203653.\n",
      " state (0)  A[0]:(0.531657040119) A[1]:(0.590464651585) A[2]:(0.590592026711) A[3]:(0.531421124935)\n",
      " state (1)  A[0]:(0.531730413437) A[1]:(-4.39584255219e-05) A[2]:(0.656277298927) A[3]:(0.590595722198)\n",
      " state (2)  A[0]:(0.590519309044) A[1]:(0.729077100754) A[2]:(0.591689288616) A[3]:(0.656201183796)\n",
      " state (3)  A[0]:(0.656652033329) A[1]:(-0.0381399951875) A[2]:(0.521283328533) A[3]:(0.551290750504)\n",
      " state (4)  A[0]:(0.590416908264) A[1]:(0.656270027161) A[2]:(0.000479698152049) A[3]:(0.531609773636)\n",
      " state (5)  A[0]:(-0.025163685903) A[1]:(0.999899327755) A[2]:(-0.805109083652) A[3]:(0.646241784096)\n",
      " state (6)  A[0]:(3.11285257339e-05) A[1]:(0.810063123703) A[2]:(-0.000190496444702) A[3]:(0.656109571457)\n",
      " state (7)  A[0]:(0.548287630081) A[1]:(-0.544567883015) A[2]:(0.427326500416) A[3]:(0.874683380127)\n",
      " state (8)  A[0]:(0.655791819096) A[1]:(0.000237613916397) A[2]:(0.7290558815) A[3]:(0.590893089771)\n",
      " state (9)  A[0]:(0.655900418758) A[1]:(0.81007027626) A[2]:(0.809987425804) A[3]:(5.63263893127e-06)\n",
      " state (10)  A[0]:(0.728974580765) A[1]:(0.900053620338) A[2]:(-0.000528097094502) A[3]:(0.729060709476)\n",
      " state (11)  A[0]:(0.129354208708) A[1]:(0.882045984268) A[2]:(-0.923241376877) A[3]:(0.801994383335)\n",
      " state (12)  A[0]:(-0.430167227983) A[1]:(0.815059185028) A[2]:(-0.94545763731) A[3]:(0.715205550194)\n",
      " state (13)  A[0]:(-0.000845089380164) A[1]:(0.80960726738) A[2]:(0.900027751923) A[3]:(0.72932767868)\n",
      " state (14)  A[0]:(0.809799790382) A[1]:(0.900180220604) A[2]:(0.999999701977) A[3]:(0.810044825077)\n",
      " state (15)  A[0]:(0.97956520319) A[1]:(0.940276861191) A[2]:(1.0) A[3]:(0.877858638763)\n",
      "Episode 523000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6070. Times reached goal: 974.               Steps done: 3796436. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0202055993861.\n",
      " state (0)  A[0]:(0.531229555607) A[1]:(0.590505540371) A[2]:(0.590430378914) A[3]:(0.531153321266)\n",
      " state (1)  A[0]:(0.531122267246) A[1]:(-9.04500484467e-05) A[2]:(0.65600413084) A[3]:(0.589950442314)\n",
      " state (2)  A[0]:(0.589971661568) A[1]:(0.728983044624) A[2]:(0.591451227665) A[3]:(0.655510425568)\n",
      " state (3)  A[0]:(0.656148850918) A[1]:(-0.0394331365824) A[2]:(0.521454393864) A[3]:(0.550103127956)\n",
      " state (4)  A[0]:(0.589840590954) A[1]:(0.655985474586) A[2]:(0.000291705131531) A[3]:(0.530476510525)\n",
      " state (5)  A[0]:(-0.0255586709827) A[1]:(0.999899089336) A[2]:(-0.805446088314) A[3]:(0.645783960819)\n",
      " state (6)  A[0]:(-0.00114569021389) A[1]:(0.809797167778) A[2]:(0.000309348106384) A[3]:(0.654684185982)\n",
      " state (7)  A[0]:(0.547232627869) A[1]:(-0.54421710968) A[2]:(0.427759706974) A[3]:(0.873926639557)\n",
      " state (8)  A[0]:(0.655026674271) A[1]:(0.0006276666536) A[2]:(0.729127943516) A[3]:(0.58911895752)\n",
      " state (9)  A[0]:(0.654888749123) A[1]:(0.810215890408) A[2]:(0.810045957565) A[3]:(-0.00328700547107)\n",
      " state (10)  A[0]:(0.727944493294) A[1]:(0.900139451027) A[2]:(-0.000291228294373) A[3]:(0.727289199829)\n",
      " state (11)  A[0]:(0.127267256379) A[1]:(0.882189929485) A[2]:(-0.923207461834) A[3]:(0.80069565773)\n",
      " state (12)  A[0]:(-0.431540966034) A[1]:(0.815355181694) A[2]:(-0.945394694805) A[3]:(0.713605284691)\n",
      " state (13)  A[0]:(-0.0019807938952) A[1]:(0.809995174408) A[2]:(0.900450348854) A[3]:(0.728029668331)\n",
      " state (14)  A[0]:(0.809556007385) A[1]:(0.900438606739) A[2]:(0.999999701977) A[3]:(0.809290766716)\n",
      " state (15)  A[0]:(0.979543268681) A[1]:(0.940434753895) A[2]:(1.0) A[3]:(0.877438485622)\n",
      "Episode 524000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6066. Times reached goal: 975.               Steps done: 3802502. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0200834032159.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5904,  0.5903,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5900,  0.6558,  0.0002,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557,  0.0006,  0.7291,  0.5905]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.8102,  0.8101,  0.0001]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8092,  0.9000,  0.7293]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9000,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531496882439) A[1]:(0.590372622013) A[2]:(0.590240120888) A[3]:(0.531496405602)\n",
      " state (1)  A[0]:(0.531434059143) A[1]:(-5.91576099396e-05) A[2]:(0.65592443943) A[3]:(0.590332746506)\n",
      " state (2)  A[0]:(0.590059161186) A[1]:(0.729019403458) A[2]:(0.591422438622) A[3]:(0.656017005444)\n",
      " state (3)  A[0]:(0.656239032745) A[1]:(-0.0404478348792) A[2]:(0.521814227104) A[3]:(0.550738334656)\n",
      " state (4)  A[0]:(0.590016841888) A[1]:(0.655836939812) A[2]:(0.000182867050171) A[3]:(0.531624317169)\n",
      " state (5)  A[0]:(-0.0243651773781) A[1]:(0.999899208546) A[2]:(-0.806016027927) A[3]:(0.647681593895)\n",
      " state (6)  A[0]:(1.57356262207e-05) A[1]:(0.810034871101) A[2]:(-0.000275731086731) A[3]:(0.655968785286)\n",
      " state (7)  A[0]:(0.548084616661) A[1]:(-0.544027328491) A[2]:(0.427685678005) A[3]:(0.874402105808)\n",
      " state (8)  A[0]:(0.655970692635) A[1]:(0.000688284519129) A[2]:(0.729064106941) A[3]:(0.591158151627)\n",
      " state (9)  A[0]:(0.656110525131) A[1]:(0.810205757618) A[2]:(0.810088932514) A[3]:(0.000737309339456)\n",
      " state (10)  A[0]:(0.729276061058) A[1]:(0.900055706501) A[2]:(0.000253081321716) A[3]:(0.729281067848)\n",
      " state (11)  A[0]:(0.130373105407) A[1]:(0.881973087788) A[2]:(-0.923158526421) A[3]:(0.802137434483)\n",
      " state (12)  A[0]:(-0.4293435812) A[1]:(0.814819157124) A[2]:(-0.945495724678) A[3]:(0.715231239796)\n",
      " state (13)  A[0]:(0.000186040997505) A[1]:(0.809230089188) A[2]:(0.900045275688) A[3]:(0.729313015938)\n",
      " state (14)  A[0]:(0.810196995735) A[1]:(0.899958491325) A[2]:(0.999999701977) A[3]:(0.810138583183)\n",
      " state (15)  A[0]:(0.97960704565) A[1]:(0.940145015717) A[2]:(1.0) A[3]:(0.877953290939)\n",
      "Episode 525000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6039. Times reached goal: 977.               Steps done: 3808541. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0199624850238.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.5314219594) A[1]:(0.591135799885) A[2]:(0.590388357639) A[3]:(0.531162858009)\n",
      " state (1)  A[0]:(0.531381428242) A[1]:(-6.27338886261e-06) A[2]:(0.656369805336) A[3]:(0.590367794037)\n",
      " state (2)  A[0]:(0.590398311615) A[1]:(0.729142546654) A[2]:(0.591514229774) A[3]:(0.655937194824)\n",
      " state (3)  A[0]:(0.656730055809) A[1]:(-0.041131477803) A[2]:(0.522283613682) A[3]:(0.550219774246)\n",
      " state (4)  A[0]:(0.590540528297) A[1]:(0.656508207321) A[2]:(8.36849212646e-05) A[3]:(0.531212091446)\n",
      " state (5)  A[0]:(-0.0235706716776) A[1]:(0.99989938736) A[2]:(-0.806434392929) A[3]:(0.648221611977)\n",
      " state (6)  A[0]:(5.84125518799e-05) A[1]:(0.810007810593) A[2]:(-0.000241160392761) A[3]:(0.656360089779)\n",
      " state (7)  A[0]:(0.547865867615) A[1]:(-0.544799208641) A[2]:(0.428210437298) A[3]:(0.874474644661)\n",
      " state (8)  A[0]:(0.655999422073) A[1]:(-0.000202015042305) A[2]:(0.729164719582) A[3]:(0.591033935547)\n",
      " state (9)  A[0]:(0.656351089478) A[1]:(0.810063004494) A[2]:(0.809996724129) A[3]:(0.00028184056282)\n",
      " state (10)  A[0]:(0.729434907436) A[1]:(0.900019168854) A[2]:(-0.000168800354004) A[3]:(0.729278922081)\n",
      " state (11)  A[0]:(0.130364269018) A[1]:(0.882037162781) A[2]:(-0.923268675804) A[3]:(0.802316784859)\n",
      " state (12)  A[0]:(-0.429832696915) A[1]:(0.815110206604) A[2]:(-0.945621609688) A[3]:(0.715481519699)\n",
      " state (13)  A[0]:(-0.000770270649809) A[1]:(0.80971968174) A[2]:(0.899922311306) A[3]:(0.729387462139)\n",
      " state (14)  A[0]:(0.809890925884) A[1]:(0.900328576565) A[2]:(0.999999701977) A[3]:(0.810026824474)\n",
      " state (15)  A[0]:(0.979582846165) A[1]:(0.940401852131) A[2]:(1.0) A[3]:(0.877770602703)\n",
      "Episode 526000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6069. Times reached goal: 986.               Steps done: 3814610. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0198416995963.\n",
      " state (0)  A[0]:(0.531162798405) A[1]:(0.590476930141) A[2]:(0.590458989143) A[3]:(0.531319439411)\n",
      " state (1)  A[0]:(0.531305491924) A[1]:(0.000276729464531) A[2]:(0.656073331833) A[3]:(0.590425252914)\n",
      " state (2)  A[0]:(0.590263068676) A[1]:(0.729047954082) A[2]:(0.592113614082) A[3]:(0.656019568443)\n",
      " state (3)  A[0]:(0.656549274921) A[1]:(-0.0423988439143) A[2]:(0.523544549942) A[3]:(0.550170302391)\n",
      " state (4)  A[0]:(0.590379118919) A[1]:(0.655973315239) A[2]:(0.00130212237127) A[3]:(0.531334996223)\n",
      " state (5)  A[0]:(-0.0228573791683) A[1]:(0.99989926815) A[2]:(-0.806679844856) A[3]:(0.648784041405)\n",
      " state (6)  A[0]:(0.000663474085741) A[1]:(0.810037076473) A[2]:(-0.000361800164683) A[3]:(0.655875921249)\n",
      " state (7)  A[0]:(0.5482006073) A[1]:(-0.54403090477) A[2]:(0.427841514349) A[3]:(0.87412250042)\n",
      " state (8)  A[0]:(0.65650165081) A[1]:(-0.000216320157051) A[2]:(0.728845596313) A[3]:(0.591241359711)\n",
      " state (9)  A[0]:(0.656630635262) A[1]:(0.809907913208) A[2]:(0.809995174408) A[3]:(0.000464528769953)\n",
      " state (10)  A[0]:(0.729771196842) A[1]:(0.89996945858) A[2]:(1.90734863281e-05) A[3]:(0.729159772396)\n",
      " state (11)  A[0]:(0.131626591086) A[1]:(0.882002353668) A[2]:(-0.923290371895) A[3]:(0.802163183689)\n",
      " state (12)  A[0]:(-0.428595244884) A[1]:(0.81497746706) A[2]:(-0.945693790913) A[3]:(0.715136766434)\n",
      " state (13)  A[0]:(0.000643759849481) A[1]:(0.809374690056) A[2]:(0.899853646755) A[3]:(0.729067444801)\n",
      " state (14)  A[0]:(0.810248970985) A[1]:(0.899989426136) A[2]:(0.999999701977) A[3]:(0.809912979603)\n",
      " state (15)  A[0]:(0.979609787464) A[1]:(0.940105557442) A[2]:(1.0) A[3]:(0.877762913704)\n",
      "Episode 527000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6022. Times reached goal: 973.               Steps done: 3820632. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0197225719347.\n",
      " state (0)  A[0]:(0.529879450798) A[1]:(0.590344309807) A[2]:(0.589958190918) A[3]:(0.532660603523)\n",
      " state (1)  A[0]:(0.530751049519) A[1]:(-0.00125531782396) A[2]:(0.655862867832) A[3]:(0.59151417017)\n",
      " state (2)  A[0]:(0.589791178703) A[1]:(0.729007482529) A[2]:(0.591191530228) A[3]:(0.65704447031)\n",
      " state (3)  A[0]:(0.656110465527) A[1]:(-0.0439742021263) A[2]:(0.522850394249) A[3]:(0.551032006741)\n",
      " state (4)  A[0]:(0.589791476727) A[1]:(0.656164050102) A[2]:(-0.000497817934956) A[3]:(0.532346606255)\n",
      " state (5)  A[0]:(-0.0231092255563) A[1]:(0.99989938736) A[2]:(-0.807614564896) A[3]:(0.650182962418)\n",
      " state (6)  A[0]:(-0.000666648033075) A[1]:(0.810327529907) A[2]:(-0.0011799329659) A[3]:(0.656132400036)\n",
      " state (7)  A[0]:(0.546490371227) A[1]:(-0.543495893478) A[2]:(0.42745539546) A[3]:(0.873798370361)\n",
      " state (8)  A[0]:(0.654464840889) A[1]:(-0.000473543972475) A[2]:(0.72829914093) A[3]:(0.58992099762)\n",
      " state (9)  A[0]:(0.653806328773) A[1]:(0.809720218182) A[2]:(0.809582591057) A[3]:(-0.00327353505418)\n",
      " state (10)  A[0]:(0.727880060673) A[1]:(0.899947583675) A[2]:(-0.000919699436054) A[3]:(0.72736209631)\n",
      " state (11)  A[0]:(0.129946634173) A[1]:(0.882182478905) A[2]:(-0.923429429531) A[3]:(0.801334023476)\n",
      " state (12)  A[0]:(-0.428293526173) A[1]:(0.815481066704) A[2]:(-0.945886254311) A[3]:(0.714616596699)\n",
      " state (13)  A[0]:(0.00240212213248) A[1]:(0.809976100922) A[2]:(0.899237632751) A[3]:(0.728864371777)\n",
      " state (14)  A[0]:(0.811149597168) A[1]:(0.900388240814) A[2]:(0.999999701977) A[3]:(0.809816837311)\n",
      " state (15)  A[0]:(0.979729950428) A[1]:(0.940431058407) A[2]:(1.0) A[3]:(0.877649486065)\n",
      "Episode 528000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6046. Times reached goal: 984.               Steps done: 3826678. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.01960368901.\n",
      " state (0)  A[0]:(0.53153270483) A[1]:(0.590742230415) A[2]:(0.590594947338) A[3]:(0.531873941422)\n",
      " state (1)  A[0]:(0.531445026398) A[1]:(-0.000432327360613) A[2]:(0.656256437302) A[3]:(0.591006755829)\n",
      " state (2)  A[0]:(0.590433180332) A[1]:(0.72902482748) A[2]:(0.592376470566) A[3]:(0.656662464142)\n",
      " state (3)  A[0]:(0.656762719154) A[1]:(-0.0451339744031) A[2]:(0.524771392345) A[3]:(0.550457775593)\n",
      " state (4)  A[0]:(0.590358793736) A[1]:(0.65630954504) A[2]:(0.00136196531821) A[3]:(0.532224416733)\n",
      " state (5)  A[0]:(-0.0223804730922) A[1]:(0.999899327755) A[2]:(-0.807545781136) A[3]:(0.651228487492)\n",
      " state (6)  A[0]:(0.000218451023102) A[1]:(0.810075640678) A[2]:(-0.000113248825073) A[3]:(0.657256007195)\n",
      " state (7)  A[0]:(0.547912478447) A[1]:(-0.543435096741) A[2]:(0.428508192301) A[3]:(0.874682247639)\n",
      " state (8)  A[0]:(0.657262682915) A[1]:(0.000645562889986) A[2]:(0.728932499886) A[3]:(0.594545543194)\n",
      " state (9)  A[0]:(0.657715916634) A[1]:(0.810470938683) A[2]:(0.810184955597) A[3]:(0.00577806960791)\n",
      " state (10)  A[0]:(0.73057949543) A[1]:(0.900329232216) A[2]:(0.000901460414752) A[3]:(0.731412529945)\n",
      " state (11)  A[0]:(0.132444307208) A[1]:(0.882429480553) A[2]:(-0.923221588135) A[3]:(0.80355489254)\n",
      " state (12)  A[0]:(-0.429307043552) A[1]:(0.815617144108) A[2]:(-0.945707321167) A[3]:(0.716253876686)\n",
      " state (13)  A[0]:(-0.0014920074027) A[1]:(0.810030579567) A[2]:(0.900189757347) A[3]:(0.729501008987)\n",
      " state (14)  A[0]:(0.809341669083) A[1]:(0.900338232517) A[2]:(0.999999701977) A[3]:(0.809968650341)\n",
      " state (15)  A[0]:(0.97949552536) A[1]:(0.940262258053) A[2]:(1.0) A[3]:(0.877674221992)\n",
      "Episode 529000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6074. Times reached goal: 980.               Steps done: 3832752. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0194849770961.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5906,  0.5908,  0.5308]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.0020,  0.6560,  0.5900]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.7289,  0.5916,  0.6556]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8102,  0.0003,  0.6553]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7295,  0.9002,  0.0005,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9003,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531447887421) A[1]:(0.590341329575) A[2]:(0.590555548668) A[3]:(0.531521320343)\n",
      " state (1)  A[0]:(0.531466662884) A[1]:(0.00155834725592) A[2]:(0.656136214733) A[3]:(0.590719819069)\n",
      " state (2)  A[0]:(0.590348362923) A[1]:(0.729021310806) A[2]:(0.592010557652) A[3]:(0.656309366226)\n",
      " state (3)  A[0]:(0.656687140465) A[1]:(-0.0457900539041) A[2]:(0.524797439575) A[3]:(0.549974799156)\n",
      " state (4)  A[0]:(0.590552091599) A[1]:(0.655883669853) A[2]:(0.00140595342964) A[3]:(0.531984329224)\n",
      " state (5)  A[0]:(-0.0208566486835) A[1]:(0.999899208546) A[2]:(-0.807711422443) A[3]:(0.651463329792)\n",
      " state (6)  A[0]:(0.000884294277057) A[1]:(0.809822618961) A[2]:(0.000276684761047) A[3]:(0.656533122063)\n",
      " state (7)  A[0]:(0.547778248787) A[1]:(-0.543784022331) A[2]:(0.428740769625) A[3]:(0.874034523964)\n",
      " state (8)  A[0]:(0.656492233276) A[1]:(-0.000123217701912) A[2]:(0.728888452053) A[3]:(0.59179610014)\n",
      " state (9)  A[0]:(0.656933784485) A[1]:(0.809964716434) A[2]:(0.80992603302) A[3]:(0.00226240954362)\n",
      " state (10)  A[0]:(0.730335950851) A[1]:(0.90000897646) A[2]:(0.000142574310303) A[3]:(0.730296015739)\n",
      " state (11)  A[0]:(0.133341878653) A[1]:(0.88212621212) A[2]:(-0.923331975937) A[3]:(0.803226947784)\n",
      " state (12)  A[0]:(-0.42753392458) A[1]:(0.815253138542) A[2]:(-0.945837140083) A[3]:(0.716354250908)\n",
      " state (13)  A[0]:(0.00133110501338) A[1]:(0.809716820717) A[2]:(0.899875998497) A[3]:(0.72988730669)\n",
      " state (14)  A[0]:(0.810323357582) A[1]:(0.900252342224) A[2]:(0.999999701977) A[3]:(0.810330510139)\n",
      " state (15)  A[0]:(0.979595065117) A[1]:(0.94029289484) A[2]:(1.0) A[3]:(0.877892613411)\n",
      "Episode 530000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6070. Times reached goal: 984.               Steps done: 3838822. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0193670615209.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531227648258) A[1]:(0.59049987793) A[2]:(0.590729653835) A[3]:(0.531395792961)\n",
      " state (1)  A[0]:(0.531123399734) A[1]:(-0.000254139304161) A[2]:(0.656213223934) A[3]:(0.590565681458)\n",
      " state (2)  A[0]:(0.590005278587) A[1]:(0.727934241295) A[2]:(0.591452002525) A[3]:(0.656071841717)\n",
      " state (3)  A[0]:(0.65618211031) A[1]:(-0.0475429259241) A[2]:(0.524292290211) A[3]:(0.549611091614)\n",
      " state (4)  A[0]:(0.589770793915) A[1]:(0.656527101994) A[2]:(0.000349640846252) A[3]:(0.531801581383)\n",
      " state (5)  A[0]:(-0.0215422399342) A[1]:(0.999899506569) A[2]:(-0.808074176311) A[3]:(0.651646494865)\n",
      " state (6)  A[0]:(-0.000504419149365) A[1]:(0.810307085514) A[2]:(-0.000327706336975) A[3]:(0.656047224998)\n",
      " state (7)  A[0]:(0.546776175499) A[1]:(-0.541811347008) A[2]:(0.428627610207) A[3]:(0.873743772507)\n",
      " state (8)  A[0]:(0.655406832695) A[1]:(0.00445960508659) A[2]:(0.729520201683) A[3]:(0.589538216591)\n",
      " state (9)  A[0]:(0.655960023403) A[1]:(0.811378240585) A[2]:(0.810293197632) A[3]:(-0.000587999762502)\n",
      " state (10)  A[0]:(0.728533267975) A[1]:(0.90026050806) A[2]:(-0.00047397610615) A[3]:(0.728574156761)\n",
      " state (11)  A[0]:(0.127896860242) A[1]:(0.881665885448) A[2]:(-0.923591673374) A[3]:(0.801528215408)\n",
      " state (12)  A[0]:(-0.430622786283) A[1]:(0.813611745834) A[2]:(-0.945921003819) A[3]:(0.714610934258)\n",
      " state (13)  A[0]:(0.000171825289726) A[1]:(0.807477295399) A[2]:(0.900141060352) A[3]:(0.729173600674)\n",
      " state (14)  A[0]:(0.810382306576) A[1]:(0.898954749107) A[2]:(0.999999701977) A[3]:(0.810389339924)\n",
      " state (15)  A[0]:(0.979591786861) A[1]:(0.939526736736) A[2]:(1.0) A[3]:(0.87816798687)\n",
      "Episode 531000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6023. Times reached goal: 979.               Steps done: 3844845. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.01925076429.\n",
      " state (0)  A[0]:(0.531197845936) A[1]:(0.590266942978) A[2]:(0.590337574482) A[3]:(0.531311988831)\n",
      " state (1)  A[0]:(0.531139969826) A[1]:(-0.000192269682884) A[2]:(0.655919849873) A[3]:(0.590631723404)\n",
      " state (2)  A[0]:(0.59020626545) A[1]:(0.728811264038) A[2]:(0.590784311295) A[3]:(0.656141042709)\n",
      " state (3)  A[0]:(0.656316518784) A[1]:(-0.0459474660456) A[2]:(0.523455739021) A[3]:(0.549356639385)\n",
      " state (4)  A[0]:(0.590126574039) A[1]:(0.655635714531) A[2]:(-0.000170469284058) A[3]:(0.53143119812)\n",
      " state (5)  A[0]:(-0.0209724996239) A[1]:(0.99989926815) A[2]:(-0.808168709278) A[3]:(0.651950597763)\n",
      " state (6)  A[0]:(-0.000153437256813) A[1]:(0.809968948364) A[2]:(-0.000262022018433) A[3]:(0.65619468689)\n",
      " state (7)  A[0]:(0.546596825123) A[1]:(-0.543849289417) A[2]:(0.428621381521) A[3]:(0.873623847961)\n",
      " state (8)  A[0]:(0.655566930771) A[1]:(-0.000416621536715) A[2]:(0.728797852993) A[3]:(0.59091937542)\n",
      " state (9)  A[0]:(0.655719876289) A[1]:(0.809940159321) A[2]:(0.809887886047) A[3]:(0.00051808351418)\n",
      " state (10)  A[0]:(0.728944659233) A[1]:(0.899940431118) A[2]:(-0.000414013833506) A[3]:(0.729159235954)\n",
      " state (11)  A[0]:(0.13008852303) A[1]:(0.882009446621) A[2]:(-0.923551678658) A[3]:(0.80232334137)\n",
      " state (12)  A[0]:(-0.429760038853) A[1]:(0.81506305933) A[2]:(-0.946027338505) A[3]:(0.715340614319)\n",
      " state (13)  A[0]:(-0.000335559248924) A[1]:(0.809560894966) A[2]:(0.899914860725) A[3]:(0.729252755642)\n",
      " state (14)  A[0]:(0.810120046139) A[1]:(0.900224208832) A[2]:(0.999999701977) A[3]:(0.810095906258)\n",
      " state (15)  A[0]:(0.97960793972) A[1]:(0.940264225006) A[2]:(1.0) A[3]:(0.877845048904)\n",
      "Episode 532000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6034. Times reached goal: 977.               Steps done: 3850879. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0191349549265.\n",
      " state (0)  A[0]:(0.5315361619) A[1]:(0.590452432632) A[2]:(0.590454101562) A[3]:(0.531454801559)\n",
      " state (1)  A[0]:(0.531519711018) A[1]:(-6.89774751663e-05) A[2]:(0.656038761139) A[3]:(0.590530633926)\n",
      " state (2)  A[0]:(0.590347290039) A[1]:(0.728977441788) A[2]:(0.590797066689) A[3]:(0.656084120274)\n",
      " state (3)  A[0]:(0.656465530396) A[1]:(-0.0453887991607) A[2]:(0.523503959179) A[3]:(0.549272119999)\n",
      " state (4)  A[0]:(0.590334773064) A[1]:(0.656051397324) A[2]:(-0.000150680541992) A[3]:(0.531498670578)\n",
      " state (5)  A[0]:(-0.0206600669771) A[1]:(0.99989926815) A[2]:(-0.808247745037) A[3]:(0.652277708054)\n",
      " state (6)  A[0]:(0.000116944313049) A[1]:(0.809963047504) A[2]:(-9.20295715332e-05) A[3]:(0.656094670296)\n",
      " state (7)  A[0]:(0.546882629395) A[1]:(-0.543537259102) A[2]:(0.428935140371) A[3]:(0.873552858829)\n",
      " state (8)  A[0]:(0.655939102173) A[1]:(-0.000154092907906) A[2]:(0.728954970837) A[3]:(0.590865135193)\n",
      " state (9)  A[0]:(0.656132936478) A[1]:(0.809969246387) A[2]:(0.80999392271) A[3]:(0.000436097354395)\n",
      " state (10)  A[0]:(0.729307174683) A[1]:(0.899982094765) A[2]:(-3.65972518921e-05) A[3]:(0.729227244854)\n",
      " state (11)  A[0]:(0.130860999227) A[1]:(0.882059752941) A[2]:(-0.923544585705) A[3]:(0.802380084991)\n",
      " state (12)  A[0]:(-0.4291690588) A[1]:(0.815076172352) A[2]:(-0.946063876152) A[3]:(0.715366959572)\n",
      " state (13)  A[0]:(7.07060098648e-05) A[1]:(0.80946803093) A[2]:(0.899986863136) A[3]:(0.72924053669)\n",
      " state (14)  A[0]:(0.810039818287) A[1]:(0.900123357773) A[2]:(0.999999701977) A[3]:(0.810059964657)\n",
      " state (15)  A[0]:(0.979577064514) A[1]:(0.940168201923) A[2]:(1.0) A[3]:(0.877811908722)\n",
      "Episode 533000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6051. Times reached goal: 980.               Steps done: 3856930. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0190195189181.\n",
      " state (0)  A[0]:(0.531833648682) A[1]:(0.590817809105) A[2]:(0.590576171875) A[3]:(0.531754434109)\n",
      " state (1)  A[0]:(0.531670451164) A[1]:(-0.000150442123413) A[2]:(0.656068921089) A[3]:(0.590395450592)\n",
      " state (2)  A[0]:(0.590333461761) A[1]:(0.729429662228) A[2]:(0.590659379959) A[3]:(0.656093001366)\n",
      " state (3)  A[0]:(0.656414866447) A[1]:(-0.0439774878323) A[2]:(0.523475050926) A[3]:(0.549332201481)\n",
      " state (4)  A[0]:(0.590265512466) A[1]:(0.6566426754) A[2]:(0.00011146068573) A[3]:(0.531650304794)\n",
      " state (5)  A[0]:(-0.020884219557) A[1]:(0.999899566174) A[2]:(-0.8082062006) A[3]:(0.652563035488)\n",
      " state (6)  A[0]:(-0.000133335590363) A[1]:(0.810457527637) A[2]:(0.000113129615784) A[3]:(0.655919790268)\n",
      " state (7)  A[0]:(0.546688199043) A[1]:(-0.543110966682) A[2]:(0.429605424404) A[3]:(0.87336063385)\n",
      " state (8)  A[0]:(0.655858099461) A[1]:(0.000818729226012) A[2]:(0.729603886604) A[3]:(0.589887022972)\n",
      " state (9)  A[0]:(0.656193733215) A[1]:(0.810267746449) A[2]:(0.81033295393) A[3]:(-0.000516533793416)\n",
      " state (10)  A[0]:(0.728633880615) A[1]:(0.900040328503) A[2]:(-0.000298142433167) A[3]:(0.728559732437)\n",
      " state (11)  A[0]:(0.128187403083) A[1]:(0.881972908974) A[2]:(-0.923740744591) A[3]:(0.801701068878)\n",
      " state (12)  A[0]:(-0.430829972029) A[1]:(0.814731121063) A[2]:(-0.946190416813) A[3]:(0.714835882187)\n",
      " state (13)  A[0]:(-0.000958621210884) A[1]:(0.808955311775) A[2]:(0.899962842464) A[3]:(0.729301929474)\n",
      " state (14)  A[0]:(0.809770464897) A[1]:(0.899824202061) A[2]:(0.999999701977) A[3]:(0.810431838036)\n",
      " state (15)  A[0]:(0.979535996914) A[1]:(0.939996242523) A[2]:(1.0) A[3]:(0.878187656403)\n",
      "Episode 534000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6062. Times reached goal: 981.               Steps done: 3862992. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0189045713525.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5906,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.6560, -0.0000,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6555,  0.0002,  0.7290,  0.5902]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556,  0.8101,  0.8100, -0.0009]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  0.8094,  0.9001,  0.7288]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9000,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531700134277) A[1]:(0.590598285198) A[2]:(0.590534806252) A[3]:(0.531545579433)\n",
      " state (1)  A[0]:(0.531815707684) A[1]:(6.53117895126e-05) A[2]:(0.656133532524) A[3]:(0.59050655365)\n",
      " state (2)  A[0]:(0.590594470501) A[1]:(0.729025185108) A[2]:(0.590910494328) A[3]:(0.656172573566)\n",
      " state (3)  A[0]:(0.656619608402) A[1]:(-0.0449547506869) A[2]:(0.523548066616) A[3]:(0.549221217632)\n",
      " state (4)  A[0]:(0.590639472008) A[1]:(0.655902147293) A[2]:(-2.13384628296e-05) A[3]:(0.53154361248)\n",
      " state (5)  A[0]:(-0.0200777631253) A[1]:(0.99989926815) A[2]:(-0.808326363564) A[3]:(0.65274643898)\n",
      " state (6)  A[0]:(0.000242203474045) A[1]:(0.810001552105) A[2]:(-7.49826431274e-05) A[3]:(0.656092047691)\n",
      " state (7)  A[0]:(0.547035813332) A[1]:(-0.543223798275) A[2]:(0.429012477398) A[3]:(0.873529791832)\n",
      " state (8)  A[0]:(0.656277179718) A[1]:(9.2014670372e-05) A[2]:(0.728916406631) A[3]:(0.591181218624)\n",
      " state (9)  A[0]:(0.656323671341) A[1]:(0.810066342354) A[2]:(0.810019910336) A[3]:(0.000369071931345)\n",
      " state (10)  A[0]:(0.729384243488) A[1]:(0.900007605553) A[2]:(0.000113487243652) A[3]:(0.729091703892)\n",
      " state (11)  A[0]:(0.130934983492) A[1]:(0.882020354271) A[2]:(-0.923630475998) A[3]:(0.802231729031)\n",
      " state (12)  A[0]:(-0.428933829069) A[1]:(0.814884126186) A[2]:(-0.946211755276) A[3]:(0.715146541595)\n",
      " state (13)  A[0]:(0.000766441051383) A[1]:(0.809174537659) A[2]:(0.899976551533) A[3]:(0.729084432125)\n",
      " state (14)  A[0]:(0.810365617275) A[1]:(0.899999380112) A[2]:(0.999999701977) A[3]:(0.80997979641)\n",
      " state (15)  A[0]:(0.979613125324) A[1]:(0.940123081207) A[2]:(1.0) A[3]:(0.877755284309)\n",
      "Episode 535000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6080. Times reached goal: 982.               Steps done: 3869072. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0187899802686.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531782269478) A[1]:(0.590354859829) A[2]:(0.5905803442) A[3]:(0.531363844872)\n",
      " state (1)  A[0]:(0.531404495239) A[1]:(6.33150339127e-05) A[2]:(0.656109988689) A[3]:(0.590525507927)\n",
      " state (2)  A[0]:(0.590230345726) A[1]:(0.72888302803) A[2]:(0.590795218945) A[3]:(0.656160354614)\n",
      " state (3)  A[0]:(0.656234383583) A[1]:(-0.0450431145728) A[2]:(0.523456573486) A[3]:(0.549220442772)\n",
      " state (4)  A[0]:(0.590246081352) A[1]:(0.655691564083) A[2]:(6.05583190918e-05) A[3]:(0.531798779964)\n",
      " state (5)  A[0]:(-0.0203661173582) A[1]:(0.999899208546) A[2]:(-0.808245778084) A[3]:(0.653529286385)\n",
      " state (6)  A[0]:(1.0684132576e-05) A[1]:(0.809873461723) A[2]:(-2.7060508728e-05) A[3]:(0.65669131279)\n",
      " state (7)  A[0]:(0.546830832958) A[1]:(-0.543977737427) A[2]:(0.42905575037) A[3]:(0.873729944229)\n",
      " state (8)  A[0]:(0.656188368797) A[1]:(-0.00106035138015) A[2]:(0.728947818279) A[3]:(0.59195792675)\n",
      " state (9)  A[0]:(0.656518697739) A[1]:(0.809744894505) A[2]:(0.810124456882) A[3]:(0.00236573372968)\n",
      " state (10)  A[0]:(0.729904055595) A[1]:(0.899902701378) A[2]:(0.000766753975768) A[3]:(0.730409383774)\n",
      " state (11)  A[0]:(0.132354393601) A[1]:(0.882022678852) A[2]:(-0.923567116261) A[3]:(0.803367137909)\n",
      " state (12)  A[0]:(-0.428259551525) A[1]:(0.815071940422) A[2]:(-0.946229219437) A[3]:(0.716517448425)\n",
      " state (13)  A[0]:(0.00080984813394) A[1]:(0.809549808502) A[2]:(0.900121688843) A[3]:(0.730145335197)\n",
      " state (14)  A[0]:(0.810202121735) A[1]:(0.900300621986) A[2]:(0.999999701977) A[3]:(0.810619175434)\n",
      " state (15)  A[0]:(0.979586899281) A[1]:(0.940323472023) A[2]:(1.0) A[3]:(0.878122925758)\n",
      "Episode 536000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6049. Times reached goal: 979.               Steps done: 3875121. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0186766627523.\n",
      " state (0)  A[0]:(0.530918776989) A[1]:(0.59056019783) A[2]:(0.590588212013) A[3]:(0.530996024609)\n",
      " state (1)  A[0]:(0.530871272087) A[1]:(-0.000154122710228) A[2]:(0.656265377998) A[3]:(0.58967089653)\n",
      " state (2)  A[0]:(0.589723050594) A[1]:(0.729088544846) A[2]:(0.590896070004) A[3]:(0.655446887016)\n",
      " state (3)  A[0]:(0.655759334564) A[1]:(-0.0444051921368) A[2]:(0.523661315441) A[3]:(0.548150539398)\n",
      " state (4)  A[0]:(0.58971452713) A[1]:(0.655949056149) A[2]:(0.00057387346169) A[3]:(0.530647516251)\n",
      " state (5)  A[0]:(-0.0212383512408) A[1]:(0.99989926815) A[2]:(-0.807992398739) A[3]:(0.652828574181)\n",
      " state (6)  A[0]:(-0.00113004399464) A[1]:(0.809934020042) A[2]:(0.00116229010746) A[3]:(0.655632734299)\n",
      " state (7)  A[0]:(0.545706152916) A[1]:(-0.543322563171) A[2]:(0.429940164089) A[3]:(0.87300568819)\n",
      " state (8)  A[0]:(0.65492784977) A[1]:(-0.000756844761781) A[2]:(0.729143381119) A[3]:(0.588684022427)\n",
      " state (9)  A[0]:(0.654891848564) A[1]:(0.809618413448) A[2]:(0.809968233109) A[3]:(-0.0047346893698)\n",
      " state (10)  A[0]:(0.72845107317) A[1]:(0.899855434895) A[2]:(-0.00023341178894) A[3]:(0.726814925671)\n",
      " state (11)  A[0]:(0.130008429289) A[1]:(0.882036328316) A[2]:(-0.923806846142) A[3]:(0.800910770893)\n",
      " state (12)  A[0]:(-0.428987443447) A[1]:(0.815100550652) A[2]:(-0.946456193924) A[3]:(0.713853776455)\n",
      " state (13)  A[0]:(0.00108906580135) A[1]:(0.809466302395) A[2]:(0.899703979492) A[3]:(0.728212237358)\n",
      " state (14)  A[0]:(0.810450911522) A[1]:(0.900209665298) A[2]:(0.999999701977) A[3]:(0.809512317181)\n",
      " state (15)  A[0]:(0.979619681835) A[1]:(0.940263032913) A[2]:(1.0) A[3]:(0.877505779266)\n",
      "Episode 537000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6030. Times reached goal: 973.               Steps done: 3881151. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0185643813446.\n",
      " state (0)  A[0]:(0.531209886074) A[1]:(0.590639770031) A[2]:(0.590428113937) A[3]:(0.531400799751)\n",
      " state (1)  A[0]:(0.531336307526) A[1]:(-3.24845314026e-06) A[2]:(0.656101107597) A[3]:(0.590526223183)\n",
      " state (2)  A[0]:(0.590331017971) A[1]:(0.729130446911) A[2]:(0.590594649315) A[3]:(0.65608561039)\n",
      " state (3)  A[0]:(0.656286478043) A[1]:(-0.0441101789474) A[2]:(0.523288846016) A[3]:(0.548979401588)\n",
      " state (4)  A[0]:(0.59026658535) A[1]:(0.656339526176) A[2]:(-0.000186324119568) A[3]:(0.531514167786)\n",
      " state (5)  A[0]:(-0.0203589834273) A[1]:(0.999899327755) A[2]:(-0.808435559273) A[3]:(0.653442502022)\n",
      " state (6)  A[0]:(4.42564487457e-06) A[1]:(0.809949874878) A[2]:(5.43594360352e-05) A[3]:(0.656323552132)\n",
      " state (7)  A[0]:(0.546767830849) A[1]:(-0.543898284435) A[2]:(0.429556638002) A[3]:(0.873483479023)\n",
      " state (8)  A[0]:(0.656009793282) A[1]:(-0.000511795224156) A[2]:(0.729129076004) A[3]:(0.590770244598)\n",
      " state (9)  A[0]:(0.656153559685) A[1]:(0.809967756271) A[2]:(0.809941530228) A[3]:(0.000300914049149)\n",
      " state (10)  A[0]:(0.729022860527) A[1]:(0.899954319) A[2]:(-0.000673293950967) A[3]:(0.729175209999)\n",
      " state (11)  A[0]:(0.129535093904) A[1]:(0.881972670555) A[2]:(-0.923947691917) A[3]:(0.80223429203)\n",
      " state (12)  A[0]:(-0.430329442024) A[1]:(0.814838111401) A[2]:(-0.946536839008) A[3]:(0.715102434158)\n",
      " state (13)  A[0]:(-0.000976428098511) A[1]:(0.809203445911) A[2]:(0.899942040443) A[3]:(0.729072570801)\n",
      " state (14)  A[0]:(0.809767484665) A[1]:(0.900111377239) A[2]:(0.999999701977) A[3]:(0.809986293316)\n",
      " state (15)  A[0]:(0.979551196098) A[1]:(0.94018381834) A[2]:(1.0) A[3]:(0.877767860889)\n",
      "Episode 538000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6064. Times reached goal: 990.               Steps done: 3887215. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0184521475728.\n",
      " state (0)  A[0]:(0.531737804413) A[1]:(0.590474247932) A[2]:(0.590090274811) A[3]:(0.531898796558)\n",
      " state (1)  A[0]:(0.531535029411) A[1]:(-6.27189874649e-05) A[2]:(0.65610063076) A[3]:(0.590558290482)\n",
      " state (2)  A[0]:(0.590422153473) A[1]:(0.729276418686) A[2]:(0.59085059166) A[3]:(0.656247019768)\n",
      " state (3)  A[0]:(0.656397759914) A[1]:(-0.0437633395195) A[2]:(0.523476362228) A[3]:(0.549119591713)\n",
      " state (4)  A[0]:(0.590530395508) A[1]:(0.655989766121) A[2]:(0.000105023384094) A[3]:(0.531670093536)\n",
      " state (5)  A[0]:(-0.019939744845) A[1]:(0.999899327755) A[2]:(-0.80845785141) A[3]:(0.653723418713)\n",
      " state (6)  A[0]:(0.000214949250221) A[1]:(0.810014307499) A[2]:(-0.000126004219055) A[3]:(0.656273126602)\n",
      " state (7)  A[0]:(0.546684145927) A[1]:(-0.544018387794) A[2]:(0.429262131453) A[3]:(0.873331546783)\n",
      " state (8)  A[0]:(0.655867815018) A[1]:(-0.00144649937283) A[2]:(0.728879928589) A[3]:(0.590791463852)\n",
      " state (9)  A[0]:(0.655771493912) A[1]:(0.809686541557) A[2]:(0.809985160828) A[3]:(-0.000114589929581)\n",
      " state (10)  A[0]:(0.72910618782) A[1]:(0.899944126606) A[2]:(2.36034393311e-05) A[3]:(0.729181528091)\n",
      " state (11)  A[0]:(0.130472183228) A[1]:(0.882201194763) A[2]:(-0.923886537552) A[3]:(0.802520513535)\n",
      " state (12)  A[0]:(-0.429999828339) A[1]:(0.815490961075) A[2]:(-0.946589887142) A[3]:(0.715444982052)\n",
      " state (13)  A[0]:(-0.00125722517259) A[1]:(0.810075283051) A[2]:(0.900002241135) A[3]:(0.729207217693)\n",
      " state (14)  A[0]:(0.809657812119) A[1]:(0.900633811951) A[2]:(0.999999701977) A[3]:(0.8099796772)\n",
      " state (15)  A[0]:(0.979560375214) A[1]:(0.940461575985) A[2]:(1.0) A[3]:(0.877738595009)\n",
      "Episode 539000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6066. Times reached goal: 981.               Steps done: 3893281. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0183405556462.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5903,  0.5905,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0001,  0.6561,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.7290,  0.5907,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8100,  0.0001,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0004,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9000,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531330525875) A[1]:(0.590284943581) A[2]:(0.590481042862) A[3]:(0.531339347363)\n",
      " state (1)  A[0]:(0.531319975853) A[1]:(0.000115528702736) A[2]:(0.656095802784) A[3]:(0.590480208397)\n",
      " state (2)  A[0]:(0.590270280838) A[1]:(0.729044795036) A[2]:(0.590734124184) A[3]:(0.656072974205)\n",
      " state (3)  A[0]:(0.656161785126) A[1]:(-0.0440615899861) A[2]:(0.523417234421) A[3]:(0.548781991005)\n",
      " state (4)  A[0]:(0.590175032616) A[1]:(0.656163215637) A[2]:(4.92334365845e-05) A[3]:(0.531339347363)\n",
      " state (5)  A[0]:(-0.0203764550388) A[1]:(0.999899327755) A[2]:(-0.808460354805) A[3]:(0.653651714325)\n",
      " state (6)  A[0]:(-2.30669975281e-05) A[1]:(0.810096561909) A[2]:(4.48226928711e-05) A[3]:(0.656037151814)\n",
      " state (7)  A[0]:(0.546696186066) A[1]:(-0.542874872684) A[2]:(0.429440736771) A[3]:(0.873231828213)\n",
      " state (8)  A[0]:(0.655927419662) A[1]:(0.000475779146655) A[2]:(0.729113340378) A[3]:(0.590424180031)\n",
      " state (9)  A[0]:(0.656033635139) A[1]:(0.810139536858) A[2]:(0.810054302216) A[3]:(3.65376472473e-05)\n",
      " state (10)  A[0]:(0.729117631912) A[1]:(0.900049269199) A[2]:(-0.000180840492249) A[3]:(0.729008674622)\n",
      " state (11)  A[0]:(0.130313917994) A[1]:(0.882088780403) A[2]:(-0.923985362053) A[3]:(0.802119731903)\n",
      " state (12)  A[0]:(-0.429441392422) A[1]:(0.814943015575) A[2]:(-0.946661174297) A[3]:(0.71494603157)\n",
      " state (13)  A[0]:(0.000193476676941) A[1]:(0.809194087982) A[2]:(0.900033175945) A[3]:(0.728999197483)\n",
      " state (14)  A[0]:(0.810128450394) A[1]:(0.900061666965) A[2]:(0.999999701977) A[3]:(0.810019910336)\n",
      " state (15)  A[0]:(0.979589402676) A[1]:(0.940095067024) A[2]:(1.0) A[3]:(0.877849578857)\n",
      "Episode 540000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6067. Times reached goal: 989.               Steps done: 3899348. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0182296203575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530599355698) A[1]:(0.59217941761) A[2]:(0.590802192688) A[3]:(0.530694365501)\n",
      " state (1)  A[0]:(0.531205654144) A[1]:(7.04228878021e-05) A[2]:(0.657134473324) A[3]:(0.590885341167)\n",
      " state (2)  A[0]:(0.591145157814) A[1]:(0.729368567467) A[2]:(0.591448903084) A[3]:(0.656696856022)\n",
      " state (3)  A[0]:(0.657030582428) A[1]:(-0.0439255237579) A[2]:(0.523872613907) A[3]:(0.54925775528)\n",
      " state (4)  A[0]:(0.591192603111) A[1]:(0.655786156654) A[2]:(0.000304222106934) A[3]:(0.531818270683)\n",
      " state (5)  A[0]:(-0.019621476531) A[1]:(0.99989926815) A[2]:(-0.808592677116) A[3]:(0.654551506042)\n",
      " state (6)  A[0]:(-0.000224471092224) A[1]:(0.81016933918) A[2]:(-0.000627517641988) A[3]:(0.656933546066)\n",
      " state (7)  A[0]:(0.546144366264) A[1]:(-0.542784452438) A[2]:(0.428642183542) A[3]:(0.873553574085)\n",
      " state (8)  A[0]:(0.655549287796) A[1]:(-0.000955015129875) A[2]:(0.728588700294) A[3]:(0.591829776764)\n",
      " state (9)  A[0]:(0.654769539833) A[1]:(0.809625983238) A[2]:(0.810078322887) A[3]:(-0.00070703017991)\n",
      " state (10)  A[0]:(0.727910518646) A[1]:(0.899980187416) A[2]:(0.000112891197205) A[3]:(0.728347420692)\n",
      " state (11)  A[0]:(0.128143668175) A[1]:(0.882295608521) A[2]:(-0.924022614956) A[3]:(0.801883935928)\n",
      " state (12)  A[0]:(-0.431144237518) A[1]:(0.815562546253) A[2]:(-0.946750462055) A[3]:(0.714875757694)\n",
      " state (13)  A[0]:(-0.00183507590555) A[1]:(0.8099386096) A[2]:(0.900043725967) A[3]:(0.729041457176)\n",
      " state (14)  A[0]:(0.809521853924) A[1]:(0.900437772274) A[2]:(0.999999701977) A[3]:(0.810088515282)\n",
      " state (15)  A[0]:(0.97954005003) A[1]:(0.940258562565) A[2]:(1.0) A[3]:(0.877914786339)\n",
      "Episode 541000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6041. Times reached goal: 983.               Steps done: 3905389. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0181198271851.\n",
      " state (0)  A[0]:(0.531590819359) A[1]:(0.590496301651) A[2]:(0.590427517891) A[3]:(0.531528711319)\n",
      " state (1)  A[0]:(0.531496286392) A[1]:(-0.000246539711952) A[2]:(0.656059682369) A[3]:(0.590479850769)\n",
      " state (2)  A[0]:(0.590415477753) A[1]:(0.729045271873) A[2]:(0.590590059757) A[3]:(0.656111061573)\n",
      " state (3)  A[0]:(0.656290769577) A[1]:(-0.0440108142793) A[2]:(0.523183345795) A[3]:(0.548749983311)\n",
      " state (4)  A[0]:(0.590356349945) A[1]:(0.655919551849) A[2]:(-0.000222444534302) A[3]:(0.531469345093)\n",
      " state (5)  A[0]:(-0.0202870089561) A[1]:(0.99989926815) A[2]:(-0.808559417725) A[3]:(0.654205918312)\n",
      " state (6)  A[0]:(8.50856304169e-06) A[1]:(0.809963285923) A[2]:(-7.9870223999e-05) A[3]:(0.656240582466)\n",
      " state (7)  A[0]:(0.546681702137) A[1]:(-0.543313860893) A[2]:(0.429392427206) A[3]:(0.873238146305)\n",
      " state (8)  A[0]:(0.655851840973) A[1]:(-0.000236287713051) A[2]:(0.728958964348) A[3]:(0.59040915966)\n",
      " state (9)  A[0]:(0.655818462372) A[1]:(0.809911251068) A[2]:(0.809852719307) A[3]:(-0.000105768442154)\n",
      " state (10)  A[0]:(0.72882604599) A[1]:(0.899944543839) A[2]:(-0.00100398028735) A[3]:(0.728909611702)\n",
      " state (11)  A[0]:(0.129469513893) A[1]:(0.882002830505) A[2]:(-0.924236118793) A[3]:(0.802025556564)\n",
      " state (12)  A[0]:(-0.430190831423) A[1]:(0.814836978912) A[2]:(-0.946922063828) A[3]:(0.714816212654)\n",
      " state (13)  A[0]:(-0.000498220266309) A[1]:(0.809103488922) A[2]:(0.899887561798) A[3]:(0.728958368301)\n",
      " state (14)  A[0]:(0.810006797314) A[1]:(0.900062561035) A[2]:(0.999999701977) A[3]:(0.810081720352)\n",
      " state (15)  A[0]:(0.979584038258) A[1]:(0.940100550652) A[2]:(1.0) A[3]:(0.877929568291)\n",
      "Episode 542000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6039. Times reached goal: 981.               Steps done: 3911428. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0180107312953.\n",
      " state (0)  A[0]:(0.531442582607) A[1]:(0.591063678265) A[2]:(0.590439200401) A[3]:(0.530798196793)\n",
      " state (1)  A[0]:(0.530413627625) A[1]:(0.00131483294535) A[2]:(0.656544446945) A[3]:(0.590221762657)\n",
      " state (2)  A[0]:(0.589279830456) A[1]:(0.729339599609) A[2]:(0.591452419758) A[3]:(0.656047701836)\n",
      " state (3)  A[0]:(0.655195236206) A[1]:(-0.0429935343564) A[2]:(0.524413228035) A[3]:(0.548664867878)\n",
      " state (4)  A[0]:(0.588583350182) A[1]:(0.657126069069) A[2]:(0.00163626519497) A[3]:(0.53141450882)\n",
      " state (5)  A[0]:(-0.0246692597866) A[1]:(0.999899804592) A[2]:(-0.807874798775) A[3]:(0.654294312)\n",
      " state (6)  A[0]:(-0.00564138544723) A[1]:(0.810692071915) A[2]:(0.00168108777143) A[3]:(0.655739784241)\n",
      " state (7)  A[0]:(0.541234254837) A[1]:(-0.542548060417) A[2]:(0.430972069502) A[3]:(0.872488081455)\n",
      " state (8)  A[0]:(0.649646878242) A[1]:(0.00101943279151) A[2]:(0.729878723621) A[3]:(0.58583676815)\n",
      " state (9)  A[0]:(0.648272037506) A[1]:(0.810000121593) A[2]:(0.809492230415) A[3]:(-0.00623066117987)\n",
      " state (10)  A[0]:(0.719890475273) A[1]:(0.900086522102) A[2]:(-0.00675930222496) A[3]:(0.725191950798)\n",
      " state (11)  A[0]:(0.107472062111) A[1]:(0.882477939129) A[2]:(-0.92543810606) A[3]:(0.79886496067)\n",
      " state (12)  A[0]:(-0.446229279041) A[1]:(0.816040456295) A[2]:(-0.947464942932) A[3]:(0.711754560471)\n",
      " state (13)  A[0]:(-0.0164932254702) A[1]:(0.810684919357) A[2]:(0.900171220303) A[3]:(0.727391600609)\n",
      " state (14)  A[0]:(0.805423617363) A[1]:(0.900936603546) A[2]:(0.999999761581) A[3]:(0.809501230717)\n",
      " state (15)  A[0]:(0.97913813591) A[1]:(0.940485954285) A[2]:(1.0) A[3]:(0.877760529518)\n",
      "Episode 543000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6049. Times reached goal: 985.               Steps done: 3917477. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0179021132283.\n",
      " state (0)  A[0]:(0.531730055809) A[1]:(0.590409040451) A[2]:(0.590368270874) A[3]:(0.531137168407)\n",
      " state (1)  A[0]:(0.531599760056) A[1]:(0.000203222036362) A[2]:(0.655986189842) A[3]:(0.590385556221)\n",
      " state (2)  A[0]:(0.590539157391) A[1]:(0.728953003883) A[2]:(0.590492129326) A[3]:(0.656057834625)\n",
      " state (3)  A[0]:(0.656410336494) A[1]:(-0.0439076758921) A[2]:(0.523186504841) A[3]:(0.548674285412)\n",
      " state (4)  A[0]:(0.590577125549) A[1]:(0.655977487564) A[2]:(-0.000107884407043) A[3]:(0.531544446945)\n",
      " state (5)  A[0]:(-0.0197737962008) A[1]:(0.99989926815) A[2]:(-0.808600664139) A[3]:(0.654602766037)\n",
      " state (6)  A[0]:(0.000460430950625) A[1]:(0.809899449348) A[2]:(-0.000136017799377) A[3]:(0.656435251236)\n",
      " state (7)  A[0]:(0.547047257423) A[1]:(-0.543121218681) A[2]:(0.429292470217) A[3]:(0.873383998871)\n",
      " state (8)  A[0]:(0.656527578831) A[1]:(-0.000636011303868) A[2]:(0.728875637054) A[3]:(0.592067837715)\n",
      " state (9)  A[0]:(0.656327009201) A[1]:(0.809782147408) A[2]:(0.810087263584) A[3]:(0.00216844328679)\n",
      " state (10)  A[0]:(0.729304075241) A[1]:(0.899899125099) A[2]:(0.000357747048838) A[3]:(0.72991412878)\n",
      " state (11)  A[0]:(0.130720227957) A[1]:(0.88194590807) A[2]:(-0.924116373062) A[3]:(0.80279135704)\n",
      " state (12)  A[0]:(-0.429369270802) A[1]:(0.814679503441) A[2]:(-0.946960091591) A[3]:(0.715673208237)\n",
      " state (13)  A[0]:(7.86185264587e-05) A[1]:(0.808862030506) A[2]:(0.900059998035) A[3]:(0.72957855463)\n",
      " state (14)  A[0]:(0.8100502491) A[1]:(0.899941682816) A[2]:(0.999999761581) A[3]:(0.810395598412)\n",
      " state (15)  A[0]:(0.979575276375) A[1]:(0.940013051033) A[2]:(1.0) A[3]:(0.878072977066)\n",
      "Episode 544000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6064. Times reached goal: 982.               Steps done: 3923541. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0177938832985.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5908,  0.5903,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.9049e-01,  6.5609e-01,  5.8413e-06,  5.3127e-01]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561, -0.0002,  0.7289,  0.5907]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561,  0.8100,  0.8100,  0.0000]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8091,  0.9000,  0.7290]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.9001,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531361818314) A[1]:(0.590735435486) A[2]:(0.59028840065) A[3]:(0.531223833561)\n",
      " state (1)  A[0]:(0.531408429146) A[1]:(-0.000163465738297) A[2]:(0.656048834324) A[3]:(0.590303897858)\n",
      " state (2)  A[0]:(0.590483903885) A[1]:(0.728926181793) A[2]:(0.590697109699) A[3]:(0.655899047852)\n",
      " state (3)  A[0]:(0.656399011612) A[1]:(-0.0438624620438) A[2]:(0.523399651051) A[3]:(0.548392534256)\n",
      " state (4)  A[0]:(0.590599656105) A[1]:(0.656145691872) A[2]:(8.52346420288e-05) A[3]:(0.531242012978)\n",
      " state (5)  A[0]:(-0.0197730511427) A[1]:(0.999899327755) A[2]:(-0.808592736721) A[3]:(0.654388070107)\n",
      " state (6)  A[0]:(0.000188067555428) A[1]:(0.809993803501) A[2]:(5.10215759277e-05) A[3]:(0.655955433846)\n",
      " state (7)  A[0]:(0.546750605106) A[1]:(-0.543017148972) A[2]:(0.429566442966) A[3]:(0.873089015484)\n",
      " state (8)  A[0]:(0.656161725521) A[1]:(-4.06801700592e-05) A[2]:(0.72895014286) A[3]:(0.590664863586)\n",
      " state (9)  A[0]:(0.656018018723) A[1]:(0.810052037239) A[2]:(0.80997222662) A[3]:(-0.000128567218781)\n",
      " state (10)  A[0]:(0.729041695595) A[1]:(0.900019288063) A[2]:(-0.000228643417358) A[3]:(0.728907704353)\n",
      " state (11)  A[0]:(0.130126968026) A[1]:(0.882071375847) A[2]:(-0.924268603325) A[3]:(0.802093982697)\n",
      " state (12)  A[0]:(-0.429755985737) A[1]:(0.814880073071) A[2]:(-0.947106301785) A[3]:(0.714830636978)\n",
      " state (13)  A[0]:(-8.55326652527e-06) A[1]:(0.809131622314) A[2]:(0.899959862232) A[3]:(0.728867411613)\n",
      " state (14)  A[0]:(0.810212910175) A[1]:(0.900162220001) A[2]:(0.999999761581) A[3]:(0.809917628765)\n",
      " state (15)  A[0]:(0.979613542557) A[1]:(0.940171539783) A[2]:(1.0) A[3]:(0.877769172192)\n",
      "Episode 545000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6028. Times reached goal: 984.               Steps done: 3929569. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0176869444076.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531698107719) A[1]:(0.590811133385) A[2]:(0.590397357941) A[3]:(0.531060576439)\n",
      " state (1)  A[0]:(0.531527280807) A[1]:(0.00010921061039) A[2]:(0.656061768532) A[3]:(0.590077340603)\n",
      " state (2)  A[0]:(0.59041082859) A[1]:(0.729165673256) A[2]:(0.591730773449) A[3]:(0.655697584152)\n",
      " state (3)  A[0]:(0.656308412552) A[1]:(-0.0449123680592) A[2]:(0.524795651436) A[3]:(0.547852635384)\n",
      " state (4)  A[0]:(0.590314686298) A[1]:(0.656098484993) A[2]:(0.000937461561989) A[3]:(0.530564844608)\n",
      " state (5)  A[0]:(-0.0201389677823) A[1]:(0.999899327755) A[2]:(-0.809013724327) A[3]:(0.653778553009)\n",
      " state (6)  A[0]:(-0.000570520700421) A[1]:(0.810076892376) A[2]:(-5.68628311157e-05) A[3]:(0.654886603355)\n",
      " state (7)  A[0]:(0.545760214329) A[1]:(-0.543057739735) A[2]:(0.430074542761) A[3]:(0.872397661209)\n",
      " state (8)  A[0]:(0.654796004295) A[1]:(8.8706612587e-05) A[2]:(0.729296505451) A[3]:(0.587608993053)\n",
      " state (9)  A[0]:(0.654587388039) A[1]:(0.809951245785) A[2]:(0.810011506081) A[3]:(-0.00464415177703)\n",
      " state (10)  A[0]:(0.727824091911) A[1]:(0.899933099747) A[2]:(-0.000688910367899) A[3]:(0.726911187172)\n",
      " state (11)  A[0]:(0.127828896046) A[1]:(0.882016599178) A[2]:(-0.924397528172) A[3]:(0.800807118416)\n",
      " state (12)  A[0]:(-0.431165456772) A[1]:(0.814873456955) A[2]:(-0.947185754776) A[3]:(0.713506817818)\n",
      " state (13)  A[0]:(-0.00130671192892) A[1]:(0.809185445309) A[2]:(0.900084137917) A[3]:(0.727942347527)\n",
      " state (14)  A[0]:(0.809784293175) A[1]:(0.900217652321) A[2]:(0.999999761581) A[3]:(0.809401512146)\n",
      " state (15)  A[0]:(0.979557156563) A[1]:(0.940191507339) A[2]:(1.0) A[3]:(0.877478539944)\n",
      "Episode 546000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6028. Times reached goal: 979.               Steps done: 3935597. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0175806482053.\n",
      " state (0)  A[0]:(0.531169235706) A[1]:(0.589885950089) A[2]:(0.590566396713) A[3]:(0.531195402145)\n",
      " state (1)  A[0]:(0.531319737434) A[1]:(-0.000684037688188) A[2]:(0.656044125557) A[3]:(0.590469181538)\n",
      " state (2)  A[0]:(0.590289592743) A[1]:(0.72868013382) A[2]:(0.592448413372) A[3]:(0.656126141548)\n",
      " state (3)  A[0]:(0.656283736229) A[1]:(-0.0479183234274) A[2]:(0.526015520096) A[3]:(0.548275709152)\n",
      " state (4)  A[0]:(0.590236008167) A[1]:(0.655761837959) A[2]:(0.00111973239109) A[3]:(0.531441628933)\n",
      " state (5)  A[0]:(-0.0192484315485) A[1]:(0.999899089336) A[2]:(-0.809667289257) A[3]:(0.655240297318)\n",
      " state (6)  A[0]:(0.000364124745829) A[1]:(0.809785962105) A[2]:(-0.000441908807261) A[3]:(0.655963242054)\n",
      " state (7)  A[0]:(0.546528697014) A[1]:(-0.542808711529) A[2]:(0.429590165615) A[3]:(0.872934877872)\n",
      " state (8)  A[0]:(0.656089007854) A[1]:(-0.000494256557431) A[2]:(0.728629291058) A[3]:(0.590911507607)\n",
      " state (9)  A[0]:(0.655874729156) A[1]:(0.809809267521) A[2]:(0.809821069241) A[3]:(-0.000117003917694)\n",
      " state (10)  A[0]:(0.729387521744) A[1]:(0.899965405464) A[2]:(8.04662704468e-05) A[3]:(0.729120373726)\n",
      " state (11)  A[0]:(0.131806030869) A[1]:(0.882200717926) A[2]:(-0.924230515957) A[3]:(0.802628993988)\n",
      " state (12)  A[0]:(-0.428980261087) A[1]:(0.81531471014) A[2]:(-0.947173476219) A[3]:(0.715464115143)\n",
      " state (13)  A[0]:(-8.11815261841e-05) A[1]:(0.809713542461) A[2]:(0.900094389915) A[3]:(0.729129552841)\n",
      " state (14)  A[0]:(0.810053050518) A[1]:(0.900495290756) A[2]:(0.999999761581) A[3]:(0.809893608093)\n",
      " state (15)  A[0]:(0.979594647884) A[1]:(0.940313518047) A[2]:(1.0) A[3]:(0.877640485764)\n",
      "Episode 547000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6071. Times reached goal: 991.               Steps done: 3941668. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0174742394207.\n",
      " state (0)  A[0]:(0.53155040741) A[1]:(0.590432941914) A[2]:(0.590529620647) A[3]:(0.531588196754)\n",
      " state (1)  A[0]:(0.531512081623) A[1]:(-8.22097063065e-05) A[2]:(0.655989289284) A[3]:(0.590547561646)\n",
      " state (2)  A[0]:(0.590334892273) A[1]:(0.728944182396) A[2]:(0.591527879238) A[3]:(0.656124472618)\n",
      " state (3)  A[0]:(0.656345427036) A[1]:(-0.0486456491053) A[2]:(0.525345683098) A[3]:(0.548068284988)\n",
      " state (4)  A[0]:(0.590247631073) A[1]:(0.655933141708) A[2]:(-0.000181794166565) A[3]:(0.531475305557)\n",
      " state (5)  A[0]:(-0.0189816504717) A[1]:(0.999899208546) A[2]:(-0.810230314732) A[3]:(0.655940771103)\n",
      " state (6)  A[0]:(-0.000109553337097) A[1]:(0.809990525246) A[2]:(-0.000385999650462) A[3]:(0.655988693237)\n",
      " state (7)  A[0]:(0.545979499817) A[1]:(-0.542263746262) A[2]:(0.430113106966) A[3]:(0.872826576233)\n",
      " state (8)  A[0]:(0.655948162079) A[1]:(0.000180289149284) A[2]:(0.728932917118) A[3]:(0.590901434422)\n",
      " state (9)  A[0]:(0.655944347382) A[1]:(0.810000777245) A[2]:(0.810012221336) A[3]:(0.000107258558273)\n",
      " state (10)  A[0]:(0.729289591312) A[1]:(0.899999678135) A[2]:(0.00011670589447) A[3]:(0.729143977165)\n",
      " state (11)  A[0]:(0.131519556046) A[1]:(0.882135510445) A[2]:(-0.924315452576) A[3]:(0.802606523037)\n",
      " state (12)  A[0]:(-0.428848564625) A[1]:(0.815039396286) A[2]:(-0.947257399559) A[3]:(0.715496838093)\n",
      " state (13)  A[0]:(0.000395327777369) A[1]:(0.80924654007) A[2]:(0.900041341782) A[3]:(0.729229807854)\n",
      " state (14)  A[0]:(0.810150682926) A[1]:(0.90015989542) A[2]:(0.999999761581) A[3]:(0.809964895248)\n",
      " state (15)  A[0]:(0.979589164257) A[1]:(0.940069437027) A[2]:(1.0) A[3]:(0.877659082413)\n",
      "Episode 548000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6040. Times reached goal: 984.               Steps done: 3947708. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.017369013118.\n",
      " state (0)  A[0]:(0.53117287159) A[1]:(0.590476512909) A[2]:(0.589904546738) A[3]:(0.531450033188)\n",
      " state (1)  A[0]:(0.531222343445) A[1]:(0.000396892399294) A[2]:(0.656091213226) A[3]:(0.590586900711)\n",
      " state (2)  A[0]:(0.590336859226) A[1]:(0.728873610497) A[2]:(0.59275662899) A[3]:(0.656224012375)\n",
      " state (3)  A[0]:(0.656509399414) A[1]:(-0.0498283132911) A[2]:(0.527835845947) A[3]:(0.54809743166)\n",
      " state (4)  A[0]:(0.59051322937) A[1]:(0.65543627739) A[2]:(0.00304948328994) A[3]:(0.531709432602)\n",
      " state (5)  A[0]:(-0.0181752387434) A[1]:(0.999899029732) A[2]:(-0.809940814972) A[3]:(0.656716346741)\n",
      " state (6)  A[0]:(0.000476837129099) A[1]:(0.809744536877) A[2]:(0.000654220464639) A[3]:(0.656349539757)\n",
      " state (7)  A[0]:(0.546315670013) A[1]:(-0.542523682117) A[2]:(0.430698096752) A[3]:(0.872918963432)\n",
      " state (8)  A[0]:(0.656608223915) A[1]:(-0.00039871034096) A[2]:(0.728978633881) A[3]:(0.591946542263)\n",
      " state (9)  A[0]:(0.656346201897) A[1]:(0.809927940369) A[2]:(0.809967517853) A[3]:(0.00099697674159)\n",
      " state (10)  A[0]:(0.72902905941) A[1]:(0.899937450886) A[2]:(-0.000628709734883) A[3]:(0.728995859623)\n",
      " state (11)  A[0]:(0.130332276225) A[1]:(0.882009983063) A[2]:(-0.924494206905) A[3]:(0.802332937717)\n",
      " state (12)  A[0]:(-0.429479330778) A[1]:(0.814768791199) A[2]:(-0.947350144386) A[3]:(0.71525657177)\n",
      " state (13)  A[0]:(0.00034287571907) A[1]:(0.808892190456) A[2]:(0.900123357773) A[3]:(0.729240059853)\n",
      " state (14)  A[0]:(0.810291051865) A[1]:(0.899933636189) A[2]:(0.999999761581) A[3]:(0.810130178928)\n",
      " state (15)  A[0]:(0.979609072208) A[1]:(0.939904391766) A[2]:(1.0) A[3]:(0.877817451954)\n",
      "Episode 549000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6048. Times reached goal: 979.               Steps done: 3953756. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0172642823517.\n",
      "q_values \n",
      "tensor([[ 0.5282,  0.5907,  0.5907,  0.5304]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5285,  0.0007,  0.6557,  0.5895]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5882,  0.7290,  0.5917,  0.6554]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0018,  0.8100,  0.0006,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7359,  0.9001,  0.0019,  0.7313]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8141,  0.9005,  1.0000,  0.8112]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.528169572353) A[1]:(0.590730428696) A[2]:(0.590604424477) A[3]:(0.5321007967)\n",
      " state (1)  A[0]:(0.528212547302) A[1]:(-0.000754088046961) A[2]:(0.656309127808) A[3]:(0.591336369514)\n",
      " state (2)  A[0]:(0.587801456451) A[1]:(0.729176878929) A[2]:(0.592227816582) A[3]:(0.657045006752)\n",
      " state (3)  A[0]:(0.654638886452) A[1]:(-0.0509040765464) A[2]:(0.527574717999) A[3]:(0.548922419548)\n",
      " state (4)  A[0]:(0.58861720562) A[1]:(0.656539559364) A[2]:(0.00146245851647) A[3]:(0.532918989658)\n",
      " state (5)  A[0]:(-0.0194031223655) A[1]:(0.999899327755) A[2]:(-0.810773074627) A[3]:(0.658379793167)\n",
      " state (6)  A[0]:(-0.000124350190163) A[1]:(0.810131549835) A[2]:(0.000338315963745) A[3]:(0.657592773438)\n",
      " state (7)  A[0]:(0.546205639839) A[1]:(-0.541653573513) A[2]:(0.430753439665) A[3]:(0.873382806778)\n",
      " state (8)  A[0]:(0.657801985741) A[1]:(0.000315219163895) A[2]:(0.728587865829) A[3]:(0.593938469887)\n",
      " state (9)  A[0]:(0.65833067894) A[1]:(0.810364842415) A[2]:(0.809558451176) A[3]:(0.00318555952981)\n",
      " state (10)  A[0]:(0.730663895607) A[1]:(0.90044927597) A[2]:(-0.00235318695195) A[3]:(0.729923188686)\n",
      " state (11)  A[0]:(0.133484840393) A[1]:(0.882996559143) A[2]:(-0.924885511398) A[3]:(0.803155839443)\n",
      " state (12)  A[0]:(-0.427521675825) A[1]:(0.816739678383) A[2]:(-0.947763085365) A[3]:(0.716334581375)\n",
      " state (13)  A[0]:(0.00184033601545) A[1]:(0.811146378517) A[2]:(0.89920437336) A[3]:(0.730084896088)\n",
      " state (14)  A[0]:(0.810666680336) A[1]:(0.901183485985) A[2]:(0.999999761581) A[3]:(0.81059384346)\n",
      " state (15)  A[0]:(0.979672193527) A[1]:(0.94063436985) A[2]:(1.0) A[3]:(0.878040969372)\n",
      "Episode 550000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6035. Times reached goal: 983.               Steps done: 3959791. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0171604061694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531926691532) A[1]:(0.59018778801) A[2]:(0.590316772461) A[3]:(0.531566739082)\n",
      " state (1)  A[0]:(0.531828522682) A[1]:(-0.000707179191522) A[2]:(0.655920505524) A[3]:(0.590133428574)\n",
      " state (2)  A[0]:(0.59042930603) A[1]:(0.72908949852) A[2]:(0.591798603535) A[3]:(0.65582716465)\n",
      " state (3)  A[0]:(0.656512498856) A[1]:(-0.0523439124227) A[2]:(0.527474343777) A[3]:(0.547126293182)\n",
      " state (4)  A[0]:(0.590257406235) A[1]:(0.655878603458) A[2]:(0.000837921921629) A[3]:(0.531061410904)\n",
      " state (5)  A[0]:(-0.0174841638654) A[1]:(0.999899148941) A[2]:(-0.811401247978) A[3]:(0.657095909119)\n",
      " state (6)  A[0]:(-0.000265225768089) A[1]:(0.809994280338) A[2]:(-0.000247955322266) A[3]:(0.655081093311)\n",
      " state (7)  A[0]:(0.544681787491) A[1]:(-0.541335940361) A[2]:(0.430547505617) A[3]:(0.871844172478)\n",
      " state (8)  A[0]:(0.654591798782) A[1]:(0.000582426728215) A[2]:(0.728858649731) A[3]:(0.588028788567)\n",
      " state (9)  A[0]:(0.654175102711) A[1]:(0.809967815876) A[2]:(0.809906423092) A[3]:(-0.00494526745752)\n",
      " state (10)  A[0]:(0.727555632591) A[1]:(0.900000870228) A[2]:(-0.000581979693379) A[3]:(0.72676384449)\n",
      " state (11)  A[0]:(0.128296539187) A[1]:(0.882285952568) A[2]:(-0.924540936947) A[3]:(0.801248133183)\n",
      " state (12)  A[0]:(-0.431002676487) A[1]:(0.815394043922) A[2]:(-0.94748789072) A[3]:(0.714105963707)\n",
      " state (13)  A[0]:(-0.00143623258919) A[1]:(0.809562325478) A[2]:(0.900013625622) A[3]:(0.728182017803)\n",
      " state (14)  A[0]:(0.809837460518) A[1]:(0.90025216341) A[2]:(0.999999761581) A[3]:(0.809341430664)\n",
      " state (15)  A[0]:(0.979577600956) A[1]:(0.940024554729) A[2]:(1.0) A[3]:(0.877257883549)\n",
      "Episode 551000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6032. Times reached goal: 975.               Steps done: 3965823. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0170572061635.\n",
      " state (0)  A[0]:(0.53006029129) A[1]:(0.590459942818) A[2]:(0.590577363968) A[3]:(0.531623601913)\n",
      " state (1)  A[0]:(0.529615402222) A[1]:(-0.00146244361531) A[2]:(0.655795693398) A[3]:(0.588885307312)\n",
      " state (2)  A[0]:(0.588240623474) A[1]:(0.729125142097) A[2]:(0.591229557991) A[3]:(0.653783023357)\n",
      " state (3)  A[0]:(0.65452927351) A[1]:(-0.0530889965594) A[2]:(0.527378678322) A[3]:(0.543438315392)\n",
      " state (4)  A[0]:(0.58783853054) A[1]:(0.655818104744) A[2]:(0.000712156179361) A[3]:(0.52605843544)\n",
      " state (5)  A[0]:(-0.0217284783721) A[1]:(0.999898970127) A[2]:(-0.811486661434) A[3]:(0.651583850384)\n",
      " state (6)  A[0]:(-0.00695044547319) A[1]:(0.809944927692) A[2]:(0.0012458555866) A[3]:(0.646336913109)\n",
      " state (7)  A[0]:(0.537760734558) A[1]:(-0.539699435234) A[2]:(0.431441575289) A[3]:(0.866530537605)\n",
      " state (8)  A[0]:(0.646503925323) A[1]:(0.00266242888756) A[2]:(0.729075729847) A[3]:(0.56710600853)\n",
      " state (9)  A[0]:(0.644835948944) A[1]:(0.810314893723) A[2]:(0.809489071369) A[3]:(-0.0417309515178)\n",
      " state (10)  A[0]:(0.720041632652) A[1]:(0.900374114513) A[2]:(-0.00287532014772) A[3]:(0.709231078625)\n",
      " state (11)  A[0]:(0.116396538913) A[1]:(0.883306860924) A[2]:(-0.92491966486) A[3]:(0.789974331856)\n",
      " state (12)  A[0]:(-0.436521053314) A[1]:(0.817683637142) A[2]:(-0.947771668434) A[3]:(0.702044606209)\n",
      " state (13)  A[0]:(-0.00398471346125) A[1]:(0.812305748463) A[2]:(0.899286746979) A[3]:(0.719045519829)\n",
      " state (14)  A[0]:(0.809886097908) A[1]:(0.901907980442) A[2]:(0.999999761581) A[3]:(0.80378049612)\n",
      " state (15)  A[0]:(0.979648947716) A[1]:(0.941141664982) A[2]:(1.0) A[3]:(0.873972952366)\n",
      "Episode 552000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6062. Times reached goal: 988.               Steps done: 3971885. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0169541181552.\n",
      " state (0)  A[0]:(0.53177189827) A[1]:(0.590493559837) A[2]:(0.590561509132) A[3]:(0.531412422657)\n",
      " state (1)  A[0]:(0.531707644463) A[1]:(-2.27689743042e-05) A[2]:(0.656116127968) A[3]:(0.590353548527)\n",
      " state (2)  A[0]:(0.5904520154) A[1]:(0.729081332684) A[2]:(0.591312646866) A[3]:(0.655927181244)\n",
      " state (3)  A[0]:(0.656651556492) A[1]:(-0.0549830645323) A[2]:(0.527787566185) A[3]:(0.5467274189)\n",
      " state (4)  A[0]:(0.590351223946) A[1]:(0.656223773956) A[2]:(-8.88109207153e-05) A[3]:(0.531110167503)\n",
      " state (5)  A[0]:(-0.0162798687816) A[1]:(0.999899208546) A[2]:(-0.812404632568) A[3]:(0.658505916595)\n",
      " state (6)  A[0]:(-5.12599945068e-05) A[1]:(0.81014251709) A[2]:(-0.000429987878306) A[3]:(0.655563294888)\n",
      " state (7)  A[0]:(0.544768095016) A[1]:(-0.540916800499) A[2]:(0.431214511395) A[3]:(0.871988892555)\n",
      " state (8)  A[0]:(0.655621051788) A[1]:(0.000955939001869) A[2]:(0.729059100151) A[3]:(0.589910447598)\n",
      " state (9)  A[0]:(0.655868530273) A[1]:(0.810147941113) A[2]:(0.810112714767) A[3]:(-0.00111454678699)\n",
      " state (10)  A[0]:(0.729177117348) A[1]:(0.900072276592) A[2]:(0.000373005837901) A[3]:(0.728571653366)\n",
      " state (11)  A[0]:(0.131601437926) A[1]:(0.882304549217) A[2]:(-0.924449324608) A[3]:(0.802462160587)\n",
      " state (12)  A[0]:(-0.429084599018) A[1]:(0.815302848816) A[2]:(-0.947496414185) A[3]:(0.715238451958)\n",
      " state (13)  A[0]:(-0.000148355960846) A[1]:(0.809362471104) A[2]:(0.900268793106) A[3]:(0.728870391846)\n",
      " state (14)  A[0]:(0.809987604618) A[1]:(0.90009868145) A[2]:(0.999999761581) A[3]:(0.809735119343)\n",
      " state (15)  A[0]:(0.97956687212) A[1]:(0.939874053001) A[2]:(1.0) A[3]:(0.877471327782)\n",
      "Episode 553000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6029. Times reached goal: 980.               Steps done: 3977914. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0168522092898.\n",
      " state (0)  A[0]:(0.531321227551) A[1]:(0.590455830097) A[2]:(0.590473890305) A[3]:(0.531517744064)\n",
      " state (1)  A[0]:(0.531338453293) A[1]:(-4.53591346741e-05) A[2]:(0.65613424778) A[3]:(0.590514421463)\n",
      " state (2)  A[0]:(0.590180516243) A[1]:(0.728882551193) A[2]:(0.591626286507) A[3]:(0.656103610992)\n",
      " state (3)  A[0]:(0.656450986862) A[1]:(-0.0569321513176) A[2]:(0.528552114964) A[3]:(0.546714305878)\n",
      " state (4)  A[0]:(0.590123176575) A[1]:(0.656082749367) A[2]:(1.58548355103e-05) A[3]:(0.531376004219)\n",
      " state (5)  A[0]:(-0.0157357584685) A[1]:(0.999899148941) A[2]:(-0.812859416008) A[3]:(0.659407317638)\n",
      " state (6)  A[0]:(9.73641872406e-05) A[1]:(0.810033559799) A[2]:(-0.000322580337524) A[3]:(0.655997872353)\n",
      " state (7)  A[0]:(0.544839262962) A[1]:(-0.541243553162) A[2]:(0.431586891413) A[3]:(0.872159302235)\n",
      " state (8)  A[0]:(0.655979514122) A[1]:(0.000392377347453) A[2]:(0.728988289833) A[3]:(0.590974032879)\n",
      " state (9)  A[0]:(0.656044542789) A[1]:(0.810081720352) A[2]:(0.80996632576) A[3]:(-0.000112503767014)\n",
      " state (10)  A[0]:(0.729147255421) A[1]:(0.900010704994) A[2]:(-0.000443816155894) A[3]:(0.728881001472)\n",
      " state (11)  A[0]:(0.131411895156) A[1]:(0.8822042346) A[2]:(-0.924644827843) A[3]:(0.802704811096)\n",
      " state (12)  A[0]:(-0.429157376289) A[1]:(0.815137863159) A[2]:(-0.94767844677) A[3]:(0.715610742569)\n",
      " state (13)  A[0]:(-0.000108867883682) A[1]:(0.80920779705) A[2]:(0.899971365929) A[3]:(0.729263305664)\n",
      " state (14)  A[0]:(0.810015201569) A[1]:(0.900056183338) A[2]:(0.999999761581) A[3]:(0.81004178524)\n",
      " state (15)  A[0]:(0.979567527771) A[1]:(0.939873337746) A[2]:(1.0) A[3]:(0.877656280994)\n",
      "Episode 554000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6025. Times reached goal: 978.               Steps done: 3983939. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0167509799883.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5905,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5311, -0.0001,  0.6561,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5900,  0.7290,  0.5915,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8100, -0.0001,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7296,  0.9000,  0.0003,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9001,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531407475471) A[1]:(0.590529203415) A[2]:(0.590530872345) A[3]:(0.531517148018)\n",
      " state (1)  A[0]:(0.531383752823) A[1]:(-4.23640012741e-05) A[2]:(0.656097769737) A[3]:(0.590620398521)\n",
      " state (2)  A[0]:(0.590238571167) A[1]:(0.729004979134) A[2]:(0.591513097286) A[3]:(0.656166195869)\n",
      " state (3)  A[0]:(0.656514585018) A[1]:(-0.0569643378258) A[2]:(0.528915762901) A[3]:(0.546685755253)\n",
      " state (4)  A[0]:(0.590232312679) A[1]:(0.656096935272) A[2]:(0.000567555369344) A[3]:(0.531537532806)\n",
      " state (5)  A[0]:(-0.0153388269246) A[1]:(0.999899208546) A[2]:(-0.812887489796) A[3]:(0.660044670105)\n",
      " state (6)  A[0]:(9.94503498077e-05) A[1]:(0.809985697269) A[2]:(4.24385070801e-05) A[3]:(0.655937790871)\n",
      " state (7)  A[0]:(0.544721245766) A[1]:(-0.541359305382) A[2]:(0.431778550148) A[3]:(0.872012376785)\n",
      " state (8)  A[0]:(0.656116008759) A[1]:(-6.31064176559e-05) A[2]:(0.728930711746) A[3]:(0.591150045395)\n",
      " state (9)  A[0]:(0.656097054482) A[1]:(0.810011088848) A[2]:(0.809983611107) A[3]:(5.15580177307e-05)\n",
      " state (10)  A[0]:(0.729061126709) A[1]:(0.899993538857) A[2]:(-0.000191688537598) A[3]:(0.728878855705)\n",
      " state (11)  A[0]:(0.131075456738) A[1]:(0.882209837437) A[2]:(-0.924632310867) A[3]:(0.80269753933)\n",
      " state (12)  A[0]:(-0.429525852203) A[1]:(0.81515789032) A[2]:(-0.947724699974) A[3]:(0.715506076813)\n",
      " state (13)  A[0]:(-0.000419765681727) A[1]:(0.809206008911) A[2]:(0.899876534939) A[3]:(0.72906768322)\n",
      " state (14)  A[0]:(0.810032784939) A[1]:(0.900065898895) A[2]:(0.999999761581) A[3]:(0.809846401215)\n",
      " state (15)  A[0]:(0.979580938816) A[1]:(0.939894139767) A[2]:(1.0) A[3]:(0.877481818199)\n",
      "Episode 555000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6021. Times reached goal: 983.               Steps done: 3989960. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0166504253613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531006038189) A[1]:(0.590337514877) A[2]:(0.590710520744) A[3]:(0.531862735748)\n",
      " state (1)  A[0]:(0.530880987644) A[1]:(-0.000279232859612) A[2]:(0.655921339989) A[3]:(0.591239392757)\n",
      " state (2)  A[0]:(0.589402675629) A[1]:(0.728842139244) A[2]:(0.591654896736) A[3]:(0.656871259212)\n",
      " state (3)  A[0]:(0.655767798424) A[1]:(-0.058793656528) A[2]:(0.529464125633) A[3]:(0.547393918037)\n",
      " state (4)  A[0]:(0.589342951775) A[1]:(0.656035065651) A[2]:(0.000263452529907) A[3]:(0.532563567162)\n",
      " state (5)  A[0]:(-0.0160258542746) A[1]:(0.999899089336) A[2]:(-0.813538551331) A[3]:(0.661386847496)\n",
      " state (6)  A[0]:(-0.0013216129737) A[1]:(0.809941589832) A[2]:(-0.0003969669051) A[3]:(0.656773090363)\n",
      " state (7)  A[0]:(0.543736219406) A[1]:(-0.54042327404) A[2]:(0.431418269873) A[3]:(0.872414946556)\n",
      " state (8)  A[0]:(0.655659198761) A[1]:(0.000745251658373) A[2]:(0.728440999985) A[3]:(0.59347641468)\n",
      " state (9)  A[0]:(0.655205011368) A[1]:(0.810239672661) A[2]:(0.809604167938) A[3]:(0.00289659877308)\n",
      " state (10)  A[0]:(0.728084146976) A[1]:(0.900113582611) A[2]:(-0.00177764706314) A[3]:(0.72968018055)\n",
      " state (11)  A[0]:(0.129465773702) A[1]:(0.882319629192) A[2]:(-0.924948096275) A[3]:(0.803144335747)\n",
      " state (12)  A[0]:(-0.429710417986) A[1]:(0.815232813358) A[2]:(-0.94799643755) A[3]:(0.716191649437)\n",
      " state (13)  A[0]:(0.000588551105466) A[1]:(0.809148430824) A[2]:(0.899253606796) A[3]:(0.729831218719)\n",
      " state (14)  A[0]:(0.810552716255) A[1]:(0.900022029877) A[2]:(0.999999761581) A[3]:(0.810396075249)\n",
      " state (15)  A[0]:(0.979637086391) A[1]:(0.939914047718) A[2]:(1.0) A[3]:(0.877783000469)\n",
      "Episode 556000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6014. Times reached goal: 982.               Steps done: 3995974. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0165505902084.\n",
      " state (0)  A[0]:(0.530913591385) A[1]:(0.590144872665) A[2]:(0.590437352657) A[3]:(0.530190110207)\n",
      " state (1)  A[0]:(0.530925393105) A[1]:(0.000251531600952) A[2]:(0.655990362167) A[3]:(0.59016674757)\n",
      " state (2)  A[0]:(0.589862704277) A[1]:(0.729505181313) A[2]:(0.591372132301) A[3]:(0.656047821045)\n",
      " state (3)  A[0]:(0.656165242195) A[1]:(-0.0581475645304) A[2]:(0.529515743256) A[3]:(0.546114683151)\n",
      " state (4)  A[0]:(0.589736819267) A[1]:(0.656764864922) A[2]:(-6.6876411438e-05) A[3]:(0.531430900097)\n",
      " state (5)  A[0]:(-0.0151443677023) A[1]:(0.999899327755) A[2]:(-0.813917994499) A[3]:(0.661292135715)\n",
      " state (6)  A[0]:(-0.00107735348865) A[1]:(0.810153186321) A[2]:(-0.000590443552937) A[3]:(0.656122803688)\n",
      " state (7)  A[0]:(0.543856680393) A[1]:(-0.540328383446) A[2]:(0.43159532547) A[3]:(0.872052013874)\n",
      " state (8)  A[0]:(0.656431615353) A[1]:(0.00087022758089) A[2]:(0.728897452354) A[3]:(0.592546582222)\n",
      " state (9)  A[0]:(0.657022297382) A[1]:(0.810304164886) A[2]:(0.810450315475) A[3]:(0.0020418735221)\n",
      " state (10)  A[0]:(0.730284810066) A[1]:(0.900015473366) A[2]:(0.00215994985774) A[3]:(0.730020284653)\n",
      " state (11)  A[0]:(0.133742392063) A[1]:(0.88202136755) A[2]:(-0.924335300922) A[3]:(0.803670942783)\n",
      " state (12)  A[0]:(-0.428423285484) A[1]:(0.814557373524) A[2]:(-0.947660267353) A[3]:(0.716489434242)\n",
      " state (13)  A[0]:(-0.000296369194984) A[1]:(0.808363199234) A[2]:(0.900098621845) A[3]:(0.72975474596)\n",
      " state (14)  A[0]:(0.809811592102) A[1]:(0.899559557438) A[2]:(0.999999761581) A[3]:(0.810372710228)\n",
      " state (15)  A[0]:(0.97952234745) A[1]:(0.939558029175) A[2]:(1.0) A[3]:(0.877851188183)\n",
      "Episode 557000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6026. Times reached goal: 987.               Steps done: 4002000. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0164511562472.\n",
      " state (0)  A[0]:(0.531704545021) A[1]:(0.590598583221) A[2]:(0.590260148048) A[3]:(0.53151512146)\n",
      " state (1)  A[0]:(0.531671524048) A[1]:(-3.14265489578e-05) A[2]:(0.656068921089) A[3]:(0.590132772923)\n",
      " state (2)  A[0]:(0.590089201927) A[1]:(0.728975057602) A[2]:(0.591127812862) A[3]:(0.65588760376)\n",
      " state (3)  A[0]:(0.656504034996) A[1]:(-0.0604126155376) A[2]:(0.52957868576) A[3]:(0.545820951462)\n",
      " state (4)  A[0]:(0.590388059616) A[1]:(0.655909657478) A[2]:(-0.000349164009094) A[3]:(0.5314270854)\n",
      " state (5)  A[0]:(-0.0131315784529) A[1]:(0.999899089336) A[2]:(-0.814268231392) A[3]:(0.661724925041)\n",
      " state (6)  A[0]:(0.000186383724213) A[1]:(0.809899389744) A[2]:(-0.000519871653523) A[3]:(0.655630409718)\n",
      " state (7)  A[0]:(0.54404270649) A[1]:(-0.541221559048) A[2]:(0.431933045387) A[3]:(0.871492922306)\n",
      " state (8)  A[0]:(0.655872344971) A[1]:(-0.00062842661282) A[2]:(0.72867667675) A[3]:(0.590669155121)\n",
      " state (9)  A[0]:(0.655920505524) A[1]:(0.809793174267) A[2]:(0.809864163399) A[3]:(-0.00032964348793)\n",
      " state (10)  A[0]:(0.729205489159) A[1]:(0.89994686842) A[2]:(-0.000109791755676) A[3]:(0.728949308395)\n",
      " state (11)  A[0]:(0.132015377283) A[1]:(0.88235014677) A[2]:(-0.924694240093) A[3]:(0.803047776222)\n",
      " state (12)  A[0]:(-0.429062515497) A[1]:(0.815609574318) A[2]:(-0.947901070118) A[3]:(0.71589243412)\n",
      " state (13)  A[0]:(-0.000341147184372) A[1]:(0.809810042381) A[2]:(0.899768471718) A[3]:(0.729246854782)\n",
      " state (14)  A[0]:(0.80998134613) A[1]:(0.900475203991) A[2]:(0.999999761581) A[3]:(0.809919476509)\n",
      " state (15)  A[0]:(0.979566037655) A[1]:(0.940158724785) A[2]:(1.0) A[3]:(0.877460062504)\n",
      "Episode 558000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6032. Times reached goal: 983.               Steps done: 4008032. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0163522215597.\n",
      " state (0)  A[0]:(0.531165122986) A[1]:(0.590711832047) A[2]:(0.590813159943) A[3]:(0.53214597702)\n",
      " state (1)  A[0]:(0.53035902977) A[1]:(0.000851779943332) A[2]:(0.656324028969) A[3]:(0.591084003448)\n",
      " state (2)  A[0]:(0.589318871498) A[1]:(0.72894012928) A[2]:(0.592538893223) A[3]:(0.656584143639)\n",
      " state (3)  A[0]:(0.655888617039) A[1]:(-0.0617025345564) A[2]:(0.532047390938) A[3]:(0.546355366707)\n",
      " state (4)  A[0]:(0.589535593987) A[1]:(0.65630543232) A[2]:(0.00236105476506) A[3]:(0.532181382179)\n",
      " state (5)  A[0]:(-0.0140464603901) A[1]:(0.99989926815) A[2]:(-0.814049720764) A[3]:(0.663201451302)\n",
      " state (6)  A[0]:(-0.00092998117907) A[1]:(0.810171425343) A[2]:(0.00111901713535) A[3]:(0.656970977783)\n",
      " state (7)  A[0]:(0.543441057205) A[1]:(-0.540518045425) A[2]:(0.433514535427) A[3]:(0.872104287148)\n",
      " state (8)  A[0]:(0.656046152115) A[1]:(0.000610023678746) A[2]:(0.72930765152) A[3]:(0.592723250389)\n",
      " state (9)  A[0]:(0.656447529793) A[1]:(0.810291528702) A[2]:(0.81007540226) A[3]:(0.0024021519348)\n",
      " state (10)  A[0]:(0.729442715645) A[1]:(0.900098204613) A[2]:(-0.000311255455017) A[3]:(0.729799509048)\n",
      " state (11)  A[0]:(0.132292360067) A[1]:(0.882291555405) A[2]:(-0.924810647964) A[3]:(0.803358376026)\n",
      " state (12)  A[0]:(-0.428271025419) A[1]:(0.815186619759) A[2]:(-0.947950541973) A[3]:(0.716174125671)\n",
      " state (13)  A[0]:(0.0013580910163) A[1]:(0.809129536152) A[2]:(0.899963378906) A[3]:(0.729568064213)\n",
      " state (14)  A[0]:(0.810606539249) A[1]:(0.900020956993) A[2]:(0.999999761581) A[3]:(0.810223042965)\n",
      " state (15)  A[0]:(0.979620575905) A[1]:(0.9398432374) A[2]:(1.0) A[3]:(0.877672672272)\n",
      "Episode 559000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6041. Times reached goal: 985.               Steps done: 4014073. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0162537355657.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5905,  0.5906,  0.5320]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.0007,  0.6562,  0.5907]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.7288,  0.5921,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8099, -0.0003,  0.6553]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7300,  0.9001,  0.0012,  0.7285]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9003,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531824827194) A[1]:(0.590574145317) A[2]:(0.590494394302) A[3]:(0.532198429108)\n",
      " state (1)  A[0]:(0.531754434109) A[1]:(0.000351041555405) A[2]:(0.656248569489) A[3]:(0.591210246086)\n",
      " state (2)  A[0]:(0.590242147446) A[1]:(0.728906273842) A[2]:(0.592197537422) A[3]:(0.656843781471)\n",
      " state (3)  A[0]:(0.656696856022) A[1]:(-0.0636102929711) A[2]:(0.531880438328) A[3]:(0.546548426151)\n",
      " state (4)  A[0]:(0.590455114841) A[1]:(0.656072497368) A[2]:(0.000791430298705) A[3]:(0.532689571381)\n",
      " state (5)  A[0]:(-0.0117344241589) A[1]:(0.999899089336) A[2]:(-0.815201997757) A[3]:(0.663997769356)\n",
      " state (6)  A[0]:(0.0006166844978) A[1]:(0.809828281403) A[2]:(-0.000649928930216) A[3]:(0.65687417984)\n",
      " state (7)  A[0]:(0.544196486473) A[1]:(-0.54121863842) A[2]:(0.432496726513) A[3]:(0.871899008751)\n",
      " state (8)  A[0]:(0.656610012054) A[1]:(-0.000648885848932) A[2]:(0.728819549084) A[3]:(0.592288017273)\n",
      " state (9)  A[0]:(0.657153248787) A[1]:(0.809845983982) A[2]:(0.81004357338) A[3]:(0.00197991472669)\n",
      " state (10)  A[0]:(0.73060965538) A[1]:(0.899895012379) A[2]:(0.000880598789081) A[3]:(0.730123400688)\n",
      " state (11)  A[0]:(0.13521014154) A[1]:(0.882185935974) A[2]:(-0.92458999157) A[3]:(0.803937554359)\n",
      " state (12)  A[0]:(-0.427154898643) A[1]:(0.815215110779) A[2]:(-0.947926163673) A[3]:(0.716701328754)\n",
      " state (13)  A[0]:(0.00114759756252) A[1]:(0.809299230576) A[2]:(0.899903297424) A[3]:(0.729671418667)\n",
      " state (14)  A[0]:(0.810358583927) A[1]:(0.900170385838) A[2]:(0.999999761581) A[3]:(0.810144901276)\n",
      " state (15)  A[0]:(0.979594171047) A[1]:(0.939939975739) A[2]:(1.0) A[3]:(0.877563357353)\n",
      "Episode 560000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6041. Times reached goal: 983.               Steps done: 4020114. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0161558427322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531020820141) A[1]:(0.590772271156) A[2]:(0.590554237366) A[3]:(0.531656563282)\n",
      " state (1)  A[0]:(0.530911564827) A[1]:(-0.000597730220761) A[2]:(0.656284809113) A[3]:(0.590897083282)\n",
      " state (2)  A[0]:(0.589676976204) A[1]:(0.729032993317) A[2]:(0.591679751873) A[3]:(0.656582951546)\n",
      " state (3)  A[0]:(0.656213343143) A[1]:(-0.0644492730498) A[2]:(0.531543135643) A[3]:(0.545851826668)\n",
      " state (4)  A[0]:(0.589898824692) A[1]:(0.656220853329) A[2]:(-0.000245332717896) A[3]:(0.532064914703)\n",
      " state (5)  A[0]:(-0.0120836421847) A[1]:(0.999899208546) A[2]:(-0.815768718719) A[3]:(0.664126157761)\n",
      " state (6)  A[0]:(-0.000669762375765) A[1]:(0.810143887997) A[2]:(-0.000654816511087) A[3]:(0.656383633614)\n",
      " state (7)  A[0]:(0.542786061764) A[1]:(-0.540757000446) A[2]:(0.433079272509) A[3]:(0.871453344822)\n",
      " state (8)  A[0]:(0.655283331871) A[1]:(0.000137075781822) A[2]:(0.729066193104) A[3]:(0.59067761898)\n",
      " state (9)  A[0]:(0.655885338783) A[1]:(0.810069322586) A[2]:(0.8099629879) A[3]:(0.00010934472084)\n",
      " state (10)  A[0]:(0.729365944862) A[1]:(0.900005102158) A[2]:(-0.000157713890076) A[3]:(0.729281663895)\n",
      " state (11)  A[0]:(0.132435902953) A[1]:(0.88235604763) A[2]:(-0.924828767776) A[3]:(0.803379893303)\n",
      " state (12)  A[0]:(-0.429054886103) A[1]:(0.815541267395) A[2]:(-0.948076725006) A[3]:(0.716203868389)\n",
      " state (13)  A[0]:(-0.000475868553622) A[1]:(0.809672296047) A[2]:(0.899898052216) A[3]:(0.72946035862)\n",
      " state (14)  A[0]:(0.810013651848) A[1]:(0.900365948677) A[2]:(0.999999761581) A[3]:(0.810144245625)\n",
      " state (15)  A[0]:(0.979573726654) A[1]:(0.94001686573) A[2]:(1.0) A[3]:(0.877624511719)\n",
      "Episode 561000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6044. Times reached goal: 983.               Steps done: 4026158. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.016058491311.\n",
      " state (0)  A[0]:(0.531372785568) A[1]:(0.590537667274) A[2]:(0.5905636549) A[3]:(0.531442940235)\n",
      " state (1)  A[0]:(0.53158390522) A[1]:(0.000496327818837) A[2]:(0.656086206436) A[3]:(0.590512573719)\n",
      " state (2)  A[0]:(0.590352714062) A[1]:(0.729172110558) A[2]:(0.591781139374) A[3]:(0.656028628349)\n",
      " state (3)  A[0]:(0.656786978245) A[1]:(-0.0648386925459) A[2]:(0.532129466534) A[3]:(0.545024871826)\n",
      " state (4)  A[0]:(0.590513706207) A[1]:(0.656645059586) A[2]:(9.48905944824e-05) A[3]:(0.5314463377)\n",
      " state (5)  A[0]:(-0.0105695659295) A[1]:(0.999899327755) A[2]:(-0.815991520882) A[3]:(0.664109826088)\n",
      " state (6)  A[0]:(0.000764459196944) A[1]:(0.810196995735) A[2]:(-0.00022292137146) A[3]:(0.655591547489)\n",
      " state (7)  A[0]:(0.543872475624) A[1]:(-0.540237307549) A[2]:(0.433528363705) A[3]:(0.871088683605)\n",
      " state (8)  A[0]:(0.656504392624) A[1]:(0.00087946630083) A[2]:(0.729213356972) A[3]:(0.590599060059)\n",
      " state (9)  A[0]:(0.656940221786) A[1]:(0.810424566269) A[2]:(0.810154795647) A[3]:(-6.13629817963e-05)\n",
      " state (10)  A[0]:(0.729958057404) A[1]:(0.900115191936) A[2]:(0.000533223093953) A[3]:(0.729101538658)\n",
      " state (11)  A[0]:(0.133350402117) A[1]:(0.882339060307) A[2]:(-0.924743413925) A[3]:(0.803220868111)\n",
      " state (12)  A[0]:(-0.42830696702) A[1]:(0.815318226814) A[2]:(-0.948032200336) A[3]:(0.715964317322)\n",
      " state (13)  A[0]:(0.000466942758067) A[1]:(0.809305429459) A[2]:(0.900053679943) A[3]:(0.729214370251)\n",
      " state (14)  A[0]:(0.810256123543) A[1]:(0.900147497654) A[2]:(0.999999761581) A[3]:(0.809937179089)\n",
      " state (15)  A[0]:(0.97958189249) A[1]:(0.939891636372) A[2]:(1.0) A[3]:(0.877447485924)\n",
      "Episode 562000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6036. Times reached goal: 989.               Steps done: 4032194. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0159618542017.\n",
      " state (0)  A[0]:(0.531871140003) A[1]:(0.590547382832) A[2]:(0.590602636337) A[3]:(0.531397044659)\n",
      " state (1)  A[0]:(0.531819164753) A[1]:(0.000356182426913) A[2]:(0.656067371368) A[3]:(0.590529203415)\n",
      " state (2)  A[0]:(0.590508520603) A[1]:(0.72911888361) A[2]:(0.591015458107) A[3]:(0.656098306179)\n",
      " state (3)  A[0]:(0.656819105148) A[1]:(-0.0645549297333) A[2]:(0.531386733055) A[3]:(0.545003294945)\n",
      " state (4)  A[0]:(0.590569853783) A[1]:(0.656158566475) A[2]:(-0.000188112258911) A[3]:(0.531577944756)\n",
      " state (5)  A[0]:(-0.0106909675524) A[1]:(0.999899148941) A[2]:(-0.815805673599) A[3]:(0.664927661419)\n",
      " state (6)  A[0]:(2.57343053818e-05) A[1]:(0.809973120689) A[2]:(-7.82012939453e-05) A[3]:(0.656130552292)\n",
      " state (7)  A[0]:(0.542919993401) A[1]:(-0.540393173695) A[2]:(0.433206439018) A[3]:(0.871110856533)\n",
      " state (8)  A[0]:(0.655469536781) A[1]:(0.000236496329308) A[2]:(0.729044556618) A[3]:(0.590540766716)\n",
      " state (9)  A[0]:(0.65581715107) A[1]:(0.810063064098) A[2]:(0.809985399246) A[3]:(0.000598311366048)\n",
      " state (10)  A[0]:(0.728741526604) A[1]:(0.900011718273) A[2]:(-0.000462174386485) A[3]:(0.729292392731)\n",
      " state (11)  A[0]:(0.13035261631) A[1]:(0.882393538952) A[2]:(-0.924968779087) A[3]:(0.803238093853)\n",
      " state (12)  A[0]:(-0.430789232254) A[1]:(0.815582633018) A[2]:(-0.948175728321) A[3]:(0.715936005116)\n",
      " state (13)  A[0]:(-0.00232484517619) A[1]:(0.809605300426) A[2]:(0.900120377541) A[3]:(0.729242920876)\n",
      " state (14)  A[0]:(0.809430778027) A[1]:(0.900227308273) A[2]:(0.999999761581) A[3]:(0.810035347939)\n",
      " state (15)  A[0]:(0.979511678219) A[1]:(0.93982988596) A[2]:(1.0) A[3]:(0.877571821213)\n",
      "Episode 563000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6043. Times reached goal: 977.               Steps done: 4038237. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0158656875769.\n",
      " state (0)  A[0]:(0.531287789345) A[1]:(0.590589523315) A[2]:(0.59034371376) A[3]:(0.531301498413)\n",
      " state (1)  A[0]:(0.531261086464) A[1]:(-6.44773244858e-05) A[2]:(0.656153321266) A[3]:(0.590349018574)\n",
      " state (2)  A[0]:(0.590102732182) A[1]:(0.729198455811) A[2]:(0.590874016285) A[3]:(0.656114041805)\n",
      " state (3)  A[0]:(0.656447470188) A[1]:(-0.0639911144972) A[2]:(0.531300008297) A[3]:(0.544929683208)\n",
      " state (4)  A[0]:(0.590326249599) A[1]:(0.655909776688) A[2]:(8.11815261841e-05) A[3]:(0.53152346611)\n",
      " state (5)  A[0]:(-0.010763855651) A[1]:(0.999899208546) A[2]:(-0.815767586231) A[3]:(0.665122866631)\n",
      " state (6)  A[0]:(6.09308481216e-05) A[1]:(0.810110330582) A[2]:(-0.000348567962646) A[3]:(0.655878067017)\n",
      " state (7)  A[0]:(0.543045520782) A[1]:(-0.540056288242) A[2]:(0.43269354105) A[3]:(0.870932221413)\n",
      " state (8)  A[0]:(0.655902266502) A[1]:(-0.000388011307223) A[2]:(0.728715538979) A[3]:(0.590963363647)\n",
      " state (9)  A[0]:(0.655862808228) A[1]:(0.809850096703) A[2]:(0.810100972652) A[3]:(-0.000220358371735)\n",
      " state (10)  A[0]:(0.728911161423) A[1]:(0.899962484837) A[2]:(0.000305771827698) A[3]:(0.728682160378)\n",
      " state (11)  A[0]:(0.131691664457) A[1]:(0.882419764996) A[2]:(-0.924884557724) A[3]:(0.802946209908)\n",
      " state (12)  A[0]:(-0.428931832314) A[1]:(0.815687596798) A[2]:(-0.948166012764) A[3]:(0.715774714947)\n",
      " state (13)  A[0]:(0.000382125348551) A[1]:(0.809706926346) A[2]:(0.900156795979) A[3]:(0.729269146919)\n",
      " state (14)  A[0]:(0.810326039791) A[1]:(0.90029489994) A[2]:(0.999999761581) A[3]:(0.810139536858)\n",
      " state (15)  A[0]:(0.979605734348) A[1]:(0.939890086651) A[2]:(1.0) A[3]:(0.877661824226)\n",
      "Episode 564000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6040. Times reached goal: 988.               Steps done: 4044277. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0157701476449.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5906,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6561, -0.0000,  0.5313]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6562,  0.0001,  0.7290,  0.5905]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6563,  0.8101,  0.8100, -0.0003]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0003,  0.8096,  0.9000,  0.7289]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9003,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531435549259) A[1]:(0.590577483177) A[2]:(0.590444624424) A[3]:(0.531549692154)\n",
      " state (1)  A[0]:(0.531383514404) A[1]:(-5.14686107635e-05) A[2]:(0.656069040298) A[3]:(0.590506613255)\n",
      " state (2)  A[0]:(0.590161442757) A[1]:(0.729019761086) A[2]:(0.590692281723) A[3]:(0.65601348877)\n",
      " state (3)  A[0]:(0.656462609768) A[1]:(-0.0641294121742) A[2]:(0.53123652935) A[3]:(0.544691026211)\n",
      " state (4)  A[0]:(0.590263307095) A[1]:(0.656115055084) A[2]:(0.0001220703125) A[3]:(0.531311511993)\n",
      " state (5)  A[0]:(-0.0109517667443) A[1]:(0.999899208546) A[2]:(-0.815692782402) A[3]:(0.66521191597)\n",
      " state (6)  A[0]:(-0.00019571185112) A[1]:(0.809985160828) A[2]:(0.00027596950531) A[3]:(0.655922174454)\n",
      " state (7)  A[0]:(0.542825102806) A[1]:(-0.540421366692) A[2]:(0.433534175158) A[3]:(0.870877981186)\n",
      " state (8)  A[0]:(0.655547559261) A[1]:(-5.52833080292e-05) A[2]:(0.729120612144) A[3]:(0.59002918005)\n",
      " state (9)  A[0]:(0.655844211578) A[1]:(0.810009598732) A[2]:(0.810060679913) A[3]:(-0.000775426451582)\n",
      " state (10)  A[0]:(0.728883862495) A[1]:(0.899995326996) A[2]:(-9.5009803772e-05) A[3]:(0.728577017784)\n",
      " state (11)  A[0]:(0.131095245481) A[1]:(0.882397651672) A[2]:(-0.925001204014) A[3]:(0.802760124207)\n",
      " state (12)  A[0]:(-0.429780215025) A[1]:(0.815586268902) A[2]:(-0.948288679123) A[3]:(0.715390324593)\n",
      " state (13)  A[0]:(-0.00056663149735) A[1]:(0.809578061104) A[2]:(0.900051474571) A[3]:(0.72889816761)\n",
      " state (14)  A[0]:(0.810186088085) A[1]:(0.900239706039) A[2]:(0.999999761581) A[3]:(0.809935033321)\n",
      " state (15)  A[0]:(0.97961217165) A[1]:(0.939851820469) A[2]:(1.0) A[3]:(0.877574205399)\n",
      "Episode 565000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6036. Times reached goal: 985.               Steps done: 4050313. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0156752457358.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53303027153) A[1]:(0.590669751167) A[2]:(0.590436577797) A[3]:(0.531890153885)\n",
      " state (1)  A[0]:(0.532622039318) A[1]:(0.000769436184783) A[2]:(0.656090140343) A[3]:(0.590841770172)\n",
      " state (2)  A[0]:(0.591290235519) A[1]:(0.729070067406) A[2]:(0.590863645077) A[3]:(0.656211793423)\n",
      " state (3)  A[0]:(0.657376170158) A[1]:(-0.0639907121658) A[2]:(0.53129529953) A[3]:(0.544810295105)\n",
      " state (4)  A[0]:(0.591333031654) A[1]:(0.656301856041) A[2]:(-0.000169396400452) A[3]:(0.531413912773)\n",
      " state (5)  A[0]:(-0.00910209305584) A[1]:(0.999899208546) A[2]:(-0.816012322903) A[3]:(0.665331959724)\n",
      " state (6)  A[0]:(0.00119027437177) A[1]:(0.809975266457) A[2]:(-0.000564694346394) A[3]:(0.655791580677)\n",
      " state (7)  A[0]:(0.543486535549) A[1]:(-0.539914131165) A[2]:(0.432874411345) A[3]:(0.870686113834)\n",
      " state (8)  A[0]:(0.655326545238) A[1]:(0.00112682534382) A[2]:(0.728920102119) A[3]:(0.588474929333)\n",
      " state (9)  A[0]:(0.655485212803) A[1]:(0.810217857361) A[2]:(0.809766530991) A[3]:(-0.00209980900399)\n",
      " state (10)  A[0]:(0.728541254997) A[1]:(0.900043845177) A[2]:(-0.00127303530462) A[3]:(0.728164851665)\n",
      " state (11)  A[0]:(0.130516842008) A[1]:(0.882428348064) A[2]:(-0.925204694271) A[3]:(0.802479922771)\n",
      " state (12)  A[0]:(-0.429791659117) A[1]:(0.815597474575) A[2]:(-0.948415756226) A[3]:(0.715145468712)\n",
      " state (13)  A[0]:(-0.000292718410492) A[1]:(0.809556663036) A[2]:(0.900028705597) A[3]:(0.728751778603)\n",
      " state (14)  A[0]:(0.810172796249) A[1]:(0.90023201704) A[2]:(0.999999761581) A[3]:(0.809798955917)\n",
      " state (15)  A[0]:(0.979590773582) A[1]:(0.939847350121) A[2]:(1.0) A[3]:(0.877444446087)\n",
      "Episode 566000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 978.               Steps done: 4056327. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0155812577133.\n",
      " state (0)  A[0]:(0.531464457512) A[1]:(0.590292036533) A[2]:(0.590379357338) A[3]:(0.531328558922)\n",
      " state (1)  A[0]:(0.531435370445) A[1]:(2.86102294922e-06) A[2]:(0.656047701836) A[3]:(0.590480387211)\n",
      " state (2)  A[0]:(0.590290546417) A[1]:(0.72893434763) A[2]:(0.590657711029) A[3]:(0.656071066856)\n",
      " state (3)  A[0]:(0.656420886517) A[1]:(-0.0641724169254) A[2]:(0.531110405922) A[3]:(0.544671595097)\n",
      " state (4)  A[0]:(0.590245902538) A[1]:(0.655828475952) A[2]:(-0.000103712081909) A[3]:(0.531482994556)\n",
      " state (5)  A[0]:(-0.0106727154925) A[1]:(0.999899148941) A[2]:(-0.815914511681) A[3]:(0.665831446648)\n",
      " state (6)  A[0]:(3.99649143219e-05) A[1]:(0.809966683388) A[2]:(-4.83989715576e-05) A[3]:(0.656141459942)\n",
      " state (7)  A[0]:(0.54302585125) A[1]:(-0.540078639984) A[2]:(0.433307826519) A[3]:(0.870949268341)\n",
      " state (8)  A[0]:(0.655748844147) A[1]:(7.9870223999e-05) A[2]:(0.72889983654) A[3]:(0.590673744678)\n",
      " state (9)  A[0]:(0.655925154686) A[1]:(0.81000071764) A[2]:(0.809940218925) A[3]:(0.000274777412415)\n",
      " state (10)  A[0]:(0.729094982147) A[1]:(0.900016605854) A[2]:(-0.000197291374207) A[3]:(0.729178845882)\n",
      " state (11)  A[0]:(0.131941363215) A[1]:(0.88245254755) A[2]:(-0.92508661747) A[3]:(0.803227543831)\n",
      " state (12)  A[0]:(-0.429109930992) A[1]:(0.815652251244) A[2]:(-0.948430657387) A[3]:(0.715861320496)\n",
      " state (13)  A[0]:(-0.000282451510429) A[1]:(0.809564709663) A[2]:(0.899971067905) A[3]:(0.729145824909)\n",
      " state (14)  A[0]:(0.809966683388) A[1]:(0.900197565556) A[2]:(0.999999761581) A[3]:(0.809956967831)\n",
      " state (15)  A[0]:(0.979558587074) A[1]:(0.939791023731) A[2]:(1.0) A[3]:(0.877507984638)\n",
      "Episode 567000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6041. Times reached goal: 983.               Steps done: 4062368. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0154874150725.\n",
      " state (0)  A[0]:(0.53134727478) A[1]:(0.590086698532) A[2]:(0.590380072594) A[3]:(0.531428933144)\n",
      " state (1)  A[0]:(0.531170487404) A[1]:(-3.46899032593e-05) A[2]:(0.655838251114) A[3]:(0.590313494205)\n",
      " state (2)  A[0]:(0.589946746826) A[1]:(0.728861689568) A[2]:(0.590625405312) A[3]:(0.655872046947)\n",
      " state (3)  A[0]:(0.656183838844) A[1]:(-0.0635599195957) A[2]:(0.531112790108) A[3]:(0.544528841972)\n",
      " state (4)  A[0]:(0.590117812157) A[1]:(0.656096100807) A[2]:(-4.3511390686e-05) A[3]:(0.531359136105)\n",
      " state (5)  A[0]:(-0.0107428021729) A[1]:(0.999899148941) A[2]:(-0.815969765186) A[3]:(0.665581703186)\n",
      " state (6)  A[0]:(1.71810388565e-05) A[1]:(0.809907436371) A[2]:(1.32322311401e-05) A[3]:(0.655618548393)\n",
      " state (7)  A[0]:(0.543197751045) A[1]:(-0.540272057056) A[2]:(0.433337062597) A[3]:(0.870759725571)\n",
      " state (8)  A[0]:(0.656062841415) A[1]:(-0.000236868858337) A[2]:(0.728845477104) A[3]:(0.590552687645)\n",
      " state (9)  A[0]:(0.65616953373) A[1]:(0.809979736805) A[2]:(0.8099386096) A[3]:(-2.80439853668e-05)\n",
      " state (10)  A[0]:(0.729258537292) A[1]:(0.900016844273) A[2]:(-0.000105500221252) A[3]:(0.729009747505)\n",
      " state (11)  A[0]:(0.132128015161) A[1]:(0.882473230362) A[2]:(-0.925109624863) A[3]:(0.803068697453)\n",
      " state (12)  A[0]:(-0.429078996181) A[1]:(0.815735697746) A[2]:(-0.948476076126) A[3]:(0.715589404106)\n",
      " state (13)  A[0]:(-0.000219956040382) A[1]:(0.809729516506) A[2]:(0.90001398325) A[3]:(0.728887081146)\n",
      " state (14)  A[0]:(0.810044288635) A[1]:(0.90035700798) A[2]:(0.999999761581) A[3]:(0.809802174568)\n",
      " state (15)  A[0]:(0.979573607445) A[1]:(0.93992215395) A[2]:(1.0) A[3]:(0.877419292927)\n",
      "Episode 568000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6031. Times reached goal: 988.               Steps done: 4068399. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0153942915684.\n",
      " state (0)  A[0]:(0.531510412693) A[1]:(0.590538144112) A[2]:(0.590738177299) A[3]:(0.531513035297)\n",
      " state (1)  A[0]:(0.531477212906) A[1]:(-4.97698783875e-06) A[2]:(0.656196713448) A[3]:(0.590690433979)\n",
      " state (2)  A[0]:(0.590411901474) A[1]:(0.728951215744) A[2]:(0.589860737324) A[3]:(0.656004667282)\n",
      " state (3)  A[0]:(0.656201303005) A[1]:(-0.0530093349516) A[2]:(0.529621005058) A[3]:(0.545545101166)\n",
      " state (4)  A[0]:(0.590661525726) A[1]:(0.656231701374) A[2]:(0.000169515609741) A[3]:(0.531574845314)\n",
      " state (5)  A[0]:(-0.011679193005) A[1]:(0.999899089336) A[2]:(-0.816565096378) A[3]:(0.664997160435)\n",
      " state (6)  A[0]:(4.73707914352e-05) A[1]:(0.809941709042) A[2]:(3.64780426025e-05) A[3]:(0.656195759773)\n",
      " state (7)  A[0]:(0.543661892414) A[1]:(-0.53998541832) A[2]:(0.433365821838) A[3]:(0.87103676796)\n",
      " state (8)  A[0]:(0.656122267246) A[1]:(7.77542591095e-05) A[2]:(0.728935837746) A[3]:(0.590454101562)\n",
      " state (9)  A[0]:(0.656289339066) A[1]:(0.810118556023) A[2]:(0.809961020947) A[3]:(0.000209361314774)\n",
      " state (10)  A[0]:(0.729213833809) A[1]:(0.900012850761) A[2]:(-0.000151991844177) A[3]:(0.729072272778)\n",
      " state (11)  A[0]:(0.131609424949) A[1]:(0.882329404354) A[2]:(-0.925179541111) A[3]:(0.802870571613)\n",
      " state (12)  A[0]:(-0.429220706224) A[1]:(0.815353929996) A[2]:(-0.948540687561) A[3]:(0.715315878391)\n",
      " state (13)  A[0]:(2.1904706955e-06) A[1]:(0.809285759926) A[2]:(0.90000385046) A[3]:(0.728855192661)\n",
      " state (14)  A[0]:(0.810071468353) A[1]:(0.900177717209) A[2]:(0.999999761581) A[3]:(0.809963345528)\n",
      " state (15)  A[0]:(0.979567945004) A[1]:(0.939871311188) A[2]:(1.0) A[3]:(0.877651453018)\n",
      "Episode 569000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6015. Times reached goal: 978.               Steps done: 4074414. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0153019728315.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5909,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5913,  0.6561,  0.0002,  0.5319]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6573, -0.0001,  0.7289,  0.5919]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6571,  0.8104,  0.8103,  0.0023]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0006,  0.8092,  0.9000,  0.7291]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9000,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? True\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531561136246) A[1]:(0.59077835083) A[2]:(0.590521097183) A[3]:(0.531283259392)\n",
      " state (1)  A[0]:(0.53162842989) A[1]:(-1.76131725311e-05) A[2]:(0.656147241592) A[3]:(0.590383052826)\n",
      " state (2)  A[0]:(0.590655982494) A[1]:(0.729117035866) A[2]:(0.589421093464) A[3]:(0.655799746513)\n",
      " state (3)  A[0]:(0.656054496765) A[1]:(-0.0440076775849) A[2]:(0.528500676155) A[3]:(0.546211063862)\n",
      " state (4)  A[0]:(0.590887308121) A[1]:(0.656230211258) A[2]:(0.000379800767405) A[3]:(0.531535565853)\n",
      " state (5)  A[0]:(-0.0128640737385) A[1]:(0.999898970127) A[2]:(-0.817159354687) A[3]:(0.663930773735)\n",
      " state (6)  A[0]:(3.36617231369e-05) A[1]:(0.810039579868) A[2]:(0.000261068344116) A[3]:(0.656220436096)\n",
      " state (7)  A[0]:(0.544038295746) A[1]:(-0.53951883316) A[2]:(0.433697938919) A[3]:(0.87112981081)\n",
      " state (8)  A[0]:(0.656209468842) A[1]:(0.000267058610916) A[2]:(0.729254722595) A[3]:(0.590480029583)\n",
      " state (9)  A[0]:(0.656152367592) A[1]:(0.810150742531) A[2]:(0.810191035271) A[3]:(0.000524848641362)\n",
      " state (10)  A[0]:(0.728492438793) A[1]:(0.899999976158) A[2]:(-0.00025200843811) A[3]:(0.728939294815)\n",
      " state (11)  A[0]:(0.129050284624) A[1]:(0.88225376606) A[2]:(-0.925344347954) A[3]:(0.802504658699)\n",
      " state (12)  A[0]:(-0.430922359228) A[1]:(0.815161824226) A[2]:(-0.948669672012) A[3]:(0.715005040169)\n",
      " state (13)  A[0]:(-0.0010774875991) A[1]:(0.809032976627) A[2]:(0.899938344955) A[3]:(0.728943705559)\n",
      " state (14)  A[0]:(0.809957385063) A[1]:(0.900034427643) A[2]:(0.999999761581) A[3]:(0.810238540173)\n",
      " state (15)  A[0]:(0.97958612442) A[1]:(0.939769387245) A[2]:(1.0) A[3]:(0.877970695496)\n",
      "Episode 570000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6058. Times reached goal: 988.               Steps done: 4080472. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0152095537003.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531610965729) A[1]:(0.590369999409) A[2]:(0.590445578098) A[3]:(0.53155040741)\n",
      " state (1)  A[0]:(0.531549572945) A[1]:(-8.05556774139e-05) A[2]:(0.656040549278) A[3]:(0.590445876122)\n",
      " state (2)  A[0]:(0.590601205826) A[1]:(0.729003667831) A[2]:(0.588904976845) A[3]:(0.655867934227)\n",
      " state (3)  A[0]:(0.655529260635) A[1]:(-0.034023232758) A[2]:(0.527094006538) A[3]:(0.547134339809)\n",
      " state (4)  A[0]:(0.590738892555) A[1]:(0.656071186066) A[2]:(0.000355005235178) A[3]:(0.531371355057)\n",
      " state (5)  A[0]:(-0.0146114407107) A[1]:(0.999898731709) A[2]:(-0.817943096161) A[3]:(0.662317872047)\n",
      " state (6)  A[0]:(0.000106319785118) A[1]:(0.809959411621) A[2]:(9.32216644287e-05) A[3]:(0.65590441227)\n",
      " state (7)  A[0]:(0.544508576393) A[1]:(-0.539157509804) A[2]:(0.43338778615) A[3]:(0.87097454071)\n",
      " state (8)  A[0]:(0.65604031086) A[1]:(0.0001120865345) A[2]:(0.728941082954) A[3]:(0.589261889458)\n",
      " state (9)  A[0]:(0.656053781509) A[1]:(0.810051143169) A[2]:(0.809936761856) A[3]:(-0.000925779051613)\n",
      " state (10)  A[0]:(0.728931665421) A[1]:(0.900010883808) A[2]:(-0.000393867463572) A[3]:(0.728484034538)\n",
      " state (11)  A[0]:(0.130871042609) A[1]:(0.882370710373) A[2]:(-0.92536431551) A[3]:(0.802159428596)\n",
      " state (12)  A[0]:(-0.429417669773) A[1]:(0.815474629402) A[2]:(-0.948726594448) A[3]:(0.714403510094)\n",
      " state (13)  A[0]:(7.07060098648e-05) A[1]:(0.809482693672) A[2]:(0.899980247021) A[3]:(0.728320837021)\n",
      " state (14)  A[0]:(0.80999225378) A[1]:(0.900354266167) A[2]:(0.999999761581) A[3]:(0.809818923473)\n",
      " state (15)  A[0]:(0.979568243027) A[1]:(0.939983904362) A[2]:(1.0) A[3]:(0.8777769804)\n",
      "Episode 571000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6043. Times reached goal: 989.               Steps done: 4086515. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0151179195189.\n",
      " state (0)  A[0]:(0.531715989113) A[1]:(0.58960211277) A[2]:(0.590569198132) A[3]:(0.53143966198)\n",
      " state (1)  A[0]:(0.531794548035) A[1]:(0.000172421336174) A[2]:(0.655956327915) A[3]:(0.590515196323)\n",
      " state (2)  A[0]:(0.591072201729) A[1]:(0.728436887264) A[2]:(0.589313924313) A[3]:(0.655977725983)\n",
      " state (3)  A[0]:(0.655813574791) A[1]:(-0.0292591508478) A[2]:(0.526892662048) A[3]:(0.547961950302)\n",
      " state (4)  A[0]:(0.591172933578) A[1]:(0.656088888645) A[2]:(0.000535607279744) A[3]:(0.531643331051)\n",
      " state (5)  A[0]:(-0.0157030131668) A[1]:(0.999898552895) A[2]:(-0.818466663361) A[3]:(0.661254048347)\n",
      " state (6)  A[0]:(0.000667765620165) A[1]:(0.809853434563) A[2]:(0.000101327896118) A[3]:(0.655758023262)\n",
      " state (7)  A[0]:(0.545686125755) A[1]:(-0.538274765015) A[2]:(0.433054864407) A[3]:(0.871166229248)\n",
      " state (8)  A[0]:(0.656928598881) A[1]:(0.000855013495311) A[2]:(0.728644907475) A[3]:(0.590269804001)\n",
      " state (9)  A[0]:(0.656538128853) A[1]:(0.810255050659) A[2]:(0.809858202934) A[3]:(8.4400177002e-05)\n",
      " state (10)  A[0]:(0.729363203049) A[1]:(0.900009155273) A[2]:(-0.000380039185984) A[3]:(0.728799760342)\n",
      " state (11)  A[0]:(0.131831422448) A[1]:(0.882181167603) A[2]:(-0.925425648689) A[3]:(0.802261710167)\n",
      " state (12)  A[0]:(-0.428630530834) A[1]:(0.814954280853) A[2]:(-0.948818981647) A[3]:(0.714415848255)\n",
      " state (13)  A[0]:(0.00117763818707) A[1]:(0.808829367161) A[2]:(0.899972736835) A[3]:(0.728307723999)\n",
      " state (14)  A[0]:(0.810433268547) A[1]:(0.899995207787) A[2]:(0.999999761581) A[3]:(0.809806704521)\n",
      " state (15)  A[0]:(0.979624986649) A[1]:(0.939758419991) A[2]:(1.0) A[3]:(0.877804100513)\n",
      "Episode 572000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6050. Times reached goal: 988.               Steps done: 4092565. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0150267322255.\n",
      " state (0)  A[0]:(0.530679345131) A[1]:(0.590588152409) A[2]:(0.590133607388) A[3]:(0.530857086182)\n",
      " state (1)  A[0]:(0.53086400032) A[1]:(0.00010934472084) A[2]:(0.656018674374) A[3]:(0.590026021004)\n",
      " state (2)  A[0]:(0.590119242668) A[1]:(0.729089379311) A[2]:(0.58905851841) A[3]:(0.655623435974)\n",
      " state (3)  A[0]:(0.654766917229) A[1]:(-0.0224898979068) A[2]:(0.525851428509) A[3]:(0.547932982445)\n",
      " state (4)  A[0]:(0.590217590332) A[1]:(0.656073212624) A[2]:(-2.76565551758e-05) A[3]:(0.530904173851)\n",
      " state (5)  A[0]:(-0.01860062778) A[1]:(0.99989849329) A[2]:(-0.819242417812) A[3]:(0.659936547279)\n",
      " state (6)  A[0]:(-0.00170129374601) A[1]:(0.810104131699) A[2]:(7.36713409424e-05) A[3]:(0.655786514282)\n",
      " state (7)  A[0]:(0.543666958809) A[1]:(-0.538425922394) A[2]:(0.43401786685) A[3]:(0.871032953262)\n",
      " state (8)  A[0]:(0.654422163963) A[1]:(0.000916391378269) A[2]:(0.729314148426) A[3]:(0.587628722191)\n",
      " state (9)  A[0]:(0.654557585716) A[1]:(0.810203671455) A[2]:(0.810014426708) A[3]:(-0.00288661522791)\n",
      " state (10)  A[0]:(0.728183627129) A[1]:(0.900087833405) A[2]:(-0.000270009040833) A[3]:(0.728124976158)\n",
      " state (11)  A[0]:(0.129693239927) A[1]:(0.882511198521) A[2]:(-0.925512135029) A[3]:(0.802059650421)\n",
      " state (12)  A[0]:(-0.430750906467) A[1]:(0.815779268742) A[2]:(-0.948948681355) A[3]:(0.714299678802)\n",
      " state (13)  A[0]:(-0.00165818480309) A[1]:(0.809894144535) A[2]:(0.899994432926) A[3]:(0.728222429752)\n",
      " state (14)  A[0]:(0.809659719467) A[1]:(0.900590419769) A[2]:(0.999999761581) A[3]:(0.809738516808)\n",
      " state (15)  A[0]:(0.979590952396) A[1]:(0.940034687519) A[2]:(1.0) A[3]:(0.877829313278)\n",
      "Episode 573000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6028. Times reached goal: 983.               Steps done: 4098593. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0149364235474.\n",
      " state (0)  A[0]:(0.531959235668) A[1]:(0.590694069862) A[2]:(0.590558290482) A[3]:(0.53166782856)\n",
      " state (1)  A[0]:(0.531997799873) A[1]:(9.96440649033e-05) A[2]:(0.656073868275) A[3]:(0.590529441833)\n",
      " state (2)  A[0]:(0.591098725796) A[1]:(0.729028761387) A[2]:(0.589275121689) A[3]:(0.656016647816)\n",
      " state (3)  A[0]:(0.655596911907) A[1]:(-0.0186121575534) A[2]:(0.525652706623) A[3]:(0.548912882805)\n",
      " state (4)  A[0]:(0.591352939606) A[1]:(0.655905127525) A[2]:(0.000213146209717) A[3]:(0.5315502882)\n",
      " state (5)  A[0]:(-0.0179357957095) A[1]:(0.999898314476) A[2]:(-0.819617927074) A[3]:(0.659682154655)\n",
      " state (6)  A[0]:(0.00024202466011) A[1]:(0.810024499893) A[2]:(-1.60932540894e-05) A[3]:(0.656066775322)\n",
      " state (7)  A[0]:(0.545412659645) A[1]:(-0.538521945477) A[2]:(0.433865278959) A[3]:(0.871298491955)\n",
      " state (8)  A[0]:(0.656099796295) A[1]:(-0.000532641948666) A[2]:(0.728961646557) A[3]:(0.589895129204)\n",
      " state (9)  A[0]:(0.655466020107) A[1]:(0.809838414192) A[2]:(0.810013234615) A[3]:(-0.000948637432884)\n",
      " state (10)  A[0]:(0.728726267815) A[1]:(0.899990916252) A[2]:(4.02927398682e-05) A[3]:(0.728594064713)\n",
      " state (11)  A[0]:(0.131047129631) A[1]:(0.882471561432) A[2]:(-0.925528645515) A[3]:(0.802373468876)\n",
      " state (12)  A[0]:(-0.429560065269) A[1]:(0.815767526627) A[2]:(-0.949007749557) A[3]:(0.714718103409)\n",
      " state (13)  A[0]:(-0.000533521117177) A[1]:(0.809861540794) A[2]:(0.900021195412) A[3]:(0.728588044643)\n",
      " state (14)  A[0]:(0.809810042381) A[1]:(0.900539457798) A[2]:(0.999999761581) A[3]:(0.809928894043)\n",
      " state (15)  A[0]:(0.979592204094) A[1]:(0.939961910248) A[2]:(1.0) A[3]:(0.877940535545)\n",
      "Episode 574000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6051. Times reached goal: 982.               Steps done: 4104644. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0148463161434.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5903,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0000,  0.6563,  0.5906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7290,  0.5895,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  0.8101,  0.0001,  0.6565]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000, -0.0003,  0.7292]], device='cuda:0')\n",
      "On state=10, selected action=2 , Random? True\n",
      "new state=11, done=True. Reward: 0.0\n",
      " state (0)  A[0]:(0.531291604042) A[1]:(0.590284824371) A[2]:(0.590522587299) A[3]:(0.53141784668)\n",
      " state (1)  A[0]:(0.531377315521) A[1]:(-2.4601817131e-05) A[2]:(0.65630710125) A[3]:(0.590560436249)\n",
      " state (2)  A[0]:(0.590593695641) A[1]:(0.72900134325) A[2]:(0.589497685432) A[3]:(0.656183481216)\n",
      " state (3)  A[0]:(0.654961585999) A[1]:(-0.0157126821578) A[2]:(0.525400400162) A[3]:(0.549299836159)\n",
      " state (4)  A[0]:(0.590500354767) A[1]:(0.656223416328) A[2]:(-2.53915786743e-05) A[3]:(0.531520605087)\n",
      " state (5)  A[0]:(-0.0200216528028) A[1]:(0.999898254871) A[2]:(-0.819940805435) A[3]:(0.659194290638)\n",
      " state (6)  A[0]:(-0.000132158398628) A[1]:(0.810043931007) A[2]:(0.000134825706482) A[3]:(0.656530022621)\n",
      " state (7)  A[0]:(0.545519828796) A[1]:(-0.538307368755) A[2]:(0.43423679471) A[3]:(0.871555387974)\n",
      " state (8)  A[0]:(0.656329631805) A[1]:(-0.000166937708855) A[2]:(0.729224979877) A[3]:(0.590153992176)\n",
      " state (9)  A[0]:(0.656297445297) A[1]:(0.80989575386) A[2]:(0.810080826283) A[3]:(0.000239491462708)\n",
      " state (10)  A[0]:(0.729429543018) A[1]:(0.899996817112) A[2]:(-0.000205039978027) A[3]:(0.729225635529)\n",
      " state (11)  A[0]:(0.132221400738) A[1]:(0.882422685623) A[2]:(-0.925669491291) A[3]:(0.802711248398)\n",
      " state (12)  A[0]:(-0.428717017174) A[1]:(0.815569043159) A[2]:(-0.94913572073) A[3]:(0.71506279707)\n",
      " state (13)  A[0]:(0.000425636739237) A[1]:(0.809515416622) A[2]:(0.900043964386) A[3]:(0.728890776634)\n",
      " state (14)  A[0]:(0.810096621513) A[1]:(0.900257706642) A[2]:(0.999999761581) A[3]:(0.810130238533)\n",
      " state (15)  A[0]:(0.979627370834) A[1]:(0.939702093601) A[2]:(1.0) A[3]:(0.878101289272)\n",
      "Episode 575000 finished after 0 timesteps with r=0.0. Running score: 0.99. Times trained:               6032. Times reached goal: 982.               Steps done: 4110676. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.014757032714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531613230705) A[1]:(0.590543746948) A[2]:(0.590484976768) A[3]:(0.531755685806)\n",
      " state (1)  A[0]:(0.531468212605) A[1]:(-0.0001110881567) A[2]:(0.656131148338) A[3]:(0.590723395348)\n",
      " state (2)  A[0]:(0.590825676918) A[1]:(0.728958368301) A[2]:(0.58941257) A[3]:(0.656214356422)\n",
      " state (3)  A[0]:(0.655280947685) A[1]:(-0.0121241677552) A[2]:(0.52502733469) A[3]:(0.549803376198)\n",
      " state (4)  A[0]:(0.591069221497) A[1]:(0.656096696854) A[2]:(0.00013279914856) A[3]:(0.531669437885)\n",
      " state (5)  A[0]:(-0.0207614712417) A[1]:(0.999898016453) A[2]:(-0.820209860802) A[3]:(0.658429861069)\n",
      " state (6)  A[0]:(0.000172987580299) A[1]:(0.809918642044) A[2]:(9.65595245361e-06) A[3]:(0.65638500452)\n",
      " state (7)  A[0]:(0.54621219635) A[1]:(-0.538048326969) A[2]:(0.433863937855) A[3]:(0.871690750122)\n",
      " state (8)  A[0]:(0.657014846802) A[1]:(-0.000234112143517) A[2]:(0.728859484196) A[3]:(0.591499328613)\n",
      " state (9)  A[0]:(0.656498908997) A[1]:(0.810006022453) A[2]:(0.810045778751) A[3]:(0.00149810197763)\n",
      " state (10)  A[0]:(0.729561090469) A[1]:(0.900024652481) A[2]:(0.000427365273936) A[3]:(0.729589939117)\n",
      " state (11)  A[0]:(0.132608383894) A[1]:(0.882384598255) A[2]:(-0.925592362881) A[3]:(0.802922308445)\n",
      " state (12)  A[0]:(-0.428662568331) A[1]:(0.815440237522) A[2]:(-0.94916343689) A[3]:(0.715172410011)\n",
      " state (13)  A[0]:(0.000178918242455) A[1]:(0.809381365776) A[2]:(0.899991214275) A[3]:(0.728832602501)\n",
      " state (14)  A[0]:(0.809975862503) A[1]:(0.900244116783) A[2]:(0.999999761581) A[3]:(0.809982180595)\n",
      " state (15)  A[0]:(0.979614317417) A[1]:(0.939731121063) A[2]:(1.0) A[3]:(0.877971231937)\n",
      "Episode 576000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6024. Times reached goal: 981.               Steps done: 4116700. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0146684035679.\n",
      " state (0)  A[0]:(0.531443893909) A[1]:(0.590355992317) A[2]:(0.590467453003) A[3]:(0.53169465065)\n",
      " state (1)  A[0]:(0.531610846519) A[1]:(-5.96046447754e-07) A[2]:(0.656112194061) A[3]:(0.59062063694)\n",
      " state (2)  A[0]:(0.590830862522) A[1]:(0.729154229164) A[2]:(0.589424133301) A[3]:(0.656212925911)\n",
      " state (3)  A[0]:(0.655265688896) A[1]:(-0.00963884498924) A[2]:(0.524761199951) A[3]:(0.550071716309)\n",
      " state (4)  A[0]:(0.591030359268) A[1]:(0.656120896339) A[2]:(5.44786453247e-05) A[3]:(0.53164434433)\n",
      " state (5)  A[0]:(-0.0218170825392) A[1]:(0.999898016453) A[2]:(-0.820458054543) A[3]:(0.657848596573)\n",
      " state (6)  A[0]:(0.000142157077789) A[1]:(0.81006526947) A[2]:(0.000139355659485) A[3]:(0.656354546547)\n",
      " state (7)  A[0]:(0.546149015427) A[1]:(-0.538086533546) A[2]:(0.434285640717) A[3]:(0.871601879597)\n",
      " state (8)  A[0]:(0.656594753265) A[1]:(-0.000466465920908) A[2]:(0.72913646698) A[3]:(0.590480089188)\n",
      " state (9)  A[0]:(0.656243920326) A[1]:(0.809831261635) A[2]:(0.810069859028) A[3]:(0.000532925070729)\n",
      " state (10)  A[0]:(0.729404985905) A[1]:(0.899993956089) A[2]:(-0.000229954719543) A[3]:(0.729336261749)\n",
      " state (11)  A[0]:(0.132294520736) A[1]:(0.882489025593) A[2]:(-0.92582243681) A[3]:(0.80281150341)\n",
      " state (12)  A[0]:(-0.429026186466) A[1]:(0.815775632858) A[2]:(-0.949352562428) A[3]:(0.715136647224)\n",
      " state (13)  A[0]:(-0.000429153413279) A[1]:(0.809827446938) A[2]:(0.900001883507) A[3]:(0.728918433189)\n",
      " state (14)  A[0]:(0.80976575613) A[1]:(0.90046274662) A[2]:(0.999999761581) A[3]:(0.810128986835)\n",
      " state (15)  A[0]:(0.979607522488) A[1]:(0.939769387245) A[2]:(1.0) A[3]:(0.878153264523)\n",
      "Episode 577000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6062. Times reached goal: 991.               Steps done: 4122762. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0145797526778.\n",
      " state (0)  A[0]:(0.532727360725) A[1]:(0.590452492237) A[2]:(0.590338349342) A[3]:(0.528165340424)\n",
      " state (1)  A[0]:(0.533050596714) A[1]:(0.000916316872463) A[2]:(0.656087219715) A[3]:(0.588253498077)\n",
      " state (2)  A[0]:(0.592290997505) A[1]:(0.729065418243) A[2]:(0.58950561285) A[3]:(0.65426582098)\n",
      " state (3)  A[0]:(0.656717896461) A[1]:(-0.00798129476607) A[2]:(0.524796366692) A[3]:(0.548112869263)\n",
      " state (4)  A[0]:(0.592852413654) A[1]:(0.656153440475) A[2]:(0.000393867463572) A[3]:(0.52959895134)\n",
      " state (5)  A[0]:(-0.019595483318) A[1]:(0.999897897243) A[2]:(-0.820528626442) A[3]:(0.655903458595)\n",
      " state (6)  A[0]:(0.00330612342805) A[1]:(0.809830307961) A[2]:(0.000295519828796) A[3]:(0.65523660183)\n",
      " state (7)  A[0]:(0.548720061779) A[1]:(-0.538647830486) A[2]:(0.434239506721) A[3]:(0.871480822563)\n",
      " state (8)  A[0]:(0.659002900124) A[1]:(-0.00198547285981) A[2]:(0.728813886642) A[3]:(0.591786563396)\n",
      " state (9)  A[0]:(0.65814781189) A[1]:(0.809503614902) A[2]:(0.810215890408) A[3]:(0.00155728927348)\n",
      " state (10)  A[0]:(0.731080889702) A[1]:(0.899771332741) A[2]:(0.00204276759177) A[3]:(0.730112791061)\n",
      " state (11)  A[0]:(0.134897202253) A[1]:(0.882103204727) A[2]:(-0.925484538078) A[3]:(0.80360853672)\n",
      " state (12)  A[0]:(-0.429717689753) A[1]:(0.815007150173) A[2]:(-0.949361681938) A[3]:(0.715731322765)\n",
      " state (13)  A[0]:(-0.00419359421358) A[1]:(0.808887779713) A[2]:(0.899661958218) A[3]:(0.729029893875)\n",
      " state (14)  A[0]:(0.808140397072) A[1]:(0.899889290333) A[2]:(0.999999761581) A[3]:(0.810158610344)\n",
      " state (15)  A[0]:(0.979427814484) A[1]:(0.939357876778) A[2]:(1.0) A[3]:(0.878247857094)\n",
      "Episode 578000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6035. Times reached goal: 982.               Steps done: 4128797. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0144920288433.\n",
      " state (0)  A[0]:(0.531751275063) A[1]:(0.590507984161) A[2]:(0.590473353863) A[3]:(0.531554698944)\n",
      " state (1)  A[0]:(0.531727254391) A[1]:(-3.49581241608e-05) A[2]:(0.65609061718) A[3]:(0.590514361858)\n",
      " state (2)  A[0]:(0.591022491455) A[1]:(0.729012191296) A[2]:(0.589621543884) A[3]:(0.656116843224)\n",
      " state (3)  A[0]:(0.655605316162) A[1]:(-0.00669794203714) A[2]:(0.524609088898) A[3]:(0.550408363342)\n",
      " state (4)  A[0]:(0.591439723969) A[1]:(0.656093835831) A[2]:(9.01222229004e-05) A[3]:(0.531579375267)\n",
      " state (5)  A[0]:(-0.0232926942408) A[1]:(0.999897837639) A[2]:(-0.820825755596) A[3]:(0.656759679317)\n",
      " state (6)  A[0]:(0.000189378857613) A[1]:(0.810006380081) A[2]:(6.31809234619e-06) A[3]:(0.656267285347)\n",
      " state (7)  A[0]:(0.546515822411) A[1]:(-0.53785610199) A[2]:(0.434172093868) A[3]:(0.871716380119)\n",
      " state (8)  A[0]:(0.656552433968) A[1]:(-0.000138908624649) A[2]:(0.728999257088) A[3]:(0.590456843376)\n",
      " state (9)  A[0]:(0.655941367149) A[1]:(0.809956848621) A[2]:(0.809986889362) A[3]:(4.65512275696e-05)\n",
      " state (10)  A[0]:(0.729239940643) A[1]:(0.899968087673) A[2]:(-0.000158309936523) A[3]:(0.729172468185)\n",
      " state (11)  A[0]:(0.132391631603) A[1]:(0.882353246212) A[2]:(-0.925883412361) A[3]:(0.802772819996)\n",
      " state (12)  A[0]:(-0.428758323193) A[1]:(0.815450429916) A[2]:(-0.949485361576) A[3]:(0.715116143227)\n",
      " state (13)  A[0]:(0.000307366251945) A[1]:(0.809459924698) A[2]:(0.899981021881) A[3]:(0.728912115097)\n",
      " state (14)  A[0]:(0.810256123543) A[1]:(0.900313019753) A[2]:(0.999999761581) A[3]:(0.810113847256)\n",
      " state (15)  A[0]:(0.979687035084) A[1]:(0.939687073231) A[2]:(1.0) A[3]:(0.87815374136)\n",
      "Episode 579000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6041. Times reached goal: 985.               Steps done: 4134838. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0144047463992.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5906,  0.5905,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5915,  0.6563,  0.0003,  0.5313]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6567, -0.0001,  0.7293,  0.5902]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560,  0.8099,  0.8100, -0.0001]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9001, -0.0001,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9001,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531395673752) A[1]:(0.590561330318) A[2]:(0.590477108955) A[3]:(0.531125068665)\n",
      " state (1)  A[0]:(0.531680107117) A[1]:(-0.000295415520668) A[2]:(0.656347334385) A[3]:(0.590308427811)\n",
      " state (2)  A[0]:(0.591153502464) A[1]:(0.729041695595) A[2]:(0.590038061142) A[3]:(0.656075000763)\n",
      " state (3)  A[0]:(0.655750393867) A[1]:(-0.00616401061416) A[2]:(0.524809837341) A[3]:(0.55036509037)\n",
      " state (4)  A[0]:(0.591462016106) A[1]:(0.656156361103) A[2]:(0.000100374221802) A[3]:(0.531362771988)\n",
      " state (5)  A[0]:(-0.0239747110754) A[1]:(0.999897837639) A[2]:(-0.820944547653) A[3]:(0.656430363655)\n",
      " state (6)  A[0]:(0.000494971813168) A[1]:(0.81003510952) A[2]:(0.000179171562195) A[3]:(0.656379461288)\n",
      " state (7)  A[0]:(0.54686164856) A[1]:(-0.537862658501) A[2]:(0.434600621462) A[3]:(0.871777415276)\n",
      " state (8)  A[0]:(0.656905353069) A[1]:(-0.000134959816933) A[2]:(0.729300379753) A[3]:(0.590596914291)\n",
      " state (9)  A[0]:(0.656533360481) A[1]:(0.809949994087) A[2]:(0.810209989548) A[3]:(0.000648975255899)\n",
      " state (10)  A[0]:(0.72974729538) A[1]:(0.899966537952) A[2]:(0.000494956912007) A[3]:(0.729444980621)\n",
      " state (11)  A[0]:(0.133395209908) A[1]:(0.882338941097) A[2]:(-0.925847411156) A[3]:(0.802944898605)\n",
      " state (12)  A[0]:(-0.428213089705) A[1]:(0.81535577774) A[2]:(-0.949507951736) A[3]:(0.715294241905)\n",
      " state (13)  A[0]:(0.000358715624316) A[1]:(0.809236824512) A[2]:(0.900096178055) A[3]:(0.729045927525)\n",
      " state (14)  A[0]:(0.810015499592) A[1]:(0.900104224682) A[2]:(0.999999761581) A[3]:(0.81018781662)\n",
      " state (15)  A[0]:(0.979639291763) A[1]:(0.939483046532) A[2]:(1.0) A[3]:(0.878212213516)\n",
      "Episode 580000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6041. Times reached goal: 987.               Steps done: 4140879. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0143179896388.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53098076582) A[1]:(0.59047460556) A[2]:(0.589971184731) A[3]:(0.53121316433)\n",
      " state (1)  A[0]:(0.530939936638) A[1]:(-9.12100076675e-05) A[2]:(0.656160354614) A[3]:(0.589913249016)\n",
      " state (2)  A[0]:(0.590085148811) A[1]:(0.729071736336) A[2]:(0.590067386627) A[3]:(0.655696034431)\n",
      " state (3)  A[0]:(0.654822349548) A[1]:(-0.0047577410005) A[2]:(0.524650335312) A[3]:(0.550155639648)\n",
      " state (4)  A[0]:(0.590478658676) A[1]:(0.656278729439) A[2]:(-9.03606414795e-05) A[3]:(0.53105700016)\n",
      " state (5)  A[0]:(-0.0257642008364) A[1]:(0.99989771843) A[2]:(-0.821158051491) A[3]:(0.655798137188)\n",
      " state (6)  A[0]:(-0.000384837359888) A[1]:(0.809973955154) A[2]:(-0.000436425179942) A[3]:(0.656204342842)\n",
      " state (7)  A[0]:(0.546569347382) A[1]:(-0.537413418293) A[2]:(0.433795213699) A[3]:(0.871783733368)\n",
      " state (8)  A[0]:(0.656729221344) A[1]:(0.000910863047466) A[2]:(0.728944778442) A[3]:(0.590472698212)\n",
      " state (9)  A[0]:(0.656727194786) A[1]:(0.810294628143) A[2]:(0.809993386269) A[3]:(0.00107130361721)\n",
      " state (10)  A[0]:(0.730114817619) A[1]:(0.90003323555) A[2]:(-0.000146150588989) A[3]:(0.729613542557)\n",
      " state (11)  A[0]:(0.134755373001) A[1]:(0.882274031639) A[2]:(-0.925981342793) A[3]:(0.802990078926)\n",
      " state (12)  A[0]:(-0.426355928183) A[1]:(0.815093159676) A[2]:(-0.94963234663) A[3]:(0.715350627899)\n",
      " state (13)  A[0]:(0.00372856622562) A[1]:(0.808898627758) A[2]:(0.899976551533) A[3]:(0.729159832001)\n",
      " state (14)  A[0]:(0.811546325684) A[1]:(0.899961352348) A[2]:(0.999999761581) A[3]:(0.810296297073)\n",
      " state (15)  A[0]:(0.979843080044) A[1]:(0.939430236816) A[2]:(1.0) A[3]:(0.878289401531)\n",
      "Episode 581000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6031. Times reached goal: 982.               Steps done: 4146910. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0142318977144.\n",
      " state (0)  A[0]:(0.531465411186) A[1]:(0.590455114841) A[2]:(0.590373635292) A[3]:(0.531390428543)\n",
      " state (1)  A[0]:(0.531569361687) A[1]:(-3.53455543518e-05) A[2]:(0.65600836277) A[3]:(0.590516328812)\n",
      " state (2)  A[0]:(0.590863406658) A[1]:(0.728916049004) A[2]:(0.589805722237) A[3]:(0.656144142151)\n",
      " state (3)  A[0]:(0.655559420586) A[1]:(-0.00481798499823) A[2]:(0.524491548538) A[3]:(0.550805687904)\n",
      " state (4)  A[0]:(0.591143488884) A[1]:(0.656001567841) A[2]:(-3.93390655518e-05) A[3]:(0.531528413296)\n",
      " state (5)  A[0]:(-0.0257161911577) A[1]:(0.999897658825) A[2]:(-0.821139454842) A[3]:(0.655649423599)\n",
      " state (6)  A[0]:(0.000237911939621) A[1]:(0.809950947762) A[2]:(-6.17504119873e-05) A[3]:(0.656308710575)\n",
      " state (7)  A[0]:(0.546757698059) A[1]:(-0.537746667862) A[2]:(0.434216201305) A[3]:(0.871803164482)\n",
      " state (8)  A[0]:(0.656527638435) A[1]:(-0.000161230564117) A[2]:(0.728957414627) A[3]:(0.590508937836)\n",
      " state (9)  A[0]:(0.655872106552) A[1]:(0.809960782528) A[2]:(0.809945702553) A[3]:(2.98023223877e-06)\n",
      " state (10)  A[0]:(0.729145646095) A[1]:(0.899974346161) A[2]:(-0.000297784805298) A[3]:(0.729113161564)\n",
      " state (11)  A[0]:(0.132451087236) A[1]:(0.882399022579) A[2]:(-0.926069378853) A[3]:(0.802812039852)\n",
      " state (12)  A[0]:(-0.428849905729) A[1]:(0.815516829491) A[2]:(-0.949741125107) A[3]:(0.71519112587)\n",
      " state (13)  A[0]:(-1.31726264954e-05) A[1]:(0.809458732605) A[2]:(0.900011539459) A[3]:(0.728938341141)\n",
      " state (14)  A[0]:(0.810177385807) A[1]:(0.900253713131) A[2]:(0.999999761581) A[3]:(0.810035347939)\n",
      " state (15)  A[0]:(0.979695260525) A[1]:(0.939529240131) A[2]:(1.0) A[3]:(0.878090381622)\n",
      "Episode 582000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6015. Times reached goal: 987.               Steps done: 4152925. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0141465497909.\n",
      " state (0)  A[0]:(0.531630277634) A[1]:(0.590504169464) A[2]:(0.590442538261) A[3]:(0.531408786774)\n",
      " state (1)  A[0]:(0.531548142433) A[1]:(8.28504562378e-05) A[2]:(0.656052112579) A[3]:(0.590583562851)\n",
      " state (2)  A[0]:(0.590876698494) A[1]:(0.729032516479) A[2]:(0.589803397655) A[3]:(0.656219244003)\n",
      " state (3)  A[0]:(0.655595004559) A[1]:(-0.00417551957071) A[2]:(0.524401843548) A[3]:(0.551044523716)\n",
      " state (4)  A[0]:(0.591116666794) A[1]:(0.656089603901) A[2]:(-9.94205474854e-05) A[3]:(0.531797528267)\n",
      " state (5)  A[0]:(-0.0261574164033) A[1]:(0.999897658825) A[2]:(-0.821139931679) A[3]:(0.655841708183)\n",
      " state (6)  A[0]:(0.000748142483644) A[1]:(0.81006193161) A[2]:(-7.97510147095e-05) A[3]:(0.656733036041)\n",
      " state (7)  A[0]:(0.547316074371) A[1]:(-0.537166833878) A[2]:(0.434082597494) A[3]:(0.871999144554)\n",
      " state (8)  A[0]:(0.657184481621) A[1]:(0.000367522210581) A[2]:(0.728978157043) A[3]:(0.591440141201)\n",
      " state (9)  A[0]:(0.656842887402) A[1]:(0.810033679008) A[2]:(0.810071706772) A[3]:(0.00217273482122)\n",
      " state (10)  A[0]:(0.73002320528) A[1]:(0.900013804436) A[2]:(0.000179171562195) A[3]:(0.730032861233)\n",
      " state (11)  A[0]:(0.134471237659) A[1]:(0.882420122623) A[2]:(-0.926046490669) A[3]:(0.803383827209)\n",
      " state (12)  A[0]:(-0.427162081003) A[1]:(0.815429389477) A[2]:(-0.949786186218) A[3]:(0.715839982033)\n",
      " state (13)  A[0]:(0.00176696293056) A[1]:(0.809171915054) A[2]:(0.899931430817) A[3]:(0.729510545731)\n",
      " state (14)  A[0]:(0.810606062412) A[1]:(0.899989128113) A[2]:(0.999999761581) A[3]:(0.810449302197)\n",
      " state (15)  A[0]:(0.979727327824) A[1]:(0.939308941364) A[2]:(1.0) A[3]:(0.878374814987)\n",
      "Episode 583000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6066. Times reached goal: 991.               Steps done: 4158991. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0140609965651.\n",
      " state (0)  A[0]:(0.532182574272) A[1]:(0.590534687042) A[2]:(0.590436935425) A[3]:(0.531056582928)\n",
      " state (1)  A[0]:(0.532593488693) A[1]:(0.000685110571794) A[2]:(0.655948638916) A[3]:(0.590357780457)\n",
      " state (2)  A[0]:(0.592104911804) A[1]:(0.729060530663) A[2]:(0.589734196663) A[3]:(0.656016945839)\n",
      " state (3)  A[0]:(0.656839430332) A[1]:(-0.00364264729433) A[2]:(0.524250149727) A[3]:(0.550933361053)\n",
      " state (4)  A[0]:(0.592549681664) A[1]:(0.656182348728) A[2]:(-0.000462889642222) A[3]:(0.5316375494)\n",
      " state (5)  A[0]:(-0.0245477259159) A[1]:(0.999897658825) A[2]:(-0.82149118185) A[3]:(0.655555903912)\n",
      " state (6)  A[0]:(0.00313216913491) A[1]:(0.810032844543) A[2]:(-0.00090217567049) A[3]:(0.656970500946)\n",
      " state (7)  A[0]:(0.54930627346) A[1]:(-0.538281917572) A[2]:(0.433853566647) A[3]:(0.872229933739)\n",
      " state (8)  A[0]:(0.659314036369) A[1]:(-0.00152717414312) A[2]:(0.728934168816) A[3]:(0.592689037323)\n",
      " state (9)  A[0]:(0.659742355347) A[1]:(0.80949306488) A[2]:(0.810458183289) A[3]:(0.00450512720272)\n",
      " state (10)  A[0]:(0.733651697636) A[1]:(0.899668991566) A[2]:(0.00328456168063) A[3]:(0.731884837151)\n",
      " state (11)  A[0]:(0.143553361297) A[1]:(0.881957769394) A[2]:(-0.925558924675) A[3]:(0.805160999298)\n",
      " state (12)  A[0]:(-0.421045571566) A[1]:(0.814648747444) A[2]:(-0.949716687202) A[3]:(0.717792272568)\n",
      " state (13)  A[0]:(0.00693567888811) A[1]:(0.808343589306) A[2]:(0.899572372437) A[3]:(0.730740904808)\n",
      " state (14)  A[0]:(0.811947405338) A[1]:(0.899613499641) A[2]:(0.999999761581) A[3]:(0.811058282852)\n",
      " state (15)  A[0]:(0.979862749577) A[1]:(0.939145505428) A[2]:(1.0) A[3]:(0.87867808342)\n",
      "Episode 584000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 986.               Steps done: 4165002. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0139767294334.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5908,  0.5906,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.9093e-01,  6.5630e-01,  3.5763e-07,  5.3162e-01]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6563, -0.0000,  0.7291,  0.5902]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558,  0.8101,  0.8101, -0.0005]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0005,  0.8094,  0.8999,  0.7288]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531509578228) A[1]:(0.590799093246) A[2]:(0.59061139822) A[3]:(0.531638562679)\n",
      " state (1)  A[0]:(0.531447410583) A[1]:(-0.000183075666428) A[2]:(0.656258940697) A[3]:(0.590703964233)\n",
      " state (2)  A[0]:(0.590879678726) A[1]:(0.729097902775) A[2]:(0.590109825134) A[3]:(0.656323254108)\n",
      " state (3)  A[0]:(0.655741453171) A[1]:(-0.00345845636912) A[2]:(0.524701595306) A[3]:(0.551245927811)\n",
      " state (4)  A[0]:(0.591039776802) A[1]:(0.656429708004) A[2]:(0.000153660774231) A[3]:(0.531677484512)\n",
      " state (5)  A[0]:(-0.0276808943599) A[1]:(0.99989759922) A[2]:(-0.821212589741) A[3]:(0.655046820641)\n",
      " state (6)  A[0]:(0.000317960977554) A[1]:(0.809994459152) A[2]:(0.000300049781799) A[3]:(0.656446397305)\n",
      " state (7)  A[0]:(0.547028899193) A[1]:(-0.537569761276) A[2]:(0.434692084789) A[3]:(0.871880471706)\n",
      " state (8)  A[0]:(0.656974613667) A[1]:(-0.000159233808517) A[2]:(0.729199409485) A[3]:(0.590907812119)\n",
      " state (9)  A[0]:(0.656710147858) A[1]:(0.809978961945) A[2]:(0.810185790062) A[3]:(0.000726133468561)\n",
      " state (10)  A[0]:(0.730032622814) A[1]:(0.899994492531) A[2]:(0.000638961733785) A[3]:(0.729431152344)\n",
      " state (11)  A[0]:(0.134744152427) A[1]:(0.882429480553) A[2]:(-0.926084160805) A[3]:(0.803086221218)\n",
      " state (12)  A[0]:(-0.427318066359) A[1]:(0.815492153168) A[2]:(-0.949902832508) A[3]:(0.715445458889)\n",
      " state (13)  A[0]:(0.00110813928768) A[1]:(0.809307813644) A[2]:(0.900107800961) A[3]:(0.729090809822)\n",
      " state (14)  A[0]:(0.810372710228) A[1]:(0.900099098682) A[2]:(0.999999761581) A[3]:(0.810118913651)\n",
      " state (15)  A[0]:(0.979714214802) A[1]:(0.939315378666) A[2]:(1.0) A[3]:(0.878180742264)\n",
      "Episode 585000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6053. Times reached goal: 986.               Steps done: 4171055. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0138923838196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531490325928) A[1]:(0.590543985367) A[2]:(0.590644598007) A[3]:(0.531680524349)\n",
      " state (1)  A[0]:(0.531152307987) A[1]:(-1.32322311401e-05) A[2]:(0.656206607819) A[3]:(0.590658843517)\n",
      " state (2)  A[0]:(0.590482831001) A[1]:(0.729014992714) A[2]:(0.590053856373) A[3]:(0.656199932098)\n",
      " state (3)  A[0]:(0.655396103859) A[1]:(-0.00380716868676) A[2]:(0.524673819542) A[3]:(0.551151931286)\n",
      " state (4)  A[0]:(0.590528130531) A[1]:(0.656140446663) A[2]:(0.000184059143066) A[3]:(0.531516909599)\n",
      " state (5)  A[0]:(-0.0290051680058) A[1]:(0.999897480011) A[2]:(-0.821154296398) A[3]:(0.654682397842)\n",
      " state (6)  A[0]:(-0.000769182865042) A[1]:(0.809851169586) A[2]:(0.000172853469849) A[3]:(0.656364321709)\n",
      " state (7)  A[0]:(0.546360552311) A[1]:(-0.537493228912) A[2]:(0.4343354702) A[3]:(0.871969163418)\n",
      " state (8)  A[0]:(0.656688332558) A[1]:(-3.49283218384e-05) A[2]:(0.729010343552) A[3]:(0.591947317123)\n",
      " state (9)  A[0]:(0.656660199165) A[1]:(0.810079097748) A[2]:(0.810240626335) A[3]:(0.00286488933489)\n",
      " state (10)  A[0]:(0.730248689651) A[1]:(0.900034606457) A[2]:(0.00144886865746) A[3]:(0.73039996624)\n",
      " state (11)  A[0]:(0.135339900851) A[1]:(0.882427215576) A[2]:(-0.925989508629) A[3]:(0.803673148155)\n",
      " state (12)  A[0]:(-0.427515506744) A[1]:(0.815389037132) A[2]:(-0.949931442738) A[3]:(0.715808987617)\n",
      " state (13)  A[0]:(0.000102087855339) A[1]:(0.809107124805) A[2]:(0.900104701519) A[3]:(0.729120671749)\n",
      " state (14)  A[0]:(0.809965133667) A[1]:(0.899951338768) A[2]:(0.999999761581) A[3]:(0.810050606728)\n",
      " state (15)  A[0]:(0.979670464993) A[1]:(0.939180791378) A[2]:(1.0) A[3]:(0.878129124641)\n",
      "Episode 586000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6027. Times reached goal: 984.               Steps done: 4177082. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0138089062347.\n",
      " state (0)  A[0]:(0.531256556511) A[1]:(0.590914726257) A[2]:(0.5905854702) A[3]:(0.531031966209)\n",
      " state (1)  A[0]:(0.530505418777) A[1]:(-0.00189377146307) A[2]:(0.655960917473) A[3]:(0.589957475662)\n",
      " state (2)  A[0]:(0.589652001858) A[1]:(0.729157865047) A[2]:(0.589848816395) A[3]:(0.655765414238)\n",
      " state (3)  A[0]:(0.654688954353) A[1]:(-0.00376224238425) A[2]:(0.524536371231) A[3]:(0.55072414875)\n",
      " state (4)  A[0]:(0.589489340782) A[1]:(0.656390607357) A[2]:(-0.00013542175293) A[3]:(0.531050920486)\n",
      " state (5)  A[0]:(-0.0314081534743) A[1]:(0.999897658825) A[2]:(-0.821361303329) A[3]:(0.654033303261)\n",
      " state (6)  A[0]:(-0.0030102788005) A[1]:(0.810140609741) A[2]:(6.05583190918e-05) A[3]:(0.655985236168)\n",
      " state (7)  A[0]:(0.544391036034) A[1]:(-0.537535369396) A[2]:(0.434924304485) A[3]:(0.871699988842)\n",
      " state (8)  A[0]:(0.654299855232) A[1]:(0.000258192420006) A[2]:(0.729478240013) A[3]:(0.589536547661)\n",
      " state (9)  A[0]:(0.654145956039) A[1]:(0.809894502163) A[2]:(0.810086071491) A[3]:(-0.000185579061508)\n",
      " state (10)  A[0]:(0.727611601353) A[1]:(0.899811625481) A[2]:(-0.000673651578836) A[3]:(0.729013085365)\n",
      " state (11)  A[0]:(0.129766702652) A[1]:(0.882113218307) A[2]:(-0.92641222477) A[3]:(0.802700042725)\n",
      " state (12)  A[0]:(-0.430262118578) A[1]:(0.81488442421) A[2]:(-0.950124979019) A[3]:(0.715173482895)\n",
      " state (13)  A[0]:(-0.000922113424167) A[1]:(0.808612704277) A[2]:(0.900177955627) A[3]:(0.729089319706)\n",
      " state (14)  A[0]:(0.810028910637) A[1]:(0.899737536907) A[2]:(0.999999761581) A[3]:(0.810140609741)\n",
      " state (15)  A[0]:(0.979689478874) A[1]:(0.939072608948) A[2]:(1.0) A[3]:(0.87817132473)\n",
      "Episode 587000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6027. Times reached goal: 982.               Steps done: 4183109. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0137259302561.\n",
      " state (0)  A[0]:(0.53132802248) A[1]:(0.590334773064) A[2]:(0.590342760086) A[3]:(0.531259894371)\n",
      " state (1)  A[0]:(0.531352043152) A[1]:(-5.12003898621e-05) A[2]:(0.656087636948) A[3]:(0.590376853943)\n",
      " state (2)  A[0]:(0.590495228767) A[1]:(0.728969573975) A[2]:(0.59007537365) A[3]:(0.656033158302)\n",
      " state (3)  A[0]:(0.655565798283) A[1]:(-0.0044259140268) A[2]:(0.524698793888) A[3]:(0.551051974297)\n",
      " state (4)  A[0]:(0.590574920177) A[1]:(0.656093835831) A[2]:(-0.000104069709778) A[3]:(0.531370282173)\n",
      " state (5)  A[0]:(-0.0296218730509) A[1]:(0.999897539616) A[2]:(-0.821364045143) A[3]:(0.654182076454)\n",
      " state (6)  A[0]:(-0.000460147828562) A[1]:(0.809976756573) A[2]:(-0.000170350074768) A[3]:(0.656161546707)\n",
      " state (7)  A[0]:(0.546188235283) A[1]:(-0.537679076195) A[2]:(0.434332668781) A[3]:(0.87174642086)\n",
      " state (8)  A[0]:(0.655863046646) A[1]:(-0.00030130147934) A[2]:(0.728951931) A[3]:(0.590215802193)\n",
      " state (9)  A[0]:(0.655433058739) A[1]:(0.809915006161) A[2]:(0.809943675995) A[3]:(-0.000156104564667)\n",
      " state (10)  A[0]:(0.728810966015) A[1]:(0.899983227253) A[2]:(-0.000187158584595) A[3]:(0.729029417038)\n",
      " state (11)  A[0]:(0.132387012243) A[1]:(0.882543981075) A[2]:(-0.92635089159) A[3]:(0.802909970284)\n",
      " state (12)  A[0]:(-0.429244786501) A[1]:(0.815826892853) A[2]:(-0.950209796429) A[3]:(0.715301632881)\n",
      " state (13)  A[0]:(-0.00125798513182) A[1]:(0.809762597084) A[2]:(0.899936556816) A[3]:(0.728980183601)\n",
      " state (14)  A[0]:(0.809627592564) A[1]:(0.900418996811) A[2]:(0.999999761581) A[3]:(0.809988737106)\n",
      " state (15)  A[0]:(0.979642152786) A[1]:(0.939480006695) A[2]:(1.0) A[3]:(0.87806892395)\n",
      "Episode 588000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6032. Times reached goal: 983.               Steps done: 4189141. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0136433846527.\n",
      " state (0)  A[0]:(0.533539772034) A[1]:(0.590252101421) A[2]:(0.590722560883) A[3]:(0.530523538589)\n",
      " state (1)  A[0]:(0.533133149147) A[1]:(0.00337884039618) A[2]:(0.655881881714) A[3]:(0.59100574255)\n",
      " state (2)  A[0]:(0.592054605484) A[1]:(0.728914618492) A[2]:(0.590140461922) A[3]:(0.65670478344)\n",
      " state (3)  A[0]:(0.65695130825) A[1]:(-0.00503537291661) A[2]:(0.524968266487) A[3]:(0.552230000496)\n",
      " state (4)  A[0]:(0.592025220394) A[1]:(0.65537238121) A[2]:(0.00061905378243) A[3]:(0.533101081848)\n",
      " state (5)  A[0]:(-0.0282245986164) A[1]:(0.999897241592) A[2]:(-0.821039557457) A[3]:(0.656354665756)\n",
      " state (6)  A[0]:(0.00142049696296) A[1]:(0.809602797031) A[2]:(0.000935077376198) A[3]:(0.659444332123)\n",
      " state (7)  A[0]:(0.548570394516) A[1]:(-0.538466453552) A[2]:(0.435165733099) A[3]:(0.873967528343)\n",
      " state (8)  A[0]:(0.659734249115) A[1]:(-0.00285745388828) A[2]:(0.729063153267) A[3]:(0.601743519306)\n",
      " state (9)  A[0]:(0.658490538597) A[1]:(0.809448003769) A[2]:(0.810614466667) A[3]:(0.0159613341093)\n",
      " state (10)  A[0]:(0.730790615082) A[1]:(0.899619460106) A[2]:(0.0020265551284) A[3]:(0.735134005547)\n",
      " state (11)  A[0]:(0.136555865407) A[1]:(0.881682097912) A[2]:(-0.926143884659) A[3]:(0.806755304337)\n",
      " state (12)  A[0]:(-0.424840569496) A[1]:(0.813826680183) A[2]:(-0.950151205063) A[3]:(0.719995081425)\n",
      " state (13)  A[0]:(0.0050382334739) A[1]:(0.807138204575) A[2]:(0.899894058704) A[3]:(0.733219742775)\n",
      " state (14)  A[0]:(0.811724126339) A[1]:(0.898849904537) A[2]:(0.999999761581) A[3]:(0.812906503677)\n",
      " state (15)  A[0]:(0.979844331741) A[1]:(0.938533604145) A[2]:(1.0) A[3]:(0.879883527756)\n",
      "Episode 589000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6036. Times reached goal: 991.               Steps done: 4195177. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0135612812203.\n",
      "q_values \n",
      "tensor([[ 0.5320,  0.5905,  0.5906,  0.5310]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5322,  0.0002,  0.6561,  0.5897]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5916,  0.7291,  0.5902,  0.6553]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0024,  0.8101,  0.0005,  0.6554]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7324,  0.9001,  0.0011,  0.7306]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8116,  0.8999,  1.0000,  0.8105]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531409263611) A[1]:(0.590520620346) A[2]:(0.590579271317) A[3]:(0.531627774239)\n",
      " state (1)  A[0]:(0.53169465065) A[1]:(-0.000298961997032) A[2]:(0.656315803528) A[3]:(0.590637683868)\n",
      " state (2)  A[0]:(0.591106653214) A[1]:(0.729008316994) A[2]:(0.590517878532) A[3]:(0.656275510788)\n",
      " state (3)  A[0]:(0.656279265881) A[1]:(-0.00463600130752) A[2]:(0.525302886963) A[3]:(0.551418960094)\n",
      " state (4)  A[0]:(0.591398239136) A[1]:(0.656441628933) A[2]:(0.000593304575887) A[3]:(0.531720757484)\n",
      " state (5)  A[0]:(-0.0282789152116) A[1]:(0.99989759922) A[2]:(-0.821111023426) A[3]:(0.654254376888)\n",
      " state (6)  A[0]:(0.00198092800565) A[1]:(0.810081660748) A[2]:(0.000814437691588) A[3]:(0.656661748886)\n",
      " state (7)  A[0]:(0.548530340195) A[1]:(-0.53730982542) A[2]:(0.435112953186) A[3]:(0.872253656387)\n",
      " state (8)  A[0]:(0.659039855003) A[1]:(-0.000434860558016) A[2]:(0.729166686535) A[3]:(0.593553066254)\n",
      " state (9)  A[0]:(0.658920049667) A[1]:(0.810075163841) A[2]:(0.810421645641) A[3]:(0.00346936378628)\n",
      " state (10)  A[0]:(0.731992125511) A[1]:(0.900027155876) A[2]:(0.00193810218479) A[3]:(0.730438828468)\n",
      " state (11)  A[0]:(0.13963201642) A[1]:(0.882410049438) A[2]:(-0.926138937473) A[3]:(0.803925693035)\n",
      " state (12)  A[0]:(-0.423519223928) A[1]:(0.815291404724) A[2]:(-0.950208246708) A[3]:(0.716396570206)\n",
      " state (13)  A[0]:(0.00531578995287) A[1]:(0.808923959732) A[2]:(0.900102853775) A[3]:(0.729796350002)\n",
      " state (14)  A[0]:(0.811825096607) A[1]:(0.899871706963) A[2]:(0.999999761581) A[3]:(0.810495972633)\n",
      " state (15)  A[0]:(0.97989076376) A[1]:(0.939079344273) A[2]:(1.0) A[3]:(0.878387510777)\n",
      "Episode 590000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6031. Times reached goal: 987.               Steps done: 4201208. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0134797392702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531552255154) A[1]:(0.590553760529) A[2]:(0.590518832207) A[3]:(0.531293749809)\n",
      " state (1)  A[0]:(0.53152269125) A[1]:(0.000114813446999) A[2]:(0.656185865402) A[3]:(0.590372860432)\n",
      " state (2)  A[0]:(0.590789198875) A[1]:(0.729070186615) A[2]:(0.590450704098) A[3]:(0.6559715271)\n",
      " state (3)  A[0]:(0.655961275101) A[1]:(-0.00476295640692) A[2]:(0.525110602379) A[3]:(0.551098108292)\n",
      " state (4)  A[0]:(0.590768694878) A[1]:(0.65631377697) A[2]:(0.000113606452942) A[3]:(0.531437516212)\n",
      " state (5)  A[0]:(-0.0303278211504) A[1]:(0.99989759922) A[2]:(-0.821357905865) A[3]:(0.653965353966)\n",
      " state (6)  A[0]:(-1.81645154953e-05) A[1]:(0.810088515282) A[2]:(0.000186324119568) A[3]:(0.656200170517)\n",
      " state (7)  A[0]:(0.54654443264) A[1]:(-0.537257790565) A[2]:(0.434762299061) A[3]:(0.871775746346)\n",
      " state (8)  A[0]:(0.65627515316) A[1]:(0.000208511948586) A[2]:(0.729138851166) A[3]:(0.590505957603)\n",
      " state (9)  A[0]:(0.655931830406) A[1]:(0.810094952583) A[2]:(0.810082554817) A[3]:(9.49203968048e-05)\n",
      " state (10)  A[0]:(0.729171514511) A[1]:(0.90005582571) A[2]:(0.000130891799927) A[3]:(0.729029476643)\n",
      " state (11)  A[0]:(0.133315682411) A[1]:(0.882598638535) A[2]:(-0.926466226578) A[3]:(0.802930533886)\n",
      " state (12)  A[0]:(-0.428454577923) A[1]:(0.815809249878) A[2]:(-0.95039743185) A[3]:(0.715342998505)\n",
      " state (13)  A[0]:(-0.000237658619881) A[1]:(0.809632599354) A[2]:(0.900110244751) A[3]:(0.729058861732)\n",
      " state (14)  A[0]:(0.810014665127) A[1]:(0.900313556194) A[2]:(0.999999761581) A[3]:(0.810056328773)\n",
      " state (15)  A[0]:(0.979690730572) A[1]:(0.939331531525) A[2]:(1.0) A[3]:(0.87813180685)\n",
      "Episode 591000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6031. Times reached goal: 997.               Steps done: 4207239. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0133986876196.\n",
      " state (0)  A[0]:(0.531358122826) A[1]:(0.590435385704) A[2]:(0.590401053429) A[3]:(0.531357347965)\n",
      " state (1)  A[0]:(0.531257748604) A[1]:(0.000143468379974) A[2]:(0.655972123146) A[3]:(0.590396046638)\n",
      " state (2)  A[0]:(0.5905636549) A[1]:(0.728966832161) A[2]:(0.590248227119) A[3]:(0.655989170074)\n",
      " state (3)  A[0]:(0.655757069588) A[1]:(-0.00517321983352) A[2]:(0.524923443794) A[3]:(0.551002025604)\n",
      " state (4)  A[0]:(0.590426564217) A[1]:(0.656066060066) A[2]:(-3.03983688354e-05) A[3]:(0.531200170517)\n",
      " state (5)  A[0]:(-0.0310527645051) A[1]:(0.999897480011) A[2]:(-0.8213134408) A[3]:(0.653761088848)\n",
      " state (6)  A[0]:(-0.000292286276817) A[1]:(0.810053825378) A[2]:(0.000131845474243) A[3]:(0.655996441841)\n",
      " state (7)  A[0]:(0.546211957932) A[1]:(-0.537158429623) A[2]:(0.434514671564) A[3]:(0.871609926224)\n",
      " state (8)  A[0]:(0.656029820442) A[1]:(-0.00016862154007) A[2]:(0.728980660439) A[3]:(0.590279579163)\n",
      " state (9)  A[0]:(0.655703663826) A[1]:(0.809908509254) A[2]:(0.810048818588) A[3]:(-0.000328063964844)\n",
      " state (10)  A[0]:(0.729046881199) A[1]:(0.900025248528) A[2]:(-0.000105261802673) A[3]:(0.728813529015)\n",
      " state (11)  A[0]:(0.133143424988) A[1]:(0.882649421692) A[2]:(-0.926603317261) A[3]:(0.802774608135)\n",
      " state (12)  A[0]:(-0.428937196732) A[1]:(0.815922319889) A[2]:(-0.950557947159) A[3]:(0.715021073818)\n",
      " state (13)  A[0]:(-0.00115297688171) A[1]:(0.809665203094) A[2]:(0.90002655983) A[3]:(0.728733181953)\n",
      " state (14)  A[0]:(0.809745430946) A[1]:(0.90020352602) A[2]:(0.999999761581) A[3]:(0.809906840324)\n",
      " state (15)  A[0]:(0.979682505131) A[1]:(0.93912178278) A[2]:(1.0) A[3]:(0.878127634525)\n",
      "Episode 592000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6035. Times reached goal: 986.               Steps done: 4213274. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.013318070048.\n",
      " state (0)  A[0]:(0.531524002552) A[1]:(0.590613722801) A[2]:(0.59076333046) A[3]:(0.531407356262)\n",
      " state (1)  A[0]:(0.531569242477) A[1]:(-3.94582748413e-05) A[2]:(0.656227111816) A[3]:(0.590596139431)\n",
      " state (2)  A[0]:(0.590809822083) A[1]:(0.728996098042) A[2]:(0.590394854546) A[3]:(0.656201720238)\n",
      " state (3)  A[0]:(0.656063973904) A[1]:(-0.00520355766639) A[2]:(0.525041460991) A[3]:(0.551232337952)\n",
      " state (4)  A[0]:(0.590696811676) A[1]:(0.656118810177) A[2]:(-5.66244125366e-05) A[3]:(0.531369924545)\n",
      " state (5)  A[0]:(-0.0313833095133) A[1]:(0.999897480011) A[2]:(-0.821438074112) A[3]:(0.653765916824)\n",
      " state (6)  A[0]:(-0.000477388472063) A[1]:(0.810019016266) A[2]:(-0.000120997428894) A[3]:(0.656271517277)\n",
      " state (7)  A[0]:(0.546219110489) A[1]:(-0.537238717079) A[2]:(0.434434503317) A[3]:(0.871843874454)\n",
      " state (8)  A[0]:(0.656256198883) A[1]:(-0.000254273414612) A[2]:(0.728877186775) A[3]:(0.591248929501)\n",
      " state (9)  A[0]:(0.655910670757) A[1]:(0.809963524342) A[2]:(0.810069918633) A[3]:(0.000540226639714)\n",
      " state (10)  A[0]:(0.729343950748) A[1]:(0.900007009506) A[2]:(0.000458598107798) A[3]:(0.729234755039)\n",
      " state (11)  A[0]:(0.134011119604) A[1]:(0.882557332516) A[2]:(-0.926538586617) A[3]:(0.803160250187)\n",
      " state (12)  A[0]:(-0.428404808044) A[1]:(0.815704703331) A[2]:(-0.950583398342) A[3]:(0.715474545956)\n",
      " state (13)  A[0]:(-0.000762522045989) A[1]:(0.809429228306) A[2]:(0.900010764599) A[3]:(0.729033350945)\n",
      " state (14)  A[0]:(0.809842705727) A[1]:(0.900127649307) A[2]:(0.999999761581) A[3]:(0.810012280941)\n",
      " state (15)  A[0]:(0.979690015316) A[1]:(0.939103186131) A[2]:(1.0) A[3]:(0.87814027071)\n",
      "Episode 593000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6055. Times reached goal: 993.               Steps done: 4219329. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0132376727822.\n",
      " state (0)  A[0]:(0.531908392906) A[1]:(0.590450525284) A[2]:(0.590401291847) A[3]:(0.532310962677)\n",
      " state (1)  A[0]:(0.531986236572) A[1]:(-1.80304050446e-06) A[2]:(0.656040966511) A[3]:(0.591099023819)\n",
      " state (2)  A[0]:(0.590954303741) A[1]:(0.728910386562) A[2]:(0.590238809586) A[3]:(0.656671404839)\n",
      " state (3)  A[0]:(0.656225919724) A[1]:(-0.00545509718359) A[2]:(0.524970114231) A[3]:(0.551979422569)\n",
      " state (4)  A[0]:(0.590848922729) A[1]:(0.655914783478) A[2]:(-7.20024108887e-05) A[3]:(0.532241821289)\n",
      " state (5)  A[0]:(-0.0313995480537) A[1]:(0.999897420406) A[2]:(-0.821471333504) A[3]:(0.654314517975)\n",
      " state (6)  A[0]:(-0.000142976641655) A[1]:(0.809969425201) A[2]:(-0.000104904174805) A[3]:(0.656731724739)\n",
      " state (7)  A[0]:(0.54640597105) A[1]:(-0.537402153015) A[2]:(0.43456158042) A[3]:(0.871973276138)\n",
      " state (8)  A[0]:(0.656164646149) A[1]:(-0.000305786728859) A[2]:(0.728980898857) A[3]:(0.591174423695)\n",
      " state (9)  A[0]:(0.655890464783) A[1]:(0.809887111187) A[2]:(0.810017466545) A[3]:(0.00088083720766)\n",
      " state (10)  A[0]:(0.72936964035) A[1]:(0.899942576885) A[2]:(-6.12735748291e-05) A[3]:(0.729398608208)\n",
      " state (11)  A[0]:(0.134522587061) A[1]:(0.882498562336) A[2]:(-0.926673293114) A[3]:(0.803264558315)\n",
      " state (12)  A[0]:(-0.427308142185) A[1]:(0.815644264221) A[2]:(-0.950693190098) A[3]:(0.715709507465)\n",
      " state (13)  A[0]:(0.00116342247929) A[1]:(0.809397399426) A[2]:(0.899937152863) A[3]:(0.729351997375)\n",
      " state (14)  A[0]:(0.810558259487) A[1]:(0.900152206421) A[2]:(0.999999761581) A[3]:(0.810252606869)\n",
      " state (15)  A[0]:(0.979771614075) A[1]:(0.939137399197) A[2]:(1.0) A[3]:(0.878283798695)\n",
      "Episode 594000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6030. Times reached goal: 985.               Steps done: 4225359. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0131580897992.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5907,  0.5906,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.6561,  0.0001,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560, -0.0001,  0.7291,  0.5906]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558,  0.8101,  0.8100,  0.0001]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  0.8093,  0.9000,  0.7291]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8105,  0.9001,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531635522842) A[1]:(0.590683221817) A[2]:(0.590590238571) A[3]:(0.531782627106)\n",
      " state (1)  A[0]:(0.531731367111) A[1]:(-2.59280204773e-05) A[2]:(0.656108736992) A[3]:(0.590806126595)\n",
      " state (2)  A[0]:(0.590770840645) A[1]:(0.728994488716) A[2]:(0.590369582176) A[3]:(0.656406223774)\n",
      " state (3)  A[0]:(0.656189739704) A[1]:(-0.005368121434) A[2]:(0.525090932846) A[3]:(0.551631629467)\n",
      " state (4)  A[0]:(0.590892136097) A[1]:(0.656154870987) A[2]:(-6.02006912231e-05) A[3]:(0.531804442406)\n",
      " state (5)  A[0]:(-0.0312037020922) A[1]:(0.999897480011) A[2]:(-0.821496427059) A[3]:(0.653792381287)\n",
      " state (6)  A[0]:(0.000312879681587) A[1]:(0.810061633587) A[2]:(1.34706497192e-05) A[3]:(0.656441450119)\n",
      " state (7)  A[0]:(0.546690821648) A[1]:(-0.537253141403) A[2]:(0.434775918722) A[3]:(0.871932089329)\n",
      " state (8)  A[0]:(0.65641283989) A[1]:(0.000215590000153) A[2]:(0.729009628296) A[3]:(0.591051697731)\n",
      " state (9)  A[0]:(0.656054496765) A[1]:(0.810137987137) A[2]:(0.8099219203) A[3]:(0.000652223709039)\n",
      " state (10)  A[0]:(0.729136228561) A[1]:(0.90003824234) A[2]:(-0.000542402209248) A[3]:(0.729293584824)\n",
      " state (11)  A[0]:(0.133339598775) A[1]:(0.882550656796) A[2]:(-0.926809191704) A[3]:(0.803147852421)\n",
      " state (12)  A[0]:(-0.428545087576) A[1]:(0.815655171871) A[2]:(-0.950815200806) A[3]:(0.715505838394)\n",
      " state (13)  A[0]:(-0.000243201851845) A[1]:(0.80938231945) A[2]:(0.899911701679) A[3]:(0.729111433029)\n",
      " state (14)  A[0]:(0.810200333595) A[1]:(0.900154054165) A[2]:(0.999999761581) A[3]:(0.810011923313)\n",
      " state (15)  A[0]:(0.979744076729) A[1]:(0.93912166357) A[2]:(1.0) A[3]:(0.878090262413)\n",
      "Episode 595000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6023. Times reached goal: 983.               Steps done: 4231382. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0130790768108.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53153014183) A[1]:(0.590606808662) A[2]:(0.590632677078) A[3]:(0.531140327454)\n",
      " state (1)  A[0]:(0.531640291214) A[1]:(-9.93311405182e-05) A[2]:(0.656124353409) A[3]:(0.590110957623)\n",
      " state (2)  A[0]:(0.590790271759) A[1]:(0.729053974152) A[2]:(0.590415418148) A[3]:(0.655724465847)\n",
      " state (3)  A[0]:(0.656229615211) A[1]:(-0.00540933711454) A[2]:(0.525144934654) A[3]:(0.550794780254)\n",
      " state (4)  A[0]:(0.590797424316) A[1]:(0.656319856644) A[2]:(-8.92877578735e-05) A[3]:(0.53099155426)\n",
      " state (5)  A[0]:(-0.0317130833864) A[1]:(0.999897539616) A[2]:(-0.821512162685) A[3]:(0.653281390667)\n",
      " state (6)  A[0]:(0.000291302800179) A[1]:(0.810099601746) A[2]:(0.000172734260559) A[3]:(0.656032204628)\n",
      " state (7)  A[0]:(0.546631574631) A[1]:(-0.537173092365) A[2]:(0.435040593147) A[3]:(0.871735692024)\n",
      " state (8)  A[0]:(0.656537055969) A[1]:(-4.33027744293e-05) A[2]:(0.729194581509) A[3]:(0.590725898743)\n",
      " state (9)  A[0]:(0.656468093395) A[1]:(0.809971451759) A[2]:(0.810123205185) A[3]:(0.000511676014867)\n",
      " state (10)  A[0]:(0.729630470276) A[1]:(0.900007009506) A[2]:(0.000200867652893) A[3]:(0.729370474815)\n",
      " state (11)  A[0]:(0.134579271078) A[1]:(0.882605433464) A[2]:(-0.926760315895) A[3]:(0.803277850151)\n",
      " state (12)  A[0]:(-0.427851289511) A[1]:(0.815810680389) A[2]:(-0.950832605362) A[3]:(0.715608596802)\n",
      " state (13)  A[0]:(1.87605619431e-05) A[1]:(0.80953669548) A[2]:(0.900117099285) A[3]:(0.729159235954)\n",
      " state (14)  A[0]:(0.810142695904) A[1]:(0.900181293488) A[2]:(0.999999761581) A[3]:(0.810071468353)\n",
      " state (15)  A[0]:(0.979736983776) A[1]:(0.939048230648) A[2]:(1.0) A[3]:(0.878178477287)\n",
      "Episode 596000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6051. Times reached goal: 989.               Steps done: 4237433. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0130001742774.\n",
      " state (0)  A[0]:(0.531338691711) A[1]:(0.589919984341) A[2]:(0.590379953384) A[3]:(0.531101882458)\n",
      " state (1)  A[0]:(0.531388819218) A[1]:(-7.46995210648e-05) A[2]:(0.655954241753) A[3]:(0.590211033821)\n",
      " state (2)  A[0]:(0.59025478363) A[1]:(0.728861033916) A[2]:(0.590303182602) A[3]:(0.655880331993)\n",
      " state (3)  A[0]:(0.655668139458) A[1]:(-0.00576616404578) A[2]:(0.52498281002) A[3]:(0.551066100597)\n",
      " state (4)  A[0]:(0.59019780159) A[1]:(0.655766487122) A[2]:(-0.000274300575256) A[3]:(0.531256437302)\n",
      " state (5)  A[0]:(-0.0322810411453) A[1]:(0.999897360802) A[2]:(-0.821641385555) A[3]:(0.653254032135)\n",
      " state (6)  A[0]:(8.25822353363e-05) A[1]:(0.80991089344) A[2]:(-0.000624418200459) A[3]:(0.655744314194)\n",
      " state (7)  A[0]:(0.546305298805) A[1]:(-0.536955237389) A[2]:(0.433895558119) A[3]:(0.871530592442)\n",
      " state (8)  A[0]:(0.655865907669) A[1]:(-5.06192445755e-05) A[2]:(0.728550195694) A[3]:(0.590205967426)\n",
      " state (9)  A[0]:(0.655275940895) A[1]:(0.80986392498) A[2]:(0.809713304043) A[3]:(-0.000379174918635)\n",
      " state (10)  A[0]:(0.728293478489) A[1]:(0.899963915348) A[2]:(-0.00126743246801) A[3]:(0.728774964809)\n",
      " state (11)  A[0]:(0.131599277258) A[1]:(0.882634997368) A[2]:(-0.927050769329) A[3]:(0.802836537361)\n",
      " state (12)  A[0]:(-0.430037438869) A[1]:(0.815955281258) A[2]:(-0.951066374779) A[3]:(0.715161800385)\n",
      " state (13)  A[0]:(-0.00208681519143) A[1]:(0.809718191624) A[2]:(0.89979749918) A[3]:(0.728894233704)\n",
      " state (14)  A[0]:(0.809640169144) A[1]:(0.900280594826) A[2]:(0.999999761581) A[3]:(0.809941351414)\n",
      " state (15)  A[0]:(0.979704618454) A[1]:(0.939083993435) A[2]:(1.0) A[3]:(0.878116488457)\n",
      "Episode 597000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6060. Times reached goal: 991.               Steps done: 4243493. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0129216314464.\n",
      " state (0)  A[0]:(0.532314896584) A[1]:(0.590578198433) A[2]:(0.590848565102) A[3]:(0.53189522028)\n",
      " state (1)  A[0]:(0.532351970673) A[1]:(-0.000258132815361) A[2]:(0.656173229218) A[3]:(0.59055519104)\n",
      " state (2)  A[0]:(0.591378629208) A[1]:(0.728951334953) A[2]:(0.590367496014) A[3]:(0.656026363373)\n",
      " state (3)  A[0]:(0.656771600246) A[1]:(-0.00567422667518) A[2]:(0.525099158287) A[3]:(0.551100254059)\n",
      " state (4)  A[0]:(0.59132540226) A[1]:(0.656056880951) A[2]:(-0.00011134147644) A[3]:(0.531172215939)\n",
      " state (5)  A[0]:(-0.0314432419837) A[1]:(0.999897420406) A[2]:(-0.821553945541) A[3]:(0.653181910515)\n",
      " state (6)  A[0]:(0.00099959934596) A[1]:(0.810008227825) A[2]:(-3.19480895996e-05) A[3]:(0.655799746513)\n",
      " state (7)  A[0]:(0.547123551369) A[1]:(-0.536910295486) A[2]:(0.434807449579) A[3]:(0.871582448483)\n",
      " state (8)  A[0]:(0.656999230385) A[1]:(0.000445917219622) A[2]:(0.729102730751) A[3]:(0.590162336826)\n",
      " state (9)  A[0]:(0.657170772552) A[1]:(0.810053050518) A[2]:(0.810065746307) A[3]:(-0.000207632780075)\n",
      " state (10)  A[0]:(0.730242192745) A[1]:(0.899939358234) A[2]:(6.96182250977e-05) A[3]:(0.728964745998)\n",
      " state (11)  A[0]:(0.136060208082) A[1]:(0.882383823395) A[2]:(-0.926889002323) A[3]:(0.802904963493)\n",
      " state (12)  A[0]:(-0.426423430443) A[1]:(0.815238773823) A[2]:(-0.951013147831) A[3]:(0.715061426163)\n",
      " state (13)  A[0]:(0.00208112294786) A[1]:(0.808733642101) A[2]:(0.900016009808) A[3]:(0.728736579418)\n",
      " state (14)  A[0]:(0.81092774868) A[1]:(0.899680435658) A[2]:(0.999999761581) A[3]:(0.809868633747)\n",
      " state (15)  A[0]:(0.979830801487) A[1]:(0.938686072826) A[2]:(1.0) A[3]:(0.878107011318)\n",
      "Episode 598000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6032. Times reached goal: 985.               Steps done: 4249525. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0128439227705.\n",
      " state (0)  A[0]:(0.531430006027) A[1]:(0.590511322021) A[2]:(0.590789914131) A[3]:(0.531307458878)\n",
      " state (1)  A[0]:(0.531490981579) A[1]:(-8.60840082169e-05) A[2]:(0.656176865101) A[3]:(0.590504646301)\n",
      " state (2)  A[0]:(0.590660810471) A[1]:(0.729011535645) A[2]:(0.590561330318) A[3]:(0.655972242355)\n",
      " state (3)  A[0]:(0.656204819679) A[1]:(-0.00576252816245) A[2]:(0.525304019451) A[3]:(0.550998210907)\n",
      " state (4)  A[0]:(0.59058380127) A[1]:(0.656126499176) A[2]:(-4.43458557129e-05) A[3]:(0.531123638153)\n",
      " state (5)  A[0]:(-0.0329988710582) A[1]:(0.999897420406) A[2]:(-0.821616113186) A[3]:(0.653265058994)\n",
      " state (6)  A[0]:(-0.000230953097343) A[1]:(0.809937357903) A[2]:(-9.72747802734e-05) A[3]:(0.655804991722)\n",
      " state (7)  A[0]:(0.546070635319) A[1]:(-0.537355184555) A[2]:(0.434748858213) A[3]:(0.871484875679)\n",
      " state (8)  A[0]:(0.655824363232) A[1]:(-0.000314727425575) A[2]:(0.728898048401) A[3]:(0.589770078659)\n",
      " state (9)  A[0]:(0.655549347401) A[1]:(0.80990421772) A[2]:(0.809864640236) A[3]:(-0.00138404872268)\n",
      " state (10)  A[0]:(0.728780031204) A[1]:(0.899981439114) A[2]:(-0.00048720833729) A[3]:(0.72837716341)\n",
      " state (11)  A[0]:(0.13312754035) A[1]:(0.882659673691) A[2]:(-0.926994144917) A[3]:(0.802652001381)\n",
      " state (12)  A[0]:(-0.428867429495) A[1]:(0.815984845161) A[2]:(-0.951115965843) A[3]:(0.714917242527)\n",
      " state (13)  A[0]:(-0.000914260512218) A[1]:(0.809746742249) A[2]:(0.899908363819) A[3]:(0.728693068027)\n",
      " state (14)  A[0]:(0.809991776943) A[1]:(0.900325298309) A[2]:(0.999999821186) A[3]:(0.809869289398)\n",
      " state (15)  A[0]:(0.979743361473) A[1]:(0.939100325108) A[2]:(1.0) A[3]:(0.878117144108)\n",
      "Episode 599000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6020. Times reached goal: 984.               Steps done: 4255545. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0127668346236.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5904,  0.5903,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.6556,  0.0003,  0.5313]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6561, -0.0003,  0.7290,  0.5902]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558,  0.8099,  0.8098, -0.0009]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0027,  0.8098,  0.8997,  0.7292]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8093,  0.9003,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531516194344) A[1]:(0.590379834175) A[2]:(0.590410113335) A[3]:(0.531411290169)\n",
      " state (1)  A[0]:(0.531306266785) A[1]:(2.77757644653e-05) A[2]:(0.656029343605) A[3]:(0.590279579163)\n",
      " state (2)  A[0]:(0.590252041817) A[1]:(0.728896737099) A[2]:(0.590644001961) A[3]:(0.655986428261)\n",
      " state (3)  A[0]:(0.655879974365) A[1]:(-0.00616459175944) A[2]:(0.525408506393) A[3]:(0.551067709923)\n",
      " state (4)  A[0]:(0.590230703354) A[1]:(0.655956029892) A[2]:(1.63316726685e-05) A[3]:(0.531355977058)\n",
      " state (5)  A[0]:(-0.0333760641515) A[1]:(0.999897360802) A[2]:(-0.821636915207) A[3]:(0.653753757477)\n",
      " state (6)  A[0]:(-3.17543745041e-05) A[1]:(0.809877991676) A[2]:(-4.78029251099e-05) A[3]:(0.656339883804)\n",
      " state (7)  A[0]:(0.546233832836) A[1]:(-0.537173628807) A[2]:(0.43473136425) A[3]:(0.871692717075)\n",
      " state (8)  A[0]:(0.655991435051) A[1]:(-0.00019109249115) A[2]:(0.728766560555) A[3]:(0.590260744095)\n",
      " state (9)  A[0]:(0.655777871609) A[1]:(0.809885919094) A[2]:(0.80966514349) A[3]:(-0.000475376815302)\n",
      " state (10)  A[0]:(0.728788256645) A[1]:(0.900021672249) A[2]:(-0.00112354708835) A[3]:(0.728944182396)\n",
      " state (11)  A[0]:(0.13256713748) A[1]:(0.882778406143) A[2]:(-0.927111387253) A[3]:(0.803152441978)\n",
      " state (12)  A[0]:(-0.429879575968) A[1]:(0.816226422787) A[2]:(-0.951179385185) A[3]:(0.7155636549)\n",
      " state (13)  A[0]:(-0.00253742397763) A[1]:(0.810000360012) A[2]:(0.900138556957) A[3]:(0.729181408882)\n",
      " state (14)  A[0]:(0.809414982796) A[1]:(0.900406241417) A[2]:(0.999999821186) A[3]:(0.810093164444)\n",
      " state (15)  A[0]:(0.979683935642) A[1]:(0.939060509205) A[2]:(1.0) A[3]:(0.878214776516)\n",
      "Episode 600000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6044. Times reached goal: 990.               Steps done: 4261589. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0126899045918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531472444534) A[1]:(0.590531468391) A[2]:(0.590596139431) A[3]:(0.531746149063)\n",
      " state (1)  A[0]:(0.531457066536) A[1]:(-5.99026679993e-06) A[2]:(0.656129717827) A[3]:(0.590559124947)\n",
      " state (2)  A[0]:(0.59040915966) A[1]:(0.72893255949) A[2]:(0.590502738953) A[3]:(0.656113743782)\n",
      " state (3)  A[0]:(0.656099796295) A[1]:(-0.00580498017371) A[2]:(0.525292277336) A[3]:(0.551284313202)\n",
      " state (4)  A[0]:(0.590590953827) A[1]:(0.656127333641) A[2]:(-1.2993812561e-05) A[3]:(0.531523108482)\n",
      " state (5)  A[0]:(-0.0328749381006) A[1]:(0.999897420406) A[2]:(-0.821630239487) A[3]:(0.65356785059)\n",
      " state (6)  A[0]:(0.000414743990405) A[1]:(0.809988737106) A[2]:(-4.99486923218e-05) A[3]:(0.656038999557)\n",
      " state (7)  A[0]:(0.546818494797) A[1]:(-0.537103116512) A[2]:(0.43476742506) A[3]:(0.871652841568)\n",
      " state (8)  A[0]:(0.656856536865) A[1]:(3.74019145966e-05) A[2]:(0.728864967823) A[3]:(0.590811014175)\n",
      " state (9)  A[0]:(0.656732439995) A[1]:(0.81008708477) A[2]:(0.809886574745) A[3]:(8.64267349243e-06)\n",
      " state (10)  A[0]:(0.729756891727) A[1]:(0.900018334389) A[2]:(-0.00056838983437) A[3]:(0.728828191757)\n",
      " state (11)  A[0]:(0.135420441628) A[1]:(0.882597744465) A[2]:(-0.927098333836) A[3]:(0.802860379219)\n",
      " state (12)  A[0]:(-0.426586955786) A[1]:(0.815742075443) A[2]:(-0.951228678226) A[3]:(0.715118527412)\n",
      " state (13)  A[0]:(0.00207306141965) A[1]:(0.80940759182) A[2]:(0.89995855093) A[3]:(0.72886288166)\n",
      " state (14)  A[0]:(0.810892045498) A[1]:(0.900152742863) A[2]:(0.999999821186) A[3]:(0.809945046902)\n",
      " state (15)  A[0]:(0.979823708534) A[1]:(0.938995420933) A[2]:(1.0) A[3]:(0.878128826618)\n",
      "Episode 601000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6006. Times reached goal: 990.               Steps done: 4267595. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0126139174427.\n",
      " state (0)  A[0]:(0.53242123127) A[1]:(0.590835928917) A[2]:(0.590376019478) A[3]:(0.531936347485)\n",
      " state (1)  A[0]:(0.532535254955) A[1]:(1.42455101013e-05) A[2]:(0.656355857849) A[3]:(0.590639591217)\n",
      " state (2)  A[0]:(0.591398835182) A[1]:(0.729172468185) A[2]:(0.590665757656) A[3]:(0.656334519386)\n",
      " state (3)  A[0]:(0.65688419342) A[1]:(-0.00547886360437) A[2]:(0.52537381649) A[3]:(0.55154722929)\n",
      " state (4)  A[0]:(0.59147131443) A[1]:(0.655950188637) A[2]:(3.49283218384e-05) A[3]:(0.531797945499)\n",
      " state (5)  A[0]:(-0.0316334441304) A[1]:(0.999897301197) A[2]:(-0.821691334248) A[3]:(0.653858721256)\n",
      " state (6)  A[0]:(0.000951230234932) A[1]:(0.809854209423) A[2]:(-0.000180006027222) A[3]:(0.656539797783)\n",
      " state (7)  A[0]:(0.546858489513) A[1]:(-0.537130475044) A[2]:(0.434703022242) A[3]:(0.871904730797)\n",
      " state (8)  A[0]:(0.656506180763) A[1]:(0.00034636259079) A[2]:(0.72886633873) A[3]:(0.59100985527)\n",
      " state (9)  A[0]:(0.656297326088) A[1]:(0.810161530972) A[2]:(0.809877216816) A[3]:(0.000439226598246)\n",
      " state (10)  A[0]:(0.729499280453) A[1]:(0.899998366833) A[2]:(-0.000218987464905) A[3]:(0.729257881641)\n",
      " state (11)  A[0]:(0.134909853339) A[1]:(0.882491648197) A[2]:(-0.927050709724) A[3]:(0.803205132484)\n",
      " state (12)  A[0]:(-0.427416533232) A[1]:(0.815451920033) A[2]:(-0.951238274574) A[3]:(0.715392887592)\n",
      " state (13)  A[0]:(0.000607043446507) A[1]:(0.809003412724) A[2]:(0.900051355362) A[3]:(0.728939652443)\n",
      " state (14)  A[0]:(0.810324251652) A[1]:(0.899891614914) A[2]:(0.999999821186) A[3]:(0.809920251369)\n",
      " state (15)  A[0]:(0.979754149914) A[1]:(0.938796758652) A[2]:(1.0) A[3]:(0.878091096878)\n",
      "Episode 602000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6050. Times reached goal: 992.               Steps done: 4273645. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0125378336278.\n",
      " state (0)  A[0]:(0.53170466423) A[1]:(0.590601563454) A[2]:(0.590510189533) A[3]:(0.531509578228)\n",
      " state (1)  A[0]:(0.531472444534) A[1]:(-1.62422657013e-05) A[2]:(0.656190752983) A[3]:(0.590501010418)\n",
      " state (2)  A[0]:(0.59045279026) A[1]:(0.729013562202) A[2]:(0.590715527534) A[3]:(0.656127631664)\n",
      " state (3)  A[0]:(0.656022191048) A[1]:(-0.00578733766451) A[2]:(0.525515019894) A[3]:(0.551222681999)\n",
      " state (4)  A[0]:(0.590397119522) A[1]:(0.656223297119) A[2]:(0.000176191329956) A[3]:(0.531346321106)\n",
      " state (5)  A[0]:(-0.0329741314054) A[1]:(0.999897420406) A[2]:(-0.821600914001) A[3]:(0.653297066689)\n",
      " state (6)  A[0]:(0.000158295035362) A[1]:(0.809888780117) A[2]:(0.000205755233765) A[3]:(0.655783236027)\n",
      " state (7)  A[0]:(0.546222686768) A[1]:(-0.537228286266) A[2]:(0.435059726238) A[3]:(0.871497273445)\n",
      " state (8)  A[0]:(0.656097769737) A[1]:(-6.65485858917e-05) A[2]:(0.729052841663) A[3]:(0.590269088745)\n",
      " state (9)  A[0]:(0.655935525894) A[1]:(0.810069262981) A[2]:(0.81007951498) A[3]:(-0.000404506892664)\n",
      " state (10)  A[0]:(0.729101896286) A[1]:(0.900069534779) A[2]:(0.000460267037852) A[3]:(0.728909671307)\n",
      " state (11)  A[0]:(0.133511140943) A[1]:(0.882773578167) A[2]:(-0.927008509636) A[3]:(0.80299782753)\n",
      " state (12)  A[0]:(-0.429553627968) A[1]:(0.816155850887) A[2]:(-0.951234281063) A[3]:(0.715010643005)\n",
      " state (13)  A[0]:(-0.00288765830919) A[1]:(0.809923231602) A[2]:(0.900390684605) A[3]:(0.728484272957)\n",
      " state (14)  A[0]:(0.809098601341) A[1]:(0.900396764278) A[2]:(0.999999821186) A[3]:(0.809597313404)\n",
      " state (15)  A[0]:(0.979638338089) A[1]:(0.939013183117) A[2]:(1.0) A[3]:(0.877932667732)\n",
      "Episode 603000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6024. Times reached goal: 985.               Steps done: 4279669. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0124625327519.\n",
      " state (0)  A[0]:(0.531469345093) A[1]:(0.590459823608) A[2]:(0.590406060219) A[3]:(0.531330704689)\n",
      " state (1)  A[0]:(0.531516432762) A[1]:(-1.25914812088e-05) A[2]:(0.656129717827) A[3]:(0.590420246124)\n",
      " state (2)  A[0]:(0.590663313866) A[1]:(0.729064285755) A[2]:(0.590497970581) A[3]:(0.656055808067)\n",
      " state (3)  A[0]:(0.656186342239) A[1]:(-0.00557483918965) A[2]:(0.525308907032) A[3]:(0.551106512547)\n",
      " state (4)  A[0]:(0.590500473976) A[1]:(0.656228303909) A[2]:(7.18832015991e-05) A[3]:(0.531284213066)\n",
      " state (5)  A[0]:(-0.0331780388951) A[1]:(0.999897420406) A[2]:(-0.821607172489) A[3]:(0.653513550758)\n",
      " state (6)  A[0]:(7.39097595215e-06) A[1]:(0.810049057007) A[2]:(0.000205278396606) A[3]:(0.655868411064)\n",
      " state (7)  A[0]:(0.545897364616) A[1]:(-0.536820411682) A[2]:(0.435099333525) A[3]:(0.871391475201)\n",
      " state (8)  A[0]:(0.655742228031) A[1]:(0.000220581889153) A[2]:(0.729046702385) A[3]:(0.589762330055)\n",
      " state (9)  A[0]:(0.655586838722) A[1]:(0.810034811497) A[2]:(0.809958696365) A[3]:(-0.00105610454921)\n",
      " state (10)  A[0]:(0.728645205498) A[1]:(0.900053024292) A[2]:(-0.000349521636963) A[3]:(0.72850060463)\n",
      " state (11)  A[0]:(0.132793456316) A[1]:(0.88278067112) A[2]:(-0.927201032639) A[3]:(0.802739620209)\n",
      " state (12)  A[0]:(-0.429371774197) A[1]:(0.816139876842) A[2]:(-0.951405882835) A[3]:(0.71493935585)\n",
      " state (13)  A[0]:(-0.00181886355858) A[1]:(0.809780180454) A[2]:(0.900028884411) A[3]:(0.72865319252)\n",
      " state (14)  A[0]:(0.809623301029) A[1]:(0.900243520737) A[2]:(0.999999821186) A[3]:(0.809779405594)\n",
      " state (15)  A[0]:(0.979712843895) A[1]:(0.93888002634) A[2]:(1.0) A[3]:(0.878064513206)\n",
      "Episode 604000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6022. Times reached goal: 986.               Steps done: 4285691. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0123877089005.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5904,  0.5904,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6559, -0.0004,  0.5312]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560, -0.0012,  0.7292,  0.5906]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560,  0.8098,  0.8100, -0.0002]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000,  0.0002,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8096,  0.9005,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531389713287) A[1]:(0.590424716473) A[2]:(0.590365052223) A[3]:(0.53134393692)\n",
      " state (1)  A[0]:(0.531468629837) A[1]:(9.75728034973e-05) A[2]:(0.656162679195) A[3]:(0.590360760689)\n",
      " state (2)  A[0]:(0.590506196022) A[1]:(0.728975832462) A[2]:(0.590349733829) A[3]:(0.655957758427)\n",
      " state (3)  A[0]:(0.656072854996) A[1]:(-0.00613771099597) A[2]:(0.525105178356) A[3]:(0.550940752029)\n",
      " state (4)  A[0]:(0.590308427811) A[1]:(0.655960798264) A[2]:(-0.000275135040283) A[3]:(0.531264126301)\n",
      " state (5)  A[0]:(-0.0336712524295) A[1]:(0.999897420406) A[2]:(-0.821725964546) A[3]:(0.653943896294)\n",
      " state (6)  A[0]:(3.78489494324e-06) A[1]:(0.810015916824) A[2]:(0.00032651424408) A[3]:(0.656522274017)\n",
      " state (7)  A[0]:(0.54598069191) A[1]:(-0.537717580795) A[2]:(0.435708075762) A[3]:(0.871663153172)\n",
      " state (8)  A[0]:(0.655947446823) A[1]:(-0.00109976483509) A[2]:(0.729193806648) A[3]:(0.590605974197)\n",
      " state (9)  A[0]:(0.655902087688) A[1]:(0.809745788574) A[2]:(0.810007691383) A[3]:(-0.000219225883484)\n",
      " state (10)  A[0]:(0.729329586029) A[1]:(0.89998292923) A[2]:(0.000296592712402) A[3]:(0.729049563408)\n",
      " state (11)  A[0]:(0.134960368276) A[1]:(0.88283097744) A[2]:(-0.927105784416) A[3]:(0.803351402283)\n",
      " state (12)  A[0]:(-0.427914738655) A[1]:(0.816398918629) A[2]:(-0.951409757137) A[3]:(0.71574139595)\n",
      " state (13)  A[0]:(-0.000994816073216) A[1]:(0.810205578804) A[2]:(0.900035917759) A[3]:(0.729278743267)\n",
      " state (14)  A[0]:(0.809581398964) A[1]:(0.900564014912) A[2]:(0.999999821186) A[3]:(0.810159623623)\n",
      " state (15)  A[0]:(0.979683935642) A[1]:(0.939111590385) A[2]:(1.0) A[3]:(0.878285169601)\n",
      "Episode 605000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6026. Times reached goal: 983.               Steps done: 4291717. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0123132850309.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531302452087) A[1]:(0.590538263321) A[2]:(0.590472579002) A[3]:(0.53086066246)\n",
      " state (1)  A[0]:(0.531438589096) A[1]:(-6.39855861664e-05) A[2]:(0.656093120575) A[3]:(0.589929282665)\n",
      " state (2)  A[0]:(0.590510845184) A[1]:(0.729098200798) A[2]:(0.590432524681) A[3]:(0.655565202236)\n",
      " state (3)  A[0]:(0.656129717827) A[1]:(-0.00548246968538) A[2]:(0.525204539299) A[3]:(0.550523161888)\n",
      " state (4)  A[0]:(0.590532124043) A[1]:(0.65621060133) A[2]:(-9.98973846436e-05) A[3]:(0.530791044235)\n",
      " state (5)  A[0]:(-0.0331593714654) A[1]:(0.999897420406) A[2]:(-0.821686387062) A[3]:(0.653310656548)\n",
      " state (6)  A[0]:(0.000151813030243) A[1]:(0.810033500195) A[2]:(0.000103235244751) A[3]:(0.655788838863)\n",
      " state (7)  A[0]:(0.546213030815) A[1]:(-0.536675930023) A[2]:(0.435023099184) A[3]:(0.871468186378)\n",
      " state (8)  A[0]:(0.656376481056) A[1]:(0.000433355540736) A[2]:(0.729006290436) A[3]:(0.590664505959)\n",
      " state (9)  A[0]:(0.656376600266) A[1]:(0.810166060925) A[2]:(0.810024619102) A[3]:(0.000360429257853)\n",
      " state (10)  A[0]:(0.729427397251) A[1]:(0.900076329708) A[2]:(-0.000161170959473) A[3]:(0.729034125805)\n",
      " state (11)  A[0]:(0.134971067309) A[1]:(0.882728934288) A[2]:(-0.927250802517) A[3]:(0.803073763847)\n",
      " state (12)  A[0]:(-0.427094042301) A[1]:(0.815957665443) A[2]:(-0.95147973299) A[3]:(0.715370059013)\n",
      " state (13)  A[0]:(0.00112748099491) A[1]:(0.809553086758) A[2]:(0.90015912056) A[3]:(0.72908270359)\n",
      " state (14)  A[0]:(0.810479938984) A[1]:(0.900163590908) A[2]:(0.999999821186) A[3]:(0.810083150864)\n",
      " state (15)  A[0]:(0.979784429073) A[1]:(0.9388499856) A[2]:(1.0) A[3]:(0.878245651722)\n",
      "Episode 606000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6017. Times reached goal: 984.               Steps done: 4297734. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0122394184454.\n",
      " state (0)  A[0]:(0.531682729721) A[1]:(0.589704930782) A[2]:(0.590377151966) A[3]:(0.531457066536)\n",
      " state (1)  A[0]:(0.531358718872) A[1]:(5.18262386322e-05) A[2]:(0.655617237091) A[3]:(0.590271234512)\n",
      " state (2)  A[0]:(0.590232372284) A[1]:(0.728738188744) A[2]:(0.589999854565) A[3]:(0.655832469463)\n",
      " state (3)  A[0]:(0.655927062035) A[1]:(-0.00608880724758) A[2]:(0.52489066124) A[3]:(0.550901770592)\n",
      " state (4)  A[0]:(0.590294957161) A[1]:(0.656057834625) A[2]:(-0.000367641419871) A[3]:(0.531145274639)\n",
      " state (5)  A[0]:(-0.0334641523659) A[1]:(0.999897241592) A[2]:(-0.821656346321) A[3]:(0.653296947479)\n",
      " state (6)  A[0]:(0.000352755159838) A[1]:(0.809688448906) A[2]:(0.000260591506958) A[3]:(0.655408024788)\n",
      " state (7)  A[0]:(0.546256661415) A[1]:(-0.537195801735) A[2]:(0.434954464436) A[3]:(0.871153652668)\n",
      " state (8)  A[0]:(0.656103372574) A[1]:(-0.000339418649673) A[2]:(0.728791117668) A[3]:(0.589433789253)\n",
      " state (9)  A[0]:(0.656152009964) A[1]:(0.809790730476) A[2]:(0.809666633606) A[3]:(-0.000571340264287)\n",
      " state (10)  A[0]:(0.729165911674) A[1]:(0.899929404259) A[2]:(-0.000984549173154) A[3]:(0.728774547577)\n",
      " state (11)  A[0]:(0.134078294039) A[1]:(0.882708787918) A[2]:(-0.927364349365) A[3]:(0.802924394608)\n",
      " state (12)  A[0]:(-0.428299605846) A[1]:(0.816110610962) A[2]:(-0.951613426208) A[3]:(0.715078651905)\n",
      " state (13)  A[0]:(-0.000561848224606) A[1]:(0.809800744057) A[2]:(0.89984613657) A[3]:(0.728690505028)\n",
      " state (14)  A[0]:(0.810026109219) A[1]:(0.900334000587) A[2]:(0.999999821186) A[3]:(0.809731721878)\n",
      " state (15)  A[0]:(0.979754507542) A[1]:(0.938968718052) A[2]:(1.0) A[3]:(0.877979695797)\n",
      "Episode 607000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6032. Times reached goal: 989.               Steps done: 4303766. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0121658124921.\n",
      " state (0)  A[0]:(0.531814575195) A[1]:(0.590616822243) A[2]:(0.590588748455) A[3]:(0.531574368477)\n",
      " state (1)  A[0]:(0.531720280647) A[1]:(4.94122505188e-05) A[2]:(0.656133413315) A[3]:(0.590478777885)\n",
      " state (2)  A[0]:(0.590668797493) A[1]:(0.729085087776) A[2]:(0.59043598175) A[3]:(0.656014561653)\n",
      " state (3)  A[0]:(0.656307339668) A[1]:(-0.00548851955682) A[2]:(0.52525305748) A[3]:(0.551079392433)\n",
      " state (4)  A[0]:(0.590625226498) A[1]:(0.656293213367) A[2]:(-7.7486038208e-05) A[3]:(0.53136330843)\n",
      " state (5)  A[0]:(-0.0335883907974) A[1]:(0.999897420406) A[2]:(-0.821723878384) A[3]:(0.653674304485)\n",
      " state (6)  A[0]:(-2.64197587967e-05) A[1]:(0.809983372688) A[2]:(0.000135779380798) A[3]:(0.655982732773)\n",
      " state (7)  A[0]:(0.546028494835) A[1]:(-0.537124097347) A[2]:(0.435181587934) A[3]:(0.871483504772)\n",
      " state (8)  A[0]:(0.656077861786) A[1]:(-5.42402267456e-06) A[2]:(0.729062736034) A[3]:(0.590248346329)\n",
      " state (9)  A[0]:(0.656051874161) A[1]:(0.810054183006) A[2]:(0.809985101223) A[3]:(-0.000305622816086)\n",
      " state (10)  A[0]:(0.729014873505) A[1]:(0.900000989437) A[2]:(-0.00036001202534) A[3]:(0.728846132755)\n",
      " state (11)  A[0]:(0.133678168058) A[1]:(0.882669210434) A[2]:(-0.927365601063) A[3]:(0.802986621857)\n",
      " state (12)  A[0]:(-0.428525626659) A[1]:(0.815914332867) A[2]:(-0.951641082764) A[3]:(0.715225994587)\n",
      " state (13)  A[0]:(-0.000718906405382) A[1]:(0.809554100037) A[2]:(0.899981856346) A[3]:(0.728890836239)\n",
      " state (14)  A[0]:(0.809935033321) A[1]:(0.900219976902) A[2]:(0.999999821186) A[3]:(0.809885323048)\n",
      " state (15)  A[0]:(0.979734003544) A[1]:(0.938898742199) A[2]:(1.0) A[3]:(0.878075957298)\n",
      "Episode 608000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6068. Times reached goal: 988.               Steps done: 4309834. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0120922138659.\n",
      " state (0)  A[0]:(0.531406283379) A[1]:(0.590517520905) A[2]:(0.590600550175) A[3]:(0.531903624535)\n",
      " state (1)  A[0]:(0.531430900097) A[1]:(7.94231891632e-05) A[2]:(0.656229674816) A[3]:(0.590812921524)\n",
      " state (2)  A[0]:(0.590534687042) A[1]:(0.729027748108) A[2]:(0.59087240696) A[3]:(0.656361341476)\n",
      " state (3)  A[0]:(0.656285822392) A[1]:(-0.005842350889) A[2]:(0.525673270226) A[3]:(0.551446676254)\n",
      " state (4)  A[0]:(0.590674698353) A[1]:(0.656245112419) A[2]:(0.000191926956177) A[3]:(0.531703591347)\n",
      " state (5)  A[0]:(-0.033413797617) A[1]:(0.999897480011) A[2]:(-0.821786284447) A[3]:(0.653899550438)\n",
      " state (6)  A[0]:(0.000163391232491) A[1]:(0.810068786144) A[2]:(0.000222086906433) A[3]:(0.656198263168)\n",
      " state (7)  A[0]:(0.546128630638) A[1]:(-0.53710103035) A[2]:(0.435383141041) A[3]:(0.871600866318)\n",
      " state (8)  A[0]:(0.656106114388) A[1]:(-0.000279635190964) A[2]:(0.729020774364) A[3]:(0.591029644012)\n",
      " state (9)  A[0]:(0.655891060829) A[1]:(0.809999167919) A[2]:(0.809965074062) A[3]:(0.000728517654352)\n",
      " state (10)  A[0]:(0.729197442532) A[1]:(0.900023877621) A[2]:(-0.000123023986816) A[3]:(0.729387044907)\n",
      " state (11)  A[0]:(0.134872153401) A[1]:(0.882750988007) A[2]:(-0.927342832088) A[3]:(0.803419053555)\n",
      " state (12)  A[0]:(-0.427458107471) A[1]:(0.816079497337) A[2]:(-0.95165258646) A[3]:(0.715622782707)\n",
      " state (13)  A[0]:(0.000199630856514) A[1]:(0.809724211693) A[2]:(0.900128602982) A[3]:(0.729027509689)\n",
      " state (14)  A[0]:(0.810068905354) A[1]:(0.90028834343) A[2]:(0.999999821186) A[3]:(0.809825360775)\n",
      " state (15)  A[0]:(0.979736328125) A[1]:(0.938896536827) A[2]:(1.0) A[3]:(0.877968430519)\n",
      "Episode 609000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6030. Times reached goal: 991.               Steps done: 4315864. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.012019517217.\n",
      "q_values \n",
      "tensor([[ 0.5305,  0.5904,  0.5904,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5310, -0.0001,  0.6561,  0.5909]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.7290,  0.5906,  0.6565]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  0.8100,  0.0002,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7287,  0.8999,  0.0001,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9002,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531669259071) A[1]:(0.590419828892) A[2]:(0.590461432934) A[3]:(0.531028151512)\n",
      " state (1)  A[0]:(0.53192281723) A[1]:(0.000368535489542) A[2]:(0.655989170074) A[3]:(0.590270400047)\n",
      " state (2)  A[0]:(0.590856552124) A[1]:(0.729045510292) A[2]:(0.590475797653) A[3]:(0.655995130539)\n",
      " state (3)  A[0]:(0.656483769417) A[1]:(-0.00569128803909) A[2]:(0.525243282318) A[3]:(0.551047027111)\n",
      " state (4)  A[0]:(0.590835928917) A[1]:(0.656215906143) A[2]:(-0.000316619873047) A[3]:(0.531404614449)\n",
      " state (5)  A[0]:(-0.0332011543214) A[1]:(0.999897480011) A[2]:(-0.821991682053) A[3]:(0.653851270676)\n",
      " state (6)  A[0]:(0.000627934874501) A[1]:(0.810124456882) A[2]:(-0.000543475092854) A[3]:(0.656003832817)\n",
      " state (7)  A[0]:(0.546387374401) A[1]:(-0.536916136742) A[2]:(0.434802979231) A[3]:(0.871430039406)\n",
      " state (8)  A[0]:(0.656160235405) A[1]:(-0.000143826007843) A[2]:(0.728845834732) A[3]:(0.59051412344)\n",
      " state (9)  A[0]:(0.655948221684) A[1]:(0.809949696064) A[2]:(0.809913754463) A[3]:(0.000382095546229)\n",
      " state (10)  A[0]:(0.729247689247) A[1]:(0.899999022484) A[2]:(-0.000291347503662) A[3]:(0.729119598866)\n",
      " state (11)  A[0]:(0.135203808546) A[1]:(0.882754266262) A[2]:(-0.927409291267) A[3]:(0.803137540817)\n",
      " state (12)  A[0]:(-0.426860272884) A[1]:(0.816101610661) A[2]:(-0.95173740387) A[3]:(0.715260386467)\n",
      " state (13)  A[0]:(0.00114940060303) A[1]:(0.809717297554) A[2]:(0.899975061417) A[3]:(0.728805541992)\n",
      " state (14)  A[0]:(0.81037735939) A[1]:(0.900278151035) A[2]:(0.999999821186) A[3]:(0.809771597385)\n",
      " state (15)  A[0]:(0.979769706726) A[1]:(0.938888728619) A[2]:(1.0) A[3]:(0.87798666954)\n",
      "Episode 610000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6045. Times reached goal: 988.               Steps done: 4321909. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0119470784023.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53139770031) A[1]:(0.590483784676) A[2]:(0.59045958519) A[3]:(0.531609714031)\n",
      " state (1)  A[0]:(0.531316995621) A[1]:(0.000131025910378) A[2]:(0.656050682068) A[3]:(0.590728223324)\n",
      " state (2)  A[0]:(0.590433478355) A[1]:(0.728992760181) A[2]:(0.590475201607) A[3]:(0.656289875507)\n",
      " state (3)  A[0]:(0.65610742569) A[1]:(-0.00573400827125) A[2]:(0.525274395943) A[3]:(0.551388204098)\n",
      " state (4)  A[0]:(0.590436816216) A[1]:(0.656005680561) A[2]:(-8.48770141602e-05) A[3]:(0.531761050224)\n",
      " state (5)  A[0]:(-0.0337988249958) A[1]:(0.999897360802) A[2]:(-0.821844875813) A[3]:(0.654218435287)\n",
      " state (6)  A[0]:(-4.91738319397e-07) A[1]:(0.809967339039) A[2]:(-9.46521759033e-05) A[3]:(0.656379461288)\n",
      " state (7)  A[0]:(0.545941829681) A[1]:(-0.536963701248) A[2]:(0.4350451231) A[3]:(0.871585011482)\n",
      " state (8)  A[0]:(0.656081676483) A[1]:(-0.000114783644676) A[2]:(0.728946983814) A[3]:(0.590867400169)\n",
      " state (9)  A[0]:(0.656200587749) A[1]:(0.809960246086) A[2]:(0.809979736805) A[3]:(0.000863343244419)\n",
      " state (10)  A[0]:(0.729419350624) A[1]:(0.89997023344) A[2]:(-1.95503234863e-05) A[3]:(0.729410946369)\n",
      " state (11)  A[0]:(0.135174050927) A[1]:(0.882681965828) A[2]:(-0.927411854267) A[3]:(0.803415775299)\n",
      " state (12)  A[0]:(-0.427344024181) A[1]:(0.81593900919) A[2]:(-0.951781451702) A[3]:(0.715653777122)\n",
      " state (13)  A[0]:(0.000397726864321) A[1]:(0.809509694576) A[2]:(0.900000452995) A[3]:(0.729222416878)\n",
      " state (14)  A[0]:(0.810235321522) A[1]:(0.90014398098) A[2]:(0.999999821186) A[3]:(0.810144722462)\n",
      " state (15)  A[0]:(0.979771971703) A[1]:(0.938766956329) A[2]:(1.0) A[3]:(0.878288507462)\n",
      "Episode 611000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6041. Times reached goal: 990.               Steps done: 4327950. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0118751236598.\n",
      " state (0)  A[0]:(0.531260550022) A[1]:(0.590346574783) A[2]:(0.590473175049) A[3]:(0.53123652935)\n",
      " state (1)  A[0]:(0.531178832054) A[1]:(0.000338286161423) A[2]:(0.655957818031) A[3]:(0.590288281441)\n",
      " state (2)  A[0]:(0.590371072292) A[1]:(0.728989481926) A[2]:(0.590448498726) A[3]:(0.65587002039)\n",
      " state (3)  A[0]:(0.656137347221) A[1]:(-0.00556656951085) A[2]:(0.525355100632) A[3]:(0.550835847855)\n",
      " state (4)  A[0]:(0.590591907501) A[1]:(0.656115055084) A[2]:(0.000104188919067) A[3]:(0.531132459641)\n",
      " state (5)  A[0]:(-0.0333748571575) A[1]:(0.999897360802) A[2]:(-0.821834206581) A[3]:(0.653609633446)\n",
      " state (6)  A[0]:(0.00054603809258) A[1]:(0.809962391853) A[2]:(-0.000182509422302) A[3]:(0.655671536922)\n",
      " state (7)  A[0]:(0.546293735504) A[1]:(-0.536833524704) A[2]:(0.434842437506) A[3]:(0.871237039566)\n",
      " state (8)  A[0]:(0.656142473221) A[1]:(0.000107020139694) A[2]:(0.72880512476) A[3]:(0.589718818665)\n",
      " state (9)  A[0]:(0.655933618546) A[1]:(0.810021400452) A[2]:(0.809797823429) A[3]:(-0.00114431930706)\n",
      " state (10)  A[0]:(0.728891015053) A[1]:(0.899985909462) A[2]:(-0.00102353061084) A[3]:(0.72830593586)\n",
      " state (11)  A[0]:(0.13380612433) A[1]:(0.882709741592) A[2]:(-0.927614510059) A[3]:(0.80257743597)\n",
      " state (12)  A[0]:(-0.42810857296) A[1]:(0.816024363041) A[2]:(-0.951895654202) A[3]:(0.714735150337)\n",
      " state (13)  A[0]:(5.47170639038e-05) A[1]:(0.809670031071) A[2]:(0.900050520897) A[3]:(0.728590965271)\n",
      " state (14)  A[0]:(0.810241341591) A[1]:(0.900282979012) A[2]:(0.999999821186) A[3]:(0.809803545475)\n",
      " state (15)  A[0]:(0.979778945446) A[1]:(0.938859701157) A[2]:(1.0) A[3]:(0.878105938435)\n",
      "Episode 612000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6039. Times reached goal: 992.               Steps done: 4333989. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0118036258928.\n",
      " state (0)  A[0]:(0.530768156052) A[1]:(0.590388298035) A[2]:(0.590155243874) A[3]:(0.530656516552)\n",
      " state (1)  A[0]:(0.53069293499) A[1]:(-0.000202536582947) A[2]:(0.65603518486) A[3]:(0.589776098728)\n",
      " state (2)  A[0]:(0.589874088764) A[1]:(0.729114174843) A[2]:(0.590659499168) A[3]:(0.65552431345)\n",
      " state (3)  A[0]:(0.655643582344) A[1]:(-0.00535590294749) A[2]:(0.525559067726) A[3]:(0.55032813549)\n",
      " state (4)  A[0]:(0.589965224266) A[1]:(0.656273603439) A[2]:(0.000288844108582) A[3]:(0.530422687531)\n",
      " state (5)  A[0]:(-0.0342594385147) A[1]:(0.999897420406) A[2]:(-0.821820378304) A[3]:(0.652688503265)\n",
      " state (6)  A[0]:(-0.00025250017643) A[1]:(0.810065448284) A[2]:(8.45193862915e-05) A[3]:(0.654494047165)\n",
      " state (7)  A[0]:(0.545542836189) A[1]:(-0.536379337311) A[2]:(0.435269802809) A[3]:(0.870677351952)\n",
      " state (8)  A[0]:(0.654981851578) A[1]:(0.00114214373752) A[2]:(0.729213953018) A[3]:(0.587980866432)\n",
      " state (9)  A[0]:(0.654752969742) A[1]:(0.810253977776) A[2]:(0.809993565083) A[3]:(-0.00206306274049)\n",
      " state (10)  A[0]:(0.728013336658) A[1]:(0.900059580803) A[2]:(-0.000747322919779) A[3]:(0.728178739548)\n",
      " state (11)  A[0]:(0.132159039378) A[1]:(0.882726550102) A[2]:(-0.927650809288) A[3]:(0.802387714386)\n",
      " state (12)  A[0]:(-0.429261118174) A[1]:(0.815916955471) A[2]:(-0.951964199543) A[3]:(0.714269399643)\n",
      " state (13)  A[0]:(-0.000932305760216) A[1]:(0.809415459633) A[2]:(0.900048494339) A[3]:(0.727988660336)\n",
      " state (14)  A[0]:(0.810104191303) A[1]:(0.900062859058) A[2]:(0.999999821186) A[3]:(0.809259414673)\n",
      " state (15)  A[0]:(0.979786992073) A[1]:(0.938652157784) A[2]:(1.0) A[3]:(0.87771242857)\n",
      "Episode 613000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6029. Times reached goal: 985.               Steps done: 4340018. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0117326759259.\n",
      " state (0)  A[0]:(0.532306969166) A[1]:(0.590236663818) A[2]:(0.590336203575) A[3]:(0.53154361248)\n",
      " state (1)  A[0]:(0.53134906292) A[1]:(0.000524908246007) A[2]:(0.656142950058) A[3]:(0.591329097748)\n",
      " state (2)  A[0]:(0.590086400509) A[1]:(0.728890836239) A[2]:(0.590613245964) A[3]:(0.657002568245)\n",
      " state (3)  A[0]:(0.655813097954) A[1]:(-0.00599481444806) A[2]:(0.525573194027) A[3]:(0.552244365215)\n",
      " state (4)  A[0]:(0.590060830116) A[1]:(0.655799627304) A[2]:(0.000540375651326) A[3]:(0.532692492008)\n",
      " state (5)  A[0]:(-0.0345404520631) A[1]:(0.999897241592) A[2]:(-0.821607351303) A[3]:(0.655052185059)\n",
      " state (6)  A[0]:(-0.000472635001643) A[1]:(0.809858798981) A[2]:(0.000902533298358) A[3]:(0.656888604164)\n",
      " state (7)  A[0]:(0.545371413231) A[1]:(-0.537044644356) A[2]:(0.435736089945) A[3]:(0.871729850769)\n",
      " state (8)  A[0]:(0.655502557755) A[1]:(-0.000810950819869) A[2]:(0.728957772255) A[3]:(0.59181112051)\n",
      " state (9)  A[0]:(0.654394805431) A[1]:(0.809861302376) A[2]:(0.809744238853) A[3]:(0.000258058309555)\n",
      " state (10)  A[0]:(0.726565003395) A[1]:(0.900042891502) A[2]:(-0.00240122806281) A[3]:(0.728272080421)\n",
      " state (11)  A[0]:(0.127578482032) A[1]:(0.882936060429) A[2]:(-0.928005933762) A[3]:(0.802345693111)\n",
      " state (12)  A[0]:(-0.433207213879) A[1]:(0.81654214859) A[2]:(-0.952154517174) A[3]:(0.714559733868)\n",
      " state (13)  A[0]:(-0.00564148975536) A[1]:(0.810273110867) A[2]:(0.900077819824) A[3]:(0.728649258614)\n",
      " state (14)  A[0]:(0.808466434479) A[1]:(0.900566697121) A[2]:(0.999999821186) A[3]:(0.80992436409)\n",
      " state (15)  A[0]:(0.97960537672) A[1]:(0.938914299011) A[2]:(1.0) A[3]:(0.87823587656)\n",
      "Episode 614000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6016. Times reached goal: 983.               Steps done: 4346034. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0116623040384.\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.5906,  0.5905,  0.5310]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5916,  0.6563,  0.0002,  0.5325]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565, -0.0005,  0.7292,  0.5919]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556,  0.8100,  0.8101,  0.0012]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7281,  0.9000, -0.0005,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8086,  0.9004,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531022548676) A[1]:(0.590575695038) A[2]:(0.590598702431) A[3]:(0.530698299408)\n",
      " state (1)  A[0]:(0.531590938568) A[1]:(5.71757555008e-05) A[2]:(0.656088352203) A[3]:(0.590241909027)\n",
      " state (2)  A[0]:(0.590724945068) A[1]:(0.729028105736) A[2]:(0.590503811836) A[3]:(0.656015992165)\n",
      " state (3)  A[0]:(0.656426429749) A[1]:(-0.00571439880878) A[2]:(0.525388419628) A[3]:(0.550984740257)\n",
      " state (4)  A[0]:(0.590809106827) A[1]:(0.656165122986) A[2]:(2.31266021729e-05) A[3]:(0.531399607658)\n",
      " state (5)  A[0]:(-0.0332198366523) A[1]:(0.999897360802) A[2]:(-0.821901679039) A[3]:(0.654104351997)\n",
      " state (6)  A[0]:(0.000910937553272) A[1]:(0.809992492199) A[2]:(0.000168085098267) A[3]:(0.655981779099)\n",
      " state (7)  A[0]:(0.546257197857) A[1]:(-0.53682756424) A[2]:(0.435399264097) A[3]:(0.871312201023)\n",
      " state (8)  A[0]:(0.65585231781) A[1]:(-0.000331684947014) A[2]:(0.728987812996) A[3]:(0.590300321579)\n",
      " state (9)  A[0]:(0.655127644539) A[1]:(0.809922635555) A[2]:(0.809989094734) A[3]:(-0.000775039021391)\n",
      " state (10)  A[0]:(0.728191256523) A[1]:(0.90002566576) A[2]:(-0.000202894210815) A[3]:(0.728644430637)\n",
      " state (11)  A[0]:(0.132199406624) A[1]:(0.882871627808) A[2]:(-0.927626490593) A[3]:(0.802980840206)\n",
      " state (12)  A[0]:(-0.430369377136) A[1]:(0.816355586052) A[2]:(-0.952030777931) A[3]:(0.715118825436)\n",
      " state (13)  A[0]:(-0.00382679328322) A[1]:(0.810002744198) A[2]:(0.900164246559) A[3]:(0.728734970093)\n",
      " state (14)  A[0]:(0.808735013008) A[1]:(0.900403499603) A[2]:(0.999999821186) A[3]:(0.809793591499)\n",
      " state (15)  A[0]:(0.979614317417) A[1]:(0.938801288605) A[2]:(1.0) A[3]:(0.878093540668)\n",
      "Episode 615000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6032. Times reached goal: 993.               Steps done: 4352066. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0115921687611.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531188964844) A[1]:(0.590254247189) A[2]:(0.590480566025) A[3]:(0.531327545643)\n",
      " state (1)  A[0]:(0.531139016151) A[1]:(-5.50597906113e-05) A[2]:(0.656038165092) A[3]:(0.590198457241)\n",
      " state (2)  A[0]:(0.590140104294) A[1]:(0.728816449642) A[2]:(0.590586066246) A[3]:(0.6557071805)\n",
      " state (3)  A[0]:(0.655861020088) A[1]:(-0.00570504134521) A[2]:(0.525531411171) A[3]:(0.5505245924)\n",
      " state (4)  A[0]:(0.590031862259) A[1]:(0.65645724535) A[2]:(0.000229120254517) A[3]:(0.530858278275)\n",
      " state (5)  A[0]:(-0.0348743870854) A[1]:(0.999897480011) A[2]:(-0.821859896183) A[3]:(0.653529047966)\n",
      " state (6)  A[0]:(-0.000523507536855) A[1]:(0.810258746147) A[2]:(5.32865524292e-05) A[3]:(0.654963970184)\n",
      " state (7)  A[0]:(0.5454403162) A[1]:(-0.535317420959) A[2]:(0.435077697039) A[3]:(0.870766341686)\n",
      " state (8)  A[0]:(0.655040323734) A[1]:(0.00230885623023) A[2]:(0.729043960571) A[3]:(0.587740182877)\n",
      " state (9)  A[0]:(0.654623985291) A[1]:(0.810566902161) A[2]:(0.809918344021) A[3]:(-0.004342737142)\n",
      " state (10)  A[0]:(0.727713346481) A[1]:(0.900156855583) A[2]:(-0.0010956521146) A[3]:(0.72689551115)\n",
      " state (11)  A[0]:(0.131611421704) A[1]:(0.882735788822) A[2]:(-0.927808761597) A[3]:(0.801595211029)\n",
      " state (12)  A[0]:(-0.429226517677) A[1]:(0.815749466419) A[2]:(-0.952126204967) A[3]:(0.713621199131)\n",
      " state (13)  A[0]:(-0.000425189704401) A[1]:(0.809092521667) A[2]:(0.900051295757) A[3]:(0.727759897709)\n",
      " state (14)  A[0]:(0.810223400593) A[1]:(0.899878740311) A[2]:(0.999999821186) A[3]:(0.809264063835)\n",
      " state (15)  A[0]:(0.979779720306) A[1]:(0.938527047634) A[2]:(1.0) A[3]:(0.877773821354)\n",
      "Episode 616000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6013. Times reached goal: 989.               Steps done: 4358079. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0115226741952.\n",
      " state (0)  A[0]:(0.531660318375) A[1]:(0.590585827827) A[2]:(0.590286850929) A[3]:(0.5310806036)\n",
      " state (1)  A[0]:(0.530890345573) A[1]:(0.000231921672821) A[2]:(0.656171917915) A[3]:(0.58978843689)\n",
      " state (2)  A[0]:(0.589698731899) A[1]:(0.729066133499) A[2]:(0.590727806091) A[3]:(0.655477643013)\n",
      " state (3)  A[0]:(0.65562415123) A[1]:(-0.00582360615954) A[2]:(0.525645077229) A[3]:(0.550198078156)\n",
      " state (4)  A[0]:(0.589991509914) A[1]:(0.65567111969) A[2]:(0.000425934762461) A[3]:(0.53078019619)\n",
      " state (5)  A[0]:(-0.0347413755953) A[1]:(0.999897241592) A[2]:(-0.821921467781) A[3]:(0.654380679131)\n",
      " state (6)  A[0]:(-0.000539332570042) A[1]:(0.80986815691) A[2]:(0.000203967094421) A[3]:(0.656386256218)\n",
      " state (7)  A[0]:(0.544991970062) A[1]:(-0.537211537361) A[2]:(0.435734838247) A[3]:(0.871272742748)\n",
      " state (8)  A[0]:(0.654440939426) A[1]:(-0.000504314841237) A[2]:(0.729318857193) A[3]:(0.58868253231)\n",
      " state (9)  A[0]:(0.65408706665) A[1]:(0.809677183628) A[2]:(0.809973478317) A[3]:(-0.00251149595715)\n",
      " state (10)  A[0]:(0.727194547653) A[1]:(0.899906396866) A[2]:(-0.000792980019469) A[3]:(0.727860450745)\n",
      " state (11)  A[0]:(0.130072742701) A[1]:(0.882860004902) A[2]:(-0.927780926228) A[3]:(0.802382707596)\n",
      " state (12)  A[0]:(-0.431439518929) A[1]:(0.816510379314) A[2]:(-0.952148020267) A[3]:(0.714521706104)\n",
      " state (13)  A[0]:(-0.00408855779096) A[1]:(0.810280442238) A[2]:(0.90013730526) A[3]:(0.728363633156)\n",
      " state (14)  A[0]:(0.80893945694) A[1]:(0.900633811951) A[2]:(0.999999821186) A[3]:(0.809551000595)\n",
      " state (15)  A[0]:(0.979662537575) A[1]:(0.938975989819) A[2]:(1.0) A[3]:(0.877912282944)\n",
      "Episode 617000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6011. Times reached goal: 984.               Steps done: 4364090. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0114536191534.\n",
      " state (0)  A[0]:(0.53152346611) A[1]:(0.590462088585) A[2]:(0.590555429459) A[3]:(0.531409502029)\n",
      " state (1)  A[0]:(0.531523942947) A[1]:(3.9353966713e-05) A[2]:(0.65596216917) A[3]:(0.59049642086)\n",
      " state (2)  A[0]:(0.590483248234) A[1]:(0.729059457779) A[2]:(0.590522289276) A[3]:(0.656180143356)\n",
      " state (3)  A[0]:(0.656209468842) A[1]:(-0.00560347829014) A[2]:(0.525350689888) A[3]:(0.551094889641)\n",
      " state (4)  A[0]:(0.590489149094) A[1]:(0.656114518642) A[2]:(-0.000107765197754) A[3]:(0.531522035599)\n",
      " state (5)  A[0]:(-0.0342070758343) A[1]:(0.999897360802) A[2]:(-0.822046816349) A[3]:(0.654305398464)\n",
      " state (6)  A[0]:(0.000209331512451) A[1]:(0.810079813004) A[2]:(-7.05718994141e-05) A[3]:(0.655871987343)\n",
      " state (7)  A[0]:(0.545964002609) A[1]:(-0.536655426025) A[2]:(0.435352802277) A[3]:(0.871220767498)\n",
      " state (8)  A[0]:(0.656056046486) A[1]:(-0.000167354941368) A[2]:(0.728964209557) A[3]:(0.590379834175)\n",
      " state (9)  A[0]:(0.655844926834) A[1]:(0.80996131897) A[2]:(0.810006558895) A[3]:(-0.000283092260361)\n",
      " state (10)  A[0]:(0.729096412659) A[1]:(0.900009214878) A[2]:(-0.00013542175293) A[3]:(0.728887915611)\n",
      " state (11)  A[0]:(0.13477884233) A[1]:(0.882805526257) A[2]:(-0.927730858326) A[3]:(0.803150177002)\n",
      " state (12)  A[0]:(-0.427790522575) A[1]:(0.816160678864) A[2]:(-0.952200651169) A[3]:(0.715321004391)\n",
      " state (13)  A[0]:(-0.000435367197497) A[1]:(0.809729814529) A[2]:(0.90003991127) A[3]:(0.72893512249)\n",
      " state (14)  A[0]:(0.809842586517) A[1]:(0.900288879871) A[2]:(0.999999821186) A[3]:(0.809925794601)\n",
      " state (15)  A[0]:(0.97972869873) A[1]:(0.938743948936) A[2]:(1.0) A[3]:(0.878160297871)\n",
      "Episode 618000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6039. Times reached goal: 991.               Steps done: 4370129. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0113846591816.\n",
      " state (0)  A[0]:(0.529992938042) A[1]:(0.590674221516) A[2]:(0.590556979179) A[3]:(0.530858278275)\n",
      " state (1)  A[0]:(0.530376911163) A[1]:(-0.00154016795568) A[2]:(0.656326889992) A[3]:(0.589259624481)\n",
      " state (2)  A[0]:(0.589691638947) A[1]:(0.729076504707) A[2]:(0.590615391731) A[3]:(0.654698371887)\n",
      " state (3)  A[0]:(0.655606389046) A[1]:(-0.00534194102511) A[2]:(0.525472760201) A[3]:(0.548996090889)\n",
      " state (4)  A[0]:(0.58990997076) A[1]:(0.656259119511) A[2]:(0.000175833702087) A[3]:(0.528961062431)\n",
      " state (5)  A[0]:(-0.0349075607955) A[1]:(0.999897301197) A[2]:(-0.821846067905) A[3]:(0.651737689972)\n",
      " state (6)  A[0]:(0.000156879425049) A[1]:(0.809966266155) A[2]:(0.000422120065195) A[3]:(0.653132617474)\n",
      " state (7)  A[0]:(0.546180307865) A[1]:(-0.535938978195) A[2]:(0.43548437953) A[3]:(0.870010435581)\n",
      " state (8)  A[0]:(0.656052052975) A[1]:(0.00197674077936) A[2]:(0.729079723358) A[3]:(0.585942387581)\n",
      " state (9)  A[0]:(0.656388282776) A[1]:(0.810485124588) A[2]:(0.809548377991) A[3]:(-0.00502918893471)\n",
      " state (10)  A[0]:(0.728981435299) A[1]:(0.90013718605) A[2]:(-0.00310586881824) A[3]:(0.7267588377)\n",
      " state (11)  A[0]:(0.133304238319) A[1]:(0.882775068283) A[2]:(-0.928272068501) A[3]:(0.801323831081)\n",
      " state (12)  A[0]:(-0.428395986557) A[1]:(0.815894722939) A[2]:(-0.952486634254) A[3]:(0.713064432144)\n",
      " state (13)  A[0]:(0.000319331884384) A[1]:(0.809316039085) A[2]:(0.899900436401) A[3]:(0.727193117142)\n",
      " state (14)  A[0]:(0.810504317284) A[1]:(0.900013625622) A[2]:(0.999999821186) A[3]:(0.808878779411)\n",
      " state (15)  A[0]:(0.979831218719) A[1]:(0.938526332378) A[2]:(1.0) A[3]:(0.877559304237)\n",
      "Episode 619000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6026. Times reached goal: 989.               Steps done: 4376155. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0113162615145.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5901,  0.5904,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.0001,  0.6558,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7291,  0.5904,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8100,  0.0001,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.8999, -0.0001,  0.7294]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.8999,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531661868095) A[1]:(0.590234279633) A[2]:(0.590404033661) A[3]:(0.53140103817)\n",
      " state (1)  A[0]:(0.531576633453) A[1]:(0.000286415219307) A[2]:(0.655819892883) A[3]:(0.590306758881)\n",
      " state (2)  A[0]:(0.590421915054) A[1]:(0.729199290276) A[2]:(0.590438425541) A[3]:(0.655960559845)\n",
      " state (3)  A[0]:(0.65613079071) A[1]:(-0.00513660861179) A[2]:(0.525359392166) A[3]:(0.550869882107)\n",
      " state (4)  A[0]:(0.590327382088) A[1]:(0.656295776367) A[2]:(7.58171081543e-05) A[3]:(0.53142529726)\n",
      " state (5)  A[0]:(-0.0347452610731) A[1]:(0.999897420406) A[2]:(-0.821992993355) A[3]:(0.654473245144)\n",
      " state (6)  A[0]:(-3.90112400055e-05) A[1]:(0.810214996338) A[2]:(0.000167369842529) A[3]:(0.655929327011)\n",
      " state (7)  A[0]:(0.545878887177) A[1]:(-0.536149740219) A[2]:(0.435642004013) A[3]:(0.871251821518)\n",
      " state (8)  A[0]:(0.656113028526) A[1]:(0.000660255434923) A[2]:(0.729161918163) A[3]:(0.590605556965)\n",
      " state (9)  A[0]:(0.656081557274) A[1]:(0.810187757015) A[2]:(0.810121178627) A[3]:(0.000545889080968)\n",
      " state (10)  A[0]:(0.72917175293) A[1]:(0.9000633955) A[2]:(0.000146389007568) A[3]:(0.729218482971)\n",
      " state (11)  A[0]:(0.134640455246) A[1]:(0.882754206657) A[2]:(-0.927764832973) A[3]:(0.803268492222)\n",
      " state (12)  A[0]:(-0.427845448256) A[1]:(0.815891504288) A[2]:(-0.952273011208) A[3]:(0.715427041054)\n",
      " state (13)  A[0]:(-9.69171524048e-05) A[1]:(0.809283912182) A[2]:(0.900089621544) A[3]:(0.729150772095)\n",
      " state (14)  A[0]:(0.810111522675) A[1]:(0.900000452995) A[2]:(0.999999821186) A[3]:(0.810214996338)\n",
      " state (15)  A[0]:(0.979770183563) A[1]:(0.938531696796) A[2]:(1.0) A[3]:(0.878426611423)\n",
      "Episode 620000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6039. Times reached goal: 986.               Steps done: 4382194. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0112481285458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531595885754) A[1]:(0.590324580669) A[2]:(0.590460062027) A[3]:(0.531849265099)\n",
      " state (1)  A[0]:(0.531494259834) A[1]:(-9.42945480347e-05) A[2]:(0.656046271324) A[3]:(0.590938210487)\n",
      " state (2)  A[0]:(0.590431988239) A[1]:(0.728865027428) A[2]:(0.590546667576) A[3]:(0.656507849693)\n",
      " state (3)  A[0]:(0.656198084354) A[1]:(-0.00612167827785) A[2]:(0.525380671024) A[3]:(0.551468372345)\n",
      " state (4)  A[0]:(0.590476930141) A[1]:(0.655895709991) A[2]:(-0.000197291374207) A[3]:(0.532027065754)\n",
      " state (5)  A[0]:(-0.0343891009688) A[1]:(0.999897301197) A[2]:(-0.822203576565) A[3]:(0.654946923256)\n",
      " state (6)  A[0]:(1.77919864655e-05) A[1]:(0.809949815273) A[2]:(-0.000422477693064) A[3]:(0.656443357468)\n",
      " state (7)  A[0]:(0.545798301697) A[1]:(-0.536667585373) A[2]:(0.435209900141) A[3]:(0.871455430984)\n",
      " state (8)  A[0]:(0.656036257744) A[1]:(-0.000128626823425) A[2]:(0.728917598724) A[3]:(0.590983331203)\n",
      " state (9)  A[0]:(0.656096696854) A[1]:(0.809887170792) A[2]:(0.809953451157) A[3]:(0.00105324352626)\n",
      " state (10)  A[0]:(0.729308009148) A[1]:(0.899942874908) A[2]:(-0.000251770019531) A[3]:(0.729542016983)\n",
      " state (11)  A[0]:(0.135067731142) A[1]:(0.882710456848) A[2]:(-0.927858412266) A[3]:(0.803581476212)\n",
      " state (12)  A[0]:(-0.427725851536) A[1]:(0.815951704979) A[2]:(-0.952389597893) A[3]:(0.715806543827)\n",
      " state (13)  A[0]:(-0.000405102939112) A[1]:(0.809430241585) A[2]:(0.899885475636) A[3]:(0.729374468327)\n",
      " state (14)  A[0]:(0.809872925282) A[1]:(0.900117397308) A[2]:(0.999999821186) A[3]:(0.810242176056)\n",
      " state (15)  A[0]:(0.979736924171) A[1]:(0.93860834837) A[2]:(1.0) A[3]:(0.878373742104)\n",
      "Episode 621000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6051. Times reached goal: 991.               Steps done: 4388245. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0111802716281.\n",
      " state (0)  A[0]:(0.531615555286) A[1]:(0.589867889881) A[2]:(0.59053593874) A[3]:(0.531492233276)\n",
      " state (1)  A[0]:(0.531561255455) A[1]:(-4.37051057816e-05) A[2]:(0.656063318253) A[3]:(0.590501725674)\n",
      " state (2)  A[0]:(0.590453028679) A[1]:(0.728939414024) A[2]:(0.590650439262) A[3]:(0.65605700016)\n",
      " state (3)  A[0]:(0.656096935272) A[1]:(-0.00579477334395) A[2]:(0.52548032999) A[3]:(0.550871729851)\n",
      " state (4)  A[0]:(0.590258955956) A[1]:(0.655915260315) A[2]:(6.52074813843e-05) A[3]:(0.531463742256)\n",
      " state (5)  A[0]:(-0.0347565859556) A[1]:(0.999897301197) A[2]:(-0.82205927372) A[3]:(0.654682457447)\n",
      " state (6)  A[0]:(0.00018846988678) A[1]:(0.810016274452) A[2]:(-0.000103235244751) A[3]:(0.655854582787)\n",
      " state (7)  A[0]:(0.545896172523) A[1]:(-0.536281585693) A[2]:(0.435250759125) A[3]:(0.871087729931)\n",
      " state (8)  A[0]:(0.655775845051) A[1]:(0.000487968296511) A[2]:(0.728884875774) A[3]:(0.589716136456)\n",
      " state (9)  A[0]:(0.655798435211) A[1]:(0.81009542942) A[2]:(0.809931397438) A[3]:(-0.00046810504864)\n",
      " state (10)  A[0]:(0.729397118092) A[1]:(0.900088310242) A[2]:(0.000131487846375) A[3]:(0.729134976864)\n",
      " state (11)  A[0]:(0.13581700623) A[1]:(0.882938683033) A[2]:(-0.927789032459) A[3]:(0.803408384323)\n",
      " state (12)  A[0]:(-0.427299618721) A[1]:(0.816359996796) A[2]:(-0.952361524105) A[3]:(0.715450644493)\n",
      " state (13)  A[0]:(-0.000255614519119) A[1]:(0.809901177883) A[2]:(0.900170087814) A[3]:(0.728874206543)\n",
      " state (14)  A[0]:(0.809868812561) A[1]:(0.900389969349) A[2]:(0.999999821186) A[3]:(0.809814572334)\n",
      " state (15)  A[0]:(0.979734301567) A[1]:(0.938752651215) A[2]:(1.0) A[3]:(0.878075897694)\n",
      "Episode 622000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6021. Times reached goal: 988.               Steps done: 4394266. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0111131574626.\n",
      " state (0)  A[0]:(0.531514406204) A[1]:(0.590738415718) A[2]:(0.590740859509) A[3]:(0.531621158123)\n",
      " state (1)  A[0]:(0.531597137451) A[1]:(-2.01612710953e-05) A[2]:(0.656560659409) A[3]:(0.59080529213)\n",
      " state (2)  A[0]:(0.590725421906) A[1]:(0.728947043419) A[2]:(0.590730845928) A[3]:(0.65640437603)\n",
      " state (3)  A[0]:(0.656435489655) A[1]:(-0.00593144213781) A[2]:(0.525547981262) A[3]:(0.551166057587)\n",
      " state (4)  A[0]:(0.590643167496) A[1]:(0.656084299088) A[2]:(-3.2901763916e-05) A[3]:(0.53173238039)\n",
      " state (5)  A[0]:(-0.0344840288162) A[1]:(0.999897360802) A[2]:(-0.822186231613) A[3]:(0.655060648918)\n",
      " state (6)  A[0]:(0.000124156475067) A[1]:(0.810037851334) A[2]:(-0.00038373467396) A[3]:(0.656502366066)\n",
      " state (7)  A[0]:(0.545872628689) A[1]:(-0.536811172962) A[2]:(0.435356080532) A[3]:(0.871435046196)\n",
      " state (8)  A[0]:(0.656092405319) A[1]:(-0.000136747956276) A[2]:(0.728940367699) A[3]:(0.59085470438)\n",
      " state (9)  A[0]:(0.65610152483) A[1]:(0.810020923615) A[2]:(0.809979319572) A[3]:(0.000491708458867)\n",
      " state (10)  A[0]:(0.729381203651) A[1]:(0.90000808239) A[2]:(2.74181365967e-06) A[3]:(0.729264497757)\n",
      " state (11)  A[0]:(0.135486006737) A[1]:(0.882775247097) A[2]:(-0.927867293358) A[3]:(0.803399920464)\n",
      " state (12)  A[0]:(-0.427313476801) A[1]:(0.816035926342) A[2]:(-0.952444255352) A[3]:(0.71553593874)\n",
      " state (13)  A[0]:(-1.46776437759e-05) A[1]:(0.809542775154) A[2]:(0.899997234344) A[3]:(0.729102015495)\n",
      " state (14)  A[0]:(0.809892177582) A[1]:(0.900248229504) A[2]:(0.999999821186) A[3]:(0.810038149357)\n",
      " state (15)  A[0]:(0.97972124815) A[1]:(0.938719451427) A[2]:(1.0) A[3]:(0.878222107887)\n",
      "Episode 623000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6036. Times reached goal: 988.               Steps done: 4400302. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0110462804819.\n",
      " state (0)  A[0]:(0.531698107719) A[1]:(0.590378761292) A[2]:(0.590391337872) A[3]:(0.530831933022)\n",
      " state (1)  A[0]:(0.532143771648) A[1]:(-0.000527918280568) A[2]:(0.655947804451) A[3]:(0.589448928833)\n",
      " state (2)  A[0]:(0.591280698776) A[1]:(0.728944420815) A[2]:(0.590348243713) A[3]:(0.654998064041)\n",
      " state (3)  A[0]:(0.656944870949) A[1]:(-0.00573110254481) A[2]:(0.525237977505) A[3]:(0.549376368523)\n",
      " state (4)  A[0]:(0.591317653656) A[1]:(0.655832648277) A[2]:(-6.98566436768e-05) A[3]:(0.529685974121)\n",
      " state (5)  A[0]:(-0.0334313735366) A[1]:(0.999897122383) A[2]:(-0.822015762329) A[3]:(0.652916193008)\n",
      " state (6)  A[0]:(0.000950529880356) A[1]:(0.809682488441) A[2]:(0.000388622254832) A[3]:(0.653448462486)\n",
      " state (7)  A[0]:(0.546050488949) A[1]:(-0.536734104156) A[2]:(0.435590058565) A[3]:(0.869871795177)\n",
      " state (8)  A[0]:(0.655647993088) A[1]:(-0.000471413106425) A[2]:(0.728929162025) A[3]:(0.586396098137)\n",
      " state (9)  A[0]:(0.654808759689) A[1]:(0.809814989567) A[2]:(0.809902310371) A[3]:(-0.007091850508)\n",
      " state (10)  A[0]:(0.727861404419) A[1]:(0.899949252605) A[2]:(-0.000673889997415) A[3]:(0.725545287132)\n",
      " state (11)  A[0]:(0.132367581129) A[1]:(0.882825195789) A[2]:(-0.928045034409) A[3]:(0.800855755806)\n",
      " state (12)  A[0]:(-0.429053395987) A[1]:(0.816219210625) A[2]:(-0.952598214149) A[3]:(0.712695360184)\n",
      " state (13)  A[0]:(-0.000958352989983) A[1]:(0.809715926647) A[2]:(0.899712204933) A[3]:(0.726945698261)\n",
      " state (14)  A[0]:(0.809899568558) A[1]:(0.900317668915) A[2]:(0.999999821186) A[3]:(0.808731257915)\n",
      " state (15)  A[0]:(0.979752659798) A[1]:(0.938741922379) A[2]:(1.0) A[3]:(0.877465724945)\n",
      "Episode 624000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6031. Times reached goal: 984.               Steps done: 4406333. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.010979860854.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5902,  0.5906,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.0000,  0.6561,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7289,  0.5906,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8099, -0.0001,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0002,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9003,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531292676926) A[1]:(0.590252995491) A[2]:(0.590587556362) A[3]:(0.53150510788)\n",
      " state (1)  A[0]:(0.531236410141) A[1]:(2.80141830444e-06) A[2]:(0.656127214432) A[3]:(0.590609788895)\n",
      " state (2)  A[0]:(0.590279221535) A[1]:(0.728919506073) A[2]:(0.590543150902) A[3]:(0.656223058701)\n",
      " state (3)  A[0]:(0.656028032303) A[1]:(-0.00586556643248) A[2]:(0.525412321091) A[3]:(0.550905287266)\n",
      " state (4)  A[0]:(0.590197980404) A[1]:(0.65594637394) A[2]:(-4.55379486084e-05) A[3]:(0.531501293182)\n",
      " state (5)  A[0]:(-0.0351511947811) A[1]:(0.999897301197) A[2]:(-0.822147071362) A[3]:(0.654985547066)\n",
      " state (6)  A[0]:(-0.000324904918671) A[1]:(0.809934735298) A[2]:(-6.40153884888e-05) A[3]:(0.656088829041)\n",
      " state (7)  A[0]:(0.545505166054) A[1]:(-0.536597132683) A[2]:(0.435427486897) A[3]:(0.871180832386)\n",
      " state (8)  A[0]:(0.655733585358) A[1]:(-0.000139772891998) A[2]:(0.728873133659) A[3]:(0.59033370018)\n",
      " state (9)  A[0]:(0.655589878559) A[1]:(0.809950828552) A[2]:(0.809893846512) A[3]:(-0.000298321247101)\n",
      " state (10)  A[0]:(0.728862047195) A[1]:(0.899986505508) A[2]:(-0.000451683969004) A[3]:(0.728885829449)\n",
      " state (11)  A[0]:(0.134515196085) A[1]:(0.88279914856) A[2]:(-0.928028464317) A[3]:(0.803169250488)\n",
      " state (12)  A[0]:(-0.427808403969) A[1]:(0.816109657288) A[2]:(-0.952596843243) A[3]:(0.715307235718)\n",
      " state (13)  A[0]:(-0.000104025006294) A[1]:(0.809622168541) A[2]:(0.900016129017) A[3]:(0.728938579559)\n",
      " state (14)  A[0]:(0.810034811497) A[1]:(0.900298237801) A[2]:(0.999999821186) A[3]:(0.809899806976)\n",
      " state (15)  A[0]:(0.979754209518) A[1]:(0.938715815544) A[2]:(1.0) A[3]:(0.878113865852)\n",
      "Episode 625000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6057. Times reached goal: 996.               Steps done: 4412390. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0109135568412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531693458557) A[1]:(0.590448021889) A[2]:(0.590535223484) A[3]:(0.531706988811)\n",
      " state (1)  A[0]:(0.531741797924) A[1]:(2.19196081161e-05) A[2]:(0.656086564064) A[3]:(0.590611577034)\n",
      " state (2)  A[0]:(0.590667366982) A[1]:(0.728966653347) A[2]:(0.590525507927) A[3]:(0.656131267548)\n",
      " state (3)  A[0]:(0.656404972076) A[1]:(-0.00568242231384) A[2]:(0.525464236736) A[3]:(0.55082577467)\n",
      " state (4)  A[0]:(0.590660214424) A[1]:(0.656193554401) A[2]:(4.19616699219e-05) A[3]:(0.531451106071)\n",
      " state (5)  A[0]:(-0.0343826599419) A[1]:(0.999897360802) A[2]:(-0.822136521339) A[3]:(0.654883563519)\n",
      " state (6)  A[0]:(0.000337898731232) A[1]:(0.810004711151) A[2]:(0.000137686729431) A[3]:(0.65584307909)\n",
      " state (7)  A[0]:(0.545999646187) A[1]:(-0.536430478096) A[2]:(0.435767948627) A[3]:(0.871123313904)\n",
      " state (8)  A[0]:(0.656335115433) A[1]:(0.000239595770836) A[2]:(0.729096412659) A[3]:(0.590467095375)\n",
      " state (9)  A[0]:(0.656298875809) A[1]:(0.810105979443) A[2]:(0.810050964355) A[3]:(0.00017523765564)\n",
      " state (10)  A[0]:(0.729270994663) A[1]:(0.900025963783) A[2]:(-9.82284545898e-05) A[3]:(0.729100227356)\n",
      " state (11)  A[0]:(0.134895443916) A[1]:(0.882749438286) A[2]:(-0.928034484386) A[3]:(0.803233742714)\n",
      " state (12)  A[0]:(-0.427652865648) A[1]:(0.815884232521) A[2]:(-0.952630341053) A[3]:(0.715301811695)\n",
      " state (13)  A[0]:(0.000113025307655) A[1]:(0.809273958206) A[2]:(0.900077581406) A[3]:(0.728893876076)\n",
      " state (14)  A[0]:(0.810119211674) A[1]:(0.900081634521) A[2]:(0.999999821186) A[3]:(0.809830844402)\n",
      " state (15)  A[0]:(0.979761838913) A[1]:(0.938563525677) A[2]:(1.0) A[3]:(0.878049373627)\n",
      "Episode 626000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6008. Times reached goal: 987.               Steps done: 4418398. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0108481847661.\n",
      " state (0)  A[0]:(0.5315990448) A[1]:(0.590674757957) A[2]:(0.590502262115) A[3]:(0.53156042099)\n",
      " state (1)  A[0]:(0.531484544277) A[1]:(0.000185430049896) A[2]:(0.656136929989) A[3]:(0.59047806263)\n",
      " state (2)  A[0]:(0.590506017208) A[1]:(0.728972434998) A[2]:(0.590733528137) A[3]:(0.656106352806)\n",
      " state (3)  A[0]:(0.656198441982) A[1]:(-0.00587200326845) A[2]:(0.525620639324) A[3]:(0.550735414028)\n",
      " state (4)  A[0]:(0.590380430222) A[1]:(0.656039834023) A[2]:(9.71555709839e-05) A[3]:(0.531403064728)\n",
      " state (5)  A[0]:(-0.0347539670765) A[1]:(0.999897360802) A[2]:(-0.822238445282) A[3]:(0.655094146729)\n",
      " state (6)  A[0]:(-0.000185310840607) A[1]:(0.810082018375) A[2]:(-2.63452529907e-05) A[3]:(0.656173348427)\n",
      " state (7)  A[0]:(0.545448184013) A[1]:(-0.536710143089) A[2]:(0.435897827148) A[3]:(0.871235191822)\n",
      " state (8)  A[0]:(0.655830621719) A[1]:(-0.000255405902863) A[2]:(0.729110360146) A[3]:(0.590517878532)\n",
      " state (9)  A[0]:(0.655757725239) A[1]:(0.809987068176) A[2]:(0.810071051121) A[3]:(-0.000383257836802)\n",
      " state (10)  A[0]:(0.728971421719) A[1]:(0.899991095066) A[2]:(0.000238418579102) A[3]:(0.728975176811)\n",
      " state (11)  A[0]:(0.134434461594) A[1]:(0.882761895657) A[2]:(-0.928011119366) A[3]:(0.803308129311)\n",
      " state (12)  A[0]:(-0.428385138512) A[1]:(0.815968990326) A[2]:(-0.952667593956) A[3]:(0.715464234352)\n",
      " state (13)  A[0]:(-0.00120711268391) A[1]:(0.809414565563) A[2]:(0.900087594986) A[3]:(0.729110002518)\n",
      " state (14)  A[0]:(0.809623777866) A[1]:(0.900182962418) A[2]:(0.999999821186) A[3]:(0.810093998909)\n",
      " state (15)  A[0]:(0.979707717896) A[1]:(0.938614249229) A[2]:(1.0) A[3]:(0.878294765949)\n",
      "Episode 627000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6031. Times reached goal: 988.               Steps done: 4424429. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.010782956258.\n",
      " state (0)  A[0]:(0.531830906868) A[1]:(0.590459942818) A[2]:(0.59051835537) A[3]:(0.53163933754)\n",
      " state (1)  A[0]:(0.531919062138) A[1]:(0.000561237276997) A[2]:(0.6561024189) A[3]:(0.590571403503)\n",
      " state (2)  A[0]:(0.590896904469) A[1]:(0.729035019875) A[2]:(0.59062230587) A[3]:(0.656104385853)\n",
      " state (3)  A[0]:(0.656588196754) A[1]:(-0.00561112258583) A[2]:(0.525475740433) A[3]:(0.550721168518)\n",
      " state (4)  A[0]:(0.590945601463) A[1]:(0.655931353569) A[2]:(1.81198120117e-05) A[3]:(0.531384348869)\n",
      " state (5)  A[0]:(-0.0337870195508) A[1]:(0.999897301197) A[2]:(-0.822208404541) A[3]:(0.655009388924)\n",
      " state (6)  A[0]:(0.000967576808762) A[1]:(0.809935748577) A[2]:(-6.74724578857e-05) A[3]:(0.655835986137)\n",
      " state (7)  A[0]:(0.546398878098) A[1]:(-0.536739110947) A[2]:(0.435536831617) A[3]:(0.871098518372)\n",
      " state (8)  A[0]:(0.656776547432) A[1]:(-0.000479698152049) A[2]:(0.728871822357) A[3]:(0.590616464615)\n",
      " state (9)  A[0]:(0.656730055809) A[1]:(0.80994528532) A[2]:(0.810017645359) A[3]:(-0.000185519456863)\n",
      " state (10)  A[0]:(0.72990334034) A[1]:(0.900002241135) A[2]:(0.000223398208618) A[3]:(0.729054808617)\n",
      " state (11)  A[0]:(0.136766299605) A[1]:(0.882832527161) A[2]:(-0.928053438663) A[3]:(0.803341507912)\n",
      " state (12)  A[0]:(-0.426338642836) A[1]:(0.816149353981) A[2]:(-0.952752053738) A[3]:(0.715370953083)\n",
      " state (13)  A[0]:(0.00123199762311) A[1]:(0.80965334177) A[2]:(0.899916052818) A[3]:(0.728882849216)\n",
      " state (14)  A[0]:(0.810406565666) A[1]:(0.900364518166) A[2]:(0.999999821186) A[3]:(0.809833049774)\n",
      " state (15)  A[0]:(0.979793012142) A[1]:(0.938758730888) A[2]:(1.0) A[3]:(0.878064095974)\n",
      "Episode 628000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6021. Times reached goal: 988.               Steps done: 4430450. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.010718227141.\n",
      " state (0)  A[0]:(0.53145813942) A[1]:(0.590047419071) A[2]:(0.590477705002) A[3]:(0.531306028366)\n",
      " state (1)  A[0]:(0.531229019165) A[1]:(-8.78274440765e-05) A[2]:(0.655951738358) A[3]:(0.590393781662)\n",
      " state (2)  A[0]:(0.590388476849) A[1]:(0.728755712509) A[2]:(0.590352892876) A[3]:(0.655896663666)\n",
      " state (3)  A[0]:(0.656087756157) A[1]:(-0.00596548989415) A[2]:(0.5252430439) A[3]:(0.550440609455)\n",
      " state (4)  A[0]:(0.590324103832) A[1]:(0.655929923058) A[2]:(-0.000197410583496) A[3]:(0.531165480614)\n",
      " state (5)  A[0]:(-0.0346557982266) A[1]:(0.999897241592) A[2]:(-0.822199106216) A[3]:(0.655074119568)\n",
      " state (6)  A[0]:(8.57412815094e-05) A[1]:(0.809857189655) A[2]:(1.38282775879e-05) A[3]:(0.655817866325)\n",
      " state (7)  A[0]:(0.545546889305) A[1]:(-0.536323547363) A[2]:(0.435476660728) A[3]:(0.870975613594)\n",
      " state (8)  A[0]:(0.655666828156) A[1]:(0.000252395868301) A[2]:(0.728809714317) A[3]:(0.589844822884)\n",
      " state (9)  A[0]:(0.655421555042) A[1]:(0.810047268867) A[2]:(0.809790968895) A[3]:(-0.000901967054233)\n",
      " state (10)  A[0]:(0.728511929512) A[1]:(0.900000572205) A[2]:(-0.00077927095117) A[3]:(0.728544056416)\n",
      " state (11)  A[0]:(0.133485749364) A[1]:(0.882782042027) A[2]:(-0.928225696087) A[3]:(0.802850604057)\n",
      " state (12)  A[0]:(-0.428793042898) A[1]:(0.81599265337) A[2]:(-0.952834188938) A[3]:(0.714827239513)\n",
      " state (13)  A[0]:(-0.0012782656122) A[1]:(0.809418559074) A[2]:(0.900043725967) A[3]:(0.72860199213)\n",
      " state (14)  A[0]:(0.809619069099) A[1]:(0.90019929409) A[2]:(0.999999821186) A[3]:(0.809789121151)\n",
      " state (15)  A[0]:(0.979701936245) A[1]:(0.938612222672) A[2]:(1.0) A[3]:(0.878117263317)\n",
      "Episode 629000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6043. Times reached goal: 988.               Steps done: 4436493. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.010653652204.\n",
      "q_values \n",
      "tensor([[ 0.5304,  0.5904,  0.5899,  0.5291]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6563, -0.0010,  0.5273]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6553,  0.0011,  0.7291,  0.5795]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6591,  0.8099,  0.8104, -0.0122]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7384,  0.9003,  0.0072,  0.7302]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8161,  0.9001,  1.0000,  0.8110]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530432224274) A[1]:(0.590492963791) A[2]:(0.590625882149) A[3]:(0.532225728035)\n",
      " state (1)  A[0]:(0.53208053112) A[1]:(-0.000382319063647) A[2]:(0.655950903893) A[3]:(0.591269850731)\n",
      " state (2)  A[0]:(0.591539263725) A[1]:(0.728687703609) A[2]:(0.589944243431) A[3]:(0.656789600849)\n",
      " state (3)  A[0]:(0.657349944115) A[1]:(-0.00632505631074) A[2]:(0.524479925632) A[3]:(0.551548957825)\n",
      " state (4)  A[0]:(0.592351615429) A[1]:(0.655470490456) A[2]:(-0.00174271885771) A[3]:(0.53236925602)\n",
      " state (5)  A[0]:(-0.0297478269786) A[1]:(0.999896883965) A[2]:(-0.822924494743) A[3]:(0.65601503849)\n",
      " state (6)  A[0]:(0.00720318593085) A[1]:(0.809247076511) A[2]:(-0.00253652990796) A[3]:(0.657108008862)\n",
      " state (7)  A[0]:(0.552657425404) A[1]:(-0.536759912968) A[2]:(0.433327674866) A[3]:(0.872043073177)\n",
      " state (8)  A[0]:(0.664699912071) A[1]:(-0.000468626589281) A[2]:(0.728243112564) A[3]:(0.59554618597)\n",
      " state (9)  A[0]:(0.668935537338) A[1]:(0.809955954552) A[2]:(0.81116014719) A[3]:(0.0110315075144)\n",
      " state (10)  A[0]:(0.744640231133) A[1]:(0.899859368801) A[2]:(0.0110737327486) A[3]:(0.736658573151)\n",
      " state (11)  A[0]:(0.174020722508) A[1]:(0.882328808308) A[2]:(-0.926091969013) A[3]:(0.809854388237)\n",
      " state (12)  A[0]:(-0.398411989212) A[1]:(0.814741551876) A[2]:(-0.951995790005) A[3]:(0.722334265709)\n",
      " state (13)  A[0]:(0.0283264722675) A[1]:(0.807660460472) A[2]:(0.900234460831) A[3]:(0.733313679695)\n",
      " state (14)  A[0]:(0.818081855774) A[1]:(0.899207353592) A[2]:(0.999999821186) A[3]:(0.812144219875)\n",
      " state (15)  A[0]:(0.980568766594) A[1]:(0.938118934631) A[2]:(1.0) A[3]:(0.87928211689)\n",
      "Episode 630000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6016. Times reached goal: 983.               Steps done: 4442509. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0105897522362.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531429290771) A[1]:(0.590320885181) A[2]:(0.590593218803) A[3]:(0.531455755234)\n",
      " state (1)  A[0]:(0.531616568565) A[1]:(-0.000175788998604) A[2]:(0.656161546707) A[3]:(0.590509295464)\n",
      " state (2)  A[0]:(0.5906727314) A[1]:(0.72887468338) A[2]:(0.590490579605) A[3]:(0.656088232994)\n",
      " state (3)  A[0]:(0.65633058548) A[1]:(-0.00571013754234) A[2]:(0.52545106411) A[3]:(0.550671815872)\n",
      " state (4)  A[0]:(0.590534389019) A[1]:(0.655974984169) A[2]:(0.000104069709778) A[3]:(0.531438291073)\n",
      " state (5)  A[0]:(-0.0346640124917) A[1]:(0.999897241592) A[2]:(-0.822209894657) A[3]:(0.655431389809)\n",
      " state (6)  A[0]:(7.05271959305e-05) A[1]:(0.809862136841) A[2]:(0.000152468681335) A[3]:(0.656207442284)\n",
      " state (7)  A[0]:(0.545565843582) A[1]:(-0.536379694939) A[2]:(0.435736954212) A[3]:(0.87117433548)\n",
      " state (8)  A[0]:(0.656105399132) A[1]:(-0.000143602490425) A[2]:(0.728893995285) A[3]:(0.591033577919)\n",
      " state (9)  A[0]:(0.655994772911) A[1]:(0.809987723827) A[2]:(0.809962153435) A[3]:(0.00040033456753)\n",
      " state (10)  A[0]:(0.72914981842) A[1]:(0.899993300438) A[2]:(-0.000135183334351) A[3]:(0.729085922241)\n",
      " state (11)  A[0]:(0.13522683084) A[1]:(0.882790565491) A[2]:(-0.928231954575) A[3]:(0.8033156991)\n",
      " state (12)  A[0]:(-0.427391707897) A[1]:(0.815996646881) A[2]:(-0.952929854393) A[3]:(0.715427279472)\n",
      " state (13)  A[0]:(0.000182762742043) A[1]:(0.809382855892) A[2]:(0.900016486645) A[3]:(0.729085803032)\n",
      " state (14)  A[0]:(0.810045778751) A[1]:(0.900157511234) A[2]:(0.999999821186) A[3]:(0.810047149658)\n",
      " state (15)  A[0]:(0.979753255844) A[1]:(0.938534319401) A[2]:(1.0) A[3]:(0.878254652023)\n",
      "Episode 631000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6015. Times reached goal: 987.               Steps done: 4448524. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0105262460628.\n",
      " state (0)  A[0]:(0.531136035919) A[1]:(0.590399801731) A[2]:(0.590359568596) A[3]:(0.531540036201)\n",
      " state (1)  A[0]:(0.531153321266) A[1]:(0.000167325139046) A[2]:(0.656028926373) A[3]:(0.590543746948)\n",
      " state (2)  A[0]:(0.590142011642) A[1]:(0.728894770145) A[2]:(0.590484440327) A[3]:(0.65607470274)\n",
      " state (3)  A[0]:(0.655916690826) A[1]:(-0.00578878307715) A[2]:(0.525398492813) A[3]:(0.550584435463)\n",
      " state (4)  A[0]:(0.59010130167) A[1]:(0.655826687813) A[2]:(1.39474868774e-05) A[3]:(0.531321287155)\n",
      " state (5)  A[0]:(-0.035366576165) A[1]:(0.999897181988) A[2]:(-0.822245299816) A[3]:(0.65533387661)\n",
      " state (6)  A[0]:(-0.000372722715838) A[1]:(0.809836864471) A[2]:(0.000192165374756) A[3]:(0.655782938004)\n",
      " state (7)  A[0]:(0.545201063156) A[1]:(-0.536563158035) A[2]:(0.435909628868) A[3]:(0.870873749256)\n",
      " state (8)  A[0]:(0.65560233593) A[1]:(-0.000144764780998) A[2]:(0.729057312012) A[3]:(0.589900612831)\n",
      " state (9)  A[0]:(0.655588328838) A[1]:(0.809983074665) A[2]:(0.810001850128) A[3]:(-0.000520467699971)\n",
      " state (10)  A[0]:(0.72883027792) A[1]:(0.900009632111) A[2]:(-0.00029194355011) A[3]:(0.728831171989)\n",
      " state (11)  A[0]:(0.134514674544) A[1]:(0.882866859436) A[2]:(-0.928325712681) A[3]:(0.803162932396)\n",
      " state (12)  A[0]:(-0.428008288145) A[1]:(0.816178143024) A[2]:(-0.953047275543) A[3]:(0.715244054794)\n",
      " state (13)  A[0]:(-0.000491648854222) A[1]:(0.809593915939) A[2]:(0.899817287922) A[3]:(0.72894346714)\n",
      " state (14)  A[0]:(0.809895217419) A[1]:(0.900280058384) A[2]:(0.999999821186) A[3]:(0.809967279434)\n",
      " state (15)  A[0]:(0.979750812054) A[1]:(0.938597559929) A[2]:(1.0) A[3]:(0.878216683865)\n",
      "Episode 632000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6048. Times reached goal: 994.               Steps done: 4454572. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0104627754552.\n",
      " state (0)  A[0]:(0.531076431274) A[1]:(0.590231597424) A[2]:(0.590400099754) A[3]:(0.531292080879)\n",
      " state (1)  A[0]:(0.531193614006) A[1]:(0.000146150588989) A[2]:(0.656010270119) A[3]:(0.590359270573)\n",
      " state (2)  A[0]:(0.59020113945) A[1]:(0.729038238525) A[2]:(0.590297758579) A[3]:(0.656016945839)\n",
      " state (3)  A[0]:(0.655974388123) A[1]:(-0.00524611445144) A[2]:(0.525194346905) A[3]:(0.550492763519)\n",
      " state (4)  A[0]:(0.590165257454) A[1]:(0.655971765518) A[2]:(-6.85453414917e-05) A[3]:(0.53142541647)\n",
      " state (5)  A[0]:(-0.0354082919657) A[1]:(0.999897241592) A[2]:(-0.822187900543) A[3]:(0.656005144119)\n",
      " state (6)  A[0]:(3.25888395309e-05) A[1]:(0.809975445271) A[2]:(0.00016188621521) A[3]:(0.656223416328)\n",
      " state (7)  A[0]:(0.545523941517) A[1]:(-0.535950064659) A[2]:(0.435665875673) A[3]:(0.870951771736)\n",
      " state (8)  A[0]:(0.655864059925) A[1]:(0.000419139832957) A[2]:(0.728996992111) A[3]:(0.590150117874)\n",
      " state (9)  A[0]:(0.656014800072) A[1]:(0.810002207756) A[2]:(0.809985339642) A[3]:(0.00063353771111)\n",
      " state (10)  A[0]:(0.72910118103) A[1]:(0.900016605854) A[2]:(-0.000439286202891) A[3]:(0.72930508852)\n",
      " state (11)  A[0]:(0.134860530496) A[1]:(0.882886946201) A[2]:(-0.928380846977) A[3]:(0.803379774094)\n",
      " state (12)  A[0]:(-0.427808642387) A[1]:(0.816187739372) A[2]:(-0.953071117401) A[3]:(0.715403914452)\n",
      " state (13)  A[0]:(-0.000394150585635) A[1]:(0.809542834759) A[2]:(0.900052726269) A[3]:(0.729048728943)\n",
      " state (14)  A[0]:(0.809813141823) A[1]:(0.900188744068) A[2]:(0.999999821186) A[3]:(0.810036242008)\n",
      " state (15)  A[0]:(0.979731619358) A[1]:(0.938470423222) A[2]:(1.0) A[3]:(0.878269791603)\n",
      "Episode 633000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6043. Times reached goal: 990.               Steps done: 4460615. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0103997395579.\n",
      " state (0)  A[0]:(0.531706571579) A[1]:(0.590695619583) A[2]:(0.590692162514) A[3]:(0.531754255295)\n",
      " state (1)  A[0]:(0.531813085079) A[1]:(-0.000178977847099) A[2]:(0.656334280968) A[3]:(0.590668201447)\n",
      " state (2)  A[0]:(0.590711832047) A[1]:(0.729107320309) A[2]:(0.590850949287) A[3]:(0.656275987625)\n",
      " state (3)  A[0]:(0.656514763832) A[1]:(-0.00539862317964) A[2]:(0.525672972202) A[3]:(0.550786912441)\n",
      " state (4)  A[0]:(0.590793728828) A[1]:(0.656190752983) A[2]:(1.7523765564e-05) A[3]:(0.531682729721)\n",
      " state (5)  A[0]:(-0.0346321798861) A[1]:(0.999897360802) A[2]:(-0.822452425957) A[3]:(0.656010627747)\n",
      " state (6)  A[0]:(0.000556975544896) A[1]:(0.810062587261) A[2]:(-9.62018966675e-05) A[3]:(0.656445741653)\n",
      " state (7)  A[0]:(0.546099901199) A[1]:(-0.536307513714) A[2]:(0.436016798019) A[3]:(0.871191561222)\n",
      " state (8)  A[0]:(0.656696438789) A[1]:(0.000194862484932) A[2]:(0.729123711586) A[3]:(0.590828597546)\n",
      " state (9)  A[0]:(0.656988143921) A[1]:(0.810111820698) A[2]:(0.8101349473) A[3]:(0.000415325135691)\n",
      " state (10)  A[0]:(0.730281472206) A[1]:(0.900023460388) A[2]:(0.000542163790669) A[3]:(0.729309558868)\n",
      " state (11)  A[0]:(0.137771710753) A[1]:(0.882796108723) A[2]:(-0.928254544735) A[3]:(0.803523659706)\n",
      " state (12)  A[0]:(-0.425660580397) A[1]:(0.815956473351) A[2]:(-0.953060269356) A[3]:(0.715563893318)\n",
      " state (13)  A[0]:(0.00200205785222) A[1]:(0.809325456619) A[2]:(0.900080621243) A[3]:(0.729129552841)\n",
      " state (14)  A[0]:(0.810685515404) A[1]:(0.900157868862) A[2]:(0.999999821186) A[3]:(0.810083091259)\n",
      " state (15)  A[0]:(0.979835271835) A[1]:(0.938507139683) A[2]:(1.0) A[3]:(0.878298640251)\n",
      "Episode 634000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6012. Times reached goal: 985.               Steps done: 4466627. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0103374038924.\n",
      "q_values \n",
      "tensor([[ 0.5319,  0.5908,  0.5903,  0.5319]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.6560,  0.0000,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564, -0.0006,  0.7290,  0.5909]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6563,  0.8099,  0.8101,  0.0002]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0001,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9005,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531817436218) A[1]:(0.590771496296) A[2]:(0.590328931808) A[3]:(0.531673431396)\n",
      " state (1)  A[0]:(0.531639456749) A[1]:(2.80141830444e-06) A[2]:(0.656136274338) A[3]:(0.590397715569)\n",
      " state (2)  A[0]:(0.590577363968) A[1]:(0.729066252708) A[2]:(0.59063911438) A[3]:(0.656020998955)\n",
      " state (3)  A[0]:(0.65643453598) A[1]:(-0.00540495617315) A[2]:(0.525573790073) A[3]:(0.550471067429)\n",
      " state (4)  A[0]:(0.590798079967) A[1]:(0.656069397926) A[2]:(9.90629196167e-05) A[3]:(0.531317830086)\n",
      " state (5)  A[0]:(-0.0344403162599) A[1]:(0.999897301197) A[2]:(-0.822404563427) A[3]:(0.655606389046)\n",
      " state (6)  A[0]:(0.000541329325642) A[1]:(0.81003767252) A[2]:(4.37498092651e-05) A[3]:(0.655913412571)\n",
      " state (7)  A[0]:(0.545862793922) A[1]:(-0.536547660828) A[2]:(0.436049699783) A[3]:(0.870942354202)\n",
      " state (8)  A[0]:(0.656417191029) A[1]:(-0.000655233743601) A[2]:(0.728977739811) A[3]:(0.59071803093)\n",
      " state (9)  A[0]:(0.656146466732) A[1]:(0.809874176979) A[2]:(0.8100233078) A[3]:(-0.000233381986618)\n",
      " state (10)  A[0]:(0.729158520699) A[1]:(0.899996459484) A[2]:(-3.85046005249e-05) A[3]:(0.728816330433)\n",
      " state (11)  A[0]:(0.134958699346) A[1]:(0.882926881313) A[2]:(-0.92839974165) A[3]:(0.80319416523)\n",
      " state (12)  A[0]:(-0.428025454283) A[1]:(0.816373944283) A[2]:(-0.953175187111) A[3]:(0.715228199959)\n",
      " state (13)  A[0]:(-0.000620916427579) A[1]:(0.809895038605) A[2]:(0.899986922741) A[3]:(0.728838443756)\n",
      " state (14)  A[0]:(0.809964954853) A[1]:(0.900502622128) A[2]:(0.999999821186) A[3]:(0.809808373451)\n",
      " state (15)  A[0]:(0.979778945446) A[1]:(0.938706934452) A[2]:(1.0) A[3]:(0.878075897694)\n",
      "Episode 635000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6008. Times reached goal: 991.               Steps done: 4472635. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0102754829666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531505227089) A[1]:(0.590368151665) A[2]:(0.590490221977) A[3]:(0.531486749649)\n",
      " state (1)  A[0]:(0.531488895416) A[1]:(5.41210174561e-05) A[2]:(0.656113743782) A[3]:(0.590522587299)\n",
      " state (2)  A[0]:(0.590590000153) A[1]:(0.729080557823) A[2]:(0.590596318245) A[3]:(0.656149923801)\n",
      " state (3)  A[0]:(0.656393170357) A[1]:(-0.00527782319114) A[2]:(0.525533676147) A[3]:(0.55054295063)\n",
      " state (4)  A[0]:(0.590704262257) A[1]:(0.656102061272) A[2]:(9.48905944824e-05) A[3]:(0.53141105175)\n",
      " state (5)  A[0]:(-0.0346046313643) A[1]:(0.999897360802) A[2]:(-0.822428047657) A[3]:(0.655925452709)\n",
      " state (6)  A[0]:(0.000311717391014) A[1]:(0.810130119324) A[2]:(-0.000196099281311) A[3]:(0.656028449535)\n",
      " state (7)  A[0]:(0.545523881912) A[1]:(-0.536387681961) A[2]:(0.435852468014) A[3]:(0.870868086815)\n",
      " state (8)  A[0]:(0.656205892563) A[1]:(-0.000537872256245) A[2]:(0.728974819183) A[3]:(0.590364098549)\n",
      " state (9)  A[0]:(0.656308710575) A[1]:(0.809834718704) A[2]:(0.8100502491) A[3]:(-2.70009040833e-05)\n",
      " state (10)  A[0]:(0.729353904724) A[1]:(0.90000885725) A[2]:(-3.55243682861e-05) A[3]:(0.729019880295)\n",
      " state (11)  A[0]:(0.135372400284) A[1]:(0.883006632328) A[2]:(-0.92845249176) A[3]:(0.803333520889)\n",
      " state (12)  A[0]:(-0.427703589201) A[1]:(0.816543877125) A[2]:(-0.95323985815) A[3]:(0.715367496014)\n",
      " state (13)  A[0]:(-0.000200748443604) A[1]:(0.810058116913) A[2]:(0.90001642704) A[3]:(0.728965640068)\n",
      " state (14)  A[0]:(0.810136854649) A[1]:(0.900562107563) A[2]:(0.999999821186) A[3]:(0.809920251369)\n",
      " state (15)  A[0]:(0.97980594635) A[1]:(0.938700079918) A[2]:(1.0) A[3]:(0.878169000149)\n",
      "Episode 636000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6039. Times reached goal: 994.               Steps done: 4478674. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0102136163193.\n",
      " state (0)  A[0]:(0.531634807587) A[1]:(0.590293765068) A[2]:(0.590331792831) A[3]:(0.531164884567)\n",
      " state (1)  A[0]:(0.531535506248) A[1]:(0.000181302428246) A[2]:(0.65592366457) A[3]:(0.590403437614)\n",
      " state (2)  A[0]:(0.59033793211) A[1]:(0.728918552399) A[2]:(0.590446591377) A[3]:(0.65585565567)\n",
      " state (3)  A[0]:(0.656093120575) A[1]:(-0.00609189178795) A[2]:(0.525367796421) A[3]:(0.549996495247)\n",
      " state (4)  A[0]:(0.590140104294) A[1]:(0.655885100365) A[2]:(-0.000201225280762) A[3]:(0.530816793442)\n",
      " state (5)  A[0]:(-0.0358282700181) A[1]:(0.999897181988) A[2]:(-0.822374343872) A[3]:(0.655620396137)\n",
      " state (6)  A[0]:(-0.000933378643822) A[1]:(0.809771418571) A[2]:(0.000596284808125) A[3]:(0.655743002892)\n",
      " state (7)  A[0]:(0.544339478016) A[1]:(-0.537213742733) A[2]:(0.436495542526) A[3]:(0.870677232742)\n",
      " state (8)  A[0]:(0.655033588409) A[1]:(-0.00248588109389) A[2]:(0.728799581528) A[3]:(0.590068995953)\n",
      " state (9)  A[0]:(0.654429912567) A[1]:(0.809269070625) A[2]:(0.809726595879) A[3]:(-0.00221350416541)\n",
      " state (10)  A[0]:(0.727780580521) A[1]:(0.899947881699) A[2]:(-0.000980615266599) A[3]:(0.727917790413)\n",
      " state (11)  A[0]:(0.132655352354) A[1]:(0.883289992809) A[2]:(-0.928616940975) A[3]:(0.802809476852)\n",
      " state (12)  A[0]:(-0.429781764746) A[1]:(0.817405343056) A[2]:(-0.953363001347) A[3]:(0.714914619923)\n",
      " state (13)  A[0]:(-0.00260081305169) A[1]:(0.811198949814) A[2]:(0.900024652481) A[3]:(0.728634119034)\n",
      " state (14)  A[0]:(0.809475421906) A[1]:(0.901212215424) A[2]:(0.999999821186) A[3]:(0.809705793858)\n",
      " state (15)  A[0]:(0.979763090611) A[1]:(0.939048826694) A[2]:(1.0) A[3]:(0.878052473068)\n",
      "Episode 637000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6046. Times reached goal: 990.               Steps done: 4484720. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0101520510943.\n",
      " state (0)  A[0]:(0.531648635864) A[1]:(0.590652346611) A[2]:(0.590519487858) A[3]:(0.531469166279)\n",
      " state (1)  A[0]:(0.531552195549) A[1]:(-2.30967998505e-06) A[2]:(0.65619790554) A[3]:(0.5904763937)\n",
      " state (2)  A[0]:(0.590584635735) A[1]:(0.729072213173) A[2]:(0.590643167496) A[3]:(0.656096458435)\n",
      " state (3)  A[0]:(0.656328082085) A[1]:(-0.00528323184699) A[2]:(0.525495648384) A[3]:(0.55046069622)\n",
      " state (4)  A[0]:(0.590599298477) A[1]:(0.655986666679) A[2]:(-1.84774398804e-05) A[3]:(0.531419634819)\n",
      " state (5)  A[0]:(-0.0349105373025) A[1]:(0.999897241592) A[2]:(-0.822469830513) A[3]:(0.656110286713)\n",
      " state (6)  A[0]:(-9.5933675766e-05) A[1]:(0.810011148453) A[2]:(-9.90629196167e-05) A[3]:(0.656103551388)\n",
      " state (7)  A[0]:(0.545196890831) A[1]:(-0.535922348499) A[2]:(0.435815185308) A[3]:(0.870902597904)\n",
      " state (8)  A[0]:(0.655693590641) A[1]:(0.000333622097969) A[2]:(0.728953480721) A[3]:(0.590086400509)\n",
      " state (9)  A[0]:(0.655743539333) A[1]:(0.810019493103) A[2]:(0.810001313686) A[3]:(-0.000478148431284)\n",
      " state (10)  A[0]:(0.729017913342) A[1]:(0.899991571903) A[2]:(-8.38041305542e-05) A[3]:(0.728925704956)\n",
      " state (11)  A[0]:(0.134917855263) A[1]:(0.88282930851) A[2]:(-0.928531885147) A[3]:(0.803298711777)\n",
      " state (12)  A[0]:(-0.428072839975) A[1]:(0.816032767296) A[2]:(-0.9533624053) A[3]:(0.715319752693)\n",
      " state (13)  A[0]:(-0.000797226850409) A[1]:(0.809342265129) A[2]:(0.89998459816) A[3]:(0.728982329369)\n",
      " state (14)  A[0]:(0.809791207314) A[1]:(0.900131821632) A[2]:(0.999999821186) A[3]:(0.810001671314)\n",
      " state (15)  A[0]:(0.97974395752) A[1]:(0.93840187788) A[2]:(1.0) A[3]:(0.878261804581)\n",
      "Episode 638000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6042. Times reached goal: 995.               Steps done: 4490762. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0100908973331.\n",
      " state (0)  A[0]:(0.531435251236) A[1]:(0.590579509735) A[2]:(0.590491652489) A[3]:(0.531412363052)\n",
      " state (1)  A[0]:(0.531483650208) A[1]:(-2.36332416534e-05) A[2]:(0.656153559685) A[3]:(0.590542912483)\n",
      " state (2)  A[0]:(0.59056532383) A[1]:(0.728975713253) A[2]:(0.590456783772) A[3]:(0.656112313271)\n",
      " state (3)  A[0]:(0.656269907951) A[1]:(-0.00538752228022) A[2]:(0.52541077137) A[3]:(0.550475537777)\n",
      " state (4)  A[0]:(0.590533852577) A[1]:(0.656002998352) A[2]:(3.79085540771e-05) A[3]:(0.531492471695)\n",
      " state (5)  A[0]:(-0.0347091555595) A[1]:(0.999897301197) A[2]:(-0.822431981564) A[3]:(0.656317293644)\n",
      " state (6)  A[0]:(0.000214487314224) A[1]:(0.81002676487) A[2]:(3.56435775757e-05) A[3]:(0.656168103218)\n",
      " state (7)  A[0]:(0.545334100723) A[1]:(-0.535978794098) A[2]:(0.436056941748) A[3]:(0.870869278908)\n",
      " state (8)  A[0]:(0.655899941921) A[1]:(0.000264853239059) A[2]:(0.729052066803) A[3]:(0.590250134468)\n",
      " state (9)  A[0]:(0.656015455723) A[1]:(0.810018420219) A[2]:(0.809997677803) A[3]:(6.71744346619e-05)\n",
      " state (10)  A[0]:(0.729102730751) A[1]:(0.899998903275) A[2]:(-0.000347256660461) A[3]:(0.728969693184)\n",
      " state (11)  A[0]:(0.135085001588) A[1]:(0.882842004299) A[2]:(-0.928620100021) A[3]:(0.803200960159)\n",
      " state (12)  A[0]:(-0.427548915148) A[1]:(0.816036701202) A[2]:(-0.953425049782) A[3]:(0.71520537138)\n",
      " state (13)  A[0]:(0.000294744968414) A[1]:(0.809315383434) A[2]:(0.900030136108) A[3]:(0.728987574577)\n",
      " state (14)  A[0]:(0.81021720171) A[1]:(0.900107800961) A[2]:(0.999999821186) A[3]:(0.810076594353)\n",
      " state (15)  A[0]:(0.979794442654) A[1]:(0.938368916512) A[2]:(1.0) A[3]:(0.8783441782)\n",
      "Episode 639000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6012. Times reached goal: 987.               Steps done: 4496774. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0100304128568.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5910,  0.5906,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5908,  0.6559,  0.0001,  0.5318]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565,  0.0009,  0.7290,  0.5908]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565,  0.8102,  0.8101,  0.0007]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0012,  0.8090,  0.9000,  0.7287]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8106,  0.9000,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530857920647) A[1]:(0.591009140015) A[2]:(0.590539395809) A[3]:(0.531748771667)\n",
      " state (1)  A[0]:(0.531004905701) A[1]:(-0.000145152211189) A[2]:(0.656014323235) A[3]:(0.590771734715)\n",
      " state (2)  A[0]:(0.590217709541) A[1]:(0.728957056999) A[2]:(0.59046959877) A[3]:(0.656318426132)\n",
      " state (3)  A[0]:(0.655994713306) A[1]:(-0.00534167280421) A[2]:(0.525472223759) A[3]:(0.550696253777)\n",
      " state (4)  A[0]:(0.590222239494) A[1]:(0.655904889107) A[2]:(0.000195145606995) A[3]:(0.531685471535)\n",
      " state (5)  A[0]:(-0.0354974381626) A[1]:(0.999897241592) A[2]:(-0.822411119938) A[3]:(0.656374692917)\n",
      " state (6)  A[0]:(-0.000379040808184) A[1]:(0.810026466846) A[2]:(0.000181913375854) A[3]:(0.656116485596)\n",
      " state (7)  A[0]:(0.545123577118) A[1]:(-0.535563886166) A[2]:(0.436109751463) A[3]:(0.87088149786)\n",
      " state (8)  A[0]:(0.655763864517) A[1]:(0.000904842978343) A[2]:(0.729062914848) A[3]:(0.590253055096)\n",
      " state (9)  A[0]:(0.655787587166) A[1]:(0.810183465481) A[2]:(0.809997618198) A[3]:(-0.000353306502802)\n",
      " state (10)  A[0]:(0.728882908821) A[1]:(0.900014281273) A[2]:(-0.000382304162486) A[3]:(0.728621244431)\n",
      " state (11)  A[0]:(0.134703963995) A[1]:(0.88277131319) A[2]:(-0.928664028645) A[3]:(0.802852153778)\n",
      " state (12)  A[0]:(-0.427565783262) A[1]:(0.815832734108) A[2]:(-0.953475832939) A[3]:(0.714709758759)\n",
      " state (13)  A[0]:(0.000812575046439) A[1]:(0.809072732925) A[2]:(0.90004491806) A[3]:(0.728561580181)\n",
      " state (14)  A[0]:(0.810564339161) A[1]:(0.900009095669) A[2]:(0.999999821186) A[3]:(0.809794723988)\n",
      " state (15)  A[0]:(0.979844272137) A[1]:(0.938328564167) A[2]:(1.0) A[3]:(0.878163516521)\n",
      "Episode 640000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6015. Times reached goal: 989.               Steps done: 4502789. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00997026101154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53139603138) A[1]:(0.590382397175) A[2]:(0.590516030788) A[3]:(0.533189356327)\n",
      " state (1)  A[0]:(0.531429052353) A[1]:(-0.000696703675203) A[2]:(0.65611499548) A[3]:(0.59185051918)\n",
      " state (2)  A[0]:(0.590580105782) A[1]:(0.729005157948) A[2]:(0.590511202812) A[3]:(0.657377243042)\n",
      " state (3)  A[0]:(0.656231582165) A[1]:(-0.00528382789344) A[2]:(0.525399327278) A[3]:(0.551936149597)\n",
      " state (4)  A[0]:(0.590140104294) A[1]:(0.656152963638) A[2]:(-0.0001460313797) A[3]:(0.532952904701)\n",
      " state (5)  A[0]:(-0.036908172071) A[1]:(0.999897301197) A[2]:(-0.822557806969) A[3]:(0.657445430756)\n",
      " state (6)  A[0]:(-0.00238792155869) A[1]:(0.81006538868) A[2]:(-8.0943107605e-05) A[3]:(0.656971216202)\n",
      " state (7)  A[0]:(0.54340338707) A[1]:(-0.535945892334) A[2]:(0.436204344034) A[3]:(0.871102452278)\n",
      " state (8)  A[0]:(0.654192149639) A[1]:(0.000689700129442) A[2]:(0.729180693626) A[3]:(0.590257585049)\n",
      " state (9)  A[0]:(0.654786467552) A[1]:(0.810021221638) A[2]:(0.809889256954) A[3]:(0.00122544111218)\n",
      " state (10)  A[0]:(0.728346824646) A[1]:(0.899911403656) A[2]:(-0.000925421423744) A[3]:(0.7296397686)\n",
      " state (11)  A[0]:(0.133872762322) A[1]:(0.882654666901) A[2]:(-0.928787589073) A[3]:(0.80355155468)\n",
      " state (12)  A[0]:(-0.428225636482) A[1]:(0.815622866154) A[2]:(-0.953600883484) A[3]:(0.715478003025)\n",
      " state (13)  A[0]:(-0.000182271003723) A[1]:(0.808790326118) A[2]:(0.899840474129) A[3]:(0.729135811329)\n",
      " state (14)  A[0]:(0.810106694698) A[1]:(0.899823606014) A[2]:(0.999999821186) A[3]:(0.810114264488)\n",
      " state (15)  A[0]:(0.97978079319) A[1]:(0.93818706274) A[2]:(1.0) A[3]:(0.878331065178)\n",
      "Episode 641000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6024. Times reached goal: 990.               Steps done: 4508813. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00991038069979.\n",
      " state (0)  A[0]:(0.531431078911) A[1]:(0.590231657028) A[2]:(0.590031147003) A[3]:(0.53106379509)\n",
      " state (1)  A[0]:(0.53200429678) A[1]:(-0.000209167599678) A[2]:(0.656296789646) A[3]:(0.590309381485)\n",
      " state (2)  A[0]:(0.59083378315) A[1]:(0.729352831841) A[2]:(0.590569794178) A[3]:(0.656057476997)\n",
      " state (3)  A[0]:(0.656505167484) A[1]:(-0.00471405172721) A[2]:(0.525454044342) A[3]:(0.55030977726)\n",
      " state (4)  A[0]:(0.590655982494) A[1]:(0.656518399715) A[2]:(-0.000113129615784) A[3]:(0.531528055668)\n",
      " state (5)  A[0]:(-0.0349161624908) A[1]:(0.999897301197) A[2]:(-0.82242757082) A[3]:(0.656929731369)\n",
      " state (6)  A[0]:(0.000745490076952) A[1]:(0.809799253941) A[2]:(0.000539660395589) A[3]:(0.656581938267)\n",
      " state (7)  A[0]:(0.545946776867) A[1]:(-0.536447644234) A[2]:(0.43654859066) A[3]:(0.871004223824)\n",
      " state (8)  A[0]:(0.656588375568) A[1]:(-0.00051434332272) A[2]:(0.729273200035) A[3]:(0.590591311455)\n",
      " state (9)  A[0]:(0.657059550285) A[1]:(0.809642791748) A[2]:(0.810001432896) A[3]:(0.00130397011526)\n",
      " state (10)  A[0]:(0.729916751385) A[1]:(0.899810194969) A[2]:(-0.000977277406491) A[3]:(0.729582965374)\n",
      " state (11)  A[0]:(0.136424943805) A[1]:(0.882665812969) A[2]:(-0.92889881134) A[3]:(0.80352383852)\n",
      " state (12)  A[0]:(-0.426690518856) A[1]:(0.815772473812) A[2]:(-0.953728020191) A[3]:(0.715474188328)\n",
      " state (13)  A[0]:(0.000953376002144) A[1]:(0.808977901936) A[2]:(0.899650156498) A[3]:(0.729151725769)\n",
      " state (14)  A[0]:(0.810247182846) A[1]:(0.899899244308) A[2]:(0.999999821186) A[3]:(0.810120046139)\n",
      " state (15)  A[0]:(0.979785919189) A[1]:(0.938188195229) A[2]:(1.0) A[3]:(0.878335773945)\n",
      "Episode 642000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6027. Times reached goal: 988.               Steps done: 4514840. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00985083047019.\n",
      " state (0)  A[0]:(0.533249020576) A[1]:(0.590545535088) A[2]:(0.590388715267) A[3]:(0.531074106693)\n",
      " state (1)  A[0]:(0.533595204353) A[1]:(-0.000240802764893) A[2]:(0.656103849411) A[3]:(0.590192377567)\n",
      " state (2)  A[0]:(0.592516243458) A[1]:(0.729123592377) A[2]:(0.59056198597) A[3]:(0.655943155289)\n",
      " state (3)  A[0]:(0.658028244972) A[1]:(-0.00515670981258) A[2]:(0.525525093079) A[3]:(0.550221323967)\n",
      " state (4)  A[0]:(0.592428147793) A[1]:(0.65606957674) A[2]:(-2.44379043579e-05) A[3]:(0.531477570534)\n",
      " state (5)  A[0]:(-0.0327096991241) A[1]:(0.999897241592) A[2]:(-0.822679102421) A[3]:(0.656925916672)\n",
      " state (6)  A[0]:(0.00199047964998) A[1]:(0.809955298901) A[2]:(-0.000370860070689) A[3]:(0.656758368015)\n",
      " state (7)  A[0]:(0.546329498291) A[1]:(-0.536386370659) A[2]:(0.435914069414) A[3]:(0.871135354042)\n",
      " state (8)  A[0]:(0.656396865845) A[1]:(-0.000853299861774) A[2]:(0.728770017624) A[3]:(0.591325819492)\n",
      " state (9)  A[0]:(0.655937433243) A[1]:(0.809664607048) A[2]:(0.809891581535) A[3]:(0.00113275600597)\n",
      " state (10)  A[0]:(0.72919768095) A[1]:(0.899880945683) A[2]:(0.000107884407043) A[3]:(0.729881048203)\n",
      " state (11)  A[0]:(0.135446727276) A[1]:(0.882847309113) A[2]:(-0.928666055202) A[3]:(0.804257690907)\n",
      " state (12)  A[0]:(-0.428184658289) A[1]:(0.816215515137) A[2]:(-0.953621387482) A[3]:(0.716616153717)\n",
      " state (13)  A[0]:(-0.00147245719563) A[1]:(0.809637784958) A[2]:(0.9000030756) A[3]:(0.730153620243)\n",
      " state (14)  A[0]:(0.809537649155) A[1]:(0.900373935699) A[2]:(0.999999821186) A[3]:(0.810818552971)\n",
      " state (15)  A[0]:(0.979726314545) A[1]:(0.938510477543) A[2]:(1.0) A[3]:(0.87880218029)\n",
      "Episode 643000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6026. Times reached goal: 990.               Steps done: 4520866. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00979164786206.\n",
      " state (0)  A[0]:(0.531453311443) A[1]:(0.590608298779) A[2]:(0.590574145317) A[3]:(0.531426310539)\n",
      " state (1)  A[0]:(0.531464576721) A[1]:(0.000170558691025) A[2]:(0.656128108501) A[3]:(0.590533435345)\n",
      " state (2)  A[0]:(0.590493381023) A[1]:(0.728986620903) A[2]:(0.590549349785) A[3]:(0.656116127968)\n",
      " state (3)  A[0]:(0.656207323074) A[1]:(-0.00538501888514) A[2]:(0.525507211685) A[3]:(0.550322651863)\n",
      " state (4)  A[0]:(0.590314745903) A[1]:(0.656251430511) A[2]:(-8.18967819214e-05) A[3]:(0.531515717506)\n",
      " state (5)  A[0]:(-0.03554873541) A[1]:(0.999897301197) A[2]:(-0.822632610798) A[3]:(0.656884849072)\n",
      " state (6)  A[0]:(-6.15268945694e-05) A[1]:(0.81003421545) A[2]:(-0.000109195709229) A[3]:(0.656318664551)\n",
      " state (7)  A[0]:(0.545341610909) A[1]:(-0.536063194275) A[2]:(0.436177611351) A[3]:(0.870934426785)\n",
      " state (8)  A[0]:(0.656242907047) A[1]:(-0.000193506479263) A[2]:(0.728898286819) A[3]:(0.591244459152)\n",
      " state (9)  A[0]:(0.656224012375) A[1]:(0.809955239296) A[2]:(0.809925556183) A[3]:(0.000971853442024)\n",
      " state (10)  A[0]:(0.729333162308) A[1]:(0.899965703487) A[2]:(-0.000407457322581) A[3]:(0.729360580444)\n",
      " state (11)  A[0]:(0.135659694672) A[1]:(0.882779300213) A[2]:(-0.928839564323) A[3]:(0.803495526314)\n",
      " state (12)  A[0]:(-0.427387148142) A[1]:(0.815852463245) A[2]:(-0.953763961792) A[3]:(0.715423583984)\n",
      " state (13)  A[0]:(0.000216826796532) A[1]:(0.809053301811) A[2]:(0.899762272835) A[3]:(0.729082763195)\n",
      " state (14)  A[0]:(0.810154080391) A[1]:(0.900009334087) A[2]:(0.999999821186) A[3]:(0.810094714165)\n",
      " state (15)  A[0]:(0.979787230492) A[1]:(0.938279926777) A[2]:(1.0) A[3]:(0.878339707851)\n",
      "Episode 644000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6020. Times reached goal: 991.               Steps done: 4526886. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00973287921305.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5904,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5311, -0.0001,  0.6560,  0.5906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.7291,  0.5904,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? True\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0005,  0.8099, -0.0001,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0001,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9001,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.5312936306) A[1]:(0.590339660645) A[2]:(0.590438485146) A[3]:(0.531494259834)\n",
      " state (1)  A[0]:(0.531392753124) A[1]:(1.02370977402e-05) A[2]:(0.655993580818) A[3]:(0.590600371361)\n",
      " state (2)  A[0]:(0.590381205082) A[1]:(0.728972911835) A[2]:(0.590328812599) A[3]:(0.656116485596)\n",
      " state (3)  A[0]:(0.656104207039) A[1]:(-0.00533648720011) A[2]:(0.525316476822) A[3]:(0.550327420235)\n",
      " state (4)  A[0]:(0.590262949467) A[1]:(0.655981719494) A[2]:(-0.000158429145813) A[3]:(0.531521916389)\n",
      " state (5)  A[0]:(-0.0356255285442) A[1]:(0.999897241592) A[2]:(-0.822668254375) A[3]:(0.656840205193)\n",
      " state (6)  A[0]:(-0.000276923179626) A[1]:(0.809961974621) A[2]:(-0.000126838684082) A[3]:(0.655959129333)\n",
      " state (7)  A[0]:(0.545067429543) A[1]:(-0.535921752453) A[2]:(0.436141699553) A[3]:(0.870682954788)\n",
      " state (8)  A[0]:(0.655880868435) A[1]:(1.07139348984e-05) A[2]:(0.72892332077) A[3]:(0.590514600277)\n",
      " state (9)  A[0]:(0.655828416348) A[1]:(0.809988260269) A[2]:(0.809961438179) A[3]:(0.000105053186417)\n",
      " state (10)  A[0]:(0.729074299335) A[1]:(0.899978637695) A[2]:(-0.00032901763916) A[3]:(0.728982329369)\n",
      " state (11)  A[0]:(0.135427296162) A[1]:(0.882804632187) A[2]:(-0.928857803345) A[3]:(0.803276658058)\n",
      " state (12)  A[0]:(-0.427413731813) A[1]:(0.815896570683) A[2]:(-0.95378267765) A[3]:(0.715228378773)\n",
      " state (13)  A[0]:(-2.65091657639e-05) A[1]:(0.809111058712) A[2]:(0.899925649166) A[3]:(0.728960514069)\n",
      " state (14)  A[0]:(0.809838652611) A[1]:(0.900058031082) A[2]:(0.999999821186) A[3]:(0.810011029243)\n",
      " state (15)  A[0]:(0.979723572731) A[1]:(0.938300430775) A[2]:(1.0) A[3]:(0.878281950951)\n",
      "Episode 645000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6037. Times reached goal: 989.               Steps done: 4532923. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00967429882406.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531172335148) A[1]:(0.590159118176) A[2]:(0.590271115303) A[3]:(0.531122088432)\n",
      " state (1)  A[0]:(0.531224012375) A[1]:(-6.03497028351e-06) A[2]:(0.655975699425) A[3]:(0.590207993984)\n",
      " state (2)  A[0]:(0.590201497078) A[1]:(0.728960037231) A[2]:(0.590400576591) A[3]:(0.655872166157)\n",
      " state (3)  A[0]:(0.656004309654) A[1]:(-0.00525405630469) A[2]:(0.525431871414) A[3]:(0.549978971481)\n",
      " state (4)  A[0]:(0.590256690979) A[1]:(0.656031370163) A[2]:(4.41074371338e-06) A[3]:(0.531163215637)\n",
      " state (5)  A[0]:(-0.0353552512825) A[1]:(0.999897241592) A[2]:(-0.822677195072) A[3]:(0.65667617321)\n",
      " state (6)  A[0]:(0.000101685523987) A[1]:(0.809982061386) A[2]:(-0.000101685523987) A[3]:(0.655832290649)\n",
      " state (7)  A[0]:(0.54530030489) A[1]:(-0.535961151123) A[2]:(0.436270177364) A[3]:(0.870607614517)\n",
      " state (8)  A[0]:(0.656045973301) A[1]:(8.45640897751e-05) A[2]:(0.728978455067) A[3]:(0.59024643898)\n",
      " state (9)  A[0]:(0.65603530407) A[1]:(0.810038328171) A[2]:(0.809985816479) A[3]:(-6.83069229126e-05)\n",
      " state (10)  A[0]:(0.729203522205) A[1]:(0.900003790855) A[2]:(-5.12599945068e-06) A[3]:(0.729001164436)\n",
      " state (11)  A[0]:(0.135410815477) A[1]:(0.882847607136) A[2]:(-0.928827524185) A[3]:(0.803342878819)\n",
      " state (12)  A[0]:(-0.427686601877) A[1]:(0.815991520882) A[2]:(-0.953795552254) A[3]:(0.71529006958)\n",
      " state (13)  A[0]:(-6.34789466858e-05) A[1]:(0.809269070625) A[2]:(0.900035381317) A[3]:(0.728959798813)\n",
      " state (14)  A[0]:(0.810144782066) A[1]:(0.900204002857) A[2]:(0.999999821186) A[3]:(0.809958696365)\n",
      " state (15)  A[0]:(0.979792892933) A[1]:(0.938406229019) A[2]:(1.0) A[3]:(0.878223657608)\n",
      "Episode 646000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6024. Times reached goal: 994.               Steps done: 4538947. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00961619602927.\n",
      " state (0)  A[0]:(0.531802177429) A[1]:(0.590737164021) A[2]:(0.590554893017) A[3]:(0.531763017178)\n",
      " state (1)  A[0]:(0.531737685204) A[1]:(6.28232955933e-05) A[2]:(0.65624833107) A[3]:(0.590602457523)\n",
      " state (2)  A[0]:(0.590512514114) A[1]:(0.729079544544) A[2]:(0.590679526329) A[3]:(0.656262993813)\n",
      " state (3)  A[0]:(0.656311869621) A[1]:(-0.00529630016536) A[2]:(0.525591850281) A[3]:(0.550417244434)\n",
      " state (4)  A[0]:(0.590628147125) A[1]:(0.65606367588) A[2]:(-5.60283660889e-06) A[3]:(0.53171145916)\n",
      " state (5)  A[0]:(-0.0349188856781) A[1]:(0.999897241592) A[2]:(-0.822702884674) A[3]:(0.657350301743)\n",
      " state (6)  A[0]:(-0.000206753611565) A[1]:(0.809955716133) A[2]:(0.000177502632141) A[3]:(0.656397819519)\n",
      " state (7)  A[0]:(0.544836640358) A[1]:(-0.53641885519) A[2]:(0.436695367098) A[3]:(0.870817840099)\n",
      " state (8)  A[0]:(0.655918836594) A[1]:(-0.000788330857176) A[2]:(0.729020774364) A[3]:(0.59081351757)\n",
      " state (9)  A[0]:(0.655827999115) A[1]:(0.809860169888) A[2]:(0.809933066368) A[3]:(-0.000494658888783)\n",
      " state (10)  A[0]:(0.728911280632) A[1]:(0.899968504906) A[2]:(-0.000532865466084) A[3]:(0.728744387627)\n",
      " state (11)  A[0]:(0.134865313768) A[1]:(0.882873415947) A[2]:(-0.9289701581) A[3]:(0.803314745426)\n",
      " state (12)  A[0]:(-0.42790132761) A[1]:(0.816104054451) A[2]:(-0.953903198242) A[3]:(0.715484976768)\n",
      " state (13)  A[0]:(-0.000236496329308) A[1]:(0.809432923794) A[2]:(0.899981677532) A[3]:(0.729211091995)\n",
      " state (14)  A[0]:(0.810003817081) A[1]:(0.90031260252) A[2]:(0.999999821186) A[3]:(0.810055017471)\n",
      " state (15)  A[0]:(0.979767918587) A[1]:(0.938462257385) A[2]:(1.0) A[3]:(0.878224372864)\n",
      "Episode 647000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6017. Times reached goal: 991.               Steps done: 4544964. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00955850910292.\n",
      " state (0)  A[0]:(0.531641304493) A[1]:(0.590531468391) A[2]:(0.590408444405) A[3]:(0.532117605209)\n",
      " state (1)  A[0]:(0.531028151512) A[1]:(-0.000165343284607) A[2]:(0.656259596348) A[3]:(0.591430366039)\n",
      " state (2)  A[0]:(0.590220272541) A[1]:(0.729216516018) A[2]:(0.590803980827) A[3]:(0.657221078873)\n",
      " state (3)  A[0]:(0.656128406525) A[1]:(-0.00463208230212) A[2]:(0.525775194168) A[3]:(0.551760435104)\n",
      " state (4)  A[0]:(0.590719938278) A[1]:(0.65635740757) A[2]:(0.000264763832092) A[3]:(0.533210158348)\n",
      " state (5)  A[0]:(-0.0334521234035) A[1]:(0.999897360802) A[2]:(-0.822726666927) A[3]:(0.658512830734)\n",
      " state (6)  A[0]:(0.00261359801516) A[1]:(0.810133755207) A[2]:(-0.000468015641673) A[3]:(0.657400488853)\n",
      " state (7)  A[0]:(0.547179222107) A[1]:(-0.535468935966) A[2]:(0.436085730791) A[3]:(0.871144294739)\n",
      " state (8)  A[0]:(0.657561779022) A[1]:(0.00169265107252) A[2]:(0.729325771332) A[3]:(0.590048789978)\n",
      " state (9)  A[0]:(0.6592233181) A[1]:(0.810293316841) A[2]:(0.810226678848) A[3]:(0.00276231067255)\n",
      " state (10)  A[0]:(0.732653975487) A[1]:(0.900026738644) A[2]:(0.00130283762701) A[3]:(0.731558799744)\n",
      " state (11)  A[0]:(0.14229349792) A[1]:(0.882778525352) A[2]:(-0.928709149361) A[3]:(0.805454969406)\n",
      " state (12)  A[0]:(-0.423997044563) A[1]:(0.815724551678) A[2]:(-0.953894555569) A[3]:(0.717656970024)\n",
      " state (13)  A[0]:(0.00218024500646) A[1]:(0.808855056763) A[2]:(0.899825930595) A[3]:(0.730693340302)\n",
      " state (14)  A[0]:(0.810530900955) A[1]:(0.899929523468) A[2]:(0.999999821186) A[3]:(0.811028957367)\n",
      " state (15)  A[0]:(0.979825258255) A[1]:(0.93817293644) A[2]:(1.0) A[3]:(0.878894507885)\n",
      "Episode 648000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6022. Times reached goal: 986.               Steps done: 4550986. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00950112073092.\n",
      " state (0)  A[0]:(0.53132712841) A[1]:(0.590401291847) A[2]:(0.590774297714) A[3]:(0.531477868557)\n",
      " state (1)  A[0]:(0.531359612942) A[1]:(3.12924385071e-05) A[2]:(0.656080842018) A[3]:(0.590579748154)\n",
      " state (2)  A[0]:(0.590482234955) A[1]:(0.728866457939) A[2]:(0.59055519104) A[3]:(0.656214773655)\n",
      " state (3)  A[0]:(0.656199753284) A[1]:(-0.00555646652356) A[2]:(0.525471329689) A[3]:(0.5502409935)\n",
      " state (4)  A[0]:(0.590472579002) A[1]:(0.655857741833) A[2]:(-0.000108480453491) A[3]:(0.531486988068)\n",
      " state (5)  A[0]:(-0.0350260734558) A[1]:(0.999897181988) A[2]:(-0.822791874409) A[3]:(0.657272934914)\n",
      " state (6)  A[0]:(2.53021717072e-05) A[1]:(0.809976518154) A[2]:(-0.000224828720093) A[3]:(0.656165659428)\n",
      " state (7)  A[0]:(0.545076847076) A[1]:(-0.535849571228) A[2]:(0.436315268278) A[3]:(0.870711922646)\n",
      " state (8)  A[0]:(0.655954122543) A[1]:(0.000126421451569) A[2]:(0.728931903839) A[3]:(0.59066671133)\n",
      " state (9)  A[0]:(0.655745625496) A[1]:(0.810075879097) A[2]:(0.809949338436) A[3]:(-0.000206977128983)\n",
      " state (10)  A[0]:(0.72875058651) A[1]:(0.899986684322) A[2]:(-0.000416636437876) A[3]:(0.728802800179)\n",
      " state (11)  A[0]:(0.134268283844) A[1]:(0.882745146751) A[2]:(-0.929032385349) A[3]:(0.803185582161)\n",
      " state (12)  A[0]:(-0.428357630968) A[1]:(0.815686523914) A[2]:(-0.953998267651) A[3]:(0.715161621571)\n",
      " state (13)  A[0]:(-0.000170841813087) A[1]:(0.808858811855) A[2]:(0.900033891201) A[3]:(0.728960156441)\n",
      " state (14)  A[0]:(0.810331702232) A[1]:(0.89999294281) A[2]:(0.999999821186) A[3]:(0.809991240501)\n",
      " state (15)  A[0]:(0.979827821255) A[1]:(0.938247263432) A[2]:(1.0) A[3]:(0.878253519535)\n",
      "Episode 649000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6030. Times reached goal: 988.               Steps done: 4557016. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00944400136089.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5904,  0.5907,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.0001,  0.6562,  0.5906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.7289,  0.5907,  0.6563]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  0.8099, -0.0001,  0.6564]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000, -0.0003,  0.7294]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9001,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531819701195) A[1]:(0.590381741524) A[2]:(0.59058380127) A[3]:(0.532048106194)\n",
      " state (1)  A[0]:(0.531839728355) A[1]:(-7.8484416008e-05) A[2]:(0.656185030937) A[3]:(0.59109967947)\n",
      " state (2)  A[0]:(0.590848147869) A[1]:(0.72887814045) A[2]:(0.590874969959) A[3]:(0.656638860703)\n",
      " state (3)  A[0]:(0.656495928764) A[1]:(-0.00564788235351) A[2]:(0.525855064392) A[3]:(0.550763010979)\n",
      " state (4)  A[0]:(0.590736031532) A[1]:(0.65602016449) A[2]:(0.000321269035339) A[3]:(0.532109022141)\n",
      " state (5)  A[0]:(-0.034584403038) A[1]:(0.999897241592) A[2]:(-0.822658002377) A[3]:(0.657919049263)\n",
      " state (6)  A[0]:(0.000657319906168) A[1]:(0.810011744499) A[2]:(0.000382065743906) A[3]:(0.656708955765)\n",
      " state (7)  A[0]:(0.545487523079) A[1]:(-0.536012172699) A[2]:(0.436965078115) A[3]:(0.870882213116)\n",
      " state (8)  A[0]:(0.65636074543) A[1]:(-0.00014166533947) A[2]:(0.729240179062) A[3]:(0.591120839119)\n",
      " state (9)  A[0]:(0.656479239464) A[1]:(0.809961080551) A[2]:(0.810075998306) A[3]:(0.00114977313206)\n",
      " state (10)  A[0]:(0.729514241219) A[1]:(0.899988651276) A[2]:(-4.75645065308e-05) A[3]:(0.729496955872)\n",
      " state (11)  A[0]:(0.135811120272) A[1]:(0.882843613625) A[2]:(-0.929024338722) A[3]:(0.803615808487)\n",
      " state (12)  A[0]:(-0.427606016397) A[1]:(0.815936267376) A[2]:(-0.954038381577) A[3]:(0.715495228767)\n",
      " state (13)  A[0]:(-8.824467659e-05) A[1]:(0.809156477451) A[2]:(0.900068223476) A[3]:(0.729093670845)\n",
      " state (14)  A[0]:(0.81010389328) A[1]:(0.9001480937) A[2]:(0.999999821186) A[3]:(0.810032904148)\n",
      " state (15)  A[0]:(0.979787051678) A[1]:(0.938309550285) A[2]:(1.0) A[3]:(0.87827527523)\n",
      "Episode 650000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6038. Times reached goal: 994.               Steps done: 4563054. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00938715028679.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531298339367) A[1]:(0.590513646603) A[2]:(0.590460896492) A[3]:(0.531587421894)\n",
      " state (1)  A[0]:(0.531325340271) A[1]:(-0.000281155109406) A[2]:(0.65620803833) A[3]:(0.590759634972)\n",
      " state (2)  A[0]:(0.590383410454) A[1]:(0.728862583637) A[2]:(0.590747237206) A[3]:(0.6564270854)\n",
      " state (3)  A[0]:(0.656083762646) A[1]:(-0.00532365776598) A[2]:(0.525646567345) A[3]:(0.550640106201)\n",
      " state (4)  A[0]:(0.590278625488) A[1]:(0.656365394592) A[2]:(-0.000166893005371) A[3]:(0.532132148743)\n",
      " state (5)  A[0]:(-0.0353795960546) A[1]:(0.999897181988) A[2]:(-0.822878241539) A[3]:(0.657960057259)\n",
      " state (6)  A[0]:(-0.000121161341667) A[1]:(0.809904515743) A[2]:(-0.000520467699971) A[3]:(0.656753718853)\n",
      " state (7)  A[0]:(0.545270502567) A[1]:(-0.535008728504) A[2]:(0.43581789732) A[3]:(0.871039509773)\n",
      " state (8)  A[0]:(0.656086564064) A[1]:(0.00179737608414) A[2]:(0.728836894035) A[3]:(0.591502070427)\n",
      " state (9)  A[0]:(0.656340360641) A[1]:(0.810429036617) A[2]:(0.809909462929) A[3]:(0.00253560603596)\n",
      " state (10)  A[0]:(0.729576706886) A[1]:(0.899966716766) A[2]:(-0.000315308570862) A[3]:(0.730143666267)\n",
      " state (11)  A[0]:(0.136219337583) A[1]:(0.882422626019) A[2]:(-0.929066300392) A[3]:(0.803918361664)\n",
      " state (12)  A[0]:(-0.426958829165) A[1]:(0.814771413803) A[2]:(-0.954092025757) A[3]:(0.715775132179)\n",
      " state (13)  A[0]:(0.000729545834474) A[1]:(0.807609081268) A[2]:(0.899943709373) A[3]:(0.729346990585)\n",
      " state (14)  A[0]:(0.810139298439) A[1]:(0.899257361889) A[2]:(0.999999821186) A[3]:(0.810215711594)\n",
      " state (15)  A[0]:(0.979743123055) A[1]:(0.937781572342) A[2]:(1.0) A[3]:(0.878380060196)\n",
      "Episode 651000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6023. Times reached goal: 988.               Steps done: 4569077. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0093307814059.\n",
      " state (0)  A[0]:(0.530825614929) A[1]:(0.590201497078) A[2]:(0.59039747715) A[3]:(0.531382203102)\n",
      " state (1)  A[0]:(0.530965209007) A[1]:(1.43647193909e-05) A[2]:(0.65566265583) A[3]:(0.590312719345)\n",
      " state (2)  A[0]:(0.589854121208) A[1]:(0.728422999382) A[2]:(0.590132594109) A[3]:(0.655857920647)\n",
      " state (3)  A[0]:(0.655668497086) A[1]:(-0.00614891666919) A[2]:(0.525204896927) A[3]:(0.549904704094)\n",
      " state (4)  A[0]:(0.590017199516) A[1]:(0.655323624611) A[2]:(-9.52482223511e-05) A[3]:(0.531291246414)\n",
      " state (5)  A[0]:(-0.0353736020625) A[1]:(0.999897122383) A[2]:(-0.822825551033) A[3]:(0.657192647457)\n",
      " state (6)  A[0]:(-9.27150249481e-05) A[1]:(0.810062527657) A[2]:(-0.000590085925069) A[3]:(0.655393958092)\n",
      " state (7)  A[0]:(0.544842481613) A[1]:(-0.535395979881) A[2]:(0.435764104128) A[3]:(0.87025564909)\n",
      " state (8)  A[0]:(0.655532479286) A[1]:(-8.76784324646e-05) A[2]:(0.728529512882) A[3]:(0.590458452702)\n",
      " state (9)  A[0]:(0.654706716537) A[1]:(0.809987902641) A[2]:(0.80992436409) A[3]:(-0.000704795005731)\n",
      " state (10)  A[0]:(0.728116750717) A[1]:(0.900018572807) A[2]:(0.000216960906982) A[3]:(0.728584647179)\n",
      " state (11)  A[0]:(0.133663117886) A[1]:(0.882900834084) A[2]:(-0.929022789001) A[3]:(0.803180277348)\n",
      " state (12)  A[0]:(-0.429086953402) A[1]:(0.816028594971) A[2]:(-0.954115986824) A[3]:(0.715082883835)\n",
      " state (13)  A[0]:(-0.00154486170504) A[1]:(0.809242486954) A[2]:(0.900024473667) A[3]:(0.728770256042)\n",
      " state (14)  A[0]:(0.809791088104) A[1]:(0.900213718414) A[2]:(0.999999821186) A[3]:(0.809805214405)\n",
      " state (15)  A[0]:(0.979771614075) A[1]:(0.938337981701) A[2]:(1.0) A[3]:(0.878129839897)\n",
      "Episode 652000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6037. Times reached goal: 995.               Steps done: 4575114. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00927462116879.\n",
      " state (0)  A[0]:(0.53143632412) A[1]:(0.59029519558) A[2]:(0.590795397758) A[3]:(0.531169593334)\n",
      " state (1)  A[0]:(0.53166782856) A[1]:(-0.000502303184476) A[2]:(0.656272292137) A[3]:(0.590359926224)\n",
      " state (2)  A[0]:(0.590720713139) A[1]:(0.728988051414) A[2]:(0.59058535099) A[3]:(0.6560587883)\n",
      " state (3)  A[0]:(0.656408905983) A[1]:(-0.00471774721518) A[2]:(0.525491833687) A[3]:(0.550066351891)\n",
      " state (4)  A[0]:(0.590684890747) A[1]:(0.656540751457) A[2]:(-0.00019896030426) A[3]:(0.531482815742)\n",
      " state (5)  A[0]:(-0.0348671525717) A[1]:(0.999897122383) A[2]:(-0.822834551334) A[3]:(0.657474637032)\n",
      " state (6)  A[0]:(0.000784456555266) A[1]:(0.809729576111) A[2]:(-0.000483632058604) A[3]:(0.655744314194)\n",
      " state (7)  A[0]:(0.545877099037) A[1]:(-0.534703493118) A[2]:(0.43534719944) A[3]:(0.87043774128)\n",
      " state (8)  A[0]:(0.656664013863) A[1]:(0.00149135186803) A[2]:(0.728354454041) A[3]:(0.590265870094)\n",
      " state (9)  A[0]:(0.657003045082) A[1]:(0.810330569744) A[2]:(0.809842407703) A[3]:(0.000118523836136)\n",
      " state (10)  A[0]:(0.73107022047) A[1]:(0.900005400181) A[2]:(0.00113725615665) A[3]:(0.729604482651)\n",
      " state (11)  A[0]:(0.141260072589) A[1]:(0.882619202137) A[2]:(-0.92881244421) A[3]:(0.804144024849)\n",
      " state (12)  A[0]:(-0.422928571701) A[1]:(0.815215885639) A[2]:(-0.954032719135) A[3]:(0.716197490692)\n",
      " state (13)  A[0]:(0.00510448263958) A[1]:(0.808138608932) A[2]:(0.900213241577) A[3]:(0.729595839977)\n",
      " state (14)  A[0]:(0.811675906181) A[1]:(0.899565756321) A[2]:(0.999999821186) A[3]:(0.810324192047)\n",
      " state (15)  A[0]:(0.979938268661) A[1]:(0.937936902046) A[2]:(1.0) A[3]:(0.878445088863)\n",
      "Episode 653000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6045. Times reached goal: 994.               Steps done: 4581159. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00921872519961.\n",
      " state (0)  A[0]:(0.531386494637) A[1]:(0.590962290764) A[2]:(0.590475380421) A[3]:(0.53132212162)\n",
      " state (1)  A[0]:(0.531453728676) A[1]:(0.000420078606112) A[2]:(0.656263530254) A[3]:(0.590479016304)\n",
      " state (2)  A[0]:(0.590409040451) A[1]:(0.729198396206) A[2]:(0.590675115585) A[3]:(0.656105160713)\n",
      " state (3)  A[0]:(0.656228065491) A[1]:(-0.00468204496428) A[2]:(0.525730133057) A[3]:(0.550078630447)\n",
      " state (4)  A[0]:(0.59048384428) A[1]:(0.656593322754) A[2]:(0.000255703926086) A[3]:(0.531564712524)\n",
      " state (5)  A[0]:(-0.0352725610137) A[1]:(0.999897420406) A[2]:(-0.822756290436) A[3]:(0.657920598984)\n",
      " state (6)  A[0]:(2.04741954803e-05) A[1]:(0.810268044472) A[2]:(0.00037062165211) A[3]:(0.656470298767)\n",
      " state (7)  A[0]:(0.544961929321) A[1]:(-0.535486698151) A[2]:(0.437115907669) A[3]:(0.870695471764)\n",
      " state (8)  A[0]:(0.656069993973) A[1]:(0.000391945213778) A[2]:(0.729259729385) A[3]:(0.590961396694)\n",
      " state (9)  A[0]:(0.656152129173) A[1]:(0.810204684734) A[2]:(0.810162365437) A[3]:(0.000615775526967)\n",
      " state (10)  A[0]:(0.729317367077) A[1]:(0.900118947029) A[2]:(0.000346422195435) A[3]:(0.729312777519)\n",
      " state (11)  A[0]:(0.135363548994) A[1]:(0.883017241955) A[2]:(-0.929129242897) A[3]:(0.803621888161)\n",
      " state (12)  A[0]:(-0.428452372551) A[1]:(0.816235899925) A[2]:(-0.954221129417) A[3]:(0.715525865555)\n",
      " state (13)  A[0]:(-0.00127233495004) A[1]:(0.809529185295) A[2]:(0.900259613991) A[3]:(0.729134082794)\n",
      " state (14)  A[0]:(0.809893250465) A[1]:(0.900386750698) A[2]:(0.999999821186) A[3]:(0.810083389282)\n",
      " state (15)  A[0]:(0.97980183363) A[1]:(0.938371598721) A[2]:(1.0) A[3]:(0.878355801105)\n",
      "Episode 654000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6024. Times reached goal: 990.               Steps done: 4587183. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00916335853084.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5904,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.3134e-01, -2.5183e-06,  6.5614e-01,  5.9042e-01]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7290,  0.5905,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-9.5576e-05,  8.1004e-01, -2.3842e-07,  6.5597e-01]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0003,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9003,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531449019909) A[1]:(0.590487957001) A[2]:(0.590557336807) A[3]:(0.531374514103)\n",
      " state (1)  A[0]:(0.531482338905) A[1]:(1.6838312149e-05) A[2]:(0.656131386757) A[3]:(0.590425014496)\n",
      " state (2)  A[0]:(0.590517163277) A[1]:(0.729070782661) A[2]:(0.590516746044) A[3]:(0.656053900719)\n",
      " state (3)  A[0]:(0.656233310699) A[1]:(-0.00502161914483) A[2]:(0.525484144688) A[3]:(0.549962580204)\n",
      " state (4)  A[0]:(0.590418696404) A[1]:(0.656162500381) A[2]:(-8.05854797363e-05) A[3]:(0.531388044357)\n",
      " state (5)  A[0]:(-0.0355215184391) A[1]:(0.999897241592) A[2]:(-0.822846591473) A[3]:(0.657665252686)\n",
      " state (6)  A[0]:(3.4362077713e-05) A[1]:(0.810033023357) A[2]:(7.55786895752e-05) A[3]:(0.656007647514)\n",
      " state (7)  A[0]:(0.545004844666) A[1]:(-0.535572826862) A[2]:(0.436651557684) A[3]:(0.870437860489)\n",
      " state (8)  A[0]:(0.655905246735) A[1]:(0.000158697366714) A[2]:(0.729020118713) A[3]:(0.590235233307)\n",
      " state (9)  A[0]:(0.655902147293) A[1]:(0.810050964355) A[2]:(0.810035228729) A[3]:(-0.000285893678665)\n",
      " state (10)  A[0]:(0.72931176424) A[1]:(0.900043904781) A[2]:(3.12328338623e-05) A[3]:(0.728897690773)\n",
      " state (11)  A[0]:(0.136154651642) A[1]:(0.882980167866) A[2]:(-0.929196238518) A[3]:(0.803391754627)\n",
      " state (12)  A[0]:(-0.427258223295) A[1]:(0.816229760647) A[2]:(-0.954304814339) A[3]:(0.715334355831)\n",
      " state (13)  A[0]:(0.000228881835938) A[1]:(0.809520602226) A[2]:(0.900041222572) A[3]:(0.729011416435)\n",
      " state (14)  A[0]:(0.810224175453) A[1]:(0.900396585464) A[2]:(0.999999821186) A[3]:(0.809984385967)\n",
      " state (15)  A[0]:(0.979818165302) A[1]:(0.938400387764) A[2]:(1.0) A[3]:(0.878265321255)\n",
      "Episode 655000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6018. Times reached goal: 991.               Steps done: 4593201. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00910837903843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531385719776) A[1]:(0.590545296669) A[2]:(0.590492010117) A[3]:(0.531038999557)\n",
      " state (1)  A[0]:(0.531486868858) A[1]:(0.000120311975479) A[2]:(0.656061589718) A[3]:(0.589931845665)\n",
      " state (2)  A[0]:(0.59067940712) A[1]:(0.729060530663) A[2]:(0.590462565422) A[3]:(0.655615329742)\n",
      " state (3)  A[0]:(0.656406521797) A[1]:(-0.0051494082436) A[2]:(0.525460600853) A[3]:(0.549299716949)\n",
      " state (4)  A[0]:(0.590632975101) A[1]:(0.656139314175) A[2]:(-0.000153541564941) A[3]:(0.530594348907)\n",
      " state (5)  A[0]:(-0.0352080762386) A[1]:(0.999897181988) A[2]:(-0.822919130325) A[3]:(0.656944930553)\n",
      " state (6)  A[0]:(0.000368580193026) A[1]:(0.809979021549) A[2]:(-9.77516174316e-06) A[3]:(0.655191302299)\n",
      " state (7)  A[0]:(0.545282959938) A[1]:(-0.535584211349) A[2]:(0.436618745327) A[3]:(0.870094656944)\n",
      " state (8)  A[0]:(0.656298875809) A[1]:(-9.78410243988e-05) A[2]:(0.728966116905) A[3]:(0.589481532574)\n",
      " state (9)  A[0]:(0.656278073788) A[1]:(0.80993950367) A[2]:(0.81003010273) A[3]:(-0.00183748989366)\n",
      " state (10)  A[0]:(0.729604125023) A[1]:(0.89997190237) A[2]:(8.46385955811e-06) A[3]:(0.72821700573)\n",
      " state (11)  A[0]:(0.136894628406) A[1]:(0.882870256901) A[2]:(-0.929236769676) A[3]:(0.803010106087)\n",
      " state (12)  A[0]:(-0.426450639963) A[1]:(0.815989375114) A[2]:(-0.954358994961) A[3]:(0.714979052544)\n",
      " state (13)  A[0]:(0.00128650595434) A[1]:(0.809173345566) A[2]:(0.899985671043) A[3]:(0.728745818138)\n",
      " state (14)  A[0]:(0.810495972633) A[1]:(0.900170862675) A[2]:(0.999999821186) A[3]:(0.809766411781)\n",
      " state (15)  A[0]:(0.97983366251) A[1]:(0.938247680664) A[2]:(1.0) A[3]:(0.878088772297)\n",
      "Episode 656000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6025. Times reached goal: 992.               Steps done: 4599226. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00905366604313.\n",
      " state (0)  A[0]:(0.531440496445) A[1]:(0.590332388878) A[2]:(0.590651869774) A[3]:(0.531615376472)\n",
      " state (1)  A[0]:(0.531538546085) A[1]:(9.05692577362e-05) A[2]:(0.65614259243) A[3]:(0.590532064438)\n",
      " state (2)  A[0]:(0.590358257294) A[1]:(0.728899717331) A[2]:(0.590537428856) A[3]:(0.656170964241)\n",
      " state (3)  A[0]:(0.656113386154) A[1]:(-0.00521467346698) A[2]:(0.525625705719) A[3]:(0.550073504448)\n",
      " state (4)  A[0]:(0.590295672417) A[1]:(0.656077980995) A[2]:(0.000262498855591) A[3]:(0.531502842903)\n",
      " state (5)  A[0]:(-0.0357344672084) A[1]:(0.999897241592) A[2]:(-0.822781682014) A[3]:(0.657811403275)\n",
      " state (6)  A[0]:(1.87605619431e-05) A[1]:(0.810138344765) A[2]:(0.000289082527161) A[3]:(0.655875325203)\n",
      " state (7)  A[0]:(0.54499578476) A[1]:(-0.535199284554) A[2]:(0.436869859695) A[3]:(0.87040501833)\n",
      " state (8)  A[0]:(0.655933499336) A[1]:(0.000479668349726) A[2]:(0.729178667068) A[3]:(0.590918779373)\n",
      " state (9)  A[0]:(0.655473530293) A[1]:(0.810211420059) A[2]:(0.810291171074) A[3]:(0.00062292808434)\n",
      " state (10)  A[0]:(0.728479027748) A[1]:(0.900114059448) A[2]:(0.000760316674132) A[3]:(0.729141712189)\n",
      " state (11)  A[0]:(0.133330121636) A[1]:(0.883001685143) A[2]:(-0.929195642471) A[3]:(0.803416132927)\n",
      " state (12)  A[0]:(-0.43028524518) A[1]:(0.816149830818) A[2]:(-0.954347729683) A[3]:(0.715183079243)\n",
      " state (13)  A[0]:(-0.00368641130626) A[1]:(0.809311926365) A[2]:(0.900299549103) A[3]:(0.728821158409)\n",
      " state (14)  A[0]:(0.808972358704) A[1]:(0.900193214417) A[2]:(0.999999821186) A[3]:(0.809875607491)\n",
      " state (15)  A[0]:(0.979692876339) A[1]:(0.938167929649) A[2]:(1.0) A[3]:(0.878238320351)\n",
      "Episode 657000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6026. Times reached goal: 990.               Steps done: 4605252. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00899927270328.\n",
      " state (0)  A[0]:(0.53149586916) A[1]:(0.590515971184) A[2]:(0.590496957302) A[3]:(0.531592845917)\n",
      " state (1)  A[0]:(0.531469583511) A[1]:(-6.97374343872e-06) A[2]:(0.656016111374) A[3]:(0.59068852663)\n",
      " state (2)  A[0]:(0.590530157089) A[1]:(0.728823065758) A[2]:(0.590670764446) A[3]:(0.656213760376)\n",
      " state (3)  A[0]:(0.656299233437) A[1]:(-0.00563502311707) A[2]:(0.525647819042) A[3]:(0.550022900105)\n",
      " state (4)  A[0]:(0.590495347977) A[1]:(0.655896782875) A[2]:(3.02791595459e-05) A[3]:(0.531491577625)\n",
      " state (5)  A[0]:(-0.0356569439173) A[1]:(0.999897181988) A[2]:(-0.822891116142) A[3]:(0.658036291599)\n",
      " state (6)  A[0]:(-7.78436660767e-05) A[1]:(0.809982538223) A[2]:(0.000144481658936) A[3]:(0.65623998642)\n",
      " state (7)  A[0]:(0.544875144958) A[1]:(-0.535395920277) A[2]:(0.436729997396) A[3]:(0.870490849018)\n",
      " state (8)  A[0]:(0.65577685833) A[1]:(0.000420749158366) A[2]:(0.729026973248) A[3]:(0.590338587761)\n",
      " state (9)  A[0]:(0.655776977539) A[1]:(0.810083031654) A[2]:(0.809996724129) A[3]:(8.61287117004e-06)\n",
      " state (10)  A[0]:(0.729112029076) A[1]:(0.900017976761) A[2]:(-0.000163197517395) A[3]:(0.728996872902)\n",
      " state (11)  A[0]:(0.135559201241) A[1]:(0.882910192013) A[2]:(-0.929329156876) A[3]:(0.803410947323)\n",
      " state (12)  A[0]:(-0.427813619375) A[1]:(0.816026628017) A[2]:(-0.954462170601) A[3]:(0.715290665627)\n",
      " state (13)  A[0]:(-0.000357076496584) A[1]:(0.80916851759) A[2]:(0.90002655983) A[3]:(0.728958249092)\n",
      " state (14)  A[0]:(0.810059428215) A[1]:(0.900133252144) A[2]:(0.999999821186) A[3]:(0.809936642647)\n",
      " state (15)  A[0]:(0.979803323746) A[1]:(0.938164889812) A[2]:(1.0) A[3]:(0.878228724003)\n",
      "Episode 658000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6035. Times reached goal: 992.               Steps done: 4611287. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00894512564561.\n",
      " state (0)  A[0]:(0.531464874744) A[1]:(0.590486764908) A[2]:(0.59045445919) A[3]:(0.531517744064)\n",
      " state (1)  A[0]:(0.531518101692) A[1]:(-1.18911266327e-05) A[2]:(0.656172037125) A[3]:(0.590481519699)\n",
      " state (2)  A[0]:(0.590401411057) A[1]:(0.729025959969) A[2]:(0.590788006783) A[3]:(0.656168341637)\n",
      " state (3)  A[0]:(0.656192302704) A[1]:(-0.00521628279239) A[2]:(0.525784313679) A[3]:(0.550060868263)\n",
      " state (4)  A[0]:(0.590417504311) A[1]:(0.656074523926) A[2]:(0.000117421150208) A[3]:(0.531607270241)\n",
      " state (5)  A[0]:(-0.0356093980372) A[1]:(0.999897181988) A[2]:(-0.822994530201) A[3]:(0.658059239388)\n",
      " state (6)  A[0]:(7.97957181931e-05) A[1]:(0.809992551804) A[2]:(-6.07967376709e-05) A[3]:(0.656148910522)\n",
      " state (7)  A[0]:(0.545061528683) A[1]:(-0.535605788231) A[2]:(0.436713308096) A[3]:(0.870494842529)\n",
      " state (8)  A[0]:(0.656107783318) A[1]:(-8.19861888885e-05) A[2]:(0.729012131691) A[3]:(0.590737879276)\n",
      " state (9)  A[0]:(0.65608215332) A[1]:(0.809959888458) A[2]:(0.810068190098) A[3]:(0.000338643789291)\n",
      " state (10)  A[0]:(0.729441642761) A[1]:(0.899980843067) A[2]:(0.00014066696167) A[3]:(0.729176521301)\n",
      " state (11)  A[0]:(0.136527821422) A[1]:(0.882910549641) A[2]:(-0.929323256016) A[3]:(0.803589701653)\n",
      " state (12)  A[0]:(-0.426974266768) A[1]:(0.816086053848) A[2]:(-0.954493284225) A[3]:(0.715517640114)\n",
      " state (13)  A[0]:(0.000356510252459) A[1]:(0.809274196625) A[2]:(0.900023698807) A[3]:(0.72911632061)\n",
      " state (14)  A[0]:(0.810105383396) A[1]:(0.900221288204) A[2]:(0.999999821186) A[3]:(0.809983491898)\n",
      " state (15)  A[0]:(0.97979092598) A[1]:(0.938228964806) A[2]:(1.0) A[3]:(0.878220200539)\n",
      "Episode 659000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6029. Times reached goal: 991.               Steps done: 4617316. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00889135772934.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5903,  0.5903,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6561,  0.0001,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.0004,  0.7290,  0.5910]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6564,  0.8101,  0.8100,  0.0009]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0011,  0.8090,  0.9001,  0.7291]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9001,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531459212303) A[1]:(0.5903403759) A[2]:(0.590352773666) A[3]:(0.531397104263)\n",
      " state (1)  A[0]:(0.531336903572) A[1]:(-2.21133232117e-05) A[2]:(0.656047582626) A[3]:(0.590286433697)\n",
      " state (2)  A[0]:(0.590253949165) A[1]:(0.728954195976) A[2]:(0.590570569038) A[3]:(0.655887782574)\n",
      " state (3)  A[0]:(0.656075596809) A[1]:(-0.00515426602215) A[2]:(0.525617897511) A[3]:(0.549694538116)\n",
      " state (4)  A[0]:(0.590273320675) A[1]:(0.656083583832) A[2]:(4.02927398682e-05) A[3]:(0.531232357025)\n",
      " state (5)  A[0]:(-0.0360545925796) A[1]:(0.999897181988) A[2]:(-0.82297116518) A[3]:(0.657776534557)\n",
      " state (6)  A[0]:(-0.000234603881836) A[1]:(0.809973478317) A[2]:(6.24656677246e-05) A[3]:(0.655740022659)\n",
      " state (7)  A[0]:(0.544906973839) A[1]:(-0.535192728043) A[2]:(0.436681836843) A[3]:(0.870309591293)\n",
      " state (8)  A[0]:(0.655887246132) A[1]:(0.000609219016042) A[2]:(0.728977799416) A[3]:(0.590182423592)\n",
      " state (9)  A[0]:(0.655670166016) A[1]:(0.810163974762) A[2]:(0.809997200966) A[3]:(-0.000623583735432)\n",
      " state (10)  A[0]:(0.728888869286) A[1]:(0.900038421154) A[2]:(-0.00029456615448) A[3]:(0.728599011898)\n",
      " state (11)  A[0]:(0.135276377201) A[1]:(0.882910013199) A[2]:(-0.929419219494) A[3]:(0.803128182888)\n",
      " state (12)  A[0]:(-0.427554637194) A[1]:(0.815990149975) A[2]:(-0.954557001591) A[3]:(0.715043187141)\n",
      " state (13)  A[0]:(0.000224530696869) A[1]:(0.809102654457) A[2]:(0.899980545044) A[3]:(0.728824734688)\n",
      " state (14)  A[0]:(0.810116648674) A[1]:(0.900131583214) A[2]:(0.999999821186) A[3]:(0.809814572334)\n",
      " state (15)  A[0]:(0.979785561562) A[1]:(0.938191652298) A[2]:(1.0) A[3]:(0.878104031086)\n",
      "Episode 660000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6016. Times reached goal: 991.               Steps done: 4623332. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00883802789821.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531639158726) A[1]:(0.590904116631) A[2]:(0.590852737427) A[3]:(0.531848907471)\n",
      " state (1)  A[0]:(0.53172659874) A[1]:(-1.42157077789e-05) A[2]:(0.656413435936) A[3]:(0.590961277485)\n",
      " state (2)  A[0]:(0.590809106827) A[1]:(0.729055404663) A[2]:(0.590731501579) A[3]:(0.656416356564)\n",
      " state (3)  A[0]:(0.65651845932) A[1]:(-0.00502239400521) A[2]:(0.525681555271) A[3]:(0.550217628479)\n",
      " state (4)  A[0]:(0.590602755547) A[1]:(0.656415522099) A[2]:(-6.00814819336e-05) A[3]:(0.531769752502)\n",
      " state (5)  A[0]:(-0.0359008349478) A[1]:(0.999897301197) A[2]:(-0.822964012623) A[3]:(0.658450484276)\n",
      " state (6)  A[0]:(-8.79168510437e-05) A[1]:(0.809986948967) A[2]:(0.000314831733704) A[3]:(0.65646135807)\n",
      " state (7)  A[0]:(0.544982194901) A[1]:(-0.535732090473) A[2]:(0.437086969614) A[3]:(0.870595574379)\n",
      " state (8)  A[0]:(0.656318366528) A[1]:(-0.000177770853043) A[2]:(0.729065060616) A[3]:(0.591053962708)\n",
      " state (9)  A[0]:(0.656307399273) A[1]:(0.810038328171) A[2]:(0.810024678707) A[3]:(0.000118732452393)\n",
      " state (10)  A[0]:(0.729323506355) A[1]:(0.900014400482) A[2]:(-0.000139594078064) A[3]:(0.72883951664)\n",
      " state (11)  A[0]:(0.135686844587) A[1]:(0.8829382658) A[2]:(-0.929433822632) A[3]:(0.803287684917)\n",
      " state (12)  A[0]:(-0.427831828594) A[1]:(0.816120803356) A[2]:(-0.954599261284) A[3]:(0.715164422989)\n",
      " state (13)  A[0]:(-0.000416979164584) A[1]:(0.809313058853) A[2]:(0.900024116039) A[3]:(0.728901326656)\n",
      " state (14)  A[0]:(0.810030579567) A[1]:(0.900258302689) A[2]:(0.999999821186) A[3]:(0.80991846323)\n",
      " state (15)  A[0]:(0.97980350256) A[1]:(0.938235640526) A[2]:(1.0) A[3]:(0.878223538399)\n",
      "Episode 661000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6029. Times reached goal: 994.               Steps done: 4629361. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00878490373173.\n",
      " state (0)  A[0]:(0.531367778778) A[1]:(0.59042942524) A[2]:(0.590406894684) A[3]:(0.531497359276)\n",
      " state (1)  A[0]:(0.531352877617) A[1]:(-5.06043434143e-05) A[2]:(0.65603941679) A[3]:(0.590596079826)\n",
      " state (2)  A[0]:(0.590280890465) A[1]:(0.729047834873) A[2]:(0.590582191944) A[3]:(0.656201958656)\n",
      " state (3)  A[0]:(0.656137108803) A[1]:(-0.00489642284811) A[2]:(0.52560210228) A[3]:(0.550072431564)\n",
      " state (4)  A[0]:(0.590363323689) A[1]:(0.656255245209) A[2]:(-4.76837158203e-05) A[3]:(0.531705379486)\n",
      " state (5)  A[0]:(-0.0358319580555) A[1]:(0.999897301197) A[2]:(-0.823053956032) A[3]:(0.658427894115)\n",
      " state (6)  A[0]:(0.000220566987991) A[1]:(0.810129880905) A[2]:(-2.30073928833e-05) A[3]:(0.656369924545)\n",
      " state (7)  A[0]:(0.545160055161) A[1]:(-0.535326898098) A[2]:(0.436793744564) A[3]:(0.870514035225)\n",
      " state (8)  A[0]:(0.65630543232) A[1]:(0.000280559062958) A[2]:(0.72901648283) A[3]:(0.590854167938)\n",
      " state (9)  A[0]:(0.656359255314) A[1]:(0.810119211674) A[2]:(0.810087561607) A[3]:(0.000567018927541)\n",
      " state (10)  A[0]:(0.729517221451) A[1]:(0.900073170662) A[2]:(0.00028657913208) A[3]:(0.729245185852)\n",
      " state (11)  A[0]:(0.136138394475) A[1]:(0.883052289486) A[2]:(-0.929415404797) A[3]:(0.803619801998)\n",
      " state (12)  A[0]:(-0.427897185087) A[1]:(0.816328525543) A[2]:(-0.954664409161) A[3]:(0.715452075005)\n",
      " state (13)  A[0]:(-0.000918507343158) A[1]:(0.809493660927) A[2]:(0.899847567081) A[3]:(0.72901058197)\n",
      " state (14)  A[0]:(0.80989164114) A[1]:(0.900312483311) A[2]:(0.999999821186) A[3]:(0.809928953648)\n",
      " state (15)  A[0]:(0.979806423187) A[1]:(0.938222467899) A[2]:(1.0) A[3]:(0.878219246864)\n",
      "Episode 662000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6015. Times reached goal: 986.               Steps done: 4635376. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00873222113753.\n",
      " state (0)  A[0]:(0.532260298729) A[1]:(0.590576410294) A[2]:(0.590677261353) A[3]:(0.531692147255)\n",
      " state (1)  A[0]:(0.532013177872) A[1]:(-0.000229299068451) A[2]:(0.656171441078) A[3]:(0.590596079826)\n",
      " state (2)  A[0]:(0.590806722641) A[1]:(0.728944003582) A[2]:(0.590669989586) A[3]:(0.656186878681)\n",
      " state (3)  A[0]:(0.656561076641) A[1]:(-0.00512064993382) A[2]:(0.525677204132) A[3]:(0.549985527992)\n",
      " state (4)  A[0]:(0.590730071068) A[1]:(0.656227827072) A[2]:(-2.80141830444e-05) A[3]:(0.531558275223)\n",
      " state (5)  A[0]:(-0.0357210710645) A[1]:(0.999897241592) A[2]:(-0.823065936565) A[3]:(0.658216714859)\n",
      " state (6)  A[0]:(0.000214621424675) A[1]:(0.809996724129) A[2]:(8.38041305542e-05) A[3]:(0.656063139439)\n",
      " state (7)  A[0]:(0.545173585415) A[1]:(-0.535450100899) A[2]:(0.436918199062) A[3]:(0.870399475098)\n",
      " state (8)  A[0]:(0.656220972538) A[1]:(0.000250399112701) A[2]:(0.729071378708) A[3]:(0.590542793274)\n",
      " state (9)  A[0]:(0.656120300293) A[1]:(0.810110628605) A[2]:(0.810075283051) A[3]:(-4.5508146286e-05)\n",
      " state (10)  A[0]:(0.729316115379) A[1]:(0.900007963181) A[2]:(3.62396240234e-05) A[3]:(0.728876829147)\n",
      " state (11)  A[0]:(0.136213243008) A[1]:(0.882892608643) A[2]:(-0.929461061954) A[3]:(0.803308606148)\n",
      " state (12)  A[0]:(-0.427024781704) A[1]:(0.815988779068) A[2]:(-0.954632878304) A[3]:(0.715143561363)\n",
      " state (13)  A[0]:(0.000690132263117) A[1]:(0.809118032455) A[2]:(0.900288403034) A[3]:(0.728847265244)\n",
      " state (14)  A[0]:(0.810324072838) A[1]:(0.900126874447) A[2]:(0.999999821186) A[3]:(0.809845864773)\n",
      " state (15)  A[0]:(0.979826033115) A[1]:(0.938108205795) A[2]:(1.0) A[3]:(0.878160595894)\n",
      "Episode 663000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6024. Times reached goal: 987.               Steps done: 4641400. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00867977635966.\n",
      " state (0)  A[0]:(0.531649947166) A[1]:(0.590439021587) A[2]:(0.590452313423) A[3]:(0.531516194344)\n",
      " state (1)  A[0]:(0.531519889832) A[1]:(0.000231623649597) A[2]:(0.656107664108) A[3]:(0.590543627739)\n",
      " state (2)  A[0]:(0.590311527252) A[1]:(0.729003250599) A[2]:(0.590647101402) A[3]:(0.656161725521)\n",
      " state (3)  A[0]:(0.65617275238) A[1]:(-0.0048466543667) A[2]:(0.525723814964) A[3]:(0.550001382828)\n",
      " state (4)  A[0]:(0.590444803238) A[1]:(0.656086206436) A[2]:(0.000273704528809) A[3]:(0.531647086143)\n",
      " state (5)  A[0]:(-0.035854075104) A[1]:(0.999897181988) A[2]:(-0.822945177555) A[3]:(0.658431828022)\n",
      " state (6)  A[0]:(8.37594270706e-05) A[1]:(0.809998393059) A[2]:(0.000427722901804) A[3]:(0.656318426132)\n",
      " state (7)  A[0]:(0.544929325581) A[1]:(-0.535196065903) A[2]:(0.436969995499) A[3]:(0.870488524437)\n",
      " state (8)  A[0]:(0.655922889709) A[1]:(0.000155091285706) A[2]:(0.728968560696) A[3]:(0.590859770775)\n",
      " state (9)  A[0]:(0.655627965927) A[1]:(0.809991836548) A[2]:(0.809998869896) A[3]:(0.000253915786743)\n",
      " state (10)  A[0]:(0.728806853294) A[1]:(0.899988532066) A[2]:(-3.1590461731e-05) A[3]:(0.729131519794)\n",
      " state (11)  A[0]:(0.134820759296) A[1]:(0.882964491844) A[2]:(-0.929520308971) A[3]:(0.803623437881)\n",
      " state (12)  A[0]:(-0.428645372391) A[1]:(0.816197156906) A[2]:(-0.954747617245) A[3]:(0.715546607971)\n",
      " state (13)  A[0]:(-0.00114718033001) A[1]:(0.809340953827) A[2]:(0.900032997131) A[3]:(0.729161381721)\n",
      " state (14)  A[0]:(0.810088276863) A[1]:(0.900212883949) A[2]:(0.999999821186) A[3]:(0.810047566891)\n",
      " state (15)  A[0]:(0.979853689671) A[1]:(0.938113570213) A[2]:(1.0) A[3]:(0.878299236298)\n",
      "Episode 664000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6020. Times reached goal: 989.               Steps done: 4647420. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00862768107013.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5907,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6560,  0.0003,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557,  0.0001,  0.7290,  0.5903]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557,  0.8100,  0.8100, -0.0003]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9000, -0.0002,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? True\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9003,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531335234642) A[1]:(0.590697050095) A[2]:(0.590472221375) A[3]:(0.531385838985)\n",
      " state (1)  A[0]:(0.531385838985) A[1]:(2.96086072922e-05) A[2]:(0.65617287159) A[3]:(0.590434551239)\n",
      " state (2)  A[0]:(0.59038734436) A[1]:(0.729107320309) A[2]:(0.590369343758) A[3]:(0.65605533123)\n",
      " state (3)  A[0]:(0.656219005585) A[1]:(-0.0044986451976) A[2]:(0.525508463383) A[3]:(0.549865365028)\n",
      " state (4)  A[0]:(0.590518772602) A[1]:(0.655962586403) A[2]:(0.000227093696594) A[3]:(0.531503021717)\n",
      " state (5)  A[0]:(-0.0358258262277) A[1]:(0.999897181988) A[2]:(-0.823017776012) A[3]:(0.658397018909)\n",
      " state (6)  A[0]:(-0.000162750482559) A[1]:(0.80999481678) A[2]:(4.30345535278e-05) A[3]:(0.656226336956)\n",
      " state (7)  A[0]:(0.544640541077) A[1]:(-0.535495698452) A[2]:(0.436829835176) A[3]:(0.870399832726)\n",
      " state (8)  A[0]:(0.65584397316) A[1]:(-9.5322728157e-05) A[2]:(0.728964567184) A[3]:(0.590617418289)\n",
      " state (9)  A[0]:(0.655860066414) A[1]:(0.809985041618) A[2]:(0.810008049011) A[3]:(0.000183075666428)\n",
      " state (10)  A[0]:(0.729126274586) A[1]:(0.900004863739) A[2]:(-0.000131845474243) A[3]:(0.729128837585)\n",
      " state (11)  A[0]:(0.135648757219) A[1]:(0.883010387421) A[2]:(-0.92959189415) A[3]:(0.803598403931)\n",
      " state (12)  A[0]:(-0.428042262793) A[1]:(0.816289722919) A[2]:(-0.954831123352) A[3]:(0.715463757515)\n",
      " state (13)  A[0]:(-0.000895604258403) A[1]:(0.809432983398) A[2]:(0.899955809116) A[3]:(0.729053616524)\n",
      " state (14)  A[0]:(0.809910774231) A[1]:(0.900250911713) A[2]:(0.999999821186) A[3]:(0.809951007366)\n",
      " state (15)  A[0]:(0.979813456535) A[1]:(0.938107609749) A[2]:(1.0) A[3]:(0.878237009048)\n",
      "Episode 665000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6018. Times reached goal: 985.               Steps done: 4653438. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00857591560396.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531515538692) A[1]:(0.59026324749) A[2]:(0.59048897028) A[3]:(0.531408309937)\n",
      " state (1)  A[0]:(0.531463027) A[1]:(5.85615634918e-06) A[2]:(0.656178474426) A[3]:(0.590591430664)\n",
      " state (2)  A[0]:(0.590482234955) A[1]:(0.72863972187) A[2]:(0.590585112572) A[3]:(0.65613669157)\n",
      " state (3)  A[0]:(0.65622407198) A[1]:(-0.00554041843861) A[2]:(0.525604844093) A[3]:(0.549856066704)\n",
      " state (4)  A[0]:(0.590366005898) A[1]:(0.655911684036) A[2]:(-5.74588775635e-05) A[3]:(0.531485795975)\n",
      " state (5)  A[0]:(-0.0361514873803) A[1]:(0.999897122383) A[2]:(-0.823151946068) A[3]:(0.658415198326)\n",
      " state (6)  A[0]:(4.61041927338e-05) A[1]:(0.809910655022) A[2]:(-0.000154733657837) A[3]:(0.65619456768)\n",
      " state (7)  A[0]:(0.544958233833) A[1]:(-0.535035133362) A[2]:(0.436621636152) A[3]:(0.870402812958)\n",
      " state (8)  A[0]:(0.655781030655) A[1]:(0.000864639645442) A[2]:(0.728900551796) A[3]:(0.590434670448)\n",
      " state (9)  A[0]:(0.655533075333) A[1]:(0.810221254826) A[2]:(0.80993938446) A[3]:(-3.17096710205e-05)\n",
      " state (10)  A[0]:(0.728842616081) A[1]:(0.900012433529) A[2]:(-0.000314474105835) A[3]:(0.728854894638)\n",
      " state (11)  A[0]:(0.135410964489) A[1]:(0.882865130901) A[2]:(-0.929626166821) A[3]:(0.803254783154)\n",
      " state (12)  A[0]:(-0.427659928799) A[1]:(0.815868556499) A[2]:(-0.954851090908) A[3]:(0.715019822121)\n",
      " state (13)  A[0]:(-3.30060720444e-05) A[1]:(0.80886900425) A[2]:(0.900033950806) A[3]:(0.728809177876)\n",
      " state (14)  A[0]:(0.810109078884) A[1]:(0.899940133095) A[2]:(0.999999821186) A[3]:(0.809934079647)\n",
      " state (15)  A[0]:(0.979810059071) A[1]:(0.937932252884) A[2]:(1.0) A[3]:(0.8782954216)\n",
      "Episode 666000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 988.               Steps done: 4659439. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00852460564394.\n",
      " state (0)  A[0]:(0.531652271748) A[1]:(0.590434789658) A[2]:(0.590472698212) A[3]:(0.530550777912)\n",
      " state (1)  A[0]:(0.531540989876) A[1]:(-0.000273287296295) A[2]:(0.656101405621) A[3]:(0.589739382267)\n",
      " state (2)  A[0]:(0.590508460999) A[1]:(0.728963494301) A[2]:(0.590533077717) A[3]:(0.655492067337)\n",
      " state (3)  A[0]:(0.656315207481) A[1]:(-0.00481798499823) A[2]:(0.525613665581) A[3]:(0.549114465714)\n",
      " state (4)  A[0]:(0.590519785881) A[1]:(0.656132936478) A[2]:(9.71555709839e-05) A[3]:(0.530796766281)\n",
      " state (5)  A[0]:(-0.0359621159732) A[1]:(0.999897181988) A[2]:(-0.823096215725) A[3]:(0.658036351204)\n",
      " state (6)  A[0]:(0.000259906053543) A[1]:(0.809978306293) A[2]:(0.000111937522888) A[3]:(0.655811846256)\n",
      " state (7)  A[0]:(0.544954180717) A[1]:(-0.53537774086) A[2]:(0.436948001385) A[3]:(0.870185792446)\n",
      " state (8)  A[0]:(0.655904233456) A[1]:(0.000177323818207) A[2]:(0.729022026062) A[3]:(0.589979231358)\n",
      " state (9)  A[0]:(0.655912637711) A[1]:(0.810020387173) A[2]:(0.809990644455) A[3]:(-6.51776790619e-05)\n",
      " state (10)  A[0]:(0.729165434837) A[1]:(0.90000718832) A[2]:(-0.00016725063324) A[3]:(0.729192256927)\n",
      " state (11)  A[0]:(0.135669216514) A[1]:(0.883047282696) A[2]:(-0.929664433002) A[3]:(0.803732514381)\n",
      " state (12)  A[0]:(-0.428228259087) A[1]:(0.816401720047) A[2]:(-0.9549254179) A[3]:(0.715714335442)\n",
      " state (13)  A[0]:(-0.0012291962048) A[1]:(0.809589266777) A[2]:(0.900057196617) A[3]:(0.7294293046)\n",
      " state (14)  A[0]:(0.809837341309) A[1]:(0.900347828865) A[2]:(0.999999821186) A[3]:(0.810384571552)\n",
      " state (15)  A[0]:(0.979816496372) A[1]:(0.938128054142) A[2]:(1.0) A[3]:(0.878621160984)\n",
      "Episode 667000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6001. Times reached goal: 986.               Steps done: 4665440. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00847360267295.\n",
      " state (0)  A[0]:(0.5315502882) A[1]:(0.590722978115) A[2]:(0.590460300446) A[3]:(0.53161251545)\n",
      " state (1)  A[0]:(0.531479418278) A[1]:(-0.000102952122688) A[2]:(0.656282424927) A[3]:(0.590690433979)\n",
      " state (2)  A[0]:(0.590470850468) A[1]:(0.729122161865) A[2]:(0.590671062469) A[3]:(0.656311035156)\n",
      " state (3)  A[0]:(0.656253457069) A[1]:(-0.0045031751506) A[2]:(0.525729298592) A[3]:(0.550052404404)\n",
      " state (4)  A[0]:(0.590405106544) A[1]:(0.656213402748) A[2]:(0.000223636627197) A[3]:(0.531636893749)\n",
      " state (5)  A[0]:(-0.0363939963281) A[1]:(0.999897241592) A[2]:(-0.823122620583) A[3]:(0.658507585526)\n",
      " state (6)  A[0]:(-0.00062492483994) A[1]:(0.810119986534) A[2]:(0.000268697738647) A[3]:(0.65608894825)\n",
      " state (7)  A[0]:(0.544129610062) A[1]:(-0.535247504711) A[2]:(0.437327742577) A[3]:(0.870188593864)\n",
      " state (8)  A[0]:(0.655129313469) A[1]:(0.000460669369204) A[2]:(0.729292631149) A[3]:(0.589378476143)\n",
      " state (9)  A[0]:(0.655122876167) A[1]:(0.810084164143) A[2]:(0.810084581375) A[3]:(-0.00159823754802)\n",
      " state (10)  A[0]:(0.728390693665) A[1]:(0.900009930134) A[2]:(-0.000505328120198) A[3]:(0.728192925453)\n",
      " state (11)  A[0]:(0.134103253484) A[1]:(0.883028626442) A[2]:(-0.929788947105) A[3]:(0.802872478962)\n",
      " state (12)  A[0]:(-0.429009318352) A[1]:(0.816354691982) A[2]:(-0.95498842001) A[3]:(0.71465909481)\n",
      " state (13)  A[0]:(-0.00163699535187) A[1]:(0.809526085854) A[2]:(0.900222480297) A[3]:(0.728581786156)\n",
      " state (14)  A[0]:(0.809763550758) A[1]:(0.900290310383) A[2]:(0.999999821186) A[3]:(0.809821665287)\n",
      " state (15)  A[0]:(0.979813337326) A[1]:(0.938041806221) A[2]:(1.0) A[3]:(0.87827539444)\n",
      "Episode 668000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 991.               Steps done: 4671445. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0084228711627.\n",
      " state (0)  A[0]:(0.531455457211) A[1]:(0.590530633926) A[2]:(0.590542197227) A[3]:(0.531529188156)\n",
      " state (1)  A[0]:(0.531548857689) A[1]:(-1.83135271072e-05) A[2]:(0.656105518341) A[3]:(0.590569257736)\n",
      " state (2)  A[0]:(0.590470075607) A[1]:(0.729014396667) A[2]:(0.590472817421) A[3]:(0.656092882156)\n",
      " state (3)  A[0]:(0.656234264374) A[1]:(-0.00465695187449) A[2]:(0.525542140007) A[3]:(0.549774765968)\n",
      " state (4)  A[0]:(0.590451180935) A[1]:(0.656093001366) A[2]:(3.89814376831e-05) A[3]:(0.531395435333)\n",
      " state (5)  A[0]:(-0.0359753631055) A[1]:(0.999897181988) A[2]:(-0.823165297508) A[3]:(0.65842550993)\n",
      " state (6)  A[0]:(-9.00030136108e-05) A[1]:(0.810015499592) A[2]:(0.000207304954529) A[3]:(0.656076192856)\n",
      " state (7)  A[0]:(0.544652819633) A[1]:(-0.535287976265) A[2]:(0.437170863152) A[3]:(0.870303869247)\n",
      " state (8)  A[0]:(0.65607380867) A[1]:(9.34302806854e-06) A[2]:(0.729048490524) A[3]:(0.590655565262)\n",
      " state (9)  A[0]:(0.656113445759) A[1]:(0.809997618198) A[2]:(0.81001406908) A[3]:(-7.31945037842e-05)\n",
      " state (10)  A[0]:(0.729201436043) A[1]:(0.899979770184) A[2]:(-0.000381827325327) A[3]:(0.728834986687)\n",
      " state (11)  A[0]:(0.136057719588) A[1]:(0.882978737354) A[2]:(-0.929790019989) A[3]:(0.803379893303)\n",
      " state (12)  A[0]:(-0.427168309689) A[1]:(0.816218018532) A[2]:(-0.955041527748) A[3]:(0.715299606323)\n",
      " state (13)  A[0]:(0.000869720941409) A[1]:(0.809293746948) A[2]:(0.900045990944) A[3]:(0.729079961777)\n",
      " state (14)  A[0]:(0.810703039169) A[1]:(0.900142610073) A[2]:(0.999999821186) A[3]:(0.81006026268)\n",
      " state (15)  A[0]:(0.979927659035) A[1]:(0.937955856323) A[2]:(1.0) A[3]:(0.878358185291)\n",
      "Episode 669000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6021. Times reached goal: 995.               Steps done: 4677466. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00837230942429.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5905,  0.5905,  0.5310]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.0003,  0.6560,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7291,  0.5906,  0.6557]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0005,  0.8101, -0.0004,  0.6559]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9001, -0.0000,  0.7288]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9004,  1.0000,  0.8096]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531430363655) A[1]:(0.5904712677) A[2]:(0.590482473373) A[3]:(0.531009614468)\n",
      " state (1)  A[0]:(0.531353354454) A[1]:(7.27027654648e-05) A[2]:(0.656105995178) A[3]:(0.590166449547)\n",
      " state (2)  A[0]:(0.590424776077) A[1]:(0.729027092457) A[2]:(0.590603470802) A[3]:(0.655646383762)\n",
      " state (3)  A[0]:(0.656191468239) A[1]:(-0.004750946071) A[2]:(0.525611162186) A[3]:(0.549138128757)\n",
      " state (4)  A[0]:(0.590368747711) A[1]:(0.656054258347) A[2]:(-7.83205032349e-05) A[3]:(0.530748486519)\n",
      " state (5)  A[0]:(-0.0361109487712) A[1]:(0.999897122383) A[2]:(-0.823315858841) A[3]:(0.658063948154)\n",
      " state (6)  A[0]:(0.00021705031395) A[1]:(0.80999481678) A[2]:(-0.000172019004822) A[3]:(0.655610918999)\n",
      " state (7)  A[0]:(0.544732809067) A[1]:(-0.535230398178) A[2]:(0.436929762363) A[3]:(0.869984507561)\n",
      " state (8)  A[0]:(0.655566215515) A[1]:(0.000110134482384) A[2]:(0.729038536549) A[3]:(0.589316725731)\n",
      " state (9)  A[0]:(0.655354321003) A[1]:(0.809933722019) A[2]:(0.810011327267) A[3]:(-0.00159454206005)\n",
      " state (10)  A[0]:(0.728641629219) A[1]:(0.899986863136) A[2]:(-0.000408291787608) A[3]:(0.728215575218)\n",
      " state (11)  A[0]:(0.13515406847) A[1]:(0.88310110569) A[2]:(-0.929826438427) A[3]:(0.802928447723)\n",
      " state (12)  A[0]:(-0.428010076284) A[1]:(0.816554069519) A[2]:(-0.955092251301) A[3]:(0.714641571045)\n",
      " state (13)  A[0]:(-0.000743761542253) A[1]:(0.80972468853) A[2]:(0.900063991547) A[3]:(0.728421926498)\n",
      " state (14)  A[0]:(0.809872925282) A[1]:(0.900383532047) A[2]:(0.999999821186) A[3]:(0.809580922127)\n",
      " state (15)  A[0]:(0.979816198349) A[1]:(0.938072919846) A[2]:(1.0) A[3]:(0.878055691719)\n",
      "Episode 670000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5998. Times reached goal: 989.               Steps done: 4683464. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00832224261283.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531229555607) A[1]:(0.590780794621) A[2]:(0.590574622154) A[3]:(0.531433641911)\n",
      " state (1)  A[0]:(0.531259715557) A[1]:(-0.000241577625275) A[2]:(0.656294882298) A[3]:(0.590449333191)\n",
      " state (2)  A[0]:(0.590289473534) A[1]:(0.729183554649) A[2]:(0.59071624279) A[3]:(0.656122267246)\n",
      " state (3)  A[0]:(0.656124353409) A[1]:(-0.00419930135831) A[2]:(0.525817453861) A[3]:(0.549740672112)\n",
      " state (4)  A[0]:(0.590319871902) A[1]:(0.656253516674) A[2]:(0.000401377648814) A[3]:(0.531402647495)\n",
      " state (5)  A[0]:(-0.0364060364664) A[1]:(0.999897301197) A[2]:(-0.823172926903) A[3]:(0.658666074276)\n",
      " state (6)  A[0]:(1.93268060684e-05) A[1]:(0.810238599777) A[2]:(0.000400066346629) A[3]:(0.656112670898)\n",
      " state (7)  A[0]:(0.544801354408) A[1]:(-0.534862697124) A[2]:(0.437608242035) A[3]:(0.870262563229)\n",
      " state (8)  A[0]:(0.656155586243) A[1]:(0.000754773500375) A[2]:(0.7294074893) A[3]:(0.590518116951)\n",
      " state (9)  A[0]:(0.656280934811) A[1]:(0.81030356884) A[2]:(0.810391008854) A[3]:(3.16500663757e-05)\n",
      " state (10)  A[0]:(0.72954428196) A[1]:(0.900160551071) A[2]:(0.00120985449757) A[3]:(0.729164540768)\n",
      " state (11)  A[0]:(0.136855781078) A[1]:(0.883210897446) A[2]:(-0.929617226124) A[3]:(0.803723633289)\n",
      " state (12)  A[0]:(-0.427210390568) A[1]:(0.816589236259) A[2]:(-0.955005526543) A[3]:(0.715582847595)\n",
      " state (13)  A[0]:(-0.000241622328758) A[1]:(0.809685707092) A[2]:(0.900347709656) A[3]:(0.729123711586)\n",
      " state (14)  A[0]:(0.810049116611) A[1]:(0.900350511074) A[2]:(0.999999821186) A[3]:(0.809976696968)\n",
      " state (15)  A[0]:(0.979842126369) A[1]:(0.938030421734) A[2]:(1.0) A[3]:(0.878276407719)\n",
      "Episode 671000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6007. Times reached goal: 991.               Steps done: 4689471. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00827240075136.\n",
      " state (0)  A[0]:(0.531556963921) A[1]:(0.590922236443) A[2]:(0.590743899345) A[3]:(0.531444370747)\n",
      " state (1)  A[0]:(0.531777143478) A[1]:(-4.88013029099e-05) A[2]:(0.65629196167) A[3]:(0.590610384941)\n",
      " state (2)  A[0]:(0.590499699116) A[1]:(0.729042172432) A[2]:(0.59046626091) A[3]:(0.656291246414)\n",
      " state (3)  A[0]:(0.656222343445) A[1]:(-0.00438918313012) A[2]:(0.525428175926) A[3]:(0.549990832806)\n",
      " state (4)  A[0]:(0.590390145779) A[1]:(0.656135082245) A[2]:(-0.000229597091675) A[3]:(0.531666755676)\n",
      " state (5)  A[0]:(-0.0364224351943) A[1]:(0.999897122383) A[2]:(-0.823352098465) A[3]:(0.658703267574)\n",
      " state (6)  A[0]:(-0.000145748257637) A[1]:(0.810045003891) A[2]:(-0.00010871887207) A[3]:(0.655977547169)\n",
      " state (7)  A[0]:(0.544753551483) A[1]:(-0.534263849258) A[2]:(0.436908632517) A[3]:(0.870217561722)\n",
      " state (8)  A[0]:(0.655830740929) A[1]:(0.00202686805278) A[2]:(0.729173541069) A[3]:(0.589956521988)\n",
      " state (9)  A[0]:(0.655829489231) A[1]:(0.810519874096) A[2]:(0.810097455978) A[3]:(-0.000387966603739)\n",
      " state (10)  A[0]:(0.728794276714) A[1]:(0.900066137314) A[2]:(-0.00065135944169) A[3]:(0.728614211082)\n",
      " state (11)  A[0]:(0.135086923838) A[1]:(0.882824659348) A[2]:(-0.92998111248) A[3]:(0.803016364574)\n",
      " state (12)  A[0]:(-0.427571713924) A[1]:(0.815625190735) A[2]:(-0.955238521099) A[3]:(0.714819252491)\n",
      " state (13)  A[0]:(0.000458523601992) A[1]:(0.808399915695) A[2]:(0.899929881096) A[3]:(0.728811502457)\n",
      " state (14)  A[0]:(0.810272932053) A[1]:(0.899590790272) A[2]:(0.999999821186) A[3]:(0.809986054897)\n",
      " state (15)  A[0]:(0.979839384556) A[1]:(0.937565743923) A[2]:(1.0) A[3]:(0.878363609314)\n",
      "Episode 672000 finished after 0 timesteps with r=1.0. Running score: 0.96. Times trained:               6004. Times reached goal: 992.               Steps done: 4695475. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00822288206112.\n",
      " state (0)  A[0]:(0.530707597733) A[1]:(0.590435564518) A[2]:(0.590464055538) A[3]:(0.532373785973)\n",
      " state (1)  A[0]:(0.530775487423) A[1]:(0.000200733542442) A[2]:(0.655903577805) A[3]:(0.591072380543)\n",
      " state (2)  A[0]:(0.589702606201) A[1]:(0.728922724724) A[2]:(0.590389490128) A[3]:(0.656433105469)\n",
      " state (3)  A[0]:(0.655558347702) A[1]:(-0.0048241391778) A[2]:(0.525424480438) A[3]:(0.550044894218)\n",
      " state (4)  A[0]:(0.589665710926) A[1]:(0.655995309353) A[2]:(-0.000224709510803) A[3]:(0.531666398048)\n",
      " state (5)  A[0]:(-0.0371926017106) A[1]:(0.999897181988) A[2]:(-0.823376774788) A[3]:(0.658765673637)\n",
      " state (6)  A[0]:(-0.00064910936635) A[1]:(0.810043036938) A[2]:(-3.51667404175e-05) A[3]:(0.65583384037)\n",
      " state (7)  A[0]:(0.544049739838) A[1]:(-0.535026073456) A[2]:(0.437231123447) A[3]:(0.869919717312)\n",
      " state (8)  A[0]:(0.654852509499) A[1]:(0.000438436836703) A[2]:(0.729243338108) A[3]:(0.588717579842)\n",
      " state (9)  A[0]:(0.654982626438) A[1]:(0.809924483299) A[2]:(0.810136079788) A[3]:(-0.00173392717261)\n",
      " state (10)  A[0]:(0.72883784771) A[1]:(0.899926424026) A[2]:(0.000358581513865) A[3]:(0.72854155302)\n",
      " state (11)  A[0]:(0.136481404305) A[1]:(0.882992386818) A[2]:(-0.929796397686) A[3]:(0.803334236145)\n",
      " state (12)  A[0]:(-0.426808595657) A[1]:(0.816292643547) A[2]:(-0.955176532269) A[3]:(0.715128064156)\n",
      " state (13)  A[0]:(0.000552758516278) A[1]:(0.809349000454) A[2]:(0.90018850565) A[3]:(0.728763103485)\n",
      " state (14)  A[0]:(0.81025493145) A[1]:(0.900157511234) A[2]:(0.999999821186) A[3]:(0.809748351574)\n",
      " state (15)  A[0]:(0.97985625267) A[1]:(0.937882721424) A[2]:(1.0) A[3]:(0.878136873245)\n",
      "Episode 673000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6004. Times reached goal: 985.               Steps done: 4701479. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00817365979034.\n",
      " state (0)  A[0]:(0.531073272228) A[1]:(0.590417981148) A[2]:(0.590402543545) A[3]:(0.531553149223)\n",
      " state (1)  A[0]:(0.531151533127) A[1]:(-0.000164791941643) A[2]:(0.656074643135) A[3]:(0.590527296066)\n",
      " state (2)  A[0]:(0.590120792389) A[1]:(0.728968262672) A[2]:(0.590573072433) A[3]:(0.65612411499)\n",
      " state (3)  A[0]:(0.655947327614) A[1]:(-0.00460721272975) A[2]:(0.525620698929) A[3]:(0.549751639366)\n",
      " state (4)  A[0]:(0.5901004076) A[1]:(0.656197607517) A[2]:(-4.82797622681e-05) A[3]:(0.531514167786)\n",
      " state (5)  A[0]:(-0.0366258919239) A[1]:(0.999897181988) A[2]:(-0.823361039162) A[3]:(0.658851444721)\n",
      " state (6)  A[0]:(0.000173017382622) A[1]:(0.809977173805) A[2]:(8.78572463989e-05) A[3]:(0.656186461449)\n",
      " state (7)  A[0]:(0.545067191124) A[1]:(-0.535077095032) A[2]:(0.43726631999) A[3]:(0.870309948921)\n",
      " state (8)  A[0]:(0.656415820122) A[1]:(0.000631049217191) A[2]:(0.729069113731) A[3]:(0.590626955032)\n",
      " state (9)  A[0]:(0.656536698341) A[1]:(0.810204327106) A[2]:(0.809927225113) A[3]:(0.000229299068451)\n",
      " state (10)  A[0]:(0.729523956776) A[1]:(0.899983108044) A[2]:(-0.000913262134418) A[3]:(0.728949427605)\n",
      " state (11)  A[0]:(0.136845692992) A[1]:(0.882851362228) A[2]:(-0.930065274239) A[3]:(0.803337395191)\n",
      " state (12)  A[0]:(-0.426348596811) A[1]:(0.815822839737) A[2]:(-0.955348908901) A[3]:(0.715127229691)\n",
      " state (13)  A[0]:(0.00179070048034) A[1]:(0.808729052544) A[2]:(0.89996778965) A[3]:(0.728939056396)\n",
      " state (14)  A[0]:(0.810827612877) A[1]:(0.899829208851) A[2]:(0.999999821186) A[3]:(0.809983372688)\n",
      " state (15)  A[0]:(0.979923844337) A[1]:(0.937699019909) A[2]:(1.0) A[3]:(0.878326237202)\n",
      "Episode 674000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6024. Times reached goal: 991.               Steps done: 4707503. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00812456967165.\n",
      "q_values \n",
      "tensor([[ 0.5312,  0.5904,  0.5905,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5313, -0.0000,  0.6561,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.7290,  0.5906,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0000,  0.8100,  0.0002,  0.6556]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0003,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9003,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531306147575) A[1]:(0.590441823006) A[2]:(0.590501010418) A[3]:(0.531266927719)\n",
      " state (1)  A[0]:(0.531376361847) A[1]:(5.96940517426e-05) A[2]:(0.656120598316) A[3]:(0.590363562107)\n",
      " state (2)  A[0]:(0.590318918228) A[1]:(0.729065001011) A[2]:(0.590586066246) A[3]:(0.656068205833)\n",
      " state (3)  A[0]:(0.656144142151) A[1]:(-0.00433242600411) A[2]:(0.525686144829) A[3]:(0.549688816071)\n",
      " state (4)  A[0]:(0.590303778648) A[1]:(0.656133055687) A[2]:(0.00019383430481) A[3]:(0.531401276588)\n",
      " state (5)  A[0]:(-0.0367009975016) A[1]:(0.999897181988) A[2]:(-0.8233076334) A[3]:(0.658614993095)\n",
      " state (6)  A[0]:(-1.2218952179e-06) A[1]:(0.810031235218) A[2]:(0.000253677368164) A[3]:(0.655654191971)\n",
      " state (7)  A[0]:(0.544794082642) A[1]:(-0.535166323185) A[2]:(0.437431454659) A[3]:(0.870015382767)\n",
      " state (8)  A[0]:(0.656089603901) A[1]:(-2.39908695221e-05) A[2]:(0.729090809822) A[3]:(0.590436220169)\n",
      " state (9)  A[0]:(0.655915439129) A[1]:(0.810021877289) A[2]:(0.81007963419) A[3]:(-2.39312648773e-05)\n",
      " state (10)  A[0]:(0.729178965092) A[1]:(0.900022149086) A[2]:(6.30617141724e-05) A[3]:(0.728945851326)\n",
      " state (11)  A[0]:(0.136465415359) A[1]:(0.883096277714) A[2]:(-0.929940581322) A[3]:(0.803483963013)\n",
      " state (12)  A[0]:(-0.427039682865) A[1]:(0.816437244415) A[2]:(-0.955327630043) A[3]:(0.715263426304)\n",
      " state (13)  A[0]:(0.000533223093953) A[1]:(0.809509694576) A[2]:(0.900050282478) A[3]:(0.728953480721)\n",
      " state (14)  A[0]:(0.810453236103) A[1]:(0.900287389755) A[2]:(0.999999821186) A[3]:(0.809964776039)\n",
      " state (15)  A[0]:(0.979902386665) A[1]:(0.937968611717) A[2]:(1.0) A[3]:(0.878322422504)\n",
      "Episode 675000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 990.               Steps done: 4713503. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00807596820383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532876074314) A[1]:(0.590153336525) A[2]:(0.590574920177) A[3]:(0.53150165081)\n",
      " state (1)  A[0]:(0.532050728798) A[1]:(0.000288620591164) A[2]:(0.6559227705) A[3]:(0.59051823616)\n",
      " state (2)  A[0]:(0.590648651123) A[1]:(0.728992819786) A[2]:(0.590483188629) A[3]:(0.656093835831)\n",
      " state (3)  A[0]:(0.656412601471) A[1]:(-0.0044501577504) A[2]:(0.52551150322) A[3]:(0.549770653248)\n",
      " state (4)  A[0]:(0.590527296066) A[1]:(0.655982971191) A[2]:(-9.81092453003e-05) A[3]:(0.531757593155)\n",
      " state (5)  A[0]:(-0.0370404459536) A[1]:(0.999897122383) A[2]:(-0.823421895504) A[3]:(0.659543991089)\n",
      " state (6)  A[0]:(-0.00120778323617) A[1]:(0.809886693954) A[2]:(-0.000159621238708) A[3]:(0.656897306442)\n",
      " state (7)  A[0]:(0.543815135956) A[1]:(-0.535352110863) A[2]:(0.436990320683) A[3]:(0.870651781559)\n",
      " state (8)  A[0]:(0.655663967133) A[1]:(-0.000486716598971) A[2]:(0.72881269455) A[3]:(0.592756211758)\n",
      " state (9)  A[0]:(0.655629754066) A[1]:(0.809869766235) A[2]:(0.809968709946) A[3]:(0.00360791315325)\n",
      " state (10)  A[0]:(0.728755474091) A[1]:(0.899928629398) A[2]:(-8.45193862915e-05) A[3]:(0.730480194092)\n",
      " state (11)  A[0]:(0.135043352842) A[1]:(0.882956147194) A[2]:(-0.929985165596) A[3]:(0.804520368576)\n",
      " state (12)  A[0]:(-0.428547412157) A[1]:(0.816163301468) A[2]:(-0.955385863781) A[3]:(0.71656024456)\n",
      " state (13)  A[0]:(-0.00141017045826) A[1]:(0.809165418148) A[2]:(0.899986743927) A[3]:(0.730139136314)\n",
      " state (14)  A[0]:(0.809817492962) A[1]:(0.900088429451) A[2]:(0.999999821186) A[3]:(0.81081533432)\n",
      " state (15)  A[0]:(0.979832410812) A[1]:(0.937836289406) A[2]:(1.0) A[3]:(0.878879249096)\n",
      "Episode 676000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6026. Times reached goal: 990.               Steps done: 4719529. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00802744875536.\n",
      " state (0)  A[0]:(0.533755064011) A[1]:(0.590006709099) A[2]:(0.590311408043) A[3]:(0.530615031719)\n",
      " state (1)  A[0]:(0.53373080492) A[1]:(0.00227449438535) A[2]:(0.655947625637) A[3]:(0.590545892715)\n",
      " state (2)  A[0]:(0.59264087677) A[1]:(0.728978931904) A[2]:(0.590956866741) A[3]:(0.656272411346)\n",
      " state (3)  A[0]:(0.658204615116) A[1]:(-0.00446580350399) A[2]:(0.526208519936) A[3]:(0.550131440163)\n",
      " state (4)  A[0]:(0.592636048794) A[1]:(0.655779838562) A[2]:(0.00114369345829) A[3]:(0.532157242298)\n",
      " state (5)  A[0]:(-0.0337895974517) A[1]:(0.999897241592) A[2]:(-0.823001861572) A[3]:(0.659651041031)\n",
      " state (6)  A[0]:(0.00151212397031) A[1]:(0.81007784605) A[2]:(0.000806808297057) A[3]:(0.656537771225)\n",
      " state (7)  A[0]:(0.544824242592) A[1]:(-0.535854816437) A[2]:(0.437633693218) A[3]:(0.87021446228)\n",
      " state (8)  A[0]:(0.655235886574) A[1]:(-0.00139905419201) A[2]:(0.728946805) A[3]:(0.590393781662)\n",
      " state (9)  A[0]:(0.654865503311) A[1]:(0.809519469738) A[2]:(0.809910833836) A[3]:(0.00123888195958)\n",
      " state (10)  A[0]:(0.728561401367) A[1]:(0.89993917942) A[2]:(0.000721096876077) A[3]:(0.730683863163)\n",
      " state (11)  A[0]:(0.133881673217) A[1]:(0.883365631104) A[2]:(-0.929877996445) A[3]:(0.805294275284)\n",
      " state (12)  A[0]:(-0.432978004217) A[1]:(0.817333698273) A[2]:(-0.955483198166) A[3]:(0.717160761356)\n",
      " state (13)  A[0]:(-0.0108318114653) A[1]:(0.81075155735) A[2]:(0.899786472321) A[3]:(0.729925394058)\n",
      " state (14)  A[0]:(0.806008040905) A[1]:(0.900983631611) A[2]:(0.999999821186) A[3]:(0.81028854847)\n",
      " state (15)  A[0]:(0.979412078857) A[1]:(0.93827778101) A[2]:(1.0) A[3]:(0.878446996212)\n",
      "Episode 677000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6019. Times reached goal: 994.               Steps done: 4725548. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00797927666065.\n",
      " state (0)  A[0]:(0.53135240078) A[1]:(0.590282559395) A[2]:(0.590538680553) A[3]:(0.531347751617)\n",
      " state (1)  A[0]:(0.531508922577) A[1]:(6.79343938828e-05) A[2]:(0.65611410141) A[3]:(0.590496122837)\n",
      " state (2)  A[0]:(0.59056198597) A[1]:(0.729038715363) A[2]:(0.590639591217) A[3]:(0.656015872955)\n",
      " state (3)  A[0]:(0.656285822392) A[1]:(-0.00446937978268) A[2]:(0.525627613068) A[3]:(0.549457371235)\n",
      " state (4)  A[0]:(0.590506255627) A[1]:(0.656196415424) A[2]:(-8.94069671631e-05) A[3]:(0.531311988831)\n",
      " state (5)  A[0]:(-0.0358719937503) A[1]:(0.999897181988) A[2]:(-0.823421835899) A[3]:(0.659241676331)\n",
      " state (6)  A[0]:(0.000500261725392) A[1]:(0.810040235519) A[2]:(3.45706939697e-05) A[3]:(0.65643620491)\n",
      " state (7)  A[0]:(0.54494535923) A[1]:(-0.535062968731) A[2]:(0.437285989523) A[3]:(0.870289087296)\n",
      " state (8)  A[0]:(0.656428277493) A[1]:(-3.74466180801e-05) A[2]:(0.729003667831) A[3]:(0.590770900249)\n",
      " state (9)  A[0]:(0.656707525253) A[1]:(0.809933722019) A[2]:(0.810025691986) A[3]:(0.000271052122116)\n",
      " state (10)  A[0]:(0.729977369308) A[1]:(0.89996868372) A[2]:(-3.38554382324e-05) A[3]:(0.729158401489)\n",
      " state (11)  A[0]:(0.138106673956) A[1]:(0.883035302162) A[2]:(-0.930065989494) A[3]:(0.803690433502)\n",
      " state (12)  A[0]:(-0.426016241312) A[1]:(0.816283047199) A[2]:(-0.95550596714) A[3]:(0.715507149696)\n",
      " state (13)  A[0]:(0.00141732301563) A[1]:(0.809248745441) A[2]:(0.899951398373) A[3]:(0.72919690609)\n",
      " state (14)  A[0]:(0.810665130615) A[1]:(0.900109887123) A[2]:(0.999999821186) A[3]:(0.810184657574)\n",
      " state (15)  A[0]:(0.979920864105) A[1]:(0.937803030014) A[2]:(1.0) A[3]:(0.878498911858)\n",
      "Episode 678000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6020. Times reached goal: 993.               Steps done: 4731568. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00793138571154.\n",
      " state (0)  A[0]:(0.532338857651) A[1]:(0.590850710869) A[2]:(0.590555071831) A[3]:(0.530639648438)\n",
      " state (1)  A[0]:(0.532101988792) A[1]:(-0.000232189893723) A[2]:(0.656338572502) A[3]:(0.590004563332)\n",
      " state (2)  A[0]:(0.590969800949) A[1]:(0.729327082634) A[2]:(0.590872526169) A[3]:(0.65581703186)\n",
      " state (3)  A[0]:(0.656745672226) A[1]:(-0.00365991750732) A[2]:(0.525928974152) A[3]:(0.549337625504)\n",
      " state (4)  A[0]:(0.591037392616) A[1]:(0.656873106956) A[2]:(0.000278234481812) A[3]:(0.531360507011)\n",
      " state (5)  A[0]:(-0.0353239811957) A[1]:(0.999897420406) A[2]:(-0.823314726353) A[3]:(0.659431695938)\n",
      " state (6)  A[0]:(0.00155423453543) A[1]:(0.810397386551) A[2]:(0.000564217509236) A[3]:(0.656902909279)\n",
      " state (7)  A[0]:(0.546597719193) A[1]:(-0.534707605839) A[2]:(0.437766104937) A[3]:(0.871000289917)\n",
      " state (8)  A[0]:(0.659534931183) A[1]:(-0.000308960676193) A[2]:(0.728925704956) A[3]:(0.596254229546)\n",
      " state (9)  A[0]:(0.660074591637) A[1]:(0.810331463814) A[2]:(0.810539186001) A[3]:(0.00772276055068)\n",
      " state (10)  A[0]:(0.733273208141) A[1]:(0.900246858597) A[2]:(0.00343726715073) A[3]:(0.732584416866)\n",
      " state (11)  A[0]:(0.145188137889) A[1]:(0.883243739605) A[2]:(-0.929560899734) A[3]:(0.806181550026)\n",
      " state (12)  A[0]:(-0.421913951635) A[1]:(0.816397190094) A[2]:(-0.955382227898) A[3]:(0.718056976795)\n",
      " state (13)  A[0]:(0.00419548666105) A[1]:(0.809223473072) A[2]:(0.899748206139) A[3]:(0.730810880661)\n",
      " state (14)  A[0]:(0.811221837997) A[1]:(0.900115072727) A[2]:(0.999999821186) A[3]:(0.810999155045)\n",
      " state (15)  A[0]:(0.979963243008) A[1]:(0.937862634659) A[2]:(1.0) A[3]:(0.878897249699)\n",
      "Episode 679000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               5998. Times reached goal: 989.               Steps done: 4737566. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00788395564501.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5903,  0.5904,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.3152e-01,  8.8662e-06,  6.5601e-01,  5.9048e-01]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7290,  0.5905,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  0.8100, -0.0000,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0002,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531471371651) A[1]:(0.590291202068) A[2]:(0.590403914452) A[3]:(0.531315147877)\n",
      " state (1)  A[0]:(0.53150343895) A[1]:(-1.62869691849e-05) A[2]:(0.656013727188) A[3]:(0.590388417244)\n",
      " state (2)  A[0]:(0.590494394302) A[1]:(0.728997707367) A[2]:(0.590530276299) A[3]:(0.655933141708)\n",
      " state (3)  A[0]:(0.656199574471) A[1]:(-0.00435833865777) A[2]:(0.525637209415) A[3]:(0.549367547035)\n",
      " state (4)  A[0]:(0.590361356735) A[1]:(0.65610909462) A[2]:(0.000112175941467) A[3]:(0.531203150749)\n",
      " state (5)  A[0]:(-0.0363172106445) A[1]:(0.999897181988) A[2]:(-0.823414504528) A[3]:(0.659056186676)\n",
      " state (6)  A[0]:(0.000313058495522) A[1]:(0.81002086401) A[2]:(8.77380371094e-05) A[3]:(0.655978441238)\n",
      " state (7)  A[0]:(0.54477250576) A[1]:(-0.535039186478) A[2]:(0.437411040068) A[3]:(0.870043158531)\n",
      " state (8)  A[0]:(0.656059920788) A[1]:(0.00010721385479) A[2]:(0.729060173035) A[3]:(0.59030354023)\n",
      " state (9)  A[0]:(0.656086742878) A[1]:(0.810014605522) A[2]:(0.810045659542) A[3]:(-1.13248825073e-06)\n",
      " state (10)  A[0]:(0.72932600975) A[1]:(0.900017917156) A[2]:(3.981590271e-05) A[3]:(0.729023814201)\n",
      " state (11)  A[0]:(0.136498883367) A[1]:(0.883102595806) A[2]:(-0.930122256279) A[3]:(0.803550004959)\n",
      " state (12)  A[0]:(-0.427495539188) A[1]:(0.816390693188) A[2]:(-0.955590367317) A[3]:(0.715266406536)\n",
      " state (13)  A[0]:(-0.000388279528124) A[1]:(0.809383273125) A[2]:(0.8999979496) A[3]:(0.72896116972)\n",
      " state (14)  A[0]:(0.810075342655) A[1]:(0.900230526924) A[2]:(0.999999821186) A[3]:(0.810012578964)\n",
      " state (15)  A[0]:(0.979855895042) A[1]:(0.937882661819) A[2]:(1.0) A[3]:(0.878385901451)\n",
      "Episode 680000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6023. Times reached goal: 995.               Steps done: 4743589. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00783661329476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531461119652) A[1]:(0.590509295464) A[2]:(0.590715110302) A[3]:(0.53153938055)\n",
      " state (1)  A[0]:(0.531541109085) A[1]:(3.1515955925e-05) A[2]:(0.65617954731) A[3]:(0.590651452541)\n",
      " state (2)  A[0]:(0.590633273125) A[1]:(0.729063272476) A[2]:(0.590475380421) A[3]:(0.656249046326)\n",
      " state (3)  A[0]:(0.656317591667) A[1]:(-0.00424759509042) A[2]:(0.525482177734) A[3]:(0.549709677696)\n",
      " state (4)  A[0]:(0.590512633324) A[1]:(0.656049132347) A[2]:(-0.000116586685181) A[3]:(0.531543493271)\n",
      " state (5)  A[0]:(-0.0362314023077) A[1]:(0.999897122383) A[2]:(-0.823465943336) A[3]:(0.659325957298)\n",
      " state (6)  A[0]:(0.00019559264183) A[1]:(0.810034036636) A[2]:(0.000112295150757) A[3]:(0.656115293503)\n",
      " state (7)  A[0]:(0.544662594795) A[1]:(-0.534903287888) A[2]:(0.437435895205) A[3]:(0.870099246502)\n",
      " state (8)  A[0]:(0.656033039093) A[1]:(0.000163599848747) A[2]:(0.729005217552) A[3]:(0.590707063675)\n",
      " state (9)  A[0]:(0.655948877335) A[1]:(0.810043156147) A[2]:(0.810026884079) A[3]:(0.000288784503937)\n",
      " state (10)  A[0]:(0.729236364365) A[1]:(0.900012135506) A[2]:(6.62803649902e-05) A[3]:(0.729050636292)\n",
      " state (11)  A[0]:(0.136683374643) A[1]:(0.883057653904) A[2]:(-0.93013292551) A[3]:(0.803569078445)\n",
      " state (12)  A[0]:(-0.427008926868) A[1]:(0.816261053085) A[2]:(-0.955611169338) A[3]:(0.715331792831)\n",
      " state (13)  A[0]:(0.000141814351082) A[1]:(0.809209406376) A[2]:(0.900018930435) A[3]:(0.729041099548)\n",
      " state (14)  A[0]:(0.810021340847) A[1]:(0.900157094002) A[2]:(0.999999821186) A[3]:(0.81003922224)\n",
      " state (15)  A[0]:(0.979816734791) A[1]:(0.937862277031) A[2]:(1.0) A[3]:(0.878367900848)\n",
      "Episode 681000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6037. Times reached goal: 993.               Steps done: 4749626. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0077894461775.\n",
      " state (0)  A[0]:(0.531316041946) A[1]:(0.590211331844) A[2]:(0.590543210506) A[3]:(0.531378567219)\n",
      " state (1)  A[0]:(0.531157374382) A[1]:(0.000272914767265) A[2]:(0.656118810177) A[3]:(0.59091091156)\n",
      " state (2)  A[0]:(0.590016722679) A[1]:(0.728860378265) A[2]:(0.590607762337) A[3]:(0.656552433968)\n",
      " state (3)  A[0]:(0.655704498291) A[1]:(-0.00461701769382) A[2]:(0.525711417198) A[3]:(0.550156474113)\n",
      " state (4)  A[0]:(0.589649796486) A[1]:(0.655819654465) A[2]:(0.000307083129883) A[3]:(0.532099127769)\n",
      " state (5)  A[0]:(-0.0382121615112) A[1]:(0.999897122383) A[2]:(-0.823358654976) A[3]:(0.659868240356)\n",
      " state (6)  A[0]:(-0.00249184155837) A[1]:(0.810001909733) A[2]:(0.000433444947703) A[3]:(0.656594097614)\n",
      " state (7)  A[0]:(0.542396605015) A[1]:(-0.535072684288) A[2]:(0.437725245953) A[3]:(0.870279431343)\n",
      " state (8)  A[0]:(0.653891682625) A[1]:(-6.77853822708e-05) A[2]:(0.729095220566) A[3]:(0.591153383255)\n",
      " state (9)  A[0]:(0.653209805489) A[1]:(0.809997200966) A[2]:(0.809875607491) A[3]:(0.000665396335535)\n",
      " state (10)  A[0]:(0.726203978062) A[1]:(0.900019705296) A[2]:(-0.00172102276701) A[3]:(0.728938698769)\n",
      " state (11)  A[0]:(0.129460453987) A[1]:(0.883126378059) A[2]:(-0.930502414703) A[3]:(0.803386807442)\n",
      " state (12)  A[0]:(-0.432258605957) A[1]:(0.81648015976) A[2]:(-0.955764293671) A[3]:(0.71541082859)\n",
      " state (13)  A[0]:(-0.00477659050375) A[1]:(0.809568703175) A[2]:(0.900281131268) A[3]:(0.729461193085)\n",
      " state (14)  A[0]:(0.808787763119) A[1]:(0.900385797024) A[2]:(0.999999821186) A[3]:(0.810451984406)\n",
      " state (15)  A[0]:(0.979714334011) A[1]:(0.937955856323) A[2]:(1.0) A[3]:(0.878676116467)\n",
      "Episode 682000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6004. Times reached goal: 987.               Steps done: 4755630. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00774281845913.\n",
      " state (0)  A[0]:(0.5323741436) A[1]:(0.59048473835) A[2]:(0.590478897095) A[3]:(0.531575322151)\n",
      " state (1)  A[0]:(0.531941831112) A[1]:(0.000631958188023) A[2]:(0.655888080597) A[3]:(0.590850830078)\n",
      " state (2)  A[0]:(0.590745151043) A[1]:(0.729024767876) A[2]:(0.590461611748) A[3]:(0.65639102459)\n",
      " state (3)  A[0]:(0.656391501427) A[1]:(-0.00442200992256) A[2]:(0.525424838066) A[3]:(0.54992634058)\n",
      " state (4)  A[0]:(0.590581417084) A[1]:(0.655940890312) A[2]:(-0.000282883644104) A[3]:(0.53199672699)\n",
      " state (5)  A[0]:(-0.0361078232527) A[1]:(0.999897062778) A[2]:(-0.823565483093) A[3]:(0.660169363022)\n",
      " state (6)  A[0]:(0.000198304653168) A[1]:(0.809852480888) A[2]:(0.000183701515198) A[3]:(0.657033443451)\n",
      " state (7)  A[0]:(0.544754385948) A[1]:(-0.53525698185) A[2]:(0.43763667345) A[3]:(0.870695054531)\n",
      " state (8)  A[0]:(0.656750440598) A[1]:(-0.0010419782484) A[2]:(0.729010283947) A[3]:(0.594257235527)\n",
      " state (9)  A[0]:(0.65611577034) A[1]:(0.809875011444) A[2]:(0.810398578644) A[3]:(0.00432721059769)\n",
      " state (10)  A[0]:(0.729027509689) A[1]:(0.899980127811) A[2]:(0.00157499185298) A[3]:(0.730606436729)\n",
      " state (11)  A[0]:(0.135540708899) A[1]:(0.88299369812) A[2]:(-0.930052161217) A[3]:(0.8046464324)\n",
      " state (12)  A[0]:(-0.428701967001) A[1]:(0.816057085991) A[2]:(-0.955708563328) A[3]:(0.716555476189)\n",
      " state (13)  A[0]:(-0.00221854075789) A[1]:(0.808855354786) A[2]:(0.899741351604) A[3]:(0.730015039444)\n",
      " state (14)  A[0]:(0.809384703636) A[1]:(0.899913370609) A[2]:(0.999999821186) A[3]:(0.810641467571)\n",
      " state (15)  A[0]:(0.979775846004) A[1]:(0.93766283989) A[2]:(1.0) A[3]:(0.878738999367)\n",
      "Episode 683000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6037. Times reached goal: 996.               Steps done: 4761667. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00769621587553.\n",
      " state (0)  A[0]:(0.531497955322) A[1]:(0.59028083086) A[2]:(0.590265750885) A[3]:(0.531053423882)\n",
      " state (1)  A[0]:(0.531611800194) A[1]:(0.000245571136475) A[2]:(0.656067252159) A[3]:(0.590347111225)\n",
      " state (2)  A[0]:(0.590558648109) A[1]:(0.729053556919) A[2]:(0.590437352657) A[3]:(0.656033039093)\n",
      " state (3)  A[0]:(0.656273007393) A[1]:(-0.00404355721548) A[2]:(0.525517463684) A[3]:(0.549513936043)\n",
      " state (4)  A[0]:(0.590527415276) A[1]:(0.656138181686) A[2]:(3.49283218384e-05) A[3]:(0.531537294388)\n",
      " state (5)  A[0]:(-0.0360652022064) A[1]:(0.999897122383) A[2]:(-0.823461771011) A[3]:(0.659635424614)\n",
      " state (6)  A[0]:(0.000366926164133) A[1]:(0.810003519058) A[2]:(0.0001140832901) A[3]:(0.656174778938)\n",
      " state (7)  A[0]:(0.544663786888) A[1]:(-0.534801483154) A[2]:(0.437426149845) A[3]:(0.870067000389)\n",
      " state (8)  A[0]:(0.656016111374) A[1]:(0.000329494476318) A[2]:(0.728998720646) A[3]:(0.590767741203)\n",
      " state (9)  A[0]:(0.655893802643) A[1]:(0.810075044632) A[2]:(0.809928357601) A[3]:(0.000927835411858)\n",
      " state (10)  A[0]:(0.728831171989) A[1]:(0.900029301643) A[2]:(-0.000925659842324) A[3]:(0.729298532009)\n",
      " state (11)  A[0]:(0.135168507695) A[1]:(0.883084475994) A[2]:(-0.930438399315) A[3]:(0.803655266762)\n",
      " state (12)  A[0]:(-0.4282348454) A[1]:(0.816287696362) A[2]:(-0.9558609128) A[3]:(0.715467810631)\n",
      " state (13)  A[0]:(-0.000916599994525) A[1]:(0.809225976467) A[2]:(0.899929463863) A[3]:(0.729248464108)\n",
      " state (14)  A[0]:(0.809812664986) A[1]:(0.900177896023) A[2]:(0.999999880791) A[3]:(0.81018614769)\n",
      " state (15)  A[0]:(0.979810059071) A[1]:(0.9378272295) A[2]:(1.0) A[3]:(0.878462553024)\n",
      "Episode 684000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6023. Times reached goal: 991.               Steps done: 4767690. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00765000088347.\n",
      "q_values \n",
      "tensor([[ 0.5305,  0.5907,  0.5904,  0.5309]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.6566, -0.0000,  0.5307]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6568,  0.0016,  0.7288,  0.5896]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6579,  0.8102,  0.8098,  0.0005]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0065,  0.8086,  0.9001,  0.7289]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8122,  0.8998,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530691444874) A[1]:(0.590621232986) A[2]:(0.590437352657) A[3]:(0.531039237976)\n",
      " state (1)  A[0]:(0.531120657921) A[1]:(-0.000311493873596) A[2]:(0.655974626541) A[3]:(0.589895009995)\n",
      " state (2)  A[0]:(0.590277433395) A[1]:(0.72923630476) A[2]:(0.590478658676) A[3]:(0.655492603779)\n",
      " state (3)  A[0]:(0.656137466431) A[1]:(-0.00345693645068) A[2]:(0.525542020798) A[3]:(0.548782467842)\n",
      " state (4)  A[0]:(0.590438842773) A[1]:(0.65654617548) A[2]:(-0.000158429145813) A[3]:(0.530768036842)\n",
      " state (5)  A[0]:(-0.0364203080535) A[1]:(0.999897122383) A[2]:(-0.823641061783) A[3]:(0.659038186073)\n",
      " state (6)  A[0]:(0.000198364257812) A[1]:(0.809883356094) A[2]:(-0.00056314462563) A[3]:(0.655618071556)\n",
      " state (7)  A[0]:(0.544770240784) A[1]:(-0.534121513367) A[2]:(0.436581224203) A[3]:(0.869806349277)\n",
      " state (8)  A[0]:(0.656113982201) A[1]:(0.00169503525831) A[2]:(0.728651821613) A[3]:(0.589161396027)\n",
      " state (9)  A[0]:(0.656734466553) A[1]:(0.810292541981) A[2]:(0.809620559216) A[3]:(-0.000577509345021)\n",
      " state (10)  A[0]:(0.730027318001) A[1]:(0.900045990944) A[2]:(-0.00148367777001) A[3]:(0.72882771492)\n",
      " state (11)  A[0]:(0.138582080603) A[1]:(0.883012652397) A[2]:(-0.930472135544) A[3]:(0.803268134594)\n",
      " state (12)  A[0]:(-0.424886673689) A[1]:(0.81604886055) A[2]:(-0.955899000168) A[3]:(0.714798390865)\n",
      " state (13)  A[0]:(0.00328696076758) A[1]:(0.808883488178) A[2]:(0.899784445763) A[3]:(0.728445589542)\n",
      " state (14)  A[0]:(0.811109185219) A[1]:(0.900005936623) A[2]:(0.999999880791) A[3]:(0.809455275536)\n",
      " state (15)  A[0]:(0.979932367802) A[1]:(0.937768638134) A[2]:(1.0) A[3]:(0.877883017063)\n",
      "Episode 685000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6014. Times reached goal: 988.               Steps done: 4773704. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0076041318446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531407177448) A[1]:(0.591406226158) A[2]:(0.590456485748) A[3]:(0.531639814377)\n",
      " state (1)  A[0]:(0.531448364258) A[1]:(-7.88122415543e-05) A[2]:(0.656386733055) A[3]:(0.590674459934)\n",
      " state (2)  A[0]:(0.590587854385) A[1]:(0.729227900505) A[2]:(0.590982317924) A[3]:(0.656368255615)\n",
      " state (3)  A[0]:(0.656382322311) A[1]:(-0.00408875150606) A[2]:(0.525961518288) A[3]:(0.549720287323)\n",
      " state (4)  A[0]:(0.590574324131) A[1]:(0.656282782555) A[2]:(0.000112771987915) A[3]:(0.531650304794)\n",
      " state (5)  A[0]:(-0.0365016795695) A[1]:(0.999897181988) A[2]:(-0.823714673519) A[3]:(0.659800469875)\n",
      " state (6)  A[0]:(-0.000171422958374) A[1]:(0.810088813305) A[2]:(-0.000162601470947) A[3]:(0.656281352043)\n",
      " state (7)  A[0]:(0.544274091721) A[1]:(-0.535189986229) A[2]:(0.437762826681) A[3]:(0.870120048523)\n",
      " state (8)  A[0]:(0.656065165997) A[1]:(-0.000358238787157) A[2]:(0.729066371918) A[3]:(0.59112226963)\n",
      " state (9)  A[0]:(0.65585154295) A[1]:(0.810015916824) A[2]:(0.810062587261) A[3]:(-5.7578086853e-05)\n",
      " state (10)  A[0]:(0.728823542595) A[1]:(0.900004506111) A[2]:(-4.05311584473e-06) A[3]:(0.728841006756)\n",
      " state (11)  A[0]:(0.135398700833) A[1]:(0.883013367653) A[2]:(-0.930316209793) A[3]:(0.803520202637)\n",
      " state (12)  A[0]:(-0.428027063608) A[1]:(0.816094458103) A[2]:(-0.955830812454) A[3]:(0.715423941612)\n",
      " state (13)  A[0]:(-0.000633567513432) A[1]:(0.808954834938) A[2]:(0.900031507015) A[3]:(0.729222774506)\n",
      " state (14)  A[0]:(0.80995118618) A[1]:(0.900030016899) A[2]:(0.999999880791) A[3]:(0.810127139091)\n",
      " state (15)  A[0]:(0.979829490185) A[1]:(0.937734365463) A[2]:(1.0) A[3]:(0.878397166729)\n",
      "Episode 686000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6017. Times reached goal: 994.               Steps done: 4779721. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00755851515872.\n",
      " state (0)  A[0]:(0.528528571129) A[1]:(0.590501308441) A[2]:(0.590646803379) A[3]:(0.532309055328)\n",
      " state (1)  A[0]:(0.526804685593) A[1]:(-0.00220377393998) A[2]:(0.656098723412) A[3]:(0.591245412827)\n",
      " state (2)  A[0]:(0.585648179054) A[1]:(0.728682994843) A[2]:(0.590259611607) A[3]:(0.656708240509)\n",
      " state (3)  A[0]:(0.651722669601) A[1]:(-0.00501254480332) A[2]:(0.525029182434) A[3]:(0.550050139427)\n",
      " state (4)  A[0]:(0.584833800793) A[1]:(0.655875205994) A[2]:(-0.00129115511663) A[3]:(0.531847238541)\n",
      " state (5)  A[0]:(-0.045993398875) A[1]:(0.999897003174) A[2]:(-0.824143111706) A[3]:(0.659680724144)\n",
      " state (6)  A[0]:(-0.00998016819358) A[1]:(0.809716403484) A[2]:(-0.00188350456301) A[3]:(0.656030058861)\n",
      " state (7)  A[0]:(0.537503242493) A[1]:(-0.535434007645) A[2]:(0.436164885759) A[3]:(0.870151042938)\n",
      " state (8)  A[0]:(0.651523888111) A[1]:(-0.000326946377754) A[2]:(0.728536128998) A[3]:(0.592881858349)\n",
      " state (9)  A[0]:(0.653561353683) A[1]:(0.810260355473) A[2]:(0.810673058033) A[3]:(0.00671725301072)\n",
      " state (10)  A[0]:(0.729721367359) A[1]:(0.900341689587) A[2]:(0.00684310263023) A[3]:(0.733885705471)\n",
      " state (11)  A[0]:(0.139103397727) A[1]:(0.883637666702) A[2]:(-0.929104983807) A[3]:(0.80763065815)\n",
      " state (12)  A[0]:(-0.430051922798) A[1]:(0.817298173904) A[2]:(-0.955388367176) A[3]:(0.718866705894)\n",
      " state (13)  A[0]:(-0.00948350038379) A[1]:(0.810387909412) A[2]:(0.900458097458) A[3]:(0.730173766613)\n",
      " state (14)  A[0]:(0.806063354015) A[1]:(0.900831580162) A[2]:(0.999999880791) A[3]:(0.809808790684)\n",
      " state (15)  A[0]:(0.979387044907) A[1]:(0.938160777092) A[2]:(1.0) A[3]:(0.877889454365)\n",
      "Episode 687000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 988.               Steps done: 4785721. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00751329984934.\n",
      " state (0)  A[0]:(0.531188249588) A[1]:(0.590492784977) A[2]:(0.590204238892) A[3]:(0.531131982803)\n",
      " state (1)  A[0]:(0.531346678734) A[1]:(-1.560151577e-05) A[2]:(0.656120717525) A[3]:(0.59006768465)\n",
      " state (2)  A[0]:(0.590319216251) A[1]:(0.729010820389) A[2]:(0.590711772442) A[3]:(0.655752062798)\n",
      " state (3)  A[0]:(0.656071066856) A[1]:(-0.0040777544491) A[2]:(0.52584540844) A[3]:(0.549020290375)\n",
      " state (4)  A[0]:(0.590396940708) A[1]:(0.655963182449) A[2]:(0.000462651223643) A[3]:(0.530961513519)\n",
      " state (5)  A[0]:(-0.0360277593136) A[1]:(0.999897062778) A[2]:(-0.823474466801) A[3]:(0.659247398376)\n",
      " state (6)  A[0]:(0.000361531943781) A[1]:(0.809992730618) A[2]:(0.000311493873596) A[3]:(0.655451536179)\n",
      " state (7)  A[0]:(0.544553041458) A[1]:(-0.53468811512) A[2]:(0.437663376331) A[3]:(0.869674861431)\n",
      " state (8)  A[0]:(0.656221866608) A[1]:(-3.96817922592e-05) A[2]:(0.728934526443) A[3]:(0.590448319912)\n",
      " state (9)  A[0]:(0.655832529068) A[1]:(0.810023665428) A[2]:(0.80997556448) A[3]:(-0.000710368040018)\n",
      " state (10)  A[0]:(0.72871619463) A[1]:(0.900022089481) A[2]:(-0.000464320153696) A[3]:(0.728219628334)\n",
      " state (11)  A[0]:(0.135640755296) A[1]:(0.883086800575) A[2]:(-0.930424571037) A[3]:(0.802942574024)\n",
      " state (12)  A[0]:(-0.426893115044) A[1]:(0.816263914108) A[2]:(-0.955892801285) A[3]:(0.714752912521)\n",
      " state (13)  A[0]:(0.00175970606506) A[1]:(0.809176266193) A[2]:(0.900128185749) A[3]:(0.728780984879)\n",
      " state (14)  A[0]:(0.810909628868) A[1]:(0.900209546089) A[2]:(0.999999880791) A[3]:(0.809891045094)\n",
      " state (15)  A[0]:(0.97993683815) A[1]:(0.937881112099) A[2]:(1.0) A[3]:(0.878256082535)\n",
      "Episode 688000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6028. Times reached goal: 994.               Steps done: 4791749. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00746814590856.\n",
      " state (0)  A[0]:(0.530928850174) A[1]:(0.590539693832) A[2]:(0.590388655663) A[3]:(0.531174838543)\n",
      " state (1)  A[0]:(0.531107485294) A[1]:(1.15483999252e-05) A[2]:(0.656032562256) A[3]:(0.590527892113)\n",
      " state (2)  A[0]:(0.590357661247) A[1]:(0.729014873505) A[2]:(0.59043854475) A[3]:(0.656123399734)\n",
      " state (3)  A[0]:(0.65608727932) A[1]:(-0.00408159894869) A[2]:(0.525509595871) A[3]:(0.549433231354)\n",
      " state (4)  A[0]:(0.590287089348) A[1]:(0.656191706657) A[2]:(-7.35521316528e-05) A[3]:(0.531407475471)\n",
      " state (5)  A[0]:(-0.0364809185266) A[1]:(0.999897122383) A[2]:(-0.823583424091) A[3]:(0.659717977047)\n",
      " state (6)  A[0]:(0.000111356377602) A[1]:(0.809970617294) A[2]:(0.000164985656738) A[3]:(0.655957102776)\n",
      " state (7)  A[0]:(0.544435381889) A[1]:(-0.534778118134) A[2]:(0.437650352716) A[3]:(0.86989402771)\n",
      " state (8)  A[0]:(0.656267642975) A[1]:(-0.000105038285255) A[2]:(0.729004740715) A[3]:(0.59095788002)\n",
      " state (9)  A[0]:(0.656333446503) A[1]:(0.809992909431) A[2]:(0.810144901276) A[3]:(0.000571459473576)\n",
      " state (10)  A[0]:(0.729588568211) A[1]:(0.900006473064) A[2]:(0.00072050082963) A[3]:(0.72921705246)\n",
      " state (11)  A[0]:(0.137630090117) A[1]:(0.883069753647) A[2]:(-0.930264234543) A[3]:(0.803782343864)\n",
      " state (12)  A[0]:(-0.426338940859) A[1]:(0.816216886044) A[2]:(-0.955883204937) A[3]:(0.715551018715)\n",
      " state (13)  A[0]:(0.000869989162311) A[1]:(0.8090903759) A[2]:(0.900052547455) A[3]:(0.72916829586)\n",
      " state (14)  A[0]:(0.810241878033) A[1]:(0.900136351585) A[2]:(0.999999880791) A[3]:(0.810016870499)\n",
      " state (15)  A[0]:(0.979841351509) A[1]:(0.937800347805) A[2]:(1.0) A[3]:(0.878300130367)\n",
      "Episode 689000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6020. Times reached goal: 992.               Steps done: 4797769. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00742332272334.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5906,  0.5904,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6561,  0.0000,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558,  0.0004,  0.7289,  0.5901]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556,  0.8102,  0.8099, -0.0009]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0006,  0.8090,  0.9001,  0.7287]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9001,  1.0000,  0.8096]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53164100647) A[1]:(0.590605258942) A[2]:(0.590410113335) A[3]:(0.531549096107)\n",
      " state (1)  A[0]:(0.531642496586) A[1]:(-5.50597906113e-05) A[2]:(0.656120598316) A[3]:(0.59045958519)\n",
      " state (2)  A[0]:(0.590494155884) A[1]:(0.729008316994) A[2]:(0.590476930141) A[3]:(0.656118869781)\n",
      " state (3)  A[0]:(0.656205654144) A[1]:(-0.00392195070162) A[2]:(0.525551319122) A[3]:(0.549523115158)\n",
      " state (4)  A[0]:(0.590388774872) A[1]:(0.65617275238) A[2]:(2.34842300415e-05) A[3]:(0.531557202339)\n",
      " state (5)  A[0]:(-0.0366786010563) A[1]:(0.999897122383) A[2]:(-0.823606312275) A[3]:(0.659736633301)\n",
      " state (6)  A[0]:(-2.32309103012e-05) A[1]:(0.809935450554) A[2]:(8.03470611572e-05) A[3]:(0.655795753002)\n",
      " state (7)  A[0]:(0.544376969337) A[1]:(-0.534723877907) A[2]:(0.437583386898) A[3]:(0.86983859539)\n",
      " state (8)  A[0]:(0.656089842319) A[1]:(0.000184550881386) A[2]:(0.728911399841) A[3]:(0.590712428093)\n",
      " state (9)  A[0]:(0.655925393105) A[1]:(0.810145437717) A[2]:(0.810059785843) A[3]:(-0.000179499387741)\n",
      " state (10)  A[0]:(0.72919523716) A[1]:(0.900037169456) A[2]:(0.000645279767923) A[3]:(0.728960156441)\n",
      " state (11)  A[0]:(0.136617109179) A[1]:(0.883029699326) A[2]:(-0.930289685726) A[3]:(0.803638577461)\n",
      " state (12)  A[0]:(-0.427442282438) A[1]:(0.816073656082) A[2]:(-0.95593804121) A[3]:(0.71529841423)\n",
      " state (13)  A[0]:(-0.000524178089108) A[1]:(0.80891919136) A[2]:(0.899932265282) A[3]:(0.728821396828)\n",
      " state (14)  A[0]:(0.809868276119) A[1]:(0.90006506443) A[2]:(0.999999880791) A[3]:(0.809669137001)\n",
      " state (15)  A[0]:(0.979812383652) A[1]:(0.937762737274) A[2]:(1.0) A[3]:(0.878028333187)\n",
      "Episode 690000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6025. Times reached goal: 996.               Steps done: 4803794. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00737873166937.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.5320956707) A[1]:(0.590663313866) A[2]:(0.59051990509) A[3]:(0.53167873621)\n",
      " state (1)  A[0]:(0.532167077065) A[1]:(-0.000219061970711) A[2]:(0.656137943268) A[3]:(0.590585947037)\n",
      " state (2)  A[0]:(0.590970575809) A[1]:(0.729127883911) A[2]:(0.590549051762) A[3]:(0.656345248222)\n",
      " state (3)  A[0]:(0.656593680382) A[1]:(-0.00390499341302) A[2]:(0.525505423546) A[3]:(0.549789607525)\n",
      " state (4)  A[0]:(0.590709805489) A[1]:(0.656074404716) A[2]:(-0.000219464302063) A[3]:(0.531933069229)\n",
      " state (5)  A[0]:(-0.0366775915027) A[1]:(0.999897062778) A[2]:(-0.823741316795) A[3]:(0.660278141499)\n",
      " state (6)  A[0]:(0.000118166208267) A[1]:(0.809916496277) A[2]:(-0.000122427940369) A[3]:(0.656411767006)\n",
      " state (7)  A[0]:(0.544427752495) A[1]:(-0.535017669201) A[2]:(0.437598615885) A[3]:(0.870032072067)\n",
      " state (8)  A[0]:(0.655979990959) A[1]:(-0.000160411000252) A[2]:(0.728968977928) A[3]:(0.59057289362)\n",
      " state (9)  A[0]:(0.655995368958) A[1]:(0.809920608997) A[2]:(0.80995029211) A[3]:(-3.11732292175e-05)\n",
      " state (10)  A[0]:(0.729202091694) A[1]:(0.899928450584) A[2]:(-0.000215530395508) A[3]:(0.729089736938)\n",
      " state (11)  A[0]:(0.136721998453) A[1]:(0.882995128632) A[2]:(-0.930453300476) A[3]:(0.803791582584)\n",
      " state (12)  A[0]:(-0.426859349012) A[1]:(0.816172003746) A[2]:(-0.956025958061) A[3]:(0.715737223625)\n",
      " state (13)  A[0]:(0.000740080955438) A[1]:(0.809151411057) A[2]:(0.899916768074) A[3]:(0.729448676109)\n",
      " state (14)  A[0]:(0.810390949249) A[1]:(0.900254487991) A[2]:(0.999999880791) A[3]:(0.810214102268)\n",
      " state (15)  A[0]:(0.979877769947) A[1]:(0.937901258469) A[2]:(1.0) A[3]:(0.878411948681)\n",
      "Episode 691000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5998. Times reached goal: 990.               Steps done: 4809792. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00733460650049.\n",
      " state (0)  A[0]:(0.531616210938) A[1]:(0.590408623219) A[2]:(0.590385377407) A[3]:(0.531264066696)\n",
      " state (1)  A[0]:(0.531523942947) A[1]:(0.000383540958865) A[2]:(0.655933380127) A[3]:(0.590558290482)\n",
      " state (2)  A[0]:(0.590495824814) A[1]:(0.729013979435) A[2]:(0.590471148491) A[3]:(0.656203866005)\n",
      " state (3)  A[0]:(0.656220197678) A[1]:(-0.00402122037485) A[2]:(0.5255163908) A[3]:(0.549520254135)\n",
      " state (4)  A[0]:(0.590315580368) A[1]:(0.65602761507) A[2]:(-6.31809234619e-05) A[3]:(0.531598925591)\n",
      " state (5)  A[0]:(-0.0372177623212) A[1]:(0.999897062778) A[2]:(-0.823709726334) A[3]:(0.660161614418)\n",
      " state (6)  A[0]:(-0.000419527263148) A[1]:(0.8099629879) A[2]:(-0.000122427940369) A[3]:(0.656277656555)\n",
      " state (7)  A[0]:(0.54379940033) A[1]:(-0.534928560257) A[2]:(0.437573820353) A[3]:(0.869920372963)\n",
      " state (8)  A[0]:(0.655409812927) A[1]:(-0.000394895643694) A[2]:(0.728850007057) A[3]:(0.59087240696)\n",
      " state (9)  A[0]:(0.655075550079) A[1]:(0.809877157211) A[2]:(0.809849619865) A[3]:(0.000615745724645)\n",
      " state (10)  A[0]:(0.728019714355) A[1]:(0.900008916855) A[2]:(-0.00069653976243) A[3]:(0.729228436947)\n",
      " state (11)  A[0]:(0.133208841085) A[1]:(0.883236050606) A[2]:(-0.930593848228) A[3]:(0.803770303726)\n",
      " state (12)  A[0]:(-0.430757433176) A[1]:(0.816723585129) A[2]:(-0.956156611443) A[3]:(0.71551322937)\n",
      " state (13)  A[0]:(-0.00477024260908) A[1]:(0.809810459614) A[2]:(0.899772047997) A[3]:(0.729163527489)\n",
      " state (14)  A[0]:(0.808517038822) A[1]:(0.900562107563) A[2]:(0.999999880791) A[3]:(0.810056388378)\n",
      " state (15)  A[0]:(0.979696333408) A[1]:(0.937982738018) A[2]:(1.0) A[3]:(0.878390729427)\n",
      "Episode 692000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6017. Times reached goal: 991.               Steps done: 4815809. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00729060667939.\n",
      " state (0)  A[0]:(0.530642151833) A[1]:(0.590309500694) A[2]:(0.590061485767) A[3]:(0.532188296318)\n",
      " state (1)  A[0]:(0.530511021614) A[1]:(-0.001768482849) A[2]:(0.656035900116) A[3]:(0.591767907143)\n",
      " state (2)  A[0]:(0.589625000954) A[1]:(0.728676438332) A[2]:(0.590472102165) A[3]:(0.657455265522)\n",
      " state (3)  A[0]:(0.655466198921) A[1]:(-0.00439535221085) A[2]:(0.525479793549) A[3]:(0.551125764847)\n",
      " state (4)  A[0]:(0.589536905289) A[1]:(0.656142950058) A[2]:(-0.000253796577454) A[3]:(0.533375859261)\n",
      " state (5)  A[0]:(-0.0378570146859) A[1]:(0.999897301197) A[2]:(-0.82388228178) A[3]:(0.661757588387)\n",
      " state (6)  A[0]:(-0.000555083097424) A[1]:(0.810392737389) A[2]:(-0.000913738971576) A[3]:(0.657932341099)\n",
      " state (7)  A[0]:(0.544182300568) A[1]:(-0.534260869026) A[2]:(0.437155634165) A[3]:(0.870841741562)\n",
      " state (8)  A[0]:(0.656273722649) A[1]:(0.000747546437196) A[2]:(0.72872364521) A[3]:(0.595067620277)\n",
      " state (9)  A[0]:(0.656339526176) A[1]:(0.810450017452) A[2]:(0.810091912746) A[3]:(0.00845996197313)\n",
      " state (10)  A[0]:(0.72968429327) A[1]:(0.900266826153) A[2]:(0.00133478560019) A[3]:(0.732790768147)\n",
      " state (11)  A[0]:(0.137251213193) A[1]:(0.883339881897) A[2]:(-0.930242061615) A[3]:(0.805915474892)\n",
      " state (12)  A[0]:(-0.428102165461) A[1]:(0.816594600677) A[2]:(-0.955954790115) A[3]:(0.717313528061)\n",
      " state (13)  A[0]:(-0.00240486394614) A[1]:(0.809511840343) A[2]:(0.900376021862) A[3]:(0.730005919933)\n",
      " state (14)  A[0]:(0.809188246727) A[1]:(0.900361716747) A[2]:(0.999999880791) A[3]:(0.81029188633)\n",
      " state (15)  A[0]:(0.979759216309) A[1]:(0.937829852104) A[2]:(1.0) A[3]:(0.878409326077)\n",
      "Episode 693000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6028. Times reached goal: 992.               Steps done: 4821837. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00724679109517.\n",
      " state (0)  A[0]:(0.531121373177) A[1]:(0.590318381786) A[2]:(0.590564012527) A[3]:(0.531037390232)\n",
      " state (1)  A[0]:(0.531212151051) A[1]:(-0.000134766101837) A[2]:(0.65606379509) A[3]:(0.590071856976)\n",
      " state (2)  A[0]:(0.590220987797) A[1]:(0.728828191757) A[2]:(0.59052926302) A[3]:(0.655703186989)\n",
      " state (3)  A[0]:(0.655997931957) A[1]:(-0.00414708862081) A[2]:(0.52546274662) A[3]:(0.548832774162)\n",
      " state (4)  A[0]:(0.590143501759) A[1]:(0.655943632126) A[2]:(-0.000223278999329) A[3]:(0.530838549137)\n",
      " state (5)  A[0]:(-0.0372644886374) A[1]:(0.999897062778) A[2]:(-0.823809206486) A[3]:(0.659487009048)\n",
      " state (6)  A[0]:(-4.10228967667e-05) A[1]:(0.809960126877) A[2]:(-0.000571966113057) A[3]:(0.65504026413)\n",
      " state (7)  A[0]:(0.544280052185) A[1]:(-0.53425270319) A[2]:(0.43709525466) A[3]:(0.86930668354)\n",
      " state (8)  A[0]:(0.655801773071) A[1]:(0.000802933995146) A[2]:(0.72883439064) A[3]:(0.589232861996)\n",
      " state (9)  A[0]:(0.6556879282) A[1]:(0.810144543648) A[2]:(0.809939622879) A[3]:(-0.00104552472476)\n",
      " state (10)  A[0]:(0.728671431541) A[1]:(0.899986028671) A[2]:(-0.000617504061665) A[3]:(0.728386461735)\n",
      " state (11)  A[0]:(0.135275125504) A[1]:(0.882950782776) A[2]:(-0.930629968643) A[3]:(0.802964448929)\n",
      " state (12)  A[0]:(-0.42784050107) A[1]:(0.815901041031) A[2]:(-0.956154942513) A[3]:(0.714424371719)\n",
      " state (13)  A[0]:(0.000148817896843) A[1]:(0.808660030365) A[2]:(0.900117278099) A[3]:(0.728290677071)\n",
      " state (14)  A[0]:(0.810397624969) A[1]:(0.899841785431) A[2]:(0.999999880791) A[3]:(0.809522807598)\n",
      " state (15)  A[0]:(0.979905903339) A[1]:(0.937483072281) A[2]:(1.0) A[3]:(0.878078341484)\n",
      "Episode 694000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6013. Times reached goal: 994.               Steps done: 4827850. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00720334688623.\n",
      "q_values \n",
      "tensor([[ 0.5304,  0.5907,  0.5905,  0.5323]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5900,  0.6560,  0.0005,  0.5326]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6590, -0.0019,  0.7288,  0.5976]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556,  0.8099,  0.8104, -0.0011]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7258,  0.8996, -0.0020,  0.7239]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8119,  0.8999,  1.0000,  0.8080]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531633973122) A[1]:(0.59036231041) A[2]:(0.590576648712) A[3]:(0.532030820847)\n",
      " state (1)  A[0]:(0.531529068947) A[1]:(0.000510826648679) A[2]:(0.656132817268) A[3]:(0.590198993683)\n",
      " state (2)  A[0]:(0.590397953987) A[1]:(0.728991389275) A[2]:(0.590758681297) A[3]:(0.655305445194)\n",
      " state (3)  A[0]:(0.656065106392) A[1]:(-0.00386675749905) A[2]:(0.525887489319) A[3]:(0.547990202904)\n",
      " state (4)  A[0]:(0.589812338352) A[1]:(0.656090736389) A[2]:(0.000678539159708) A[3]:(0.529606342316)\n",
      " state (5)  A[0]:(-0.0396078936756) A[1]:(0.999897181988) A[2]:(-0.823414742947) A[3]:(0.658189475536)\n",
      " state (6)  A[0]:(-0.00351008819416) A[1]:(0.810241222382) A[2]:(0.0013313285308) A[3]:(0.652973234653)\n",
      " state (7)  A[0]:(0.541102290154) A[1]:(-0.534269213676) A[2]:(0.438897311687) A[3]:(0.86797106266)\n",
      " state (8)  A[0]:(0.652830600739) A[1]:(-0.000198945403099) A[2]:(0.729305565357) A[3]:(0.584556758404)\n",
      " state (9)  A[0]:(0.651210725307) A[1]:(0.80991601944) A[2]:(0.809875011444) A[3]:(-0.0134247820824)\n",
      " state (10)  A[0]:(0.724053263664) A[1]:(0.899980187416) A[2]:(-0.00258623971604) A[3]:(0.721226215363)\n",
      " state (11)  A[0]:(0.126762092113) A[1]:(0.883150517941) A[2]:(-0.931011795998) A[3]:(0.79785656929)\n",
      " state (12)  A[0]:(-0.431346148252) A[1]:(0.816478252411) A[2]:(-0.956310153008) A[3]:(0.709057271481)\n",
      " state (13)  A[0]:(-0.000480279297335) A[1]:(0.809423744678) A[2]:(0.899901986122) A[3]:(0.724616289139)\n",
      " state (14)  A[0]:(0.810663342476) A[1]:(0.900376141071) A[2]:(0.999999880791) A[3]:(0.807451605797)\n",
      " state (15)  A[0]:(0.979942083359) A[1]:(0.937928974628) A[2]:(1.0) A[3]:(0.876887381077)\n",
      "Episode 695000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6026. Times reached goal: 995.               Steps done: 4833876. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00716007004198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53164601326) A[1]:(0.590684235096) A[2]:(0.590639710426) A[3]:(0.532252430916)\n",
      " state (1)  A[0]:(0.531683444977) A[1]:(3.04579734802e-05) A[2]:(0.656165122986) A[3]:(0.591223061085)\n",
      " state (2)  A[0]:(0.590503394604) A[1]:(0.729105353355) A[2]:(0.590623497963) A[3]:(0.656786203384)\n",
      " state (3)  A[0]:(0.656274735928) A[1]:(-0.00365738430992) A[2]:(0.52562224865) A[3]:(0.550161898136)\n",
      " state (4)  A[0]:(0.590370476246) A[1]:(0.656427919865) A[2]:(-9.51290130615e-05) A[3]:(0.532259464264)\n",
      " state (5)  A[0]:(-0.0373171046376) A[1]:(0.999897181988) A[2]:(-0.82379001379) A[3]:(0.660716712475)\n",
      " state (6)  A[0]:(-0.000296205282211) A[1]:(0.810044527054) A[2]:(9.21487808228e-05) A[3]:(0.656523406506)\n",
      " state (7)  A[0]:(0.54406452179) A[1]:(-0.5345454216) A[2]:(0.438009679317) A[3]:(0.869943678379)\n",
      " state (8)  A[0]:(0.655868411064) A[1]:(0.000528037489858) A[2]:(0.729143261909) A[3]:(0.590406894684)\n",
      " state (9)  A[0]:(0.656034290791) A[1]:(0.810130357742) A[2]:(0.809979856014) A[3]:(-0.000233560800552)\n",
      " state (10)  A[0]:(0.729027807713) A[1]:(0.899995267391) A[2]:(-0.000452399224741) A[3]:(0.728763580322)\n",
      " state (11)  A[0]:(0.135757148266) A[1]:(0.883022844791) A[2]:(-0.930664300919) A[3]:(0.803340554237)\n",
      " state (12)  A[0]:(-0.42807880044) A[1]:(0.816133320332) A[2]:(-0.956248283386) A[3]:(0.714966654778)\n",
      " state (13)  A[0]:(-0.000709503772669) A[1]:(0.809042334557) A[2]:(0.900005578995) A[3]:(0.728863418102)\n",
      " state (14)  A[0]:(0.810079097748) A[1]:(0.900139272213) A[2]:(0.999999880791) A[3]:(0.810041069984)\n",
      " state (15)  A[0]:(0.979878664017) A[1]:(0.937689185143) A[2]:(1.0) A[3]:(0.878485918045)\n",
      "Episode 696000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5997. Times reached goal: 992.               Steps done: 4839873. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00711725959736.\n",
      " state (0)  A[0]:(0.530508160591) A[1]:(0.590721607208) A[2]:(0.590292572975) A[3]:(0.530574142933)\n",
      " state (1)  A[0]:(0.531009614468) A[1]:(0.000195488333702) A[2]:(0.655834317207) A[3]:(0.589824199677)\n",
      " state (2)  A[0]:(0.590066671371) A[1]:(0.729345202446) A[2]:(0.59030932188) A[3]:(0.655624747276)\n",
      " state (3)  A[0]:(0.655924499035) A[1]:(-0.00306749879383) A[2]:(0.525390565395) A[3]:(0.548791885376)\n",
      " state (4)  A[0]:(0.590179920197) A[1]:(0.65623652935) A[2]:(-4.76837158203e-06) A[3]:(0.530992269516)\n",
      " state (5)  A[0]:(-0.037010833621) A[1]:(0.999897062778) A[2]:(-0.823666155338) A[3]:(0.660060465336)\n",
      " state (6)  A[0]:(0.000368043751223) A[1]:(0.809992074966) A[2]:(0.000494956912007) A[3]:(0.655825018883)\n",
      " state (7)  A[0]:(0.544596493244) A[1]:(-0.534492373466) A[2]:(0.43817999959) A[3]:(0.869647741318)\n",
      " state (8)  A[0]:(0.65628361702) A[1]:(0.000242695212364) A[2]:(0.72916072607) A[3]:(0.590263128281)\n",
      " state (9)  A[0]:(0.656364917755) A[1]:(0.810036659241) A[2]:(0.810056626797) A[3]:(3.81767749786e-05)\n",
      " state (10)  A[0]:(0.72957187891) A[1]:(0.900007784367) A[2]:(-1.78813934326e-05) A[3]:(0.729048609734)\n",
      " state (11)  A[0]:(0.137412473559) A[1]:(0.883110761642) A[2]:(-0.930625557899) A[3]:(0.803635776043)\n",
      " state (12)  A[0]:(-0.426840096712) A[1]:(0.816321015358) A[2]:(-0.956250667572) A[3]:(0.715244054794)\n",
      " state (13)  A[0]:(0.000413462490542) A[1]:(0.80922216177) A[2]:(0.900139808655) A[3]:(0.728911876678)\n",
      " state (14)  A[0]:(0.810374259949) A[1]:(0.900183320045) A[2]:(0.999999880791) A[3]:(0.809911429882)\n",
      " state (15)  A[0]:(0.979913055897) A[1]:(0.937649071217) A[2]:(1.0) A[3]:(0.878336012363)\n",
      "Episode 697000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 994.               Steps done: 4845884. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00707460607352.\n",
      " state (0)  A[0]:(0.53143966198) A[1]:(0.59015327692) A[2]:(0.590366661549) A[3]:(0.531353712082)\n",
      " state (1)  A[0]:(0.531200170517) A[1]:(-5.56111335754e-05) A[2]:(0.655638158321) A[3]:(0.590272903442)\n",
      " state (2)  A[0]:(0.590225756168) A[1]:(0.729357540607) A[2]:(0.589895784855) A[3]:(0.655851125717)\n",
      " state (3)  A[0]:(0.655984699726) A[1]:(-0.00285273022018) A[2]:(0.525007367134) A[3]:(0.54901099205)\n",
      " state (4)  A[0]:(0.590245008469) A[1]:(0.655361533165) A[2]:(3.68356704712e-05) A[3]:(0.531120121479)\n",
      " state (5)  A[0]:(-0.0374483950436) A[1]:(0.999896943569) A[2]:(-0.823682844639) A[3]:(0.660098910332)\n",
      " state (6)  A[0]:(-3.31848859787e-05) A[1]:(0.810067415237) A[2]:(0.000109076499939) A[3]:(0.655431628227)\n",
      " state (7)  A[0]:(0.544027745724) A[1]:(-0.534543156624) A[2]:(0.437699228525) A[3]:(0.86931347847)\n",
      " state (8)  A[0]:(0.655481934547) A[1]:(-0.000512659491505) A[2]:(0.728797614574) A[3]:(0.589976787567)\n",
      " state (9)  A[0]:(0.654897809029) A[1]:(0.80982863903) A[2]:(0.809999465942) A[3]:(-0.000749826314859)\n",
      " state (10)  A[0]:(0.728428006172) A[1]:(0.89998716116) A[2]:(0.000474691361887) A[3]:(0.728685736656)\n",
      " state (11)  A[0]:(0.135850265622) A[1]:(0.883268773556) A[2]:(-0.930527627468) A[3]:(0.803656935692)\n",
      " state (12)  A[0]:(-0.427854239941) A[1]:(0.816805839539) A[2]:(-0.956245005131) A[3]:(0.71551156044)\n",
      " state (13)  A[0]:(-0.000960021920037) A[1]:(0.809888601303) A[2]:(0.899952292442) A[3]:(0.729224324226)\n",
      " state (14)  A[0]:(0.809791564941) A[1]:(0.900645136833) A[2]:(0.999999880791) A[3]:(0.810109376907)\n",
      " state (15)  A[0]:(0.979836642742) A[1]:(0.938013374805) A[2]:(1.0) A[3]:(0.878426551819)\n",
      "Episode 698000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6003. Times reached goal: 994.               Steps done: 4851887. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00703226442886.\n",
      " state (0)  A[0]:(0.531554102898) A[1]:(0.590518712997) A[2]:(0.59053260088) A[3]:(0.531766414642)\n",
      " state (1)  A[0]:(0.531478285789) A[1]:(8.97496938705e-05) A[2]:(0.656262040138) A[3]:(0.59063154459)\n",
      " state (2)  A[0]:(0.590406358242) A[1]:(0.729079544544) A[2]:(0.590710878372) A[3]:(0.656195878983)\n",
      " state (3)  A[0]:(0.656098663807) A[1]:(-0.00363123300485) A[2]:(0.525750517845) A[3]:(0.54940533638)\n",
      " state (4)  A[0]:(0.590042114258) A[1]:(0.656211018562) A[2]:(0.000199675559998) A[3]:(0.531702041626)\n",
      " state (5)  A[0]:(-0.038247499615) A[1]:(0.999897122383) A[2]:(-0.82375317812) A[3]:(0.660842359066)\n",
      " state (6)  A[0]:(-0.000965967483353) A[1]:(0.81007874012) A[2]:(0.000178217887878) A[3]:(0.65625500679)\n",
      " state (7)  A[0]:(0.543557405472) A[1]:(-0.534150183201) A[2]:(0.438013613224) A[3]:(0.869728624821)\n",
      " state (8)  A[0]:(0.655532121658) A[1]:(0.000841483299155) A[2]:(0.729194045067) A[3]:(0.590440869331)\n",
      " state (9)  A[0]:(0.655741095543) A[1]:(0.810254096985) A[2]:(0.810204982758) A[3]:(0.000235795974731)\n",
      " state (10)  A[0]:(0.729081332684) A[1]:(0.900088429451) A[2]:(0.000623702944722) A[3]:(0.729084312916)\n",
      " state (11)  A[0]:(0.136655971408) A[1]:(0.883149504662) A[2]:(-0.930560290813) A[3]:(0.803682923317)\n",
      " state (12)  A[0]:(-0.427288621664) A[1]:(0.816280841827) A[2]:(-0.956247329712) A[3]:(0.715391159058)\n",
      " state (13)  A[0]:(-0.000560924352612) A[1]:(0.809094667435) A[2]:(0.900167822838) A[3]:(0.729148924351)\n",
      " state (14)  A[0]:(0.809607088566) A[1]:(0.900121867657) A[2]:(0.999999880791) A[3]:(0.810128331184)\n",
      " state (15)  A[0]:(0.979773044586) A[1]:(0.937633395195) A[2]:(1.0) A[3]:(0.878484725952)\n",
      "Episode 699000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6016. Times reached goal: 996.               Steps done: 4857903. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.006990085328.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5905,  0.5904,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6561, -0.0001,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560, -0.0001,  0.7290,  0.5907]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560,  0.8099,  0.8101,  0.0006]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000,  0.0002,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8101,  0.9003,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53142285347) A[1]:(0.590517640114) A[2]:(0.590437591076) A[3]:(0.53150254488)\n",
      " state (1)  A[0]:(0.531392097473) A[1]:(2.01910734177e-05) A[2]:(0.656119287014) A[3]:(0.590653479099)\n",
      " state (2)  A[0]:(0.590364694595) A[1]:(0.729023456573) A[2]:(0.590556502342) A[3]:(0.656231641769)\n",
      " state (3)  A[0]:(0.656158745289) A[1]:(-0.00387960206717) A[2]:(0.525592923164) A[3]:(0.549347758293)\n",
      " state (4)  A[0]:(0.590237617493) A[1]:(0.656079351902) A[2]:(-2.59876251221e-05) A[3]:(0.531586289406)\n",
      " state (5)  A[0]:(-0.0376179553568) A[1]:(0.999897062778) A[2]:(-0.823824346066) A[3]:(0.660805583)\n",
      " state (6)  A[0]:(-0.000101044774055) A[1]:(0.809992671013) A[2]:(6.28232955933e-05) A[3]:(0.656265497208)\n",
      " state (7)  A[0]:(0.544179201126) A[1]:(-0.534535765648) A[2]:(0.437932789326) A[3]:(0.869750499725)\n",
      " state (8)  A[0]:(0.656039476395) A[1]:(-0.000114038586617) A[2]:(0.728982806206) A[3]:(0.590937137604)\n",
      " state (9)  A[0]:(0.655968785286) A[1]:(0.809956371784) A[2]:(0.810040891171) A[3]:(0.00067383039277)\n",
      " state (10)  A[0]:(0.729229211807) A[1]:(0.900002002716) A[2]:(0.000160098075867) A[3]:(0.729247331619)\n",
      " state (11)  A[0]:(0.136971220374) A[1]:(0.883156001568) A[2]:(-0.930663883686) A[3]:(0.803819835186)\n",
      " state (12)  A[0]:(-0.427104115486) A[1]:(0.816421687603) A[2]:(-0.956350922585) A[3]:(0.715491890907)\n",
      " state (13)  A[0]:(1.27255916595e-05) A[1]:(0.809315800667) A[2]:(0.900002837181) A[3]:(0.729150056839)\n",
      " state (14)  A[0]:(0.810141921043) A[1]:(0.900252580643) A[2]:(0.999999880791) A[3]:(0.810081005096)\n",
      " state (15)  A[0]:(0.979877471924) A[1]:(0.937692701817) A[2]:(1.0) A[3]:(0.878440260887)\n",
      "Episode 700000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6022. Times reached goal: 993.               Steps done: 4863925. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00694811752603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532406210899) A[1]:(0.590247750282) A[2]:(0.590355634689) A[3]:(0.531694769859)\n",
      " state (1)  A[0]:(0.532016277313) A[1]:(-8.24928283691e-05) A[2]:(0.656120657921) A[3]:(0.590359091759)\n",
      " state (2)  A[0]:(0.590616703033) A[1]:(0.728948354721) A[2]:(0.590738475323) A[3]:(0.655845701694)\n",
      " state (3)  A[0]:(0.656244158745) A[1]:(-0.00382206961513) A[2]:(0.525839149952) A[3]:(0.548843562603)\n",
      " state (4)  A[0]:(0.590330541134) A[1]:(0.656336784363) A[2]:(0.000196099281311) A[3]:(0.530963420868)\n",
      " state (5)  A[0]:(-0.0367673970759) A[1]:(0.999897003174) A[2]:(-0.823783874512) A[3]:(0.66004383564)\n",
      " state (6)  A[0]:(0.000421151489718) A[1]:(0.809725403786) A[2]:(-6.77108764648e-05) A[3]:(0.655138909817)\n",
      " state (7)  A[0]:(0.54391682148) A[1]:(-0.534898042679) A[2]:(0.437457501888) A[3]:(0.86918348074)\n",
      " state (8)  A[0]:(0.65450078249) A[1]:(-0.000727653387003) A[2]:(0.728586196899) A[3]:(0.589346408844)\n",
      " state (9)  A[0]:(0.653030276299) A[1]:(0.809771239758) A[2]:(0.809895932674) A[3]:(-0.00185251026414)\n",
      " state (10)  A[0]:(0.726585149765) A[1]:(0.899926304817) A[2]:(0.00104129279498) A[3]:(0.728342056274)\n",
      " state (11)  A[0]:(0.130153924227) A[1]:(0.883121848106) A[2]:(-0.930516719818) A[3]:(0.80318582058)\n",
      " state (12)  A[0]:(-0.435579806566) A[1]:(0.816424369812) A[2]:(-0.956418454647) A[3]:(0.713985741138)\n",
      " state (13)  A[0]:(-0.0134277911857) A[1]:(0.809312343597) A[2]:(0.899569511414) A[3]:(0.72712790966)\n",
      " state (14)  A[0]:(0.805019855499) A[1]:(0.900207102299) A[2]:(0.999999880791) A[3]:(0.808448374271)\n",
      " state (15)  A[0]:(0.979284286499) A[1]:(0.937598049641) A[2]:(1.0) A[3]:(0.877365887165)\n",
      "Episode 701000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6005. Times reached goal: 991.               Steps done: 4869930. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00690651910455.\n",
      " state (0)  A[0]:(0.531060576439) A[1]:(0.590067028999) A[2]:(0.590312123299) A[3]:(0.531178116798)\n",
      " state (1)  A[0]:(0.531054735184) A[1]:(-9.11951065063e-05) A[2]:(0.655995488167) A[3]:(0.590222477913)\n",
      " state (2)  A[0]:(0.590085983276) A[1]:(0.72892165184) A[2]:(0.590583324432) A[3]:(0.655894756317)\n",
      " state (3)  A[0]:(0.65597474575) A[1]:(-0.0039263763465) A[2]:(0.525604426861) A[3]:(0.548931956291)\n",
      " state (4)  A[0]:(0.590276122093) A[1]:(0.655630707741) A[2]:(8.27312469482e-05) A[3]:(0.531184792519)\n",
      " state (5)  A[0]:(-0.0371094048023) A[1]:(0.999896943569) A[2]:(-0.823881745338) A[3]:(0.660612821579)\n",
      " state (6)  A[0]:(7.53104686737e-05) A[1]:(0.809949100018) A[2]:(-0.0002201795578) A[3]:(0.655848264694)\n",
      " state (7)  A[0]:(0.544118523598) A[1]:(-0.534527182579) A[2]:(0.437634170055) A[3]:(0.869455456734)\n",
      " state (8)  A[0]:(0.655625939369) A[1]:(0.000170826911926) A[2]:(0.728839159012) A[3]:(0.589777946472)\n",
      " state (9)  A[0]:(0.655514419079) A[1]:(0.810054183006) A[2]:(0.809922516346) A[3]:(-0.000671863439493)\n",
      " state (10)  A[0]:(0.729121029377) A[1]:(0.900022566319) A[2]:(-3.5285949707e-05) A[3]:(0.728699326515)\n",
      " state (11)  A[0]:(0.137293696404) A[1]:(0.883155882359) A[2]:(-0.930722117424) A[3]:(0.803438782692)\n",
      " state (12)  A[0]:(-0.42668813467) A[1]:(0.816380679607) A[2]:(-0.956429719925) A[3]:(0.714971125126)\n",
      " state (13)  A[0]:(0.000551462115254) A[1]:(0.809245765209) A[2]:(0.899974167347) A[3]:(0.728718280792)\n",
      " state (14)  A[0]:(0.810318231583) A[1]:(0.90022456646) A[2]:(0.999999880791) A[3]:(0.809868991375)\n",
      " state (15)  A[0]:(0.979894995689) A[1]:(0.937666535378) A[2]:(1.0) A[3]:(0.87835919857)\n",
      "Episode 702000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6026. Times reached goal: 994.               Steps done: 4875956. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00686502556602.\n",
      " state (0)  A[0]:(0.531206905842) A[1]:(0.589966535568) A[2]:(0.590554893017) A[3]:(0.531359016895)\n",
      " state (1)  A[0]:(0.531498074532) A[1]:(-0.000114187598228) A[2]:(0.656313300133) A[3]:(0.590456485748)\n",
      " state (2)  A[0]:(0.590498209) A[1]:(0.729252457619) A[2]:(0.590662062168) A[3]:(0.656287550926)\n",
      " state (3)  A[0]:(0.656329154968) A[1]:(-0.00323316827416) A[2]:(0.525515675545) A[3]:(0.549503207207)\n",
      " state (4)  A[0]:(0.590714335442) A[1]:(0.655954957008) A[2]:(-0.000293493270874) A[3]:(0.53198492527)\n",
      " state (5)  A[0]:(-0.0362005382776) A[1]:(0.999897003174) A[2]:(-0.824063062668) A[3]:(0.661607801914)\n",
      " state (6)  A[0]:(0.00190834468231) A[1]:(0.809952378273) A[2]:(-0.00086760497652) A[3]:(0.657360196114)\n",
      " state (7)  A[0]:(0.546257972717) A[1]:(-0.534189283848) A[2]:(0.43711155653) A[3]:(0.870379805565)\n",
      " state (8)  A[0]:(0.658454120159) A[1]:(0.00115276826546) A[2]:(0.72889059782) A[3]:(0.592655658722)\n",
      " state (9)  A[0]:(0.660016059875) A[1]:(0.810200035572) A[2]:(0.810015618801) A[3]:(0.00620690966025)\n",
      " state (10)  A[0]:(0.733198285103) A[1]:(0.899938225746) A[2]:(-4.42266464233e-05) A[3]:(0.73206448555)\n",
      " state (11)  A[0]:(0.145692124963) A[1]:(0.882818996906) A[2]:(-0.930779695511) A[3]:(0.805507183075)\n",
      " state (12)  A[0]:(-0.419703781605) A[1]:(0.815553665161) A[2]:(-0.956480383873) A[3]:(0.717202007771)\n",
      " state (13)  A[0]:(0.00901735760272) A[1]:(0.808202981949) A[2]:(0.899966716766) A[3]:(0.730469107628)\n",
      " state (14)  A[0]:(0.813073933125) A[1]:(0.899640142918) A[2]:(0.999999880791) A[3]:(0.810947299004)\n",
      " state (15)  A[0]:(0.980186283588) A[1]:(0.937316894531) A[2]:(1.0) A[3]:(0.878968179226)\n",
      "Episode 703000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 991.               Steps done: 4881966. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00682389049707.\n",
      " state (0)  A[0]:(0.531346023083) A[1]:(0.59061563015) A[2]:(0.590468883514) A[3]:(0.531817793846)\n",
      " state (1)  A[0]:(0.53142541647) A[1]:(-4.34368848801e-05) A[2]:(0.656265258789) A[3]:(0.590935945511)\n",
      " state (2)  A[0]:(0.590475559235) A[1]:(0.729222476482) A[2]:(0.590697288513) A[3]:(0.65658724308)\n",
      " state (3)  A[0]:(0.656269669533) A[1]:(-0.00342992087826) A[2]:(0.525694012642) A[3]:(0.549706697464)\n",
      " state (4)  A[0]:(0.590438961983) A[1]:(0.656048893929) A[2]:(8.42809677124e-05) A[3]:(0.531976699829)\n",
      " state (5)  A[0]:(-0.0372908115387) A[1]:(0.999897062778) A[2]:(-0.823860406876) A[3]:(0.661309719086)\n",
      " state (6)  A[0]:(1.4990568161e-05) A[1]:(0.810026526451) A[2]:(0.000323057174683) A[3]:(0.656641423702)\n",
      " state (7)  A[0]:(0.544091463089) A[1]:(-0.534415364265) A[2]:(0.438317716122) A[3]:(0.86982524395)\n",
      " state (8)  A[0]:(0.655943989754) A[1]:(0.000299647450447) A[2]:(0.729205012321) A[3]:(0.590632677078)\n",
      " state (9)  A[0]:(0.656251788139) A[1]:(0.80999070406) A[2]:(0.809989929199) A[3]:(0.00110542727634)\n",
      " state (10)  A[0]:(0.729317903519) A[1]:(0.900002896786) A[2]:(-0.000601410807576) A[3]:(0.729523837566)\n",
      " state (11)  A[0]:(0.136836841702) A[1]:(0.883185863495) A[2]:(-0.930902659893) A[3]:(0.803957760334)\n",
      " state (12)  A[0]:(-0.427094876766) A[1]:(0.81650018692) A[2]:(-0.95654964447) A[3]:(0.71568274498)\n",
      " state (13)  A[0]:(0.000308781862259) A[1]:(0.809414803982) A[2]:(0.899978995323) A[3]:(0.729354202747)\n",
      " state (14)  A[0]:(0.810302495956) A[1]:(0.900323033333) A[2]:(0.999999880791) A[3]:(0.810186982155)\n",
      " state (15)  A[0]:(0.979905188084) A[1]:(0.937695384026) A[2]:(1.0) A[3]:(0.878481924534)\n",
      "Episode 704000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6017. Times reached goal: 992.               Steps done: 4887983. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00678295442762.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5904,  0.5905,  0.5324]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.0001,  0.6561,  0.5911]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.7290,  0.5906,  0.6566]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0007,  0.8098, -0.0004,  0.6564]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9001, -0.0000,  0.7287]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.9002,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531947851181) A[1]:(0.590388655663) A[2]:(0.590475976467) A[3]:(0.532086133957)\n",
      " state (1)  A[0]:(0.531810998917) A[1]:(-0.000391557783587) A[2]:(0.656058251858) A[3]:(0.590798854828)\n",
      " state (2)  A[0]:(0.590726494789) A[1]:(0.728872299194) A[2]:(0.590603649616) A[3]:(0.656387388706)\n",
      " state (3)  A[0]:(0.656547188759) A[1]:(-0.00421099876985) A[2]:(0.525662302971) A[3]:(0.549475669861)\n",
      " state (4)  A[0]:(0.590806245804) A[1]:(0.655897080898) A[2]:(-0.000135064125061) A[3]:(0.531702160835)\n",
      " state (5)  A[0]:(-0.0365771576762) A[1]:(0.999896943569) A[2]:(-0.824033558369) A[3]:(0.660844922066)\n",
      " state (6)  A[0]:(0.00065660465043) A[1]:(0.809828400612) A[2]:(-0.000195860862732) A[3]:(0.656163215637)\n",
      " state (7)  A[0]:(0.544590592384) A[1]:(-0.534834444523) A[2]:(0.437873542309) A[3]:(0.869699776173)\n",
      " state (8)  A[0]:(0.656258165836) A[1]:(-0.000219419598579) A[2]:(0.728918910027) A[3]:(0.59029853344)\n",
      " state (9)  A[0]:(0.656124949455) A[1]:(0.809943437576) A[2]:(0.80992436409) A[3]:(-0.00070452678483)\n",
      " state (10)  A[0]:(0.729209959507) A[1]:(0.899934530258) A[2]:(-0.000346422195435) A[3]:(0.728697776794)\n",
      " state (11)  A[0]:(0.136692330241) A[1]:(0.883031368256) A[2]:(-0.930861055851) A[3]:(0.80348443985)\n",
      " state (12)  A[0]:(-0.427339434624) A[1]:(0.816180586815) A[2]:(-0.95655810833) A[3]:(0.715101718903)\n",
      " state (13)  A[0]:(-1.08927488327e-05) A[1]:(0.809066534042) A[2]:(0.899948477745) A[3]:(0.728774189949)\n",
      " state (14)  A[0]:(0.810243725777) A[1]:(0.900186240673) A[2]:(0.999999880791) A[3]:(0.809712409973)\n",
      " state (15)  A[0]:(0.979894459248) A[1]:(0.937658429146) A[2]:(1.0) A[3]:(0.878122985363)\n",
      "Episode 705000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6023. Times reached goal: 995.               Steps done: 4894006. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00674222347749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.53152513504) A[1]:(0.590541481972) A[2]:(0.590480566025) A[3]:(0.531442284584)\n",
      " state (1)  A[0]:(0.531590223312) A[1]:(-6.36279582977e-05) A[2]:(0.656034469604) A[3]:(0.590465664864)\n",
      " state (2)  A[0]:(0.590513885021) A[1]:(0.728961288929) A[2]:(0.59085470438) A[3]:(0.65619456768)\n",
      " state (3)  A[0]:(0.656233549118) A[1]:(-0.00413603195921) A[2]:(0.525843024254) A[3]:(0.549221038818)\n",
      " state (4)  A[0]:(0.590443193913) A[1]:(0.655606806278) A[2]:(8.89301300049e-05) A[3]:(0.531463861465)\n",
      " state (5)  A[0]:(-0.0369632579386) A[1]:(0.999896943569) A[2]:(-0.824044942856) A[3]:(0.660766661167)\n",
      " state (6)  A[0]:(-5.19156455994e-05) A[1]:(0.81000739336) A[2]:(-0.000286221504211) A[3]:(0.655900895596)\n",
      " state (7)  A[0]:(0.543897509575) A[1]:(-0.534220337868) A[2]:(0.437699228525) A[3]:(0.869547009468)\n",
      " state (8)  A[0]:(0.655739188194) A[1]:(0.000253707170486) A[2]:(0.728775501251) A[3]:(0.590588092804)\n",
      " state (9)  A[0]:(0.655370593071) A[1]:(0.810074329376) A[2]:(0.80985981226) A[3]:(-0.000212520360947)\n",
      " state (10)  A[0]:(0.728363752365) A[1]:(0.900035321712) A[2]:(-0.000833391968627) A[3]:(0.728582382202)\n",
      " state (11)  A[0]:(0.134908095002) A[1]:(0.883179485798) A[2]:(-0.930978655815) A[3]:(0.803276181221)\n",
      " state (12)  A[0]:(-0.428409516811) A[1]:(0.816418170929) A[2]:(-0.956624269485) A[3]:(0.714916467667)\n",
      " state (13)  A[0]:(-0.000897675519809) A[1]:(0.809277951717) A[2]:(0.89997035265) A[3]:(0.728815793991)\n",
      " state (14)  A[0]:(0.809960782528) A[1]:(0.900260686874) A[2]:(0.999999880791) A[3]:(0.809901535511)\n",
      " state (15)  A[0]:(0.979861974716) A[1]:(0.937662422657) A[2]:(1.0) A[3]:(0.87832993269)\n",
      "Episode 706000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6018. Times reached goal: 994.               Steps done: 4900024. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00670177062133.\n",
      " state (0)  A[0]:(0.533705413342) A[1]:(0.590330541134) A[2]:(0.590384364128) A[3]:(0.533917188644)\n",
      " state (1)  A[0]:(0.53389018774) A[1]:(0.00137332000304) A[2]:(0.655626654625) A[3]:(0.593484342098)\n",
      " state (2)  A[0]:(0.592752337456) A[1]:(0.729074597359) A[2]:(0.590272784233) A[3]:(0.658877015114)\n",
      " state (3)  A[0]:(0.658156514168) A[1]:(-0.00382752344012) A[2]:(0.525285959244) A[3]:(0.552492976189)\n",
      " state (4)  A[0]:(0.592602491379) A[1]:(0.655963897705) A[2]:(-0.000582218111958) A[3]:(0.534838497639)\n",
      " state (5)  A[0]:(-0.0333336889744) A[1]:(0.999896943569) A[2]:(-0.824144721031) A[3]:(0.663450777531)\n",
      " state (6)  A[0]:(0.00338581390679) A[1]:(0.809760630131) A[2]:(-0.000552773417439) A[3]:(0.657909333706)\n",
      " state (7)  A[0]:(0.545845150948) A[1]:(-0.534847140312) A[2]:(0.437334865332) A[3]:(0.870175242424)\n",
      " state (8)  A[0]:(0.657545685768) A[1]:(-0.00187918322627) A[2]:(0.728691875935) A[3]:(0.592730820179)\n",
      " state (9)  A[0]:(0.657353401184) A[1]:(0.809653401375) A[2]:(0.811022818089) A[3]:(0.000788301054854)\n",
      " state (10)  A[0]:(0.731790661812) A[1]:(0.900143444538) A[2]:(0.00772663485259) A[3]:(0.730287432671)\n",
      " state (11)  A[0]:(0.14309194684) A[1]:(0.883720517159) A[2]:(-0.929609298706) A[3]:(0.805446267128)\n",
      " state (12)  A[0]:(-0.427326738834) A[1]:(0.817648768425) A[2]:(-0.956214725971) A[3]:(0.716656088829)\n",
      " state (13)  A[0]:(-0.0072843930684) A[1]:(0.810656130314) A[2]:(0.8997194767) A[3]:(0.728917002678)\n",
      " state (14)  A[0]:(0.806305229664) A[1]:(0.900945007801) A[2]:(0.999999880791) A[3]:(0.809428453445)\n",
      " state (15)  A[0]:(0.979403436184) A[1]:(0.938000380993) A[2]:(1.0) A[3]:(0.877938508987)\n",
      "Episode 707000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6032. Times reached goal: 994.               Steps done: 4906056. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00666146721821.\n",
      " state (0)  A[0]:(0.531707704067) A[1]:(0.590670585632) A[2]:(0.590644538403) A[3]:(0.531861484051)\n",
      " state (1)  A[0]:(0.53111243248) A[1]:(-0.000139683485031) A[2]:(0.656206309795) A[3]:(0.590632796288)\n",
      " state (2)  A[0]:(0.589932620525) A[1]:(0.729097485542) A[2]:(0.590702176094) A[3]:(0.656227588654)\n",
      " state (3)  A[0]:(0.655780792236) A[1]:(-0.00363702955656) A[2]:(0.525743246078) A[3]:(0.549288034439)\n",
      " state (4)  A[0]:(0.589787840843) A[1]:(0.65627849102) A[2]:(-0.00014591217041) A[3]:(0.53162664175)\n",
      " state (5)  A[0]:(-0.0385801978409) A[1]:(0.999897062778) A[2]:(-0.824152469635) A[3]:(0.660949885845)\n",
      " state (6)  A[0]:(-0.00147163763177) A[1]:(0.810014665127) A[2]:(-0.000533819140401) A[3]:(0.655850887299)\n",
      " state (7)  A[0]:(0.543205142021) A[1]:(-0.534403562546) A[2]:(0.437793076038) A[3]:(0.869589924812)\n",
      " state (8)  A[0]:(0.65553176403) A[1]:(0.000656396034174) A[2]:(0.729028582573) A[3]:(0.590840101242)\n",
      " state (9)  A[0]:(0.655860066414) A[1]:(0.810231387615) A[2]:(0.810035705566) A[3]:(0.00169718102552)\n",
      " state (10)  A[0]:(0.728890419006) A[1]:(0.900012075901) A[2]:(-0.000343561172485) A[3]:(0.729589343071)\n",
      " state (11)  A[0]:(0.135561972857) A[1]:(0.882973194122) A[2]:(-0.930975139141) A[3]:(0.803660452366)\n",
      " state (12)  A[0]:(-0.428295761347) A[1]:(0.8158634305) A[2]:(-0.956669926643) A[3]:(0.714870929718)\n",
      " state (13)  A[0]:(-0.00103369320277) A[1]:(0.80855935812) A[2]:(0.900005757809) A[3]:(0.728431105614)\n",
      " state (14)  A[0]:(0.809853553772) A[1]:(0.899855911732) A[2]:(0.999999880791) A[3]:(0.809488892555)\n",
      " state (15)  A[0]:(0.979840755463) A[1]:(0.937399983406) A[2]:(1.0) A[3]:(0.878006994724)\n",
      "Episode 708000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6018. Times reached goal: 993.               Steps done: 4912074. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00662149889381.\n",
      " state (0)  A[0]:(0.531482815742) A[1]:(0.590370774269) A[2]:(0.590601563454) A[3]:(0.531593501568)\n",
      " state (1)  A[0]:(0.5314219594) A[1]:(0.000250071287155) A[2]:(0.655833184719) A[3]:(0.590569138527)\n",
      " state (2)  A[0]:(0.590362787247) A[1]:(0.728912353516) A[2]:(0.590271949768) A[3]:(0.656159043312)\n",
      " state (3)  A[0]:(0.656145751476) A[1]:(-0.00393129372969) A[2]:(0.525498151779) A[3]:(0.549166023731)\n",
      " state (4)  A[0]:(0.590344429016) A[1]:(0.65612077713) A[2]:(-9.08374786377e-05) A[3]:(0.531446874142)\n",
      " state (5)  A[0]:(-0.0369604043663) A[1]:(0.999897062778) A[2]:(-0.823999285698) A[3]:(0.660829424858)\n",
      " state (6)  A[0]:(0.000178217887878) A[1]:(0.809994757175) A[2]:(6.65187835693e-05) A[3]:(0.655893087387)\n",
      " state (7)  A[0]:(0.544098317623) A[1]:(-0.534400165081) A[2]:(0.438136637211) A[3]:(0.869536221027)\n",
      " state (8)  A[0]:(0.656149864197) A[1]:(-9.11056995392e-05) A[2]:(0.728959560394) A[3]:(0.590659677982)\n",
      " state (9)  A[0]:(0.65617108345) A[1]:(0.809966921806) A[2]:(0.809997320175) A[3]:(0.000101566314697)\n",
      " state (10)  A[0]:(0.729391992092) A[1]:(0.899986386299) A[2]:(-3.54051589966e-05) A[3]:(0.729016423225)\n",
      " state (11)  A[0]:(0.137518271804) A[1]:(0.883146762848) A[2]:(-0.930931210518) A[3]:(0.80371427536)\n",
      " state (12)  A[0]:(-0.426625728607) A[1]:(0.816374778748) A[2]:(-0.956689119339) A[3]:(0.715371727943)\n",
      " state (13)  A[0]:(0.000426813930972) A[1]:(0.809219777584) A[2]:(0.899955809116) A[3]:(0.729113578796)\n",
      " state (14)  A[0]:(0.81009721756) A[1]:(0.900240302086) A[2]:(0.999999880791) A[3]:(0.810081243515)\n",
      " state (15)  A[0]:(0.97985291481) A[1]:(0.937631607056) A[2]:(1.0) A[3]:(0.878446340561)\n",
      "Episode 709000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6027. Times reached goal: 992.               Steps done: 4918101. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00658171114081.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5906,  0.5904,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5906,  0.6563,  0.0003,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6555,  0.0003,  0.7290,  0.5890]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557,  0.8101,  0.8100, -0.0018]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0003,  0.8093,  0.9002,  0.7289]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8106,  0.9001,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.5311383605) A[1]:(0.590618669987) A[2]:(0.590416550636) A[3]:(0.5313757658)\n",
      " state (1)  A[0]:(0.531336724758) A[1]:(0.000581785978284) A[2]:(0.656027197838) A[3]:(0.590530991554)\n",
      " state (2)  A[0]:(0.590298771858) A[1]:(0.729155659676) A[2]:(0.590494573116) A[3]:(0.656131267548)\n",
      " state (3)  A[0]:(0.656155526638) A[1]:(-0.0034434213303) A[2]:(0.525593042374) A[3]:(0.549218356609)\n",
      " state (4)  A[0]:(0.590478420258) A[1]:(0.656216263771) A[2]:(-0.000129461288452) A[3]:(0.531612634659)\n",
      " state (5)  A[0]:(-0.0365966819227) A[1]:(0.999897122383) A[2]:(-0.824128270149) A[3]:(0.661078572273)\n",
      " state (6)  A[0]:(0.000658079865389) A[1]:(0.810186982155) A[2]:(-0.000247716903687) A[3]:(0.656194865704)\n",
      " state (7)  A[0]:(0.544486761093) A[1]:(-0.534251928329) A[2]:(0.438088566065) A[3]:(0.869632363319)\n",
      " state (8)  A[0]:(0.656428158283) A[1]:(0.000453993648989) A[2]:(0.729085206985) A[3]:(0.590228796005)\n",
      " state (9)  A[0]:(0.656846165657) A[1]:(0.810102105141) A[2]:(0.810094714165) A[3]:(-0.000197052955627)\n",
      " state (10)  A[0]:(0.730183720589) A[1]:(0.899991452694) A[2]:(0.000379204720957) A[3]:(0.729134082794)\n",
      " state (11)  A[0]:(0.139129713178) A[1]:(0.883106172085) A[2]:(-0.930909514427) A[3]:(0.803873956203)\n",
      " state (12)  A[0]:(-0.425653487444) A[1]:(0.816281557083) A[2]:(-0.956730246544) A[3]:(0.715528249741)\n",
      " state (13)  A[0]:(0.00169808999635) A[1]:(0.809129118919) A[2]:(0.899891912937) A[3]:(0.729208111763)\n",
      " state (14)  A[0]:(0.810825228691) A[1]:(0.900207281113) A[2]:(0.999999880791) A[3]:(0.810155510902)\n",
      " state (15)  A[0]:(0.979973435402) A[1]:(0.937602460384) A[2]:(1.0) A[3]:(0.87851536274)\n",
      "Episode 710000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6020. Times reached goal: 992.               Steps done: 4924121. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0065422082627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531502485275) A[1]:(0.59016919136) A[2]:(0.590220034122) A[3]:(0.531350016594)\n",
      " state (1)  A[0]:(0.531280636787) A[1]:(-0.000122636556625) A[2]:(0.655842900276) A[3]:(0.590122461319)\n",
      " state (2)  A[0]:(0.59011220932) A[1]:(0.728918194771) A[2]:(0.590360939503) A[3]:(0.655829787254)\n",
      " state (3)  A[0]:(0.6560100317) A[1]:(-0.00374920410104) A[2]:(0.525582253933) A[3]:(0.548918008804)\n",
      " state (4)  A[0]:(0.590363383293) A[1]:(0.655693888664) A[2]:(0.000176906585693) A[3]:(0.531374633312)\n",
      " state (5)  A[0]:(-0.0369321443141) A[1]:(0.999896943569) A[2]:(-0.823997735977) A[3]:(0.660950601101)\n",
      " state (6)  A[0]:(0.000150516629219) A[1]:(0.809898376465) A[2]:(0.000131487846375) A[3]:(0.655865907669)\n",
      " state (7)  A[0]:(0.543969213963) A[1]:(-0.534651935101) A[2]:(0.438177108765) A[3]:(0.869480073452)\n",
      " state (8)  A[0]:(0.655882179737) A[1]:(-0.000347226858139) A[2]:(0.728950560093) A[3]:(0.590277552605)\n",
      " state (9)  A[0]:(0.655867159367) A[1]:(0.809885919094) A[2]:(0.809940874577) A[3]:(-0.000328689813614)\n",
      " state (10)  A[0]:(0.729151964188) A[1]:(0.89992916584) A[2]:(-0.00026524066925) A[3]:(0.728996038437)\n",
      " state (11)  A[0]:(0.137009963393) A[1]:(0.883119523525) A[2]:(-0.931023776531) A[3]:(0.803794622421)\n",
      " state (12)  A[0]:(-0.427173674107) A[1]:(0.816425442696) A[2]:(-0.956781387329) A[3]:(0.715469360352)\n",
      " state (13)  A[0]:(5.03659248352e-05) A[1]:(0.809366106987) A[2]:(0.900025069714) A[3]:(0.729144096375)\n",
      " state (14)  A[0]:(0.810303866863) A[1]:(0.900337636471) A[2]:(0.999999880791) A[3]:(0.810050547123)\n",
      " state (15)  A[0]:(0.979925334454) A[1]:(0.937638282776) A[2]:(1.0) A[3]:(0.87842041254)\n",
      "Episode 711000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6006. Times reached goal: 991.               Steps done: 4930127. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00650303351939.\n",
      " state (0)  A[0]:(0.531818389893) A[1]:(0.590186357498) A[2]:(0.590740919113) A[3]:(0.531355977058)\n",
      " state (1)  A[0]:(0.530901968479) A[1]:(-0.000300794839859) A[2]:(0.6563539505) A[3]:(0.590335488319)\n",
      " state (2)  A[0]:(0.58970105648) A[1]:(0.729143023491) A[2]:(0.590779662132) A[3]:(0.656019210815)\n",
      " state (3)  A[0]:(0.65546476841) A[1]:(-0.00340904458426) A[2]:(0.525837421417) A[3]:(0.548952996731)\n",
      " state (4)  A[0]:(0.58932697773) A[1]:(0.656177341938) A[2]:(0.000182151794434) A[3]:(0.531449556351)\n",
      " state (5)  A[0]:(-0.0393855124712) A[1]:(0.999897062778) A[2]:(-0.82398802042) A[3]:(0.661493182182)\n",
      " state (6)  A[0]:(-0.00213834317401) A[1]:(0.810086727142) A[2]:(0.000195026397705) A[3]:(0.656257271767)\n",
      " state (7)  A[0]:(0.541864156723) A[1]:(-0.534148097038) A[2]:(0.438192993402) A[3]:(0.869255125523)\n",
      " state (8)  A[0]:(0.65337228775) A[1]:(0.000596374215093) A[2]:(0.728946149349) A[3]:(0.587359428406)\n",
      " state (9)  A[0]:(0.653935730457) A[1]:(0.809765577316) A[2]:(0.809425473213) A[3]:(-0.00320376851596)\n",
      " state (10)  A[0]:(0.727661192417) A[1]:(0.899835169315) A[2]:(-0.00239133369178) A[3]:(0.72803401947)\n",
      " state (11)  A[0]:(0.134025335312) A[1]:(0.88310533762) A[2]:(-0.931296348572) A[3]:(0.803247928619)\n",
      " state (12)  A[0]:(-0.429413318634) A[1]:(0.816509187222) A[2]:(-0.956869006157) A[3]:(0.715061247349)\n",
      " state (13)  A[0]:(-0.00288452906534) A[1]:(0.809476912022) A[2]:(0.900292634964) A[3]:(0.729059457779)\n",
      " state (14)  A[0]:(0.80898308754) A[1]:(0.900336742401) A[2]:(0.999999880791) A[3]:(0.810175836086)\n",
      " state (15)  A[0]:(0.979737699032) A[1]:(0.937552690506) A[2]:(1.0) A[3]:(0.878594279289)\n",
      "Episode 712000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6011. Times reached goal: 993.               Steps done: 4936138. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00646406103406.\n",
      " state (0)  A[0]:(0.531550705433) A[1]:(0.591381788254) A[2]:(0.590700089931) A[3]:(0.530804395676)\n",
      " state (1)  A[0]:(0.531975209713) A[1]:(-0.000211954116821) A[2]:(0.656636118889) A[3]:(0.590052247047)\n",
      " state (2)  A[0]:(0.591300666332) A[1]:(0.729277968407) A[2]:(0.590880513191) A[3]:(0.655804038048)\n",
      " state (3)  A[0]:(0.657097876072) A[1]:(-0.00329612498172) A[2]:(0.525927782059) A[3]:(0.548584282398)\n",
      " state (4)  A[0]:(0.591385602951) A[1]:(0.656814992428) A[2]:(2.50339508057e-05) A[3]:(0.531108558178)\n",
      " state (5)  A[0]:(-0.0356983914971) A[1]:(0.999897181988) A[2]:(-0.82397967577) A[3]:(0.661447405815)\n",
      " state (6)  A[0]:(0.0021135029383) A[1]:(0.810070335865) A[2]:(0.000702619436197) A[3]:(0.656821370125)\n",
      " state (7)  A[0]:(0.545908093452) A[1]:(-0.534323036671) A[2]:(0.438847154379) A[3]:(0.870048940182)\n",
      " state (8)  A[0]:(0.65872824192) A[1]:(4.43458557129e-05) A[2]:(0.729292392731) A[3]:(0.59260892868)\n",
      " state (9)  A[0]:(0.659763276577) A[1]:(0.810165703297) A[2]:(0.810417175293) A[3]:(0.00286137266085)\n",
      " state (10)  A[0]:(0.732710957527) A[1]:(0.900135755539) A[2]:(0.00209760363214) A[3]:(0.730390429497)\n",
      " state (11)  A[0]:(0.144108325243) A[1]:(0.88336867094) A[2]:(-0.930711150169) A[3]:(0.804686069489)\n",
      " state (12)  A[0]:(-0.422864019871) A[1]:(0.816773891449) A[2]:(-0.956686913967) A[3]:(0.716133832932)\n",
      " state (13)  A[0]:(0.0032276250422) A[1]:(0.809665441513) A[2]:(0.900207281113) A[3]:(0.729403853416)\n",
      " state (14)  A[0]:(0.810875058174) A[1]:(0.90046030283) A[2]:(0.999999880791) A[3]:(0.810208022594)\n",
      " state (15)  A[0]:(0.979960858822) A[1]:(0.937672734261) A[2]:(1.0) A[3]:(0.878560364246)\n",
      "Episode 713000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6024. Times reached goal: 994.               Steps done: 4942162. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00642523858102.\n",
      " state (0)  A[0]:(0.53144967556) A[1]:(0.590873658657) A[2]:(0.590676128864) A[3]:(0.531533837318)\n",
      " state (1)  A[0]:(0.531474232674) A[1]:(-5.86360692978e-05) A[2]:(0.655993700027) A[3]:(0.590774059296)\n",
      " state (2)  A[0]:(0.590578317642) A[1]:(0.72906255722) A[2]:(0.590371966362) A[3]:(0.656293392181)\n",
      " state (3)  A[0]:(0.656434178352) A[1]:(-0.00346085545607) A[2]:(0.525471448898) A[3]:(0.549153625965)\n",
      " state (4)  A[0]:(0.590584516525) A[1]:(0.656297266483) A[2]:(-0.000305771827698) A[3]:(0.531514704227)\n",
      " state (5)  A[0]:(-0.0375359356403) A[1]:(0.999897062778) A[2]:(-0.824138522148) A[3]:(0.661390185356)\n",
      " state (6)  A[0]:(-7.50571489334e-05) A[1]:(0.809954881668) A[2]:(-7.87973403931e-05) A[3]:(0.656221389771)\n",
      " state (7)  A[0]:(0.543957293034) A[1]:(-0.534309506416) A[2]:(0.437953591347) A[3]:(0.869590401649)\n",
      " state (8)  A[0]:(0.65619301796) A[1]:(-9.2014670372e-05) A[2]:(0.728756844997) A[3]:(0.59082710743)\n",
      " state (9)  A[0]:(0.656091570854) A[1]:(0.809954762459) A[2]:(0.809784889221) A[3]:(-6.17504119873e-05)\n",
      " state (10)  A[0]:(0.728765368462) A[1]:(0.899979293346) A[2]:(-0.00115179968998) A[3]:(0.728633880615)\n",
      " state (11)  A[0]:(0.135216906667) A[1]:(0.883194446564) A[2]:(-0.931242525578) A[3]:(0.803333103657)\n",
      " state (12)  A[0]:(-0.428648144007) A[1]:(0.816534638405) A[2]:(-0.956928372383) A[3]:(0.714925885201)\n",
      " state (13)  A[0]:(-0.00114081753418) A[1]:(0.809426188469) A[2]:(0.900050222874) A[3]:(0.728857636452)\n",
      " state (14)  A[0]:(0.810118615627) A[1]:(0.900330364704) A[2]:(0.999999880791) A[3]:(0.810000181198)\n",
      " state (15)  A[0]:(0.979923784733) A[1]:(0.93758058548) A[2]:(1.0) A[3]:(0.878458797932)\n",
      "Episode 714000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6013. Times reached goal: 994.               Steps done: 4948175. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00638671954497.\n",
      "q_values \n",
      "tensor([[ 0.5315,  0.5905,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5314, -0.0000,  0.6561,  0.5907]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5904,  0.7290,  0.5907,  0.6562]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8100,  0.0000,  0.6558]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7288,  0.9000, -0.0002,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.9005,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531494736671) A[1]:(0.590505361557) A[2]:(0.590527236462) A[3]:(0.531475782394)\n",
      " state (1)  A[0]:(0.531323313713) A[1]:(2.22474336624e-05) A[2]:(0.656066000462) A[3]:(0.590651392937)\n",
      " state (2)  A[0]:(0.590404748917) A[1]:(0.728998064995) A[2]:(0.590687274933) A[3]:(0.656189799309)\n",
      " state (3)  A[0]:(0.656331956387) A[1]:(-0.00362411048263) A[2]:(0.525865912437) A[3]:(0.54908823967)\n",
      " state (4)  A[0]:(0.590557217598) A[1]:(0.656126379967) A[2]:(0.000172972679138) A[3]:(0.531391263008)\n",
      " state (5)  A[0]:(-0.0374725908041) A[1]:(0.999897062778) A[2]:(-0.824126243591) A[3]:(0.660985946655)\n",
      " state (6)  A[0]:(-0.000184580683708) A[1]:(0.809999167919) A[2]:(-2.52723693848e-05) A[3]:(0.655756473541)\n",
      " state (7)  A[0]:(0.543790817261) A[1]:(-0.534439146519) A[2]:(0.438107162714) A[3]:(0.869412720203)\n",
      " state (8)  A[0]:(0.655793368816) A[1]:(-0.000131532549858) A[2]:(0.728923618793) A[3]:(0.590339660645)\n",
      " state (9)  A[0]:(0.655670940876) A[1]:(0.809980273247) A[2]:(0.809987485409) A[3]:(-0.00020694732666)\n",
      " state (10)  A[0]:(0.728811979294) A[1]:(0.900010943413) A[2]:(-0.000221252441406) A[3]:(0.728965163231)\n",
      " state (11)  A[0]:(0.13581982255) A[1]:(0.88327562809) A[2]:(-0.931140303612) A[3]:(0.803707659245)\n",
      " state (12)  A[0]:(-0.428540110588) A[1]:(0.816719889641) A[2]:(-0.956936001778) A[3]:(0.715228557587)\n",
      " state (13)  A[0]:(-0.00159706035629) A[1]:(0.809663176537) A[2]:(0.899994969368) A[3]:(0.728844761848)\n",
      " state (14)  A[0]:(0.809904754162) A[1]:(0.900479614735) A[2]:(0.999999880791) A[3]:(0.809800863266)\n",
      " state (15)  A[0]:(0.979903578758) A[1]:(0.937671244144) A[2]:(1.0) A[3]:(0.878244757652)\n",
      "Episode 715000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6013. Times reached goal: 993.               Steps done: 4954188. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00634843142893.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531991541386) A[1]:(0.590379118919) A[2]:(0.590185403824) A[3]:(0.531843066216)\n",
      " state (1)  A[0]:(0.53219294548) A[1]:(-0.000118598341942) A[2]:(0.656018018723) A[3]:(0.590599656105)\n",
      " state (2)  A[0]:(0.590990424156) A[1]:(0.729047775269) A[2]:(0.590609312057) A[3]:(0.656096875668)\n",
      " state (3)  A[0]:(0.656774520874) A[1]:(-0.00347575638443) A[2]:(0.525722444057) A[3]:(0.548948824406)\n",
      " state (4)  A[0]:(0.591117978096) A[1]:(0.656277656555) A[2]:(-0.000145673751831) A[3]:(0.53137421608)\n",
      " state (5)  A[0]:(-0.0358852520585) A[1]:(0.999897122383) A[2]:(-0.824313521385) A[3]:(0.661366820335)\n",
      " state (6)  A[0]:(0.00173745874781) A[1]:(0.810207664967) A[2]:(-0.000498056353536) A[3]:(0.656068325043)\n",
      " state (7)  A[0]:(0.544843435287) A[1]:(-0.533872067928) A[2]:(0.437855809927) A[3]:(0.869272053242)\n",
      " state (8)  A[0]:(0.656271100044) A[1]:(0.000633612216916) A[2]:(0.728979170322) A[3]:(0.588385105133)\n",
      " state (9)  A[0]:(0.656595289707) A[1]:(0.81009054184) A[2]:(0.81016510725) A[3]:(-0.00363992038183)\n",
      " state (10)  A[0]:(0.730333447456) A[1]:(0.900110542774) A[2]:(0.0012812607456) A[3]:(0.727806568146)\n",
      " state (11)  A[0]:(0.140211552382) A[1]:(0.883514285088) A[2]:(-0.930895984173) A[3]:(0.803206980228)\n",
      " state (12)  A[0]:(-0.425380170345) A[1]:(0.817201852798) A[2]:(-0.956877946854) A[3]:(0.714574813843)\n",
      " state (13)  A[0]:(0.000994622358121) A[1]:(0.810167908669) A[2]:(0.899949729443) A[3]:(0.728141903877)\n",
      " state (14)  A[0]:(0.8104596138) A[1]:(0.900716125965) A[2]:(0.999999880791) A[3]:(0.809315741062)\n",
      " state (15)  A[0]:(0.979955255985) A[1]:(0.937775373459) A[2]:(1.0) A[3]:(0.87797832489)\n",
      "Episode 716000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6016. Times reached goal: 993.               Steps done: 4960204. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00631035391745.\n",
      " state (0)  A[0]:(0.531204223633) A[1]:(0.590533971786) A[2]:(0.590495884418) A[3]:(0.531369566917)\n",
      " state (1)  A[0]:(0.531262636185) A[1]:(-0.00033363699913) A[2]:(0.656185269356) A[3]:(0.590475022793)\n",
      " state (2)  A[0]:(0.590224027634) A[1]:(0.729067444801) A[2]:(0.59070032835) A[3]:(0.656218290329)\n",
      " state (3)  A[0]:(0.656129598618) A[1]:(-0.00348265562207) A[2]:(0.525837302208) A[3]:(0.549202561378)\n",
      " state (4)  A[0]:(0.590307474136) A[1]:(0.656075954437) A[2]:(0.000130891799927) A[3]:(0.531691670418)\n",
      " state (5)  A[0]:(-0.0378259457648) A[1]:(0.999896943569) A[2]:(-0.824149608612) A[3]:(0.661575675011)\n",
      " state (6)  A[0]:(-0.000497683824506) A[1]:(0.809958100319) A[2]:(0.000267744064331) A[3]:(0.656454920769)\n",
      " state (7)  A[0]:(0.543550133705) A[1]:(-0.534288167953) A[2]:(0.438389748335) A[3]:(0.869716107845)\n",
      " state (8)  A[0]:(0.656032681465) A[1]:(-0.000235885381699) A[2]:(0.728978514671) A[3]:(0.591317355633)\n",
      " state (9)  A[0]:(0.65593624115) A[1]:(0.809964001179) A[2]:(0.81000328064) A[3]:(-3.33786010742e-05)\n",
      " state (10)  A[0]:(0.728658556938) A[1]:(0.900018334389) A[2]:(-0.000619769038167) A[3]:(0.728570342064)\n",
      " state (11)  A[0]:(0.135197892785) A[1]:(0.883302688599) A[2]:(-0.931280374527) A[3]:(0.803315281868)\n",
      " state (12)  A[0]:(-0.428705155849) A[1]:(0.816761493683) A[2]:(-0.957028150558) A[3]:(0.714916229248)\n",
      " state (13)  A[0]:(-0.00142510142177) A[1]:(0.809649825096) A[2]:(0.900045633316) A[3]:(0.728897571564)\n",
      " state (14)  A[0]:(0.80996799469) A[1]:(0.900411248207) A[2]:(0.999999880791) A[3]:(0.810076355934)\n",
      " state (15)  A[0]:(0.97991502285) A[1]:(0.937552630901) A[2]:(1.0) A[3]:(0.87855386734)\n",
      "Episode 717000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6015. Times reached goal: 991.               Steps done: 4966219. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00627251106511.\n",
      " state (0)  A[0]:(0.53124153614) A[1]:(0.590597331524) A[2]:(0.590430855751) A[3]:(0.531335234642)\n",
      " state (1)  A[0]:(0.53101849556) A[1]:(-0.000190749764442) A[2]:(0.656071782112) A[3]:(0.590615928173)\n",
      " state (2)  A[0]:(0.590039074421) A[1]:(0.728948771954) A[2]:(0.590661942959) A[3]:(0.656239748001)\n",
      " state (3)  A[0]:(0.655926465988) A[1]:(-0.0037114599254) A[2]:(0.525818705559) A[3]:(0.549202084541)\n",
      " state (4)  A[0]:(0.590018510818) A[1]:(0.656138539314) A[2]:(4.97102737427e-05) A[3]:(0.531659960747)\n",
      " state (5)  A[0]:(-0.038270637393) A[1]:(0.999897003174) A[2]:(-0.824168086052) A[3]:(0.661432147026)\n",
      " state (6)  A[0]:(-0.00085026002489) A[1]:(0.80994874239) A[2]:(0.000353574723704) A[3]:(0.656168460846)\n",
      " state (7)  A[0]:(0.543198168278) A[1]:(-0.534241080284) A[2]:(0.438553243876) A[3]:(0.869490563869)\n",
      " state (8)  A[0]:(0.655192255974) A[1]:(0.000363334984286) A[2]:(0.729136884212) A[3]:(0.590059459209)\n",
      " state (9)  A[0]:(0.65524661541) A[1]:(0.809999704361) A[2]:(0.809883058071) A[3]:(-0.000213503837585)\n",
      " state (10)  A[0]:(0.728240728378) A[1]:(0.899943113327) A[2]:(-0.00117421092) A[3]:(0.728630781174)\n",
      " state (11)  A[0]:(0.134610712528) A[1]:(0.883160471916) A[2]:(-0.931359052658) A[3]:(0.803268671036)\n",
      " state (12)  A[0]:(-0.42899826169) A[1]:(0.816497802734) A[2]:(-0.957085549831) A[3]:(0.71479344368)\n",
      " state (13)  A[0]:(-0.00199293834157) A[1]:(0.809355735779) A[2]:(0.899940311909) A[3]:(0.728763580322)\n",
      " state (14)  A[0]:(0.809528172016) A[1]:(0.900271832943) A[2]:(0.999999880791) A[3]:(0.809963285923)\n",
      " state (15)  A[0]:(0.979834377766) A[1]:(0.937488794327) A[2]:(1.0) A[3]:(0.878459274769)\n",
      "Episode 718000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6009. Times reached goal: 993.               Steps done: 4972228. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0062349325638.\n",
      " state (0)  A[0]:(0.531622052193) A[1]:(0.59048473835) A[2]:(0.5905636549) A[3]:(0.531492114067)\n",
      " state (1)  A[0]:(0.53148663044) A[1]:(3.45408916473e-05) A[2]:(0.656120359898) A[3]:(0.590330123901)\n",
      " state (2)  A[0]:(0.590458989143) A[1]:(0.729046702385) A[2]:(0.590672492981) A[3]:(0.656000375748)\n",
      " state (3)  A[0]:(0.65626937151) A[1]:(-0.00344659504481) A[2]:(0.525812268257) A[3]:(0.548898637295)\n",
      " state (4)  A[0]:(0.590407192707) A[1]:(0.656179845333) A[2]:(2.80141830444e-05) A[3]:(0.531330108643)\n",
      " state (5)  A[0]:(-0.0376192331314) A[1]:(0.999897003174) A[2]:(-0.82427573204) A[3]:(0.661221265793)\n",
      " state (6)  A[0]:(-0.00028757750988) A[1]:(0.810036182404) A[2]:(5.3882598877e-05) A[3]:(0.655878424644)\n",
      " state (7)  A[0]:(0.54343098402) A[1]:(-0.534141421318) A[2]:(0.438410639763) A[3]:(0.869304895401)\n",
      " state (8)  A[0]:(0.655353784561) A[1]:(0.000368565291865) A[2]:(0.729109883308) A[3]:(0.589547455311)\n",
      " state (9)  A[0]:(0.655426204205) A[1]:(0.810074388981) A[2]:(0.810025691986) A[3]:(-0.00133547105361)\n",
      " state (10)  A[0]:(0.728786706924) A[1]:(0.900020480156) A[2]:(-0.000336170196533) A[3]:(0.72851395607)\n",
      " state (11)  A[0]:(0.136423885822) A[1]:(0.883291304111) A[2]:(-0.931270122528) A[3]:(0.803527593613)\n",
      " state (12)  A[0]:(-0.427789717913) A[1]:(0.816733300686) A[2]:(-0.957074403763) A[3]:(0.715191364288)\n",
      " state (13)  A[0]:(-0.000928878493141) A[1]:(0.809611797333) A[2]:(0.900135219097) A[3]:(0.728946447372)\n",
      " state (14)  A[0]:(0.809940040112) A[1]:(0.900365412235) A[2]:(0.999999880791) A[3]:(0.809914708138)\n",
      " state (15)  A[0]:(0.979904294014) A[1]:(0.937457919121) A[2]:(1.0) A[3]:(0.878370821476)\n",
      "Episode 719000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6006. Times reached goal: 991.               Steps done: 4978234. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00619759778739.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5905,  0.5906,  0.5310]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5316, -0.0005,  0.6562,  0.5906]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7290,  0.5906,  0.6564]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0000,  0.8101,  0.0006,  0.6564]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7296,  0.8998,  0.0004,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8110,  0.9002,  1.0000,  0.8106]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531381964684) A[1]:(0.590469062328) A[2]:(0.590599060059) A[3]:(0.531298279762)\n",
      " state (1)  A[0]:(0.531483411789) A[1]:(-0.000164315104485) A[2]:(0.656140446663) A[3]:(0.590773820877)\n",
      " state (2)  A[0]:(0.590510725975) A[1]:(0.729084253311) A[2]:(0.590642333031) A[3]:(0.65648162365)\n",
      " state (3)  A[0]:(0.656372368336) A[1]:(-0.00329020922072) A[2]:(0.525769352913) A[3]:(0.54950016737)\n",
      " state (4)  A[0]:(0.590553283691) A[1]:(0.656163394451) A[2]:(2.72989273071e-05) A[3]:(0.532035291195)\n",
      " state (5)  A[0]:(-0.0375086739659) A[1]:(0.999897122383) A[2]:(-0.824311971664) A[3]:(0.661968588829)\n",
      " state (6)  A[0]:(0.000498324574437) A[1]:(0.810291528702) A[2]:(2.18152999878e-05) A[3]:(0.65657222271)\n",
      " state (7)  A[0]:(0.544272661209) A[1]:(-0.533900141716) A[2]:(0.4385651052) A[3]:(0.869620025158)\n",
      " state (8)  A[0]:(0.656312465668) A[1]:(0.000580742897) A[2]:(0.729190587997) A[3]:(0.590760469437)\n",
      " state (9)  A[0]:(0.656653404236) A[1]:(0.810156226158) A[2]:(0.810137987137) A[3]:(0.000874548917636)\n",
      " state (10)  A[0]:(0.730063557625) A[1]:(0.900058686733) A[2]:(0.000290632247925) A[3]:(0.729672729969)\n",
      " state (11)  A[0]:(0.139524370432) A[1]:(0.88333427906) A[2]:(-0.931178987026) A[3]:(0.80441826582)\n",
      " state (12)  A[0]:(-0.425182074308) A[1]:(0.816816627979) A[2]:(-0.957020103931) A[3]:(0.716283679008)\n",
      " state (13)  A[0]:(0.00228857598267) A[1]:(0.809745788574) A[2]:(0.900421500206) A[3]:(0.729860782623)\n",
      " state (14)  A[0]:(0.8111115098) A[1]:(0.900466501713) A[2]:(0.999999880791) A[3]:(0.810524582863)\n",
      " state (15)  A[0]:(0.980047821999) A[1]:(0.937516033649) A[2]:(1.0) A[3]:(0.878754734993)\n",
      "Episode 720000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6026. Times reached goal: 994.               Steps done: 4984260. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00616036336311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531426548958) A[1]:(0.590786516666) A[2]:(0.590615093708) A[3]:(0.531544446945)\n",
      " state (1)  A[0]:(0.531491994858) A[1]:(-5.11109828949e-06) A[2]:(0.656207442284) A[3]:(0.590714931488)\n",
      " state (2)  A[0]:(0.590515851974) A[1]:(0.729031324387) A[2]:(0.59060382843) A[3]:(0.656349182129)\n",
      " state (3)  A[0]:(0.656350255013) A[1]:(-0.00355610251427) A[2]:(0.525691449642) A[3]:(0.549163520336)\n",
      " state (4)  A[0]:(0.590479016304) A[1]:(0.655970692635) A[2]:(-0.000140190124512) A[3]:(0.531556248665)\n",
      " state (5)  A[0]:(-0.0377791486681) A[1]:(0.999896943569) A[2]:(-0.824392855167) A[3]:(0.661564588547)\n",
      " state (6)  A[0]:(-0.000117346644402) A[1]:(0.809966564178) A[2]:(-0.000221252441406) A[3]:(0.655929744244)\n",
      " state (7)  A[0]:(0.543658852577) A[1]:(-0.534553766251) A[2]:(0.438246250153) A[3]:(0.869326412678)\n",
      " state (8)  A[0]:(0.655951857567) A[1]:(-0.00101076031569) A[2]:(0.728761553764) A[3]:(0.59094452858)\n",
      " state (9)  A[0]:(0.655616521835) A[1]:(0.809762716293) A[2]:(0.809930205345) A[3]:(-0.000326901674271)\n",
      " state (10)  A[0]:(0.728928029537) A[1]:(0.899950563908) A[2]:(-0.000240087509155) A[3]:(0.728739321232)\n",
      " state (11)  A[0]:(0.137086763978) A[1]:(0.883307516575) A[2]:(-0.931302905083) A[3]:(0.803752660751)\n",
      " state (12)  A[0]:(-0.427274554968) A[1]:(0.816839754581) A[2]:(-0.957164764404) A[3]:(0.715477705002)\n",
      " state (13)  A[0]:(-0.000643983366899) A[1]:(0.809706866741) A[2]:(0.899942576885) A[3]:(0.729165196419)\n",
      " state (14)  A[0]:(0.809904456139) A[1]:(0.900382637978) A[2]:(0.999999880791) A[3]:(0.810031652451)\n",
      " state (15)  A[0]:(0.979898452759) A[1]:(0.937427282333) A[2]:(1.0) A[3]:(0.878437519073)\n",
      "Episode 721000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6012. Times reached goal: 993.               Steps done: 4990272. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00612343836633.\n",
      " state (0)  A[0]:(0.531568944454) A[1]:(0.590614557266) A[2]:(0.590502142906) A[3]:(0.531654477119)\n",
      " state (1)  A[0]:(0.5316593647) A[1]:(0.000159397721291) A[2]:(0.656131386757) A[3]:(0.590547740459)\n",
      " state (2)  A[0]:(0.590508282185) A[1]:(0.729105889797) A[2]:(0.590679764748) A[3]:(0.65614938736)\n",
      " state (3)  A[0]:(0.656394004822) A[1]:(-0.00329909031279) A[2]:(0.525817453861) A[3]:(0.548991799355)\n",
      " state (4)  A[0]:(0.590556204319) A[1]:(0.656365334988) A[2]:(-0.000145792961121) A[3]:(0.531474471092)\n",
      " state (5)  A[0]:(-0.0376027189195) A[1]:(0.999897003174) A[2]:(-0.824462532997) A[3]:(0.661539316177)\n",
      " state (6)  A[0]:(0.00014628469944) A[1]:(0.810000121593) A[2]:(-0.000270485877991) A[3]:(0.656136512756)\n",
      " state (7)  A[0]:(0.543982088566) A[1]:(-0.534269213676) A[2]:(0.438261091709) A[3]:(0.869476079941)\n",
      " state (8)  A[0]:(0.656159281731) A[1]:(-9.3087553978e-05) A[2]:(0.728892326355) A[3]:(0.590732097626)\n",
      " state (9)  A[0]:(0.656285643578) A[1]:(0.81000828743) A[2]:(0.80999404192) A[3]:(-7.18235969543e-06)\n",
      " state (10)  A[0]:(0.729844331741) A[1]:(0.900002360344) A[2]:(0.000195026397705) A[3]:(0.729134917259)\n",
      " state (11)  A[0]:(0.139351963997) A[1]:(0.883284449577) A[2]:(-0.931235194206) A[3]:(0.804022014141)\n",
      " state (12)  A[0]:(-0.425488114357) A[1]:(0.816716730595) A[2]:(-0.957146286964) A[3]:(0.715671598911)\n",
      " state (13)  A[0]:(0.00129924644716) A[1]:(0.809555411339) A[2]:(0.900009810925) A[3]:(0.729218244553)\n",
      " state (14)  A[0]:(0.810483813286) A[1]:(0.900335788727) A[2]:(0.999999880791) A[3]:(0.810025990009)\n",
      " state (15)  A[0]:(0.979950785637) A[1]:(0.937431633472) A[2]:(1.0) A[3]:(0.878409922123)\n",
      "Episode 722000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6030. Times reached goal: 998.               Steps done: 4996302. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00608662513642.\n",
      " state (0)  A[0]:(0.530109643936) A[1]:(0.590311288834) A[2]:(0.590221643448) A[3]:(0.532802820206)\n",
      " state (1)  A[0]:(0.529881298542) A[1]:(0.000109121203423) A[2]:(0.655922651291) A[3]:(0.59151661396)\n",
      " state (2)  A[0]:(0.588792324066) A[1]:(0.728834092617) A[2]:(0.590619623661) A[3]:(0.657009422779)\n",
      " state (3)  A[0]:(0.654911220074) A[1]:(-0.00406389683485) A[2]:(0.525876164436) A[3]:(0.549921631813)\n",
      " state (4)  A[0]:(0.58887386322) A[1]:(0.655767560005) A[2]:(0.000243186950684) A[3]:(0.53229868412)\n",
      " state (5)  A[0]:(-0.0401984080672) A[1]:(0.99989682436) A[2]:(-0.824197590351) A[3]:(0.662046432495)\n",
      " state (6)  A[0]:(-0.00220767804421) A[1]:(0.809772610664) A[2]:(0.000844001595397) A[3]:(0.656228423119)\n",
      " state (7)  A[0]:(0.542280435562) A[1]:(-0.535010933876) A[2]:(0.439176082611) A[3]:(0.869449436665)\n",
      " state (8)  A[0]:(0.65508890152) A[1]:(-0.0022896041628) A[2]:(0.729104995728) A[3]:(0.592008590698)\n",
      " state (9)  A[0]:(0.65452837944) A[1]:(0.809460043907) A[2]:(0.810415625572) A[3]:(5.50746917725e-05)\n",
      " state (10)  A[0]:(0.728468298912) A[1]:(0.89990645647) A[2]:(0.00232147751376) A[3]:(0.728952765465)\n",
      " state (11)  A[0]:(0.137506619096) A[1]:(0.883422791958) A[2]:(-0.930916666985) A[3]:(0.804235100746)\n",
      " state (12)  A[0]:(-0.426525175571) A[1]:(0.817193388939) A[2]:(-0.957011282444) A[3]:(0.716275513172)\n",
      " state (13)  A[0]:(0.00035253164242) A[1]:(0.810140311718) A[2]:(0.900069355965) A[3]:(0.729888081551)\n",
      " state (14)  A[0]:(0.810296773911) A[1]:(0.900673985481) A[2]:(0.999999880791) A[3]:(0.810498714447)\n",
      " state (15)  A[0]:(0.979947805405) A[1]:(0.937671005726) A[2]:(1.0) A[3]:(0.878695368767)\n",
      "Episode 723000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6012. Times reached goal: 995.               Steps done: 5002314. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00605014212392.\n",
      " state (0)  A[0]:(0.531373679638) A[1]:(0.590370833874) A[2]:(0.590702056885) A[3]:(0.531472921371)\n",
      " state (1)  A[0]:(0.531757891178) A[1]:(-0.000438615650637) A[2]:(0.65618622303) A[3]:(0.590427339077)\n",
      " state (2)  A[0]:(0.59068274498) A[1]:(0.728970766068) A[2]:(0.590675532818) A[3]:(0.655979096889)\n",
      " state (3)  A[0]:(0.656554341316) A[1]:(-0.00339093990624) A[2]:(0.525825500488) A[3]:(0.548678636551)\n",
      " state (4)  A[0]:(0.590825438499) A[1]:(0.656192779541) A[2]:(1.7523765564e-05) A[3]:(0.531107962132)\n",
      " state (5)  A[0]:(-0.0370231084526) A[1]:(0.999897003174) A[2]:(-0.824358463287) A[3]:(0.661302626133)\n",
      " state (6)  A[0]:(0.000872894888744) A[1]:(0.81007951498) A[2]:(1.14440917969e-05) A[3]:(0.655663371086)\n",
      " state (7)  A[0]:(0.54463660717) A[1]:(-0.533653020859) A[2]:(0.438282072544) A[3]:(0.869279503822)\n",
      " state (8)  A[0]:(0.657032191753) A[1]:(0.00040943917702) A[2]:(0.728882193565) A[3]:(0.59080016613)\n",
      " state (9)  A[0]:(0.657128930092) A[1]:(0.810166537762) A[2]:(0.810119867325) A[3]:(-0.000419110030634)\n",
      " state (10)  A[0]:(0.730466783047) A[1]:(0.900075554848) A[2]:(0.000611782015767) A[3]:(0.728654563427)\n",
      " state (11)  A[0]:(0.141013294458) A[1]:(0.883327543736) A[2]:(-0.931220054626) A[3]:(0.803563535213)\n",
      " state (12)  A[0]:(-0.423454105854) A[1]:(0.816674470901) A[2]:(-0.957166314125) A[3]:(0.715063631535)\n",
      " state (13)  A[0]:(0.00430161086842) A[1]:(0.809356808662) A[2]:(0.899989545345) A[3]:(0.728697419167)\n",
      " state (14)  A[0]:(0.811493396759) A[1]:(0.900172650814) A[2]:(0.999999880791) A[3]:(0.809637069702)\n",
      " state (15)  A[0]:(0.980053842068) A[1]:(0.937320590019) A[2]:(1.0) A[3]:(0.878124177456)\n",
      "Episode 724000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6006. Times reached goal: 994.               Steps done: 5008320. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00601391387267.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5904,  0.5905,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 5.3142e-01,  3.0398e-06,  6.5604e-01,  5.9067e-01]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.7291,  0.5915,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0001,  0.8100, -0.0007,  0.6564]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7296,  0.9000,  0.0004,  0.7296]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9003,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53132468462) A[1]:(0.590497970581) A[2]:(0.590527057648) A[3]:(0.531507730484)\n",
      " state (1)  A[0]:(0.531409919262) A[1]:(-1.41113996506e-05) A[2]:(0.65596562624) A[3]:(0.590623915195)\n",
      " state (2)  A[0]:(0.590246558189) A[1]:(0.729033112526) A[2]:(0.591422557831) A[3]:(0.656101465225)\n",
      " state (3)  A[0]:(0.656226694584) A[1]:(-0.0040595754981) A[2]:(0.527059793472) A[3]:(0.54885405302)\n",
      " state (4)  A[0]:(0.590253472328) A[1]:(0.656304121017) A[2]:(0.00105226004962) A[3]:(0.531428456306)\n",
      " state (5)  A[0]:(-0.038122035563) A[1]:(0.999897003174) A[2]:(-0.824714243412) A[3]:(0.661747634411)\n",
      " state (6)  A[0]:(0.000334486365318) A[1]:(0.809999465942) A[2]:(-0.000912070041522) A[3]:(0.656460523605)\n",
      " state (7)  A[0]:(0.544235348701) A[1]:(-0.534774303436) A[2]:(0.438279092312) A[3]:(0.869655251503)\n",
      " state (8)  A[0]:(0.656315326691) A[1]:(-0.000381514400942) A[2]:(0.728986024857) A[3]:(0.590757012367)\n",
      " state (9)  A[0]:(0.656605243683) A[1]:(0.809888780117) A[2]:(0.809910058975) A[3]:(0.000688522937708)\n",
      " state (10)  A[0]:(0.729776859283) A[1]:(0.899919986725) A[2]:(-0.000302910804749) A[3]:(0.729579091072)\n",
      " state (11)  A[0]:(0.138491913676) A[1]:(0.883203923702) A[2]:(-0.931355893612) A[3]:(0.804274320602)\n",
      " state (12)  A[0]:(-0.426253318787) A[1]:(0.816611886024) A[2]:(-0.957248151302) A[3]:(0.715986847878)\n",
      " state (13)  A[0]:(0.000734835746698) A[1]:(0.809427797794) A[2]:(0.899943470955) A[3]:(0.729557871819)\n",
      " state (14)  A[0]:(0.810468673706) A[1]:(0.900271058083) A[2]:(0.999999880791) A[3]:(0.810293674469)\n",
      " state (15)  A[0]:(0.979964017868) A[1]:(0.937390267849) A[2]:(1.0) A[3]:(0.878574609756)\n",
      "Episode 725000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6029. Times reached goal: 992.               Steps done: 5014349. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.005977765066.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531560182571) A[1]:(0.589760184288) A[2]:(0.590638041496) A[3]:(0.531827628613)\n",
      " state (1)  A[0]:(0.531259000301) A[1]:(6.45518302917e-05) A[2]:(0.656089603901) A[3]:(0.590809583664)\n",
      " state (2)  A[0]:(0.590030193329) A[1]:(0.728866815567) A[2]:(0.592414498329) A[3]:(0.656433939934)\n",
      " state (3)  A[0]:(0.65607970953) A[1]:(-0.00643917964771) A[2]:(0.528535366058) A[3]:(0.549060225487)\n",
      " state (4)  A[0]:(0.589929521084) A[1]:(0.655955314636) A[2]:(0.00175511662383) A[3]:(0.531737625599)\n",
      " state (5)  A[0]:(-0.0380789302289) A[1]:(0.999896883965) A[2]:(-0.825156450272) A[3]:(0.662183225155)\n",
      " state (6)  A[0]:(0.000202462077141) A[1]:(0.809864699841) A[2]:(-0.00106024707202) A[3]:(0.656395852566)\n",
      " state (7)  A[0]:(0.54389154911) A[1]:(-0.534764111042) A[2]:(0.438390135765) A[3]:(0.869511127472)\n",
      " state (8)  A[0]:(0.655763626099) A[1]:(-0.000531494559254) A[2]:(0.728991508484) A[3]:(0.590581834316)\n",
      " state (9)  A[0]:(0.65593624115) A[1]:(0.809783577919) A[2]:(0.809919476509) A[3]:(0.00103545153979)\n",
      " state (10)  A[0]:(0.729469060898) A[1]:(0.899945855141) A[2]:(-0.000193238258362) A[3]:(0.72981107235)\n",
      " state (11)  A[0]:(0.138259381056) A[1]:(0.883385419846) A[2]:(-0.931354999542) A[3]:(0.804434657097)\n",
      " state (12)  A[0]:(-0.426802545786) A[1]:(0.817074120045) A[2]:(-0.95726442337) A[3]:(0.715913772583)\n",
      " state (13)  A[0]:(-0.000491201819386) A[1]:(0.810000360012) A[2]:(0.900085091591) A[3]:(0.729173898697)\n",
      " state (14)  A[0]:(0.810000181198) A[1]:(0.900547325611) A[2]:(0.999999880791) A[3]:(0.80986315012)\n",
      " state (15)  A[0]:(0.979918777943) A[1]:(0.937486469746) A[2]:(1.0) A[3]:(0.878229320049)\n",
      "Episode 726000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 992.               Steps done: 5020356. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00594196426661.\n",
      " state (0)  A[0]:(0.531345963478) A[1]:(0.590090990067) A[2]:(0.590288639069) A[3]:(0.531298160553)\n",
      " state (1)  A[0]:(0.531372785568) A[1]:(0.000100836157799) A[2]:(0.655831813812) A[3]:(0.590331077576)\n",
      " state (2)  A[0]:(0.590236544609) A[1]:(0.728931427002) A[2]:(0.592823028564) A[3]:(0.656097531319)\n",
      " state (3)  A[0]:(0.656410694122) A[1]:(-0.00762330135331) A[2]:(0.529592633247) A[3]:(0.54857635498)\n",
      " state (4)  A[0]:(0.590310573578) A[1]:(0.655925214291) A[2]:(0.00256561674178) A[3]:(0.531411767006)\n",
      " state (5)  A[0]:(-0.0374048240483) A[1]:(0.99989682436) A[2]:(-0.825308680534) A[3]:(0.662101566792)\n",
      " state (6)  A[0]:(8.83489847183e-05) A[1]:(0.809854269028) A[2]:(-0.000623583735432) A[3]:(0.655577778816)\n",
      " state (7)  A[0]:(0.543744564056) A[1]:(-0.533715963364) A[2]:(0.438205420971) A[3]:(0.869134604931)\n",
      " state (8)  A[0]:(0.655966162682) A[1]:(-0.000163570046425) A[2]:(0.728585720062) A[3]:(0.590569198132)\n",
      " state (9)  A[0]:(0.655745267868) A[1]:(0.809775173664) A[2]:(0.809799015522) A[3]:(-0.000434577435954)\n",
      " state (10)  A[0]:(0.729410409927) A[1]:(0.899893403053) A[2]:(-0.000311970710754) A[3]:(0.728919386864)\n",
      " state (11)  A[0]:(0.138905778527) A[1]:(0.883253276348) A[2]:(-0.931385993958) A[3]:(0.804012358189)\n",
      " state (12)  A[0]:(-0.425842434168) A[1]:(0.816725432873) A[2]:(-0.957331478596) A[3]:(0.715631604195)\n",
      " state (13)  A[0]:(0.000815376464743) A[1]:(0.809436023235) A[2]:(0.899797320366) A[3]:(0.729107260704)\n",
      " state (14)  A[0]:(0.810370922089) A[1]:(0.90014231205) A[2]:(0.999999880791) A[3]:(0.809939324856)\n",
      " state (15)  A[0]:(0.979941964149) A[1]:(0.937205433846) A[2]:(1.0) A[3]:(0.878308713436)\n",
      "Episode 727000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6002. Times reached goal: 988.               Steps done: 5026358. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00590640740995.\n",
      " state (0)  A[0]:(0.530097663403) A[1]:(0.5903455019) A[2]:(0.590495109558) A[3]:(0.532542824745)\n",
      " state (1)  A[0]:(0.530554592609) A[1]:(-6.3419342041e-05) A[2]:(0.655814051628) A[3]:(0.590867042542)\n",
      " state (2)  A[0]:(0.589489221573) A[1]:(0.728740334511) A[2]:(0.592844188213) A[3]:(0.656331181526)\n",
      " state (3)  A[0]:(0.655812382698) A[1]:(-0.00986544042826) A[2]:(0.529903113842) A[3]:(0.548425316811)\n",
      " state (4)  A[0]:(0.589642763138) A[1]:(0.655913114548) A[2]:(0.00191449886188) A[3]:(0.531115412712)\n",
      " state (5)  A[0]:(-0.0371982865036) A[1]:(0.999896764755) A[2]:(-0.825809299946) A[3]:(0.661821722984)\n",
      " state (6)  A[0]:(0.000323414802551) A[1]:(0.809870839119) A[2]:(-0.000241279602051) A[3]:(0.65489834547)\n",
      " state (7)  A[0]:(0.544452190399) A[1]:(-0.532763659954) A[2]:(0.438718467951) A[3]:(0.869071125984)\n",
      " state (8)  A[0]:(0.657848119736) A[1]:(0.000167846679688) A[2]:(0.728433012962) A[3]:(0.592419147491)\n",
      " state (9)  A[0]:(0.65686416626) A[1]:(0.810189545155) A[2]:(0.810029864311) A[3]:(-0.00253936112858)\n",
      " state (10)  A[0]:(0.730005979538) A[1]:(0.900093317032) A[2]:(0.000630617083516) A[3]:(0.726797819138)\n",
      " state (11)  A[0]:(0.140521764755) A[1]:(0.883284091949) A[2]:(-0.931290626526) A[3]:(0.802336573601)\n",
      " state (12)  A[0]:(-0.423627436161) A[1]:(0.816457867622) A[2]:(-0.957284390926) A[3]:(0.713532924652)\n",
      " state (13)  A[0]:(0.00469071697444) A[1]:(0.808907091618) A[2]:(0.89988732338) A[3]:(0.727330386639)\n",
      " state (14)  A[0]:(0.811966598034) A[1]:(0.899790942669) A[2]:(0.999999880791) A[3]:(0.808750092983)\n",
      " state (15)  A[0]:(0.980127573013) A[1]:(0.936987936497) A[2]:(1.0) A[3]:(0.877537190914)\n",
      "Episode 728000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6008. Times reached goal: 989.               Steps done: 5032366. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0058710281001.\n",
      " state (0)  A[0]:(0.531426072121) A[1]:(0.590650558472) A[2]:(0.590502202511) A[3]:(0.530729174614)\n",
      " state (1)  A[0]:(0.531077384949) A[1]:(-0.000307604670525) A[2]:(0.656041383743) A[3]:(0.589206218719)\n",
      " state (2)  A[0]:(0.589874088764) A[1]:(0.728761196136) A[2]:(0.593804597855) A[3]:(0.655003368855)\n",
      " state (3)  A[0]:(0.656344473362) A[1]:(-0.0116295786574) A[2]:(0.531586885452) A[3]:(0.546815633774)\n",
      " state (4)  A[0]:(0.590196013451) A[1]:(0.655691862106) A[2]:(0.00347708258778) A[3]:(0.530144929886)\n",
      " state (5)  A[0]:(-0.0364483445883) A[1]:(0.999896943569) A[2]:(-0.825788140297) A[3]:(0.662423729897)\n",
      " state (6)  A[0]:(2.60174274445e-05) A[1]:(0.810435414314) A[2]:(0.00150275113992) A[3]:(0.655373752117)\n",
      " state (7)  A[0]:(0.543222904205) A[1]:(-0.531691074371) A[2]:(0.440858721733) A[3]:(0.868855953217)\n",
      " state (8)  A[0]:(0.655410170555) A[1]:(0.00310558569618) A[2]:(0.730048418045) A[3]:(0.588729858398)\n",
      " state (9)  A[0]:(0.655894160271) A[1]:(0.810680866241) A[2]:(0.810686707497) A[3]:(-0.00236206804402)\n",
      " state (10)  A[0]:(0.730036139488) A[1]:(0.900367379189) A[2]:(0.00275468127802) A[3]:(0.728656888008)\n",
      " state (11)  A[0]:(0.140087947249) A[1]:(0.883894681931) A[2]:(-0.930951178074) A[3]:(0.80411529541)\n",
      " state (12)  A[0]:(-0.426567167044) A[1]:(0.817883968353) A[2]:(-0.957090437412) A[3]:(0.715457320213)\n",
      " state (13)  A[0]:(-0.00171671644785) A[1]:(0.810827374458) A[2]:(0.900694549084) A[3]:(0.728487253189)\n",
      " state (14)  A[0]:(0.809442996979) A[1]:(0.900923848152) A[2]:(0.999999880791) A[3]:(0.809324622154)\n",
      " state (15)  A[0]:(0.979850649834) A[1]:(0.937619030476) A[2]:(1.0) A[3]:(0.877846539021)\n",
      "Episode 729000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6014. Times reached goal: 993.               Steps done: 5038380. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00583582569683.\n",
      "q_values \n",
      "tensor([[ 0.5316,  0.5897,  0.5905,  0.5317]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5312, -0.0010,  0.6558,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5898,  0.7289,  0.5921,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8102, -0.0004,  0.6554]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7298,  0.9000,  0.0010,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8106,  0.9006,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531669735909) A[1]:(0.58972376585) A[2]:(0.590483903885) A[3]:(0.531697511673)\n",
      " state (1)  A[0]:(0.531277298927) A[1]:(-0.00107055855915) A[2]:(0.655737876892) A[3]:(0.590569376945)\n",
      " state (2)  A[0]:(0.589797735214) A[1]:(0.728946626186) A[2]:(0.592034459114) A[3]:(0.656168401241)\n",
      " state (3)  A[0]:(0.656238555908) A[1]:(-0.0120636932552) A[2]:(0.529730021954) A[3]:(0.548010349274)\n",
      " state (4)  A[0]:(0.589954674244) A[1]:(0.656427621841) A[2]:(0.000218033790588) A[3]:(0.531396389008)\n",
      " state (5)  A[0]:(-0.0360021479428) A[1]:(0.999897062778) A[2]:(-0.826933801174) A[3]:(0.663580417633)\n",
      " state (6)  A[0]:(0.000261053442955) A[1]:(0.810216069221) A[2]:(-0.000686287763529) A[3]:(0.655490636826)\n",
      " state (7)  A[0]:(0.543354868889) A[1]:(-0.53351688385) A[2]:(0.439714252949) A[3]:(0.86882853508)\n",
      " state (8)  A[0]:(0.656564056873) A[1]:(-0.000929176516365) A[2]:(0.729074478149) A[3]:(0.591630578041)\n",
      " state (9)  A[0]:(0.656349658966) A[1]:(0.809699416161) A[2]:(0.810242056847) A[3]:(0.000433117122157)\n",
      " state (10)  A[0]:(0.729863405228) A[1]:(0.899995207787) A[2]:(0.000898360973224) A[3]:(0.729065775871)\n",
      " state (11)  A[0]:(0.140049412847) A[1]:(0.883581221104) A[2]:(-0.931311964989) A[3]:(0.8043384552)\n",
      " state (12)  A[0]:(-0.425192594528) A[1]:(0.81749022007) A[2]:(-0.957353591919) A[3]:(0.716118693352)\n",
      " state (13)  A[0]:(0.00128967990167) A[1]:(0.810365438461) A[2]:(0.899876832962) A[3]:(0.729414105415)\n",
      " state (14)  A[0]:(0.810582101345) A[1]:(0.900648117065) A[2]:(0.999999880791) A[3]:(0.810018062592)\n",
      " state (15)  A[0]:(0.979981005192) A[1]:(0.93746972084) A[2]:(1.0) A[3]:(0.878250479698)\n",
      "Episode 730000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 992.               Steps done: 5044387. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00580087497147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531672954559) A[1]:(0.590707361698) A[2]:(0.59055185318) A[3]:(0.531542956829)\n",
      " state (1)  A[0]:(0.531526565552) A[1]:(-0.000610724033322) A[2]:(0.656180739403) A[3]:(0.590524673462)\n",
      " state (2)  A[0]:(0.590178906918) A[1]:(0.729047417641) A[2]:(0.593306899071) A[3]:(0.656184792519)\n",
      " state (3)  A[0]:(0.656614303589) A[1]:(-0.0125414859504) A[2]:(0.531767487526) A[3]:(0.54797577858)\n",
      " state (4)  A[0]:(0.590388834476) A[1]:(0.656125307083) A[2]:(0.00275432365015) A[3]:(0.531645774841)\n",
      " state (5)  A[0]:(-0.0354587733746) A[1]:(0.999896883965) A[2]:(-0.826708316803) A[3]:(0.664435744286)\n",
      " state (6)  A[0]:(-0.000145152211189) A[1]:(0.81002831459) A[2]:(-0.000161409378052) A[3]:(0.655969619751)\n",
      " state (7)  A[0]:(0.542754292488) A[1]:(-0.532925963402) A[2]:(0.439612716436) A[3]:(0.868915736675)\n",
      " state (8)  A[0]:(0.655666828156) A[1]:(0.000253543257713) A[2]:(0.728986620903) A[3]:(0.59084880352)\n",
      " state (9)  A[0]:(0.655497074127) A[1]:(0.809911966324) A[2]:(0.810036659241) A[3]:(-0.000420659751398)\n",
      " state (10)  A[0]:(0.729098200798) A[1]:(0.899994015694) A[2]:(0.00016450881958) A[3]:(0.728935837746)\n",
      " state (11)  A[0]:(0.138105943799) A[1]:(0.883454561234) A[2]:(-0.931392371655) A[3]:(0.804363012314)\n",
      " state (12)  A[0]:(-0.426962137222) A[1]:(0.817121863365) A[2]:(-0.957359611988) A[3]:(0.716236948967)\n",
      " state (13)  A[0]:(-0.000803634349722) A[1]:(0.809852957726) A[2]:(0.900130629539) A[3]:(0.729526281357)\n",
      " state (14)  A[0]:(0.80986136198) A[1]:(0.900312304497) A[2]:(0.999999880791) A[3]:(0.81006026268)\n",
      " state (15)  A[0]:(0.979880571365) A[1]:(0.937226295471) A[2]:(1.0) A[3]:(0.878229498863)\n",
      "Episode 731000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6038. Times reached goal: 996.               Steps done: 5050425. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00576595481842.\n",
      " state (0)  A[0]:(0.531930208206) A[1]:(0.589838027954) A[2]:(0.590383648872) A[3]:(0.531368017197)\n",
      " state (1)  A[0]:(0.531398057938) A[1]:(-0.000102356076241) A[2]:(0.655775308609) A[3]:(0.590530872345)\n",
      " state (2)  A[0]:(0.58977240324) A[1]:(0.728661775589) A[2]:(0.591646373272) A[3]:(0.656173229218)\n",
      " state (3)  A[0]:(0.656012058258) A[1]:(-0.0142431911081) A[2]:(0.529916405678) A[3]:(0.547784626484)\n",
      " state (4)  A[0]:(0.589691162109) A[1]:(0.655583500862) A[2]:(-5.97238540649e-05) A[3]:(0.531638979912)\n",
      " state (5)  A[0]:(-0.0351022146642) A[1]:(0.999896705151) A[2]:(-0.827432990074) A[3]:(0.664971888065)\n",
      " state (6)  A[0]:(-2.79992818832e-05) A[1]:(0.809842944145) A[2]:(-0.00109922839329) A[3]:(0.656004309654)\n",
      " state (7)  A[0]:(0.542551040649) A[1]:(-0.532622516155) A[2]:(0.439013659954) A[3]:(0.868820548058)\n",
      " state (8)  A[0]:(0.655101776123) A[1]:(0.000661298516206) A[2]:(0.728599905968) A[3]:(0.591060876846)\n",
      " state (9)  A[0]:(0.654445528984) A[1]:(0.810050606728) A[2]:(0.809834063053) A[3]:(6.56247138977e-05)\n",
      " state (10)  A[0]:(0.728351831436) A[1]:(0.900019168854) A[2]:(-6.19888305664e-05) A[3]:(0.729016423225)\n",
      " state (11)  A[0]:(0.137149348855) A[1]:(0.883395075798) A[2]:(-0.931414663792) A[3]:(0.804370641708)\n",
      " state (12)  A[0]:(-0.427459865808) A[1]:(0.816865622997) A[2]:(-0.957408428192) A[3]:(0.716165065765)\n",
      " state (13)  A[0]:(-0.00114105595276) A[1]:(0.809413194656) A[2]:(0.899963259697) A[3]:(0.729470312595)\n",
      " state (14)  A[0]:(0.809824347496) A[1]:(0.900004208088) A[2]:(0.999999880791) A[3]:(0.810091853142)\n",
      " state (15)  A[0]:(0.979875981808) A[1]:(0.937006115913) A[2]:(1.0) A[3]:(0.878289818764)\n",
      "Episode 732000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6010. Times reached goal: 993.               Steps done: 5056435. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0057314053551.\n",
      " state (0)  A[0]:(0.531307578087) A[1]:(0.59031021595) A[2]:(0.590603590012) A[3]:(0.53110396862)\n",
      " state (1)  A[0]:(0.530753552914) A[1]:(0.00201493222266) A[2]:(0.655949354172) A[3]:(0.590246677399)\n",
      " state (2)  A[0]:(0.589365363121) A[1]:(0.728653132915) A[2]:(0.592185497284) A[3]:(0.655603170395)\n",
      " state (3)  A[0]:(0.655906438828) A[1]:(-0.0160877536982) A[2]:(0.531059622765) A[3]:(0.546924114227)\n",
      " state (4)  A[0]:(0.589401245117) A[1]:(0.655423521996) A[2]:(0.00075221050065) A[3]:(0.530901074409)\n",
      " state (5)  A[0]:(-0.0361386463046) A[1]:(0.999896645546) A[2]:(-0.827538073063) A[3]:(0.664651155472)\n",
      " state (6)  A[0]:(-0.00130665232427) A[1]:(0.809487223625) A[2]:(-1.43051147461e-05) A[3]:(0.655676484108)\n",
      " state (7)  A[0]:(0.541871190071) A[1]:(-0.534299314022) A[2]:(0.440410971642) A[3]:(0.868796348572)\n",
      " state (8)  A[0]:(0.655375599861) A[1]:(-0.00178211741149) A[2]:(0.729036688805) A[3]:(0.591317296028)\n",
      " state (9)  A[0]:(0.655140399933) A[1]:(0.809345126152) A[2]:(0.80983453989) A[3]:(-0.000115752220154)\n",
      " state (10)  A[0]:(0.728485941887) A[1]:(0.899686932564) A[2]:(-0.00100100005511) A[3]:(0.728772819042)\n",
      " state (11)  A[0]:(0.136009916663) A[1]:(0.88312792778) A[2]:(-0.931641817093) A[3]:(0.804112970829)\n",
      " state (12)  A[0]:(-0.429324686527) A[1]:(0.816708385944) A[2]:(-0.957572042942) A[3]:(0.715772509575)\n",
      " state (13)  A[0]:(-0.00408912403509) A[1]:(0.809510886669) A[2]:(0.899662256241) A[3]:(0.729074597359)\n",
      " state (14)  A[0]:(0.808746814728) A[1]:(0.900173187256) A[2]:(0.999999880791) A[3]:(0.809807300568)\n",
      " state (15)  A[0]:(0.979759931564) A[1]:(0.937146008015) A[2]:(1.0) A[3]:(0.878083646297)\n",
      "Episode 733000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5994. Times reached goal: 993.               Steps done: 5062429. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00569715406506.\n",
      " state (0)  A[0]:(0.531579613686) A[1]:(0.590211868286) A[2]:(0.590311050415) A[3]:(0.531590044498)\n",
      " state (1)  A[0]:(0.531536877155) A[1]:(0.000547125877347) A[2]:(0.655902683735) A[3]:(0.591150224209)\n",
      " state (2)  A[0]:(0.589993238449) A[1]:(0.728888392448) A[2]:(0.59255027771) A[3]:(0.656547904015)\n",
      " state (3)  A[0]:(0.656528353691) A[1]:(-0.0164666045457) A[2]:(0.532087624073) A[3]:(0.547768712044)\n",
      " state (4)  A[0]:(0.59018611908) A[1]:(0.655930995941) A[2]:(0.00159597257152) A[3]:(0.53168129921)\n",
      " state (5)  A[0]:(-0.0339817330241) A[1]:(0.999896943569) A[2]:(-0.827852845192) A[3]:(0.665495753288)\n",
      " state (6)  A[0]:(-6.36279582977e-06) A[1]:(0.810003876686) A[2]:(-0.000464796990855) A[3]:(0.655977010727)\n",
      " state (7)  A[0]:(0.542045474052) A[1]:(-0.533751368523) A[2]:(0.440357357264) A[3]:(0.868598043919)\n",
      " state (8)  A[0]:(0.654952049255) A[1]:(-0.00135521509219) A[2]:(0.729016423225) A[3]:(0.590678215027)\n",
      " state (9)  A[0]:(0.654690265656) A[1]:(0.809573829174) A[2]:(0.810017466545) A[3]:(-0.000326246023178)\n",
      " state (10)  A[0]:(0.729170382023) A[1]:(0.900030314922) A[2]:(0.000683903577738) A[3]:(0.729338645935)\n",
      " state (11)  A[0]:(0.139805942774) A[1]:(0.88388222456) A[2]:(-0.931326329708) A[3]:(0.805042624474)\n",
      " state (12)  A[0]:(-0.425870090723) A[1]:(0.818303704262) A[2]:(-0.957410156727) A[3]:(0.716939151287)\n",
      " state (13)  A[0]:(-0.000135749578476) A[1]:(0.811467468739) A[2]:(0.900080084801) A[3]:(0.729687809944)\n",
      " state (14)  A[0]:(0.81018447876) A[1]:(0.901324927807) A[2]:(0.999999880791) A[3]:(0.809862494469)\n",
      " state (15)  A[0]:(0.979944586754) A[1]:(0.937883496284) A[2]:(1.0) A[3]:(0.877918839455)\n",
      "Episode 734000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6017. Times reached goal: 994.               Steps done: 5068446. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00566297721322.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5905,  0.5906,  0.5318]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.0008,  0.6561,  0.5908]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.7291,  0.5928,  0.6564]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0004,  0.8099,  0.0007,  0.6563]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000, -0.0003,  0.7290]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8102,  0.9002,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531459569931) A[1]:(0.590477883816) A[2]:(0.590673446655) A[3]:(0.531868755817)\n",
      " state (1)  A[0]:(0.531336069107) A[1]:(0.000738084199838) A[2]:(0.656018435955) A[3]:(0.590886354446)\n",
      " state (2)  A[0]:(0.58997130394) A[1]:(0.729041695595) A[2]:(0.592548191547) A[3]:(0.656465947628)\n",
      " state (3)  A[0]:(0.656498014927) A[1]:(-0.0167142674327) A[2]:(0.532532453537) A[3]:(0.547728061676)\n",
      " state (4)  A[0]:(0.59011387825) A[1]:(0.656225323677) A[2]:(0.00193202251103) A[3]:(0.532052397728)\n",
      " state (5)  A[0]:(-0.0337689816952) A[1]:(0.999896943569) A[2]:(-0.827944517136) A[3]:(0.666474282742)\n",
      " state (6)  A[0]:(-5.84870576859e-05) A[1]:(0.810066521168) A[2]:(-0.000121474266052) A[3]:(0.656380355358)\n",
      " state (7)  A[0]:(0.542155504227) A[1]:(-0.532344281673) A[2]:(0.440145134926) A[3]:(0.868748128414)\n",
      " state (8)  A[0]:(0.655277609825) A[1]:(0.000588595808949) A[2]:(0.728733539581) A[3]:(0.591119885445)\n",
      " state (9)  A[0]:(0.65480029583) A[1]:(0.810048162937) A[2]:(0.809595108032) A[3]:(-0.000505715550389)\n",
      " state (10)  A[0]:(0.728472948074) A[1]:(0.90001386404) A[2]:(-0.00193035358097) A[3]:(0.728556036949)\n",
      " state (11)  A[0]:(0.137320250273) A[1]:(0.883451461792) A[2]:(-0.931772887707) A[3]:(0.804124474525)\n",
      " state (12)  A[0]:(-0.427069306374) A[1]:(0.817094266415) A[2]:(-0.957591593266) A[3]:(0.715992331505)\n",
      " state (13)  A[0]:(-0.000350043177605) A[1]:(0.809799909592) A[2]:(0.900108516216) A[3]:(0.729348897934)\n",
      " state (14)  A[0]:(0.81014084816) A[1]:(0.90024715662) A[2]:(0.999999880791) A[3]:(0.809975147247)\n",
      " state (15)  A[0]:(0.979908704758) A[1]:(0.937111079693) A[2]:(1.0) A[3]:(0.878137290478)\n",
      "Episode 735000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6006. Times reached goal: 992.               Steps done: 5074452. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00562906730547.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531734585762) A[1]:(0.590516507626) A[2]:(0.590408742428) A[3]:(0.531170725822)\n",
      " state (1)  A[0]:(0.531580924988) A[1]:(-0.000135272741318) A[2]:(0.656064212322) A[3]:(0.590010643005)\n",
      " state (2)  A[0]:(0.5900785923) A[1]:(0.729014158249) A[2]:(0.592925608158) A[3]:(0.655791819096)\n",
      " state (3)  A[0]:(0.656595110893) A[1]:(-0.0182447154075) A[2]:(0.533415794373) A[3]:(0.546711862087)\n",
      " state (4)  A[0]:(0.590106844902) A[1]:(0.656239628792) A[2]:(0.00226354203187) A[3]:(0.531208515167)\n",
      " state (5)  A[0]:(-0.033221129328) A[1]:(0.999896943569) A[2]:(-0.828305363655) A[3]:(0.666269421577)\n",
      " state (6)  A[0]:(7.28964805603e-05) A[1]:(0.810062587261) A[2]:(-0.000104904174805) A[3]:(0.655591785908)\n",
      " state (7)  A[0]:(0.542099058628) A[1]:(-0.532127976418) A[2]:(0.440394818783) A[3]:(0.86839723587)\n",
      " state (8)  A[0]:(0.65540343523) A[1]:(0.000637918652501) A[2]:(0.72891330719) A[3]:(0.590995371342)\n",
      " state (9)  A[0]:(0.655000805855) A[1]:(0.810088157654) A[2]:(0.809968471527) A[3]:(-6.12437725067e-05)\n",
      " state (10)  A[0]:(0.728842616081) A[1]:(0.900077104568) A[2]:(-0.000182271003723) A[3]:(0.729130506516)\n",
      " state (11)  A[0]:(0.13772778213) A[1]:(0.88355755806) A[2]:(-0.931561350822) A[3]:(0.804641246796)\n",
      " state (12)  A[0]:(-0.428310424089) A[1]:(0.817252457142) A[2]:(-0.957576394081) A[3]:(0.716277778149)\n",
      " state (13)  A[0]:(-0.00384687958285) A[1]:(0.809888124466) A[2]:(0.899911463261) A[3]:(0.72922283411)\n",
      " state (14)  A[0]:(0.808518767357) A[1]:(0.900221824646) A[2]:(0.999999880791) A[3]:(0.809781432152)\n",
      " state (15)  A[0]:(0.979703724384) A[1]:(0.93703687191) A[2]:(1.0) A[3]:(0.877992630005)\n",
      "Episode 736000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6008. Times reached goal: 987.               Steps done: 5080460. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00559534925953.\n",
      " state (0)  A[0]:(0.531736552715) A[1]:(0.590590417385) A[2]:(0.59065502882) A[3]:(0.531780958176)\n",
      " state (1)  A[0]:(0.531814038754) A[1]:(-0.000282153487206) A[2]:(0.656413912773) A[3]:(0.590665817261)\n",
      " state (2)  A[0]:(0.590202331543) A[1]:(0.729133367538) A[2]:(0.59288841486) A[3]:(0.656346857548)\n",
      " state (3)  A[0]:(0.656677603722) A[1]:(-0.0199533291161) A[2]:(0.533433496952) A[3]:(0.546948850155)\n",
      " state (4)  A[0]:(0.590019106865) A[1]:(0.656333327293) A[2]:(0.00104975665454) A[3]:(0.53165435791)\n",
      " state (5)  A[0]:(-0.0329228676856) A[1]:(0.999896943569) A[2]:(-0.828985214233) A[3]:(0.667445421219)\n",
      " state (6)  A[0]:(-8.45491886139e-05) A[1]:(0.810020446777) A[2]:(-0.000355124444468) A[3]:(0.656294226646)\n",
      " state (7)  A[0]:(0.541879415512) A[1]:(-0.532462239265) A[2]:(0.440941214561) A[3]:(0.868526935577)\n",
      " state (8)  A[0]:(0.655603706837) A[1]:(0.000249043107033) A[2]:(0.729191064835) A[3]:(0.591262578964)\n",
      " state (9)  A[0]:(0.655528664589) A[1]:(0.809956729412) A[2]:(0.810057997704) A[3]:(0.000182151794434)\n",
      " state (10)  A[0]:(0.729134082794) A[1]:(0.89999461174) A[2]:(-0.000349521636963) A[3]:(0.728850543499)\n",
      " state (11)  A[0]:(0.138654634356) A[1]:(0.883472681046) A[2]:(-0.931580841541) A[3]:(0.804351329803)\n",
      " state (12)  A[0]:(-0.426373958588) A[1]:(0.817136287689) A[2]:(-0.957539916039) A[3]:(0.716149628162)\n",
      " state (13)  A[0]:(-0.000368863315089) A[1]:(0.809782028198) A[2]:(0.900036215782) A[3]:(0.729367017746)\n",
      " state (14)  A[0]:(0.809727370739) A[1]:(0.900231182575) A[2]:(0.999999880791) A[3]:(0.809961736202)\n",
      " state (15)  A[0]:(0.979812383652) A[1]:(0.937137007713) A[2]:(1.0) A[3]:(0.878068864346)\n",
      "Episode 737000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6019. Times reached goal: 996.               Steps done: 5086479. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00556177200446.\n",
      " state (0)  A[0]:(0.529642820358) A[1]:(0.590214848518) A[2]:(0.59032869339) A[3]:(0.531606793404)\n",
      " state (1)  A[0]:(0.529886007309) A[1]:(0.000461786956294) A[2]:(0.656111299992) A[3]:(0.5903673172)\n",
      " state (2)  A[0]:(0.588549733162) A[1]:(0.729013800621) A[2]:(0.59360897541) A[3]:(0.655840158463)\n",
      " state (3)  A[0]:(0.655371189117) A[1]:(-0.0211503244936) A[2]:(0.535051882267) A[3]:(0.546106815338)\n",
      " state (4)  A[0]:(0.588788330555) A[1]:(0.655840754509) A[2]:(0.00299834297039) A[3]:(0.530772209167)\n",
      " state (5)  A[0]:(-0.0336357392371) A[1]:(0.99989682436) A[2]:(-0.82898837328) A[3]:(0.666805505753)\n",
      " state (6)  A[0]:(-0.000868096714839) A[1]:(0.810003936291) A[2]:(-1.52587890625e-05) A[3]:(0.654676795006)\n",
      " state (7)  A[0]:(0.541462302208) A[1]:(-0.531757831573) A[2]:(0.440945327282) A[3]:(0.867625832558)\n",
      " state (8)  A[0]:(0.655548334122) A[1]:(0.00122162641492) A[2]:(0.729122042656) A[3]:(0.588654756546)\n",
      " state (9)  A[0]:(0.655988812447) A[1]:(0.810193181038) A[2]:(0.809993267059) A[3]:(-0.00374667090364)\n",
      " state (10)  A[0]:(0.730054616928) A[1]:(0.900053143501) A[2]:(-0.000519990862813) A[3]:(0.727066397667)\n",
      " state (11)  A[0]:(0.142090380192) A[1]:(0.883470594883) A[2]:(-0.931592345238) A[3]:(0.803139448166)\n",
      " state (12)  A[0]:(-0.422128081322) A[1]:(0.817031085491) A[2]:(-0.957542657852) A[3]:(0.714666962624)\n",
      " state (13)  A[0]:(0.00634374143556) A[1]:(0.809599995613) A[2]:(0.900052070618) A[3]:(0.728088617325)\n",
      " state (14)  A[0]:(0.812421441078) A[1]:(0.900147914886) A[2]:(0.999999880791) A[3]:(0.809096813202)\n",
      " state (15)  A[0]:(0.980137228966) A[1]:(0.937131166458) A[2]:(1.0) A[3]:(0.877481341362)\n",
      "Episode 738000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6028. Times reached goal: 998.               Steps done: 5092507. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00552834648853.\n",
      " state (0)  A[0]:(0.531189084053) A[1]:(0.590461850166) A[2]:(0.590649485588) A[3]:(0.531349420547)\n",
      " state (1)  A[0]:(0.53122740984) A[1]:(5.20795583725e-05) A[2]:(0.655966579914) A[3]:(0.590389370918)\n",
      " state (2)  A[0]:(0.58989739418) A[1]:(0.729083538055) A[2]:(0.592332303524) A[3]:(0.655927538872)\n",
      " state (3)  A[0]:(0.656478822231) A[1]:(-0.0222285278141) A[2]:(0.533640742302) A[3]:(0.546059131622)\n",
      " state (4)  A[0]:(0.589792013168) A[1]:(0.656039655209) A[2]:(0.000426530808909) A[3]:(0.531133294106)\n",
      " state (5)  A[0]:(-0.0322888717055) A[1]:(0.99989682436) A[2]:(-0.829669475555) A[3]:(0.668060541153)\n",
      " state (6)  A[0]:(-0.000426590413554) A[1]:(0.809980392456) A[2]:(-0.000447869271738) A[3]:(0.655608952045)\n",
      " state (7)  A[0]:(0.541338801384) A[1]:(-0.53163176775) A[2]:(0.440861403942) A[3]:(0.868061244488)\n",
      " state (8)  A[0]:(0.655287861824) A[1]:(0.000774487678427) A[2]:(0.728870868683) A[3]:(0.590776801109)\n",
      " state (9)  A[0]:(0.654862046242) A[1]:(0.810088217258) A[2]:(0.809914410114) A[3]:(-0.00106644595508)\n",
      " state (10)  A[0]:(0.728575706482) A[1]:(0.900042951107) A[2]:(-0.00054204458138) A[3]:(0.728452086449)\n",
      " state (11)  A[0]:(0.13754580915) A[1]:(0.883486926556) A[2]:(-0.931632101536) A[3]:(0.804313600063)\n",
      " state (12)  A[0]:(-0.427581548691) A[1]:(0.817064404488) A[2]:(-0.957609415054) A[3]:(0.716113209724)\n",
      " state (13)  A[0]:(-0.00177617184818) A[1]:(0.809589505196) A[2]:(0.90002143383) A[3]:(0.729224085808)\n",
      " state (14)  A[0]:(0.809474825859) A[1]:(0.900044798851) A[2]:(0.999999880791) A[3]:(0.8098077178)\n",
      " state (15)  A[0]:(0.979805886745) A[1]:(0.93694537878) A[2]:(1.0) A[3]:(0.877928197384)\n",
      "Episode 739000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6023. Times reached goal: 996.               Steps done: 5098530. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00549514933123.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5905,  0.5905,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.6561,  0.0006,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6558,  0.0003,  0.7291,  0.5911]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559,  0.8099,  0.8101,  0.0007]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7296,  0.9000, -0.0001,  0.7294]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8106,  0.9001,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531398177147) A[1]:(0.590530276299) A[2]:(0.590473592281) A[3]:(0.531251013279)\n",
      " state (1)  A[0]:(0.531277418137) A[1]:(-8.12858343124e-05) A[2]:(0.656125128269) A[3]:(0.590478301048)\n",
      " state (2)  A[0]:(0.590039849281) A[1]:(0.729014992714) A[2]:(0.592358231544) A[3]:(0.656133115292)\n",
      " state (3)  A[0]:(0.656745672226) A[1]:(-0.0230748075992) A[2]:(0.533991932869) A[3]:(0.546226024628)\n",
      " state (4)  A[0]:(0.590248465538) A[1]:(0.656008243561) A[2]:(0.000485181779368) A[3]:(0.531598806381)\n",
      " state (5)  A[0]:(-0.0309312138706) A[1]:(0.999896883965) A[2]:(-0.829963505268) A[3]:(0.669070839882)\n",
      " state (6)  A[0]:(0.000615447701421) A[1]:(0.810093641281) A[2]:(-0.000472187966807) A[3]:(0.656033933163)\n",
      " state (7)  A[0]:(0.54205262661) A[1]:(-0.531860530376) A[2]:(0.44125238061) A[3]:(0.868130266666)\n",
      " state (8)  A[0]:(0.656305909157) A[1]:(0.000396981806261) A[2]:(0.729097008705) A[3]:(0.591700673103)\n",
      " state (9)  A[0]:(0.656312465668) A[1]:(0.810013711452) A[2]:(0.810106635094) A[3]:(0.00145259394776)\n",
      " state (10)  A[0]:(0.729858458042) A[1]:(0.899996399879) A[2]:(-3.3974647522e-05) A[3]:(0.729589164257)\n",
      " state (11)  A[0]:(0.140274301171) A[1]:(0.883441627026) A[2]:(-0.931589543819) A[3]:(0.80505502224)\n",
      " state (12)  A[0]:(-0.425237774849) A[1]:(0.81702876091) A[2]:(-0.957604408264) A[3]:(0.71696138382)\n",
      " state (13)  A[0]:(0.00136163749266) A[1]:(0.809607744217) A[2]:(0.900027453899) A[3]:(0.729908943176)\n",
      " state (14)  A[0]:(0.810685694218) A[1]:(0.900111854076) A[2]:(0.999999880791) A[3]:(0.810235440731)\n",
      " state (15)  A[0]:(0.979954361916) A[1]:(0.93702852726) A[2]:(1.0) A[3]:(0.87815618515)\n",
      "Episode 740000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6012. Times reached goal: 994.               Steps done: 5104542. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00546221160347.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531497120857) A[1]:(0.5905854702) A[2]:(0.590392827988) A[3]:(0.529286563396)\n",
      " state (1)  A[0]:(0.530950546265) A[1]:(0.000871687894687) A[2]:(0.655940115452) A[3]:(0.588352799416)\n",
      " state (2)  A[0]:(0.58960211277) A[1]:(0.728755950928) A[2]:(0.59326839447) A[3]:(0.654195070267)\n",
      " state (3)  A[0]:(0.656281650066) A[1]:(-0.0245948005468) A[2]:(0.535902738571) A[3]:(0.543687283993)\n",
      " state (4)  A[0]:(0.589368104935) A[1]:(0.655769824982) A[2]:(0.00290631433018) A[3]:(0.529021799564)\n",
      " state (5)  A[0]:(-0.0329640842974) A[1]:(0.999896764755) A[2]:(-0.829622805119) A[3]:(0.66711139679)\n",
      " state (6)  A[0]:(-0.00332856434397) A[1]:(0.809845089912) A[2]:(0.00133085169364) A[3]:(0.65299987793)\n",
      " state (7)  A[0]:(0.537839412689) A[1]:(-0.532154738903) A[2]:(0.442371040583) A[3]:(0.866388559341)\n",
      " state (8)  A[0]:(0.651239395142) A[1]:(-0.000310152769089) A[2]:(0.729218244553) A[3]:(0.585722565651)\n",
      " state (9)  A[0]:(0.649652242661) A[1]:(0.80957365036) A[2]:(0.809507548809) A[3]:(-0.00799960736185)\n",
      " state (10)  A[0]:(0.723217844963) A[1]:(0.89980006218) A[2]:(-0.00362621154636) A[3]:(0.725364923477)\n",
      " state (11)  A[0]:(0.125257670879) A[1]:(0.883432865143) A[2]:(-0.932189285755) A[3]:(0.802449285984)\n",
      " state (12)  A[0]:(-0.437259942293) A[1]:(0.817359089851) A[2]:(-0.957921564579) A[3]:(0.714441418648)\n",
      " state (13)  A[0]:(-0.0122086005285) A[1]:(0.810184597969) A[2]:(0.899699509144) A[3]:(0.728250265121)\n",
      " state (14)  A[0]:(0.806483507156) A[1]:(0.900439918041) A[2]:(0.999999880791) A[3]:(0.809323847294)\n",
      " state (15)  A[0]:(0.979526460171) A[1]:(0.937158107758) A[2]:(1.0) A[3]:(0.877661049366)\n",
      "Episode 741000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6016. Times reached goal: 993.               Steps done: 5110558. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00542944958542.\n",
      " state (0)  A[0]:(0.53136241436) A[1]:(0.590122401714) A[2]:(0.590200841427) A[3]:(0.531055808067)\n",
      " state (1)  A[0]:(0.531224370003) A[1]:(-1.56760215759e-05) A[2]:(0.656143724918) A[3]:(0.590154767036)\n",
      " state (2)  A[0]:(0.589946746826) A[1]:(0.728904247284) A[2]:(0.591399848461) A[3]:(0.65555024147)\n",
      " state (3)  A[0]:(0.656538844109) A[1]:(-0.0241603907198) A[2]:(0.533592700958) A[3]:(0.545105695724)\n",
      " state (4)  A[0]:(0.58992266655) A[1]:(0.656240940094) A[2]:(-0.000189542770386) A[3]:(0.530760288239)\n",
      " state (5)  A[0]:(-0.0305654387921) A[1]:(0.999896883965) A[2]:(-0.83015871048) A[3]:(0.66953754425)\n",
      " state (6)  A[0]:(-0.000136226415634) A[1]:(0.8099193573) A[2]:(0.000204801559448) A[3]:(0.655441045761)\n",
      " state (7)  A[0]:(0.541004598141) A[1]:(-0.531920671463) A[2]:(0.441668272018) A[3]:(0.8675776124)\n",
      " state (8)  A[0]:(0.655500769615) A[1]:(-5.35845756531e-05) A[2]:(0.729001164436) A[3]:(0.590279102325)\n",
      " state (9)  A[0]:(0.655445337296) A[1]:(0.809920072556) A[2]:(0.810014307499) A[3]:(-0.0013114803005)\n",
      " state (10)  A[0]:(0.729276180267) A[1]:(0.899991929531) A[2]:(0.000203490257263) A[3]:(0.728435635567)\n",
      " state (11)  A[0]:(0.139275997877) A[1]:(0.883526444435) A[2]:(-0.931541085243) A[3]:(0.804449915886)\n",
      " state (12)  A[0]:(-0.426560968161) A[1]:(0.817260026932) A[2]:(-0.957618117332) A[3]:(0.716166675091)\n",
      " state (13)  A[0]:(-0.00097641319735) A[1]:(0.809904813766) A[2]:(0.900041699409) A[3]:(0.729105949402)\n",
      " state (14)  A[0]:(0.809712290764) A[1]:(0.900303304195) A[2]:(0.999999880791) A[3]:(0.809683442116)\n",
      " state (15)  A[0]:(0.979827523232) A[1]:(0.937154710293) A[2]:(1.0) A[3]:(0.877796173096)\n",
      "Episode 742000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6022. Times reached goal: 996.               Steps done: 5116580. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00539685169079.\n",
      " state (0)  A[0]:(0.530447661877) A[1]:(0.590660572052) A[2]:(0.590398967266) A[3]:(0.531857252121)\n",
      " state (1)  A[0]:(0.531064629555) A[1]:(-3.74168157578e-05) A[2]:(0.656052112579) A[3]:(0.590992093086)\n",
      " state (2)  A[0]:(0.590070605278) A[1]:(0.729096651077) A[2]:(0.590942144394) A[3]:(0.656605303288)\n",
      " state (3)  A[0]:(0.65671646595) A[1]:(-0.0232106335461) A[2]:(0.533091545105) A[3]:(0.546407103539)\n",
      " state (4)  A[0]:(0.590829372406) A[1]:(0.656214594841) A[2]:(-0.000506997050252) A[3]:(0.532101154327)\n",
      " state (5)  A[0]:(-0.0264765713364) A[1]:(0.999896883965) A[2]:(-0.830253362656) A[3]:(0.67068696022)\n",
      " state (6)  A[0]:(0.00485014077276) A[1]:(0.809980094433) A[2]:(-0.000615000666585) A[3]:(0.656144380569)\n",
      " state (7)  A[0]:(0.544470787048) A[1]:(-0.531324446201) A[2]:(0.440901845694) A[3]:(0.867567658424)\n",
      " state (8)  A[0]:(0.657410025597) A[1]:(0.00156666210387) A[2]:(0.729141712189) A[3]:(0.588504910469)\n",
      " state (9)  A[0]:(0.65792119503) A[1]:(0.810140252113) A[2]:(0.810194969177) A[3]:(-0.00135159411002)\n",
      " state (10)  A[0]:(0.731905281544) A[1]:(0.899960160255) A[2]:(0.00118946970906) A[3]:(0.729082286358)\n",
      " state (11)  A[0]:(0.145585030317) A[1]:(0.883363902569) A[2]:(-0.9313814044) A[3]:(0.805015802383)\n",
      " state (12)  A[0]:(-0.421032190323) A[1]:(0.816801071167) A[2]:(-0.957579851151) A[3]:(0.71686732769)\n",
      " state (13)  A[0]:(0.00582792703062) A[1]:(0.809228658676) A[2]:(0.899911344051) A[3]:(0.729719877243)\n",
      " state (14)  A[0]:(0.811977148056) A[1]:(0.899907886982) A[2]:(0.999999880791) A[3]:(0.810119748116)\n",
      " state (15)  A[0]:(0.98006939888) A[1]:(0.936947464943) A[2]:(1.0) A[3]:(0.878064453602)\n",
      "Episode 743000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6024. Times reached goal: 997.               Steps done: 5122604. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00536443878191.\n",
      " state (0)  A[0]:(0.530758321285) A[1]:(0.590980052948) A[2]:(0.590408980846) A[3]:(0.531706094742)\n",
      " state (1)  A[0]:(0.53113502264) A[1]:(-0.000247836112976) A[2]:(0.656431555748) A[3]:(0.59029340744)\n",
      " state (2)  A[0]:(0.590003967285) A[1]:(0.729227364063) A[2]:(0.591214179993) A[3]:(0.65570473671)\n",
      " state (3)  A[0]:(0.656448125839) A[1]:(-0.0229965429753) A[2]:(0.533320963383) A[3]:(0.54502850771)\n",
      " state (4)  A[0]:(0.589812755585) A[1]:(0.656560301781) A[2]:(-0.00034511089325) A[3]:(0.530675768852)\n",
      " state (5)  A[0]:(-0.0307617709041) A[1]:(0.999897062778) A[2]:(-0.830255866051) A[3]:(0.670012533665)\n",
      " state (6)  A[0]:(-0.000817239109892) A[1]:(0.810123920441) A[2]:(-0.000219821929932) A[3]:(0.655498504639)\n",
      " state (7)  A[0]:(0.540658116341) A[1]:(-0.531907260418) A[2]:(0.441614538431) A[3]:(0.867461800575)\n",
      " state (8)  A[0]:(0.656148731709) A[1]:(7.49826431274e-05) A[2]:(0.728998541832) A[3]:(0.590296626091)\n",
      " state (9)  A[0]:(0.656907320023) A[1]:(0.810021996498) A[2]:(0.809867143631) A[3]:(-0.00107639981434)\n",
      " state (10)  A[0]:(0.730252146721) A[1]:(0.900001347065) A[2]:(-0.00130736758001) A[3]:(0.727904140949)\n",
      " state (11)  A[0]:(0.141704142094) A[1]:(0.883423507214) A[2]:(-0.931830763817) A[3]:(0.803725123405)\n",
      " state (12)  A[0]:(-0.422233641148) A[1]:(0.816911578178) A[2]:(-0.957736909389) A[3]:(0.715588927269)\n",
      " state (13)  A[0]:(0.00678619742393) A[1]:(0.809398472309) A[2]:(0.899981558323) A[3]:(0.729160428047)\n",
      " state (14)  A[0]:(0.812574148178) A[1]:(0.900067210197) A[2]:(0.999999880791) A[3]:(0.809990346432)\n",
      " state (15)  A[0]:(0.980134248734) A[1]:(0.937099039555) A[2]:(1.0) A[3]:(0.878049373627)\n",
      "Episode 744000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 995.               Steps done: 5128611. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00533231118997.\n",
      "q_values \n",
      "tensor([[ 0.5310,  0.5898,  0.5902,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5309, -0.0005,  0.6557,  0.5904]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5895,  0.7285,  0.5905,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0012,  0.8097, -0.0003,  0.6567]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7323,  0.8999,  0.0007,  0.7314]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8121,  0.9001,  1.0000,  0.8105]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530685305595) A[1]:(0.589921236038) A[2]:(0.590064704418) A[3]:(0.531293392181)\n",
      " state (1)  A[0]:(0.530527114868) A[1]:(-0.000324890017509) A[2]:(0.655776500702) A[3]:(0.59034717083)\n",
      " state (2)  A[0]:(0.589191496372) A[1]:(0.728564798832) A[2]:(0.590478301048) A[3]:(0.655943274498)\n",
      " state (3)  A[0]:(0.655881524086) A[1]:(-0.0239416342229) A[2]:(0.532726883888) A[3]:(0.545610189438)\n",
      " state (4)  A[0]:(0.589666962624) A[1]:(0.655612051487) A[2]:(-0.000568747462239) A[3]:(0.531637072563)\n",
      " state (5)  A[0]:(-0.0297215040773) A[1]:(0.99989682436) A[2]:(-0.830173790455) A[3]:(0.67112493515)\n",
      " state (6)  A[0]:(0.000471770734293) A[1]:(0.809795498848) A[2]:(-0.000211834907532) A[3]:(0.656580090523)\n",
      " state (7)  A[0]:(0.54179084301) A[1]:(-0.532256007195) A[2]:(0.441280901432) A[3]:(0.868058204651)\n",
      " state (8)  A[0]:(0.656942367554) A[1]:(-0.000192329287529) A[2]:(0.728851437569) A[3]:(0.592575550079)\n",
      " state (9)  A[0]:(0.657866239548) A[1]:(0.809990525246) A[2]:(0.810011804104) A[3]:(0.00420523202047)\n",
      " state (10)  A[0]:(0.731685161591) A[1]:(0.899993896484) A[2]:(0.000681161764078) A[3]:(0.731019496918)\n",
      " state (11)  A[0]:(0.144763365388) A[1]:(0.883432388306) A[2]:(-0.93148958683) A[3]:(0.806042134762)\n",
      " state (12)  A[0]:(-0.421599507332) A[1]:(0.816969037056) A[2]:(-0.957631886005) A[3]:(0.717861533165)\n",
      " state (13)  A[0]:(0.0055124796927) A[1]:(0.809541761875) A[2]:(0.900075435638) A[3]:(0.730410933495)\n",
      " state (14)  A[0]:(0.811965882778) A[1]:(0.900203406811) A[2]:(0.999999880791) A[3]:(0.810463964939)\n",
      " state (15)  A[0]:(0.98007184267) A[1]:(0.937199771404) A[2]:(1.0) A[3]:(0.878197550774)\n",
      "Episode 745000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6024. Times reached goal: 995.               Steps done: 5134635. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00530028590437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531624317169) A[1]:(0.590347230434) A[2]:(0.590225756168) A[3]:(0.531101703644)\n",
      " state (1)  A[0]:(0.531217098236) A[1]:(-1.78068876266e-05) A[2]:(0.656006455421) A[3]:(0.590265989304)\n",
      " state (2)  A[0]:(0.589770615101) A[1]:(0.72888469696) A[2]:(0.590745806694) A[3]:(0.655841588974)\n",
      " state (3)  A[0]:(0.656202316284) A[1]:(-0.0231428984553) A[2]:(0.533115386963) A[3]:(0.545302271843)\n",
      " state (4)  A[0]:(0.589815855026) A[1]:(0.655915856361) A[2]:(0.000146627426147) A[3]:(0.531173706055)\n",
      " state (5)  A[0]:(-0.0299898143858) A[1]:(0.999896943569) A[2]:(-0.83000421524) A[3]:(0.670770168304)\n",
      " state (6)  A[0]:(-0.000643834355287) A[1]:(0.809982061386) A[2]:(0.000451207131846) A[3]:(0.655797600746)\n",
      " state (7)  A[0]:(0.540451645851) A[1]:(-0.532066464424) A[2]:(0.442056119442) A[3]:(0.867512404919)\n",
      " state (8)  A[0]:(0.655404746532) A[1]:(-0.000209257006645) A[2]:(0.729296684265) A[3]:(0.590710997581)\n",
      " state (9)  A[0]:(0.655695736408) A[1]:(0.809993267059) A[2]:(0.810362458229) A[3]:(8.78572463989e-05)\n",
      " state (10)  A[0]:(0.729699373245) A[1]:(0.900031924248) A[2]:(0.00166630593594) A[3]:(0.728976249695)\n",
      " state (11)  A[0]:(0.14075063169) A[1]:(0.883526206017) A[2]:(-0.931385159492) A[3]:(0.804719746113)\n",
      " state (12)  A[0]:(-0.424480170012) A[1]:(0.817150533199) A[2]:(-0.957571446896) A[3]:(0.716492593288)\n",
      " state (13)  A[0]:(0.00266146031208) A[1]:(0.809727847576) A[2]:(0.900304377079) A[3]:(0.729535102844)\n",
      " state (14)  A[0]:(0.811169326305) A[1]:(0.900297939777) A[2]:(0.999999880791) A[3]:(0.81010299921)\n",
      " state (15)  A[0]:(0.979993581772) A[1]:(0.937244713306) A[2]:(1.0) A[3]:(0.878093481064)\n",
      "Episode 746000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 997.               Steps done: 5140644. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00526853198657.\n",
      " state (0)  A[0]:(0.531498253345) A[1]:(0.590703606606) A[2]:(0.590283036232) A[3]:(0.531592845917)\n",
      " state (1)  A[0]:(0.531400561333) A[1]:(-0.000332280993462) A[2]:(0.656156897545) A[3]:(0.590511202812)\n",
      " state (2)  A[0]:(0.590252995491) A[1]:(0.728808403015) A[2]:(0.590690672398) A[3]:(0.656222760677)\n",
      " state (3)  A[0]:(0.656538724899) A[1]:(-0.0235973149538) A[2]:(0.532962203026) A[3]:(0.545673489571)\n",
      " state (4)  A[0]:(0.590118646622) A[1]:(0.655887365341) A[2]:(-0.00026524066925) A[3]:(0.531611025333)\n",
      " state (5)  A[0]:(-0.0293028336018) A[1]:(0.999896943569) A[2]:(-0.830128192902) A[3]:(0.671361804008)\n",
      " state (6)  A[0]:(0.000192642211914) A[1]:(0.809923529625) A[2]:(0.000242590904236) A[3]:(0.65646135807)\n",
      " state (7)  A[0]:(0.541068553925) A[1]:(-0.532427549362) A[2]:(0.442129701376) A[3]:(0.867781996727)\n",
      " state (8)  A[0]:(0.656111299992) A[1]:(-0.000526115240064) A[2]:(0.729325294495) A[3]:(0.590921998024)\n",
      " state (9)  A[0]:(0.656694293022) A[1]:(0.809797286987) A[2]:(0.810078203678) A[3]:(0.000724583747797)\n",
      " state (10)  A[0]:(0.729799747467) A[1]:(0.899931192398) A[2]:(-0.000437855691416) A[3]:(0.729068517685)\n",
      " state (11)  A[0]:(0.139125868678) A[1]:(0.883421123028) A[2]:(-0.931790232658) A[3]:(0.804540216923)\n",
      " state (12)  A[0]:(-0.426375091076) A[1]:(0.816990375519) A[2]:(-0.957815408707) A[3]:(0.716304659843)\n",
      " state (13)  A[0]:(-0.00013142824173) A[1]:(0.809526562691) A[2]:(0.899889349937) A[3]:(0.729551196098)\n",
      " state (14)  A[0]:(0.809874415398) A[1]:(0.900158405304) A[2]:(0.999999880791) A[3]:(0.81021296978)\n",
      " state (15)  A[0]:(0.979810833931) A[1]:(0.937133252621) A[2]:(1.0) A[3]:(0.878200590611)\n",
      "Episode 747000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6017. Times reached goal: 995.               Steps done: 5146661. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00523692641033.\n",
      " state (0)  A[0]:(0.531334102154) A[1]:(0.590813875198) A[2]:(0.590655088425) A[3]:(0.531513690948)\n",
      " state (1)  A[0]:(0.531287193298) A[1]:(-0.000571012438741) A[2]:(0.656360507011) A[3]:(0.590019226074)\n",
      " state (2)  A[0]:(0.590170860291) A[1]:(0.728890538216) A[2]:(0.590932011604) A[3]:(0.655489802361)\n",
      " state (3)  A[0]:(0.656505405903) A[1]:(-0.023161560297) A[2]:(0.533210217953) A[3]:(0.544656872749)\n",
      " state (4)  A[0]:(0.590351223946) A[1]:(0.656391978264) A[2]:(-2.74181365967e-05) A[3]:(0.53039753437)\n",
      " state (5)  A[0]:(-0.027741689235) A[1]:(0.999897062778) A[2]:(-0.830057442188) A[3]:(0.670105993748)\n",
      " state (6)  A[0]:(0.00140674319118) A[1]:(0.810042619705) A[2]:(0.000575065554585) A[3]:(0.654866695404)\n",
      " state (7)  A[0]:(0.541841745377) A[1]:(-0.532341480255) A[2]:(0.442223876715) A[3]:(0.867170512676)\n",
      " state (8)  A[0]:(0.657045125961) A[1]:(-0.00138211157173) A[2]:(0.729064822197) A[3]:(0.591051280499)\n",
      " state (9)  A[0]:(0.657051742077) A[1]:(0.809824168682) A[2]:(0.81040763855) A[3]:(-0.000825732771773)\n",
      " state (10)  A[0]:(0.730827689171) A[1]:(0.900111079216) A[2]:(0.00277446978725) A[3]:(0.728546619415)\n",
      " state (11)  A[0]:(0.142481341958) A[1]:(0.883788645267) A[2]:(-0.931263566017) A[3]:(0.804460525513)\n",
      " state (12)  A[0]:(-0.424946814775) A[1]:(0.817742109299) A[2]:(-0.957626819611) A[3]:(0.715643048286)\n",
      " state (13)  A[0]:(-0.000161543488503) A[1]:(0.810454368591) A[2]:(0.900076508522) A[3]:(0.72816246748)\n",
      " state (14)  A[0]:(0.809766411781) A[1]:(0.900723934174) A[2]:(0.999999880791) A[3]:(0.808808803558)\n",
      " state (15)  A[0]:(0.979820370674) A[1]:(0.937495112419) A[2]:(1.0) A[3]:(0.877132296562)\n",
      "Episode 748000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6016. Times reached goal: 993.               Steps done: 5152677. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00520551563938.\n",
      " state (0)  A[0]:(0.532104372978) A[1]:(0.590549349785) A[2]:(0.590549349785) A[3]:(0.531084060669)\n",
      " state (1)  A[0]:(0.531789362431) A[1]:(-0.000347569584846) A[2]:(0.65600502491) A[3]:(0.59003919363)\n",
      " state (2)  A[0]:(0.590382754803) A[1]:(0.729064226151) A[2]:(0.59054517746) A[3]:(0.65560901165)\n",
      " state (3)  A[0]:(0.65654027462) A[1]:(-0.0224935766309) A[2]:(0.532797753811) A[3]:(0.544741511345)\n",
      " state (4)  A[0]:(0.59009206295) A[1]:(0.656235933304) A[2]:(-0.000299692153931) A[3]:(0.530481338501)\n",
      " state (5)  A[0]:(-0.0296553112566) A[1]:(0.999897062778) A[2]:(-0.830146849155) A[3]:(0.670280992985)\n",
      " state (6)  A[0]:(-0.00149263325147) A[1]:(0.810130238533) A[2]:(0.000195980072021) A[3]:(0.654187440872)\n",
      " state (7)  A[0]:(0.538778424263) A[1]:(-0.531754136086) A[2]:(0.441912919283) A[3]:(0.866253376007)\n",
      " state (8)  A[0]:(0.652792334557) A[1]:(0.000389620632632) A[2]:(0.729344785213) A[3]:(0.584605097771)\n",
      " state (9)  A[0]:(0.652689576149) A[1]:(0.809965193272) A[2]:(0.810183823109) A[3]:(-0.00966465193778)\n",
      " state (10)  A[0]:(0.726845622063) A[1]:(0.900075733662) A[2]:(0.000693082693033) A[3]:(0.725050091743)\n",
      " state (11)  A[0]:(0.133736133575) A[1]:(0.883797645569) A[2]:(-0.931624770164) A[3]:(0.802366435528)\n",
      " state (12)  A[0]:(-0.431224286556) A[1]:(0.817850470543) A[2]:(-0.957804203033) A[3]:(0.713886022568)\n",
      " state (13)  A[0]:(-0.00636992184445) A[1]:(0.810614347458) A[2]:(0.899897754192) A[3]:(0.727436423302)\n",
      " state (14)  A[0]:(0.807941257954) A[1]:(0.900820612907) A[2]:(0.999999880791) A[3]:(0.808758199215)\n",
      " state (15)  A[0]:(0.979629993439) A[1]:(0.937546014786) A[2]:(1.0) A[3]:(0.877292394638)\n",
      "Episode 749000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6026. Times reached goal: 997.               Steps done: 5158703. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00517424152568.\n",
      "q_values \n",
      "tensor([[ 0.5311,  0.5906,  0.5905,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.6561,  0.0001,  0.5316]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6563,  0.0001,  0.7290,  0.5912]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6567,  0.8100,  0.8100,  0.0003]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0000,  0.8095,  0.9000,  0.7295]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9002,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531124532223) A[1]:(0.590551137924) A[2]:(0.590495824814) A[3]:(0.531460523605)\n",
      " state (1)  A[0]:(0.531291306019) A[1]:(-5.40167093277e-05) A[2]:(0.656147003174) A[3]:(0.590512573719)\n",
      " state (2)  A[0]:(0.589984178543) A[1]:(0.729000687599) A[2]:(0.59095287323) A[3]:(0.656061530113)\n",
      " state (3)  A[0]:(0.656229794025) A[1]:(-0.0225859768689) A[2]:(0.533203780651) A[3]:(0.545398056507)\n",
      " state (4)  A[0]:(0.589975237846) A[1]:(0.656108379364) A[2]:(0.000130176544189) A[3]:(0.53154361248)\n",
      " state (5)  A[0]:(-0.0289600994438) A[1]:(0.999896943569) A[2]:(-0.830109894276) A[3]:(0.671894311905)\n",
      " state (6)  A[0]:(-0.000199273228645) A[1]:(0.809986293316) A[2]:(0.000140786170959) A[3]:(0.656417608261)\n",
      " state (7)  A[0]:(0.54067337513) A[1]:(-0.531883358955) A[2]:(0.441707015038) A[3]:(0.867694616318)\n",
      " state (8)  A[0]:(0.655866503716) A[1]:(0.000129088759422) A[2]:(0.729086399078) A[3]:(0.590736627579)\n",
      " state (9)  A[0]:(0.656242132187) A[1]:(0.810042560101) A[2]:(0.810016989708) A[3]:(-0.000217616558075)\n",
      " state (10)  A[0]:(0.729203164577) A[1]:(0.900014042854) A[2]:(-0.000756144407205) A[3]:(0.728599190712)\n",
      " state (11)  A[0]:(0.137651070952) A[1]:(0.88345181942) A[2]:(-0.931910812855) A[3]:(0.804225862026)\n",
      " state (12)  A[0]:(-0.427249491215) A[1]:(0.816968560219) A[2]:(-0.957905650139) A[3]:(0.716002225876)\n",
      " state (13)  A[0]:(-0.000452354521258) A[1]:(0.809509038925) A[2]:(0.900009274483) A[3]:(0.72938400507)\n",
      " state (14)  A[0]:(0.809960246086) A[1]:(0.900213241577) A[2]:(0.999999880791) A[3]:(0.810110032558)\n",
      " state (15)  A[0]:(0.979829609394) A[1]:(0.937192559242) A[2]:(1.0) A[3]:(0.878114581108)\n",
      "Episode 750000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6012. Times reached goal: 997.               Steps done: 5164715. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00514322730778.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.530050754547) A[1]:(0.590509235859) A[2]:(0.590288043022) A[3]:(0.531071662903)\n",
      " state (1)  A[0]:(0.530372202396) A[1]:(-0.000520124973264) A[2]:(0.656093597412) A[3]:(0.589955329895)\n",
      " state (2)  A[0]:(0.589559972286) A[1]:(0.728927373886) A[2]:(0.590926527977) A[3]:(0.655579507351)\n",
      " state (3)  A[0]:(0.655906736851) A[1]:(-0.0225874055177) A[2]:(0.533175170422) A[3]:(0.544711232185)\n",
      " state (4)  A[0]:(0.589779853821) A[1]:(0.655981361866) A[2]:(0.000110387802124) A[3]:(0.530707001686)\n",
      " state (5)  A[0]:(-0.0287255123258) A[1]:(0.999896883965) A[2]:(-0.830171227455) A[3]:(0.671000003815)\n",
      " state (6)  A[0]:(0.000507175864186) A[1]:(0.809930920601) A[2]:(-0.000209331512451) A[3]:(0.655116796494)\n",
      " state (7)  A[0]:(0.541475772858) A[1]:(-0.531853437424) A[2]:(0.441331386566) A[3]:(0.867196261883)\n",
      " state (8)  A[0]:(0.656590402126) A[1]:(-7.38054513931e-05) A[2]:(0.728926181793) A[3]:(0.590410232544)\n",
      " state (9)  A[0]:(0.656839489937) A[1]:(0.809982419014) A[2]:(0.810163736343) A[3]:(-8.71419906616e-05)\n",
      " state (10)  A[0]:(0.730071783066) A[1]:(0.899958670139) A[2]:(0.00054204458138) A[3]:(0.728852629662)\n",
      " state (11)  A[0]:(0.140158072114) A[1]:(0.883330106735) A[2]:(-0.931691586971) A[3]:(0.804391205311)\n",
      " state (12)  A[0]:(-0.425173074007) A[1]:(0.816705226898) A[2]:(-0.957780182362) A[3]:(0.7158857584)\n",
      " state (13)  A[0]:(0.00208194251172) A[1]:(0.809225082397) A[2]:(0.900422215462) A[3]:(0.728962302208)\n",
      " state (14)  A[0]:(0.810898602009) A[1]:(0.900093793869) A[2]:(0.999999880791) A[3]:(0.809663116932)\n",
      " state (15)  A[0]:(0.979938030243) A[1]:(0.937138557434) A[2]:(1.0) A[3]:(0.877756714821)\n",
      "Episode 751000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 991.               Steps done: 5170723. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00511241943763.\n",
      " state (0)  A[0]:(0.532032370567) A[1]:(0.590755403042) A[2]:(0.590651094913) A[3]:(0.531937837601)\n",
      " state (1)  A[0]:(0.532156586647) A[1]:(-0.000257670879364) A[2]:(0.656327009201) A[3]:(0.590819239616)\n",
      " state (2)  A[0]:(0.59096634388) A[1]:(0.729099750519) A[2]:(0.59094285965) A[3]:(0.656397759914)\n",
      " state (3)  A[0]:(0.657079160213) A[1]:(-0.0222228523344) A[2]:(0.533146500587) A[3]:(0.545750498772)\n",
      " state (4)  A[0]:(0.590955138206) A[1]:(0.656288981438) A[2]:(4.39882278442e-05) A[3]:(0.531942248344)\n",
      " state (5)  A[0]:(-0.0274953655899) A[1]:(0.999897003174) A[2]:(-0.830154836178) A[3]:(0.672335624695)\n",
      " state (6)  A[0]:(0.00130034913309) A[1]:(0.810029745102) A[2]:(0.000301361083984) A[3]:(0.656576871872)\n",
      " state (7)  A[0]:(0.541768431664) A[1]:(-0.531876206398) A[2]:(0.442026972771) A[3]:(0.867731511593)\n",
      " state (8)  A[0]:(0.656596779823) A[1]:(0.000197231769562) A[2]:(0.72917920351) A[3]:(0.59084713459)\n",
      " state (9)  A[0]:(0.657026708126) A[1]:(0.810052216053) A[2]:(0.810036122799) A[3]:(0.000508576573338)\n",
      " state (10)  A[0]:(0.730273067951) A[1]:(0.900013625622) A[2]:(-0.000176191329956) A[3]:(0.72929418087)\n",
      " state (11)  A[0]:(0.140379816294) A[1]:(0.883452653885) A[2]:(-0.931843221188) A[3]:(0.804785132408)\n",
      " state (12)  A[0]:(-0.425412356853) A[1]:(0.816967129707) A[2]:(-0.957926750183) A[3]:(0.716453313828)\n",
      " state (13)  A[0]:(0.00118561030831) A[1]:(0.809526085854) A[2]:(0.900064945221) A[3]:(0.729533314705)\n",
      " state (14)  A[0]:(0.810441374779) A[1]:(0.900253176689) A[2]:(0.999999880791) A[3]:(0.810123085976)\n",
      " state (15)  A[0]:(0.979884624481) A[1]:(0.937213182449) A[2]:(1.0) A[3]:(0.878100574017)\n",
      "Episode 752000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6035. Times reached goal: 999.               Steps done: 5176758. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00508165889961.\n",
      " state (0)  A[0]:(0.531515359879) A[1]:(0.590701818466) A[2]:(0.590521633625) A[3]:(0.531480908394)\n",
      " state (1)  A[0]:(0.531694293022) A[1]:(-0.00018884241581) A[2]:(0.656148612499) A[3]:(0.590702414513)\n",
      " state (2)  A[0]:(0.590512156487) A[1]:(0.72918087244) A[2]:(0.590749263763) A[3]:(0.656341433525)\n",
      " state (3)  A[0]:(0.656652987003) A[1]:(-0.021736074239) A[2]:(0.533021330833) A[3]:(0.54564011097)\n",
      " state (4)  A[0]:(0.590462625027) A[1]:(0.65646648407) A[2]:(1.87158584595e-05) A[3]:(0.531755566597)\n",
      " state (5)  A[0]:(-0.0283932052553) A[1]:(0.999897062778) A[2]:(-0.83019554615) A[3]:(0.672099888325)\n",
      " state (6)  A[0]:(0.000258415937424) A[1]:(0.810106158257) A[2]:(-9.89437103271e-06) A[3]:(0.656183838844)\n",
      " state (7)  A[0]:(0.541125178337) A[1]:(-0.531673908234) A[2]:(0.441782623529) A[3]:(0.86763137579)\n",
      " state (8)  A[0]:(0.655971169472) A[1]:(0.000740036251955) A[2]:(0.72922873497) A[3]:(0.590839207172)\n",
      " state (9)  A[0]:(0.656135678291) A[1]:(0.810235977173) A[2]:(0.81010979414) A[3]:(0.000907391076908)\n",
      " state (10)  A[0]:(0.729213297367) A[1]:(0.899987995625) A[2]:(-0.000656843069009) A[3]:(0.729331612587)\n",
      " state (11)  A[0]:(0.137845978141) A[1]:(0.883219897747) A[2]:(-0.931991755962) A[3]:(0.804610490799)\n",
      " state (12)  A[0]:(-0.426849752665) A[1]:(0.816348910332) A[2]:(-0.958009064198) A[3]:(0.71626996994)\n",
      " state (13)  A[0]:(0.000220388174057) A[1]:(0.80871963501) A[2]:(0.899997591972) A[3]:(0.729522705078)\n",
      " state (14)  A[0]:(0.81011813879) A[1]:(0.899797916412) A[2]:(0.999999880791) A[3]:(0.81017267704)\n",
      " state (15)  A[0]:(0.979829370975) A[1]:(0.936940908432) A[2]:(1.0) A[3]:(0.87813359499)\n",
      "Episode 753000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6006. Times reached goal: 994.               Steps done: 5182764. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00505122992594.\n",
      " state (0)  A[0]:(0.531534433365) A[1]:(0.590273439884) A[2]:(0.590424180031) A[3]:(0.531152486801)\n",
      " state (1)  A[0]:(0.531449377537) A[1]:(0.000232264399529) A[2]:(0.656044363976) A[3]:(0.590603947639)\n",
      " state (2)  A[0]:(0.590161204338) A[1]:(0.729025125504) A[2]:(0.59052491188) A[3]:(0.656342625618)\n",
      " state (3)  A[0]:(0.656316101551) A[1]:(-0.0221942719072) A[2]:(0.532683849335) A[3]:(0.545598089695)\n",
      " state (4)  A[0]:(0.590230941772) A[1]:(0.6559009552) A[2]:(-0.000326752662659) A[3]:(0.531858146191)\n",
      " state (5)  A[0]:(-0.0281810928136) A[1]:(0.999896883965) A[2]:(-0.830242037773) A[3]:(0.672620773315)\n",
      " state (6)  A[0]:(0.00035764274071) A[1]:(0.809948563576) A[2]:(-1.20401382446e-05) A[3]:(0.656602859497)\n",
      " state (7)  A[0]:(0.541181504726) A[1]:(-0.531801819801) A[2]:(0.441774487495) A[3]:(0.867852032185)\n",
      " state (8)  A[0]:(0.656164884567) A[1]:(0.000313326716423) A[2]:(0.729008674622) A[3]:(0.591977953911)\n",
      " state (9)  A[0]:(0.655868828297) A[1]:(0.810111343861) A[2]:(0.809798598289) A[3]:(0.00192045932636)\n",
      " state (10)  A[0]:(0.728254318237) A[1]:(0.899915575981) A[2]:(-0.00225925049745) A[3]:(0.729283332825)\n",
      " state (11)  A[0]:(0.134733274579) A[1]:(0.883069872856) A[2]:(-0.93227660656) A[3]:(0.804305493832)\n",
      " state (12)  A[0]:(-0.42918202281) A[1]:(0.815991044044) A[2]:(-0.958153665066) A[3]:(0.715934932232)\n",
      " state (13)  A[0]:(-0.00181314151268) A[1]:(0.808203995228) A[2]:(0.899893760681) A[3]:(0.729468822479)\n",
      " state (14)  A[0]:(0.809624612331) A[1]:(0.89943754673) A[2]:(0.999999880791) A[3]:(0.810307919979)\n",
      " state (15)  A[0]:(0.979785978794) A[1]:(0.936657965183) A[2]:(1.0) A[3]:(0.878305315971)\n",
      "Episode 754000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6016. Times reached goal: 997.               Steps done: 5188780. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00502093295138.\n",
      "q_values \n",
      "tensor([[ 0.5323,  0.5906,  0.5906,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5918,  0.6562,  0.0002,  0.5321]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6590,  0.0023,  0.7288,  0.5924]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6593,  0.8108,  0.8097,  0.0038]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0050,  0.8065,  0.8997,  0.7294]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8121,  0.8985,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531040787697) A[1]:(0.590517282486) A[2]:(0.590514183044) A[3]:(0.531105458736)\n",
      " state (1)  A[0]:(0.531704425812) A[1]:(-0.00118721963372) A[2]:(0.656302332878) A[3]:(0.590044677258)\n",
      " state (2)  A[0]:(0.590764641762) A[1]:(0.728927135468) A[2]:(0.590904951096) A[3]:(0.655945539474)\n",
      " state (3)  A[0]:(0.656906247139) A[1]:(-0.0218803677708) A[2]:(0.533131837845) A[3]:(0.545222878456)\n",
      " state (4)  A[0]:(0.591085016727) A[1]:(0.656415522099) A[2]:(6.28232955933e-05) A[3]:(0.531577050686)\n",
      " state (5)  A[0]:(-0.026066089049) A[1]:(0.999896883965) A[2]:(-0.83022582531) A[3]:(0.672273397446)\n",
      " state (6)  A[0]:(0.00340157933533) A[1]:(0.809819698334) A[2]:(-0.00044214722584) A[3]:(0.65597474575)\n",
      " state (7)  A[0]:(0.544140934944) A[1]:(-0.530861675739) A[2]:(0.440935820341) A[3]:(0.867754459381)\n",
      " state (8)  A[0]:(0.658928871155) A[1]:(0.00271067838185) A[2]:(0.728749036789) A[3]:(0.591761469841)\n",
      " state (9)  A[0]:(0.659523546696) A[1]:(0.810856878757) A[2]:(0.809701800346) A[3]:(0.00335727864876)\n",
      " state (10)  A[0]:(0.731807231903) A[1]:(0.900025010109) A[2]:(-0.00193333381321) A[3]:(0.730296492577)\n",
      " state (11)  A[0]:(0.142552062869) A[1]:(0.882721066475) A[2]:(-0.932176113129) A[3]:(0.804810285568)\n",
      " state (12)  A[0]:(-0.422590762377) A[1]:(0.814825356007) A[2]:(-0.95807158947) A[3]:(0.716120064259)\n",
      " state (13)  A[0]:(0.00633238721639) A[1]:(0.806626915932) A[2]:(0.90025550127) A[3]:(0.729293584824)\n",
      " state (14)  A[0]:(0.81230866909) A[1]:(0.898542881012) A[2]:(0.999999880791) A[3]:(0.810013711452)\n",
      " state (15)  A[0]:(0.980053544044) A[1]:(0.9361307621) A[2]:(1.0) A[3]:(0.878014981747)\n",
      "Episode 755000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6035. Times reached goal: 999.               Steps done: 5194815. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00499072287162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531416833401) A[1]:(0.590475797653) A[2]:(0.590410590172) A[3]:(0.531697154045)\n",
      " state (1)  A[0]:(0.531569123268) A[1]:(0.000274792313576) A[2]:(0.656128287315) A[3]:(0.590847671032)\n",
      " state (2)  A[0]:(0.590441584587) A[1]:(0.729000329971) A[2]:(0.590821266174) A[3]:(0.656523644924)\n",
      " state (3)  A[0]:(0.656555593014) A[1]:(-0.0221703518182) A[2]:(0.53303027153) A[3]:(0.545860230923)\n",
      " state (4)  A[0]:(0.590551018715) A[1]:(0.65617787838) A[2]:(-7.86781311035e-05) A[3]:(0.532512962818)\n",
      " state (5)  A[0]:(-0.0272388420999) A[1]:(0.999896943569) A[2]:(-0.830299079418) A[3]:(0.673881113529)\n",
      " state (6)  A[0]:(0.00147451355588) A[1]:(0.809959709644) A[2]:(-0.000196576118469) A[3]:(0.657817661762)\n",
      " state (7)  A[0]:(0.541929841042) A[1]:(-0.532024741173) A[2]:(0.441600322723) A[3]:(0.868279337883)\n",
      " state (8)  A[0]:(0.656747341156) A[1]:(-0.000299081206322) A[2]:(0.728888750076) A[3]:(0.592978358269)\n",
      " state (9)  A[0]:(0.656832814217) A[1]:(0.80995285511) A[2]:(0.810006082058) A[3]:(0.00340129621327)\n",
      " state (10)  A[0]:(0.730233550072) A[1]:(0.899934649467) A[2]:(0.000128984451294) A[3]:(0.730614185333)\n",
      " state (11)  A[0]:(0.140603452921) A[1]:(0.883285820484) A[2]:(-0.931896924973) A[3]:(0.805678129196)\n",
      " state (12)  A[0]:(-0.425243079662) A[1]:(0.816572070122) A[2]:(-0.958073556423) A[3]:(0.717407405376)\n",
      " state (13)  A[0]:(0.00126458634622) A[1]:(0.809006333351) A[2]:(0.899824738503) A[3]:(0.73030269146)\n",
      " state (14)  A[0]:(0.810369253159) A[1]:(0.899995207787) A[2]:(0.999999880791) A[3]:(0.810657024384)\n",
      " state (15)  A[0]:(0.979863226414) A[1]:(0.937071204185) A[2]:(1.0) A[3]:(0.878437876701)\n",
      "Episode 756000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6006. Times reached goal: 994.               Steps done: 5200821. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00496083842289.\n",
      " state (0)  A[0]:(0.530997514725) A[1]:(0.590537190437) A[2]:(0.590648710728) A[3]:(0.532229304314)\n",
      " state (1)  A[0]:(0.531019449234) A[1]:(-0.000341072678566) A[2]:(0.656079649925) A[3]:(0.590977907181)\n",
      " state (2)  A[0]:(0.589860439301) A[1]:(0.72894269228) A[2]:(0.590692341328) A[3]:(0.65638256073)\n",
      " state (3)  A[0]:(0.655973732471) A[1]:(-0.0221420247108) A[2]:(0.532892763615) A[3]:(0.545326888561)\n",
      " state (4)  A[0]:(0.589766383171) A[1]:(0.655999839306) A[2]:(-6.63995742798e-05) A[3]:(0.531375169754)\n",
      " state (5)  A[0]:(-0.0290506519377) A[1]:(0.999896943569) A[2]:(-0.830274879932) A[3]:(0.672195672989)\n",
      " state (6)  A[0]:(-0.000932052440476) A[1]:(0.810036838055) A[2]:(0.000270128250122) A[3]:(0.655244231224)\n",
      " state (7)  A[0]:(0.539973735809) A[1]:(-0.531540632248) A[2]:(0.442069083452) A[3]:(0.866887331009)\n",
      " state (8)  A[0]:(0.65543615818) A[1]:(-0.000126734375954) A[2]:(0.728955924511) A[3]:(0.589286983013)\n",
      " state (9)  A[0]:(0.655195116997) A[1]:(0.810016334057) A[2]:(0.80990254879) A[3]:(-0.00426516309381)\n",
      " state (10)  A[0]:(0.728391587734) A[1]:(0.899993062019) A[2]:(-0.00123250426259) A[3]:(0.726306080818)\n",
      " state (11)  A[0]:(0.137080147862) A[1]:(0.883370101452) A[2]:(-0.932169854641) A[3]:(0.802492856979)\n",
      " state (12)  A[0]:(-0.426202893257) A[1]:(0.816689729691) A[2]:(-0.95817387104) A[3]:(0.713847994804)\n",
      " state (13)  A[0]:(0.002597236773) A[1]:(0.809080600739) A[2]:(0.899917662144) A[3]:(0.727737367153)\n",
      " state (14)  A[0]:(0.811342120171) A[1]:(0.900004923344) A[2]:(0.999999880791) A[3]:(0.809183597565)\n",
      " state (15)  A[0]:(0.980003833771) A[1]:(0.937048971653) A[2]:(1.0) A[3]:(0.877618610859)\n",
      "Episode 757000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6019. Times reached goal: 995.               Steps done: 5206840. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00493106881792.\n",
      " state (0)  A[0]:(0.530988991261) A[1]:(0.590562105179) A[2]:(0.590654313564) A[3]:(0.531789839268)\n",
      " state (1)  A[0]:(0.531103253365) A[1]:(0.000472947926028) A[2]:(0.656014800072) A[3]:(0.590570390224)\n",
      " state (2)  A[0]:(0.590054869652) A[1]:(0.729180276394) A[2]:(0.590666174889) A[3]:(0.656134963036)\n",
      " state (3)  A[0]:(0.656163573265) A[1]:(-0.0216567777097) A[2]:(0.532878160477) A[3]:(0.545122742653)\n",
      " state (4)  A[0]:(0.590072751045) A[1]:(0.656213641167) A[2]:(-8.28504562378e-05) A[3]:(0.531437397003)\n",
      " state (5)  A[0]:(-0.0282732434571) A[1]:(0.99989682436) A[2]:(-0.830208599567) A[3]:(0.672664880753)\n",
      " state (6)  A[0]:(5.37633895874e-05) A[1]:(0.809749245644) A[2]:(0.000605344714131) A[3]:(0.656044125557)\n",
      " state (7)  A[0]:(0.540850043297) A[1]:(-0.531610012054) A[2]:(0.442285627127) A[3]:(0.867330908775)\n",
      " state (8)  A[0]:(0.655902981758) A[1]:(0.000755712273531) A[2]:(0.7292060256) A[3]:(0.589845180511)\n",
      " state (9)  A[0]:(0.656522154808) A[1]:(0.810139358044) A[2]:(0.80983954668) A[3]:(-0.000307947397232)\n",
      " state (10)  A[0]:(0.729864954948) A[1]:(0.900006353855) A[2]:(-0.00125694205053) A[3]:(0.728699564934)\n",
      " state (11)  A[0]:(0.139806032181) A[1]:(0.883355498314) A[2]:(-0.932154178619) A[3]:(0.803988218307)\n",
      " state (12)  A[0]:(-0.424872070551) A[1]:(0.816647291183) A[2]:(-0.958171963692) A[3]:(0.715181469917)\n",
      " state (13)  A[0]:(0.00317483069375) A[1]:(0.809073805809) A[2]:(0.900102198124) A[3]:(0.728465020657)\n",
      " state (14)  A[0]:(0.811309576035) A[1]:(0.900037765503) A[2]:(0.999999880791) A[3]:(0.80948382616)\n",
      " state (15)  A[0]:(0.979983210564) A[1]:(0.937074005604) A[2]:(1.0) A[3]:(0.877729713917)\n",
      "Episode 758000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6019. Times reached goal: 1000.               Steps done: 5212859. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00490147785804.\n",
      " state (0)  A[0]:(0.531117498875) A[1]:(0.590451359749) A[2]:(0.590457975864) A[3]:(0.532874286175)\n",
      " state (1)  A[0]:(0.53098654747) A[1]:(-0.000673502567224) A[2]:(0.656117558479) A[3]:(0.591999530792)\n",
      " state (2)  A[0]:(0.589720129967) A[1]:(0.728922426701) A[2]:(0.590602755547) A[3]:(0.657587885857)\n",
      " state (3)  A[0]:(0.65586745739) A[1]:(-0.022121604532) A[2]:(0.532698869705) A[3]:(0.546914935112)\n",
      " state (4)  A[0]:(0.589748442173) A[1]:(0.655967473984) A[2]:(-0.000519752444234) A[3]:(0.533325910568)\n",
      " state (5)  A[0]:(-0.0287963096052) A[1]:(0.99989682436) A[2]:(-0.830490291119) A[3]:(0.67419064045)\n",
      " state (6)  A[0]:(-0.000491425336804) A[1]:(0.809817910194) A[2]:(-0.000653505208902) A[3]:(0.657339990139)\n",
      " state (7)  A[0]:(0.540617346764) A[1]:(-0.531946003437) A[2]:(0.441243171692) A[3]:(0.867985129356)\n",
      " state (8)  A[0]:(0.656315326691) A[1]:(-0.000492542923894) A[2]:(0.728703856468) A[3]:(0.592994689941)\n",
      " state (9)  A[0]:(0.657006144524) A[1]:(0.809926092625) A[2]:(0.810149371624) A[3]:(0.00390140223317)\n",
      " state (10)  A[0]:(0.730933070183) A[1]:(0.900002241135) A[2]:(0.00159191945568) A[3]:(0.731169641018)\n",
      " state (11)  A[0]:(0.142429560423) A[1]:(0.883446931839) A[2]:(-0.931729614735) A[3]:(0.80617660284)\n",
      " state (12)  A[0]:(-0.424582242966) A[1]:(0.816882550716) A[2]:(-0.958067595959) A[3]:(0.717676877975)\n",
      " state (13)  A[0]:(0.00126072694547) A[1]:(0.80937319994) A[2]:(0.900034427643) A[3]:(0.730113267899)\n",
      " state (14)  A[0]:(0.810363948345) A[1]:(0.900223374367) A[2]:(0.999999880791) A[3]:(0.810279965401)\n",
      " state (15)  A[0]:(0.97987651825) A[1]:(0.937188446522) A[2]:(1.0) A[3]:(0.878101408482)\n",
      "Episode 759000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6020. Times reached goal: 998.               Steps done: 5218879. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00487205959913.\n",
      "q_values \n",
      "tensor([[ 0.5324,  0.5904,  0.5905,  0.5311]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5325,  0.0003,  0.6562,  0.5905]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5913,  0.7289,  0.5909,  0.6563]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0015,  0.8100,  0.0002,  0.6566]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7297,  0.9000, -0.0003,  0.7296]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9004,  1.0000,  0.8102]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532498955727) A[1]:(0.590462088585) A[2]:(0.590510249138) A[3]:(0.531011223793)\n",
      " state (1)  A[0]:(0.532548904419) A[1]:(0.00021930038929) A[2]:(0.656185209751) A[3]:(0.59025156498)\n",
      " state (2)  A[0]:(0.59130948782) A[1]:(0.72896027565) A[2]:(0.59096711874) A[3]:(0.656070590019)\n",
      " state (3)  A[0]:(0.657283902168) A[1]:(-0.0219984762371) A[2]:(0.533236265182) A[3]:(0.545035660267)\n",
      " state (4)  A[0]:(0.591344594955) A[1]:(0.656161189079) A[2]:(0.000321745872498) A[3]:(0.531563282013)\n",
      " state (5)  A[0]:(-0.0265373103321) A[1]:(0.999896943569) A[2]:(-0.830252170563) A[3]:(0.673217535019)\n",
      " state (6)  A[0]:(0.00132997252513) A[1]:(0.809970915318) A[2]:(0.000189185142517) A[3]:(0.656346917152)\n",
      " state (7)  A[0]:(0.541528046131) A[1]:(-0.531926572323) A[2]:(0.441992998123) A[3]:(0.86751806736)\n",
      " state (8)  A[0]:(0.656571865082) A[1]:(-0.000673815491609) A[2]:(0.728902101517) A[3]:(0.591534137726)\n",
      " state (9)  A[0]:(0.656344175339) A[1]:(0.809885382652) A[2]:(0.809988200665) A[3]:(0.000622779072728)\n",
      " state (10)  A[0]:(0.72952580452) A[1]:(0.900018692017) A[2]:(-0.000167727470398) A[3]:(0.72934782505)\n",
      " state (11)  A[0]:(0.13860283792) A[1]:(0.883530020714) A[2]:(-0.932067096233) A[3]:(0.804848253727)\n",
      " state (12)  A[0]:(-0.426991373301) A[1]:(0.817112863064) A[2]:(-0.958209037781) A[3]:(0.716418385506)\n",
      " state (13)  A[0]:(-0.000447243422968) A[1]:(0.809704601765) A[2]:(0.900088191032) A[3]:(0.72949719429)\n",
      " state (14)  A[0]:(0.810001373291) A[1]:(0.900434911251) A[2]:(0.999999880791) A[3]:(0.81011557579)\n",
      " state (15)  A[0]:(0.979846596718) A[1]:(0.937311053276) A[2]:(1.0) A[3]:(0.878100752831)\n",
      "Episode 760000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 995.               Steps done: 5224886. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0048428808632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531629562378) A[1]:(0.590946614742) A[2]:(0.590390622616) A[3]:(0.53186237812)\n",
      " state (1)  A[0]:(0.531717896461) A[1]:(-5.28693199158e-05) A[2]:(0.656381130219) A[3]:(0.590637266636)\n",
      " state (2)  A[0]:(0.590389251709) A[1]:(0.729175984859) A[2]:(0.590614318848) A[3]:(0.656440734863)\n",
      " state (3)  A[0]:(0.656413316727) A[1]:(-0.0214245319366) A[2]:(0.532789349556) A[3]:(0.545331656933)\n",
      " state (4)  A[0]:(0.5903403759) A[1]:(0.656492948532) A[2]:(-0.00012993812561) A[3]:(0.531834125519)\n",
      " state (5)  A[0]:(-0.027934493497) A[1]:(0.999897062778) A[2]:(-0.830195844173) A[3]:(0.673574030399)\n",
      " state (6)  A[0]:(-0.000107571482658) A[1]:(0.810030639172) A[2]:(0.000528931559529) A[3]:(0.656330168247)\n",
      " state (7)  A[0]:(0.540490269661) A[1]:(-0.531777262688) A[2]:(0.442354738712) A[3]:(0.867392301559)\n",
      " state (8)  A[0]:(0.655747652054) A[1]:(-4.57167625427e-05) A[2]:(0.729152500629) A[3]:(0.5905854702)\n",
      " state (9)  A[0]:(0.655902504921) A[1]:(0.810007929802) A[2]:(0.809939265251) A[3]:(-8.76486301422e-05)\n",
      " state (10)  A[0]:(0.728968262672) A[1]:(0.900012254715) A[2]:(-0.00102651084308) A[3]:(0.72897708416)\n",
      " state (11)  A[0]:(0.136953756213) A[1]:(0.883439898491) A[2]:(-0.932243466377) A[3]:(0.804407656193)\n",
      " state (12)  A[0]:(-0.428193509579) A[1]:(0.816871702671) A[2]:(-0.958306610584) A[3]:(0.715844988823)\n",
      " state (13)  A[0]:(-0.00179636280518) A[1]:(0.809384822845) A[2]:(0.900002658367) A[3]:(0.729095578194)\n",
      " state (14)  A[0]:(0.809398412704) A[1]:(0.900245606899) A[2]:(0.999999880791) A[3]:(0.809914708138)\n",
      " state (15)  A[0]:(0.979758262634) A[1]:(0.937181055546) A[2]:(1.0) A[3]:(0.878005445004)\n",
      "Episode 761000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6016. Times reached goal: 996.               Steps done: 5230902. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00481383355384.\n",
      " state (0)  A[0]:(0.531051814556) A[1]:(0.589982926846) A[2]:(0.590451002121) A[3]:(0.531209111214)\n",
      " state (1)  A[0]:(0.530975401402) A[1]:(0.000128194689751) A[2]:(0.655668497086) A[3]:(0.590243935585)\n",
      " state (2)  A[0]:(0.589872360229) A[1]:(0.728977560997) A[2]:(0.590277314186) A[3]:(0.655896484852)\n",
      " state (3)  A[0]:(0.656014919281) A[1]:(-0.0214425381273) A[2]:(0.532653093338) A[3]:(0.544707655907)\n",
      " state (4)  A[0]:(0.590061426163) A[1]:(0.656483530998) A[2]:(-9.77516174316e-06) A[3]:(0.531279981136)\n",
      " state (5)  A[0]:(-0.0278356429189) A[1]:(0.999897062778) A[2]:(-0.830127775669) A[3]:(0.673205971718)\n",
      " state (6)  A[0]:(0.000278696417809) A[1]:(0.810100078583) A[2]:(0.000792383973021) A[3]:(0.655648589134)\n",
      " state (7)  A[0]:(0.54092669487) A[1]:(-0.531563639641) A[2]:(0.442501813173) A[3]:(0.867110311985)\n",
      " state (8)  A[0]:(0.656075417995) A[1]:(-5.51342964172e-07) A[2]:(0.72917330265) A[3]:(0.59045279026)\n",
      " state (9)  A[0]:(0.656028449535) A[1]:(0.810045003891) A[2]:(0.810102939606) A[3]:(-0.000238984823227)\n",
      " state (10)  A[0]:(0.729450404644) A[1]:(0.899996101856) A[2]:(0.000253319740295) A[3]:(0.729075551033)\n",
      " state (11)  A[0]:(0.138920500875) A[1]:(0.883358061314) A[2]:(-0.932029902935) A[3]:(0.804634213448)\n",
      " state (12)  A[0]:(-0.426340132952) A[1]:(0.816656887531) A[2]:(-0.958226084709) A[3]:(0.716137230396)\n",
      " state (13)  A[0]:(0.000318855047226) A[1]:(0.809120297432) A[2]:(0.900012373924) A[3]:(0.729299902916)\n",
      " state (14)  A[0]:(0.810006916523) A[1]:(0.900157094002) A[2]:(0.999999880791) A[3]:(0.810033977032)\n",
      " state (15)  A[0]:(0.979807674885) A[1]:(0.937199175358) A[2]:(1.0) A[3]:(0.878056406975)\n",
      "Episode 762000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 997.               Steps done: 5236907. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00478501310335.\n",
      " state (0)  A[0]:(0.530873954296) A[1]:(0.590469837189) A[2]:(0.590512514114) A[3]:(0.531311511993)\n",
      " state (1)  A[0]:(0.530679345131) A[1]:(0.000389263004763) A[2]:(0.656252384186) A[3]:(0.590887784958)\n",
      " state (2)  A[0]:(0.58971208334) A[1]:(0.729063391685) A[2]:(0.590940833092) A[3]:(0.656473398209)\n",
      " state (3)  A[0]:(0.655893921852) A[1]:(-0.02147375606) A[2]:(0.533229827881) A[3]:(0.545223236084)\n",
      " state (4)  A[0]:(0.589913964272) A[1]:(0.656409621239) A[2]:(0.000495552958455) A[3]:(0.531765699387)\n",
      " state (5)  A[0]:(-0.0283734314144) A[1]:(0.999897062778) A[2]:(-0.830170154572) A[3]:(0.673884987831)\n",
      " state (6)  A[0]:(-0.000975340313744) A[1]:(0.810177147388) A[2]:(0.000713825109415) A[3]:(0.656445145607)\n",
      " state (7)  A[0]:(0.539801239967) A[1]:(-0.531388521194) A[2]:(0.442733108997) A[3]:(0.867314815521)\n",
      " state (8)  A[0]:(0.655134856701) A[1]:(0.00072400260251) A[2]:(0.729340553284) A[3]:(0.58982861042)\n",
      " state (9)  A[0]:(0.65543204546) A[1]:(0.810147464275) A[2]:(0.809834301472) A[3]:(-0.00121632160153)\n",
      " state (10)  A[0]:(0.728533744812) A[1]:(0.899974882603) A[2]:(-0.00190591579303) A[3]:(0.728116631508)\n",
      " state (11)  A[0]:(0.136700734496) A[1]:(0.883248150349) A[2]:(-0.932401418686) A[3]:(0.803595662117)\n",
      " state (12)  A[0]:(-0.426556557417) A[1]:(0.816367864609) A[2]:(-0.95837277174) A[3]:(0.715067923069)\n",
      " state (13)  A[0]:(0.00267275539227) A[1]:(0.80872297287) A[2]:(0.90001732111) A[3]:(0.728911399841)\n",
      " state (14)  A[0]:(0.811511576176) A[1]:(0.899911940098) A[2]:(0.999999880791) A[3]:(0.810153663158)\n",
      " state (15)  A[0]:(0.980025291443) A[1]:(0.937030553818) A[2]:(1.0) A[3]:(0.878307044506)\n",
      "Episode 763000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 991.               Steps done: 5242916. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00475634617566.\n",
      " state (0)  A[0]:(0.531680345535) A[1]:(0.590536832809) A[2]:(0.590488433838) A[3]:(0.531754255295)\n",
      " state (1)  A[0]:(0.531672477722) A[1]:(1.77770853043e-05) A[2]:(0.656206607819) A[3]:(0.590659976006)\n",
      " state (2)  A[0]:(0.590515673161) A[1]:(0.728939294815) A[2]:(0.590655267239) A[3]:(0.656316399574)\n",
      " state (3)  A[0]:(0.65652269125) A[1]:(-0.0217056591064) A[2]:(0.532715976238) A[3]:(0.545008599758)\n",
      " state (4)  A[0]:(0.590637207031) A[1]:(0.656166493893) A[2]:(-0.000463604897959) A[3]:(0.531563162804)\n",
      " state (5)  A[0]:(-0.0272067673504) A[1]:(0.999896943569) A[2]:(-0.830571055412) A[3]:(0.673764944077)\n",
      " state (6)  A[0]:(-4.11570072174e-05) A[1]:(0.80997133255) A[2]:(-0.000633716525044) A[3]:(0.6561409235)\n",
      " state (7)  A[0]:(0.540549576283) A[1]:(-0.531587123871) A[2]:(0.441418945789) A[3]:(0.867275297642)\n",
      " state (8)  A[0]:(0.656501412392) A[1]:(-0.00063715869328) A[2]:(0.728579640388) A[3]:(0.591512560844)\n",
      " state (9)  A[0]:(0.65657222271) A[1]:(0.809871435165) A[2]:(0.809952199459) A[3]:(-0.000241547822952)\n",
      " state (10)  A[0]:(0.729679346085) A[1]:(0.899968385696) A[2]:(8.08238983154e-05) A[3]:(0.728486895561)\n",
      " state (11)  A[0]:(0.139087215066) A[1]:(0.883382439613) A[2]:(-0.932097375393) A[3]:(0.804111659527)\n",
      " state (12)  A[0]:(-0.426201164722) A[1]:(0.816730260849) A[2]:(-0.958295166492) A[3]:(0.715571522713)\n",
      " state (13)  A[0]:(0.000692948582582) A[1]:(0.809188008308) A[2]:(0.89997959137) A[3]:(0.729033231735)\n",
      " state (14)  A[0]:(0.810197353363) A[1]:(0.900192379951) A[2]:(0.999999880791) A[3]:(0.810053110123)\n",
      " state (15)  A[0]:(0.97983700037) A[1]:(0.937204658985) A[2]:(1.0) A[3]:(0.878183364868)\n",
      "Episode 764000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6000. Times reached goal: 994.               Steps done: 5248916. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00472789354187.\n",
      "q_values \n",
      "tensor([[ 0.5318,  0.5905,  0.5903,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5910,  0.6560,  0.0001,  0.5315]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6569,  0.0002,  0.7290,  0.5912]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6573,  0.8100,  0.8100,  0.0018]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7304,  0.9000, -0.0001,  0.7299]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.9001,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531666517258) A[1]:(0.590468883514) A[2]:(0.590329289436) A[3]:(0.531304955482)\n",
      " state (1)  A[0]:(0.531975746155) A[1]:(-0.000136211514473) A[2]:(0.656175076962) A[3]:(0.590273857117)\n",
      " state (2)  A[0]:(0.590753436089) A[1]:(0.728978991508) A[2]:(0.59078335762) A[3]:(0.656013131142)\n",
      " state (3)  A[0]:(0.656677484512) A[1]:(-0.0215724166483) A[2]:(0.532904744148) A[3]:(0.544721484184)\n",
      " state (4)  A[0]:(0.591002345085) A[1]:(0.655954360962) A[2]:(2.63452529907e-05) A[3]:(0.53145301342)\n",
      " state (5)  A[0]:(-0.0256672147661) A[1]:(0.999896883965) A[2]:(-0.830361306667) A[3]:(0.673970401287)\n",
      " state (6)  A[0]:(0.00183595507406) A[1]:(0.809990882874) A[2]:(-9.81092453003e-05) A[3]:(0.656357884407)\n",
      " state (7)  A[0]:(0.541852176189) A[1]:(-0.53137499094) A[2]:(0.441800683737) A[3]:(0.867357969284)\n",
      " state (8)  A[0]:(0.657078623772) A[1]:(0.000232145190239) A[2]:(0.728925764561) A[3]:(0.59133207798)\n",
      " state (9)  A[0]:(0.657445013523) A[1]:(0.809996306896) A[2]:(0.809979617596) A[3]:(0.00200429279357)\n",
      " state (10)  A[0]:(0.73043513298) A[1]:(0.899973034859) A[2]:(-0.000364541978342) A[3]:(0.729884386063)\n",
      " state (11)  A[0]:(0.140133202076) A[1]:(0.883340895176) A[2]:(-0.932219028473) A[3]:(0.80490732193)\n",
      " state (12)  A[0]:(-0.425793498755) A[1]:(0.816612124443) A[2]:(-0.95837944746) A[3]:(0.716220498085)\n",
      " state (13)  A[0]:(0.00101281667594) A[1]:(0.809041440487) A[2]:(0.900000393391) A[3]:(0.729337871075)\n",
      " state (14)  A[0]:(0.810379266739) A[1]:(0.900091290474) A[2]:(0.999999880791) A[3]:(0.810114443302)\n",
      " state (15)  A[0]:(0.979870200157) A[1]:(0.937096118927) A[2]:(1.0) A[3]:(0.878162801266)\n",
      "Episode 765000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 994.               Steps done: 5254924. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00469957351601.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531472623348) A[1]:(0.590230345726) A[2]:(0.59040081501) A[3]:(0.531582117081)\n",
      " state (1)  A[0]:(0.531341433525) A[1]:(-8.36700201035e-05) A[2]:(0.655978798866) A[3]:(0.590705692768)\n",
      " state (2)  A[0]:(0.590126335621) A[1]:(0.728918194771) A[2]:(0.590499639511) A[3]:(0.656139969826)\n",
      " state (3)  A[0]:(0.656104326248) A[1]:(-0.0215129889548) A[2]:(0.532684326172) A[3]:(0.54472386837)\n",
      " state (4)  A[0]:(0.590189635754) A[1]:(0.656089186668) A[2]:(-0.000160694122314) A[3]:(0.531393766403)\n",
      " state (5)  A[0]:(-0.0275383833796) A[1]:(0.999896883965) A[2]:(-0.830378234386) A[3]:(0.674064099789)\n",
      " state (6)  A[0]:(-0.000109225511551) A[1]:(0.809958934784) A[2]:(-0.000122904777527) A[3]:(0.656222105026)\n",
      " state (7)  A[0]:(0.540370225906) A[1]:(-0.531269550323) A[2]:(0.441781014204) A[3]:(0.867148995399)\n",
      " state (8)  A[0]:(0.655677556992) A[1]:(0.000551953853574) A[2]:(0.728963375092) A[3]:(0.590313494205)\n",
      " state (9)  A[0]:(0.655981838703) A[1]:(0.810080468655) A[2]:(0.80997043848) A[3]:(0.000355362863047)\n",
      " state (10)  A[0]:(0.729196071625) A[1]:(0.899981021881) A[2]:(-0.000503778399434) A[3]:(0.728991866112)\n",
      " state (11)  A[0]:(0.137777760625) A[1]:(0.883307218552) A[2]:(-0.932251751423) A[3]:(0.804242134094)\n",
      " state (12)  A[0]:(-0.427164822817) A[1]:(0.816489219666) A[2]:(-0.95839715004) A[3]:(0.715511798859)\n",
      " state (13)  A[0]:(4.66406345367e-05) A[1]:(0.80884885788) A[2]:(0.900010883808) A[3]:(0.728908479214)\n",
      " state (14)  A[0]:(0.810182988644) A[1]:(0.899980247021) A[2]:(0.999999880791) A[3]:(0.809954941273)\n",
      " state (15)  A[0]:(0.979850232601) A[1]:(0.937034130096) A[2]:(1.0) A[3]:(0.878118276596)\n",
      "Episode 766000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6018. Times reached goal: 994.               Steps done: 5260942. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00467137641277.\n",
      " state (0)  A[0]:(0.531270205975) A[1]:(0.590502679348) A[2]:(0.590430498123) A[3]:(0.531333088875)\n",
      " state (1)  A[0]:(0.531332850456) A[1]:(-6.36279582977e-06) A[2]:(0.656096458435) A[3]:(0.590245902538)\n",
      " state (2)  A[0]:(0.590230107307) A[1]:(0.729068994522) A[2]:(0.590721607208) A[3]:(0.655920922756)\n",
      " state (3)  A[0]:(0.656248092651) A[1]:(-0.0213303845376) A[2]:(0.532982051373) A[3]:(0.544614851475)\n",
      " state (4)  A[0]:(0.590365231037) A[1]:(0.656110346317) A[2]:(0.000216126441956) A[3]:(0.5314219594)\n",
      " state (5)  A[0]:(-0.0273885168135) A[1]:(0.999896943569) A[2]:(-0.830283820629) A[3]:(0.674093365669)\n",
      " state (6)  A[0]:(0.000176191329956) A[1]:(0.81000328064) A[2]:(0.000397443742258) A[3]:(0.656242907047)\n",
      " state (7)  A[0]:(0.540650486946) A[1]:(-0.531502008438) A[2]:(0.442196249962) A[3]:(0.867179274559)\n",
      " state (8)  A[0]:(0.655854225159) A[1]:(0.000122383236885) A[2]:(0.729059636593) A[3]:(0.59050154686)\n",
      " state (9)  A[0]:(0.655986785889) A[1]:(0.810015559196) A[2]:(0.810010313988) A[3]:(0.000331103801727)\n",
      " state (10)  A[0]:(0.729261517525) A[1]:(0.9000005126) A[2]:(-0.000315308570862) A[3]:(0.728937149048)\n",
      " state (11)  A[0]:(0.13812212646) A[1]:(0.883414745331) A[2]:(-0.932234466076) A[3]:(0.804232180119)\n",
      " state (12)  A[0]:(-0.426909595728) A[1]:(0.816778957844) A[2]:(-0.958406567574) A[3]:(0.715501189232)\n",
      " state (13)  A[0]:(9.75430011749e-05) A[1]:(0.809255123138) A[2]:(0.899994671345) A[3]:(0.728911399841)\n",
      " state (14)  A[0]:(0.810074925423) A[1]:(0.900268852711) A[2]:(0.999999880791) A[3]:(0.810002923012)\n",
      " state (15)  A[0]:(0.979823112488) A[1]:(0.937264919281) A[2]:(1.0) A[3]:(0.878156721592)\n",
      "Episode 767000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6024. Times reached goal: 997.               Steps done: 5266966. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00464332063012.\n",
      " state (0)  A[0]:(0.53159570694) A[1]:(0.590667963028) A[2]:(0.590563595295) A[3]:(0.53164768219)\n",
      " state (1)  A[0]:(0.531566381454) A[1]:(-0.000176444649696) A[2]:(0.656207919121) A[3]:(0.590592741966)\n",
      " state (2)  A[0]:(0.590399861336) A[1]:(0.729060173035) A[2]:(0.590694665909) A[3]:(0.65629029274)\n",
      " state (3)  A[0]:(0.65638178587) A[1]:(-0.0213111694902) A[2]:(0.532917022705) A[3]:(0.545037984848)\n",
      " state (4)  A[0]:(0.590395689011) A[1]:(0.65635740757) A[2]:(-6.67572021484e-06) A[3]:(0.531811416149)\n",
      " state (5)  A[0]:(-0.0276548657566) A[1]:(0.999897003174) A[2]:(-0.830386161804) A[3]:(0.674272537231)\n",
      " state (6)  A[0]:(0.000115990638733) A[1]:(0.810045301914) A[2]:(0.00032365322113) A[3]:(0.656199336052)\n",
      " state (7)  A[0]:(0.540780901909) A[1]:(-0.53150510788) A[2]:(0.442413628101) A[3]:(0.867177307606)\n",
      " state (8)  A[0]:(0.656263113022) A[1]:(0.000401243538363) A[2]:(0.729190528393) A[3]:(0.590498805046)\n",
      " state (9)  A[0]:(0.656698107719) A[1]:(0.810142397881) A[2]:(0.8100258708) A[3]:(0.000545799674001)\n",
      " state (10)  A[0]:(0.729715526104) A[1]:(0.899999082088) A[2]:(-0.000445365876658) A[3]:(0.729153990746)\n",
      " state (11)  A[0]:(0.138530656695) A[1]:(0.883278012276) A[2]:(-0.932292759418) A[3]:(0.804377555847)\n",
      " state (12)  A[0]:(-0.42666810751) A[1]:(0.816375434399) A[2]:(-0.958452105522) A[3]:(0.715720057487)\n",
      " state (13)  A[0]:(0.000672623398714) A[1]:(0.808703303337) A[2]:(0.900004029274) A[3]:(0.729167342186)\n",
      " state (14)  A[0]:(0.810390710831) A[1]:(0.899941027164) A[2]:(0.999999880791) A[3]:(0.810202240944)\n",
      " state (15)  A[0]:(0.979865193367) A[1]:(0.937044322491) A[2]:(1.0) A[3]:(0.878291785717)\n",
      "Episode 768000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6020. Times reached goal: 996.               Steps done: 5272986. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00461545180924.\n",
      " state (0)  A[0]:(0.53095716238) A[1]:(0.590235710144) A[2]:(0.5903673172) A[3]:(0.532051324844)\n",
      " state (1)  A[0]:(0.530862569809) A[1]:(5.40316104889e-05) A[2]:(0.656112670898) A[3]:(0.590915262699)\n",
      " state (2)  A[0]:(0.589761435986) A[1]:(0.728928506374) A[2]:(0.590510189533) A[3]:(0.656291723251)\n",
      " state (3)  A[0]:(0.655828893185) A[1]:(-0.0216884873807) A[2]:(0.532693624496) A[3]:(0.544956922531)\n",
      " state (4)  A[0]:(0.589950799942) A[1]:(0.65572810173) A[2]:(-0.000242948532104) A[3]:(0.531728029251)\n",
      " state (5)  A[0]:(-0.0278848521411) A[1]:(0.99989682436) A[2]:(-0.830529391766) A[3]:(0.674383282661)\n",
      " state (6)  A[0]:(-0.000303447246552) A[1]:(0.809799313545) A[2]:(-0.000268340110779) A[3]:(0.656471729279)\n",
      " state (7)  A[0]:(0.540304780006) A[1]:(-0.532041251659) A[2]:(0.441755473614) A[3]:(0.867289543152)\n",
      " state (8)  A[0]:(0.655594408512) A[1]:(-0.000827401701827) A[2]:(0.728746473789) A[3]:(0.590682864189)\n",
      " state (9)  A[0]:(0.655566811562) A[1]:(0.809748470783) A[2]:(0.809875011444) A[3]:(-0.000308871269226)\n",
      " state (10)  A[0]:(0.729017794132) A[1]:(0.89988899231) A[2]:(-0.000255346298218) A[3]:(0.728875041008)\n",
      " state (11)  A[0]:(0.137817591429) A[1]:(0.883336961269) A[2]:(-0.932238101959) A[3]:(0.804416358471)\n",
      " state (12)  A[0]:(-0.42727008462) A[1]:(0.816729843616) A[2]:(-0.958443403244) A[3]:(0.715777039528)\n",
      " state (13)  A[0]:(-0.00025263428688) A[1]:(0.809285461903) A[2]:(0.900086224079) A[3]:(0.729005217552)\n",
      " state (14)  A[0]:(0.81010723114) A[1]:(0.900349497795) A[2]:(0.999999880791) A[3]:(0.809870839119)\n",
      " state (15)  A[0]:(0.979843378067) A[1]:(0.937331557274) A[2]:(1.0) A[3]:(0.877958774567)\n",
      "Episode 769000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6001. Times reached goal: 993.               Steps done: 5278987. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00458783742277.\n",
      "q_values \n",
      "tensor([[ 0.5309,  0.5905,  0.5905,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5902,  0.6562, -0.0004,  0.5323]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6559, -0.0001,  0.7287,  0.5921]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6560,  0.8101,  0.8098,  0.0026]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0014,  0.8087,  0.9000,  0.7290]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8099,  0.8999,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.53090429306) A[1]:(0.590490758419) A[2]:(0.590484619141) A[3]:(0.531244277954)\n",
      " state (1)  A[0]:(0.531009912491) A[1]:(-2.79694795609e-05) A[2]:(0.656014442444) A[3]:(0.590654790401)\n",
      " state (2)  A[0]:(0.589996218681) A[1]:(0.728964924812) A[2]:(0.590348482132) A[3]:(0.656470417976)\n",
      " state (3)  A[0]:(0.655997574329) A[1]:(-0.0213884431869) A[2]:(0.532534956932) A[3]:(0.545297622681)\n",
      " state (4)  A[0]:(0.590090811253) A[1]:(0.656099677086) A[2]:(-0.000485062570078) A[3]:(0.532105267048)\n",
      " state (5)  A[0]:(-0.0276970937848) A[1]:(0.999896883965) A[2]:(-0.830635428429) A[3]:(0.67455726862)\n",
      " state (6)  A[0]:(-0.000561177672353) A[1]:(0.809914946556) A[2]:(-0.000524997652974) A[3]:(0.656548976898)\n",
      " state (7)  A[0]:(0.540124058723) A[1]:(-0.531727552414) A[2]:(0.441750586033) A[3]:(0.867419660091)\n",
      " state (8)  A[0]:(0.655854463577) A[1]:(-0.000215843319893) A[2]:(0.728793978691) A[3]:(0.591967701912)\n",
      " state (9)  A[0]:(0.655981063843) A[1]:(0.809995889664) A[2]:(0.809842944145) A[3]:(0.00250216806307)\n",
      " state (10)  A[0]:(0.728905439377) A[1]:(0.899950861931) A[2]:(-0.00115930987522) A[3]:(0.72982776165)\n",
      " state (11)  A[0]:(0.136502414942) A[1]:(0.883232414722) A[2]:(-0.932477712631) A[3]:(0.804685354233)\n",
      " state (12)  A[0]:(-0.428434222937) A[1]:(0.816304147243) A[2]:(-0.958594560623) A[3]:(0.715894699097)\n",
      " state (13)  A[0]:(-0.00120514573064) A[1]:(0.808620929718) A[2]:(0.899958610535) A[3]:(0.729125142097)\n",
      " state (14)  A[0]:(0.809959709644) A[1]:(0.899872899055) A[2]:(0.999999880791) A[3]:(0.809994459152)\n",
      " state (15)  A[0]:(0.979844987392) A[1]:(0.936935484409) A[2]:(1.0) A[3]:(0.878084480762)\n",
      "Episode 770000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6024. Times reached goal: 994.               Steps done: 5285011. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00456028336628.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531536102295) A[1]:(0.590829968452) A[2]:(0.590241611004) A[3]:(0.531822681427)\n",
      " state (1)  A[0]:(0.531357884407) A[1]:(0.000336512923241) A[2]:(0.655986785889) A[3]:(0.590550899506)\n",
      " state (2)  A[0]:(0.590130448341) A[1]:(0.729082107544) A[2]:(0.590607047081) A[3]:(0.65616184473)\n",
      " state (3)  A[0]:(0.656011223793) A[1]:(-0.0213232059032) A[2]:(0.532885074615) A[3]:(0.544769883156)\n",
      " state (4)  A[0]:(0.590117454529) A[1]:(0.655795454979) A[2]:(0.000148415565491) A[3]:(0.531514167786)\n",
      " state (5)  A[0]:(-0.02752420865) A[1]:(0.999896883965) A[2]:(-0.830496251583) A[3]:(0.674261689186)\n",
      " state (6)  A[0]:(-0.00062417978188) A[1]:(0.810037732124) A[2]:(-3.23057174683e-05) A[3]:(0.656305611134)\n",
      " state (7)  A[0]:(0.539886474609) A[1]:(-0.531448960304) A[2]:(0.442028880119) A[3]:(0.867247164249)\n",
      " state (8)  A[0]:(0.655789732933) A[1]:(-0.000544205249753) A[2]:(0.728699684143) A[3]:(0.591292202473)\n",
      " state (9)  A[0]:(0.655946731567) A[1]:(0.809923410416) A[2]:(0.809939146042) A[3]:(-0.000397294730647)\n",
      " state (10)  A[0]:(0.72964990139) A[1]:(0.900037646294) A[2]:(0.00038647648762) A[3]:(0.728941559792)\n",
      " state (11)  A[0]:(0.139654293656) A[1]:(0.883524894714) A[2]:(-0.932200431824) A[3]:(0.804712116718)\n",
      " state (12)  A[0]:(-0.426228314638) A[1]:(0.816972374916) A[2]:(-0.95850199461) A[3]:(0.716245770454)\n",
      " state (13)  A[0]:(-0.000176221132278) A[1]:(0.809447109699) A[2]:(0.900010168552) A[3]:(0.729435324669)\n",
      " state (14)  A[0]:(0.809662938118) A[1]:(0.900381088257) A[2]:(0.999999880791) A[3]:(0.81019204855)\n",
      " state (15)  A[0]:(0.979756057262) A[1]:(0.937291800976) A[2]:(1.0) A[3]:(0.87820315361)\n",
      "Episode 771000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 998.               Steps done: 5291021. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00453295825745.\n",
      " state (0)  A[0]:(0.531378626823) A[1]:(0.590406894684) A[2]:(0.590480327606) A[3]:(0.531521439552)\n",
      " state (1)  A[0]:(0.531370162964) A[1]:(9.41753387451e-06) A[2]:(0.656122148037) A[3]:(0.590630173683)\n",
      " state (2)  A[0]:(0.59034216404) A[1]:(0.728981614113) A[2]:(0.590665221214) A[3]:(0.656163811684)\n",
      " state (3)  A[0]:(0.656175613403) A[1]:(-0.0213715676218) A[2]:(0.532937884331) A[3]:(0.544777274132)\n",
      " state (4)  A[0]:(0.590258181095) A[1]:(0.65604865551) A[2]:(0.000115633010864) A[3]:(0.531541347504)\n",
      " state (5)  A[0]:(-0.0271989069879) A[1]:(0.999896883965) A[2]:(-0.830495357513) A[3]:(0.674256920815)\n",
      " state (6)  A[0]:(-2.07424163818e-05) A[1]:(0.810034513474) A[2]:(9.53674316406e-07) A[3]:(0.656179189682)\n",
      " state (7)  A[0]:(0.540294408798) A[1]:(-0.531213819981) A[2]:(0.442117989063) A[3]:(0.867119431496)\n",
      " state (8)  A[0]:(0.655701994896) A[1]:(0.000257134437561) A[2]:(0.729013562202) A[3]:(0.590413272381)\n",
      " state (9)  A[0]:(0.655969381332) A[1]:(0.809998869896) A[2]:(0.810015082359) A[3]:(-4.09185886383e-05)\n",
      " state (10)  A[0]:(0.729367256165) A[1]:(0.900000989437) A[2]:(-0.000271677970886) A[3]:(0.728995323181)\n",
      " state (11)  A[0]:(0.138500794768) A[1]:(0.883427679539) A[2]:(-0.93236887455) A[3]:(0.804407536983)\n",
      " state (12)  A[0]:(-0.426680386066) A[1]:(0.816766917706) A[2]:(-0.958582937717) A[3]:(0.71573472023)\n",
      " state (13)  A[0]:(0.000287681818008) A[1]:(0.809207201004) A[2]:(0.900021255016) A[3]:(0.729071497917)\n",
      " state (14)  A[0]:(0.810091972351) A[1]:(0.900255024433) A[2]:(0.999999880791) A[3]:(0.810016334057)\n",
      " state (15)  A[0]:(0.979822695255) A[1]:(0.937209784985) A[2]:(1.0) A[3]:(0.878120720387)\n",
      "Episode 772000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6012. Times reached goal: 992.               Steps done: 5297033. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00450578786844.\n",
      " state (0)  A[0]:(0.531454324722) A[1]:(0.590471863747) A[2]:(0.590481579304) A[3]:(0.53126013279)\n",
      " state (1)  A[0]:(0.531517207623) A[1]:(-0.000363230676157) A[2]:(0.656150221825) A[3]:(0.590522587299)\n",
      " state (2)  A[0]:(0.590486049652) A[1]:(0.728964149952) A[2]:(0.590591788292) A[3]:(0.656260967255)\n",
      " state (3)  A[0]:(0.656393647194) A[1]:(-0.0214254241437) A[2]:(0.532802581787) A[3]:(0.544944226742)\n",
      " state (4)  A[0]:(0.590597987175) A[1]:(0.656077504158) A[2]:(-0.000190615653992) A[3]:(0.531818389893)\n",
      " state (5)  A[0]:(-0.0266476646066) A[1]:(0.999896883965) A[2]:(-0.830590963364) A[3]:(0.674550056458)\n",
      " state (6)  A[0]:(0.000890910392627) A[1]:(0.809965729713) A[2]:(-0.000106930732727) A[3]:(0.656509280205)\n",
      " state (7)  A[0]:(0.541364312172) A[1]:(-0.531071305275) A[2]:(0.442034333944) A[3]:(0.867357075214)\n",
      " state (8)  A[0]:(0.65699672699) A[1]:(0.000657677534036) A[2]:(0.729023098946) A[3]:(0.591338038445)\n",
      " state (9)  A[0]:(0.657821893692) A[1]:(0.810143947601) A[2]:(0.810137867928) A[3]:(0.00182300608139)\n",
      " state (10)  A[0]:(0.731398701668) A[1]:(0.900027096272) A[2]:(0.000672936323099) A[3]:(0.729980289936)\n",
      " state (11)  A[0]:(0.143447175622) A[1]:(0.883375108242) A[2]:(-0.932225167751) A[3]:(0.805104613304)\n",
      " state (12)  A[0]:(-0.422582387924) A[1]:(0.816573560238) A[2]:(-0.958540797234) A[3]:(0.716469228268)\n",
      " state (13)  A[0]:(0.00494555057958) A[1]:(0.808948874474) A[2]:(0.900042891502) A[3]:(0.729611754417)\n",
      " state (14)  A[0]:(0.811524868011) A[1]:(0.900151431561) A[2]:(0.999999880791) A[3]:(0.810366749763)\n",
      " state (15)  A[0]:(0.979963898659) A[1]:(0.937197268009) A[2]:(1.0) A[3]:(0.878328084946)\n",
      "Episode 773000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 996.               Steps done: 5303043. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00447878929532.\n",
      " state (0)  A[0]:(0.531452000141) A[1]:(0.590586900711) A[2]:(0.590527892113) A[3]:(0.531457901001)\n",
      " state (1)  A[0]:(0.53155040741) A[1]:(-3.44663858414e-05) A[2]:(0.656272172928) A[3]:(0.590507209301)\n",
      " state (2)  A[0]:(0.590553224087) A[1]:(0.729108929634) A[2]:(0.590878069401) A[3]:(0.656165361404)\n",
      " state (3)  A[0]:(0.656422555447) A[1]:(-0.0211528129876) A[2]:(0.533135533333) A[3]:(0.544729590416)\n",
      " state (4)  A[0]:(0.590501308441) A[1]:(0.656268000603) A[2]:(0.000263810157776) A[3]:(0.531529128551)\n",
      " state (5)  A[0]:(-0.0271975360811) A[1]:(0.999897003174) A[2]:(-0.83048582077) A[3]:(0.674350619316)\n",
      " state (6)  A[0]:(0.000146478414536) A[1]:(0.810091376305) A[2]:(0.000241041183472) A[3]:(0.656131148338)\n",
      " state (7)  A[0]:(0.540469408035) A[1]:(-0.531248211861) A[2]:(0.442427426577) A[3]:(0.867070198059)\n",
      " state (8)  A[0]:(0.655805706978) A[1]:(0.000249490141869) A[2]:(0.729170918465) A[3]:(0.590428709984)\n",
      " state (9)  A[0]:(0.655928075314) A[1]:(0.810045361519) A[2]:(0.81009966135) A[3]:(2.44379043579e-05)\n",
      " state (10)  A[0]:(0.729214847088) A[1]:(0.900046408176) A[2]:(-0.000296711921692) A[3]:(0.728901982307)\n",
      " state (11)  A[0]:(0.137984469533) A[1]:(0.883526146412) A[2]:(-0.932450413704) A[3]:(0.804266393185)\n",
      " state (12)  A[0]:(-0.42715844512) A[1]:(0.816997051239) A[2]:(-0.958661019802) A[3]:(0.715537011623)\n",
      " state (13)  A[0]:(-0.000219106674194) A[1]:(0.809521794319) A[2]:(0.900070548058) A[3]:(0.728954434395)\n",
      " state (14)  A[0]:(0.809989154339) A[1]:(0.900460898876) A[2]:(0.999999880791) A[3]:(0.81000483036)\n",
      " state (15)  A[0]:(0.979824185371) A[1]:(0.93732213974) A[2]:(1.0) A[3]:(0.878157436848)\n",
      "Episode 774000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               5993. Times reached goal: 994.               Steps done: 5309036. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00445202818085.\n",
      "q_values \n",
      "tensor([[ 0.5334,  0.5905,  0.5906,  0.5314]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5337, -0.0005,  0.6562,  0.5901]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5925,  0.7291,  0.5907,  0.6554]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0016,  0.8100, -0.0000,  0.6544]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7260,  0.9000, -0.0038,  0.7246]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8082,  0.9012,  1.0000,  0.8085]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.533238708973) A[1]:(0.590550065041) A[2]:(0.590711236) A[3]:(0.5305737257)\n",
      " state (1)  A[0]:(0.533679366112) A[1]:(-0.000737070920877) A[2]:(0.656030476093) A[3]:(0.589266002178)\n",
      " state (2)  A[0]:(0.592614173889) A[1]:(0.728962898254) A[2]:(0.590413689613) A[3]:(0.654944419861)\n",
      " state (3)  A[0]:(0.658260941505) A[1]:(-0.0214640311897) A[2]:(0.532583475113) A[3]:(0.543072521687)\n",
      " state (4)  A[0]:(0.592622637749) A[1]:(0.655996203423) A[2]:(-0.000500202120747) A[3]:(0.529722869396)\n",
      " state (5)  A[0]:(-0.0241733621806) A[1]:(0.99989682436) A[2]:(-0.830669999123) A[3]:(0.672933280468)\n",
      " state (6)  A[0]:(0.00267700222321) A[1]:(0.809763908386) A[2]:(-0.000340223312378) A[3]:(0.654650807381)\n",
      " state (7)  A[0]:(0.541836500168) A[1]:(-0.531524181366) A[2]:(0.441850662231) A[3]:(0.866316914558)\n",
      " state (8)  A[0]:(0.655994653702) A[1]:(0.000750660779886) A[2]:(0.728915572166) A[3]:(0.587024092674)\n",
      " state (9)  A[0]:(0.656154930592) A[1]:(0.810158371925) A[2]:(0.80975985527) A[3]:(-0.0036820305977)\n",
      " state (10)  A[0]:(0.729604959488) A[1]:(0.900132060051) A[2]:(-0.00076603872003) A[3]:(0.72799295187)\n",
      " state (11)  A[0]:(0.138295650482) A[1]:(0.883735477924) A[2]:(-0.932487368584) A[3]:(0.803881406784)\n",
      " state (12)  A[0]:(-0.428361356258) A[1]:(0.817483186722) A[2]:(-0.958741426468) A[3]:(0.714879870415)\n",
      " state (13)  A[0]:(-0.00313397217542) A[1]:(0.810167074203) A[2]:(0.899918079376) A[3]:(0.728087067604)\n",
      " state (14)  A[0]:(0.808860063553) A[1]:(0.900853991508) A[2]:(0.999999880791) A[3]:(0.809288859367)\n",
      " state (15)  A[0]:(0.979704678059) A[1]:(0.937554657459) A[2]:(1.0) A[3]:(0.877674221992)\n",
      "Episode 775000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6015. Times reached goal: 997.               Steps done: 5315051. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0044253296078.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531038343906) A[1]:(0.590695500374) A[2]:(0.590466499329) A[3]:(0.53128027916)\n",
      " state (1)  A[0]:(0.531181454659) A[1]:(-0.000119507312775) A[2]:(0.656028807163) A[3]:(0.590627908707)\n",
      " state (2)  A[0]:(0.590236663818) A[1]:(0.728940606117) A[2]:(0.590714395046) A[3]:(0.656366586685)\n",
      " state (3)  A[0]:(0.656181573868) A[1]:(-0.0216386504471) A[2]:(0.532947361469) A[3]:(0.544998109341)\n",
      " state (4)  A[0]:(0.590299248695) A[1]:(0.655940830708) A[2]:(-1.32322311401e-05) A[3]:(0.531910419464)\n",
      " state (5)  A[0]:(-0.0273924022913) A[1]:(0.999896883965) A[2]:(-0.83054292202) A[3]:(0.674828886986)\n",
      " state (6)  A[0]:(-2.92360782623e-05) A[1]:(0.809883594513) A[2]:(0.00034761428833) A[3]:(0.656936526299)\n",
      " state (7)  A[0]:(0.540652692318) A[1]:(-0.531747758389) A[2]:(0.442627489567) A[3]:(0.867656826973)\n",
      " state (8)  A[0]:(0.656324386597) A[1]:(-0.000246345996857) A[2]:(0.729079604149) A[3]:(0.592664957047)\n",
      " state (9)  A[0]:(0.656642436981) A[1]:(0.809989333153) A[2]:(0.809986650944) A[3]:(0.00367717281915)\n",
      " state (10)  A[0]:(0.729999184608) A[1]:(0.899984657764) A[2]:(-0.00014328956604) A[3]:(0.730720162392)\n",
      " state (11)  A[0]:(0.139463067055) A[1]:(0.883371055126) A[2]:(-0.93244189024) A[3]:(0.805511474609)\n",
      " state (12)  A[0]:(-0.426533252001) A[1]:(0.816655635834) A[2]:(-0.958704471588) A[3]:(0.716821014881)\n",
      " state (13)  A[0]:(0.000308901071548) A[1]:(0.809141755104) A[2]:(0.900107562542) A[3]:(0.729827344418)\n",
      " state (14)  A[0]:(0.810318768024) A[1]:(0.900292932987) A[2]:(0.999999880791) A[3]:(0.810482859612)\n",
      " state (15)  A[0]:(0.979874372482) A[1]:(0.937235534191) A[2]:(1.0) A[3]:(0.878404438496)\n",
      "Episode 776000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 993.               Steps done: 5321062. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00439880873985.\n",
      " state (0)  A[0]:(0.530938625336) A[1]:(0.590611577034) A[2]:(0.59051835537) A[3]:(0.53242456913)\n",
      " state (1)  A[0]:(0.530936062336) A[1]:(-0.000439077586634) A[2]:(0.655936598778) A[3]:(0.591236710548)\n",
      " state (2)  A[0]:(0.590018749237) A[1]:(0.729058325291) A[2]:(0.59025478363) A[3]:(0.656926393509)\n",
      " state (3)  A[0]:(0.655908823013) A[1]:(-0.0211085006595) A[2]:(0.532448232174) A[3]:(0.545727372169)\n",
      " state (4)  A[0]:(0.589954733849) A[1]:(0.656216144562) A[2]:(-0.000614404620137) A[3]:(0.532652258873)\n",
      " state (5)  A[0]:(-0.0278293136507) A[1]:(0.999896943569) A[2]:(-0.830723524094) A[3]:(0.675304353237)\n",
      " state (6)  A[0]:(-0.00101369584445) A[1]:(0.809990823269) A[2]:(-0.000390052766306) A[3]:(0.656671285629)\n",
      " state (7)  A[0]:(0.53902900219) A[1]:(-0.531260192394) A[2]:(0.441998958588) A[3]:(0.866979241371)\n",
      " state (8)  A[0]:(0.653468847275) A[1]:(0.000743001583032) A[2]:(0.729124069214) A[3]:(0.588180959225)\n",
      " state (9)  A[0]:(0.653365492821) A[1]:(0.809998035431) A[2]:(0.809921205044) A[3]:(-0.00294145103544)\n",
      " state (10)  A[0]:(0.727150678635) A[1]:(0.899994492531) A[2]:(-0.000864267116413) A[3]:(0.727767586708)\n",
      " state (11)  A[0]:(0.13331823051) A[1]:(0.883542656898) A[2]:(-0.932595670223) A[3]:(0.803488492966)\n",
      " state (12)  A[0]:(-0.431689321995) A[1]:(0.817133665085) A[2]:(-0.958806753159) A[3]:(0.714497566223)\n",
      " state (13)  A[0]:(-0.00613724952564) A[1]:(0.809743702412) A[2]:(0.900055170059) A[3]:(0.728128790855)\n",
      " state (14)  A[0]:(0.808033049107) A[1]:(0.900598227978) A[2]:(0.999999880791) A[3]:(0.809651255608)\n",
      " state (15)  A[0]:(0.979616582394) A[1]:(0.937358796597) A[2]:(1.0) A[3]:(0.878078997135)\n",
      "Episode 777000 finished after 0 timesteps with r=1.0. Running score: 0.98. Times trained:               6006. Times reached goal: 996.               Steps done: 5327068. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00437246867296.\n",
      " state (0)  A[0]:(0.531337082386) A[1]:(0.59034883976) A[2]:(0.590475201607) A[3]:(0.531401872635)\n",
      " state (1)  A[0]:(0.531204938889) A[1]:(-2.93254852295e-05) A[2]:(0.656036138535) A[3]:(0.590716958046)\n",
      " state (2)  A[0]:(0.590213179588) A[1]:(0.728979825974) A[2]:(0.590708971024) A[3]:(0.656158328056)\n",
      " state (3)  A[0]:(0.656088113785) A[1]:(-0.021322356537) A[2]:(0.532985687256) A[3]:(0.544604182243)\n",
      " state (4)  A[0]:(0.59017932415) A[1]:(0.655965030193) A[2]:(0.000130295753479) A[3]:(0.53145968914)\n",
      " state (5)  A[0]:(-0.027473077178) A[1]:(0.999896883965) A[2]:(-0.830582618713) A[3]:(0.674623250961)\n",
      " state (6)  A[0]:(8.00490379333e-05) A[1]:(0.809953987598) A[2]:(-0.000148177146912) A[3]:(0.65633481741)\n",
      " state (7)  A[0]:(0.540450751781) A[1]:(-0.531193912029) A[2]:(0.441991180182) A[3]:(0.867123425007)\n",
      " state (8)  A[0]:(0.65555280447) A[1]:(0.000349044799805) A[2]:(0.728924870491) A[3]:(0.590357363224)\n",
      " state (9)  A[0]:(0.655552268028) A[1]:(0.809989452362) A[2]:(0.809938430786) A[3]:(-1.70767307281e-05)\n",
      " state (10)  A[0]:(0.72895270586) A[1]:(0.899974107742) A[2]:(-0.000584721507039) A[3]:(0.728862702847)\n",
      " state (11)  A[0]:(0.137295022607) A[1]:(0.883423924446) A[2]:(-0.932580113411) A[3]:(0.804189503193)\n",
      " state (12)  A[0]:(-0.427976250648) A[1]:(0.816810369492) A[2]:(-0.958823740482) A[3]:(0.715325057507)\n",
      " state (13)  A[0]:(-0.000874563818797) A[1]:(0.809306740761) A[2]:(0.899974942207) A[3]:(0.728777289391)\n",
      " state (14)  A[0]:(0.810092747211) A[1]:(0.900364041328) A[2]:(0.999999880791) A[3]:(0.809973061085)\n",
      " state (15)  A[0]:(0.979869306087) A[1]:(0.937241435051) A[2]:(1.0) A[3]:(0.878195643425)\n",
      "Episode 778000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6017. Times reached goal: 998.               Steps done: 5333085. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0043462385215.\n",
      " state (0)  A[0]:(0.531550526619) A[1]:(0.590661644936) A[2]:(0.590547204018) A[3]:(0.531601190567)\n",
      " state (1)  A[0]:(0.531695783138) A[1]:(-0.000114619731903) A[2]:(0.656323194504) A[3]:(0.591017723083)\n",
      " state (2)  A[0]:(0.590739488602) A[1]:(0.728854656219) A[2]:(0.590778052807) A[3]:(0.656533002853)\n",
      " state (3)  A[0]:(0.656481266022) A[1]:(-0.0216514002532) A[2]:(0.532894313335) A[3]:(0.544973254204)\n",
      " state (4)  A[0]:(0.590534210205) A[1]:(0.656132638454) A[2]:(-0.000337362289429) A[3]:(0.531774759293)\n",
      " state (5)  A[0]:(-0.026995267719) A[1]:(0.999896883965) A[2]:(-0.830758869648) A[3]:(0.674772024155)\n",
      " state (6)  A[0]:(0.000349923968315) A[1]:(0.809919595718) A[2]:(-0.000642537954263) A[3]:(0.656491041183)\n",
      " state (7)  A[0]:(0.540807962418) A[1]:(-0.53116106987) A[2]:(0.441514134407) A[3]:(0.867348909378)\n",
      " state (8)  A[0]:(0.656235814095) A[1]:(8.18371772766e-05) A[2]:(0.728559136391) A[3]:(0.592066645622)\n",
      " state (9)  A[0]:(0.65604197979) A[1]:(0.810075283051) A[2]:(0.809943318367) A[3]:(0.00158426037524)\n",
      " state (10)  A[0]:(0.729701638222) A[1]:(0.900022804737) A[2]:(2.34842300415e-05) A[3]:(0.729571223259)\n",
      " state (11)  A[0]:(0.139574691653) A[1]:(0.883403778076) A[2]:(-0.932511329651) A[3]:(0.804763615131)\n",
      " state (12)  A[0]:(-0.426342904568) A[1]:(0.816651463509) A[2]:(-0.958843946457) A[3]:(0.71588665247)\n",
      " state (13)  A[0]:(0.000120639801025) A[1]:(0.809056520462) A[2]:(0.899873554707) A[3]:(0.729010462761)\n",
      " state (14)  A[0]:(0.809976875782) A[1]:(0.900225818157) A[2]:(0.999999880791) A[3]:(0.809939146042)\n",
      " state (15)  A[0]:(0.979810237885) A[1]:(0.937162220478) A[2]:(1.0) A[3]:(0.878077864647)\n",
      "Episode 779000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6013. Times reached goal: 998.               Steps done: 5339098. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00432018300369.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5905,  0.5908,  0.5313]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5316, -0.0001,  0.6561,  0.5903]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5905,  0.7289,  0.5907,  0.6561]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8100, -0.0002,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7295,  0.9000, -0.0004,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9003,  1.0000,  0.8101]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531251132488) A[1]:(0.590503692627) A[2]:(0.590773522854) A[3]:(0.531481564045)\n",
      " state (1)  A[0]:(0.531456589699) A[1]:(-0.000124678015709) A[2]:(0.656146109104) A[3]:(0.590451657772)\n",
      " state (2)  A[0]:(0.590400695801) A[1]:(0.728895902634) A[2]:(0.590695500374) A[3]:(0.656151413918)\n",
      " state (3)  A[0]:(0.656245291233) A[1]:(-0.0214152820408) A[2]:(0.53286921978) A[3]:(0.544609427452)\n",
      " state (4)  A[0]:(0.590306758881) A[1]:(0.656404733658) A[2]:(-0.000304937362671) A[3]:(0.53149920702)\n",
      " state (5)  A[0]:(-0.0272515583783) A[1]:(0.999896943569) A[2]:(-0.830723643303) A[3]:(0.674584269524)\n",
      " state (6)  A[0]:(9.80496406555e-06) A[1]:(0.809985876083) A[2]:(-0.000171184539795) A[3]:(0.656053721905)\n",
      " state (7)  A[0]:(0.540360331535) A[1]:(-0.531300008297) A[2]:(0.442220300436) A[3]:(0.866995692253)\n",
      " state (8)  A[0]:(0.655790448189) A[1]:(0.000177875161171) A[2]:(0.728990197182) A[3]:(0.590435504913)\n",
      " state (9)  A[0]:(0.655916810036) A[1]:(0.8100451231) A[2]:(0.810008108616) A[3]:(-0.000227391719818)\n",
      " state (10)  A[0]:(0.729383826256) A[1]:(0.900000572205) A[2]:(-0.000438928575022) A[3]:(0.728822171688)\n",
      " state (11)  A[0]:(0.138382285833) A[1]:(0.8834182024) A[2]:(-0.932637810707) A[3]:(0.804261744022)\n",
      " state (12)  A[0]:(-0.42713919282) A[1]:(0.816741228104) A[2]:(-0.958904802799) A[3]:(0.715492129326)\n",
      " state (13)  A[0]:(1.6912817955e-05) A[1]:(0.809211552143) A[2]:(0.900000214577) A[3]:(0.728960752487)\n",
      " state (14)  A[0]:(0.810348510742) A[1]:(0.900319993496) A[2]:(0.999999880791) A[3]:(0.81009632349)\n",
      " state (15)  A[0]:(0.979895055294) A[1]:(0.937185645103) A[2]:(1.0) A[3]:(0.878281891346)\n",
      "Episode 780000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6015. Times reached goal: 997.               Steps done: 5345113. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00429427509906.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531600356102) A[1]:(0.590426921844) A[2]:(0.590337872505) A[3]:(0.531479120255)\n",
      " state (1)  A[0]:(0.531621932983) A[1]:(6.85304403305e-05) A[2]:(0.656082630157) A[3]:(0.590573310852)\n",
      " state (2)  A[0]:(0.590577661991) A[1]:(0.728967905045) A[2]:(0.590548157692) A[3]:(0.656242012978)\n",
      " state (3)  A[0]:(0.656421303749) A[1]:(-0.0212517119944) A[2]:(0.532760620117) A[3]:(0.544771909714)\n",
      " state (4)  A[0]:(0.59062230587) A[1]:(0.655917525291) A[2]:(-0.000126600265503) A[3]:(0.53166949749)\n",
      " state (5)  A[0]:(-0.0268382932991) A[1]:(0.99989682436) A[2]:(-0.830654501915) A[3]:(0.674646258354)\n",
      " state (6)  A[0]:(0.000393941969378) A[1]:(0.809925734997) A[2]:(3.69548797607e-05) A[3]:(0.656033039093)\n",
      " state (7)  A[0]:(0.540691137314) A[1]:(-0.53147649765) A[2]:(0.442342013121) A[3]:(0.867037892342)\n",
      " state (8)  A[0]:(0.656050741673) A[1]:(-0.000311031937599) A[2]:(0.728952109814) A[3]:(0.590857565403)\n",
      " state (9)  A[0]:(0.655864834785) A[1]:(0.809856235981) A[2]:(0.809925973415) A[3]:(5.92768192291e-05)\n",
      " state (10)  A[0]:(0.729109168053) A[1]:(0.899864792824) A[2]:(-0.000991463311948) A[3]:(0.728795170784)\n",
      " state (11)  A[0]:(0.137704193592) A[1]:(0.883226633072) A[2]:(-0.932751715183) A[3]:(0.80419164896)\n",
      " state (12)  A[0]:(-0.427364140749) A[1]:(0.816420435905) A[2]:(-0.958977341652) A[3]:(0.715487241745)\n",
      " state (13)  A[0]:(-3.5896897316e-05) A[1]:(0.808878421783) A[2]:(0.899896681309) A[3]:(0.72904920578)\n",
      " state (14)  A[0]:(0.810231328011) A[1]:(0.900176465511) A[2]:(0.999999880791) A[3]:(0.810161173344)\n",
      " state (15)  A[0]:(0.979863464832) A[1]:(0.937126040459) A[2]:(1.0) A[3]:(0.878300309181)\n",
      "Episode 781000 finished after 0 timesteps with r=1.0. Running score: 0.97. Times trained:               6018. Times reached goal: 992.               Steps done: 5351131. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00426850975719.\n",
      " state (0)  A[0]:(0.531225442886) A[1]:(0.590349018574) A[2]:(0.590249419212) A[3]:(0.531991243362)\n",
      " state (1)  A[0]:(0.531410813332) A[1]:(-0.00105306471232) A[2]:(0.655933260918) A[3]:(0.590586304665)\n",
      " state (2)  A[0]:(0.590443015099) A[1]:(0.728984594345) A[2]:(0.590391337872) A[3]:(0.656047224998)\n",
      " state (3)  A[0]:(0.656370639801) A[1]:(-0.0212632250041) A[2]:(0.532616734505) A[3]:(0.544362068176)\n",
      " state (4)  A[0]:(0.590635180473) A[1]:(0.655909359455) A[2]:(-0.000342965126038) A[3]:(0.531178414822)\n",
      " state (5)  A[0]:(-0.0267711225897) A[1]:(0.999896883965) A[2]:(-0.830773353577) A[3]:(0.674332261086)\n",
      " state (6)  A[0]:(0.000571787299123) A[1]:(0.809994459152) A[2]:(-0.000340223312378) A[3]:(0.655692815781)\n",
      " state (7)  A[0]:(0.540841221809) A[1]:(-0.531598806381) A[2]:(0.442146360874) A[3]:(0.866847813129)\n",
      " state (8)  A[0]:(0.656369686127) A[1]:(-0.000659435871057) A[2]:(0.728820681572) A[3]:(0.590508103371)\n",
      " state (9)  A[0]:(0.65672314167) A[1]:(0.809788644314) A[2]:(0.810019135475) A[3]:(0.000124782323837)\n",
      " state (10)  A[0]:(0.730657696724) A[1]:(0.899963021278) A[2]:(0.000728845479898) A[3]:(0.729358196259)\n",
      " state (11)  A[0]:(0.142034173012) A[1]:(0.883553922176) A[2]:(-0.93244022131) A[3]:(0.804842352867)\n",
      " state (12)  A[0]:(-0.424573987722) A[1]:(0.817188739777) A[2]:(-0.958879828453) A[3]:(0.715930461884)\n",
      " state (13)  A[0]:(0.00210731918924) A[1]:(0.809870243073) A[2]:(0.899915218353) A[3]:(0.728845477104)\n",
      " state (14)  A[0]:(0.810812830925) A[1]:(0.900813281536) A[2]:(0.999999880791) A[3]:(0.809646368027)\n",
      " state (15)  A[0]:(0.979933559895) A[1]:(0.937585055828) A[2]:(1.0) A[3]:(0.877793550491)\n",
      "Episode 782000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 994.               Steps done: 5357141. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00424293294884.\n",
      " state (0)  A[0]:(0.533122181892) A[1]:(0.589898347855) A[2]:(0.590864419937) A[3]:(0.531911253929)\n",
      " state (1)  A[0]:(0.53329706192) A[1]:(0.00557054765522) A[2]:(0.65585064888) A[3]:(0.593767881393)\n",
      " state (2)  A[0]:(0.592228889465) A[1]:(0.728917479515) A[2]:(0.590593338013) A[3]:(0.659275114536)\n",
      " state (3)  A[0]:(0.657640099525) A[1]:(-0.0217220429331) A[2]:(0.532756984234) A[3]:(0.54876768589)\n",
      " state (4)  A[0]:(0.591616392136) A[1]:(0.656189441681) A[2]:(-0.000692605855875) A[3]:(0.536471545696)\n",
      " state (5)  A[0]:(-0.0257276874036) A[1]:(0.999896883965) A[2]:(-0.831146001816) A[3]:(0.679734170437)\n",
      " state (6)  A[0]:(0.000330880284309) A[1]:(0.809745907784) A[2]:(-0.00186562316958) A[3]:(0.661762833595)\n",
      " state (7)  A[0]:(0.538854777813) A[1]:(-0.532928168774) A[2]:(0.441296160221) A[3]:(0.869006574154)\n",
      " state (8)  A[0]:(0.6525785923) A[1]:(-0.00218610116281) A[2]:(0.72898042202) A[3]:(0.592331409454)\n",
      " state (9)  A[0]:(0.653586983681) A[1]:(0.808889687061) A[2]:(0.810700297356) A[3]:(0.0052126175724)\n",
      " state (10)  A[0]:(0.729977488518) A[1]:(0.899559378624) A[2]:(0.00764784216881) A[3]:(0.734690666199)\n",
      " state (11)  A[0]:(0.140063837171) A[1]:(0.883444666862) A[2]:(-0.931214153767) A[3]:(0.8103043437)\n",
      " state (12)  A[0]:(-0.43415504694) A[1]:(0.817519843578) A[2]:(-0.958399653435) A[3]:(0.722544550896)\n",
      " state (13)  A[0]:(-0.0192808900028) A[1]:(0.810581445694) A[2]:(0.900968372822) A[3]:(0.733750283718)\n",
      " state (14)  A[0]:(0.801706969738) A[1]:(0.901136338711) A[2]:(0.999999880791) A[3]:(0.812865674496)\n",
      " state (15)  A[0]:(0.978851795197) A[1]:(0.937499582767) A[2]:(1.0) A[3]:(0.880027592182)\n",
      "Episode 783000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6029. Times reached goal: 997.               Steps done: 5363170. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0042174292642.\n",
      " state (0)  A[0]:(0.531727790833) A[1]:(0.590674519539) A[2]:(0.590849041939) A[3]:(0.531321287155)\n",
      " state (1)  A[0]:(0.531910479069) A[1]:(-0.000160962343216) A[2]:(0.656236112118) A[3]:(0.590462088585)\n",
      " state (2)  A[0]:(0.59100151062) A[1]:(0.729062795639) A[2]:(0.590568065643) A[3]:(0.656234622002)\n",
      " state (3)  A[0]:(0.656788408756) A[1]:(-0.0209794547409) A[2]:(0.532744169235) A[3]:(0.544597148895)\n",
      " state (4)  A[0]:(0.590970754623) A[1]:(0.656350195408) A[2]:(-0.000368833512766) A[3]:(0.531493425369)\n",
      " state (5)  A[0]:(-0.026500724256) A[1]:(0.99989682436) A[2]:(-0.83080381155) A[3]:(0.674714386463)\n",
      " state (6)  A[0]:(0.000577166618314) A[1]:(0.80977088213) A[2]:(-0.000190615653992) A[3]:(0.656057357788)\n",
      " state (7)  A[0]:(0.54087293148) A[1]:(-0.531369328499) A[2]:(0.44230863452) A[3]:(0.867010772228)\n",
      " state (8)  A[0]:(0.656582593918) A[1]:(0.000254839658737) A[2]:(0.72900146246) A[3]:(0.590494453907)\n",
      " state (9)  A[0]:(0.657293200493) A[1]:(0.810060858727) A[2]:(0.809978842735) A[3]:(0.000130355358124)\n",
      " state (10)  A[0]:(0.731019616127) A[1]:(0.899963259697) A[2]:(-0.000406146020396) A[3]:(0.729351878166)\n",
      " state (11)  A[0]:(0.142848923802) A[1]:(0.883319020271) A[2]:(-0.932709515095) A[3]:(0.804874062538)\n",
      " state (12)  A[0]:(-0.423031508923) A[1]:(0.816510081291) A[2]:(-0.959017038345) A[3]:(0.716416060925)\n",
      " state (13)  A[0]:(0.00466151116416) A[1]:(0.808966159821) A[2]:(0.900019407272) A[3]:(0.729788184166)\n",
      " state (14)  A[0]:(0.811529636383) A[1]:(0.900269508362) A[2]:(0.999999880791) A[3]:(0.810570299625)\n",
      " state (15)  A[0]:(0.979981482029) A[1]:(0.937189459801) A[2]:(1.0) A[3]:(0.878510892391)\n",
      "Episode 784000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6011. Times reached goal: 995.               Steps done: 5369181. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00419215433679.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5907,  0.5905,  0.5319]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5903,  0.6564, -0.0002,  0.5320]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6553, -0.0004,  0.7290,  0.5913]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6556,  0.8099,  0.8101,  0.0005]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7293,  0.9000,  0.0003,  0.7293]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8089,  0.9005,  1.0000,  0.8103]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.532112002373) A[1]:(0.590750575066) A[2]:(0.590521335602) A[3]:(0.531906962395)\n",
      " state (1)  A[0]:(0.532103657722) A[1]:(0.000280916690826) A[2]:(0.656121373177) A[3]:(0.590932369232)\n",
      " state (2)  A[0]:(0.590962409973) A[1]:(0.729088306427) A[2]:(0.590454816818) A[3]:(0.656630277634)\n",
      " state (3)  A[0]:(0.656726896763) A[1]:(-0.0207756068558) A[2]:(0.532676935196) A[3]:(0.545193314552)\n",
      " state (4)  A[0]:(0.590903162956) A[1]:(0.656257271767) A[2]:(-0.000206232070923) A[3]:(0.532263755798)\n",
      " state (5)  A[0]:(-0.0267041902989) A[1]:(0.999896883965) A[2]:(-0.830719172955) A[3]:(0.675613045692)\n",
      " state (6)  A[0]:(4.33027744293e-05) A[1]:(0.809997856617) A[2]:(1.74045562744e-05) A[3]:(0.65694051981)\n",
      " state (7)  A[0]:(0.540229201317) A[1]:(-0.5313539505) A[2]:(0.442481964827) A[3]:(0.867359995842)\n",
      " state (8)  A[0]:(0.656101703644) A[1]:(-0.000364482373698) A[2]:(0.72895014286) A[3]:(0.592123806477)\n",
      " state (9)  A[0]:(0.656231939793) A[1]:(0.809950828552) A[2]:(0.810049295425) A[3]:(0.00162547687069)\n",
      " state (10)  A[0]:(0.729595899582) A[1]:(0.899993360043) A[2]:(-6.46114349365e-05) A[3]:(0.729566395283)\n",
      " state (11)  A[0]:(0.138352140784) A[1]:(0.883467793465) A[2]:(-0.932717084885) A[3]:(0.80481928587)\n",
      " state (12)  A[0]:(-0.428291112185) A[1]:(0.816896319389) A[2]:(-0.959071099758) A[3]:(0.716053366661)\n",
      " state (13)  A[0]:(-0.00286931497976) A[1]:(0.809470772743) A[2]:(0.899975478649) A[3]:(0.729341030121)\n",
      " state (14)  A[0]:(0.808938324451) A[1]:(0.900536715984) A[2]:(0.999999880791) A[3]:(0.810328245163)\n",
      " state (15)  A[0]:(0.979709267616) A[1]:(0.937292993069) A[2]:(1.0) A[3]:(0.878444969654)\n",
      "Episode 785000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 995.               Steps done: 5375190. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00416703921534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531688928604) A[1]:(0.590292811394) A[2]:(0.590487480164) A[3]:(0.531991958618)\n",
      " state (1)  A[0]:(0.531908273697) A[1]:(-0.000708818319254) A[2]:(0.656093835831) A[3]:(0.590311408043)\n",
      " state (2)  A[0]:(0.590798020363) A[1]:(0.728957474232) A[2]:(0.590501487255) A[3]:(0.655730307102)\n",
      " state (3)  A[0]:(0.656581521034) A[1]:(-0.0208135582507) A[2]:(0.532803058624) A[3]:(0.543788194656)\n",
      " state (4)  A[0]:(0.590751290321) A[1]:(0.656305670738) A[2]:(9.40561294556e-05) A[3]:(0.530406117439)\n",
      " state (5)  A[0]:(-0.0268203355372) A[1]:(0.999896943569) A[2]:(-0.830598533154) A[3]:(0.673574090004)\n",
      " state (6)  A[0]:(0.000252902507782) A[1]:(0.810099184513) A[2]:(0.000469088525278) A[3]:(0.654068946838)\n",
      " state (7)  A[0]:(0.540185987949) A[1]:(-0.530981302261) A[2]:(0.442805677652) A[3]:(0.865825712681)\n",
      " state (8)  A[0]:(0.655368089676) A[1]:(0.000167846679688) A[2]:(0.729039430618) A[3]:(0.587384939194)\n",
      " state (9)  A[0]:(0.654908776283) A[1]:(0.810065567493) A[2]:(0.809935450554) A[3]:(-0.00587075157091)\n",
      " state (10)  A[0]:(0.728430747986) A[1]:(0.900058269501) A[2]:(-0.00070440757554) A[3]:(0.726043343544)\n",
      " state (11)  A[0]:(0.136851847172) A[1]:(0.88359606266) A[2]:(-0.932802557945) A[3]:(0.80241215229)\n",
      " state (12)  A[0]:(-0.427881658077) A[1]:(0.817150115967) A[2]:(-0.959103882313) A[3]:(0.713310956955)\n",
      " state (13)  A[0]:(-0.000400125951273) A[1]:(0.809759438038) A[2]:(0.899887800217) A[3]:(0.727122664452)\n",
      " state (14)  A[0]:(0.810248851776) A[1]:(0.900752782822) A[2]:(0.999999880791) A[3]:(0.808798909187)\n",
      " state (15)  A[0]:(0.979882001877) A[1]:(0.937506437302) A[2]:(1.0) A[3]:(0.877412319183)\n",
      "Episode 786000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6021. Times reached goal: 994.               Steps done: 5381211. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00414202485353.\n",
      " state (0)  A[0]:(0.531272828579) A[1]:(0.590475022793) A[2]:(0.590626955032) A[3]:(0.531415224075)\n",
      " state (1)  A[0]:(0.531330347061) A[1]:(-0.00026036798954) A[2]:(0.656198620796) A[3]:(0.590556740761)\n",
      " state (2)  A[0]:(0.590326249599) A[1]:(0.728739440441) A[2]:(0.590748012066) A[3]:(0.656228065491)\n",
      " state (3)  A[0]:(0.656155347824) A[1]:(-0.0215938054025) A[2]:(0.533044934273) A[3]:(0.54454934597)\n",
      " state (4)  A[0]:(0.590268969536) A[1]:(0.655788123608) A[2]:(0.000296235084534) A[3]:(0.531542301178)\n",
      " state (5)  A[0]:(-0.0274311620742) A[1]:(0.99989682436) A[2]:(-0.830646157265) A[3]:(0.675046086311)\n",
      " state (6)  A[0]:(-3.65376472473e-05) A[1]:(0.809979319572) A[2]:(0.00029456615448) A[3]:(0.655972599983)\n",
      " state (7)  A[0]:(0.540152192116) A[1]:(-0.531110882759) A[2]:(0.442852139473) A[3]:(0.8667781353)\n",
      " state (8)  A[0]:(0.65543115139) A[1]:(0.000521883310284) A[2]:(0.729286432266) A[3]:(0.589802622795)\n",
      " state (9)  A[0]:(0.65544885397) A[1]:(0.810165345669) A[2]:(0.810145974159) A[3]:(-0.000873028999195)\n",
      " state (10)  A[0]:(0.728949785233) A[1]:(0.900084972382) A[2]:(1.74045562744e-05) A[3]:(0.72863817215)\n",
      " state (11)  A[0]:(0.137456819415) A[1]:(0.883575856686) A[2]:(-0.932698905468) A[3]:(0.80421513319)\n",
      " state (12)  A[0]:(-0.42798948288) A[1]:(0.817059218884) A[2]:(-0.958992362022) A[3]:(0.715455293655)\n",
      " state (13)  A[0]:(-0.0011150832288) A[1]:(0.809662282467) A[2]:(0.900589704514) A[3]:(0.728899478912)\n",
      " state (14)  A[0]:(0.809866011143) A[1]:(0.900672793388) A[2]:(0.999999880791) A[3]:(0.809961855412)\n",
      " state (15)  A[0]:(0.979826748371) A[1]:(0.937387943268) A[2]:(1.0) A[3]:(0.878139913082)\n",
      "Episode 787000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6021. Times reached goal: 999.               Steps done: 5387232. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00411716065068.\n",
      " state (0)  A[0]:(0.531133055687) A[1]:(0.58985799551) A[2]:(0.591082394123) A[3]:(0.531222879887)\n",
      " state (1)  A[0]:(0.531153440475) A[1]:(7.3254108429e-05) A[2]:(0.655884683132) A[3]:(0.590888261795)\n",
      " state (2)  A[0]:(0.590155482292) A[1]:(0.728883743286) A[2]:(0.590229809284) A[3]:(0.656264066696)\n",
      " state (3)  A[0]:(0.656027793884) A[1]:(-0.0211094841361) A[2]:(0.532406210899) A[3]:(0.544579863548)\n",
      " state (4)  A[0]:(0.590177714825) A[1]:(0.656026721001) A[2]:(-0.000594139040913) A[3]:(0.531528711319)\n",
      " state (5)  A[0]:(-0.0275225546211) A[1]:(0.99989682436) A[2]:(-0.830865740776) A[3]:(0.674946427345)\n",
      " state (6)  A[0]:(-0.000127330422401) A[1]:(0.80998146534) A[2]:(-0.000241637229919) A[3]:(0.655960083008)\n",
      " state (7)  A[0]:(0.540227770805) A[1]:(-0.531024217606) A[2]:(0.442354083061) A[3]:(0.86686450243)\n",
      " state (8)  A[0]:(0.655636429787) A[1]:(0.000517189444508) A[2]:(0.72898542881) A[3]:(0.590214669704)\n",
      " state (9)  A[0]:(0.655828595161) A[1]:(0.810120940208) A[2]:(0.809998273849) A[3]:(-0.000154465436935)\n",
      " state (10)  A[0]:(0.72958868742) A[1]:(0.899992585182) A[2]:(-0.000100493431091) A[3]:(0.729217290878)\n",
      " state (11)  A[0]:(0.139244675636) A[1]:(0.883374929428) A[2]:(-0.9327455163) A[3]:(0.804729104042)\n",
      " state (12)  A[0]:(-0.426625102758) A[1]:(0.816611766815) A[2]:(-0.959119200706) A[3]:(0.716018676758)\n",
      " state (13)  A[0]:(0.000372320384486) A[1]:(0.809072434902) A[2]:(0.900000572205) A[3]:(0.729237437248)\n",
      " state (14)  A[0]:(0.81040430069) A[1]:(0.900342047215) A[2]:(0.999999880791) A[3]:(0.810064375401)\n",
      " state (15)  A[0]:(0.979892909527) A[1]:(0.937205433846) A[2]:(1.0) A[3]:(0.878134250641)\n",
      "Episode 788000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 997.               Steps done: 5393241. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00409249481506.\n",
      " state (0)  A[0]:(0.531341910362) A[1]:(0.590373039246) A[2]:(0.590522825718) A[3]:(0.531341552734)\n",
      " state (1)  A[0]:(0.531387329102) A[1]:(-0.000149920582771) A[2]:(0.656108021736) A[3]:(0.5904302001)\n",
      " state (2)  A[0]:(0.590294599533) A[1]:(0.728909730911) A[2]:(0.590537548065) A[3]:(0.656059026718)\n",
      " state (3)  A[0]:(0.656191587448) A[1]:(-0.0212610661983) A[2]:(0.532695710659) A[3]:(0.544349193573)\n",
      " state (4)  A[0]:(0.590350627899) A[1]:(0.655872404575) A[2]:(-0.000382065743906) A[3]:(0.531292319298)\n",
      " state (5)  A[0]:(-0.0274958722293) A[1]:(0.999896883965) A[2]:(-0.830971837044) A[3]:(0.674671173096)\n",
      " state (6)  A[0]:(-0.000132873654366) A[1]:(0.810051441193) A[2]:(-0.000601768435445) A[3]:(0.655503511429)\n",
      " state (7)  A[0]:(0.540178775787) A[1]:(-0.531441092491) A[2]:(0.442318886518) A[3]:(0.866670906544)\n",
      " state (8)  A[0]:(0.655765712261) A[1]:(-0.00035353002022) A[2]:(0.728955626488) A[3]:(0.590313434601)\n",
      " state (9)  A[0]:(0.65610742569) A[1]:(0.809916734695) A[2]:(0.810158908367) A[3]:(0.000438481540186)\n",
      " state (10)  A[0]:(0.73022454977) A[1]:(0.899984240532) A[2]:(0.0012698166538) A[3]:(0.729909062386)\n",
      " state (11)  A[0]:(0.140988171101) A[1]:(0.883498489857) A[2]:(-0.932532429695) A[3]:(0.805456876755)\n",
      " state (12)  A[0]:(-0.426001697779) A[1]:(0.816955149174) A[2]:(-0.95905995369) A[3]:(0.716796755791)\n",
      " state (13)  A[0]:(-0.000115171074867) A[1]:(0.809515357018) A[2]:(0.900015294552) A[3]:(0.729649782181)\n",
      " state (14)  A[0]:(0.809996604919) A[1]:(0.900603175163) A[2]:(0.999999880791) A[3]:(0.810200214386)\n",
      " state (15)  A[0]:(0.979841172695) A[1]:(0.937365651131) A[2]:(1.0) A[3]:(0.87816798687)\n",
      "Episode 789000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6022. Times reached goal: 997.               Steps done: 5399263. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00406792386865.\n",
      "q_values \n",
      "tensor([[ 0.5313,  0.5904,  0.5904,  0.5315]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.6561, -0.0002,  0.5314]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6557, -0.0004,  0.7289,  0.5909]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6555,  0.8099,  0.8099,  0.0001]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7291,  0.9000, -0.0004,  0.7289]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9007,  1.0000,  0.8100]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531279683113) A[1]:(0.59038567543) A[2]:(0.59037655592) A[3]:(0.531485557556)\n",
      " state (1)  A[0]:(0.531102061272) A[1]:(6.49392604828e-05) A[2]:(0.656050682068) A[3]:(0.590473294258)\n",
      " state (2)  A[0]:(0.590049266815) A[1]:(0.728934049606) A[2]:(0.590621888638) A[3]:(0.656003654003)\n",
      " state (3)  A[0]:(0.655959248543) A[1]:(-0.0211494918913) A[2]:(0.532846927643) A[3]:(0.544232606888)\n",
      " state (4)  A[0]:(0.590085148811) A[1]:(0.656116247177) A[2]:(-0.000165104866028) A[3]:(0.531337380409)\n",
      " state (5)  A[0]:(-0.0276679843664) A[1]:(0.99989682436) A[2]:(-0.830852270126) A[3]:(0.675196051598)\n",
      " state (6)  A[0]:(-0.000198230147362) A[1]:(0.809938728809) A[2]:(-9.60826873779e-05) A[3]:(0.656131923199)\n",
      " state (7)  A[0]:(0.54005241394) A[1]:(-0.53144210577) A[2]:(0.442596137524) A[3]:(0.866862952709)\n",
      " state (8)  A[0]:(0.655544638634) A[1]:(-0.000443845958216) A[2]:(0.728920638561) A[3]:(0.590669929981)\n",
      " state (9)  A[0]:(0.655383110046) A[1]:(0.809905290604) A[2]:(0.809910595417) A[3]:(-0.000198036432266)\n",
      " state (10)  A[0]:(0.729023218155) A[1]:(0.899990737438) A[2]:(-0.000444650620921) A[3]:(0.728825688362)\n",
      " state (11)  A[0]:(0.138211652637) A[1]:(0.883529186249) A[2]:(-0.932819068432) A[3]:(0.804387629032)\n",
      " state (12)  A[0]:(-0.427175402641) A[1]:(0.817034363747) A[2]:(-0.959167003632) A[3]:(0.715665459633)\n",
      " state (13)  A[0]:(-0.000458642811282) A[1]:(0.809614539146) A[2]:(0.900019407272) A[3]:(0.729040384293)\n",
      " state (14)  A[0]:(0.809837639332) A[1]:(0.900673210621) A[2]:(0.999999880791) A[3]:(0.809982597828)\n",
      " state (15)  A[0]:(0.979800224304) A[1]:(0.937423110008) A[2]:(1.0) A[3]:(0.878096461296)\n",
      "Episode 790000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 998.               Steps done: 5405273. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00404354896615.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531628131866) A[1]:(0.590918660164) A[2]:(0.590648710728) A[3]:(0.530791521072)\n",
      " state (1)  A[0]:(0.532175302505) A[1]:(0.00358645571396) A[2]:(0.656428337097) A[3]:(0.590413928032)\n",
      " state (2)  A[0]:(0.591341912746) A[1]:(0.729292154312) A[2]:(0.591424643993) A[3]:(0.655772924423)\n",
      " state (3)  A[0]:(0.657364487648) A[1]:(-0.0208681914955) A[2]:(0.533786892891) A[3]:(0.54394441843)\n",
      " state (4)  A[0]:(0.591781497002) A[1]:(0.656418323517) A[2]:(0.000942945189308) A[3]:(0.531133770943)\n",
      " state (5)  A[0]:(-0.0253325589001) A[1]:(0.99989682436) A[2]:(-0.830544292927) A[3]:(0.675119698048)\n",
      " state (6)  A[0]:(0.00253383279778) A[1]:(0.809691607952) A[2]:(0.0011560912244) A[3]:(0.656163573265)\n",
      " state (7)  A[0]:(0.542001485825) A[1]:(-0.532674729824) A[2]:(0.443777292967) A[3]:(0.866738796234)\n",
      " state (8)  A[0]:(0.657100319862) A[1]:(-0.00133949436713) A[2]:(0.729339838028) A[3]:(0.588732242584)\n",
      " state (9)  A[0]:(0.658066749573) A[1]:(0.809608280659) A[2]:(0.809595108032) A[3]:(-0.00134095468093)\n",
      " state (10)  A[0]:(0.731574892998) A[1]:(0.899988055229) A[2]:(-0.00205230433494) A[3]:(0.729104399681)\n",
      " state (11)  A[0]:(0.143346309662) A[1]:(0.883881628513) A[2]:(-0.933065295219) A[3]:(0.804808497429)\n",
      " state (12)  A[0]:(-0.423540383577) A[1]:(0.81814289093) A[2]:(-0.959300398827) A[3]:(0.716075897217)\n",
      " state (13)  A[0]:(0.00382263585925) A[1]:(0.811282932758) A[2]:(0.900095522404) A[3]:(0.729019343853)\n",
      " state (14)  A[0]:(0.811692774296) A[1]:(0.901748776436) A[2]:(0.999999880791) A[3]:(0.809620738029)\n",
      " state (15)  A[0]:(0.980082094669) A[1]:(0.938101232052) A[2]:(1.0) A[3]:(0.877695143223)\n",
      "Episode 791000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6020. Times reached goal: 1000.               Steps done: 5411293. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00401927992449.\n",
      " state (0)  A[0]:(0.531715512276) A[1]:(0.589787721634) A[2]:(0.590313971043) A[3]:(0.531623482704)\n",
      " state (1)  A[0]:(0.531441926956) A[1]:(-4.96506690979e-05) A[2]:(0.655831873417) A[3]:(0.590899944305)\n",
      " state (2)  A[0]:(0.590177893639) A[1]:(0.728773713112) A[2]:(0.590383052826) A[3]:(0.656308412552)\n",
      " state (3)  A[0]:(0.655905842781) A[1]:(-0.0211942046881) A[2]:(0.532554626465) A[3]:(0.544555187225)\n",
      " state (4)  A[0]:(0.589819014072) A[1]:(0.655744433403) A[2]:(-0.000503659190144) A[3]:(0.531385779381)\n",
      " state (5)  A[0]:(-0.028785277158) A[1]:(0.999896883965) A[2]:(-0.831161737442) A[3]:(0.674591422081)\n",
      " state (6)  A[0]:(-0.00131104816683) A[1]:(0.810243844986) A[2]:(-0.00143527891487) A[3]:(0.654674530029)\n",
      " state (7)  A[0]:(0.53913974762) A[1]:(-0.530538082123) A[2]:(0.441643416882) A[3]:(0.866118073463)\n",
      " state (8)  A[0]:(0.654463410378) A[1]:(0.00094908446772) A[2]:(0.728907585144) A[3]:(0.588729500771)\n",
      " state (9)  A[0]:(0.653912186623) A[1]:(0.810238242149) A[2]:(0.810145616531) A[3]:(-0.00223162397742)\n",
      " state (10)  A[0]:(0.727099657059) A[1]:(0.900044381618) A[2]:(-0.000483393640025) A[3]:(0.72786796093)\n",
      " state (11)  A[0]:(0.133011937141) A[1]:(0.883412718773) A[2]:(-0.932949244976) A[3]:(0.803524494171)\n",
      " state (12)  A[0]:(-0.431075572968) A[1]:(0.816605865955) A[2]:(-0.959254980087) A[3]:(0.714636325836)\n",
      " state (13)  A[0]:(-0.00376248080283) A[1]:(0.808971524239) A[2]:(0.899987518787) A[3]:(0.728325784206)\n",
      " state (14)  A[0]:(0.809251189232) A[1]:(0.900264203548) A[2]:(0.999999880791) A[3]:(0.809575080872)\n",
      " state (15)  A[0]:(0.979775428772) A[1]:(0.937130153179) A[2]:(1.0) A[3]:(0.8778668046)\n",
      "Episode 792000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6005. Times reached goal: 997.               Steps done: 5417298. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00399521647137.\n",
      " state (0)  A[0]:(0.531488895416) A[1]:(0.59078168869) A[2]:(0.590387582779) A[3]:(0.531485080719)\n",
      " state (1)  A[0]:(0.531629800797) A[1]:(-1.34706497192e-05) A[2]:(0.656229913235) A[3]:(0.590480625629)\n",
      " state (2)  A[0]:(0.59057700634) A[1]:(0.729172646999) A[2]:(0.590708732605) A[3]:(0.656052708626)\n",
      " state (3)  A[0]:(0.656349718571) A[1]:(-0.0202848501503) A[2]:(0.532978475094) A[3]:(0.54435646534)\n",
      " state (4)  A[0]:(0.590417504311) A[1]:(0.656487345695) A[2]:(0.00019645690918) A[3]:(0.531589031219)\n",
      " state (5)  A[0]:(-0.0275606587529) A[1]:(0.999896943569) A[2]:(-0.830720484257) A[3]:(0.675565004349)\n",
      " state (6)  A[0]:(-3.33935022354e-05) A[1]:(0.810147285461) A[2]:(0.000309944152832) A[3]:(0.656190991402)\n",
      " state (7)  A[0]:(0.540191054344) A[1]:(-0.53074914217) A[2]:(0.442855417728) A[3]:(0.866872549057)\n",
      " state (8)  A[0]:(0.655945599079) A[1]:(0.000357747048838) A[2]:(0.729103922844) A[3]:(0.590810775757)\n",
      " state (9)  A[0]:(0.656022906303) A[1]:(0.810087442398) A[2]:(0.810065031052) A[3]:(0.000205516815186)\n",
      " state (10)  A[0]:(0.729315519333) A[1]:(0.900017738342) A[2]:(-0.000236988067627) A[3]:(0.729172348976)\n",
      " state (11)  A[0]:(0.138066500425) A[1]:(0.883454740047) A[2]:(-0.932898283005) A[3]:(0.804644942284)\n",
      " state (12)  A[0]:(-0.427680701017) A[1]:(0.816762566566) A[2]:(-0.959264039993) A[3]:(0.71596455574)\n",
      " state (13)  A[0]:(-0.00085762119852) A[1]:(0.809195756912) A[2]:(0.900053977966) A[3]:(0.729313492775)\n",
      " state (14)  A[0]:(0.809987783432) A[1]:(0.900374114513) A[2]:(0.999999880791) A[3]:(0.810177922249)\n",
      " state (15)  A[0]:(0.979855656624) A[1]:(0.937145888805) A[2]:(1.0) A[3]:(0.878246188164)\n",
      "Episode 793000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6018. Times reached goal: 996.               Steps done: 5423316. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00397124545976.\n",
      " state (0)  A[0]:(0.531470835209) A[1]:(0.590199828148) A[2]:(0.59056532383) A[3]:(0.531489789486)\n",
      " state (1)  A[0]:(0.531407475471) A[1]:(5.89787960052e-05) A[2]:(0.655970931053) A[3]:(0.590458214283)\n",
      " state (2)  A[0]:(0.590286076069) A[1]:(0.72897875309) A[2]:(0.590456604958) A[3]:(0.655961871147)\n",
      " state (3)  A[0]:(0.656070232391) A[1]:(-0.0206058956683) A[2]:(0.532706141472) A[3]:(0.544212341309)\n",
      " state (4)  A[0]:(0.590096235275) A[1]:(0.656441867352) A[2]:(-0.000190615653992) A[3]:(0.53142875433)\n",
      " state (5)  A[0]:(-0.0278609991074) A[1]:(0.999896883965) A[2]:(-0.830803334713) A[3]:(0.675383090973)\n",
      " state (6)  A[0]:(1.92224979401e-06) A[1]:(0.810052752495) A[2]:(0.000175476074219) A[3]:(0.655787229538)\n",
      " state (7)  A[0]:(0.540283203125) A[1]:(-0.530697286129) A[2]:(0.442749023438) A[3]:(0.866665840149)\n",
      " state (8)  A[0]:(0.655951142311) A[1]:(0.000653833034448) A[2]:(0.729074835777) A[3]:(0.590253591537)\n",
      " state (9)  A[0]:(0.655985355377) A[1]:(0.81016767025) A[2]:(0.810011327267) A[3]:(-0.000221878290176)\n",
      " state (10)  A[0]:(0.729072570801) A[1]:(0.900002002716) A[2]:(-0.000445246667368) A[3]:(0.728916049004)\n",
      " state (11)  A[0]:(0.13732406497) A[1]:(0.883354723454) A[2]:(-0.932933390141) A[3]:(0.804376780987)\n",
      " state (12)  A[0]:(-0.427928954363) A[1]:(0.816491007805) A[2]:(-0.95928633213) A[3]:(0.715645074844)\n",
      " state (13)  A[0]:(-0.000510945857968) A[1]:(0.80882370472) A[2]:(0.899992763996) A[3]:(0.729091525078)\n",
      " state (14)  A[0]:(0.810238778591) A[1]:(0.900181293488) A[2]:(0.999999880791) A[3]:(0.81002497673)\n",
      " state (15)  A[0]:(0.979883134365) A[1]:(0.937063753605) A[2]:(1.0) A[3]:(0.87812101841)\n",
      "Episode 794000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6002. Times reached goal: 993.               Steps done: 5429318. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0039474814317.\n",
      "q_values \n",
      "tensor([[ 0.5320,  0.5908,  0.5906,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5911,  0.6559,  0.0001,  0.5311]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6565, -0.0002,  0.7290,  0.5909]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.8100,  0.8102,  0.0012]], device='cuda:0')\n",
      "On state=9, selected action=2 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7298,  0.9000,  0.0005,  0.7296]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8098,  0.9005,  1.0000,  0.8099]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531584739685) A[1]:(0.590898871422) A[2]:(0.590648293495) A[3]:(0.531337141991)\n",
      " state (1)  A[0]:(0.531676709652) A[1]:(-0.000174403190613) A[2]:(0.65634906292) A[3]:(0.590327143669)\n",
      " state (2)  A[0]:(0.590686917305) A[1]:(0.729188740253) A[2]:(0.590713083744) A[3]:(0.655930399895)\n",
      " state (3)  A[0]:(0.656517982483) A[1]:(-0.0204619206488) A[2]:(0.532872617245) A[3]:(0.544068038464)\n",
      " state (4)  A[0]:(0.590795040131) A[1]:(0.655885636806) A[2]:(2.07424163818e-05) A[3]:(0.531272888184)\n",
      " state (5)  A[0]:(-0.0267473571002) A[1]:(0.99989682436) A[2]:(-0.830927491188) A[3]:(0.675541043282)\n",
      " state (6)  A[0]:(0.000899985199794) A[1]:(0.810016274452) A[2]:(-0.00038111206959) A[3]:(0.656041145325)\n",
      " state (7)  A[0]:(0.540729880333) A[1]:(-0.531237483025) A[2]:(0.442526638508) A[3]:(0.8667075634)\n",
      " state (8)  A[0]:(0.656127750874) A[1]:(-0.000205159187317) A[2]:(0.729017734528) A[3]:(0.59035462141)\n",
      " state (9)  A[0]:(0.656102776527) A[1]:(0.809889137745) A[2]:(0.810003221035) A[3]:(0.000287145376205)\n",
      " state (10)  A[0]:(0.7292740345) A[1]:(0.899944841862) A[2]:(-0.000469088525278) A[3]:(0.729133248329)\n",
      " state (11)  A[0]:(0.137858167291) A[1]:(0.883447825909) A[2]:(-0.932972431183) A[3]:(0.804477214813)\n",
      " state (12)  A[0]:(-0.427848517895) A[1]:(0.816847085953) A[2]:(-0.959338605404) A[3]:(0.715616583824)\n",
      " state (13)  A[0]:(-0.00113770319149) A[1]:(0.809330642223) A[2]:(0.899969577789) A[3]:(0.728959083557)\n",
      " state (14)  A[0]:(0.809812545776) A[1]:(0.90046954155) A[2]:(0.999999880791) A[3]:(0.809925198555)\n",
      " state (15)  A[0]:(0.979831278324) A[1]:(0.937207520008) A[2]:(1.0) A[3]:(0.878080606461)\n",
      "Episode 795000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6012. Times reached goal: 994.               Steps done: 5435330. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00392382036975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.532448291779) A[1]:(0.589743793011) A[2]:(0.590681433678) A[3]:(0.530992627144)\n",
      " state (1)  A[0]:(0.532675683498) A[1]:(0.000131949782372) A[2]:(0.656141042709) A[3]:(0.59034204483)\n",
      " state (2)  A[0]:(0.591494500637) A[1]:(0.728727340698) A[2]:(0.590435147285) A[3]:(0.655684173107)\n",
      " state (3)  A[0]:(0.656926095486) A[1]:(-0.0212405119091) A[2]:(0.532495737076) A[3]:(0.543586015701)\n",
      " state (4)  A[0]:(0.590882182121) A[1]:(0.655780851841) A[2]:(-0.000692129018717) A[3]:(0.530527949333)\n",
      " state (5)  A[0]:(-0.0273157190531) A[1]:(0.999896645546) A[2]:(-0.831160068512) A[3]:(0.674592018127)\n",
      " state (6)  A[0]:(-0.000255972146988) A[1]:(0.809886217117) A[2]:(-0.00120973528828) A[3]:(0.654800772667)\n",
      " state (7)  A[0]:(0.539698123932) A[1]:(-0.529891729355) A[2]:(0.441489011049) A[3]:(0.866142570972)\n",
      " state (8)  A[0]:(0.654651880264) A[1]:(0.00216017337516) A[2]:(0.728647947311) A[3]:(0.58815407753)\n",
      " state (9)  A[0]:(0.654081702232) A[1]:(0.810478925705) A[2]:(0.809706389904) A[3]:(-0.00341554149054)\n",
      " state (10)  A[0]:(0.72720015049) A[1]:(0.900046467781) A[2]:(-0.00205242354423) A[3]:(0.72705334425)\n",
      " state (11)  A[0]:(0.133452057838) A[1]:(0.883216977119) A[2]:(-0.93322712183) A[3]:(0.802685558796)\n",
      " state (12)  A[0]:(-0.430371135473) A[1]:(0.815971195698) A[2]:(-0.95943158865) A[3]:(0.713424563408)\n",
      " state (13)  A[0]:(-0.00338533706963) A[1]:(0.807997465134) A[2]:(0.900010347366) A[3]:(0.727329492569)\n",
      " state (14)  A[0]:(0.808750271797) A[1]:(0.899594902992) A[2]:(0.999999880791) A[3]:(0.809003472328)\n",
      " state (15)  A[0]:(0.9796448946) A[1]:(0.936601519585) A[2]:(1.0) A[3]:(0.877577662468)\n",
      "Episode 796000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6014. Times reached goal: 997.               Steps done: 5441344. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00390029333077.\n",
      " state (0)  A[0]:(0.531865239143) A[1]:(0.590704202652) A[2]:(0.590380907059) A[3]:(0.531928181648)\n",
      " state (1)  A[0]:(0.531866192818) A[1]:(-0.000341609120369) A[2]:(0.656417131424) A[3]:(0.590641856194)\n",
      " state (2)  A[0]:(0.590772867203) A[1]:(0.729086518288) A[2]:(0.591063141823) A[3]:(0.656120538712)\n",
      " state (3)  A[0]:(0.65649497509) A[1]:(-0.0206357296556) A[2]:(0.533351063728) A[3]:(0.544213056564)\n",
      " state (4)  A[0]:(0.590545475483) A[1]:(0.656233668327) A[2]:(0.000584602297749) A[3]:(0.531433999538)\n",
      " state (5)  A[0]:(-0.0273960065097) A[1]:(0.99989682436) A[2]:(-0.830732941628) A[3]:(0.675829768181)\n",
      " state (6)  A[0]:(0.000331372022629) A[1]:(0.809984445572) A[2]:(0.000498056353536) A[3]:(0.656423091888)\n",
      " state (7)  A[0]:(0.54032766819) A[1]:(-0.530981898308) A[2]:(0.44320243597) A[3]:(0.866890847683)\n",
      " state (8)  A[0]:(0.656151533127) A[1]:(4.51058149338e-05) A[2]:(0.729176878929) A[3]:(0.59088408947)\n",
      " state (9)  A[0]:(0.656265377998) A[1]:(0.810016274452) A[2]:(0.81000238657) A[3]:(0.00010472536087)\n",
      " state (10)  A[0]:(0.729401707649) A[1]:(0.899993777275) A[2]:(-0.00064873683732) A[3]:(0.728997468948)\n",
      " state (11)  A[0]:(0.1384242028) A[1]:(0.883435964584) A[2]:(-0.933054447174) A[3]:(0.8044911623)\n",
      " state (12)  A[0]:(-0.427021950483) A[1]:(0.816683590412) A[2]:(-0.959424138069) A[3]:(0.715823292732)\n",
      " state (13)  A[0]:(-4.97251749039e-05) A[1]:(0.808993220329) A[2]:(0.899837434292) A[3]:(0.729265093803)\n",
      " state (14)  A[0]:(0.810038983822) A[1]:(0.900211751461) A[2]:(0.999999880791) A[3]:(0.810143351555)\n",
      " state (15)  A[0]:(0.979840278625) A[1]:(0.936997890472) A[2]:(1.0) A[3]:(0.878215193748)\n",
      "Episode 797000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6019. Times reached goal: 998.               Steps done: 5447363. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00387688797429.\n",
      " state (0)  A[0]:(0.526168465614) A[1]:(0.590439498425) A[2]:(0.590499341488) A[3]:(0.529272794724)\n",
      " state (1)  A[0]:(0.526695489883) A[1]:(-0.00147330656182) A[2]:(0.656113505363) A[3]:(0.586806893349)\n",
      " state (2)  A[0]:(0.586187720299) A[1]:(0.729073822498) A[2]:(0.590275764465) A[3]:(0.652479529381)\n",
      " state (3)  A[0]:(0.652635991573) A[1]:(-0.0202438589185) A[2]:(0.532246351242) A[3]:(0.539458155632)\n",
      " state (4)  A[0]:(0.586562752724) A[1]:(0.656587362289) A[2]:(-0.00118577422109) A[3]:(0.526303827763)\n",
      " state (5)  A[0]:(-0.0321796387434) A[1]:(0.999896883965) A[2]:(-0.83128452301) A[3]:(0.671438753605)\n",
      " state (6)  A[0]:(-0.00295824441127) A[1]:(0.810061514378) A[2]:(-0.00160491326824) A[3]:(0.651152849197)\n",
      " state (7)  A[0]:(0.538798809052) A[1]:(-0.529639959335) A[2]:(0.441384762526) A[3]:(0.864141821861)\n",
      " state (8)  A[0]:(0.655073523521) A[1]:(0.00354252755642) A[2]:(0.729005992413) A[3]:(0.580220162868)\n",
      " state (9)  A[0]:(0.659135699272) A[1]:(0.810388922691) A[2]:(0.809815108776) A[3]:(-0.00975813250989)\n",
      " state (10)  A[0]:(0.735128521919) A[1]:(0.899852514267) A[2]:(0.000351905822754) A[3]:(0.72659933567)\n",
      " state (11)  A[0]:(0.156550616026) A[1]:(0.883011639118) A[2]:(-0.932721734047) A[3]:(0.803490936756)\n",
      " state (12)  A[0]:(-0.408607631922) A[1]:(0.815640687943) A[2]:(-0.959273576736) A[3]:(0.714742779732)\n",
      " state (13)  A[0]:(0.0241442620754) A[1]:(0.807624459267) A[2]:(0.899783492088) A[3]:(0.728242397308)\n",
      " state (14)  A[0]:(0.81830072403) A[1]:(0.899534523487) A[2]:(0.999999880791) A[3]:(0.809369206429)\n",
      " state (15)  A[0]:(0.980741500854) A[1]:(0.936775326729) A[2]:(1.0) A[3]:(0.877611815929)\n",
      "Episode 798000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6008. Times reached goal: 997.               Steps done: 5453371. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00385366546162.\n",
      " state (0)  A[0]:(0.531093478203) A[1]:(0.590617895126) A[2]:(0.590427994728) A[3]:(0.531432032585)\n",
      " state (1)  A[0]:(0.531296610832) A[1]:(0.000118285417557) A[2]:(0.65611743927) A[3]:(0.590846061707)\n",
      " state (2)  A[0]:(0.590483546257) A[1]:(0.729089736938) A[2]:(0.590537667274) A[3]:(0.656545162201)\n",
      " state (3)  A[0]:(0.65636909008) A[1]:(-0.0204183980823) A[2]:(0.53273576498) A[3]:(0.544892132282)\n",
      " state (4)  A[0]:(0.590655505657) A[1]:(0.656214058399) A[2]:(-0.000226616859436) A[3]:(0.532423257828)\n",
      " state (5)  A[0]:(-0.0267350878567) A[1]:(0.999896883965) A[2]:(-0.831021666527) A[3]:(0.676986336708)\n",
      " state (6)  A[0]:(0.00156663230155) A[1]:(0.810112595558) A[2]:(-0.000708699109964) A[3]:(0.657628536224)\n",
      " state (7)  A[0]:(0.541668534279) A[1]:(-0.530675888062) A[2]:(0.44229605794) A[3]:(0.867539048195)\n",
      " state (8)  A[0]:(0.657586455345) A[1]:(0.000906020170078) A[2]:(0.729045033455) A[3]:(0.593394935131)\n",
      " state (9)  A[0]:(0.658900976181) A[1]:(0.810254096985) A[2]:(0.810288906097) A[3]:(0.00739274872467)\n",
      " state (10)  A[0]:(0.732774972916) A[1]:(0.9000633955) A[2]:(0.0019369100919) A[3]:(0.733141541481)\n",
      " state (11)  A[0]:(0.146519616246) A[1]:(0.883426010609) A[2]:(-0.932634830475) A[3]:(0.807409405708)\n",
      " state (12)  A[0]:(-0.421644806862) A[1]:(0.816535294056) A[2]:(-0.959263563156) A[3]:(0.718749403954)\n",
      " state (13)  A[0]:(0.00427204743028) A[1]:(0.80878329277) A[2]:(0.900199115276) A[3]:(0.73113411665)\n",
      " state (14)  A[0]:(0.810935497284) A[1]:(0.90011960268) A[2]:(0.999999880791) A[3]:(0.811147928238)\n",
      " state (15)  A[0]:(0.979897201061) A[1]:(0.936950266361) A[2]:(1.0) A[3]:(0.878757476807)\n",
      "Episode 799000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6007. Times reached goal: 996.               Steps done: 5459378. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00383058588211.\n",
      "q_values \n",
      "tensor([[ 0.5305,  0.5904,  0.5905,  0.5312]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5307, -0.0003,  0.6560,  0.5901]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5897,  0.7290,  0.5905,  0.6557]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[-0.0004,  0.8099, -0.0002,  0.6555]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7290,  0.9001,  0.0002,  0.7284]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8104,  0.9002,  1.0000,  0.8095]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.530271887779) A[1]:(0.590436160564) A[2]:(0.590337634087) A[3]:(0.532108783722)\n",
      " state (1)  A[0]:(0.530600309372) A[1]:(-0.000777557317633) A[2]:(0.656108438969) A[3]:(0.590663552284)\n",
      " state (2)  A[0]:(0.589673399925) A[1]:(0.728969693184) A[2]:(0.590621948242) A[3]:(0.656103372574)\n",
      " state (3)  A[0]:(0.655602931976) A[1]:(-0.0206372793764) A[2]:(0.532889723778) A[3]:(0.544094622135)\n",
      " state (4)  A[0]:(0.589706301689) A[1]:(0.656200289726) A[2]:(0.000100493431091) A[3]:(0.531245470047)\n",
      " state (5)  A[0]:(-0.0282579511404) A[1]:(0.99989682436) A[2]:(-0.830832958221) A[3]:(0.675620913506)\n",
      " state (6)  A[0]:(-0.000395953626139) A[1]:(0.810024142265) A[2]:(0.000215530395508) A[3]:(0.655686199665)\n",
      " state (7)  A[0]:(0.539900124073) A[1]:(-0.530768454075) A[2]:(0.442938774824) A[3]:(0.866529345512)\n",
      " state (8)  A[0]:(0.655962884426) A[1]:(7.43120908737e-05) A[2]:(0.729032516479) A[3]:(0.590774297714)\n",
      " state (9)  A[0]:(0.655610799789) A[1]:(0.810113608837) A[2]:(0.810011982918) A[3]:(-0.000612556876149)\n",
      " state (10)  A[0]:(0.728390693665) A[1]:(0.900055468082) A[2]:(-0.00106310809497) A[3]:(0.728053331375)\n",
      " state (11)  A[0]:(0.135950312018) A[1]:(0.88349366188) A[2]:(-0.933219671249) A[3]:(0.803448200226)\n",
      " state (12)  A[0]:(-0.428383529186) A[1]:(0.816745638847) A[2]:(-0.959537982941) A[3]:(0.714354157448)\n",
      " state (13)  A[0]:(-0.000400155753596) A[1]:(0.809040307999) A[2]:(0.899869859219) A[3]:(0.728077292442)\n",
      " state (14)  A[0]:(0.810321688652) A[1]:(0.900256574154) A[2]:(0.999999880791) A[3]:(0.809454739094)\n",
      " state (15)  A[0]:(0.979905664921) A[1]:(0.937017261982) A[2]:(1.0) A[3]:(0.877840280533)\n",
      "Episode 800000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 996.               Steps done: 5465386. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00380764071842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531245589256) A[1]:(0.590248227119) A[2]:(0.590510249138) A[3]:(0.531373739243)\n",
      " state (1)  A[0]:(0.53118622303) A[1]:(2.67773866653e-05) A[2]:(0.655946791172) A[3]:(0.590659976006)\n",
      " state (2)  A[0]:(0.590116977692) A[1]:(0.728897809982) A[2]:(0.590414762497) A[3]:(0.656259059906)\n",
      " state (3)  A[0]:(0.655987203121) A[1]:(-0.0207346007228) A[2]:(0.532608747482) A[3]:(0.544418871403)\n",
      " state (4)  A[0]:(0.590229094028) A[1]:(0.65579533577) A[2]:(-0.000214695930481) A[3]:(0.531702518463)\n",
      " state (5)  A[0]:(-0.0274378918111) A[1]:(0.999896705151) A[2]:(-0.830958545208) A[3]:(0.67604213953)\n",
      " state (6)  A[0]:(0.000287756323814) A[1]:(0.80989664793) A[2]:(-0.000413894624216) A[3]:(0.65612077713)\n",
      " state (7)  A[0]:(0.540283858776) A[1]:(-0.530588746071) A[2]:(0.442201793194) A[3]:(0.866740822792)\n",
      " state (8)  A[0]:(0.655919253826) A[1]:(0.000496461929288) A[2]:(0.728733897209) A[3]:(0.5910384655)\n",
      " state (9)  A[0]:(0.655745387077) A[1]:(0.810122072697) A[2]:(0.809895157814) A[3]:(0.00101140106563)\n",
      " state (10)  A[0]:(0.728936553001) A[1]:(0.900005280972) A[2]:(-0.000649094465189) A[3]:(0.729445278645)\n",
      " state (11)  A[0]:(0.137367501855) A[1]:(0.88338971138) A[2]:(-0.933131217957) A[3]:(0.804680585861)\n",
      " state (12)  A[0]:(-0.427994072437) A[1]:(0.816521883011) A[2]:(-0.959523499012) A[3]:(0.715798974037)\n",
      " state (13)  A[0]:(-0.000852048164234) A[1]:(0.808776259422) A[2]:(0.900008916855) A[3]:(0.729129314423)\n",
      " state (14)  A[0]:(0.810039103031) A[1]:(0.900101423264) A[2]:(0.999999880791) A[3]:(0.810060739517)\n",
      " state (15)  A[0]:(0.979867577553) A[1]:(0.936887443066) A[2]:(1.0) A[3]:(0.878189086914)\n",
      "Episode 801000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 995.               Steps done: 5471396. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00378482542633.\n",
      " state (0)  A[0]:(0.531774401665) A[1]:(0.590485930443) A[2]:(0.590532839298) A[3]:(0.531304359436)\n",
      " state (1)  A[0]:(0.531678080559) A[1]:(-2.23517417908e-06) A[2]:(0.656083106995) A[3]:(0.590446054935)\n",
      " state (2)  A[0]:(0.590615272522) A[1]:(0.728989839554) A[2]:(0.590579867363) A[3]:(0.656070232391)\n",
      " state (3)  A[0]:(0.656460762024) A[1]:(-0.0203949231654) A[2]:(0.532844364643) A[3]:(0.544208526611)\n",
      " state (4)  A[0]:(0.590742468834) A[1]:(0.655948638916) A[2]:(0.000251412391663) A[3]:(0.531646430492)\n",
      " state (5)  A[0]:(-0.0270307809114) A[1]:(0.999896764755) A[2]:(-0.830806314945) A[3]:(0.676396906376)\n",
      " state (6)  A[0]:(0.000134721398354) A[1]:(0.810007989407) A[2]:(0.000108599662781) A[3]:(0.656169295311)\n",
      " state (7)  A[0]:(0.539777159691) A[1]:(-0.530707240105) A[2]:(0.442735999823) A[3]:(0.866522848606)\n",
      " state (8)  A[0]:(0.655622839928) A[1]:(7.19130039215e-05) A[2]:(0.728939950466) A[3]:(0.590448617935)\n",
      " state (9)  A[0]:(0.655724048615) A[1]:(0.809998869896) A[2]:(0.809996128082) A[3]:(0.000317066907883)\n",
      " state (10)  A[0]:(0.729034543037) A[1]:(0.900018692017) A[2]:(-0.000196218490601) A[3]:(0.729059636593)\n",
      " state (11)  A[0]:(0.13788048923) A[1]:(0.883528828621) A[2]:(-0.933085203171) A[3]:(0.804465889931)\n",
      " state (12)  A[0]:(-0.427556872368) A[1]:(0.816854000092) A[2]:(-0.959530293941) A[3]:(0.715591073036)\n",
      " state (13)  A[0]:(-0.000548705400433) A[1]:(0.809148073196) A[2]:(0.899990558624) A[3]:(0.72900468111)\n",
      " state (14)  A[0]:(0.810030400753) A[1]:(0.90029489994) A[2]:(0.999999880791) A[3]:(0.810024142265)\n",
      " state (15)  A[0]:(0.979861557484) A[1]:(0.93699079752) A[2]:(1.0) A[3]:(0.878197968006)\n",
      "Episode 802000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6007. Times reached goal: 991.               Steps done: 5477403. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00376215812938.\n",
      " state (0)  A[0]:(0.531251788139) A[1]:(0.590515255928) A[2]:(0.590442895889) A[3]:(0.531627058983)\n",
      " state (1)  A[0]:(0.531522870064) A[1]:(-0.000259354710579) A[2]:(0.656128048897) A[3]:(0.590647935867)\n",
      " state (2)  A[0]:(0.590469002724) A[1]:(0.728971123695) A[2]:(0.590638279915) A[3]:(0.656338810921)\n",
      " state (3)  A[0]:(0.656300902367) A[1]:(-0.020687205717) A[2]:(0.532838106155) A[3]:(0.544357419014)\n",
      " state (4)  A[0]:(0.590544223785) A[1]:(0.65601670742) A[2]:(1.19209289551e-06) A[3]:(0.531590402126)\n",
      " state (5)  A[0]:(-0.027013970539) A[1]:(0.999896764755) A[2]:(-0.830893993378) A[3]:(0.676120400429)\n",
      " state (6)  A[0]:(0.000475436419947) A[1]:(0.809930026531) A[2]:(7.42673873901e-05) A[3]:(0.655914247036)\n",
      " state (7)  A[0]:(0.540176868439) A[1]:(-0.530673503876) A[2]:(0.44279897213) A[3]:(0.866405904293)\n",
      " state (8)  A[0]:(0.656182348728) A[1]:(0.000281855463982) A[2]:(0.728986442089) A[3]:(0.589648604393)\n",
      " state (9)  A[0]:(0.656523108482) A[1]:(0.80998301506) A[2]:(0.809841811657) A[3]:(-0.00121805013623)\n",
      " state (10)  A[0]:(0.729310929775) A[1]:(0.899953067303) A[2]:(-0.00165235844906) A[3]:(0.728178024292)\n",
      " state (11)  A[0]:(0.137992978096) A[1]:(0.883377254009) A[2]:(-0.933362305164) A[3]:(0.803793311119)\n",
      " state (12)  A[0]:(-0.42666515708) A[1]:(0.816515982151) A[2]:(-0.959663629532) A[3]:(0.71509873867)\n",
      " state (13)  A[0]:(0.00156015029643) A[1]:(0.808724105358) A[2]:(0.899894475937) A[3]:(0.728986382484)\n",
      " state (14)  A[0]:(0.81081032753) A[1]:(0.900064170361) A[2]:(0.999999880791) A[3]:(0.810207784176)\n",
      " state (15)  A[0]:(0.979937553406) A[1]:(0.936856865883) A[2]:(1.0) A[3]:(0.87837511301)\n",
      "Episode 803000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6022. Times reached goal: 997.               Steps done: 5483425. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00373957049276.\n",
      " state (0)  A[0]:(0.53134572506) A[1]:(0.590629637241) A[2]:(0.590671479702) A[3]:(0.53190767765)\n",
      " state (1)  A[0]:(0.531261205673) A[1]:(0.00015489757061) A[2]:(0.656135320663) A[3]:(0.590389430523)\n",
      " state (2)  A[0]:(0.590170264244) A[1]:(0.728946089745) A[2]:(0.590859949589) A[3]:(0.655796825886)\n",
      " state (3)  A[0]:(0.656016111374) A[1]:(-0.0207273624837) A[2]:(0.533124566078) A[3]:(0.54373216629)\n",
      " state (4)  A[0]:(0.590167403221) A[1]:(0.656598806381) A[2]:(0.000131607055664) A[3]:(0.531333327293)\n",
      " state (5)  A[0]:(-0.0271829739213) A[1]:(0.999897003174) A[2]:(-0.830947518349) A[3]:(0.676794290543)\n",
      " state (6)  A[0]:(0.000368103355868) A[1]:(0.810164690018) A[2]:(0.000109314918518) A[3]:(0.657119989395)\n",
      " state (7)  A[0]:(0.539974689484) A[1]:(-0.53078019619) A[2]:(0.443162381649) A[3]:(0.867003679276)\n",
      " state (8)  A[0]:(0.655915796757) A[1]:(3.21120023727e-05) A[2]:(0.729153156281) A[3]:(0.591280460358)\n",
      " state (9)  A[0]:(0.655866146088) A[1]:(0.810060858727) A[2]:(0.810079336166) A[3]:(7.00354576111e-06)\n",
      " state (10)  A[0]:(0.728838145733) A[1]:(0.900015175343) A[2]:(-0.000531196536031) A[3]:(0.728692531586)\n",
      " state (11)  A[0]:(0.136988282204) A[1]:(0.883449554443) A[2]:(-0.933239459991) A[3]:(0.804173529148)\n",
      " state (12)  A[0]:(-0.428233534098) A[1]:(0.816638350487) A[2]:(-0.959648370743) A[3]:(0.715343952179)\n",
      " state (13)  A[0]:(-0.00129552115686) A[1]:(0.808897972107) A[2]:(0.899973988533) A[3]:(0.728934049606)\n",
      " state (14)  A[0]:(0.80965590477) A[1]:(0.900187730789) A[2]:(0.999999880791) A[3]:(0.810030341148)\n",
      " state (15)  A[0]:(0.979801356792) A[1]:(0.936918079853) A[2]:(1.0) A[3]:(0.878218829632)\n",
      "Episode 804000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6012. Times reached goal: 999.               Steps done: 5489437. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00371715564152.\n",
      "q_values \n",
      "tensor([[ 0.5314,  0.5904,  0.5908,  0.5322]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5308,  0.0002,  0.6558,  0.5910]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5897,  0.7290,  0.5901,  0.6564]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0001,  0.8102,  0.0001,  0.6560]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7297,  0.9001,  0.0005,  0.7291]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8103,  0.9000,  1.0000,  0.8097]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531445860863) A[1]:(0.590240120888) A[2]:(0.590709328651) A[3]:(0.532104849815)\n",
      " state (1)  A[0]:(0.530977725983) A[1]:(-0.000171735882759) A[2]:(0.655841529369) A[3]:(0.590902268887)\n",
      " state (2)  A[0]:(0.589899897575) A[1]:(0.728931069374) A[2]:(0.5901927948) A[3]:(0.656337201595)\n",
      " state (3)  A[0]:(0.655783176422) A[1]:(-0.0208021942526) A[2]:(0.532342791557) A[3]:(0.544330239296)\n",
      " state (4)  A[0]:(0.589984893799) A[1]:(0.656087040901) A[2]:(-0.000710487249307) A[3]:(0.531585097313)\n",
      " state (5)  A[0]:(-0.0274054314941) A[1]:(0.999896764755) A[2]:(-0.831079244614) A[3]:(0.676169872284)\n",
      " state (6)  A[0]:(0.000675335410051) A[1]:(0.809892773628) A[2]:(2.86102294922e-06) A[3]:(0.656104564667)\n",
      " state (7)  A[0]:(0.540624320507) A[1]:(-0.53099489212) A[2]:(0.443025410175) A[3]:(0.866680622101)\n",
      " state (8)  A[0]:(0.65685749054) A[1]:(-0.000383928389056) A[2]:(0.728949487209) A[3]:(0.591577529907)\n",
      " state (9)  A[0]:(0.656958281994) A[1]:(0.809976816177) A[2]:(0.810029566288) A[3]:(0.001048504957)\n",
      " state (10)  A[0]:(0.730020523071) A[1]:(0.899959146976) A[2]:(-2.13384628296e-05) A[3]:(0.72930842638)\n",
      " state (11)  A[0]:(0.139810979366) A[1]:(0.883329629898) A[2]:(-0.933161735535) A[3]:(0.804604232311)\n",
      " state (12)  A[0]:(-0.426165610552) A[1]:(0.816362440586) A[2]:(-0.959666311741) A[3]:(0.71561986208)\n",
      " state (13)  A[0]:(0.000821515743155) A[1]:(0.808544933796) A[2]:(0.89977902174) A[3]:(0.728859782219)\n",
      " state (14)  A[0]:(0.810311734676) A[1]:(0.900014460087) A[2]:(0.999999880791) A[3]:(0.809762477875)\n",
      " state (15)  A[0]:(0.979869663715) A[1]:(0.936843693256) A[2]:(1.0) A[3]:(0.877936422825)\n",
      "Episode 805000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 999.               Steps done: 5495445. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00369488992361.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531782627106) A[1]:(0.590425372124) A[2]:(0.590409874916) A[3]:(0.531527996063)\n",
      " state (1)  A[0]:(0.531620979309) A[1]:(-8.3789229393e-05) A[2]:(0.656067728996) A[3]:(0.590474367142)\n",
      " state (2)  A[0]:(0.590593457222) A[1]:(0.729003548622) A[2]:(0.59060972929) A[3]:(0.656096637249)\n",
      " state (3)  A[0]:(0.656408667564) A[1]:(-0.0206706728786) A[2]:(0.532915472984) A[3]:(0.544050931931)\n",
      " state (4)  A[0]:(0.590609192848) A[1]:(0.655917286873) A[2]:(0.000186443328857) A[3]:(0.531324148178)\n",
      " state (5)  A[0]:(-0.0273588709533) A[1]:(0.999896764755) A[2]:(-0.830906927586) A[3]:(0.675995707512)\n",
      " state (6)  A[0]:(3.44216823578e-06) A[1]:(0.809998512268) A[2]:(0.000225424766541) A[3]:(0.655888676643)\n",
      " state (7)  A[0]:(0.539815545082) A[1]:(-0.530688226223) A[2]:(0.443028062582) A[3]:(0.866484880447)\n",
      " state (8)  A[0]:(0.655658125877) A[1]:(0.000107660889626) A[2]:(0.729053616524) A[3]:(0.590305089951)\n",
      " state (9)  A[0]:(0.655748724937) A[1]:(0.810020089149) A[2]:(0.810093045235) A[3]:(-0.000418752402766)\n",
      " state (10)  A[0]:(0.729477763176) A[1]:(0.89999216795) A[2]:(0.000146985054016) A[3]:(0.728966116905)\n",
      " state (11)  A[0]:(0.139868751168) A[1]:(0.88344424963) A[2]:(-0.933133244514) A[3]:(0.804569602013)\n",
      " state (12)  A[0]:(-0.425564199686) A[1]:(0.816650032997) A[2]:(-0.959634304047) A[3]:(0.715720593929)\n",
      " state (13)  A[0]:(0.00130282272585) A[1]:(0.808948636055) A[2]:(0.900043606758) A[3]:(0.728962063789)\n",
      " state (14)  A[0]:(0.810107350349) A[1]:(0.900284767151) A[2]:(0.999999880791) A[3]:(0.809786379337)\n",
      " state (15)  A[0]:(0.979802250862) A[1]:(0.937033057213) A[2]:(1.0) A[3]:(0.87790709734)\n",
      "Episode 806000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6010. Times reached goal: 997.               Steps done: 5501455. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00367275023159.\n",
      " state (0)  A[0]:(0.531447291374) A[1]:(0.590346932411) A[2]:(0.590440392494) A[3]:(0.531270742416)\n",
      " state (1)  A[0]:(0.531655669212) A[1]:(5.11109828949e-06) A[2]:(0.656014382839) A[3]:(0.590436339378)\n",
      " state (2)  A[0]:(0.59047472477) A[1]:(0.728982925415) A[2]:(0.590587437153) A[3]:(0.656030774117)\n",
      " state (3)  A[0]:(0.656185150146) A[1]:(-0.0208816118538) A[2]:(0.532867193222) A[3]:(0.54395866394)\n",
      " state (4)  A[0]:(0.590184032917) A[1]:(0.656259179115) A[2]:(-9.19103622437e-05) A[3]:(0.531453371048)\n",
      " state (5)  A[0]:(-0.0277158096433) A[1]:(0.999896883965) A[2]:(-0.830999910831) A[3]:(0.676544308662)\n",
      " state (6)  A[0]:(0.000203773379326) A[1]:(0.810029864311) A[2]:(0.000160336494446) A[3]:(0.656474769115)\n",
      " state (7)  A[0]:(0.540114820004) A[1]:(-0.531015753746) A[2]:(0.443281471729) A[3]:(0.866832971573)\n",
      " state (8)  A[0]:(0.656444072723) A[1]:(-0.000451222033007) A[2]:(0.728999316692) A[3]:(0.591887593269)\n",
      " state (9)  A[0]:(0.656534135342) A[1]:(0.809994280338) A[2]:(0.809994399548) A[3]:(0.00135025300551)\n",
      " state (10)  A[0]:(0.729616820812) A[1]:(0.900004386902) A[2]:(-0.000171422958374) A[3]:(0.729685783386)\n",
      " state (11)  A[0]:(0.138912633061) A[1]:(0.883428037167) A[2]:(-0.933209896088) A[3]:(0.805083394051)\n",
      " state (12)  A[0]:(-0.426962137222) A[1]:(0.816560387611) A[2]:(-0.959693491459) A[3]:(0.716451108456)\n",
      " state (13)  A[0]:(-0.000327408313751) A[1]:(0.808790802956) A[2]:(0.900008678436) A[3]:(0.72975230217)\n",
      " state (14)  A[0]:(0.809775173664) A[1]:(0.900167286396) A[2]:(0.999999880791) A[3]:(0.810447692871)\n",
      " state (15)  A[0]:(0.979790627956) A[1]:(0.936924934387) A[2]:(1.0) A[3]:(0.878398358822)\n",
      "Episode 807000 finished after 0 timesteps with r=1.0. Running score: 0.99. Times trained:               6000. Times reached goal: 994.               Steps done: 5507455. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00365077970768.\n",
      " state (0)  A[0]:(0.531520485878) A[1]:(0.590310335159) A[2]:(0.590398788452) A[3]:(0.531433701515)\n",
      " state (1)  A[0]:(0.531531214714) A[1]:(7.36713409424e-05) A[2]:(0.656036973) A[3]:(0.590810954571)\n",
      " state (2)  A[0]:(0.590544700623) A[1]:(0.728856146336) A[2]:(0.590740323067) A[3]:(0.656389951706)\n",
      " state (3)  A[0]:(0.656374692917) A[1]:(-0.021005878225) A[2]:(0.533112347126) A[3]:(0.544449687004)\n",
      " state (4)  A[0]:(0.590489983559) A[1]:(0.656330347061) A[2]:(0.000205278396606) A[3]:(0.531961917877)\n",
      " state (5)  A[0]:(-0.0273586325347) A[1]:(0.999896883965) A[2]:(-0.83098757267) A[3]:(0.676779568195)\n",
      " state (6)  A[0]:(0.000367403001292) A[1]:(0.810109198093) A[2]:(-9.81092453003e-05) A[3]:(0.656340956688)\n",
      " state (7)  A[0]:(0.540030479431) A[1]:(-0.530782163143) A[2]:(0.442930549383) A[3]:(0.866598963737)\n",
      " state (8)  A[0]:(0.655864536762) A[1]:(-0.000166893005371) A[2]:(0.728990733624) A[3]:(0.590824604034)\n",
      " state (9)  A[0]:(0.655771493912) A[1]:(0.810004115105) A[2]:(0.810108661652) A[3]:(0.000192731618881)\n",
      " state (10)  A[0]:(0.729209780693) A[1]:(0.90006428957) A[2]:(0.000316858291626) A[3]:(0.72917342186)\n",
      " state (11)  A[0]:(0.138567060232) A[1]:(0.883636474609) A[2]:(-0.933169901371) A[3]:(0.80468159914)\n",
      " state (12)  A[0]:(-0.427154451609) A[1]:(0.817075312138) A[2]:(-0.959710836411) A[3]:(0.715797185898)\n",
      " state (13)  A[0]:(-0.000432699889643) A[1]:(0.809467017651) A[2]:(0.899967432022) A[3]:(0.729057073593)\n",
      " state (14)  A[0]:(0.809903502464) A[1]:(0.900591135025) A[2]:(0.999999880791) A[3]:(0.809939324856)\n",
      " state (15)  A[0]:(0.979830682278) A[1]:(0.937207043171) A[2]:(1.0) A[3]:(0.878068685532)\n",
      "Episode 808000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 997.               Steps done: 5513464. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00362890795192.\n",
      " state (0)  A[0]:(0.529595077038) A[1]:(0.59077501297) A[2]:(0.590264081955) A[3]:(0.529430031776)\n",
      " state (1)  A[0]:(0.530226945877) A[1]:(-0.00176100246608) A[2]:(0.655874669552) A[3]:(0.587817788124)\n",
      " state (2)  A[0]:(0.589546918869) A[1]:(0.728733479977) A[2]:(0.590235352516) A[3]:(0.653363823891)\n",
      " state (3)  A[0]:(0.655428051949) A[1]:(-0.0213312190026) A[2]:(0.532597899437) A[3]:(0.540036082268)\n",
      " state (4)  A[0]:(0.589478492737) A[1]:(0.656253516674) A[2]:(-7.58171081543e-05) A[3]:(0.526714682579)\n",
      " state (5)  A[0]:(-0.0282068811357) A[1]:(0.999896466732) A[2]:(-0.830486357212) A[3]:(0.671990871429)\n",
      " state (6)  A[0]:(-0.000151559710503) A[1]:(0.809501767159) A[2]:(0.00276326434687) A[3]:(0.65053653717)\n",
      " state (7)  A[0]:(0.539491295815) A[1]:(-0.529675126076) A[2]:(0.444440305233) A[3]:(0.863675653934)\n",
      " state (8)  A[0]:(0.654967427254) A[1]:(0.000557616294827) A[2]:(0.729130029678) A[3]:(0.581770658493)\n",
      " state (9)  A[0]:(0.65224802494) A[1]:(0.810096144676) A[2]:(0.809318184853) A[3]:(-0.0209492482245)\n",
      " state (10)  A[0]:(0.723373770714) A[1]:(0.900023341179) A[2]:(-0.00721871247515) A[3]:(0.716405272484)\n",
      " state (11)  A[0]:(0.126117557287) A[1]:(0.883501648903) A[2]:(-0.934416413307) A[3]:(0.795143961906)\n",
      " state (12)  A[0]:(-0.42995852232) A[1]:(0.816771626472) A[2]:(-0.960065543652) A[3]:(0.706104278564)\n",
      " state (13)  A[0]:(0.00527931330726) A[1]:(0.809065103531) A[2]:(0.900315403938) A[3]:(0.723130941391)\n",
      " state (14)  A[0]:(0.813231647015) A[1]:(0.90038305521) A[2]:(0.999999880791) A[3]:(0.807055830956)\n",
      " state (15)  A[0]:(0.980231463909) A[1]:(0.937129199505) A[2]:(1.0) A[3]:(0.876612782478)\n",
      "Episode 809000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6009. Times reached goal: 997.               Steps done: 5519473. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00360716722946.\n",
      "q_values \n",
      "tensor([[ 0.5317,  0.5906,  0.5907,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=2 , Random? False\n",
      "new state=1, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5317, -0.0001,  0.6561,  0.5902]], device='cuda:0')\n",
      "On state=1, selected action=2 , Random? False\n",
      "new state=2, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5907,  0.7290,  0.5905,  0.6560]], device='cuda:0')\n",
      "On state=2, selected action=1 , Random? False\n",
      "new state=6, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0002,  0.8101,  0.0002,  0.6558]], device='cuda:0')\n",
      "On state=6, selected action=1 , Random? False\n",
      "new state=10, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.7292,  0.9000, -0.0006,  0.7283]], device='cuda:0')\n",
      "On state=10, selected action=1 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8100,  0.9001,  1.0000,  0.8098]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.531438589096) A[1]:(0.590526938438) A[2]:(0.590727329254) A[3]:(0.531401157379)\n",
      " state (1)  A[0]:(0.531482040882) A[1]:(9.71108675003e-05) A[2]:(0.656102180481) A[3]:(0.589901804924)\n",
      " state (2)  A[0]:(0.59053426981) A[1]:(0.729017496109) A[2]:(0.590614318848) A[3]:(0.655795574188)\n",
      " state (3)  A[0]:(0.656376242638) A[1]:(-0.0207422282547) A[2]:(0.53293555975) A[3]:(0.543704032898)\n",
      " state (4)  A[0]:(0.590440511703) A[1]:(0.65641361475) A[2]:(-2.8133392334e-05) A[3]:(0.531121015549)\n",
      " state (5)  A[0]:(-0.0278010834008) A[1]:(0.999896883965) A[2]:(-0.831014156342) A[3]:(0.675954937935)\n",
      " state (6)  A[0]:(-0.000278368592262) A[1]:(0.810063064098) A[2]:(0.000427961320383) A[3]:(0.655354857445)\n",
      " state (7)  A[0]:(0.539498567581) A[1]:(-0.530475854874) A[2]:(0.443559378386) A[3]:(0.866095006466)\n",
      " state (8)  A[0]:(0.655501186848) A[1]:(0.000590160430875) A[2]:(0.729328155518) A[3]:(0.589091300964)\n",
      " state (9)  A[0]:(0.65562993288) A[1]:(0.810155153275) A[2]:(0.810073137283) A[3]:(-0.00224297866225)\n",
      " state (10)  A[0]:(0.728714466095) A[1]:(0.900042116642) A[2]:(-0.000836729828734) A[3]:(0.727744340897)\n",
      " state (11)  A[0]:(0.136921137571) A[1]:(0.883483588696) A[2]:(-0.933437049389) A[3]:(0.803456366062)\n",
      " state (12)  A[0]:(-0.42799115181) A[1]:(0.816661953926) A[2]:(-0.959847033024) A[3]:(0.714420676231)\n",
      " state (13)  A[0]:(-0.000594124139752) A[1]:(0.808898031712) A[2]:(0.900070309639) A[3]:(0.728228926659)\n",
      " state (14)  A[0]:(0.809933304787) A[1]:(0.900225758553) A[2]:(0.999999880791) A[3]:(0.809631943703)\n",
      " state (15)  A[0]:(0.979825854301) A[1]:(0.936915636063) A[2]:(1.0) A[3]:(0.878001213074)\n",
      "Episode 810000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6017. Times reached goal: 999.               Steps done: 5525490. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00358552807093.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.531365096569) A[1]:(0.59057533741) A[2]:(0.590486347675) A[3]:(0.531560480595)\n",
      " state (1)  A[0]:(0.531381368637) A[1]:(-4.32431697845e-05) A[2]:(0.656120181084) A[3]:(0.589760124683)\n",
      " state (2)  A[0]:(0.590307891369) A[1]:(0.729106783867) A[2]:(0.590464711189) A[3]:(0.655285358429)\n",
      " state (3)  A[0]:(0.6563590765) A[1]:(-0.0167629215866) A[2]:(0.532871842384) A[3]:(0.543590068817)\n",
      " state (4)  A[0]:(0.591075778008) A[1]:(0.656341195107) A[2]:(0.00121593417134) A[3]:(0.531120121479)\n",
      " state (5)  A[0]:(-0.0269421990961) A[1]:(0.99989682436) A[2]:(-0.830863118172) A[3]:(0.676292181015)\n",
      " state (6)  A[0]:(0.00108396960422) A[1]:(0.810198545456) A[2]:(0.000406980485423) A[3]:(0.65626347065)\n",
      " state (7)  A[0]:(0.540988922119) A[1]:(-0.530084013939) A[2]:(0.442926317453) A[3]:(0.866585373878)\n",
      " state (8)  A[0]:(0.657454967499) A[1]:(0.000522285641637) A[2]:(0.729028761387) A[3]:(0.590941369534)\n",
      " state (9)  A[0]:(0.65848171711) A[1]:(0.810114264488) A[2]:(0.810219287872) A[3]:(0.00115725351498)\n",
      " state (10)  A[0]:(0.731737971306) A[1]:(0.900024950504) A[2]:(0.000848531548399) A[3]:(0.729453742504)\n",
      " state (11)  A[0]:(0.143926590681) A[1]:(0.883485913277) A[2]:(-0.933175206184) A[3]:(0.804629206657)\n",
      " state (12)  A[0]:(-0.422722280025) A[1]:(0.816705346107) A[2]:(-0.959793329239) A[3]:(0.715493917465)\n",
      " state (13)  A[0]:(0.00497417524457) A[1]:(0.808999538422) A[2]:(0.899848937988) A[3]:(0.728793501854)\n",
      " state (14)  A[0]:(0.811689734459) A[1]:(0.900385379791) A[2]:(0.999999880791) A[3]:(0.809850931168)\n",
      " state (15)  A[0]:(0.980020880699) A[1]:(0.937125682831) A[2]:(1.0) A[3]:(0.878066539764)\n",
      "Episode 811000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6003. Times reached goal: 995.               Steps done: 5531493. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0035640686209.\n",
      " state (0)  A[0]:(0.53104019165) A[1]:(0.59042429924) A[2]:(0.590236663818) A[3]:(0.530921816826)\n",
      " state (1)  A[0]:(0.53126013279) A[1]:(-0.00125937093981) A[2]:(0.656095981598) A[3]:(0.590076327324)\n",
      " state (2)  A[0]:(0.590079307556) A[1]:(0.728947639465) A[2]:(0.589659690857) A[3]:(0.655712485313)\n",
      " state (3)  A[0]:(0.655912995338) A[1]:(-0.0128214936703) A[2]:(0.531699061394) A[3]:(0.544410467148)\n",
      " state (4)  A[0]:(0.59079760313) A[1]:(0.656042277813) A[2]:(0.000808358017821) A[3]:(0.531511902809)\n",
      " state (5)  A[0]:(-0.0277058929205) A[1]:(0.999896526337) A[2]:(-0.831107676029) A[3]:(0.676249325275)\n",
      " state (6)  A[0]:(0.000722780707292) A[1]:(0.809807538986) A[2]:(0.000407338113291) A[3]:(0.656638503075)\n",
      " state (7)  A[0]:(0.540552556515) A[1]:(-0.530684232712) A[2]:(0.442895740271) A[3]:(0.866672873497)\n",
      " state (8)  A[0]:(0.656427323818) A[1]:(-0.000302359461784) A[2]:(0.728924393654) A[3]:(0.590986251831)\n",
      " state (9)  A[0]:(0.656804978848) A[1]:(0.809842407703) A[2]:(0.809875965118) A[3]:(0.00201174337417)\n",
      " state (10)  A[0]:(0.729601681232) A[1]:(0.899882555008) A[2]:(-0.00117909850087) A[3]:(0.729588806629)\n",
      " state (11)  A[0]:(0.138373374939) A[1]:(0.883320748806) A[2]:(-0.933529913425) A[3]:(0.804498553276)\n",
      " state (12)  A[0]:(-0.426854997873) A[1]:(0.816452622414) A[2]:(-0.959949970245) A[3]:(0.715540468693)\n",
      " state (13)  A[0]:(0.00106431508902) A[1]:(0.808743357658) A[2]:(0.899885714054) A[3]:(0.729247808456)\n",
      " state (14)  A[0]:(0.810655295849) A[1]:(0.900228798389) A[2]:(0.999999880791) A[3]:(0.810433626175)\n",
      " state (15)  A[0]:(0.979932248592) A[1]:(0.936971187592) A[2]:(1.0) A[3]:(0.878599524498)\n",
      "Episode 812000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6013. Times reached goal: 997.               Steps done: 5537506. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00354270217887.\n",
      " state (0)  A[0]:(0.53129273653) A[1]:(0.590632021427) A[2]:(0.590547919273) A[3]:(0.531528711319)\n",
      " state (1)  A[0]:(0.531577110291) A[1]:(0.000122621655464) A[2]:(0.656154572964) A[3]:(0.590666770935)\n",
      " state (2)  A[0]:(0.590466499329) A[1]:(0.729120731354) A[2]:(0.5893445611) A[3]:(0.656194210052)\n",
      " state (3)  A[0]:(0.656042337418) A[1]:(-0.0104408496991) A[2]:(0.531173884869) A[3]:(0.545158863068)\n",
      " state (4)  A[0]:(0.590826392174) A[1]:(0.656267285347) A[2]:(0.000553727091756) A[3]:(0.53200417757)\n",
      " state (5)  A[0]:(-0.0285274442285) A[1]:(0.999896585941) A[2]:(-0.831301748753) A[3]:(0.676173090935)\n",
      " state (6)  A[0]:(0.000336572527885) A[1]:(0.810084223747) A[2]:(0.000373721093638) A[3]:(0.656648755074)\n",
      " state (7)  A[0]:(0.540362358093) A[1]:(-0.5300822258) A[2]:(0.443095326424) A[3]:(0.86665391922)\n",
      " state (8)  A[0]:(0.65591865778) A[1]:(0.000752523425035) A[2]:(0.729165256023) A[3]:(0.590473651886)\n",
      " state (9)  A[0]:(0.656225562096) A[1]:(0.810169816017) A[2]:(0.810031533241) A[3]:(0.00160381058231)\n",
      " state (10)  A[0]:(0.72919690609) A[1]:(0.899991154671) A[2]:(-0.000787615601439) A[3]:(0.729466795921)\n",
      " state (11)  A[0]:(0.13776011765) A[1]:(0.883342206478) A[2]:(-0.933496892452) A[3]:(0.804324388504)\n",
      " state (12)  A[0]:(-0.427061468363) A[1]:(0.816347181797) A[2]:(-0.959929406643) A[3]:(0.715243935585)\n",
      " state (13)  A[0]:(0.000807180826087) A[1]:(0.808562397957) A[2]:(0.90003657341) A[3]:(0.728928208351)\n",
      " state (14)  A[0]:(0.810346245766) A[1]:(0.900146722794) A[2]:(0.999999880791) A[3]:(0.810140490532)\n",
      " state (15)  A[0]:(0.979863584042) A[1]:(0.936946213245) A[2]:(1.0) A[3]:(0.878368079662)\n",
      "Episode 813000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6008. Times reached goal: 996.               Steps done: 5543514. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00352148143513.\n",
      " state (0)  A[0]:(0.531725525856) A[1]:(0.590571165085) A[2]:(0.590585827827) A[3]:(0.531714677811)\n",
      " state (1)  A[0]:(0.530550181866) A[1]:(-0.000105127692223) A[2]:(0.656055569649) A[3]:(0.590751409531)\n",
      " state (2)  A[0]:(0.589242219925) A[1]:(0.728895545006) A[2]:(0.589025855064) A[3]:(0.65621650219)\n",
      " state (3)  A[0]:(0.65486317873) A[1]:(-0.00904510077089) A[2]:(0.53057384491) A[3]:(0.545289933681)\n",
      " state (4)  A[0]:(0.589652061462) A[1]:(0.655881524086) A[2]:(-2.39610671997e-05) A[3]:(0.53172314167)\n",
      " state (5)  A[0]:(-0.0299753583968) A[1]:(0.999896466732) A[2]:(-0.831749975681) A[3]:(0.675202488899)\n",
      " state (6)  A[0]:(0.000165909528732) A[1]:(0.810012996197) A[2]:(-0.000649571302347) A[3]:(0.655653655529)\n",
      " state (7)  A[0]:(0.540422379971) A[1]:(-0.530718564987) A[2]:(0.442651361227) A[3]:(0.866176724434)\n",
      " state (8)  A[0]:(0.655519366264) A[1]:(0.000424161524279) A[2]:(0.729132056236) A[3]:(0.588873386383)\n",
      " state (9)  A[0]:(0.656357526779) A[1]:(0.810096025467) A[2]:(0.810107707977) A[3]:(0.00189727311954)\n",
      " state (10)  A[0]:(0.730125904083) A[1]:(0.899993777275) A[2]:(0.000631809176411) A[3]:(0.730751156807)\n",
      " state (11)  A[0]:(0.140198245645) A[1]:(0.883461833) A[2]:(-0.933264434338) A[3]:(0.805622577667)\n",
      " state (12)  A[0]:(-0.42648100853) A[1]:(0.816715836525) A[2]:(-0.959899067879) A[3]:(0.716622591019)\n",
      " state (13)  A[0]:(-0.000240668654442) A[1]:(0.80913233757) A[2]:(0.899930417538) A[3]:(0.729691982269)\n",
      " state (14)  A[0]:(0.809810936451) A[1]:(0.900551259518) A[2]:(0.999999880791) A[3]:(0.810440301895)\n",
      " state (15)  A[0]:(0.979815721512) A[1]:(0.937229990959) A[2]:(1.0) A[3]:(0.878494143486)\n",
      "Episode 814000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6013. Times reached goal: 996.               Steps done: 5549527. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.00350037030149.\n",
      "q_values \n",
      "tensor([[ 0.5310,  0.5906,  0.5904,  0.5316]], device='cuda:0')\n",
      "On state=0, selected action=1 , Random? False\n",
      "new state=4, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.5901,  0.6560,  0.0002,  0.5317]], device='cuda:0')\n",
      "On state=4, selected action=1 , Random? False\n",
      "new state=8, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6566,  0.0007,  0.7292,  0.5909]], device='cuda:0')\n",
      "On state=8, selected action=2 , Random? False\n",
      "new state=9, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.6583,  0.8103,  0.8102,  0.0033]], device='cuda:0')\n",
      "On state=9, selected action=1 , Random? False\n",
      "new state=13, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.0080,  0.8081,  0.8999,  0.7296]], device='cuda:0')\n",
      "On state=13, selected action=2 , Random? False\n",
      "new state=14, done=False. Reward: 0.0\n",
      "q_values \n",
      "tensor([[ 0.8134,  0.9000,  1.0000,  0.8104]], device='cuda:0')\n",
      "On state=14, selected action=2 , Random? False\n",
      "new state=15, done=True. Reward: 1.0\n",
      " state (0)  A[0]:(0.529831171036) A[1]:(0.590525507927) A[2]:(0.590591132641) A[3]:(0.531487703323)\n",
      " state (1)  A[0]:(0.530636191368) A[1]:(0.000122338533401) A[2]:(0.656131982803) A[3]:(0.590600609779)\n",
      " state (2)  A[0]:(0.58987557888) A[1]:(0.729104936123) A[2]:(0.589106559753) A[3]:(0.656041502953)\n",
      " state (3)  A[0]:(0.655385911465) A[1]:(-0.00709161208943) A[2]:(0.530545592308) A[3]:(0.545121669769)\n",
      " state (4)  A[0]:(0.590177834034) A[1]:(0.656448602676) A[2]:(0.000185012817383) A[3]:(0.531558871269)\n",
      " state (5)  A[0]:(-0.029994726181) A[1]:(0.999896526337) A[2]:(-0.831681132317) A[3]:(0.675430417061)\n",
      " state (6)  A[0]:(0.000888734823093) A[1]:(0.81015175581) A[2]:(5.69820404053e-05) A[3]:(0.656562209129)\n",
      " state (7)  A[0]:(0.541903376579) A[1]:(-0.530003190041) A[2]:(0.44299608469) A[3]:(0.866878986359)\n",
      " state (8)  A[0]:(0.658393621445) A[1]:(0.000575095356908) A[2]:(0.729067921638) A[3]:(0.592094898224)\n",
      " state (9)  A[0]:(0.660022974014) A[1]:(0.810229599476) A[2]:(0.810258626938) A[3]:(0.00460904557258)\n",
      " state (10)  A[0]:(0.7335537076) A[1]:(0.899978339672) A[2]:(0.0012984268833) A[3]:(0.731352210045)\n",
      " state (11)  A[0]:(0.148620337248) A[1]:(0.88319593668) A[2]:(-0.933195590973) A[3]:(0.805813848972)\n",
      " state (12)  A[0]:(-0.418086886406) A[1]:(0.815928220749) A[2]:(-0.959854781628) A[3]:(0.71676659584)\n",
      " state (13)  A[0]:(0.0112570235506) A[1]:(0.808031618595) A[2]:(0.900093793869) A[3]:(0.729819357395)\n",
      " state (14)  A[0]:(0.813848316669) A[1]:(0.899908602238) A[2]:(0.999999880791) A[3]:(0.810483574867)\n",
      " state (15)  A[0]:(0.98026150465) A[1]:(0.936854958534) A[2]:(1.0) A[3]:(0.878470718861)\n",
      "Episode 815000 finished after 0 timesteps with r=1.0. Running score: 1.0. Times trained:               6017. Times reached goal: 999.               Steps done: 5555544. EPS_DECAY: 1000000. EPS_THRESHOLD: 0.0034793718107.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-90b7818e16c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mtimes_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimes_trained\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-20ca5710fb78>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mnext_state_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m#action_batch = torch.cat([torch.LongTensor([[a.item()]]) for a in batch.action])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0maction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;31m#reward_batch = torch.cat([torch.tensor([r]) for r in batch.reward])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mreward_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "# TO BE DELETED_______\n",
    "#steps_done = 40000000\n",
    "#EPS_END = 0.0\n",
    "\n",
    "# TO BE DELETED_______\n",
    "\n",
    "\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    episode_series = []\n",
    "    reward = 0\n",
    "    episode_step = 0\n",
    "    while not done:\n",
    "        # Get action from pi\n",
    "        # action = env.action_space.sample()\n",
    "        np_observation = np.array(observation)\n",
    "        #np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        observation_tensor = FloatTensor(np_observation)\n",
    "        #print(observation_tensor)\n",
    "        #net.eval()\n",
    "        #print(\"before eval\")\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        steps_done += 1\n",
    "        q_values = online_net(observation_tensor)\n",
    "        if sample >= eps_threshold: \n",
    "            #print \"observation_tensor\"\n",
    "            #print observation_tensor.type()\n",
    "            \n",
    "            action = q_values.max(1)[1] # First 1 is the dimension, second 1 is the index (this is argmax)\n",
    "        else:\n",
    "            action = torch.LongTensor([[random.randint(0,3)]], device=device)\n",
    "        \n",
    "            \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        old_state = observation                    \n",
    "            \n",
    "        \n",
    "        observation, reward, done, info = env.step(action.item()) \n",
    "        new_state = observation\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#         if done and reward != 1.0:\n",
    "#             if episode_step > 50:\n",
    "#                 reward = -0.2\n",
    "#             if episode_step > 80:\n",
    "#                 reward = -0.5\n",
    "#             if new_state in [5,7,11,12]:\n",
    "#                 reward = -1.0\n",
    "\n",
    "        \n",
    "        \n",
    "        # Store the transition in memory\n",
    "        #if old_state != new_state:\n",
    "\n",
    "        memory.push(old_state, action, new_state, reward, done)\n",
    "        if k%5000 == 0:\n",
    "            #print(\"old_state != new_state\")\n",
    "            #print(old_state != new_state)\n",
    "            #print(\"oldstate \" + str(old_state) + \" newstate \" + str(new_state))\n",
    "            print(\"q_values \")\n",
    "            print(q_values)\n",
    "            print(\"On state=\"+ str(old_state) + \", selected action=\" + str(action.item()) + \" , \" + \\\n",
    "              \"Random? \" + str( sample < eps_threshold ))\n",
    "            print(\"new state=\"+ str(new_state) + \", done=\"+str(done) + \\\n",
    "             \". Reward: \" + str(reward))\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        if k > BATCH_SIZE :\n",
    "            optimize_model()\n",
    "            times_trained = times_trained + 1\n",
    "\n",
    "        episode_step += 1\n",
    "        #env.render()\n",
    "    if k % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "    \n",
    "    if k % 1000 ==0:\n",
    "        print_q_table()\n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "\n",
    "    if k%1000 == 0:\n",
    "        print(\"Episode {} finished after {} timesteps with r={}. Running score: {}. Times trained: \\\n",
    "              {}. Times reached goal: {}. \\\n",
    "              Steps done: {}. EPS_DECAY: {}. EPS_THRESHOLD: {}.\".format(k, len(episode_series), \\\n",
    "                                                                    reward, np.mean(score), times_trained, \\\n",
    "                                                                       times_reach_goal, steps_done, \\\n",
    "                                                                       EPS_DECAY, eps_threshold))\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "        #print(\"Game finished. \" + \"-\" * 5)\n",
    "        #print(len(episode_series))\n",
    "#         for param in net.parameters():\n",
    "#             print(param.data)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if reward > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state (0)  A[0]:(0.0467716865242) A[1]:(0.0158739089966) A[2]:(-0.015961188823) A[3]:(0.0109659750015)\n",
      " state (1)  A[0]:(0.0459862612188) A[1]:(0.0138985225931) A[2]:(-0.0122543023899) A[3]:(0.0107089262456)\n",
      " state (2)  A[0]:(0.045201510191) A[1]:(0.0119225373492) A[2]:(-0.00854972098023) A[3]:(0.0104518923908)\n",
      " state (3)  A[0]:(0.0444191098213) A[1]:(0.00994489248842) A[2]:(-0.00485114799812) A[3]:(0.0101966680959)\n",
      " state (4)  A[0]:(0.0436407215893) A[1]:(0.00796458125114) A[2]:(-0.00116223143414) A[3]:(0.00994502007961)\n",
      " state (5)  A[0]:(0.0428679808974) A[1]:(0.00598067371175) A[2]:(0.00251346104778) A[3]:(0.00969867687672)\n",
      " state (6)  A[0]:(0.0421024933457) A[1]:(0.0039923498407) A[2]:(0.0061724819243) A[3]:(0.00945930648595)\n",
      " state (7)  A[0]:(0.0413458012044) A[1]:(0.00199890718795) A[2]:(0.00981149822474) A[3]:(0.00922849774361)\n",
      " state (8)  A[0]:(0.0405994020402) A[1]:(-2.03028321266e-07) A[2]:(0.0134273516014) A[3]:(0.00900775659829)\n",
      " state (9)  A[0]:(0.0398647263646) A[1]:(-0.00200537685305) A[2]:(0.0170170515776) A[3]:(0.00879848189652)\n",
      " state (10)  A[0]:(0.0391431190073) A[1]:(-0.00401683431119) A[2]:(0.020577788353) A[3]:(0.00860196724534)\n",
      " state (11)  A[0]:(0.0384358540177) A[1]:(-0.00603463267908) A[2]:(0.0241069570184) A[3]:(0.00841938331723)\n",
      " state (12)  A[0]:(0.0377441160381) A[1]:(-0.00805865135044) A[2]:(0.0276021603495) A[3]:(0.00825177785009)\n",
      " state (13)  A[0]:(0.0370689891279) A[1]:(-0.0100885927677) A[2]:(0.0310611948371) A[3]:(0.00810006540269)\n",
      " state (14)  A[0]:(0.0364114679396) A[1]:(-0.0121240094304) A[2]:(0.0344820953906) A[3]:(0.00796503666788)\n",
      " state (15)  A[0]:(0.035772446543) A[1]:(-0.0141642903909) A[2]:(0.0378630720079) A[3]:(0.00784734543413)\n",
      " state (0)  A[0]:(0.0467716865242) A[1]:(0.0158739089966) A[2]:(-0.015961188823) A[3]:(0.0109659750015)\n",
      " state (1)  A[0]:(0.0459862612188) A[1]:(0.0138985225931) A[2]:(-0.0122543023899) A[3]:(0.0107089262456)\n",
      " state (2)  A[0]:(0.045201510191) A[1]:(0.0119225373492) A[2]:(-0.00854972098023) A[3]:(0.0104518923908)\n",
      " state (3)  A[0]:(0.0444191098213) A[1]:(0.00994489248842) A[2]:(-0.00485114799812) A[3]:(0.0101966680959)\n",
      " state (4)  A[0]:(0.0436407215893) A[1]:(0.00796458125114) A[2]:(-0.00116223143414) A[3]:(0.00994502007961)\n",
      " state (5)  A[0]:(0.0428679808974) A[1]:(0.00598067371175) A[2]:(0.00251346104778) A[3]:(0.00969867687672)\n",
      " state (6)  A[0]:(0.0421024933457) A[1]:(0.0039923498407) A[2]:(0.0061724819243) A[3]:(0.00945930648595)\n",
      " state (7)  A[0]:(0.0413458012044) A[1]:(0.00199890718795) A[2]:(0.00981149822474) A[3]:(0.00922849774361)\n",
      " state (8)  A[0]:(0.0405994020402) A[1]:(-2.03028321266e-07) A[2]:(0.0134273516014) A[3]:(0.00900775659829)\n",
      " state (9)  A[0]:(0.0398647263646) A[1]:(-0.00200537685305) A[2]:(0.0170170515776) A[3]:(0.00879848189652)\n",
      " state (10)  A[0]:(0.0391431190073) A[1]:(-0.00401683431119) A[2]:(0.020577788353) A[3]:(0.00860196724534)\n",
      " state (11)  A[0]:(0.0384358540177) A[1]:(-0.00603463267908) A[2]:(0.0241069570184) A[3]:(0.00841938331723)\n",
      " state (12)  A[0]:(0.0377441160381) A[1]:(-0.00805865135044) A[2]:(0.0276021603495) A[3]:(0.00825177785009)\n",
      " state (13)  A[0]:(0.0370689891279) A[1]:(-0.0100885927677) A[2]:(0.0310611948371) A[3]:(0.00810006540269)\n",
      " state (14)  A[0]:(0.0364114679396) A[1]:(-0.0121240094304) A[2]:(0.0344820953906) A[3]:(0.00796503666788)\n",
      " state (15)  A[0]:(0.035772446543) A[1]:(-0.0141642903909) A[2]:(0.0378630720079) A[3]:(0.00784734543413)\n"
     ]
    }
   ],
   "source": [
    "def print_q_table():\n",
    "    for i in range(16):\n",
    "        st = np.array(i)\n",
    "        st = np.expand_dims(st, axis=0)\n",
    "        q_vals = online_net(FloatTensor(st))\n",
    "        outp = \" state (\" +str(i) + \") \"\n",
    "        n = 0\n",
    "        for tensr in q_vals:\n",
    "            for cell in tensr:\n",
    "                outp = outp + \" A[\" + str(n) + \"]:(\" + str(cell.item()) + \")\"\n",
    "                n += 1\n",
    "        print(outp)\n",
    "        \n",
    "print_q_table()\n",
    "print_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
