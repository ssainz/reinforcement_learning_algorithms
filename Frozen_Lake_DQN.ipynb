{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import gym\n",
    "\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "   id='FrozenLakeNotSlippery-v0',\n",
    "   entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "   kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "   max_episode_steps=100,\n",
    "   reward_threshold=0.78, # optimum = .8196\n",
    ")\n",
    "\n",
    "#env = gym.make('FrozenLake8x8-v0')\n",
    "#env = gym.make('FrozenLake-v0')\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.render()\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "class q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(q_net, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 20)\n",
    "        self.linear2 = nn.Linear(20, 4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "#         print(x.shape)\n",
    "#         print(x)\n",
    "#         print(\"Q_Net: Input \" + \"-\" *5)\n",
    "        x = x.view(-1,1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        #x = F.softmax(self.linear2(x), dim=0)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = x.view(-1,4)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "import math\n",
    "\n",
    "# custom weights initialization \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        #m.weight.data.normal_(0.0, 0.02)\n",
    "        #m.weight.data.uniform_(0.0, 0.02)\n",
    "        m.weight.data.fill_(0.5)\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mse = nn.MSELoss()\n",
    "NUM_EPISODES = 1000000\n",
    "BATCH_SIZE = 1000\n",
    "GAMMA = 0.9\n",
    "TARGET_UPDATE = 1000\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.00\n",
    "EPS_DECAY = 100000\n",
    "online_net = q_net().to(device)\n",
    "online_net.apply(weights_init)\n",
    "target_net = q_net().to(device)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "optimizer = optim.RMSprop(online_net.parameters(), lr=0.01)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda d: d is False,\n",
    "                                          batch.done)), device=device, dtype=torch.bool).unsqueeze(1)\n",
    "    #final_mask = torch.tensor(tuple(map(lambda d: d is False,\n",
    "    #                                      batch.done)), device=device, dtype=torch.uint8).unsqueeze(1)\n",
    "    \n",
    "    non_final_next_states = torch.cat([FloatTensor([s]) for s,d in zip(batch.next_state,batch.done)\n",
    "                                                if d is False])\n",
    "    \n",
    "    #state_batch = torch.cat([torch.FloatTensor([s]) for s in batch.state])\n",
    "    state_batch = FloatTensor(batch.state)\n",
    "    state_batch = state_batch.view(BATCH_SIZE, 1)\n",
    "    #action_batch = torch.cat([torch.LongTensor([[a.item()]]) for a in batch.action])\n",
    "    action_batch = LongTensor(batch.action).view(BATCH_SIZE,1)\n",
    "    #reward_batch = torch.cat([torch.tensor([r]) for r in batch.reward])\n",
    "    reward_batch = Tensor(batch.reward).view(BATCH_SIZE,1)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "#     print(\"state_batch \"+\"-\" * 10)\n",
    "#     print(state_batch.shape)\n",
    "#     print(\"action_batch \"+\"-\" * 10)\n",
    "#     print(action_batch.shape)\n",
    "    state_action_values = online_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device).view(BATCH_SIZE,1)\n",
    "#     print(\"non_final_mask\")\n",
    "#     print(non_final_mask.shape)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "#     print(\"next_state_values\")\n",
    "#     print(next_state_values.shape)\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "#     print(\"expected_state_action_values\")\n",
    "#     print(expected_state_action_values.shape)\n",
    "\n",
    "    # Compute Huber loss (this is like MSE , but less sensitive to outliers )\n",
    "    # loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in online_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_values \n",
      "tensor([[2.5455, 2.2331, 2.3225, 2.4486]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=0, selected action=2.0 , \n",
      "new state=1, done=False\n",
      "q_values \n",
      "tensor([[5.8892, 5.5768, 5.6661, 5.7923]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=1, selected action=0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "tensor([[2.5455, 2.2331, 2.3225, 2.4486]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=0, selected action=2.0 , \n",
      "new state=1, done=False\n",
      "q_values \n",
      "tensor([[5.8892, 5.5768, 5.6661, 5.7923]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=1, selected action=0.0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "tensor([[2.5455, 2.2331, 2.3225, 2.4486]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=0, selected action=3.0 , \n",
      "new state=0, done=False\n",
      "q_values \n",
      "tensor([[2.5455, 2.2331, 2.3225, 2.4486]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=0, selected action=2.0 , \n",
      "new state=1, done=False\n",
      "q_values \n",
      "tensor([[5.8892, 5.5768, 5.6661, 5.7923]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=1, selected action=1.0 , \n",
      "new state=5, done=True\n",
      "Episode 0 finished after 0 timesteps with r=0.0. Running score: 0.0. Times trained: 0. Times reached goal: 0.\n",
      "Episode 1000 finished after 0 timesteps with r=0.0. Running score: 0.02. Times trained: 0. Times reached goal: 9.\n",
      "Episode 2000 finished after 0 timesteps with r=0.0. Running score: 0.07. Times trained: 6641. Times reached goal: 40.\n",
      "Episode 3000 finished after 0 timesteps with r=0.0. Running score: 0.03. Times trained: 7245. Times reached goal: 46.\n",
      "Episode 4000 finished after 0 timesteps with r=0.0. Running score: 0.06. Times trained: 8108. Times reached goal: 37.\n",
      "Episode 5000 finished after 0 timesteps with r=0.0. Running score: 0.01. Times trained: 8425. Times reached goal: 20.\n",
      "Episode 6000 finished after 0 timesteps with r=0.0. Running score: 0.05. Times trained: 8941. Times reached goal: 34.\n",
      "Episode 7000 finished after 0 timesteps with r=0.0. Running score: 0.09. Times trained: 10345. Times reached goal: 81.\n",
      "Episode 8000 finished after 0 timesteps with r=0.0. Running score: 0.11. Times trained: 19181. Times reached goal: 126.\n",
      "Episode 9000 finished after 0 timesteps with r=0.0. Running score: 0.19. Times trained: 12501. Times reached goal: 190.\n",
      "q_values \n",
      "tensor([[25.2356, 31.2690, 24.6284, 24.7401]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=0, selected action=1 , \n",
      "new state=4, done=False\n",
      "q_values \n",
      "tensor([[30.7015, 37.7368,  0.0000, 25.3931]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=4, selected action=1 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "tensor([[37.8320, 17.2854, 37.2850, 28.3655]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=8, selected action=0.0 , \n",
      "new state=8, done=False\n",
      "q_values \n",
      "tensor([[39.5100,  9.9552, 39.9013, 30.1986]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=8, selected action=2 , \n",
      "new state=9, done=False\n",
      "q_values \n",
      "tensor([[40.5618, 40.8690, 49.1401,  0.0000]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "On state=9, selected action=3.0 , \n",
      "new state=5, done=True\n",
      "Episode 10000 finished after 0 timesteps with r=0.0. Running score: 0.24. Times trained: 13481. Times reached goal: 191.\n",
      "Episode 11000 finished after 0 timesteps with r=0.0. Running score: 0.14. Times trained: 12988. Times reached goal: 125.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fb96a2b7e4f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mtimes_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimes_trained\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0c6e8d254e3c>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mstate_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m#action_batch = torch.cat([torch.LongTensor([[a.item()]]) for a in batch.action])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0maction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;31m#reward_batch = torch.cat([torch.tensor([r]) for r in batch.reward])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mreward_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score = []\n",
    "times_trained = 0\n",
    "times_reach_goal = 0\n",
    "\n",
    "steps_done = 0\n",
    "for k in range(NUM_EPISODES):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    episode_series = []\n",
    "    reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Get action from pi\n",
    "        # action = env.action_space.sample()\n",
    "        np_observation = np.array(observation)\n",
    "        #np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        np_observation = np.expand_dims(np_observation, axis=0)\n",
    "        observation_tensor = FloatTensor(np_observation)\n",
    "        #print(observation_tensor)\n",
    "        #net.eval()\n",
    "        #print(\"before eval\")\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        steps_done += 1\n",
    "        q_values = online_net(observation_tensor)\n",
    "        if sample >= eps_threshold: \n",
    "            #print \"observation_tensor\"\n",
    "            #print observation_tensor.type()\n",
    "            \n",
    "            action = q_values.max(1)[1] # First 1 is the dimension, second 1 is the index (this is argmax)\n",
    "        else:\n",
    "            action = FloatTensor([[random.randrange(4)]])\n",
    "        \n",
    "            \n",
    "        #break\n",
    "        # Execute action in environment.\n",
    "        old_state = observation\n",
    "        if k%10000 == 0:\n",
    "            print(\"q_values \")\n",
    "            print(q_values)\n",
    "            print(\"On state=\"+ str(observation) + \", selected action=\" + str(action.item()) + \" , \")\n",
    "        \n",
    "        observation, reward, done, info = env.step(action.item()) \n",
    "        new_state = observation\n",
    "        \n",
    "        # Store the transition in memory\n",
    "        memory.push(old_state, action, new_state, reward, done)\n",
    "\n",
    "        \n",
    "        if k%10000 == 0:\n",
    "            print(\"new state=\"+ str(observation) + \", done=\"+str(done))\n",
    "        #if done and reward != 1.0:\n",
    "        #    reward = -1.0\n",
    "\n",
    "        \n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        if k > BATCH_SIZE :\n",
    "            optimize_model()\n",
    "            times_trained = times_trained + 1\n",
    "        #env.render()\n",
    "    if k % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "        \n",
    "   \n",
    "    if len(score) < 100:\n",
    "        score.append(reward)\n",
    "    else:\n",
    "        score[k % 100] = reward\n",
    "\n",
    "    if k%1000 == 0:\n",
    "        print(\"Episode {} finished after {} timesteps with r={}. Running score: {}. Times trained: {}. Times reached goal: {}.\".format(k, len(episode_series), reward, np.mean(score), times_trained, times_reach_goal))\n",
    "        times_trained = 0\n",
    "        times_reach_goal = 0\n",
    "        #print(\"Game finished. \" + \"-\" * 5)\n",
    "        #print(len(episode_series))\n",
    "#         for param in net.parameters():\n",
    "#             print(param.data)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if reward > 0.0:\n",
    "        times_reach_goal = times_reach_goal + 1\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
